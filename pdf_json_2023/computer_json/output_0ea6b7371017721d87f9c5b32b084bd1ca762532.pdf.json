{
    "abstractText": "Answering multi-hop questions over hybrid factual knowledge from the given text and table (TextTableQA) is a challenging task. Existing models mainly adopt a retriever-reader framework, which have several deficiencies, such as noisy labeling in training retriever, insufficient utilization of heterogeneous information over text and table, and deficient ability for different reasoning operations. In this paper, we propose a three-stage TextTableQA framework SHQA, which comprises of retriever, selector, and reasoner. We use a retriever with refinement training to solve the noisy labeling problem. Then, a hybrid selector considers the linked relationships between heterogeneous data to select the most relevant factual knowledge. For the final stage, instead of adapting a reading comprehension module like in previous methods, we employ a generation-based reasoner to obtain answers. This includes two approaches: a row-wise generator and an LLM prompting generator (first time used in this task). The experimental results demonstrate that our method achieves competitive results in the few-shot setting. When trained on the full dataset, our approach outperforms all baseline methods, ranking first on the HybridQA leaderboard.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Fangyu Lei"
        },
        {
            "affiliations": [],
            "name": "Xiang Li"
        },
        {
            "affiliations": [],
            "name": "Yifan Wei"
        },
        {
            "affiliations": [],
            "name": "Shizhu He"
        },
        {
            "affiliations": [],
            "name": "Yiming Huang"
        },
        {
            "affiliations": [],
            "name": "Jun Zhao"
        },
        {
            "affiliations": [],
            "name": "Kang Liu"
        }
    ],
    "id": "SP:48c04c343d6ced2d7d19d787d131f0e131e6c577",
    "references": [
        {
            "authors": [
                "Steven Bird."
            ],
            "title": "Nltk: the natural language toolkit",
            "venue": "Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 69\u201372.",
            "year": 2006
        },
        {
            "authors": [
                "Wenhu Chen",
                "Ming-Wei Chang",
                "Eva Schlinger",
                "William Yang Wang",
                "William W Cohen."
            ],
            "title": "Open question answering over tables and text",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Wenhu Chen",
                "Hanwen Zha",
                "Zhiyu Chen",
                "Wenhan Xiong",
                "Hong Wang",
                "William Yang Wang."
            ],
            "title": "Hybridqa: A dataset of multi-hop question answering over tabular and textual data",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP",
            "year": 2020
        },
        {
            "authors": [
                "Zhiyu Chen",
                "Wenhu Chen",
                "Charese Smiley",
                "Sameena Shah",
                "Iana Borova",
                "Dylan Langdon",
                "Reema Moussa",
                "Matt Beane",
                "Ting-Hao Huang",
                "Bryan R Routledge"
            ],
            "title": "Finqa: A dataset of numerical reasoning over financial data",
            "year": 2021
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Qingxiu Dong",
                "Lei Li",
                "Damai Dai",
                "Ce Zheng",
                "Zhiyong Wu",
                "Baobao Chang",
                "Xu Sun",
                "Jingjing Xu",
                "Zhifang Sui."
            ],
            "title": "A survey for in-context learning",
            "venue": "arXiv preprint arXiv:2301.00234.",
            "year": 2022
        },
        {
            "authors": [
                "Julian Eisenschlos",
                "Maharshi Gor",
                "Thomas Mueller",
                "William Cohen."
            ],
            "title": "Mate: Multi-view attention for table transformer efficiency",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7606\u20137619.",
            "year": 2021
        },
        {
            "authors": [
                "Yue Feng",
                "Zhen Han",
                "Mingming Sun",
                "Ping Li."
            ],
            "title": "Multi-hop open-domain question answering over structured and unstructured knowledge",
            "venue": "Findings of the Association for Computational Linguistics: NAACL 2022, pages 151\u2013156.",
            "year": 2022
        },
        {
            "authors": [
                "Pengcheng He",
                "Xiaodong Liu",
                "Jianfeng Gao",
                "Weizhu Chen."
            ],
            "title": "Deberta: Decoding-enhanced bert with disentangled attention",
            "venue": "arXiv preprint arXiv:2006.03654.",
            "year": 2020
        },
        {
            "authors": [
                "Vishwajeet Kumar",
                "Saneem Chemmengath",
                "Yash Gupta",
                "Jaydeep Sen",
                "Samarth Bharadwaj",
                "Soumen Chakrabarti."
            ],
            "title": "Multi-instance training for question answering across table and linked text",
            "venue": "arXiv preprint arXiv:2112.07337.",
            "year": 2021
        },
        {
            "authors": [
                "Sung-Min Lee",
                "Eunhwan Park",
                "Daeryong Seo",
                "Donghyeon Jeon",
                "Inho Kang",
                "Seung-Hoon Na."
            ],
            "title": "Mafid: Moving average equipped fusion-indecoder for question answering over tabular and textual data",
            "venue": "Findings of the Association for Compu-",
            "year": 2023
        },
        {
            "authors": [
                "Fangyu Lei",
                "Shizhu He",
                "Xiang Li",
                "Jun Zhao",
                "Kang Liu."
            ],
            "title": "Answering numerical reasoning questions in table-text hybrid contents with graph-based encoder and tree-based decoder",
            "venue": "Proceedings of the 29th International Conference on Computational",
            "year": 2022
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "Bart: Denoising sequence-to-sequence pretraining for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Xiao Li",
                "Yin Zhu",
                "Sichen Liu",
                "Jiangzhou Ju",
                "Yuzhong Qu",
                "Gong Cheng."
            ],
            "title": "Dyrren: A dynamic retriever-reranker-generator model for numerical reasoning over tabular and textual data",
            "venue": "arXiv preprint arXiv:2211.12668.",
            "year": 2022
        },
        {
            "authors": [
                "Liangming Pan",
                "Wenhu Chen",
                "Wenhan Xiong",
                "MinYen Kan",
                "William Yang Wang."
            ],
            "title": "Unsupervised multi-hop question answering by question generation",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for",
            "year": 2021
        },
        {
            "authors": [
                "Panupong Pasupat",
                "Percy Liang."
            ],
            "title": "Compositional semantic parsing on semi-structured tables",
            "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language",
            "year": 2015
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga"
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning",
            "year": 2019
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Jian Zhang",
                "Konstantin Lopyrev",
                "Percy Liang."
            ],
            "title": "Squad: 100,000+ questions for machine comprehension of text",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383\u20132392.",
            "year": 2016
        },
        {
            "authors": [
                "Haitian Sun",
                "William W Cohen",
                "Ruslan Salakhutdinov"
            ],
            "title": "End-to-end multihop retrieval for compositional question answering over long documents",
            "year": 2021
        },
        {
            "authors": [
                "Dingzirui Wang",
                "Longxu Dou",
                "Wanxiang Che."
            ],
            "title": "A survey on table-and-text hybridqa: Concepts, methods, challenges and future directions",
            "venue": "arXiv preprint arXiv:2212.13465.",
            "year": 2022
        },
        {
            "authors": [
                "Yingyao Wang",
                "Junwei Bao",
                "Chaoqun Duan",
                "Youzheng Wu",
                "Xiaodong He",
                "Tiejun Zhao."
            ],
            "title": "MuGER2: Multi-granularity evidence retrieval and reasoning for hybrid question answering",
            "venue": "Findings of the Association for Computational Linguis-",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Ed Chi",
                "Quoc Le",
                "Denny Zhou."
            ],
            "title": "Chain of thought prompting elicits reasoning in large language models",
            "venue": "arXiv preprint arXiv:2201.11903.",
            "year": 2022
        },
        {
            "authors": [
                "Yifan Wei",
                "Fangyu Lei",
                "Yuanzhe Zhang",
                "Jun Zhao",
                "Kang Liu."
            ],
            "title": "Multi-view graph representation learning for answering hybrid numerical reasoning question",
            "venue": "arXiv preprint arXiv:2305.03458.",
            "year": 2023
        },
        {
            "authors": [
                "Tao Yu",
                "Rui Zhang",
                "Kai Yang",
                "Michihiro Yasunaga",
                "Dongxu Wang",
                "Zifan Li",
                "James Ma",
                "Irene Li",
                "Qingning Yao",
                "Shanelle Roman"
            ],
            "title": "Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql",
            "year": 2018
        },
        {
            "authors": [
                "Yilun Zhao",
                "Yunxiang Li",
                "Chenying Li",
                "Rui Zhang."
            ],
            "title": "Multihiertt: Numerical reasoning over multi hierarchical tabular and textual data",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2022
        },
        {
            "authors": [
                "Yongwei Zhou",
                "Junwei Bao",
                "Chaoqun Duan",
                "Youzheng Wu",
                "Xiaodong He",
                "Tiejun Zhao."
            ],
            "title": "Unirpg: Unified discrete reasoning over table and text as program generation",
            "venue": "arXiv preprint arXiv:2210.08249.",
            "year": 2022
        },
        {
            "authors": [
                "Fengbin Zhu",
                "Wenqiang Lei",
                "Youcheng Huang",
                "Chao Wang",
                "Shuo Zhang",
                "Jiancheng Lv",
                "Fuli Feng",
                "Tat-Seng Chua."
            ],
            "title": "Tat-qa: A question answering benchmark on a hybrid of tabular and textual content in finance",
            "venue": "Proceedings of the 59th Annual Meet-",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Question answering systems devote to answering various questions with the evidence located in the structured knowledge base (e.g., table) (Pasupat and Liang, 2015; Yu et al., 2018) or unstructured texts (Rajpurkar et al., 2016). Considering that many questions need to utilize multiple sources of knowledge jointly in real-world applications, the hybrid form of question answering over texts and tables (TextTableQA) has been proposed and attracted more and more attention (Chen et al.,\n1https://codalab.lisn.upsaclay.fr/ competitions/7979.\nYear Score Athlete Place 1960 8,683 Rafer Johnson Eugene 1960 8,709 Philip Mulkey Memphis 1963 8,089 Chuan-Kwang Yang Walnut"
        },
        {
            "heading": "Q1: Who is the athlete in a city located on the Mississippi River?",
            "text": ""
        },
        {
            "heading": "A1: Philip Mulkey",
            "text": "Q2: In which year did Walnut-born athletes participate in the Rome Olympics? A2: 1960 Q3: Who is the higher scoring athlete from the cities of Eugene and Walnut?\nComparison A3: Rafer Johnson\nR1\nH\nR2 R3\nP1 P2\n\u2026Memphis is a city located along the Mississippi River in southwestern Shelby County, Tennessee, United States \u2026 \u2026Chuan-Kwang Yang competed in the decathlon at the 1960 Olympic Games in Rome\u2026\nFigure 1: The examples of HybridQA.\n2020b,a; Zhu et al., 2021; Chen et al., 2021; Zhao et al., 2022; Wang et al., 2022a). Fact reasoning (Chen et al., 2020a,b) is a critical question type of TextTableQA. It requires jointly using multiple evidence from tables and texts to reasoning the answers with different operations, such as correlation (e.g., multi-hop) and aggregation (e.g., comparison). Hyperlinks among some table cells and linked passages are essential resources to establish their relationship and support the retrieval and reasoning for multi-hop questions. As shown in Figure 1, answering a complex question Q1 requires jointly reasoning from textual evidence (P1) to table evidence ([R2, Place]) and then to other table evidence ([R2, Athlete]).\nExisting methods consist of two main stages: retriever and reader (Chen et al., 2020b; Feng et al., 2022). The retriever filters out the cells and passages with high relevance to the question, and then the reader extracts a span from the retrieval results as the final answer. However, current methods with two stages still have three limitations as follows.\n1) Noisy labeling for training retriever. Existing retrieval methods usually ignore the weakly supervised answer annotation (Chen et al., 2020b; Wang et al., 2022b; Feng et al., 2022). For the Q2 of Figure 1, we cannot know the specific location\nar X\niv :2\n30 5.\n11 72\n5v 1\n[ cs\n.C L\n] 1\n9 M\nay 2\n02 3\nof the hybrid evidence, only given the final answer \"1960\". Therefore, there is a lot of pseudo-true evidence labeled (Marked in green) automatically by string matching, which introduces a lot of evidence noise.\n2) Insufficient utilization of heterogeneous information. After retrieval, existing methods selected a particular cell or passage for reading to extract the final answer (Chen et al., 2020b; Wang et al., 2022b). As for Q1 in Figure 1, previous models were more likely to choose P1 or the coordinates [R2,Place] to extract the answer. However, these methods seldomly used the hybrid information of table schema and cell-passage hyperlinks, which is the key factor in answering multi-hop questions.\n3) Deficient ability for different reasoning operations. Previous methods (Eisenschlos et al., 2021; Kumar et al., 2021; Wang et al., 2022b) mainly used an extraction module to obtain answers, which cannot support knowledge reasoning that requires comparison, calculation, and other operations.\nIn this paper, we propose a three-stage approach S3HQA to solve the above problems. (1) Retriever with Refinement Training, we propose a two-step training method, splitting the training data into two parts, so that the noise in the retrieval phase can be alleviated. (2) Hybrid Selector has been proposed and selects supporting facts with different granularity and resources depending on the question type. By considering the hybrid data of tables and text, this paper proposes a hybrid selection algorithm that can effectively utilize the heterogeneous information of tables and passages. (3) Generationbased reasoner utilizes a generation-based model\nfor addressing different question types. The model allows better aggregation of information on the input side, which not only have better multi-hop reasoning capabilities but also be able to handle comparison and counting questions. Furthermore, we are the first to use the LLM in-context learning approach for table-text hybrid question-answering tasks.\nWe evaluate our proposed model on the challenging TextTableQA benchmark HybridQA. The empirical results show that our approach outperforms all the existing models2."
        },
        {
            "heading": "2 Our Approach",
            "text": ""
        },
        {
            "heading": "2.1 Problem Definition",
            "text": "Given a natural language question Q = {qi}|Q|i=1 and a table T with \u3008H,R\u3009, H indicates the table headers, andR = {ri}|R|i=1 indicates the rows with number |R|. Each row ri is consists of N cells ri = {cij}Nj=1. The header\u2019s number is also N . Some cells have a linked passage Pij . Our goal aims to generate the answer A with model \u0398, which is a span from table cells or linked passage or a derivation result of counting questions."
        },
        {
            "heading": "2.2 Retriever with Refinement Training",
            "text": "The retriever aims to perform initial filtering of heterogeneous resources. However, accurately labeling the location of answers consumes high labeling costs. For TextTableQA data, the answer A usually appears in multiple locations, which makes it difficult for us to generate precise retrieval la-\n2We released the source code at https://github. com/lfy79001/S3HQA\nbels. We use a two-step training method, with a row-based retriever and a passage-based retriever for each step.\nInspired by (Kumar et al., 2021), the retrieval has two steps. First, we divide the dataD into two folds according to the string matching labels Gi. Specifically, for a question-answer instance, the answer A appears one time as D1, and the instance whose answer A appears multiple times as D2. Take the example in Figure 1, Q1, Q3 belongs to D1 while Q2 belongs to D2. The data is organized in the form of [CLS]q1q2...q|Q|[SEP]ci1ci2...ciN[SEP] or [CLS]q1q2...q|Q|[SEP]pij[SEP].\nIn the first step, we only use D1 to train a model \u03981, which data are noiseless. Then in the second step, we use the trained weight \u03981 to train the model \u03982. For the input x, the loss function is:\nL(\u03982, x,R) = \u2211 z\u2208R \u2212q(z) log p\u03981(z|x)\nwhere q(z) = p\u03981(z|x, z \u2208 R) is the probability distribution given by the model restricted to candidate rows R containing the answer span, taken here as a constant with zero gradients (Eisenschlos et al., 2021).\nMeanwhile, we use a passage-based retriever to enhance the performance of a row-based retriever (PassageFilter). Specifically, we use the passage-based retriever to obtain a prediction score of passage relevance. Based on this score, we reorder the input of the row-based retriever. It avoids the limitation on input sequence length imposed by the pre-trained model."
        },
        {
            "heading": "2.3 Hybrid Selector",
            "text": "This module needs to combine the results of the two granularity retrievers. As for this task, we consider the question type and the relationships between the table and linked passages essential. As shown in Figure 2, the hybrid selector chooses the appropriate data source from the two retrieval results depending on question types.\nSpecifically, for general bridge multi-hop questions, we use a single row and its linked passage. While for comparison/count questions, we consider multiple rows and further filter the related sentences, delete the linked paragraphs with the low scores. This not only enables the generation module to obtain accurate information, but also prevents the introduction of a large amount of unrelated information. The selector algorithm outputs a\nmixed sequence with high relevance based on the relationship between the question, the table, and the passages. The algorithm is shown in Algorithm 1.\nAlgorithm 1 Hybrid Selector Algorithm. Input: question Q, table rows R, linked passages P , row-\nbased retriever \u0398R, passage-based retriever \u0398P , selector target row count NS\nOutput: generator input S Get the row/passage ordered list by relevant scores\n1: OR \u2190 sort(\u0398R(Q,R)) 2: OP \u2190 sort(\u0398P (Q,P)) 3: ptype \u2190 Classification(Q) 4: if ptype = bridge then 5: if OP [0] in OR[0] then 6: S \u2190 Q+OR[0] 7: else 8: S \u2190 Q+OR[0] +OP [0] 9: end if\n10: else 11: OPC \u2190 P[len(OP)//2 :] 12: S \u2190 Q+OR[0 : NS ]\u2212OPC 13: end if 14: return S"
        },
        {
            "heading": "2.4 Generation-based Reasoner",
            "text": "The results of the selector take into account both two granularity. Unlike the previous approaches, which were based on a span extraction module, we use a generation-based model for answer prediction."
        },
        {
            "heading": "2.4.1 Row-wise generator",
            "text": "To generate an accurate answer string A = (a1, a2, ..., an) given the question Q and selection evidence S , we perform lexical analysis to identify the question type, such as counting or comparison, by looking for certain keywords or comparative adjectives. We utilize two special tags \u3008Count\u3009 and \u3008Compare\u3009, which indicates the question types.\nWe then use the results of the passage retriever to rank the passages in order of their relevance, eliminating the impact of model input length limitations. Finally, we train a Seq2Seq language model with parameters \u0398, using the input sequence Q,S and the previous outputs a<i to optimize the product of the probabilities of the output sequence a1, a2, ..., an:\nA = argmax n\u220f\ni=1\nP (ai|a<i,Q,S; \u0398)"
        },
        {
            "heading": "2.4.2 LLM prompting generator",
            "text": "With the emergence of large language models, InContext Learning (Dong et al., 2022) and Chain-ofThought prompting (Wei et al., 2022) have become\ntwo particularly popular research topics in this field. In this paper, we introduce a prompting strategy for multi-hop TextTableQA.\nWe utilize selection evidence S and apply LLMbased prompting. We conducted experiments on both vanilla prompting and chain-of-thought prompting in zero-shot and few-shot scenarios."
        },
        {
            "heading": "3 Experiment",
            "text": ""
        },
        {
            "heading": "3.1 Experiment Setup",
            "text": "Datasets We conduct experiments on HybridQA (Chen et al., 2020b). The detailed statistics are shown in Appendix A. For evaluation, we followed the official evaluation to report exact match accuracy and F1 score. Implementation details The implementation details are shown in Appendix B. The experimental results are the average of five times results."
        },
        {
            "heading": "3.2 Fully-supervised Results",
            "text": "Table 1 shows the comparison results between our models with previous typical approaches on both development and test sets. It shows that our proposed S3HQA works significantly better than the baselines in terms of EM and F1 on HybridQA. The results indicate that S3HQA is an effective model for multi-hop question answering over tabular and textual data. Specifically, it can effectively handle multi-hop reasoning and make full use of heterogeneous information.\nHowever, we found that our approach was outperformed by the DEHG model (Feng et al., 2022) in terms of F1 score on the Dev set. We speculate that this might be because the DEHG approach uses their own Open Information Extraction (OIE) tool."
        },
        {
            "heading": "3.3 LLM-prompting Results",
            "text": "We present our zero-shot and few-shot results in Table 2. \"Direct\" refers to a simple prompting method where only the question, context, and answer are provided to the model without any additional reasoning process. In contrast, \"CoT\" involves a human-authored Chain-of-Thought reasoning process that provides a more structured and logical way of prompting the model. The experiments demonstrate that in-context learning used to prompt large language models can achieve promising results. Specifically, utilizing the Chain-ofThought prompt method can significantly enhance the model\u2019s performance.\nHowever, it\u2019s worth noting that there is still a performance gap compared to fine-tuning the model on the full dataset (Table 1). Fine-tuning allows the model to learn more specific information about the TextTableQA task, resulting in better performance. Nevertheless, our results show that the LLM-prompting method can be a useful alternative to fine-tuning, especially when there is a limited amount of labeled data available."
        },
        {
            "heading": "3.4 Ablation Studies",
            "text": "We conduct ablation studies on the test set. We validate the effects of three modules: retriever with refinement training, hybrid selector, and generation-based reasoner. The retriever performs initial filtering of heterogeneous resources; Selectors combined with hyperlinks further identify the exact evidence needed to answer multi-hop questions; and the reasoner uses the selection evidence to obtain the final answer.\nEffect of proposed retriever. As shown in the Table 3, under the setting of using the BERTbase-uncased model, sing the BERT-base-uncased model setting, the retriever with refinement training achieved 87.2. When we use Deberta-base, the top1 retrieval performance improved by 0.8%. For w/o refinement training, we use the entire data directly for training, the top1 recall drops about 3.2%. For w/o PassageFilter, we remove the mechanism, the top1 recall drops about 3.2%. For VanillaRetriever, we use the row-based retriever (Kumar et al., 2021) and remove all our mechanisms, the top1 score drops about 5.3%. This shows that our model can solve the weakly supervised data noise problem well.\nEffect of hybrid selector. As shown in the Table 4, we removed the selector of S3HQA and replaced it with the previous cell-based selector (Wang et al., 2022b). This method directly uses the top1 result of the row retriever as input to\nthe generator. w/o hybrid selector shows that the EM drops 2.9% and F1 drops 1.6%, which proves the effectiveness of our selector approach.\nEffect of reasoner. As shown in the Table 4, we design two baselines. BERT-large reader (Chen et al., 2020b; Wang et al., 2022b) uses BERT (Devlin et al., 2018) as encoder and solves this task by predicting the start/end tokens. w/o special tags deletes the special tags. Both the two experiments demonstrate our S3HQA reasoner performs the best for HybridQA task."
        },
        {
            "heading": "4 Related Work",
            "text": "The TextTableQA task (Wang et al., 2022a) has attracted more and more attention. As for multi-hop type dataset, previous work used pipeline approach (Chen et al., 2020b), unsupervised approach (Pan et al., 2021), multigranularity (Wang et al., 2022b), table pre-trained language model (Eisenschlos et al., 2021), multiinstance learning (Kumar et al., 2021) and graph neural network (Feng et al., 2022) to solve this task. As for numerical reasoning task, which is quite different from multi-hop type dataset, there is also a lot of work (Zhu et al., 2021; Zhao et al., 2022; Zhou et al., 2022; Lei et al., 2022; Li et al., 2022; Wei et al., 2023) to look at these types of questions. Unlike these methods, our proposed three-stage model S3HQA can alleviate noises from weakly supervised and solve different types of multi-hop TextTableQA questions by handling the relationship between tables and text."
        },
        {
            "heading": "5 Conclusion",
            "text": "This paper proposes a three-stage model consisting of retriever, selector, and reasoner, which can effectively address multi-hop TextTableQA. The proposed method solves three drawbacks of the previous methods: noisy labeling for training retriever, insufficient utilization of heterogeneous information, and deficient ability for reasoning. It achieves new state-of-the-art performance on the widely used benchmark HybridQA. In future work, we will design more interpretable TextTableQA models to predict the explicit reasoning path.\nLimitations\nSince the multi-hop TextTableQA problem has only one dataset HybridQA, our model has experimented on only one dataset. This may lead to a lack\nof generalizability of our model. Transparency and interpretability are important in multi-hop question answering. While our model achieves the best results, the model does not fully predict the reasoning path explicitly and can only predict the row-level path and passage-level path. In future work, we will design more interpretable TextTableQA models."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported by the National Key R&D Program of China (2022ZD0160503) and the National Natural Science Foundation of China (No.U1936207, No.61976211). This work was supported by the Strategic Priority Research Program of Chinese Academy of Sciences (No.XDA27020100), the Youth Innovation Promotion Association CAS, Yunnan Provincial Major Science and Technology Special Plan Projects (No.202202AD080004) and CCF-DiDi GAIA Collaborative Research Funds for Young Scholars."
        },
        {
            "heading": "A HybridQA Dataset",
            "text": "HybridQA is a large-scale, complex, and multihop TextTableQA benchmark. Tables and texts are crawled from Wikipedia. Each row in the table describes several attributes of an instance. Each table has its hyperlinked Wikipedia passages that describe the detail of attributes. It contains 62,682 instances in the train set, 3466 instances in the dev set and 3463 instances in the test set.\nB Implementation Details\nB.1 Fully-supervised Setting\nWe utilize PyTorch (Paszke et al., 2019) to implement our proposed model. During pre-processing, the input of questions, tables and passages are tokenized and lemmatized with the NLTK (Bird, 2006) toolkit. We conducted the experiments on a single NVIDIA GeForce RTX 3090.\nIn the retriever stage, we use BERT-baseuncased (Devlin et al., 2018) and Deberta-base (He et al., 2020) to obtain the initial representations. For the first step, batch size is 1, epoch number is 5, learning rate is 7e-6 (selected from 1e-5, 7e-6, 5e6). The training process may take around 10 hours. For the second step, we use a smaller learning rate 2e-6 (selected from 5e-6, 3e-6, 2e-6), epoch number is 5. The training process may take around 8 hours. In the selector stage, target row count NS is 3. In the generator stage, we use BART-large language model (Lewis et al., 2020), the learning rate is 1e-5 (selected from 5e-5, 1e-5, 5e-6), batch size is 8, epoch number is 10, beam size is 3 and max generate length is 20.\nB.2 LLM-prompting Setting We use the OpenAI GPT-3.5 (text-davinci-003) API model with the setting temperature = 0 in our experiments. For the few-shot setting, we use 2 shots. To elicit the LLM\u2019s capability to perform multi-hop reasoning, we use the text \"Read the following table and text information, answer a question. Let\u2019s think step by step.\" as our prompt."
        }
    ],
    "title": "SHQA: A Three-Stage Approach for Multi-hop Text-Table Hybrid Question Answering",
    "year": 2023
}