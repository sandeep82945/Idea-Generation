{
    "abstractText": "Deep model fusion/merging is an emerging technique that merges the parameters or predictions of multiple deep learning models into a single one. It combines the abilities of different models to make up for the biases and errors of a single model to achieve better performance. However, deep model fusion on large-scale deep learning models (e.g., LLMs and foundation models) faces several challenges, including high computational cost, high-dimensional parameter space, interference between different heterogeneous models, etc. Although model fusion has attracted widespread attention due to its potential to solve complex real-world tasks, there is still a lack of complete and detailed survey research on this technique. Accordingly, in order to understand the model fusion method better and promote its development, we present a comprehensive survey to summarize the recent progress. Specifically, we categorize existing deep model fusion methods as four-fold: (1) \u201cMode connectivity\u201d, which connects the solutions in weight space via a path of non-increasing loss, in order to obtain better initialization for model fusion; (2) \u201cAlignment\u201d matches units between neural networks to create better conditions for fusion; (3) \u201cWeight average\u201d, a classical model fusion method, averages the weights of multiple models to obtain more accurate results closer to the optimal solution. (4) \u201cEnsemble learning\u201d combines the outputs of diverse models, which is a foundational technique for improving the accuracy and robustness of the final model. In addition, we analyze the challenges faced by deep model fusion and propose possible research directions for model fusion in the future. Our review is helpful in deeply understanding the correlation between different model fusion methods and practical application methods, which can enlighten the research in the field of deep model fusion.",
    "authors": [
        {
            "affiliations": [],
            "name": "Weishi Li"
        },
        {
            "affiliations": [],
            "name": "Yong Peng"
        },
        {
            "affiliations": [],
            "name": "Miao Zhang"
        },
        {
            "affiliations": [],
            "name": "Liang Ding"
        },
        {
            "affiliations": [],
            "name": "Han Hu"
        },
        {
            "affiliations": [],
            "name": "Li Shen"
        }
    ],
    "id": "SP:bf4ac1b3ca5d1312c37d30e0c76075a2681f6df4",
    "references": [
        {
            "authors": [
                "Imad Afyouni",
                "Zaher Al Aghbari",
                "Reshma Abdul Razack"
            ],
            "title": "Multi-feature, multi-modal, and multisource social event detection: A comprehensive survey",
            "venue": "Information Fusion,",
            "year": 2022
        },
        {
            "authors": [
                "Sungsoo Ahn",
                "Shell Xu Hu",
                "Andreas Damianou",
                "Neil D Lawrence",
                "Zhenwen Dai"
            ],
            "title": "Variational information distillation for knowledge transfer",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Samuel K Ainsworth",
                "Jonathan Hayase",
                "Siddhartha Srinivasa"
            ],
            "title": "Git re-basin: Merging models modulo permutation symmetries",
            "venue": "arXiv preprint arXiv:2209.04836,",
            "year": 2022
        },
        {
            "authors": [
                "Aditya Kumar Akash",
                "Sixu Li",
                "Nicol\u00e1s Gar\u0107\u0131a Trillos"
            ],
            "title": "Wasserstein barycenter-based model fusion and linear mode connectivity of neural networks",
            "venue": "arXiv preprint arXiv:2210.06671,",
            "year": 2022
        },
        {
            "authors": [
                "Milad I Akhlaghi",
                "Sergey V Sukhov"
            ],
            "title": "Knowledge fusion in feedforward artificial neural networks",
            "venue": "Neural Processing Letters,",
            "year": 2018
        },
        {
            "authors": [
                "Rohan Anil",
                "Gabriel Pereyra",
                "Alexandre Passos",
                "Robert Ormandi",
                "George E Dahl",
                "Geoffrey E Hinton"
            ],
            "title": "Large scale distributed neural network training through online distillation",
            "venue": "arXiv preprint arXiv:1804.03235,",
            "year": 2018
        },
        {
            "authors": [
                "Anna Aniol",
                "Marcin Pietron",
                "Jerzy Duda"
            ],
            "title": "Ensemble approach for natural language question answering problem",
            "venue": "Seventh International Symposium on Computing and Networking Workshops (CANDARW),",
            "year": 2019
        },
        {
            "authors": [
                "Sanjeev Arora",
                "Rong Ge",
                "Behnam Neyshabur",
                "Yi Zhang"
            ],
            "title": "Stronger generalization bounds for deep nets via a compression approach",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Devansh Arpit",
                "Huan Wang",
                "Yingbo Zhou",
                "Caiming Xiong"
            ],
            "title": "Ensemble of averages: Improving model selection and boosting performance in domain generalization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Peter Auer",
                "Mark Herbster",
                "Manfred KK Warmuth"
            ],
            "title": "Exponentially many local minima for single neurons",
            "venue": "Advances in neural information processing systems,",
            "year": 1995
        },
        {
            "authors": [
                "Gregory Benton",
                "Wesley Maddox",
                "Sanae Lotfi",
                "Andrew Gordon Gordon Wilson"
            ],
            "title": "Loss surface simplexes for mode connecting volumes and fast ensembling",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Frederik Benzing",
                "Simon Schug",
                "Robert Meier",
                "Johannes Von Oswald",
                "Yassir Akram",
                "Nicolas Zucchet",
                "Laurence Aitchison",
                "Angelika Steger"
            ],
            "title": "Random initialisations performing above chance and how to find them",
            "venue": "arXiv preprint arXiv:2209.07509,",
            "year": 2022
        },
        {
            "authors": [
                "Charles Blundell",
                "Julien Cornebise",
                "Koray Kavukcuoglu",
                "Daan Wierstra"
            ],
            "title": "Weight uncertainty in neural network",
            "venue": "In International conference on machine learning,",
            "year": 2015
        },
        {
            "authors": [
                "Johanni Brea",
                "Berfin Simsek",
                "Bernd Illing",
                "Wulfram Gerstner"
            ],
            "title": "Weight-space symmetry in deep networks gives rise to permutation saddles, connected by equal-loss valleys across the loss landscape",
            "year": 1907
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Mathilde Caron",
                "Hugo Touvron",
                "Ishan Misra",
                "Herv\u00e9 J\u00e9gou",
                "Julien Mairal",
                "Piotr Bojanowski",
                "Armand Joulin"
            ],
            "title": "Emerging properties in self-supervised vision transformers",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "\u00c1ngela Casado-Gar\u0107\u0131a",
                "J\u00f3nathan Heras"
            ],
            "title": "Ensemble methods for object detection",
            "venue": "ECAI",
            "year": 2020
        },
        {
            "authors": [
                "Junbum Cha",
                "Sanghyuk Chun",
                "Kyungjae Lee",
                "Han-Cheol Cho",
                "Seunghyun Park",
                "Yunsung Lee",
                "Sungrae Park"
            ],
            "title": "Swad: Domain generalization by seeking flat minima",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "An Mei Chen",
                "Haw-minn Lu",
                "Robert Hecht-Nielsen"
            ],
            "title": "On the geometry of feedforward neural network error surfaces",
            "venue": "Neural computation,",
            "year": 1993
        },
        {
            "authors": [
                "Guanzheng Chen",
                "Fangyu Liu",
                "Zaiqiao Meng",
                "Shangsong Liang"
            ],
            "title": "Revisiting parameter-efficient tuning: Are we really there yet",
            "venue": "arXiv preprint arXiv:2202.07962,",
            "year": 2022
        },
        {
            "authors": [
                "Hong-You Chen",
                "Wei-Lun Chao"
            ],
            "title": "Fedbe: Making bayesian model ensemble applicable to federated learning",
            "venue": "arXiv preprint arXiv:2009.01974,",
            "year": 2020
        },
        {
            "authors": [
                "Hugh Chen",
                "Scott Lundberg",
                "Su-In Lee"
            ],
            "title": "Checkpoint ensembles: Ensemble methods from a single training process",
            "venue": "arXiv preprint arXiv:1710.03282,",
            "year": 2017
        },
        {
            "authors": [
                "Minghui Chen",
                "Meirui Jiang",
                "Qi Dou",
                "Zehua Wang",
                "Xiaoxiao Li"
            ],
            "title": "Fedsoup: Improving generalization and personalization in federated learning via selective model interpolation, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Anna Choromanska",
                "Mikael Henaff",
                "Michael Mathieu",
                "G\u00e9rard Ben Arous",
                "Yann LeCun"
            ],
            "title": "The loss surfaces of multilayer networks",
            "venue": "In Artificial intelligence and statistics,",
            "year": 2015
        },
        {
            "authors": [
                "Leshem Choshen",
                "Elad Venezian",
                "Shachar Don-Yehia",
                "Noam Slonim",
                "Yoav Katz"
            ],
            "title": "Where to start? analyzing the potential value of intermediate models",
            "venue": "arXiv preprint arXiv:2211.00107,",
            "year": 2022
        },
        {
            "authors": [
                "Leshem Choshen",
                "Elad Venezian",
                "Noam Slonim",
                "Yoav Katz"
            ],
            "title": "Fusing finetuned models for better pretraining",
            "venue": "arXiv preprint arXiv:2204.03044,",
            "year": 2022
        },
        {
            "authors": [
                "KR1442 Chowdhary",
                "KR Chowdhary"
            ],
            "title": "Natural language processing",
            "venue": "Fundamentals of artificial intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "venue": "arXiv preprint arXiv:2204.02311,",
            "year": 2022
        },
        {
            "authors": [
                "Alexandra Chronopoulou",
                "Matthew E Peters",
                "Alexander Fraser",
                "Jesse Dodge"
            ],
            "title": "Adaptersoup: Weight averaging to improve generalization of pretrained language models",
            "venue": "arXiv preprint arXiv:2302.07027,",
            "year": 2023
        },
        {
            "authors": [
                "Yaim Cooper"
            ],
            "title": "Global minima of overparameterized neural networks",
            "venue": "SIAM Journal on Mathematics of Data Science,",
            "year": 2021
        },
        {
            "authors": [
                "Francesco Croce",
                "Sylvestre-Alvise Rebuffi",
                "Evan Shelhamer",
                "Sven Gowal"
            ],
            "title": "Seasoning model soups for robustness to adversarial and natural distribution shifts",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2023
        },
        {
            "authors": [
                "Gavin E Crooks"
            ],
            "title": "Measuring thermodynamic length",
            "venue": "Physical Review Letters,",
            "year": 2007
        },
        {
            "authors": [
                "Wojciech Marian Czarnecki",
                "Simon Osindero",
                "Razvan Pascanu",
                "Max Jaderberg"
            ],
            "title": "A deep neural network\u2019s loss surface contains every low-dimensional pattern",
            "year": 1912
        },
        {
            "authors": [
                "Nico Daheim",
                "Nouha Dziri",
                "Mrinmaya Sachan",
                "Iryna Gurevych",
                "Edoardo M Ponti"
            ],
            "title": "Elastic weight removal for faithful and abstractive dialogue generation",
            "venue": "arXiv preprint arXiv:2303.17574,",
            "year": 2023
        },
        {
            "authors": [
                "Zihang Dai",
                "Hanxiao Liu",
                "Quoc V Le",
                "Mingxing Tan"
            ],
            "title": "Coatnet: Marrying convolution and attention for all data sizes",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Yann N Dauphin",
                "Razvan Pascanu",
                "Caglar Gulcehre",
                "Kyunghyun Cho",
                "Surya Ganguli",
                "Yoshua Bengio"
            ],
            "title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization",
            "venue": "Advances in neural information processing systems,",
            "year": 2014
        },
        {
            "authors": [
                "Li Deng",
                "John Platt"
            ],
            "title": "Ensemble deep learning for speech recognition",
            "venue": "In Proc. interspeech,",
            "year": 2014
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova. Bert"
            ],
            "title": "Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805,",
            "year": 2018
        },
        {
            "authors": [
                "Nikolaos Dimitriadis",
                "Pascal Frossard",
                "Fran\u00e7ois Fleuret"
            ],
            "title": "Pareto manifold learning: Tackling multiple tasks via ensembles of single-task models",
            "venue": "In International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Laurent Dinh",
                "Razvan Pascanu",
                "Samy Bengio",
                "Yoshua Bengio"
            ],
            "title": "Sharp minima can generalize for deep nets",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Shachar Don-Yehiya",
                "Elad Venezian",
                "Colin Raffel",
                "Noam Slonim",
                "Yoav Katz",
                "Leshem Choshen"
            ],
            "title": "Cold fusion: Collaborative descent for distributed multitask finetuning, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Xibin Dong",
                "Zhiwen Yu",
                "Wenming Cao",
                "Yifan Shi",
                "Qianli Ma"
            ],
            "title": "A survey on ensemble learning",
            "venue": "Frontiers of Computer Science,",
            "year": 2020
        },
        {
            "authors": [
                "Felix Draxler",
                "Kambis Veschgini",
                "Manfred Salmhofer",
                "Fred Hamprecht"
            ],
            "title": "Essentially no barriers in neural network energy landscape",
            "venue": "In International conference on machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "Shangchen Du",
                "Shan You",
                "Xiaojie Li",
                "Jianlong Wu",
                "Fei Wang",
                "Chen Qian",
                "Changshui Zhang"
            ],
            "title": "Agree to disagree: Adaptive ensemble knowledge distillation in gradient space. advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Long Duong",
                "Trevor Cohn",
                "Steven Bird",
                "Paul Cook"
            ],
            "title": "Low resource dependency parsing: Crosslingual parameter sharing in a neural network parser. In Proceedings of the 53rd annual meeting of the Association for Computational Linguistics and the 7th international joint conference on natural language processing (volume 2: short papers)",
            "year": 2015
        },
        {
            "authors": [
                "Nikita Dvornik",
                "Cordelia Schmid",
                "Julien Mairal"
            ],
            "title": "Diversity with cooperation: Ensemble methods for few-shot classification",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2019
        },
        {
            "authors": [
                "Rahim Entezari",
                "Hanie Sedghi",
                "Olga Saukh",
                "Behnam Neyshabur"
            ],
            "title": "The role of permutation invariance in linear mode connectivity of neural networks",
            "venue": "arXiv preprint arXiv:2110.06296,",
            "year": 2021
        },
        {
            "authors": [
                "Alireza Fallah",
                "Aryan Mokhtari",
                "Asuman Ozdaglar"
            ],
            "title": "Personalized federated learning with theoretical guarantees: A model-agnostic meta-learning approach",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Rida T Farouki"
            ],
            "title": "The bernstein polynomial basis: A centennial retrospective",
            "venue": "Computer Aided Geometric Design,",
            "year": 2012
        },
        {
            "authors": [
                "Chris Fifty",
                "Ehsan Amid",
                "Zhe Zhao",
                "Tianhe Yu",
                "Rohan Anil",
                "Chelsea Finn"
            ],
            "title": "Efficiently identifying task groupings for multi-task learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Stanislav Fort",
                "Gintare Karolina Dziugaite",
                "Mansheej Paul",
                "Sepideh Kharaghani",
                "Daniel M Roy",
                "Surya Ganguli"
            ],
            "title": "Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the neural tangent kernel",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Stanislav Fort",
                "Huiyi Hu",
                "Balaji Lakshminarayanan"
            ],
            "title": "Deep ensembles: A loss landscape perspective",
            "venue": "arXiv preprint arXiv:1912.02757,",
            "year": 2019
        },
        {
            "authors": [
                "Stanislav Fort",
                "Stanislaw Jastrzebski"
            ],
            "title": "Large scale structure of neural network loss landscapes",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Stanislav Fort",
                "Adam Scherlis"
            ],
            "title": "The goldilocks zone: Towards better understanding of neural network loss landscapes",
            "venue": "In Proceedings of the aaai conference on artificial intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Jonathan Frankle"
            ],
            "title": "Revisiting\u201d qualitatively characterizing neural network optimization problems",
            "venue": "arXiv preprint arXiv:2012.06898,",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan Frankle",
                "Michael Carbin"
            ],
            "title": "The lottery ticket hypothesis: Finding sparse, trainable neural networks",
            "venue": "arXiv preprint arXiv:1803.03635,",
            "year": 2018
        },
        {
            "authors": [
                "Jonathan Frankle",
                "Gintare Karolina Dziugaite",
                "Daniel Roy",
                "Michael Carbin"
            ],
            "title": "Linear mode connectivity and the lottery ticket hypothesis",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "C.D. Freeman",
                "J. Bruna"
            ],
            "title": "Topology and geometry of half-rectified network optimization",
            "year": 2016
        },
        {
            "authors": [
                "Yoav Freund",
                "Robert E Schapire"
            ],
            "title": "A decision-theoretic generalization of on-line learning and an application to boosting",
            "venue": "Journal of computer and system sciences,",
            "year": 1997
        },
        {
            "authors": [
                "Rinon Gal",
                "Or Patashnik",
                "Haggai Maron",
                "Amit H Bermano",
                "Gal Chechik",
                "Daniel Cohen-Or"
            ],
            "title": "Stylegan-nada: Clip-guided domain adaptation of image generators",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2022
        },
        {
            "authors": [
                "Wen Gao",
                "Yonghong Tian",
                "Jian Wang"
            ],
            "title": "Digital retina: revolutionizing camera systems for the smart city",
            "venue": "Science China Information Science,",
            "year": 2018
        },
        {
            "authors": [
                "Yingbo Gao",
                "Christian Herold",
                "Zijian Yang",
                "Hermann Ney"
            ],
            "title": "Revisiting checkpoint averaging for neural machine translation",
            "venue": "arXiv preprint arXiv:2210.11803,",
            "year": 2022
        },
        {
            "authors": [
                "Timur Garipov",
                "Pavel Izmailov",
                "Dmitrii Podoprikhin",
                "Dmitry P Vetrov",
                "Andrew G Wilson"
            ],
            "title": "Loss surfaces, mode connectivity, and fast ensembling of dnns",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Jean-Baptiste Gaya",
                "Laure Soulier",
                "Ludovic Denoyer"
            ],
            "title": "Learning a subspace of policies for online adaptation in reinforcement learning, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Charles Godfrey",
                "Davis Brown",
                "Tegan Emerson",
                "Henry Kvinge"
            ],
            "title": "On the symmetries of deep learning models and their internal representations",
            "venue": "arXiv preprint arXiv:2205.14258,",
            "year": 2022
        },
        {
            "authors": [
                "Jonas Gomes",
                "Luiz Velho",
                "Mario Costa Sousa"
            ],
            "title": "Computer graphics: theory and practice",
            "year": 2012
        },
        {
            "authors": [
                "Xuan Gong",
                "Abhishek Sharma",
                "Srikrishna Karanam",
                "Ziyan Wu",
                "Terrence Chen",
                "David Doermann",
                "Arun Innanje"
            ],
            "title": "Ensemble attention distillation for privacy-preserving federated learning",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Ian J Goodfellow",
                "Oriol Vinyals",
                "Andrew M Saxe"
            ],
            "title": "Qualitatively characterizing neural network optimization problems",
            "venue": "arXiv preprint arXiv:1412.6544,",
            "year": 2014
        },
        {
            "authors": [
                "Akhilesh Gotmare",
                "Nitish Shirish Keskar",
                "Caiming Xiong",
                "Richard Socher"
            ],
            "title": "Using mode connectivity for loss landscape analysis",
            "venue": "arXiv preprint arXiv:1806.06977,",
            "year": 2018
        },
        {
            "authors": [
                "Frithjof Gressmann",
                "Zach Eaton-Rosen",
                "Carlo Luschi"
            ],
            "title": "Improving neural network training in low dimensional random bases",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Sorin Grigorescu",
                "Bogdan Trasnea",
                "Tiberiu Cocias",
                "Gigel Macesanu"
            ],
            "title": "A survey of deep learning techniques for autonomous driving",
            "venue": "Journal of Field Robotics,",
            "year": 2020
        },
        {
            "authors": [
                "Xiaozhe Gu",
                "Zixun Zhang",
                "Yuncheng Jiang",
                "Tao Luo",
                "Ruimao Zhang",
                "Shuguang Cui",
                "Zhen Li"
            ],
            "title": "Hierarchical weight averaging for deep neural networks",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Neel Guha",
                "Ameet Talwalkar",
                "Virginia Smith"
            ],
            "title": "One-shot federated learning",
            "venue": "arXiv preprint arXiv:1902.11175,",
            "year": 2019
        },
        {
            "authors": [
                "Hao Guo",
                "Jiyong Jin",
                "Bin Liu"
            ],
            "title": "Stochastic weight averaging revisited",
            "venue": "Applied Sciences,",
            "year": 2023
        },
        {
            "authors": [
                "Vipul Gupta",
                "Santiago Akle Serrano",
                "Dennis DeCoste"
            ],
            "title": "Stochastic weight averaging in parallel: Large-batch training that generalizes well",
            "venue": "arXiv preprint arXiv:2001.02312,",
            "year": 2020
        },
        {
            "authors": [
                "Kai Han",
                "An Xiao",
                "Enhua Wu",
                "Jianyuan Guo",
                "Chunjing Xu",
                "Yunhe Wang"
            ],
            "title": "Transformer in transformer",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Lars Kai Hansen",
                "Peter Salamon"
            ],
            "title": "Neural network ensembles",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 1990
        },
        {
            "authors": [
                "Robert Hecht-Nielsen"
            ],
            "title": "On the algebraic structure of feedforward network weight spaces",
            "venue": "In Advanced Neural Computers,",
            "year": 1990
        },
        {
            "authors": [
                "Clare Elizabeth Heinbaugh",
                "Emilio Luz-Ricca",
                "Huajie Shao"
            ],
            "title": "Data-free one-shot federated learning under very high statistical heterogeneity",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Geoffrey Hinton",
                "Oriol Vinyals",
                "Jeff Dean"
            ],
            "title": "Distilling the knowledge in a neural network",
            "year": 2015
        },
        {
            "authors": [
                "Minh Hoang",
                "Nghia Hoang",
                "Bryan Kian Hsiang Low",
                "Carleton Kingsford"
            ],
            "title": "Collective model fusion for multiple black-box experts",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Elad Hoffer",
                "Itay Hubara",
                "Daniel Soudry"
            ],
            "title": "Train longer, generalize better: closing the generalization gap in large batch training of neural networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Kevin Hsieh",
                "Amar Phanishayee",
                "Onur Mutlu",
                "Phillip Gibbons"
            ],
            "title": "The non-iid data quagmire of decentralized machine learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Tzu-Ming Harry Hsu",
                "Hang Qi",
                "Matthew Brown"
            ],
            "title": "Measuring the effects of non-identical data distribution for federated visual classification, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Tzu-Ming Harry Hsu",
                "Hang Qi",
                "Matthew Brown"
            ],
            "title": "Federated visual classification with real-world data distribution",
            "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Chengsong Huang",
                "Qian Liu",
                "Bill Yuchen Lin",
                "Tianyu Pang",
                "Chao Du",
                "Min Lin"
            ],
            "title": "Lorahub: Efficient cross-task generalization via dynamic lora composition, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Gao Huang",
                "Yixuan Li",
                "Geoff Pleiss",
                "Zhuang Liu",
                "John E Hopcroft",
                "Kilian Q Weinberger"
            ],
            "title": "Snapshot ensembles: Train 1, get m for free",
            "venue": "arXiv preprint arXiv:1704.00109,",
            "year": 2017
        },
        {
            "authors": [
                "Tiansheng Huang",
                "Shiwei Liu",
                "Li Shen",
                "Fengxiang He",
                "Weiwei Lin",
                "Dacheng Tao"
            ],
            "title": "Achieving personalized federated learning with sparse local models",
            "venue": "arXiv preprint arXiv:2201.11380,",
            "year": 2022
        },
        {
            "authors": [
                "Yongqi Huang",
                "Peng Ye",
                "Xiaoshui Huang",
                "Sheng Li",
                "Tao Chen",
                "Wanli Ouyang"
            ],
            "title": "Experts weights averaging: A new general training scheme for vision transformers",
            "venue": "arXiv preprint arXiv:2308.06093,",
            "year": 2023
        },
        {
            "authors": [
                "Gabriel Ilharco",
                "Marco Tulio Ribeiro",
                "Mitchell Wortsman",
                "Suchin Gururangan",
                "Ludwig Schmidt",
                "Hannaneh Hajishirzi",
                "Ali Farhadi"
            ],
            "title": "Editing models with task arithmetic",
            "venue": "arXiv preprint arXiv:2212.04089,",
            "year": 2022
        },
        {
            "authors": [
                "Gabriel Ilharco",
                "Mitchell Wortsman",
                "Samir Yitzhak Gadre",
                "Shuran Song",
                "Hannaneh Hajishirzi",
                "Simon Kornblith",
                "Ali Farhadi",
                "Ludwig Schmidt"
            ],
            "title": "Patching open-vocabulary models by interpolating weights",
            "venue": "arXiv preprint arXiv:2208.05592,",
            "year": 2022
        },
        {
            "authors": [
                "B. Imek",
                "F. Ged",
                "A. Jacot",
                "F. Spadaro",
                "C. Hongler",
                "W. Gerstner",
                "J. Brea"
            ],
            "title": "Geometry of the loss landscape in overparameterized neural networks: Symmetries and invariances",
            "year": 2021
        },
        {
            "authors": [
                "Gautier Izacard",
                "Edouard Grave"
            ],
            "title": "Leveraging passage retrieval with generative models for open domain question answering",
            "venue": "arXiv preprint arXiv:2007.01282,",
            "year": 2020
        },
        {
            "authors": [
                "Alan Julian Izenman"
            ],
            "title": "Introduction to manifold learning",
            "venue": "Wiley Interdisciplinary Reviews: Computational Statistics,",
            "year": 2012
        },
        {
            "authors": [
                "Pavel Izmailov",
                "Dmitrii Podoprikhin",
                "Timur Garipov",
                "Dmitry Vetrov",
                "Andrew Gordon Wilson"
            ],
            "title": "Averaging weights leads to wider optima and better generalization",
            "venue": "arXiv preprint arXiv:1803.05407,",
            "year": 2018
        },
        {
            "authors": [
                "Arthur Jacot",
                "Franck Gabriel",
                "Cl\u00e9ment Hongler"
            ],
            "title": "Neural tangent kernel: Convergence and generalization in neural networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Prateek Jain",
                "Sham Kakade",
                "Rahul Kidambi",
                "Praneeth Netrapalli",
                "Aaron Sidford"
            ],
            "title": "Parallelizing stochastic gradient descent for least squares regression: mini-batching, averaging, and model misspecification",
            "venue": "Journal of Machine Learning Research,",
            "year": 2018
        },
        {
            "authors": [
                "Joel Jang",
                "Seungone Kim",
                "Seonghyeon Ye",
                "Doyoung Kim",
                "Lajanugen Logeswaran",
                "Moontae Lee",
                "Kyungjae Lee",
                "Minjoon Seo"
            ],
            "title": "Exploring the benefits of training expert language models over instruction tuning, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Anubhav Jangra",
                "Sourajit Mukherjee",
                "Adam Jatowt",
                "Sriparna Saha",
                "Mohammad Hasanuzzaman"
            ],
            "title": "A survey on multi-modal summarization",
            "venue": "ACM Computing Surveys,",
            "year": 2023
        },
        {
            "authors": [
                "Eunjeong Jeong",
                "Seungeun Oh",
                "Hyesung Kim",
                "Jihong Park",
                "Mehdi Bennis",
                "Seong-Lyun Kim"
            ],
            "title": "Communication-efficient on-device machine learning: Federated distillation and augmentation under non-iid private data",
            "venue": "arXiv preprint arXiv:1811.11479,",
            "year": 2018
        },
        {
            "authors": [
                "Rahul Jha",
                "Alex Marin",
                "Suvamsh Shivaprasad",
                "Imed Zitouni"
            ],
            "title": "Bag of experts architectures for model reuse in conversational language understanding",
            "venue": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2018
        },
        {
            "authors": [
                "Divyansh Jhunjhunwala",
                "Shiqiang Wang",
                "Gauri Joshi"
            ],
            "title": "Fedexp: Speeding up federated averaging via extrapolation",
            "venue": "arXiv preprint arXiv:2301.09604,",
            "year": 2023
        },
        {
            "authors": [
                "Dongfu Jiang",
                "Xiang Ren",
                "Bill Yuchen Lin"
            ],
            "title": "Llm-blender: Ensembling large language models with pairwise ranking and generative fusion",
            "venue": "arXiv preprint arXiv:2306.02561,",
            "year": 2023
        },
        {
            "authors": [
                "Zetian Jiang",
                "Tianzhe Wang",
                "Junchi Yan"
            ],
            "title": "Unifying offline and online multi-graph matching via finding shortest paths on supergraph",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Xisen Jin",
                "Xiang Ren",
                "Daniel Preotiuc-Pietro",
                "Pengxiang Cheng"
            ],
            "title": "Dataless knowledge fusion by merging weights of language models",
            "venue": "arXiv preprint arXiv:2212.09849,",
            "year": 2022
        },
        {
            "authors": [
                "Alexia Jolicoeur-Martineau",
                "Emy Gervais",
                "Kilian Fatras",
                "Yan Zhang",
                "Simon Lacoste-Julien"
            ],
            "title": "Population parameter averaging (papa)",
            "venue": "arXiv preprint arXiv:2304.03094,",
            "year": 2023
        },
        {
            "authors": [
                "Keller Jordan",
                "Hanie Sedghi",
                "Olga Saukh",
                "Rahim Entezari",
                "Behnam Neyshabur"
            ],
            "title": "Repair: Renormalizing permuted activations for interpolation repair",
            "venue": "arXiv preprint arXiv:2211.08403,",
            "year": 2022
        },
        {
            "authors": [
                "Jeevesh Juneja",
                "Rachit Bansal",
                "Kyunghyun Cho",
                "Jo\u00e3o Sedoc",
                "Naomi Saphra"
            ],
            "title": "Linear connectivity reveals generalization strategies",
            "venue": "arXiv preprint arXiv:2205.12411,",
            "year": 2022
        },
        {
            "authors": [
                "Jean Kaddour"
            ],
            "title": "Stop wasting my time! saving days of imagenet and bert training with latest weight averaging",
            "venue": "arXiv preprint arXiv:2209.14981,",
            "year": 2022
        },
        {
            "authors": [
                "Peter Kairouz",
                "H Brendan McMahan",
                "Brendan Avent",
                "Aur\u00e9lien Bellet",
                "Mehdi Bennis",
                "Arjun Nitin Bhagoji",
                "Kallista Bonawitz",
                "Zachary Charles",
                "Graham Cormode",
                "Rachel Cummings"
            ],
            "title": "Advances and open problems in federated learning",
            "venue": "Foundations and Trends\u00ae in Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Shigeki Karita",
                "Yotaro Kubo",
                "Michiel Adriaan Unico Bacchiani",
                "Llion Jones"
            ],
            "title": "A comparative study on neural architectures and training methods for japanese speech recognition",
            "venue": "arXiv preprint arXiv:2106.05111,",
            "year": 2021
        },
        {
            "authors": [
                "Kenji Kawaguchi"
            ],
            "title": "Deep learning without poor local minima",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "Alex Kendall",
                "Yarin Gal",
                "Roberto Cipolla"
            ],
            "title": "Multi-task learning using uncertainty to weigh losses for scene geometry and semantics",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Nitish Shirish Keskar",
                "Dheevatsa Mudigere",
                "Jorge Nocedal",
                "Mikhail Smelyanskiy",
                "Ping Tak Peter Tang"
            ],
            "title": "On large-batch training for deep learning: Generalization gap and sharp minima",
            "venue": "arXiv preprint arXiv:1609.04836,",
            "year": 2016
        },
        {
            "authors": [
                "Jangho Kim",
                "SeongUk Park",
                "Nojun Kwak"
            ],
            "title": "Paraphrasing complex network: Network compression via factor transfer",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Hiroaki Kingetsu",
                "Kenichi Kobayashi",
                "Taiji Suzuki"
            ],
            "title": "Neural network module decomposition and recomposition",
            "venue": "arXiv preprint arXiv:2112.13208,",
            "year": 2021
        },
        {
            "authors": [
                "James Kirkpatrick",
                "Razvan Pascanu",
                "Neil Rabinowitz",
                "Joel Veness",
                "Guillaume Desjardins",
                "Andrei A Rusu",
                "Kieran Milan",
                "John Quan",
                "Tiago Ramalho",
                "Agnieszka Grabska-Barwinska"
            ],
            "title": "Overcoming catastrophic forgetting in neural networks",
            "venue": "Proceedings of the national academy of sciences,",
            "year": 2017
        },
        {
            "authors": [
                "Esben L Kolsbjerg",
                "Michael N Groves",
                "Bj\u00f8rk Hammer"
            ],
            "title": "An automated nudged elastic band method",
            "venue": "The Journal of chemical physics,",
            "year": 2016
        },
        {
            "authors": [
                "Peter Kontschieder",
                "Madalina Fiterau",
                "Antonio Criminisi",
                "Samuel Rota Bulo"
            ],
            "title": "Deep neural decision forests",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2015
        },
        {
            "authors": [
                "Animesh Koratana",
                "Daniel Kang",
                "Peter Bailis",
                "Matei Zaharia"
            ],
            "title": "Lit: Learned intermediate representation training for model compression",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Rohith Kuditipudi",
                "Xiang Wang",
                "Holden Lee",
                "Yi Zhang",
                "Zhiyuan Li",
                "Wei Hu",
                "Rong Ge",
                "Sanjeev Arora"
            ],
            "title": "Explaining landscape connectivity of low-cost solutions for multilayer nets",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Samuli Laine",
                "Timo Aila"
            ],
            "title": "Temporal ensembling for semi-supervised learning",
            "venue": "arXiv preprint arXiv:1610.02242,",
            "year": 2016
        },
        {
            "authors": [
                "Thanh Chi Lam",
                "Nghia Hoang",
                "Bryan Kian Hsiang Low",
                "Patrick Jaillet"
            ],
            "title": "Model fusion for personalized learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Isabelle Leang",
                "Ganesh Sistu",
                "Fabian B\u00fcrger",
                "Andrei Bursuc",
                "Senthil Yogamani"
            ],
            "title": "Dynamic task weighting methods for multi-task networks in autonomous driving systems",
            "venue": "IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC),",
            "year": 2020
        },
        {
            "authors": [
                "Spyridon Leonardos",
                "Xiaowei Zhou",
                "Kostas Daniilidis"
            ],
            "title": "Distributed consistent data association via permutation synchronization",
            "venue": "IEEE International Conference on Robotics and Automation (ICRA),",
            "year": 2017
        },
        {
            "authors": [
                "Mikhail Iu Leontev",
                "Viktoriia Islenteva",
                "Sergey V Sukhov"
            ],
            "title": "Non-iterative knowledge fusion in deep convolutional neural networks",
            "venue": "Neural Processing Letters,",
            "year": 2020
        },
        {
            "authors": [
                "Chunyuan Li",
                "Heerad Farkhoor",
                "Rosanne Liu",
                "Jason Yosinski"
            ],
            "title": "Measuring the intrinsic dimension of objective landscapes",
            "venue": "arXiv preprint arXiv:1804.08838,",
            "year": 2018
        },
        {
            "authors": [
                "Daliang Li",
                "Junpu Wang"
            ],
            "title": "Fedmd: Heterogenous federated learning via model distillation",
            "venue": "arXiv preprint arXiv:1910.03581,",
            "year": 2019
        },
        {
            "authors": [
                "Hao Li",
                "Zheng Xu",
                "Gavin Taylor",
                "Christoph Studer",
                "Tom Goldstein"
            ],
            "title": "Visualizing the loss landscape of neural nets",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Margaret Li",
                "Suchin Gururangan",
                "Tim Dettmers",
                "Mike Lewis",
                "Tim Althoff",
                "Noah A Smith",
                "Luke Zettlemoyer"
            ],
            "title": "Branch-train-merge: Embarrassingly parallel training of expert language models",
            "venue": "arXiv preprint arXiv:2208.03306,",
            "year": 2022
        },
        {
            "authors": [
                "Tao Li",
                "Zhehao Huang",
                "Qinghua Tao",
                "Yingwen Wu",
                "Xiaolin Huang"
            ],
            "title": "Trainable weight averaging: Efficient training by optimizing historical solutions",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Tao Li",
                "Zhehao Huang",
                "Qinghua Tao",
                "Yingwen Wu",
                "Xiaolin Huang"
            ],
            "title": "Trainable weight averaging: A general approach for subspace training, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Tao Li",
                "Lei Tan",
                "Zhehao Huang",
                "Qinghua Tao",
                "Yipeng Liu",
                "Xiaolin Huang"
            ],
            "title": "Low dimensional trajectory hypothesis is true: Dnns can be trained in tiny subspaces",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Tian Li",
                "Anit Kumar Sahu",
                "Ameet Talwalkar",
                "Virginia Smith"
            ],
            "title": "Federated learning: Challenges, methods, and future directions",
            "venue": "IEEE signal processing magazine,",
            "year": 2020
        },
        {
            "authors": [
                "Yixuan Li",
                "Jason Yosinski",
                "Jeff Clune",
                "Hod Lipson",
                "John Hopcroft"
            ],
            "title": "Convergent learning: Do different neural networks learn the same representations",
            "venue": "arXiv preprint arXiv:1511.07543,",
            "year": 2015
        },
        {
            "authors": [
                "Tao Lin",
                "Lingjing Kong",
                "Sebastian U Stich",
                "Martin Jaggi"
            ],
            "title": "Ensemble distillation for robust model fusion in federated learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Chang Liu",
                "Chenfei Lou",
                "Runzhong Wang",
                "Alan Yuhan Xi",
                "Li Shen",
                "Junchi Yan"
            ],
            "title": "Deep neural network fusion via graph matching with applications to model ensemble and federated learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Iou-Jen Liu",
                "Jian Peng",
                "Alexander G. Schwing"
            ],
            "title": "Knowledge flow: Improve upon your teachers, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Quande Liu",
                "Cheng Chen",
                "Jing Qin",
                "Qi Dou",
                "Pheng-Ann Heng"
            ],
            "title": "Feddg: Federated domain generalization on medical image segmentation via episodic learning in continuous frequency space",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Shiwei Liu",
                "Tianlong Chen",
                "Xiaohan Chen",
                "Zahra Atashgahi",
                "Lu Yin",
                "Huanyu Kou",
                "Li Shen",
                "Mykola Pechenizkiy",
                "Zhangyang Wang",
                "Decebal Constantin Mocanu"
            ],
            "title": "Sparse training via boosting pruning plasticity with neuroregeneration",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Shuchang Liu",
                "Shuyuan Xu",
                "Wenhui Yu",
                "Zuohui Fu",
                "Yongfeng Zhang",
                "Amelie Marian"
            ],
            "title": "Fedct: Federated collaborative transfer for recommendation",
            "venue": "In Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval,",
            "year": 2021
        },
        {
            "authors": [
                "Yajing Liu",
                "Yuning Lu",
                "Hao Liu",
                "Yaozu An",
                "Zhuoran Xu",
                "Zhuokun Yao",
                "Baofeng Zhang",
                "Zhiwei Xiong",
                "Chenguang Gui"
            ],
            "title": "Hierarchical prompt learning for multi-task learning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov"
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692,",
            "year": 1907
        },
        {
            "authors": [
                "Yuchen Liu",
                "Long Zhou",
                "Yining Wang",
                "Yang Zhao",
                "Jiajun Zhang",
                "Chengqing Zong"
            ],
            "title": "A comparable study on model averaging, ensembling and reranking in nmt",
            "venue": "In Natural Language Processing and Chinese Computing: 7th CCF International Conference,",
            "year": 2018
        },
        {
            "authors": [
                "Eliane Maria Loiola",
                "Nair Maria Maia De Abreu",
                "Paulo Oswaldo Boaventura-Netto",
                "Peter Hahn",
                "Tania Querido"
            ],
            "title": "A survey for the quadratic assignment problem",
            "venue": "European journal of operational research,",
            "year": 2007
        },
        {
            "authors": [
                "Yihang Lou",
                "Ling-Yu Duan",
                "Yong Luo",
                "Ziqian Chen",
                "Tongliang Liu",
                "Shiqi Wang",
                "Wen Gao"
            ],
            "title": "Towards digital retina in smart cities: A model generation, utilization and communication paradigm",
            "venue": "IEEE International Conference on Multimedia and Expo (ICME),",
            "year": 2019
        },
        {
            "authors": [
                "Ekdeep Singh Lubana",
                "Eric J Bigelow",
                "Robert P Dick",
                "David Krueger",
                "Hidenori Tanaka"
            ],
            "title": "Mechanistic mode connectivity",
            "venue": "In International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Yong Luo",
                "Ling-Yu Duan",
                "Yan Bai",
                "Tongliang Liu",
                "Yihang Lou",
                "Yonggang Wen"
            ],
            "title": "Nonlinear multimodel reuse",
            "venue": "IEEE 24th International Workshop on Multimedia Signal Processing (MMSP),",
            "year": 2022
        },
        {
            "authors": [
                "Xingtai Lv",
                "Ning Ding",
                "Yujia Qin",
                "Zhiyuan Liu",
                "Maosong Sun"
            ],
            "title": "Parameter-efficient weight ensembling facilitates task-level knowledge transfer",
            "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),",
            "year": 2023
        },
        {
            "authors": [
                "Wesley J Maddox",
                "Pavel Izmailov",
                "Timur Garipov",
                "Dmitry P Vetrov",
                "Andrew Gordon Wilson"
            ],
            "title": "A simple baseline for bayesian uncertainty in deep learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Aleksander Madry",
                "Aleksandar Makelov",
                "Ludwig Schmidt",
                "Dimitris Tsipras",
                "Adrian Vladu"
            ],
            "title": "Towards deep learning models resistant to adversarial attacks",
            "venue": "arXiv preprint arXiv:1706.06083,",
            "year": 2017
        },
        {
            "authors": [
                "Andrey Malinin",
                "Bruno Mlodozeniec",
                "Mark Gales"
            ],
            "title": "Ensemble distribution distillation",
            "venue": "arXiv preprint arXiv:1905.00076,",
            "year": 2019
        },
        {
            "authors": [
                "Kevis-Kokitsi Maninis",
                "Ilija Radosavovic",
                "Iasonas Kokkinos"
            ],
            "title": "Attentive single-tasking of multiple tasks",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Michael S Matena",
                "Colin A Raffel"
            ],
            "title": "Merging models with fisher-weighted averaging",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Brendan McMahan",
                "Eider Moore",
                "Daniel Ramage",
                "Seth Hampson",
                "Blaise Aguera y Arcas"
            ],
            "title": "Communication-efficient learning of deep networks from decentralized data",
            "venue": "In Artificial intelligence and statistics,",
            "year": 2017
        },
        {
            "authors": [
                "Mehryar Mohri",
                "Gary Sivek",
                "Ananda Theertha Suresh"
            ],
            "title": "Agnostic federated learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Vaishnavh Nagarajan",
                "J Zico Kolter"
            ],
            "title": "Uniform convergence may be unable to explain generalization in deep learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Giung Nam",
                "Hyungi Lee",
                "Byeongho Heo",
                "Juho Lee"
            ],
            "title": "Improving ensemble distillation with weight averaging and diversifying perturbation",
            "venue": "arXiv preprint arXiv:2206.15047,",
            "year": 2022
        },
        {
            "authors": [
                "Kirill Neklyudov",
                "Dmitry Molchanov",
                "Arsenii Ashukha",
                "Dmitry Vetrov"
            ],
            "title": "Variance networks: When expectation does not meet your expectations",
            "venue": "arXiv preprint arXiv:1803.03764,",
            "year": 2018
        },
        {
            "authors": [
                "Arkadi Nemirovski",
                "Anatoli Juditsky",
                "Guanghui Lan",
                "Alexander Shapiro"
            ],
            "title": "Robust stochastic approximation approach to stochastic programming",
            "venue": "SIAM Journal on optimization,",
            "year": 2009
        },
        {
            "authors": [
                "Gergely Neu",
                "Lorenzo Rosasco"
            ],
            "title": "Iterate averaging as regularization for stochastic gradient descent",
            "venue": "In Conference On Learning Theory,",
            "year": 2018
        },
        {
            "authors": [
                "Behnam Neyshabur",
                "Hanie Sedghi",
                "Chiyuan Zhang"
            ],
            "title": "What is being transferred in transfer learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Quynh Nguyen"
            ],
            "title": "On connected sublevel sets in deep learning",
            "venue": "In International conference on machine learning,",
            "year": 2019
        },
        {
            "authors": [
                "Quynh Nguyen",
                "Mahesh Chandra Mukkamala",
                "Matthias Hein"
            ],
            "title": "On the loss landscape of a class of deep neural networks with no bad local valleys",
            "venue": "arXiv preprint arXiv:1809.10749,",
            "year": 2018
        },
        {
            "authors": [
                "Takayuki Nishio",
                "Ryo Yonetani"
            ],
            "title": "Client selection for federated learning with heterogeneous resources in mobile edge",
            "venue": "IEEE international conference on communications (ICC),",
            "year": 2019
        },
        {
            "authors": [
                "Jaehoon Oh",
                "Sangmook Kim",
                "Se-Young Yun"
            ],
            "title": "Fedbabu: Towards enhanced representation for federated image classification",
            "venue": "arXiv preprint arXiv:2106.06042,",
            "year": 2021
        },
        {
            "authors": [
                "Maxime Oquab",
                "Leon Bottou",
                "Ivan Laptev",
                "Josef Sivic"
            ],
            "title": "Learning and transferring mid-level image representations using convolutional neural networks",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2014
        },
        {
            "authors": [
                "Guillermo Ortiz-Jimenez",
                "Alessandro Favero",
                "Pascal Frossard"
            ],
            "title": "Task arithmetic in the tangent space: Improved editing of pre-trained models",
            "venue": "arXiv preprint arXiv:2305.12827,",
            "year": 2023
        },
        {
            "authors": [
                "Niall O\u2019Mahony",
                "Sean Campbell",
                "Anderson Carvalho",
                "Suman Harapanahalli",
                "Gustavo Velasco Hernandez",
                "Lenka Krpalkova",
                "Daniel Riordan",
                "Joseph Walsh"
            ],
            "title": "Deep learning vs. traditional computer vision",
            "venue": "In Advances in Computer Vision: Proceedings of the 2019 Computer Vision Conference (CVC), Volume",
            "year": 2019
        },
        {
            "authors": [
                "Sinno Jialin Pan",
                "Qiang Yang"
            ],
            "title": "A survey on transfer learning",
            "venue": "IEEE Transactions on knowledge and data engineering,",
            "year": 2009
        },
        {
            "authors": [
                "Manas Pathak",
                "Shantanu Rane",
                "Bhiksha Raj"
            ],
            "title": "Multiparty differential privacy via aggregation of locally trained classifiers",
            "venue": "Advances in neural information processing systems,",
            "year": 2010
        },
        {
            "authors": [
                "Fidel A Guerrero Pe\u00f1a",
                "Heitor Rapela Medeiros",
                "Thomas Dubail",
                "Masih Aminbeidokhti",
                "Eric Granger",
                "Marco Pedersoli"
            ],
            "title": "Re-basin via implicit sinkhorn differentiation",
            "year": 2022
        },
        {
            "authors": [
                "Gabriel Peyr\u00e9",
                "Marco Cuturi"
            ],
            "title": "Computational optimal transport: With applications to data science",
            "venue": "Foundations and Trends\u00ae in Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Jason Phang",
                "Iacer Calixto",
                "Phu Mon Htut",
                "Yada Pruksachatkun",
                "Haokun Liu",
                "Clara Vania",
                "Katharina Kann",
                "Samuel R Bowman"
            ],
            "title": "English intermediate-task training improves zero-shot cross-lingual transfer too",
            "year": 2005
        },
        {
            "authors": [
                "Jason Phang",
                "Thibault F\u00e9vry",
                "Samuel R Bowman"
            ],
            "title": "Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks",
            "venue": "arXiv preprint arXiv:1811.01088,",
            "year": 2018
        },
        {
            "authors": [
                "Fabrizio Pittorino",
                "Antonio Ferraro",
                "Gabriele Perugini",
                "Christoph Feinauer",
                "Carlo Baldassi",
                "Riccardo Zecchina"
            ],
            "title": "Deep networks on toroids: removing symmetries reveals the structure of flat regions in the landscape geometry",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Boris T Polyak"
            ],
            "title": "New stochastic approximation type procedures",
            "venue": "Automat. i Telemekh,",
            "year": 1990
        },
        {
            "authors": [
                "Boris T Polyak",
                "Anatoli B Juditsky"
            ],
            "title": "Acceleration of stochastic approximation by averaging",
            "venue": "SIAM journal on control and optimization,",
            "year": 1992
        },
        {
            "authors": [
                "Yada Pruksachatkun",
                "Jason Phang",
                "Haokun Liu",
                "Phu Mon Htut",
                "Xiaoyi Zhang",
                "Richard Yuanzhe Pang",
                "Clara Vania",
                "Katharina Kann",
                "Samuel R. Bowman"
            ],
            "title": "Intermediate-task transfer learning with pretrained models for natural language understanding",
            "venue": "When and why does it work?,",
            "year": 2020
        },
        {
            "authors": [
                "Yujia Qin",
                "Cheng Qian",
                "Jing Yi",
                "Weize Chen",
                "Yankai Lin",
                "Xu Han",
                "Zhiyuan Liu",
                "Maosong Sun",
                "Jie Zhou"
            ],
            "title": "Exploring mode connectivity for pre-trained language models",
            "venue": "arXiv preprint arXiv:2210.14102,",
            "year": 2022
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Alexandre Rame",
                "Kartik Ahuja",
                "Jianyu Zhang",
                "Matthieu Cord",
                "Leon Bottou",
                "David Lopez-Paz"
            ],
            "title": "Model ratatouille: Recycling diverse models for out-of-distribution generalization",
            "year": 2023
        },
        {
            "authors": [
                "Alexandre Rame",
                "Guillaume Couairon",
                "Mustafa Shukor",
                "Corentin Dancette",
                "Jean-Baptiste Gaya",
                "Laure Soulier",
                "Matthieu Cord"
            ],
            "title": "Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Alexandre Rame",
                "Matthieu Kirchmeyer",
                "Thibaud Rahier",
                "Alain Rakotomamonjy",
                "Patrick Gallinari",
                "Matthieu Cord"
            ],
            "title": "Diverse weight averaging for out-of-distribution generalization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Mathieu Ravaut",
                "Shafiq Joty",
                "Nancy F Chen"
            ],
            "title": "Towards summary candidates fusion",
            "venue": "arXiv preprint arXiv:2210.08779,",
            "year": 2022
        },
        {
            "authors": [
                "Sashank Reddi",
                "Zachary Charles",
                "Manzil Zaheer",
                "Zachary Garrett",
                "Keith Rush",
                "Jakub Kone\u010dn\u00fd",
                "Sanjiv Kumar",
                "H. Brendan McMahan"
            ],
            "title": "Adaptive federated optimization, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Lior Rokach"
            ],
            "title": "Ensemble-based classifiers",
            "venue": "Artificial intelligence review,",
            "year": 2010
        },
        {
            "authors": [
                "David Ruppert"
            ],
            "title": "Efficient estimations from a slowly convergent robbins-monro process",
            "venue": "Technical report, Cornell University Operations Research and Industrial Engineering,",
            "year": 1988
        },
        {
            "authors": [
                "Omer Sagi",
                "Lior Rokach"
            ],
            "title": "Ensemble learning: A survey",
            "venue": "Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery,",
            "year": 2018
        },
        {
            "authors": [
                "Levent Sagun",
                "Utku Evci",
                "V Ugur Guney",
                "Yann Dauphin",
                "Leon Bottou"
            ],
            "title": "Empirical analysis of the hessian of over-parametrized neural networks",
            "venue": "arXiv preprint arXiv:1706.04454,",
            "year": 2017
        },
        {
            "authors": [
                "Felix Sattler",
                "Tim Korjakow",
                "Roman Rischke",
                "Wojciech Samek"
            ],
            "title": "Fedaux: Leveraging unlabeled auxiliary data in federated learning",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Robert E Schapire"
            ],
            "title": "A brief introduction to boosting",
            "venue": "In Ijcai,",
            "year": 1999
        },
        {
            "authors": [
                "J\u00fcrgen Schmidhuber"
            ],
            "title": "Learning complex, extended sequences using the principle of history compression",
            "venue": "Neural Computation,",
            "year": 1992
        },
        {
            "authors": [
                "Murray Shanahan"
            ],
            "title": "Talking about large language models",
            "venue": "arXiv preprint arXiv:2212.03551,",
            "year": 2022
        },
        {
            "authors": [
                "Noam Shazeer",
                "Azalia Mirhoseini",
                "Krzysztof Maziarz",
                "Andy Davis",
                "Quoc Le",
                "Geoffrey Hinton",
                "Jeff Dean"
            ],
            "title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer",
            "venue": "arXiv preprint arXiv:1701.06538,",
            "year": 2017
        },
        {
            "authors": [
                "Alexander Shevchenko",
                "Marco Mondelli"
            ],
            "title": "Landscape connectivity and dropout stability of sgd solutions for over-parameterized neural networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Yang Shu",
                "Zhi Kou",
                "Zhangjie Cao",
                "Jianmin Wang",
                "Mingsheng Long"
            ],
            "title": "Zoo-tuning: Adaptive transfer from a zoo of models",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Sidak Pal Singh",
                "Martin Jaggi"
            ],
            "title": "Model fusion via optimal transport",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Anton Sinitsin",
                "Vsevolod Plokhotnyuk",
                "Dmitriy Pyrkin",
                "Sergei Popov",
                "Artem Babenko"
            ],
            "title": "Editable neural networks",
            "venue": "arXiv preprint arXiv:2004.00345,",
            "year": 2020
        },
        {
            "authors": [
                "Ivan Skorokhodov",
                "Mikhail Burtsev"
            ],
            "title": "Loss landscape sightseeing with multi-point optimization",
            "venue": "arXiv preprint arXiv:1910.03867,",
            "year": 2019
        },
        {
            "authors": [
                "Joshua Smith",
                "Michael Gashler"
            ],
            "title": "An investigation of how neural networks learn from the experiences of peers through periodic weight averaging",
            "venue": "IEEE International Conference on Machine Learning and Applications (ICMLA),",
            "year": 2017
        },
        {
            "authors": [
                "Virginia Smith",
                "Chao-Kai Chiang",
                "Maziar Sanjabi",
                "Ameet S Talwalkar"
            ],
            "title": "Federated multi-task learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Nitish Srivastava",
                "Geoffrey Hinton",
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Ruslan Salakhutdinov"
            ],
            "title": "Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning",
            "year": 1929
        },
        {
            "authors": [
                "Trevor Standley",
                "Amir Zamir",
                "Dawn Chen",
                "Leonidas Guibas",
                "Jitendra Malik",
                "Silvio Savarese"
            ],
            "title": "Which tasks should be learned together in multi-task learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "George Stoica",
                "Daniel Bolya",
                "Jakob Bjorner",
                "Taylor Hearn",
                "Judy Hoffman"
            ],
            "title": "Zipit! merging models from different tasks without training",
            "venue": "arXiv preprint arXiv:2305.03053,",
            "year": 2023
        },
        {
            "authors": [
                "Tianxiang Sun",
                "Zhengfu He",
                "Qin Zhu",
                "Xipeng Qiu",
                "Xuan-Jing Huang"
            ],
            "title": "Multitask pre-training of modular prompt for chinese few-shot learning",
            "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2023
        },
        {
            "authors": [
                "Yi-Lin Sung",
                "Linjie Li",
                "Kevin Lin",
                "Zhe Gan",
                "Mohit Bansal",
                "Lijuan Wang"
            ],
            "title": "An empirical study of multimodal model merging",
            "venue": "arXiv preprint arXiv:2304.14933,",
            "year": 2023
        },
        {
            "authors": [
                "Christian Szegedy",
                "Vincent Vanhoucke",
                "Sergey Ioffe",
                "Jon Shlens",
                "Zbigniew Wojna"
            ],
            "title": "Rethinking the inception architecture for computer vision",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Charlie Tan",
                "Theodore Long",
                "Sarah Zhao",
                "Rudolf Laine"
            ],
            "title": "Geodesic mode connectivity",
            "year": 2023
        },
        {
            "authors": [
                "Anke Tang",
                "Yong Luo",
                "Han Hu",
                "Fengxiang He",
                "Kehua Su",
                "Bo Du",
                "Yixin Chen",
                "Dacheng Tao"
            ],
            "title": "Improving heterogeneous model reuse by density estimation",
            "venue": "arXiv preprint arXiv:2305.13871,",
            "year": 2023
        },
        {
            "authors": [
                "Antti Tarvainen",
                "Harri Valpola"
            ],
            "title": "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Norman Tatro",
                "Pin-Yu Chen",
                "Payel Das",
                "Igor Melnyk",
                "Prasanna Sattigeri",
                "Rongjie Lai"
            ],
            "title": "Optimizing mode connectivity via neuron alignment",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Romain Thibaux",
                "Michael I Jordan"
            ],
            "title": "Hierarchical beta processes and the indian buffet process",
            "venue": "In Artificial intelligence and statistics,",
            "year": 2007
        },
        {
            "authors": [
                "Linh Tran",
                "Bastiaan S Veeling",
                "Kevin Roth",
                "Jakub Swiatkowski",
                "Joshua V Dillon",
                "Jasper Snoek",
                "Stephan Mandt",
                "Tim Salimans",
                "Sebastian Nowozin",
                "Rodolphe Jenatton"
            ],
            "title": "Hydra: Preserving ensemble diversity for model distillation",
            "venue": "arXiv preprint arXiv:2001.04694,",
            "year": 2020
        },
        {
            "authors": [
                "Frederick Tung",
                "Greg Mori"
            ],
            "title": "Similarity-preserving knowledge distillation",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2019
        },
        {
            "authors": [
                "Thomas Uriot",
                "Dario Izzo"
            ],
            "title": "Safe crossover of neural networks through neuron alignment",
            "venue": "In Proceedings of the 2020 Genetic and Evolutionary Computation Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Joachim Utans"
            ],
            "title": "Weight averaging for neural networks and local resampling schemes",
            "venue": "In Proc. AAAI-96 Workshop on Integrating Multiple Learned Models. AAAI Press,",
            "year": 1996
        },
        {
            "authors": [
                "Tu Vu",
                "Tong Wang",
                "Tsendsuren Munkhdalai",
                "Alessandro Sordoni",
                "Adam Trischler",
                "Andrew Mattarella- Micke",
                "Subhransu Maji",
                "Mohit Iyyer"
            ],
            "title": "Exploring and predicting transferability across nlp tasks",
            "year": 2005
        },
        {
            "authors": [
                "Benyou Wang",
                "Jiabin Niu",
                "Liqun Ma",
                "Yuhua Zhang",
                "Lipeng Zhang",
                "Jingfei Li",
                "Peng Zhang",
                "Dawei Song"
            ],
            "title": "A chinese question answering approach integrating count-based and embedding-based features",
            "year": 2016
        },
        {
            "authors": [
                "Feng Wang",
                "Guoyizhe Wei",
                "Qiao Liu",
                "Jinxiang Ou",
                "Hairong Lv"
            ],
            "title": "Boost neural networks by checkpoints",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Hongyi Wang",
                "Mikhail Yurochkin",
                "Yuekai Sun",
                "Dimitris Papailiopoulos",
                "Yasaman Khazaeni"
            ],
            "title": "Federated learning with matched averaging",
            "venue": "arXiv preprint arXiv:2002.06440,",
            "year": 2020
        },
        {
            "authors": [
                "Huan Wang",
                "Nitish Shirish Keskar",
                "Caiming Xiong",
                "Richard Socher"
            ],
            "title": "Identifying generalization properties in neural networks",
            "venue": "arXiv preprint arXiv:1809.07402,",
            "year": 2018
        },
        {
            "authors": [
                "Ren Wang",
                "Yuxuan Li",
                "Sijia Liu"
            ],
            "title": "Exploring diversified adversarial robustness in neural networks via robust mode connectivity",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Tianzhe Wang",
                "Zetian Jiang",
                "Junchi Yan"
            ],
            "title": "Clustering-aware multiple graph matching via decayed pairwise matching composition",
            "venue": "In Proceedings of the The Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI-20),",
            "year": 2020
        },
        {
            "authors": [
                "Zhenyi Wang",
                "Xiaoyang Wang",
                "Li Shen",
                "Qiuling Suo",
                "Kaiqiang Song",
                "Dong Yu",
                "Yan Shen",
                "Mingchen Gao"
            ],
            "title": "Meta-learning without data via wasserstein distributionally-robust model fusion",
            "venue": "In Uncertainty in Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Tianxin Wei",
                "Zeming Guo",
                "Yifan Chen",
                "Jingrui He"
            ],
            "title": "Ntk-approximating mlp fusion for efficient language model fine-tuning",
            "year": 2023
        },
        {
            "authors": [
                "Guihua Wen",
                "Zhi Hou",
                "Huihui Li",
                "Danyang Li",
                "Lijun Jiang",
                "Eryang Xun"
            ],
            "title": "Ensemble of deep neural networks with probability-based fusion for facial expression recognition",
            "venue": "Cognitive Computation,",
            "year": 2017
        },
        {
            "authors": [
                "Haitao Wen",
                "Haoyang Cheng",
                "Heqian Qiu",
                "Lanxiao Wang",
                "Lili Pan",
                "Hongliang Li"
            ],
            "title": "Optimizing mode connectivity for class incremental learning",
            "year": 2023
        },
        {
            "authors": [
                "Yeming Wen",
                "Dustin Tran",
                "Jimmy Ba"
            ],
            "title": "Batchensemble: an alternative approach to efficient ensemble and lifelong learning",
            "venue": "arXiv preprint arXiv:2002.06715,",
            "year": 2020
        },
        {
            "authors": [
                "David H Wolpert"
            ],
            "title": "Stacked generalization",
            "venue": "Neural networks,",
            "year": 1992
        },
        {
            "authors": [
                "Mitchell Wortsman",
                "Suchin Gururangan",
                "Shen Li",
                "Ali Farhadi",
                "Ludwig Schmidt",
                "Michael Rabbat",
                "Ari S Morcos"
            ],
            "title": "lo-fi: distributed fine-tuning without communication",
            "year": 2022
        },
        {
            "authors": [
                "Mitchell Wortsman",
                "Maxwell C Horton",
                "Carlos Guestrin",
                "Ali Farhadi",
                "Mohammad Rastegari"
            ],
            "title": "Learning neural network subspaces",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Mitchell Wortsman",
                "Gabriel Ilharco",
                "Samir Ya Gadre",
                "Rebecca Roelofs",
                "Raphael Gontijo-Lopes",
                "Ari S Morcos",
                "Hongseok Namkoong",
                "Ali Farhadi",
                "Yair Carmon",
                "Simon Kornblith"
            ],
            "title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Mitchell Wortsman",
                "Gabriel Ilharco",
                "Jong Wook Kim",
                "Mike Li",
                "Simon Kornblith",
                "Rebecca Roelofs",
                "Raphael Gontijo Lopes",
                "Hannaneh Hajishirzi",
                "Ali Farhadi",
                "Hongseok Namkoong"
            ],
            "title": "Robust finetuning of zero-shot models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Ancong Wu",
                "Wei-Shi Zheng",
                "Xiaowei Guo",
                "Jian-Huang Lai"
            ],
            "title": "Distilled person re-identification: Towards a more scalable system",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2019
        },
        {
            "authors": [
                "Guile Wu",
                "Shaogang Gong"
            ],
            "title": "Peer collaborative learning for online knowledge distillation",
            "venue": "In Proceedings of the AAAI Conference on artificial intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Xi-Zhu Wu",
                "Song Liu",
                "Zhi-Hua Zhou"
            ],
            "title": "Heterogeneous model reuse via optimizing multiparty multiclass margin",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Xi-Zhu Wu",
                "Wenkai Xu",
                "Song Liu",
                "Zhi-Hua Zhou"
            ],
            "title": "Model reuse with reduced kernel mean embedding specification",
            "venue": "IEEE Transactions on Knowledge and Data Engineering,",
            "year": 2021
        },
        {
            "authors": [
                "Yang Yang De-Chuan Zhan Xiang",
                "Yu Guo Yuan Jiang"
            ],
            "title": "Modal consistency based pre-trained multimodel reuse",
            "venue": "In Proc. IJCAI,",
            "year": 2017
        },
        {
            "authors": [
                "Junchi Yan",
                "Minsu Cho",
                "Hongyuan Zha",
                "Xiaokang Yang",
                "Stephen M Chu"
            ],
            "title": "Multi-graph matching via affinity optimization with graduated consistency regularization",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2015
        },
        {
            "authors": [
                "Junchi Yan",
                "Shuang Yang",
                "Edwin R Hancock"
            ],
            "title": "Learning for graph matching and related combinatorial optimization problems",
            "venue": "In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Guandao Yang",
                "Tianyi Zhang",
                "Polina Kirichenko",
                "Junwen Bai",
                "Andrew Gordon Wilson",
                "Chris De Sa"
            ],
            "title": "Swalp: Stochastic weight averaging in low precision training",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Yang Yang",
                "De-Chuan Zhan",
                "Ying Fan",
                "Yuan Jiang",
                "Zhi-Hua Zhou"
            ],
            "title": "Deep learning for fixed model reuse",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2017
        },
        {
            "authors": [
                "Kaixuan Yao",
                "Feilong Cao",
                "Yee Leung",
                "Jiye Liang"
            ],
            "title": "Deep neural network compression through interpretability-based filter pruning",
            "venue": "Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Kaichao You",
                "Yong Liu",
                "Ziyang Zhang",
                "Jianmin Wang",
                "Michael I Jordan",
                "Mingsheng Long"
            ],
            "title": "Ranking and tuning pre-trained models: a new paradigm for exploiting model hubs",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "Shan You",
                "Chang Xu",
                "Chao Xu",
                "Dacheng Tao"
            ],
            "title": "Learning from multiple teacher networks",
            "venue": "In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
            "year": 2017
        },
        {
            "authors": [
                "EungGu Yun",
                "Hyungi Lee",
                "Giung Nam",
                "Juho Lee"
            ],
            "title": "Traversing between modes in function space for fast ensembling",
            "venue": "arXiv preprint arXiv:2306.11304,",
            "year": 2023
        },
        {
            "authors": [
                "Mikhail Yurochkin",
                "Mayank Agarwal",
                "Soumya Ghosh",
                "Kristjan Greenewald",
                "Nghia Hoang",
                "Yasaman Khazaeni"
            ],
            "title": "Bayesian nonparametric federated learning of neural networks",
            "venue": "In International conference on machine learning,",
            "year": 2019
        },
        {
            "authors": [
                "Xiaohua Zhai",
                "Alexander Kolesnikov",
                "Neil Houlsby",
                "Lucas Beyer"
            ],
            "title": "Scaling vision transformers",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Chris Zhang",
                "Mengye Ren",
                "Raquel Urtasun"
            ],
            "title": "Graph hypernetworks for neural architecture search",
            "venue": "arXiv preprint arXiv:1810.05749,",
            "year": 2018
        },
        {
            "authors": [
                "Jie Zhang",
                "Chen Chen",
                "Bo Li",
                "Lingjuan Lyu",
                "Shuang Wu",
                "Shouhong Ding",
                "Chunhua Shen",
                "Chao Wu"
            ],
            "title": "Dense: Data-free one-shot federated learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Lin Zhang",
                "Li Shen",
                "Liang Ding",
                "Dacheng Tao",
                "Ling-Yu Duan"
            ],
            "title": "Fine-tuning global model via datafree knowledge distillation for non-iid federated learning",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Michael Zhang",
                "James Lucas",
                "Jimmy Ba",
                "Geoffrey E Hinton"
            ],
            "title": "Lookahead optimizer: k steps forward, 1 step back",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Yi-Kai Zhang",
                "Lu Ren",
                "Chao Yi",
                "Qi-Wei Wang",
                "De-Chuan Zhan",
                "Han-Jia Ye"
            ],
            "title": "Zhijian: A unifying and rapidly deployable toolbox for pre-trained model reuse",
            "venue": "arXiv preprint arXiv:2308.09158,",
            "year": 2023
        },
        {
            "authors": [
                "Yu Zhang",
                "Qiang Yang"
            ],
            "title": "An overview of multi-task learning",
            "venue": "National Science Review,",
            "year": 2018
        },
        {
            "authors": [
                "Yuchen Zhang",
                "Martin J Wainwright",
                "John C Duchi"
            ],
            "title": "Communication-efficient algorithms for statistical optimization",
            "venue": "Advances in neural information processing systems,",
            "year": 2012
        },
        {
            "authors": [
                "Pu Zhao",
                "Pin-Yu Chen",
                "Payel Das",
                "Karthikeyan Natesan Ramamurthy",
                "Xue Lin"
            ],
            "title": "Bridging mode connectivity in loss landscapes and adversarial robustness",
            "venue": "arXiv preprint arXiv:2005.00060,",
            "year": 2020
        },
        {
            "authors": [
                "Wayne Xin Zhao",
                "Kun Zhou",
                "Junyi Li",
                "Tianyi Tang",
                "Xiaolei Wang"
            ],
            "title": "A survey of large language models, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Yanlin Zhou",
                "George Pu",
                "Xiyao Ma",
                "Xiaolin Li",
                "Dapeng Wu"
            ],
            "title": "Distilled one-shot federated learning",
            "venue": "arXiv preprint arXiv:2009.07999,",
            "year": 2020
        },
        {
            "authors": [
                "Zhi-Hua Zhou"
            ],
            "title": "Learnware: on the future of machine learning",
            "venue": "Frontiers Comput. Sci.,",
            "year": 2016
        },
        {
            "authors": [
                "Zhuangdi Zhu",
                "Junyuan Hong",
                "Jiayu Zhou"
            ],
            "title": "Data-free knowledge distillation for heterogeneous federated learning",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "Max Zimmer",
                "Christoph Spiegel",
                "Sebastian Pokutta"
            ],
            "title": "Sparse model soups: A recipe for improved pruning via model averaging",
            "venue": "arXiv preprint arXiv:2306.16788,",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "In recent years, deep neural networks (DNNs) [129] have made remarkable development, which is widely used in computer vision (CV) [175], natural language processing (NLP) [30] and other fields. Generally speaking, a single deep learning model often has certain limitations and cannot fully capture all underlying information behind complex networks [195]. Therefore, the classic ensemble learning [15, 193, 198] combines the outputs of multiple models to improve the final performance of model in deep learning (DL). But it suffers from the high cost of storing and running multiple models at test time [65, 204], especially as the complexity and size of models increase. Especially, for example, GPT-3 [172] has billions of parameters, and PaLM [31] even reaches 540 billion parameters and 780 billion tokens. In addition, from the perspective of loss landscape of DNNs [134, 196], gradient-optimized solutions usually converge to points near the boundary of the wide flat region instead of the central point [99]. It means that a trained network is not exactly\n*Corresponding author \u2020 Equal Contribution\nar X\niv :2\n30 9.\n15 69\n8v 1\n[ cs\n.L G\n] 2\n7 Se\nclose to the optimal solution with minimum test error. The solutions near the relative optimal point need to be fused for a better result. It inspires researchers not only to limit the the fusion scope to predictions (e.g., logits, etc.), but also to include the fusion of model parameters without accessing the training data or maintaining all individual models [110]. Accordingly, deep model fusion [111, 159] aims at fusing several DNNs into a single network, which preserves their original capabilities and even outperforms multi-task training [3, 135]. In addition, deep model fusion can reduce the tendency of a single model to overfit particular samples or noise so as to improve the accuracy, diversity and robustness of predictions [207, 223].\nDeep model fusion has attracted increasing interest due to the data privacy and practical resource-saving issues. Although the development of deep model fusion has brought many technical breakthroughs, it also produces a series of challenges, such as high computational load, model heterogeneity, and slow speed of alignment via combinatorial optimization [133, 204], etc. Some approaches are limited to specific scenarios [227, 254], which inspires researchers to investigate the principles of model fusion in different cases. Nevertheless, there is a lack of comprehensive reviews to summarize the approaches so as to indicate the internal mechanism of deep model fusion currently. Some work only focuses on model fusion from a single perspective (e.g., feature fusion, etc.) [45, 195] and a specific scene [213], or the fusion of information from different ways (multi-modal fusion [1, 103]) rather than the fusion of parameters. In order to give the developers insight into deep model fusion, we analyze the principles and methodologies of deep model fusion. In addition, we review the recent progress and representative applications, such as federated learning (FL) [160] and fine-tuning [29], etc. Our survey aims to illustrate the latest trends and potential directions in deep model fusion and provide a guideline for researchers to enhance the performance and reduce costs. Accoordingly, we group the approaches into four-fold according to the internal mechanisms and purposes as Figure 1. For the models trained independently that are not in the vicinity of each other, \u201cmode connectivity\u201d and \u201calignment\u201d bring the solutions closer so as to obtain better original conditions of average. For the similar models with certain differences in the weights space, \u201cweight average (WA)\u201d tends to average the models directly and obtain solutions closer to the optimal point in the region of the parameter space where the value of loss function is low [118]. Furthermore, for the predictions of existing models, \u201censemble learning\u201d integrates different forms of predictions of the models to get better results. Specifically, the four categories are as follows:\n\u2022 Mode connectivity. [61, 162], The solutions obtained by gradient-based optimization can be connected in weight space by a path (connector) with no obstacles, which is referred to as mode connectivity [46, 50]. We can obtain other models that are more suitable for model fusion along the low-loss path. According to the mathematical form of path and the space where the connector is located, we divide this section into three parts \u201clinear mode connectivity (LMC) [66]\u201d, \u201cnon-linear mode connectivity\u201d and \u201cmode connectivity in subspace\u201d. Mode connectivity can solve local optimization problems during training. The geometric relationships of paths of mode connectivity [61, 162] could also be used to accelerate the convergence, stability and accuracy of optimization procedures like stochastic gradient descent (SGD). In a word, mode connectivity provides a new perspective for interpreting and understanding the behaviors of model fusion [66]. But the difficulties of computational complexity and parameter tuning should be solved, especially when training models on large datasets.\n\u2022 Alignment. Alignment [140, 218] matches the units of multiple models and average the models to obtain the final model. The specific mathematical metrics (e.g., Euclidean distance [218]) between different models can be closer after alignment, which can reduce the differences between models, thus enhancing the effect of deep model fusion. Alignment can be divided into \u201cactivation matching\u201d and \u201cweight matching\u201d depending on whether data distribution needs to be considered. Moreover, Re-basin [3] is introduced based on alignment, which explores the mechanism that solutions can be transported into a single basin (i.e., area of the flat parameter space where with relatively low loss [61, 96]) by permutation invariance [50]. However, it is often faced with the obstacles of large computation, slow speed pf combinatorial optimization and architecture difference, which makes it is not easy to be extended to other scenarios with different objectives. For example, the memory burden that comes with graph matching [142, 230] limits the application of deep model fusion.\n\u2022 Weight average. WA [227] is the most direct and efficient way to fuse several parent networks into a single network [159, 204]. Compared to mode connectivity and alignment, WA does not require additional computational complexity or training to find a superior starting point, which performs well on models contain a degree of similarities. According to the space of aggregation, WA can be classified into two parts \u201cweight average\u201d and \u201caverage in subspace\u201d . In addition, the typical approaches \u201cmodel soup\u201d, \u201cmodel arithmetic\u201c and \u201cstochastic weight averaging (SWA)\u201d also provide significant improvements over the existing methods. Furthermore, some bias may be introduced in the case of large differences in model structure or number of parameters when the parameters are normalized and merged. Nonetheless, WA is still the mainstream method of deep model fusion because of its simplicity and efficiency.\n\u2022 Ensemble Learning. The outputs of several different models are combined to improve the prediction performance and robustness, which is regarded as \u201censemble learning\u201d [195]. In this review, we focus on the ensemble learning in DL. Based on ensemble learning, \u201cmodel reuse\u201d provides specifications for each model so that useful models can be identified and merged from the pool of models when given new learning tasks [177, 266]. Ensemble learning has various frameworks with convenient interfaces, which is often used in practical areas such as object detection [20], etc. Although ensemble learning requires maintaining the multiple trained models and running each of them at test time [204], it is still one of the powerful techniques that have been widely adopted in DL.\n\u2022 Applications of Model Fusion. As a technology to improve the accuracy and robustness of deep models, model fusion promote the improvement to many application fields. \u201cfederated learning [160]\u201d, an application of aggregating clients\u2019 models on a central server, makes it possible for various parties to contribute data to the computation of functions (e.g., various statistics, classifiers [177]) without the risks of privacy disclosure. \u201cfine-tuning\u201d makes small adjustments to pre-trained models, which combined with model fusion to reduce training costs and adapt to the needs of a specific task or domain. Model fusion is also involved in \u201cdistillation\u201d. That is, combine soft target knowledge from multiple complex models (teachers) to train a small model for specific requirements. \u201cmodel fusion on foundation/LLMs\u201d includes the work on large foundation models or large language models (LLMs),\nIn brief, our survey reviews deep model fusion techniques. In the first three sections \u201cmode connectivity\u201d, \u201calignment\u201d and \u201cweight average\u201d, we mainly conduct a comprehensive study from the perspective of the fusion of model parameters. In the \u201censemble learning\u201d, we mainly investigate the issue from the perspective of model outputs aggregation. The main contributions of this work are summarized as:\n\u2022 We propose a new deep model fusion classification method from the perspectives of \u201cmode connectivity\u201d, \u201calignment\u201d, \u201cweight average\u201d and \u201densemble learning\u201d, which covers the theoretical synthesis approaches of model fusion, and provides guidance for the realization of high generalization and accuracy training of DNNs.\n\u2022 We compare the advantages and disadvantages of fusion approaches, and explain the mechanism and relationship between them, which provides inspiration for designing advanced model fusion methods in the future.\n\u2022 We summarize extensive application of deep model fusion. We also discuss current research trends so as to attract more attention and reflection in the future.\nMoreover, the remainder of the paper is organized as follows: In Section 2 to Section 5, we introduce the approaches of deep model fusion according to the four perspectives \u201cmode connectivity\u201c, \u201calignment\u201c, \u201cweight average\u201c and \u201censemble learning\u201c. Section 6 introduces the applications of deep model fusion \u201cfederated learning\u201c, \u201cfine-tuning\u201c, \u201cdistillation\u201c and \u201cmodel fusion on foundation/LLMs\u201c. Finally, in Section 7, we summarize the deep model fusion and discuss the challenges and potential directions in the future.\nIn addition, we illustrate the notations and their corresponding definitions in the full text. W i is the ith neural network with weights Wi \u2208 Rd(i = 1, 2, ...k) and bias term b. \u03bb denotes weighted parameters. \u03c3 denotes a non-linear neuron activation function. L is loss function that quantify the discrepancy between the predicted and actual values."
        },
        {
            "heading": "2 Mode Connectivity",
            "text": "In this section, we introduce the definition, principles and related methods of mode connectivity. When training neural networks, the solutions trained by gradient-based optimization algorithms (e.g., SGD, etc.) can be merged without superior results [46, 61]. It is discovered that solutions can be connected via continuous paths (connectors) in the network weight space without increasing loss, which is referred to as mode connectivity [50, 66]. The models on the low-loss path can be fused to leverage the advantages of multiple models by mode connectivity, which is of great significance to produce a better aggregation model.\nFirst, we explain the principles of mode connectivity. In a representative process of DL, the minima is usually described as a point at the bottom of a convex valley, the network parameters are determined by the location of the minima [85, 116, 118]. The traditional view is that the number of local minima and saddle points is large [71, 228], and different local minima will converge to different isolated regions in the parameter space [10, 27, 39]. Recent work [125, 196] demonstrates that the minima obtained by gradient-based optimizer are not walled off in isolated valleys [61]. Gotmare et al. [72] explore the potential relationship between the minima found by different training process. Other work [33, 46, 169, 182] manifest that neural network solutions form a connected manifold (i.e., solutions in the loss landscape are connected by pipelines in weight space). Compared with mode connectivity, a direct linear path connecting two such independently trained networks usually always leaves a low-loss manifold, which creates a high loss barrier at the points on the linear path. For example, the error at the midpoint of the line segment directly connecting two points is closed to 90% (VGG-16 on CIFAR-10 [66]). The above work proves the existence and effect of mode connectivity.\nSecond, some work [50, 59, 66] quantifies the pipelines of the mode connectivity. Let L (tw1 + (1\u2212 t)w2) for t \u2208 (0, 1) be the loss (train or test error) of a neural network created by linearly interpolating between W 1 and W 2. The random data augmentations in each epoch can be seen as noise when using SGD with the initialization and hyperparameters fixed. To determine whether the result of a trained network is stable to SGD noise, the loss barrier (error barrier) B (w1, w2) [60] is defined as the maximum difference between the linear interpolation of the loss at each point and the loss of the linear connection of two points [50], as shown in Eq. (1):\nB (w1, w2) = sup t\n[L (tw1 + (1\u2212 t)w2)]\u2212 [tL (w1) + (1\u2212 t)L (w2)] . (1)\nThe loss barrier illustrates whether the error is constant or increased when we optimize the landscape [56, 61] along the path between W 1 and W 2. If there is a tunnel between two networks with a barrier approximately equal to 0, which is equivalent to mode connectivity [46, 59, 60]. That is to say, the local minima obtained by SGD can be connected by a path \u03d5 with the lowest maximum loss as shown in Eq. (2):\n\u03d5 (w1, w2) = argmin \u03d5 from W 1 to W 2\n{ max w\u2208\u03d5 L(w) } , (2)\nwhich means that the loss is low along the pathway and the network is stable to SGD noise [46], as shown in Figure 2. There are two steps to conduct mode connectivity: first determine the form of the tunnels (e.g., polygonal chain, Bezier curve [66], etc.) as Table 1; then find the optimal low-loss pathway to connect different solutions, as shown in Table 2. According to the form of path and the space in which it is located, this section introduces \u201cLinear mode connectivity\u201d, \u201cNon-linear mode connectivity\u201d and \u201cMode connectivity in subspace\u201d."
        },
        {
            "heading": "2.1 Linear Mode Connectivity",
            "text": "In order to connect two points on an optimized low-loss path, we first need to determine the form of the tunnel. If the optimal path \u03d5\u2217 is linear, then it is called LMC. Common linear paths are linear segment, polygonal chain as Eq.(3):\n\u03d5w(t) = { 2 (tw + (0.5\u2212 t)w1) , 0 \u2264 t \u2264 0.5 2 ((t\u2212 0.5)w2 + (1\u2212 t)w) , 0.5 \u2264 t \u2264 1 , (3)\nThe parametric path train using the same hyperparameters from different random initialization. \u03d5w(0) = w1, \u03d5w(1) = w2. After deciding on the mathematical form of tunnel, the specific parameters need to be determined. Garipov et al. [66] suggest to minimize the expectation of loss \u2113(w) over a uniform distribution as Eq.(4):\nmin w \u2113(w) = min w\nEt\u223cU(0,1) [L (\u03d5w(t))] , (4)\nIn addition, the tunnel found by this way is not unique. Nevertheless, vanilla mode connectivity are not robust enough to resolve various types of adversarial attacks. Robust mode connectivity (RMC) [229] uses adversarial training (AT) [156] to find tunnels between neural networks that exhibit robustness to different types of adversarial attacks as Eq.(5):\nmin w \u2113(w) = min w\nEt\u223cU(0,1) \u2211\nmax Disti(x\u2032,x)\u2264\u03b4i\nL (\u03d5w(t); (x\u2032, y)) , (5)\nwhere \u03b4i are minimal values, Disti denotes distance measurement function. The RMC path in the parameter space improves robustness to different types of attack. Some work complements the LMC from a global connectivity perspective. Nguyen et al. [168] prove that when the number of neurons in a hidden layer is larger than a certain amount of training samples, the loss function has no so-called bad local valleys, and all the global minima are connected in a large global valley. Shevchenko et al. [202] demonstrate that as the number of neurons increases (over-parameterization), the landscape of the multi-layer network is connected, which is more conducive to LMC. Although previous studies speculate that interconnected local minima in over-parameterized networks mean the mode connectivity of the loss function, which does not always hold true (e.g., over-parameterized two-layer networks [125]). Kuditipudi et al. [125] explain mode connectivity by noise stability [8, 60], which is somewhat equivalent to dropout stability. In other words, all noise stabilization solutions can be connected in a sufficiently over-parameterized network.\nAs for the practical application of LMC, Zhao et al. [263] suggest to use LMC to repair backdoored or error-injected models. Neyshabur et al. [167] show the application of LMC to pre-trained visual models. Qin et al. [186] explore the relationship between different downstream configurations and mode connectivity of language model models."
        },
        {
            "heading": "2.2 Non-linear Mode Connectivity",
            "text": "In this subsection, we focus on the non-linear pathway connected solutions in weight space, which is known as non-linear mode connectivity [112, 186]. Bezier curve is one of the representative form of non-\nlinear path as Eq.(6): \u03d5w(t) = (1\u2212 t)2w1 + 2t(1\u2212 t)w + t2w2, 0 \u2264 t \u2264 1. (6)\nCompared with non-linear connectivity, the convex combinations (LMC) of minima within the loss basin remain in the same basin. in contrast, the nonlinear connectivity between minima are not located in the same basin, which means that the LMC is not available in some cases.\nRecent work [46, 125, 152] show that different independently trained networks can be connected by nonlinear pathways that remain in the low-loss manifold in the weight space. Qin et al. [186] speculate that there may be multiple loss basins connected by low loss nonlinear paths. Yun et al. [253] indicate that output can be obtained by connecting the Bezier curves of the two network parameters in the absence of an actual forward passing network in the Bridge network. Gotmare et al. [72] manifest that non-linear mode connectivity is widely applied to networks trained with different optimizers, data enhancement strategies and learning rate schedules. Futhermore, Lubana et al. [152] explain the principle of mode connectivity by mechanistic similarity, which is defined as the fact that two models are mechanistically similar if they make predictions using the same properties (e.g., shape or background) of the input. The mechanistic similarity of the induced models is related to LMC of two minimizers (minima). There is no LMC between mechanistically dissimilar minimizers, but mode connections can be made via relatively non-linear paths. The representative approach for finding nonlinear path [66] is similar to LMC, as Eq.(7):\nmin w \u2113(w) = min w E\u03b1\u223cqw(t) [L (\u03d5w(t))] , (7)\nwhere qw(t) is the distribution for sampling the models along the path. Moreover, Draxler et al. [46] use AutoNEB [122] and minimum spanning tree (MST) to generate the approximation of \u03d5\u2217 connecting the minima of networks on CIFAR-10 and CIFAR-100. AutoNEB connects two solutions, which updates the pivot after each iteration until AutoNEB-tunnel approaches the optimal low-loss path \u03d5\u2217. Nevertheless, the approximation of \u03d5\u2217 may fall into a local minima tunnel with unreasonable high saddle point losses.\nTo sum up, both linear and nonlinear paths can result in low test errors. While linearly connected pathways are simple, it could have certain limitations. As for non-linear mode connectivity, it is difficult to calculate the gradient on some non-linear path such as Bezier curve."
        },
        {
            "heading": "2.3 Mode Connectivity in Subspace",
            "text": "Previous work of mode connectivity [54, 56, 66] focuses on low-loss tunnels in weight space without explicitly addressing other dimensional structure. This subsection explores the mode connectivity and model training in subspace of another dimension rather than in a native parameter space. Subspace in machine learning typically describe linear structures generated by vectors in the initial vector space. There are also concepts of non-linear subspace, such as nonlinear dimensionality reduction and manifold learning [98]. Standard neural network training is performed on a full parameter space RD. Limiting the optimization to a random low-dimensional affine subspace (e.g., low-dimensional hyperplanes and hyperspheres, etc.) also leads to the similar results as full-space optimization in some cases [57, 132] , which lay the foundation for mode connectivity in subspace. Definitely, mode connectivity in oriented subspace constrain the representation ability of the model and the value range of the weights, so as to overcome the over-fitting problem of model fusion.\nRecent work attempts to implement mode connectivity in different subspace. Fort et al. [56] extend the concept of low-loss connectors (tunnels) between solutions to m-dimensional connectors (m is smaller than the dimension of full parameter space). Randomly initialized points that are not on the same wedge (i.e., a union of m-dimensional manifolds) can always pass through the intersection of their wedges, thus building a low-loss path between the different minima, as shown in Figure 2.Based on the speculation, the mdimensional hyperplanes are constructed on the piece-wise linear interpolation between the points, in which the low-loss connectors can be found. Benton et al. [11] propose simplicial point-wise random optimization (SPRO) to connect models through a multi-dimensional manifold.K ( S(w0,\u03b50), S(w1,\u03b50) ) denote simplicial complex composed of disjoint 0-simplexes. SPRO adds the join points \u03b5i to connect 0-simplexes in the complex iteratively so as to keep the loss low within the simplicial complex. It obtains a complex K by sharing\nmultiple \u03b5i. When a join point \u03b5k connects the two modes, the pathway of complex K ( S(w0,\u03b50), ..., S(wn,\u03b50) ) can be found by previous method [66]. When some joint points connects multiple modes, the solution to K ( S(w0,\u03b50,\u03b51,\u03b52), ..., S(wn,\u03b50,\u03b51,\u03b52) ) is similar to the above work [56]. For narrow architectures of networks, geodesic optimization [215] finds a low-loss pathway connecting the solutions where general tunnels of mode connectivity can not pass through a region of high loss. The mode connectivity pathways in weight space is associated to the geodesics \u03b3 (i.e., shortest paths in the space of parameterized distributions, which is regarded as a Riemannian manifold with fisher information matrix fij). The geodesics \u03b3 is obtained by minimizing the loss L(\u03b3), which is equivalent to the integral of the square root Jensen-Shannon Divergence (JSD) [35] as Eq.(8):\nL(\u03b3) = \u222b t \u221a d\u03b3i dt fij d\u03b3j dt dt = \u221a 8 \u222b \u03b3 \u221a dJSD (8)\nFurther, the mode connectivity in subspace is affected by the properties of the subspace, such as the relationship between dimension of the plane and the inherent dimension specific to the problem, the radius in the weight space, the dimensions of the hyperplane [132], etc. Moreover, Fort et al. [55] explore training tracks and subspace sampling (e.g., dropout, diagonal Gaussian, low-rank Gaussian and random subspace), which further complement relevant work of mode connectivity in subspace. In addition, recent work [42] inspires us to explore the mode connectivity in Pareto manifold to be applied to multi-task learning. In sum, the trained solutions can be found in both the full parameter space and the random low-dimensional hyperplane, as long as the points are distributed densely enough in most cases."
        },
        {
            "heading": "2.4 Discussion",
            "text": "In summary, mode connectivity provides a more novel and flexible perspective for deep model fusion. The training of neural networks tends to fall into local optima, which leads to degradation of performance. On the basis of model connectivity, we can find other models with better performance and use that as a starting point for further optimization and fusion. We can use the already trained model to move in the parameter space to reach the new target model, which can save time and computing overhead, and is suitable for situations where data is limited. Nevertheless, additional complexity and flexibility may be introduced\nto increasing the risk of overfitting when connecting different models. Therefore, the relevant hyperparameters and degree of variation should be carefully controlled. Also, mode connectivity requires fine-tuning or parameter changes, which can increase training time and resource consumption. In summary, model connectivity has many advantages in model fusion, including helping to overcome local optimal problems, providing new perspectives to explain network behavior, etc. In the future, mode connectivity is expected to help understand the inner mechanism of neural networks and provides guidance for more efficient deep model fusion designs in the future."
        },
        {
            "heading": "3 Alignment",
            "text": "Due to the randomness of channels and components from diverse networks, the active components of the networks interfere with each other [204]. So unaligned weighted averages could ignore correspondence between units from diverse models and damage useful information. For example, there is a relationship between two neurons in different models that could be completely different but functionally similar. Alignment matches the units of different models so as to obtain better initial conditions for deep model fusion. It aims to make multiple models have smaller differences and , thus enhancing the deep model fusion effects. Also, alignment can be regarded as a combinatorial optimization issue in essence. In this section, we introduce a representative mechanism \u201cRe-basin\u201d, which delivers solutions to individual basins so as to merge models with better original conditions. Following this, we divide the alignment into two types \u201cActivation matching\u201d and \u201cWeight matching\u201d depending on whether the aligned target is data-driven as Table 3."
        },
        {
            "heading": "3.1 Re-basin",
            "text": "Before introducing the specifics, we illustrate the permutation symmetry and Re-basin, which is the basic premise of alignment. Generally speaking, the number of saddle points and local optima can increase exponentially with the number of parameters even for shallow neural networks [10, 66]. It is discovered that there are invariances in training that leads to the same representation of some points among these local optima [22, 81, 140]. Specifically, the function of the network will not change if the units of hidden layer are exchanged by permutation, which is referred to as permutation symmetry [43, 50]. Formally, a \u2113-layer function of DNN f (\u2113)(x,w) = \u03c3(W (\u2113\u22121)f (\u2113\u22121) + b(\u2113\u22121)) can be described as Eq.(9) [3]:\nf (\u2113)(x,w) = P T\u03c3 ( PW (\u2113\u22121)f (\u2113\u22121) + P b(\u2113\u22121) ) , (9)\nwhere P denotes the permutation matrix. We can obtain the functional equivalent model f(x;w) = f (x;\u03c0(w)) by rearranging the input. On the basis of permutation symmetry, solutions from diverse area in weight space can generate equivalent solutions. A equivalent solution is located in a same region as the original solution with low-loss barrier (basin), as shown in Figure 2, which is referred to as \u201cRe-basin\u201d [3] as Eq.(10):\nRe-basin: f (\u2113)(x,w) = \u03c3 ( P (\u2113)W (\u2113)(P (\u2113\u22121))T f (\u2113) + P (\u2113)b(\u2113) ) (10)\nOnce the optimal permutation matrix P \u2217 is obtained, it is theoretically possible to implement model fusion: W = \u03bb1W (\u2113) 1 + \u03bb2P (\u2113)W (\u2113) 2 (P\n(\u2113\u22121))T . Compared with mode connectivity, Re-basin tends to transport the points into a basin by permutation instead of low-loss tunnels. At present, alignment is a representative approach of Re-basin[3, 178]. However, how to efficiently search for all possibilities of permutation symmetry so that all solutions point to the same basin is a current challenge.\nPermutation symmetries imposed by these invariances help us understand the structure of loss landscapes better [22, 66]. The invariances also can be seen as the source of saddle points in loss landscapes [14]. Godfrey et al. [68] investigate the algebraic structure of symmetries in neural networks and how this structure manifests itself in loss landscape geometry. Brea et al. [14] introduce permutation point in high-dimensional plateaus, at which the neurons can be exchanged without increasing losses or parameter jumps as Figure 3. Conduct gradient descent on the loss and adjust the parameter vectors \u03d1m and \u03d1n of neuron m and n, until the vectors reach the permutation point. At this time, the parameter configuration is called permutation point, and the parameter vectors and function of the two neurons are the same . Furthermore, Tatro et al. [218] explore the permutation symmetry of the nonlinear mode connectivity. Benzing et al. [12] speculate that two random initialization of a network after permutation can lead to a good performance. Furthermore, the alignment method does not always generate good low-loss connections between solutions due to variance collapse of activations. REnormalizing Permuted Activations for Interpolation Repair (REPAIR) [111] mitigates the variance collapse by rescaling the preactivation of networks, which eliminate the 90% barrier for ResNet-18 on CIFAR-10 after alignment."
        },
        {
            "heading": "3.2 Activation Matching",
            "text": "In this subsection, based on permutation symmetry, we focus on the matching of activation values. The initial models for fusion can be improved by reducing the differences in activation. Minimizing the cost functions between activations is a representative way to calculate P \u2217, which can be transformed into assignment\nproblems, such as linear assignment problem (LAP) or quadratic allocation problem (QAP), etc. They can be solved by Hungarian algorithm or Sinkhorn algorithm. The common cost functions C used in alignment are cross-correlation [218] as Eq.(11), mutual information (information entropy) [140] as Eq.(12), \u21132 distance [3] as Eq.(13), KL divergence, Wasserstein distance, etc.\nC(Am, An)cor = E [(Am \u2212 E [Am]) (An \u2212 E [An])] /\u03bem\u03ben, (11)\nC(Am, An)info = \u2211\na\u2208A(W1)m \u2211 b\u2208A(W2)n p(a, b) log ( p(a, b) p(a)p(b) ) , (12)\nC(Am, An)\u21132 = \u2225\u2225\u2225A(W 1)m \u2212 PA(W 2)n \u2225\u2225\u22252 , (13)\nwhere Am denotes the activation of unit m with standard deviation \u03be. p(a) denotes marginal probability distributions. In addition, it is discovered that using post-activation is better than using pre-activation in some cases [218]. Besides the cost functions, Singh et al. [204] use the optimal transport (OT) and Wasserstein barycenter to match the activations of different neural networks. The transport map T \u2208 R(n\u00d7m) transports neurons of W 1 optimally to neurons of W 2 in the same layer. The permutation matrix and T have a similar function, which can be obtained as Eq.(14):\nT \u2190 OT(\u00b5, \u03bd, ds) , (14)\nwhere ds denotes the support measure (reflect the \u21132 distance between activations here). \u03bd and \u00b5 are the probability measure. This kind of methods based on OT lay the foundation for some recent work [3, 4, 178]. Nevertheless, if the alignment problem is simply defined as linear problems, the second-order proximity of weights and the abundant edge information between channels could be ignored [142]."
        },
        {
            "heading": "3.3 Weight Matching",
            "text": "Instead of matching activation, we could alternatively align the models based on weight without data distribution. First, the basic approaches of weight matching is also based on minimizing the cost function to obtain P \u2217. Singh et al. [204] use the weights of the incoming edges to calculate support and probability measures to obtain the transport map T as Eq.(14). Ainsworth et al. [3] arrange the rows and columns of the modes to minimize the \u21132 distance between the weight vectors (restricted by ordinary least squares) as Eq.(15):\nC(w1, w2)\u21132 = \u2225vec (w1)\u2212 vec (\u03c0 (w2))\u22252 . (15)\nIt results in the sum of bilinear linear assignment problem (SOBLAP), which can be divided into sub-problems and solved by LAP. Different from activation matching, weight matching is not affected by data distribution. It means that all P need to be obtained by LAP, which is a complicated issue in essence. And it is difficult to leverage the gradient-based optimization. Pena et al. [178] extend the scope of cost function to all differentiable objectives, such as a midpoint as Eq.(16) and random point between w1 and w2 as Eq.(17):\nCmid (w1, w2) = C ( w1 + \u03c0 (w2)\n2\n) , (16)\nCrandom (w1, w2) = C [(1\u2212 \u03b1)w1 + \u03b1\u03c0 (w2)] , (17)\nwhere \u03b1 \u223c U(0, 1). Moreover, Sinkhorn operator S\u03c4 is added to the LAP process and Sinkhorn Re-basin is shown as Eq.(18):\nf (\u2113)(x,w) = \u03c3 [ S\u03c4 ( P (\u2113) ) W (\u2113)S\u03c4 ( (P (\u2113\u22121))T ) f (\u2113\u22121) + S\u03c4 ( P (\u2113) ) b(\u2113) ] . (18)\nIt solves non-differentiable problems and can be applied to more scenarios, such as FL [160]. Based on Beta-Bernoulli Process (BBP) [219], Yurochkin et al. [254] max the posterior of random variables pi that match neurons at any batch and the global neurons. Hungarian algorithm can be used to solve this problem to obtained Pi. In addition to minimizing the cost function, Wang et al. [227] regard the units of the model as a random permutation of global nodes based on the Beta-Bernoulli Process (BBP) [219] The permutation matrix can be obtained by BBP-MAP [254]. A simulated annealing (SA)-based method [50] searches for the valid permutations in the weight space Re-basin . Due to the high cost, it unrealistic to be applied, especially for large models. Stoica et al. [211] calculate merge matrix Pi and unmerge matrix P\u0304i to fuse the models and unmerge operations, which can be applied within the model or across the models. Instead of calculating the optimal matrix, Ainsworth et al. [3] optimize the approximate equivalent model w\u03032 iteratively and keep looking for the closest equivalent model until convergence, which minimizes L as Eq.(19):\nmin w\u03032 L ( 1 2 (w1 + proj (w\u03032)) ) , (19)\nwhere projection operations can be solved by straight-through estimator (STE), which is expensive in practic. Based on Gromov-Wasserstein barycenter (GWB) [179], Akash et al. [4] update the coupling matrix \u03a0 and W alternately to optimize Gromov-Wasserstein barycenter distance until convergence. Let k be the number of nodes, the final aligned model can be obtained as Eq.(20):\nW \u2113 \u2190 k\u2113k\u2113\u22121 1 \u22aekl\u22121\u22aeTkl 1 n n\u2211 i=1 \u03a0\u2113iW \u2113 i ( \u03a0\u2113\u22121i )\u2217T (20)\nMoreover, recent research [227, 254] proposes to alternate for a number of iterations between finding an alignment and retraining to minimize the loss barriers between SGD minimas.\nFurthermore, another significant approach of alignment is graph matching (GM) [150], which aims to match nodes in the graph using structural characteristics in the graph. Since network channels and weight can be treated as nodes and edges, the alignment issues could be turned into GM [142, 247]. General approaches could use Bipartite semi-matching or Bipartite matching [127, 140] to solve GM. Liu et al. [142] propose graduated assignment model fusion (GAMF) [230] uses second-order similarity of model weights to align neurons build on gradient assignment as Eq.(21):\nmax P = d\u03a3\u22121\u2211 i=0 d\u03a3\u22121\u2211 j=0 d\u03a3\u22121\u2211 a=0 d\u03a3\u22121\u2211 b=0 P [i,j]K [i,j,a,b]P [a,b], (21)\nwhere d\u2211 denotes the sum of dimensions,K denotes affinity tensor that calculate the affinity between the edges (i, a) and (j, b). The problem can be transformed into QAP by unifying the relationships of nodes and edges into a incidence matrix. In contrast, multi-graph matching (MGM) [108, 130, 246] ensures that the matching of two graphs is not affected by another graph, and it applies to the alignment of multiple models. Further, Uriot et al. [222] explore merging models that take into account more possible permutations."
        },
        {
            "heading": "3.4 Discussion",
            "text": "Alignment makes the models more similar by adjusting the parameters of the models, which can improve the information sharing between the models, and thus improve the generalization ability of the fused model. In addition, alignment helps improve the performance and robustness of the model on complex tasks. However, alignment methods face the problems of slow combinatorial optimization. Alignment requires additional computational overhead to adjust the model\u2019s parameters, which can lead to a more complex and time-consuming training process, especially in large depth models [142, 204].\nIn summary, alignment can improve the consistency and overall effect between different models. With the diversification of DL application scenarios, alignment will become one of the key methods to optimize deep model fusion, improve generalization ability. In the future, alignment could play a role in areas such as\ntransfer learning, domain adaptive [63], knowledge distillation, etc. For example, alignment can reduce the differences between source and target domains in transfer learning, improve the learning on new domains."
        },
        {
            "heading": "4 Weight Average",
            "text": "\u201cWeight average\u201d combines multiple weights of networks for the final model with better performance, robustness and generalization. It is also known as vanilla average [204], weight summation [131], as shown in Eq.(22): \u2211\n\u03bbiWi, (22)\nwhere each model is assigned a weighted parameter \u03bbi that controls how much it contributes to the fused model. However, different from alignment or mode connectivity, the pre-conditions of WA are relatively strict. For example, the original models must share part of the training trajectory or located in the same basin [99, 133], etc. It means that the final model can benefit from all models when the weights are similar enough but have certain differences [110]. In a flat basin, the solutions tend to demonstrate good performance. Conversely, points in narrow regions are easily accessible to energy-barriers, resulting in increased losses [167]. Previous sections focus on transporting solutions from different regions to the same basin through mode connectivity or alignment. This section will focus on the fusion of convex combinations of solutions in the same basin, which makes the merged solution closer to the midpoint (optima) of the basin with better generalization performance than endpoints, such as SWA [99], model soup [239], etc. The models discussed in this section includes the following cases:\n\u2022 Multiple similar models with certain differences.\n\u2022 Multiple models after appropriate fine-tuning on foundation models (e.g., model soup, model arithmetic, etc.).\n\u2022 Multiple checkpoints from networks with the same architectures and sharing part of the training trajectory (e.g. SWA [99], tail average [166], etc.).\nAccordingly, in this section, we review two-fold approaches of weight average \u201cWeight average\u201d and \u201cAverage in subspace\u201d. Next, we introduce representative approaches of WA \u201cModel soup\u201d , \u201cModel arithmetic\u201d and \u201cSWA\u201d. The representative approaches are listed in Table 4."
        },
        {
            "heading": "4.1 Weight Average",
            "text": "Because of the high redundancy of neural network parameters, there is usually no one-to-one correspondence between weights of different neural networks. Accordingly, there is usually no guarantee that WA will perform well by default. For trained networks with widely varying weights, the vanilla average performs poorly [204]. From a statistical point of view, WA allows the individual model parameters in the model to be controlled, which reduces the variance of the final model, resulting in a reliable effect on regularization properties and output result [77, 166].\nFirst, the weights of neural networks could be merged directly. Generally speaking, the linear interpolation of two well-trained model in different regions does not necessarily generate a well-performing model because of the nonlinear structure of neural networks [167]. However, for the solutions before and after fine-tuning are usually within a basin [95, 240], the linear interpolation of the solutions could improve he accuracy of fused model and the robustness of the distribution shift as Eq.(23):\nW = (1\u2212 t) \u00b7W0 + t \u00b7Wft. (23)\nIn addition to simple linear interpolation, the fusion of weights could be transformed into another mathematical form of aggregation. Matena et al. [159] propose Fisher merging, which regards model fusion as a approximately maximization of the joint likelihood of the posterior distribution over parameters. It use the\nFisher information Fi of the model as the posterior precision matrix to perform a Laplacian approximation, so as to obtain the Gaussian approximation log p (w | wi, Fi) of the posterior distribution as Eq.(24):\nmax w\n\u2211 \u03bbscale log p (w | wi, Fi) , (24)\nwhere \u03bbscale denotes model scalar hyperparameters. Jin et al. [109] tend to minimize the \u21132 distance between the merged model and other multiple models trained on different datasets \u27e8Xi, Yi\u27e9, which is called Regression Mean (RegMean). Accordingly, the optimization problem can be converted into linear regression problem as Eq.(25):\nmin W \u2225\u2225WTX1 \u2212WT1 X1\u2225\u22252 + \u2225\u2225WTX2 \u2212WT2 X2\u2225\u22252 . (25) Compared with Fisher average [159], RegMean obtain the inner product matrix of the linear layer input in the forward pass process, which improves the efficiency of the operation. Besides, Wei et al. [232] regard each layer of multi-layer perceptrons (MLPs) as the distribution of corresponding weights. The sub-MLPs can be clustered by neural tangent kernel (NTK) approximating, which can be solved with GWB [179]. Moreover, other works choose to average the weights of multiple experts[135] or leverage Bayesian algorithm [254] to improve the generalization and efficiency.\nAlso, some recent work focuses on increasing the diversity of models with well-behaved and varieties of weights. PopulAtion Parameter Averaging (PAPA) [110] start at the same initialization and train each models on a slightly different data set (e.g., data orderings, augmentations, regularizations, etc.), averaging these models every few epochs. It is equivalent to training a larger batch size, helping to improve the generalization of the model [86]. Further, another possible interpretation is that PAPA fuse the models under better initial conditions by improving the cosine similarity between networks (29%-61% to 95%-99%), which is similar to some work on alignment [3]. Based on the idea of maximizing the diversity of weights, Rame et al. [188] fine-tune the base model for multiple times on different auxiliary tasks and re-fine-tune these auxiliary weights so as to obtain a variety of weights. Gao et al. [65] utilize development data and softmax normalized logarithm with temperature to adjust the parameters. The models are re-parameterized and updated iteratively to ensure normalization, which could reduce overfitting and increase robustness. In addition, the mean of gradient information \u2207Xgradient could be used to optimize the WA [65]. Let \u03b7 be step size. The merged model is shown as Eq.(26):\nW = \u2211 \u03bbiWi \u2212 \u03b7\u2207Xgradient. (26)\nNext, from the perspective of iterative averaging, we can average the weights at different times during the training process of the same or architecturally identical model [65, 131, 149]. It reduces the variance and updates the model more smoothly but need to share a portion of the training history [207]. Early iterative average has the problem of convergence rate [183, 194] , especially for high-dimensional problems. Then, geometric Polyak-Ruppert [166] use the weight average instead of uniform average, and its weights decay geometrically. It uses regularization properties (control deviation characteristics of corresponding SGD estimators) to produce stable fusion results. Geometric Polyak-Ruppert helps to capture the overall trend of the gradient when training conditions are poor. In contrast, tail average [101] is more appropriate when data conditions are good. Tail average average the weights of each iteration during the last period of the training, which can prevent large fluctuations of parameter in the late stage. When the model is close to convergence, and the tail part of the gradient may contain information closer to the real gradient. Moreover, a great deal of factors (e.g., decaying step size [183], constant step size [165], form of linear interpolation, etc.) in the iteration average will affect the final result. Further, checkpoint average [25, 91, 149, 226] uses checkpoints from the same training run.\nNevertheless, simple coordinate-wise weight average may result in poor performance. Hierarchical aggregation improves model performance by combining parameters from multiple models at different layers or structures. The network architecture suitable for a specific aggregation approach has certain limitations [159, 254], so recursively processing layers with matching averages may affect the final performance. Wang et al. [227] propose a hierarchical aggregation scheme. The server obtains the first layer weight of the model\nand broadcasts it to the client, which continues to train all the layers with the matching layers frozen. And then repeats the procedures until the last layer before aggregation. Hierarchical Prompt learning (HiPro) [147] constructs a hierarchical task tree and average classifier weights generated from the global prompt and individual prompt pi. The classifier average weights on ith task \u03c4i is shown as Eq.(27):\nWi =\n\u2211 W (pi) I\u03c4j\u2211\nI\u03c4i , (27)\nwhere I is the indicator function. Its layer-wise structure helps to gain knowledge of diverse granularity. Some other work [186, 203] propose layer-wise, module-wise and matrix-wise structure of parameter division, which reduces the cost of calculation and storage and inspires more directions of WA.\nFurther, WA is often used to weight scaling rules, which average the predictions of the distribution over the weights [164, 209]. To ensure the efficiency of model average, Akhlaghi et al. [5] propose that activation functions should restrict postsynaptic activity to a limited range(e.g., sigmoid, hyperbolic tangent, etc.). Leontev et al. [131]propose other constraints that network generates presynaptic activity in the presence of native features and the mean of the weights\u2019 probability distribution should be zero [13]. In addition, for heterogeneous issue, they can be approximated by introducing additional zero-valued weights [131]."
        },
        {
            "heading": "4.2 SWA",
            "text": "Inspired by Fast Geometric Ensembling (FGE) [66] and checkpoint average [149], Izmailov et al. [99] utilize a constant or periodic learning rate to average multiple points along the SGD trajectory, which is regarded as SWA. SWA improves the training on a series of important baslines, providing better time scalability. Instead of training a set of collected models like vanilla fusion, SWA trains a single model to find smoother solutions than SGD. In Table 5, we list the approaches related to SWA . Also, SWA can be applied to any architecture or datasets and demonstrate performance than snapshot ensemble (SSE) [91] and FGE. At the end of each cycle, the SWA model WSWA is updated by averaging the newly obtained weights over the existing weights , as shown in Eq.(28):\nWSWA \u2190 WSWA \u00b7 n+W\nn+ 1 . (28)\nNevertheless, SWA can only average the points near the local optimal point, and finally get a relatively minimum value rather than accurately approximating the optima. Also, the final input sample deviation\nlength h, i.e. wi =\n\u2211i\nt=i\u2212h+1 wt h .\ncould be large or insufficient due to some factors (e.g., poor convergence at early stage, large learning rate, fast weight change rate, etc.), which results in bad overall effect. There is a good deal of work tends to change the sampling schedule of SWA. For example, SWA-Densely (SWAD) [21] uses more dense sampling points to solve the problem of insufficient random weights. Periodic-SWA (PSWA) [77] is initialized during the early stage of the operation of SGD instead of in the late convergence phase like SGD. Latest weight averaging (LAWA) [113] averages only the checkpoints collected at the end of each epoch given the large weight variation during the initial training phase. In Figure 4, we summarize several ways to optimize SWA with different sampling schedules. Some work based on SWA optimizes the polymerization process to gain competitive outcome. SWA in Low-Precision (SWALP) [248] tends to reduce the influence of quantization noise and low learning rate so as to converge to the optima. SWA-Gaussian (SWAG) [155] obtains Gaussian distribution from the points of SWA, then average the Bayesian models sampled from the distribution. Trainable Weight Averaging (TWA) [137] adjusts the fuse solution to better approximate the minimum by projecting the gradient onto the subspace as Eq.(29):\nWTWA \u2190WTWA \u2212 \u03b7lB ( B\u22a4g ) , (29)\nwhere B denotes the matrix of a set of base vectors. \u03b7l is the learning rate. g is the gradient. TWA could eliminate errors caused by static averaging in full parameters space. Different from the above approaches, Hierarchical Weighted Average (HWA) [75] combines online and offline WA into a common training framework, Online WA is designed to speed up convergence, offline WA tends to increase generalization performance. HWA tends to combines the advantages of both. Similar to SWA, Exponential Moving Average (EMA) [184, 214] is often used to smooth the model weights in order to reduce the noise and volatility of update on weights as Eq.(30):\nWEMA \u2190 \u03bbdWEMA + (1\u2212 \u03bbd)W, (30)\nwhere \u03bbd denotes the decay rate (\u2248 0.99). Some recent work [19] combines KD with EMA, using the weights of EMA (e.g., student models [217] or branches [242]) as teacher models to transfer knowledge. Huang et al. [93] replace the networks with Mixture-of-Experts (MoEs) [201] and perform the EMA on MOEs at the end of each iteration. It can be used to improve generalization on a variety of 2D and 3D vision tasks on ViT architectures. Arput et al. [9] propose simple moving average (SMA), which conducts moving average in the later stages of training (after t0 rounds of iteration) to improve the performance in out of domain as Eq.(31):\nW\u0302t = { Wt t \u2264 t0 t\u2212t0\nt\u2212t0+1W\u0302t\u22121 + 1 t\u2212t0+1Wt t \u2264 t0 . (31)\nLookahead algorithm [259] interpolates fast and slow weights linearly from the optimized trajectory. as Eq.(32):\nwslow,t+1 = t [ wfast,t + (1\u2212 t)wfast,(t\u22121) + . . .+ (1\u2212 t)(t\u22121)wfast,0 ] + (1\u2212 t)(t)wslow,0. (32)\nThe trajectories of fast weights wfast,t are updated quickly by EMA in the direction of low curvature. The slow weights wslow,t smooth the oscillations by interpolating the parameters. Lookahead reduces variance, speeds up convergence and bring the results closer to the regions with high test accuracy."
        },
        {
            "heading": "4.3 Model Soup",
            "text": "Model soup [239] refers to the method of averaging the models fine-tuned with different hyperparameters. It is simple but effective, achieving an accuracy of 90.94% on the ImageNet-1K, which surpasses the previous work on CoAtNet-7 (90.88%) [38] and ViT-G (90.45%) [255]. In Table 6, we summarize the different soups. Model soup reduces the inference time required for ensemble learning 1n \u2211n i=1 f (x,Wi) [195], which includes three soups as follows: The uniform soup average all the weights of the model directly f ( x, 1n \u2211n i=1 Wi ) . The greedy soup adds the models to the soup in sequence, keeping the model in the soup if the accuracy of the verification set does not decrease, which performs the best of the three soups as Eq.(33):\ningredients \u2190 ingredients \u222a {Wi} if Acc ( Avg ( ingredients \u222a{Wi})) \u2265 Acc(Avg(ingredients)). (33)\nGreedy soups [239] can be regarded as another form of SWA [99], which take a subset of weights as the input sample of the SWA. The learned soup removes the order rules of greedy soup, learns the mixing coefficient \u03bbmix and temperature scaling parameters \u03bbtemp for each component in the verification set, and optimizes the soup by gradient-based optimization as Eq.(34):\nargmin \u03bbmix\u2208Rk,\u03bbtemp\u2208R n\u2211 j=1 \u2113\n( \u03bbtemp \u00b7 f ( xj ,\nn\u2211 i=1 \u03bbmix,iWi\n) , yj ) . (34)\nThe adversarially-robust model soup [34] moves the convex hull of parameters of each classifier to adjust the weights of soup, in order to balance the robustness to different threat models and adapt to potential attacks. Based on reinforcement learning from human feedback (RLHF), rewarded soup [189] fine-tunes\nthe models according to the diverse rewards. It selects the proper interpolating coefficients { \u03bbji }N i=1 form N -simplex that maximize the reward R\u0302 as Eq.(35):\nargmaxnj=1 R\u0302 ( N\u2211 i=1 \u03bbjiWi ) . (35)"
        },
        {
            "heading": "4.4 Model Arithmetic",
            "text": "Different from traditional single-task learning, MTL is a kind of joint learning. The multiple tasks are learned in parallel so as to take advantage of data resources for different tasks [44, 261]. In general, MTL could be regarded as a parameter sharing, or ensemble [42], that can include major information of multiple individual tasks. In the process of MTL, participants fine-tune the latest model on the corresponding task in each iteration. The multiple fine-tuned models are merged to produce the final model or base model for the next iteration [29, 44]. The general fusion method adopted in MTL is linear combination. Patching with interpolation (PAINT) [95]combines fine-tuning and initial model so as to improve performance for specific task while also maintaining accuracy for other tasks. PAINT reduces the time of migration and adaptation between multi-tasks. HiPro [147] explore the shared information from a plenty of tasks via hierarchical structure, which adapts pre-trained vision-language models (VLMs) to multiple downstream tasks. In addition, there are some other approaches group similar tasks could together, which is conducive to obtain shared model parameters conveniently [48, 53, 158, 210]. Moreover, recent work set up metrics to measure the performance of the shared model, such as, uncertainty to weight tasks [117], loss weighting strategies [128], etc. Huang et al. [90] introduce Low-rank adaptations Hub (LoraHub), a framework that ensembles LoRA modules trained on different given tasks, which improves flexibility and scalability in MTL.\nIn MTL, the pre-trained model and tasks vectors (i.e., \u03c4i = Wft \u2212Wpre, the difference between the pretrained model and the fine-tuned model) are combined to result in better performance on all tasks. Based on this observation, task arithmetic [94] improves the performance of the model on tasks by adding and linear combination of fine-tuning task vectors, which has become a flexible and efficient method for editing pre-trained models directly as Figure 5. Ortiz et al. [174] fine-tune the pre-trained model in the tangent space and provide a more reliable way to edit the pre-trained model by NTK linearization [100], improving the task algorithm significantly by reducing the accuracy gap of individual tasks [205]. Similar to the task algorithm, Daheim et al. [37] propose elastic weight removal (EWR), which calculates difference vectors between original models and expert models (fine-tuned on positive behaviours). EWR uses Fisher [159] to average the weights of the model and task vectors as Eq.(37):\nW = \u03bb0 \u00b7 fW0 \u00b7W0 \u2212 \u03bb1 \u00b7 f\u03c41 \u00b7 \u03c41 + \u03bb2 \u00b7 f\u03c42 \u00b7 \u03c42\n\u03bb0 \u00b7 fW0 + \u03bb1 \u00b7 f\u03c41 + \u03bb2 \u00b7 f\u03c42 (36)\nIt combine Fisher merging and task arithmetic to preserve positive behaviour in the model while removing the negative behaviours. Jang et al. [102] add the sum of the vectors of a particular experts to the pretrained language models (LMs) so as to cover the information from multiple experts trained on diverse tasks. In sum, the essence of task arithmetic is to preserve pre-trained model behavior, thereby avoiding expensive joint fine-tuning on multiple tasks [95, 135, 240]."
        },
        {
            "heading": "4.5 Average in Subspace",
            "text": "Due to the large dimension of conventional full-parameter space, from tens of millions to hundreds of millions of dimensions, model fusion in subspace will constrain the training trajectory in a low-dim subspace so as to reduce the loads and difficulties [73, 132, 136, 138]. In general, DNNS are over-parameterized. The\nLow-dimensional Trajectory Hypothesis [138] speculates that the intrinsic dimension required for network training is not as large as the number of parameters given. The parameters trained and redundant information are reduced in a subspace, which could accelerate the convergence speed and improves robustness and generalization [136, 138]. Recently, Li et al. [137] demonstrate that each point in the subspace corresponds to a base. The linear combination of bases is equivalent to a weighted average [132]. Liu et al. [145] extract submodels by sparse training to fuse multiple local models in low-dimensional subspace. Leontev et al. [131] propose Elastic Weight Consolidation (EWC) to average the models in multi-dimensional space as Eq.(37):\nW = H1W1 +H2W2\nH1, +H2 , (37)\nwhere Hi = Ep(x|w) [(\n\u2202L \u2202wi\n)2] represents Hessian matrix. EWC changes the weights of individual models in\nthe direction of the minimum change in the loss function so as to prevents catastrophic forgetting [121]. But there are difficulties in the applications of WA in subspace, such as low efficiency of random basis [132], or expensive computation cost[138], etc. Moreover, when working with high-dimensional or large models, the projection matrix for projecting the gradient into the subspace can be too large for a single GPU to bear. Wortsman et al. [238] provide a way to learn model subspace in a supervised learning. Gaya et al. [67] learn a convex subspace in online adaptation in reinforcement learning. In short, how to explore the mechanism of vanilla average in subspace with numerous examples of training DNNs in subspace is a challenge for the future."
        },
        {
            "heading": "4.6 Discussion",
            "text": "WA gets the final model by averaging the weights of different deep models without additional computational complexity or training processes [109, 159]. In general, if random models have significant differences in presentation capabilities, structure, or training data, the results of fusion may not achieve the expected performance. The linear interpolation of models from scratch using the same hyperparameter configuration but with different data orders is even less effective than stochastic models [59]. Therefore, a large number of approaches described in this section aim to optimize the WA process in other mathematical ways. Further,\nwhen models share part of their optimized trajectories (e.g., checkpoint averaging, tail averaginhg, SWA [99, 149], etc.) or fine-tuned on the same pre-trained model (e.g., model soup [239], etc), the accuracy of interpolated models performs better [167]. Moreover, model soup [239] averages the models with different hyperparameter configurations to get the final result. In addition, selection of proper weights in model average can also be a challenge, which is often fraught with subjectivity. More complex weight selection mechanisms may need plenty of complex trials and cross-validation.\nWA is a promising technique in DL, which can be used as model optimization techniques in the future to reduce the weight fluctuation between different iterations, and improve the stability and convergence rate. WA can improve the aggregation stage of FL to protect privacy better and reduce communication costs in the future. Moreover, it is expected to reduce the storage space and computing overhead of the model on resource-constrained devices by implementing network compression on the terminal devices [250]. In short, WA is a promising and cost-effective DL technique, which can be applied in areas such as FL to improve performance and reduce storage overhead."
        },
        {
            "heading": "5 Ensemble Learning",
            "text": "Ensemble learning, or multi-classifier system, is a technique that integrates multiple single models to generate final predictions, including voting, average [195], etc. It improves overall performance and reduces the variance of the models, addressing issues such as overfitting, instability, and limited data volume. In this section, we demonstrate \u201cEnsemble learning\u201c in DL and related techniques \u201cModel reuse\u201c."
        },
        {
            "heading": "5.1 Ensemble Learning",
            "text": "Ensemble learning combines the outputs of networks, which surpasses than the result obtained from any model alone [7, 198, 225]. The general WA averages the model weights, that is, f ( x, 1n \u2211n i=1 Wi ) , which ends up with only one model. In contrast, ensemble learning averages the output value after inference 1 n \u2211n i=1 f (x,Wi), resulting in multiple models [239]. Ensemble learning has a long history of research. There are plenty of typical algorithms, such as Adaboost [62], Bagging [15], Stacking [236], etc. In order to make the network show better generalization ability, some previous work [16, 80] applies the ensemble learning (e.g., random forest, etc.) to DNNs, which can be used to adjust the output and take full advantages in feature selection, noise filtering. Kontschieder et al. [123] propose deep neural decision forests, which uses the random decision function in the optimization algorithm of CNN to reduce the complexity of parameters. Zhou et al. [267] introduce a decision-tree ensemble approach to demonstrates the possibility of building models without backpropagation, which needs fewer hyperparameters than a typical deep neural network. Moreover, Dropout [209] typically needs to ensemble the output of all subnets to reduce prediction errors.Nevertheless, if multiple models are too similar, the predictions of different networks will be too close to make sense of ensemble learning. To find enough diverse models, snapshot ensemble [91] uses long learning rates, combining the predictions of multiple neural networks saved at the end of each learning rate cycle to produce one final result. As an improvement on snapshot, FGE [66] uses a linear piece-wise cyclic learning rate and smaller steps to find models along the low-loss path [46], which inspires the relevant work of LMC. Similarly, Laine et al. [126] tend to ensemble the predictions over multiple previous training epochs. Arpit et al. [9] ensemble a set includes independent models and corresponding moving average models, which is referred to as ensemble of averages (EoA) as Eq.38:\ny\u0302 = argmax n\nSoftmax (\u2211 f ( x; W\u0302i )) n\n(38)\nWAK et al. [231] present a distributed robust optimization (DRO) framework to learn from a black box model, fusing multiple models using a distributed robust optimization approach. Hoang et al. [84] demonstrate the ensemble of black-box experts with no access to black-box architectures. Besides, there is a variety\nof work [126, 135] combines the ensemble learning with WA. The ensemble learning in DL achieves remarkable results and is widely used in facial recognition [233], speech recognition [40], and other practical fields."
        },
        {
            "heading": "5.2 Model Reuse",
            "text": "Based on existing pre-trained source models, model reuse [266] provides a required model applied to a new task without having to retrain the new model from scratch. It can save time and computing resources and provide better performance in the case of limited resources [249]. In addition, because the focus of transfer learning is to solve prediction tasks on the target domain, model reuse can be regarded as a kind of transfer learning. But transfer learning requires labeled data for both source and target, while in model reuse, only unlabeled data can be collected and data from source domain can not be used [153].\nDifferent from multi-classifiers ensemble learning, most current approaches reuse the existing features, labels or modalities to obtain the final prediction [176, 266] without storing a large amount of training data [245]. Fixed model reuse (FMR) [249] could be regarded as features reuse essentially. Based on fixed models or features, FMR decreases the data required during training and provides privacy protection for fixed components. But it can only use one type of source feature. Jha et al. [105] present Bag of Experts (BoE) architecture to reuse annotated data from reusable slots rather than one source domain train the target model training. Pre-trained multi-model reuse (PM2R) forms the predictions from pre-trained models into matrices and obtains the final predictions based on the consistency among different modalities. But these type of methods ignore the potential information and only can be applied to limited scenarios. Another crucial challenge of model reuse is to identify useful models from a set of pre-trained models for a given learning task. Wu et al. [244] propose reduced kernel mean embedding (RKME) specification to obtain available pre-trained models in the deployment stage. Tang et al.[216] use optional calibration strategies and types of specifications, which combines the advantages of RKME and HMR.\nUsing a single model for model reuse produces too much homogenous information (e.g., a model trained in one domain may not fit data in another domain), and it is difficult to find a single pre-trained model that is perfectly suited to the target domain. In general, we use a set of similar models to produce better performance than a single model, which is denoted as Multiple Model-Reuse (MMR) [153]. Based on MMR, Xiang et al. [245] propose PM2R without training data or validation instances. Heterogeneous model reuse (HMR) [244] tends to reuse the local models for global predictions at first and improve the local model by the multiparty multiclass margin (MPMC-margin). Instead of using the output features or labels, Lou et al. [151] improve the way of representation, and use the hidden layer representation of the source model to train the target depth model, which is superior to the approach using the limited data in target domain. nonlinear multi-model reuse (NMMR) Nevertheless, some MMR methods will assume the linear relationship between the source model and the target model strictly, which is difficult to define in practice. NMMR [153] improves\nperformance significantly by introducing a manifold regularization scheme to take advantage of arbitrary nonlinear relationships between the source and target models. Specifically, we compare the characteristics of different reuse methods in Table 7, Brifly, model reuse can significantly reduce the amount of data required by using pre-trained models to solve the problem of consuming a lot of bandwidth when transferring data between different ends. Multi-model reuse also has a wide range of applications, such as speech recognition, security and privacy interaction system, digital retina [64], etc."
        },
        {
            "heading": "5.3 Discussion",
            "text": "Compared with related model fusion algorithms such as federated learning [88, 89, 160], which have certain requirements on model parameters and sizes, ensemble methods use prediction to combine multiple heterogeneous weak classifiers without such limitations. In addition, networks with different architectures in the ensemble approacesh will have a more obvious comparison effect than weight averge. Ensemble methods, however, requires maintaining and running multiple trained models and running them together when tested. Given the larger scale and complexity of deep learning models, this approach is not suitable for applications with limited computational resources and costs [204]. Due to the diversity of ensemble learning frameworks, it is possible to achieve model diversity and enhance generalization. In the future, this will be important for dealing with changes in data and adversarial attacks. Ensemble learning in DL is expected to provide confidence estimation and uncertainty measurement for model predictions, which is critical for safety and reliability in decision support systems, autonomous driving [74], medical diagnostics, etc."
        },
        {
            "heading": "6 Application",
            "text": "In recent years, a plenty of new research has appeared in the field of deep model fusion, which has also promoted the development of this related application field. Based on the reviews of the development of model fusion and the current mainstream methods, this section summarizes some representative applications of the existing model fusion research \u201cFederated Learning\u201c, \u201cFine-tuning\u201c, \u201cDistillation\u201c and \u201cModel Fusion on Foundation Models/LLMs\u201c. In the future, more work will try to further improve the accuracy and ease of model fusion, and gradually apply the model fusion method to real-world problems."
        },
        {
            "heading": "6.1 Federated Learning",
            "text": "With the development of artificial intelligence, mobile devices, edge devices (e.g., IoT devices, sensors, etc.), and cloud computing platforms access to large amount of data. However, due to the restrictions of practical scenarios and network bandwidth, it is is fraught with risk to collect all data from edge devices [139, 208]. To address the challenges of security and centralization of data storage, FL [160, 170] allows many participants to collaborate to train shared global models while protecting data privacy, without the need to centralize datasets on a central server. It also could be regarded as a multi-party learning problem [177]. Particularly, aggregation is a significant procedure of FL, which incorporates model or parameter updates trained by various parties (such as devices, organizations, or individuals). In Figure 6, we demonstrate two different aggregation approaches in centralized and decentralized FL. Because of the efficient use of computing resources, low-cost nature (i.e., no need to transfer the entire datasets or maintain local parameters during training, etc.), Federated Averaging (FedAvg) [160] is the most influential FL algorithms. In the process of FedAvg, the local clients update the weights as Eq.39:\nw (t+1) i = w (t) i \u2212 \u03b7\u2207gi\n( w\n(t) i , \u03be (t) i\n) , (39)\nwhere \u2207gi ( w (t) i , \u03be (t) ) represents stochastic gradient on the mini-batch \u03be(t)i at tth round [114, 160]. The global model w(t) is updated as Eq.40:\nw(t+1) = 1\nn n\u2211 i=1 w (t) i . (40)\nDue to the heterogeneity of models (e.g., data distribution, bandwidth environment, network structure, permutation invariance [50], etc.), a simple aggregation of weights can adversely affect the performance of the final model and put the pressure on communication [161]. We list the common aggregation methods in Table 8. Probabilistic federated neural matching (PFNM) [254] uses the Bayesian nonparametric mechanism to adjust the global model size to accommodate the heterogeneity of data. But it can only be applied to simple architectures. FedMA [227] proposes to hierarchically match neurons of a network, which is quite difficult in practice (participant models need to have the same number of layers and structure). FedBABU [171] only aggregates the body in the aggregation phase instead of the whole network, where body is related the generality of the network and head represents personalization. It is more adaptable to adapt to the heterogeneous data of each client, and improves the representation and personalization ability of a single global model.\nMoreover, centralized gradient aggregation puts pressure on communication bandwidth and computing costs. In order to avoid the risk of failure of large-scale centralized fusion of local models, Hoang et al. [84] compare centralized and distributed gradient aggregation that occurs only in the local experts. Other recent work [88, 192] regards client updates as pseudo-gradients \u2206i, which is aggregated as Eq.(41), and the global model is updated as Eq.(42):\n\u2206\u0304(t) = 1\nn n\u2211 i=1 \u2206 (t) i (41)\nw(t+1) = w(t) \u2212 \u03b7\u2206\u0304(t) (42)\nBased on it, Jhunjhunwala et al. [106] propose FedExp, a dynamically varying pseudo-gradient self-adaptive method for caculating the server step size. FedExp accelerates convergence and reduces the overhead, which\nuses the extrapolation to accelerate Projection Onto Convex Sets (POCS) as Eq.(43):\nw (t+1) POCS = w (t) POCS \u2212 \u03bb\n( 1\nn n\u2211 i=1 Pi ( w (t) POCS ) \u2212w(t)POCS ) (43)\nHuang et al. [92] aggregate personalized sparse gradients and masks trained from local models to generate new global model as Eq.(44):\nw(t+1) = w(t) \u2212 1 |St|\n\u2211( w\u0303\n(t) 0 \u2212 w\u0303(t)n\n) , (44)\nwhere St denotes the clients. It reduces the communication overhead and solves the issues of sparse personalized FL. In addition, the application of personalized model to FL could adapt the preferences of local users and decrease the costs [43, 51, 127].\nSince ensemble learning does not require averaging weights, it could be a good tool for aggregation and support heterogeneous client models. One-shot [76] utilizes ensemble learning to aggregate the local model, which achieves a relative gain of 51.5 % over the baseline on the AUC. Similarly, there are plenty of researches that applies the ensemble learning to FL [82, 257]. Under certain conditions ( im < \u221a is where im denotes machines, is is samples), the performance of the direct weight aggregation can be comparable to centralized algorithm that can access all samples in data distributed communication [262]. Nevertheless, it is not available to apply ensemble learning techniques directly in FL due to the heavy burden of keeping all the received models on the server. KD could solve these problems and regularize the size of global model and local learning using multi-teachers ensemble methods [268]. Recent work [70, 104, 141] present some novel FL framework based on ensemble distillation. FedFTG [258] does not directly broadcast the aggregate model back to each client, but uses knowledge extracted from the local model to fine-tune this preliminary global model in the server, mitigating the performance degradation after the model is aggregated. FedDF breaks the communication barrier between heterogeneous client models [87]. FedCVAE-KD [82] uses a lightweight knowledge distillation process to aggregate the client decoders, which generates substantially samples than FedAvg. It address the statistical heterogeneity and pipeline security [265] (i.e., outside attacker who obtains transferred data cannot train a classifier) concerns.\nIn short, the essence of the aggregation step in FL is a model fusion technique. Selecting a reasonable model fusion method can reduce the impact of specific participants or individual data on the final model, so\nas to improve the generalization ability and adaptability of the model in the global scope. In future work, a good aggregation approach is expected to be helpful in facing a series of challenges in federated learning. In future work, a high-quality and scalable aggregation approache are expected to face a series of challenges in FL, such as client heterogeneity, non-i.i.d heterogeneous data, limited computing resources [141], etc. FL is expected to show its potential in many more areas, such as NLP, recommendation systems [146], medical image analysis [144], etc."
        },
        {
            "heading": "6.2 Fine-tuning",
            "text": "Fine-tuning a base mode, such as pre-trained model, is an efficient approach for adjusting models to perform downstream tasks [23, 41], which results in better generalization and more accurate output with less labeled data. Compared with random initialization, a pre-trained model is trained by a relatively set of task-specific data, which is always a better standard starting point for training. Nevertheless. the average of existing fine-tuned models [28, 29] is even a better base model than the vanilla pre-trained model for fine-tuning on the downstream tasks. Besides, there is a great deal of recent work combining WA with fine-tuning as shown in Figure 7, such as model soup [239], DiWA [190], etc. Fine-tuning improves the accuracy on target distribution, but often leads to a decrease in the robustness of distribution shift. WiSE-FT [240] combines the weights of the zero-shot and fine-tuned models to improve the distribution shift accuracy while retaining the high accuracy of the target distribution. Local fine-tuning (Lo-fi) [237] fine-tunes each node independently without any communication, and then averages the nodes. Lo-fi can also improve the\nperformance of distributed shifts. Collaborative Descent fusion (ColD) [44] replaces base models with fusion models that can be recycled, which can continually improve the pre-trained models on which they are based. ColD [44] is superior to RoBERTa [148] and even previous multitasking models. While these strategies for averaging the fine-tuned models may be simple, they do not take full advantage of the connections between each fine-tuned model. Therefore, training on an intermediate task before before training on a target task can explore the capabilities of the base models [180, 185, 224]. Inspired by inter-training strategies [185], Rame et al. [188] fine-tune the models on auxiliary tasks, which utilize diverse auxiliary tasks and improve the out-of-distribution (OOD) generalization.\nThe average of fine-tuned models reduces the training time required to achieve the goal [28] and generates more accurate and better generalized models. Essentially, different ways of fine-tuning (e.g., fine-tuning with frozen layers, top-layer fine-tuning, etc.) also have a certain impact on final accuracy and distribution shift [240]. However, the combination of WA and fine-tuning is an expensive overhead, which has a certain limitation on specific application. Also, it may face a problem of explosion of preservation checkpoints, or catastrophic forgetting [121], especially applied to transfer learning."
        },
        {
            "heading": "6.3 Distillation",
            "text": "Knowledge distillation (KD) [83] is a significant method to ensemble multiple models, which involves the following two types of models. A teacher model denotes large and powerful model trained on large-scale\ndata and has high predictive and expressive power. A student models is a relatively smaller model with fewer parameters and computational resource [18, 199]. Using the knowledge of the teacher (e.g., the output probability distribution, hidden layer representation, etc.) to guide the training, the student could achieve the prediction ability closed to the large model with fewer resources and faster speed [2, 119, 124, 221]. Given that multiple teachers or students are expected to have a preferable performance than a single model [6], we divide KD into two categories according to the aggregated objects as Figure 8.\nThe first type of approach is to merge multiple teacher models and distill the student model directly, as shown in Table 9. Currently, recent work mainly integrates the output of teachers (e.g., logits [6, 49, 252] or feature-based knowledge [143, 241], etc.). Ensemble distillation (ED) [141, 157] distills the average output of multiple teachers to a student model, which can make up for the shortcomings of a single teacher model and provide more diversified and comprehensive information to the student model. FedDF [141] distills a collection of client-teacher models |St| into a server-student model. It averages the logit output f ( x\u0302kt ) of teachers as Eq.(45):\nxt,j := xt,j\u22121 \u2212 \u03b7 \u2202KL\n( \u03c3 (\n1 |St| \u2211 k\u2208St f ( x\u0302kt )) , \u03c3 (f (xt,j\u22121)) )\n\u2202xt,j\u22121 , (45)\nwhere t is the communication round, KL means KL divergence. The ensemble part of FedDF does not affect the overall workflows of clients and solves the loss problem of network batch normalization (BN) [87], Wu et al. [241] propose a multi-teacher adaptive distillation framework that can transfer knowledge from multiple teachers to student without the need for source domain data. Although merging multiple teachers makes up for the shortcomings of a single teacher, some of the teacher\u2019s information may be overlooked when conflict exist among teachers.\nThe other way is to use the teacher model to distill multiple students and then merge these student models. Co-distillation (CD) [6] regards each client device as a student model, treats the average of the logits output of the other devices as teacher\u2018s output . However, the same training data sample should be used to synchronize the output of the teacher and the local student model. In order to solve the problem of CD, FD [104] uploads these local average logit vectors to the server periodically. Each average logits with the associated label as the current training sample will be used as the distillation regularizer for the next round of local device computation. FD improves performance and reduces communication rounds significantly. However, merging multi-students also has some problems, such as large demand of computing resources, poor interpretation and over-dependence on the original model."
        },
        {
            "heading": "6.4 Model Fusion on Foundation Models/LLMs",
            "text": "Foundation models show strong performance and emergent abilities when dealing with complex tasks, Large foundation models are characterized by their sheer size, containing billions of parameters that help them learn complex patterns in the data. Especially, with the emergence of new LLMs [200, 264] recently, such as GPT-3 [17, 172], T5 [187], BERT [41], Megatron-LM, the application of WA [154, 212, 256] to LLMs attracts more attention. You et al. [251] propose B-tuning using Bayesian learning to calculate posterior prediction distribution, which tunes top-K ranked pre-trained models by their transferabilities. Zoo-tuning [203] aggregates the weights of pre-trained model with aligned channels to obtain the final model adapt to downstream tasks, which improve the issue of high cost of migrating on large models.\nBesides, recent work [120, 256] tends to craft better framework and modules adapted to the application LLMs. Izacard et al. [97] present fusion-in-decoder (FiD) , a novel framework to perform evidence fusion in the decoder only, which aims to efficiently aggregate multiple passages. Based on FiD, Ravaut et al. [191] introduce Summa Fusion to concatenate the representations of summary candidates, which further explores the effectiveness of fusion in the context of text summarization. However, their results improve little because they do not filter out poor quality candidates before using the algorithm. In contrast, Jiang et al. [107] propose an ensemble framework LLM-BLENDER, which focus on identifying subtle differences in the output of different candidates by PairRanker algorithm, and then ranking and summarizing the candidates to achieve better performance. Huang et al. [90] introduce Low-rank adaptations hub (LoRAHub), a framework to combine multiple LoRA modules trained on different tasks, which is designed to increase the adaptability of the LLMs and reduce training costs.\ndue to the high performance and low computational resources, the application of fine-tuning to large foundation models improve obustness of distribution shifts [240] . Branch-Train-Merge (BTM) [135] reduce the large amount of multi-node synchronization required for parallel LLMs training. In addition, the negative task vector of task arithmetic [174] can reduce the number of toxic generations of LLMs. For example, it decreases the amount from 4.8 % to 0.8 % in GPT-2 [94]."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this survey, we review the deep model fusion techniques which aims at improving the performance of the model. We propose a new categorization that groups the tecnologies of deep model fusion into four perspective: \u201cmode connectivity\u201d, \u201calignment,\u201d \u201cweight average\u201d and \u201censemble learning\u201d. In the first three chapters, we describe the fusion of model\u2019s weight to obtain the superior final fused model. In the \u201censemble learning\u201d, we focus on the fusion of the output of deep models with a wealth of available methods and a large number of ensemble frameworks. We summarize the common methods from the point of view of algorithm design and performance, and compare the differences, advantages and disadvantages of different approaches. Finally, we discusses the applications and engineering prospects of deep model fusion technology in FL, distillation, LLMs, etc.\nWe not only summarize current technologies of deep model fusion, but also point out the bottlenecks and breakthrough. The survey is expected to help the developers improve the performance of deep model fusion technologies, and indicate the promising and valuable directions. In the future, it is worth designing novel deep model fusion strategies from innovative aggregation patterns, better initial conditions, diverse ensemble frameworks and other perspectives. The abundant information in the loss landscape and the potential relationships between the components of networks remain to be further exploited. In addition, better adaptive methods are expected to be applied in heterogeneous models and complex real scenarios, such as FL, large-scale models, transfer learning, etc. Also, we need to pay attention to the practical effects to promote the development and application of deep model fusion technologies."
        }
    ],
    "title": "Deep Model Fusion: A Survey",
    "year": 2023
}