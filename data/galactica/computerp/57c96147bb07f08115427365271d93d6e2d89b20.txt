This paper proposes a fine-grained self-localization method for outdoor robotics that utilizes a flexible number of onboard cameras and readily accessible satellite images. The proposed method addresses limitations in existing cross-view localization methods that struggle to handle noise sources such as moving objects and seasonal variations. It is the first sparse visual-only method that enhances perception in dynamic environments by detecting view-consistent key points and their corresponding deep features from ground and satellite views, while removing off-the-ground objects and establishing homography transformation between the two views. Moreover, the proposed method incorporates a spatial embedding approach that leverages camera intrinsic and extrinsic information to reduce the ambiguity of purely visual matching, leading to improved feature matching and overall pose estimation accuracy. The method exhibits strong generalization and is robust to environmental changes, requiring only geo-poses as ground truth. Extensive experiments on the KITTI and Ford Multi-AV Seasonal datasets demonstrate that our proposed method outperforms existing state-of-the-art methods, achieving median spatial accuracy errors below 0.5 meters along the lateral and longitudinal directions, and a median orientation accuracy error below 2◦ 1. 1. conclusion This paper presents PureACL, a novel cross-view localization approach for accurate 3-DoF pose estimation that supports flexible multi-camera inputs. Our approach utilizes a view-consistent on-ground keypoint detector to handle dynamic objects and viewpoint variations while removing off-the-ground objects to establish the homography transformer assumption. Additionally, PureACL incorporates a spatial embedding that maximizes the use of camera intrinsic and extrinsic information to reduce visual matching ambiguity. PureACL is the first sparse visual-only approach and the first visual-only cross-view method capable of achieving a mean translation error of less than one meter. 8. acknowledgements The research is funded in part by an ARC Discovery Grant (grant ID: DP220100800) to HL. a. evaluation of other fordav-cvl dataset logs 12  b. performance with different initial poses 12 C. Visualization of Confidence Maps 13 a. evaluation of other fordav-cvl dataset logs The ’Log4’ trajectory was chosen for method evaluation in SIBCL [33] owing to its alignment accuracy with the satellite image. Furthermore, we also evaluated other logs from the FordAV-CVL Dataset in Tab. 4 to supplement the results presented in Tab. 2 of the main paper. There are three travelings included in every log: ’2017-08-04-26’, ’2017-07-24’, and ’2017-10- 26’. For the purpose of training, evaluation, and test dataset split, we use ’2017-07-24’ as the evaluation dataset for all logs. We select the traveling sequence with a higher number of images as the training dataset. Specifically, the training dataset of ’Log1’ and ’Log3’ is ’2017-10-26’, whereas the training dataset of ’Log4’ and ’Log5’ is ’2017-08-04-26’. The results demonstrate that our method is capable of estimating accurate 3-DoF pose with low spatial and angular errors in various scenarios, including freeway (log1), residential (log3, log5), university (log4) and vegetation (log4, log5). b. performance with different initial poses In our main paper, we presented a chart illustration in Fig.7. Here, we further provide complete metrics results in Tab. 5 and Tab. 6. By presenting these tables, we aim to provide a comprehensive view of the data and enable readers to analyze the metrics more thoroughly. In order to facilitate comparison, we have included a performance analysis with a single front onboard camera from the FordAV-CVL dataset, as depicted in Fig. 8. To bring a comprehensive view of the data and enable readers to analyze the metrics more thoroughly, we further provide complete metrics results in Tab. 5 and Tab. 6, as a supplementary of the chat illustration in Fig. 7 in our main paper. In order to facilitate comparison, we have included a performance analysis with a single front onboard camera from the FordAV-CVL dataset, as depicted in Fig. 8