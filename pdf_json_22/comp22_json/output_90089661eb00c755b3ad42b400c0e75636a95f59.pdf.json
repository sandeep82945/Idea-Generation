{
    "abstractText": "Given an irregular dense tensor, how can we efficiently analyze it? An irregular tensor is a collection of matrices whose columns have the same size and rows have different sizes from each other. PARAFAC2 decomposition is a fundamental tool to deal with an irregular tensor in applications including phenotype discovery and trend analysis. Although several PARAFAC2 decomposition methods exist, their efficiency is limited for irregular dense tensors due to the expensive computations involved with the tensor. In this paper, we propose DPAR2, a fast and scalable PARAFAC2 decomposition method for irregular dense tensors. DPAR2 achieves high efficiency by effectively compressing each slice matrix of a given irregular tensor, careful reordering of computations with the compression results, and exploiting the irregularity of the tensor. Extensive experiments show that DPAR2 is up to 6.0\u00d7 faster than competitors on real-world irregular tensors while achieving comparable accuracy. In addition, DPAR2 is scalable with respect to the tensor size and target rank.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jun-Gi Jang"
        },
        {
            "affiliations": [],
            "name": "U Kang"
        }
    ],
    "id": "SP:97fc42c21a493e1140ce9248f63e9eecabb72de5",
    "references": [
        {
            "authors": [
                "Y.-R. Lin",
                "J. Sun",
                "P. Castro",
                "R. Konuru",
                "H. Sundaram",
                "A. Kelliher"
            ],
            "title": "Metafac: community discovery via relational hypergraph factorization",
            "venue": "KDD, 2009, pp. 527\u2013536.",
            "year": 2009
        },
        {
            "authors": [
                "S. Spiegel",
                "J. Clausen",
                "S. Albayrak",
                "J. Kunegis"
            ],
            "title": "Link prediction on evolving data using tensor factorization",
            "venue": "PAKDD. Springer, 2011, pp. 100\u2013110.",
            "year": 2011
        },
        {
            "authors": [
                "J.-G. Jang",
                "U. Kang"
            ],
            "title": "Fast and memory-efficient tucker decomposition for answering diverse time range queries",
            "venue": "Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, 2021, pp. 725\u2013735.",
            "year": 2021
        },
        {
            "authors": [
                "S. Oh",
                "N. Park",
                "J.-G. Jang",
                "L. Sael",
                "U. Kang"
            ],
            "title": "High-performance tucker factorization on heterogeneous platforms",
            "venue": "IEEE Transactions on Parallel and Distributed Systems, vol. 30, no. 10, pp. 2237\u20132248, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "T. Kwon",
                "I. Park",
                "D. Lee",
                "K. Shin"
            ],
            "title": "Slicenstitch: Continuous cp decomposition of sparse tensor streams",
            "venue": "ICDE. IEEE, 2021, pp. 816\u2013827.",
            "year": 2021
        },
        {
            "authors": [
                "D. Ahn",
                "S. Kim",
                "U. Kang"
            ],
            "title": "Accurate online tensor factorization for temporal tensor streams with missing values",
            "venue": "CIKM \u201921: The 30th ACM International Conference on Information and Knowledge Management, Virtual Event, Queensland, Australia, November 1 - 5, 2021, G. Demartini, G. Zuccon, J. S. Culpepper, Z. Huang, and H. Tong, Eds. ACM, 2021, pp. 2822\u20132826.",
            "year": 2021
        },
        {
            "authors": [
                "D. Ahn",
                "J.-G. Jang",
                "U. Kang"
            ],
            "title": "Time-aware tensor decomposition for sparse tensors",
            "venue": "Machine Learning, Sep 2021.",
            "year": 2021
        },
        {
            "authors": [
                "D. Ahn",
                "S. Son",
                "U. Kang"
            ],
            "title": "Gtensor: Fast and accurate tensor analysis system using gpus",
            "venue": "CIKM \u201920: The 29th ACM International Conference on Information and Knowledge Management, Virtual Event, Ireland, October 19-23, 2020, M. d\u2019Aquin, S. Dietze, C. Hauff, E. Curry, and P. Cudr\u00e9-Mauroux, Eds. ACM, 2020, pp. 3361\u20133364.",
            "year": 2020
        },
        {
            "authors": [
                "D. Choi",
                "J.-G. Jang",
                "U. Kang"
            ],
            "title": "S3cmtf: Fast, accurate, and scalable method for incomplete coupled matrix-tensor factorization",
            "venue": "PLOS ONE, vol. 14, no. 6, pp. 1\u201320, 06 2019.",
            "year": 2019
        },
        {
            "authors": [
                "N. Park",
                "S. Oh",
                "U. Kang"
            ],
            "title": "Fast and scalable method for distributed boolean tensor factorization",
            "venue": "The VLDB Journal, Mar 2019.",
            "year": 2019
        },
        {
            "authors": [
                "I. Perros",
                "E.E. Papalexakis",
                "F. Wang",
                "R.W. Vuduc",
                "E. Searles",
                "M. Thompson",
                "J. Sun"
            ],
            "title": "Spartan: Scalable PARAFAC2 for large & sparse data",
            "venue": "SIGKDD. ACM, 2017, pp. 375\u2013384.",
            "year": 2017
        },
        {
            "authors": [
                "A. Afshar",
                "I. Perros",
                "E.E. Papalexakis",
                "E. Searles",
                "J.C. Ho",
                "J. Sun"
            ],
            "title": "COPA: constrained PARAFAC2 for sparse & large datasets",
            "venue": "CIKM. ACM, 2018, pp. 793\u2013802.",
            "year": 2018
        },
        {
            "authors": [
                "N.E. Helwig"
            ],
            "title": "Estimating latent trends in multivariate longitudinal data via parafac2 with functional and structural constraints",
            "venue": "Biometrical Journal, vol. 59, no. 4, pp. 783\u2013803, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "B.M. Wise",
                "N.B. Gallagher",
                "E.B. Martin"
            ],
            "title": "Application of parafac2 to fault detection and diagnosis in semiconductor etch",
            "venue": "Journal of Chemometrics: A Journal of the Chemometrics Society, vol. 15, no. 4, pp. 285\u2013298, 2001.",
            "year": 2001
        },
        {
            "authors": [
                "Y. Ren",
                "J. Lou",
                "L. Xiong",
                "J.C. Ho"
            ],
            "title": "Robust irregular tensor factorization and completion for temporal health data analysis",
            "venue": "CIKM. ACM, 2020, pp. 1295\u20131304.",
            "year": 2020
        },
        {
            "authors": [
                "A. Afshar",
                "I. Perros",
                "H. Park",
                "C. Defilippi",
                "X. Yan",
                "W. Stewart",
                "J. Ho",
                "J. Sun"
            ],
            "title": "Taste: Temporal and static tensor factorization for phenotyping electronic health records",
            "venue": "Proceedings of the ACM Conference on Health, Inference, and Learning, 2020, pp. 193\u2013203.",
            "year": 2020
        },
        {
            "authors": [
                "K. Yin",
                "A. Afshar",
                "J.C. Ho",
                "W.K. Cheung",
                "C. Zhang",
                "J. Sun"
            ],
            "title": "Logpar: Logistic parafac2 factorization for temporal binary data with missing values",
            "venue": "SIGKDD, 2020, pp. 1625\u20131635.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Cheng",
                "M. Haardt"
            ],
            "title": "Efficient computation of the PARAFAC2 decomposition",
            "venue": "ACSCC. IEEE, 2019, pp. 1626\u20131630.",
            "year": 2019
        },
        {
            "authors": [
                "T.G. Kolda",
                "B.W. Bader"
            ],
            "title": "Tensor decompositions and applications",
            "venue": "SIAM Review, vol. 51, no. 3, pp. 455\u2013500, 2009.",
            "year": 2009
        },
        {
            "authors": [
                "N. Halko",
                "P. Martinsson",
                "J.A. Tropp"
            ],
            "title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions",
            "venue": "SIAM Review, vol. 53, no. 2, pp. 217\u2013288, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "F. Woolfe",
                "E. Liberty",
                "V. Rokhlin",
                "M. Tygert"
            ],
            "title": "A fast randomized algorithm for the approximation of matrices",
            "venue": "Applied and Computational Harmonic Analysis, vol. 25, no. 3, pp. 335\u2013366, 2008.",
            "year": 2008
        },
        {
            "authors": [
                "K.L. Clarkson",
                "D.P. Woodruff"
            ],
            "title": "Low-rank approximation and regression in input sparsity time",
            "venue": "JACM, vol. 63, no. 6, p. 54, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "R.A. Harshman"
            ],
            "title": "Parafac2: Mathematical and technical notes",
            "venue": "UCLA working papers in phonetics, vol. 22, no. 3044, p. 122215, 1972.",
            "year": 1972
        },
        {
            "authors": [
                "H.A. Kiers",
                "J.M. Ten Berge",
                "R. Bro"
            ],
            "title": "Parafac2\u2014part i. a direct fitting algorithm for the parafac2 model",
            "venue": "Journal of Chemometrics: A Journal of the Chemometrics Society, vol. 13, no. 3-4, pp. 275\u2013294, 1999.",
            "year": 1999
        },
        {
            "authors": [
                "M. Defferrard",
                "K. Benzi",
                "P. Vandergheynst",
                "X. Bresson"
            ],
            "title": "FMA: A dataset for music analysis",
            "venue": "ISMIR, 2017. [Online]. Available: https://arxiv.org/abs/1612.01840",
            "year": 2017
        },
        {
            "authors": [
                "J. Salamon",
                "C. Jacoby",
                "J.P. Bello"
            ],
            "title": "A dataset and taxonomy for urban sound research",
            "venue": "Proceedings of the ACM International Conference on Multimedia, MM \u201914, Orlando, FL, USA, November 03 - 07, 2014. ACM, 2014, pp. 1041\u20131044.",
            "year": 2014
        },
        {
            "authors": [
                "J. Wang",
                "Z. Liu",
                "Y. Wu",
                "J. Yuan"
            ],
            "title": "Mining actionlet ensemble for action recognition with depth cameras",
            "venue": "2012 IEEE Conference on Computer Vision and Pattern Recognition, Providence, RI, USA, June 16-21, 2012. IEEE Computer Society, 2012, pp. 1290\u20131297.",
            "year": 2012
        },
        {
            "authors": [
                "F. Karim",
                "S. Majumdar",
                "H. Darabi",
                "S. Harford"
            ],
            "title": "Multivariate lstmfcns for time series classification",
            "venue": "2018.",
            "year": 2018
        },
        {
            "authors": [
                "F. Schimbinschi",
                "X.V. Nguyen",
                "J. Bailey",
                "C. Leckie",
                "H. Vu",
                "R. Kotagiri"
            ],
            "title": "Traffic forecasting in complex urban networks: Leveraging big data and machine learning",
            "venue": "Big Data (Big Data), 2015 IEEE International Conference on. IEEE, 2015, pp. 1019\u20131024.",
            "year": 2015
        },
        {
            "authors": [
                "B.W. Bader",
                "T.G. Kolda"
            ],
            "title": "Matlab tensor toolbox version 3.0-dev",
            "venue": "Available online, Oct. 2017. [Online]. Available: https: //www.tensortoolbox.org",
            "year": 2017
        },
        {
            "authors": [
                "J. Jung",
                "J. Yoo",
                "U. Kang"
            ],
            "title": "Signed random walk diffusion for effective representation learning in signed graphs",
            "venue": "PLOS ONE, vol. 17, no. 3, pp. 1\u201319, 03 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J. Jung",
                "W. Jin",
                "H. Park",
                "U. Kang"
            ],
            "title": "Accurate relational reasoning in edge-labeled graphs by multi-labeled random walk with restart",
            "venue": "World Wide Web, vol. 24, no. 4, pp. 1369\u20131393, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J. Jung",
                "W. Jin",
                "U. Kang"
            ],
            "title": "Random walk-based ranking in signed social networks: model and algorithms",
            "venue": "Knowl. Inf. Syst., vol. 62, no. 2, pp. 571\u2013610, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "W. Jin",
                "J. Jung",
                "U. Kang"
            ],
            "title": "Supervised and extended restart in random walks for ranking and link prediction in networks",
            "venue": "PLOS ONE, vol. 14, no. 3, pp. 1\u201323, 03 2019.",
            "year": 2019
        },
        {
            "authors": [
                "L. Page",
                "S. Brin",
                "R. Motwani",
                "T. Winograd"
            ],
            "title": "The pagerank citation ranking: Bringing order to the web.",
            "year": 1999
        },
        {
            "authors": [
                "J. Jung",
                "N. Park",
                "S. Lee",
                "U. Kang"
            ],
            "title": "Bepi: Fast and memory-efficient method for billion-scale random walk with restart",
            "venue": "Proceedings of the 2017 ACM International Conference on Management of Data, 2017, pp. 789\u2013804.",
            "year": 2017
        },
        {
            "authors": [
                "J. Jang",
                "U. Kang"
            ],
            "title": "D-tucker: Fast and memory-efficient tucker decomposition for dense tensors",
            "venue": "ICDE. IEEE, 2020, pp. 1850\u2013 1853.",
            "year": 2020
        },
        {
            "authors": [
                "O.A. Malik",
                "S. Becker"
            ],
            "title": "Low-rank tucker decomposition of large tensors using tensorsketch",
            "venue": "NeurIPS, 2018, pp. 10 117\u201310 127.",
            "year": 2018
        },
        {
            "authors": [
                "Y. Wang",
                "H.F. Tung",
                "A.J. Smola",
                "A. Anandkumar"
            ],
            "title": "Fast and guaranteed tensor decomposition via sketching",
            "venue": "NeurIPS, 2015, pp. 991\u2013999.",
            "year": 2015
        },
        {
            "authors": [
                "C.E. Tsourakakis"
            ],
            "title": "MACH: fast randomized tensor decompositions",
            "venue": "SDM, 2010, pp. 689\u2013700.",
            "year": 2010
        },
        {
            "authors": [
                "C. Battaglino",
                "G. Ballard",
                "T.G. Kolda"
            ],
            "title": "A practical randomized CP tensor decomposition",
            "venue": "SIAM J. Matrix Anal. Appl., vol. 39, no. 2, pp. 876\u2013901, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "A. Gittens",
                "K.S. Aggour",
                "B. Yener"
            ],
            "title": "Adaptive sketching for fast and convergent canonical polyadic decomposition",
            "venue": "ICML, ser. Proceedings of Machine Learning Research, vol. 119. PMLR, 2020, pp. 3566\u20133575.",
            "year": 2020
        },
        {
            "authors": [
                "A.H. Phan",
                "A. Cichocki"
            ],
            "title": "PARAFAC algorithms for large-scale problems",
            "venue": "Neurocomputing, vol. 74, no. 11, pp. 1970\u20131984, 2011.",
            "year": 1970
        },
        {
            "authors": [
                "X. Li",
                "S. Huang",
                "K.S. Candan",
                "M.L. Sapino"
            ],
            "title": "2pcp: Twophase CP decomposition for billion-scale dense tensors",
            "venue": "32nd IEEE International Conference on Data Engineering, ICDE 2016, Helsinki, Finland, May 16-20, 2016, 2016, pp. 835\u2013846.",
            "year": 2016
        },
        {
            "authors": [
                "W. Austin",
                "G. Ballard",
                "T.G. Kolda"
            ],
            "title": "Parallel tensor compression for large-scale scientific data",
            "venue": "IPDPS, 2016, pp. 912\u2013922.",
            "year": 2016
        },
        {
            "authors": [
                "D. Chen",
                "Y. Hu",
                "L. Wang",
                "A.Y. Zomaya",
                "X. Li"
            ],
            "title": "H-PARAFAC: hierarchical parallel factor analysis of multidimensional big data",
            "venue": "IEEE Trans. Parallel Distrib. Syst., vol. 28, no. 4, pp. 1091\u20131104, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "E. Gujral",
                "G. Theocharous",
                "E.E. Papalexakis"
            ],
            "title": "Spade: S treaming pa rafac2 de composition for large datasets",
            "venue": "SDM. SIAM, 2020, pp. 577\u2013585.",
            "year": 2020
        },
        {
            "authors": [
                "K. Yin",
                "W.K. Cheung",
                "B.C. Fung",
                "J. Poon"
            ],
            "title": "Tedpar: Temporally dependent parafac2 factorization for phenotype-based disease progression modeling",
            "venue": "SDM. SIAM, 2021, pp. 594\u2013602.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "In this paper, we propose DPAR2, a fast and scalable PARAFAC2 decomposition method for irregular dense tensors. DPAR2 achieves high efficiency by effectively compressing each slice matrix of a given irregular tensor, careful reordering of computations with the compression results, and exploiting the irregularity of the tensor. Extensive experiments show that DPAR2 is up to 6.0\u00d7 faster than competitors on real-world irregular tensors while achieving comparable accuracy. In addition, DPAR2 is scalable with respect to the tensor size and target rank.\nIndex Terms\u2014irregular dense tensor, PARAFAC2 decomposition, efficiency\nI. INTRODUCTION\nHow can we efficiently analyze an irregular dense tensor? Many real-world multi-dimensional arrays are represented as irregular dense tensors; an irregular tensor is a collection of matrices with different row lengths. For example, stock data can be represented as an irregular dense tensor; the listing period is different for each stock (irregularity), and almost all of the entries of the tensor are observable during the listing period (high density). The irregular tensor of stock data is the collection of the stock matrices whose row and column dimension corresponds to time and features (e.g., the opening price, the closing price, the trade volume, etc.), respectively. In addition to stock data, many real-world data including music song data and sound data are also represented as irregular dense tensors. Each song can be represented as a slice matrix (e.g., time-by-frequency matrix) whose rows correspond to the time dimension. Then, the collection of songs is represented as an irregular tensor consisting of slice matrices of songs each of whose time length is different. Sound data are represented similarly.\nTensor decomposition has attracted much attention from the data mining community to analyze tensors [1]\u2013[10]. Specifically, PARAFAC2 decomposition has been widely used for modeling irregular tensors in various applications including\nphenotype discovery [11], [12], trend analysis [13], and fault detection [14]. However, existing PARAFAC2 decomposition methods are not fast and scalable enough for irregular dense tensors. Perros et al. [11] improve the efficiency for handling irregular sparse tensors, by exploiting the sparsity patterns of a given irregular tensor. Many recent works [12], [15]\u2013[17] adopt their idea to handle irregular sparse tensors. However, they are not applicable to irregular dense tensors that have no sparsity pattern. Although Cheng and Haardt [18] improve the efficiency of PARAFAC2 decomposition by preprocessing a given tensor, there is plenty of room for improvement in terms of computational costs. Moreover, there remains a need for fully employing multicore parallelism. The main challenge to successfully design a fast and scalable PARAFAC2 decomposition method is how to minimize the computational costs involved with an irregular dense tensor and the intermediate data generated in updating factor matrices.\nIn this paper, we propose DPAR2 (Dense PARAFAC2 decomposition), a fast and scalable PARAFAC2 decomposition method for irregular dense tensors. Based on the characteristics of real-world data, DPAR2 compresses each slice matrix of a given irregular tensor using randomized Singular Value Decomposition (SVD). The small compressed results and our careful ordering of computations considerably reduce the computational costs and the intermediate data. In addition, DPAR2 maximizes multi-core parallelism by considering the difference in sizes between slices. With these ideas, DPAR2 achieves higher efficiency and scalability than existing PARAFAC2 decomposition methods on irregular dense tensors. Extensive experiments show that DPAR2 outperforms the existing methods in terms of speed, space, and scalability, while achieving a comparable fitness, where the fitness indicates how a method approximates a given data well (see Section IV-A).\nThe contributions of this paper are as follows. \u2022 Algorithm. We propose DPAR2, a fast and scalable\nPARAFAC2 decomposition method for decomposing irregular dense tensors. \u2022 Analysis. We provide analysis for the time and the space complexities of our proposed method DPAR2. \u2022 Experiment. DPAR2 achieves up to 6.0\u00d7 faster running time than previous PARAFAC2 decomposition methods\nar X\niv :2\n20 3.\n12 79\n8v 2\n[ cs\n.L G\n] 2\nJ un\n2 02\n2\n840 860 880 900 920\n0.70\n0.72\n0.74\n0.76\nFMA Urban US Stock KR Stock Activity Action Traic PEMS-SF0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 0.0\n0.5\n1.0\nDPar2 RD-ALS PARAFAC2-ALS SPARTAN\n100 1000 Total Running Time (sec)\n0.7\n0.75\n0.8\nFi tn\nes s\n\ud835\udfd4. \ud835\udfce\u00d7\n\ud835\udfd1. \ud835\udfcf\u00d7 Best\n\ud835\udc79 \u2191\n(a) Trade-off\n20 100 Total Running Time (sec)\n0.75\n0.8\n0.85\nFi tn\nes s\n\ud835\udfd1. \ud835\udfd3\u00d7\n\ud835\udfd1. \ud835\udfce\u00d7\nBest\n(b) Trade-off\n4 8 Total Running Time (sec)\n0.93\n0.95\n0.97\nFi tn\nes s\nBest\n\ud835\udfcf. \ud835\udfd5\u00d7\n\ud835\udfcf. \ud835\udfd3\u00d7\n(c) Trade-off\n4 8 16 32 Total Running Time (sec)\n0.9\n0.92\n0.94\nFi tn\nes s\n\ud835\udfd0. \ud835\udfce\u00d7\n\ud835\udfd0. \ud835\udfd2\u00d7Best\n(d) Trade-off\nFig. 1. [Best viewed in color] Measurement of the running time and fitness on real-world datasets for three target ranks R: 10, 15, and 20. DPAR2 provides the best trade-off between speed and fitness. DPAR2 is up to 6.0\u00d7 faster than the competitors while having a comparable fitness.\nIn the rest of this paper, we describe the preliminaries in Section II, propose our method DPAR2 in Section III, present experimental results in Section IV, discuss related works in Section V, and conclude in Section VI. The code and datasets are available at https://datalab.snu.ac.kr/dpar2."
        },
        {
            "heading": "II. PRELIMINARIES",
            "text": "In this section, we describe tensor notations, tensor operations, Singular Value Decomposition (SVD), and PARAFAC2 decomposition. We use the symbols listed in Table I."
        },
        {
            "heading": "A. Tensor Notation and Operation",
            "text": "We use boldface lowercases (e.g. x) and boldface capitals (e.g. X) for vectors and matrices, respectively. In this paper, indices start at 1. An irregular tensor is a 3-order tensor X whose k-frontal slice X(:, :, k) is Xk \u2208 RIk\u00d7J . We denote irregular tensors by {Xk}Kk=1 instead of X where K is the number of k-frontal slices of the tensor. An example\nAlgorithm 1: Randomized SVD [20] Input: A \u2208 RI\u00d7J Output: U \u2208 RI\u00d7R, S \u2208 RR\u00d7R, and V \u2208 RJ\u00d7R. Parameters: target rank R, and an exponent q\n1: generate a Gaussian test matrix \u2126 \u2208 RJ\u00d7(R+s) 2: construct Y \u2190 (AAT )qA\u2126 3: QR\u2190 Y using QR factorization 4: construct B\u2190 QTA 5: U\u0303\u03a3VT \u2190 B using truncated SVD at rank R 6: return U = QU\u0303, \u03a3, and V\nis described in Fig. 2. We refer the reader to [19] for the definitions of tensor operations including Frobenius norm, matricization, Kronecker product, and Khatri-Rao product."
        },
        {
            "heading": "B. Singular Value Decomposition (SVD)",
            "text": "Singular Value Decomposition (SVD) decomposes A \u2208 RI\u00d7J to X = U\u03a3VT. U \u2208 RI\u00d7R is the left singular vector matrix of A; U = [ u1 \u00b7 \u00b7 \u00b7ur ] is a column orthogonal matrix where R is the rank of A and u1, \u00b7 \u00b7 \u00b7 , uR are the eigenvectors of AAT. \u03a3 is an R\u00d7R diagonal matrix whose diagonal entries are singular values. The i-th singular value \u03c3i is in \u03a3i,i where \u03c31 \u2265 \u03c32 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3R \u2265 0. V \u2208 RJ\u00d7R is the right singular vector matrix of A; V = [ v1 \u00b7 \u00b7 \u00b7vR ] is a column orthogonal matrix where v1, \u00b7 \u00b7 \u00b7 , vR are the eigenvectors of ATA. Randomized SVD. Many works [20]\u2013[22] have introduced efficient SVD methods to decompose a matrix A \u2208 RI\u00d7J by applying randomized algorithms. We introduce a popular randomized SVD in Algorithm 1. Randomized SVD finds a column orthogonal matrix Q \u2208 RI\u00d7(R+s) of (AAT )qA\u2126 using random matrix \u2126, constructs a smaller matrix B = QTA (\u2208 R(R+s)\u00d7J), and finally obtains the SVD result U (= QU\u0303), \u03a3, V of A by computing SVD for B, i.e., B \u2248 U\u0303\u03a3VT . Given a matrix A, the time complexity of randomized SVD is O(IJR) where R is the target rank."
        },
        {
            "heading": "C. PARAFAC2 decomposition",
            "text": "PARAFAC2 decomposition proposed by Harshman [23] successfully deals with irregular tensors. The definition of PARAFAC2 decomposition is as follows:\nDefinition 1 (PARAFAC2 Decomposition). Given a target rank R and a 3-order tensor {Xk}Kk=1 whose k-frontal slice is Xk \u2208 RIk\u00d7J for k = 1, ...,K, PARAFAC2 decomposition approximates each k-th frontal slice Xk by UkSkVT . Uk is a matrix of the size Ik \u00d7 R, Sk is a diagonal matrix of the\nAlgorithm 2: PARAFAC2-ALS [24] Input: Xk \u2208 RIk\u00d7J for k = 1, ...,K Output: Uk \u2208 RIk\u00d7R, Sk \u2208 RR\u00d7R for k = 1, ...,K, and\nV \u2208 RJ\u00d7R. Parameters: target rank R\n1: initialize matrices H \u2208 RR\u00d7R, V, and Sk for k = 1, ...,K 2: repeat 3: for k = 1, ...,K do 4: compute Z\u2032k\u03a3 \u2032 kP \u2032T k \u2190 XkVSkHT by performing truncated SVD at rank R 5: Qk \u2190 Z\u2032kP \u2032T k 6: end for 7: for k = 1, ...,K do 8: Yk \u2190 QTk Xk 9: end for\n10: construct a tensor Y \u2208 RR\u00d7J\u00d7K from slices Yk \u2208 RR\u00d7J for k = 1, ...,K /* running a single iteration of CP-ALS on Y */ 11: H\u2190 Y(1)(W V)(WTW \u2217VTV)\u2020 12: V\u2190 Y(2)(W H)(WTW \u2217HTH)\u2020 13: W\u2190 Y(3)(V H)(VTV \u2217HTH)\u2020 14: for k = 1, ...,K do 15: Sk \u2190 diag(W(k, :)) 16: end for 17: until the maximum iteration is reached, or the error ceases to\ndecrease; 18: for k = 1, ...,K do 19: Uk \u2190 QkH 20: end for\nsize R \u00d7 R, and V is a matrix of the size J \u00d7 R which are common for all the slices.\nThe objective function of PARAFAC2 decomposition [23] is given as follows.\nmin {Uk},{Sk},V K\u2211 k=1 ||Xk \u2212UkSkVT ||2F (1)\nFor uniqueness, Harshman [23] imposed the constraint (i.e., UTk U = \u03a6 for all k), and replace U T k with QkH where Qk is a column orthogonal matrix and H is a common matrix for all the slices. Then, Equation (1) is reformulated with QkH:\nmin {Qk},{Sk},H,V K\u2211 k=1 ||Xk \u2212QkHSkVT ||2F (2)\nFig. 2 shows an example of PARAFAC2 decomposition for a given irregular tensor. A common approach to solve the above problem is ALS (Alternating Least Square) which iteratively updates a target factor matrix while fixing all factor matrices except for the target. Algorithm 2 describes PARAFAC2-ALS. First, we update each Qk while fixing H, V, Sk for k = 1, ...,K (lines 4 and 5). By computing SVD of XkVSkHT as Z\u2032k\u03a3 \u2032 kP \u2032T k , we update Qk as Z \u2032 kP \u2032T k , which minimizes Equation (3) over Qk [11], [24], [25]. After updating Qk, the remaining factor matrices H, V, Sk is updated by minimizing the following objective function:\nmin {Sk},H,V\nK\u2211\nk=1 ||QTk Xk \u2212HSkVT ||2F (3)\nMinimizing this function is to update H, V, Sk using CP decomposition of a tensor Y \u2208 RR\u00d7J\u00d7K whose k-th frontal\nslice is QTk Xk (lines 8 and 10). We run a single iteration of CP decomposition for updating them [24] (lines 11 to 16). Qk, H, Sk, and V are alternatively updated until convergence.\nIterative computations with an irregular dense tensor require high computational costs and large intermediate data. RDALS [18] reduces the costs by preprocessing a given tensor and performing PARAFAC2 decomposition using the preprocessed result, but the improvement of RD-ALS is limited. Also, recent works successfully have dealt with sparse irregular tensors by exploiting sparsity. However, the efficiency of their models depends on the sparsity patterns of a given irregular tensor, and thus there is little improvement on irregular dense tensors. Specifically, computations with large dense slices Xk for each iteration are burdensome as the number of iterations increases. We focus on improving the efficiency and scalability in irregular dense tensors."
        },
        {
            "heading": "III. PROPOSED METHOD",
            "text": "In this section, we propose DPAR2, a fast and scalable PARAFAC2 decomposition method for irregular dense tensors."
        },
        {
            "heading": "A. Overview",
            "text": "Before describing main ideas of our method, we present main challenges that need to be tackled. C1. Dealing with large irregular tensors. PARAFAC2 de-\ncomposition (Algorithm 2) iteratively updates factor matrices (i.e., Uk, Sk, and V) using an input tensor. Dealing with a large input tensor is burdensome to update the factor matrices as the number of iterations increases. C2. Minimizing numerical computations and intermediate data. How can we minimize the intermediate data and overall computations? C3. Maximizing multi-core parallelism. How can we parallelize the computations for PARAFAC2 decomposition?\nThe main ideas that address the challenges mentioned above are as follows: I1. Compressing an input tensor using randomized SVD\nconsiderably reduces the computational costs to update factor matrices (Section III-B). I2. Careful reordering of computations with the compression results minimizes the intermediate data and the number of operations (Sections III-C to III-E). I3. Careful distribution of work between threads enables DPAR2 to achieve high efficiency by considering various lengths Ik for k = 1, ...,K (Section III-F).\nAs shown in Fig. 3, DPAR2 first compresses each slice of an irregular tensor using randomized SVD (Section III-B). The compression is performed once before iterations, and only the compression results are used at iterations. It significantly reduces the time and space costs in updating factor matrices. After compression, DPAR2 updates factor matrices at each iteration, by exploiting the compression results (Sections III-C to III-E). Careful reordering of computations is required to achieve high efficiency. Also, by carefully allocating input slices to threads, DPAR2 accelerates the overall process (Section III-F)."
        },
        {
            "heading": "B. Compressing an irregular input tensor",
            "text": "DPAR2 (see Algorithm 3) is a fast and scalable PARAFAC2 decomposition method based on ALS described in Algorithm 2. The main challenge that needs to be tackled is to minimize the number of heavy computations involved with a given irregular tensor {Xk}Kk=1 consisting of slices Xk for k = 1, ...,K (in lines 4 and 8 of Algorithm 2). As the number of iterations increases (lines 2 to 17 in Algorithm 2), the heavy computations make PARAFAC2-ALS slow. For efficiency, we preprocess a given irregular tensor into small matrices, and then update factor matrices by carefully using the small ones.\nOur approach to address the above challenges is to compress a given irregular tensor {Xk}Kk=1 before starting iterations. As shown in Fig. 4, our main idea is two-stage lossy compression with randomized SVD for the given tensor: 1) DPAR2 performs randomized SVD for each slice Xk for k = 1, ...,K at target rank R, and 2) DPAR2 performs randomized SVD for a matrix, the horizontal concatenation of singular value matrices and right singular vector matrices of slices Xk. Randomized SVD allows us to compress slice matrices with low computational costs and low errors.\nFirst Stage. In the first stage, DPAR2 compresses a given irregular tensor by performing randomized SVD for each slice Xk at target rank R (line 3 in Algorithm 3). Xk \u2248 AkBkCTk (4) where Ak \u2208 RIk\u00d7R is a matrix consisting of left singular vectors, Bk \u2208 RR\u00d7R is a diagonal matrix whose elements are singular values, and Ck \u2208 RJ\u00d7R is a matrix consisting of right singular vectors.\nSecond Stage. Although small compressed data are generated in the first step, there is a room to further compress the intermediate data from the first stage. In the second stage, we\nM = [C1B1; \u00b7 \u00b7 \u00b7 ; CKBK ] = \u2016Kk=1(CkBk) \u2248 DEFT (5) where D \u2208 RJ\u00d7R is a matrix consisting of left singular vectors, E \u2208 RR\u00d7R is a diagonal matrix whose elements are singular values, and F \u2208 RKR\u00d7R is a matrix consisting of right singular vectors.\nWith the two stages, we obtain the compressed results D, E, F, and Ak for k = 1, ...,K. Before describing how to update factor matrices, we re-express the k-th slice Xk by using the compressed results: Xk \u2248 AkF(k)EDT (6) where F(k) \u2208 RR\u00d7R is the kth vertical block matrix of F:\nF = F (1)\n... F(K)  (7) Since CkBk is the kth horizontal block of M and DEF(k)T is the kth horizontal block of DEFT , BkCTk corresponds to F(k)EDT . Therefore, we obtain Equation (6) by replacing BkC T k with F (k)EDT from Equation (4).\nIn updating factor matrices, we use AkF(k)EDT instead of Xk. The two-stage compression lays the groundwork for efficient updates."
        },
        {
            "heading": "C. Overview of update rule",
            "text": "Our goal is to efficiently update factor matrices, H, V, and Sk and Qk for k = 1, ...,K, using the compressed results AkF\n(k)EDT . The main challenge of updating factor matrices is to minimize numerical computations and intermediate data by exploiting the compressed results obtained in Section III-B. A naive approach would reconstruct X\u0303k = AkF(k)EDT from the compressed results, and then update the factor matrices. However, this approach fails to improve the efficiency of updating factor matrices. We propose an efficient update rule using the compressed results to 1) find Qk and Yk (lines 5 and 8 in Algorithm 2), and 2) compute a single iteration of CP-ALS (lines 11 to 13 in Algorithm 2).\nThere are two differences between our update rule and PARAFAC2-ALS (Algorithm 2). First, we avoid explicit computations of Qk and Yk. Instead, we find small factorized matrices of Qk and Yk, respectively, and then exploit the small ones to update H, V, and W. The small matrices are computed efficiently by exploiting the compressed results AkF\n(k)EDT instead of Xk. The second difference is that DPAR2 obtains H, V, and W using the small factorized matrices of Yk. Careful ordering of computations with them considerably reduces time and space costs at each iteration. We describe how to find the factorized matrices of Qk and Yk in Section III-D, and how to update factor matrices in Section III-E."
        },
        {
            "heading": "D. Finding the factorized matrices of Qk and Yk",
            "text": "The first goal of updating factor matrices is to find the factorized matrices of Qk and Yk for k = 1, ...,K, respectively. In Algorithm 2, finding Qk and Yk is expensive due to the computations involved with Xk (lines 4 and 8 in Algorithm 2). To reduce the costs for Qk and Yk, our main idea is to exploit the compressed results Ak, D, E, and F(k), instead of Xk. Additionally, we exploit the column orthogonal property of Ak, i.e., ATk Ak = I, where I is the identity matrix.\nWe first re-express Qk using the compressed results obtained in Section III-B. DPAR2 reduces the time and space costs for Qk by exploiting the column orthogonal property of Ak. First, we express XkVSkHT as AkF(k)EDTVSkHT by replacing Xk with AkF(k)EDT . Next, we need to obtain left and right singular vectors of AkF(k)EDT VSkHT . A naive approach is to compute SVD of AkF(k)EDTVSkHT , but there is a more efficient way than this approach. Thanks to the column orthogonal property of Ak, DPAR2 performs SVD of F(k)EDTVSk HT \u2208 RR\u00d7R, not AkF(k)EDTVSkHT \u2208 RIk\u00d7R, at target rank R (line 9 in Algorithm 3):\nF(k)EDTVSkH T SVD= Zk\u03a3kP T k (8)\nwhere \u03a3k is a diagonal matrix whose entries are the singular values of F(k)EDTVSkHT , the column vectors of Zk and Pk are the left singular vectors and the right singular vectors of F(k)EDTVSkHT , respectively. Then, we obtain the factorized matrices of Qk as follows:\nQk = AkZkP T k (9)\nAlgorithm 3: DPAR2 Input: Xk \u2208 RIk\u00d7J for k = 1, ...,K Output: Uk \u2208 RIk\u00d7R, Sk \u2208 RR\u00d7R for k = 1, ...,K, and\nV \u2208 RJ\u00d7R. Parameters: target rank R\n1: initialize matrices H \u2208 RR\u00d7R, V, and Sk for k = 1, ...,K /* Compressing slices in parallel */ 2: for k = 1, ...,K do 3: compute AkBkCTk \u2190 SVD(Xk) by performing randomized SVD at rank R 4: end for 5: M\u2190 \u2016Kk=1(CkBk) 6: compute DEFT \u2190 SVD(M) by performing randomized SVD\nat rank R /* Iteratively updating factor matrices */\n7: repeat 8: for k = 1, ...,K do 9: compute Zk\u03a3kPTk \u2190 SVD(F(k)EDTVSkHT ) by\nperforming SVD at rank R 10: end for /* no explicit computation of Yk */ 11: for k = 1, ...,K do 12: Yk \u2190 PkZTk F(k)EDT 13: end for\n/* running a single iteration of CP-ALS on Y */\n14: compute G(1) \u2190 Y(1)(W V) based on Lemma 1 15: H\u2190 G(1)(WTW \u2217VTV)\u2020 . Normalize H 16: compute G(2) \u2190 Y(2)(W H) based on Lemma 2 17: V\u2190 G(2)(WTW \u2217HTH)\u2020 . Normalize V 18: compute G(3) \u2190 Y(3)(V H) based on Lemma 3 19: W\u2190 G(3)(VTV \u2217HTH)\u2020 20: for k = 1, ...,K do 21: Sk \u2190 diag(W(k, :)) 22: end for 23: until the maximum iteration is reached, or the error ceases to\ndecrease; 24: for k = 1, ...,K do 25: Uk \u2190 AkZkPTk H 26: end for\nwhere AkZk and Pk are the left and the right singular vectors of AkF(k)EDTVSkHT , respectively. We avoid the explicit construction of Qk, and use AkZkPTk instead of Qk. Since Ak is already column-orthogonal, we avoid performing SVD of AkF(k)EDTVSkHT , which are much larger than F(k)EDTVSkH T .\nNext, we find the factorized matrices of Yk. DPAR2 reexpresses QTk Xk (line 8 in Algorithm 2) as Q T k AkF (k)EDT using Equation (6). Instead of directly computing QTk AkF (k)EDT , we replace QTk with PkZ T k A T k . Then, we represent Yk as the following expression (line 12 in Algorithm 3):\nYk \u2190QTk AkF(k)EDT = PkZTk ATk AkF(k)EDT\n= PkZ T k F (k)EDT\nNote that we use the property ATk Ak = IR\u00d7R, where IR\u00d7R is the identity matrix of size R \u00d7 R, for the last equality. By exploiting the factorized matrices of Qk, we compute Yk without involving Ak in the process."
        },
        {
            "heading": "E. Updating H, V, and W",
            "text": "The next goal is to efficiently update the matrices H, V, and W using the small factorized matrices of Yk. Naively, we would compute Y and run a single iteration of CPALS with Y to update H, V, and W (lines 11 to 13 in Algorithm 2). However, multiplying a matricized tensor and a Khatri-Rao product (e.g., Y(1)(W V)) is burdensome, and thus we exploit the structure of the decomposed results PkZ T k F\n(k)EDT of Yk to reduce memory requirements and computational costs. In other word, we do not compute Yk, and use only PkZTk F\n(k)EDT in updating H, V, and W. Note that the k-th frontal slice of Y, Y(:, :, k), is PkZTk F\n(k)EDT . Updating H. In Y(1)(W V)(WTW\u2217VTV)\u2020, we focus on efficiently computing Y(1)(W V) based on Lemma 1. A naive computation for Y(1)(W V) requires a high computational cost O(JKR2) due to the explicit reconstruction of Y(1). Therefore, we compute that term without the reconstruction by carefully determining the order of computations and exploiting the factorized matrices of Y(1), D, E, Pk, Zk, and F(k) for k = 1, ...,K. With Lemma 1, we reduce the computational cost of Y(1)(W V) to O(JR2 +KR3). Lemma 1. Let us denote Y(1)(W V) with G(1) \u2208 RR\u00d7R. G(1)(:, r) is equal to((\u2211K\nk=1 W(k, r) ( PkZ T k F (k) )) EDTV(:, r) ) .\nProof. Y(1) is represented as follows: Y(1) = [ P1Z T 1 F (1)EDT ; \u00b7 \u00b7 \u00b7 ; PKZTKF(K)EDT ]\n= ( \u2016Kk=1 ( PkZ T k F (k) ))ED T \u00b7 \u00b7 \u00b7 O ... . . . ...\nO \u00b7 \u00b7 \u00b7 EDT  = ( \u2016Kk=1 ( PkZ T k F (k) )) ( IK\u00d7K \u2297EDT )\nwhere IK\u00d7K is the identity matrix of size K \u00d7 K. Then, G(1) = Y(1)(W V) is expressed as follows: G(1) = ( \u2016Kk=1 ( PkZ T k F (k) ))\n\u00d7 ( IK\u00d7K \u2297EDT ) ( \u2016Rr=1(W(:, r)\u2297V(:, r)) ) = ( \u2016Kk=1 ( PkZ T k F (k) )) ( \u2016Rr=1 ( W(:, r)\u2297EDTV(:, r)\n)) The mixed-product property (i.e., (A \u2297 B)(C \u2297 D) = AC\u2297BD)) is used in the above equation. Therefore, G(1)(: , r) is equal to ( \u2016Kk=1 ( PkZ T k F (k) )) ( W(:, r)\u2297EDTV(:, r) ) .\nWe represent it as \u2211K k=1 W(k, r) ( PkZ T k F (k) )\nEDTV(: , r) using block matrix multiplication since the k-th ver-\ntical block vector of\n(\nW(:, r)\u2297EDTV(:, r)\n)\n\u2208 RKR is W(k, r)EDTV(:, r) \u2208 RR. As shown in Fig. 5, we compute Y(1)(W V) column by column. In computing G(1)(:, r), we compute EDTV(:, r), sum up W(k, r) ( PkZ T k F (k) )\nfor all k, and then perform matrix multiplication between the two preceding results (line 14 in Algorithm 3). After computing G(1) \u2190 Y(1)(W V), we update H by computing G(1)(WTW \u2217 VTV)\u2020 where \u2020 denotes the Moore-Penrose pseudoinverse (line 15 in Algorithm 3). Note that the pseudoinverse operation requires a lower computational cost compared to computing G(1) since the size of (WTW \u2217VTV) \u2208 RR\u00d7R is small.\nUpdating V. In computing Y(2)(W U)(WTW\u2217UTU)\u2020, we need to efficiently compute Y(2)(W U) based on Lemma 2. As in updating H, a naive computation for Y(2)(W U) requires a high computational cost O(JKR2). We efficiently compute Y(2)(W U) with the cost O(JR2 + KR3), by carefully determining the order of computations and exploiting the factorized matrices of Y(2).\nLemma 2. Let us denote Y(2)(W H) with G(2) \u2208 RJ\u00d7R. G(2)(:, r) is equal to DE (\u2211K k=1 ( W(k, r)F(k)TZkP T k H(:, r) )) .\nProof. Y(2) is represented as follows: Y(2) = [ DEF(1)TZ1P T 1 ; \u00b7 \u00b7 \u00b7 ; DEF(K)TZKPTK ] = DE ( \u2016Kk=1F(k)TZkPTk\n) Then, G(2) = Y(2)(W H) is expressed as follows:\nG(2) = DE ( \u2016Kk=1F(k)TZkPTk ) \u00d7\nW(1, 1)H(:, 1); \u00b7 \u00b7 \u00b7 ; W(1, R)H(:, R)... ... ... W(K, 1)H(:, 1); \u00b7 \u00b7 \u00b7 ; W(K,R)H(:, R)  G(2)(:, r) is equal to DE \u2211K k=1 ( W(k, r)F(k)TZkP T k H(:, r)\n) according to the above equation.\nAs shown in Fig. 6, we compute G(2) \u2190 Y(2)(W H) column by column. After computing G(2), we update V by computing G(2)(WTW \u2217 HTH)\u2020 (lines 16 and 17 in Algorithm 3).\nUpdating W. In computing Y(3)(V H)(VTV\u2217HTH)\u2020, we efficiently compute Y(3)(V H) based on Lemma 3. As in updating H and V, a naive computation for Y(3)(V H) requires a high computational cost O(JKR2). We compute Y(3)(V H) with the cost O(JR2+KR3) based on Lemma 3.\nExploiting the factorized matrices of Y(3) and carefully determining the order of computations improves the efficiency.\nLemma 3. Let us denote Y(3)(V H) with G(3) \u2208 RK\u00d7R. G(3)(k, r) is equal to( vec ( PkZ T k F (k) ))T ( EDTV(:, r)\u2297H(:, r) )\nwhere vec(\u00b7) denotes the vectorization of a matrix.\nProof. Y(3) is represented as follows:\nY(3) =  ( vec ( P1Z T 1 F (1)EDT ))T\n...( vec ( PKZ T KF (K)EDT ))T \n= ( \u2016Kk=1 ( vec ( PkZ T k F (k)EDT )))T\n= ( \u2016Kk=1(DE\u2297 I) vec ( PkZ T k F (k) ))T\n= ( \u2016Kk=1 ( vec ( PkZ T k F (k) )))T ( EDT \u2297 IR\u00d7R )\nwhere IR\u00d7R is the identity matrix of size R\u00d7R. The property of the vectorization (i.e., vec(AB) = (BT \u2297 I)vec(A)) is used. Then, G(3) = Y(3)(V H) is expressed as follows:\nG(3) = ( \u2016Kk=1 ( vec ( PkZ T k F (k) )))T\n\u00d7 ( \u2016Rr=1 ( EDTV(:, r)\u2297H(:, r) )) G(3)(k, r) is ( vec ( PkZ T k F (k) ))T ( EDTV(:, r)\u2297H(:, r) ) according to the above equation. We compute G(3) = Y(3)(V H) row by row. Fig. 7 shows how we compute G(3)(k, r). In computing G(3), we first compute EDTV, and then obtain G(3)(k, :) for all k (line 18 in Algorithm 3). After computing G(3), we update W by computing G(3)(VTV \u2217HTH)\u2020 where \u2020 denotes the MoorePenrose pseudoinverse (line 19 in Algorithm 3). We obtain Sk whose diagonal elements correspond to the kth row vector of W (line 21 in Algorithm 3).\nAfter convergence, we obtain the factor matrices, (Uk \u2190 AkZkP T k H = QkH), Sk, and V (line 25 in Algorithm 3).\nConvergence Criterion. At the end of each iteration, we determine whether to stop or not (line 23 in Algorithm 3) based on the variation of e = (\u2211K k=1 \u2016Xk \u2212 X\u0302k\u20162F ) where X\u0302k = QkHSkV T is the kth reconstructed slice. However, measuring\nreconstruction errors \u2211K\nk=1 \u2016Xk\u2212X\u0302k\u20162F is inefficient since it requires high time and space costs proportional to input slices Xk. To efficiently verify the convergence, our idea is to exploit AkF\n(k)EDT instead of Xk, since the objective of our update process is to minimize the difference between AkF(k)EDT\nAlgorithm 4: Careful distribution of work in DPAR2 Input: the number T of threads, Xk \u2208 RIk\u00d7J for k = 1, ...,K Output: sets Ti for i = 1, ..., T .\n1: initialize Ti \u2190 \u2205 for i = 1, ..., T . 2: construct a list S of size T whose elements are zero 3: construct a list Linit containing the number of rows of Xk for\nk = 1, ...,K 4: sort Linit in descending order, and obtain lists Lval and Lind\nthat contain sorted values and those corresponding indices 5: for k = 1, ...,K do 6: tmin \u2190 argminS 7: l\u2190 Lind[k] 8: Ttmin \u2190 Ttmin \u222a {Xl} 9: S[tmin]\u2190 S[tmin] + Lval[k]\n10: end for\nand X\u0302k = QkHSkVT . With this idea, we improve the efficiency by computing \u2211K k=1 \u2016PkZTk F(k)EDT \u2212HSkVT \u20162F, not the reconstruction errors. Our computation requires the time O(JKR2) and space costs O(JKR) which are much lower than the costs O( \u2211K k=1 IkJR) and O( \u2211K k=1 IkJ) of\nnaively computing \u2211K\nk=1 \u2016Xk \u2212 X\u0302k\u20162F, respectively. From \u2016PkZTk F(k)EDT \u2212HSkVT \u20162F, we derive \u2016AkF(k)EDT \u2212 X\u0302k\u20162F. Since the Frobenius norm is unitarily invariant, we modify the computation as follows:\n\u2016PkZTk F(k)EDT \u2212HSkVT \u20162F = \u2016QkPkZTk F(k)EDT \u2212QkHSkVT \u20162F = \u2016AkZkPTk PkZTk F(k)EDT \u2212QkHSkVT \u20162F = \u2016AkF(k)EDT \u2212 X\u0302k\u20162F\nwhere PTk Pk and ZkZ T k are equal to I \u2208 RR\u00d7R since Pk and Zk are orthonormal matrices. Note that the size of PkZ T k F\n(k)EDT and HSkVT is R\u00d7J which is much smaller than the size Ik \u00d7 J of input slices Xk. This modification completes the efficiency of our update rule."
        },
        {
            "heading": "F. Careful distribution of work",
            "text": "The last challenge for an efficient and scalable PARAFAC2 decomposition method is how to parallelize the computations described in Sections III-B to III-E. Although a previous work [11] introduces the parallelization with respect to the K slices, there is still a room for maximizing parallelism. Our main idea is to carefully allocate input slices Xk to threads by considering the irregularity of a given tensor.\nThe most expensive operation is to compute randomized SVD of input slices Xk for all k; thus we first focus on how well we parallelize this computation (i.e., lines 2 to 4 in Algorithm 3). A naive approach is to randomly allocate input slices to threads, and let each thread compute randomized SVD of the allocated slices. However, the completion time of each thread can vary since the computational cost of computing randomized SVD is proportional to the number of rows of slices; the number of rows of input slices is different from each other as shown in Fig. 8. Therefore, we need to distribute Xk fairly across each thread considering their numbers of rows.\nFor i = 1, .., T , consider that an ith thread performs randomized SVD for slices in a set Ti where T is the number of threads. To reduce the completion time, the sums of rows\nof slices in the sets should be nearly equal to each other. To achieve it, we exploit a greedy number partitioning technique that repeatedly adds a slice into a set with the smallest sum of rows. Algorithm 4 describes how to construct the sets Ti for compressing input slices in parallel. Let Linit be a list containing the number of rows of Xk for k = 1, ...,K (line 3 in Algorithm 4). We first obtain lists Lval and Lind, sorted values and those corresponding indices, by sorting Linit in descending order (line 4 in Algorithm 4). We repeatedly add a slice Xk to a set Ti that has the smallest sum. For each k, we find the index tmin of the minimum in S whose ith element corresponds to the sum of row sizes of slices in the ith set Ti (line 6 in Algorithm 4). Then, we add a slice Xl to the set Ttmin where l is equal to Lind[k], and update the list S by S[tmin]\u2190 S[tmin]+Lval[k] (lines 7 to 9 in Algorithm 4). Note that S[k], Lind[k], and Lval[k] denote the kth element of S, Lind, and Lval, respectively. After obtaining the sets Ti for i = 1, .., T , ith thread performs randomized SVD for slices in the set Ti.\nAfter decomposing Xk for all k, we do not need to consider the irregularity for parallelism since there is no computation with Ak which involves the irregularity. Therefore, we uniformly allocate computations across threads for all k slices. In each iteration (lines 8 to 22 in Algorithm 3), we easily parallelize computations. First, we parallelize the iteration (lines 8 to 10) for all k slices. To update H, V, and W, we need to compute G(1), G(2), and G(3) in parallel. In Lemmas 1 and 2, DPAR2 parallelly computes W(k, r) ( PkZ T k F (k) )\nand W(k, r)F(k)ZkPTk H(:, r) for k, respectively. In Lemma 3, DPAR2 parallelly computes( vec ( PkZ T k F (k) ))T ( EDTV(:, r)\u2297H(:, r) ) for k."
        },
        {
            "heading": "G. Complexities",
            "text": "We analyze the time complexity of DPAR2.\nLemma 4. Compressing input slices takes O ((\u2211K k=1 IkJR ) + JKR2 ) time.\nProof. The SVD in the first stage takes O (\u2211K k=1 IkJR ) times since computing randomized SVD of Xk takes O(IkJR) time. Then, the SVD in the second stage takes O ( JKR2 ) due to randomized SVD of M(2) \u2208 RJ\u00d7KR. Therefore, the time complexity of the SVD in the two stages is O ((\u2211K k=1 IkJR ) + JKR2 ) .\nLemma 5. At each iteration, computing Yk and updating H, V, and W takes O(JR2 +KR3) time.\nProof. For Yk, computing F(k)EDTVSkHT and performing SVD of it for all k take O(JR2+KR3). Updating each of H, V, and W takes O(JR2 + KR3 + R3) time. Therefore, the complexity for Yk, H, V, and W is O ( JR2 +KR3 ) . Theorem 1. The time complexity of DPAR2 is O ((\u2211K k=1 IkJR ) + JKR2 +MKR3 )\nwhere M is the number of iterations.\nProof. The overall time complexity of DPAR2 is the summation of the compression cost (see Lemma 4) and the iteration cost (see Lemma 5): O ((\u2211K k=1 IkJR ) + JKR2 +M(JR2 +KR3) )\n. Note that MJR2 term is omitted since it is much smaller than(\u2211K k=1 IkJR ) and JKR2. Theorem 2. The size of preprocessed data of DPAR2 is O ((\u2211K k=1 IkR ) +KR2 + JR ) .\nProof. The size of preprocessed data of DPAR2 is proportional to the size of E, D, Ak, and F(k) for k = 1, ...,K. The size of E and D is R and J \u00d7 R, respectively. For each k, the size of A and F is Ik \u00d7 R and R \u00d7 R, respectively. Therefore, the size of preprocessed data of DPAR2 is O ((\u2211K k=1 IkR ) +KR2 + JR ) ."
        },
        {
            "heading": "IV. EXPERIMENTS",
            "text": "In this section, we experimentally evaluate the performance of DPAR2. We answer the following questions: Q1 Performance (Section IV-B). How quickly and accu-\nrately does DPAR2 perform PARAFAC2 decomposition compared to other methods? Q2 Data Scalability (Section IV-C). How well does DPAR2 scale up with respect to tensor size and target rank? Q3 Multi-core Scalability (Section IV-D). How much does the number of threads affect the running time of DPAR2? Q4 Discovery (Section IV-E). What can we discover from real-world tensors using DPAR2?"
        },
        {
            "heading": "A. Experimental Settings",
            "text": "We describe experimental settings for the datasets, competitors, parameters, and environments.\nMachine. We use a workstation with 2 CPUs (Intel Xeon E5-2630 v4 @ 2.2GHz), each of which has 10 cores, and 512GB memory for the experiments.\nReal-world Data. We evaluate the performance of DPAR2 and competitors on real-world datasets summarized in Table II.\n0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 0.0\n0.5\n1.0\nDPar2 RD-ALS PARAFAC2-ALS SPARTAN\nFMA Urban US Stock KR Stock Activity Action Traic PEMS-SF Data 0.1\n1\n10\n100\nPr ep\nro ce\nss in\ng Ti\nm e\n(s ec\n)\n\ud835\udfd4.\ud835\udfd6\u00d7 \ud835\udfcf\ud835\udfce. \ud835\udfce\u00d7\n(a) Preprocessing time\nFMA Urban US Stock KR Stock Activity Action Traic PEMS-SF\nData\n0.1\n1\n10\nTi m\ne pe\nr It\ner at\nio n\n(s ec\n)\n\ud835\udfcf\ud835\udfce. \ud835\udfd1\u00d7\n\ud835\udfd0.\ud835\udfd2\u00d7 \ud835\udfd3.\ud835\udfd0\u00d7 \ud835\udfd1.\ud835\udfd6\u00d7\n\ud835\udfd0.\ud835\udfd0\u00d7 \ud835\udfcf.\ud835\udfd7\u00d7 \ud835\udfd1.\ud835\udfd0\u00d7 \ud835\udfd0.\ud835\udfd1\u00d7\n(b) Iteration time\nFig. 9. [Best viewed in color] (a) DPAR2 efficiently preprocesses a given irregular dense tensor, which is up to 10\u00d7 faster compared to RD-ALS. (b) At each iteration, DPAR2 runs by up to 10.3\u00d7 faster than the second best method.\nFMA Urban US Stock KR Stock Activity Action Traic PEMS-SF Data\n0.01\n0.1\n1\n10\n100\nSi ze\nof Pr\nep ro\nce ss\ned D\nat a\n(G B\n)\n\ud835\udfd0\ud835\udfce\ud835\udfcf\u00d7 \ud835\udfcf\ud835\udfd6\ud835\udfd7\u00d7\n\ud835\udfd6.\ud835\udfd6\u00d7 \ud835\udfd6.\ud835\udfd6\u00d7\n\ud835\udfd3\ud835\udfd1. \ud835\udfcf\u00d7 \ud835\udfd2\ud835\udfd0. \ud835\udfd7\u00d7\n\ud835\udfd7.\ud835\udfd3\u00d7 \ud835\udfcf\ud835\udfd2. \ud835\udfd0\u00d7\n0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4\n0.0\n0.5\n1.0\nDPar2 RD-ALS Input Tensor\nFig. 10. The size of preprocessed data. DPAR2 generates up to 201\u00d7 smaller preprocessed data than input tensors used for SPARTan and PARAFAC2-ALS.\nFMA dataset1 [26] is the collection of songs. Urban Sound dataset2 [27] is the collection of urban sounds such as drilling, siren, and street music. For the two datasets, we convert each time series into an image of a log-power spectrogram so that their forms are (time, frequency, song; value) and (time, frequency, sound; value), respectively. US Stock dataset3 is the collection of stocks on the US stock market. Korea Stock dataset4 [3] is the collection of stocks on the South Korea stock market. Each stock is represented as a matrix of (date, feature) where the feature dimension includes 5 basic features and 83 technical indicators. The basic features collected daily are the opening, the closing, the highest, and the lowest prices and trading volume, and technical indicators are calculated based on the basic features. The two stock datasets have the form of (time, feature, stock; value). Activity data5 and Action data5 are the collection of features for motion videos. The two datasets have the form of (frame, feature, video; value). We refer the reader to [28] for their feature extraction. Traffic data6 is the collection of traffic volume around Melbourne, and its form is (sensor, frequency, time; measurement). PEMSSF data7 contain the occupancy rate of different car lanes of San Francisco bay area freeways: (station, timestamp, day; measurement). Traffic data and PEMS-SF data are 3-order regular tensors, but we can analyze them using PARAFAC2 decomposition approaches.\nSynthetic Data. We evaluate the scalability of DPAR2 and\n1https://github.com/mdeff/fma 2https://urbansounddataset.weebly.com/urbansound8k.html 3https://datalab.snu.ac.kr/dpar2 4https://github.com/jungijang/KoreaStockData 5https://github.com/titu1994/MLSTM-FCN 6https://github.com/florinsch/BigTrafficData 7http://www.timeseriesclassification.com/\ncompetitors on synthetic tensors. Given the number K of slices, and the slice sizes I and J , we generate a synthetic tensor using tenrand(I, J, K) function in Tensor Toolbox [31], which randomly generates a tensor X \u2208 RI\u00d7J\u00d7K . We construct a tensor {Xk}Kk=1 where Xk is equal to X(:, :, k) for k = 1, ...K.\nCompetitors. We compare DPAR2 with PARAFAC2 decomposition methods based on ALS. All the methods including DPAR2 are implemented in MATLAB (R2020b). \u2022 DPAR2: the proposed PARAFAC2 decomposition model\nwhich preprocesses a given irregular dense tensor and updates factor matrices using the preprocessing result. \u2022 RD-ALS [18]: PARAFAC2 decomposition which preprocesses a given irregular tensor. Since there is no public code, we implement it using Tensor Toolbox [31] based on its paper [18]. \u2022 PARAFAC2-ALS: PARAFAC2 decomposition based on ALS approach. It is implemented based on Algorithm 2 using Tensor Toolbox [31]. \u2022 SPARTan [11]: fast and scalable PARAFAC2 decomposition for irregular sparse tensors. Although it targets on sparse irregular tensors, it can be adapted to irregular dense tensors. We use the code implemented by authors8.\nParameters. We use the following parameters. \u2022 Number of threads: we use 6 threads except in Sec-\ntion IV-D. \u2022 Max number of iterations: the maximum number of\niterations is set to 32. \u2022 Rank: we set the target rank R to 10 except in the trade-\noff experiments of Section IV-B and Section IV-D. We also set the rank of randomized SVD to 10 which is the same as the target rank R of PARAFAC2 decomposition.\nTo compare running time, we run each method 5 times, and report the average.\nFitness. We evaluate the fitness defined as follows: 1\u2212 (\u2211K\nk=1 \u2016Xk \u2212 X\u0302k\u20162F\u2211K k=1 \u2016Xk\u20162F ) where Xk is the k-th input slice and X\u0302k is the k-th reconstructed slice of PARAFAC2 decomposition. Fitness close to 1 indicates that a model approximates a given input tensor well.\n8https://github.com/kperros/SPARTan\n1.0 1.5 2.0 2.5 3.0 3.5 4.0\n100\n2\u00a3 100 3\u00a3 100"
        },
        {
            "heading": "B. Performance (Q1)",
            "text": "We evaluate the fitness and the running time of DPAR2, RD-ALS, SPARTan, and PARAFAC2-ALS.\nTrade-off. Fig. 1 shows that DPAR2 provides the best trade-off of running time and fitness on real-world irregular tensors for the three target ranks: 10, 15, and 20. DPAR2 achieves 6.0\u00d7 faster running time than the competitors for FMA dataset while having a comparable fitness. In addition, DPAR2 provides at least 1.5\u00d7 faster running times than the competitors for the other datasets. The performance gap is large for FMA and Urban datasets whose sizes are larger than those of the other datasets. It implies that DPAR2 is more scalable than the competitors in terms of tensor sizes.\nPreprocessing time. We compare DPAR2 with RD-ALS and exclude SPARTan and PARAFAC2-ALS since only RDALS has a preprocessing step. As shown in Fig. 9(a), DPAR2 is up to 10\u00d7 faster than RD-ALS. There is a large performance gap on FMA and Urban datasets since RD-ALS cannot avoid the overheads for the large tensors. RD-ALS performs SVD of the concatenated slice matrices \u2016Kk=1XTk , which leads to its slow preprocessing time.\nIteration time. Fig. 9(b) shows that DPAR2 outperforms competitors for running time at each iteration. Compared to SPARTan and PARAFAC2-ALS, DPAR2 significantly reduces the running time per iteration due to the small size of the preprocessed results. Although RD-ALS reduces the computational cost at each iteration by preprocessing a given tensor, DPAR2 is up to 10.3\u00d7 faster than RDALS. Compared to RD-ALS that computes the variation of(\u2211K\nk=1 \u2016Xk \u2212QkHSkVT \u20162F )\nfor the convergence criterion, DPAR2 efficiently verifies the convergence by computing the variation of \u2211K k=1 \u2016PkZTk F(k)EDT \u2212HSkVT \u20162F, which affects the running time at each iteration. In summary, DPAR2 obtains Uk, Sk, and V in a reasonable running time even if the number of iterations increases.\nSize of preprocessed data. We measure the size of preprocessed data on real-world datasets. For PARAFAC2-ALS and SPARTan, we report the size of input irregular tensor since they have no preprocessing step. Compared to an input irregular tensor, DPAR2 generates much smaller preprocessed\ndata by up to 201 times as shown in Fig. 10. Given input slices Xk of size Ik\u00d7J , the compression ratio increases as the number J of columns increases; the compression ratio is larger on FMA, Urban, Activity, and Action datasets than on US Stock, KR Stock, Traffic, and PEMS-SF. This is because the compression ratio is proportional to Size of an irregular tensorSize of the preprocessed results \u2248 IJK IKR+KR2+JR = 1 R/J+R2/IJ+R/IK assuming I1 = ... = IK = I; R/J is the dominant term which is much larger than R2/IJ and R/IK."
        },
        {
            "heading": "C. Data Scalability (Q2)",
            "text": "We evaluate the data scalability of DPAR2 by measuring the running time on several synthetic datasets. We first compare the performance of DPAR2 and the competitors by increasing the size of an irregular tensor. Then, we measure the running time by changing a target rank.\nTensor Size. To evaluate the scalability with respect to the tensor size, we generate 5 synthetic tensors of the following sizes I \u00d7 J \u00d7 K: {1000 \u00d7 1000 \u00d7 1000, 1000 \u00d7 1000 \u00d7 2000, 2000\u00d71000\u00d72000, 2000\u00d72000\u00d72000, 2000\u00d72000\u00d7 4000}. For simplicity, we set I1 = \u00b7 \u00b7 \u00b7 = IK = I . Fig. 11(a) shows that DPAR2 is up to 15.3\u00d7 faster than competitors on all synthetic tensors; in addition, the slope of DPAR2 is lower than that of competitors. Also note that only DPAR2 obtains factor matrices of PARAFAC2 decomposition within a minute for all the datasets.\nRank. To evaluate the scalability with respect to rank, we generate the following synthetic data: I1 = \u00b7 \u00b7 \u00b7 = IK = 2, 000, J = 2, 000, and K = 4, 000. Given the synthetic tensors, we measure the running time for 5 target ranks: 10, 20, 30, 40, and 50. DPAR2 is up to 15.9\u00d7 faster than the secondfastest method with respect to rank in Fig. 11(b). For higher ranks, the performance gap slightly decreases since DPAR2 depends on the performance of randomized SVD which is designed for a low target rank. Still, DPAR2 is up to 7.0\u00d7 faster than competitors with respect to the highest rank used in our experiment."
        },
        {
            "heading": "D. Multi-core Scalability (Q3)",
            "text": "We generate the following synthetic data: I1 = \u00b7 \u00b7 \u00b7 = IK = 2, 000, J = 2, 000, and K = 4, 000, and evaluate the\nmulti-core scalability of DPAR2 with respect to the number of threads: 1, 2, 4, 6, 8, and 10. TM indicates the running time when using the number M of threads. As shown in Fig. 11(c), DPAR2 gives near-linear scalability, and accelerates 5.5\u00d7 when the number of threads increases from 1 to 10."
        },
        {
            "heading": "E. Discoveries (Q4)",
            "text": "We discover various patterns using DPAR2 on real-world datasets.\n1) Feature Similarity on Stock Dataset: We measure the similarities between features on US Stock and Korea Stock datasets, and compare the results. We compute Pearson Correlation Coefficient (PCC) between V(i, :), which represents a latent vector of the ith feature. For effective visualization, we select 4 price features (the opening, the closing, the highest, and the lowest prices), and 4 representative technical indicators described as follows: \u2022 OBV (On Balance Volume): a technical indicator for\ncumulative trading volume. If today\u2019s closing price is higher than yesterday\u2019s price, OBV increases by the amount of today\u2019s volume. If not, OBV decreases by the amount of today\u2019s volume. \u2022 ATR (Average True Range): a technical indicator for volatility developed by J. Welles Wilder, Jr. It increases in high volatility while decreasing in low volatility. \u2022 MACD (Moving Average Convergence and Divergence): a technical indicator for trend developed by Gerald Appel. It indicates the difference between longterm and short-term exponential moving averages (EMA). \u2022 STOCH (Stochastic Oscillator): a technical indicator for momentum developed by George Lane. It indicates the position of the current closing price compared to the highest and the lowest prices in a duration.\nFig. 12(a) and 12(b) show correlation heatmaps for US Stock data and Korea Stock data, respectively. We analyze correlation patterns between price features and technical indicators. On both datasets, STOCH has a negative correlation and MACD has a weak correlation with the price features. On the other hand, OBV and ATR indicators have different\npatterns on the two datasets. On the US stock dataset, ATR and OBV have a positive correlation with the price features. On the Korea stock dataset, OBV has little correlation with the price features. Also, ATR has little correlation with the price features except for the closing price. These different patterns are due to the difference of the two markets in terms of market size, market stability, tax, investment behavior, etc.\n2) Finding Similar Stocks: On US Stock dataset, which stock is similar to a target stock sT in a time range that a user is curious about? In this section, we provide analysis by setting the target stock sT to Microsoft (Ticker: MSFT), and the range a duration when the COVID-19 was very active (Jan. 2, 2020 - Apr. 15, 2021). We efficiently answer the question by 1) constructing the tensor included in the range, 2) obtaining factor matrices with DPAR2, and 3) post-processing the factor matrices of DPAR2. Since Uk represents temporal latent vectors of the kth stock, the similarity sim(si, sj) between stocks si and sj is computed as follows:\nsim(si, sj) = exp ( \u2212\u03b3\u2016Usi \u2212Usj\u20162F ) (10)\nwhere exp is the exponential function. We set \u03b3 to 0.01 in this section. Note that we use only the stocks that have the same target range since Usi \u2212Usj is defined only when the two matrices are of the same size.\nBased on sim(si, sj), we find similar stocks to sT using two different techniques: 1) k-nearest neighbors, and 2) Random Walks with Restarts (RWR). The first approach simply finds stocks similar to the target stock, while the second one finds similar stocks by considering the multi-faceted relationship between stocks. k-nearest neighbors. We compute sim(sT , sj) for j = 1, ...,K where K is the number of stocks to be compared, and find top-10 similar stocks to sT , Microsoft (Ticker: MSFT). In Table III(a), the Microsoft stock is similar to stocks of the Technology sector or with a large capitalization (e.g., Amazon.com, Apple, and Alphabet) during the COVID-19. Moody\u2019s is also similar to the target stock.\nRandom Walks with Restarts (RWR). We find similar stocks using another approach, Random Walks with Restarts (RWR) [32]\u2013[35]. To exploit RWR, we first a similarity graph\nbased on the similarities between stocks. The elements of the adjacency matrix A of the graph is defined as follows:\nA(i, j) = { sim(si, sj) if i 6= j 0 if i = j\n(11)\nWe ignore self-loops by setting A(i, i) to 0 for i = 1, ...,K. After constructing the graph, we find similar stocks using RWR. The scores r is computed by using the power iteration [36] as described in [37]: r(i) \u2190 (1\u2212 c)A\u0303T r(i\u22121) + cq (12) where A\u0303 is the row-normalized adjacency matrix, r(i) is the score vector at the ith iteration, c is a restart probability, and q is a query vector. We set c to 0.15, the maximum iteration to 100, and q to the one-hot vector where the element corresponding to Microsoft is 1, and the others are 0.\nAs shown in Table III, the common pattern of the two approaches is that many stocks among the top-10 belong to the technology sector. There is also a difference. In Table III, the blue color indicates the stocks that appear only in one of the two approaches among the top-10. In Table III(a), the k-nearest neighbor approach simply finds the top-10 stocks which are closest to Microsoft based on distances. On the other hand, the RWR approach finds the top-10 stocks by considering more complicated relationships. There are 4 stocks appearing only in Table III(b). S&P Global is included since it is very close to Moody\u2019s which is ranked 4th in Table III(a). Netflix, Autodesk, and NVIDIA are relatively far from the target stock compared to stocks such as Intuit and Alphabet, but they are included in the top-10 since they are very close to Amazon.com, Adobe, ANSYS, and Synopsys. This difference comes from the fact that the k-nearest neighbors approach considers only distances from the target stock while the RWR approach considers distances between other stocks in addition to the target stock.\nDPAR2 allows us to efficiently obtain factor matrices, and find interesting patterns in data."
        },
        {
            "heading": "V. RELATED WORKS",
            "text": "We review related works on tensor decomposition methods for regular and irregular tensors.\nTensor decomposition on regular dense tensors. There are efficient tensor decomposition methods on regular dense tensors. Pioneering works [38]\u2013[43] efficiently decompose a regular tensor by exploiting techniques that reduce time and space costs. Also, a lot of works [44]\u2013[47] proposed scalable tensor decomposition methods with parallelization to handle large-scale tensors. However, the aforementioned methods fail to deal with the irregularity of dense tensors since they are designed for regular tensors.\nPARAFAC2 decomposition on irregular tensors. Cheng and Haardt [18] proposed RD-ALS which preprocesses a given tensor and performs PARAFAC2 decomposition using the preprocessed result. However, RD-ALS requires high computational costs to preprocess a given tensor. Also, RDALS is less efficient in updating factor matrices since it computes reconstruction errors for the convergence criterion at each iteration. Recent works [11], [12], [15] attempted to analyze irregular sparse tensors. SPARTan [11] is a scalable PARAFAC2-ALS method for large electronic health records (EHR) data. COPA [12] improves the performance of PARAFAC2 decomposition by applying various constraints (e.g., smoothness). REPAIR [15] strengthens the robustness of PARAFAC2 decomposition by applying low-rank regularization. We do not compare DPAR2 with COPA and REPAIR since they concentrate on imposing practical constraints to handle irregular sparse tensors, especially EHR data. However, we do compare DPAR2 with SPARTan which the efficiency of COPA and REPAIR is based on. TASTE [16] is a joint PARAFAC2 decomposition method for large temporal and static tensors. Although the above methods are efficient in PARAFAC2 decomposition for irregular tensors, they concentrate only on irregular sparse tensors, especially EHR data. LogPar [17], a logistic PARAFAC2 decomposition method, analyzes temporal binary data represented as an irregular binary tensor. SPADE [48] efficiently deals with irregular tensors in a streaming setting. TedPar [49] improves the performance of PARAFAC2 decomposition by explicitly modeling the temporal dependency. Although the above methods effectively deal with irregular sparse tensors, especially EHR data, none of them focus on devising an efficient PARAFAC2 decomposition\nmethod on irregular dense tensors. On the other hand, DPAR2 is a fast and scalable PARAFAC2 decomposition method for irregular dense tensors."
        },
        {
            "heading": "VI. CONCLUSION",
            "text": "In this paper, we propose DPAR2, a fast and scalable PARAFAC2 decomposition method for irregular dense tensors. By compressing an irregular input tensor, careful reordering of the operations with the compressed results in each iteration, and careful partitioning of input slices, DPAR2 successfully achieves high efficiency to perform PARAFAC2 decomposition for irregular dense tensors. Experimental results show that DPAR2 is up to 6.0\u00d7 faster than existing PARAFAC2 decomposition methods while achieving comparable accuracy, and it is scalable with respect to the tensor size and target rank. With DPAR2, we discover interesting patterns in real-world irregular tensors. Future work includes devising an efficient PARAFAC2 decomposition method in a streaming setting."
        },
        {
            "heading": "ACKNOWLEDGMENT",
            "text": "This work was partly supported by the National Research Foundation of Korea(NRF) funded by MSIT(2022R1A2C3007921), and Institute of Information & communications Technology Planning & Evaluation(IITP) grant funded by MSIT [No.2021-0-01343, Artificial Intelligence Graduate School Program (Seoul National University)] and [NO.2021-0-02068, Artificial Intelligence Innovation Hub (Artificial Intelligence Institute, Seoul National University)]. The Institute of Engineering Research and ICT at Seoul National University provided research facilities for this work. U Kang is the corresponding author."
        }
    ],
    "title": "DPar2: Fast and Scalable PARAFAC2 Decomposition for Irregular Dense Tensors",
    "year": 2022
}