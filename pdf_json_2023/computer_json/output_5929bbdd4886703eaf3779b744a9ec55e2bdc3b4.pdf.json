{
    "abstractText": "In this paper, we introduce a no v el data augmentation methodology based on Conditional Progressi ve Generati v e Adv ersarial Networks (CPGAN) to generate diverse black hole (BH) images, accounting for variations in spin and electron temperature prescriptions. These generated images are valuable resources for training deep learning algorithms to accurately estimate black hole parameters from observational data. Our model can generate BH images for any spin value within the range of [ \u22121, 1], given an electron temperature distribution. To validate the effectiveness of our approach, we employ a convolutional neural network to predict the BH spin using both the GRMHD images and the images generated by our proposed model. Our results demonstrate a significant performance impro v ement when training is conducted with the augmented data set while testing is performed using GRMHD simulated data, as indicated by the high R 2 score. Consequently, we propose that GANs can be employed as cost-ef fecti ve models for black hole image generation and reliably augment training data sets for other parametrization algorithms.",
    "authors": [
        {
            "affiliations": [],
            "name": "Arya Mohan"
        },
        {
            "affiliations": [],
            "name": "Pavlos Protopapas"
        },
        {
            "affiliations": [],
            "name": "Keerthi Kunnumkai"
        },
        {
            "affiliations": [],
            "name": "Cecilia Garraffo"
        },
        {
            "affiliations": [],
            "name": "Lindy Blackburn"
        },
        {
            "affiliations": [],
            "name": "Sheperd S. Doeleman"
        },
        {
            "affiliations": [],
            "name": "Razieh Emami"
        },
        {
            "affiliations": [],
            "name": "Christian M. Fromm"
        },
        {
            "affiliations": [],
            "name": "Yosuke Mizuno"
        },
        {
            "affiliations": [],
            "name": "Angelo Ricarte"
        },
        {
            "affiliations": [],
            "name": "John A. Paulson"
        }
    ],
    "id": "SP:4821d8f2d2e30ba0524b1c01dee891667c7606fb",
    "references": [],
    "sections": [
        {
            "text": "MNRAS 527, 10965\u201310974 (2024) https://doi.org/10.1093/mnras/stad3797 Advance Access publication 2023 December 11\nGenerating images of the M87 \u2217 black hole using GANs\nArya Mohan , 1 \u2039 Pavlos Protopapas, 2 Keerthi Kunnumkai, 3 Cecilia Garraffo, 4 Lindy Blackburn, 5 , 6 Koushik Chatterjee , 5 , 6 Sheperd S. Doeleman, 5 , 6 Razieh Emami, 6 Christian M. Fromm, 7 , 8 , 9 Yosuke Mizuno 10 , 11 , 12 and Angelo Ricarte 5 , 6 1 Univ.AI, Singapore, 050531 2 John A. Paulson School of Engineering and Applied Science, Harvard University, Cambridge, MA 02138, USA 3 Department of Physics, Carnegie Mellon University, Pittsburgh, PA 15213, USA 4 AstroAI at the Center for Astrophysics | Harvard & Smithsonian, 60 Garden St, Cambridg e , MA 02138, USA 5 Black Hole Initiative at Harvard University, 20 Garden St, Cambridg e , MA 02138, USA 6 Center for Astrophysics | Harvard & Smithsonian, 60 Garden St, Cambridg e , MA 02138, USA 7 Institut f \u0308ur Theoretische Physik, Goethe-Universit \u0308at, Max-von-Laue-Strasse 1, 60438 Frankfurt, Germany 8 Institut f \u0308ur Theoretische Physik und Astrophysik, Universit \u0308at W \u0308urzburg, Emil-Fisc her-Str asse 31, 97074 W \u0308urzburg, Germany, D-60438 Frankfurt, Germany 9 Max-Planck-Institut f \u0308ur Radioastronomie, Auf dem H \u0308ugel 69, D-53121 Bonn, Germany 10 Tsung-Dao Lee Institute, Shanghai Jiao-Tong University, Shanghai, 520 Shengrong Road, 201210, P. R. China 11 School of Physics & Astronomy, Shanghai Jiao-Tong University, Shanghai, 800 Dongchuan Road, 200240, P. R. China 12 Institut f \u0308ur Theoretische Physik, Goethe Universit \u0308at, Max-von-Laue-Str. 1, D-60438 Frankfurt am Main, Germany\nAccepted 2023 No v ember 21. Received 2023 November 20; in original form 2023 August 13\nA B S T R A C T In this paper, we introduce a no v el data augmentation methodology based on Conditional Progressi ve Generati v e Adv ersarial Networks (CPGAN) to generate diverse black hole (BH) images, accounting for variations in spin and electron temperature prescriptions. These generated images are valuable resources for training deep learning algorithms to accurately estimate black hole parameters from observational data. Our model can generate BH images for any spin value within the range of [ \u22121, 1], given an electron temperature distribution. To validate the effectiveness of our approach, we employ a convolutional neural network to predict the BH spin using both the GRMHD images and the images generated by our proposed model. Our results demonstrate a significant performance impro v ement when training is conducted with the augmented data set while testing is performed using GRMHD simulated data, as indicated by the high R 2 score. Consequently, we propose that GANs can be employed as cost-ef fecti ve models for black hole image generation and reliably augment training data sets for other parametrization algorithms.\nKey words: black hole physics \u2013 methods: data analysis \u2013 techniques: image processing \u2013 software: data analysis.\n1\nO ( b a C r 1 B a a a a h B\n2 a c b T T i w G o t C\nh d G\n\u00a9 P C p\nD ow nloaded from https://academ ic.oup.com /m nras/article/527/4/10965/7469481 by Indian Institute of Technology Patna user on 10 January 2024 I N T RO D U C T I O N ne of the key predictions of Einstein\u2019s theory of general relativity GR) is the existence of black holes (BHs). The synchrotron emission y the hot, orbiting electrons near the BH\u2019s event horizon provides no v el way to directly observe them (v. Laue 1921 ; Bardeen, arter & Hawking 1973 ), which is manifested through a bright\ning of emission, singling an interior dark BH \u2018shadow\u2019 (Luminet 979 ; Falcke, Melia & Agol 2000 ). The shadow diameter for a H of mass M is roughly 10 GM / c 2 , depending on the BH spin s well as the observer inclination. For nearby low-luminosity ctive galactic nuclei (AGN) sources, initial theory, simulations, nd observations indicated that such a shadow could be observed t millimeter wavelengths, where hot plasma surrounding the black ole becomes transparent (Noble et al. 2007 ; Doeleman et al. 2008 ; roderick & Loeb 2009 ; Doeleman et al. 2012 ; Mo \u0301scibrodzka et al.\nE-mail: arya.mohan@outlook.com\nw p (\n2023 The Author(s). ublished by Oxford University Press on behalf of Royal Astronomical Society. Th ommons Attribution License ( https:// creativecommons.org/ licenses/ by/ 4.0/ ), whic rovided the original work is properly cited.\n012 ; Yuan & Narayan 2014 ). At a distance of D 16.8 Mpc, with mass of M 6 . 5 \u00d7 10 9 M , the supermassive black hole at the entre of galaxy M87 has a shadow diameter that can be resolved y millimeter wavelength very-long-baseline interferometry (VLBI). he Event Horizon Telescope Collaboration (EHTC; Event Horizon elescope Collaboration et al. 2019a , b , c , d , e , f ) captured the first mages of M87\u2019s shado w, re vealing an asymmetric ring-like structure ith angular diameter of 42 \u00b1 3 \u03bcas , consistent with predictions of R (Fig. 1 ). Further, in 2022, the EHTC published the first image f the shadow surrounding Sagittarius A \u2217 (SgrA \u2217), the black hole at he centre of our own Milky Way galaxy (Event Horizon Telescope ollaboration et al. 2022 ). To model the appearance of M87 \u2217, EHT ran general relativistic ydrodynamic (GRMHD) simulations that described the complex ynamics of the gas and plasma surrounding the black hole. The RMHD simulations are parametrized by the black hole spin ( a \u2217), ith the simulation library consisting of five values of this spin arameter. Next, the EHT performed general relativistic ray tracing GRRT) based on these GRMHD simulations to create a library of\nis is an Open Access article distributed under the terms of the Creative h permits unrestricted reuse, distribution, and reproduction in any medium,\nM\nFigure 1. The Event Horizon Telescope (EHT) captured these images using a technique called very-long-baseline interferometry (VLBI). Eight radio telescopes were combined, each operating at millimeter and submillimeter wavelengths, to create a virtual telescope as big as the Earth. With an angular resolution of 20 \u03bcas when operating at 1.3 mm, EHT (Event Horizon Telescope Collaboration et al. 2019b ) obtained an image of the M87 \u2217 at the centre of the Messier 87 galaxy.\ns i F o o ( b b m\nT\nw\nR\nd T t i g w\nv a u t e l t a s p\nG fi a R w c v n\nm T a c b o\ng N a d a m G s i p d e\n2\n2\nG h t K Z\nb t g c w t P n p b d f\nw ( E c P l i g\no g i g i T a\nA w\nD ow nloaded from https://academ ic.oup.com /m nras/article/527/4/10965/7469481 by Indian Institute of Technology Patna user on 10 January 2024\nynthetic black hole images to compare directly with the observed mage of M87 \u2217 (Event Horizon Telescope Collaboration et al. 2019e ). urther, to calculate the synchrotron emission from the simulations, ne needs the electron temperature ( T e ). Since GRMHD simulations nly evolve a single temperature fluid, we use the R \u2212 \u03b2 prescription Mo \u0301scibrodzka, F alcke & Shioka wa 2016 ) to define a relationship etween the electron temperature and the GRMHD fluid properties ased on the local gas plasma- \u03b2, which is the ratio of the gas and agnetic pressures: e = 2 m p U gas 3 k B \u03c1(2 + R) , (1) here = 1 + R high \u03b2 2 1 + \u03b22 . (2) Here, m p , \u03c1, U gas , and k B are the proton mass, GRMHD fluid ensity and internal energy, and finally, the Boltzmann constant. hus, in this work, we use the black hole spin parameter a \u2217 and he temperature parameter R high to characterize a library of raytraced mages from (Roelofs et al. 2021 ). Understanding these images will i ve us v aluable insights into the nature of space, time, and gravity, hich are fundamental to our understanding of the universe. In recent years, deep-learning-based models have succeeded in arious computer vision tasks. In particular, its applications in strophysics have yielded impressive results. One example is the se of computer vision techniques to study black holes, showcasing he versatility and ef fecti veness of this technology. (van der Gucht t al. 2020 ; Qiu et al. 2023 ; Tsui in prep.) explored various machine earning methods to parametrize the M87 \u2217 black hole. Ho we ver, hese frameworks rely heavily on having large training data sets nd the high computational cost of GRMHD simulations leads to parse training data sets. As a result, the development of accurate arametrization algorithms presents a significant challenge. To o v ercome this problem, we propose a Conditional Progressiv e\nenerativ e Adv ersarial Network (CPGAN) to synthesize highdelity data that resemble the simulated images of GRMHD. Our rchitecture is capable of synthesizing images based on spin a \u2217 and high . For a given R high , our model can generate black hole images ith any spin in the continuous interval [ \u22121, 1]. Considering the high ost associated with directly simulating finely-grained parameter alues, for example, a spin of 0.64, the images generated by our etwork, if appropriately validated, pro v e to be valuable resources for\nNRAS 527, 10965\u201310974 (2024)\nore ef fecti vely sampling the parameter space in theoretical studies. hrough our experiments, we demonstrate that our model can be used s a data augmentation tool for other deep learning algorithms, which an play a vital role in improving the parametrization of supermassive lack holes, as well as facilitating a more comprehensive exploration f their characteristics and properties. This paper is organized as follows. Section 2 presents the backround information, including an o v ervie w of Generati v e Adv ersarial etworks (GANs), Wasserstein GANs, Progressive Growing GANs, nd various evaluation metrics for GANs. Section 3 focuses on the ata used in our study, co v ering simulated data, data set structure, nd data augmentation techniques. In Section 4 , we describe our ethodology, including the general approach, specific details of the AN implementation, the parametrization network, and the model election process. Section 5 presents the results of our experiments, ncluding an analysis of the generated samples and classification erformance. Finally, in Section 6 , we summarize the conclusions rawn from our research and discuss future directions for further xploration in this field.\nB AC K G R O U N D\n.1 Generati v e Adv ersarial Netw orks\nenerativ e Adv ersarial Networks (GANs; Goodfellow et al. 2020 ) ave shown remarkable results in image generation due to their ability o produce realistic and diverse images (Karras, Laine & Aila 2019 ; arras et al. 2017, 2020 , 2021 ; Brock, Donahue & Simonyan 2018 ; hu et al. 2017 ). GANs utilize game theory to approximate the probability distriution of a set of images by training two players, the generator and he discriminator, through a min\u2013max game. The generator aims to enerate realistic images to deceive the discriminator, while the disriminator strives to distinguish between real and generated images ith high accuracy. Ef fecti vely, GANs learn a data distribution P g ( x ) hat matches the real data distribution P r ( x ). Given real images x r \u223c r , GANs learn a generator network G that transforms some input oise z \u223c P z to a sample x g \u223c P g . These generated samples are then assed on to a discriminator network D whose role is to distinguish etween samples from the real data distribution P r and the generated istribution P g . This goal is achieved by optimizing the objective unction:\nmin G max D\nV ( D, G ) = E x\u223cP r [ log D( x)] + E z\u223cnoise [ log (1 \u2212 D( G ( z)))] , (3)\nhere min G max D denote the minimization o v er the generator G ) and the maximization o v er the discriminator ( D ). The term x\u223cP r [ log D( x)] corresponds to the expected logarithm of the disriminator\u2019s output when e v aluating real data samples x drawn from r . Similarly, E z \u223c noise [log(1 \u2212 D ( G ( z)))] represents the expected ogarithm of the discriminator\u2019s output when considering generated mages G ( z) obtained by passing noise samples z through the enerator. Thus, one network, the generator, tries to create realistic data. The ther network, the discriminator, tries to distinguish between the enerated data and the real data. The generator keeps improving ts output until it fools the discriminator into thinking that the enerated data is real. The discriminator, on the other hand, keeps mproving its ability to distinguish between the two types of data. his process continues until the generator can produce samples that re indistinguishable from the real data.\nIn order to have control o v er the images generated by a GAN, the uxiliary Conditional Generativ e Adv ersarial Network (ACGAN) as introduced (Odena, Olah & Shlens 2017 ). This framework\ne t c v i t c\nc c d s g g a\np d i b h d s a t\n2\nW a i a f t a t d\nt s T L s w o u s c\ni I f l\nL\nL\nw a i r\n2\nP d i w b\na t w l a i o T n s m\ni a q m G\n2\nT i H i i t o b p a a\nt g d o e o s s g d\n2\nF t f t g I w\nF\nD ow nloaded from https://academ ic.oup.com /m nras/article/527/4/10965/7469481 by Indian Institute of Technology Patna user on 10 January 2024\nnhances the GAN architecture by training the generator not only o deceive the discriminator but also to produce images with specific lass labels. In ACGAN, the generator is conditioned on a class label ector ( y ), which is concatenated with the latent noise vector ( z). By ncorporating this class label information into the generator\u2019s input, he generator learns to generate images that correspond to a particular ategory or class.\nConversely, the discriminator network in ACGAN is modified to lassify both real images and generated images into their respective lasses. This modification enables the discriminator to not only ifferentiate between real and f ak e images but also discern the pecific class to which an image belongs. By jointly training the enerator and discriminator in this manner , A CGAN facilitates the eneration of realistic images that possess the characteristics and ttributes associated with a particular class.\nTo further enhance ACGAN, we introduce a separate regressor in arallel with the discriminator, thereby separating the regression and iscriminator tasks. This separation is necessary because the discrimnator alone has limited flexibility in both, ef fecti vely discriminating etween real and f ak e images, and capturing the spin of the black ole. The parallel operation allows the discriminator to focus on iscrimination while the regressor specializes in the prediction of pin a \u2217. During training, the generator , discriminator , and classifier re optimized to minimize a composite loss function that includes he GAN loss as well as a regressor loss.\n.2 Wasserstein GANs\nasserstein GANs (WGANs; Arjo vsk y, Chintala & Bottou 2017 ) re a variant of GANs specifically designed to impro v e the qualty of generated samples. In WGANs, the Wasserstein distance, lso known as the Earth Mo v er\u2019s distance, is used as the loss unction for the GAN to train the generator and the discriminaor, commonly called the critic. This distance measure provides more informative and stable measure of the dissimilarity beween the real data distribution ( P r ) and the generated samples istribution ( P g ). To ensure the training process is stable, WGANs introduce a echnique called weight clipping. Weight clipping involves contraining the weights of the critic network within a specific range. his constraint enforces that the critic function remains a 1- ipschitz function, meaning its deri v ati ve is bounded by a contant value of 1. By enforcing the 1-Lipschitz condition through eight clipping, WGANs can alleviate issues such as vanishing r exploding gradients, thereby enabling the generator to learn the nderlying data distribution more reliably, resulting in impro v ed ample generation capabilities and reducing issues such as mode ollapse.\nWGAN-GP (WGAN with Gradient Penalty; Gulrajani et al. 2017 ) s an extension of WGAN that further stabilizes the training process. nstead of weight clipping, it adds a gradient penalty term to the loss unction in order to enforce the Lipschitz constraint on the critic. The oss function for the critic in a WGAN-GP is given by:\nG = E x r \u223cP r [ D( x r )] \u2212 E x g \u223cP g [ D( x g )] (4) D = \u2212L G + \u03bbE \u02c6 x\u223cP \u02c6 x [ ( \u2016\u2207 \u02c6 xD( \u0302 x) \u2016 2 \u2212 1) 2 ] , (5)\nhere P \u02c6 x is the distribution implicitly defined by sampling uniformly long linear paths between points sampled from P r and P g , and \u03bb s the penalty coefficient that controls the strength of the gradient egularization.\n.3 Pr ogr essi v e Gro wing GANs\nrogressi ve Gro wing of GANs (PGGANs; Karras et al. 2017 ) was esigned to tackle challenges faced while generating high-resolution mages. The main idea behind PGGANs is to start training GANs ith low-resolution images and gradually increase the resolution of oth the generator and discriminator o v er time. PGGAN starts with a low-resolution version of the target image nd gradually increases the resolution of the generated images as raining progresses. This is achieved through progressive growth, here the generator and discriminator are trained on increasingly arger images at each stage. To facilitate this transition, a smooth lpha-fading technique is employed. This technique involves blendng the parameters of the existing network with the newly introduced nes, ensuring a seamless transition in the network\u2019s architecture. he weights from the previous stage are used as initialization for the ext stage, allowing the network to build on its knowledge at each tage and helping prevent the generator from collapsing into a single ode. Another key aspect of PGGAN is the use of a multiscale discrimnator, which is capable of processing images at multiple resolutions nd provides more detailed feedback to the generator about the uality of the generated samples. This can help the generator produce ore realistic images and impro v e the o v erall performance of the AN.\n.4 Evaluation of GANs\nhe loss functions used in GANs ef fecti vely address the adversaral problem by optimizing the generator-discriminator dynamics. o we ver, solely using the loss function as an e v aluation metric is nadequate for assessing the quality of the generated samples. This s because the loss function is designed specifically for the generator rained within the GAN framework and may not generalize well to ther data sets or downstream tasks. In other words, the e v aluation ased solely on the loss function does not capture the full range of erformance characteristics. Therefore, it is crucial to incorporate dditional e v aluation metrics that provide a more comprehensi ve ssessment of the generated samples.\nTo assess the performance of GANs, it is necessary to measure he distance between the probability distributions of real ( P r ) and enerated ( P g ) data. Ho we ver, when dealing with high-dimensional istributions, devising a suitable metric is challenging and remains an pen problem. Furthermore, the e v aluation metrics for GANs must f fecti vely measure both fidelity and diversity. Fidelity is a measure f similarity between the characteristics of the real and generated amples, while diversity measures the variance of the generated amples. In other words, fidelity is an assessment of realism in the enerated samples, while diversity assesses how much of the real ata distribution is captured by the generated samples.\n.4.1 Frech \u0301et Distance (FID)\nrechet Distance introduced by (Heusel et al. 2017 ) is a measure of he distance between two probability distributions. To compute the rechet distance, we first need to extract feature vectors by mapping he samples x r or x g to an intermediate feature space. This mapping is enerally performed using a convolutional neural network (usually nceptionV3 (Szegedy et al. 2016 )). Once we have the feature vectors, e can compute the FID as follows:\nID = \u2016 \u03bcg \u2212 \u03bcr \u2016 2 + Tr ( C r + C g \u2212 2 \u221a C g C r ) (6)\nMNRAS 527, 10965\u201310974 (2024)\nM\nw g c r\n2\nT b fi o\nf O m t\n2\nA ( t c g s\np s p a t 1 d a p t t ( m o s\n3\nI E s t w s\no u b\nf t G m s\n3\nI m c b 2 r T m S T s\nt a r e fl ( e N ( t w s ( p h\ns r i a m p t 2 o ( t 2 a p d 2\ni i q c i t w 2 E\nfi H a t\nD ow nloaded from https://academ ic.oup.com /m nras/article/527/4/10965/7469481 by Indian Institute of Technology Patna user on 10 January 2024\nhere \u03bcg and \u03bcr are the means of the feature vectors for the enerated and reference samples, respectively, C g and C r are the ovariance matrices of the feature vectors for the generated and eference samples, respectively, and Tr ( A ) is the trace of matrix A .\n.4.2 Precision and recall\nhe use of precision and recall as an e v aluation metric, as suggested y (Sajjadi et al. 2018 ), provides us with separate measurements for delity and di versity, allo wing for a more comprehensi ve e v aluation f the generated samples. Precision assesses the fidelity of the image by measuring the raction of generated samples that belong to the real distribution. n the other hand, recall assesses the diversity of the image by easuring the fraction of real samples that are correctly captured by he generated distribution.\n.4.3 TSTR and TRTR\nn indirect approach to measuring the similarity between the real P r ) and f ak e distributions ( P g ) is to compare the performance of he generated samples on downstream tasks. If a generative model an successfully learn P r , then the data drawn from either the real or enerated distributions will yield similar results on the appropriately elected downstream task.\nIn our work, we define the do wnstream objecti ve as the arametrization of the target image using a regressor. The R 2 core, also known as the coefficient of determination, measures the roportion of the variance in the target variable, in our case the spin \u2217, that can be explained by the regressor. It indicates how well he regression model fits the observed data, with values closer to indicating a better fit. By comparing the R 2 scores obtained in ifferent scenarios, such as training on synthetic or GRMHD data nd testing on GRMHD data, we can assess the similarity between the erformance of the generative model and real data on the downstream ask of image parametrization.. Performance in these two scenarios, rain on synthetic test on real (TSTR), and train on real test on real TRTR; Yang et al. 2017 ) are expected to be similar. Additionally, we easure performance when the parametrization network is trained n an augmented data set, consisting of a mix of real and generated amples, and tested on the real data set (T(S + R)TR).\nDATA\nn addition to providing the event-horizon scale images of M87 \u2217, vent Horizon Telescope Collaboration (EHTC Event Horizon Telecope Collaboration et al. 2019a , b , c , d , e , f ) successfully modelled he appearance of M87 \u2217 using GRMHD simulations. Each simulation as used to describe several different physical scenarios, and each cenario was then used to generate several simulated images. Since the GRMHD-simulated data was generated based on a range f parameters and labelled accordingly, it was a sensible choice for se as the training data. The data used in this project are labelled ased on four key parameters: a \u2217 (spin), R high , frequency and frame. Using these labelled data, a generative model can learn to identify eatures that characterize a given label and generate new data points hat are virtually indistinguishable to simulation experts from the RMHD simulated data, allowing for the creation of a larger and ore diverse data set for parametrization algorithms and detailed tudy.\nNRAS 527, 10965\u201310974 (2024)\n.1 Simulated data\nn the EHT collaboration, the accretion flow of the M87 \u2217 was odelled using GRMHD simulations, which was found to sucessfully describe a turbulent, hot, magnetized disc orbiting a Kerr lack hole (EHTC Event Horizon Telescope Collaboration et al. 019a , b , c , d , e , f ). To further explore the imaging results, the esearchers created a Simulation Library (EHTC Event Horizon elescope Collaboration et al. 2019a , b , c , d , e , f ) by fitting GRMHD odels to the image of M87 \u2217. They then used the elements of this imulation Library to create an Image Library (EHTC Event Horizon elescope Collaboration et al. 2019a , b , c , d , e , f ) through GRRT imulations.\nA typical GRMHD simulation in the library is characterized by wo parameters: (1) the dimensionless spin a \u2217 \u2261 Jc / GM 2 , where J nd M are the spin angular momentum and mass of the black hole, espectively, and (2) the net dimensionless magnetic flux over the vent horizon \u03c6 = / ( M\u0307 R 2 g ) 1 / 2 , where and M\u0307 are the magnetic ux and mass flux (or accretion rate) across the horizon, respectively EHTC Event Horizon Telescope Collaboration et al. 2019a , b , c , d , , f ). Based on these parameters, the models are either \u2018Standard and ormal Evolution\u2019 (SANE) if \u03c6 \u223c 1 or \u2018Magnetically Arrested Disc\u2019 MAD) if \u03c6 \u223c 15. The models also include five a \u2217 values that make he accretion disc either \u2018prograde\u2019 if the motion of the accretion disc ith respect to its spin axis is in the same direction of black hole pin ( a \u2217 \u2265 0) or \u2018retrograde\u2019 if the motion is in the opposite direction a \u2217 < 0). By varying a \u2217 and \u03c6, they have thoroughly explored the hysical properties of magnetized accretion flows onto Kerr black oles. Once a simulation is completed, the image library is then contructed on the basis of these GRMHD models by performing general elativistic ray-tracing (GRRT) simulations. A GRRT simulation s characterized by the properties of the fluid, the emission and bsorption coefficients, the inclination angle between the angularomentum vector of the accretion-flow and the line of sight, the osition angle, the mass of the black hole, and the distance from he observ er (EHTC Ev ent Horizon Telescope Collaboration et al. 019a , b , c , d , e , f ). Here, we created a new image library consisting f SANE GRMHD models generated with the GRMHD code BHAC Porth et al. 2017 ) and the GRRT images performed by the radiative ransfer code BHOSS code (Younsi, Wu & Fuerst 2012 ; Younsi et al. 020 ). During the GRRT, we assumed a thermal electron distribution nd seven values of the temperature ratio of electrons to protons, arametrized by R high , one mass, and two inclinations (for more etails on the GRMHD and GRRT simulations see Roelofs et al. 021 ; Fromm et al. 2022 ). Depending on the frequency mode determined at the time of GRRT maging, the features exhibited by a black hole can vary, as the change n energy distribution can affect the radiation spectra. A higher freuency corresponds to higher resolution and a deeper view of the acretion flow, since the angular resolution is proportional to the observng wavelength ( \u03bb) and the maximum projected baseline length beween the telescopes in the array L (Thompson 1999 ). In this project, e utilized SANE images operating at a single frequency mode of 30 GHz, which corresponds to the current operating frequency of the HT. Therefore, the simulated images from the Image Library, which\ntted closure phases and amplitudes of the April 11 (EHTC Event orizon Telescope Collaboration et al. 2019a , b , c , d , e , f ) data, best\ns seen in Fig. 2 can correspond to different spins and different flow ypes.\n3\nT ( s p a h\n1 f m w t v\n3\nD b o z c n f t l g K\nt S p o a\nu p d s t\nFigure 4. Diagram of the methodology.\no i a e\n4\n4\nW C m s t g a a i a f t a t b G e\nl d a p ( c a\ni\nnloaded from https://academ ic.oup.com /m nras/article/527/4/10965/7469481 by Indian Institute of Technology Patna user on 10 January 2024\n.2 Data set structure he GRMHD simulated images are characterized by two parameters: 1) a \u2217, consisting of five dif ferent v alues, and (2) R high , consisting of e ven dif ferent v alues, which correspond to a total of 35 unique arameter combinations. Furthermore, each simulation generates series of frames that represent the rotating motion of the black ole. In our study, we have created a data set (Table 1 ) by treating all 01 frames of each simulation as individual static images, allowing or a more detailed analysis of the image properties and facilitating achine learning-based parameter estimation algorithms. It is also orth noting that the time spacing between the frames corresponds o 3.5 days. While our study focuses on image generation, generating ideos is an area for further investigation. .3 Data augmentation ata augmentation is a process of creating additional training data y applying various transformations to existing data. In the case f image data, these transformations can include flipping, rotating, ooming, and cropping, among others. While data augmentation is ommonly used to enforce symmetries in the learned function of a eural network, it is also widely used to increase the fidelity of the unction by enriching the data set. Increasing the size and diversity of he training data set through data augmentation can help the machine earning model learn more robust features and reduce o v erfitting by eneralizing better to unseen data (Perez & Wang 2017 ; Shorten & hoshgoftaar 2019 ). In this paper, we performed these data augmentation transforma-\nions on the existing primary data source from GRMHD simulations. pecifically, the primary data set was rotated in 90 \u25e6 steps, quadruling the training data. This technique allowed the GAN to be trained n a larger data set, which ultimately led to better model performance nd the generation of more realistic simulated images.\nIt is important to note that this transformation did not alter the nderlying physics in any way, as the position angle is arbitrary. In articular, the direction of the brightness asymmetry of the image epends on the projection of the black hole\u2019s spin axis onto the ky plane, which is not of physical significance. On the contrary, his augmentation made the model robust against different rotations\nf the images as a result of the observer\u2019s line of sight. It is mportant for the model not to use the direction of the brightness symmetry to distinguish retrograde from prograde models, for xample.\nM E T H O D O L O G Y\n.1 General description\ne propose a data augmentation methodology that involves a onditional Progressive Generativ e Adv ersarial Network (CPGAN) odel designed to generate new images of black holes based on\npecific spin and electron distribution parameters. In this framework, he CPGAN consists of three convolutional neural networks: a enerator that produces the synthetic images based on a \u2217 and R high , critic that differentiates between real and synthetic images, and an uxiliary regressor that predicts the spin of a given image as shown n Fig. 3 . Our proposed CPGAN model can generate new images for ny a \u2217 in the [ \u22121, 1] range. Additionally, by training one CPGAN or every R high , we can ensure that our model performs optimally for he entire range of R high and a \u2217 values, resulting in a more robust nd accurate approach. This set up helps us increase the size of the raining data set, allowing for more accurate parametrization of a lack hole image. The details of how the conditional progressive AN is built, including the architecture and loss function, will be xplained in Section 4.2 . To e v aluate the ef fecti veness of our proposed methodology, we utiize two e v aluation metrics: TSTR and TRTR, which were pre viously iscussed in Section 2.4.3 . To compute these metrics, we employ convolutional neural network (CNN), as proposed in (Tsui in rep.). The CNN takes as input either the GRMHD-simulated images for TRTR) or the generated images (for TSTR) and predicts the orresponding values of a \u2217. Further details of the CNN architecture nd training process will be explained in Section 4.3 .\nFig. 4 provides a summary of the proposed methodology, illustratng the flow of data and models involved in the process.\nMNRAS 527, 10965\u201310974 (2024)\nM\nTable 2. Generator architecture.\nGenerator Acti v ation Output shape\nLatent vector \u2013 1 \u00d7 1 \u00d7 512 a \u2217 \u2013 1 \u00d7 1 \u00d7 1 Concatenate \u2013 1 \u00d7 1 \u00d7 513 Dense LReLU 1 \u00d7 1 \u00d7 8192 Reshape \u2013 4 \u00d7 4 \u00d7 512 Conv 4 \u00d7 4 LReLU 4 \u00d7 4 \u00d7 512 Conv 3 \u00d7 3 LReLU 4 \u00d7 4 \u00d7 512 Upsample \u2013 8 \u00d7 8 \u00d7 512 Conv 3 \u00d7 3 LReLU 8 \u00d7 8 \u00d7 512 Conv 3 \u00d7 3 LReLU 8 \u00d7 8 \u00d7 256 Upsample \u2013 16 \u00d7 16 \u00d7 256 Conv 3 \u00d7 3 LReLU 16 \u00d7 16 \u00d7 256 Conv 3 \u00d7 3 LReLU 16 \u00d7 16 \u00d7 128 Upsample \u2013 32 \u00d7 32 \u00d7 128 Conv 3 \u00d7 3 LReLU 32 \u00d7 32 \u00d7 128 Conv 3 \u00d7 3 LReLU 32 \u00d7 32 \u00d7 64 Upsample \u2013 64 \u00d7 64 \u00d7 64 Conv 3 \u00d7 3 LReLU 64 \u00d7 64 \u00d7 64 Conv 3 \u00d7 3 LReLU 64 \u00d7 64 \u00d7 32 Upsample \u2013 128 \u00d7 128 \u00d7 32 Conv 3 \u00d7 3 LReLU 128 \u00d7 128 \u00d7 32 Conv 3 \u00d7 3 LReLU 128 \u00d7 128 \u00d7 16 Conv 1 \u00d7 1 tanh 128 \u00d7 128 \u00d7 2\n4\nI a c\nt n a n 5 a h\nt i t o i a d f i a\no g s i a ( R t i g s\nTable 3. Critic architecture.\nCritic Acti v ation Output shape\nInput image \u2013 128 \u00d7 128 \u00d7 2 Conv 1 \u00d7 1 LReLU 128 \u00d7 128 \u00d7 16 Conv 3 \u00d7 3 LReLU 128 \u00d7 128 \u00d7 16 Conv 3 \u00d7 3 LReLU 128 \u00d7 128 \u00d7 32 Downsample \u2013 64 \u00d7 64 \u00d7 32 Conv 3 \u00d7 3 LReLU 64 \u00d7 64 \u00d7 32 Conv 3 \u00d7 3 LReLU 128 \u00d7 128 \u00d7 64 Downsample \u2013 32 \u00d7 32 \u00d7 64 Conv 3 \u00d7 3 LReLU 32 \u00d7 32 \u00d7 64 Conv 3 \u00d7 3 LReLU 32 \u00d7 32 \u00d7 128 Downsample \u2013 16 \u00d7 16 \u00d7 128 Conv 3 \u00d7 3 LReLU 16 \u00d7 16 \u00d7 128 Conv 3 \u00d7 3 LReLU 16 \u00d7 16 \u00d7 256 Downsample \u2013 8 \u00d7 8 \u00d7 256 Conv 3 \u00d7 3 LReLU 8 \u00d7 8 \u00d7 256 Conv 3 \u00d7 3 LReLU 8 \u00d7 8 \u00d7 512 Downsample \u2013 4 \u00d7 4 \u00d7 512 Minibatch stddev \u2013 4 \u00d7 4 \u00d7 513 Conv 3 \u00d7 3 LReLU 4 \u00d7 4 \u00d7 512 Conv 4 \u00d7 4 LReLU 1 \u00d7 1 \u00d7 512 Dense Linear 1 \u00d7 1 \u00d7 1\np l\nL\nL\nL\nw a d i r\na g 1 t c i h a D f c w B c fi p\ne d t S\nD ow nloaded from https://academ ic.oup.com /m nras/article/527/4/10965/7469481 by Indian Institute of Technology Patna user on 10 January 2024 .2 GAN details n this section, we provide a detailed explanation of the GAN rchitecture used in our proposed methodology. Our GAN framework onsists of three distinct networks: generator, critic, and regressor. Following the conditional GAN (Mirza & Osindero 2014 ) archiecture, the generator takes as input a conditioning variable a \u2217 and the oise vector. The input is a concatenated vector z\u0304 = [ z, a \u2217], where \u2217 is sampled from five discrete spins in our data set and z is a oise vector such that z \u2208 R l \u223c N (0 , I ). Here, we choose l to be a 12-dimensional noise vector. Table 2 shows the detailed generator rchitecture used to generate a 128 \u00d7 128 pixel image of a black ole. The critic network takes an image as its input, either belonging o the real data set or an image generated by the generator, and s trained to distinguish between real and generated images. Here, he output of the critic is a scalar value that represents the realness f the image. The critic is trained to output high values for real mages and low values for generated images. In addition, our rchitecture incorporates a minibatch standard deviation layer, as escribed in Table 3 , which calculates the standard deviation of eatures within a mini-batch of data. This layer introduces diversity nto the generated samples by considering the measure of diversity, llowing the generator to produce more varied outputs.\nThe regressor network takes the generated image as input and utputs its prediction of spin ( a \u2217) as seen in Fig. 3 and Table 4 . Our oal with the regressor network is to accurately predict the value of pin for each generated image. The gradients from the regressor help n reinforcing the connection between the generated image and its ssociated spin. All three of our networks using the Leakly ReLU LReLU) acti v ation function. It is an extension of the traditional eLU (Rectified Linear Unit) acti v ation function. In a ReLU, when he input is greater than zero, it returns the input value, and when t\u2019s less than zero, it returns zero. In contrast, LReLU allows a small radient for ne gativ e values, typically by multiplying the input by a mall positive constant. This slight \u2018leak\u2019 for negative inputs helps\nNRAS 527, 10965\u201310974 (2024)\nre vent the v anishing gradient problem and enables the network to earn more ef fecti vely.\nThe loss function used to train this GAN is as follows:\nG = E x r \u223cP r [ D( x r )] \u2212 E x g \u223cP g [ D( x g )] + N \u2211\ni= 1 ( a \u2217 \u2212 R( x g )) 2 (7)\nD = \u2212L G + \u03bbE \u02c6 x\u223cP \u02c6 x [ ( \u2016\u2207 \u02c6 xD( \u0302 x) \u2016 2 \u2212 1) 2 ] (8)\nR = N \u2211\ni= 1 ( a \u2217 \u2212 R( x g )) 2 +\nN \u2211 i= 1 ( a \u2217 \u2212 R( x r )) 2 , (9)\nhere P \u02c6 x is the distribution implicitly defined by sampling uniformly long linear paths between points sampled from P r and P g , x r is a ata point sampled from P r , x g is a data point sampled from P g , \u03bb s the penalty coefficient that controls the strength of the gradient egularization and a \u2217 is all of the spins from the real data set.\nTo enhance the quality of the generated images, we employed progressive training strategy, training all three networks while radually increasing the image resolution from 4 \u00d7 4 pixels to 28 \u00d7 128 pixels. This approach allows the generator to learn he fundamental structure of the images at lower resolutions before apturing finer details. It is worth noting that the GRMHD simulation mages themselves were not generated at different resolutions; o we ver, to ensure a fair comparison between the synthetic and ctual data, we applied a downsampling process to the real images. uring progressive training, we also employed a smooth alphaading technique to blend the images generated by different network onfigurations. This technique mitigates artefacts that can arise hen the generator abruptly transitions to a new configuration. y smoothly blending the outputs, the network produces visually oherent images at each training step, allowing us to e v aluate the delity and realism of the generated images throughout the training rocess. We conducted various experiments to achieve optimal results, xploring different configurations. Through these experiments, we isco v ered that a training approach involving alternating between he generator and critic networks yielded fa v ourable outcomes. pecifically, we trained the critic network five times for every training\ni l t ( s w s\ni H \u2212 m\na o w p\n4\nT d n n t c s F ( s R i v r t\ni f t S\n4\nT w c o c\nt t s T R\np i u ( S\nTable 4. Regressor architecture.\nRegressor Acti v ation Output shape\nInput Image \u2013 128 \u00d7 128 \u00d7 2 Conv 3 \u00d7 3 LReLU 128 \u00d7 128 \u00d7 10 Downsample \u2013 64 \u00d7 64 \u00d7 10 Conv 3 \u00d7 3 LReLU 64 \u00d7 64 \u00d7 20 Downsample \u2013 32 \u00d7 32 \u00d7 20 Conv 3 \u00d7 3 LReLU 32 \u00d7 32 \u00d7 50 Downsample \u2013 16 \u00d7 16 \u00d7 50 Conv 3 \u00d7 3 LReLU 16 \u00d7 16 \u00d7 50 Downsample \u2013 8 \u00d7 8 \u00d7 50 Conv 3 \u00d7 3 LReLU 8 \u00d7 8 \u00d7 50 Downsample \u2013 4 \u00d7 4 \u00d7 50 Conv 3 \u00d7 3 LReLU 4 \u00d7 4 \u00d7 50 Downsample \u2013 1 \u00d7 1 \u00d7 50 Dense LReLU 1 \u00d7 1 \u00d7 16 Dense Linear 1 \u00d7 1 \u00d7 1\nFigure 5. Tsui (in prep.)\u2019s architecture of the CNN regressor, which takes in images with format 128 \u00d7 128 \u00d7 2. The convolutional layers transform the original image into a vector z, where z \u2208 R 5 0. Eventually, the fully-connected neural network (FCNN) projects z into a single variable output a \u2217.\nFigure 6 Generated sample for a \u2217 = 0.0 and R high = 1.\n5\n5\nO i R h r\nh t b e i 0\nD ow nloaded from https://academ ic.oup.com /m nras/article/527/4/10965/7469481 by Indian Institute of Technology Patna user on 10 January 2024\nteration of the generator. Additionally, we utilized the WGAN-GP oss (Arjo vsk y, Chintala & Bottou 2017 ), which further contributed o the superior performance of our Generative Adversarial Network GAN) in generating images of black holes. We added a mean quared error regularization term to train the regressor network, hich measures the average squared difference between the predicted pin values and the true spin values. It is worth noting that we are training the GAN on discrete labels, .e. five discrete values of a \u2217 and seven discrete values of R high . o we ver, our GAN can generate images for any spin value between 1 and 1, given a discrete R high value. This allows us to generate a ore diverse set of black hole images. Overall, our proposed GAN framework as seen in Fig. 3 provides\npowerful tool for generating new images of a black hole based n specific spin and electron distribution parameters. In Section 5 , e will explain in detail how we e v aluate the performance of our roposed methodology.\n.3 P arametrization netw ork\no e v aluate the performance of our generated model, we use a ownstream parametrization task. We use a convolutional neural etwork (CNN) since they are translationally invariant, and the eurons in each layer are locally connected. This guarantees that he position of black holes in the input images is not restricted to the entre and that the spatial characteristics of the black hole image, uch as asymmetry in the emission ring can be ef fecti vely captured. or our project, we use the CNN regressor architecture proposed by Tsui in prep.), since it has been shown to be ef fecti ve in relating patial features in black hole images to parameters such as a and high . The regressor consists of CNN layers that transform the input mage to a vector (z) and a fully connected layer that maps this ector to a single neuron that estimates a \u2217. Fig. 5 provides a visual epresentation of this process. The architecture used can be seen in he Appendix.\nWe train this network on both synthetic images (TSTR), real mages (TRTR), and an augmented data set consisting of real and ak e images (T(S + R)TR). We then compute the R 2 score in each of he three scenarios. A detailed o v erview of the results is presented in ection 5 .\n.4 Model selection\no select the best generator model for our proposed methodology, e utilized the Fr \u0301echet Inception Distance (FID) metric, which is ommonly used to e v aluate the similarity between the distribution f real images and generated images. The lower the FID score, the loser the generated images are to the real images.\nWe plotted the FID score against the number of training epochs to rack the performance of the generator during training. We identified he epoch at which the FID score had reached its minimum value and elected that as the stopping point for training our generator model. he results of computing the FID for the GANs trained with different high values. After selecting the best generator model, we used it in our arametrization network to estimate a \u2217 and R high of the black hole mage. We e v aluated the accuracy of our parametrization network sing three different evaluation methods: Train Real Test Real TRTR), Train Synthetic Test Real (TSTR), and Train Real + ynthetic Test Real (T(S + R)TR).\nRESULTS\n.1 Generated samples\nur proposed CPGAN model can successfully generate black hole mages for any spin between \u22121 and 1 and a given value of the seven high . It is clear from the images that our model is able to generate igh-quality samples that capture the characteristic features of the eal images. These images are presented in Fig. 6 and Fig. 7 .\nThe generator successfully captures the photon ring and black ole shadow across different spin and R high values. We observed hat increasing the value of R high leads to a reduction in the central rightness depression. This effect can be attributed to changes in the lectron temperature distribution within the disc and jet. Specifically, ncreasing R high decreases the temperature in the disc regions ( \u03b2 > ) while in the low \u03b2 regions ( \u03b2 < 10 \u22122 ), the temperature is not\nMNRAS 527, 10965\u201310974 (2024)\nM\nc i f r\ns l t a G\no w p\n5\nW u T t\nTable 5. Classification R 2 scores on different train and test data sets, where values closer to one correspond to a better match.\nR high TRTR TSTR T(S + R)TR 1 0.975 0.978 0.985 5 0.926 0.909 0.931 10 0.917 0.935 0.943 20 0.988 0.879 0.915 40 0.976 0.903 0.981 80 0.905 0.971 0.963 160 0.938 0.910 0.950\np t S o t s s a t\nT a h w h w s R s t t b a d m g p p\n\u2212 l g a s\nm h g\n6\nI C t i i a\ns\nD ow nloaded from https://academ ic.oup.com /m nras/article/527/4/10965/7469481 by Indian Institute of Technology Patna user on 10 January 2024\nhanged. Consequently, more emission is generated in the jet, which s positioned in front of the shadow. The emitted radiation, primarily rom the forward jet, blends o v er the flux depression, resulting in a eduction in the brightness depression. Fig. 7 illustrates that higher R high values introduce additional tructure within the inner ring of the generated images. Conversely, o wer R high v alues do not exhibit significant inner ring structure, alhough ne gativ e spins display prominent spirals attributed to plasma ccretion. These observations align with the findings observed in the RMHD data, ensuring consistent and reliable results. Overall, the generated samples indicate that our proposed method-\nlogy is able to generate high-quality images of a black hole for a ide range of spin and R high values, which will enable more accurate arametrization of other black hole images.\n.2 Classification\ne then e v aluated the performance of our proposed methodology sing the TSTR, TRTR, and augmented T(S + R)TR R 2 scores. he R-squared ( R 2 ) score measures the proportion of the variance in he predicted parameter values that can be explained by the actual\nNRAS 527, 10965\u201310974 (2024)\narameter values. To compute the TRTR score, we trained and tested he regressor model using real data from the GRMHD simulations. imilarly, for the TSTR score, we generated synthetic samples using ur proposed methodology and trained the regressor model using this raining set. We then tested the regression model on the real testing et. Finally, to compute the T(S + R)TR, we combined the real and ynthetic data sets and trained our parametrization network on this ugmented data set. We then tested the regression model on the real esting set.\nTable 5 displays the R 2 scores for the TSTR, TRTR, and augmented (S + R)TR experiments, where values closer to one indicate higher ccurac y. We observ e that the R 2 score on the TSTR data set is igher compared to the TRTR data set. More importantly, ho we ver, e found that the R 2 score on the TS + RTR data set was significantly igher compared to both the TSTR and TRTR data sets. Here, it is orth noting that the R 2 score can differ when calculated on different ubsets of the test data. In our study, we consistently found that the 2 scores for TSTR and T(S + R)TR were higher for every test data et compared to TRTR. The results presented in this section represent he highest R 2 scores obtained across all the test sets. We attribute his impro v ement in performance to the data augmentation provided y the GAN, which increases the size of the training data set and llows for more accurate parametrization of black hole images. This emonstrates the ef fecti veness of our proposed data augmentation ethodology using a conditional progressive GAN and its ability to eneralize well to the real data. As shown in Table 5 , the R2 scores erformed well, indicating that our generation network identified atterns in the spin and the images. Furthermore, we trained our generator using only four a \u2217 ( \u22120.94, 0.50, 0.50, 0.94) and seven R high values (1, 5, 10, 20, 40, 80, 160), eaving out one value ( a \u2217 = 0.00) to understand if our model could eneralize well to continuous spin values. In this case, we achieved T(S + R\u2019)TR of 0.984, where R\u2019 indicates the data set without one pin value.\nIn summary, the results of our e v aluation indicate that our proposed ethodology can help in the accurate parametrization of a black\nole image for a wide range of spin and R high values, and is able to eneralize well to real data.\nC O N C L U S I O N S A N D F U T U R E WO R K\nn this study, we present a data augmentation methodology based on onditional Progressi ve Generati ve Adversarial Networks (CPGAN) o generate diverse black hole (BH) images. The methodology takes nto account variations in spin ( a \u2217) and R high prescriptions, resulting n a valuable resource for training deep learning algorithms to ccurately estimate black hole parameters from observational data.\nOur proposed model is capable of generating BH images for any pin value within the range of [ \u22121, 1], given a single R high . To assess\nt n G t a u R\ni t c h t e o t\nb h A m\na t i\nw m i ( o t (\nA\nC s N a t c a U E 0 I S 1 M s w t w \u2013\nD\nT o u /\nR\nA\nB\nB\nB D D D\nE\nE E E E E E\nF F G\nG\nH\nK\nK\nK\nK\nL M M\nM\nM N\nO\nP P\nQ\nR\nR\nS\nS S\nT\nD ow nloaded from https://academ ic.oup.com /m nras/article/527/4/10965/7469481 by Indian Institute of Technology Patna user on 10 January 2024\nhe ef fecti veness of our approach, we employed a convolutional eural network (CNN) to predict the BH spin using both the eneral Relativistic Magnetohydrodynamics (GRMHD) images and he images generated by our CPGAN model. The results demonstrate significant impro v ement in performance when training is performed sing the augmented data set. This impro v ement is indicated by a high\n2 score, highlighting the enhanced accuracy in BH spin prediction. Consequently, we propose that leveraging GANs for black hole mage generation can be a cost-ef fecti ve method to reliably augment raining data sets for other parametrization algorithms. This paper ontributes to the field of image-generation techniques for black oles and highlights the potential of deep learning algorithms for esimating important physical parameters from observational data. By xpanding the training data set with diverse and realistic BH images, ur methodology enables more accurate parametrization and estimaion and can lead to impro v ed insights into the nature of black holes.\nIn our future work, we aim to expand the scope of our research eyond M87 \u2217 and explore the application of our model to other black ole images, including the recently released images of Sagittarius\n\u2217. Additionally, while our primary focus was on SANE accretion odels, we also intend to include MAD models in our investigations. Furthermore, we plan to explore the impact of incorporating\ndditional physical parameters, such as the inclination angle, as input o our model, to enhance the realism and diversity of the resulting mages.\nIn order to impro v e the quality and stability of image generation, e will consider alternative architectures, specifically diffusion odels, which have demonstrated the ability to produce high-quality mages without suffering from training instability and mode collapse Rombach et al. 2022 ). By integrating these advanced networks into ur data set, we anticipate developing more sophisticated generaive models tailored for the task of astronomical data generation Doorenbos et al. 2022 ; Mudur & Finkbeiner 2022 ).\nC K N OW L E D G E M E N T S\nMF is supported by the DFG research grant \u2018Jet physics on horizon cales and beyond\u2019 (Grant No. FR 4069/2-1). YM is supported by the ational Natural Science Foundation of China (Grant No. 12273022) nd Shanghai target program of basic research for international scienists (Grant No. 22JC1410600). The GRMHD simulations and GRRT alculations were performed on LOEWE at the CSC-Frankfurt, Iboga t ITP Frankfurt and Pi2.0 and Siyuan Mark-I at Shanghai Jiao Tong niversity and on MISTRAL at the University of W \u0308urzburg. Razieh mami acknowledges the support from grant numbers 21-atp21077, NSF AST-1816420, and HST-GO-16173.001-A as well as the nstitute for Theory and Computation at the Center for Astrophysics. upport for this w ork w as provided by the NSF through grants AST952099, AST-1935980, AST-1828513, and by the Gordon and Betty oore Foundation through grant GBMF-10423. This work has been upported in part by the Black Hole Initiative at Harv ard Uni versity, hich is funded by grants from the John Templeton Foundation and he Gordon and Betty Moore Foundation to Harvard University. This ork received guidance from AstroAI at the Center for Astrophysics Harvard & Smithsonian.\nATA AVAILABILITY\nhe primary raytraced images presented in this work is available n request to CMF at christian.fr omm@uni-wuer zbur g.de . The code sed for generating these images is available on GitHub at https: /github.com/aryamohan23/EHT-GANs .\nEFERENCES\nrjo vsk y M. Chintala S. Bottou L., 2017, in Proceedings of the 34th International Conference on Machine Learning, Vol. 70. JMLR.org, Sydney, NSW, Australia, p. 214 ardeen J. M. , Carter B., Hawking S. W., 1973, Commun. Math. Phys. , 31, 161 rock A. , Donahue J., Simonyan K., 2018, International Conference on Learning Representations abs/1809.11096 roderick A. E. , Loeb A., 2009, ApJ , 697, 1164 oeleman S. S. et al., 2008, Nature , 455, 78 oeleman S. S. et al., 2012, Science , 338, 355 oorenbos L. , Cavuoti S., Longo G., Brescia M., Sznitman R., M \u0301arquez-Neila\nP., 2022, preprint ( arXiv:2211.05556 ) vent Horizon Telescope Collaboration et al., 2019a, ApJ , 875,\nL1 vent Horizon Telescope Collaboration et al., 2019b, ApJ , 875, L2 vent Horizon Telescope Collaboration et al., 2019c, ApJ , 875, L3 vent Horizon Telescope Collaboration et al., 2019d, ApJ , 875, L4 vent Horizon Telescope Collaboration et al., 2019e, ApJ , 875, L5 vent Horizon Telescope Collaboration et al., 2019f, ApJ , 875, L6 vent Horizon Telescope Collaboration et al., 2022, ApJ , 930,\nL12 alcke H. , Melia F., Agol E., 2000, ApJ , 528, L13 romm C. M. et al., 2022, A&A , 660, A107 oodfellow I. , Pouget-Abadie J., Mirza M., Xu B., Warde-F arle y D., Ozair\nS., Courville A., Bengio Y., 2020, Commun. ACM, 63, 139 ulrajani I. , Ahmed F., Arjo vsk y M., Dumoulin V., Courville A. C., 5769\u2013\n5779, 2017, Adv. Neur. Inf. Proc. Syst., 30 eusel M. , Ramsauer H., Unterthiner T., Nessler B., Hochreiter S., 2017,\nAdvances in Neural Information Processing Systems, 30 arras T. , Aila T., Laine S., Lehtinen J., 2018, International Conference on\nLearning Representations arras T. , Aittala M., Laine S., H \u0308ark \u0308onen E., Hellsten J., Lehtinen J., Aila T.,\n2021, Adv. Neur. Inf. Proc. Syst., 34, 852 arras T. , Laine S., Aila T., 2019, 2019 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR). p. 4396 arras T. , Laine S., Aittala M., Hellsten J., Lehtinen J., Aila T., 2020,\nin Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). p. 8110 uminet J.-P. , 1979, A&A, 75, 228 irza M. , Osindero S., 2014, preprint ( arXiv:1411.1784 ) o \u0301scibrodzka M. , Falcke H., Shiokawa H., 2016, A&A , 586,\nA38 o \u0301scibrodzka M. , Shiokawa H., Gammie C. F., Dolence J. C., 2012, ApJ ,\n752, L1 udur N. , Finkbeiner D. P., 2022, preprint ( arXiv:2211.12444 ) D oble S. C. , Leung P. K., Gammie C. F., Book L. G., 2007, Class. Quantum\nGravity , 24, S259 dena A. , Olah C., Shlens J., 2017, in International conference on machine\nlearning. p. 2642 erez L. , Wang J., 2017, preprint ( arXiv:1712.04621 ) orth O. , Oli v ares H., Mizuno Y., Younsi Z., Rezzolla L., Moscibrodzka M.,\nFalcke H., Kramer M., 2017, Comput. Astrophys. Cosmol. , 4, 1 iu R. , Ricarte A., Narayan R., Wong G. N., Chael A., Palumbo D., 2023,\nMNRAS , 520, 4867 oelofs F. , Fromm C. M., Mizuno Y., Davelaar J., Janssen M., Younsi Z.,\nRezzolla L., Falcke H., 2021, A&A , 650, A56 ombach R. , Blattmann A., Lorenz D., Esser P., Ommer B., 2022, in\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. p. 10684 ajjadi M. S. , Bachem O., Lucic M., Bousquet O., Gelly S., 2018, Adv. Neur. Inf. Proc. Syst., 31 horten C. , Khoshgoftaar T. M., 2019, J. Big Data, 6, 1 zegedy C. , Vanhoucke V., Ioffe S., Shlens J., Wojna Z., 2016, in Proceedings\nof the IEEE conference on computer vision and pattern recognition. p. 2818\nhompson A. , 1999, in ASP Conf. Ser.. p. 11\nMNRAS 527, 10965\u201310974 (2024)\nM\nTsui T. , 2023, Parameterization of M87 \u2217 Blackhole. Manuscript in preparation v v\nY Y\nY\nY Z\nA\nT t s R\nT\nd fro\n. Laue M. , 1921, Die Lorentz-Kontraktion. an der Gucht J. , Davelaar J., Hendriks L., Porth O., Oli v ares H., Mizuno Y.,\nFromm C. M., Falcke H., 2020, A&A ang J. , Kannan A., Batra D., Parikh D., 2017, preprint ( arXiv:1703.01560 ) ounsi Z. , Porth O., Mizuno Y., Fromm C. M., Oli v ares H., 2020, in Asada K.,\nde Gouveia Dal Pino E., Giroletti M., Nagai H., Nemmen R., eds, Proc. IAU Vol. 342, Perseus in Sicily: From Black Hole to Cluster Outskirts. p. 9 preprint( arXiv:1907.09196)\nounsi Z. , Wu K., Fuerst S. V., 2012, A&A , 545, A13\nNRAS 527, 10965\u201310974 (2024)\nTable A1. Architecture used for calculation of TSTR and TRTR.\nOperation layer Number of filters\nInput image \u2013 Conv olution Lay er Convolution 10 Leaky Relu \u2013 Pooling Layer Max Pooling \u2013 Conv olution Lay er Convolution 20 Leaky Relu \u2013 Pooling Layer Max Pooling \u2013 Conv olution Lay er Convolution 50 Leaky Relu \u2013 Pooling Layer Max Pooling \u2013 Conv olution Lay er Convolution 50 (two times) Leaky Relu \u2013 Pooling Layer Max Pooling \u2013 Conv olution Lay er Convolution 50 (two times) Leaky Relu \u2013 Pooling Layer Avg Pooling \u2013 FC Layer Fully Connected \u2013\nhis paper has been typeset from a T E X/L A T E X file prepared by the author.\nPublished by Oxford University Press on behalf of Royal Astronomical Society. This is an ( https://cr eativecommons.or g/licenses/by/4.0/), which permits unrestricted reus\ninternational conference on computer vision. p. 2223\nPPENDI X A :\nhe architecture seen in Table A1 used by (Tsui in prep.) was used o e v aluate our model as it has been shown to be ef fecti ve in relating patial features in black hole images to parameters such as a \u2217 and high .\nuan F. , Narayan R., 2014, ARA&A , 52, 529 hu J.-Y. , Park T., Isola P., Efros A. A., 2017, in Proceedings of the IEEE\nSize of Stride Padding Size of each filter value value output image\n\u2013 \u2013 \u2013 160 \u00d7 160 \u00d7 2 3 \u00d7 3 1 \u00d7 1 1 \u00d7 1 160 \u00d7 160 \u00d7 10\n\u2013 \u2013 \u2013 160 \u00d7 160 \u00d7 10 \u2013 \u2013 \u2013 80 \u00d7 80 \u00d7 10\n3 \u00d7 3 1 \u00d7 1 1 \u00d7 1 80 \u00d7 80 \u00d7 20 \u2013 \u2013 \u2013 80 \u00d7 80 \u00d7 20 \u2013 \u2013 \u2013 40 \u00d7 40 \u00d7 20 3 \u00d7 3 1 \u00d7 1 1 \u00d7 1 40 \u00d7 40 \u00d7 50 \u2013 \u2013 \u2013 40 \u00d7 40 \u00d7 50 3 \u00d7 3 1 \u00d7 1 1 \u00d7 1 20 \u00d7 20 \u00d7 50 3 \u00d7 3 1 \u00d7 1 1 \u00d7 1 20 \u00d7 20 \u00d7 50\n\u2013 \u2013 \u2013 20 \u00d7 20 \u00d7 50 3 \u00d7 3 1 \u00d7 1 1 \u00d7 1 10 \u00d7 10 \u00d7 50 3 \u00d7 3 1 \u00d7 1 1 \u00d7 1 10 \u00d7 10 \u00d7 50\n\u2013 \u2013 \u2013 10 \u00d7 10 \u00d7 50 10 \u00d7 10 1 \u00d7 1 1 \u00d7 1 1 \u00d7 1 \u00d7 50\n\u2013 \u2013 \u2013 1\n\u00a9 2023 The Author(s). Open Access article distributed under the terms of the Creative Commons Attribution License e, distribution, and reproduction in any medium, provided the original work is properly cited.\nD ow nloade\nm https://academ ic.oup.com /m nras/article/527/4/10965/7469481 by Indian Institute of Technology Patna user on 10 January 2024"
        }
    ],
    "title": "Generating images of the M87* black hole using GANs",
    "year": 2024
}