{
    "abstractText": "Time series analysis is widely used in extensive areas. Recently, to reduce labeling expenses and benefit various tasks, self-supervised pre-training has attracted immense interest. One mainstream paradigm is masked modeling, which successfully pre-trains deep models by learning to reconstruct the masked content based on the unmasked part. However, since the semantic information of time series is mainly contained in temporal variations, the standard way of randomly masking a portion of time points will seriously ruin vital temporal variations of time series, making the reconstruction task too difficult to guide representation learning. We thus present SimMTM, a Simple pre-training framework for Masked Time-series Modeling. By relating masked modeling to manifold learning, SimMTM proposes to recover masked time points by the weighted aggregation of multiple neighbors outside the manifold, which eases the reconstruction task by assembling ruined but complementary temporal variations from multiple masked series. SimMTM further learns to uncover the local structure of the manifold, which is helpful for masked modeling. Experimentally, SimMTM achieves state-of-the-art fine-tuning performance compared to the most advanced time series pre-training methods in two canonical time series analysis tasks: forecasting and classification, covering both inand cross-domain settings. Code is available at https://github.com/thuml/SimMTM.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jiaxiang Dong"
        },
        {
            "affiliations": [],
            "name": "Haixu Wu"
        },
        {
            "affiliations": [],
            "name": "Haoran Zhang"
        },
        {
            "affiliations": [],
            "name": "Li Zhang"
        },
        {
            "affiliations": [],
            "name": "Jianmin Wang"
        },
        {
            "affiliations": [],
            "name": "Mingsheng LongB"
        }
    ],
    "id": "SP:05a90c4d046b103bc50cfba7175e5ee9a51f93bf",
    "references": [
        {
            "authors": [
                "Ralph G Andrzejak",
                "Klaus Lehnertz",
                "Florian Mormann",
                "Christoph Rieke",
                "Peter David",
                "Christian E Elger"
            ],
            "title": "Indications of nonlinear deterministic and finite-dimensional structures in time series of brain electrical activity: Dependence on recording region and brain state",
            "venue": "Physical Review E,",
            "year": 2001
        },
        {
            "authors": [
                "Alexei Baevski",
                "Wei-Ning Hsu",
                "Qiantong Xu",
                "Arun Babu",
                "Jiatao Gu",
                "Michael Auli"
            ],
            "title": "Data2vec: A general framework for self-supervised learning in speech, vision and language",
            "year": 2022
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Mathilde Caron",
                "Ishan Misra",
                "Julien Mairal",
                "Priya Goyal",
                "Piotr Bojanowski",
                "Armand Joulin"
            ],
            "title": "Unsupervised learning of visual features by contrasting cluster assignments",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton"
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "In ICML,",
            "year": 2020
        },
        {
            "authors": [
                "Mingyue Cheng",
                "Qi Liu",
                "Zhiding Liu",
                "Hao Zhang",
                "Rujiao Zhang",
                "Enhong Chen"
            ],
            "title": "Timemae: Selfsupervised representations of time series with decoupled masked autoencoders",
            "venue": "T-KDE,",
            "year": 2023
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "venue": "In NAACL,",
            "year": 2018
        },
        {
            "authors": [
                "Emadeldeen Eldele",
                "Mohamed Ragab",
                "Zhenghua Chen",
                "Min Wu",
                "Chee Keong Kwoh",
                "Xiaoli Li",
                "Cuntai Guan"
            ],
            "title": "Time-series representation learning via temporal and contextual contrasting",
            "venue": "In IJCAI,",
            "year": 2021
        },
        {
            "authors": [
                "Jean-Yves Franceschi",
                "Aymeric Dieuleveut",
                "Martin Jaggi"
            ],
            "title": "Unsupervised scalable representation learning for multivariate time series",
            "venue": "In NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "Tianyu Gao",
                "Adam Fisch",
                "Danqi Chen"
            ],
            "title": "Making pre-trained language models better few-shot learners",
            "venue": "In IJCNLP,",
            "year": 2020
        },
        {
            "authors": [
                "Kaiming He",
                "Xinlei Chen",
                "Saining Xie",
                "Yanghao Li",
                "Piotr Doll\u00e1r",
                "Ross Girshick"
            ],
            "title": "Masked autoencoders are scalable vision learners",
            "year": 2022
        },
        {
            "authors": [
                "Kaiming He",
                "Haoqi Fan",
                "Yuxin Wu",
                "Saining Xie",
                "Ross Girshick"
            ],
            "title": "Momentum contrast for unsupervised visual representation learning",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Kaiming He",
                "X. Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "year": 2016
        },
        {
            "authors": [
                "Ashish Jaiswal",
                "Ashwin Ramesh Babu",
                "Mohammad Zaki Zadeh",
                "Debapriya Banerjee",
                "Fillia Makedon"
            ],
            "title": "A survey on contrastive self-supervised learning",
            "year": 2020
        },
        {
            "authors": [
                "Junguang Jiang",
                "Yang Shu",
                "Jianmin Wang",
                "Mingsheng Long"
            ],
            "title": "Transferability in deep learning: A survey",
            "venue": "arXiv preprint arXiv:2201.05867,",
            "year": 2022
        },
        {
            "authors": [
                "Bob Kemp",
                "Aeilko H Zwinderman",
                "Bert Tuk",
                "Hilbert AC Kamphuisen",
                "Josefien JL"
            ],
            "title": "Oberye. Analysis of a sleep-dependent neuronal feedback loop: the slow-wave microcontinuity of the eeg",
            "venue": "TBME,",
            "year": 2000
        },
        {
            "authors": [
                "Alex Kendall",
                "Yarin Gal",
                "Roberto Cipolla"
            ],
            "title": "Multi-task learning using uncertainty to weigh losses for scene geometry and semantics",
            "year": 2018
        },
        {
            "authors": [
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Honglak Lee",
                "Geoffrey E. Hinton"
            ],
            "title": "Similarity of neural network representations revisited",
            "venue": "In ICML,",
            "year": 2019
        },
        {
            "authors": [
                "Christian Lessmeier",
                "James Kuria Kimotho",
                "Detmar Zimmer",
                "Walter Sextro"
            ],
            "title": "Condition monitoring of bearing damage in electromechanical drive systems by using motor current signals of electric motors: A benchmark data set for data-driven classification",
            "venue": "In PHM,",
            "year": 2016
        },
        {
            "authors": [
                "Yanghao Li",
                "Haoqi Fan",
                "Ronghang Hu",
                "Christoph Feichtenhofer",
                "Kaiming He"
            ],
            "title": "Scaling language-image pre-training via masking",
            "venue": "In CVPR,",
            "year": 2023
        },
        {
            "authors": [
                "Zhe Li",
                "Zhongwen Rao",
                "Lujia Pan",
                "Pengyun Wang",
                "Zenglin Xu"
            ],
            "title": "Ti-mae: Self-supervised masked time series autoencoders",
            "venue": "arXiv preprint arXiv:2301.08871,",
            "year": 2023
        },
        {
            "authors": [
                "Jiayang Liu",
                "Lin Zhong",
                "Jehan Wickramasuriya",
                "Venu Vasudevan"
            ],
            "title": "uwave: Accelerometer-based personalized gesture recognition and its applications",
            "venue": "In PMC,",
            "year": 2009
        },
        {
            "authors": [
                "Xiao Liu",
                "Fanjin Zhang",
                "Zhenyu Hou",
                "Li Mian",
                "Zhaoyu Wang",
                "Jing Zhang",
                "Jie Tang"
            ],
            "title": "Self-supervised learning: Generative or contrastive",
            "year": 2021
        },
        {
            "authors": [
                "Yong Liu",
                "Haixu Wu",
                "Jianmin Wang",
                "Mingsheng Long"
            ],
            "title": "Non-stationary transformers: Exploring the stationarity in time series forecasting",
            "venue": "In NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Abdullah Al Mueen",
                "Eamonn J. Keogh"
            ],
            "title": "Extracting optimal performance from dynamic time warping",
            "year": 2016
        },
        {
            "authors": [
                "Yuqi Nie",
                "Nam H Nguyen",
                "Phanwadee Sinthong",
                "Jayant Kalagnanam"
            ],
            "title": "A time series is worth 64 words: Long-term forecasting with transformers",
            "venue": "In ICLR,",
            "year": 2023
        },
        {
            "authors": [
                "Manuel T Nonnenmacher",
                "Lukas Oldenburg",
                "Ingo Steinwart",
                "David Reeb"
            ],
            "title": "Utilizing expert features for contrastive learning of time-series representations",
            "year": 2022
        },
        {
            "authors": [
                "Adam Paszke",
                "S. Gross",
                "Francisco Massa",
                "A. Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Z. Lin",
                "N. Gimelshein",
                "L. Antiga",
                "Alban Desmaison",
                "Andreas K\u00f6pf",
                "Edward Yang",
                "Zach DeVito",
                "Martin Raison",
                "Alykhan Tejani",
                "Sasank Chilamkurthy",
                "Benoit Steiner",
                "Lu Fang",
                "Junjie Bai",
                "Soumith Chintala"
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "venue": "NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "year": 2020
        },
        {
            "authors": [
                "Quentin Rebjock",
                "Baris Kurt",
                "Tim Januschowski",
                "Laurent Callot"
            ],
            "title": "Online false discovery rate control for anomaly detection in time series",
            "venue": "NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Pritam Sarkar",
                "Ali Etemad"
            ],
            "title": "Self-supervised learning for ecg-based emotion recognition",
            "venue": "In ICASSP,",
            "year": 2020
        },
        {
            "authors": [
                "Florian Schroff",
                "Dmitry Kalenichenko",
                "James Philbin"
            ],
            "title": "Facenet: A unified embedding for face recognition and clustering",
            "venue": "In CVPR,",
            "year": 2015
        },
        {
            "authors": [
                "Fan-Keng Sun",
                "Chris Lang",
                "Duane Boning"
            ],
            "title": "Adjusting for autocorrelated errors in neural networks for time series",
            "venue": "In NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Chi Ian Tang",
                "Ignacio Perez-Pozuelo",
                "Dimitris Spathis",
                "Cecilia Mascolo"
            ],
            "title": "Exploring contrastive learning in human activity recognition for healthcare",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "NeurIPS,",
            "year": 2017
        },
        {
            "authors": [
                "Pascal Vincent",
                "H. Larochelle",
                "Isabelle Lajoie",
                "Yoshua Bengio",
                "Pierre-Antoine Manzagol"
            ],
            "title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion",
            "year": 2010
        },
        {
            "authors": [
                "Tongzhou Wang",
                "Phillip Isola"
            ],
            "title": "Understanding contrastive representation learning through alignment and uniformity on the hypersphere",
            "venue": "In ICML,",
            "year": 2020
        },
        {
            "authors": [
                "Zhiyuan Wang",
                "Xovee Xu",
                "Weifeng Zhang",
                "Goce Trajcevski",
                "Ting Zhong",
                "Fan Zhou"
            ],
            "title": "Learning latent seasonal-trend representations for time series forecasting",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Alexander Wettig",
                "Tianyu Gao",
                "Zexuan Zhong",
                "Danqi Chen"
            ],
            "title": "Should you mask 15% in masked language modeling",
            "venue": "In EACL,",
            "year": 2023
        },
        {
            "authors": [
                "Kristoffer Wickstr\u00f8m",
                "Michael Kampffmeyer",
                "Karl \u00d8yvind Mikalsen",
                "Robert Jenssen"
            ],
            "title": "Mixing up contrastive learning: Self-supervised representation learning for time series",
            "year": 2022
        },
        {
            "authors": [
                "Gerald Woo",
                "Chenghao Liu",
                "Doyen Sahoo",
                "Akshat Kumar",
                "Steven Hoi"
            ],
            "title": "Cost: Contrastive learning of disentangled seasonal-trend representations for time series forecasting",
            "venue": "In ICLR,",
            "year": 2022
        },
        {
            "authors": [
                "Haixu Wu",
                "Jiehui Xu",
                "Jianmin Wang",
                "Mingsheng Long"
            ],
            "title": "Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting",
            "venue": "In NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Zhirong Wu",
                "Yuanjun Xiong",
                "Stella X. Yu",
                "Dahua Lin"
            ],
            "title": "Unsupervised feature learning via nonparametric instance discrimination",
            "year": 2018
        },
        {
            "authors": [
                "Zhenda Xie",
                "Zigang Geng",
                "Jingcheng Hu",
                "Zheng Zhang",
                "Han Hu",
                "Yue Cao"
            ],
            "title": "Revealing the dark secrets of masked image modeling",
            "year": 2023
        },
        {
            "authors": [
                "Zhenda Xie",
                "Zheng Zhang",
                "Yue Cao",
                "Yutong Lin",
                "Jianmin Bao",
                "Zhuliang Yao",
                "Qi Dai",
                "Han Hu"
            ],
            "title": "Simmim: A simple framework for masked image modeling",
            "year": 2022
        },
        {
            "authors": [
                "Jiehui Xu",
                "Haixu Wu",
                "Jianmin Wang",
                "Mingsheng Long"
            ],
            "title": "Anomaly transformer: Time series anomaly detection with association discrepancy",
            "venue": "In ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "Ling Yang",
                "Shenda Hong"
            ],
            "title": "Unsupervised time-series representation learning with iterative bilinear temporal-spectral fusion",
            "venue": "In ICML,",
            "year": 2022
        },
        {
            "authors": [
                "Xinyu Yang",
                "Zhenguo Zhang",
                "Rongyi Cui"
            ],
            "title": "Timeclr: A self-supervised contrastive learning framework for univariate time series representation",
            "year": 2022
        },
        {
            "authors": [
                "Hugo Y\u00e8che",
                "Gideon Dresdner",
                "Francesco Locatello",
                "Matthias H\u00fcser",
                "Gunnar R\u00e4tsch"
            ],
            "title": "Neighborhood contrastive learning applied to online patient monitoring",
            "venue": "In ICML,",
            "year": 2021
        },
        {
            "authors": [
                "Zhihan Yue",
                "Yujing Wang",
                "Juanyong Duan",
                "Tianmeng Yang",
                "Congrui Huang",
                "Yunhai Tong",
                "Bixiong Xu"
            ],
            "title": "TS2Vec: Towards Universal Representation of Time Series",
            "venue": "In AAAI,",
            "year": 2022
        },
        {
            "authors": [
                "George Zerveas",
                "Srideepika Jayaraman",
                "Dhaval Patel",
                "Anuradha Bhamidipaty",
                "Carsten Eickhoff"
            ],
            "title": "A transformer-based framework for multivariate time series representation learning",
            "venue": "In SIGKDD,",
            "year": 2021
        },
        {
            "authors": [
                "Xiang Zhang",
                "Ziyuan Zhao",
                "Theodoros Tsiligkaridis",
                "Marinka Zitnik"
            ],
            "title": "Self-supervised contrastive pre-training for time series via time-frequency consistency",
            "venue": "In NeurIPS,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "1 Introduction\nTime series analysis has attached immense importance in extensive real-world applications, such as financial analysis, energy planning and etc [47, 51]. Vast amounts of time series are incrementally collected from IoT and wearable devices. However, the semantic information of time series is mainly buried in human-indiscernible temporal variations, making it difficult to annotate. Recently, self-supervised pre-training has been widely explored [23, 15], which benefits deep models from pretext knowledge learned over large-scale unlabeled data and further promotes the performance of various downstream tasks. Mainly, as a well-recognized pre-training paradigm, masked modeling has achieved great success in many areas, such as masked language modeling (MLM) [7, 31, 32, 3, 10] and masked image modeling (MIM) [11, 50, 20]. This paper extends pre-training methods to time series, especially masked time-series modeling (MTM).\nThe canonical technique of masked modeling is to optimize the model by learning to reconstruct the masked content based on the unmasked part [7]. However, unlike images and natural languages whose patches or words contain much even redundant semantic information, we observe that the valuable semantic information of time series is mainly included in the temporal variations, such as the trend, periodicity, and peak valley, which can correspond to unique weather processes, abnormal faults, etc. in the real world. Therefore, directly masking a portion of time points will seriously ruin\n\u2217Equal Contribution\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\nar X\niv :2\n30 2.\n00 86\n1v 4\n[ cs\n.L G\n] 2\n3 O\nct 2\n(a) Canonical Masked Modeling (b) SimMTM\nManifold\nOriginal Series\nMasked Series\nDirect Reconstruction\nManifold\nOriginal Series\nMultiple Masked Series\nNeighborhood Aggregation\n0 20 40 60 80\n-0.8\n-1.0\n-1.2\n-1.4\n-1.6\n-1.8\nOriginal Time Series Reconstructed Time Series\nTime\nValue\n0 20 40 60 80\n-0.8\n-1.0\n-1.2\n-1.4\n-1.6\n-1.8\nOriginal Time Series Reconstructed Time Series\nTime\nValue\nthe temporal variations of the original time series, which makes the reconstruction task too difficult to guide representation learning of time series. (Figure 1).\nAccording to the analysis in stacked denoising autoencoders [40], as shown in Figure 1, we can view the andomly masked series as the \u201cn ighbor\u201d of the original time series outside the manifold and the reconstruction process is to project the masked series back to the manifold of original series. However, as we analyzed above, direct reconstruction may fail since the essential temporal variations are ruined by random masking. Inspired by the manifold perspective, we go beyond the straightforward reconstruction convention of masked modeling and propose a natural idea of reconstructing the original data from its multiple \u201cneighbors\u201d, i.e. multiple masked series. Although the temporal variations of the original time series have been partially dropped in each randomly masked series, the multiple randomly masked series will complement each other, making the reconstruction process much more accessible than directly reconstructing the original series from a single masked series. This process will also pre-train the model to uncover the local structure of the time series manifold implicitly, thereby benefiting masked modeling and representation learning [35, 41].\nBased on the ab ve insights, we propose the SimMTM as a simple but effective pre-training framework for time series in this paper. Instead of directly reconstructing the masked time points from unmasked parts, SimMTM recovers the original time series from multiple randomly masked time series. Technically, SimMTM presents a neighborhood aggregation design for reconstruction, which is to aggregate the point-wise representations of time series based on similarities learned in the series-wise representation space. In addition to the reconstruction loss, a constraint loss is presented to guide the series-wise representation learning based on the neighborhood assumption of the time series manifold. Empowering by above designs, SimMTM achieves consistent state-of-the-art in various time series analysis tasks when fine-tuning the pre-trained model into downstream tasks, covering both the low-level forecasting and high-level classification tasks, even if there is a clear domain shift between pre-training and fine-tuning datasets. Overall, our contributions are summarized as follows:\n\u2022 Inspired by the manifold perspective of masking, we propose a new task for masked timeseries modeling, which is to reconstruct the original time series on the manifold based on multiple masked series outside the manifold.\n\u2022 Technically, we present SimMTM as a simple but effective pre-training framework, which aggregates point-wise representations for reconstruction based on the similarities learned in series-wise representation space.\n\u2022 SimMTM consistently achieves the state-of-the-art fine-tuning performance in typical time series analysis tasks, including low-level forecasting and high-level classification, covering both in- and cross-domain settings.\n2 Related Work\n2.1 Self-supervised Pre-training\nSelf-supervised pre-training is an important research topic for learning generalizable and shared knowledge from large-scale data and further benefiting downstream tasks [15]. Originally, this topic has been widely explored in computer vision and natural language processing. Elaborative manuallydesigned self-supervised tasks are presented, which can be roughly categorized into contrastive learning [12, 5, 4] and masked modeling [7, 11]. Recently, following the well-established contrastive learning and masked modeling paradigms, some self-supervised pre-training methods for time series have been proposed [9, 27, 34, 33, 36, 54, 52, 6].\nContrastive learning. The critical insight of contrastive learning is to optimize the representation space based on the manually designed positive and negative pairs. where representations of positive pairs are optimized to be close to each other. In contrast, negative ones tend to be far apart [48, 14]. The canonical design presented in SimCLR [37] views different augmentations of the same sample as positive pairs and augmentations among different samples as negative pairs.\nRecently, in time series pre-training, many designs of positive and negative pairs have been proposed by utilizing the invariant properties of time series. Concretely, to make the representation learning seamlessly related to temporal variations, TimCLR [53] adopts the DTW [25] to generate phase-shift and amplitude-change augmentations, which is more suitable for time series context. TS2Vec [55] splits multiple time series into several patches and further defines the contrastive loss in both instancewise and patch-wise aspects. TS-TCC [8] presents a new temporal contrastive learning task as making the augmentations predict each other\u2019s future. Mixing-up [45] exploits a data augmentation scheme in which new samples are generated by mixing two data samples. LaST [42] aims to disentangle the seasonal-trend representations in the latent space based on variational inference. Afterward, CoST [46] employs contrastive losses in both time and frequency domain to learn discriminative seasonal and trend representations. Besides, TF-C [57] proposes a novel time-frequency consistency architecture and optimizes time-based and frequency-based representations of the same example to be close to each other. Note that contrastive learning mainly focuses on the high-level information [49], and the series-wise or patch-wise representations inherently mismatch the low-level tasks, such as time series forecasting. Thus, in this paper, we focus on the masked modeling paradigm.\nMasked modeling. The masked modeling paradigm optimizes the model by learning to reconstruct the masked content from the unmasked part. This paradigm has been widely explored in computer vision and natural language processing, which is to predict the masked words of a sentence [7] and masked patches of an image [2, 11, 50] respectively. As for the time series analysis, TST [56] follows the canonical masked modeling paradigm and learns to predict removed time points based on the remaining time points. Next, PatchTST [26] proposes to predict masked subseries-level patches to capture the local semantic information and reduce memory usage. Ti-MAE [21] uses mask modeling as an auxiliary task to boost the forecasting and classification performances of advanced Transformer-based methods. However, as we stated before, directly masking time series will ruin the essential temporal variations, making the reconstruction too difficult to guide the representation learning. Unlike the direct reconstruction in previous works, SimMTM presents a new masked modeling task, which is reconstructing the original time series from multiple randomly masked series.\n2.2 Understanding Masked Modeling\nMasked modeling has been explored in stacked denoising autoencoders [40], where masking is viewed as adding noise to the original data and the masked modeling is to project masked data from the neighborhood back to the original manifold, namely denoising. It has recently been widely used in pre-training, which can learn valuable low-level information from data unsupervisedly [49]. Inspired by the manifold perspective, we go beyond the classical denoising process and project the masked data back to the manifold by aggregating multiple masked time series within the neighborhood.\n3 SimMTM\nAs aforementioned, to tackle the problem that randomly masking time series will ruin the essential temporal variation information, SimMTM proposes to reconstruct the original time series from multiple masked time series. To implement this, SimMTM first learns similarities among multiple time series in the series-wise representation space and then aggregates the point-wise representations of these time series based on pre-learned series-wise similarities. Next, we will detail the techniques in both model architecture and pre-training protocol aspects.\n3.1 Overall Architecture\nThe reconstruction process of SimMTM involves the following four modules: masking, representation learning, series-wise similarity learning and point-wise reconstruction.\n055 056 057 058 059 060 061 062 063 064 065 066 067 068 069 070 071 072 073 074 075 076 077 078 079 080 081 082 083 084 085 086 087 088 089 090 091 092 093 094 095 096 097 098 099 100 101 102 103 104 105 106 107 108 109 SimMTM: A Simple Pre-Training Framework for Masked Time-Series Modeling aggregate multiple series based on the series-wise similarity. Benefiting from the above designs, SimMTM can successively unify the series-wise and point-wise properties of time series into representation learning. Experimentally, SimMTM achieves consistent state-of-the-art in various time series analysis tasks when finetuning the pre-trained model into downstream tasks, even if there is a clear domain gap between the pre-training and finetuning datasets. Our contributions are summarized as follows: \u2022 To unify the series-wise and point-wise properties of time series, SimMTM presents a new mask modeling task, that is reconstructing the original time series from multiple randomly masked augmentations. \u2022 Based on the basic operation of masking modeling and data augmentation views of temporal masking, we implement SimMTM with series-wise contrastive and point-wise reconstruction modules. \u2022 SimMTM achieves consistent state-of-the-art finetuning performance in various tasks, including time series forecasting, classification and imputation, covering both in-domain and cross-domain settings. 2. Related Work 2.1. Self-supervised Pre-training for time series Self-supervised pre-training has obtained breakthrough progress in natural language processing (NLP) (Brown et al., 2020; Devlin et al., 2018; Gao et al., 2020; Radford et al., 2019) and computer vision (CV) (He et al., 2022; Li et al., 2022; Radford et al., 2021) to alleviate the problem of overreliance on large-scale labeled data in deep learning. The core of pre-training is to learn generalizable and shared knowledge that can transfer to different but related tasks. Although, there are already some self-supervised pre-training methods for time series (Eldele et al., 2021; Shi et al., 2021; Zerveas et al., 2021; Zhang et al., 2022). Unlike CV and NLP, which try to learn general visual elements and latent semantic and grammatical associations through pre-training, time series still has not yet established such pre-training principles and self-supervised signals due to insufficient labeled data, potential distribution shifts, and temporal dynamics (Zhang et al., 2022). 2.2. Contrastive Learning for time series contrastive learning: pulling positive close neighbors and pushing apart neg- ative non-neighbor In comparison to the augmentations of other instances, each instance is more similar to its own augmentation (Chen et al., 2020; He et al., 2020). Contrastive learning aims to learn effective representation by pulling positive close neighbors and pushing apart negative non-neighbors. It assumes a set of positive examples D = (xi, x + i ) M i=1 . We follow the standard contrastive framework and take a cross-entropy objective with in-batch negatives: let hi and h+i denote the representations of xi and x+i , the training objective for xi, x + i with a mini-batch of N pairs is: `i = log esim(hi,h + i )/\u2327 PN j=1 e sim(hi,h + j )/\u2327 . (1) 2.3. Masked Modeling for time series masked modeling: removing a portion of the data and learning to predict the removed content. 3. Method To adapt to various downstream time series analysis tasks, we present a simple time series pre-training framework, naming by SimMTM, which can unify the series-wise and pointwise properties into learned representations. As shown in Figure 1, SimMTM involves three key designs: temporal masking for creating self-supervised tasks, contrastive among series for series-wise properties and reconstruction of original time series for point-wise properties. 3.1. Temporal Masking At the crossroads of contrastive learning and mask modeling, we find that temporal masking of time series will bring two types of self-supervised tasks, which corresponds to serieswise and point-wise properties of time series respectively. Technically, given a batch of N time series samples {xi}Ni=1, where xi 2 RL\u21e5C contains L time points and C observed variates, we can easily generate a set of series for each sample xi by randomly masking along the temporal dimension, which can be formalized as follows: {xji}Mj=1 = Temporal-Maskr(xi), (2) where r 2 [0, 1] denotes the masked ratio. M is a hyperparameter for the number of masked time series. xji 2 RL\u21e5C represents the j-th masked time series of xi, where the values of masked time points are replaced by zeros. By doing this, we can obtain a batch of augmented time series. For clarity, we present the input series in a matrix as follows: X = N[ i=1 \u21e3 {xi} [ {xji}Mj=1 \u2318 . (3) From Eq. (3), we can naturally observe the following two types of self-supervised tasks: Encoder 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 SimMTM: A Simple Pre-Training Framework for Masked Time-Series Modeling\nMasked Oral Time Series\nMasked( ) Masked( )\n\u2026 L Masked( )\ntest haixu wu April 2021 1 Introduction \u23271 \u23272 \u2327k R(\u23271) R(\u23272) R(\u2327k) 1 test haixu wu April 2021 1 Introduction \u23271 \u23272 \u2327k R(\u23271) R(\u23272) R(\u2327k) 1 test haixu wu April 2021 1 Introduction \u23271 \u23272 \u2327k R(\u23271) R(\u23272) R(\u2327k) 1 Oral Time-Series\n\u2026 Projector Pooler Weight MatrixRepresentations Lcontrastive Lrebuild Similarity Matrix + SoftMax Encoder Encoder Encoder Pooler Projection Head Similarity Matrix Z00 Zt0 Z00 Zt+k0 Lcontrastive x Z0n Ztn Zt+kn SoftMax & Weight Matrix C0 Cn Masked T0 Tn Lreconstraction Oral Time Series Masked Time Series Encoder \u2026 Pooler Lcontrastive Time Va lu e Original Time Series \u2026 Masked Time Series Encoder Pooler Time Va lu e Original Time Series \u2026 Masked Time Series (a) Augmentation via Masking (b) Representation Learning Temporal Pooler Lcontrastive (c) Series-wise Contrastive (d) Point-wise Reconstruction Lreconstraction \u2026 \u2026 \u2026 X Original Masked Time Va lu e Original Time Series \u2026 Masked Time Ser es (a) Temporal Masking (b) Representation Learning Lcontrastive (c) Series-wise Contrastive (d) Point-wise Rescontruction \u2026 \u2026 \u2026 X Original Masked reconstruction Lreconstractionreconstruction Aggregate Other Time Series Masked to Masked Masked to Original Original to Masked Original to OriginalEncoder Tem poral Pooler Projector Similarity Matrix Figure 1. Overall Architecture of SimMTM, which can reconstruct the original time series based on the aggregation of masked time series. \u2022 Relation between each pair of elements: If we view the temporal masking as data augmentation, for the original time series xi, its own maskings {xji}Mj=1 are positive samples since they still share the same series properties (e.g. trend, periodicity) even though the latter is masked, while the masked series from other series are negative samples w.r.t xi. Thus, we can obtain a series-wise contrastive task by temporal masking.\n\u2022 Relation between original series and its masked series: Following the well-acknowledged mask modeling paradigm, the temporal masking can also be seen as missing values, where the task is to reconstruct the masked parts based on the reserved time points, namely the point-wise reconstruction task.\nInstead of directly combining these two types of tasks, we present a special mask modeling task to unify the serieswise and point-wise properties of time series and make them collaborate with each other.\n3.2. Series-wise Contrastive\nGiven the input X after temporal masking, we feed it into the encoder and obtain the deep representations:\nZ = N[\ni=1\n\u21e3 {zi} [ {zji}Mj=1 \u2318 = Enocder(X ) (4)\nwhere zi, z j i 2 RL\u21e5dmodel . Further, as shown in Figure 1, to obtain series-wise representations, we employ a temporal pooler to summarize the temporal information and obtain the series-wise representations:\nS = N[\ni=1\n\u21e3 {si} [ {sji}Mj=1 \u2318 = Temporal-Pool(Z), (5)\nwhere si, s j i 2 R1\u21e5dmodels denotes series-wise representations for original time series xi and its mask-augmentation xji respectively. In specific, S contains (M + 1) \u21e5 N\nrepresentations. Thus, the similarity matrix of S is in R((M+1)\u21e5N)\u21e5((M+1)\u21e5N), which includes four types of similarities: original series to original series, original series to masked series, masked series to original series and masked series to masked series, where the pair of positive samples only exists in the latter three types of relations.\nBy analyzing the relation between each pair of elements in S , we can derive the series-wise contrastive loss as follows:\nLcontrastive = NX\ni=1\n\u21e3 MX\nj=1\nlog eSim(si,s\nj i )/\u2327\nP s02S\\{si} eSim(si,s0)/\u2327\n\u2318\nNX\ni=1\nMX\nj=1\nlog\neSim(s j i ,si)/\u2327\nP s02S\\{sji } \u21e3 eSim(s j i ,s 0)/\u2327 \u2318\n+ X\n1kM,k 6=j log\neSim(s j i ,s k i )/\u2327\nP s02S\\{sji } \u21e3 eSim(s j i ,s\n0)/\u2327 \u2318 ! ,\n(6)\nwhere Sim(x,y) = xyT and \u2327 is the temperature hyperparameter. By optimizing with Lcontrastive, the model can extract valuable series-wise properties to representations.\n3.3. Point-wise Reconstruction\nInstead of directly generating missing values for masked data, we attempt to reconstruct the original time series {xi}Ni=1 by weighted aggregating the point-wise representations Z of other series, which is based on the similarity matrix calculated from series-wise representations S . Since the similarity matrix of S has already been calculated in Eq. (6). As shown in Figure 1, we can present the reconstruction of i-th original time series as follows:\nz\u0302i = X\ns02S\\{si}\neSim(si,s 0)/\u2327\nP s002S\\{si} eSim(si,s0 0)/\u2327 z0, (7)\nwhere z0 denote the corresponding point-wise representation of s0 and z\u0302i 2 RL\u21e5dmodel are the reconstructed point-wise\nProj ctor 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 SimMTM: A Simple Pre-Training Framework for Masked Time-Series Modeling\nMasked Oral Time Series\nMasked( ) Masked( )\n\u2026 L Masked( )\ntest haixu wu April 2021 1 Introduction \u23271 \u23272 \u2327k R(\u23271) R(\u23272) R(\u2327k) 1 test haixu wu April 2021 1 Introduction \u23271 \u23272 \u2327k R(\u23271) R(\u23272) R(\u2327k) 1 test haixu wu April 2021 1 Introduction \u23271 \u23272 \u2327k R(\u23271) R(\u23272) R(\u2327k) 1 Oral Time-Series\n\u2026 Projector Pooler Weight MatrixRepresentations Lcontrastive Lrebuild Similarity Matrix + SoftMax Encoder Encoder Encoder Pooler Projection Head Similarity Matrix Z00 Zt0 Z00 Zt+k0 Lcontrastive x Z0n Ztn Zt+kn SoftMax & Weight Matrix C0 Cn Masked T0 Tn Lreconstraction Oral Time Series Masked Time Series Encoder \u2026 Pooler Lcontrastive Time Va lu e Original Time Series \u2026 Masked Time Series Encoder Pooler Time Va lu e Original Time Series \u2026 Masked Time Series (a) Augmentation via Masking (b) Representation Learning Temporal Pooler Lcontrastive (c) Series-wise Contrastive (d) Point-wise Reconstruction Lreconstraction \u2026 \u2026 \u2026 X Original Masked Time Va lu e Original Time Series \u2026 Masked Time Series (a) Temporal Masking (b) Representation Learning Lcontrastive (c) Series-wise Contrastive (d) Point-wise Rescontruction \u2026 \u2026 \u2026 X Original Masked reconstruction Lreconstractionreconstruction Aggregate Other Time Series Masked to Masked Masked to Original Original to Masked Original to OriginalEncoder Tem poral Pooler Projector Similarity Matrix Figure 1. Overall Architecture of SimMTM, which can reconstruct the original time series based on the aggregation of masked time series. \u2022 Relation between each pair of elements: If we view the temporal masking as data augmentation, for the original time series xi, its own maskings {xji}Mj=1 are positive samples since they still share the same series properties (e.g. trend, periodicity) even though the latter is masked, while the masked series from other series are negative samples w.r.t xi. Thus, we can obtain a series-wise contrastive task by temporal maski g. \u2022 Relation between original series and its masked series: Following the well-acknowledged mask modeling paradigm, the temporal masking can also be seen as missing values, where the task is to reconstruct the masked parts based on the reserved time points, namely the point-wise reco struction task. Instead of directly combining these two types of tasks, we present a special as modeling task to unify the serieswise and point-wise properties of time series and make them collaborate with each other.\n.2. Series-wise Contrastive\nGiven the input X af er te poral asking, we feed it into the encoder and obtain the deep representations:\nZ = N[\ni=1\n\u21e3 {zi} [ {zji}Mj=1 \u2318 = Enocder(X ) (4)\nwhere zi, z j i 2 RL\u21e5dmodel . Further, as shown in Figure 1, to obtain series-wise representations, we employ a temporal pooler to summarize the temporal information and obtain the series-wise representations:\nS = N[\ni=1\n\u21e3 {si} [ {sji}Mj=1 \u2318 = Temporal-Pool(Z), (5)\nwhere si, s j i 2 R1\u21e5dmodels denotes s ries-wi representations for original time series xi a d its mask-augmentation xji respectively. In specific, S contains (M + 1) \u21e5 N\nrepresentations. Thus, the similarity matrix of S is in R((M+1)\u21e5N)\u21e5((M+1)\u21e5N), which includes four types of similarities: original series to original series, original series to masked series, masked series to original series and masked series to masked series, where the pair of positive samples only exists in the latter three types of relations. By analyzing the relation between each pair of elements in S , we can derive the series-wise contrastive loss as follows: Lcontrastive = NX i=1 \u21e3 MX j=1 l g eSim(si,s j i )/\u2327 P s02S\\{si} eSim(si,s0)/\u2327 \u2318 NX i=1 MX j=1 log eSim(s j i , i)/\u2327 P s02S\\{sji } \u21e3 eSim(s j i ,s 0)/\u2327 \u2318 + X 1kM,k 6=j log eSim(s j i ,s k i )/\u2327 P s02S\\{sji } \u21e3 Sim(sji ,s 0)/\u2327 \u2318 ! , (6)\nwhere Sim(x,y) = xyT and \u2327 is the temperature hyperparameter. By optimizing with Lcontrastive, the model can extract valuable series-wise properties to representations.\n3.3. Point-wise Reconstruction\nInstead of directly generating missing values for masked data, we attempt to reconstruct the original time series {xi}Ni=1 by weighted aggregating the point-wise representations Z of other series, which is based on the similarity matrix calculated from series-wise representations S . Since the similarity matrix of S has already been calculated in Eq. (6). As shown in Figure 1, we can present the reconstruction of i-th original time series as follows:\nz\u0302i = X\ns02S\\{si}\neSim(si,s 0)/\u2327\nP s002S\\{si} eSim(si,s0 0)/\u2327 z0, (7)\nwhere z0 denote the corresponding poin -wise representation of s0 and z\u0302i 2 RL\u21e5dmodel are the reconstructed point-wis\nSeries-wise Similarity 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 SimMTM: A Simple Pre-Training Framework for Masked Time-Series Modeling Table 1. In-Domain evaluation in forecasting. The input sequence length is 96 and all results are averaged from 4 different prediction lengths O 2 {96, 192, 336, 720}. A lower MSE or MAE indicates a better prediction. Full results can be found in Table 11. MODELS RANDOM INIT. MASKING CONTRASTIVE DEEP MODEL TST TF-C TS-TCC MIXING-UP TS2VEC NSTRANS.? SIMMTM (2021) (2022) (2021) (2022) (2022) (2022) (OURS) MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE ETTH1 0.605 0.549 0.877 0.720 1.162 0.863 1.152 0.857 1.098 1.933 0.897 0.752 0.570 0.537 0.497 0.476 ETTH2 0.457 0.455 2.550 1.361 2.850 1.349 3.101 .509 2.723 1.348 2.628 1.381 0.526 0.516 0.415 0.428 ETTM1 0.478 0.464 0.669 0.589 0.744 0.652 1.298 0.893 0.734 0.635 0.669 0.600 0.481 0.456 0.414 0.422 ETTM2 0.416 0.388 1.125 0.771 1.755 0.947 1.153 0.857 1.420 0.912 1.466 0.957 0.306 0.347 0.302 0.342 representations. After a channel projector, we can obtain the reconstructed original time series x\u0302i = Project r(z\u0302i), x\u0302i 2 RL\u21e5C . Thus, the point-wise re ons ruction loss can be formalized as follows: Lreconstruction = NX i=1 kxi x\u0302ik22. (8) Giving consid ation to both series-wise and point-wise properties, The overall optimization process of SimMTM can be repres nted as follows: min \u21e5 Lcontrastive + Lreconstruction, (9) where \u21e5 denotes the set of all parameters of the backbone. Since the point-wise reconstruction is based o the serieswise similarity matrix, the seri s-wise contrastive learning will also benefit the reconstruction process. And the optimization of point-wise representations with reconstruction loss will also bring better series-wise representations for the contrastive loss. Thus, both parts will collaborate with each other for better representation learning of time series. 4. Experiments To evaluate the proposed SimMTM, we extensively experiment on nine real-world benchmarks, covering three mainstream time series analysis tasks: multivariate time series forecasting, classification, and imputation. The nine benchmarks cover different numbers of channels, varying series lengths, and application scenarios, including electricity systems, neurological healthcare, human activity recognition, mechanical fault detection, and physical status monitoring. Then we do self-supervised pre-training to evaluate whether SimMTM can capture effective representations with two setups, in-domain and cross-domain, in every type of time series analysis task. In In-Domain evaluation, we pre-train and fine-tune using the same or same domain dataset. But in Cross-Domain evaluation, we pre-train a model on one pre-training dataset and use it for fine-tuning on different datasets. Implementation Table 3 is a summary of experiment benchmarks. Mor details about datasets, experiment implementation and model configuration can be found in Appendix Baselines To verify the generality and effectiveness of SimMTM, we compared three advanced and prevalent foundation models for time series on four ETT datasets, including NSTransformer (Liu t al., 2022), AutoFormer (Wu et al., 2021), and vanilla Transformer (Vaswani et al., 2017); Besides, w also compared five state-of-the-art selfsupervised time series pre-training methods, including the contrastive lear ing methods: TF-C (Zhang et al., 2022), TS-TCC (Eldele et al., 2021), Mixing-up (Wickstr\u00f8m et al., 2022), TS2Vec (Yue et al., 2022), and masked modeling method: TST (Zerveas et al., 2021) 4.1. Main results As a self-supervised time series pre-training method, our proposed SimMTM consistently achieves state-of-the-art performance on nine real-world time series benchmarks of three main-stream time series analysis tasks, including multivariate time series forecasting, classification, and imputation.(Figure x-axis: classification; y-axis: forecasting; z-axis: imputation) 4.2. Forecasting In-Domain All results of the in-domain evaluation for the forecasting task in Table 1. We pre-train the model on one pre-training dataset and finetune it to the same target dataset to compare the effect of different time series self-supervised pre-training methods. As shown in Table1, TST outperforms all the contrastive pre-training baselines as a masked pre-training method by randomly masking single time steps of the time series and reconstructing the missing content, indicating that masked time series modeling by temporal-wise reconstruction learns more benefit forecasting representations than the series-wise contrastive pre-training. Nevertheless, our Decoder Point-wise Aggregation Self-supervised Pre-training Far away from each other Close to ac other Representations of Original Seri s Representations of Masked Series Time Va lu e Encoder Projector \u2026 Point-wise Representations \u2026 Series-wise Representations \u2026 \u2026 \u2026 \u2026 \u2026 Series-wise Similarity Point-wise Aggregation Decoder Reconstructed Original Series Original & Masked Series + \u2026 \u2026 \u2026 + Figur 2: Architecture of SimMTM, which reconstructs the original time series by adaptive aggregating multiple masked time series based on series-wise similariti s learned c ntrastively from data. Masking. Given {xi}Ni=1 a a mini-batch of N ti e s ries mples, where xi \u2208 RL\u00d7C contains L time points and C observed variates, we can easily generate a set of masked series for each sample xi by randomly masking a portion of time points along the temporal dimension, formalizing by: {xji}Mj=1 = Maskr(xi), (1) wh re r \u2208 [0, 1] de otes the masked portion. M is a hyperparameter for the number of masked time series. xji \u2208 RL\u00d7C represents the j-th masked time series of xi, where the values of masked time points are replaced by zeros. With above process, we can obtain a batch of augmented time series. For clarity, we present all the (N(M + 1)) input series in a set as X = \u22c3Ni=1 ( {xi} \u222a {xji}Mj=1 ) . Representation learning. After the encoder and projector layer, we can obtain the point-wise repres ntati ns Z and serie -wise representations S, which can be formalized s: Z = N\u22c3\ni=1\n( {zi} \u222a {zji}Mj=1 ) = Enocder(X ), S = N\u22c3\ni=1\n( {si} \u222a {sji}Mj=1 ) = Projector(Z), (2)\nwhere zi, z j i \u2208 RL\u00d7dmodel and si, sji \u2208 R1\u00d7dmodel . Encoder(\u00b7) denotes the model encoder, which can project the input data into deep representations and will be transferred to downstream tasks during the fine-tuning process. In this paper, we implement the encoder as a well-acknowledged Transformer [39] and ResNet [13]. As for the Projector(\u00b7), we employ a simple MLP layer along the temporal dimension to obtain series-wise representations. More details can be found in Section 4. Technically, the encoder is applied to input series separately, namely \u22c3N i=1(Encoder(xi) \u222a {Encoder(x j i )}Mj=1), and so does for projector. Here we adopt the set-style formalization for conciseness.\nSeries-wise similarity learning. Note that directly averaging multiple masked time series will result in the over-smoothing problem [40], impeding the representation learning. Thus, to precisely reconstruct the original time series, we attempt to utilize the similarities among series-wise representations S for weighted aggregation, namely exploiting the local structure of the time series manifold. For simplification, we formalize the calculation of series-wise similarities as follows:\nR = Sim(S) \u2208 RD\u00d7D, D = N(M + 1), Ru,v = uvT\n\u2225u\u2225\u2225v\u2225 ,u,v \u2208 S, (3)\nwhere R is the matrix of pair-wise similarities for (N(M + 1)) input samples in series-wise representation space, which are measured by the cosine distance. Ru,v is the calculated similarity between series-wise representations u,v \u2208 S.\nPoint-wise aggregation. As shown in Figure 2, based on the learned series-wise similarities, the aggregation process for the i-th original time series is:\nz\u0302i = \u2211\ns\u2032\u2208S\\{si}\nexp(Rsi,s\u2032/\u03c4)\u2211 s\u2032\u2032\u2208S\\{si} exp(Rsi,s\u2032\u2032/\u03c4) z\u2032, (4)\nwhere z\u2032 represents the corresponding point-wise representation of s\u2032, i.e. z\u2032 = Projector(s\u2032). z\u0302i \u2208 RL\u00d7dmodel is the reconstructed point-wise representation. \u03c4 denotes the temperature hyperparameter\nof softmax normalization for series-wise similarities. It is notable that as described in Eq. (4), for each time series xi, the reconstruction is not only based its own masked series {xji}Mj=1. We also introduce other series representations S\\{si} into aggregation, which requires the model to suppress the interference of less-related noise series and precisely learn similar representations for both the masked and the original series, namely guiding the model to learn the manifold structure better. After the decoder, we can obtain the reconstructed original time series, namely\n{x\u0302i}Ni=1 = Decoder({z\u0302i}Ni=1), (5)\nwhere x\u0302i \u2208 RL\u00d7C is the reconstruction to xi. Decoder(\u00b7) is instantiated as a simple MLP layer along the channel dimension following [50].\n3.2 Self-supervised Pre-training\nFollowing the masked modeling paradigm, SimMTM is supervised by a reconstruction loss:\nLreconstruction = N\u2211\ni=1\n\u2225xi \u2212 x\u0302i\u222522. (6)\nNote that the reconstruction process is directly based on the series-wise similarities, while it is hard to guarantee the model captures the precise similarities without explicit constraints in the series-wise representation space. Thus, to avoid trivial aggregation, we also utilize the neighborhood assumption of the time series manifold to calibrate the structure of series-wise representation space S . For clarity, we formalize the neighborhood assumption by defining positive and negative pairs as follows:\nPositive pairs: (si, s+i ), s + i \u2208 {sji}Mj=1\nNegative pairs: (si, s\u2212i ), s \u2212 i \u2208 {sk} \u222a {sjk}Mj=1, i \u0338= k\n(7)\nwhere s+i and s \u2212 i mean the elements that are assumed as close to and far away from si respectively. Eq. (7) indicates that the original time series and its masked series will present close representations and be far away from the representations from other series in S . For each series-wise representation s \u2208 S, we define the set of its assumed close series as S+ \u2282 S. Note that to avoid the dominating representation, we assume that s /\u2208 S+. With the above formalization, we can define manifold constraint to series-wise representation space as:\nLconstraint = \u2212 \u2211\ns\u2208S\n  \u2211\ns\u2032\u2208S+ log exp(Rs,s\u2032/\u03c4)\u2211 s\u2032\u2032\u2208S\\{s} exp(Rs,s\u2032\u2032/\u03c4)\n  , (8)\nwhich can optimize the learned series-wise representations to satisfy the neighborhood assumption in Eq. (7) better. Finally, the overall optimization process of SimMTM can be represented as follows:\nmin \u0398\nLreconstruction + \u03bbLconstraint, (9)\nwhere \u0398 denotes the set of all parameters of the deep architecture. To trade off the two parts in Eq. (9), we adopt the tuning strategy presented by [17], which can adjust the hyperparameters \u03bb adaptively according to the homoscedastic uncertainty of each loss.\n4 Experiments\nTo fully evaluate SimMTM, we conduct experiments on two typical time series analysis tasks: forecasting and classification, covering low-level and high-level representation learning. Further, we present the fine-tuning performance for each task under in- and cross-domain settings.\nBenchmarks. We summarize the experiment benchmarks in Table 1, comprising twelve real-world datasets that cover two mainstream time series analysis tasks: time series forecasting and classification. Concretely, we have followed the standard experimental setups in Autoformer [47] for the forecasting tasks and the experiment settings proposed by TF-C [57] for classification.\nTable 1: Summary of experiment benchmarks.\nTasks Datasets Semantic\nForecasting\nETTh1,ETTh2 Electricity ETTm1,ETTm2 Electricity\nWeather Weather Electricity Electricity\nTraffic Transportation\nClassification\nSleepEEG EEG Epilepsy EEG\nFD-B Faulty Detection Gesture Hand Movement EMG Muscle Responses\nImplementations. We compare SimMTM with six competitive state-of-the-art baseline methods, including the contrastive learning methods: TF-C [57], CoST [46], TS2Vec [55], LaST [42], the masked modeling method: TiMAE [21], TST [56]. TF-C [57] and TiMAE [21] are the previous best pre-training methods. We experiment on both in- and crossdomain settings. For the in-domain setting, we pre-train and fine-tune the model using the same dataset. As for the cross-domain setting, we pretrain the model on a certain dataset and fine-tune the encoder to different datasets. More details are provided in Appendix A.\nUnified encoder. To make a fair comparison, we attempt to unify encoder for these pretraining methods. Concretely, we adopt vanilla Transformer [39] with the channel independence [26] as the unified encoder for forecasting. The channel-independent design allows models to accomplish cross-domain transfer between datasets with different variate numbers. As for the classification, we use 1D-ResNet [13] as the shared encoder following [57]. Notably, for LaST [42] and TFC [57], since their designs are closely related to model structures, we directly report results from their papers or reproduce with official codes. For other baselines and SimMTM, the results in the main text are from the unified encoder. The results from their official papers are also compared in Appendix A.2. For all baselines, the results with a unified encoder generally surpass results reported by themselves.\n4.1 Main results\nWe summarize the results in forecasting and classification of in- and cross-domain settings in Figure 3. In all these settings, SimMTM outperforms other baselines significantly. It is also notable that although the masking-based method Ti-MAE [21] achieves good performance in the forecasting task (x-axis of Figure 3), it fails in the classification task (y-axis). Besides, contrastive-based methods fail in low-level forecasting tasks. These results indicate that previous methods cannot simultaneously cover high-level and low-level tasks, highlighting the advantages of SimMTM in task generality.\n4.2 Forecasting\nIn-domain. As shown in Table 2, empowering by SimMTM pre-training, the model performance is promoted significantly (SimMTM vs. Random init.). Besides, SimMTM consistently outperforms other pre-training methods. On the average of all benchmarks, SimMTM achieves 8.3% MSE reduction and 4.3% MAE reduction compared to the advanced masked modeling baseline Ti-MAE [21], 14.7% MSE reduction and 12.0% MAE reduction compared to the contrastive baseline CoST [46]. It is also notable that both Ti-MAE [21] and TST [56] outperform all the contrastive-based baselines. This indicates that masked modeling based on point-wise reconstruction suits the forecasting task better than the series-wise contrastive pre-training.\nCross-domain. As demonstrated in Table 3, we present multiple scenarios to verify the effectiveness under the cross-domain setting, where SimMTM consistently outperforms other baselines. Note that the channel-independent encoder enables the comparing baselines to be capable of transferring pre-trained models between datasets with different variate numbers: Weather \u2192 {ETTh1, ETTm1}. But for LaST and TF-C with model-specific designs, they cannot be applied to these scenarios. While negative migration has been observed in some cross-domain scenarios, such as Weather \u2192 ETTh1 and ETTh2 \u2192 ETTm1, SimMTM is still significantly overall superior to other baselines.\n4.3 Classification\nIn-domain. We investigate the in-domain pre-training effect on the classification tasks in Table 4. Note that different from forecasting, the classification task requires the model to learn high-level time series representations. From Table 4, we can find that the contrastive pre-training baselines\nachieve competitive performances. In contrast, the masking-based model Ti-MAE [21] and TST [56] perform poorly, and TST even exhibits a negative transfer phenomenon compared to the random initialization, indicating that contrastive learning is generally more suitable for classification tasks. Surprisingly, although SimMTM follows the masked modeling paradigm, with our specially-designed reconstruction task, it can still achieve the best performance in the classification task. This is benefited from the neighborhood aggregation from multiple masked series, which enables the model to exploit the local structure of the time series manifold.\nCross-domain. We experiment with four cross-domain transfer scenarios in Table 4: SleepEEG \u2192 {Epilepsy, FD-B, Gesture, EMG}, where the target datasets are distinct from the pre-training dataset. Due to the large gap between pre-training and fine-tuning datasets, the baselines perform poorly in most cases, while SimMTM still surpasses other baselines and the random initialization significantly. Especially for SleepEEG \u2192 EMG, SimMTM remarkably surpasses previous state-of-the-art TF-C (Accuracy: 97.56% vs. 81.74%). These results demonstrate that SimMTM can precisely capture valuable knowledge from pre-training datasets and uniformly benefit extensive downstream datasets.\n4.4 Model Analysis\nAblations. As shown in Figure 4, we provide ablations to two parts of the training loss in SimMTM. It is observed that both Lreconstruction and Lconstraint are essential to the final performance. Especially, for the SleepEEG \u2192 EMG experiment, SimMTM surpasses the random initialization remarkably, where reconstruction and constraint losses provide 9.7% and 16.0% absolute improvement respectively. Besides, we can also find that in comparison to Lreconstruction, Lconstraint provides more contributions to the final results. This comes from our design that the constraint loss uncovers a proper time series manifold helpful for reconstruction from multiple masked series, without which the neighborhood aggregation will degenerate to the trivial average.\nRepresentation analysis. To illustrate the advantages of SimMTM intuitively, we provide a representation analysis in Table 5, where we can find the following observations. Firstly, we can find that the CKA value of SimMTM in the classification task is clearly smaller than the values in the\nforecasting task, where the former is a high-level task and the latter requires low-level representations. These results demonstrate that SimMTM can learn adaptive representations for different tasks, which can be benefited from our design in the pre-training loss. Concretely, the temporal variations of classification pre-training datasets are much more diverse than the forecasting datasets. Thus, the Lconstraint will be easier for optimization in classification, deriving a smaller CKA value. Secondly, from |\u2206CKA|, it is observed that the models pre-trained from SimMTM present a smaller gap with respect to the final fine-tuned model in representation learning properties, which is why SimMTM can consistently improve downstream tasks.\nModel generality. From Table 6, we can find that as a general time series pre-training framework, SimMTM can consistently improve the forecasting performance of diverse advanced base models, even for the state-of-the-art time series forecasting model PatchTST [26]. This generality also indicates that we can further improve the model\u2019s performance by employing advanced base models as encoders. It is also notable that different from the negative transfer phenomenon caused by the canonical sub-series masked modeling used in the PatchTST paper [26], the consistency improvement of SimMTM further verifies the effectiveness of our design.\nFine-tuning to limited data scenarios. One essential application of pre-training models is to provide prior knowledge for downstream tasks, especially for limited data scenarios, which is critical to the fast-adaption of deep models. Thus, to verify the effectiveness of SimMTM and other pretraining methods in data-limited scenarios, we pre-train a model on ETTh2 and fine-tune it to ETTh1 with different choices for the remaining proportions of training data. All results are presented in Figure 5. We can find that SimMTM achieves significant performance gains in different data proportions compared to other time series pre-training methods. Specifically, for the 10% data finetuning setting, SimMTM significantly outperforms the advanced masking-based method Ti-MAE [21] (MSE: 0.591 vs. 0.660). Compared with the contrastive-based method TF-C [57], SimMTM also achieves 26.0% MSE reduction. These results further verify that SimMTM can effectively capture valuable knowledge from datasets and boost the final performance, even in limited data scenarios.\nMasking strategy. Note that the difficulty of reconstructing the original time series increases along with the increase of the masked ratio, but decreases when the number of neighbor masked series increases. We explore the potential relationship between the masked ratio and the number of masked series used for reconstruction, namely r and M in Eq. (1) respectively. The experimental results in Figure 5 show that we need to set M \u221d r to obtain better results. Experimentally, we choose the masking ratio as 50% and adopt three masked series for reconstruction throughout this paper. In addtion, we can find that only using one masked series (M = 1) for pre-training generally performs worse than the settings with larger M , where the latter enables the model to discover the relations between input series and its neighbors. See Figure 1 for an intuitive understanding. These results further highlight the advantage of our design in manifold learning.\n5 Conclusion\nThis paper presents SimMTM, a simple pre-training framework for masked time-series modeling. Going beyond the previous convention in reconstructing the original time series from unmasked time points, SimMTM proposes a new masked modeling task as reconstructing the original series from its multiple neighbor masked series. Concretely, SimMTM aggregates the point-wise representations based on the series-wise similarities, which are carefully constrained by the neighborhood assumption on the time series manifold. Experimentally, SimMTM can furthest bridge the gap between pretrained and fine-tuned models, thereby achieving consistent state-of-the-art in distinct forecasting and classification tasks compared to the most advanced time series pre-training methods, covering both in-domain and cross-domain settings. In the future, we will further extend SimMTM to large-scale and diverse pre-training datasets in pursuing the foundation model for time series analysis.\nAcknowledgments\nThis work was supported by the National Key Research and Development Plan (2021YFB1715200), National Natural Science Foundation of China (62022050 and 62021002), and Beijing Nova Program (Z201100006820041).\nReferences [1] Ralph G Andrzejak, Klaus Lehnertz, Florian Mormann, Christoph Rieke, Peter David, and Christian E\nElger. Indications of nonlinear deterministic and finite-dimensional structures in time series of brain electrical activity: Dependence on recording region and brain state. Physical Review E, 2001.\n[2] Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and Michael Auli. Data2vec: A general framework for self-supervised learning in speech, vision and language. In ICML, 2022.\n[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In NeurIPS, 2020.\n[4] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In NeurIPS, 2020.\n[5] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In ICML, 2020.\n[6] Mingyue Cheng, Qi Liu, Zhiding Liu, Hao Zhang, Rujiao Zhang, and Enhong Chen. Timemae: Selfsupervised representations of time series with decoupled masked autoencoders. T-KDE, 2023.\n[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In NAACL, 2018.\n[8] Emadeldeen Eldele, Mohamed Ragab, Zhenghua Chen, Min Wu, Chee Keong Kwoh, Xiaoli Li, and Cuntai Guan. Time-series representation learning via temporal and contextual contrasting. In IJCAI, 2021.\n[9] Jean-Yves Franceschi, Aymeric Dieuleveut, and Martin Jaggi. Unsupervised scalable representation learning for multivariate time series. In NeurIPS, 2019.\n[10] Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learners. In IJCNLP, 2020.\n[11] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. In CVPR, 2022.\n[12] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In CVPR, 2020.\n[13] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.\n[14] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A survey on contrastive self-supervised learning. Technologies, 2020.\n[15] Junguang Jiang, Yang Shu, Jianmin Wang, and Mingsheng Long. Transferability in deep learning: A survey. arXiv preprint arXiv:2201.05867, 2022.\n[16] Bob Kemp, Aeilko H Zwinderman, Bert Tuk, Hilbert AC Kamphuisen, and Josefien JL Oberye. Analysis of a sleep-dependent neuronal feedback loop: the slow-wave microcontinuity of the eeg. TBME, 2000.\n[17] Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task learning using uncertainty to weigh losses for scene geometry and semantics. In CVPR, 2018.\n[18] Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey E. Hinton. Similarity of neural network representations revisited. In ICML, 2019.\n[19] Christian Lessmeier, James Kuria Kimotho, Detmar Zimmer, and Walter Sextro. Condition monitoring of bearing damage in electromechanical drive systems by using motor current signals of electric motors: A benchmark data set for data-driven classification. In PHM, 2016.\n[20] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, and Kaiming He. Scaling language-image pre-training via masking. In CVPR, 2023.\n[21] Zhe Li, Zhongwen Rao, Lujia Pan, Pengyun Wang, and Zenglin Xu. Ti-mae: Self-supervised masked time series autoencoders. arXiv preprint arXiv:2301.08871, 2023.\n[22] Jiayang Liu, Lin Zhong, Jehan Wickramasuriya, and Venu Vasudevan. uwave: Accelerometer-based personalized gesture recognition and its applications. In PMC, 2009.\n[23] Xiao Liu, Fanjin Zhang, Zhenyu Hou, Li Mian, Zhaoyu Wang, Jing Zhang, and Jie Tang. Self-supervised learning: Generative or contrastive. TKDE, 2021.\n[24] Yong Liu, Haixu Wu, Jianmin Wang, and Mingsheng Long. Non-stationary transformers: Exploring the stationarity in time series forecasting. In NeurIPS, 2022.\n[25] Abdullah Al Mueen and Eamonn J. Keogh. Extracting optimal performance from dynamic time warping. KDD, 2016.\n[26] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64 words: Long-term forecasting with transformers. In ICLR, 2023.\n[27] Manuel T Nonnenmacher, Lukas Oldenburg, Ingo Steinwart, and David Reeb. Utilizing expert features for contrastive learning of time-series representations. In ICML, 2022.\n[28] Adam Paszke, S. Gross, Francisco Massa, A. Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Z. Lin, N. Gimelshein, L. Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, 2019.\n[29] PeMS. Traffic Dataset. http://pems.dot.ca.gov/.\n[30] PhysioToolkit PhysioBank. Physionet: components of a new research resource for complex physiologic signals. Circulation, 2000.\n[31] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 2019.\n[32] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unified text-to-text transformer. JMLR, 2020.\n[33] Quentin Rebjock, Baris Kurt, Tim Januschowski, and Laurent Callot. Online false discovery rate control for anomaly detection in time series. In NeurIPS, 2021.\n[34] Pritam Sarkar and Ali Etemad. Self-supervised learning for ecg-based emotion recognition. In ICASSP, 2020.\n[35] Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face recognition and clustering. In CVPR, 2015.\n[36] Fan-Keng Sun, Chris Lang, and Duane Boning. Adjusting for autocorrelated errors in neural networks for time series. In NeurIPS, 2021.\n[37] Chi Ian Tang, Ignacio Perez-Pozuelo, Dimitris Spathis, and Cecilia Mascolo. Exploring contrastive learning in human activity recognition for healthcare. In NeurIPS, 2020.\n[38] UCI. UCI Electricity Load Time Series Dataset. https://archive.ics.uci.edu/ml/datasets/ ElectricityLoadDiagrams20112014.\n[39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.\n[40] Pascal Vincent, H. Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. JMLR, 2010.\n[41] Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In ICML, 2020.\n[42] Zhiyuan Wang, Xovee Xu, Weifeng Zhang, Goce Trajcevski, Ting Zhong, and Fan Zhou. Learning latent seasonal-trend representations for time series forecasting. In NeurIPS, 2022.\n[43] Wetterstation. Weather Dataset. https://www.bgc-jena.mpg.de/wetter/.\n[44] Alexander Wettig, Tianyu Gao, Zexuan Zhong, and Danqi Chen. Should you mask 15% in masked language modeling? In EACL, 2023.\n[45] Kristoffer Wickstr\u00f8m, Michael Kampffmeyer, Karl \u00d8yvind Mikalsen, and Robert Jenssen. Mixing up contrastive learning: Self-supervised representation learning for time series. PRL, 2022.\n[46] Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven Hoi. Cost: Contrastive learning of disentangled seasonal-trend representations for time series forecasting. In ICLR, 2022.\n[47] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. In NeurIPS, 2021.\n[48] Zhirong Wu, Yuanjun Xiong, Stella X. Yu, and Dahua Lin. Unsupervised feature learning via nonparametric instance discrimination. In CVPR, 2018.\n[49] Zhenda Xie, Zigang Geng, Jingcheng Hu, Zheng Zhang, Han Hu, and Yue Cao. Revealing the dark secrets of masked image modeling. In CVPR, 2023.\n[50] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A simple framework for masked image modeling. In CVPR, 2022.\n[51] Jiehui Xu, Haixu Wu, Jianmin Wang, and Mingsheng Long. Anomaly transformer: Time series anomaly detection with association discrepancy. In ICLR, 2021.\n[52] Ling Yang and Shenda Hong. Unsupervised time-series representation learning with iterative bilinear temporal-spectral fusion. In ICML, 2022.\n[53] Xinyu Yang, Zhenguo Zhang, and Rongyi Cui. Timeclr: A self-supervised contrastive learning framework for univariate time series representation. KBS, 2022.\n[54] Hugo Y\u00e8che, Gideon Dresdner, Francesco Locatello, Matthias H\u00fcser, and Gunnar R\u00e4tsch. Neighborhood contrastive learning applied to online patient monitoring. In ICML, 2021.\n[55] Zhihan Yue, Yujing Wang, Juanyong Duan, Tianmeng Yang, Congrui Huang, Yunhai Tong, and Bixiong Xu. TS2Vec: Towards Universal Representation of Time Series. In AAAI, 2022.\n[56] George Zerveas, Srideepika Jayaraman, Dhaval Patel, Anuradha Bhamidipaty, and Carsten Eickhoff. A transformer-based framework for multivariate time series representation learning. In SIGKDD, 2021.\n[57] Xiang Zhang, Ziyuan Zhao, Theodoros Tsiligkaridis, and Marinka Zitnik. Self-supervised contrastive pre-training for time series via time-frequency consistency. In NeurIPS, 2022.\n[58] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In AAAI, 2021.\nA Implementation Details\nAll the experiments are repeated five times, implemented in PyTorch [28] and conducted on NVIDIA A100 SXM4 40GB GPU. We implement the baselines based on their official implementation and follow the configuration from their original papers. For the metrics, we adopt the mean square error (MSE) and mean absolute error (MAE) for the time series forecasting. As for the classification, accuracy, precision, recall, F1 score, and their average value are recorded.\nA.1 Dataset Description\nWe conduct experiments to evaluate the effect of our method under in-domain and cross-domain settings on twelve real-world datasets for two typical time series analysis tasks: forecasting and classification, covering diverse application scenarios (electricity system, neurological healthcare, human activity recognition, mechanical fault detection, and physical status monitoring), different types of signals (ECG, EMG, acceleration, vibration, power load, weather, and transoirtation), multivariate channel dimensions (from 1 to 862), varying times series lengths (from 96 to 5120) and large span sampling ratio (from 100 Hz to 4000 Hz). The detailed descriptions of these datasets are summarized in Table 7.\n(1) ETT (4 subsets) [58] contains the time series of oil temperature and power load collected by electricity transformers from July 2016 to July 2018. ETT is a group of four subsets with different recorded frequencies: ETTh1/ETTh2 are recorded every hour, and ETTm1/ETTm2 are recorded every 15 minutes.\n(2) WEATHER [43] includes meteorological time series with 21 weather indicators collected every 10 minutes from the Weather Station of the Max Planck Biogeochemistry Institute in 2020.\n(3) ELECTRICITY [38] records the hourly electricity consumption of 321 clients from 2012 to 2014. Values are in kW of each 15 min. All time labels report to Portuguese hour. However, all days present 96 measures (24\u00d74). For every year in March, time change day (which has only 23 hours), values between 1:00 am and 2:00 am are zero for all points. For every year in October, time change day (which has 25 hours), the values between 1:00 am and 2:00 am aggregated consumption of two hours.\n(4) TRAFFIC [29] encompasses the hourly measures of road occupancy rates obtained from 862 sensors situated in the San Francisco Bay area freeways. These measurements were carried out between January 2015 and December 2016.\n(5) SLEEPEEG [16] contains 153 whole-night sleeping electroencephalography (EEG) recordings from 82 healthy subjects. We follow the same data preprocessing approach as [57] and get 371,055 univariate brainwaves. Each brainwave is sampled at a frequency of 100 Hz and associated with one of five sleeping stages: Wake, Non-rapid eye movement (3 sub-states), and Rapid Eye Movement.\n(6) EPILEPSY [1] monitors the brain activities of 500 subjects with a single-channel EEG sensor. Every subject is recorded for 23.6 seconds of brain activities. The dataset is sampled at 178 Hz and contains 11,500 samples in total. We follow the procedure described by [57]. The first four classes (eyes open, eyes closed, EEG measured in the healthy brain region, and EEG measured in the tumor region) of the original five categories of each sample are classified as positive, and the remaining classes are used as negative.\n(7) FD-B [19] is generated by electromechanical drive systems. It monitors the condition of rolling bearings and detects their failures based on the monitoring conditions, which include speed, load torque, and radial force. Concretely, FD-B has 13,640 samples in total. Each recording is sampled at 64k Hz with 3-class labels: undamaged, inner damaged, and outer damaged.\n(8) GESTURE [22] are collected from 8 hand gestures based on the paths of hand movement recorded by an accelerometer. The eight gestures are hand swiping left, right, up, and down, hand waving in a counterclockwise or clockwise circle, hand waving in a square, and waving a right arrow, respectively. This dataset contains 440 examples of balanced classification labels that can be used, and each sample includes eight different kinds of gesture categories.\n(9) EMG [30] is sampled with 4K Hz and consists of 163 single-channel EMG recordings from the anterior tibialis muscle of three healthy volunteers suffering from neuropathy and myopathy. Each patient is a classification category, so each sample is associated with one of three classes.\nA.2 Baselines Implementation\nWe compare SimMTM against six state-of-the-art baselines. To make a fair and comprehensive comparison, we tried two baseline implementation approaches for forecasting and classification tasks: the unified encoder and reproduced with their official implementation encoder. Notably, LaST [42] and TF-C [57] are closely related to model structures. We directly report results from their papers or reproduce codes with official implementation.\n(1) Unified encoder. We attempt to unify the encoder for these pre-training methods. Specifically, we adopt the vanilla Transformer [39] with channel independent [26] for forecasting to accomplish cross-domain transfer between datasets with different variate numbers. As for the classification, we use 1D-ResNet [13] as the\nencoder following [57]. Besides, we do a comprehensive hyperparameter search for all baselines. For the Transformer encoder, we vary the number of Transformer layers in {1, 2, 3, 4}, select the model dimension from {16, 32, 64, 128, 256}, and the attention head from {4, 8, 16, 32}. For the 1D-ResNet, we search the number of 1D-ResNet layers from {1, 2, 3, 4}, the kernel size from {3, 5, 8} respectively. Additionally, for the masked modeling methods TST [56], Ti-MAE [21], we also searched the masked ratio r = {0.125, 0.25, 0.5, 0.75} for better performance.\n(2) Official implementation. We also implement the baselines following the corresponding official codes, including encoder, hyperparameters, etc. The comparisons are included in Section E of this supplementary material. We directly report the results from their original papers for the same set. For mismatched settings, the results are from our implementation.\nFinally, for baselines Ti-MAE [21], TST [56], CoST [46], and TS2Vec [55], we report the results based on the unified encoder in the main text. But for baselines LaST [42] and TF-C [57], we report the results of the official code implementation or their original paper, which are limited by their model structures. As a result, the performances of all baselines with unified encoder (that we reported in the main text) generally surpass their official implementation and results reported in their own paper. Table 8 shows more details. Full experimental results are in Section E.\nA.3 Pre-training and Fine-tuning Configuration\nWe built two types of pre-training and fine-tuning scenarios, in-domain and cross-domain, based on the benchmarks of forecasting and classification tasks to compare the effectiveness of our method and other time series pre-training methods.\nWe pre-train a model on one subset for forecasting tasks and fine-tune it to the same dataset to build seven in-domain transfer evaluation scenarios. In cross-domain evaluation, we pre-train a model on one specific dataset and use other datasets for fine-tuning. Based on the above settings, we constructed fifteen in- and cross-domain pre-training and fine-tuning experiments, covering the same dataset with the same sampled frequency, different datasets with the same sampled frequency, and different datasets with different sampled frequencies.\nWe use the same dataset, Epilepsy, to construct the in-domain setting for classification tasks. For the crossdomain setting, we pre-train a model for classification tasks on a univariate time series dataset SleepEEG with the most complex temporal dynamics and the most samples. And then fine-tune the model separately on Epilepsy, FD-B, Gesture, and EMG. Furthermore, we constructed four cross-domain evaluation scenarios by pre-training from SleepEEG and fine-tuning to Epilepsy, FD-B, Gesture, and EMG because of fewer commonalities and the enormous gap among these datasets. Table 9 shows detailed pre-training and fine-tuning settings.\nA.4 Model and Training Configuration\nFollowing the previous convention, we choose the encoder part of Transformer [39] with channel independent as the feature extractor for forecasting tasks. For the classification tasks, we adopt 1D-ResNet [13] as the encoder following [57]. In the pre-training stages, we pre-train the model with different learning rates and batch sizes according to the pre-train datasets. Then we fine-tune it to downstream forecasting and classification tasks supervised by L2 and Cross-Entropy losses, respectively. The configuration details are in Table 10.\nWe verify the hyperparameter sensitivity of SimMTM on ETTh1 in Table 11, including masked ratio (r), the number of masked series (M), temperature (\u03c4 ), masked function (Mask), encoder depth (elayers), and the hidden dimension (dmodel). Lower MSE and MAE represent better performance.\nAs shown in Table 11 (a) and 11 (b), we can observe the effect of the method is closely related to the trade-off of the masked ratio and the number of masked series. Hence, a reasonable balance between the two kinds of parameters is critical. For the temperature hyperparameter of softmax normalization (\u03c4 ), we use an appropriately small \u03c4 that leads to higher differences and diversity of masked sequences. For the masked methods, we chose two masked methods for verification: masking following random distribution and masking following geometric distribution [56]. The results show that the method based on geometric masking is better than random masking modeling. Besides, we can find that 2 encoder layers are enough for reconstruction tasks. Note our method SimMTM consistently performs better than training from scratch under various hyperparameter changes.\nC Ablations on Aggregation Setting\nSimMTM proposes to recover masked time points by the weighted aggregation of multiple neighbors outside the manifold. We explored two types of aggregation settings.\n(1) Positive Samples Aggregation (PSA): only aggregate multiple positive neighbors (the masked series of the same sample) to reconstruct masked time points.\n(2) Positive and Negative Samples Aggregation (PNSA): aggregate both positive and negative neighbors (the masked series of all samples) to reconstruct masked time points.\nAs shown in Table 12, although PSA made good progress compared to training from scratch (Random Init.), PNSA is consistently better than SimMTM PSA in all ablation settings. In masked time-series modeling, masking can be viewed as adding noise to the original data, and masked modeling is to project masked data from the neighborhood back to the original manifold. We use positive and negative masked time series as the reconstruction candidates to drive the model to select the positive samples adaptively, which can make the model learn the structure of the manifold better. Therefore, as stated in the Method Section of the main text, we choose positive and negative sample aggregation (PNSA) as the standard aggregation setting of SimMTM.\nTo investigate the reconstruction process of different masked modeling methods, we plot both original and reconstructed time series from TST and SimMTM in Figure 6, where TST [56] follows the canonical masked modeling paradigm and learns to predict removed time points based on the remaining time points. In Figure 6, we can find that direct reconstruction is too difficult in time series, even for the 12.5% masking ratio. As for the 75% masking ratio, TST degenerates more seriously. Because of this poor reconstruction effect, direct reconstruction is difficult to provide reliable guidance to model pre-training. In contrast, our proposed SimMTM can precisely reconstruct the original time series, benefiting the representation learning. These results also support our design in neighborhood reconstruction.\nE Full Results\nDue to the limited length of the text, we summarize all the experiments in the main text into two parts: the main experiment and the analytical experiment. We categorize and index them in Tabel 13, 14.\nF Limitations\nSimMTM is inspired by the manifold perspective of masked modeling. Although we have provided relatively comprehensive results to verify the model\u2019s effectiveness, the model performance still needs theoretical guarantees. In fact, the most high-impact works in the self-supervised pre-training community are without theoretical analysis, such as BERT [7], GPT-3 [10], MAE [11] and SimMIM [50]. Thus, we would like to leave this problem as a future work.\nThe masking ratio of masked modeling methods is an essential hyper-parameter. Although we have provided a chosen principle to masking ratio r and the number of masked time series M as M \u221d r in the main text, we still need to tune these two hyperparameters for different datasets to achieve the best performance. Notably, previous methods also chose the masking ratio solely based on the empirical results [7, 11]. Thus, despite there exist limitations of SimMTM in choosing hyperparameters, the principle of M \u221d r can somewhat ease this problem. And the chosen strategy of the masking ratio can also be a potential topic in masked modeling [44].\nG Social Impacts\nThis paper presents SimMTM as a new masked modeling method for time series. SimMTM achieves state-ofthe-art in two mainstream time series analysis tasks, which can be a good supplement for the self-supervised pre-training community. We will also publish the codebase of time-series pre-training to facilitate future research.\nThis paper only focuses on the algorithm design. Using all the codes and datasets strictly follows the corresponding licenses (Appendix A.1). There is no potential ethical risk or negative social impact."
        }
    ],
    "title": "SimMTM: A Simple Pre-Training Framework for Masked Time-Series Modeling",
    "year": 2023
}