{
    "abstractText": "Reservoir computing is a computational framework of recurrent neural networks and is gaining attentions because of its drastically simplified training process. For a given task to solve, however, the methodology has not yet been established how to construct an optimal reservoir. While, \u201csmall\u2010 world\u201d network has been known to represent networks in real\u2010world such as biological systems and social community. This network is categorized amongst those that are completely regular and totally disordered, and it is characterized by highly\u2010clustered nodes with a short path length. This study aims at providing a guiding principle of systematic synthesis of desired reservoirs by taking advantage of controllable parameters of the small\u2010world network. We will validate the methodology using two different types of benchmark tests\u2014classification task and prediction task.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ken\u2010ichi Kitayama"
        }
    ],
    "id": "SP:37b054adebfeb5260ca16646e9b877e57407138f",
    "references": [
        {
            "authors": [
                "D. Verstraeten",
                "B. Schrauwen",
                "D. Stroobandt",
                "J. Van Campenhout"
            ],
            "title": "Isolated word recognition with the liquid state machine: A case study",
            "venue": "Inf. Process. Lett. 95,",
            "year": 2005
        },
        {
            "authors": [
                "M. Luko\u0161evicius",
                "H. Jaeger"
            ],
            "title": "Reservoir computing approaches to recurrent neural network training",
            "venue": "Comput. Sci. Rev",
            "year": 2009
        },
        {
            "authors": [
                "H. Jaeger"
            ],
            "title": "The \u2018echo state\u2019 approach to analyzing and training recurrent neural networks",
            "venue": "Technical Report GMD Report 148, German National Research Center for Information Technology",
            "year": 2001
        },
        {
            "authors": [
                "H. Jaeger",
                "H. Hass"
            ],
            "title": "Harnessing nonlinearity: Predicting chaotic systems and saving energy in wireless communication",
            "venue": "Science 304,",
            "year": 2004
        },
        {
            "authors": [
                "W. Maass",
                "T. Natschl\u00e4ger",
                "H. Markram"
            ],
            "title": "Real-time computing without stable states: A new framework for neural computation based on perturbations",
            "venue": "Neural Comput",
            "year": 2002
        },
        {
            "authors": [
                "W. Maass"
            ],
            "title": "Liquid state machines: Motivation, theory, and applications",
            "venue": "(World Scientific,",
            "year": 2011
        },
        {
            "authors": [
                "P Antonik"
            ],
            "title": "Online training of an opto-electronic reservoir computer applied to real-time channel equalization",
            "venue": "IEEE Trans. Neural Netw. Learn. Syst",
            "year": 2017
        },
        {
            "authors": [
                "A. Argyris",
                "J. Bueno",
                "I. Fischer"
            ],
            "title": "Photonic machine learning implementation for signal recovery in optical communications",
            "venue": "Sci. Rep",
            "year": 2018
        },
        {
            "authors": [
                "N. Schaetti",
                "M. Salomon",
                "R. Couturier"
            ],
            "title": "Echo State Networks-based Reservoir Computing for MNIST Handwritten Digits Recognition",
            "venue": "IEEE Intl Conference on Computational Science and Engineering (CSE) and IEEE Intl Conference on Embedded and Ubiquitous Computing (EUC) and 15th Intl Symposium on Distributed Computing and Applications for Business Engineering",
            "year": 2016
        },
        {
            "authors": [
                "K Vandoorne"
            ],
            "title": "Experimental demonstration of reservoir computing on a silicon photonics",
            "venue": "chip. Nat. Commun",
            "year": 2014
        },
        {
            "authors": [
                "K. Vandoorne",
                "J. Dambre",
                "D. Verstraeten",
                "B. Schrauwen",
                "P. Bienstman"
            ],
            "title": "Parallel reservoir computing using optical amplifiers",
            "venue": "IEEE Trans. Neural Netw",
            "year": 2011
        },
        {
            "authors": [
                "S.H. Strogatz",
                "I. Stewart"
            ],
            "title": "Collective dynamics of \u2018small-world",
            "venue": "networks. Nature 393,",
            "year": 1998
        },
        {
            "authors": [
                "M. Luko\u0161evi\u010dius"
            ],
            "title": "A practical guide to applying echo state networks",
            "venue": "Neural Netw. Tricks Trade 20,",
            "year": 2012
        },
        {
            "authors": [
                "K Kitayama"
            ],
            "title": "Novel frontier of photonics for data processing\u2014photonic accelerator",
            "venue": "APL Photon",
            "year": 2019
        },
        {
            "authors": [
                "M.C. Soriano",
                "D. Brunner",
                "C.R. Escalona-Mor\u00e1n",
                "I. Fischer"
            ],
            "title": "Minimal approach to neuro-inspired information processing",
            "venue": "Front. Comput. Neurosci",
            "year": 2015
        },
        {
            "authors": [
                "A. Polepalli",
                "N. Soures",
                "D. Kudithipudi"
            ],
            "title": "Digital neuromorphic design of a liquid state machine for real-time processing",
            "venue": "In IEEE International Conference on Rebooting Computing (ICRC)",
            "year": 2016
        },
        {
            "authors": [
                "L Appeltant"
            ],
            "title": "Information processing using a single dynamical node as complex system",
            "venue": "Nat. Commun",
            "year": 2011
        },
        {
            "authors": [
                "D. Brunner",
                "M.C. Soriano",
                "C.R. Mirasso",
                "I. Fischer"
            ],
            "title": "Parallel photonic information processing at gigabyte per second data rates using transient states",
            "venue": "Nat. Commun",
            "year": 2013
        },
        {
            "authors": [
                "L Larger"
            ],
            "title": "Photonic information processing beyond Turing: An optoelectronic implementation of reservoir computing",
            "venue": "Opt. Express 20,",
            "year": 2012
        },
        {
            "authors": [
                "Y Paquot"
            ],
            "title": "Optoelectronic reservoir computing",
            "venue": "Sci. Rep. 2,",
            "year": 2012
        },
        {
            "authors": [
                "F. Duport",
                "B. Schneider",
                "A. Smerieri",
                "A.M. Haelterman",
                "S. Masser"
            ],
            "title": "All-optical reservoir computing",
            "venue": "Opt. Express 20,",
            "year": 2012
        },
        {
            "authors": [
                "J. Nakayama",
                "K. Kanno",
                "A. Uchida"
            ],
            "title": "Laser dynamical reservoir computing with consistency: An approach of a chaos mask signal",
            "venue": "Opt. Express 24(8),",
            "year": 2016
        },
        {
            "authors": [
                "J Bueno"
            ],
            "title": "Reinforcement learning in a large-scale photonic recurrent neural network. Optica",
            "year": 2018
        },
        {
            "authors": [
                "M Freiberger"
            ],
            "title": "Improving time series recognition and prediction with networks and ensembles of passive photonic reservoirs",
            "venue": "IEEE J. Sel. Top. Quntum Electron",
            "year": 2020
        },
        {
            "authors": [
                "Bowers",
                "J. E"
            ],
            "title": "Recent advances in silicon photonic integrated circuits",
            "venue": "Proc. SPIE 9774,",
            "year": 2017
        },
        {
            "authors": [
                "K Nozaki"
            ],
            "title": "Femtofarad optoelectronic integration demonstrating energy-saving signal conversion and nonlinear functions",
            "venue": "Nat. Photon",
            "year": 2019
        },
        {
            "authors": [
                "Z. Deng",
                "Y. Zhang"
            ],
            "title": "Collective behavior of a small-world recurrent neural system with scale-free distribution",
            "venue": "IEEE Trans. Neural Netw",
            "year": 2007
        },
        {
            "authors": [
                "H. Cui",
                "X. Liu",
                "L. Li"
            ],
            "title": "The architecture of dynamic reservoir in the echo state network. Chaos 22, 033127",
            "venue": "https:// doi",
            "year": 2012
        },
        {
            "authors": [
                "T.L. Carroll"
            ],
            "title": "Path length statistics in reservoir computers. Chaos 30, 083130",
            "venue": "https:// doi",
            "year": 2020
        },
        {
            "authors": [
                "A.L. Barab\u00e1si",
                "R. Albert"
            ],
            "title": "Emergence of scaling in random networks",
            "venue": "Science 286,",
            "year": 1999
        },
        {
            "authors": [
                "S. Milgram"
            ],
            "title": "The small world problem",
            "venue": "Psychol. Today",
            "year": 1967
        },
        {
            "authors": [
                "L. de Oliveira Jr.",
                "F. Stelzer",
                "L. Zhao"
            ],
            "title": "Clustered and deep echo state networks for signal noise reduction",
            "venue": "Mach. Learn. https:// doi",
            "year": 2022
        },
        {
            "authors": [
                "M.D. Humphries",
                "K. Gurney"
            ],
            "title": "Network \u2018Small-World-Ness\u2019: A Quantitative method for determining canonical network equivalence",
            "venue": "PLoS One 3,",
            "year": 2008
        },
        {
            "authors": [
                "D. Anguita",
                "A. Ghio",
                "L. Oneto",
                "X. Parra",
                "J.L. Reyes-Ortiz"
            ],
            "title": "Human activity recognition on smartphones using a multiclass hardware-friendly support vector machine",
            "venue": "In International Workshop of Ambient Assisted Living (IWAAL2012) (Vitoria-Gasteiz,",
            "year": 2012
        }
    ],
    "sections": [
        {
            "text": "1 Vol.:(0123456789) Scientific Reports | (2022) 12:16697 | https://doi.org/10.1038/s41598-022-21235-y\nwww.nature.com/scientificreports"
        },
        {
            "heading": "Guiding principle of reservoir",
            "text": "computing based on \u201csmall\u2011world\u201d network"
        },
        {
            "heading": "Ken\u2011ichi Kitayama",
            "text": "Reservoir computing is a computational framework of recurrent neural networks and is gaining attentions because of its drastically simplified training process. For a given task to solve, however, the methodology has not yet been established how to construct an optimal reservoir. While, \u201csmall\u2011 world\u201d network has been known to represent networks in real\u2011world such as biological systems and social community. This network is categorized amongst those that are completely regular and totally disordered, and it is characterized by highly\u2011clustered nodes with a short path length. This study aims at providing a guiding principle of systematic synthesis of desired reservoirs by taking advantage of controllable parameters of the small\u2011world network. We will validate the methodology using two different types of benchmark tests\u2014classification task and prediction task.\nReservoir computing (RC) is a unified computational framework1,2, independently proposed recurrent neural network (RNN) models of echo state networks (ESNs)3,4 and liquid state machines (LSMs)5,6. It is a special class of RNN models, consisting of three layers\u2014an input layer and an output layers and a reservoir between the input and output layers (Fig.\u00a01). The primary difference between the RC and deep learning or multi-layer neural networks is that only the connections between the reservoir and the output layer are trainable, and the training requires much less data than in the deep learning. Owing to an excellent memory capability of the recurrent nature, it can be used in speech recognition and temporal waveform forecast. It has been shown that the RC can be used for the prediction and recognition of temporal and sequential data such as spoken word5, time series signals4,6, and wireless and optical a channel equalizations7,8. In addition, RC can also be used for handwritten digits recognition by transforming the images into temporal signals9.\nThere have been several studies on various reservoir network topologies such as sparsely random network1,2 and topologies of swirl10 and waterfall11. However, for every given task, one has to empirically seek an optimum condition, and these topologies are not sufficiently flexible because there are few adjustable parameters. Hence, a variety of networks has been tested for the reservoir. In a conventional random reservoir, it is treated like a \u201cblack box\u201d, which only allows one to specify the density or the sparseness of the weight matrix of the reservoir. The topologies of the swirl and the waterfall are fixed, and there is no variable except for the size, that is, the number of nodes. Therefore, a guiding principle to find an optimal reservoir for a given task is required. The forementioned factors have motivated us toward using \u201csmall-world\u201d network12 for the reservoir.\nThe reservoir state vector x(t) and the output vector y(t) at time t are given by13\nwhere \u03b1 \u2208 [0, 1] is the leaking rate, f (\u00b7) the activation function of node, and \u03b3 the input gain. When \u03b1 is equal to zero, the states are totally governed by previous states, while for the case with \u03b1 = 1 , the next state of the reservoir depends only on the current state and the external input. Their weights are uniquely determined using Woutxtr \u223c= y by employing the regularized least squares method as13\nwhere xty is the training input vector, the regularization parameter, and I the N \u00d7 N identity matrix. Because the training is simple, the computation cost is low.\n(1)x(t + 1) = (1\u2212 \u03b1)x(t)+ \u03b1f (Wresx(t)+ \u03b3Winu(t)),\n(2)y(t + 1) = Woutx(t)\n(3)Wout = yxTtr(xtrx T tr + IN\u00d7N )\n\u22121\nOPEN\n1National Institute of Information and Communications Technology, Tokyo 184-8795, Japan. 2Hamamatsu Photonics K.K., Hamamatsu 434-8601, Japan. email: kitayama@ieee.org\n2 Vol:.(1234567890) Scientific Reports | (2022) 12:16697 | https://doi.org/10.1038/s41598-022-21235-y\nVarious hardware implementations of the RC based on electronic and photonic components have been reported. The hardware could serve as an accelerator at the frontend of digital computers14, which is optimized to perform a specific function but does so faster with less power consumption compared to a general-purpose processor. The electronic RC implementations include analog circuits15 and VLSIs16, while the photonic hardware of RC exploits its parallelism and high-speed operations with a potentially low power consumption10,11,17\u201324. However, a stumbling block is the absence of nonlinear devices on a large scale acting as the activation function of a reservoir node. To address this issue, two types of architecture of photonic RC have been proposed\u2014delay-loop reservoir and spatial reservoir. The delay-loop RC can simplify a complicated network using a single nonlinear node in a loop-back configuration with the time-delayed feedback. The virtual nodes are distributed along the delay line, and the data injection is realized using time multiplexing17,18. In the delay-loop reservoir, an electrooptic modulator19,20, a semiconductor optical amplifier11,21, and a laser diode22,23 can be used as the nonlinear node. The nonlinearity is yielded in the optical output against the applied input voltage of the electro-optic modulator, while it appears in the optical output against the optical input of both the semiconductor optical amplifier and the laser diode, subject to the optical feedback. On the other hand, the spatial implementation of RC is basically a spatially-distributed network23. This model uses two key components; a spatial light modulator which consists of over a few tens of thousands of pixels that act as the reservoir nodes and a digital micro-mirror device realizing the reconfigurable output weights. Recently, an alternative approach to building larger reservoirs based on the combination of several blocks of small reservoir has been proposed; the model demonstrated enhanced computational capability24. As the photonic integrated circuit (PIC) technology is recently making a rapid progress25,26, PIC-based hardwares of the RC will be developed in the near future. This study will also serve as a design guideline of the PIC RC.\nWith regard to the forementioned factors, we will explore a guiding principle for optimizing the reservoir by shedding light on an article of small-world network in 199812. With the aim of achieving a better performance of the RC, we will refer to several preceding works relevant to small-world network27\u201329. In Ref.27, a reservoir model, scale-free highly-clustered echo state network (SHESN) having characteristics of both networks of the small-world and the scale-free30 has been proposed. SHESN features a spatially hierarchical and distributed topology where the intradomain connections are much denser than those of interdomain ones. In each domain, the small-world network characteristics such as a short path length and a high clustering are preserved, while the power law degree distribution of the scale-free network is embedded. It is numerically shown that time-series prediction capability of the Mackey\u2013Glass (MG) dynamic system is enhanced, compared with a conventional reservoir having random connections. Reference28 analyzes three types of network, including the scale-free network and small-work network as well as their mixture and demonstrates enhanced capability in the prediction of two types of time serial signals generated by NARMAX mode. Reference29 investigates characteristics of the path length of three types of network for reservoir, including the small-world network, the scale-free network, and the conventional random one, in order to narrow down the search space of the parameter of the reservoir for\nFigure\u00a01. Reservoir computing architecture. Input weight matrix Win is a fixed N \u00d7 L matrix where N is the number of nodes in the reservoir, and L is the dimension of the inputs at each time step. Reservoir weight matrix Wres is a fixed N \u00d7 N matrix, which is typically sparse with nonzero elements having an either a symmetrical uniform, discrete bi-valued, or normal distribution centered around zero13. Output weight matrix Wout is a learned M \u00d7 N matrix where M is the number of classes of the output data.\n3 Vol.:(0123456789) Scientific Reports | (2022) 12:16697 | https://doi.org/10.1038/s41598-022-21235-y\npredicting chaotic signals. In our present work, the reservoir model is solely based on the small-world network. We examine that the dependence of the parameters such as the degree of nodes k and the rewiring probability p on the computing capability, which has not been studied in details in the preceding works, Refs.27\u201329. We also show that there is a sweet spot of the small-world network, which gives the optimum performance of the RC for two typical tasks of neural networks; classification and regression. For a classification task, we chose the classification of human activities which has not been tested in the preceding articles, while for a regression task, we conducted the prediction of the MG chaotic signals as Refs.27,29 did.\nThe term, small-world network, is derived by an analogy with the small-world phenomenon31. It has been shown that the small-world network can well characterize the social and natural phenomena in a real world, including human behavior in social lives, power grid networks, and biological neural networks. We recall a statement related to the ongoing pandemic, presented in the article12, \u201cinfectious diseases are predicted to spread much more easily and quickly in a small world; the alarming and less obvious point is how few short cuts are needed to make the world small\u201d. The small-world network is based upon the Watts\u2013Strogatz graph, which explores a simple model of network with an arbitrarily-tuned magnitude of disorder by rewiring the links between the nodes. The small-world network is categorized between a regular network ( p = 0 ) and a completely disordered one ( p = 1 ), where a small amount of the links between the nodes are rewired to introduce disorder. Here, p is the probability of rewiring the links at random. For the purpose of illustration, three examples of 10-node ( N = 10 ) network with the node degree k = 2 are illustrated in Fig.\u00a02, which indicates connections with 2k neighboring nodes, including the regularly connected ( p = 0 ), the modestly disordered ( p = 0.5 ), and the totally disordered ( p = 1 ). It is possible to exploit the high flexibility and build up a desired reservoir from scratch. The weight matrix of the reservoir Wres can be generated from the table of the link connections shown alongside of the graphs (Fig.\u00a02) (see \u201cMethods\u201d section). The link may be either bidirectional or partially bidirectional, and in this study, it is assumed that all the links are bidirectional, thus resulting in a symmetric matrix Wres . There has been another network model referred to as the Erdo\u030bs\u2013R\u00e9nyi model32, the primary difference between this model and the Watts\u2013Strogatz graph is in the number of parameters, wherein the former contains one parameter, the node degree, while the latter has two parameters, the node degree and the probability of rewiring."
        },
        {
            "heading": "Results",
            "text": "In the Watts\u2013Strogatz graph, the clustering coefficient of node i with the node degree ki is defined as33\nFigure\u00a02. Architectures of 10-node ( N = 10 ) networks with the node degree k = 2 ; regularly connected ( p = 0 ) on the l.h.s. on the top, modestly disordered ( p = 0.5 ) on the r.h.s. on the top, and totally disordered ( p = 1 ) at the bottom. Each node is connected with four (= 2k) neighboring nodes. Each table represents the pairs of nodes for each p . For example, node#1 is connected with nodes#2, 3, 9, and 10 for the case with p = 0.\n4 Vol:.(1234567890) Scientific Reports | (2022) 12:16697 | https://doi.org/10.1038/s41598-022-21235-y\nand the total clustering coefficient of N-node network is expressed by\nThe density of the weight matrix Wres of the small-world network-based reservoir is calculated by\nThe characteristic path length L(p) is calculated as the mean value of distances of the shortest paths between all the nodes. The clustering coefficient C(p)/C(0) and the characteristic path lengths L(p)/L(0) as a function of the probability of rewiring p for the case with 1000-node and the node degrees, k = 4 and 6 are shown in Fig.\u00a03a. As p increases, the clustering coefficient rapidly decreases beyond p > 0.01 , and the average path length also monotonically decreases. The small-world network is indicated by the shaded area of p = 0.01\u20130.7, which is characterized highly-clustered with the relatively short path length. As shown in Supple-Fig.\u00a01, there are several clustering hubs for the case with the 1000-node network of ( k, p )\n= (4, 0.5) , which connect with up to 14 ~ 16 nodes are indicated in orange and yellow of the color bar. When the degree k is greater than 20, the small-world characteristic of highly-clustered with the relatively short path length is almost lost (Fig.\u00a03b). Based on this observation, the degree k up to around 20 preserves the characteristic of the small-world.\nHereafter, we will introduce the small-world network to the RC and synthesize the reservoir accordingly. We will investigate how the small-world network acts as the reservoir by comparing with a conventional sparsely random weight matrix Wres . We investigate the performance of the RC by processing temporal and serial data. The recognition of images such as handwritten digits and letters are beyond the scope of this study because they require a peculiar preprocessing of image deformation techniques such as reframing and resizing9. We implement two benchmark tests; classification of human activity34 and time series prediction of Mackey\u2013Glass (MG) chaotic signal35. We first study the dependence of the probability of rewiring p on both the performance of the classification of human activity and prediction of the MG chaotic signal shown in Fig.\u00a04a,b, respectively. The human activity includes six motions\u2014walking, walking upstairs, walking downstairs, sitting, standing, and lying down (Fig.\u00a05a). The motions along x-, y-, and z-axes are captured by the accelerator of a smartphone as shown on the bottom of in Fig.\u00a04a34. The 1000-node reservoir Wres is generated for the degree k = 4 . The impact of the degree k on the performance will be discussed later (Fig.\u00a07). In all the benchmark tests, the reservoir network is generated for ten time, and each of ten test runs is conducted by using a different reservoir. The classification accuracy of the human 6-activity improves as p increases from 65.3% at p = 0.0001 and peaks out to 74.9% at p = 0.5 (Fig.\u00a04a). While, in the mean square error (MSE) of the MG chaotic signal prediction (Fig.\u00a04b), a 1000- node reservoir Wres is also generated with the degree k = 4 . The prediction accuracy, which is represented by the MSE is minimized to be 4.98\u00d7 10\u22126 at p = 0.1 . From Fig.\u00a04, it is observed that the sweet spot of the optimum performance of these tests is situated in the range of small-world network, the shaded area of p = 0.01\u20130.7. The hyperbolic tangent is used for the activation function f in Eq.\u00a0(1) throughout this study, and sets of parameters such the leaking rate \u03b1 and input gain \u03b3 in Eq.\u00a0(1) and the regularization parameter in Eq.\u00a0(3) are tuned for\n(4)Ci = Number of triangles involving nodei\nNumber of links at mostki(ki \u2212 1)/2 ,\n(5)C = 1\nN\nN \u2211\ni=1\nCi .\n(6)Density = nonzeroWres elements\nN2 =\nN \u00d7 2k\nN2 =\n2k N .\nFigure\u00a03. a Clustering coefficient C(p)/C(0) and average path length or hop count L(p)/L(0) vs. probability of rewiring p for the case with 1000-node ( N = 1000 ) and the node degrees k = 4 and 6. Range roughly p = 0.01\u20130.7 of small-world is indicated by the shaded area. b Clustering coefficients C(p)/C(0) and average path lengths L(p)/L(0) vs. probability of rewiring p for the case with the node degrees k = 20 and 100.\n5 Vol.:(0123456789) Scientific Reports | (2022) 12:16697 | https://doi.org/10.1038/s41598-022-21235-y\nFigure\u00a04. (a) Classification accuracy of human 6-activity vs. probability of rewiring p for the case with N = 1000 and k = 4 . The accuracy is maximized to be 74.9% at p = 0.5 . For an example, the temporal waveforms of accelerations of walking on x-, y, and z-axes are also shown on the bottom. (b) Prediction accuracy represented by the mean square error (MSE) of MG chaotic signal vs. p for the case with N = 1000 and k = 4 . MSE is minimized to be 4.98\u00d7 10\u22126 at p = 0.1.\nFigure\u00a05. Confusion matrices of human 6-activity classification are compared for 1000-node reservoir. (a) Reservoir weight matrix Wres of small-world network ( k, p )\n= (4, 0.5) . Accuracy (in green) is 75.2%. (b) Conventional sparsely random matrix Wres with the density of 0.008. Accuracy is 73.0%.\n6 Vol:.(1234567890) Scientific Reports | (2022) 12:16697 | https://doi.org/10.1038/s41598-022-21235-y\nthe optimum performance. For the case with the node count N = 1000 , the typical values of the leaking rate \u03b1 , the input gain \u03b3 , and the regularization parameter are (\u03b1, \u03b3 , ) = (0.3, 1.0, 1.0\u00d7 10\u22128) for the human motion classification and (0.7, 1.0, 0.2\u00d7 10\u22128) for the MG signal prediction. The results of 2000-node will be discussed later in the manuscript.\nWe will present the results of the two benchmark tests in details. First, in the human activity classification the captured temporal waveforms along x-, y-, and z-axes for 2.56\u00a0s were sampled into 128-sample at a rate of 20\u00a0ms as shown on the bottom of Fig.\u00a04a. The data sets of the training and the testing include 7352 and 2947, respectively (see \u201cMethods\u201d section). The accuracy of the human activity classification in the confusion matrix is maximized to 75.2% when ( k, p )\n= (4, 0.5) (Fig.\u00a05a), and it is slightly better than that of conventional random weight matrix (73.0%) with the same density of 0.008 as the 1000-node small-world network, which is calculated from Eq.\u00a0(6) (Fig.\u00a05b).\nIn the benchmark test of the MG temporal chaotic signal, a 10,000 timestep-long signal is used. The first 2000 of data are used for the training, and the output weight matrix Wout is determined by Eq.\u00a0(3) (see \u201cMethods\u201d section). Then, the next 2000 of the data are used for the prediction. The reservoir consists of 1000-node. The plots of waveforms provide a comparison of the RC results using small-world network as the reservoir with that of sparsely random weight matrix (Fig.\u00a06a,b). The results are summarized in Table\u00a01, including the minimum/ maximum mean square errors (MSEs) along with the mean values and the standard deviations for the 10-run. The best MSE of the 1000-node small-world is as low as 2.46\u00d7 10\u22126 (Fig.\u00a06a), slightly larger compared to 1.38\u00d7 10\u22126 of the random weight matrix with the density of 0.008, which is equal to the density of small-world reservoir (Fig.\u00a06b). It can be confirmed that the optimum performance is obtained in the range of small-world as the classification of human activity does. It should be noted that the standard deviation was 1.42\u00d7 10\u22126 which\nFigure\u00a06. 2000 timestep-long waveforms of predicted and that of target MG chaotic time series for 1000-node reservoir. (a) Result of reservoir weight matrix Wres of 1000-node small-world network ( k, p )\n= (4, 0.1) . Mean square error (MSE) is 2.46\u00d7 10\u22126 . (b) Conventional sparsely random matrix Wres with the density of 0.008. MSE is 1.38\u00d7 10\u22126 . Results of the two benchmark tests are summarized in Table\u00a01.\n7 Vol.:(0123456789) Scientific Reports | (2022) 12:16697 | https://doi.org/10.1038/s41598-022-21235-y\namounts to 28.5% of the MSE value. The impact of the length of training data is examined using the 1000 and the 3000 timestep-long training data, and it is confirmed that the 2000 timestep-long training data is sufficient. For instance, the prediction accuracy was 2.07\u00d7 10\u22126 when the 3000 training data are used, while it is 3.88\u00d7 10\u22125 for the case with the 1000 training data.\nThe results of two benchmark tests with the case of the 2000-node are also summarized in Table\u00a01. The confusion matrix of the classification accuracy of human 6-activity and the waveforms of predicted MG chaotic time series are shown in Supple-Figs.\u00a02 and 3, respectively. The performance is considerably improved compared to the results of N = 1000 . The mean value of classification accuracy is increased to 77.4% for the small-world of (\nk, p )\n= (4, 0.5) , and it outperforms the sparsely random weight matrix, whose accuracy is 75.4%. The density of the weight matrix is 0.004, which is equal to the density of small-world reservoir. In the chaotic signal prediction, the mean value of MSE of the small-world reservoir of ( k, p )\n= (4, 0.1) is 6.01\u00d7 10\u22128 , better than that of random weight matrix, whose mean MSE is 9.00\u00d7 10\u22128.\nFinally, we investigate the RC performance against the parameters of the small-world network such as the number of nodes N and the degree of node k . We will focus on the small-world range p = 0.1\u20130.7, which is indicated by the shaded area in Fig.\u00a03a, and hence, we assume ( k, p )\n= (4, 0.5) . First, the dependence of the classification accuracy of the human 6-activity on N is investigated (Fig.\u00a07a). The classification accuracy monotonically improves as the network scales up. The mean value is 54.4% at N = 50 , and it continues to increase to 77.4% at N = 2000 . Next, we will observe the impact of the node degree k on the classification accuracy (Fig.\u00a07b). The accuracy is maximized with relatively small number of degrees around 2 \u2264 k \u2264 4 , and it monotonically degrades as the degree k is increased."
        },
        {
            "heading": "Discussion",
            "text": "We have conducted two benchmark tests\u2014the classification of human activity and the time series prediction of the Mackey\u2013Glass chaotic system. It has been demonstrated that the optimum performance is obtained from the reservoir in the range of small-world network bounded by p =0.01\u20130.7 and k < 20 . Based on this observation, a guiding principle has been presented to systematically synthesize a reservoir of RC by exploring the smallworld network nature of highly-clustered with the short characteristic path length. We expect that this study will draw attention to the versatile capability of the small-world network in the research of neural networks and help understand architectures of the RC in-depth."
        },
        {
            "heading": "Methods",
            "text": "Generation of weight matrix of reservoir. The method of generating the weight matrix Wres of the reservoir of 10-node ( N = 10 ) network with the node degree k = 2 is illustrated (Fig.\u00a08). The table on the l.h.s. represents the pairs of connected nodes for p = 0.5 . From this table, the weight matrix Wres of the reservoir for the bidirectional connection can be generated, as shown on the r.h.s. Throughout the simulation, the nonzero elements of Wres take a binary value, and it is assumed that all the connections are bidirectional, resulting in a symmetric weight matrix. Although, the nonzero elements can take an arbitrary positive value, and the asymmetric matrix may be another option.\nFigure\u00a07. Performance of human activity classification using reservoir weight matrix Wres generated from the Watts\u2013Strogatz graph. (a) Classification accuracy versus the number of nodes N . Node degree k = 4 and the probability of rewiring p = 0.5 within the range of small-world network are assumed. (b) Classification accuracy against the degree of node k . p = 0.5 of 1000-node small-world network is assumed. Horizontal axis on the top is density of matrix Wres calculated by Eq.\u00a0(6).\n8 Vol:.(1234567890) Scientific Reports | (2022) 12:16697 | https://doi.org/10.1038/s41598-022-21235-y\nGeneration algorithm of Watts\u2013Strogatz graph12. Creating the Watts\u2013Strogatz graph through two basic steps:\n1. Create a ring lattice with N-node of the mean degree 2 k (on the l.h.s. on the top of Fig.\u00a02). Each node is connected to its nearest neighboring 2 k nodes. 2. For each edge or link in the graph, rewire the target node with probability p . The rewired edge cannot be a duplicate or self-loop. This results in a partially ( p = 0.5 ) or totally disordered ( p = 1 ) topologies (on the r.h.s. on the top and at the bottom of Fig.\u00a02, respectively).\nThe basic Matlab code is available at: https:// jp. mathw orks. com/ help/ matlab/ math/ build- watts- strog atz- smallworld- graph- model. html? lang= en. The clustering coefficient C(p) is calculated using Eqs. (4) and (5) by following the aglgorithm33. The Matlab code of C(p) is available at https:// github. com/ mdhum phries/ Small World Ness.\nClassification of human activity34. The data set of the human activity is available at http:// archi ve. ics. uci. edu/ ml/ machi ne- learn ing- datab ases/ 00240/ UCI HAR Dataset.zip.\nIt includes six motions\u2014walking, walking upstairs, walking downstairs, sitting, standing, and laying down. The captured temporal waveforms along x-, y-, and z-axes for 2.56\u00a0s were sampled into 128-sample at the rate of 20\u00a0ms. The data sets of the training and the testing are 7352 and 2947, respectively. For the training and classification of the human activity in RC (Fig.\u00a01), the dimensions of weight matrices Win , Wres , and Wout of Eqs. (1), (2) and (3) are L = 1,M = 6 and N is specified, for instance 1000 etc.\nTime series prediction of Mackey\u2013Glass (MG) chaotic signal35. The time series data and the basic Matlab code are available at:\nFigure\u00a08. Method for generating the weight matrix Wres of the reservoir of 10-node ( N = 10 ) networks with (\nk, p )\n= (2, 0.5) . Table of pairs of connected nodes on the l.h.s. and 10\u00d7 10 weight matrix Wres . For instance, connections of node 1, pairs of nodes, (1, 6) and (1, 8) reflect on the weight matrix Wres , as marked by circles.\n9 Vol.:(0123456789) Scientific Reports | (2022) 12:16697 | https://doi.org/10.1038/s41598-022-21235-y\nA minimalistic sparse Echo State Networks demo with Mackey\u2013Glass (delay 17) data in \"plain\" Matlab/Octave from https:// mantas. info/ code/ simple_ esn (c) 2012\u20132020 Mantas Lukosevicius.\nDistributed under MIT license https:// opens ource. org/ licen ses/ MIT. For the training and prediction of MG chaotic time series, the dimensions of weight matrices Win , Wres , and\nWout of Eqs. (1), (2) and (3) are L = 1,M = 1 and N is specified, for instance 1000 etc."
        },
        {
            "heading": "Data availability",
            "text": "The raw data sets used in the simulations are available (see \u201cMethods\u201d)."
        },
        {
            "heading": "Code availability",
            "text": "The Matlab-based codes used for simulations are partly available (see \u201cMethods\u201d).\nReceived: 13 March 2022; Accepted: 26 September 2022"
        },
        {
            "heading": "Acknowledgements",
            "text": "We would like to thank for invaluable discussions H. Furukawa and S. Shimizu of the National Institute of Information and Communications Technology, T. Hara and H. Toyoda of Hamamatsu Photonics Central Laboratory, and K. Ishii of the Graduate School for the Creation of New Photonic Industries. We would also like to thank the anonymous reviewers for their constructive comments and advices."
        },
        {
            "heading": "Author contributions",
            "text": "100% of contribution owes the sole author."
        },
        {
            "heading": "Competing interests",
            "text": "The author declares no competing interests."
        },
        {
            "heading": "Additional information",
            "text": "Supplementary Information The online version contains supplementary material available at https:// doi. org/ 10. 1038/ s41598- 022- 21235-y.\nCorrespondence and requests for materials should be addressed to K.K.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher\u2019s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or\nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article\u2019s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article\u2019s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n\u00a9 The Author(s) 2022"
        }
    ],
    "title": "Guiding principle of reservoir computing based on \u201csmall-world\u201d network",
    "year": 2022
}