{
    "abstractText": "Noisy-OR Bayesian Networks (BNs) are a family of probabilistic graphical models which express rich statistical dependencies in binary data. Variational inference (VI) has been the main method proposed to learn noisy-OR BNs with complex latent structures (Jaakkola and Jordan, 1999; Ji et al., 2020; Buhai et al., 2020). However, the proposed VI approaches either (a) use a recognition network with standard amortized inference that cannot induce \u201cexplaining-away\u201d; or (b) assume a simple mean-field (MF) posterior which is vulnerable to bad local optima. Existing MF VI methods also update the MF parameters sequentially which makes them inherently slow. In this paper, we propose parallel max-product as an alternative algorithm for learning noisy-OR BNs with complex latent structures and we derive a fast stochastic training scheme that scales to large datasets. We evaluate both approaches on several benchmarks where VI is the state-of-the-art and show that our method (a) achieves better test performance than Ji et al. (2020) for learning noisy-OR BNs with hierarchical latent structures on large sparse real datasets; (b) recovers a higher number of ground truth parameters than Buhai et al. (2020) from cluttered synthetic scenes; and (c) solves the 2D blind deconvolution problem from Lazaro-Gredilla et al. (2021) and variants\u2014including binary matrix factorization\u2014while VI catastrophically fails and is up to two orders of magnitude slower.",
    "authors": [
        {
            "affiliations": [],
            "name": "Antoine Dedieu"
        },
        {
            "affiliations": [],
            "name": "Guangyao Zhou"
        },
        {
            "affiliations": [],
            "name": "Dileep George"
        },
        {
            "affiliations": [],
            "name": "Miguel L\u00e1zaro-Gredilla"
        }
    ],
    "id": "SP:714e506ac1dc6c6682f43cc1e5cc1fffdbf1ce66",
    "references": [
        {
            "authors": [
                "Vanhoucke",
                "Vijay Vasudevan",
                "Fernanda Vi\u00e9gas",
                "Oriol Vinyals",
                "Pete Warden",
                "Martin Wattenberg",
                "Martin Wicke",
                "Yuan Yu",
                "Xiaoqiang Zheng"
            ],
            "title": "TensorFlow: Large-scale machine learning on heterogeneous systems",
            "year": 2015
        },
        {
            "authors": [
                "Christopher M Bishop",
                "Nasser M Nasrabadi"
            ],
            "title": "Pattern recognition and machine learning, volume",
            "year": 2006
        },
        {
            "authors": [
                "James Bradbury",
                "Roy Frostig",
                "Peter Hawkins",
                "Matthew James Johnson",
                "Chris Leary",
                "Dougal Maclaurin",
                "George Necula",
                "Adam Paszke",
                "Jake VanderPlas",
                "Skye Wanderman-Milne",
                "Qiao Zhang"
            ],
            "title": "JAX: composable transformations of Python+NumPy programs, 2018",
            "venue": "URL http://github.com/ google/jax",
            "year": 2018
        },
        {
            "authors": [
                "Rares-Darius Buhai",
                "Yoni Halpern",
                "Yoon Kim",
                "Andrej Risteski",
                "David Sontag"
            ],
            "title": "Empirical study of the benefits of overparameterization in learning latent variable models",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Amir Globerson",
                "Gal Chechik",
                "Fernando Pereira",
                "Naftali Tishby"
            ],
            "title": "Euclidean embedding of cooccurrence data",
            "venue": "Advances in neural information processing systems,",
            "year": 2004
        },
        {
            "authors": [
                "Yonatan Halpern",
                "David Sontag"
            ],
            "title": "Unsupervised learning of noisy-or bayesian networks",
            "venue": "arXiv preprint arXiv:1309.6834,",
            "year": 2013
        },
        {
            "authors": [
                "Tommi S Jaakkola",
                "Michael I Jordan"
            ],
            "title": "Variational probabilistic inference and the qmr-dt network",
            "venue": "Journal of artificial intelligence research,",
            "year": 1999
        },
        {
            "authors": [
                "Geng Ji",
                "Dehua Cheng",
                "Huazhong Ning",
                "Changhe Yuan",
                "Hanning Zhou",
                "Liang Xiong",
                "Erik B Sudderth"
            ],
            "title": "Variational training for large-scale noisy-or bayesian networks",
            "venue": "In Uncertainty in Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Max Welling"
            ],
            "title": "Auto-encoding variational bayes",
            "venue": "arXiv preprint arXiv:1312.6114,",
            "year": 2013
        },
        {
            "authors": [
                "Daphne Koller",
                "Nir Friedman"
            ],
            "title": "Probabilistic graphical models: principles and techniques",
            "venue": "MIT press,",
            "year": 2009
        },
        {
            "authors": [
                "Miguel Lazaro-Gredilla",
                "Antoine Dedieu",
                "Dileep George"
            ],
            "title": "Perturb-and-max-product: Sampling and learning in discrete energy-based models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Jialu Liu",
                "Xiang Ren",
                "Jingbo Shang",
                "Taylor Cassidy",
                "Clare R Voss",
                "Jiawei Han"
            ],
            "title": "Representing documents via latent keyphrase inference",
            "venue": "In Proceedings of the 25th international conference on World wide web,",
            "year": 2016
        },
        {
            "authors": [
                "Kevin Murphy",
                "Yair Weiss",
                "Michael I Jordan"
            ],
            "title": "Loopy belief propagation for approximate inference: An empirical study",
            "venue": "arXiv preprint arXiv:1301.6725,",
            "year": 2013
        },
        {
            "authors": [
                "George Papandreou",
                "Alan L Yuille"
            ],
            "title": "Perturb-and-map random fields: Using discrete optimization to learn and sample from energy models",
            "venue": "In 2011 International Conference on Computer Vision,",
            "year": 2011
        },
        {
            "authors": [
                "Judea Pearl"
            ],
            "title": "Probabilistic reasoning in intelligent systems: networks of plausible inference",
            "venue": "Morgan kaufmann,",
            "year": 1988
        },
        {
            "authors": [
                "F. Pedregosa",
                "G. Varoquaux",
                "A. Gramfort",
                "V. Michel",
                "B. Thirion",
                "O. Grisel",
                "M. Blondel",
                "P. Prettenhofer",
                "R. Weiss",
                "V. Dubourg",
                "J. Vanderplas",
                "A. Passos",
                "D. Cournapeau",
                "M. Brucher",
                "M. Perrot",
                "E. Duchesnay"
            ],
            "title": "Scikit-learn: Machine learning in Python",
            "venue": "Journal of Machine Learning Research,",
            "year": 2011
        },
        {
            "authors": [
                "Siamak Ravanbakhsh",
                "Barnab\u00e1s P\u00f3czos",
                "Russell Greiner"
            ],
            "title": "Boolean matrix factorization and noisy completion via message passing",
            "venue": "In International Conference on Machine Learning,",
            "year": 2016
        },
        {
            "authors": [
                "Herbert Robbins",
                "Sutton Monro"
            ],
            "title": "A stochastic approximation method",
            "venue": "The annals of mathematical statistics,",
            "year": 1951
        },
        {
            "authors": [
                "Tom\u00e1\u0161 \u0160ingliar",
                "Milo\u0161 Hauskrecht"
            ],
            "title": "Noisy-or component analysis and its application to link analysis",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2006
        },
        {
            "authors": [
                "Martin J Wainwright",
                "Michael I Jordan"
            ],
            "title": "Graphical models, exponential families, and variational inference",
            "venue": "Foundations and Trends\u00ae in Machine Learning,",
            "year": 2008
        },
        {
            "authors": [
                "Yair Weiss"
            ],
            "title": "Belief propagation and revision in networks with loops",
            "year": 1997
        },
        {
            "authors": [
                "Guangyao Zhou",
                "Antoine Dedieu",
                "Nishanth Kumar",
                "Miguel L\u00e1zaro-Gredilla",
                "Shrinu Kushagra",
                "Dileep George"
            ],
            "title": "Pgmax: Factor graphs for discrete probabilistic graphical models and loopy belief propagation in jax",
            "venue": "arXiv preprint arXiv:2202.04110,",
            "year": 2022
        },
        {
            "authors": [
                "Globerson"
            ],
            "title": "Therefore, if the jth and kth words are independent, the limit of R jk in the case of an infinite amount of data is 1. If the limit of R jk is higher than 1, then p(x j = 1, xk = 1) is higher than the case where the variables are independent",
            "year": 2004
        },
        {
            "authors": [
                "G. MP"
            ],
            "title": "Additional material for the overparametrization experiment This section contains some additional materials for the overparametrization experiment presented in Section 6.5. First, we discuss the method proposed in Buhai et al. (2020) to compute the number of GT parameters recovered during training",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "2023-2-2\nLearning noisy-OR Bayesian Networks with Max-Product Belief Propagation Antoine Dedieu1, Guangyao Zhou1, Dileep George1 and Miguel L\u00e1zaro-Gredilla1 1DeepMind\nNoisy-OR Bayesian Networks (BNs) are a family of probabilistic graphical models which express rich statistical dependencies in binary data. Variational inference (VI) has been the main method proposed to learn noisy-OR BNs with complex latent structures (Jaakkola and Jordan, 1999; Ji et al., 2020; Buhai et al., 2020). However, the proposed VI approaches either (a) use a recognition network with standard amortized inference that cannot induce \u201cexplaining-away\u201d; or (b) assume a simple mean-field (MF) posterior which is vulnerable to bad local optima. Existing MF VI methods also update the MF parameters sequentially which makes them inherently slow. In this paper, we propose parallel max-product as an alternative algorithm for learning noisy-OR BNs with complex latent structures and we derive a fast stochastic training scheme that scales to large datasets. We evaluate both approaches on several benchmarks where VI is the state-of-the-art and show that our method (a) achieves better test performance than Ji et al. (2020) for learning noisy-OR BNs with hierarchical latent structures on large sparse real datasets; (b) recovers a higher number of ground truth parameters than Buhai et al. (2020) from cluttered synthetic scenes; and (c) solves the 2D blind deconvolution problem from Lazaro-Gredilla et al. (2021) and variants\u2014including binary matrix factorization\u2014while VI catastrophically fails and is up to two orders of magnitude slower."
        },
        {
            "heading": "1. Introduction",
            "text": "Probabilistic graphical models (PGMs) propose a rigorous and elegant way to represent the full joint probability density function of high-dimensional data and to express assumptions about its hidden structure. Learning and inference algorithms let us analyze data under those assumptions and recover the hidden structure that best explains our observations. However, performing exact inference in complex PGMs is often intractable. To mitigate this problem, several techniques have been proposed for approximate inference, among which a popular one is variational inference (VI) (Wainwright et al., 2008; Bishop and Nasrabadi, 2006). In this paper, we consider directed acyclic PGMs\u2014also named Bayesian networks (BNs)\u2014with binary variables and noisy-OR conditional distribution (Pearl, 1988). The resulting noisy-OR BNs have been used for medical diagnosis (Jaakkola and Jordan, 1999), data compression (\u0160ingliar and Hauskrecht, 2006), text mining (Liu et al., 2016), and more recently overparametrized learning (Buhai et al., 2020) and topic modeling on large sparse datasets (Ji et al., 2020). Noisy-OR BNs have an intractable posterior: most of the aforementioned applications rely on VI for approximate inference. Some existing VI methods (Buhai et al., 2020) use a recognition network and amortize the approximate inference via a single forward pass, which cannot induce \u201cexplaining-away\u201d (see Section 2). In contrast, Jaakkola and Jordan (1999); \u0160ingliar and Hauskrecht (2006); Ji et al. (2020) assume a mean-field (MF) posterior, which is vulnerable to bad local optima. These existing MF methods also update the MF parameters sequentially\u2014i.e. one by one\u2014which is prohibitively slow. To scale MF VI, Ji et al. (2020) propose a local heuristic that updates fewer MF parameters (see Section 3). However, their approach only applies to large sparse datasets (i.e. most of the observations are 0s). In this work, we propose a fast and efficient stochastic scheme for learning noisy-OR BNs that use\nCorresponding author(s): adedieu@deepmind.com \u00a9 2023 DeepMind. All rights reserved\nar X\niv :2\n30 2.\n00 09\n9v 1\n[ cs\n.L G\n] 3\n1 Ja\nn 20\n23\nthe parallel max-product (MP) algorithm (Pearl, 1988; Murphy et al., 2013) as an alternative to VI. Similar to Ji et al. (2020), our method supports multi-layered noisy-OR networks and relies on stochastic optimization (Robbins and Monro, 1951) for scaling. However, (a) contrary to Ji et al. (2020), our approach runs in parallel which allows it to scale to large dense datasets; and (b) in contrast with Buhai et al. (2020), our method induces explaining-away. We show that our approach efficiently explores the parameters space, which allows better performance on the experiments of Ji et al. (2020). We additionally show that several challenging problems including (a) binary matrix factorization; (b) the noisy-OR BNs experiments from Buhai et al. (2020); and (c) the complex 2D blind deconvolution problem from Lazaro-Gredilla et al. (2021) can be expressed as learning problems in noisy-OR BNs, for which MP outperforms VI while being up to two orders of magnitude faster. Our code is written in JAX (Bradbury et al., 2018) and will be made public after publication. The rest of this paper is organized as follows. Section 2 reviews noisy-OR BNs, while Section 3 discusses existing learning methods for these models. Section 4 introduces the max-product algorithm, which Section 5 integrates into our training scheme for BNs. Finally, Section 6 compares our method with VI in a wide variety of experiments."
        },
        {
            "heading": "2. Noisy-OR Bayesian networks",
            "text": "Given binary observations \ud835\udc65 \u2208 {0, 1}\ud835\udc5d, we model its statistical dependencies using binary BNs (Koller and Friedman, 2009) as in Figure 1. The nodes in the graph are divided into \ud835\udc5d visible nodes\u2014which are the leaves\u2014and \ud835\udc5a hidden nodes. Each visible (resp. hidden) node \ud835\udc56 is associated with a binary random variable \ud835\udc65\ud835\udc56 (resp. \u210e\ud835\udc56). We denote \u210e = (\u210e1, . . . , \u210e\ud835\udc5a) and \ud835\udc65 = (\ud835\udc651, . . . , \ud835\udc65\ud835\udc5d). Similar to Ji et al. (2020), we introduce a leak node 0 that connects to all the nodes, whose variable \ud835\udc670 is always active, i.e. \ud835\udc670 = 1. The leak node allows any active variable to be explained by other factors than its parents. For convenience, we denote \ud835\udc67 = (\ud835\udc670, \u210e, \ud835\udc65) the vector of all variables: \u210e = (\ud835\udc671, . . . , \ud835\udc67\ud835\udc5a) and \ud835\udc65 = (\ud835\udc67\ud835\udc5a+1, . . . \ud835\udc67\ud835\udc5a+\ud835\udc5d). Let P(\ud835\udc56) be the set of parents (excluding the leak node) of the node \ud835\udc56 \u2265 1. The activation probability of the variable \ud835\udc67\ud835\udc56 is given by the noisy-OR conditional distribution\n\ud835\udc5d(\ud835\udc67\ud835\udc56 = 0 | \ud835\udc67P(\ud835\udc56) , \u0398) = exp ( \u2212\ud835\udf030\u2192\ud835\udc56 \u2212 \u2211\ufe01 \ud835\udc58\u2208P(\ud835\udc56) \ud835\udf03\ud835\udc58\u2192\ud835\udc56\ud835\udc67\ud835\udc58 ) (1)\nwhere \ud835\udf030\u2192\ud835\udc56 \u2265 0, \ud835\udf03\ud835\udc58\u2192\ud835\udc56 \u2265 0, \u2200\ud835\udc58 \u2208 P(\ud835\udc56) and \u0398 is the vector collecting all these parameters. This conditional distribution possesses three important properties. First, if all the parents are inactive, the activation probability is given by the leak node: \ud835\udc5d(\ud835\udc67\ud835\udc56 = 0 | \ud835\udc67P(\ud835\udc56) = 0, \u0398) = exp(\u2212\ud835\udf030\u2192\ud835\udc56). As in Buhai et al. (2020), we refer to 1 \u2212 exp(\u2212\ud835\udf030\u2192\ud835\udc56) as the \u201cprior probability\u201d when P(\ud835\udc56) is empty and the \u201cnoise probability\u201d otherwise. Second, if only the variable \ud835\udc58 is connected to the variable \ud835\udc56 and there is no leak node, \ud835\udc5d(\ud835\udc67\ud835\udc56 = 0 | \ud835\udc67\ud835\udc58 = 1, \u0398) = exp(\u2212\ud835\udf03\ud835\udc58\u2192\ud835\udc56)\u2014which we refer to as the \u201cfailure probability\u201d. Finally, noisy-OR BNs can induce \u201cexplaining-away\u201d: explaining-away creates competition between a-priori unlikely causes, which allows inference to pick the smallest subset of causes that explain the effects."
        },
        {
            "heading": "3. Related Work",
            "text": "The QMR-DT network (Jaakkola and Jordan, 1999) is one of the first models which exploits the properties of noisy-OR BNs. It consists of a two-layer bipartite graph created by domain experts which models how 600 diseases explain 4, 000 findings. The probability of a finding given diseases is expressed by Equation (1). After learning, the QMR-DT network is used to infer the probabilities of different diseases given a set of observed symptoms. For approximate inference in the intractable noisy-OR BN, the authors assumed a MF posterior\u2014which can induce explaining-away (see Section 2)\u2014and introduced a family of variational bounds as well as a heuristic to increase the graph sparsity. Other approaches have been proposed for learning bipartite noisy-OR BNs. \u0160ingliar and Hauskrecht (2006) introduced a variational EM procedure that exploits the bounds of Jaakkola and Jordan (1999) while assuming a fully connected graph. Halpern and Sontag (2013) proposed a method of moments that requires the graph to be sparse. Liu et al. (2016) introduced a Monte-Carlo EM algorithm that requires a large number of sampling steps for good performance. None of these methods would scale to large datasets. Recently, Buhai et al. (2020) discussed the effect of overparameterization in PGMs and showed that, on synthetic datasets, increasing the number of latent variables of noisy-OR BNs improves their performance at recovering the ground truth parameters. Their method considered VI with a recognition network. However, the authors amortize the inference via a single forward pass: inference results in picking all causes that are consistent with the effects and cannot induce explaining-away (see Section 2). In another recent work, Ji et al. (2020) proposed a stochastic variational training algorithm for noisy-OR BNs. The authors assumed a MF posterior and extended the bounds of Jaakkola and Jordan (1999). For scalability, the authors introduced \u201clocal models\u201d: they only update the variational parameters associated with the ancestors of the active visible variables. They showed that this is equivalent to optimizing a constrained variational bound, and derived state-of-the-art performance for multi-layered BNs on large sparse real datasets, while significantly outperforming Liu et al. (2016). The method we propose in Section 5 for learning noisy-OR BNs has the same appealing properties as Ji et al. (2020): it induces explaining-away, it supports multi-layered graphs and it scales to large sparse datasets. In addition, (a) it is faster as it runs parallel max-product; (b) it also scales to large dense datasets; and (c) it considers a richer posterior than MF VI which allows it to find better local optima."
        },
        {
            "heading": "4. Background on max-product",
            "text": "We first review the parallel max-product algorithm. We then discuss how this algorithm can be used for sampling in PGMs, and how it can be easily accelerated on GPUs."
        },
        {
            "heading": "4.1. Max-product message passing",
            "text": "We consider a PGM with variables \ud835\udc67 described by a set of \ud835\udc34 factors {\ud835\udf13>\ud835\udc4e \ud835\udf19\ud835\udc4e(\ud835\udc67\ud835\udc4e)}\ud835\udc34\ud835\udc4e=1 and \ud835\udc3c unary terms {\ud835\udf06>\n\ud835\udc56 \ud835\udf02\ud835\udc56 (\ud835\udc67\ud835\udc56)}\ud835\udc3c\ud835\udc56=1. \ud835\udc67\ud835\udc4e is the vector of variables used in the factor \ud835\udc4e, \ud835\udf13\ud835\udc4e is a vector of factor parameters and \ud835\udf19\ud835\udc4e(\ud835\udc67\ud835\udc4e) is a vector of factor sufficient statistics. For the unary terms, the sufficient statistics are the indicator functions \ud835\udf02\ud835\udc56 (\ud835\udc65\ud835\udc56) = (1(\ud835\udc65\ud835\udc56 = 0), 1(\ud835\udc65\ud835\udc56 = 1)). For a Bayesian network, a factor involving \ud835\udc67\ud835\udc4e = (\ud835\udc67\ud835\udc4e, \ud835\udc67P(\ud835\udc4e) , \ud835\udc670) is defined for the \ud835\udc4eth variable. The corresponding \ud835\udf13\ud835\udc4e can be derived from the parameters {\ud835\udf030\u2192\ud835\udc4e} \u222a { \ud835\udf03\ud835\udc58\u2192\ud835\udc4e } \ud835\udc58\u2208P(\ud835\udc4e) defined in Equation (1).\nThe energy of the model can be expressed as \ud835\udc38(\ud835\udc67) = \u2212\u2211\ud835\udc34\ud835\udc4e=1 \ud835\udf13>\ud835\udc4e \ud835\udf19\ud835\udc4e(\ud835\udc67\ud835\udc4e) \u2212 \u2211\ud835\udc3c\ud835\udc56=1 \ud835\udf06>\ud835\udc56 \ud835\udf02\ud835\udc56 (\ud835\udc67\ud835\udc56) or, collecting the parameters and sufficient statistics in corresponding vectors, \ud835\udc38(\ud835\udc67) = \u2212\u03a8>\u03a6(\ud835\udc67) \u2212 \u039b>\ud835\udf02(\ud835\udc67). The probability of a configuration \ud835\udc67 satisfies \ud835\udc5d(\ud835\udc67) \u221d exp(\u2212\ud835\udc38(\ud835\udc67)). The maximum a posteriori (MAP) problem consists in finding the variable assignment with the lowest energy, that is\n\ud835\udc67MAP \u2208 argmin \ud835\udc67 \ud835\udc38(\ud835\udc67) = argmax \ud835\udc67 \u03a8>\u03a6(\ud835\udc67) + \u039b>\ud835\udf02(\ud835\udc67) (2)\nThe max-product algorithm estimates this solution by iterating the fixed-point updates for \ud835\udc41MP iterations:\n\ud835\udc5a\ud835\udc56\u2192\ud835\udc4e(\ud835\udc67\ud835\udc56) = \ud835\udf06>\ud835\udc56 \ud835\udf02\ud835\udc56 (\ud835\udc67\ud835\udc56) + \u2211\ufe01\n\ud835\udc4f\u2208nb(\ud835\udc56)\\\ud835\udc4e \ud835\udc5a\ud835\udc4f\u2192\ud835\udc56 (\ud835\udc67\ud835\udc56) (3)\n\ud835\udc5a\ud835\udc4e\u2192\ud835\udc56 (\ud835\udc67\ud835\udc56) = max \ud835\udc67\ud835\udc58\\\ud835\udc56\n{ \ud835\udf13>\ud835\udc4e \ud835\udf19\ud835\udc4e(\ud835\udc67\ud835\udc4e) + \u2211\ufe01 \ud835\udc58\u2208nb(\ud835\udc4e)\\\ud835\udc56 \ud835\udc5a\ud835\udc58\u2192\ud835\udc4e(\ud835\udc67\ud835\udc58) }\nwhere nb(\u00b7) denotes the neighbors of a factor or variable. Equations (3) are derived by setting the gradients of the Lagrangian of the Bethe free energy to 0\u2014see Wainwright et al. (2008). \ud835\udc5a\ud835\udc56\u2192\ud835\udc4e(\ud835\udc67\ud835\udc56) (resp. \ud835\udc5a\ud835\udc4e\u2192\ud835\udc56 (\ud835\udc67\ud835\udc56)) are called the \u201cmessages\u201d from variables to factors (resp. from factors to variables): max-product is a \u201cmessage-passing\u201d algorithm. After \ud835\udc41MP iterations of Equation (3), max-product estimates the solution to Problem (2) by\n\ud835\udc67\ud835\udc56 = argmax \ud835\udc50\n{ \ud835\udf06>\ud835\udc56 \ud835\udf02\ud835\udc56 (\ud835\udc67\ud835\udc56 = \ud835\udc50) + \u2211\ufe01 \ud835\udc4f\u2208nb(\ud835\udc56) \ud835\udc5a\ud835\udc4f\u2192\ud835\udc56 (\ud835\udc67\ud835\udc56 = \ud835\udc50) } , \u2200\ud835\udc56.\nMP is guaranteed to converge in trees like BNs (Weiss, 1997). A damping factor \ud835\udefc \u2208 (0, 1) in the updates can be used to improve convergence, so that \ud835\udc5anew\n\ud835\udc4e\u2192\ud835\udc56 (\ud835\udc67\ud835\udc56) = \ud835\udefc\ud835\udc5a\ud835\udc4e\u2192\ud835\udc56 (\ud835\udc67\ud835\udc56) + (1 \u2212 \ud835\udefc)\ud835\udc5aold\ud835\udc4e\u2192\ud835\udc56 (\ud835\udc67\ud835\udc56). \ud835\udefc = 0.5 offers a good trade-off between accuracy and speed in most cases. Max-product in BNs: The noisy-OR factor in Equation (1) connects the variables {\ud835\udc67\ud835\udc56} \u222a {\ud835\udc670} \u222a \ud835\udc67P(\ud835\udc56) and has 22+|P (\ud835\udc56) | valid configurations. At first sight, the max-product updates in Equations (3) have an exponential complexity in O(2 |P (\ud835\udc56) |). To scale to large factors, we derive in Appendix A an equivalent representation of this noisy-OR factor for which the updates have a linear complexity O(|P(\ud835\udc56) |)."
        },
        {
            "heading": "4.2. Sampling in PGMs via perturb-and-max-product",
            "text": "In this work, we are interested in answering two types of inference queries in PGMs: MAP queries as in Problem (2) and sampling queries. The perturb-and-MAP framework (Papandreou and Yuille, 2011) unifies these two types of queries by considering the problem:\nargmax \ud835\udc67\n{ \u03a8>\u03a6(\ud835\udc67) + (\u039b + \ud835\udc47 \ud835\udf00)>\ud835\udf02(\ud835\udc67) } (4) where \ud835\udf00 \u2208 \u211d2\ud835\udc3c is a perturbation vector added to the vector of unaries \u039b, and \ud835\udc47 is a temperature parameter. When \ud835\udc47 = 0, Problem (4) is the MAP Problem (2). When \ud835\udc47 = 1, Papandreou and Yuille (2011) showed that if the entries of \ud835\udf00 are independently drawn from a Gumbel distribution, the solution of Problem (4) approximates a sample from the PGM distribution. Lazaro-Gredilla et al. (2021) recently showed state-of-the-art learning and sampling performance on several PGMs including Ising models and Restricted Boltzmann Machines by using max-product to solve Problem (4). We use their method, named perturb-and-max-product (PMP), in the rest of this paper."
        },
        {
            "heading": "4.3. Accelerating max-product on GPUs",
            "text": "Recently Zhou et al. (2022) open-sourced PGMax, a Python package to run GPU-accelerated parallel max-product on general factor graphs with discrete variables. The authors showed timing improvements of two to three orders of magnitude compared with alternatives. We use this package to solve the families of perturbed MAP Problems (4) for noisy-OR BNs, while performing GPU-accelerated message updates with linear complexity (see Appendix A)."
        },
        {
            "heading": "5. Noisy-OR Bayesian Networks learning",
            "text": "We now derive a scheme for learning noisy-OR BNs that uses parallel max-product for fast approximate inference."
        },
        {
            "heading": "5.1. Deriving the Elbo",
            "text": "Noisy-OR BNs are directed models with intractable likelihood. Therefore, a standard approach is to maximize the evidence lower bound (Elbo) (Kingma and Welling, 2013):\nlog \ud835\udc5d(\ud835\udc65 |\u0398) \u2265 \ud835\udd3c\ud835\udc5e(\u210e |\ud835\udc65,\ud835\udf19) {log \ud835\udc5d(\u210e, \ud835\udc65 |\u0398) \u2212 log \ud835\udc5e(\u210e|\ud835\udc65, \ud835\udf19)} = \ud835\udd3c\ud835\udc5e(\u210e |\ud835\udc65,\ud835\udf19) {log \ud835\udc5d(\u210e, \ud835\udc65 |\u0398)} + \u210d {\ud835\udc5e(\u210e|\ud835\udc65, \ud835\udf19)} = L(\ud835\udc65,\u0398, \ud835\udf19), (5)\nwhere \ud835\udc5e(\u210e|\ud835\udc65, \ud835\udf19) is an approximate posterior, which VI assumes to be the output of a recognition network (Buhai et al., 2020) or a MF posterior (Jaakkola and Jordan, 1999; Ji et al., 2020). The first term in Equation (5) is the expectation of the joint log-likelihood under the approximate posterior distribution, while the second term is the entropy of the approximate posterior. If we set \ud835\udc5e(\u210e|\ud835\udc65, \ud835\udf19) = \ud835\udc5d(\u210e|\ud835\udc65,\u0398), then the bound in Equation (5) becomes tight. However, the exact posterior of a noisy-OR BN is intractable. We propose to derive an approximate posterior for a binary observation \ud835\udc65 as follows. We first use max-product to either (a) estimate the mode of the model posterior \u210e\u0303(\ud835\udc65, \ud835\udc47 = 0) \u2248 argmax \ud835\udc5d(\u210e|\ud835\udc65,\u0398) or (b) get a sample from the model posterior \u210e\u0303(\ud835\udc65, \ud835\udc47 = 1) \u223c \ud835\udc5d(\u210e|\ud835\udc65,\u0398). Similar to Lazaro-Gredilla et al. (2021), we address these posterior queries by clamping the visible variables to their observed value and running max-product, i.e., we set \ud835\udf06 \ud835\udc56 = (0,\u2212\u221e) if \ud835\udc65\ud835\udc56 = 0, \ud835\udf06 \ud835\udc56 = (\u2212\u221e, 0) if \ud835\udc65\ud835\udc56 = 1 in Problem (4). We then solve Problem (4) with a temperature \ud835\udc47 = 0 for (a) and \ud835\udc47 = 1 for (b) using the PMP method described in Section 4.2. We refer to the posterior inference query (a) or (b) as:\n\u210e\u0303(\ud835\udc65, \ud835\udc47) = PMP(\ud835\udc65, \u0398, \ud835\udc47). (6) After addressing (a) or (b), we define the approximate posterior \ud835\udc5e(\u210e|\ud835\udc65) by a Dirac delta centered at \u210e\u0303(\ud835\udc65, \ud835\udc47): \ud835\udc5e(\u210e|\ud835\udc65) = 1(\u210e = \u210e\u0303(\ud835\udc65, \ud835\udc47)). The lower bound in Equation (5) becomes L(\ud835\udc65,\u0398) = log \ud835\udc5d(\u210e\u0303(\ud835\udc65, \ud835\udc47), \ud835\udc65 | \u0398). L does not depend on \ud835\udf19, and the entropy of \ud835\udc5e(\u210e|\ud835\udc65) is 0. Let \ud835\udc67 = (\ud835\udc670, \u210e\u0303(\ud835\udc65, \ud835\udc47), \ud835\udc65). Equation (1) can then be used to decompose the Elbo as a sum over the different factors:\nL(\ud835\udc65,\u0398) = \ud835\udc5a+\ud835\udc5b\u2211\ufe01 \ud835\udc56=1 \ud835\udc67\ud835\udc56 log ( 1 \u2212 exp ( \u2212\ud835\udf030\u2192\ud835\udc56 \u2212 \u2211\ufe01 \ud835\udc58\u2208P(\ud835\udc56) \ud835\udf03\ud835\udc58\u2192\ud835\udc56\ud835\udc67\ud835\udc58 ))\n+ (1 \u2212 \ud835\udc67\ud835\udc56) ( \u2212\ud835\udf030\u2192\ud835\udc56 \u2212 \u2211\ufe01 \ud835\udc58\u2208P(\ud835\udc56) \ud835\udf03\ud835\udc58\u2192\ud835\udc56\ud835\udc67\ud835\udc58 ) . (7)"
        },
        {
            "heading": "5.2. Optimizing the Elbo",
            "text": "The Elbo in Equation (7) admits a closed-form gradient. Let us denote \ud835\udc53 (\ud835\udefd) = log(1 \u2212 exp(\u2212\ud835\udefd)), whose derivative is \ud835\udc53 \u2032(\ud835\udefd) = exp(\u2212\ud835\udefd)1\u2212exp(\u2212\ud835\udefd) . Let \ud835\udc58 \u2208 P(\ud835\udc56). Then the partial derivative of the Elbo w.r.t. \ud835\udf03\ud835\udc58\u2192\ud835\udc56\nis: \ud835\udf15L(\ud835\udc67,\u0398) \ud835\udf15\ud835\udf03\ud835\udc58\u2192\ud835\udc56\n= \ud835\udc67\ud835\udc56\ud835\udc67\ud835\udc58 \ud835\udc53 \u2032 ( \ud835\udf030\u2192\ud835\udc56 + \u2211\ufe01 \ud835\udc58\u2208P(\ud835\udc56) \ud835\udf03\ud835\udc58\u2192\ud835\udc56\ud835\udc67\ud835\udc58 ) + (\ud835\udc67\ud835\udc56 \u2212 1)\ud835\udc67\ud835\udc58 (8)\nA similar relationship holds for \ud835\udf15L(\ud835\udc67,\u0398) \ud835\udf15\ud835\udf030\u2192\ud835\udc56 , by setting \ud835\udc670 = 1. Parameter sharing: In Sections 6.4 and 6.6, several parent-child pairs (\ud835\udc58, \ud835\udc56) of the noisy-OR BN use the same parameter \ud835\udf03. The chain rule generalizes the partial derivative w.r.t. \ud835\udf03 by summing the right-hand side of Equation (8) over the pairs sharing this parameter. Stochastic gradients updates: We iterate through the data via mini-batches (Robbins and Monro, 1951), and we form a noisy estimate of the gradient of the Elbo on each mini-batch. This allows (a) scalability of our approach to large datasets (b) escaping local optima. We then use Adam (Kingma and Ba, 2014) to update the parameters \u0398. Finally, as in Ji et al. (2020), we clip the parameters \u0398 = max(\u0398, \ud835\udf16) to keep the Elbo in Equation (7) finite. Algorithm 1 summarizes one step of parameters updates.\nAlgorithm 1 Stochastic gradient updates with max-product Input: Current parameters \u0398 (\ud835\udc61) Current mini-batch B (\ud835\udc61) of size \ud835\udc46 Max-product temperature \ud835\udc47 Learning rate lr Clipping value \ud835\udf16 Output: Updated parameters \u0398 (\ud835\udc61+1) function UpdateParameters\nfor \ud835\udc65\ud835\udc56 \u2208 B (\ud835\udc61) do \u210e\u0303\ud835\udc56 (\ud835\udc65\ud835\udc56, \ud835\udc47) = PMP(\ud835\udc65\ud835\udc56, \u0398 (\ud835\udc61) , \ud835\udc47) as in Equation (6) Compute \u2207L(\ud835\udc65\ud835\udc56,\u0398 (\ud835\udc61) ) using Equation (8) end for \u2207LB (\ud835\udc61) (\u0398 (\ud835\udc61) ) = 1\ud835\udc46 \u2211 \ud835\udc65\ud835\udc56\u2208B (\ud835\udc61) L(\ud835\udc65\ud835\udc56,\u0398\n(\ud835\udc61) ) \u0398 (\ud835\udc61+1) = ADAM(\u0398 (\ud835\udc61) ,\u2207LB (\ud835\udc61) (\u0398 (\ud835\udc61) ), lr) \u0398 (\ud835\udc61+1) = max(\u0398 (\ud835\udc61+1) , \ud835\udf16)\nend function"
        },
        {
            "heading": "5.3. Robustifying VI using MP",
            "text": "Our objective value differs from the one in Ji et al. (2020). Algorithm 1 optimizes the Elbo defined in Equation (7)\u2014referred to as ElboMP\u2014w.r.t. the model parameters for a given binary configuration\u2014 while Ji et al. (2020) optimize an Elbo derived using MF VI\u2014referred to as ElboVI. When both are defined, ElboMP and ElboVI are two valid lower bounds of the log-likelihood of a noisy-OR BN. Thus, in the rest of this paper, we refer to the Elbo of a method as the maximum of ElboVI and ElboMP\u2014Appendix D discusses how we can also define ElboMP for any VI posterior. When the approximate posterior is concentrated into a single Dirac delta, ElboMP is tighter than ElboVI: ElboVI is derived from ElboMP using Jensen\u2019s inequality in Ji et al. (2020, Eq. (6))\u2014see Appendix E.1 for more details. However, the non-zero entropy term present in ElboVI makes it often tighter when the approximate posterior is not a Dirac delta. The optimization of ElboVI using the simplistic MF posterior is hard and often gets stuck in bad local optima. This explains the catastrophic failures of MF VI in Sections 6.4 and 6.6. In contrast, MP uses a richer posterior which makes the optimization of ElboMP easier. As a result, our approach seems better at parameter search. We then\npropose to robustify MF VI with a hybrid approach, which uses the parameters \u0398Alg1 learned with Algorithm 1 to initialize the VI training from Ji et al. (2020). This initialization should guide the parameter search of VI and lead to a better optima than standalone VI, while returning a tighter Elbo."
        },
        {
            "heading": "6. Computational results",
            "text": "We assess the performance of our methods on five categories of binary datasets (a) the tiny20 dataset discussed in Ji et al. (2020) (b) five large sparse Tensorflow datasets, (c) binary matrix factorization datasets (d) seven synthetic datasets introduced in Buhai et al. (2020) (e) the 2D blind deconvolution dataset from Lazaro-Gredilla et al. (2021). Each experiment is run on a NVIDIA Tesla P100."
        },
        {
            "heading": "6.1. Methods compared",
            "text": "We compare the following methods in our experiments: \u2022 Full VI: this is the approach from Ji et al. (2020). The authors did not release their code. To efficiently use their method in our experiments, we re-implemented it in JAX (Bradbury et al., 2018), using the variational hyperparameters reported. We use ADAM (Kingma and Welling, 2013) as we observe that it leads to better performance than the preconditioning proposed by the authors. \u2022 Local VI: This is our re-implementation of the local models proposed by Ji et al. (2020) and described in Section 3, which are required to scale VI to large sparse datasets. \u2022 MP: this is the proposed max-product training described in Algorithm 1. Max-product is run with a damping \ud835\udefc = 0.5 for \ud835\udc41MP = 100 iterations. We select the temperature \ud835\udc47 \u2208 {0, 1} with better empirical performance. \u2022 MP + VI: this is the hybrid training proposed in Section 5.3. We first run Algorithm 1 to learn the parameters \u0398Alg1, then run VI training for a few iterations starting from \u0398Alg1. All the methods consider a clipping value \ud835\udf16 = 10\u22125 for the parameters \u0398. For a given experiment, all the methods use the same learning rate and mini-batch size, and we report the best performance of each method over several initializations\u2014which we describe in Appendix C."
        },
        {
            "heading": "6.2. Tiny20 dataset",
            "text": "Dataset: We first consider the tiny20 dataset1 on which Ji et al. (2020) illustrate many of their findings. As in Ji et al. (2020), we build a three-layers graph with 100 visible and 44 hidden nodes using the procedure in Appendix B and we train on 70% of the data at random (i.e. 11, 369 samples). Training: We train full VI and local VI for 1, 500 gradient steps, and for 5, 000 steps. For MP + VI, we first run 1, 000 gradient steps using Algorithm 1 with \ud835\udc47 = 0, then 500 gradient steps using VI. All the methods use full-batch gradients as in Ji et al. (2020) and a learning rate of 0.01. Results: Table 1 reports the test Elbo (defined in Section 5.3 as the best value between ElboVI and ElboMP) of the different methods averaged over 10 random train-test splits. Our hybrid MP + VI approach outperforms all the variational methods by a statistically significant margin. Interestingly, we observe that (a) increasing the number of training iterations slightly improves full and local VI, but it does not make them competitive with our best method; (b) standalone MP is competitive; and (c) as reported in Ji et al. (2020), full VI performs slightly better than local VI, as the latter optimizes a constrained VI objective.\n1Accessible at https://cs.nyu.edu/\u223croweis/data/20news_w100.mat\nIn addition, we note that Ji et al. (2020) reported a lower Elbo of \u221214.50 for their best full VI method, using 145 nodes (as we do) with a different graph heuristic and a different initialization procedure\u2014both not described. Finally, to illustrate the distinction between ElboMP and ElboVI, we report these two metrics in Appendix E.2, Table 5. In particular, standalone MP is the best performer for ElboMP."
        },
        {
            "heading": "6.3. Large sparse Tensorflow text datasets",
            "text": "Dataset: We compare our hybrid method with Ji et al. (2020) on five large sparse Tensorflow text datasets (Abadi et al., 2015), which respectively contain scientific documents, news, movie reviews, patent descriptions and Yelp reviews. Note that Ji et al. (2020) only consider two datasets and do not detail their processing procedure. To process each dataset, we first tokenize and vectorize it using a vocabulary size of 10, 000 (removing all the words outside the vocabulary) and a maximum sequence length of 500. Second, we represent each sentence by a binary vector \ud835\udc65 \u2208 {0, 1}10,000, where \ud835\udc65 \ud835\udc57 = 1 if the \ud835\udc57th word is present. Our datasets\u2019 statistics are summarized in Appendix F.1, Table 7. Finally, as in the large sparse experiments of Ji et al. (2020), we build a five-layers graph for each dataset. Training: We train local VI for 4, 000 gradient steps. For hybrid training, we use 3, 600 steps of Algorithm 1 with \ud835\udc47 = 0, then 400 steps of local VI training. Both methods use a mini-batch size of 128 and a learning rate of 3 \u00d7 10\u22124. Results: Table 2 averages the test Elbo of both methods over 10 runs\u2014each run shuffles the training and test set separately. Our hybrid method outperforms local VI on four datasets and is tied on one. In addition, Table 6 in Appendix E.3 compares ElboMP with ElboVI on each dataset: the hybrid approach\nis the best performer for ElboMP on all the datasets, which shows that hybrid training improves the overall performance of the noisy-OR models. Timings comparison: Table 8 in Appendix F.2 reports the update times (defined as the average time for one gradient step) of MP and local VI on each dataset: MP is two to four times faster. Despite updating the variational parameters one by one, local VI runs at a reasonable speed as it uses small arrays to represent large sparse datasets. Note that MP runs in parallel and does not exploit the sparsity of the data."
        },
        {
            "heading": "6.4. Binary Matrix Factorization",
            "text": "Problem: Our next problem is Binary Matrix Factorization (BMF). Let \ud835\udc5b, \ud835\udc5f, \ud835\udc5d be three integers with \ud835\udc5f < min(\ud835\udc5b, \ud835\udc5d) and let \ud835\udc48 \u2208 {0, 1}\ud835\udc5b\u00d7\ud835\udc5f, \ud835\udc49 \u2208 {0, 1}\ud835\udc5f\u00d7\ud835\udc5d be two binary matrices. We assume that addition is performed on the Boolean semi-ring, i.e. 1 + 1 = 1, and we define a binary matrix \ud835\udc4b = \ud835\udc48\ud835\udc49 \u2208 {0, 1}\ud835\udc5b\u00d7\ud835\udc5d. The BMF problem consists in recovering the binary matrices \ud835\udc48 and \ud835\udc49 given the observations \ud835\udc4b . This problem is equivalent to learning a noisy-OR BN with \ud835\udc5d visible nodes and \ud835\udc5f hidden nodes, and with the parameters \ud835\udf03\ud835\udc65 , \ud835\udf03\ud835\udc62 \u2208 \u211d+, \ud835\udc49 \u2208 \u211d\ud835\udc5f\u00d7\ud835\udc5d+ , such that (a) the failure probability between the \ud835\udc56th hidden and the \ud835\udc57th visible variable is given by exp(\u2212\ud835\udc49\ud835\udc56 \ud835\udc57) (b) the prior probability of each hidden variable is equal to 1 \u2212 exp(\u2212\ud835\udf03\ud835\udc62) (c) the noise probability of each visible variable is 1 \u2212 exp(\u2212\ud835\udf03\ud835\udc65). Note that \ud835\udf03\ud835\udc65 (resp. \ud835\udf03\ud835\udc62) is shared across all the visible (resp. hidden) variables. Let \u0398 = (\ud835\udf03\ud835\udc65 , \ud835\udf03\ud835\udc62, \ud835\udc49). For \ud835\udc65 \u2208 {0, 1}\ud835\udc5d, the conditional probability of the \ud835\udc57th entry \ud835\udc65 \ud835\udc57 is\n\ud835\udc5d(\ud835\udc65 \ud835\udc57 = 1 | \ud835\udc621, . . . , \ud835\udc62\ud835\udc5f,\u0398) = 1 \u2212 exp ( \u2212 \ud835\udf03\ud835\udc65 \u2212 \ud835\udc5f\u2211\ufe01 \ud835\udc56=1 \ud835\udc49\ud835\udc56 \ud835\udc57\ud835\udc62\ud835\udc56 ) .\nThe rows of \ud835\udc4b give access to \ud835\udc5b such observations, and our Algorithm 1 naturally extends to the BMF problem. Dataset: We fix \ud835\udc5b = \ud835\udc5d and consider two increasing sequences of values for \ud835\udc5b \u2208 {100, 200, 400} and for \ud835\udc5f/\ud835\udc5b \u2208 {0.2, 0.4, 0.6}. We additionally fix the probability \ud835\udc5d\ud835\udc4b = \ud835\udc5d(\ud835\udc4b\ud835\udc56 \ud835\udc57 = 1) = 0.25, \u2200\ud835\udc56, \ud835\udc57. To do this, we first set \ud835\udc5d\ud835\udc48\ud835\udc49 = \ud835\udc5d(\ud835\udc48\ud835\udc56\ud835\udc58 = 1) = \ud835\udc5d(\ud835\udc49\ud835\udc58 \ud835\udc57 = 1) = \u221a\ufe01 1 \u2212 (1 \u2212 \ud835\udc5d\ud835\udc4b )1/\ud835\udc5f,\u2200\ud835\udc56, \ud835\udc57, \ud835\udc58. We then generate three matrices \ud835\udc49 \u2208 {0, 1}\ud835\udc5f\u00d7\ud835\udc5d, \ud835\udc48train \u2208 {0, 1}\ud835\udc5b\u00d7\ud835\udc5f, \ud835\udc48test \u2208 {0, 1}\ud835\udc5b\u00d7\ud835\udc5f with prior \ud835\udc5d\ud835\udc48\ud835\udc49 and define \ud835\udc4b train = \ud835\udc48train\ud835\udc49, \ud835\udc4b test = \ud835\udc48test\ud835\udc49. Related work: Ravanbakhsh et al. (2016) proposed to learn \ud835\udc48 and \ud835\udc49 with max-product by estimating the mode of the joint posterior max\ud835\udc48,\ud835\udc49 \ud835\udc5d(\ud835\udc48,\ud835\udc49 |\ud835\udc4b), using non-symmetric priors for \ud835\udc48 and \ud835\udc49. Their method is very similar to PMP (Lazaro-Gredilla et al., 2021) which proposes to sample from the joint multimodal posterior to solve the 2D blind deconvolution problem, Section 6.6. Both approaches do not consider training and directly solve max-product inference, which cannot be expressed in a mini-batch format and has to run on all the training data simultaneously. These two methods are then memory-intensive, and cannot scale to datasets orders of magnitude larger than the ones used here. In comparison, our MP approach computes the gradient of ElboMP for each training sample, which is memory-light and allows scaling to larger datasets. We report the results of PMP here, which we accelerate on GPU with PGMax (Zhou et al., 2022), and we use \ud835\udc5d\ud835\udc48\ud835\udc49 as priors for \ud835\udc48 and \ud835\udc49. Training: We train full VI and BP for 40, 000 gradient steps with batch size 20 and learning rate 0.001. We use MP with \ud835\udc47 = 1 to sample from the posterior as it allows to escape local optima during training. For PMP, there is no training and we directly turn to inference using 1, 000 max-product iterations as in Lazaro-Gredilla et al. (2021). Metrics: We report the Elbo of each method, as well as its update time. We also report its test reconstruction error, which is defined as 1\n\ud835\udc5b2 \u2016\ud835\udc48 test \ud835\udc49 thre \u2212 \ud835\udc4b test\u20161, where \ud835\udc48test and \ud835\udc49 thre are binary\nmatrices and have used 1 + 1 = 1 for multiplication. \ud835\udc49 thre is derived by thresholding the learned \ud835\udc49 with a threshold of log(2): a 1 in \ud835\udc49 thre corresponds to a failure probability lower than 0.5 in \ud835\udc49. \ud835\udc48test is the mode of posterior, estimated as detailed in Appendix D. For PMP, \ud835\udc49 is already binary and we only report its test RE\u2014the update times are not defined for PMP as there is no training. Results: Table 3 averages the results over 10 runs\u2014each run generate new \ud835\udc49,\ud835\udc48train, \ud835\udc48test. For \ud835\udc5b = 100, there is no clear winner: MP achieves a higher Elbo, while PMP and VI reach lower REs. However, the performance of VI decreases as \ud835\udc5b increases: for \ud835\udc5b = 200, \ud835\udc5f \u2208 {80, 120}, VI has a test RE very close to \ud835\udc5d\ud835\udc4b = 0.25, which is what would return the trivial estimate \ud835\udc49 = 0. In addition, the sequential MF parameters updates make full VI prohibitively slow here: MP is 55 times faster for \ud835\udc5b = 200, \ud835\udc5f = 120, and 195 times faster for \ud835\udc5b = 400, \ud835\udc5f = 240. Finally, for \ud835\udc5b = 400, VI did not finish training after three weeks and the test REs are close to \ud835\udc5d\ud835\udc4b , which shows that no latent structure has been recovered. PMP is a solid competitor: it is faster than our method as it has no learning, and it leads to better performance when \ud835\udc5f/\ud835\udc5b = 0.6. However it cannot scale and runs out of GPU memory for the large \ud835\udc5b = 400, \ud835\udc5f = 240."
        },
        {
            "heading": "6.5. Overparametrization experiments",
            "text": "Problem: Here, we reproduce the noisy-OR experiment from Buhai et al. (2020). The authors introduced seven synthetic datasets2\u2014which we refer to as OVPM. Five datasets (IMG, PLNT, UNIF, CON8, CON24) are generated from ground truth (GT) noisy-OR BNs while two (IMG-FLIP and IMG-UNIF) additionally perturb the observations. The five GT noisy-OR BNs have the same structure, defined as follows. \ud835\udc3e\u2217 = 8 latent variables \ud835\udc621, . . . , \ud835\udc628, are each associated with a continuous vector of parameters \ud835\udc49\ud835\udc58 \u2208 \u211d\ud835\udc5d+. \ud835\udc491, . . . , \ud835\udc498 are shared across all the observations while \ud835\udc62\ud835\udc58 expresses whether the \ud835\udc58th latent variable is active for a given observation. Each latent variable has a prior 1 \u2212 exp(\u2212\ud835\udf03\ud835\udc58) with \ud835\udf03\ud835\udc58 \u2265 0. Let \u0398\u2217 = (\ud835\udc491, . . . , \ud835\udc498, \ud835\udf031, . . . , \ud835\udf038, \ud835\udf03\ud835\udc65). An observation \ud835\udc65 is generated such that\n\ud835\udc5d(\ud835\udc65 \ud835\udc57 = 1 | \ud835\udc621, . . . , \ud835\udc62\ud835\udc5f,\u0398\u2217) = 1 \u2212 exp ( \u2212 \ud835\udf03\ud835\udc65 \u2212 8\u2211\ufe01 \ud835\udc58=1 \ud835\udc62\ud835\udc58\ud835\udc49 \ud835\udc58 \ud835\udc57 ) , \u2200 \ud835\udc57.\nThe GT parameters \u0398\u2217 are different for each GT noisy-OR BN. Figure 3[left] shows \ud835\udc491, . . . , \ud835\udc498 and eight cluttered binary samples from one of the datasets, IMG3.\n2All the datasets are at https://github.com/clinicalml/overparam 3IMG is originally from \u0160ingliar and Hauskrecht (2006).\nTraining: Buhai et al. (2020) learned the noisy-OR BN above for increasing values \ud835\udc3e \u2265 8 of latent variables and study how overparametrization improves the recovery of the GT parameters \ud835\udc491, . . . , \ud835\udc498. We compare our MP approach with their results, using \ud835\udc47 = 1 for MP. For each dataset, we then consider an increasing sequence of latent variables \ud835\udc3e \u2208 {8, 10, 16, 32}. We use the same training parameters as Buhai et al. (2020): 9, 000 training samples, 100 epochs, a batch size of 20 and a learning rate of 0.001. Metrics: We compare the performance of our method with the VI results reported in the main Figure 2 of Buhai et al. (2020, Fig. 2) (the numerical values are in Tables 2 and 5 in their appendices). As the authors, we report the averaged number of GT parameters \ud835\udc491, . . . , \ud835\udc498 recovered during training\u2014 which we compute as detailed in Appendix G.1\u2014and the percentage of runs with full recovery. Results: Figure 2 compares our method (blue) averaged over 50 repetitions, with VI (orange). Both MP and VI benefit from overparametrization: when \ud835\udc3e increases, both methods recover more GT parameters. In addition, MP outperforms VI. In particular, for each dataset, for a model with 16 or 32 latent variables, our method (a) always recovers on average at least seven (out of eight) GT parameters (b) always performs better than VI. This gap is larger for the first five datasets, which do not perturb the observations."
        },
        {
            "heading": "6.6. 2D Blind Deconvolution",
            "text": "Problem: Our last experiment is the 2D blind deconvolution (BD) problem from Lazaro-Gredilla et al. (2021, Section 5.6). The task consists in recovering two binary variables \ud835\udc4a and \ud835\udc46 from 100 binary images4 \ud835\udc4b \u2208 {0, 1}\ud835\udc5b\u00d7\ud835\udc5d. \ud835\udc4a (size: \ud835\udc5bfeat \u00d7 featheight \u00d7 featwidth) contains 2D binary features. \ud835\udc46 (size: \ud835\udc5bimages \u00d7 \ud835\udc5bfeat \u00d7 actheight \u00d7 actwidth) is a set of binary indicator variables. \ud835\udc46 and \ud835\udc4a are combined by convolution, placing the features defined by\ud835\udc4a at the locations specified by \ud835\udc46 in order to form \ud835\udc4b . Unlike \ud835\udc46,\ud835\udc4a is shared by all images. The dimensions of the GT\ud835\udc4a used to generate \ud835\udc4b are 4 \u00d7 5 \u00d7 5, but the authors set the dimensions of the learned ?\u0302? to 5\u00d7 6\u00d7 6, which we do too. Figure 3[center] shows the ground truth \ud835\udc4a and four samples from \ud835\udc4b from the BD dataset. Appendix H.1 presents another example from Lazaro-Gredilla et al. (2021), which visualizes \ud835\udc46,\ud835\udc4a and \ud835\udc4b on a simpler dataset. The BMF experiment, Section 6.5, is a particular case of BD: BD is a harder problem. BD is also equivalent to learning a noisy-OR BN, which we describe in Appendix H.2.\n4To generate the datasets, we use the publicly released code at https://github.com/vicariousinc/perturb_and_max_product\nMethods compared: We compare full VI and MP with PMP (discussed in Section 6.4), which directly learns a binary ?\u0302? by sampling from the joint posterior \ud835\udc5d(\ud835\udc46,\ud835\udc4a |\ud835\udc4b). Training: We train MP and full VI for 3, 000 steps on 80% of the data, using full-batch gradients and a learning rate of 0.01. PMP has no training and, for inference, we use the same priors as by Lazaro-Gredilla et al. (2021) Metrics: We report the test Elbo, the update time and the test RE of each method. Here, the test RE is defined as 1\n\ud835\udc5b\ud835\udc5d \u2016\ud835\udc4b testRE \u2212 \ud835\udc4b test\u20161, where \ud835\udc4b testRE is computed by convolving the estimated test feature locations \ud835\udc46test with the thresholded learned features ?\u0302?thre. Finally, we match ?\u0302?thre with the GT \ud835\udc4a using intersection over union (IOU) for matching and report the features IOU\u2014defined in Appendix H.3. For PMP, we only report the test RE and features IOU. Results: Table 4 averages the four metrics over 10 repetitions with random train-test splits. VI is 50 times slower than our method and completely fails at recovering the latent structure of the data, which leads to worse test metrics. Again, PMP is a strong competitor. However, (a) it is sensitive to the value of the priors of \ud835\udc4b , \ud835\udc46 and \ud835\udc4a, (b) it leads to a test RE twice higher than MP, (c) it is memory-intensive. Figure 3[right] shows the binary ?\u0302?thre learned with MP and VI for a random seed\u2014all the results are in Appendix H.4. MP recovers the four GT features and adds a noisier feature in the first position (which has a smaller prior and can be easily discarded) while VI fails. Finally, we refer to Appendix H.5 for a comparison of the reconstructed test images returned by our method and PMP."
        },
        {
            "heading": "7. Discussion",
            "text": "We have developed a fast, memory-efficient, stochastic algorithm for training noisy-OR BNs. Contrary to existing VI approaches with a recognition network, our MP method induces explaining-away and recovers more GT parameters on the OVPM datasets. In contrast with MF VI approaches, our method (a) finds better local optima; and (b) scales to large dense datasets. This explains, respectively, why (a) it solves the BD and the BMF problems while MF VI catastrophically fails; and (b) it is up to two orders of magnitude slower. Finally, our method is more memory-efficient than PMP. In addition, we have proposed to use our method to guide VI and help it find better local optima on the large sparse real Tensorflow datasets. Our next line of work is to use our algorithm to train noisy-OR BNs on complex synthetic scenes and extract rich latent representations."
        },
        {
            "heading": "A. An equivalent representation of a noisy-OR factor",
            "text": "We discuss herein an equivalent representation of the noisy-OR conditional distribution in Equation (1) that uses the tractable factors supported by PGMax. First, as we have observed in Section 4.1, for the messages from factors to variables, the max-product updates detailed in Equations (3) require to loop through all the valid configurations of a factor. Let \ud835\udc56 be a variable in the graph, let \ud835\udc41\ud835\udc56 = |P(\ud835\udc56) | be the cardinality of the set P(\ud835\udc56) of parents of \ud835\udc56, and let P(\ud835\udc56) = { \ud835\udc571, . . . , \ud835\udc57\ud835\udc41\ud835\udc56}. The noisy-OR factor associated with \ud835\udc56, and described in Equation (1), connects the 2 + \ud835\udc41\ud835\udc56 variables {\ud835\udc67\ud835\udc56} \u222a {\ud835\udc670} \u222a \ud835\udc67P(\ud835\udc56) , and the has 22+\ud835\udc41\ud835\udc56 valid configuration. Consequently, a naive implementation of the max-product message updates in Equations (3) has a complexity exponential in the number of variables of the noisy-OR factors, which is prohibitively slow for large factors. To remedy this, let us introduce a family of \u201cnoise-free\u201d OR factors which are described by the conditional distribution\n\ud835\udc5d(\ud835\udc67\ud835\udc56 = 0 | \ud835\udc67P(\ud835\udc56) ) = \u220f\n\ud835\udc58\u2208P(\ud835\udc56) (1 \u2212 \ud835\udc67\ud835\udc58). (9)\nThe noise-free OR factors simply express the logical condition \ud835\udc67\ud835\udc56 = OR(\ud835\udc67 \ud835\udc571 , . . . , \ud835\udc67 \ud835\udc57\ud835\udc41\ud835\udc56 ). They do notinvolve the noisy-OR parameters \u0398 defined in Equation (1). It turns out that, for a \u201cnoise-free\u201d OR factor, the messages updates derived in PGMax have a complexity linear in the number of variables connected to this factor. Consequently, if we derive an equivalent representation of the noisy-OR conditional distribution in Equation (1) that uses the noise-free OR conditional distribution in Equation (9), the cost of the max-product messages updates (using PGMax) would go from O(2 |P (\ud835\udc56) |) down to O(|P(\ud835\udc56) |).\nTo this end, we define two factor graphs, which we both represent in Figure 4:\n1. The first factor graph considers a single noisy-OR factor\u2014nOR in Figure 4\u2014which connects the leak variable \ud835\udc670 and the parents variables \ud835\udc67P(\ud835\udc56) to the child variable \ud835\udc67\ud835\udc56 via the noisy-OR conditional distribution defined in Equation (1). 2. The second factor graph introduces the auxiliary binary variables ?\u0303?0, ?\u0303? \ud835\udc571 , . . . , \u02dc\ud835\udc67 \ud835\udc57\ud835\udc41\ud835\udc56 and connectsthem to the child variable \ud835\udc67\ud835\udc56 via the noise-free OR factor defined in Equation (9). In addition,\nfor each \ud835\udc58 \u2208 {0} \u222a P(\ud835\udc56), there is a pairwise factor\u2014referred to as PW in Figure 4\u2014that connects the variables \ud835\udc67\ud835\udc58 and ?\u0303?\ud835\udc58 and that is defined by{\n\ud835\udc5d( ?\u0303?\ud835\udc58 = 0 | \ud835\udc67\ud835\udc58 = 0) = 1 \ud835\udc5d( ?\u0303?\ud835\udc58 = 0 | \ud835\udc67\ud835\udc58 = 1) = exp(\u2212\ud835\udf03\ud835\udc58\u2192\ud835\udc56)\nwhich can be represented in a more compact form by\n\ud835\udc5d( ?\u0303?\ud835\udc58 = 0 | \ud835\udc67\ud835\udc58) = exp(\u2212\ud835\udf03\ud835\udc58\u2192\ud835\udc56\ud835\udc67\ud835\udc58). (10)\nWe aim at showing the equivalence between the two factor graphs. To this end, let us derive the conditional distribution of \ud835\udc67\ud835\udc56 given \ud835\udc67P(\ud835\udc56) for the second factor graph:\n\ud835\udc5d(\ud835\udc67\ud835\udc56 = 0 | \ud835\udc67P(\ud835\udc56) ) = \u2211\ufe01\n?\u0303?0\u2208{0,1}, ?\u0303?P(\ud835\udc56) \u2208{0,1}\ud835\udc41\ud835\udc56 \ud835\udc5d(\ud835\udc67\ud835\udc56 = 0, ?\u0303?0, ?\u0303?P(\ud835\udc56) | \ud835\udc67P(\ud835\udc56) )\n= \u2211\ufe01\n?\u0303?0\u2208{0,1}, ?\u0303?P(\ud835\udc56) \u2208{0,1}\ud835\udc41\ud835\udc56 \ud835\udc5d(\ud835\udc67\ud835\udc56 = 0 | ?\u0303?0, ?\u0303?P(\ud835\udc56) ) \ud835\udc5d( ?\u0303?0, ?\u0303?P(\ud835\udc56) | \ud835\udc67P(\ud835\udc56) ) by conditional independence\n= \u2211\ufe01\n?\u0303?0\u2208{0,1}, ?\u0303?P(\ud835\udc56) \u2208{0,1}\ud835\udc41\ud835\udc56\n\u220f \ud835\udc58\u2208{0}\u222aP(\ud835\udc56) (1 \u2212 ?\u0303?\ud835\udc58) \ud835\udc5d( ?\u0303?\ud835\udc58 | \ud835\udc67\ud835\udc58)\n= \u220f\n\ud835\udc58\u2208{0}\u222aP(\ud835\udc56) \ud835\udc5d( ?\u0303?\ud835\udc58 = 0 | \ud835\udc67\ud835\udc58) as the product cancels if any ?\u0303?\ud835\udc58 = 1\n= exp(\u2212\ud835\udf030\u2192\ud835\udc56) \u220f\n\ud835\udc58\u2208P(\ud835\udc56) exp(\u2212\ud835\udf03\ud835\udc58\u2192\ud835\udc56\ud835\udc67\ud835\udc58) using Equation (10) and \ud835\udc670 = 1,\nwhich is exactly the noisy-OR conditional distribution Equation (1). This proves the equivalence between the two factor graphs. In particular, we can use the second factor graph to represent a noisy-OR factor and benefit from the GPU-accelerated messages updates from PGMax which have a complexity linear in the number of variables."
        },
        {
            "heading": "B. Graph generation procedure for multi-layered noisy-OR Bayesian networks",
            "text": "We describe below the graph generation procedure we use to build the multi-layered noisy-OR BNs in the tiny20 and the Tensorflow experiments, Sections 6.2 and 6.3. We assume we are given a binary matrix \ud835\udc4b \u2208 {0, 1}\ud835\udc5b\u00d7\ud835\udc5d, where each row is a binary observation: as in Section 2 there are \ud835\udc5d visible variables. In the case of the tiny20 and Tensorflow datasets, each visible variable corresponds to a word, and each observation to a sentence or a document: \ud835\udc4b\ud835\udc56 \ud835\udc57 = 1 indicates that the \ud835\udc57th word is present in the \ud835\udc56th document. Given an integer \ud835\udc5blayers, we aim at building a noisy-OR Bayesian network with \ud835\udc5blayers + 1 layers. The bottom layer (with index \ud835\udc5blayers) of the network contains all the visible nodes, while the trivial top layer (with index 0) only contains the leak node. Our procedure builds the graph iteratively, from the top to the bottom by repeating the two following steps (for \ud835\udc57 running from \ud835\udc5blayers down to 1)\n1. Build a distance matrix for the \ud835\udc57th layer. 2. Create the variables of the \ud835\udc57 \u2212 1th layer and add edges connecting the parents of the \ud835\udc57 \u2212 1th\nlayer to the children of the \ud835\udc57th layer.\nWe detail these two steps further below.\nDistance matrix for the bottom layer: We first detail how we build the distance matrix for the bottom layer\u2014which contains all the visible variables. We start by building a vector of empirical word frequencies \ud835\udc36 \u2208 {0, 1}\ud835\udc5d such that\n\ud835\udc36 \ud835\udc57 = 1 \ud835\udc41 \ud835\udc5b\u2211\ufe01 \ud835\udc56=1 \ud835\udc4b\ud835\udc56 \ud835\udc57, \u2200 \ud835\udc57,\nis the empirical probability that the \ud835\udc57th word appears in a document. We also define a matrix of empirical co-occurrence frequencies \ud835\udc42 \u2208 {0, 1}\ud835\udc5d\u00d7\ud835\udc5d where\n\ud835\udc42 \ud835\udc57\ud835\udc58 = 1 \ud835\udc41 \ud835\udc5b\u2211\ufe01 \ud835\udc56=1 \ud835\udc4b\ud835\udc56 \ud835\udc57\ud835\udc4b\ud835\udc56\ud835\udc58, \u2200 \ud835\udc57, \ud835\udc58,\nis the empirical probability that the \ud835\udc57th and \ud835\udc58th words co-occur in a document. From there, we can define the empirical ratio\n\ud835\udc45 \ud835\udc57\ud835\udc58 = \ud835\udc42 \ud835\udc57\ud835\udc58\n\ud835\udc36 \ud835\udc57\ud835\udc36\ud835\udc58 .\n\ud835\udc45 \ud835\udc57\ud835\udc58 possesses a few interesting properties. First, from the law of large numbers, when \ud835\udc5b grows to infinity, \ud835\udc36 \ud835\udc57 \u2192 \ud835\udc5d(\ud835\udc65 \ud835\udc57 = 1), \ud835\udc42 \ud835\udc57\ud835\udc58 \u2192 \ud835\udc5d(\ud835\udc65 \ud835\udc57 = 1, \ud835\udc65\ud835\udc58 = 1) and consequently \ud835\udc45 \ud835\udc57\ud835\udc58 \u2192 \ud835\udc5d(\ud835\udc65 \ud835\udc57=1,\ud835\udc65\ud835\udc58=1)\ud835\udc5d(\ud835\udc65 \ud835\udc57=1) \ud835\udc5d(\ud835\udc65\ud835\udc58=1) . Therefore, if the \ud835\udc57th and \ud835\udc58th words are independent, the limit of \ud835\udc45 \ud835\udc57\ud835\udc58 in the case of an infinite amount of data is 1. If the limit of \ud835\udc45 \ud835\udc57\ud835\udc58 is higher than 1, then \ud835\udc5d(\ud835\udc65 \ud835\udc57 = 1, \ud835\udc65\ud835\udc58 = 1) is higher than the case where the variables are independent. Finally, \ud835\udc45 \ud835\udc57\ud835\udc58 can also be connected with the mutual information, commonly used in information theory\u2014see Globerson et al. (2004). Given these properties, we propose to define the distance matrix associated with the bottom layer by\n\ud835\udc37 (\ud835\udc5blayers) \ud835\udc57\ud835\udc58 = exp(\u2212\ud835\udc45 \ud835\udc57\ud835\udc58), \u2200 \ud835\udc57, \ud835\udc58.\nBuilding the \ud835\udc57 \u2212 1th layer and connecting it to the \ud835\udc57th layer: Let \ud835\udc57 \u2265 2. We assume that the \ud835\udc57th layer has \ud835\udc51 variables and that we are given a distance matrix \ud835\udc37( \ud835\udc57) \u2208 \u211d\ud835\udc51\u00d7\ud835\udc51+ \u2014we have described above how to build \ud835\udc37(\ud835\udc5blayers) for the bottom layer which has \ud835\udc5d variables. We now describe how our procedure builds the \ud835\udc57 \u2212 1th layer and adds edges between the \ud835\udc57th and \ud835\udc57 \u2212 1th layers. To this end, we use two hyperparameters: (a) the ratio between the number of variables of the \ud835\udc57th layer and of the \ud835\udc57 \u2212 1th layer, \ud835\udc5fchildren to parents (which we set to 3 in our experiments) (b) the number of nodes of the \ud835\udc57 \u2212 1th layer that each node of the \ud835\udc57th layer will be connected with, \ud835\udc5bparents by node (which we set to 5). As a first step, we use hierarchical clustering5 with average linkage on the distance matrix \ud835\udc37( \ud835\udc57) to form b \ud835\udc51\n\ud835\udc5fchildren to parents c clusters, with indices 1, . . . , b \ud835\udc51 \ud835\udc5fchildren to parents c. For each cluster \ud835\udc5a, we then create a\nvariable for the \ud835\udc57 \u2212 1th layer, \ud835\udc67 ( \ud835\udc57\u22121)\ud835\udc5a . We refer to label(\ud835\udc67 ( \ud835\udc57)\n\ud835\udc58 ) as the label returned by this clustering step for the \ud835\udc58th variable \ud835\udc67 ( \ud835\udc57) \ud835\udc58 of the \ud835\udc57th\nlayer. We could then add edges between the \ud835\udc57th and \ud835\udc57 \u2212 1th layers by going through the pairs of variables ( \ud835\udc67 ( \ud835\udc57) , \ud835\udc67\n( \ud835\udc57\u22121) label(\ud835\udc67 ( \ud835\udc57) )\n) . However, if we were doing so, each variable of the \ud835\udc57th layer would only\nbe connected to one variable of the \ud835\udc57 \u2212 1th layer. The resulting noisy-OR BN would not be able to induce explaining-away (see Section 2) as each effect would be connected to a single cause. To allow inference to induce this appealing property, we propose to add extra edges to the graph by connecting each variable of the \ud835\udc57th layer to multiple variables of the \ud835\udc57 \u2212 1th layer as follows. First, we define the\n5We use the AgglomerativeClustering procedure from scikit-learn (Pedregosa et al., 2011).\ndistance from a variable \ud835\udc67 ( \ud835\udc57) \ud835\udc58 of the \ud835\udc57th layer to a variable \ud835\udc67 ( \ud835\udc57\u22121)\ud835\udc5a of the \ud835\udc57 \u2212 1th layer as the average distance from \ud835\udc67 ( \ud835\udc57)\n\ud835\udc58 to all the elements of the \ud835\udc57th layer with label \ud835\udc5a:\ndist(\ud835\udc67 ( \ud835\udc57) \ud835\udc58 , \ud835\udc67 ( \ud835\udc57\u22121) \ud835\udc5a ) = 1 { : label(\ud835\udc67 ( \ud835\udc57) ) = \ud835\udc5a } \u2211\ufe01 : label(\ud835\udc67 ( \ud835\udc57) )=\ud835\udc5a \ud835\udc37 ( \ud835\udc57) \ud835\udc58 , \u2200\ud835\udc58, \ud835\udc5a.\nSecond, we add an edge connecting \ud835\udc67 ( \ud835\udc57) \ud835\udc58 to the \ud835\udc5bparents by node variables of the \ud835\udc57\u22121th layer with smallest dist(\ud835\udc67 ( \ud835\udc57)\n\ud835\udc58 , \ud835\udc67 ( \ud835\udc57\u22121) \ud835\udc5a ): this intuitively connects \ud835\udc67 ( \ud835\udc57)\ud835\udc58 to the \ud835\udc5bparents by node labels it is the \u201cclosest\u201d. Each variable\nof the \ud835\udc57th layer is now connected to the same number of variables of the \ud835\udc57 \u2212 1th layer above. However, each variable of the \ud835\udc57 \u2212 1th layer may be connected to a different number of variables of the \ud835\udc57th layer: we denote C(\ud835\udc67 ( \ud835\udc57\u22121)\ud835\udc5a ) the set of indices of the variables of the \ud835\udc57th layer connected to \ud835\udc67 ( \ud835\udc57\u22121)\ud835\udc5a . Let us note that, by definition, each node of the \ud835\udc57 \u2212 1th and \ud835\udc57th layer is also connected to the leak node. Our last step is to define the symmetric distance matrix \ud835\udc37( \ud835\udc57\u22121) between two variables of the \ud835\udc57 \u2212 1th layer, which we set to the average distance of all the variables of \ud835\udc57th layer connected to these two variables:\n\ud835\udc37 ( \ud835\udc57\u22121) \ud835\udc5a1,\ud835\udc5a2 = 1 C(\ud835\udc67 ( \ud835\udc57\u22121)\ud835\udc5a1 ) C(\ud835\udc67 ( \ud835\udc57\u22121)\ud835\udc5a2 ) \u2211\ufe01\n\ud835\udc58\u2208C(\ud835\udc67 ( \ud835\udc57\u22121)\ud835\udc5a1 ) \u2211\ufe01 \u2208C(\ud835\udc67 ( \ud835\udc57\u22121)\ud835\udc5a2 ) \ud835\udc37 ( \ud835\udc57) \ud835\udc58 , \u2200\ud835\udc5a1, \ud835\udc5a2.\nCase \ud835\udc57=1: When \ud835\udc57 = 1, as the 0th layer only consists of the leak node, we simply connect each node of the first layer to it.\nA comment for the tiny20 graph: We mentioned that, for the tiny20 experiment, our graph contains 145 nodes and three layers (excluding the top layer). Our graph can be indeed decomposed as follows. The bottom layer contains 100 visible nodes, the second layer contains b 1003 c = 33 hidden nodes, the first layer contains b 333 c = 11 hidden nodes and the top layer only contains the leak node.\nC. Initialization procedures\nThis section describes the initialization procedures used in the different experiments.\nC.1. Tiny20 and large Tensorflow experiments\nFor each method used in the tiny20 and the Tensorflow experiments, Sections 6.2 and 6.3, we consider the four following initializations for the failure, prior, and noise, probabilities:\n1. all the failure probabilities, all the prior probabilities and all the noise probabilities are set to 0.5. 2. all the failure probabilities are set to 0.5, all the prior and noise probabilities are set to 0.1. 3. all the failure probabilities are set to 0.9, all the prior and noise probabilities are set to 0.1. 4. all the failure probabilities are set to 0.9, all the prior and noise probabilities are set to 0.5,\nOnce we have initialized the aforementioned probabilities, we initialize the parameters \u0398 accordingly by using the fact that, that for a node \ud835\udc56 and a node \ud835\udc58 \u2208 P(\ud835\udc56), the failure probability is exp(\u2212\ud835\udf03\ud835\udc58\u2192\ud835\udc56), while the noise probability\u2014or prior probability when P(\ud835\udc56) is empty\u2014is \ud835\udc5d(\ud835\udc67\ud835\udc56 = 1 | \ud835\udc67P(\ud835\udc56) = 0, \u0398) = 1 \u2212 exp(\u2212\ud835\udf030\u2192\ud835\udc56). For a given method and a given dataset, we run each initialization for 10 different seeds. We then report the results for the initialization that leads to the best averaged test results.\nC.2. BMF and BD experiments\nFor each method used in the BMF and BD experiments, Sections 6.4 and Sections 6.6, we initialize all the noise probabilities to 0.01 and keep them fixed during training. We have found this to be particularly useful to avoid a local minima where (a) the noise probabilities converge to the average number of activations of the visible variables (b) the prior probabilities converge to 0. We consider the four following initializations of the remaining failure and prior probabilities:\n1. all the failure probabilities and all the prior probabilities are set to 0.5. 2. all the failure probabilities are set to 0.5, all the prior probabilities are set to 0.1. 3. all the failure probabilities are set to 0.9, all the prior probabilities are set to 0.1. 4. all the failure probabilities are set to 0.9, all the prior probabilities are set to 0.5,\nIn addition, the solution to the BMF and to the BP problems are invariant to certain permutations. For instance, a solution to the BMF problem is invariant to applying the same permutation on the columns of \ud835\udc48 and the rows of \ud835\udc49, while a solution to the BD problem is invariant to applying the same permutation on the features indices (the first dimension) of both \ud835\udc4a and \ud835\udc46. A uniform initialization would then induce symmetries in the parameters during training. To break these symmetries, we add some centered Gaussian noise N(0, 0.1) to the failure and prior probabilities, before projecting them to [0, 1]. As before, after initializing the failure and prior probabilities (and adding the Gaussian noise), we initialize the parameters \u0398 accordingly. For a given method and experiment, we run each experiment for 10 different seeds and report the initialization that leads to the best averaged test results.\nC.3. OVPM experiment\nFor the overparametrization experiment, Section 6.5, we follow a very similar procedure to Section C.2, but we only consider the initialization methods 3 and 4, and run each initialization for 50 seeds."
        },
        {
            "heading": "D. Estimating the mode of the model posterior after inference",
            "text": "Given a test sample \ud835\udc65, we discuss how to estimate themode of themodel posterior \u210eMAP \u2248 argmax \ud835\udc5d(\u210e|\ud835\udc65,\u0398) when we use VI and MP at inference time. We use this posterior mode estimation in our experiments to compute (a) ElboMP in the tiny20 and the Tensorflow experiments, Sections 6.2 and 6.3, and (b) the test reconstruction errors in the BMF and BD experiments, Sections 6.4 and 6.6. For MP, we estimate \u210eMAP by clamping the visible variables to their observed value and running max-product with a temperature \ud835\udc47 = 0. This is exactly the inference query (b) discussed in Section 5. For VI, the inference from Ji et al. (2020) gives access to themean-field posterior parameters, that is, the parameters such that, the approximate posterior distribution factorizes as \ud835\udc5e(\u210e|\ud835\udc65) = \u220f\ud835\udc56\u2208H \ud835\udc5e\u210e\ud835\udc56\ud835\udc56 (1\u2212\ud835\udc5e\ud835\udc56)1\u2212\u210e\ud835\udc56 . We then estimate the mode of the posterior element-wise via rounding: \u210eMAP\n\ud835\udc56 = 1(\ud835\udc5e\ud835\udc56 \u2265 0.5), \u2200\ud835\udc56."
        },
        {
            "heading": "E. Performance comparisons of ElboMP and ElboVI",
            "text": "This section compares ElboMP with ElboVI for themethods evaluated in the tiny20 and the Tensorflow experiments, Sections 6.2 and 6.3.\n1. ElboVI is computed by running the inference algorithm of Ji et al. (2020)\u2014which we have reimplemented.\n2. To compute ElboMP, we estimate the posterior mode \u210eMAP as detailed in Section D, then plug it into Equation (7).\nE.1. For a binary posterior, ElboVI is a lower-bound of ElboMP\nLet us briefly start by presenting the proof of a claim we made in Section 5.3. We said that, for a binary observation \ud835\udc65 \u2208 {0, 1}\ud835\udc5d, if the posterior \u210e\u0303(\ud835\udc65, \ud835\udc47) is binary, then ElboVI is a lower-bound of ElboMP. To prove this point, let us assume that \u210e\u0303(\ud835\udc65, \ud835\udc47) is binary, let us introduce \ud835\udc67 = (\ud835\udc670, \u210e\u0303(\ud835\udc65, \ud835\udc47), \ud835\udc65) and let us recall that ElboMP is defined in Equation (7) as follows:\nL(\ud835\udc65,\u0398) = \ud835\udc5a+\ud835\udc5b\u2211\ufe01 \ud835\udc56=1 \ud835\udc67\ud835\udc56 log ( 1 \u2212 exp ( \u2212\ud835\udf030\u2192\ud835\udc56 \u2212 \u2211\ufe01 \ud835\udc58\u2208P(\ud835\udc56) \ud835\udf03\ud835\udc58\u2192\ud835\udc56\ud835\udc67\ud835\udc58 )) + (1 \u2212 \ud835\udc67\ud835\udc56) ( \u2212\ud835\udf030\u2192\ud835\udc56 \u2212 \u2211\ufe01 \ud835\udc58\u2208P(\ud835\udc56) \ud835\udf03\ud835\udc58\u2192\ud835\udc56\ud835\udc67\ud835\udc58 )\n= \ud835\udc5a+\ud835\udc5b\u2211\ufe01 \ud835\udc56=1 \ud835\udc67\ud835\udc56 \ud835\udc53 ( \ud835\udf030\u2192\ud835\udc56 + \u2211\ufe01 \ud835\udc58\u2208P(\ud835\udc56) \ud835\udf03\ud835\udc58\u2192\ud835\udc56\ud835\udc67\ud835\udc58 ) + (1 \u2212 \ud835\udc67\ud835\udc56) ( \u2212\ud835\udf030\u2192\ud835\udc56 \u2212 \u2211\ufe01 \ud835\udc58\u2208P(\ud835\udc56) \ud835\udf03\ud835\udc58\u2192\ud835\udc56\ud835\udc67\ud835\udc58 ) ,\n(11)\nwhere we have used \ud835\udc53 (\ud835\udefd) = log(1\u2212 exp(\u2212\ud835\udefd)). Equation (11) is exactly Equation (3) in Ji et al. (2020) in the case of a binary posterior. From there, as \ud835\udf030\u2192\ud835\udc56 \u2265 0 and \ud835\udf03\ud835\udc58\u2192\ud835\udc56\ud835\udc67\ud835\udc58 \u2265 0, \u2200\ud835\udc58 \u2208 P(\ud835\udc56), the authors introduced an auxiliary parameter \ud835\udc5f\ud835\udc58\u2192\ud835\udc56 for each edge connecting a non-leak parent variable to a child variable such that\n\ud835\udc5f\ud835\udc58\u2192\ud835\udc56 \u2265 0, \u2200\ud835\udc58 \u2208 P(\ud835\udc56); and \u2211\ufe01\n\ud835\udc58\u2208P(\ud835\udc56) \ud835\udc5f\ud835\udc58\u2192\ud835\udc56 = 1.\nConsequently \u2211\ud835\udc58\u2208P(\ud835\udc56) \ud835\udc5f\ud835\udc58\u2192\ud835\udc56\ud835\udc67\ud835\udc58 \u2208 [0, 1]. As \ud835\udc53 is concave, the authors use Jensen\u2019s inequality to get the following lower-bound:\n\ud835\udc53 ( \ud835\udf030\u2192\ud835\udc56 + \u2211\ufe01 \ud835\udc58\u2208P(\ud835\udc56) \ud835\udf03\ud835\udc58\u2192\ud835\udc56\ud835\udc67\ud835\udc58 ) = \ud835\udc53 (( 1 \u2212 \u2211\ufe01 \ud835\udc58\u2208P(\ud835\udc56) \ud835\udc5f\ud835\udc58\u2192\ud835\udc56\ud835\udc67\ud835\udc58 ) \ud835\udf030\u2192\ud835\udc56 + \u2211\ufe01 \ud835\udc58\u2208P(\ud835\udc56) \ud835\udc5f\ud835\udc58\u2192\ud835\udc56\ud835\udc67\ud835\udc58 ( \ud835\udf030\u2192\ud835\udc56 + \ud835\udf03\ud835\udc58\u2192\ud835\udc56 \ud835\udc5f\ud835\udc58\u2192\ud835\udc56 )) \u2265 ( 1 \u2212\n\u2211\ufe01 \ud835\udc58\u2208P(\ud835\udc56) \ud835\udc5f\ud835\udc58\u2192\ud835\udc56\ud835\udc67\ud835\udc58 ) \ud835\udc53 (\ud835\udf030\u2192\ud835\udc56) + \u2211\ufe01 \ud835\udc58\u2208P(\ud835\udc56) \ud835\udc5f\ud835\udc58\u2192\ud835\udc56\ud835\udc67\ud835\udc58 \ud835\udc53 (\ud835\udc62\ud835\udc58\u2192\ud835\udc56) where \ud835\udc62\ud835\udc58\u2192\ud835\udc56 = \ud835\udf030\u2192\ud835\udc56 + \ud835\udf03\ud835\udc58\u2192\ud835\udc56 \ud835\udc5f\ud835\udc58\u2192\ud835\udc56\n= \ud835\udc53 (\ud835\udf030\u2192\ud835\udc56) + \u2211\ufe01\n\ud835\udc58\u2208P(\ud835\udc56) \ud835\udc5f\ud835\udc58\u2192\ud835\udc56\ud835\udc67\ud835\udc58\n( \ud835\udc53 (\ud835\udc62\ud835\udc58\u2192\ud835\udc56) \u2212 \ud835\udc53 (\ud835\udf030\u2192\ud835\udc56) ) (12)\nBy pairing Equations (11) and (12) we get:\nL(\ud835\udc65,\u0398) \u2265 \ud835\udc5a+\ud835\udc5b\u2211\ufe01 \ud835\udc56=1 \ud835\udc67\ud835\udc56  \ud835\udc53 (\ud835\udf030\u2192\ud835\udc56) + \u2211\ufe01 \ud835\udc58\u2208P(\ud835\udc56) \ud835\udc5f\ud835\udc58\u2192\ud835\udc56\ud835\udc67\ud835\udc58 ( \ud835\udc53 (\ud835\udc62\ud835\udc58\u2192\ud835\udc56) \u2212 \ud835\udc53 (\ud835\udf030\u2192\ud835\udc56) ) + (1 \u2212 \ud835\udc67\ud835\udc56) ( \u2212\ud835\udf030\u2192\ud835\udc56 \u2212 \u2211\ufe01 \ud835\udc58\u2208P(\ud835\udc56) \ud835\udf03\ud835\udc58\u2192\ud835\udc56\ud835\udc67\ud835\udc58 ) . (13)\nThe right-hand size of Equation (13) is exactly ElboVI in the case of a binary posterior, as defined in Ji et al. (2020, Equation (9)). Consequently, Equation (13) proves that for a binary posterior, ElboMP is a tighter lower-bound of the intractable log-likelihood of a noisy-OR BN than ElboVI. Hence, in all our experiments, we never compute ElboVI for a binary posterior.\nE.2. Performance comparisons on the tiny20 dataset\nTable 5 reports the averaged test ElboMP and ElboVI on the tiny20 dataset. Standalone MP is trained with Algorithm 1 to optimize ElboMP. As a result, the MP parameters land in a local optima of this loss and MP reaches the highest test ElboMP. MP is also the worst performer for ElboVI as it has not been exposed to this loss during training.\nIn comparison, all the methods trained with ElboVI (including the hybrid method MP+VI) perform better at test time for ElboVI than for ElboMP. Full VI performs particularly poorly for ElboMP as it has never been exposed to it during training. Finally, Table 5 suggests that initializing the VI training with MP helps VI find a better local optima of ElboVI, which is why our hybrid method reaches the best overall lower bound\u2014while maintaining a high ElboMP.\nE.3. Performance comparisons on the large sparse Tensorflow datasets\nTable 6 reports the averaged ElboMP and ElboVI on the large sparse Tensorflow datasets. As before, standalone MP is the worst performer for ElboVI as it has not been exposed to this loss during training. Local VI is also the worst overall method for ElboMP for a similar reason. However, it performs better than MP on two datasets, which suggests that, for these datasets, standalone MP is stuck in a local optima during its training.\nOur hybrid method is the best performer for both ElboMP and ElboVI, which shows that our MP approach finds a good area of the parameters space, that is further refined during the VI optimization of ElboVI. As a result, the hybrid scheme improves the overall performance of each noisy-OR model."
        },
        {
            "heading": "F. Additional materials for the large sparse Tensorflow datasets",
            "text": "This section reports some statistics for the large Tensorflow datasets used in Section 6.3, as well as a timing comparison of the different methods used.\nF.1. Datasets statistics\nFor the five large Tensorflow datasets, Table 7 below gives access to (a) the full name of the dataset, as it appears in the catalog accessible at https://www.tensorflow.org/datasets/catalog (b) the feature name used when loading the dataset (c) the number of edges in the BNs returned by our graph generation procedure (detailed in Appendix B) (d) the train and test set sizes. In particular, the BNs returned by our procedure have a similar number of edges. This is explained by the fact that, for all the datasets, we use the same number of visible variables\u201410, 000\u2014during the preprocessing, and the same hyperparameters during the BN generation.\nF.2. Update times for local VI and MP\nTable 8 reports the update time of local VI and MP on the Tensorflow datasets, which we have defined in Section 6.3 as the average time for one gradient step. The MP gradients updates detailed in Algorithm 1 run at a very similar speed on all the datasets. Indeed, the complexity of the messages updates is similar across the datasets as (a) as the different BNs have a similar number of edges (as seen in Table 7) (b) MP does not use exploit the sparsity of the data and represents each sentence by a vector \ud835\udc65 \u2208 {0, 1}10,000.\nIn contrast, as explained in Section 3, the local models in VI represent each sentence by its active visible variables and by their ancestors. We have set the number of active visible variables per sentence\nto be at most 500, and in practice it can be lower\u2014some datasets only have a few tenths of active variables on average. Consequently, local VI represents sparse data using arrays three orders of magnitudes smaller than MP. Hence, although local VI updates its variational parameters sequentially, it is reasonably fast. Nonetheless, its update time is dataset-specific and it is two to four times slower than MP."
        },
        {
            "heading": "G. Additional material for the overparametrization experiment",
            "text": "This section contains some additional materials for the overparametrization experiment presented in Section 6.5. First, we discuss the method proposed in Buhai et al. (2020) to compute the number of GT parameters recovered during training. Second, we report the table of results associated with Figure 2.\nG.1. Computing the number of ground truth parameters recovered\nWe consider a trained noisy-OR BN with \ud835\udc3e \u2265 8 latent variables and learned parameters \u0398\u0302 = (\ud835\udc491, . . . , \ud835\udc49\ud835\udc3e , \ud835\udf031, . . . , \ud835\udf03\ud835\udc3e , \ud835\udf03\ud835\udc65). We follow the procedure of Buhai et al. (2020) to count the number of recovered GT parameters \ud835\udc491, . . . , \ud835\udc498\u2014let us trivially note that are at most eight recovered GT parameters. First, we discard the \ud835\udc49\ud835\udc58 with a prior probability 1 \u2212 exp(\u2212\ud835\udf03\ud835\udc58) lower than 0.02. Second, we perform minimum cost bipartite matching between the non-discarded learned parameters and the GT ones \ud835\udc491, . . . , \ud835\udc498, using the inf norm as the matching cost. Finally, we count as recovered all the GT parameters with a matching cost lower than 1.0.\nG.2. Table of results\nTable 9 reports the numerical results of the OVPM experiment which are displayed in Figure 2, Section 6.5. For VI, we take the numbers from Tables 2 and 5 in the appendices of Buhai et al. (2020), which are averaged over 500 repetitions. For MP, our results are averaged over 50 seeds. As in Buhai et al. (2020), we report the 95% confidence intervals of each method."
        },
        {
            "heading": "H. Additional material for the 2D blind deconvolution experiment",
            "text": "This section contains some additional materials for the 2D blind deconvolution (BD) experiment presented in Section 6.6. First, we discuss a simple example from Lazaro-Gredilla et al. (2021) which illustrates the generative process of the BD dataset. Second, we express the BD problem as a learning problem in a noisy-OR BN. Third, we define the features IOU metric used in Table 4. Finally, we display the continuous and binary features learned by each method, as well as the reconstructed test images for MP and PMP.\nH.1. A simple example\nFigure 5 uses a simple example from Lazaro-Gredilla et al. (2021) to illustrate the generative process of the BD dataset. The small dataset considered here only contains two independent binary images: each image \ud835\udc4b \u2208 {0, 1}15\u00d715 is formed by convolving the shared binary features \ud835\udc4a \u2208 {0, 1}5\u00d76\u00d76 with the image-specific binary locations \ud835\udc46 \u2208 {0, 1}5\u00d710\u00d710.\n\ud835\udc4a contains five features, each of size 6\u00d76. \ud835\udc46 contains the locations of the features, which are sampled at random using an independent Bernoulli prior per entry: \ud835\udc5d(\ud835\udc46 \ud835\udc53 ,\ud835\udc56, \ud835\udc57 = 1) = 0.01, \u2200 \ud835\udc53 , \ud835\udc56, \ud835\udc57. The top (resp. bottom) row of \ud835\udc46 indicates the locations of the features in the top (resp. bottom) image of \ud835\udc4b . The \ud835\udc57th column of \ud835\udc46 corresponds to the locations of the \ud835\udc57th feature in \ud835\udc4a. For instance, the two activations on the right of the top-left block of \ud835\udc46, means that the first feature in \ud835\udc4a will appear twice on the right of the first image of \ud835\udc4b . This is verified by the two anti-diagonal lines in the top row of \ud835\udc4b .\nH.2. The BD problem can be expressed as learning a noisy-OR Bayesian network\nThe 2D BD problem can be expressed as a learning problem in the noisy-OR BN detailed below. Let \ud835\udc41 \u00d7 \ud835\udc43 be the size of an image \ud835\udc4b . As \ud835\udc4a is of size \ud835\udc5bfeat \u00d7 featheight \u00d7 featwidth, \ud835\udc46 is of size \ud835\udc5bimages \u00d7 actheight \u00d7 actwidth, and \ud835\udc4b is produced from \ud835\udc46 and \ud835\udc4a by convolution, let us first note that\n\ud835\udc41 = actheight + featheight \u2212 1 \ud835\udc43 = actwidth + featwidth \u2212 1\nIn addition, for a pixel with indices (\ud835\udc5b, \ud835\udc5d), let us introduce the set of indices:\nI(\ud835\udc5b, \ud835\udc5d) =  (\ud835\udc56, \ud835\udc57, \ud835\udc58, ) : 1 \u2264 \ud835\udc56 \u2264 actheight 1 \u2264 \ud835\udc57 \u2264 actwidth 1 \u2264 \ud835\udc58 \u2264 featheight 1 \u2264 \u2264 featwidth \ud835\udc56 + \ud835\udc58 \u2212 1 = \ud835\udc5b \ud835\udc57 + \u2212 1 = \ud835\udc5d  .\nThe BD problem is equivalent to learning a noisy-OR BN where (a) the visible nodes are \ud835\udc4b (b) the hidden nodes are \ud835\udc46 (c) the positive continuous parameters are \ud835\udf03\ud835\udc65 \u2208 \u211d+, \ud835\udf031, . . . , \ud835\udf03\ud835\udc5bfeat \u2208 \u211d+, and ?\u0302? \u2208\u211d \ud835\udc5bfeat\u00d7featheight\u00d7featwidth + and we denote \u0398 = (\ud835\udf03\ud835\udc65 , \ud835\udf031, . . . , \ud835\udf03\ud835\udc5bfeat , ?\u0302?) (d) the prior probability of each entry of S, for the \ud835\udc53 th feature \ud835\udc53 is \ud835\udc5d(\ud835\udc46 \ud835\udc53 ,\ud835\udc56, \ud835\udc57 = 1) = 1 \u2212 exp(\u2212\ud835\udf03 \ud835\udc53 ),\u2200\ud835\udc56, \ud835\udc57 (e) the conditional probability of the pixel \ud835\udc4b\ud835\udc5b\ud835\udc5d is given by\n\ud835\udc5d(\ud835\udc4b\ud835\udc5b\ud835\udc5d = 1 | \ud835\udc46,\u0398) = 1 \u2212 exp ( \u2212 \ud835\udf03\ud835\udc65 \u2212 \u2211\ufe01 1\u2264 \ud835\udc53 \u2264\ud835\udc5bfeat \u2211\ufe01 (\ud835\udc56, \ud835\udc57,\ud835\udc58, ) \u2208I (\ud835\udc5b,\ud835\udc5d) \ud835\udc46 \ud835\udc53 ,\ud835\udc56, \ud835\udc57?\u0302? \ud835\udc53 ,\ud835\udc58, ) In particular, the noise probability of each visible variable is equal to 1 \u2212 exp(\u2212\ud835\udf03\ud835\udc65).\nH.3. Computing the features intersection-over-union\nLet us first define the intersection-over-union (IOU) between a thresholded learned feature ?\u0302?thre \ud835\udc57 \u2208 {0, 1}6\u00d76 and a GT feature\ud835\udc4a\ud835\udc58 \u2208 {0, 1}5\u00d75. To do so, we introduce the four sub-features ?\u0302?thre\ud835\udc57,1 , . . . , ?\u0302?thre\ud835\udc57,4 \u2208 {0, 1}5\u00d75 of the same size as\ud835\udc4a\ud835\udc58, obtained by removing the first or last row, and the first or last column of ?\u0302?thre\n\ud835\udc57 . We then compute:\nIOU(?\u0302?thre\ud835\udc57 ,\ud835\udc4a\ud835\udc58) = max =1,...,4  \u2211 1\u2264\ud835\udc5b,\ud835\udc5d\u22645 AND ( ( ?\u0302?thre \ud835\udc57, ) \ud835\udc5b\ud835\udc5d = 1, (\ud835\udc4a\ud835\udc58)\ud835\udc5b\ud835\udc5d = 1)\u2211 1\u2264\ud835\udc5b,\ud835\udc5d\u22645 OR ( ( ?\u0302?thre \ud835\udc57, ) \ud835\udc5b\ud835\udc5d = 1, (\ud835\udc4a\ud835\udc58)\ud835\udc5b\ud835\udc5d = 1)  .\nThe IOU is always between 0 and 1: an IOU of 0 means that ?\u0302?thre \ud835\udc57 = 0 whereas an IOU of one means that one of the sub-features ?\u0302?thre\n\ud835\udc57,1 , . . . , ?\u0302? thre \ud835\udc57,4 is equal to \ud835\udc4a\ud835\udc58.\nAfter training our noisy-OR BN on the BD problem, we perform minimum bipartite matching between the learned binary features ?\u0302?thre1 , . . . , ?\u0302?thre5 and the GT binary features\ud835\udc4a1, . . . ,\ud835\udc4a4, using the opposite of the IOU as the matching cost\u2014as we want to maximize the IOU. We then define the features IOU as the average matching cost: a feature IOU of 1.0 means that we have recovered the four GT features whereas an IOU of 0 means that training has not learned any information.\nH.4. Learned binary features\nOur next Figure 6 plots the five continuous parameters ?\u0302? and the corresponding binary features ?\u0302?thre learned by MP, VI, and PMP for each of the 10 seeds. Note that the order of the features is not relevant here, as it depends on the random noise added to the unaries of each model during the initialization\u2014as discussed in Appendix A. VI completely fails at this task and does not learn any features. As PMP directly turns to posterior inference, the learned features ?\u0302? are binary so we only have one plot. PMP perfectly recovers the\nfour GT features \ud835\udc4a for seven of the ten runs. It misses two features on one run, and only misses one pixel of one feature on two runs. As the learned ?\u0302?thre contains five features while the GT \ud835\udc4a only contains four features, each run also learns an extra feature. However, PMP does not provide a way to discard this extra element. In contrast, MP successfully recovers the four GT features\u2014as well as an extra one\u2014for nine runs, and only misses one pixel of one feature for the other run. This is why MP reaches the highest features IOU in Table 4. The noisy-OR BN trained with MP also learns a prior probability for each feature: the additional feature is always the one with the lowest prior, and can be easily discarded.\nH.5. Reconstructed test images\nOur last Figure 7 compares the performance of MP and PMP for reconstructing the test scenes on one seed selected at random. We see that PMP performs well, and that our method achieves an almost perfect test reconstruction, which explains that it reaches the lowest test RE in Table 4, Section 6.6."
        }
    ],
    "title": "Learning noisy-OR Bayesian Networks with Max-Product Belief Propagation",
    "year": 2023
}