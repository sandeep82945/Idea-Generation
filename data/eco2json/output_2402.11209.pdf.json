{
    "abstractText": "Fraudulent or illegal activities are ubiquitous across many applications and involve users bypassing the rule of law, often with the strategic aim of obtaining some benefit that would otherwise be unattainable within the bounds of lawful conduct. However, user fraud is detrimental, as it may compromise safety or impose disproportionate negative externalities on particular population groups. To mitigate the potential harms of users engaging in fraudulent activities, we study the problem of policing such fraud as a security game between an administrator and users. In this game, an administrator deploys R security resources (e.g., police officers) across L locations and levies fines against users engaging in fraudulent or illegal activities at those locations. For this security game, we study both welfare and revenue maximization administrator objectives. In both settings, we show that the problem of computing the optimal administrator strategy is NP-hard and develop natural greedy algorithm variants for the respective settings that achieve at least half the welfare or revenue as the welfare-maximizing or revenue-maximizing solutions, respectively. We also establish a resource augmentation guarantee that our proposed greedy algorithms with just one additional resource, i.e., R + 1 resources, achieve at least the same welfare (revenue) as the welfare-maximizing (revenue-maximizing) outcome with R resources that is NP-hard to compute. Finally, since the welfare and revenue-maximizing solutions can differ significantly, we present a framework inspired by contract theory, wherein a revenue-maximizing administrator is compensated through contracts for the level of welfare it contributes to the system. Beyond extending our theoretical results in the welfare and revenue maximization settings to studying equilibrium strategies in the contract game, we also present numerical experiments highlighting the effectiveness of using contracts in bridging the gap between the revenue and welfare-maximizing administrator outcomes.",
    "authors": [
        {
            "affiliations": [],
            "name": "Devansh Jalota"
        },
        {
            "affiliations": [],
            "name": "Michael Ostrovsky"
        },
        {
            "affiliations": [],
            "name": "Marco Pavone"
        }
    ],
    "id": "SP:681c72ba7218f38704f5ca09b42e707902af3236",
    "references": [
        {
            "authors": [
                "Andreas Bjerre-Nielsen",
                "Lykke Sterll Christensen",
                "Mikkel Host Gandil",
                "Hans Henrik Sievertsen"
            ],
            "title": "Playing the system: address manipulation and access to schools",
            "venue": "Papers 2305.18949,",
            "year": 2023
        },
        {
            "authors": [
                "Philippe Askenazy",
                "Thomas Breda",
                "Flavien Moreau",
                "Vladimir Pecheu"
            ],
            "title": "Do French companies underreport their workforce at 49 employees to get around the law",
            "venue": "PSE Working Papers halshs-03828729,",
            "year": 2022
        },
        {
            "authors": [
                "Andrew Estornell",
                "Sanmay Das",
                "Yevgeniy Vorobeychik"
            ],
            "title": "Incentivizing truthfulness through audits in strategic classification",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Andrew Estornell",
                "Yatong Chen",
                "Sanmay Das",
                "Yang Liu",
                "Yevgeniy Vorobeychik"
            ],
            "title": "Incentivizing recourse through auditing in strategic classification",
            "venue": "Proceedings of the Thirty- Second International Joint Conference on Artificial Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Eduardo Perez-Richet",
                "Vasiliki Skreta"
            ],
            "title": "Fraud-proof non-market allocation mechanisms",
            "venue": "Technical report, Working paper,",
            "year": 2023
        },
        {
            "authors": [
                "Taylor Lundy",
                "Alexander Wei",
                "Hu Fu",
                "Scott Duke Kominers",
                "Kevin Leyton-Brown"
            ],
            "title": "Allocation for social good: Auditing mechanisms for utility maximization",
            "venue": "In Proceedings of the 2019 ACM Conference on Economics and Computation, EC",
            "year": 2019
        },
        {
            "authors": [
                "Lee Bolton"
            ],
            "title": "Manipulation of the waitlist priority of the organ allocation system through the escalation of medical therapies",
            "venue": "OPTN/UNOS EthicsCommittee,",
            "year": 2018
        },
        {
            "authors": [
                "Milind Tambe"
            ],
            "title": "Security and game theory: algorithms, deployed systems, lessons learned",
            "venue": "Cambridge university press,",
            "year": 2011
        },
        {
            "authors": [
                "Robert Cervero"
            ],
            "title": "Informal transport in the developing world",
            "venue": "UN-HABITAT,",
            "year": 2000
        },
        {
            "authors": [
                "Robert Cervero",
                "Aaron Golub"
            ],
            "title": "Informal transport: A global perspective",
            "venue": "Transport Policy,",
            "year": 2007
        },
        {
            "authors": [
                "Andreas Neumann",
                "Daniel R\u00f6der",
                "Johan W. Joubert"
            ],
            "title": "Toward a simulation of minibuses in south africa",
            "venue": "Journal of Transport and Land Use,",
            "year": 2015
        },
        {
            "authors": [
                "Ilan Salomon",
                "Lionel Silman"
            ],
            "title": "Scheduled bus and sherut taxi operation in israel",
            "venue": "Transportation Research Part B: Methodological,",
            "year": 1985
        },
        {
            "authors": [
                "F Zwick"
            ],
            "title": "Analysis and simulation of Santiago de Chile\u2019s colectivo system",
            "venue": "PhD thesis, Bachelor\u2019s thesis. TU Berlin,",
            "year": 2017
        },
        {
            "authors": [
                "Ravi Gadapalli"
            ],
            "title": "Role of intermediate public transport in indian cities",
            "venue": "Economic and Political Weekly,",
            "year": 2016
        },
        {
            "authors": [
                "Rupa Subramanya"
            ],
            "title": "Economics journal: Stupid taxi rules in mumbai",
            "venue": "https://www.wsj.com/articles/ BL-IRTB-16187,",
            "year": 2012
        },
        {
            "authors": [
                "Chris Wood"
            ],
            "title": "The art of queue-jumping on hong kong trams",
            "venue": "https://www.scmp.com/magazines/ post-magazine/short-reads/article/2106273/art-queue-jumping-hong-kong-trams,",
            "year": 2017
        },
        {
            "authors": [
                "Yongbin Li",
                "Jing Xu",
                "Fang Wang",
                "Bin Wang",
                "Liqun Liu",
                "Wanli Hou",
                "Hong Fan",
                "Yeqing Tong",
                "Juan Zhang",
                "Zuxun Lu"
            ],
            "title": "Overprescribing in china, driven by financial incentives, results in very high use of antibiotics, injections, and corticosteroids",
            "venue": "Health affairs,",
            "year": 2012
        },
        {
            "authors": [
                "Yan S Kim",
                "Eric C Kleerup",
                "Patricia A Ganz",
                "Ninez A Ponce",
                "Karl A Lorenz",
                "Jack Needleman"
            ],
            "title": "Medicare payment policy creates incentives for long-term care hospitals to time discharges for maximum reimbursement",
            "venue": "Health Affairs,",
            "year": 2015
        },
        {
            "authors": [
                "James Pita",
                "Manish Jain",
                "Janusz Marecki",
                "Fernando Ord\u00f3\u00f1ez",
                "Christopher Portway",
                "Milind Tambe",
                "Craig Western",
                "Praveen Paruchuri",
                "Sarit Kraus"
            ],
            "title": "Deployed armor protection: the application of a game theoretic model for security at the los angeles international airport",
            "venue": "In Proceedings of the 7th International Joint Conference on Autonomous Agents and Multiagent Systems: Industrial Track,",
            "year": 2008
        },
        {
            "authors": [
                "Eric Shieh",
                "Bo An",
                "Rong Yang",
                "Milind Tambe",
                "Craig Baldwin",
                "Joseph DiRenzo",
                "Ben Maule",
                "Garrett Meyer"
            ],
            "title": "Protect: an application of computational game theory for the security of the ports of the united states",
            "venue": "In Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence,",
            "year": 2012
        },
        {
            "authors": [
                "Zhengyu Yin",
                "Albert Xin Jiang",
                "Milind Tambe",
                "Christopher Kiekintveld",
                "Kevin Leyton-Brown",
                "Tuomas Sandholm",
                "John P Sullivan"
            ],
            "title": "Trusts: Scheduling randomized patrols for fare inspection in transit systems using game theory",
            "venue": "AI magazine,",
            "year": 2012
        },
        {
            "authors": [
                "Lily Xu",
                "Elizabeth Bondi",
                "Fei Fang",
                "Andrew Perrault",
                "Kai Wang",
                "Milind Tambe"
            ],
            "title": "Dual-mandate patrols: Multi-armed bandits for green security",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Rong Yang",
                "Benjamin Ford",
                "Milind Tambe",
                "Andrew Lemieux"
            ],
            "title": "Adaptive resource allocation for wildlife protection against illegal poachers",
            "venue": "In Proceedings of the 2014 International Conference on Autonomous Agents and Multi-Agent Systems,",
            "year": 2014
        },
        {
            "authors": [
                "Fei Fang",
                "Peter Stone",
                "Milind Tambe"
            ],
            "title": "When security games go green: designing defender strategies to prevent poaching and illegal fishing",
            "venue": "In Proceedings of the 24th International Conference on Artificial Intelligence,",
            "year": 2015
        },
        {
            "authors": [
                "Fei Fang",
                "Thanh H. Nguyen",
                "Rob Pickles",
                "Wai Y. Lam",
                "Gopalasamy R. Clements",
                "Bo An",
                "Amandeep Singh",
                "Milind Tambe",
                "Andrew Lemieux"
            ],
            "title": "Deploying paws: field optimization of the protection assistant for wildlife security",
            "venue": "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence,",
            "year": 2016
        },
        {
            "authors": [
                "Arunesh Sinha",
                "Fei Fang",
                "Bo An",
                "Christopher Kiekintveld",
                "Milind Tambe"
            ],
            "title": "Stackelberg security games: Looking beyond a decade of success",
            "venue": "In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Dorit S Hochba"
            ],
            "title": "Approximation algorithms for np-hard problems",
            "venue": "ACM Sigact News,",
            "year": 1997
        },
        {
            "authors": [
                "David P Williamson",
                "David B Shmoys"
            ],
            "title": "The design of approximation algorithms",
            "venue": "Cambridge university press,",
            "year": 2011
        },
        {
            "authors": [
                "David Kempe",
                "Jon Kleinberg",
                "\u00c9va Tardos"
            ],
            "title": "Maximizing the spread of influence through a social 19 network",
            "venue": "In Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
            "year": 2003
        },
        {
            "authors": [
                "Tim Roughgarden"
            ],
            "title": "Resource Augmentation, page 72\u201392",
            "year": 2021
        },
        {
            "authors": [
                "Mohammad Akbarpour",
                "Suraj Malladi",
                "Amin Saberi"
            ],
            "title": "Just a Few Seeds More: Value of Network Information for Diffusion",
            "venue": "Research Papers 3678,",
            "year": 2018
        },
        {
            "authors": [
                "Mohammad Akbarpour",
                "Yeganeh Alimohammadi",
                "Shengwu Li",
                "Amin Saberi"
            ],
            "title": "The value of excess supply in spatial matching markets",
            "venue": "In Proceedings of the 23rd ACM Conference on Economics and Computation, EC \u201922,",
            "year": 2022
        },
        {
            "authors": [
                "Moshe Babaioff",
                "Kira Goldner",
                "Yannai A. Gonczarowski"
            ],
            "title": "Bulow-Klemperer-Style Results for Welfare Maximization in Two-Sided Markets, pages 2452\u20132471",
            "year": 2020
        },
        {
            "authors": [
                "Jeremy Bulow",
                "Paul Klemperer"
            ],
            "title": "Auctions versus negotiations",
            "venue": "The American Economic Review,",
            "year": 1996
        },
        {
            "authors": [
                "Jason D. Hartline",
                "Tim Roughgarden"
            ],
            "title": "Simple versus optimal mechanisms",
            "venue": "In Proceedings of the 10th ACM Conference on Electronic Commerce, EC",
            "year": 2009
        },
        {
            "authors": [
                "Sergiu Hart",
                "Noam Nisan"
            ],
            "title": "Approximate revenue maximization with multiple items",
            "venue": "Journal of Economic Theory,",
            "year": 2017
        },
        {
            "authors": [
                "Oliver Hart",
                "Bengt Holmstr\u00f6m"
            ],
            "title": "The theory of contracts, page 71\u2013156",
            "venue": "Econometric Society Monographs",
            "year": 1987
        },
        {
            "authors": [
                "Bengt Holmstr\u00f6m"
            ],
            "title": "Moral hazard and observability",
            "venue": "The Bell journal of economics,",
            "year": 1979
        },
        {
            "authors": [
                "Sanford J Grossman",
                "Oliver D Hart"
            ],
            "title": "An analysis of the principal-agent problem",
            "venue": "In Foundations of Insurance Economics: Readings in Economics and Finance,",
            "year": 1992
        },
        {
            "authors": [
                "Paul D\u00fctting",
                "Tomer Ezra",
                "Michal Feldman",
                "Thomas Kesselheim"
            ],
            "title": "Combinatorial contracts",
            "venue": "IEEE 62nd Annual Symposium on Foundations of Computer Science (FOCS),",
            "year": 2021
        },
        {
            "authors": [
                "Paul Dutting",
                "Michal Feldman",
                "Yoav Gal Tzur"
            ],
            "title": "Combinatorial contracts beyond gross substitutes",
            "venue": "In Proceedings of the 2024 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA),",
            "year": 2024
        },
        {
            "authors": [
                "Moshe Babaioff",
                "Michal Feldman",
                "Noam Nisan"
            ],
            "title": "Combinatorial agency",
            "venue": "In Proceedings of the 7th ACM Conference on Electronic Commerce, EC",
            "year": 2006
        },
        {
            "authors": [
                "Paul D\u00fctting",
                "Tomer Ezra",
                "Michal Feldman",
                "Thomas Kesselheim"
            ],
            "title": "Multi-agent contracts",
            "venue": "In Proceedings of the 55th Annual ACM Symposium on Theory of Computing,",
            "year": 2023
        },
        {
            "authors": [
                "Matthew Prensky",
                "Rob Johnson"
            ],
            "title": "It\u2019s a trap! small towns across us use traffic tickets to collect big money from drivers",
            "venue": "https://www.usatoday.com/story/opinion/2023/12/26/ police-speeding-traffic-tickets-revenue-civil-rights/71970613007/,",
            "year": 2023
        },
        {
            "authors": [
                "Christopher Kiekintveld",
                "Manish Jain",
                "Jason Tsai",
                "James Pita",
                "Fernando Ord\u00f3\u00f1ez",
                "Milind Tambe"
            ],
            "title": "Computing optimal randomized resource allocations for massive security games",
            "venue": "In Proceedings of The 8th International Conference on Autonomous Agents and Multiagent Systems - Volume 1,",
            "year": 2009
        },
        {
            "authors": [
                "Mikhail Solodov"
            ],
            "title": "An explicit descent method for bilevel convex optimization",
            "venue": "Journal of Convex Analysis,",
            "year": 2007
        },
        {
            "authors": [
                "Vincent Conitzer",
                "Tuomas Sandholm"
            ],
            "title": "Computing the optimal strategy to commit to",
            "venue": "In Proceedings of the 7th ACM Conference on Electronic Commerce, EC",
            "year": 2006
        },
        {
            "authors": [
                "Gary S Becker"
            ],
            "title": "Crime and punishment: An economic approach",
            "venue": "Journal of political economy,",
            "year": 1968
        },
        {
            "authors": [
                "The Economist"
            ],
            "title": "Looking for the african middle class? head to the bus park",
            "venue": "https://www.economist.com/middle-east-and-africa/2023/05/18/ looking-for-the-african-middle-class-head-to-the-bus-park,",
            "year": 2017
        }
    ],
    "sections": [
        {
            "text": "To mitigate the potential harms of users engaging in fraudulent activities, we study the problem of policing such fraud as a security game between an administrator and users. In this game, an administrator deploys R security resources (e.g., police officers) across L locations and levies fines against users engaging in fraudulent or illegal activities at those locations. For this security game, we study both welfare and revenue maximization administrator objectives. In both settings, we show that the problem of computing the optimal administrator strategy is NP-hard and develop natural greedy algorithm variants for the respective settings that achieve at least half the welfare or revenue as the welfare-maximizing or revenue-maximizing solutions, respectively. We also establish a resource augmentation guarantee that our proposed greedy algorithms with just one additional resource, i.e., R + 1 resources, achieve at least the same welfare (revenue) as the welfare-maximizing (revenue-maximizing) outcome with R resources that is NP-hard to compute.\nFinally, since the welfare and revenue-maximizing solutions can differ significantly, we present a framework inspired by contract theory, wherein a revenue-maximizing administrator is compensated through contracts for the level of welfare it contributes to the system. Beyond extending our theoretical results in the welfare and revenue maximization settings to studying equilibrium strategies in the contract game, we also present numerical experiments highlighting the effectiveness of using contracts in bridging the gap between the revenue and welfare-maximizing administrator outcomes."
        },
        {
            "heading": "1 Introduction",
            "text": "Fraudulent or illegal activities that involve users bypassing the rule of law are ubiquitous across applications and arise as users seek to strategically obtain some benefit that would otherwise not be attainable within the bounds of lawful conduct. For instance, in transportation networks, users often drive above the speed limit to reduce their travel times, and, in school choice contexts, parents often misreport their home addresses to admit their children to better public schools [1]. Similar issues of users engaging in fraud arise in other domains, including labor markets [2], strategic classification [3, 4], non-market allocation mechanisms [5], and resource allocation [6].\nHowever, fraudulent or illegal activities can be detrimental, as even a few users engaging in such activities can compromise safety, result in disproportionate negative externalities to particular groups of the population, or hamper efficiency. In transportation networks, users driving above the speed limit can compromise road safety, and, in school choice settings, sophisticated gaming by some parents, who typically belong to higherincome strata, often adversely affects users not engaging in such practices [1]. Moreover, in healthcare contexts, the manipulation of patient\u2019s priority by transplant providers in organ transplant waiting lists often results in significant reductions in organ donations [7, 8]. We present further detailed examples of users engaging in fraudulent activities and the associated harms to other groups of the population in Section 1.1.\nThe prevalence of fraud and its associated harms across applications poses critical security challenges and requires policing to deter users from engaging in such activities. Central to the problem of policing\nar X\niv :2\n40 2.\n11 20\n9v 2\n[ cs\n.G T\n] 2\n1 Fe\nb 20\nfraudulent or illegal activities is a resource allocation task, which involves allocating a limited set of security resources (e.g., police officers) to mitigate the level of fraud in the system. However, the challenge in policing fraudulent activities is that only limited security resources are available, i.e., providing complete coverage to prevent fraud is not possible. This resource limitation raises a fundamental question of how to best allocate the available resources to protect against fraudulent or illegal activities at various susceptible nodes or locations in a system.\nTo answer this question, we study the problem of policing fraudulent activities as a security game between an administrator and fraudulent users at different nodes or locations in a system. In our security game, the administrator can allocate a budget of security resources across various locations and levy fines against users engaging in fraud. For our studied security game, we show that simple algorithms with a low computational overhead can achieve a good performance relative to the administrator\u2019s optimal strategies, which are NP-hard to compute. To analyze the associated equilibria and the corresponding optimal administrator strategies, we leverage and combine techniques from optimization theory, linear programming, and approximation algorithms.\nOur modeling assumptions and, in particular, considering fines as a mechanism to deter fraudulent users are motivated by several real-world applications, e.g., users typically pay fines for road traffic or speeding violations. Moreover, in line with prior work on security games [9], we adopt a game-theoretic framework to study the problem of deploying security resources to mitigate fraudulent and illegal activities. We do so as a game-theoretic framework enables us to incorporate the preferences of both the administrator and users and predictions for how a fraudulent user will respond to a resource allocation policy and the corresponding fines set by the administrator."
        },
        {
            "heading": "1.1 Examples",
            "text": "This section presents detailed examples of real-world settings where users engage in fraudulent or illegal activities. Our examples help further contextualize our studied problem setting and provide a grounding for the model we develop in Section 3. While we present two examples, our developed framework more generally applies to a broad range of settings where users engage in fraud.\nIntermediate Public Transport (IPT): IPT comprises informal modes of transport that are pivotal in serving the last-mile connectivity needs in many developing nation cities, particularly when the formal public transportation system has inadequate capacity to serve the travel demands of users [10, 11]. IPT services typically entail mini-buses [12], share-taxis [13, 14], or auto-rickshaws [15].\nWhile IPT services offer affordable on-demand last-mile connectivity, commuters often face long wait times during rush hours [16], which frequently results in disorderly queues and, in particular, illicit behavior by some commuters who jump the queue to reduce their wait times [17, 10]. For instance, some male passengers who have recently arrived to avail of an IPT service, e.g., at a mini-bus stand, might bypass the queue and board a moving vehicle as it arrives to pick up passengers. Such alighting of moving vehicles is more challenging for women or less-abled travelers, who typically bear the burden of disproportionately high wait times when using IPT systems.\nHealthcare: In healthcare, medication over-prescription is a global problem and is particularly prevalent in places where a large portion of a healthcare provider\u2019s income relies on drug sales [18]. These perverse financial incentives for health care providers not only fleece patients as they purchase more medications but also could be potentially detrimental to their health. Analogous to the over-prescription of medications, hospitals often keep patients longer than is required, which enables the hospital to earn more money from the patient occupying a bed [19].\nBoth of these examples highlight the need for policing to deter defaulting users from engaging in such fraudulent activities at susceptible nodes or locations in a system, e.g., hospitals in a healthcare context or mini-bus stands where users queue to avail a mini-bus in a transportation context."
        },
        {
            "heading": "1.2 Our Contributions",
            "text": "Motivated by the prevalence and detrimental effects of fraudulent and illegal activities across applications, we study a security game between an administrator and users engaging in fraud at susceptible nodes or locations in a system. In this security game, the administrator has R security resources that it can allocate across these locations and levy fines against defaulting users to deter them from engaging in fraud. For\nthis security game, we study the resulting equilibria and the optimal administrator strategies under both the welfare maximization and revenue maximization objectives, which we introduce in Section 3, under two informational assumptions: (i) deterministic, where the administrator knows each location\u2019s type, i.e., the administrator has complete knowledge of the total number of fraudulent users, the benefit that fraudulent users derive when engaging in fraud, and the value of preventing users from engaging in fraud at each location, and (ii) probabilistic, where the administrator only has distributional information on each location\u2019s type.\nWe first study the deterministic setting in Section 4, where we develop a natural greedy procedure to compute the administrator\u2019s revenue-maximizing strategy and show that the problem of computing the administrator\u2019s welfare-maximizing strategy is NP-hard. To that end, in the welfare-maximization setting, we develop a variant of a greedy algorithm, different from that in the revenue-maximization setting, that achieves at least half the welfare compared to that of the welfare-maximizing solution. Moreover, we establish a resource augmentation guarantee that our proposed greedy algorithm with just one additional resource, i.e., R+1 resources, achieves at least the same welfare as the welfare-maximizing solution with R resources that is NP-hard to compute. To establish the hardness, approximation, and resource augmentation guarantees, we develop and leverage properties of the optimal solution to the administrator\u2019s welfare-maximization problem and geometric insights based on the structure of the administrator\u2019s welfare function.\nNext, in Section 5, we study the probabilistic setting, where we show that, unlike in the deterministic case, computing the expected revenue maximizing strategy of the administrator is NP-hard. Yet, we develop variants of greedy algorithms that achieve similar approximation and resource augmentation guarantees to those in the deterministic setting for the expected revenue and welfare maximization administrator objectives. The crux of establishing our algorithmic guarantees involves constructing a monotone concave upper approximation (MCUA) of the expected revenue and welfare functions of the administrator (see Section 5.3 for more details on the MCUA) and analyzing the optimal solutions of the linear programs that optimize the corresponding MCUAs. Our results shed light on the benefits of using simple algorithms, e.g., variants of the greedy algorithm, and highlight the value of recruiting one additional security resource rather than expending computational effort in solving for the optimal administrator strategies that are NP-hard to compute.\nFinally, since the revenue and welfare maximizing outcomes can differ significantly and, in particular, a revenue-maximization administrator objective can substantially hamper welfare, in Section 6, we extend our security game framework to incorporate contracts. In this contract game, a welfare-maximizing principal offers contracts to a revenue-maximizing administrator to compensate it for the welfare it contributes to the system. For this contract game, we show that our developed algorithmic approaches and theoretical guarantees for the earlier studied welfare and revenue maximization administrator objectives naturally carry forward in studying optimal administrator strategies. Moreover, we introduce a dense-sampling approach to compute a near-optimal solution to the principal\u2019s problem of selecting an optimal contract to maximize its payoff. Finally, we present numerical experiments based on a case study of queue jumping in IPT services, which highlight the effectiveness of contracts in bridging the gap between welfare and revenue-maximizing outcomes.\nIn the appendix, we provide proofs and extensions of theoretical results omitted from the main text, present numerical implementation details along with additional numerical results, and highlight extensions of the model presented in this work, which opens directions for future work."
        },
        {
            "heading": "2 Related Literature",
            "text": "Game theory has served as a foundational paradigm in studying multi-agent systems wherein agents pursue their selfish interests [20]. Among the many successful applications of game theory, it has, in particular, gained traction in security applications, where the problem of protecting essential security resources is formulated as a game, wherein the objective of the security agency is to compute an optimal strategy to deploy its resources to prevent security breaches, given that the adversary will optimize its utility on observing this strategy. Security games have found many applications, including protecting security checkpoints at airports [21], protecting shipping ports [22], fare inspections in transit systems [23], and more recently, in green security contexts [24, 25, 26, 27]. For a more detailed review of the state-of-the-art on security games, see [9, 28].\nWhile our work contributes to the security games literature and, more generally, to the literature on\nusing game theoretic approaches to mitigate fraud, our modeling framework, game structure, and solution methodology differs from prior works on security games in several ways. First, unlike classical security games, where adversaries typically have an allocation task of determining a utility-maximizing set of locations to target to perform security breaches, in our setting, the role of users (i.e., the adversaries) is to decide whether or not to commit fraud at their respective locations. Next, while we also consider mixed-strategy (or randomized) resource allocation policies of the administrator as in prior work on security games, we depart from much of the previous security games literature as we explicitly model fines that administrators levy on defaulting users. Consequently, given the differences in our modeling assumptions and game structure, rather than developing large-scale mixed-integer linear programs to compute equilibria as in past works on security games, we leverage the structure of the payoffs of the administrator and users induced by the fines in our setting to uncover novel geometric and structural insights and develop algorithms based on linear programming upper bounds with constant factor approximation guarantees.\nMethodologically, our work aligns with the literature on approximation algorithms for NP-hard problems [29, 30, 31], as we also develop polynomial time algorithms that achieve a constant factor approximation to the optimal solutions that are NP-hard to compute. Moreover, our work contributes to the literature on beyond worst-case algorithm design by developing resource augmentation guarantees [32], wherein an algorithm\u2019s performance is compared to the optimal solution handicapped with fewer resources. In particular, as has been demonstrated in several contexts [33, 34, 35], we obtain Bulow-Klemperer [36] style guarantees for our problem setting as we establish that simple greedy-like algorithms with one additional resource achieve the same or better performance in terms of maximizing revenue or welfare as the optimal solutions (which are NP-hard to compute) with no extra resources. Overall, our obtained guarantees for the developed greedy algorithms contribute to the broader literature on simplicity versus optimality in algorithm design [37, 38].\nOur work is also related to contract theory [39, 40], which typically considers a principal-agent problem where a principal delegates a task to an agent who takes a (possibly) costly action, unobservable to the principal, that triggers a distribution over rewards [41, 42, 43, 44, 45, 46]. While we also study an optimal contract problem, unlike classical principal-agent models, we use contracts as a mechanism to bridge the gap between the revenue and welfare maximizing outcomes."
        },
        {
            "heading": "3 Model and Preliminaries",
            "text": "This section presents a model of our security game and introduces the strategies and payoffs of the administrator and users. For additional discussions of some of our modeling assumptions beyond those presented in this section, we refer to Appendix A."
        },
        {
            "heading": "3.1 Parameters of Security Game",
            "text": "We consider a security game where an administrator seeks to allocate a budget of R security resources (e.g., police officers) across a set of L locations susceptible to fraud and levies a fine of k against users found engaging in fraud. In this security game, each location l \u2208 L corresponds to a type, specified by a triple \u0398l = (\u039bl, dl, vl), where \u039bl > 0 denotes the number of users engaging in fraud, dl > 0 corresponds to the benefit received by users who engage in fraud, and vl > 0 represents the value or welfare to the administrator for allocating a security resource to mitigate fraud at location l. We note that the administrator\u2019s welfare vl can serve as a proxy to capture many possible administrator objectives, e.g., the total level of fraud at a given location that an administrator can mitigate or the extent of the negative externality imposed by fraudulent users on those not engaging in fraud.\nTo make our model more concrete, we now elucidate an example of a location\u2019s type \u0398l in the context of queue jumping in intermediate public transport services. In this context, \u039bl corresponds to the number of users engaging in queue jumping at a mini-bus or share-taxi stand corresponding to a location l, dl represents the monetary equivalent of the wait time that users save when engaging in queue jumping, and vl can, for instance, represent the total fraud that the presence of a security resource at location l can prevent, given by vl = \u039bldl, i.e., the total benefits that fraudulent users accrue, which is equal to the monetary equivalent of the additional wait time faced by non-defaulting users. As another example, a welfare function vl = x\u039bldl or vl = \u039bl(dl)\nx for some x \u2265 1 can serve as a proxy to capture the fact that an administrator may place a higher value in reducing additional wait time of non-defaulting users. While many other formulations for the administrator\u2019s welfare vl can be used depending on the administrator\u2019s goals in a specific context, for\ngenerality, we subsume these different objectives into the term vl to represent the value that the administrator places on allocating a security resource to mitigate fraud at location l.\nFurthermore, for ease of exposition, in this section, we introduce the model in the deterministic setting when the type at each location is known to the administrator, a setting we study in Section 4. We then introduce and investigate the probabilistic setting wherein the administrator only knows the probability distribution from which each location\u2019s type is drawn in Section 5."
        },
        {
            "heading": "3.2 Strategies of Administrator and Users",
            "text": "As in prior security games literature [9], we model our problem as a Stackelberg game, where an administrator (the leader) selects a strategy to allocate its available security resources to which the users (the followers) respond by deciding whether to engage in fraud. Here, we present the strategies of the administrator and users in our studied security game.\nAdministrator Strategy: We denote \u03c3 = (\u03c3l)l\u2208L as the resource allocation (mixed)-strategy of the administrator, where \u03c3l \u2208 [0, 1] denotes the probability with which a security resource is allocated to location l. The mixed strategy of the administrator satisfies the administrator\u2019s resource budget, i.e.,\u2211\nl\u2208L \u03c3l \u2264 R. For brevity of notation, we define the feasible set of the administrator\u2019s mixed-strategy vector as \u2126R = {\u03c3 = (\u03c3l)l\u2208L : \u03c3l \u2208 [0, 1] for all l \u2208 L and \u2211 l\u2208L \u03c3l \u2264 R}, where the subscript R represents the number of security resources available to the administrator. User Strategy: In response to the administrator\u2019s strategy \u03c3, users at each location l decide whether to engage in fraud at that location. To model the strategy of users, we let yl(\u03c3) \u2208 [0, 1] denote the probability with which users at location l will engage in fraud, where the outcome yl(\u03c3) = 1 (yl(\u03c3) = 0) corresponds to a setting where users at location l do (do not) engage in fraud."
        },
        {
            "heading": "3.3 User and Administrator Objectives",
            "text": "In this work, we assume users to be utility maximizers, as is standard in the security games literature [23], and study the administrator\u2019s resource allocation strategies under two objectives: (i) revenue maximization and (ii) welfare maximization. A revenue maximization objective aligns with the model of a selfish administrator, which is a standard assumption in prior work on security games [23] and closely resembles practice. For instance, in road traffic scenarios, police often place speed traps where users are likely to violate the speed limit even though other locations may be more accident-prone [47]. Moreover, a welfare maximization objective is a natural choice for an administrator seeking to, for instance, ensure it targets the locations most impacted by fraud.\nWe first elucidate the utility maximization problem of users, who at each location l are assumed to best respond to the administrator\u2019s strategy \u03c3. In particular, if the administrator allocates security resources to location l with probability \u03c3l, users at location l choose whether to engage in fraud based on whether the gains from committing fraud, given by (1\u2212\u03c3l)dl, outweigh the risk of potential losses through fines, given by \u03c3lk. Then, the utility maximization problem of the users, given an administrator strategy \u03c3, at a location l is given by:\nmax yl\u2208[0,1]\nUl(\u03c3, yl) = yl[(1\u2212 \u03c3l)dl \u2212 \u03c3lk]. (1)\nThe above problem represents the additional gains to users at location l when engaging in fraud with probability yl, where, without loss of generality, we normalize the utility of not engaging in fraud (which happens with probability 1\u2212 yl) to zero.\nNext, we elucidate the administrator\u2019s welfare and revenue maximization problems. Revenue Maximization: To elucidate the administrator\u2019s revenue maximization problem, note that the revenue accrued under an administrator strategy \u03c3 at each location l is given by \u03c3lyl(\u03c3)k\u039bl, where yl(\u03c3) represents the best-response of users at location l, given by the solution of Problem (1). In other words, if yl(\u03c3) = 0, i.e., users do not engage in fraud, the administrator collects no revenue from location l, while if yl(\u03c3) > 0, then the administrator levies a fine k on the \u039bl fraudulent users at location l, resulting in a revenue of k\u039bl from those users. Then, the administrator\u2019s revenue maximization problem is given by the following bi-level program:\nmax \u03c3\u2208\u2126R\nyl(\u03c3)\u2208[0,1],\u2200l\u2208L\nQR(\u03c3) = \u2211 l\u2208L \u03c3lyl(\u03c3)k\u039bl, (2a)\ns.t. yl(\u03c3) \u2208 argmax y\u2208[0,1] Ul(\u03c3, y), for all l \u2208 L, (2b)\nwhere in the upper level problem the administrator deploys a strategy \u03c3 to maximize its revenue to which users best-respond by maximizing their utilities in the lower-level problem. Here, we express the administrator revenue QR(\u03c3) only as a function of \u03c3 and not the user strategy vector y = (yl)l\u2208L for notational simplicity, as the best-response function yl(\u03c3) of users at each location l can itself be expressed as a function of the administrator\u2019s strategy \u03c3, as given by Equation (2b). In the remainder of this work, we will clarify the formulation of yl(\u03c3) based on context.\nWelfare Maximization: To elucidate the administrator\u2019s welfare maximization problem, we first note that the welfare under an administrator strategy \u03c3 at each location l is given by \u03c3lvl + (1 \u2212 \u03c3l)(1 \u2212 yl(\u03c3))vl = vl\u2212 (1\u2212\u03c3l)yl(\u03c3). In other words, the administrator accrues a value vl if it allocates a resource to location l, as the presence of a security resource can prevent fraud at that location, but only accrues vl with probability 1 \u2212 yl(\u03c3), i.e., the probability that users at location l do not engage in fraud given strategy \u03c3, if it does not allocate a resource to location l. Then, defining the level of welfare not accrued by the administrator under a strategy \u03c3 with R resources as W\u2212R (\u03c3) = \u2211 l\u2208L(1\u2212 \u03c3l)yl(\u03c3)vl, the administrator\u2019s welfare is given\nby WR(\u03c3) = \u2211 l\u2208L vl \u2212W \u2212 R (\u03c3). Finally, since the term \u2211 l\u2208L vl is a constant independent of \u03c3, the welfaremaximizing strategy is equivalent to one that minimizes W\u2212R (\u03c3) and can be computed using the following bi-level program:\nmin \u03c3\u2208\u2126R\nyl(\u03c3)\u2208[0,1],\u2200l\u2208L\nW\u2212R (\u03c3) = \u2211 l\u2208L (1\u2212 \u03c3l)yl(\u03c3)vl, (3a)\ns.t. yl(\u03c3) \u2208 argmax y\u2208[0,1] Ul(\u03c3, y), for all l \u2208 L, (3b)\nwhere the administrator employs a strategy that minimizes the objective W\u2212R (\u03c3) in the upper level problem to which users best-respond by maximizing their utilities in the lower-level problem.\nHaving defined the administrator\u2019s and users\u2019 objectives, our goal is to find a tuple of strategies (\u03c3\u2217, (yl(\u03c3 \u2217))l\u2208L)\nthat solve the above-defined bi-level programs. Note that such a tuple of strategies constitutes an equilibrium of our security game corresponding to the respective administrator objectives. Moreover, we note that as the structure of the equilibrium best-response function yl(\u03c3) takes on a simple form (see Section 4), analyzing equilibria in our security game reduces to studying the optimal administrator strategies, which will be the main focus for the remainder of this work."
        },
        {
            "heading": "4 Revenue and Welfare Maximization in Deterministic Setting",
            "text": "We begin with the study of our security game and the corresponding resource allocation strategies of the administrator under both revenue (Section 4.1) and welfare (Section 4.2) maximization objectives in the deterministic setting when the administrator knows each location\u2019s type."
        },
        {
            "heading": "4.1 Revenue Maximization",
            "text": "This section studies the administrator\u2019s revenue maximization problem and analyzes a greedy algorithm, shown in Algorithm 1, which allocates resources to locations in the descending order of their \u039bl values, where the total spending at each location l is no more than a threshold of dldl+k .\nWe show that Algorithm 1 achieves a revenue-maximizing outcome for the administrator.\nTheorem 1 (Optimality of Greedy Algorithm for Revenue Maximization Setting). Suppose that the administrator knows the type \u0398l at each location l \u2208 L. Then, the allocation strategy corresponding to Algorithm 1 achieves a revenue-maximizing outcome, i.e., it solves Problem (2a)-(2b).\nTheorem 1\u2019s proof relies on the fact that, in the revenue maximization setting, given an administrator strategy \u03c3, the best-response function yl(\u03c3) of users, given by the solution of Problem (1), at each location\nAlgorithm 1: Greedy Algorithm for Administrator\u2019s Revenue Maximization Objective\nInput : Total Resource capacity R, Locations types \u0398l = (\u039bl, dl, vl) for all locations l Order locations in descending order of \u039bl ; for l = 1, 2, ..., |L| do\n\u03c3l \u2190 min{R, dldl+k } ; Allocate the minimum of the remaining resources and dl dl+k to location l ;\nR\u2190 R\u2212 \u03c3l; Update amount of remaining resources ; end\nl is given by a threshold policy:\nyl(\u03c3) =\n{ 0, if \u03c3l >\ndl dl+k ,\n1, otherwise. (4)\nNotice that when \u03c3l = dl dl+k , any yl(\u03c3) \u2208 [0, 1] is a best-response for users at location l, i.e., users at location l are indifferent between engaging and not engaging in fraud. However, at the threshold \u03c3l = dl\ndl+k ,\nthe administrator\u2019s revenue is maximized when yl(\u03c3) = 1 with any yl(\u03c3) < 1 resulting in a strictly lower administrator revenue. Thus, in the revenue maximization setting, our security game has an equilibrium and, consequently, yl(\u03c3) corresponds to a solution of the lower-level problem of the bi-level Program (2a)-(2b) if and only if yl(\u03c3) = 1 when \u03c3l =\ndl dl+k . We also note that selecting yl(\u03c3) = 1 when \u03c3l = dl dl+k aligns with the\nnotion of strong Stackelberg equilibria [48], where the ties of the followers (users) are broken to optimize the leader\u2019s (administrator\u2019s) payoff.\nEquation (4) implies that if the probability of allocating resources to a location exceeds a threshold, users at that location will stop engaging in fraud as the risk of bearing the fines outweighs the gains from fraud. Given users\u2019 best-response function in Equation (4), the revenue at each location as a function of the amount of resources allocated to that location is depicted in Figure 1 (left).\nWe now use users\u2019 best response function in Equation (4) to complete the proof of Theorem 1.\nProof (Sketch) of Theorem 1. Leveraging Equation (4), Theorem 1\u2019s proof relies on establishing that the bilevel Program (2a)-(2b) can be reduced to solving a linear program. The key observation to this reduction relies on the fact that it suffices to consider administrator strategies satisfying \u03c3l \u2208 [0, dldl+k ] for all locations l, as allocating more than dldl+k resources to any location will result in reduced revenues (see Figure 1), as yl(\u03c3) = 0 when \u03c3l > dl\ndl+k . We then leverage the structure of the linear program to show that Algorithm 1\nthat allocates resources to locations in the descending order of the \u039bl values achieves an optimal solution to this linear program.\nFor a complete proof of Theorem 1, see Appendix B.1. Theorem 1 is in contrast to general hardness results for solving bi-level programs, as it establishes that a simple greedy algorithm (Algorithm 1) computes\nthe administrator\u2019s revenue-maximizing strategy, i.e., solves Problem (2a)-(2b), in polynomial time. In particular, Algorithm 1\u2019s complexity is O(|L| log(|L|)), since the complexity of sorting locations is O(|L| log(|L|)) while that of iterating through the locations in linear in |L|."
        },
        {
            "heading": "4.2 Welfare Maximization",
            "text": "This section studies the administrator\u2019s welfare maximization problem in the deterministic setting when the administrator knows each location\u2019s type \u0398l. To this end, we first present several properties of the administrator\u2019s welfare-maximizing strategy in Section 4.2.1. Then, we establish that computing the welfare-maximizing strategy is NP-hard (Section 4.2.2) and present a greedy algorithm with its associated approximation and resource augmentation guarantees (Section 4.2.3)."
        },
        {
            "heading": "4.2.1 Properties of Welfare Maximizing Strategy",
            "text": "In this section, we present properties of the structure of the optimal solution of the administrator\u2019s welfaremaximization problem, which play a pivotal role in establishing the hardness of solving Problem (3a)-(3b) (see Section 4.2.2) and analyzing the performance of the greedy algorithm to approximate the administrator\u2019s welfare-maximizing strategy (see Section 4.2.3). To elucidate these properties, we first note in the welfaremaximization setting that the best-response function of users, given by the solution of Problem (1), at each location l given an administrator strategy \u03c3 is given by the following threshold policy:\nyl(\u03c3) = { 0, if \u03c3l \u2265 dldl+k , 1, otherwise.\n(5)\nAs in the revenue maximization setting, we recall that when \u03c3l = dl dl+k , any yl(\u03c3) \u2208 [0, 1] is a best-response for users at location l. However, at the threshold \u03c3l = dl\ndl+k , the administrator\u2019s welfare is maximized\nand equal to vl when yl(\u03c3) = 0 with any yl(\u03c3) > 0 resulting in a strictly lower administrator welfare of vl \u2212 (1 \u2212 \u03c3l)yl(\u03c3)vl at that location. Thus, in the welfare maximization setting, our security game has an equilibrium and, consequently, yl(\u03c3) corresponds to a solution of the lower-level problem of the bi-level Program (3a)-(3b) if and only if yl(\u03c3) = 0 when \u03c3l =\ndl dl+k . We note here that unlike the best response of\nusers in the revenue-maximization setting, where yl(\u03c3) = 1 when \u03c3l = dl\ndl+k , in the welfare maximization\nsetting, we take yl(\u03c3) = 0 when \u03c3l = dl\ndl+k . Such a difference in the outcomes is attributable to the nature\nof the revenue and welfare functions at each location l, as depicted in Figure 1, where the welfare increases to vl once the probability of allocating resources to location l exceeds the threshold\ndl dl+k\nwhile the revenue drops to zero.\nGiven the best-response function of users at each location l, we now characterize several properties of the administrator\u2019s welfare-maximizing strategy.\nProposition 1 (Properties of Welfare-Maximizing Strategy). Suppose that users at each location bestrespond using Equation (5). Then, there exists a solution \u03c3\u0303\u2217 of Problem (3a)-(3b) satisfying:\n1. \u03c3\u0303\u2217l \u2208 [ 0, dldl+k ] for all locations l \u2208 L.\n2. There exists a set L1 with \u03c3\u0303 \u2217 l = dl dl+k for all l \u2208 L1, a set L2 with \u03c3\u0303\u2217l = 0 for all l \u2208 L2, and at most one location l\u2032 with \u03c3\u0303\u2217l\u2032 \u2208 ( 0, dl\u2032dl\u2032+k ) , where L1, L2, and {l\u2032} are disjoint and L1 \u222a L2 \u222a {l\u2032} = L.\nMoreover, the administrator\u2019s optimal welfare under the strategy \u03c3\u0303\u2217 is: WR(\u03c3\u0303 \u2217) = \u2211 l\u2208L1 vl + \u03c3\u0303 \u2217 l\u2032vl\u2032 .\nThe proof of the first claim in the proposition statement follows as spending more than dldl+k at any location does not increase the welfare at that location (see right of Figure 1). The proof of the second claim follows from the linearity of the welfare function in the region [ 0, dldl+k ) for all locations l. Together, both these claims in Proposition 1 establish that, without loss of generality, for the administrator\u2019s welfare maximization problem, it suffices to restrict attention to administrator strategies where the total allocation of security resources \u03c3l at any location l does not exceed\ndl dl+k and where there is at most one location l\u2032 such that \u03c3l\u2032 \u2208 ( 0, dl\u2032dl\u2032+k ) . Consequently, the optimal welfare of the administrator takes a relatively simple form, as in the statement of the proposition. We refer to Appendix B.2 for a complete proof of Proposition 1."
        },
        {
            "heading": "4.2.2 NP-Hardness of Welfare Maximization",
            "text": "This section establishes that the problem of computing the administrator\u2019s welfare-maximizing strategy is NP-hard.\nTheorem 2 (NP-Hardness of Welfare Maximization). The problem of computing the administrator\u2019s welfaremaximizing strategy, i.e., solving Problem (3a)-(3b), is NP-hard.\nProof (Sketch) of Theorem 2. We prove this result through a reduction from an instance of the partition problem, which consists of a sequence of numbers a1, . . . , an with \u2211 l\u2208[n] al = A and involves the task of\ndeciding whether there is some subset S1 of numbers such that \u2211\nl\u2208S1 al = A 2 .\nWe now construct an instance of the welfare maximization problem (WMP) with n+ 1 locations, where the first n locations correspond to each number of the partition instance, where we define vl = al and let dl\ndl+k = alA for all locations l \u2208 [n]. We also consider a location n + 1 with vn+1 = maxl\u2208[n] vl + \u03f5 and\ndn+1 dn+1+k = 12 + \u03b4, where \u03f5, \u03b4 > 0 are small constants. Finally, we let the number of resources R = 1 2 . We then show that a sequence of numbers correspond to a \u201cYes\u201d instance of partition if and only if the optimal total welfare of the above defined WMP instance is at least A2 .\nTo prove the forward direction of this claim, for any \u201cYes\u201d instance of partition with a set S such that\u2211 l\u2208S al = A 2 , we construct an allocation strategy \u03c3 such that \u03c3l = al A for all l \u2208 S and \u03c3l = 0 for all l \u2208 [n+ 1]\\S. We then verify that \u03c3 is feasible and achieves WR(\u03c3) \u2265 A2 . To establish the reverse direction, we leverage Proposition 1 to show that the only way for both the upper bound resource constraint of A2 and the lower bound welfare constraint of A 2 to be satisfied is if the location l\u2032 defined in the statement of Proposition 1 is such that l\u2032 = \u2205.\nFor a complete proof of Theorem 2, see Appendix B.3. Theorem 2 establishes that Problem (3a)-(3b) cannot be solved in polynomial time unless P = NP . This result contrasts the polynomial time algorithm we developed in the revenue-maximization setting (see Theorem 1). We note that the NP-hardness of the welfare maximization setting stems from the fact that, unlike the revenue function, the administrator\u2019s welfare function is discontinuous at \u03c3l =\ndl dl+k (see Figure 1)."
        },
        {
            "heading": "4.2.3 Greedy Algorithm for Welfare Maximization",
            "text": "Given the impossibility of developing a polynomial time algorithm for the administrator\u2019s welfare-maximization problem unless P = NP (see Theorem 2), this section presents a computationally efficient algorithm to compute an administrator strategy with strong approximation guarantees to the optimal solution of Problem (3a)-(3b). In particular, we develop a variant of a greedy algorithm, described in Algorithm 2, to compute an allocation of resources that achieves at least half the welfare as the welfare-maximizing allocation. Moreover, we show that running the greedy algorithm with one additional resource, i.e., R + 1 resources, results in an outcome with at least the same welfare as the welfare maximizing outcome.\nWe begin by first presenting our algorithmic approach, which proceeds as follows. First, we order the\nlocations in descending order of their bang-per-buck ratios, given by vldl dl+k = vl(dl+k)dl , and find the solution \u03c3\u0303 corresponding to the greedy algorithm that allocates at most dldl+k resources to each location l in descending order of their bang-per-buck ratios. We define the quantity vldl\ndl+k\nas the bang-per-buck ratio as the total\nwelfare received on allocating a fraction dldl+k of resources to location l is given by vl (see right of Figure 1). Next, we compute an allocation \u03c3\u2032 corresponding to spending all the available resources at a single location that yields the highest welfare for the administrator. Finally, we return the resource allocation strategy between the two computed strategies \u03c3\u0303 and \u03c3\u2032 that achieves a higher administrator welfare. This procedure is formally presented in Algorithm 2.\nA few comments about Algorithm 2 are in order. First, as with Algorithm 1, Algorithm 2 is computationally efficient with a running time of O(|L| log(|L|)), which corresponds to the complexity of sorting the locations in descending order of vl(dl+k)dl , and the remaining steps of Algorithm 2 can be performed in linear time in the number of locations. Next, Algorithm 2, which orders locations based on their bang-per-buck ratios, resembles analogous algorithms from the literature on the knapsack problem, which is the problem of finding the value-maximizing subset of items of given sizes that fits within the capacity of a knapsack.\nAlgorithm 2: Greedy Algorithm for Administrator\u2019s Welfare Maximization Objective\nInput : Total Resource capacity R, Location Types \u0398l = (\u039bl, dl, vl) for all locations l Step 1: Find Greedy Solution \u03c3\u0303: Order locations in descending order of vl(dl+k)\ndl ;\nfor l = 1, 2, ..., |L| do \u03c3\u0303l \u2190 min{R, dldl+k } ; Allocate the minimum of the remaining resources and dl dl+k to location l ;\nR\u2190 R\u2212 \u03c3\u0303l; Update amount of remaining resources ; end Step 2: Find Solution \u03c3\u2032 that Maximizes Welfare from Spending on Single Location \u03c3l \u2190 argmax\u03c3\u2208\u2126R:\u03c3l\u2032=0,\u2200l\u2032 \u0338=l WR(\u03c3), \u2200l ; Compute allocation \u03c3 l maximizing welfare from only spending on l ; \u03c3\u2032 \u2190 argmaxl\u2208L WR(\u03c3l) ; Step 3: Return Solution with Higher Welfare: \u03c3\u2217A \u2190 argmax{WR(\u03c3\u0303),WR(\u03c3 \u2032)} ;\nDespite the connection between Algorithm 2 and the knapsack literature, our problem setting differs from several well-studied variants of the knapsack problem. Unlike the 0-1 knapsack problem, wherein the decision space is binary, in our problem setting, the administrator\u2019s decision space of mixed strategies is continuous. Moreover, unlike the fractional knapsack setting, which has a continuous decision space and where a greedy algorithm that allocates resources in descending order of the bang-per-buck ratios is optimal, in our setting, computing the administrator\u2019s welfare maximizing strategy is NP-hard (Theorem 2).\nConsequently, given the similarities and differences between the welfare maximization setting and the knapsack literature, we can interpret our studied welfare maximization problem as a novel variant of the knapsack problem. In particular, we can consider locations as items, whose value (or welfare) function increases linearly as a higher fraction of it is packed in the knapsack (i.e., as the probability of allocating resources to a location is increased) up to some threshold. At this location-specific threshold, dldl+k , which we interpret as the size of the item, the value function of the item has a jump discontinuity and equals the item\u2019s value vl, as depicted on the right of Figure 1. Note in the case when the fine k = 0, our studied welfare function has no jump discontinuity at dldl+k (see Figure 1); thus, when k = 0, our welfare maximization problem reduces to a fractional knapsack problem that is solvable in polynomial time. Hence, the presence of fines in our security game introduces discontinuities in the administrator\u2019s welfare function, which corresponds to the source of the NP-hardness of the welfare maximization Problem (3a)-(3b) (Theorem 2).\nWe now present the approximation guarantees of Algorithm 2 to the optimal welfare corresponding to the solution of Problem (3a)-(3b). Our first result establishes that Algorithm 2 achieves at least half the welfare as that corresponding to the welfare-maximizing allocation. Theorem 3 (1/2 Approximation of Greedy Algorithm for Welfare Maximization). Denote \u03c3\u2217A as the solution corresponding to Algorithm 2 and let \u03c3\u2217 be the welfare-maximizing allocation that solves Problem (3a)-(3b). Then, \u03c3\u2217A achieves at least half the welfare as \u03c3 \u2217, i.e., WR(\u03c3 \u2217 A) \u2265 12WR(\u03c3 \u2217).\nFor a detailed proof sketch and proof of Theorem 3, see Appendix B.4. The key insight in developing Algorithm 2 and establishing Theorem 3 despite the discontinuity in the welfare function (see Figure 1) is in recognizing that the administrator\u2019s welfare-maximization objective can be upper bounded by the objective of a linear program that resembles a fractional knapsack optimization. Further, we note that the obtained approximation ratio of 12 of Algorithm 2 aligns with the approximation guarantee of an analogous greedy algorithm for the NP-hard 0-1 knapsack problem. Yet, unlike the 0-1 knapsack problem for which there exists a polynomial-time approximation scheme (PTAS) using dynamic programming, extending this idea to the setting considered in this work is challenging due to the administrator\u2019s continuous action space. We defer settling the question of whether a PTAS exists for the welfare maximization problem as a direction for future research.\nNext, we show that if the administrator had an extra resource, i.e., R + 1 resources, then Algorithm 2 achieves a higher welfare than that of the welfare-maximizing outcome with R resources.\nTheorem 4 (Resource Augmentation Guarantee for Welfare Maximization). Denote \u03c3\u2217A as the solution corresponding to Algorithm 2 with R + 1 resources and let \u03c3\u2217 be the welfare-maximizing allocation that solves Problem (3a)-(3b) with R resources. Then, the total welfare under the allocation \u03c3\u2217A is at least that corresponding to the allocation \u03c3\u2217, i.e., WR+1(\u03c3 \u2217 A) \u2265 WR(\u03c3\u2217).\nThe proof of this result relies on much of the machinery developed in proving Theorem 3, and we present its proof in Appendix B.5. We note that the administrator only requires R + maxl\u2208L\ndl dl+k resources to\nobtain the guarantee in Theorem 4; however, we present the result with R+1 resources for ease of exposition. Theorem 4 highlights the benefit to administrators for recruiting one additional security resource (e.g., police officer) and applying a simple algorithm, i.e., Algorithm 2, rather than investing computational effort to solve the NP-hard welfare-maximization problem."
        },
        {
            "heading": "5 Revenue Maximization in Probabilistic Setting",
            "text": "In this section, we study the administrator\u2019s revenue maximization objective in the probabilistic setting when the administrator only has distributional information on each location\u2019s type. To this end, we first introduce the notation for the probabilistic setting and the administrator\u2019s expected revenue maximization problem in Section 5.1. We then show that computing the administrator\u2019s expected revenue maximizing strategy is NP-hard in Section 5.2 and present a variant of a greedy algorithm and its associated approximation ratio and resource augmentation guarantees in Section 5.3.\nWhile we focus on the administrator\u2019s revenue maximization objective in this section, our developed algorithmic ideas and corresponding guarantees naturally generalize to the administrator\u2019s welfare maximization objective in the probabilistic setting, which we elucidate in Appendix C."
        },
        {
            "heading": "5.1 Model for Probabilistic Setting and Expected Revenue Maximization Objective",
            "text": "To model the probabilistic setting, we assume that the type \u0398l at each location l is drawn independently from some discrete probability distribution with finite support (\u0398il)i\u2208I = (\u039b i l, d i l, v i l)i\u2208I , where the support, defined by I, satisfies |I| \u2208 N. We define the probability that a location has a type \u0398il as qil for all i \u2208 I. In other words, P(\u0398l = \u0398il) = qil for all i \u2208 I, where qil \u2265 0 and \u2211 i\u2208I q i l = 1 for all locations l \u2208 L. We also assume, without loss of generality, that each location\u2019s types are ordered such that d1l\nd1l +k \u2264 d\n2 l\nd2l +k \u2264 . . . \u2264 d\n|I| l\nd |I| l +k\n.\nIn this setting, in line with Bayesian Stackelberg games [21], we assume that while users at each location know the realization of the type i \u2208 I at that location, the administrator only knows the distribution of each location\u2019s type. Consequently, denoting the best response of users corresponding to type i at location l as yil(\u03c3), given an administrator strategy \u03c3, we formulate the following expected revenue maximization problem (ERMP) of the administrator:\nmax \u03c3\u2208\u2126R\nyil (\u03c3)\u2208[0,1],\u2200l\u2208L,i\u2208I\nQR(\u03c3) = \u2211 i\u2208I qil \u2211 l\u2208L \u03c3ly i l(\u03c3)k\u039b i l, (6a)\ns.t. yil(\u03c3) \u2208 argmax y\u2208[0,1] U il (\u03c3, y) = y[(1\u2212 \u03c3l)dil \u2212 \u03c3lk], for all l \u2208 L, i \u2208 I, (6b)\nwhere in upper-level problem, the administrator uses a strategy \u03c3 that maximizes its expected revenue to which the users at each location best respond by maximizing their utilities in the lower-level problem based on the realized type at that location. As in the deterministic setting, we note that the best response of users is given by the threshold function\nyil(\u03c3) =\n{ 0, if \u03c3l >\ndil dil+k ,\n1, otherwise. (7)\nWe reiterate, as with the best response of users in the deterministic setting in Equation (4), that when \u03c3l = dil\ndil+k , any yil(\u03c3) \u2208 [0, 1] is a best-response for users at a location l with type i. Yet, we let yil(\u03c3) = 1 when\n\u03c3l = dil\ndil+k as it corresponds to the highest expected revenue outcome for the administrator (see Section 4.1\nfor a further discussion). Finally, due to the best-response function of users given by Equation (7), the resulting expected revenue at each location l as a function of the amount of resources allocated is both non-monotone and discontinuous at the resource amounts dil\ndil+k for all i, as is depicted in Figure 2 (left)."
        },
        {
            "heading": "5.2 NP-Hardness of Probabilistic Setting",
            "text": "In the probabilistic setting, when the administrator only knows the distribution of location types, we show that computing the administrator\u2019s expected revenue maximizing strategy is NP-hard.\nTheorem 5 (NP-Hardness of Expected Revenue Maximization). The problem of computing the administrator\u2019s expected revenue maximizing strategy, i.e., solving Problem (6a)-(6b), is NP-hard.\nProof (Sketch). We prove this result through a reduction from partition. In particular, given a partition instance with a sequence of numbers a1, . . . , an, we now construct an ERMP instance with two types, i.e., where |I| = 2, and n locations, where each number al corresponds to a location. In this setting, for ease of exposition, we drop the fine k from Objective (6a) as it is a uniform constant that applies to all locations l and types i. Then, we define: (i) d1l\nd1l +k = 12 al A , (ii) d2l d2l +k = alA , (iii) q 1 l \u039b 1 l = A\n( 2 maxl\u2032\u2208[n] al\u2032 al \u2212 1 ) , and (iv)\nq2l \u039b 2 l = A\n( 1 + 2maxl\u2032\u2208[n] al\u2032\nal\n) for all l \u2208 [n]. Moreover, we let the number of resources R = 34 . Then, we\nclaim that we have a \u201cYes\u201d instance of partition if and only if the optimal expected revenue for this ERMP instance is at least A2 + 2n\u00d7maxl\u2208[n] al.\nTo prove the forward direction of this claim, for any \u201cYes\u201d instance of partition with a set S such that\u2211 l\u2208S al = A 2 , we construct a resource allocation strategy \u03c3 such that \u03c3l = al A for all l \u2208 S and \u03c3l = 1 2 al A for all l \u2208 [n]\\S. We then verify that \u03c3 is feasible and achieves QR(\u03c3) \u2265 A2 + 2n\u00d7maxl\u2208[n] al. To prove the reverse direction, we first show that when R = 34 there exists an optimal administrator strategy \u03c3\u0303\u2217 that satisfies \u03c3\u0303\u2217l \u2265 d1l\nd1l +k for all locations l. Leveraging this property and another structural\nproperty of the expected revenue-maximizing strategy of the administrator (see Appendix B.6) akin to that established in Proposition 1, we show that the administrator\u2019s optimal strategy can only satisfy the resource constraint and achieve an expected revenue that is at least A2 +2n\u00d7maxl\u2208[n] al, if there is some set S\n\u2032 \u2286 [n], satisfying \u2211 l\u2208S\u2032 al = A 2 .\nFor a complete proof of Theorem 5, see Appendix B.6. Theorem 5 establishes that Problem (6a)-(6b) cannot be solved in polynomial time unless P = NP . This result contrasts the polynomial time algorithm we developed in the deterministic revenue maximization setting (see Theorem 1), as, while, for any location l, the administrator\u2019s revenue function in the deterministic setting is continuous and monotone in the range\n\u03c3l \u2208 [ 0, dldl+k ] , the expected revenue function is discontinuous and non-monotone in the range \u03c3l \u2208 [ 0, d |I| l\nd |I| l +k ] (see Figure 2), even when the number of types |I| = 2.\nWe note that the fundamental challenge in developing both our hardness results, i.e., for deterministic welfare maximization (Theorem 2) and expected revenue maximization (Theorem 5), lies in constructing the right instances or gadgets to achieve our desired reductions, which we develop leveraging the structural properties of the respective problem settings. For example, in the welfare maximization setting, our reduction crucially relies on Proposition 1, and, hence, the discontinuity of the welfare function at \u03c3l =\ndl dl+k\n. Analogously, in the expected revenue maximization setting, our reduction leverages the non-monotonicity of the expected revenue function."
        },
        {
            "heading": "5.3 Greedy Algorithm for Expected Revenue Maximization",
            "text": "Given the impossibility of developing a polynomial time algorithm for the administrator\u2019s expected revenue maximization problem unless P = NP (see Theorem 5), this section develops a computationally efficient algorithm to compute an administrator strategy and presents its associated approximation ratio and resource augmentation guarantees, akin to that for the welfare maximization problem in the deterministic setting (see Section 4.2.3), to the solution of Problem (6a)-(6b).\nTo motivate our algorithmic approach, we first note that the difficulty in solving Problem (6a)-(6b) is attributable to the non-monotonicity and discontinuity of the expected revenue function at the resource amounts dil\ndil+k for all i due to the best-response Problem (7) of users, an example of which is depicted in\nFigure 2 for a setting with five types (i.e., |I| = 5). Given this difficulty of directly optimizing the expected revenue function, we define its upper bound, which we term the monotone concave upper approximation (MCUA), that is tractable to compute. To develop this approximation, we first define a piece-wise linear\nupper bound of the expected revenue function that connects the origin (0, 0) to its points of discontinuity, as depicted by the green curve in Figure 2 (center). While this upper bound is continuous, it may be non-monotone and non-concave. Thus, we finally construct the MCUA of the expected revenue function by concavifying this upper bound and only retaining the portions of this concave upper bound where the expected revenue is (strictly) increasing in the resources allocated, as depicted by the orange line in Figure 2 (right).\nSince the MCUA of the expected revenue function for each location l is piece-wise linear (e.g., see right of Figure 2), we characterize it via a set S corresponding to the set of all piece-wise linear segments of this MCUA across all locations. We associate each segment s \u2208 S with a triple (ls, cs, xs), where ls represents the location corresponding to segment s, cs corresponds to its slope, and xs represents its horizontal width, i.e., resource requirement, as depicted on the right in Figure 2.\nHaving introduced the notion of the MCUA of the expected revenue function, we now elucidate our algorithmic approach, presented in Algorithm 3, which involves two key steps. First, rather than directly optimizing the administrator\u2019s expected revenue, an NP-hard problem (see Theorem 5), we optimize its corresponding MCUA using a greedy-like procedure. In particular, we order the segments of the MCUA of the expected revenue function in the set S in the descending order of the slopes cs and allocate at most xs to each segment in the descending order of the slopes of the MCUA of the expected revenue function. Our greedy procedure results in an allocation \u03c3\u0303 and terminates when a segment\u2019s resource requirement xs exceeds the available resources.\n1 Next, we compute an allocation \u03c3\u2032 corresponding to spending all the available resources at a single location that yields the highest expected revenue for the administrator. Finally, we return the resource allocation strategy between the two computed strategies \u03c3\u0303 and \u03c3\u2032 that achieves a higher expected revenue. While we focus on discrete distributions, we note that Algorithm 3 along with the approach of generating a piece-wise linear MCUA of the expected revenue function can also be applied to continuous probability distributions (see Appendix G).\nWe now present the main results of this section, which establish the approximation guarantees of Algorithm 3 to the optimal solution of Problem (6a)-(6b). Our first result establishes that Algorithm 3 achieves at least half the expected revenue as that corresponding to the solution of Problem (6a)-(6b).\nTheorem 6 (1/2 Approximation for Expected Revenue Maximization). Denote \u03c3\u2217A as the allocation corresponding to Algorithm 3 and let \u03c3\u2217 be the solution of Problem (6a)-(6b) with R \u2265 1 resources. Then, \u03c3\u2217A achieves at least half the expected revenue as \u03c3\u2217, i.e., QR(\u03c3 \u2217 A) \u2265 12QR(\u03c3 \u2217).\nThe crux of establishing Theorem 6 involves showing that optimizing the MCUA of the expected revenue function can be reduced to solving a linear program and that an analogous greedy-like process to that in Step 1 of Algorithm 3 achieves the optimal solution of this linear program. We note that this linear program is different from the fractional knapsack linear program to prove Theorem 3 in the welfare maximization setting, as, unlike the welfare function in the deterministic setting, the MCUA of the expected revenue function at each location is piece-wise linear with (potentially) multiple segments with differing slopes. For a proof of Theorem 6, see Appendix B.7.\nNext, we show that if the administrator had an extra resource, i.e., R + 1 resources, then Algorithm 3 achieves at least the same revenue as the expected revenue maximizing outcome with R resources.\n1We terminate our greedy procedure at the point in the algorithm when a segment\u2019s resource requirement xs exceeds the available resources, as the expected revenue function is non-monotone in the resources allocated, and the MCUA, by construction, is only guaranteed to coincide with the expected revenue function at each location l at the points where the resources allocated equal dil\ndi l +k\nfor some values of i (e.g., see Figure 2).\nAlgorithm 3: Greedy Algorithm for Administrator\u2019s Expected Revenue Maximization Objective\nInput : Total Resource capacity R, Location Types \u0398il = (\u039b i l , d i l , v i l ) for all locations l and types i Output: Resource Allocation Strategy \u03c3\u2217A Step 1: Greedy Allocation Based on Slopes of MCUA of Expected Revenue Function: Generate MCUA of the expected revenue function for each location l ; S\u0303 \u2190 Ordered list of segments s across all locations of this MCUA in descending order of slopes cs ; Initialize allocation strategy \u03c3\u0303 \u2190 0 ; for segment s \u2208 S\u0303 do\nif xs \u2264 R then \u03c3\u0303ls \u2190 \u03c3\u0303ls + xs ; Allocate xs to location ls ; R\u2190 R\u2212 xs; Update amount of remaining resources ; else break ; Only allocate resources if xs \u2264 R end\nend Step 2: Find Solution \u03c3\u2032 that Maximizes expected revenue from spending on Single Location: \u03c3l \u2190 argmax\u03c3\u2208\u2126R:\u03c3l\u2032=0,\u2200l\u2032 \u0338=l QR(\u03c3) for all locations l ; \u03c3\u2032 \u2190 argmaxl\u2208L QR(\u03c3l) ; Step 3: Return Solution with a Higher Expected Revenue: \u03c3\u2217A \u2190 argmax{QR(\u03c3\u0303), QR(\u03c3 \u2032)} ;\nTheorem 7 (Resource Augmentation Guarantee for Expected Revenue Maximization). Denote \u03c3\u2217A as the solution corresponding to Algorithm 3 with R + 1 resources and let \u03c3\u2217 be the expected revenue maximizing allocation that solves Problem (6a)-(6b) with R resources. Then, the total revenue under the allocation \u03c3\u2217A is at least that corresponding to \u03c3\u2217, i.e., QR+1(\u03c3 \u2217 A) \u2265 QR(\u03c3\u2217).\nThe proof of Theorem 7, as with that of Theorem 6, relies on the fact that a greedy-like process akin to step one of Algorithm 3 optimizes the MCUA of the expected revenue function. For a complete proof of Theorem 7, see Appendix B.8. Akin to Theorem 4, we note that the administrator only requires R+maxi\u2208I maxl\u2208L dil\ndil+k\nresources to obtain the guarantee in Theorem 7; however, we present the result with R + 1 resources for ease of exposition. Theorem 7 highlights the benefit to administrators for recruiting one additional security resource (e.g., police officer) and applying a simple algorithm, i.e., Algorithm 3, that relies on computing a tractable MCUA of the expected revenue function rather than investing computational effort to solve the NP-hard Problem (6a)-(6b)."
        },
        {
            "heading": "6 Contracts to Bridge Revenue and Welfare Maximization",
            "text": "Thus far, we have studied our security game under the revenue and welfare maximization administrator objectives. While a welfare maximization objective captures an idealized representation of administrator behavior, in practice, an administrator often seeks to maximize revenues, which usually compromises the system\u2019s welfare (see Appendix F for experiments depicting the contrast in the revenue and welfare maximization outcomes). For instance, in road traffic scenarios, police often place speed traps to collect fines and increase revenues at locations where users are likely to violate the speed limit even though other locations may be more accident-prone [47].\nTo address this concern of the gulf between the revenue and welfare maximization outcomes, in this section, we extend our security game framework to incorporate contracts, wherein a revenue-maximizing administrator is compensated for the welfare it contributes to the system. Section 6.1 introduces the contract framework and studies equilibria in the associated contract game. Then, Section 6.2 presents numerical experiments highlighting the efficacy of contracts in bridging the gap between the welfare and revenue maximization outcomes."
        },
        {
            "heading": "6.1 Contract Framework",
            "text": "We consider a contract game between three players: (i) a welfare-maximizing principal, (ii) a revenuemaximizing administrator, and (iii) fraudulent users at different locations in a system.2 In our contract game, a welfare-maximizing principal offers a contract, specified by a parameter \u03b1 \u2208 [0, 1], to a revenuemaximizing administrator, where the contract level determines the payment made by the principal to the\n2To distinguish between a principal and an administrator, we can, for instance, interpret the principal as the government maximizing welfare and the administrator as a police department within the government maximizing its own revenues.\nadministrator as a proportion of the total welfare it contributes to the system. In particular, the principal selects the contract level \u03b1 \u2208 [0, 1] to optimize its payoff, given by the difference between the welfare accrued and the total payments it makes to the administrator, to which the administrator best responds by choosing a revenue-maximizing strategy \u03c3(\u03b1), which can be computed via the following bi-level program:\nmax \u03c3\u2208\u2126R\nyl(\u03c3)\u2208[0,1],\u2200l\u2208L\nQR(\u03c3) + \u03b1WR(\u03c3), (8a)\ns.t. yl(\u03c3) \u2208 argmax y\u2208[0,1] Ul(\u03c3, y), for all l \u2208 L. (8b)\nIn the upper level problem, the administrator selects a strategy \u03c3(\u03b1) to maximize its total revenue to which users best respond by maximizing utilities in the lower-level problem. Note here that, in our contract game, since the principal gives \u03b1 fraction of the total welfare accrued to the administrator, the administrator\u2019s objective of maximizing revenues corresponds to Equation (8a), which represents the sum of the fines collected by the administrator from allocating its security resources and the payment it receives from the principal for its contribution to the system\u2019s welfare. The corresponding payoff of the principal, given a contract level \u03b1 and the administrator strategy \u03c3(\u03b1), as given by the solution of Problem (8a)-(8b), is thus given by (1\u2212 \u03b1)WR(\u03c3(\u03b1)).\nAn equilibrium of our contract game is specified by a triple (\u03b1\u2217,\u03c3(\u03b1\u2217), (yl(\u03c3(\u03b1 \u2217)))l\u2208L), where (\u03c3(\u03b1 \u2217), (yl(\u03c3(\u03b1 \u2217)))l\u2208L)\nrepresent the solutions of the bi-level Program (8a)-(8b) given parameter \u03b1\u2217, which the principal selects to maximizes its payoff (1\u2212\u03b1)WR(\u03c3(\u03b1)). In the following, we first note that our earlier developed algorithmic approaches and theoretical guarantees in the revenue and welfare maximization settings naturally apply in studying the administrator and user strategies, i.e., the solution of Problem (8a)-(8b), given any contract \u03b1 \u2208 [0, 1]. Then, we present a dense-sampling approach to compute a near-optimal solution to the principal\u2019s problem of selecting a contract \u03b1 \u2208 [0, 1] that maximizes its payoff (1\u2212 \u03b1)WR(\u03c3(\u03b1)).\nAdministrator and User Strategies: In studying the equilibrium strategies of the administrator and users, we first note that solving Problem (8a)-(8b) is, in general, NP-hard, which follows from an analogous reduction to that in the proof of Theorem 2 (see Appendix D). Thus, we develop an algorithm, which we henceforth refer to as Contract-Greedy, akin to the greedy algorithms developed in the earlier studied revenue and welfare maximization settings, to compute an administrator strategy in our contract game for any contract \u03b1 \u2208 [0, 1]. For instance, in the deterministic setting when the administrator knows each location\u2019s type, Contract-Greedy (see Algorithm 6 in Appendix D) is akin to Algorithm 2 in the welfare maximization setting other than in the process of sorting locations, as the administrator in our contract game, maximizes a linear combination of the revenue and welfare objectives. Moreover, since the administrator maximizes a linear combination of the revenue and welfare objectives, our earlier developed approximation ratio and resource augmentation guarantees also extend to this contract game. Thus, as many of the ideas developed in earlier sections apply in studying optimal administrator strategies in our contract game, for brevity, we defer the details of the algorithm, Contract-Greedy, and its guarantees to Appendix D.\nPrincipal\u2019s Strategy: Thus far, we have studied the strategies of the administrator and users given a contract \u03b1. We now consider the principal\u2019s problem of selecting a contract \u03b1 \u2208 [0, 1] to maximize its payoff (1 \u2212 \u03b1)WR(\u03c3(\u03b1)). To this end, since obtaining an exact functional form of the welfare WR(\u03c3(\u03b1)) for all contracts \u03b1 \u2208 [0, 1] is challenging, we present a dense-sampling method to compute a near-optimal solution to the principal\u2019s payoff maximization problem. In particular, consider the solutions \u03c3(\u03b1) of the bi-level Program (8a)-(8b) for \u03b1 taken from a finite set As = {0, s, 2s, . . . , 1} for some step-size s \u2208 (0, 1). We then evaluate the total welfare of each of the solutions \u03c3(\u03b1) for \u03b1 \u2208 As and return the value \u03b1\u2217s from this discrete set that results in the highest payoff to the principal, i.e., (1 \u2212 \u03b1\u2217s)WR(\u03c3(\u03b1\u2217s)) \u2265 (1 \u2212 \u03b1)WR(\u03c3(\u03b1)) for all \u03b1 \u2208 As.3\nWe now show that applying the above dense-sampling procedure approximately maximizes the principal\u2019s payoff across all contract parameters \u03b1 \u2208 [0, 1], when the administrator strategy, given any parameter \u03b1, corresponds to the optimal solution of the bi-level Program (8a)-(8b).\n3Methods beyond dense sampling, e.g., gradient-based methods [49], can also be used; however, we use dense sampling, as it is computationally tractable when optimizing over a single variable and achieves the guarantee in Theorem 8.\nTheorem 8 (Near-Optimality of Dense Sampling). Let \u03b1\u2217 \u2208 [0, 1] be the principal\u2019s payoff maximizing contract and \u03b1\u2217s \u2208 As be the contract computed through dense-sampling. Further, given any \u03b1, let \u03c3(\u03b1) be the solution of Problem (8a)-(8b). Then, for a step-size s \u2264 \u03f5\u2211\nl vl , the loss in the principal\u2019s payoff through\ndense sampling is bounded by \u03f5, i.e., (1\u2212 \u03b1\u2217)WR(\u03c3(\u03b1\u2217)) \u2264 (1\u2212 \u03b1\u2217s)WR(\u03c3(\u03b1\u2217s)) + \u03f5.\nThe challenge in establishing Theorem 8 is that the welfare WR(\u03c3(\u03b1)) is, in general, not continuous in \u03b1 (e.g., the welfare function is discontinuous at \u03c3l =\ndl dl+k for any location l, as depicted in Figure 1). Thus, the\nkey idea in proving this result involves showing that the welfare WR(\u03c3(\u03b1)) is monotonically (non)-decreasing in \u03b1. For a complete proof of Theorem 8, see Appendix E.\nThe near-optimality of dense sampling, more generally, applies beyond administrator strategies corresponding to the solution of Problem (8a)-(8b) and, in particular, holds for any strategy \u03c3\u0303(\u03b1) such that the welfare WR(\u03c3\u0303(\u03b1)) is monotonically non-decreasing in \u03b1 (see Appendix E.1). For instance, this monotonicity condition is satisfied by the solution computed using Contract-Greedy under a correlation assumption on the valuations vl and the welfare bang-per-buck ratios vl(dl+k)\ndl (see Appendix E.1). Moreover, we ob-\nserve from our experiments in Section 6.2 that the welfare corresponding to the strategies computed using Contract-Greedy is non-decreasing in \u03b1."
        },
        {
            "heading": "6.2 Numerical Experiments",
            "text": "This section studies our contract game through numerical experiments based on a case study of queue jumping in IPT services (see Section 1.1) in Mumbai, India. In the following, we present model parameters for our experiment and results demonstrating the variation in the welfare accrued by a revenue-maximizing administrator for different model parameters as the contract level \u03b1 is varied.\nModel Parameters: We consider a problem instance with L = 448 locations, representing the locations to hail the IPT service in Mumbai [50]. We assume that each location l has one type, i.e., |I| = 1, where the number of fraudulent users \u039bl are exponentially distributed with rate 80, i.e., \u039bl \u223c Exp(80) for all l, and the benefits dl from engaging in fraud are exponentially distributed with rate 20, i.e., dl \u223c Exp(20) for all l. Moreover, we vary the number of resources R \u2208 {1, 2, . . . , 30}, the fine k \u2208 {50, 100, . . . , 500}, and consider a welfare function given by vl = \u039bl(dl)\nx (see Section 3.1) for x lying in the range {1, 1.25, 1.5, 1.75, 2} for all locations l. For a detailed overview of the motivations behind the choice and calibration of our model parameters, we refer to Appendix F.\nResults: Figure 3 depicts the variation in the welfare achieved by the allocation corresponding to Contract-Greedy as the contract \u03b1 is varied as a fraction of the welfare achieved using Algorithm 2 in the welfare maximization setting for different model parameters. In the left of Figure 3, we fix the fine to k = 500 and the number of resources R = 10 and vary the exponent x of the welfare function vl = \u039bl(dl)\nx. Analogously, in the center of Figure 3, we fix the welfare function vl = \u039bl(dl)\n1.25 and the fine k = 500, with each curve corresponding to a different number of resources R. Finally, the right of Figure 3 depicts curves for different fines, where the welfare function vl = \u039bl(dl)\n1.25 and the number of resources R = 15. For the results in Figure 3, we consider the contract \u03b1 to be chosen from a discrete set between zero and one with 0.05 increments, i.e., the step-size s = 0.05.\nFrom Figure 3, we first observe that regardless of the model parameters, the welfare corresponding to an administrator strategy computed using Contract-Greedy is monotonically non-decreasing in the contract \u03b1. Such a monotonic relation aligns with the proof of Theorem 8 and is natural as a higher contract \u03b1 implies that the administrator is compensated more for the welfare it contributes. Further, we note from Figure 3 that the revenue-maximizing solution, corresponding to \u03b1 = 0, achieves only a small fraction of the welfare achieved using Algorithm 2 in the welfare maximization setting, suggesting that the administrator\u2019s revenue and welfare maximization goals can often be at odds. However, for most tested parameters, an appropriately chosen contract \u03b1 can recover a majority of the welfare achieved by Algorithm 2. For instance, when vl = \u039bl(dl)\n1.25, a contract of 0.5 maximizes the principal\u2019s payoff and achieves about 86% of Algorithm 2\u2019s welfare.\nNext, we note from Figure 3 (left) that as we increase the exponent x of the welfare function vl = \u039bl(dl) x, the fraction of the welfare achieved by the allocation computed using Contract-Greedy to that achieved by Algorithm 2 increases for each contract \u03b1. Such a relation naturally follows as the welfare term in the administrator\u2019s Objective (8a) increasingly dominates the revenues from the collected fines at each location with an increase in the exponent of the welfare function. Consequently, from the left of Figure 3, our\nresults, for the studied welfare functions with an exponent x > 1, demonstrate that using even small values of the contract \u03b1 can recover most of the system welfare, thus bridging the gap between the welfare and revenue-maximizing outcomes.\nFurther, the fraction of the welfare achieved by the strategy computed using Contract-Greedy to that achieved by Algorithm 2 in the welfare maximization setting for any contract \u03b1 (i) remains nearly constant regardless of the number of resources (center of Figure 3) and (ii) increases for lower fines (right of Figure 3). Such a result holds as while varying the number of resources R does not influence any property of the locations, e.g., a location\u2019s type \u0398l = (\u039bl, dl, vl) is independent of R, changing fines impacts the threshold dl dl+k at each location l at which the revenue and welfare functions have a jump discontinuity (see Figure 1). Consequently, while our greedy-like algorithms are not influenced by a change in the number of resources other than that the algorithms either terminate sooner or later depending on the number of resources, a change in the fine influences our algorithms\u2019 outcomes as the locations are sorted in a (potentially) different order for each fine k.\nMoreover, Figure 3 (right) implies that at lower fines, the allocation computed using Contract-Greedy recovers a higher proportion of the welfare compared to that achieved using Algorithm 2 in the welfare maximization setting. Such a result holds, as at lower fines, an administrator maximizing Objective (8a) is more likely to optimize welfare as the compensation it receives from the principal outweighs its low fine collections. Thus, Figure 3 (right) highlights that setting high fines can be detrimental to the system\u2019s welfare in the presence of a revenue-maximizing administrator. Such a result thus highlights the value of setting low to moderate fines, as often happens in practice, in deterring an administrator from solely maximizing revenues through the collected fines and instead incorporating welfare maximization in its objective even at low contract levels \u03b1.\nOverall, our results present several sensitivity relations that elucidate the impact of the welfare function vl, number of resources R, and fine k on the welfare achieved in the system using Contract-Greedy for different contract parameters \u03b1. Moreover, our results highlight the effectiveness of contracts in bridging the gap between the welfare and revenue maximization administrator objectives. For a further discussion and analysis of the results in Figure 3, we refer to Appendix F.3."
        },
        {
            "heading": "7 Conclusion and Future Work",
            "text": "In this work, we studied the problem of policing and monitoring fraudulent activities at potentially susceptible nodes or locations in a system as a security game between an administrator and fraudulent users. Motivated by several real-world settings where fraudulent and illegal activities by certain groups of users are prevalent (see Section 1.1), we introduced a model of a security game wherein the administrator can deploy a budget of security resources across locations and levy fines against users found engaging in illegal activities. We studied our security game under both welfare and revenue maximization administrator objectives. In both settings, we showed that, in general, the problem of computing the optimal resource allocation strategy of the administrator is NP-hard, and we developed greedy-like algorithms for both administrator objectives with associated approximation ratio and resource augmentation guarantees. Finally, given that the revenue and welfare-maximizing outcomes of the administrator can differ significantly from each other, we presented a framework inspired by contract theory that helps ensure that a revenue-maximizing administrator incorporates welfare maximization as part of its objective when allocating security resources. Beyond extending our theoretical results in the welfare and revenue maximization settings to the contract framework, we also\npresented numerical experiments based on a real-world application case of intermediate public transport services in Mumbai, India. Our numerical results validated our theoretical insights and highlighted the effectiveness of using contracts in bridging the gap between the revenue and welfare-maximizing outcomes of the administrator.\nThere are several future research directions. First, it would be interesting to investigate whether our greedy algorithms achieve the best approximation ratios for the studied problem setting or if algorithms with improved approximation guarantees can be derived, e.g., a PTAS. Next, while we studied the probabilistic setting, where the administrator knows the distribution of each location\u2019s type, it would be interesting to study a repeated game setting where the administrator learns these distributions. Finally, it would be worthwhile to explore more properties of our contract framework and develop (and compare) other alternative formulations to incentivize a revenue-maximizing administrator to incorporate welfare in its objective. For further directions of future research, we refer to Appendix G, where we present several extensions of the model studied in this work."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported by the Stanford Interdisciplinary Graduate Fellowship (SIGF)."
        },
        {
            "heading": "A Additional Discussion on Modeling Assumptions",
            "text": "In this section, we present some additional discussions on our modeling assumptions for the security game that we study.\nFirst, as in prior literature on security games [9], we model our problem as a Stackelberg game, wherein users at each location observe and best respond to the administrator\u2019s mixed strategy [51]. Such an assumption on the game structure is reasonable for several applications of interest to this paper. For instance, in the case of queue jumping in intermediate public transport services, users typically use the IPT service at the same location every day; thus, users can estimate the likelihood of being caught by a security resource (e.g., a police officer) on a given day. We note that similar assumptions on the game structure have been considered even in related examples in the literature, e.g., the scheduling of randomized patrols for inspecting whether users have paid their fares in transit systems [23]. While we assume that users observe the probability of a security resource being allocated to their respective locations precisely, exploring alternative models of equilibrium formation when such perfect observability is not possible is an exciting direction for future research.\nNext, in this work, we assume that if the administrator allocates a security resource to a given location, then the security resource will collect fines from any defaulting users and prevent any fraudulent or illegal activities from occurring at that location. Such an assumption on the efficacy of a security resource in preventing a security breach at the location to which it is allocated is ubiquitous in the literature on security games [9] and also natural, as, for instance, the presence of a police officer is likely to deter users from engaging in queue jumping in IPT services.\nMoreover, we assume that the fine levied on defaulting users is a fixed (potentially small) constant. While setting high fines is likely to deter fraud, in most practical applications, fines for engaging in fraudulent or illegal activities are not arbitrarily high, e.g., traffic light or speeding violations cost about $100 in California. Furthermore, we note that our numerical experiments in Section 6.2 and Appendix F highlight the benefits of setting low to moderate fines in practice as it deters an administrator from solely maximizing revenues through the collected fines.\nIn addition, in this work, we assume that fines are the only penalty for users engaging in fraudulent or illegal activities. Such an assumption is particularly applicable for the setting with queue jumping in IPT services, as the penalty imposed on defaulting users is likely to only come in the form of a fine. However, for other application settings, different forms of punishment may be pertinent depending on the magnitude of the fraud [52], e.g., users may be sent to prison for certain crimes they commit. While we do not account for other forms of punishment beyond levying fines on defaulting users, there is scope to generalize the model presented in this work to account for different forms of punishment, which we defer to future research.\nFinally, while we consider allocating security resources, e.g., police officers, to monitor and police susceptible nodes or locations in the system, which is typical in the literature on security games [9], policing can be performed through many other security mechanisms. Most notably, modern security technology, such as security cameras, is frequently used to monitor certain fraudulent or illegal activities, e.g., store theft. Yet we note that while such technology has proven effective in some applications, e.g., in detecting theft at grocery stores, such technology is unlikely to be as effective for many examples of interest in this paper, e.g., the transportation and healthcare examples presented in Section 1.1. For instance, in the case of healthcare, human intervention is often necessary to conduct thorough audits to ensure that medications have not been over-prescribed or that patients do not have to stay longer than necessary at a hospital.\nFurthermore, while security cameras are often effective in detecting and fining users for road speeding violations, as user records can be determined using license plate information, such technology is likely far more ineffective in monitoring queue jumping in IPT services. In particular, using cameras to identify and fine users that skip queues is difficult, particularly in developing nation cities where IPT services are prevalent, as this would require advanced facial recognition software and a database of records of the entire population in the city. Moreover, given the commotion during the hailing of IPT services and that users often steal public property such as security cameras, a frequent occurrence in developing nation contexts, camera technology on its own may not be the most effective in policing queue jumping in IPT services. Consequently, the traditional mechanism of deploying security resources, such as police officers, as is typically the case for monitoring many traffic violations, such as speeding, despite the prevalence of camera technology, is likely the most effective mechanism in policing queue jumping in IPT services. Yet, we note that modern security technology, such as security cameras, has a role in augmenting traditional enforcement mechanisms, e.g., deploying police officers, in policing and monitoring fraudulent or illegal activities in the applications we consider. To that end, incorporating features of modern-day technology, such as security cameras, and how they can augment policing through traditional mechanisms into our studied framework is a worthwhile direction for future research."
        },
        {
            "heading": "B Proofs",
            "text": ""
        },
        {
            "heading": "B.1 Proof of Theorem 1",
            "text": "We prove Theorem 1 using two intermediate lemmas. Our first intermediate lemma shows that the optimal solution of the revenue maximization bi-level Program (2a)-(2b) can be computed using a linear program, as is elucidated by the following lemma.\nLemma 1 (Linear Programming Characterization of Revenue Maximization Problem). The optimal solution of the revenue maximization bi-level Program (2a)-(2b) can be computed through the solution of the following linear program:\nmax \u03c3 \u2208 R|L| \u2211 l\u2208L \u03c3lk\u039bl, (9a)\ns.t. \u03c3l \u2208 [0, dl\ndl + k ], for all l \u2208 L, (9b)\u2211\nl\u2208L\n\u03c3l \u2264 R. (9c)\nFor a proof of Lemma 1, see Appendix B.1.1. Lemma 1 establishes that computing an optimal revenue maximizing strategy of the administrator can be done efficiently in polynomial time. Our next intermediate lemma states that Algorithm 1 achieves the optimal solution of the linear Program (9a)-(9c).\nLemma 2 (Algorithm 1). The allocation strategy corresponding to Algorithm 1 achieves an optimal solution to the linear Program (9a)-(9c).\nFor a proof of Lemma 2, see Appendix B.1.2. Lemma 2 implies that the linear Program (9a)-(9c) can be solved in O(|L| log(|L|)) time as the time taken to sort the locations is O(|L| log(|L|)) and that taken to iterate through the locations in linear in |L|.\nNotice that jointly Lemmas 1 and 2 imply Theorem 1."
        },
        {
            "heading": "B.1.1 Proof of Lemma 1",
            "text": "To prove this claim, we first present the linear program that can be used to compute the solution of the bi-level Program (2a)-(2b). To construct this linear program, we note that any optimal solution to the bi-level Program (2a)-(2b) must satisfy \u03c3l \u2208 [0, dldl+k ] for all locations l \u2208 L. Note that if this relation is not satisfied and \u03c3l >\ndl dl+k for some location l, then we can consider an alternative allocation strategy\n\u03c3\u0303 where \u03c3\u0303l = min{\u03c3l, dldl+k} for all locations l. We note that \u03c3\u0303 is clearly feasible, i.e., \u03c3\u0303 \u2208 \u2126R, as the\nstrategy \u03c3 is feasible. Next, by the best-response function of users in the revenue maximization setting given by Equation (4), we note that yl(\u03c3\u0303) = 1 for all locations l, while yl(\u03c3) = 0 if \u03c3l >\ndl dl+k\nand yl(\u03c3) = 1 otherwise. As a consequence, we obtain for any strategy \u03c3 that:\nQR(\u03c3) = \u2211 l\u2208L \u03c3lyl(\u03c3)k\u039bl = \u2211\nl:\u03c3l> dl\ndl+k\n\u03c3lyl(\u03c3)k\u039bl + \u2211\nl:\u03c3l\u2264 dl\ndl+k\n\u03c3lyl(\u03c3)k\u039bl,\n(a) = \u2211 l:\u03c3l\u2264\ndl dl+k\n\u03c3lk\u039bl (b) = \u2211 l:\u03c3l\u2264\ndl dl+k\n\u03c3\u0303lk\u039bl,\n\u2264 \u2211 l\u2208L \u03c3\u0303lk\u039bl,\n(c) = QR(\u03c3\u0303)\nwhere (a) follows as yl(\u03c3) = 0 if \u03c3l > dl dl+k and yl(\u03c3) = 1 otherwise, (b) follows as \u03c3\u0303l = \u03c3l when \u03c3l \u2264 dldl+k , and (c) follows as yl(\u03c3\u0303) = 1 for all locations l. The above analysis implies that there exists an revenue-maximizing allocation such that \u03c3\u0303l \u2208 [0, dldl+k ] for all locations l, where it holds by Equation (4) that yl(\u03c3\u0303) = 1 for all locations l. Thus, imposing the restriction that \u03c3l \u2208 [0, dldl+k ] and that yl(\u03c3\u0303) = 1 for all locations l, we have that the bi-level Program (2a)-(2b) can be reformulated as the following linear program:\nmax \u03c3 \u2208 R|L| QR(\u03c3) = \u2211 l\u2208L \u03c3lk\u039bl, (10a)\ns.t. \u03c3l \u2208 [0, dl\ndl + k ], for all l \u2208 L, (10b)\u2211\nl\u2208L\n\u03c3l \u2264 R, (10c)\nwhere Constraints (10b) captures the upper bound on the administrator strategy required for optimality, Constraint (10c) represents the resource constraint of the administrator, and Objective (10a) represents the revenue maximization objective of the administrator when yl(\u03c3\u0303) = 1, which follows from Equation (4) under Constraint (10c). This establishes our claim."
        },
        {
            "heading": "B.1.2 Proof of Lemma 2",
            "text": "For simplicity of exposition, we prove this result in the setting when the values of \u039bl are all distinct, i.e., for any two locations l, l\u2032 \u2208 L, it holds that \u039bl \u0338= \u039bl\u2032 . We note that our analysis can be extended naturally to the setting when there are ties.\nTo prove this claim, let \u03c3G = (\u03c3Gl )l\u2208L denote the allocations of the greedy algorithm and \u03c3 \u2217 = (\u03c3\u2217l )l\u2208L denote the revenue-maximizing allocation. Further, suppose for contradiction that the greedy algorithm is not optimal, i.e., \u2211 l\u2208L \u03c3 G l \u039bl < \u2211 l\u2208L \u03c3 \u2217 l \u039bl, where we have dropped the fine k from the objective of the linear Program (10a)-(10c). Note that this implies that there is some location l such that \u03c3Gl \u0338= \u03c3\u2217l , which consequently implies by the nature of the greedy algorithm that there is a location l\u0303 such that \u03c3G\nl\u0303 > \u03c3\u2217 l\u0303 . However, by the feasibility constraint that \u2211\nl\u2208L \u03c3l \u2264 R and the optimality of the revenue-maximizing solution it must hold that there is some location l\u2032 such that \u03c3Gl\u2032 < \u03c3 \u2217 l\u2032 .\nTo derive our desired contradiction, we now construct another feasible strategy with a strictly higher objective than the strategy \u03c3\u2217. In particular, consider \u03f5 > 0 as some small positive constant such that \u03c3\u2217 l\u0303 + \u03f5 \u2264 \u03c3G l\u0303 and \u03c3\u2217l\u2032 \u2265 \u03c3Gl\u2032 + \u03f5. Then, define the strategy \u03c3\u2032 = (\u03c3\u2032l)l\u2208L, where \u03c3\u2032l = \u03c3\u2217l for all l \u0338= l\u2032, l\u0303, \u03c3l\u2032 = \u03c3 \u2217 l\u2032 \u2212 \u03f5 and \u03c3\u2032l\u0303 = \u03c3 \u2217 l\u0303 + \u03f5. Note that this new strategy is feasible. In particular, it is straightforward to check that \u03c3\u2032l \u2208 [0, dl\ndl+k ] for all locations l and that the resource constraint is satisfied as:\u2211\nl\n\u03c3\u2032l = \u2211\nl \u0338={l\u2032,l\u0303}\n\u03c3\u2032l + \u03c3 \u2032 l\u2032 + \u03c3 \u2032 l\u0303 = \u2211 l \u0338={l\u2032,l\u0303} \u03c3\u2217l + \u03c3 \u2217 l\u2032 \u2212 \u03f5+ \u03c3\u2217l\u0303 + \u03f5 = \u2211 l \u03c3\u2217l \u2264 R,\nwhere the final inequality follows by the feasibility of \u03c3\u2217. Next, we show that the revenue of the new strategy \u03c3\u2032 is greater than that of \u03c3\u2217. To see this note that:\nQR(\u03c3 \u2032) = \u2211 l\u2208L \u03c3\u2032l\u039bl,\n= \u2211\nl \u0338={l\u2032,l\u0303}\n\u03c3\u2217l \u039bl + (\u03c3 \u2217 l\u2032 \u2212 \u03f5)\u039bl\u2032 + (\u03c3\u2217l\u0303 + \u03f5)\u039bl\u0303,\n= \u2211 l\u2208L \u03c3\u2217l \u039bl + \u03f5(\u039bl\u0303 \u2212 \u039bl\u2032),\n> \u2211 l\u2208L \u03c3\u2217l \u039bl = QR(\u03c3 \u2217),\nwhere the inequality follows as \u039bl\u2032 < \u039bl\u0303 due to the nature of the greedy algorithm which allocates resources to locations in the descending order of the values of \u039bl. The above relationship implies that the strategy \u03c3 \u2032 achieves a greater revenue compared to \u03c3\u2217, a contradiction. Thus, it follows that Algorithm 1 cannot have a strictly lower objective than the optimal algorithm, implying its optimality, which established our claim."
        },
        {
            "heading": "B.2 Proof of Proposition 1",
            "text": "Given the best-response function of the users at a given location l in Equation (5), we prove Proposition 1 using two intermediate lemmas. Our first lemma establishes that, without loss of generality, it suffices to restrict our attention to administrator strategies where the total allocation of security personnel \u03c3l at any location l does not exceed dldl+k , i.e., establishing the first point in the statement of Proposition 1.\nLemma 3 (Upper Bound on Administrator\u2019s Mixed Strategy Vector). Suppose that users at each location best-respond by solving Problem (1). Then, there exists an administrator strategy \u03c3\u0303\u2217 that satisfies \u03c3\u0303\u2217l \u2264 dl dl+k for all locations l and is a solution to the Problem (3a)-(3b).\nFor a proof of Lemma 3, see Appendix B.2.1. Lemma 3 establishes that allocating more than a certain threshold of resources, as specified by the fraction dldl+k , at any location l will not improve the administrator\u2019s objective. Such a result holds as, by Equation (5), allocating \u03c3l = dl\ndl+k is enough to deter users from engaging\nin fraudulent behavior at a given location l; hence, allocating any more resources than this threshold at a given location will not result in a further improvement in the administrator\u2019s objective (see right of Figure 1).\nOur second intermediate lemma establishes the existence of an optimal administrator strategy \u03c3\u0303\u2217 such that for at most one location l\u2032 \u03c3\u0303\u2217l\u2032 \u2208 ( 0, dl\u2032dl\u2032+k ) , thereby establishing the second condition in Proposition 1.\nLemma 4 (Structure of Optimal Mixed Strategy Solution). Suppose that users at each location best-respond by solving Problem (1). Then, there exists an administrator strategy \u03c3\u0303\u2217 that is a solution to Problem (3a)(3b) such that for at most one location l\u2032 \u03c3\u0303\u2217l\u2032 \u2208 ( 0, dl\u2032dl\u2032+k ) . For all other locations l, it holds that either \u03c3\u0303\u2217l = 0 or \u03c3\u0303 \u2217 l = dl dl+k .\nFor a proof of Lemma 4, see Appendix B.2.2. Using the structure of the optimal administrator strategy established in Lemmas 3 and 4, it is straightforward to see that the optimal administrator objective satisfies the relation in the statement of Proposition 1. To see this, let L1, L2, and l\n\u2032 be as defined in the statement of Proposition 1. Then, we have that:\nW\u2212R (\u03c3\u0303 \u2217) = \u2211 l\u2208L (1\u2212 \u03c3\u0303\u2217l )yl(\u03c3\u0303\u2217)vl,\n= \u2211 l\u2208L1 (1\u2212 \u03c3\u0303\u2217)yl(\u03c3\u0303\u2217)vl + \u2211 l\u2208L2 (1\u2212 \u03c3\u0303\u2217l )yl(\u03c3\u0303\u2217)vl + (1\u2212 \u03c3\u0303\u2217l\u2032)yl\u2032(\u03c3\u0303\u2217)vl\u2032 ,\n(a) = \u2211 l\u2208L2 (1\u2212 \u03c3\u0303\u2217l )yl(\u03c3\u0303\u2217)vl + (1\u2212 \u03c3\u0303\u2217l\u2032)yl\u2032(\u03c3\u0303\u2217)vl\u2032 ,\n(b) = \u2211 l\u2208L2 vl + (1\u2212 \u03c3\u0303\u2217l\u2032)vl\u2032 ,\nwhere (a) follows from the fact that yl(\u03c3\u0303 \u2217) = 0 for l \u2208 L1 by Equation (5) as \u03c3\u0303\u2217l = dl dl+k for all l \u2208 L1 and (b) follows as yl(\u03c3\u0303 \u2217) = 1 for l \u2208 L\\L1 and the fact that \u03c3\u0303\u2217l = 0 for all l \u2208 L2 and that \u03c3\u0303\u2217l\u2032 \u2208 ( 0, dl\u2032dl\u2032+k ) .\nUsing the above inequality, we have the following relation for the welfare under the strategy \u03c3\u0303\u2217:\nWR(\u03c3\u0303 \u2217) = \u2211 l\u2208L vl \u2212W\u2212R (\u03c3\u0303 \u2217), (11)\n= \u2211 l\u2208L vl \u2212 (\u2211 l\u2208L2 vl + (1\u2212 \u03c3\u0303\u2217l\u2032)vl\u2032 ) , (12)\n= \u2211 l\u2208L1 vl + \u03c3\u0303 \u2217 l\u2032vl\u2032 , (13)\nwhich establishes the desired relation for the administrator\u2019s objective under the optimal strategy \u03c3\u0303\u2217, thereby proving our claim."
        },
        {
            "heading": "B.2.1 Proof of Lemma 3",
            "text": "Suppose that \u03c3\u2217 is a welfare maximizing strategy of the administrator and y(\u03c3\u2217) is the corresponding bestresponse of users given by Equation (5), i.e., (\u03c3\u2217,y(\u03c3\u2217)) is an optimal solution to Problem (3a)-(3b). Then, we define a new feasible strategy \u03c3\u0303\u2217 that satisfies \u03c3\u0303\u2217l \u2264 dl dl+k for all locations l and show that it achieves the same welfare as the strategy \u03c3\u2217 using users\u2019 best response function given by Equation (5). In particular, define \u03c3\u0303\u2217 such that \u03c3\u0303\u2217l = min{\u03c3\u2217l , dl dl+k\n} for all locations l. It is straightforward to check that \u03c3\u0303\u2217 is a feasible strategy, i.e., \u03c3\u0303\u2217 \u2208 \u2126R. To see this, note that\n\u03c3\u0303\u2217l = min{\u03c3\u2217l , dl dl+k } \u2265 0 for all locations l as \u03c3\u2217l \u2265 0 by its feasibility to Problem (3a)-(3b). Furthermore, we have that \u03c3\u0303\u2217l = min{\u03c3\u2217l , dl dl+k } \u2264 1 as dldl+k \u2264 1 for all locations l. Finally, since it holds that \u2211 l\u2208L \u03c3 \u2217 l \u2264 R by the feasibility of \u03c3\u2217 to Problem (3a)-(3b), it follows that\u2211 l\u2208L \u03c3\u0303\u2217l = \u2211 l\u2208L min{\u03c3\u2217l , dl dl + k } \u2264 \u2211 l\u2208L \u03c3\u2217l \u2264 R,\nwhich establishes the feasibility of strategy \u03c3\u0303\u2217 for Problem (3a)-(3b). Next, to see that \u03c3\u0303\u2217 achieves the same welfare for the administrator as \u03c3\u2217, we first note that the bestresponse strategy of users at each location is unchanged under \u03c3\u2217 and \u03c3\u0303\u2217, i.e., y(\u03c3\u2217) = y(\u03c3\u0303\u2217). To see this, we show that yl(\u03c3 \u2217) = yl(\u03c3\u0303 \u2217) for all locations l by considering two cases: (i) \u03c3\u2217l < dl dl+k and (ii) \u03c3\u2217l \u2265 dl dl+k . Note that in the first case, \u03c3\u0303\u2217l = \u03c3 \u2217 l and thus it must be that yl(\u03c3 \u2217) = yl(\u03c3\u0303 \u2217) = 1. On the other hand, in the second case, note that yl(\u03c3 \u2217) = 0 and that \u03c3\u0303\u2217l = dl dl+k , from which it follows by Equation (5) that yl(\u03c3\u0303 \u2217) = 0 as well. Having shown that y(\u03c3\u2217) = y(\u03c3\u0303\u2217), we now obtain that:\nW\u2212R (\u03c3 \u2217) = \u2211 l\u2208L (1\u2212 \u03c3\u2217l )y\u2217l (\u03c3\u2217)vl,\n(a) = \u2211 l\u2208L:yl(\u03c3\u2217)=0 (1\u2212 \u03c3\u2217l )y\u2217l (\u03c3\u2217)vl + \u2211 l\u2208L:yl(\u03c3\u2217)=1 (1\u2212 \u03c3\u2217l )y\u2217l (\u03c3\u2217)vl,\n(b) = \u2211 l\u2208L:yl(\u03c3\u0303\u2217)=1 (1\u2212 \u03c3\u0303l\u2217)y\u2217l (\u03c3\u0303\u2217)vl,\n(c) = W\u2212R (\u03c3\u0303 \u2217)\nwhere (a) follows by splitting the sum, (b) follows from the fact that \u03c3\u0303\u2217l = \u03c3 \u2217 l for all l where yl(\u03c3 \u2217) = 1, as in this regime \u03c3\u2217l < dl dl+k\nby Equation (5), and (c) follows from the fact that y(\u03c3\u2217) = y(\u03c3\u0303\u2217). Finally, by the optimality of \u03c3\u2217 and from the above obtained relations for the new administrator strategy\n\u03c3\u0303\u2217, it holds W\u2212R (\u03c3\u0303 \u2217) = W\u2212R (\u03c3 \u2217) \u2264 W\u2212R (\u03c3), for all feasible \u03c3 \u2208 \u2126R. Thus, we have established our claim that there exists an administrator strategy \u03c3\u0303\u2217 that satisfies \u03c3\u0303\u2217l \u2264 dl dl+k for all locations l and is a solution to Problem (3a)-(3b)."
        },
        {
            "heading": "B.2.2 Proof of Lemma 4",
            "text": "To prove this claim, we show that any administrator strategy \u03c3\u2217 can be transformed into another strategy \u03c3\u0303\u2217 satisfying the condition in the statement of the lemma with at most the same objective as that corresponding to \u03c3\u2217.\nTo see this, consider an optimal administrator strategy \u03c3\u2217 such that the strategies (\u03c3\u2217,y(\u03c3\u2217)) are an optimal solution to Problem (3a)-(3b). Further, suppose that there are at least two locations l1 and l2 such\nthat \u03c3\u2217l1 \u2208 ( 0, dl1 dl1+k ) and \u03c3\u2217l2 \u2208 ( 0, dl2 dl2+k ) . In particular, let L\u2032 \u2286 L be the set of locations such that for any\nl\u2032 \u2208 L\u2032 it holds that \u03c3\u2217l\u2032 \u2208 ( 0, dl\u2032dl\u2032+k ) . Then, we order the locations in the set L\u2032 in descending order of the values vl and without loss of generality number these locations 1, . . . , |L\u2032|. Then, we construct \u03c3\u0303\u2217 through a water-filling approach. To elucidate this approach, we begin by transferring the mass \u03c3\u2217|L\u2032| corresponding the location with the smallest value of vl in the set L \u2032 to the location with the highest value of vl in the set L\u2032. In particular, we transfer min{\u03c3\u2217|L\u2032|, d1 d1+k \u2212\u03c3\u22171} from location |L\u2032| to location 1. If location |L\u2032|\u2019s mass is exhausted before filling up location one to d1d1+k , then we transfer further mass of location |L \u2032\u22121| to location one. On the other hand, if location 1 is filled up to d1d1+k with location |L \u2032|, then the remaining mass from location |L\u2032| is transferred to location two in an analogous way. We then repeat this process of transferring mass from locations with lower values of vl to those with higher values of vl in the set L \u2032 until there is at\nmost one location remaining such that \u03c3\u0303\u2217l\u2032 \u2208 ( 0, dl\u2032dl\u2032+k ) , where \u03c3\u0303\u2217 is the administrator strategy constructed by the above procedure. Having constructed the strategy \u03c3\u0303\u2217 in the above manner, it is then straightforward to see that W\u2212R (\u03c3\u0303\n\u2217) \u2264 W\u2212R (\u03c3 \u2217) as:\nW\u2212R (\u03c3 \u2217) = \u2211 l\u2208L (1\u2212 \u03c3\u2217l )yl(\u03c3\u2217)vl,\n= \u2211 l\u2208L\u2032 (1\u2212 \u03c3\u2217l )yl(\u03c3\u2217)vl + \u2211 l\u2208L\\L\u2032 (1\u2212 \u03c3\u2217l )yl(\u03c3\u2217)vl,\n(a) = \u2211 l\u2208L\u2032 (1\u2212 \u03c3\u2217l )yl(\u03c3\u2217)vl + \u2211 l\u2208L\\L\u2032 (1\u2212 \u03c3\u0303\u2217l )yl(\u03c3\u0303\u2217)vl,\n(b) \u2265 \u2211 l\u2208L\u2032 (1\u2212 \u03c3\u0303\u2217l )yl(\u03c3\u2217)vl + \u2211 l\u2208L\\L\u2032 (1\u2212 \u03c3\u0303\u2217l )yl(\u03c3\u0303\u2217)vl,\n(c) \u2265 \u2211 l\u2208L\u2032 (1\u2212 \u03c3\u0303\u2217l )yl(\u03c3\u0303\u2217)vl + \u2211 l\u2208L\\L\u2032 (1\u2212 \u03c3\u0303\u2217l )yl(\u03c3\u0303\u2217)vl, = W\u2212R (\u03c3\u0303 \u2217),\nwhere (a) follows as \u03c3\u2217l = \u03c3\u0303 \u2217 l for all l \u2208 L\\L\u2032, (b) follows by our algorithmic procedure of constructing \u03c3\u0303\u2217 as we transfer mass from lower values of vl to higher values of vl in the set L \u2032, and (c) follows as yl(\u03c3\u0303 \u2217) \u2264 1 = yl(\u03c3\u2217) for all l \u2208 L\u2032. The above analysis establishes our claim that there exists an administrator strategy \u03c3\u0303\u2217 that is a solution\nto Problem (3a)-(3b) such that for at most one location l\u2032, it holds that \u03c3\u0303\u2217l\u2032 \u2208 ( 0, dl\u2032dl\u2032+k ) , which establishes our claim."
        },
        {
            "heading": "B.3 Proof of Theorem 2",
            "text": "We prove this result through a reduction from an instance of the partition problem. A partition instance consists of a sequence of numbers a1, . . . , an with \u2211 l\u2208[n] al = A and involves the task of deciding whether\nthere is some subset S1 of numbers such that \u2211\nl\u2208S1 al = A 2 . Without loss of generality, we consider a\npartition instance where maxl\u2208[n] al \u2264 A2 . Note that if maxl\u2208[n] al > A 2 , then clearly, there is no subset S1 of numbers such that \u2211\nl\u2208S1 al = A 2 , i.e., such an instance of partition can be solved in polynomial time.\nWe now construct an instance of the welfare maximization problem. In particular, we consider an instance with n+ 1 locations, where the first n locations correspond to each number of the partition instance, where we define vl = al and let\ndl dl+k = alA for all locations l \u2208 [n]. Furthermore, we consider a location n + 1 such that vn+1 = maxl\u2208[n] vl + \u03f5 and\ndn+1 dn+1+k = 12 + \u03b4, where \u03f5 > 0 and \u03b4 > 0 can be interpreted as small\nconstants. We choose \u03f5 such that maxl\u2208[n] al+\u03f5 < A, which is well defined as we consider partition instances where maxl\u2208[n] al \u2264 A2 . Finally, we let the total resource budget R = 1 2 . Given this instance of the welfare maximization problem, we now show that a sequence of numbers correspond to a \u201cYes\u201d instance of the partition problem if and only if the objective of Problem (3a)-(3b) for above defined instance is at most A 2 + vn+1, i.e., the total welfare is at least A 2 .\n( =\u21d2 :) We first suppose that we have a \u201cYes\u201d instance of the partition problem, i.e., there exists a set S of numbers such that \u2211 l\u2208S al = A 2 . In this case, we present a method to construct a feasible resource allocation strategy \u03c3 of the administrator that achieves WR(\u03c3) \u2265 A2 . In particular, we consider an allocation \u03c3 for the above defined instance of the welfare maximization problem, where \u03c3l =\ndl dl+k units of resources\nare invested in for all locations l \u2208 S and \u03c3l = 0 for all l \u2208 [n+ 1]\\S. Such an allocation is feasible for this instance of the welfare maximization problem as:\u2211\nl\u2208[n+1] \u03c3l = \u2211 l\u2208S dl dl + k = \u2211 l\u2208S al A = 1 A \u2211 l\u2208S al = 1 A \u00d7 A 2 = 1 2 = R,\ni.e., the resource constraint is satisfied. Furthermore, the objective of Problem (3a)-(3b) under the strategy \u03c3 is given by\nW\u2212R (\u03c3) = \u2211\nl\u2208[n+1]\n(1\u2212 \u03c3l)yl(\u03c3)vl,\n(a) = \u2211 l\u2208S (1\u2212 dl dl + k )yl(\u03c3)vl + \u2211 l\u2208[n+1]\\S (1\u2212 0)yl(\u03c3)vl,\n(b) = \u2211 l\u2208[n]\\S vl + vn+1,\n(c) =\nA 2 + vn+1,\nwhere (a) follows as \u03c3l = dl dl+k for all l \u2208 S and \u03c3l = 0 for all l \u2208 [n + 1]\\S, (b) follows as yl(\u03c3) = 0 when \u03c3l = dl\ndl+k and yl(\u03c3) = 1 when \u03c3l = 0 by the best-response of users in Equation (5), and (c) follows as\u2211\nl\u2208[n]\\S vl = A 2 . The above relation implies that the welfare of the allocation \u03c3 is given by\nWR(\u03c3) = \u2211\nl\u2208[n+1]\nvl \u2212W\u2212R (\u03c3) = A+ vn+1 \u2212 ( A\n2 + vn+1\n) = A\n2 .\nThe above relation implies that WR(\u03c3) \u2265 A2 , and thus we have shown that a \u201cYes\u201d instance of the partition problem implies that the administrator\u2019s optimal welfare for the above defined instance is at least A2 , which establishes the forward direction of our claim.\n( \u21d0= :) Next, suppose that the optimal resource allocation strategy \u03c3 of the administrator is such that WR(\u03c3) \u2265 A2 , i.e., W \u2212 R (\u03c3) \u2264 A 2 +vn+1 where \u03c3 \u2208 \u2126R satisfies \u2211 l\u2208[n+1] \u03c3l \u2264 R = 1 2 . From the structure of the optimal administrator strategy established in Proposition 1, we note that the objective of Problem (3a)-(3b) is given by:\nW\u2212R (\u03c3) = \u2211 l\u2208L2 vl + (1\u2212 \u03c3l\u2032)vl\u2032 , (14)\nfor some location l\u2032 and a set of locations L2, where it holds that \u2211\nl\u2208[n+1]\\L2 \u03c3l \u2264 1 2 . Then, we show that\nwe have a \u201cYes\u201d instance of the partition problem by considering two cases: (i) l\u2032 = \u2205 and (ii) l\u2032 \u0338= \u2205.\nCase (i): In the setting when l\u2032 = \u2205, without loss of generality, for ease of notation, we define L2\u222a{n+1} as the set of locations for which \u03c3l = 0 and L1 as the set of locations for which \u03c3l =\ndl dl+k . Note here\nthat as R = 12 < 1 2 + \u03b4 = dn+1 dn+1+k it follows for location n + 1 that \u03c3n+1 = 0 as l \u2032 = \u2205. Next, for our\nwelfare maximization instance, we note by assumption that the following two inequalities are satisfied: (i)\u2211 l\u2208L2\u222a{n+1} vl \u2264 A 2 + vn+1 and (ii) \u2211 l\u2208L1 al A = \u2211 l\u2208L1 dl dl+k\n\u2264 12 . From the first inequality, we have that\u2211 l\u2208L1 al = \u2211 l\u2208L1 vl = \u2211 l\u2208L vl \u2212 \u2211 l\u2208L2\u222a{n+1} vl \u2265 A 2 ,\nwhere the inequality follows as \u2211 l\u2208[n+1] al = A + vn+1 and the fact that \u2211 l\u2208L2\u222a{n+1} vl \u2264 A 2 + vn+1.\nCombining this derived inequality with the resource constraint that \u2211\nl\u2208L1 al A \u2264 1 2 , it follows that \u2211 l\u2208L1 al =\nA 2 , i.e., we have a \u201cYes\u201d instance of the partition problem.\nCase (ii): In the setting when l\u2032 \u0338= \u2205, we first show that it must follow that l\u2032 = {n+1}. At a high-level, this result follows as location n+ 1 has the highest value of vl among all the locations. We now proceed by contradiction to establish this claim by supposing that l\u2032 \u0338= {n + 1}. Next, note that by our constructed instance that dn+1dn+1+k = 1 2 + \u03b4 > R and thus the location n+ 1 \u2208 L2.\nTo derive our contradiction, we consider an allocation \u03c3\u0303 where \u03c3\u0303l\u2032 = 0 and \u03c3\u0303n+1 = \u03c3l\u2032 . Note that \u03c3l\u2032 \u2208 ( 0, d l \u2032\nd l \u2032+k\n) and thus it also holds that \u03c3l\u2032 \u2208 ( 0, dn+1dn+1+k ) , as dn+1dn+1+k > 1 2 \u2265 maxl\u2208[n] al A = d l \u2032\nd l \u2032+k\n. Clearly\nthe new allocation \u03c3\u0303 is feasible as \u03c3 is feasible. Next, we observe that W\u2212R (\u03c3) (a) = \u2211 l\u2208L2 vl + (1\u2212 \u03c3l\u2032)vl\u2032 = \u2211 l\u2208L2\\{n+1} vl + vn+1 + (1\u2212 \u03c3l\u2032)vl\u2032 ,\n(b) > \u2211 l\u2208L2\u222a{l\u2032} vl + (1\u2212 \u03c3l\u2032)vn+1,\n(c) = \u2211 l\u2208L2\u222a{l\u2032} vl + (1\u2212 \u03c3\u0303n+1)vn+1,\n(d) = W\u2212R (\u03c3\u0303),\nwhere (a) follows from Equation (14), (b) follows as vn+1 = maxl\u2208[n] vl + \u03f5 > vl\u2032 , (c) follows as \u03c3\u0303n+1 = \u03c3l\u2032 , and (d) follows by the definition of W\u2212R (\u03c3\u0303) and the fact that the only change in the allocation between \u03c3 and \u03c3\u0303 is for locations l\u2032 and n+1. The above relations imply that the allocation strategy \u03c3\u0303 achieves a lower objective for Problem (3a)-(3b) than the strategy \u03c3, a contradiction. Thus, it follows that if l\u2032 \u0338= \u2205, then it must be that l\u2032 = {n+ 1} for our above defined instance of the welfare maximization problem.\nNext, recall that L2 are the set of locations for which \u03c3l = 0 and L1 are the set of locations for which \u03c3l =\ndl dl+k\n. Then, for our welfare maximization instance, we have that the following two inequalities hold:\u2211 l\u2208L2 vl + (1\u2212 \u03c3n+1)vn+1 \u2264 A 2 + vn+1, (15)\n\u2211 l\u2208L2 dl dl + k + \u03c3n+1 \u2264 1 2 . (16)\nThe first inequality represents the fact that the optimal objective of the Problem (3a)-(3b) is at most A2 +vn+1 and the second inequality implies that the total resource spending does not exceed the available resources.\nThen, noting that \u2211\nl\u2208[n+1] vl = A+ vn+1, we have from the Equation (15) that:\u2211 l\u2208L1 vl + \u03c3n+1vn+1 = \u2211 l\u2208[n+1] vl \u2212 (\u2211 l\u2208L2 vl + (1\u2212 \u03c3n+1)vn+1 ) \u2265 A+ vn+1 \u2212 ( A 2 + vn+1 ) = A 2 .\nSubstituting vl = al and vn+1 = maxl\u2208[n] al + \u03f5 in the above inequality, we obtain that:\u2211 l\u2208L1 al + \u03c3n+1(max l\u2208[n] al + \u03f5) \u2265 A 2 . (17)\nMoreover, substituting dldl+k = al A in Equation (16), we obtain that:\u2211\nl\u2208L1\nal + \u03c3n+1A \u2264 A\n2 . (18)\nCombining Equations (17) and (18), we obtain that:\n\u03c3n+1(max l\u2208[n]\nal + \u03f5) \u2265 \u03c3n+1A.\nNote that this inequality can only be satisfied if either \u03c3n+1 = 0 or maxl\u2208[n] al + \u03f5 \u2265 A = \u2211\nl\u2208[n] al. Note that the latter inequality cannot be satisfied as \u03f5 has been chosen to be a small constant such that maxl\u2208[n] al + \u03f5 < A, where, recall that maxl\u2208[n] al \u2264 A2 . As a result, the above relation implies \u03c3n+1 = 0, which, from Equations (17) and (18), implies the following two relations:\u2211\nl\u2208L1\nal \u2265 A\n2 and \u2211 l\u2208L1 al \u2264 A 2 ,\nwhich together imply that \u2211\nl\u2208L1 al = A 2 . Thus, again, we have a \u201cYes\u201d instance of partition, which\nestablishes our claim."
        },
        {
            "heading": "B.4 Proof of Theorem 3",
            "text": ""
        },
        {
            "heading": "B.4.1 Proof Overview",
            "text": "We first define a linear program resembling a fractional knapsack like optimization and leverage Proposition 1 to show that the optimal administrator welfare is upper bounded by the optimal objective of this linear program. Next, since the optimal objective of the fractional knapsack problem satisfies \u2211 l\u2208S vl + xl\u0303vl\u0303 for some subset of locations S and a location l\u0303 with xl\u0303 \u2264 dl\u0303\ndl\u0303+k , to establish the desired half approximation, we\nshow that (i) WR(\u03c3 \u2032) \u2265 xl\u0303vl\u0303 and (ii) WR(\u03c3\u0303) \u2265 \u2211 l\u2208S vl, where the allocations \u03c3\n\u2032 and \u03c3\u0303 are as defined in Algorithm 2. Notice that the proof of claim (i) is by construction as \u03c3\u2032 by definition is chosen to maximize the welfare from spending on a single location. The proof of claim (ii) relies on the fact that the greedy algorithm in Step 1 on Algorithm 2 precisely corresponds to the greedy algorithm to optimize the knapsack linear program. Finally, combining the results of claims (i) and (ii), the desired result follows."
        },
        {
            "heading": "B.4.2 Complete Proof of Theorem 3",
            "text": "To prove this claim, we first recall from Proposition 1 that the welfare maximizing objective of the administrator at the solution \u03c3\u2217 is given by WR(\u03c3 \u2217) = \u2211\nl\u2208L1 vl + \u03c3 \u2217 l\u0303 vl\u0303 for some subset of locations L1 where\n\u03c3\u2217l = dl dl+k for all l \u2208 L1 and a location l\u0303 such that \u2211 l\u2208L1 dl dl+k + \u03c3\u2217 l\u0303 \u2264 R. Furthermore, we consider the following knapsack optimization problem:\nG\u2032R = max 0\u2264xl\u22641 \u2211 l\u2208L xlvl s.t. \u2211 l\u2208L dl dl + k xl \u2264 R. (19)\nThen, to prove this claim, we proceed in two steps. First, we show that the optimal welfare-maximizing objective of the administrator is no more than G\u2032R, the optimal objective corresponding to Problem (19). Then, we use this result and the property of the optimal welfare-maximizing objective of the administrator from Proposition1 to establish the half approximation guarantee of Algorithm 2."
        },
        {
            "heading": "Proof of WR(\u03c3",
            "text": "\u2217) \u2264 G\u2032R: To prove this claim, first note by the structure of the optimal solution established in Proposition 1 that \u03c3\u2217l \u2265 0 and \u03c3\u2217l \u2264 dl dl+k for all locations l, and by the feasibility of \u03c3\u2217 that \u2211 l\u2208L \u03c3 \u2217 l \u2264 R. Then, we transform \u03c3\u2217 to a feasible solution of Problem (19) by defining xl = dl+k dl \u03c3\u2217l for all l. Note that\nx = (xl)l\u2208L is a feasible solution to Problem (19) as, by definition of x it holds that 0 \u2264 xl \u2264 1, and the resource constraint is satisfied:\u2211\nl\u2208L\ndl dl + k xl = \u2211 l\u2208L dl dl + k dl + k dl \u03c3\u2217l = \u2211 l\u2208L \u03c3\u2217l \u2264 R.\nNext, to show that G\u2032R \u2265 WR(\u03c3\u2217), we have:\nG\u2032R (a) \u2265 \u2211 l\u2208L xlvl,\n(b) = \u2211 l\u2208L1 dl + k dl \u03c3\u2217l vl + dl\u0303 + k dl\u0303 \u03c3\u2217 l\u0303 vl\u0303,\n(c) \u2265 \u2211 l\u2208L1 vl + \u03c3 \u2217 l\u0303 vl\u0303,\n(d) = WR(\u03c3 \u2217),\nwhere (a) follows from the fact that x = (xl)l\u2208L is one of the feasible solutions to Problem (19), (b) follows from the fact that xl =\ndl+k dl \u03c3\u2217l and that \u03c3 \u2217 l = 0 for l /\u2208 L1 \u222a {l\u0303}, (c) follows from the fact that \u03c3\u2217l = dl dl+k\nfor all l \u2208 L1 and that dl\u0303+k dl\u0303 \u2265 1, and (d) follows by the definition of WR(\u03c3\u2217). Thus, we have shown that G\u2032R \u2265 WR(\u03c3\u2217).\nProving the Approximation Ratio: Finally, to complete the proof of our claim, notice that as Problem (19) is a fractional knapsack problem that G\u2032R = \u2211 l\u2208S vl + x \u2217 l\u2032vl\u2032 for some subset S and allocation x\n\u2217, where resources are allocated to locations in the descending orders of the bang-per-buck. Next, note that by Algorithm 2 that\nWR(\u03c3 \u2217 A) \u2265 WR(\u03c3\u0303) \u2265 \u2211 l\u2208S vl, (20)\nwhere recall that \u03c3\u0303 is the allocation corresponding to the greedy algorithm in Step 1 of Algorithm 2 and the second inequality follows as the greedy algorithm selects at least the first S locations in the descending order of their bang-per-buck values. Furthermore, by Step 2 of Algorithm 2 and the construction of the allocation \u03c3\u2032, it follows that\nWR(\u03c3 \u2217 A) \u2265 WR(\u03c3\u2032) \u2265 x\u2217\u2032 vl\u2032 . (21)\nThen, summing Equations (20) and (21), we obtain that:\n2WR(\u03c3 \u2217 A) \u2265 \u2211 l\u2208S vl + x \u2217 l\u2032vl\u2032 = G \u2032 R \u2265 WR(\u03c3\u2217),\nwhich establishes our claim that WR(\u03c3 \u2217 A) \u2265 12WR(\u03c3 \u2217), i.e., Algorithm 2 is a half approximation to the optimal solution to the bi-level Program (3a)-(3b)."
        },
        {
            "heading": "B.5 Proof of Theorem 4",
            "text": "To prove this result, we again consider the fractional knapsack Problem (19) defined in the proof of Theorem 3 and use the fact that the objective of the fractional knapsack Problem (19) upper bounds the optimal administrator welfare, given by WR(\u03c3\n\u2217). In particular, to establish the desired resource augmentation guarantee, we first recall from the optimal solution of the fractional knapsack problem that G\u2032R = \u2211 l\u2208S vl + x\u2217l\u2032vl\u2032 for some subset S and allocation x \u2217, where resources are allocated to locations in the descending orders of the bang-per-buck. Next, given R+1 resources, we define \u03c3\u0303R+1 as the allocation corresponding to Step 1 of Algorithm 2 and let \u03c3R+1A be the allocation corresponding to Algorithm 2 with R+ 1 resources. We now show that WR+1(\u03c3 R+1 A ) \u2265 WR(\u03c3\u2217) to prove our claim.\nTo show this, we note by our greedy procedure in Step 1 of Algorithm 2 that \u03c3\u0303R+1 \u2265 \u03c3\u0303 and that \u03c3\u0303R+1l\u2032 = dl\u2032 dl\u2032+k\n. Such a result holds as the additional resource ensures that the administrator will allocate dl\u2032\ndl\u2032+k \u2264 1 units of resources to location l\u2032, as location l\u2032 has the highest bang-per-buck ratio among the remaining locations by definition (as G\u2032R = \u2211 l\u2208S vl + x \u2217 l\u2032vl\u2032 for some subset S and allocation x \u2217, where resources are allocated to locations in the descending orders of the bang-per-buck). Since \u03c3\u0303R+1l\u2032 = dl\u2032\ndl\u2032+k , note\nby the best-response Problem (7) of users that yl\u2032(\u03c3\u0303 R+1) = 0. Consequently, as the allocation \u03c3\u0303R+1 allocates resources to locations in the descending order of the bang-per-buck ratios and, in particular, allocates dl\u2032\ndl\u2032+k \u2264 1 fraction of resources to location l\u2032, it follows that\nWR+1(\u03c3 R+1 A )\n(a) \u2265 WR(\u03c3\u0303R+1) (b) \u2265 \u2211 l\u2208S vl + vl\u2032 (c) \u2265 \u2211 l\u2208S vl + x \u2217 l\u2032vl\u2032 = G \u2032 R (d) \u2265 W \u2217R(\u03c3\u2217)\nwhere (a) follows from the fact that \u03c3R+1A corresponds to the outcome with a greater welfare from steps 1 and 2 of Algorithm 2, (b) follows as \u03c3\u0303R+1 corresponds to an allocation of resources in the descending orders of the bang-per-buck when an administrator has an additional resource that it can spend on location l\u2032 and that yl\u2032(\u03c3\u0303 R+1) = 0. Furthermore, (c) follows from the fact that x\u2217l\u2032 \u2208 (0, dl\u2032\ndl\u2032+k ) and (d) follows from our\nrelation derived in proof of Theorem 3 that the optimal objective of the fractional knapsack Problem (19) is at least the welfare-maximizing objective. Thus, we have shown that WR+1(\u03c3 R+1 A ) \u2265 WR(\u03c3\u2217), which establishes our claim."
        },
        {
            "heading": "B.6 Proof of Theorem 5",
            "text": "We prove Theorem 5 through a reduction from an instance of the partition problem. Recall that a partition instance consists of a sequence of numbers a1, . . . , an with \u2211 l\u2208[n] al = A and involves the task of deciding\nwhether there is some subset S1 of numbers such that \u2211\nl\u2208S1 al = A 2 .\nIn the following, we construct an instance of the expected revenue maximization problem, henceforth referred to as ERMP for brevity, based on a partition instance and present four intermediate lemmas to complete the proof of Theorem 5. We then present the detailed proofs of these lemmas in later subsections.\nTo this end, we begin by constructing an instance of the ERMP problem with with two types, i.e., where |I| = 2, and n locations, where each number al corresponds to a given location. In this setting, for ease of exposition, we drop the fine k from the expected revenue maximization Objective (6a) of the administrator as this is a uniform constant that applies to all locations l and types i. Then, we define the following four quantities: (i) d1l\nd1l +k = 12 al A , (ii) d2l d2l +k = alA , (iii) q 1 l \u039b 1 l = A\n( 2 maxl\u2032\u2208[n] al\u2032 al \u2212 1 ) , and (iv)\nq2l \u039b 2 l = A\n( 1 + 2maxl\u2032\u2208[n] al\u2032\nal\n) for all l \u2208 [n]. Moreover, we define the resource budget as R = 34 .\nGiven this instance of ERMP, we claim that we have a \u201cYes\u201d instance of partition if and only if there is a feasible allocation of resources to locations with total expected revenue that is at least A2 + 2n\u00d7maxl\u2208[n] al for the above defined ERMP instance.\nTo establish this claim, we first prove its forward direction that if we have a \u201cYes\u201d instance of partition, then there exists a feasible allocation of resources to locations with total expected revenue that is at least A 2 + 2n\u00d7maxl\u2208[n] al for the above defined ERMP instance.\nLemma 5 (Forward Direction of NP-Hardness of ERMP). Consider a partition instance with a sequence of numbers a1, . . . , an with \u2211 l\u2208[n] al = A, such that there exists a subset S1 of numbers such that \u2211 l\u2208S1 al = A 2 . Then, there exists a feasible allocation of resources to locations with total expected revenue that is at least A 2 + 2n\u00d7maxl\u2208[n] al for the above defined ERMP instance.\nTo prove this claim, we present a method to construct a feasible administrator strategy \u03c3 for the above defined ERMP instance and show that the expected revenue of this administrator strategy satisfies QR(\u03c3) \u2265 A 2 + 2n\u00d7maxl\u2208[n] al. For a proof of Lemma 5, see Appendix B.6.1.\nNext, to prove the reverse direction of this claim, we suppose that the optimal resource allocation strategy corresponding to the above defined ERMP instance is such that the expected revenue QR(\u03c3) \u2265 A2 + 2n \u00d7 maxl\u2208[n] al and the total resources \u2211 l\u2208[n] \u03c3l \u2264 3 4 = R. To prove that such a setting corresponds to a \u201cYes\u201d\ninstance of partition, we use three intermediate lemmas. Our first two lemmas establish certain structural properties of the administrator\u2019s expected revenue maximizing solution. In particular, we first establish a lower bound on the resources allocated to each location under the expected revenue maximizing strategy of the administrator, as is elucidated through the following lemma.\nLemma 6 (Lower Bound on Administrator Allocation Strategy). Suppose that there is some feasible administrator strategy \u03c3 that satisfies QR(\u03c3) \u2265 A2 +2n\u00d7maxl\u2208[n] al for R = 3 4 . Then, there exists an expected revenue maximizing administrator strategy \u03c3\u0303\u2217 for the above defined ERMP instance such that \u03c3\u0303\u2217l \u2265 d1l\nd1l +k for\nall locations l.\nFor a proof of Lemma 6, see Appendix B.6.2. Given the result obtained in Lemma 6, for the remainder\nof this proof, we focus on expected-revenue maximizing administrator strategies \u03c3 that satisfy \u03c3l \u2265 d 1 l\nd1l +k for\nall locations l for our above defined ERMP instance. Next, following a similar analysis in the proof of Proposition 1, we can establish the following property on the optimal solution of Problem (6a)-(6b).\nLemma 7. Suppose that users at each location best-respond based on Equation (7). Then, there exists an expected revenue maximizing strategy \u03c3\u0303\u2217 of the administrator corresponding to the solution of Problem (6a)(6b) such that:\n\u2022 There exists a set of locations L1 such that \u03c3\u0303\u2217l = d il l\nd il l +k for all l \u2208 L1 for some type il \u2208 I that can be location specific, a set of locations L2 such that \u03c3\u0303 \u2217 l = 0 for all l \u2208 L2, and at most one location l\u2032 such that either \u03c3\u0303\u2217l\u2032 \u2208 (0, d1 l\u2032\nd1 l\u2032+k\n) or \u03c3\u0303\u2217l\u2032 \u2208 ( di\u22121 l\u2032\ndi\u22121 l\u2032 +k\n, di l\u2032\ndi l\u2032+k\n) for some i > 1. Here, L1, L2, and l \u2032 are all disjoint\nand L1 \u222a L2 \u222a {l\u2032} = L.\nSince the analysis of the above result follows similar arguments to that in the proof of Proposition1, we omit it for brevity.\nFinally, we use the structure of the optimal (expected revenue maximizing) solution of the above defined ERMP instance established in Lemmas 6 and 7 to show that our ERMP instance with QR(\u03c3) \u2265 A2 + 2n\u00d7 maxl\u2208[n] al corresponds to a \u201cYes\u201d instance of partition.\nLemma 8 (Reverse Direction of NP-Hardness of ERMP). Suppose that there is some feasible administrator strategy \u03c3 that satisfies QR(\u03c3) \u2265 A2 + 2n \u00d7 maxl\u2208[n] al for R = 3 4 . Then, for the above defined ERMP\ninstance, there exists a subset S1 of numbers such that \u2211\nl\u2208S1 al = A 2 , i.e., we have a \u201cYes\u201d instance of\npartition.\nFor a proof of Lemma 8, see Appendix B.6.3. Note that Lemmas 5-8 prove Theorem 5."
        },
        {
            "heading": "B.6.1 Proof of Lemma 5",
            "text": "We first suppose that we have a \u201cYes\u201d instance of partition, i.e., there exists a set of S numbers such that\u2211 l\u2208S al = A 2 . Then, consider an administrator strategy \u03c3 such that \u03c3l = al A for all l \u2208 S and \u03c3l = 1 2 al A for all l \u2208 [n]\\S. We first note that \u03c3 is feasible, i.e., \u03c3 \u2208 \u2126R, as \u03c3l \u2208 [0, 1] by construction and the resource constraint is satisfied as: \u2211\nl\u2208[n] \u03c3l = \u2211 l\u2208S \u03c3l + \u2211 l\u2208S\\[n] \u03c3l = \u2211 l\u2208S al A + \u2211 l\u2208[n]\\S 1 2 al A ,\n(a) =\n1\nA\nA 2 + 1 2A A 2 = 3 4 = R,\nwhere (a) follows as \u2211\nl\u2208S al = A 2 and \u2211 l\u2208[n]\\S al = A 2 as we have a \u201cYes\u201d instance of partition.\nNext, we obtain the following expected revenue objective for the administrator when it plays the above defined strategy \u03c3:\nQR(\u03c3) = \u2211 i\u2208I \u2211 l\u2208L \u03c3ly i l(\u03c3)q i l\u039b i l (a) = \u2211 l\u2208S q2l \u039b 2 l d2l d2l + k + \u2211 l\u2208[n]\\S (q1l \u039b 1 l + q 2 l \u039b 2 l ) d1l d1l + k ,\n(b) = \u2211 l\u2208S A ( 1+ 2maxl\u2032\u2208[n] al\u2032 al ) al A + \u2211 l\u2208S\\[n] ( A ( 2 maxl\u2032\u2208[n] al\u2032 al \u22121 ) +A ( 1+ 2maxl\u2032\u2208[n] al\u2032 al )) 1 2 al A ,\n= \u2211 l\u2208S ( al + 2 max l\u2032\u2208[n] al\u2032 ) + \u2211 l\u2208S\\[n] 2 max l\u2032\u2208[n] al\u2032 ,\n= A\n2 + 2 \u2211 l\u2208[n] max l\u2032\u2208[n] al\u2032 ,\n= A\n2 + 2nmax l\u2032\u2208[n] al\u2032 ,\nwhere we have dropped the fine k from the objective for ease of exposition, (a) follows by the definition of yil(\u03c3) as given in Equation (7), and (b) follows by plugging in the values of the respective quantities in the above defined instance. The above analysis implies that \u03c3 is both feasible and achieves a total expected revenue of A2 + 2n\u00d7maxl\u2208[n] al, which establishes the forward direction of our claim."
        },
        {
            "heading": "B.6.2 Proof of Lemma 6",
            "text": "To prove this claim, we first show that in the setting when R = 34 any optimal administrator strategy \u03c3\u0303 \u2217 satisfies \u03c3\u0303\u2217l \u2265 d1l\nd1l +k for all locations l. To this end, suppose for contradiction that there is an optimal solution\n\u03c3\u2217 to the ERMP instance such that there is some set of locations S\u2032 for which \u03c3\u2217l\u2032 < d1 l\u2032\nd1 l\u2032+k\nfor all l\u2032 \u2208 S\u2032. Furthermore, define \u03b3 = \u2211\nl\u2208L min{\u03c3\u2217l , d1l\nd1l +k } to represent the total spending across locations to cover the\nfirst type i = 1. In this case we note that \u03b3 < 12 , as\n\u03b3 = \u2211 l\u2208L min{\u03c3\u2217l , d1l d1l + k } (a) < \u2211 l\u2208L d1l d1l + k (b) = \u2211 l\u2208L 1 2 al A (c) = 1 2 ,\nwhere (a) follows as there is some set of location S\u2032 for which \u03c3\u2217l\u2032 < d1 l\u2032\nd1 l\u2032+k\nfor all l\u2032 \u2208 S\u2032 by our contradiction\nassumption, (b) follows by substituting d1l\nd1l +k = 12 al A for our defined ERMP instance, and (c) follows as\u2211\nl\u2208[n] al = A. We now define an allocation strategy \u03c3\u0303 with a strictly higher expected revenue than \u03c3\u2217 to derive our\ndesired contradiction. To do so, we first define \u03b4 = \u2211\nl\u2208L max{0, \u03c3\u2217l \u2212 d1l\nd1l +k } as the cumulative resources\nallocated to locations beyond d1l\nd1l +k . In this case, we note that \u03b4 \u2265 12 \u2212\u03b3, as if this were not the case then \u03c3\u0303\n\u2217\nwould not be optimal (as the amount of resources 12 \u2212 \u03b3 could be spent on locations in the set S \u2032 for which \u03c3\u2217l\u2032 < d1 l\u2032\nd1 l\u2032+k\nfor all l\u2032 \u2208 S\u2032 to obtain a higher expected revenue for the administrator). Then, we construct\n\u03c3\u0303 by transferring 12 \u2212 \u03b3 of mass from locations l \u2208 [n]\\S \u2032 with \u03c3\u2217l > d1l d1l +k to locations in the set S\u2032 with \u03c3\u2217l < d1l\nd1l +k , such that \u03c3\u0303l \u2265 d\n1 l\nd1l +k for all locations l and the total resource spending\n\u2211 l\u2208L \u03c3\u0303l = \u2211 l\u2208L \u03c3 \u2217 l .\nWe now show that such a defined strategy \u03c3\u0303 achieves a strictly higher expected revenue than \u03c3\u2217. To\nsee this, first note that the maximum additional gain in the expected revenue from spending \u03c3\u2032l = d2l\nd2l +k as\ncompared to \u03c3\u2032l = d1l\nd1l +k is given by q2l \u039b 2 l d2l d2l +k \u2212(q1l \u039b1l +q2l \u039b2l ) d1l d1l +k = al for all locations l (see left of Figure 2).\nFurthermore, the spending required to gain this amount al is given by d2l d2l +k \u2212 d 1 l d1l +k = 12 al A . In other words, for the additional spending of 12 \u2212 \u03b3 on locations in the set [n]\\S \u2032, the maximum gain in the administrator\u2019s\nexpected revenue is given by (12 \u2212\u03b3)maxl\u2208[n] al, regardless of which location(s) this additional amount 1 2 \u2212\u03b3 is spent on. However, spending the same amount on locations in the set S\u2032 with \u03c3\u2217l\u2032 < d1 l\u2032\nd1 l\u2032+k for all l\u2032 \u2208 S\u2032, will result in an increase in the administrator revenue by\u2211\nl\u2208S\u2032 (q1l \u039b 1 l + q 2 l \u039b 2 l )( d1l d1l + k \u2212 \u03c3\u2217l ) (a) = \u2211 l\u2208S\u2032 4 A al max l\u2208[n] al( d1l d1l + k \u2212 \u03c3\u2217l ) (b) \u2265 4max l\u2208[n] al \u2211 l\u2208S\u2032 ( d1l d1l + k \u2212 \u03c3\u2217l ),\n(c) = 4max\nl\u2208[n] al(\n1 2 \u2212 \u03b3) > (1 2 \u2212 \u03b3)max l\u2208[n] al,\nwhere (a) follows by substituting the relations for q1l \u039b 1 l and q 2 l \u039b 2 l for our above defined ERMP instance, (b) follows A \u2265 al for all locations l as \u2211 l al = A, and (c) follows as \u2211 l\u2208S\u2032( d1l\nd1l +k \u2212\u03c3\u2217l ) = 12 \u2212\u03b3 by our definition\nof \u03b3. Thus, we have obtained that the increase in the administrator\u2019s expected revenue corresponding to shifting 12 \u2212 \u03b3 of resources from locations l \u2208 [n]\\S\n\u2032 to locations l \u2208 S\u2032 (as in our allocation \u03c3\u0303) outweighs the maximum loss in the administrator objective from shifting these resources from \u03c3\u2217 to \u03c3\u0303, given by ( 12 \u2212 \u03b3)maxl\u2208[n] al. As a result, this relation implies that \u03c3 \u2217 is not optimal, a contradiction. Consequently, any optimal solution to the above defined ERMP instance must be such that \u03c3\u0303\u2217l \u2265 d1l\nd1l +k for all locations l,\nwhich establishes our claim."
        },
        {
            "heading": "B.6.3 Proof of Lemma 8",
            "text": "We now use the structure of the optimal solution established in Lemmas 6 and 7 to show that our ERMP instance with QR(\u03c3) \u2265 A2 + 2n \u00d7 maxl\u2208[n] al corresponds to a \u201cYes\u201d instance of partition. To do so, we first note that the total expected revenue when spending exactly\nd1l d1l +k at each location l is given by:\u2211 l\u2208[n](q 1 l \u039b 1 l + q 2 l \u039b 2 l ) d1l d1l +k = 2nmaxl\u2032\u2208[n] al\u2032 . Note that the total such spending on locations is \u2211 l\u2208[n] d1l d1l +k = 1 2 ; hence, the total additional spending beyond d1l d1l +k across all locations l must not be more than 14 as the\ntotal available resources R = 34 . In particular, we have that \u2211 l\u2208[n] xl \u2264 1 4 , where \u03c3l = dl dl+k\n+ xl for all locations l.\nNext, from Lemma 7, we have that there exists a solution to the ERMP problem such that there is\nat most one location l\u0303 for which xl\u0303 \u2208 (0, d2 l\u0303\nd2 l\u0303 +k\n\u2212 d 1 l\u0303\nd1 l\u0303 +k\n) = (0, 12 al\u0303 A ). Such a result holds as from Lemma 6\nwe have that \u03c3l \u2265 d 1 l\nd1l +k for all locations l and that from Lemma 7, there exists at most one location\nl\u0303 for which \u03c3l \u2208 ( d1 l\u0303\nd1 l\u0303 +k\n, d2 l\u0303\nd2 l\u0303 +k\n). As a consequence, the spending of the administrator beyond d1l\nd1l +k must satisfy \u2211\nl\u2208S\u2032\u2032 1 2 al A + xl\u0303 \u2264 1 4 for some location set S \u2032\u2032 and some location l\u0303, and the expected revenue of the administrator must satisfy:\nQR(\u03c3) = \u2211 l\u2208S\u2032\u2032 q2l \u039b 2 l d2l d2l + k + q2 l\u0303 \u039b2 l\u0303 ( xl\u0303 + d1 l\u0303 d1 l\u0303 + k ) + \u2211 l\u2208[n]\\(S\u2032\u2032\u222a{l\u0303}) (q1l \u039b 1 l + q 2 l \u039b 2 l ) d1l d1l + k ,\n(a) = \u2211 l\u2208S\u2032\u2032 q2l \u039b 2 l d2l d2l + k \u2212 \u2211 l\u2208S\u2032\u2032 (q1l \u039b 1 l + q 2 l \u039b 2 l ) d1l d1l + k + q2 l\u0303 \u039b2 l\u0303 ( xl\u0303 + d1 l\u0303 d1 l\u0303 + k ) \u2212 d1 l\u0303\nd1 l\u0303 + k\n(q1 l\u0303 \u039b1 l\u0303 + q2 l\u0303 \u039b2 l\u0303 ) + \u2211 l\u2208[n] (q1l \u039b 1 l + q 2 l \u039b 2 l ) d1l d1l + k ,\n(b) = \u2211 l\u2208S\u2032\u2032 al + ( xl\u0303 + d1 l\u0303 d1 l\u0303 + k ) q2 l\u0303 \u039b2 l\u0303 \u2212 d1 l\u0303 d1 l\u0303 + k (q1 l\u0303 \u039b1 l\u0303 + q2 l\u0303 \u039b2 l\u0303 ) + 2nmax l\u2032\u2208[n] al\u2032 ,\nwhere (a) follows from adding and subtracting \u2211\nl\u2208S\u2032\u2032\u222al\u0303(q 1 l \u039b 1 l + q 2 l \u039b 2 l ) d1l d1l +k , and (b) follows by substituting\nthe relations for q1l \u039b 1 l , q 2 l \u039b 2 l , and dil dil+k for our above defined ERMP instance.\nThen, since QR(\u03c3) \u2265 A2 +2nmaxl\u2032\u2208[n] al\u2032 , the above equalities implies that, we have the following relation is satisfied:\n\u2211 l\u2208S\u2032\u2032 al + ( xl\u0303 + d1 l\u0303 d1 l\u0303 + k ) q2 l\u0303 \u039b2 l\u0303 \u2212 d1 l\u0303 d1 l\u0303 + k (q1 l\u0303 \u039b1 l\u0303 + q2 l\u0303 \u039b2 l\u0303 ) \u2265 A 2 . (22)\nWe now show that we have a \u201cYes\u201d instance of the partition problem by showing that l\u0303 = \u2205. To show this, we proceed by contradiction and suppose that there is some location l\u0303 such that xl\u0303 \u2208\n(0, 12 al A ). In this setting, we first note that the resource constraint 1 4 \u2265 \u2211 l\u2208S\u2032\u2032\u222a{l\u0303} xl = \u2211 l\u2208S\u2032\u2032 1 2 al A + xl\u0303 can be rewritten as: \u2211 l\u2208S\u2032\u2032 1 2 al A + xl\u0303 \u2264 1 4 ,\n=\u21d2 \u2211 l\u2208S\u2032\u2032 al + 2Axl\u0303 \u2264 A 2 (23)\nFurthermore, from Equation (22), we have that:\nA 2 \u2264 \u2211 l\u2208S\u2032\u2032 al + ( xl\u0303 + d1 l\u0303 d1 l\u0303 + k ) q2 l\u0303 \u039b2 l\u0303 \u2212 d1 l\u0303 d1 l\u0303 + k (q1 l\u0303 \u039b1 l\u0303 + q2 l\u0303 \u039b2 l\u0303 ),\n< \u2211 l\u2208S\u2032\u2032 al + xl\u0303 d2 l\u0303\nd2 l\u0303 +k\n\u2212 d 1 l\u0303\nd1 l\u0303 +k\nal\u0303, (24)\nwhere the second inequality follows as the total gain from spending d2 l\u0303\nd2 l\u0303 +k\ncompared to d1 l\u0303\nd1 l\u0303 +k\nis given by\nd2 l\u0303\nd2 l\u0303 +k\nq2 l\u0303 \u039b2 l\u0303 \u2212 d\n1 l\u0303\nd1 l\u0303 +k\n(q1 l\u0303 \u039b1 l\u0303 + q2 l\u0303 \u039b2 l\u0303 ) = al\u0303 and the fact that xl\u0303 d2 l\u0303\nd2 l\u0303 +k\n\u2212 d1 l\u0303\nd1 l\u0303 +k\nal\u0303 represents a strict upper bound on the\nexpected revenue that the administrator can gain for any xl\u0303 \u2208 (0, d2 l\u0303\nd2 l\u0303 +k\n\u2212 d 1 l\u0303\nd1 l\u0303 +k\n). For more intuition on the\nderivation of the second inequality, we refer to Figure 4, which provides a geometric intuition for this claim through a depiction of the social revenue function (in blue) and its corresponding upper bound (in orange).\nFigure 4: Depiction of the expected revenue of the above defined ERMP instance with the amount of resources spent\nat location l\u0303 in the range\n[\n0, d2 l\u0303\nd2 l\u0303 +k\n]\n. The curve in blue represents the expected revenue as a function of the number of\nresources allocated to location l\u0303, while the dashed line in orange represents the upper bound on the expected revenue function. Moreover, the two points marked in yellow represent the expected revenue (bottom yellow point) and its corresponding upper bound (top yellow point) corresponding to a resource allocation of d1 l\u0303\nd1 l\u0303 +k\n+ xl\u0303 at location l\u0303.\nFrom Equations (23) and (24), we obtain that:\n\u2211 l\u2208S\u2032\u2032 al + xl\u0303\nd2l d2l +k \u2212 d 1 l d1l +k\nal\u0303 (a) = \u2211 l\u2208S\u2032\u2032 al + xl\u0303 1 2 al\u0303 A al\u0303 = \u2211 l\u2208S\u2032\u2032 al + 2Axl\u0303 (b) \u2264 A 2 (c) < \u2211 l\u2208S\u2032\u2032 al + xl\u0303 d2 l\u0303\nd2 l\u0303 +k\n\u2212 d 1 l\u0303\nd1 l\u0303 +k\nal\u0303,\nfor any xl\u0303 \u2208 (0, 1 2 al A ), a contradiction. Here, (a) follows by substituting for the values of d2l d2l +k and d1l d1l +k for our above defined ERMP instance, (b) follows by Equation (23), and (c) follows by Equation (24). Thus, we have that l\u0303 \u0338= \u2205 is not possible.\nFinally, since l\u0303 = \u2205, we have that Equation (22) reduces to:\u2211 l\u2208S\u2032\u2032 al \u2265 A 2 . (25)\nMoreover, we have by the resource constraint that for these locations S\u2032\u2032 that 14 \u2265 \u2211 l\u2208S\u2032\u2032 xl = d2 l\u0303\nd2 l\u0303 +k\n\u2212 d 1 l\u0303\nd1 l\u0303 +k =\u2211 l\u2208S\u2032\u2032\n1 2 al A . This inequality for the resource constraint implies the following relation:\u2211\nl\u2208S\u2032\u2032 al \u2264\nA 2 . (26)\nTogether, Equations (25) and (26) imply that there exists a set S\u2032\u2032 such that \u2211\nl\u2208S\u2032\u2032 al = A 2 , i.e., we have a\n\u201cYes\u201d instance of partition. This establishes our claim."
        },
        {
            "heading": "B.7 Proof of Theorem 6",
            "text": "For ease of exposition, we suppose that the MCUA of expected revenue function is identical to the upper bound of the expected revenue function, as depicted in Figure 2, i.e., the MCUA corresponding to each location l has as many segments as the number of types |I|. We note here that our proof readily generalizes to the setting when the MCUA and the upper bound of the expected revenue function are not identical with a more cumbersome linear program to optimize the MCUA of the expected revenue function. Moreover, for this proof, for ease of notation, we drop the fine k from the objective of the expected revenue function of the administrator, which is equivalent to normalizing the fine k to one. We now prove our claim in three steps. First, we show that optimizing the MCUA of the expected revenue function is equivalent to solving a linear program. Then, we show that the optimal solution of this linear program corresponds to an analogous version of the greedy algorithm in step one of Algorithm 3. Finally, we use these results to establish our desired approximation ratio.\nLinear Program to Optimize MCUA of Expected Revenue Function: We begin the proof of our claim, by presenting a linear program, which we claim optimizes the MCUA of the expected revenue function and whose optimal solution corresponds to the greedy-like solution akin to that in Step 1 of Algorithm 3. To present this linear program, we first define a variable cil for each location l and type i as follows:\n\u2022 c1l = \u2211 i q i l\u039b i l \u2022 cil = \u2211 j\u2265i q j l\u039b j l \u2212\nd i\u22121 l\nd i\u22121 l +k\ndi l\ndi l +k\n\u2212 d i\u22121 l\nd i\u22121 l +k\n(qi\u22121l \u039b i\u22121 l ) for all i \u2265 2.\nThen, we consider the following linear program:\nmax xil \u2200l \u2208 L, i \u2208 I V (Z) = \u2211 l\u2208L \u2211 i\u2208I zilc i l, (27a)\ns.t. z1l \u2208 [0, d1l\nd1l + k ],\u2200l \u2208 [L], for all l \u2208 L, (27b)\nzil \u2208 [0, dil\ndil + k \u2212 di\u22121l di\u22121l + k\n], for all l \u2208 L, i > 1, (27c)\u2211 l\u2208L \u2211 i\u2208I zil \u2264 R, for all l \u2208 L, (27d)\nwhere Z = (zil )l\u2208L,i\u2208I . We now show that the objective of the MCUA of the expected revenue function for any allocation strategy \u03c3, given by M(\u03c3), corresponds to the objective of the linear Program (27a)-(27d) for an appropriately chosen transformation of \u03c3 to the matrix Z = (zil )l\u2208L,i\u2208I . In particular, for each location l, define \u2211 i\u2208I z i l = \u03c3l, where \u03c3l at each location is allocated in order of increasing types at each location l, i.e., type one is allocated until z1l = min{\u03c3l, d1l\nd1l +k }, following which type two is allocated until z2l = min{\u03c3l \u2212 d1l d1l +k , d2l d2l +k \u2212 d 1 l d1l +k } and\nso on until the amount \u03c3l is exhausted. To achieve the goal mentioned in the previous paragraph and correspondingly show that the linear Program (27a)-(27d) indeed solves for the optimal resource allocation strategy of the MCUA of the expected revenue function, we show that the objective value of the above linear program interpolates the point (0, 0) and the points of discontinuity dil\ndil+k for all i \u2208 I of the expected revenue function for all locations l, e.g.,\nthe optimal objective of the linear program exactly corresponds to the orange curve corresponding to the MCUA for the three type instance in Figure 5.\nTo show this claim, fix a location l and define Ml(\u03c3) as the MCUA objective corresponding to location l and let Vl(Z) = \u2211 i\u2208I c i lz i l be the objective of the linear Program (27a)-(27d) corresponding to location l. Then, we show that Ml(\u03c3) = Vl(Z) for the above defined construction of Z for any allocation \u03c3l where \u03c3l = 0 or \u03c3l = dil\ndil+k . Note that showing that the MCUA of the expected revenue function and the objective of the\nlinear Program (27a)-(27d) coincides at (0, 0) and the points of discontinuity dil\ndil+k for all i \u2208 I implies, by\nthe linearity of the objective of Problem (27a)-(27d), that the objective of the linear Program (27a)-(27d) also interpolates between these points.\nThus, we now show that Ml(\u03c3) = Vl(Z) for the above defined construction of Z for any allocation \u03c3l\nthat satisfies either \u03c3l = 0 or \u03c3l = dil\ndil+k . To do so, we begin by recalling that the MCUA of the expected\nrevenue function coincides with the expected revenue function for any \u03c3l = 0 or \u03c3l = dil\ndil+k for any i \u2265 1.\nWe first consider the case when \u03c3l = 0. In this case, by our construction of Z, we have that z i l = 0 for all locations l and types i. Hence, it holds that \u2211\ni c i lz i l = 0 = \u03c3l \u2211 i \u03c3lq i ly i l(\u03c3)\nNext, we consider the case when \u03c3l = dil\ndil+k for some type i \u2265 1. In this setting, we first note that when\n\u03c3l = d sl l\nd sl l +k\nfor some sl, the expected revenue function takes on the value \u03c3l \u2211\ni\u2265sl q i l\u039b i l by the best-response function yil(\u03c3) in Equation (7). We now prove by induction on the value sl that \u2211 i x i lc i l = \u03c3l \u2211 i\u2265sl q i l\u039b i l for any location l. To see this, first consider the base case when sl = 1. In this case, it holds that:\n\u2211 i zilc i l (a) = z1l c 1 l = d1l d1l + k (\u2211 i qil\u039b i l ) (b) = d1l d1l + k \u2211 i\u2265sl qil\u039b i l  (c)= \u03c3\u2217l \u2211 i\u2265sl qil\u039b i l,\nwhere (a) follows as sl = 1 and thus z i l = 0 for all i \u2265 2, (b) follows as sl = 1 and thus \u2211 i q i l\u039b i l = \u2211 i\u2265sl q i l\u039b i l, and (c) follows as \u03c3\u2217l = d1l\nd1l +k for location l as sl = 1.\nNext, by induction, suppose that the above relation holds for sl = j. We now show that it also holds for sl = j + 1. In particular, we have for the setting when sl = j + 1 that:\n\u2211 i zilc i l (a) = j+1\u2211 i=1 zilc i l = z j+1 l c j+1 l + j\u2211 i=1 zilc i l,\n(b) =\n( dj+1l\ndj+1l + k \u2212 djl djl + k ) \u2211 i\u2265j+1 qil\u039b i l \u2212\ndjl djl+k\ndj+1l dj+1l +k \u2212 d j l djl+k\n(qjl\u039b j l ) + djl djl + k \u2211 i\u2265j qil\u039b i l  , =\ndj+1l dj+1l + k \u2211 i\u2265j+1 qil\u039b i l \u2212 djl djl + k \u2211 i\u2265j+1 qil\u039b i l \u2212 djl djl + k qjl\u039b j l + djl djl + k \u2211 i\u2265j qil\u039b i l,\n= dj+1l\ndj+1l + k \u2211 i\u2265j+1 qil\u039b i l \u2212 djl djl + k \u2211 i\u2265j qil\u039b i l + djl djl + k \u2211 i\u2265j qil\u039b i l,\n= dj+1l\ndj+1l + k \u2211 i\u2265j+1 qil\u039b i l (c) = \u03c3\u2217l \u2211 i\u2265j+1 qil\u039b i l, (28)\nwhere (a) follows as sl = j+1 and thus z i l = 0 for all i \u2265 j+2, (b) follows by our inductive assumption and substituting for the respective values of zj+1l and c j+1 l , and (c) follows as \u03c3 \u2217 l =\ndj+1l dj+1l +k as sl = j + 1. Thus, we have shown for all locations i \u2265 1 that \u2211\ni z i lc i l = \u03c3l \u2211 i\u2265j+1 q i l\u039b i l. This establishes that\nMl(\u03c3) = Vl(Z) when \u03c3l satisfies either \u03c3l = 0 or \u03c3l = dil\ndil+k for some i \u2265 1. Consequently, by the linearity\nof the objective of the linear Program (27a)-(27d), we have that its objective interpolates between the point (0, 0) and the points of discontinuity at \u03c3l = dil\ndil+k .\nFurthermore, we note that the above defined cil values correspond to the slopes of the segments of the piece-wise linear MCUA of the expected revenue function that connects the point (0, 0) with the points of discontinuity of the expected revenue function, as depicted in Figure 5. Also, since the MCUA of the expected revenue function for each location l is a concave function, it holds that the slopes, i.e., cil values, form a decreasing sequence for each location l, i.e., c1l \u2265 c2l \u2265 . . . \u2265 c |I| l . Consequently, given our above analysis, we have that solving the linear Program (27a)-(27d) coupled with our earlier defined method to transform an solution Z\u2217 to an allocation \u03c3\u2217 is equivalent to optimizing the MCUA of the expected revenue function, where we have already established earlier that the objective of the linear Program (27a)-(27d) interpolates between the point (0, 0) and the points of discontinuity at \u03c3l = dil\ndil+k .\nGreedy Algorithm to Optimize Linear Program: Next, to show that a greedy procedure optimizes the linear program that maximizes the MCUA of the expected revenue function, we recall that the above defined cil values correspond to the slopes of the segments of the piece-wise linear MCUA of the expected revenue function that connects the point (0, 0) with the points of discontinuity of the expected revenue function, as depicted in Figure 5. Then, as the cil values form a decreasing sequence for each location l (by the concavity of the expected revenue function), we can use the greedy procedure presented in Algorithm 4, where the segments of the MCUA of the expected revenue function are ordered in descending order of their cil values, to find an optimal solution to the linear Program (27a)-(27d). Notice that Algorithm 4 is analogous to the greedy allocation rule in step one of Algorithm 3 other than that the resources are allocated until either all the resources are exhausted or there are no further segments remaining to iterate over. We also note that following a similar line of reasoning to that in the proof of Theorem 1, the solution computed using Algorithm 3 can be shown to correspond to a solution of the linear Program (27a)-(27d). Thus, since optimizing the MCUA of the expected revenue function can be done by solving the linear Program (27a)(27d) and setting \u03c3l = \u2211 i z i l , where (z i l )l\u2208L,i\u2208I is a solution to the linear program, we have that greedy procedure in Algorithm 4 also solves for the optimal solution that maximizes the MCUA of the expected revenue function.\nAlgorithm 4: Greedy Algorithm for MCUA of Expected Revenue Function\nInput : Total Resource capacity R, Segment set S = {(ls, cs, xs)}s\u2208S of MCUA of expected revenue function Output: Resource Allocation Strategy \u03c3\u0303\u2217 S\u0303 \u2190 Ordered list of segments s across all locations of this MCUA in descending order of slopes cs ; Initialize available resources R\u0303\u2190 R ; Initialize allocation strategy \u03c3\u0303 \u2190 0 ; for segment s \u2208 S\u0303 do\n\u03c3\u0303ls \u2190 \u03c3\u0303ls +min{R\u0303, xs} ; Allocate the minimum of the available resources and xs to location ls ; R\u0303\u2190 R\u0303\u2212min{R\u0303, xs}; Update amount of remaining resources ;\nend\nProving the Approximation Ratio: Finally, we use our above developed results to complete the proof of our claim that QR(\u03c3 \u2217 A) \u2265 12QR(\u03c3 \u2217). To this end, we first introduce some notation. In particular, we define \u03c3\u0303\u2217 as the optimal solution corresponding to directly optimizing the MCUA of the expected revenue function.\nThen, to establish our approximation guarantee, we first recall from our earlier obtained results that maximizing the MCUA of the expected revenue function corresponds to the solution of Algorithm 4. As a result, we have that the solution maximizing the MCUA of the expected revenue function exactly corresponds to the solution in step one of Algorithm 3 other than that resources are allocated until either all the resources are exhausted or there are no further segments remaining to iterate over. Consequently, by the termination condition of step one of Algorithm 4, there is at most one segment s\u2032 and associated location ls\u2032 = l\n\u2032 such that the solutions of step one of Algorithm 3 and that of Algorithm 4 differ. In particular, it holds that \u03c3\u0303\u2217l = \u03c3\u0303l for all l \u0338= l\u2032 and for l = l\u2032 it holds that \u03c3\u0303\u2217l\u2032 \u2265 \u03c3\u0303l\u2032 , where \u03c3\u0303\u2217l\u2032 = \u03c3\u0303l\u2032 + xs\u2032 . For the remainder of this proof, we assume without loss of generality that l\u2032 \u0338= \u2205. Then, we can write the optimal objective of the MCUA of the expected revenue function as:\nM(\u03c3\u0303\u2217) = \u2211\nl\u2208L\\{l\u2032} \u03c3\u0303\u2217l \u2211 i\u2265sl qil\u039b i l + \u03c3\u0303 \u2217 l\u2032 \u2211 i>sl\u2032 qil\u2032\u039b i l\u2032 . (29)\nFrom the above form of the optimal objective of the MCUA of the expected revenue function, we now establish three inequalities: (i) QR(\u03c3 \u2217) \u2264 M(\u03c3\u0303\u2217), (ii) QR(\u03c3\u2032) \u2265 \u03c3\u2217l\u2032 \u2211 i>sl\u2032 qil\u2032\u039b i l\u2032 , and (iii) QR(\u03c3\u0303) \u2265\u2211\nl\u2208L\\{l\u2032} \u03c3 \u2217 l \u2211 i\u2265sl q i l\u039b i l."
        },
        {
            "heading": "Inequality (i): QR(\u03c3",
            "text": "\u2217) \u2264 M(\u03c3\u0303\u2217): By construction, the MCUA of the expected revenue function upper bounds the expected revenue for all allocation strategies \u03c3. Hence, it follows that:\nQR(\u03c3 \u2217) \u2264 M(\u03c3\u2217) \u2264 M(\u03c3\u0303\u2217),\nwhere the first inequality follows as the MCUA of the expected revenue function upper bounds the expected revenue at \u03c3\u2217 and the latter inequality follows by the optimality of \u03c3\u0303\u2217 for the MCUA of the expected revenue function.\nInequality (ii): QR(\u03c3 \u2032) \u2265 \u03c3\u2217l\u2032 \u2211 i>sl\u2032 qil\u2032\u039b i l\u2032 : To establish this inequality, we first note by construction of \u03c3\u2032 for R \u2265 1 resources that it corresponds to an allocation that maximizes the expected revenue from spending on a single location. In particular, we have that:\nQR(\u03c3 \u2032) \u2265\nd sl\u2032+1 l\u2032\nd sl\u2032+1 l\u2032 + k \u2211 i>sl\u2032 qil\u2032\u039b i l\u2032 \u2265 \u03c3\u2217l\u2032 \u2211 i>sl\u2032 qil\u2032\u039b i l\u2032 ,\nwhere recall that \u03c3\u2217l\u2032 \u2208 ( d s l\u2032\u22121 l\u2032\nd s l\u2032\u22121 l\u2032 +k ,\nd s l\u2032\nl\u2032 d s l\u2032\nl\u2032 +k ) by the definition of sl\u2032 . Here, the first inequality follows as \u03c3\n\u2032 by\ndefinition is the solution with the highest expected revenue among all allocation corresponding to a single location and the second inequality follows as d s l\u2032+1 l\u2032\nd s l\u2032+1 l\u2032 +k > \u03c3\u2217l\u2032 . Thus, we have established our desired inequality\nthat QR(\u03c3 \u2032) \u2265 \u03c3\u2217l\u2032 \u2211 i>sl\u2032 qil\u2032\u039b i l\u2032 . Inequality (iii): QR(\u03c3\u0303) \u2265 \u2211 l\u2208L\\{l\u2032} \u03c3 \u2217 l \u2211 i\u2265sl q i l\u039b i l: We recall from earlier that the solution maximizing the MCUA of the expected revenue function exactly corresponds to the solution in step one of Algorithm 3 other than that resources are allocated until either all the resources are exhausted or there are no further segments remaining to iterate over. Consequently, it holds that \u03c3\u0303\u2217l = \u03c3\u0303l for all l \u0338= l\u2032 and for l = l\u2032 it holds that \u03c3\u0303\u2217l\u2032 \u2265 \u03c3\u0303l\u2032 , where \u03c3\u0303\u2217l\u2032 = \u03c3\u0303l\u2032 + xs\u2032 . Thus, it follows that QR(\u03c3\u0303) = \u2211 l\u2208L\\{l\u2032} \u03c3 \u2217 l \u2211 i\u2265sl q i l\u039b i l, which implies our desired inequality. Together, the inequalities (i), (ii), and (iii) imply that\nQR(\u03c3\u0303) +QR(\u03c3 \u2032) \u2265 \u2211 l\u2208L\\{l\u2032} \u03c3\u0303\u2217l \u2211 i\u2265sl qil\u039b i l + \u03c3\u0303 \u2217 l\u2032 \u2211 i>sl\u2032 qil\u2032\u039b i l\u2032 = M(\u03c3\u0303 \u2217) \u2265 QR(\u03c3\u2217), (30)\nwhere the equality follows from Equation (29). Finally, since \u03c3\u2217A = argmax{QR(\u03c3\u0303), QR(\u03c3\u2032)} from step 3 of Algorithm 3, it follows from the above inequality that\n2QR(\u03c3 \u2217 A) \u2265 QR(\u03c3\u0303) +QR(\u03c3\u2032)\n(a) \u2265 QR(\u03c3\u2217),\nwhere inequality (a) follows from Equation (30). Thus, we have proved the desired half-approximation that QR(\u03c3 \u2217 A) \u2265 12QR(\u03c3 \u2217), which establishes our result."
        },
        {
            "heading": "B.8 Proof of Theorem 7",
            "text": "To prove this claim, we first denote the objective of the MCUA of the expected revenue function summed across all locations given by M(\u03c3) for an administrator\u2019s resource allocation strategy \u03c3. Then, we recall that, by construction, the developed MCUA is an upper bound on the expected revenue function for all feasible administrator strategies, i.e., M(\u03c3) \u2265 QR(\u03c3) for all \u03c3 \u2208 \u2126R (also see proof of Theorem 6). Next, to establish our resource augmentation guarantee with R + 1 resources, we define \u03c3\u0303R+1 as the allocation corresponding to Step 1 of Algorithm 3 with R + 1 resources and \u03c3\u0303R+1A as the allocation corresponding to Algorithm 3 with R + 1 resources. Moreover, we let \u03c3\u0303\u2217 be the optimal solution corresponding to directly optimizing the MCUA of the expected revenue function. We now show that QR(\u03c3\n\u2217) \u2264 QR+1(\u03c3\u0303R+1A ) to establish our claim.\nTo show thatQR(\u03c3 \u2217) \u2264 QR+1(\u03c3\u0303R+1A ), we establish four key relations: (i)QR(\u03c3\u2217) \u2264 M(\u03c3\u0303\u2217), (ii)M(\u03c3\u0303\u2217) \u2264\nM(\u03c3\u0303R+1), (iii) M(\u03c3\u0303R+1) = QR+1(\u03c3\u0303 R+1), and (iv) QR+1(\u03c3\u0303 R+1) \u2264 QR+1(\u03c3\u0303R+1A )."
        },
        {
            "heading": "Relation (i): QR(\u03c3",
            "text": "\u2217) \u2264 M(\u03c3\u0303\u2217): By construction, the MCUA of the expected revenue function upper bounds the expected revenue for all allocation strategies \u03c3. Hence, it follows that:\nQR(\u03c3 \u2217) \u2264 M(\u03c3\u2217) \u2264 M(\u03c3\u0303\u2217),\nwhere the first inequality follows as the MCUA of the expected revenue function upper bounds the expected revenue at \u03c3\u2217 and the latter inequality follows by the optimality of \u03c3\u0303\u2217 for the MCUA of the expected revenue function.\nRelation (ii): M(\u03c3\u0303\u2217) \u2264 M(\u03c3\u0303R+1): We recall from the proof of Theorem 6 that the allocation maximizing the MCUA of the expected revenue function corresponds to the solution of Algorithm 4. In other words, the allocation maximizing the MCUA of the expected revenue function corresponds to the solution in the step 1 of Algorithm 3, where resources are allocated until either all the resources are exhausted or there are no further segments are remaining to iterate over. Consequently, by the termination condition of step one of Algorithm 4, there is at most one segment s\u2032 and associated location ls\u2032 such that the solutions of step one of Algorithm 3 and that of Algorithm 4 differ. In particular, it holds that \u03c3\u0303\u2217l = \u03c3\u0303l for all l \u0338= ls\u2032 and for l = ls\u2032 it holds that \u03c3\u0303\u2217ls\u2032 \u2265 \u03c3\u0303ls\u2032 , where \u03c3\u0303 \u2217 ls\u2032 \u2264 \u03c3\u0303ls\u2032 + xs\u2032 . Without loss of generality, we assume that \u03c3\u0303 \u2217 ls\u2032\n> \u03c3\u0303ls\u2032 , as if this were not the case, then our desired inequality trivially holds.\nNext, with R + 1 resources, it must hold that the allocation \u03c3\u0303R+1 corresponding to the solution of Algorithm 3 satisfies \u03c3\u0303R+1l \u2265 \u03c3\u0303l = \u03c3\u0303\u2217l for all l \u0338= ls\u2032 , as the greedy allocation with R + 1 resources will at least allocate the same amount of resources to all locations as that with R resources. Furthermore, since xs\u2032 \u2264 1 for all segments s, it also holds for l = ls\u2032 that the greedy allocation will completely allocate xs\u2032 to location ls\u2032 ; hence, it holds that \u03c3\u0303\nR+1 ls\u2032 \u2265 \u03c3\u0303ls\u2032 + xs\u2032 \u2265 \u03c3\u0303 \u2217 ls\u2032 .\nThus, we have established for all locations l that \u03c3\u0303R+1l \u2265 \u03c3\u0303\u2217l . Finally, our desired inequality that M(\u03c3\u0303\u2217) \u2264 M(\u03c3\u0303R+1) follows by the monotonicity of the MCUA of the expected revenue function in the resource spending at each location.\nRelation (iii): M(\u03c3\u0303R+1) = QR+1(\u03c3\u0303 R+1): We note that the allocation corresponding to the greedy procedure in Step 1 of Algorithm 3 corresponds to a point where both the expected revenue function and its corresponding MCUA coincide. Consequently, it follows that M(\u03c3\u0303R+1) = QR+1(\u03c3\u0303 R+1)."
        },
        {
            "heading": "Relation (iv): QR+1(\u03c3\u0303",
            "text": "R+1) \u2264 QR+1(\u03c3\u0303R+1A ): This result directly follows, by definition, from the fact that \u03c3\u0303R+1A = argmax{QR+1(\u03c3\u0303), QR+1(\u03c3\u2032)}. Finally, combining the above four relations that we have shown, we obtain that:\nQR(\u03c3 \u2217) \u2264 M(\u03c3\u0303\u2217) \u2264 M(\u03c3\u0303R+1) = QR+1(\u03c3\u0303R+1) \u2264 QR+1(\u03c3\u0303R+1A ),\nwhich proves our claim that QR(\u03c3 \u2217) \u2264 QR+1(\u03c3\u0303R+1A ), i.e., Algorithm 3 with R+ 1 resources achieves a total expected revenue that is at least that corresponding to the optimal solution of Problem (6a)-(6b). This establishes our claim."
        },
        {
            "heading": "C Welfare Maximization in Probabilistic Setting",
            "text": "In this section, we extend our results obtained in the probabilistic setting for the revenue maximization objective of the administrator to the welfare maximization objective. To this end, we first introduce the problem of maximizing the expected total welfare in the system (Section C.1), following which we present the associated greedy algorithm for the administrator\u2019s welfare maximization objective and its corresponding approximation ratio and resource augmentation guarantees (Section C.2)."
        },
        {
            "heading": "C.1 Administrator\u2019s Expected Welfare Maximization Problem",
            "text": "We recall that, in the probabilistic setting, we assume that while users know the realization of the type i \u2208 I at each location, the administrator only knows the distribution of each location\u2019s type. Consequently, to elucidate the expected welfare maximization problem of the administrator, as with the welfare maximization\nproblem of the administrator in the deterministic setting (see Section 3.3), we first notice that the expected total social welfare in the system is given by \u2211 l\u2208L \u2211 i\u2208I q i lv i l and the level of welfare not accrued by the\nadministrator under a strategy \u03c3 with R resources is given by W\u2212R (\u03c3) = \u2211 l\u2208L(1\u2212\u03c3l) \u2211 i\u2208I q i lv i ly i l(\u03c3). Then,\nthe total accrued welfare under a strategy \u03c3 with R resources is given byWR(\u03c3) = \u2211 l\u2208L \u2211 i\u2208I q i lv i l\u2212W \u2212 R (\u03c3); hence, the expected welfare maximizing strategy of the administrator is equivalent to the strategy that minimizes W\u2212R (\u03c3). Consequently, the expected welfare maximizing strategy of the administrator can be computed through the solution of the following bi-level program:\nmax \u03c3\u2208\u2126R\nyil (\u03c3)\u2208[0,1],\u2200l\u2208L,i\u2208I\nW\u2212R (\u03c3) = \u2211 l\u2208L (1\u2212 \u03c3l) \u2211 i\u2208I qilv i ly i l(\u03c3), (31a)\ns.t. yil(\u03c3) \u2208 argmax y\u2208[0,1] U il (\u03c3, y) = y[(1\u2212 \u03c3l)dil \u2212 \u03c3lk], for all l \u2208 L, i \u2208 I, (31b)\nwhere in upper-level problem, the administrator employs a strategy \u03c3 that maximizes its expected welfare to which the users at each location of each type best respond by maximizing their utilities in the lower-level problem based on the realized type at that location. As in the deterministic setting, we note that the best response of users is given by the threshold function\nyil(\u03c3) =\n{ 0, if \u03c3l \u2265 d i l\ndil+k ,\n1, otherwise. (32)\nWe reiterate, as with the best response of users in the deterministic setting in Equation (5), that when \u03c3l = dil\ndil+k , any yil(\u03c3) \u2208 [0, 1] is a best-response for users at a location l with type i. Yet, we let yil(\u03c3) = 0 when\n\u03c3l = dil\ndil+k as it corresponds to the highest expected welfare outcome for the administrator (see Section 4.2 for\na further discussion). We note that unlike the best-response function in the expected revenue maximization setting, where yil(\u03c3) = 1 when \u03c3l = dil\ndil+k , in the expected welfare maximization setting, yil(\u03c3) = 0 when\n\u03c3l = dil\ndil+k . For a further discussion on this difference between the best-response function of users under the\nwelfare and revenue maximization administrator objectives, see Section 4.2.1."
        },
        {
            "heading": "C.2 Greedy Algorithm for Expected Welfare Maximization",
            "text": "In studying the expected welfare maximization problem of the administrator, we first note that the problem of computing the administrator\u2019s welfare-maximizing strategy in the probabilistic setting is NP-hard, which follows by our hardness result on solving for the administrator\u2019s welfare-maximizing strategy in the deterministic setting (Theorem 2). Thus, in this section, we introduce a greedy algorithm, described in Algorithm 5, to approximately solve for the administrator\u2019s expected welfare maximizing resource allocation strategy and highlight its corresponding approximation ratio and resource augmentation guarantees.\nWe begin by first describing our algorithmic approach to achieve an approximate solution to the administrator\u2019s expected welfare maximization problem, which proceeds as follows. First, rather than directly optimizing the expected welfare of the administrator, which as noted earlier is NP-hard to optimize, we maximize its corresponding monotone concave upper approximation (MCUA), which we depict in Figure 6. In particular, similar of maximizing the MCUA of the expected revenue function (see Section 5.3), optimizing the MCUA of the expected welfare function can be reduced to solving a linear program whose solution boils down to a greedy-like procedure presented in Step 1 of Algorithm 5 (see Theorem 9).\nTo elucidate this greedy-like procedure, we note that the MCUA of the expected welfare function for each location l is a piece-wise linear function; thus, we define S as the set of all such piece-wise linear segments of the MCUA of the expected welfare function across all locations. We then characterize each segment s \u2208 S by three parameters: (i) ls, which represents the location corresponding to segment s, (ii) cs, which corresponds to the slope of segment s, and (iii) xs, which represents the horizontal width, i.e., resource requirement, of segment s. In particular, Figure 6 (right) depicts the associated slopes cs and resource requirements xs corresponding to the segments for the MCUA of the expected welfare function for a given location l. We then order the segments in the set S in the descending order of the slopes of the MCUA of the expected\nwelfare function and find the solution corresponding to a greedy algorithm that allocates at most zs to each segment in the descending order of the slopes of the MCUA of the expected welfare function.\nNext, as with Algorithm 3, we compute an allocation corresponding to spending all the available resources at a single location that yields the highest expected welfare of the administrator. Finally, we return the best of the greedy allocation corresponding to optimizing the MCUA of the expected welfare function and the allocation corresponding to spending all the available resources at a single location that achieves a higher expected welfare for the administrator. This procedure is formally presented in Algorithm 5.\nAlgorithm 5: Greedy Algorithm for Administrator\u2019s Expected Welfare Maximization Objective\nInput : Total Resource capacity R, Location Types \u0398il = (\u039b i l , d i l , v i l ) for all locations l and types i Output: Resource Allocation Strategy \u03c3\u2217A Step 1: Greedy Allocation Based on Slopes of MCUA of Expected Welfare Function: Generate MCUA of the expected welfare function for each location l ; S\u0303 \u2190 Ordered list of segments s across all locations of this MCUA in descending order of slopes cs ; Initialize allocation strategy \u03c3\u0303 \u2190 0 ; for segment s \u2208 S\u0303 do\n\u03c3\u0303ls \u2190 \u03c3\u0303ls +min{R, xs} ; Allocate xs to location ls ; R\u2190 R\u2212min{R, xs}; Update amount of remaining resources ;\nend Step 2: Find Solution \u03c3\u2032 that Maximizes expected welfare from spending on Single Location: \u03c3l \u2190 argmax\u03c3\u2208\u2126R:\u03c3l\u2032=0,\u2200l\u2032 \u0338=l WR(\u03c3) for all locations l ; \u03c3\u2032 \u2190 argmaxl\u2208L WR(\u03c3l) ; Step 3: Return Solution with a Higher Expected Welfare: \u03c3\u2217A \u2190 argmax{WR(\u03c3\u0303),WR(\u03c3 \u2032)} ;\nWe now present the main results of this section, which establish the approximation guarantees of Algorithm 5 to the optimal expected welfare of the administrator. Our first result establishes that Algorithm 5 achieves at least half the expected welfare as that corresponding to the solution of Problem (31a)-(31b).\nTheorem 9 (1/2 Approximation of Greedy Algorithm for Expected Welfare Maximization). Denote \u03c3\u2217A as the solution corresponding to Algorithm 5 and let \u03c3\u2217 be the expected welfare maximizing allocation that solves Problem (31a)-(31b). Then, \u03c3\u2217A achieves at least half the expected welfare as compared to \u03c3\n\u2217, i.e., WR(\u03c3 \u2217 A) \u2265 12WR(\u03c3 \u2217).\nOur next result establishes that if the administrator had just one additional resource, i.e., R+1 resources, then Algorithm 5 will achieve at least the expected welfare as the welfare maximizing solution of the NP-hard bi-level Program (31a)-(31b) with R resources.\nTheorem 10 (Resource Augmentation Guarantee for Expected Welfare Maximization). Denote \u03c3\u2217A as the solution corresponding to Algorithm 5 with R + 1 resources and let \u03c3\u2217 be the expected welfare-maximizing\nallocation that solves Problem (31a)-(31b) with R resources. Then, the expected welfare under the allocation \u03c3\u2217A with R+ 1 resources is at least that corresponding to \u03c3 \u2217 with R resources, i.e., WR+1(\u03c3 \u2217 A) \u2265 WR(\u03c3\u2217).\nWe note that the administrator only requires R + maxl\u2208L,i\u2208I dil\ndil+k resources to obtain the resource\naugmentation guarantee in Theorem 3; however, we present the result with R + 1 resources for ease of exposition. Theorems 9 and 10 imply that we obtain the same approximation ratio and resource augmentation guarantees for the administrator\u2019s welfare maximization problem in the probabilistic setting as we did in the deterministic welfare maximization setting (see Section 4.2.3). Furthermore, we note that Algorithm 5 in the welfare maximization setting follows a similar idea to Algorithm 3 in the revenue maximization setting in terms of generating a monotone concave upper approximation to reduce the originally NP-hard bi-level optimization into a tractable linear program that can be solved with a greedy like procedure. Consequently, the proofs of Theorems 9 and 10 follow similarly to the corresponding results in the expected revenue maximization; hence, we omit these proofs for brevity.\nYet, we note that while Algorithm 5 in the welfare maximization setting is akin to Algorithm 3 in the revenue maximization setting, there is one key point of difference. In particular, unlike in Algorithm 3, we do not terminate the greedy procedure at the point in the algorithm when there is a segment in the ordered set S\u0303 such that the segment\u2019s resource requirement xs exceeds the available resources. Such a termination of the greedy procedure in step one of Algorithm 5 is not necessary in the welfare maximization setting, as, unlike the expected revenue function, the expected welfare function is monotonically (non)-decreasing in the amount of resources allocated to each location (see Figures 2 and 6). Yet, we note from our analysis in the expected revenue maximization setting that the obtained approximation ratio and resource augmentation guarantees obtained in Theorems 9 and 10 would continue to hold even in the setting when the greedy procedure in step 1 of Algorithm 5 is terminated in a similar manner to Algorithm 3.\nFinally, we note that Theorems 9 and 10 further highlight the benefit to administrators for recruiting one additional security personnel and applying a simple algorithm, i.e., Algorithm 5, rather than investing significant computational power and effort to compute the NP-hard welfare-maximizing strategy of the administrator."
        },
        {
            "heading": "D Equilibrium Strategies of Administrator and Users in Contract",
            "text": "Game\nIn this section, we study the strategies of the administrator and users in our contract game, given a contract parameter \u03b1 \u2208 [0, 1]. To simplify exposition and elucidate the main ideas of this contract game, we focus on the deterministic setting, where each location\u2019s type is known to the administrator, and, following ideas developed in Section 5, note that our proposed framework and results can be naturally generalized to the probabilistic setting as well.\nIn studying the equilibrium strategies of the administrator and users, we first note that the problem of computing equilibria of this contract game is NP-hard, as solving the bi-level Program (8a)-(8b) is NPhard, in general. The proof of this claim follows almost entirely analogously to the proof of Theorem 2. In particular, as in the proof of Theorem 2, we reduce from an instance of the partition problem and consider an instance of the contract game that is akin to the instance in the proof of Theorem 2 and where the number of fraudulent users \u039bl = \u03b4 for all locations l is a small constant. In this setting, Objective (8a) is dominated by the welfare term for a sufficiently large \u03b1 \u2208 [0, 1], e.g., \u03b1 = 1. Thus, on constructing an instance akin to that in the proof of Theorem 2 and where the number of fraudulent users \u039b at each location is a small constant, the remainder of the proof follows from an almost entirely analogous line of reasoning to the proof of Theorem 2; thus, we omit the remaining proof details for brevity.\nGiven the hardness of solving Problem (8a)-(8b), we now present a variant of a greedy algorithm to compute an administrator strategy with an approximation guarantee to the optimal solution of Problem (8a)(8b) for any given contract parameter \u03b1 \u2208 [0, 1]. To this end, we first note that the best-response strategy yl(\u03c3, \u03b1) of users at a given location l, corresponding to the solution of the lower-level Problem (8b), given a\ncontract parameter \u03b1 and an administrator strategy \u03c3, is\nyl(\u03c3, \u03b1) =\n{ 0, if \u03c3l >\ndl dl+k\nor ( \u03c3l =\ndl dl+k and (k\u039bl + \u03b1vl) dl dl+k \u2264 \u03b1vl\n) ,\n1, otherwise. (33)\nNotice that as with our earlier studied best-response function of users in the revenue and welfare maximization settings, when \u03c3l =\ndl dl+k , any yl(\u03c3, \u03b1) \u2208 [0, 1] is a best-response for users at location l. However, at the threshold \u03c3l =\ndl dl+k , we let yl(\u03c3, \u03b1) take on the value zero or one depending on whether the administrator\u2019s\nrevenue corresponding to location l, given by \u03b1vl, when yl(\u03c3, \u03b1) = 0 is smaller than the administrator\u2019s revenue at location l, given by (\u039bl+\u03b1vl)\ndl dl+k , when yl(\u03c3, \u03b1) = 1 or not. Moreover, in the case when \u03c3l = dl dl+k\nand the administrator is indifferent between the outcomes corresponding to yl(\u03c3, \u03b1) = 1 or yl(\u03c3, \u03b1) = 0, i.e., (k\u039bl + \u03b1vl)\ndl dl+k = \u03b1vl, we set yl(\u03c3, \u03b1) = 0, as this maximizes the welfare of the principal. Thus,\nthe best-response function of users in Equation (33) is in alignment with the notion of strong Stackelberg equilibria [48].\nHaving presented the best-response function of users in our studied contract game, we now present Algorithm 6, Contract-Greedy, which extends our greedy-like algorithmic approach for the welfare maximization setting (see Algorithm 2) to compute an administrator strategy in our contract game for any contract \u03b1 \u2208 [0, 1]. We note that Algorithm 6 is entirely analogous to Algorithm 2 in the welfare maximization setting other than in the process of sorting locations, as the administrator in our contract game, maximizes a linear combination of the revenue and welfare objectives.\nAlgorithm 6: Contract-Greedy\nInput : Contract parameter \u03b1, Resource capacity R, Locations types \u0398l = (\u039bl, dl, vl) for all locations l Step 1: Find Greedy Solution \u03c3\u0303: Define zl \u2190 max{\u03b1vl, (k\u039bl + \u03b1vl) dldl+k } for all locations l ; Order locations in descending order of zl(dl+k)\ndl ;\nfor l = 1, 2, ..., |L| do \u03c3\u0303l \u2190 min{R, dldl+k } ; Allocate the minimum of the remaining resources and dl dl+k to location l ;\nR\u2190 R\u2212 \u03c3\u0303l; Update amount of remaining resources ; end Step 2: Find Solution \u03c3\u2032 that Maximizes Revenue from Spending on Single Location: \u03c3l \u2190 argmax\u03c3\u2208\u2126R:\u03c3l\u2032=0,\u2200l\u2032 \u0338=l QR(\u03c3) + \u03b1WR(\u03c3) for all locations l ; \u03c3\u2032 \u2190 argmaxl\u2208L QR(\u03c3l) + \u03b1WR(\u03c3l) ; Step 3: Return Solution with a Higher Administrator Revenue: \u03c3A\u03b1 \u2190 argmax{QR(\u03c3\u0303) + \u03b1WR(\u03c3\u0303), QR(\u03c3\u2032) + \u03b1WR(\u03c3\u2032)} ;\nWe now show that Algorithm 6 achieves at least half the total revenue for the administrator as the optimal solution to the bi-level Program (8a)-(8b) for any contract parameter \u03b1 \u2208 [0, 1].\nTheorem 11 (1/2 Approximation of Greedy Algorithm for Contract Game). For any given contract parameter \u03b1, let \u03c3A\u03b1 be the solution corresponding to Algorithm 6 and let \u03c3 \u2217 \u03b1 be the optimal solution of the bi-level Program (8a)-(8b). Then, \u03c3A\u03b1 achieves at least half the revenue for the administrator as the optimal solution \u03c3\u2217\u03b1, i.e., QR(\u03c3 A \u03b1 ) + \u03b1WR(\u03c3 A \u03b1 ) \u2265 12 (QR(\u03c3 \u2217 \u03b1) + \u03b1WR(\u03c3 \u2217 \u03b1)).\nThe proof of Theorem 11 follows an almost entirely analogous line of reasoning to that of Theorem 3, and thus we omit it for brevity. Moreover, as in the earlier studied revenue and welfare maximization settings, we can also establish an analogous resource augmentation guarantee for Algorithm 6. Overall, our earlier developed algorithmic approaches and associated theoretical guarantees naturally carry forward to computing the administrator strategies in the contract game, as Objective (8a) is a linear combination of our earlier studied revenue and welfare objectives."
        },
        {
            "heading": "E Theoretical Results on Dense Sampling",
            "text": "In this section, we study the near-optimality of our proposed dense sampling procedure presented in Section 6.1. In particular, we prove two key results, which establish that applying the above dense-sampling\nprocedure approximately maximizes the principal\u2019s payoff across all contract parameters \u03b1 \u2208 [0, 1], where the loss in the principal\u2019s payoff depends on the chosen step-size s. Our first result is a restatement of Theorem 8, which establishes the near-optimality of dense sampling in the setting when, given any contract parameter \u03b1, the administrator strategy corresponds to the solution of the bi-level Program (8a)-(8b).\nTheorem 12 (Near-Optimality of Dense Sampling under Optimal Administrator Strategy). Let \u03b1\u2217 \u2208 [0, 1] be the principal\u2019s payoff maximizing contract and \u03b1\u2217s \u2208 As be the contract computed through dense-sampling. Further, given any \u03b1, let \u03c3(\u03b1) be the solution of Problem (8a)-(8b). Then, for a step-size s \u2264 \u03f5\u2211\nl vl ,\nthe loss in the principal\u2019s payoff through dense sampling is bounded by \u03f5, i.e., (1 \u2212 \u03b1\u2217)WR(\u03c3(\u03b1\u2217)) \u2264 (1 \u2212 \u03b1\u2217s)WR(\u03c3(\u03b1 \u2217 s)) + \u03f5.\nOur second result establishes the near-optimality of dense sampling in the setting when, given any contract parameter \u03b1, the administrator strategy is computed using Algorithm 6 under a correlation assumption on the valuations vl and welfare bang-per-buck ratios vl(dl+k)\ndl at each location l.\nTheorem 13 (Near-Optimality of Dense Sampling under Greedy Administrator Strategy). Let \u03b1\u2217 \u2208 [0, 1] denote the optimal contract, \u03b1\u2217s \u2208 As be the contract computed through dense-sampling, and suppose that the valuations vl are positively correlated with the welfare bang-per-buck ratios\nvl(dl+k) dl at each location,\ni.e., if v1 \u2264 . . . \u2264 v|L|, then v1(d1+k)d1 \u2264 v2(d2+k) d2 \u2264 . . . \u2264 v|L|(d|L|+k)d|L| . Further, given R \u2265 1 resources, suppose that, given any contract \u03b1, the administrator strategy \u03c3A\u03b1 is computed using Algorithm 6. Then, for a step-size s \u2264 \u03f5\u2211\nl vl , the loss in the principal\u2019s payoff through dense sampling is bounded by \u03f5, i.e.,\n(1\u2212 \u03b1\u2217)WR(\u03c3A\u03b1\u2217) \u2264 (1\u2212 \u03b1\u2217s)WR(\u03c3A\u03b1\u2217s ) + \u03f5.\nIn the remainder of this section, we first present the key lemmas necessary to prove the above results (Appendix E.1), following which we present the proofs of these lemmas in subsequent sections (Appendices E.2-E.4)."
        },
        {
            "heading": "E.1 Key Lemmas",
            "text": "To establish Theorems 8 and 13, we follow a two-step procedure. In particular, as a first step, we show that if the administrator strategy \u03c3(\u03b1) for any contract parameter \u03b1 is such that the welfare function WR(\u03c3(\u03b1)) is monotonically non-decreasing in \u03b1, then our dense-sampling procedure approximately maximizes the principal\u2019s payoff across all contract parameters \u03b1 \u2208 [0, 1], as is elucidated by the following lemma.\nLemma 9 (Monotonicity of Welfare Function Implies Near-Optimality). Let \u03c3(\u03b1) be an administrator strategy, given a contract parameter \u03b1, such that the welfare function is monotonically non-decreasing in \u03b1, i.e., if \u03b11 < \u03b12, WR(\u03c3(\u03b12)) \u2265 WR(\u03c3(\u03b11)). Furthermore, denote \u03b1\u2217 \u2208 [0, 1] as the optimal contract and let \u03b1\u2217s \u2208 As be the contract computed through dense-sampling. Then, given R \u2265 1 resources, for a step-size s \u2264 \u03f5\u2211\nl vl , the loss in the principal\u2019s payoff is bounded by \u03f5, i.e., (1\u2212 \u03b1\u2217)W (\u03c3A\u03b1\u2217) \u2264 (1\u2212 \u03b1\u2217s)W (\u03c3A\u03b1\u2217s ) + \u03f5.\nFor a proof of Lemma 9, see Appendix E.2. Lemma 9 establishes that if the algorithm or procedure of computing the administrator\u2019s strategy given a contract parameter \u03b1 satisfies a natural monotonicity property, then dense sampling is near optimal for an appropriately chosen step size of the discretized set As. Note that such a monotonicity property is natural as increasing the contract parameter \u03b1 is synonymous with providing the administrator a higher compensation for its contribution to the welfare of the system.\nOur next two results establish that this monotonicity of the welfare function in the parameter \u03b1 is indeed satisfied for the optimal administrator strategy corresponding to the solution of the bi-level Program (8a)-(8b) as well as the administrator strategy that is computed using Algorithm 6 under a correlation assumption on the valuations vl and welfare bang-per-buck ratios vl(dl+k)\ndl at each location l, as is elucidated by the\nfollowing two lemmas.\nLemma 10 (Monotonicity of Optimal Welfare in Contract \u03b1). Let \u03c3\u2217(\u03b1) be the optimal solution of the bi-level Program (8a)-(8b), given a contract parameter \u03b1. Then, the optimal welfare is monotonically nondecreasing in \u03b1, i.e., for any two contract parameters \u03b11, \u03b12 \u2208 [0, 1] with \u03b11 < \u03b12, it holds that WR(\u03c3\u2217(\u03b12)) \u2265 WR(\u03c3 \u2217(\u03b11)).\nLemma 11 (Monotonicity of Optimal Welfare in Contract \u03b1). Let \u03c3A\u03b1 be the allocation computed using Algorithm 6 given a contract parameter \u03b1 and suppose that the vl are positively correlated with the welfare bang-per-buck ratios vl(dl+k)dl at each location, i.e., if v1 \u2264 . . . \u2264 v|L|, then v1(d1+k) d1 \u2264 v2(d2+k)d2 \u2264 . . . \u2264 v|L|(d|L|+k)\nd|L| . Then, the welfare of the solutions corresponding to Algorithm 6 is monotonically non-decreasing\nin \u03b1, i.e., for any two contract parameters \u03b11, \u03b12 \u2208 [0, 1] with \u03b11 < \u03b12, it holds that WR(\u03c3A\u03b12) \u2265 WR(\u03c3 A \u03b11).\nWe note here that there are several settings when the correlation condition in the statement of Lemma 10 is satisfied. For instance, this condition is trivially satisfied in settings when the benefits dl are fixed regardless of the location, i.e., if dl = dl\u2032 for all l, l\n\u2032 \u2208 L. Such a setting may be pertinent in several applications of interest where the benefits accrued from engaging in fraud may not vary across locations and represents a fixed constant that is accrued if a user engages in fraud. Furthermore, while, for many instances, the above correlation condition may not be exactly met, in our conducted numerical experiments (e.g., see Figure 3), we observe that the welfare function corresponding to the administrator strategies computed using Algorithm 6 given contract parameters \u03b1 is monotonically increasing in \u03b1 (for the discrete set of values As where the step size is s = 0.05).\nFor proofs of Lemmas 10 and 11, see Appendices E.3 and E.4, respectively. Note that Lemmas 9 and 10 jointly establish Theorem 8, while Lemmas 9 and 11 jointly establish Theorem 13."
        },
        {
            "heading": "E.2 Proof of Lemma 9",
            "text": "Let \u03b1\u2217s \u2208 As be the optimal solution to the principal\u2019s payoff maximization problem for \u03b1 lying in the discrete set of values in the set As = {0, s, 2s, . . . , 1}. Furthermore, let \u03c3(\u03b1) be an administrator strategy, given a contract parameter \u03b1, such that the welfare function is monotonically non-decreasing in \u03b1, i.e., WR(\u03c3(\u03b12)) \u2265 WR(\u03c3(\u03b11)) if \u03b11 < \u03b12.\nThen, it follows by the optimality of \u03b1\u2217s in the set As that:\n(1\u2212 \u03b1\u2217s)WR(\u03c3(\u03b1\u2217s)) \u2265 (1\u2212 \u03b1)WR(\u03c3(\u03b1)), for all \u03b1 \u2208 As. (34)\nNext, define \u03b1\u2217 \u2208 [0, 1] as the contract parameter that maximizes the administrator\u2019s welfare, i.e., it holds that\n(1\u2212 \u03b1\u2217)WR(\u03c3(\u03b1\u2217)) \u2265 (1\u2212 \u03b1)WR(\u03c3(\u03b1)), for all \u03b1 \u2208 [0, 1].\nNext, without loss of generality, we suppose that \u03b1\u2217 /\u2208 As, as otherwise \u03b1\u2217s = \u03b1\u2217 by the optimality of \u03b1\u2217s in the discrete set As of \u03b1 values. Thus, it holds that there are two neighboring contract parameters \u03b1s1 , \u03b1s2 , such that \u03b1s1 < \u03b1\n\u2217 < \u03b1s2 and \u03b1s2 \u2212 \u03b1s1 \u2264 s. Consequently, it holds that \u03b1\u2217 \u2212 \u03b1s1 \u2264 s and \u03b1s2 \u2212 \u03b1\u2217 \u2264 s. Moreover, by the monotonicity of the welfare in \u03b1, we have that:\n(1\u2212 \u03b1\u2217)WR(\u03c3(\u03b1\u2217) (a)\n\u2264 (1\u2212 \u03b1\u2217)WR(\u03c3(\u03b1s2)), = (1\u2212 \u03b1s2)WR(\u03c3(\u03b1s2)) + (\u03b1s2 \u2212 \u03b1\u2217)WR(\u03c3(\u03b1s2)), (b)\n\u2264 (1\u2212 \u03b1\u2217s)WR(\u03c3(\u03b1\u2217s)) + s \u2211 l\u2208L vl,\n(c) \u2264 (1\u2212 \u03b1\u2217s)WR(\u03c3(\u03b1\u2217s)) + \u03f5,\nwhere (a) follows by the monotonocity of the welfare in \u03b1 and thus the fact that WR(\u03c3(\u03b1s2)) \u2265 WR(\u03c3(\u03b1\u2217), as \u03b1s2 > \u03b1\n\u2217, (b) follows by Equation (34), the fact that \u03b1s2 \u2212 \u03b1\u2217 \u2264 s, and that the maximum achievable welfare can never exceed \u2211 l\u2208L vl, and (c) follows as s \u2264 \u03f5\u2211 l vl . The final inequality above implies our desired sub-optimailty result that the loss in the optimal welfare from our dense sampling approach is bounded by at most \u03f5 for a step-size s \u2264 \u03f5\u2211\nl vl ."
        },
        {
            "heading": "E.3 Proof of Lemma 10",
            "text": "Let \u03b11 < \u03b12 be two contract parameters and let the optimal solution of the administrator\u2019s bi-level Program (8a)-(8b) given \u03b11 be \u03c3 \u2217(\u03b11) and given \u03b12 be \u03c3 \u2217(\u03b12). Then, by the optimality of \u03c3 \u2217(\u03b11) for Prob-\nlem (8a)-(8b), given contract parameter \u03b11, we have that:\nQR(\u03c3 \u2217(\u03b11)) + \u03b11WR(\u03c3 \u2217(\u03b11)) \u2265 QR(\u03c3\u2217(\u03b12)) + \u03b11WR(\u03c3\u2217(\u03b12)) (35)\nand by the optimality of \u03c3\u2217(\u03b12) for Problem (8a)-(8b), given contract parameter \u03b12, we have that:\nQR(\u03c3 \u2217(\u03b12)) + \u03b12WR(\u03c3 \u2217(\u03b12)) \u2265 QR(\u03c3\u2217(\u03b11)) + \u03b12WR(\u03c3\u2217(\u03b11)). (36)\nNext, summing Equations (35) and (36), we obtain:\n\u03b11WR(\u03c3 \u2217(\u03b11)) + \u03b12WR(\u03c3 \u2217(\u03b12)) \u2265 \u03b11WR(\u03c3\u2217(\u03b12)) + \u03b12WR(\u03c3\u2217(\u03b11)).\nThe above inequality implies that:\n(\u03b12 \u2212 \u03b11)(WR(\u03c3\u2217(\u03b12))\u2212WR(\u03c3\u2217(\u03b11))) \u2265 0.\nFinally, since \u03b12 > \u03b11, the above inequality can only hold if WR(\u03c3 \u2217(\u03b12)) \u2265 WR(\u03c3\u2217(\u03b11)), i.e., the welfare function corresponding to the optimal solution of Problem (8a)-(8b) is monotone in the contract parameter \u03b1, which establishes our desired result."
        },
        {
            "heading": "E.4 Proof of Lemma 11",
            "text": "Let \u03b11 < \u03b12 be two contract parameters, \u03c3\u0303 \u03b11 , \u03c3\u0303\u03b12 be the the solutions corresponding to step one of Algorithm 6, and \u03c3\u03b1 \u2032 1 ,\u03c3\u03b1 \u2032 2 be the the solutions corresponding to step two of Algorithm 6. In the following we show that: (i) WR(\u03c3 \u03b1\u20321) \u2264 WR(\u03c3\u03b1 \u2032 2) and (ii) WR(\u03c3\u0303\n\u03b11) \u2264 WR(\u03c3\u0303\u03b12). Note that proving both these claims establishes our desired result as \u03c3A\u03b1 = argmax{WR(\u03c3\u0303\u03b1),WR(\u03c3\u03b1 \u2032 )}, and hence, from the above two claims, it is straightforward to see that WR(\u03c3 A \u03b11) \u2264 WR(\u03c3 A \u03b12). Thus, in the remainder of this proof, we establish claims (i) and (ii).\nProof of Claim (i): Notice that in step two of Algorithm 6, only one location is allocated resources by the administrator. In particular, at the contract level \u03b11, suppose l\u03b11 is the location to which the administrator allocates resources. In this case, note that either (a) l\u03b11 = argmaxl\u2208L \u03b11vl or (b) l\u03b11 = argmaxl\u2208L(k\u039bl + \u03b11vl)\ndl dl+k , depending on whether maxl\u2208L \u03b11vl or maxl\u2208L(k\u039bl + \u03b11vl) dl dl+k is greater. Analogously, we can\ndefine l\u03b12 Case (a): We first suppose that case (a) holds for the parameter \u03b11. We now show that l\u03b12 = argmaxl\u2208L \u03b12vl, and consequently that l\u03b11 = l\u03b12 , which subsequently establishes our claim that WR(\u03c3 \u03b1\u20321) \u2264 WR(\u03c3 \u03b1\u20322).\nTo see this, first note by the definition of l\u03b11 that \u03b11vl\u03b11 \u2265 \u03b11vl for all l, i.e., vl\u03b11 \u2265 vl for all l. Thus, if l\u03b12 = argmaxl\u2208L \u03b12vl, it follows as l\u03b12 = l\u03b11 .\nMoreover, it holds by the definition of l\u03b11 that \u03b11vl\u03b11 \u2265 (k\u039bl\u03b11 + \u03b11vl\u03b11 ) dl\u03b11\ndl\u03b11 +k . Consequently, at \u03b12, it\nholds for any location l that\n(k\u039bl + \u03b12vl) dl\ndl + k = (k\u039bl + \u03b11vl) dl dl + k + (\u03b12 \u2212 \u03b11)vl dl dl + k ,\n(a) \u2264 \u03b11vl\u03b11 + (\u03b12 \u2212 \u03b11)vl, (b) \u2264 \u03b12vl\u03b11\nwhere (a) follows by the definition of l\u03b11 and the fact that dl dl+k \u2264 1 and (b) follows from the fact that vl\u03b11 \u2265 vl for all locations l, as noted above. Thus, we have that l\u03b12 = argmaxl\u2208L \u03b12vl, as, in particular, \u03b12vl\u03b11 \u2265 (k\u039bl + \u03b12vl) dl dl+k\n. Finally, since vl\u03b11 \u2265 vl for all locations l, it follows that l\u03b12 = l\u03b11 and hence that the welfare WR(\u03c3 \u03b1\u20321) = WR(\u03c3 \u03b1\u20322), which establishes our desired inequality on the welfare functions for case (a). Case (b): Next, suppose case (b) holds for the parameter \u03b11. In this case, the welfare accrued by the strategy \u03c3\u03b1 \u2032 1 is given by WR(\u03c3 \u03b1\u20321) = vl\u03b11 dl\u03b11\ndl\u03b11 +k . We now show that the corresponding welfare accrued\nby \u03c3\u0303\u03b12 is (weakly) higher. To see this, we consider two cases for l\u03b12 : (i) l\u03b12 = argmaxl\u2208L \u03b12vl and (ii) l\u03b12 = argmaxl\u2208L(k\u039bl + \u03b12vl) dl dl+k\n. In the first case, it holds that \u03b12vl\u03b12 \u2265 \u03b12vl\u03b11 by the definition of l\u03b12 , i.e., it follows that vl\u03b12 \u2265 vl\u03b11 .\nThus, the welfare accrued under the parameter \u03b12 is given by\nWR(\u03c3 \u03b1\u20322) = vl\u03b12 \u2265 vl\u03b11 \u2265 vl\u03b11 dl\u03b11 dl\u03b11 + k = WR(\u03c3 \u03b1\u20321),\nwhich implies that WR(\u03c3 \u03b1\u20322) \u2265 WR(\u03c3\u03b1 \u2032 1) in the first case.\nIn the second case, it holds that (k\u039bl\u03b12 +\u03b12vl\u03b12 ) dl\u03b12\ndl\u03b12 +k \u2265 (k\u039bl\u03b11 +\u03b12vl\u03b11 ) dl\u03b11 dl\u03b11 +k by the definition of l\u03b12 .\nMoreover, by the definition of l\u03b11 it holds that (k\u039bl\u03b12 + \u03b11vl\u03b12 ) dl\u03b12\ndl\u03b12 +k \u2264 (k\u039bl\u03b11 + \u03b11vl\u03b11 ) dl\u03b11 dl\u03b11 +k . Adding\nthese two inequalities, we get:\n\u03b12vl\u03b12 dl\u03b12\ndl\u03b12 + k + \u03b11vl\u03b11 dl\u03b11 dl\u03b11 + k \u2265 \u03b11vl\u03b12 dl\u03b12 dl\u03b12 + k + \u03b12vl\u03b11 dl\u03b11 dl\u03b11 + k .\nBy rearranging the above inequality, we obtain that:\n(\u03b12 \u2212 \u03b11)vl\u03b12 dl\u03b12\ndl\u03b12 + k \u2265 (\u03b12 \u2212 \u03b11)vl\u03b11 dl\u03b11 dl\u03b11 + k .\nSince, \u03b12 > \u03b11, we cancel this expression from the above equation and obtain in the second case that the welfare\nWR(\u03c3 \u03b1\u20322) = vl\u03b12 dl\u03b12 dl\u03b12 + k \u2265 vl\u03b11 dl\u03b11 dl\u03b11 + k = WR(\u03c3 \u03b1\u20321),\nwhich implies that WR(\u03c3 \u03b1\u20322) \u2265 WR(\u03c3\u03b1 \u2032 1) in the second case.\nProof of Claim (ii): We first introduce some notation. In particular, in step 1 of Algorithm 6, we let S1 be the set of locations to which resources are allocated given a parameter \u03b11 and let S2 be the set of locations to which resources are allocated given a parameter \u03b12. Furthermore, we define S 1 1 as the set of locations among S1 such that \u03b11vl(dl+k)\ndl \u2265 k\u039bl + \u03b11vl and S21 as the set of locations such that\n\u03b11vl(dl+k) dl < k\u039bl + \u03b11vl. Analogously, we can define the sets S 1 2 and S 2 2 . Then, the total welfare given the\nparameter \u03b11 corresponding to the allocation computed using step 1 of Algorithm 6 is given by WR(\u03c3\u0303 \u03b11) =\u2211\nl\u2208S21\\{l\u2032} vl dl dl+k\n+ \u2211\nl\u2208S11 vl + \u03c3\u0303\n\u03b11 l\u2032 vl\u2032 for at most one location l\n\u2032, where \u2211\nl\u2208L \u03c3\u0303 \u03b11 l \u2264 R. Here, we assume\nwithout loss of generality that the location l\u2032 with an allocation \u03c3\u03b11l\u2032 \u2208 (0, dl\u2032 dl\u2032+k ) belongs to the set S21 , but note that the following arguments can be readily generalized to cover the other case when l\u2032 \u2208 S11 as well. To prove our desired result, we consider two cases, \u2211 l\u2208L \u03c3\u0303 \u03b11 l < R and \u2211 l\u2208L \u03c3\u0303 \u03b11 l = R.\nFirst, consider the case when the total resource spending \u2211\nl\u2208L \u03c3\u0303 \u03b11 l < R. Note that in this case it must hold by the nature of step 1 of Algorithm 6 that \u2211\nl\u2208L dl dl+k < R. Consequently, \u03c3\u0303\u03b11l = dl dl+k for all l.\nAnalogously, by the nature of step 1 of Algorithm 6, it also holds that \u03c3\u0303\u03b12l = dl dl+k for all locations l, i.e., \u03c3\u0303\u03b11 = \u03c3\u0303\u03b12 . Then, the total welfare under given the parameter \u03b11 corresponding to the allocation computed using step 1 of Algorithm 6 is given by WR(\u03c3\u0303 \u03b11) = \u2211\nl\u2208S21 vl dl dl+k\n+ \u2211\nl\u2208S11 vl.\nWe now show that if l \u2208 S11 , then l \u2208 S12 . To see this, we first recall by the definition of S11 that \u03b11vl(dl+k)\ndl \u2265 k\u039bl +\u03b11vl for all locations l \u2208 S11 . Then, we have at \u03b12 that the following relation holds for all\nl \u2208 S11 :\n\u03b12vl(dl + k)\ndl =\n\u03b11vl(dl + k)\ndl + (\u03b12 \u2212 \u03b11)vl(dl + k) dl ,\n\u2265 k\u039bl + \u03b11vl + (\u03b12 \u2212 \u03b11)vl, = k\u039bl + \u03b12vl,\nwhere the inequality follows by the fact that l \u2208 S11 and thus \u03b11vl(dl+k) dl \u2265 k\u039bl + \u03b11vl, and the fact that dl+k dl \u2265 1. Thus, the above relation implies that if l \u2208 S11 , it holds that max{ \u03b12vl(dl+k) dl , k\u039bl + \u03b12vl} = \u03b12vl(dl+k) dl\n. Consequently, l \u2208 S12 as well, i.e., S11 \u2286 S12 . Finally, we have the following relation on the welfare given the parameter \u03b12 corresponding to the\nallocation computed using step 1 of Algorithm 6:\nWR(\u03c3\u0303 \u03b12) = \u2211 l\u2208S22 vl dl dl + k + \u2211 l\u2208S12 vl,\n(a) \u2265 \u2211 l\u2208S22 vl dl dl + k + \u2211 l\u2208S11 vl + \u2211 l\u2208S12\\{S11} vl,\n(b) \u2265 \u2211 l\u2208S22 vl dl dl + k + \u2211 l\u2208S11 vl + \u2211 l\u2208S12\\{S11} vl dl dl + k ,\n= \u2211 l\u2208S21 vl dl dl + k + \u2211 l\u2208S11 vl, = WR(\u03c3\u0303 \u03b11),\nwhere (a) follows from the fact that S11 \u2286 S12 and (b) follows as dldl+k \u2264 1. Consequently, in the setting when\u2211 l\u2208L \u03c3\u0303 \u03b11 l < R, we have established our desired relation that WR(\u03c3\u0303\n\u03b12) \u2265 WR(\u03c3\u0303\u03b11). Thus, for the remainder of this proof, we consider the setting when \u2211 l\u2208L \u03c3\u0303 \u03b11 l = R and \u2211 l\u2208L \u03c3\u0303 \u03b12 l = R. In the following, recall that, given any contract parameter \u03b1, in step one of Algorithm 6 resources are allocated to locations in descending order of their bang-per-buck ratios given by max{\u03b1vl(dl+k)dl , k\u039bl + \u03b1vl}. We now analyse two cases separately. First, as in the setting when \u2211 l\u2208L \u03c3\u0303 \u03b11 l < R, we consider the location set S 1 1 and show that if l \u2208 S11 then l \u2208 S12 as well. Next, we consider the location set S21 and show that Locations in set S11 : We now show that all locations in the set S 1 1 also belong to the set S 1 2 , where it holds that \u03c3\u0303\u03b12l = \u03c3\u0303 \u03b11 l . We show this using two intermediate results. First, we show that if l \u2208 S11 , then it holds that max{\u03b12vl(dl+k)dl , k\u039bl + \u03b12vl} = \u03b12vl(dl+k) dl . Next, we show that if l \u2208 S11 , and there are locations with a lower bang-per-buck ratio than location l under the parameter \u03b11, then so too will the bang-per-buck ratios of all those locations remain lower than that of l under the parameter \u03b12.\nTo show the first result, we first recall by the definition of S11 that \u03b11vl(dl+k) dl \u2265 k\u039bl+\u03b11vl for all locations\nl \u2208 S11 . Then, we have at \u03b12 that the following relation holds for all l \u2208 S11 :\n\u03b12vl(dl + k)\ndl =\n\u03b11vl(dl + k)\ndl + (\u03b12 \u2212 \u03b11)vl(dl + k) dl ,\n\u2265 k\u039bl + \u03b11vl + (\u03b12 \u2212 \u03b11)vl, = k\u039bl + \u03b12vl,\nwhere the inequality follows by the fact that l \u2208 S11 and thus \u03b11vl(dl+k) dl \u2265 k\u039bl + \u03b11vl, and the fact that dl+k dl \u2265 1. Thus, the above relation implies that if l \u2208 S11 , it holds that max{ \u03b12vl(dl+k) dl , k\u039bl + \u03b12vl} = \u03b12vl(dl+k) dl , which establishes the first result.\nTo show the second result, consider a location l \u2208 S11 and another location l1 with a lower bang-per-buck ratio under the parameter \u03b11. Then, it holds that\n\u03b11vl(dl+k) dl \u2265 \u03b11vl1 (dl1+k)dl1 for all locations l \u2208 S 1 1 , which\nimplies that vl(dl+k)dl \u2265 vl1 (dl1+k) dl1 and consequently that \u03b12vl(dl+k)dl \u2265 \u03b12vl1 (dl1+k) dl1 . Furthermore, it holds that\n\u03b11vl(dl+k) dl \u2265 k\u039bl1 + \u03b11vl1 for all locations l \u2208 S1. Consequently, it holds that:\n\u03b12vl(dl + k)\ndl =\n\u03b11vl(dl + k)\ndl + (\u03b12 \u2212 \u03b11)vl(dl + k) dl ,\n(a) \u2265 k\u039bl1 + \u03b11vl1 + (\u03b12 \u2212 \u03b11)vl1(dl1 + k)\ndl1 ,\n(b) = k\u039bl1 + \u03b11vl1 + (\u03b12 \u2212 \u03b11)vl1 , = k\u039bl + \u03b12vl1 ,\nwhere (a) follows as \u03b11vl(dl+k)dl \u2265 k\u039bl1 + \u03b11vl1 and that vl(dl+k) dl \u2265 vl1 (dl1+k)dl1 and (b) follows as dl+k dl \u2265 1. The above relations imply that if l \u2208 S11 , and a location l1 has a lower bang-per-buck ratio at \u03b11, then it also has a lower bang-per-buck ratio at \u03b12.\nThe above results have two key implications. First, the bang-per-buck ratios of all locations in set S11 retain the same ordering after sorting under \u03b12 as they did under \u03b11. Moreover, since all locations l1 /\u2208 S1 have a lower bang-per-buck ratio compared to any location in the set S11 at \u03b11, it also holds that all allocation l1 /\u2208 S1 have a bang-per-buck ratio compared to any location in the set S11 at \u03b12. Consequently, given that step 1 of Algorithm 6 will still allocate R resources under \u03b12, it holds that all locations l \u2208 S11 also satisfy l \u2208 S12 , i.e., S11 \u2286 S12 .\nLocations in Set S21 : We note that unlike for locations in set S 1 1 , there may be locations in set S 2 1 that may not be in the set S22 . In particular, there are three possibilities for locations in the set S 2 1 under \u03b11 when the contract parameter is changed to \u03b12: (i) a location l \u2208 S21 also belongs to S22 , (ii) a location l \u2208 S21 belongs to S12 , (iii) a location l \u2208 S21 no longer belongs to S2 and instead is replaced by a location l1 /\u2208 S1. We denote the location set in case (i) as L1, the location set in case (ii) as L2, and the locations that are no longer allocated in S21 as L \u2032 3 and the new locations in L\\S1 that are allocated under \u03b12 as L\u2032\u20323 .\nClearly, the welfare corresponding to the locations in case (i) remain unchanged, including the welfare accrued and the resource spending. Furthermore, the welfare corresponding to locations in case (ii) increases from vl\ndl dl+k to vl for all locations l \u2208 L2 without a change in the resource spending. Thus, we finally consider case (iii). Case (iii) only happens under two possibilities for some location l \u2208 S21 and l1 /\u2208 S2: (a) max{k\u039bl + \u03b12vl, \u03b12vl(dl+k)dl } \u2264 k\u039bl1 + \u03b12vl1 or (b) max{k\u039bl + \u03b12vl, \u03b12vl(dl+k) dl\n} \u2264 \u03b12vl1 (dl1+k)dl1 . In case (a), we first note that since l \u2208 S21 that k\u039bl + \u03b11vl \u2265 k\u039bl1 + \u03b11vl1 . Thus, the inequality in case\n(a) under \u03b12 can only hold if it holds that vl1 \u2265 vl. Next, we note for case (b) that vl1 (dl1+k)\ndl1 \u2265 vl(dl+k)dl , i.e., the welfare bang-per-buck of location l1 is\nhigher than that of location l. By our assumption that higher welfare bang-per-buck ratios correspond to higher valuations, we have that vl1 \u2265 vl in case (b) as well.\nThus, from the fact that vl1 \u2265 vl in both cases (a) and (b), it follows that the total welfare corresponding to the locations in case (iii) can only increase, as all the resource spending on locations in set L\u20323 is allocated to locations with an increased valuation in set L\u2032\u20323 .\nFinally, using the fact that S11 \u2286 S12 and that the welfare is higher under the contract parameter \u03b12 for all three cases corresponding to the S21 analyzed above, we have that, it is straightforward to see that WR(\u03c3\u0303 \u03b12) \u2265 WR(\u03c3\u0303\u03b11), which establishes our claim."
        },
        {
            "heading": "F Numerical Experiments",
            "text": "In this section, we first present additional details of our experimental setup and model calibration procedure based on the application case of queue jumping in intermediate public transport services in Mumbai, India (Appendix F.1). Next, in Appendix F.2, we present results depicting the contrast in the outcomes corresponding to revenue and welfare maximization administrator objectives. Finally, in Appendix F.3, we present a further discussion and analysis of the results presented in Figure 3 in Section 6.2."
        },
        {
            "heading": "F.1 Experimental Setup",
            "text": "We design a numerical experiment based on an application case of queue jumping in the context of intermediate public transport services in Mumbai, India. In particular, we consider a problem instance with L = 448 locations, representing the share-taxi and share-auto-rickshaw stands across the greater metropolitan region in Mumbai [50], where users can potentially engage in fraudulent activities, e.g., queue jumping. We ssume that each location has one type, i.e., |I| = 1, where the total number of (potentially) fraudulent users that arrive at each location are exponentially distributed with rate 80 for all locations l, i.e., \u039bl \u223c Exp(80) for all l, and the benefits dl at each location from engaging in fraud are exponentially distributed with rate 20, i.e., dl \u223c Exp(20).\nThese numbers are calibrated based on observational data collected on the number of users that arrived at a share-auto-rickshaw location in an hour at the Aakhruli Mhada share auto-rickshaw stand in Kandivali West in Mumbai and their corresponding average waiting times. In particular, the total number of users that arrive in an hour is about 125 at the studied share-auto-rickshaw location, where about 80 were males, which we assume as the group of potentially fraudulent users that may engage in queue jumping (see Section 1.1). Consequently, we assume that the number of fraudulent users that arrive at each location are exponentially distributed with rate 80 for all locations, which accounts for the variability in the number of fraudulent users arriving across these locations.\nMoreover, users waited between 4-5 minutes on average from their time of arrival to enter a share-auto (though, in general, the wait time for IPT services can often be on the order of hours [53]); thus, we calibrated the mean of the benefits of users from engaging in queue jumping as the product of the reduction in the wait time (of 5 minutes, i.e., 112 hours) and the average hourly wage of Rs.240 in Mumbai, resulting in an average gain of Rs.20 from engaging in queue jumping. We note that Rs.20 can be quite substantial for daily wage workers in Mumbai. As with the number of arriving users, we assume that the benefits of engaging in fraud are exponentially distributed with rate 20, which accounts for the variability in the quantity across the different locations. We also note here that despite an average wait time of about five minutes, the wait times faced by some users, particularly women, were often observed to be as high as 15 minutes, which is attributable to the fact that some users engage in queue jumping. More generally, in other share-taxi, share auto-rickshaw, or mini-bus locations across the world, the wait times are often variable and can be quite high for passengers.\nWe note here that obtaining more granular information for each of the 448 share-taxi and share-autorickshaw locations would result in more accurate results, but the above calibration process serves as a natural starting point to derive key insights and sensitivity relations in our studied security game.\nMoreover, for our experiments we vary the number of resources R to lie in the range from one to thirty, with an increment of one, i.e., R \u2208 {1, 2, . . . , 30} and the fine to lie in the range from Rs.50 to Rs.500, with increments of Rs.50, i.e., k \u2208 {50, 100, . . . , 500}. We note here that these fines are on the same order of magnitude as that for traditional traffic violations in Mumbai [54]. Furthermore, we consider a welfare function given by vl = \u039bl(dl)\nx for x lying in the range {1, 1.25, 1.5, 1.75, 2} for all locations l. Our choice of the welfare function stems from the fact that vl = \u039bldl corresponds to the total benefits derived by fraudulent users at location l, which, in the context of IPT services, can serve as a proxy to capture the additional value of the wait time faced by users that have not engaged in fraudulent behavior, as users engaging in fraud skip the queue and take their place in the vehicle. Moreover, setting vl = \u039bl(dl)\nx with an exponent x > 1 serves as a proxy to capture the fact that the administrator may place a greater value in reducing the additional wait time of non-defaulting users, which is likely a more realistic scenario in practice. We note that the welfare vl at each location l can be quite general and context dependent, and we use the above functional form of the welfare for the purposes of our experiments."
        },
        {
            "heading": "F.2 Gap Between Revenue and Welfare Maximization Outcomes",
            "text": "Figure 7 depicts the variation in the welfare and revenue of the allocation corresponding to Algorithm 2 in the welfare maximization setting and that of Algorithm 1 in the revenue maximization setting with the number of resources for a fine of k = 500 and a welfare function vl = \u039bl(dl)\n1.25. We note from Figure 7 that the welfare corresponding to Algorithm 2 in the welfare maximization setting monotonically increases in the number of resources as does the revenue corresponding to the revenue-maximizing solution computed using Algorithm 1. However, the results demonstrate that the objectives of maximizing revenue\nand welfare are at odds for the above defined problem instance. In particular, the solution computed using Algorithm 1 in the revenue maximization setting achieves only a small fraction of welfare of about 8% of that of the allocation corresponding to Algorithm 2 in the welfare maximization setting. Moreover, the outcome computed using Algorithm 2 results in almost no revenues due to the nature of the best-response of users to a welfare-maximizing administrator (see Equation (5)) and the structure of the optimal solution of the bi-level Program (3a)-(3b) (see Proposition 1). Furthermore, the welfare corresponding to the revenue maximizing outcome also monotonically increases with the number of resources as now more resources can be allocated, which not only results in more revenues but also improved welfare. Finally, we note that the welfare corresponding to both Algorithms 1 and 2 and the revenue corresponding to the revenue-maximizing outcome appear concave in the total number of resources, indicating a diminishing marginal returns in these quantities as the number of resources is increased.\nFigure 8 depicts the variation in the welfare and revenue of the allocation corresponding to Algorithm 2 in the welfare maximization setting and that of Algorithm 1 in the revenue maximization setting with the fine for R = 15 resources and a welfare function vl = \u039bl(dl)\n1.25. Our results demonstrate that the welfare corresponding to Algorithm 2 and the revenue corresponding to Algorithm 1 monotonically increase in the fine. Such monotonicity relations are expected as a higher fine implies that the threshold fraction of dldl+k resources to deter users from engaging in fraud at any given location l is reduced; thus, more resources can be spent at other locations that could not have been targeted with a smaller fine. Furthermore, altering the fine has almost no impact on the revenue of the allocation computed using Algorithm 2 in the welfare maximization setting. Such a result holds as revenues are only accumulated at a single location where \u03c3l <\ndl dl+k under the welfare maximization objective due to the best-response of users to a welfare-maximizing\nadministrator (see Equation (5)) and the structure of the optimal solution of the bi-level Program (3a)-(3b) (see Proposition 1). Thus, the revenues corresponding to the allocation computed using Algorithm 2 is negligible compared to the revenue obtained by Algorithm 1 in the revenue maximization setting.\nFrom Figure 8, we also observe that the revenue maximizing solution computed using Algorithm 1 obtains a welfare that monotonically decreases from about 44% of the welfare corresponding to Algorithm 2 in the welfare maximization setting for a fine of k = 50 to 8.4% of the welfare achieved by Algorithm 2 for k = 500. To parse this result, we first note that the welfare of the revenue-maximizing outcome serves as a constant fraction of the optimal social welfare (unlike the revenue of the allocation corresponding to Algorithm 2 that is negligible compared to the optimal revenue) as welfare is accumulated at all locations to which resources are allocated. In particular, by the best-response Problem (4) of users under the revenue-maximization objective, the total welfare corresponding to a revenue-maximizing solution is given by \u2211 l \u03c3lvl, where it\nholds that the resource constraint is satisfied, i.e., \u2211\nl \u03c3l \u2264 R, and the total resources spent at any location corresponding to a revenue maximizing solution satisfy \u03c3l \u2208 [0, dldl+k ] for all locations l.\nThus, as the fine k increases, the total spending under the revenue-maximizing solution on the locations that have been allocated resources at lower fines decreases while the administrator can now spend the remaining resources on new locations that it did not spend on at lower fines. Given this observation, notice that if the welfare vl at each location are independent of the revenue, then the total welfare is likely to not change much in response to the fine for all allocation strategies satisfying \u2211 l \u03c3l = R. However, the welfare function at each location satisfies vl = \u039bld 1.25 l , which is positively correlated with the revenue function which\nhas the term \u039bl in the objective. As a result, we obtain a monotonically decreasing relation between the fine and the welfare of the revenue maximizing solution, as fewer resources will be deployed in locations with higher values of \u039bl (which is positively correlated with the welfare values vl) with an increase in the fine while the remaining resources are spent on locations with lower values of \u039bl, which is correlated with lower values of vl. Such a monotonically decreasing relation in the welfare corresponding to the revenue maximizing outcome suggests that simply increasing the fines may not be a solution to reducing fraud, particularly in the presence of a revenuemaximizing administrator, which highlights the importance of setting low to moderate fines, as is often the case in many practical applications, e.g., road traffic fines are typically not arbitrarily large."
        },
        {
            "heading": "F.3 Additional Analysis of Numerical Experiments",
            "text": "In this section, we present some additional analysis of the results presented in the left of Figure 3 in Section 6.2.\nWe note from Figure 3 (left) that as we increase the exponent x in the welfare function vl = \u039bl(dl) x, the corresponding fraction of the welfare achieved by the administrator strategy computed using Algorithm 6 to that achieved using Algorithm 2 in the welfare maximization setting increases for each contract level \u03b1. Such a relation naturally follows as the welfare term in the administrator\u2019s Objective (8a) in the contract game increasingly dominates the associated revenues from the collected fines at each location with an increase in the exponent of the welfare function. We note that in the setting when the exponent x = 1, the administrator is only incentivized to change its resource allocation strategy from the one that maximizes revenues from the collected fines when the contract level \u03b1 is one, as the welfare term in the administrator\u2019s objective is not large enough to dominate the revenues the administrator obtains through its collected fines unless the contract level \u03b1 = 1. On the other hand, when the exponent x > 1, the welfare term in the administrator\u2019s objective begins to dominate the revenue obtained from the collected fines for lower levels of \u03b1, which thus incentivizes the administrator to alter its resource allocation strategy from one that solely maximizes revenues from the collected fines to one that achieves a higher level of system welfare for lower values of \u03b1. Finally, we note that while \u039bldl represents the total amount of benefit gained by defaulting users (if they are not found defaulting), the term \u039bl(dl)\nx for an exponent x > 1 serves as a reasonable proxy for the externality imposed on non-defaulting users (see Appendix F.1), which is likely to be valued higher, as non-defaulting users\u2019 costs may be strictly convex (rather than just linear) in the wait times they incur. Consequently, from the left of Figure 3, our results, for the studied welfare functions with an exponent x > 1, demonstrate that using even small values of the contract \u03b1 can recover most of the system welfare, thereby bridging the gap between the welfare and revenue-maximizing outcomes."
        },
        {
            "heading": "G Model Extensions and Further Directions for Future Research",
            "text": "In this section, we present several natural extensions and generalizations of the model studied in this work, which opens directions for future work.\nFines Varying Across Locations: In this work, we considered a setting where the fine remains fixed across all locations in the system. While a fixed fine across all nodes or locations is natural for many applications, there are often settings when the fines levied on defaulting users vary across locations, e.g., the fine for violating a traffic light is typically higher in a city center compared to a rural area. We note that we can model the variations in the fine across locations through a location-specific fine kl for each location l and that the techniques and algorithms developed in this work naturally generalize and apply to this setting with slightly more cumbersome notation, as our algorithms and the corresponding proofs do not rely on the fines being fixed across locations.\nContinuous Probability Distributions: In this work, we studied the setting when the type at each location is drawn from a discrete distribution with finite support. Thus, a natural generalization is to consider the setting when the location types are drawn from a continuous distribution. In the setting with continuous distributions, we note that our developed techniques and algorithms naturally generalize, as the expected revenue and welfare functions at each location, even in the setting with continuous distributions, can still be upper bounded by a piece-wise linear monotone concave upper approximation. Yet, we note that studying and formalizing the setting with continuous probability distributions offers several new exciting directions for future research. For instance, if each location\u2019s type is drawn from a continuous distribution, the discontinuities of the expected revenue function in the setting with discrete distributions will likely be smoothed out to some extent. Consequently, it would be interesting to investigate if there were certain classes of (continuous) distributions for which it would be possible to improve on the approximation guarantees obtained in this work.\nIncorporating Costs of Deploying Security Resources: We considered a model where the administrator has a limit on the resources it can allocate to monitor fraudulent or illegal activities in a system. An alternate model that is also of interest is to study equilibrium outcomes in a setting where the administrator additionally incurs a cost for each security personnel it allocates, which influences the administrator\u2019s objective function. In particular, we can model this additional cost to the administrator through a quantity cl for deploying a security resource to location l.\nStrategic Security Personnel: While we consider a setting when the administrator maximizes its revenues from allocating R security resources (e.g., police officers), another natural setting to consider is one where each of the R security resources are individual decision makers seeking to maximize their own revenues. In this case, we note that rather than one bi-level program to characterize the revenue maximization problem of the administrator, the optimal strategy of the security resources can be characterized through a sequence of R bi-level programs, where each bi-level program corresponds to the optimization problem of a single security resource seeking to maximize its individual revenue, given the actions of the other security resources. In particular, the revenue maximization problem (in the deterministic setting) for a security resource j, given the strategies (\u03c3j\u0303)j\u0303 \u0338=j adopted by a all other security resources, can be formulated as\nmax \u03c3j\u2208\u21261\nyl(\u03c3)\u2208[0,1],\u2200l\u2208L\nQjR(\u03c3 j , (\u03c3j\u0303)j\u0303 \u0338=j) = \u2211 l\u2208L \u03c3jl yl(\u03c3)k\u039bl, (37a)\ns.t. yl(\u03c3) \u2208 argmax y\u2208[0,1] Ul(\u03c3, y), for all l \u2208 L, (37b)\n\u03c3 = \u03c3j + \u2211 j\u0303 \u0338=j \u03c3j\u0303 , (37c)\nwhere \u03c3j \u2208 \u21261 for all j. We note that in the upper level problem, the administrator deploys a strategy \u03c3j to maximize its revenue, given the strategies (\u03c3j\u0303)j\u0303 \u0338=j adopted by all other security resources, to which users best-respond by maximizing their utilities in the lower-level problem. Moreover, note that the aggregate strategy \u03c3, corresponding to the sum of the strategies of all security resources, satisfies Constraint (37c).\nThe above model of the strategic behavior of each security resources to maximize their individual revenues opens up new avenues in terms of studying the resulting equilibrium outcomes that emerge from such selfish behavior of individual security resources."
        }
    ],
    "title": "When Simple is Near-Optimal in Security Games",
    "year": 2024
}