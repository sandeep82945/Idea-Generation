{
    "abstractText": "We obtain expressions for the asymptotic distributions of the R\u00e9nyi and Tsallis of order q entropies and Fisher information when computed on the maximum likelihood estimator of probabilities from multinomial random samples. We verify that these asymptotic models, two of which (Tsallis and Fisher) are normal, describe well a variety of simulated data. In addition, we obtain test statistics for comparing (possibly different types of) entropies from two samples without requiring the same number of categories. Finally, we apply these tests to social survey data and verify that the results are consistent but more general than those obtained with a \u03c72 test.",
    "authors": [
        {
            "affiliations": [],
            "name": "Takuya Yamano"
        },
        {
            "affiliations": [],
            "name": "Andrea A. Rey"
        },
        {
            "affiliations": [],
            "name": "Alejandro C. Frery"
        },
        {
            "affiliations": [],
            "name": "Magdalena Lucini"
        },
        {
            "affiliations": [],
            "name": "Juliana Gambini"
        },
        {
            "affiliations": [],
            "name": "Eduarda T. C. Chagas"
        },
        {
            "affiliations": [],
            "name": "Heitor S. Ramos"
        }
    ],
    "id": "SP:9892d53f8962c304d1f970dd1dfacc05ab3d3272",
    "references": [
        {
            "authors": [
                "N.L. Johnson",
                "S. Kotz",
                "N. Balakrishnan"
            ],
            "title": "Discrete Multivariate Distributions; Wiley-Interscience",
            "venue": "Hoboken, NJ, USA,",
            "year": 1997
        },
        {
            "authors": [
                "T. Modis"
            ],
            "title": "Links between entropy, complexity, and the technological singularity",
            "venue": "Technol. Forecast",
            "year": 2022
        },
        {
            "authors": [
                "K. Hutcheson"
            ],
            "title": "A test for comparing diversities based on the Shannon formula",
            "venue": "J. Theor. Biol",
            "year": 1970
        },
        {
            "authors": [
                "K. Hutcheson",
                "L.R. Shenton"
            ],
            "title": "Some moments of an estimate of Shannon's measure of information",
            "venue": "Commun. Stat. Theory Methods 1974,",
            "year": 1974
        },
        {
            "authors": [
                "P. Jacquet",
                "W. Szpankowski"
            ],
            "title": "Entropy computations via analytic depoissonization",
            "venue": "IEEE Trans. Inf. Theory",
            "year": 1999
        },
        {
            "authors": [
                "J. Cicho\u0144",
                "Z. Gol\u0119biewski"
            ],
            "title": "On Bernoulli Sums and Bernstein Polynomials. In Proceedings of the 23rd International Meeting on Probabilistic, Combinatorial, and Asymptotic Methods in the Analysis of Algorithms, Montreal",
            "venue": "QC, Canada,",
            "year": 2012
        },
        {
            "authors": [
                "G.W. Cook",
                "D.F. Kerridge",
                "J.D. Pryce"
            ],
            "title": "Estimations of Functions of a Binomial Parameter",
            "venue": "Sankhya\u0304 Indian J. Stat. Ser. A 1974,",
            "year": 1974
        },
        {
            "authors": [
                "E.T.C. Chagas",
                "A.C. Frery",
                "J. Gambini",
                "M.M. Lucini",
                "H.S. Ramos",
                "A.A. Rey"
            ],
            "title": "Statistical Properties of the Entropy from Ordinal Patterns",
            "venue": "Chaos Interdiscip. J. Nonlinear Sci",
            "year": 2022
        },
        {
            "authors": [
                "C. Tsallis"
            ],
            "title": "Possible generalization of Boltzmann-Gibbs statistics",
            "venue": "J. Stat. Phys",
            "year": 1988
        },
        {
            "authors": [
                "A. R\u00e9nyi"
            ],
            "title": "On Measures of Entropy and Information",
            "venue": "In Proceedings of the 4th Berkeley Symposium on Mathematical Statistics and Probability, Berkeley, CA, USA,",
            "year": 1961
        },
        {
            "authors": [
                "B.R. Frieden"
            ],
            "title": "Science from Fisher Information: A Unification",
            "year": 2010
        },
        {
            "authors": [
                "P. S\u00e1nchez-Moreno",
                "R.J. Y\u00e1\u00f1ez",
                "J.S. Dehesa"
            ],
            "title": "Discrete densities and Fisher information",
            "venue": "In Proceedings of the 14th International Conference on Difference Equations and Applications, Istanbul, Turkey,",
            "year": 2009
        },
        {
            "authors": [
                "E.L. Lehmann",
                "G. Casella"
            ],
            "title": "Theory of Point Estimation",
            "venue": "Springer Science & Business Media: Berlin/Heidelberg, Germany,",
            "year": 2006
        },
        {
            "authors": [
                "E.L. Lehman",
                "J.P. Romano"
            ],
            "title": "Testing Statistical Hypothesis",
            "venue": "3rd ed.; Springer: Berlin/Heidelberg, Germany,",
            "year": 2005
        },
        {
            "authors": [
                "D. Freedman",
                "P. Diaconis"
            ],
            "title": "On the histogram as a density estimator: L2 theory",
            "venue": "Zeitschrift fu\u0308r Wahrscheinlichkeitstheorie und Verwandte Gebiete 1981,",
            "year": 1981
        },
        {
            "authors": [
                "A. Agresti"
            ],
            "title": "An Introduction to Categorical Data Analysis; Wiley-Interscience",
            "venue": "Hoboken, NJ, USA,",
            "year": 2007
        },
        {
            "authors": [
                "J.B. Borges",
                "H.S. Ramos",
                "A.A.F. Loureiro"
            ],
            "title": "A Classification Strategy for Internet of Things Data Based on the Class Separability Analysis of Time Series Dynamics",
            "venue": "ACM Trans. Internet Things 2022,",
            "year": 2022
        },
        {
            "authors": [
                "B. Beranger",
                "H. Lin",
                "S. Sisson"
            ],
            "title": "New models for symbolic data analysis",
            "venue": "Adv. Data Anal. Classif",
            "year": 2022
        },
        {
            "authors": [
                "J.B. Borges",
                "J.P.S. Medeiros",
                "L.P.A. Barbosa",
                "H.S. Ramos",
                "A.A. Loureiro"
            ],
            "title": "IoT Botnet Detection based on Anomalies of Multiscale Time Series Dynamics",
            "venue": "IEEE Trans. Knowl. Data Eng",
            "year": 2022
        },
        {
            "authors": [
                "E.T.C. Chagas",
                "A.C. Frery",
                "O.A. Rosso",
                "H.S. Ramos"
            ],
            "title": "Analysis and Classification of SAR Textures using Information Theory",
            "venue": "IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. 2021,",
            "year": 2021
        },
        {
            "authors": [
                "C. Bandt",
                "B. Pompe"
            ],
            "title": "Permutation entropy: A natural complexity measure for time series",
            "venue": "Phys. Rev. Lett",
            "year": 2002
        },
        {
            "authors": [
                "M. Zanin",
                "L. Zunino",
                "O.A. Rosso",
                "D. Papo"
            ],
            "title": "Permutation Entropy and Its Main Biomedical and Econophysics Applications: A Review",
            "venue": "Entropy",
            "year": 2012
        }
    ],
    "sections": [
        {
            "text": "Citation: Rey, A.A.; Frery, A.C.;\nLucini, M.; Gambini, J.; Chagas,\nE.T.C.; Ramos, H.S. Asymptotic\nDistribution of Certain Types of\nEntropy under the Multinomial Law.\nEntropy 2023, 25, 734. https://\ndoi.org/10.3390/e25050734\nAcademic Editor: Takuya Yamano\nReceived: 14 February 2023\nRevised: 24 March 2023\nAccepted: 21 April 2023\nPublished: 28 April 2023\nCopyright: \u00a9 2023 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\nKeywords: multinomial distribution; entropy; asymptotic distributions; hypothesis tests"
        },
        {
            "heading": "1. Introduction",
            "text": "The multinomial distribution is an adequate model for describing how observations fall into categories. Quoting Johnson et al. [1], \u201cThe Multinomial distribution, like the Multivariate Normal distribution among the continuous multivariate distributions, consumed a sizable amount of the attention that numerous theoretical as well as applied researchers directed towards the area of discrete multivariate distributions.\u201d The entropy of a (multivariate, in our case) random variable is a substantial quantity. It quantifies the predictability of a system whose outputs can be described by such a model. Entropy has several definitions, both conceptual and mathematical. The concept of entropy originated as a way to relate a system\u2019s energy and temperature [2]. The same concept was used to describe the number of ways the particles of a system can be arranged. Entropy has been seldom studied as a random variable. Hutcheson [3] and Hutcheson and Shenton [4] discussed the exact expected value and variance of the Shannon entropy under the multinomial model. These works also provided approximate expressions that circumvent the numerical issues when using the exact value. Jacquet and Szpankowski [5] studied high-quality analytic approximations of the R\u00e9nyi entropy, of which the Shannon entropy is a particular case, under the binomial model. With the same approach, Cichon\u0301 and Gole\u0328biewski [6] obtained expressions for\nEntropy 2023, 25, 734. https://doi.org/10.3390/e25050734 https://www.mdpi.com/journal/entropy\nmore general functionals that include the multinomial distribution. These works treat the entropy as a fixed quantity. Cook et al. [7] studied almost unbiased estimators of functions of the parameter of the binomial distribution. The authors extended those results to find an almost-unbiased estimator for the entropy under multinomial laws. Chagas et al. [8] treated the Shannon entropy as a random variable. The authors obtained its asymptotic distribution when indexing by the maximum likelihood estimators of the proportions under the multinomial distribution. This result allowed the devising of unilateral and bilateral tests for comparing the entropy from two samples in a very general way. These tests do not require having the same number of categories. In this work, our attention is directed toward the asymptotic distribution of other forms of entropy under the multinomial model. This allows the comparison of large samples throughout their entropies and, with this, they may have different numbers of classes. The comparison also allows using different types of entropy. We firstly apply the multivariate delta method and, in the case of the R\u00e9nyi entropy, we transform the resulting multivariate normal distribution into that of the logarithm of the absolute value of a normally distributed random variable. Then, we provide the general expression of a test statistic that suits our needs. This paper unfolds as follows. Section 2 recalls the main properties of the multinomial distribution and defines the four types of entropies we will study. In Section 3, we present the central results, i.e., the asymptotic distribution of those entropies. We describe the techniques we used and left for Appendix A.1 technical details. We validate our results with simulation studies in Section 4: we show the adequacy of the normal distribution as limit law for the entropies under three probability models of different support, considering various sample sizes. In Section 5, we show that these asymptotic properties lead to a helpful hypothesis test between samples with different categories. We conclude the article in Section 6. Appendix A.2 comments on applications that justify our choices of the number of categories and sample sizes in the simulation studies. Appendix A.3 discloses relevant computational information, including reproducibility."
        },
        {
            "heading": "2. Entropies and the Multinomial Distribution",
            "text": "Consider a series of n independent trials, where only one of k mutually exclusive events \u03c01, \u03c02, . . . , \u03c0k must be observed in each one, with probability p = {p1, p2, . . . , pk} such that p` \u2265 0 and \u2211k`=1 p` = 1. Let N = (N1, N2, . . . , Nk) be the random vector that counts the number of occurrences of the events \u03c01, \u03c02, . . . , \u03c0k in the n trials, with N` \u2265 0 and \u2211k`=1 N` = n. A sample from N, say n, is a k-variate vector of integer values n = (n1, n2, . . . , nk). Then, the joint distribution of N is\nPr(N = n) = Pr(N1 = n1, N2 = n2, . . . , Nk = nk) = n! k\n\u220f `=1 pn`` n`! . (1)\nWe denote this situation as N \u223c Mult(n, p). In practice, one does not know the true values of p, the probabilities that index\nthis multinomial distribution. Such values are estimated by computing p\u0302`, the proportion of times the class (category, event) \u03c0` was observed among the k possible categories \u03c0 = {\u03c01, \u03c02, . . . , \u03c0k} during the n trials. The maximum likelihood estimator for p\u0302 = ( p\u03021, p\u03022, . . . , p\u0302k) is the random vector of proportions. This maximum likelihood estimator coincides with the intuitive estimator based on the distribution\u2019s first moments, and is the most frequently used in applications. We study the distribution of several forms of entropy of the random vector p\u0302 for fixed k. Notice that p\u0302 is computed over a single k-variate measurement of random proportions corresponding to a single random sample from N \u223c Mult(n, p). The asymptotic behaviors we derive hold for typical cases in which n k. The Shannon entropy measures the disorder or unpredictability of systems characterized by a probability distribution. On the one hand, the minimum Shannon value occurs\nwhen there is complete knowledge about the system behavior and total confidence in predicting the following observation. On the other hand, when a uniform distribution describes the system\u2019s behavior, that is, when all possibilities have the same probability of occurrence, the knowledge about the behavior of the data is minimal. In Chagas et al. [8], we studied the asymptotic distribution of the Shannon entropy. In this work, we extend those results to three other forms of entropy. Other types of descriptors have been proposed in the literature to extract additional information not captured by the Shannon entropy. Tsallis [9] and R\u00e9nyi [10], for instance, proposed parametric versions, which include the Shannon entropy. Fisher information [11] is defined by an average logarithm derivative of a continuous probability density function. In the case of discrete densities, this measure can be approximated using differences of probabilities between consecutive distribution elements. While the Shannon entropy captures the degree of unpredictability of a system, the Fisher information is related to the rate of change of consecutive observations and, thus, quantifies small changes and perturbations. Given a type of entropy H, we are interested in the distribution of H(p) when indexed by p\u0302, the maximum likelihood estimator of p. Our problem then becomes finding the distribution of H(p\u0302) for the following:\n\u2022 The Shannon entropy\nHS(p\u0302) = \u2212 k\n\u2211 `=1 p\u0302` log p\u0302`, (2)\n\u2022 The Tsallis entropy with index q \u2208 R \\ {1}\nHqT(p\u0302) = k\n\u2211 `=1\np\u0302` \u2212 p\u0302 q `\nq\u2212 1 , (3)\n\u2022 The R\u00e9nyi entropy of order q \u2208 R+ \\ {1}\nHqR(p\u0302) = 1 1\u2212 q log k\n\u2211 `=1\np\u0302q` , (4)\n\u2022 The Fisher information, also termed \u201cFisher Information Measure\u201d in the literature, with renormalization coefficient F0 = 4\nHF(p\u0302) = F0 k\u22121 \u2211 `=1\n(\u221a p\u0302`+1 \u2212 \u221a p\u0302` )2. (5)\nAmong other possibilities, we used Equation (2.7) from Ref. [12]."
        },
        {
            "heading": "3. Asymptotic Distributions of Entropies",
            "text": "The main results of this section are the asymptotic distributions of the Shannon (2), Tsallis of order q (3), and R\u00e9nyi of order q (4) entropies, and Fisher information (5). These results are presented, respectively, in Equations (30)\u2013(32) and (35). Notice that the R\u00e9nyi entropy is not asymptotically normally distributed, while the other three are. We recall the following theorems known respectively as the delta method and its multivariate version. We refer to Lehmann and Casella [13] for their proofs.\nTheorem 1. Let Xn be a sequence of independent and identically distributed random variables such that \u221a\nn[Xn \u2212 \u03b8] converges in distribution to aN (0, \u03c32). If \u2202h/\u2202\u03b8 exists and does not vanish, then\u221a n[h(Xn)\u2212 h(\u03b8)] converges in distribution to a N (0, \u03c32[\u2202h/\u2202\u03b8]2).\nTheorem 2. Let Xn = (X1n, X2n, . . . , Xkn) be a sequence of independent and identically distributed vectors of random variables such that \u221a n[X1n \u2212 \u03b81, X2n \u2212 \u03b82, . . . , Xkn \u2212 \u03b8k] converges in\ndistribution to a multivariate normal distribution Nn(0, \u03a3), where \u03a3 is the covariance matrix. Suppose that h1, h2, . . . , hk are real functions continuously differentiable in a neighborhood of the parameter point \u03b8 = (\u03b81, \u03b82, . . . , \u03b8k) and such that the matrix of partial derivatives B = (\u2202h`/\u2202\u03b8)k`,=1 is non-singular in the mentioned neighborhood. Then, the following convergence in distribution holds:\n\u221a n [ h1(Xn)\u2212 h1(\u03b8), h2(Xn)\u2212 h2(\u03b8), . . . , hk(Xn)\u2212 hk(\u03b8) ] D\u2212\u2192 N (0, B\u03a3B\u2032), where B\u2032 denotes the transpose of B.\nNow, we focus on the case N \u223c Mult(n, p). Let p\u0302 = N/n be the vector of sample proportions which coincides with the maximum likelihood estimator (MLE) of p and Yn = \u221a n(p\u0302\u2212 p). Then\nYn D\u2212\u2192 N (0, Dp \u2212 pp\u2032),\nwhere Dp = Diag(p1, p2, . . . , pk). Let us explore the covariance matrix in this case:\nDp \u2212 pp\u2032 =  p1 0 \u00b7 \u00b7 \u00b7 0 0 p2 \u00b7 \u00b7 \u00b7 0 ... ... . . .\n... 0 0 \u00b7 \u00b7 \u00b7 pk\n\u2212  p1 p2 ...\npk\n(p1 p2 \u00b7 \u00b7 \u00b7 pk) (6)\n=  p1 \u2212 p21 \u2212p1 p2 \u00b7 \u00b7 \u00b7 \u2212p1 pk \u2212p2 p1 p2 \u2212 p22 \u00b7 \u00b7 \u00b7 \u2212p2 pk ... ... . . . ...\n\u2212pk p1 \u2212pk p2 \u00b7 \u00b7 \u00b7 pk \u2212 p2k  (7) It means that the covariance matrix \u03a3p \u2208 Rk\u00d7k we are interested in is of the form\n(\u03a3p)` = { p`(1\u2212 p`) if ` = , \u2212p`p if ` 6= .\n(8)\nThe above statements are generalized. In the following, we obtain new results for the Tsallis and R\u00e9nyi entropies, and for the Fisher information. For the sake of completeness, we also include the results for the Shannon entropy.\nIn order to apply the delta method using Theorem 2, we consider the following functions:\nhS` (p1, p2, . . . , pk) = p` log p`, (9) hT` (p1, p2, . . . , pk) = p` \u2212 p q ` , (10) hR` (p1, p2, . . . , pk) = p q ` , (11)\nhF` (p1, p2, . . . , pk) = (\u221a p`+1 \u2212 \u221a p` )2, (12)\nfor ` = 1, 2, . . . , k except for (12) that holds for ` = 1, 2, . . . , k \u2212 1. The assumptions are verified, and thus,\n\u2202hS` \u2202p` = log p` + 1 and \u2202hS` \u2202p = 0 if  6= `, (13) \u2202hT` \u2202p` = 1\u2212 qpq\u22121` and \u2202hT` \u2202p = 0 if  6= `, (14) \u2202hR` \u2202p` = qpq\u22121` and \u2202hR` \u2202p = 0 if  6= `, (15) \u2202hF` \u2202p = \u221a p`+1 \u2212 \u221a p` (\u22121)`+\u22121\u221ap if  = `, `+ 1 and \u2202hF` \u2202p = 0 if  6= `, `+ 1. (16)\nFinally, we need the covariance matrix of the multivariate normal limit distribution, which is\n\u03a3\u2206Mp = ( \u2202hM` \u2202p )k `,=1 \u03a3p ( \u2202hM` \u2202p )k `,=1 \u2032 , (17)\nwhereM \u2208 {S, T, R, F}. Since (\u2202hM` /\u2202p) k `,=1 are diagonal matrices forM \u2208 {S, T, R}, we can use Equation (A1) to conclude that\n(\u03a3\u2206Sp )` =\n{ (p` \u2212 p2`)(log p` + 1)\n2 if ` = , \u2212p`p(log p` + 1)(log p + 1) if ` 6= ;\n(18)\n(\u03a3\u2206Tp )` =\n{ (p` \u2212 p2`)(1\u2212 qp q\u22121 ` ) 2 if ` = ,\n\u2212p`p(1\u2212 qp q\u22121 ` )(1\u2212 qp q\u22121  ) if ` 6= ;\n(19)\n(\u03a3\u2206Rp )` =\n{ q2(p` \u2212 p2`)p 2(q\u22121) ` if ` = ,\n\u2212q2(p`p)q if ` 6= . (20)\nIn the case of \u03a3\u2206Fp , from Equations (A3) and (A4) we have the following:\n\u2022 For `,  = 1, 2, . . . , k\u2212 2 and ` 6= \u2212 1, ,  + 1:\n(\u03a3\u2206Fp )` = (\u221a p`+1 \u2212 \u221a p` )(\u221a p+1 \u2212 \u221a p )(\u221a p`+1 p + \u221a p`p+1 \u2212 \u221a p`p \u2212 \u221a p`+1 p+1 ) . (21)\n\u2022 For ` = 1, 2, . . . , k\u2212 2:\n(\u03a3\u2206Fp )`,`\u22121 = (\u221a p`+1 \u2212 \u221a p` )(\u221a p` \u2212 \u221a p`\u22121 )(\u221a\np`+1 p`\u22121 + p` \u2212 1\u2212 \u221a p`p`\u22121 \u2212 \u221a p`+1 p` ) . (22)\n\u2022 For ` = 1, 2, . . . , k\u2212 2:\n(\u03a3\u2206Fp )`` = (\u221a p`+1 \u2212 \u221a p` )2(2\u221ap`p`+1 + 2\u2212 p` \u2212 p`+1). (23)\n\u2022 For ` = 1, 2, . . . , k\u2212 2:\n(\u03a3\u2206Fp )`,`+1 = (\u221a p`+1 \u2212 \u221a p` )(\u221a p`+2 \u2212 \u221a p`+1 )( p`+1 \u2212 1 + \u221a p`p`+2 \u2212 \u221a p`p`+1 \u2212 \u221a p`+1 p`+2 ) . (24)\n\u2022 For  = 1, 2, . . . , k\u2212 2:\n(\u03a3\u2206Fp )k\u22121, = ( \u221a pk \u2212 \u221a pk\u22121)( \u221a p+1 \u2212 \u221a p) ( pk p+1\u221ap+1 \u2212 pk\u22121 p\u221ap ) . (25)\n\u2022 Finally, (\u03a3\u2206Fp )k\u22121,k\u22121 = ( \u221a pk \u2212 \u221a pk\u22121)2(1\u2212 pk\u22121). (26)\nHence, we conclude that\n\u221a n [ hM1 ( p\u03021)\u2212 hM1 (p1), hM2 ( p\u03022)\u2212 hM2 (p2), . . . , hMk\u2032 p\u0302k\u2032 \u2212 h M k\u2032 (pk\u2032) ] D\u2212\u2192 N (0, \u03a3\u2206Mp ), (27) whereM \u2208 {S, T, R, F} and k\u2032 = k in all cases except for the case of the Fisher information in which k\u2032 = k\u2212 1. An equivalent expression is\n\u221a n [ hM1 ( p\u03021), h M 2 ( p\u03022), . . . , h M k\u2032 ( p\u0302k\u2032) ] D\u2212\u2192 N \u221an  hM1 (p1) hM2 (p2)\n... hMk\u2032 (pk\u2032)\n, \u03a3\u2206Mp . (28)\nIf Y is a vector of random variables such that \u221a nY D\u2212\u2192 N ( \u221a\nn\u00b5, \u03a3), then it can be proved that E( \u221a nY) \u2192 \u221a n\u00b5 and Var( \u221a nY) \u2192 \u03a3. Provided well-known properties, it holds that E(Y)\u2192 \u00b5 and Var(Y)\u2192 1/n\u03a3. Applying this to (28),\n[ hM1 ( p\u03021), h M 2 ( p\u03022), . . . , h M k\u2032 ( p\u0302k\u2032) ] D\u2212\u2192 N   hM1 (p1) hM2 (p2)\n... hMk\u2032 (pk\u2032)\n, 1n \u03a3\u2206Mp . (29)\nNow, using (29), we find the asymptotic distribution of (2)\u2013(5). In order to do so, we need to know the distribution of the sum of k Gaussian random variables with different means and an arbitrary covariance matrix. For any k-dimensional multivariate normal distribution Z \u223c N (\u00b5, \u03a3), with \u00b5 \u2208 Rk and covariance matrix \u03a3 = (\u03c3`), holds that the distribution of W = aTZ, with a \u2208 Rk, is N ( aT\u00b5, \u2211k`=1 a 2 `\u03c3`` + 2 \u2211 k\u22121 `=1 \u2211 k =i+1 a`a\u03c3` ) . Using the limit distribution presented in (29) and a = (\u22121,\u22121, . . . ,\u22121), we directly have the asymptotic distribution of the Shannon entropy as follows:\nHS(p\u0302) = \u2212 k\n\u2211 `=1\np\u0302` log p\u0302` D\u2212\u2192\nN ( \u2212 k\n\u2211 `=1\np` log p`, 1 n\nk\n\u2211 `=1\np`(1\u2212 p`)(log p` + 1)2 \u2212 2 n k\u22121 \u2211 =1 k \u2211 `=+1 p`p(log p` + 1)(log p + 1)\n) . (30)\nWith similar arguments and a = (1, 1, . . . , 1), we obtain the asymptotic distribution for the Tsallis entropy of order q:\nHqT(p\u0302) = k\n\u2211 `=1\np\u0302` \u2212 p\u0302 q `\nq\u2212 1 D\u2212\u2192 N\n( k\n\u2211 `=1\np` \u2212 p q ` q\u2212 1 , k\n\u2211 `=1\n(p` \u2212 p2`)(1\u2212 qp q\u22121 ` ) 2 n(q\u2212 1)2 \u2212 2 k\u22121 \u2211 =1 k \u2211 `=+1 p`p(1\u2212 qp q\u22121 ` )(1\u2212 qp q\u22121  ) n(q\u2212 1)2\n) . (31)\nThe procedure is analogous for the Fisher information but with a = (1, 1, . . . , 1) \u2208 R k\u22121. Hence, it can be proved that\nHF(p\u0302) = F0 k\u22121 \u2211 `=1\n( \u221a p\u0302`\u22121 \u2212 \u221a p\u0302`)2 D\u2212\u2192 N ( F0\nk\u22121 \u2211 `=1 ( \u221a p`\u22121 \u2212 \u221a p`)2, F0 n\n\u03a3\u2217 ) , (32)\nwhere\n\u03a3\u2217 = (\u221a pk \u2212 \u221a pk\u22121 )2 (1\u2212 pk\u22121) +\nk\u22122 \u2211 `=1\n(\u221a p`+1 \u2212 \u221a p` )2(2\u221ap`p`+1 \u2212 p` \u2212 p`+1 + 2)+\n2 k\u22122 \u2211 `=3 `\u22122 \u2211 j=1\n[(\u221a p`+1 \u2212 \u221a p` )(\u221a p+1 \u2212 \u221a p )(\u221a p`+1 p + \u221a p`p+1 \u2212 \u221a p`p \u2212 \u221a p`+1 p+1 )] +\n2 k\u22122 \u2211 =1\n[(\u221a pk \u2212 \u221a pk\u22121 )(\u221a p+1 \u2212 \u221a p )( pk \u221a p+1 \u2212 pk\u22121 \u221a p )] +\n2 k\u22122 \u2211 `=2\n[(\u221a p`+1 \u2212 \u221a p` )(\u221a p` \u2212 \u221a p`\u22121 )(\u221a p`+1 p`\u22121 \u2212 \u221a p`p`\u22121 \u2212 \u221a p`+1 p` + p` \u2212 1 )] . (33)\nTo obtain expression (33), we use the symmetry of the covariance matrix which implies that \u2211k\u22121`=1 \u2211 k =`+1 a`a\u03c3` = \u2211 k\u22121 `=2 \u2211 `\u22121 =1 a`a\u03c3`. It is worth noticing that the expression of the covariance matrix for Fisher information is more complicated than the previously analyzed entropies since the matrix of partial derivatives is not diagonal in this case. The case of R\u00e9nyi entropy is different because, following the previous methodology, we can prove that\nk\n\u2211 `=1\np\u0302q` D\u2212\u2192 N\n( k\n\u2211 `=1\npq` , 1 n\nk\n\u2211 `=1\nq2(p` \u2212 p2`)p 2(q\u22121) ` \u2212 2 n k\u22121 \u2211 `=1 k \u2211 =`+1\nq2(p p`)q ) . (34)\nHence,\nHqR(p\u0302) = 1 1\u2212 q log k\n\u2211 `=1\np\u0302q` D\u2212\u2192PqR, (35)\nwhere\nP q R(x) = 1\u2212 q \u03c3\u2217 \u221a 2\u03c0 exp[(1\u2212 q)x log(k)] exp\n{ \u22121\n2\n( exp[(1\u2212 q)x log(k)]\u2212 \u00b5\u2217\n\u03c3\u2217\n)2} , (36)\nwith \u00b5\u2217 = \u2211k`=1 p q ` and \u03c3 \u2217 = n\u22121 \u2211k`=1 q 2(p` \u2212 p2`)p 2(q\u22121) ` \u2212 2n \u22121 \u2211k\u22121`=1 \u2211 k =`+1 q 2(p`p)q. Notice that this is not a normal distribution but the distribution of the logarithm of the absolute value of a normally distributed random variable. Often, in practice, these entropies are scaled to be in [0, 1]; these are called \u201cnormalized entropies\u201d. The following modifications must be considered in the normalized versions of the entropies. For the normalized Shannon entropy, the asymptotic mean and variance are multiplied by 1/ log k and 1/(log k)2, respectively. In the case of the normalized Tsallis entropy, the asymptotic mean and variance are multiplied by (q\u2212 1)/(1\u2212 k1\u2212q) and (q\u2212 1)2/(1\u2212 k1\u2212q)2, respectively. Finally, the asymptotic distribution of the normalized R\u00e9nyi entropy is P\u0303qR(x) = log kP q R(x log k). Notice that normalized entropies do not depend on the logarithm basis. The Fisher information is, as defined in (5), already normalized."
        },
        {
            "heading": "4. Analysis and Validation",
            "text": "In this section, we study the empirical distribution of the entropies computed from p\u0302 under three models, four categories (k \u2208 {6, 24, 120, 720}), and three sample sizes (n \u2208 {102k, 103k, 104k}) that depend on the number of categories. These choices of k and n are based on the values that appear in signal analysis with ordinal patterns; see details of this technique in Appendix A.2.\nWe considered the following probability functions p = (p1, p2, . . . , pk):\n1. Linear: p` = 2`/(k(k + 1)), 1 \u2264 ` \u2264 k.\n2. One-Almost-Zero: p` = 1/k for 1 \u2264 ` \u2264 k\u2212 2, pk\u22121 = e0, and pk = 2/k\u2212 e0 with e0 = 2.220 446\u00d7 10\u221216 (the smallest positive number for which, in our computer platform, 1 + e0 > 1).\n3. Half-and-Half: p` = 1/k + e/k for 1 \u2264 ` \u2264 k/2, and p` = 1/k\u2212 e/k for k/2 + 1 \u2264 ` \u2264 k, with e \u2208 {0.1, 0.3, 0.5, 0.8}.\nThese probability functions are illustrated, for k = 6 and e = 0.3, in Figure 1. We studied the behavior of the Shannon entropy, the R\u00e9nyi entropy with q \u2208 {1/3, 2/5}, the Tsallis entropy with q \u2208 {1/2, 3/2}, and the Fisher information computed on samples of sizes n \u2208 {102k, 103k, 104k}. We used 300 independent samples (replicates).\nAlthough Equation (35) shows that the R\u00e9nyi entropy is not asymptotically normal, we verified that its density is similar to that of a Gaussian distribution. With this in mind, we also checked of the normality of R\u00e9nyi entropies. We used the Anderson\u2013Darling test to verify the null hypothesis that the data follow a normal distribution. We chose this test because it uses the hypothesized distribution in calculating critical values. This test is more sensitive than other alternatives; see, for instance, the book by Lehman and Romano [14]. From Table 1, we notice that the Fisher information is the one that fails most times to pass the normality test at the 1 %. The situation that appears with p-value = 0.0010 in the table has, in fact, p-value = 9.606 130\u00d7 10\u22123; the table shows rounded values. Figure 2 shows four of these cases, namely for k = 6, n = 600, and e = 0.1, 0.3, 0.5, 0.8. We notice that the deviation from the normal hypothesis is more prevalent in both tails, being that the observations are larger than the theoretical quantiles.\nThe normality hypothesis was rejected at the 1% level by the Anderson\u2013Darling test in only 24 out of 432 situations, showing that the asymptotic Gaussian model for the entropies is a good description for these data. Table 1 shows those situations.\nWith the aim to assess the goodness of fit of the asymptotic models, we applied the Kolmogorov\u2013Smirnov test to fifty replicates of samples. Table 2 shows the results where the p-value of the test is at least equal to 0.05.\nIt is worth noticing that even in those cases where the p-value is lesser than 0.05, the asymptotic models are a good fit to the data as can be seen in several examples exhibited in Figure 3. The Fisher information shows the worst fitting. Additionally, notice in Figure 3d that, although the asymptotic distribution of the R\u00e9nyi entropy is not normal, the probability density function is visually very close to the Gaussian model. We verified this similarity in all the cases we considered."
        },
        {
            "heading": "5. Application",
            "text": "Inspired by an example from Agresti [16] (p. 200), we extracted data from the General Social Survey (GSS, a project of the independent research organization NORC at the University of Chicago, with principal funding from the National Science Foundation, available at https://gss.norc.org/. The data were downloaded on 24 December 2022). Table 3 shows the level of agreement to the assertion \u201cReligious people are often too intolerant\u201d as measured in three years.\nThe p-values of pairwise \u03c72 tests for the null hypotheses that the underlying probabilities are equal are"
        },
        {
            "heading": "1998 and 2008: 3.43\u00d7 10\u221222,",
            "text": ""
        },
        {
            "heading": "1998 and 2018: 2.01\u00d7 10\u22128,",
            "text": "2008 and 2018: 1.06\u00d7 10\u22123.\nOn the one hand, these values attest that 1998 and 2008 and 1998 and 2018 are very different. On the other hand, although significant, the change between 2008 and 2018 is not so significant. Table 4 shows the asymptotic mean and variance (in entropies normalized units) of the entropies of the proportions reported in Table 3.\nWe perform the same hypothesis test with the asymptotic quantities presented in Table 4. Table 5 shows the p-values of the null hypothesis that the entropies are equal, using the test discussed by Chagas et al. [8] (Section 5):\np-value \u2248 2 ( 1\u2212\u03a6 ( \u2223\u2223H(p\u03021)\u2212 H(p\u03022)\u2223\u2223\u221a\n\u03c3\u03022n1,p\u03021 + \u03c3\u0302 2 n2,p\u03022\n)) , (37)\nwhere \u03a6 is the cumulative distribution function of a standard normal random variable, H is any of the considered entropies computed with the observed proportions p\u0302i, i = 1, 2, and \u03c3\u03022ni ,p\u0302i is the corresponding sample asymptotic variance that takes into account the sample size ni. Notice that the test based on entropies compares only these features, and not the underlying distribution.\nTable 7 presents the p-values of the tests that verify the null hypothesis of the same entropy between the collapsed 1998 data (three categories), and 2008 and 2018 (five categories). These results agree with those presented in Table 5. Such an agreement suggests that, although the number of categories was reduced in 1998 from five to three, the tests based on entropies cope with the loss of information."
        },
        {
            "heading": "6. Conclusions",
            "text": "We presented expressions for the asymptotic distribution of the R\u00e9nyi and Tsallis entropies of order q, and Fisher information. The Fisher information and the Tsallis and Shannon entropies have limit normal distribution with means and variances that depend on the underlying probability of patterns and the number of patterns. The R\u00e9nyi entropy follows, asymptotically, a different distribution, cf. (35), but a Gaussian law can well approximate it. Those expressions pose no numerical challenges other than setting 0 log 0 .= 0. We verified that these asymptotic distributions are good models for data arising from both simulations with a variety of models and from the analysis of actual data. On the one hand, the Fisher information is the one that fails more frequently to pass the Anderson\u2013Darling normality tests. On the other hand, it does not provide evidence to reject the same hypothesis under the One-Almost-Zero model. The distributions we present here can be used for building test statistics, as discussed by Chagas et al. [8]. Moreover, Equation (37) allows performing tests with mixed types of distributions, a situation that may appear in Internet of Things applications, in which, citing Borges et al. [17], one has to deal with \u201clarge time series data generated at different rates, of different types and magnitudes, possibly having issues concerning uncertainty, inconsistency, and incompleteness due to missing readings and sensor failures.\u201d\nAuthor Contributions: Conceptualization, A.A.R., A.C.F., J.G.; methodology, A.A.R., A.C.F., M.L., J.G., H.S.R.; software, A.A.R., A.C.F., M.L., J.G., E.T.C.C.; validation, A.A.R., A.C.F., M.L., J.G., E.T.C.C., H.S.R.; formal analysis, A.A.R., A.C.F., M.L., J.G., E.T.C.C., H.S.R.; investigation, A.A.R., A.C.F., M.L., J.G., E.T.C.C., H.S.R.; resources, A.C.F., H.S.R.; data curation, A.A.R., A.C.F., M.L., J.G., E.T.C.C., H.S.R.; writing\u2014original draft preparation, A.A.R., A.C.F., M.L., J.G., E.T.C.C.; writing\u2014review and editing, A.A.R., A.C.F., M.L., J.G., E.T.C.C., H.S.R.; visualization, A.A.R., A.C.F., M.L., J.G., E.T.C.C.; supervision, A.C.F., H.S.R.; project administration, A.A.R., A.C.F., J.G.; funding acquisition, A.C.F., H.S.R. All authors have read and agreed to the published version of the manuscript.\nFunding: This research and the APC were funded by Project 410695 from Victoria University of Wellington. It was also partially funded by Project 2020/05121-4 from Funda\u00e7\u00e3o de Amparo \u00e0\nPesquisa do Estado de S\u00e3o Paulo (FAPESP), and project APQ-00426-22 from Funda\u00e7\u00e3o de Amparo \u00e0 Pesquisa do Estado de Minas Gerais (FAPEMIG).\nInstitutional Review Board Statement: Not applicable.\nData Availability Statement: Not applicable.\nConflicts of Interest: The authors declare no conflicts of interest.\nNotation The following notation is used in this manuscript:\np vector of probabilities p\u2032 the transpose of p p\u0302 an estimator of p N multivariate discrete random variable n a sample from N HS Shannon entropy HqT Tsallis entropy of order q HqR R\u00e9nyi entropy of order q HF Fisher information measure \u03a3 covariance matrix"
        },
        {
            "heading": "Appendix A",
            "text": ""
        },
        {
            "heading": "Appendix A.1. Matrix Operations",
            "text": "Consider the real matrix M \u2208Rk\u00d7k, and denote as M\u2032 its transpose. If D = Diag (d1, d2, . . . , dk) \u2208 Rk\u00d7k, then\n(DMD\u2032)ij = k\n\u2211 r=1\n(DM)irD\u2032rj = k\n\u2211 r=1\nk\n\u2211 s=1\nDis MsrD\u2032rj\n= Dii MijDjj = { d2i Mii if i = j, didj Mij if i 6= j.\n(A1)\nWe consider now\nB =  b11 b12 0 0 \u00b7 \u00b7 \u00b7 0 0 0 b22 b23 0 \u00b7 \u00b7 \u00b7 0 0 ... ... ... ... ... ... ...\n0 0 0 0 \u00b7 \u00b7 \u00b7 bk\u22121,k bkk 0 0 0 0 \u00b7 \u00b7 \u00b7 0 bkk\n \u2208 Rk\u00d7k.\nAnalogous to the computation in Equation (A1), it can be seen that\n(BMB\u2032)ij = k\n\u2211 r=1\nk\n\u2211 s=1\nBis MsrB\u2032rj = k\n\u2211 r=1\nk\n\u2211 s=1 Bis MsrBjr. (A2)\nDue to the form of B, if i, j = 1, 2, . . . , k\u2212 1 then\n(BMB\u2032)ij = k\n\u2211 r=1 (Bii Mir + Bi,i+1Mi+1,r)Bjr\n= (Bii Mij + Bi,i+1Mi+1,j)Bjj + (Bii Mi,j+1 + Bi,i+1Mi+1,j+1)Bj,j+1. (A3)\nIf i = k, replacing in Equation (A2),\n(BMB\u2032)kj = k\n\u2211 r=1 Bkk MkrBjr = { Bkk(MkjBjj + Mk,j+1Bj,j+1) if j 6= k, B2kk Mkk if j = k.\n(A4)"
        },
        {
            "heading": "Appendix A.2. Ordinal Patterns",
            "text": "Symbolic data analysis [18] encompasses methods that study the statistical properties of data aggregated by criteria that meet some scientific question. Such methods have attracted lots of attention because they present competitive results in many data analysis applications [17,19,20]. Ordinal patterns [21] belong to this class of techniques. They impose low computational complexity and are inherently robust. This approach consists of constructing a set of symbolic ordinal patterns based on intrinsic data characteristics without any prior model. Ordinal patterns often reveal and quantify the underlying time series dynamics. In spite of their successful application to biomedicine, economics, mechanics and electronics engineering, image analysis and remote sensing, to name a few (see, for instance, Refs. [20,22,23]), little is known about the statistical properties of the features they induce. One of these features is entropy, in its several forms. Signal analysis with ordinal patterns requires coding D observations into k = D! categories, in which D is typically small [8,20,24]. Motivated by these applications, we chose k \u2208 {6, 24, 120, 720}, which allows checking results in various categories. Bear in mind that, when using ordinal patterns, the subsequent patterns are not independent and, thus, the multinomial distribution is an approximation."
        },
        {
            "heading": "Appendix A.3. Computational Information",
            "text": "This article was written in Rmarkdown and is fully reproducible. We used RStudio version 2022.07.2 and R version 4.2.1. The code and data are available at https://gitlab.ecs.vuw.ac.nz/ freryal/asymptotic-distribution-of-various-types-of-entropyunder-the-multinomial-law, accessed on 26 April 2023.\nReferences 1. Johnson, N.L.; Kotz, S.; Balakrishnan, N. Discrete Multivariate Distributions; Wiley-Interscience: Hoboken, NJ, USA, 1997. 2. Modis, T. Links between entropy, complexity, and the technological singularity. Technol. Forecast. Soc. Chang. 2022, 176, 121457. [CrossRef] 3. Hutcheson, K. A test for comparing diversities based on the Shannon formula. J. Theor. Biol. 1970, 29, 151\u2013154. [CrossRef] [PubMed] 4. Hutcheson, K.; Shenton, L.R. Some moments of an estimate of Shannon's measure of information. Commun. Stat. Theory Methods 1974, 3, 89\u201394. [CrossRef] 5. Jacquet, P.; Szpankowski, W. Entropy computations via analytic depoissonization. IEEE Trans. Inf. Theory 1999, 45, 1072\u20131081. [CrossRef] 6. Cichon\u0301, J.; Gole\u0328biewski, Z. On Bernoulli Sums and Bernstein Polynomials. In Proceedings of the 23rd International Meeting on\nProbabilistic, Combinatorial, and Asymptotic Methods in the Analysis of Algorithms, Montreal, QC, Canada, 18\u201322 June 2012; pp. 179\u2013190.\n7. Cook, G.W.; Kerridge, D.F.; Pryce, J.D. Estimations of Functions of a Binomial Parameter. Sankhya\u0304 Indian J. Stat. Ser. A 1974, 36, 443\u2013448. 8. Chagas, E.T.C.; Frery, A.C.; Gambini, J.; Lucini, M.M.; Ramos, H.S.; Rey, A.A. Statistical Properties of the Entropy from Ordinal Patterns. Chaos Interdiscip. J. Nonlinear Sci. 2022, 32, 113118. [CrossRef] [PubMed] 9. Tsallis, C. Possible generalization of Boltzmann-Gibbs statistics. J. Stat. Phys. 1988, 52, 479\u2013487. [CrossRef] 10. R\u00e9nyi, A. On Measures of Entropy and Information. In Proceedings of the 4th Berkeley Symposium on Mathematical Statistics and Probability, Berkeley, CA, USA, 20 June\u201330 July 1961; Volume 1, pp. 547\u2013561. 11. Frieden, B.R. Science from Fisher Information: A Unification; Cambridge University Press: Cambridge, UK, 2010. 12. S\u00e1nchez-Moreno, P.; Y\u00e1\u00f1ez, R.J.; Dehesa, J.S. Discrete densities and Fisher information. In Proceedings of the 14th International Conference on Difference Equations and Applications, Istanbul, Turkey, 19\u201323 October 2009; pp. 291\u2013298. 13. Lehmann, E.L.; Casella, G. Theory of Point Estimation; Springer Science & Business Media: Berlin/Heidelberg, Germany, 2006. 14. Lehman, E.L.; Romano, J.P. Testing Statistical Hypothesis, 3rd ed.; Springer: Berlin/Heidelberg, Germany, 2005. 15. Freedman, D.; Diaconis, P. On the histogram as a density estimator: L2 theory. Zeitschrift f\u00fcr Wahrscheinlichkeitstheorie und\nVerwandte Gebiete 1981, 57, 453\u2013476. [CrossRef]\n16. Agresti, A. An Introduction to Categorical Data Analysis; Wiley-Interscience: Hoboken, NJ, USA, 2007. 17. Borges, J.B.; Ramos, H.S.; Loureiro, A.A.F. A Classification Strategy for Internet of Things Data Based on the Class Separability Analysis of Time Series Dynamics. ACM Trans. Internet Things 2022, 3, 1\u201330. [CrossRef] 18. Beranger, B.; Lin, H.; Sisson, S. New models for symbolic data analysis. Adv. Data Anal. Classif. 2022, 1\u201341. [CrossRef] 19. Borges, J.B.; Medeiros, J.P.S.; Barbosa, L.P.A.; Ramos, H.S.; Loureiro, A.A. IoT Botnet Detection based on Anomalies of Multiscale Time Series Dynamics. IEEE Trans. Knowl. Data Eng. 2022, Early Access. [CrossRef] 20. Chagas, E.T.C.; Frery, A.C.; Rosso, O.A.; Ramos, H.S. Analysis and Classification of SAR Textures using Information Theory. IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. 2021, 14, 663\u2013675. [CrossRef] 21. Bandt, C.; Pompe, B. Permutation entropy: A natural complexity measure for time series. Phys. Rev. Lett. 2002, 88, 174102. [CrossRef] [PubMed] 22. Zanin, M.; Zunino, L.; Rosso, O.A.; Papo, D. Permutation Entropy and Its Main Biomedical and Econophysics Applications: A Review. Entropy 2012, 14, 1553\u20131577. [CrossRef] 23. Sigaki, H.Y.D.; Perc, M.; Ribeiro, H.V. History of art paintings through the lens of entropy and complexity. Proc. Natl. Acad. Sci. USA 2018, 115, E8585\u2013E8594. [CrossRef] [PubMed] 24. Chagas, E.T.C.; Queiroz-Oliveira, M.; Rosso, O.A.; Ramos, H.S.; Freitas, C.G.S.; Frery, A.C. White Noise Test from Ordinal Patterns\nin the Entropy-Complexity Plane. Int. Stat. Rev. 2022, 90, 374\u2013396. [CrossRef]\nDisclaimer/Publisher\u2019s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content."
        }
    ],
    "title": "Asymptotic Distribution of Certain Types of Entropy under the Multinomial Law",
    "year": 2023
}