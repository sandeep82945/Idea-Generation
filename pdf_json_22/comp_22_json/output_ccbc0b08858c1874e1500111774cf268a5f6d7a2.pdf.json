{
    "abstractText": "As an important fine-grained sentiment analysis problem, aspect-based sentiment analysis (ABSA), aiming to analyze and understand people\u2019s opinions at the aspect level, has been attracting considerable interest in the last decade. To handle ABSA in different scenarios, various tasks are introduced for analyzing different sentiment elements and their relations, including the aspect term, aspect category, opinion term, and sentiment polarity. Unlike early ABSA works focusing on a single sentiment element, many compound ABSA tasks involving multiple elements have been studied in recent years for capturing more complete aspect-level sentiment information. However, a systematic review of various ABSA tasks and their corresponding solutions is still lacking, which we aim to fill in this survey. More specifically, we provide a new taxonomy for ABSA which organizes existing studies from the axes of concerned sentiment elements, with an emphasis on recent advances of compound ABSA tasks. From the perspective of solutions, we summarize the utilization of pre-trained language models for ABSA, which improved the performance of ABSA to a new stage. Besides, techniques for building more practical ABSA systems in cross-domain/lingual scenarios are discussed. Finally, we review some emerging topics and discuss some open challenges to outlook potential future directions of ABSA.",
    "authors": [
        {
            "affiliations": [],
            "name": "Wenxuan Zhang"
        },
        {
            "affiliations": [],
            "name": "Xin Li"
        },
        {
            "affiliations": [],
            "name": "Yang Deng"
        },
        {
            "affiliations": [],
            "name": "Lidong Bing"
        },
        {
            "affiliations": [],
            "name": "Wai Lam"
        }
    ],
    "id": "SP:487d6497eaec0d0817f9860fa5fcc6e1c31cbb73",
    "references": [
        {
            "authors": [
                "B. Liu"
            ],
            "title": "Sentiment analysis and opinion mining",
            "venue": "Synthesis lectures on human language technologies, vol. 5, no. 1, pp. 1\u2013167, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "P.D. Turney"
            ],
            "title": "Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews",
            "venue": "ACL, 2002, pp. 417\u2013424.",
            "year": 2002
        },
        {
            "authors": [
                "B. Pang",
                "L. Lee",
                "S. Vaithyanathan"
            ],
            "title": "Thumbs up? sentiment classification using machine learning techniques",
            "venue": "EMNLP, 2002, pp. 79\u201386.",
            "year": 2002
        },
        {
            "authors": [
                "H. Yu",
                "V. Hatzivassiloglou"
            ],
            "title": "Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences",
            "venue": "EMNLP, 2003, pp. 129\u2013136.",
            "year": 2003
        },
        {
            "authors": [
                "K. Schouten",
                "F. Frasincar"
            ],
            "title": "Survey on aspect-level sentiment analysis",
            "venue": "IEEE Trans. Knowl. Data Eng., vol. 28, no. 3, pp. 813\u2013830, 2016. 16",
            "year": 2016
        },
        {
            "authors": [
                "A. Nazir",
                "Y. Rao",
                "L. Wu",
                "L. Sun"
            ],
            "title": "Issues and challenges of aspect-based sentiment analysis: A comprehensive survey",
            "venue": "IEEE Trans. Affect. Comput., 2020.",
            "year": 2020
        },
        {
            "authors": [
                "W. Zhang",
                "Y. Deng",
                "X. Li",
                "Y. Yuan",
                "L. Bing",
                "W. Lam"
            ],
            "title": "Aspect sentiment quad prediction as paraphrase generation",
            "venue": "EMNLP, 2021, pp. 9209\u20139219.",
            "year": 2021
        },
        {
            "authors": [
                "P. Liu",
                "S.R. Joty",
                "H.M. Meng"
            ],
            "title": "Fine-grained opinion mining with recurrent neural networks and word embeddings",
            "venue": "EMNLP, 2015, pp. 1433\u20131443.",
            "year": 2015
        },
        {
            "authors": [
                "L. Jiang",
                "M. Yu",
                "M. Zhou",
                "X. Liu",
                "T. Zhao"
            ],
            "title": "Targetdependent twitter sentiment classification",
            "venue": "ACL, 2011, pp. 151\u2013160.",
            "year": 2011
        },
        {
            "authors": [
                "M. Mitchell",
                "J. Aguilar",
                "T. Wilson",
                "B.V. Durme"
            ],
            "title": "Open domain targeted sentiment",
            "venue": "EMNLP, 2013, pp. 1643\u20131654.",
            "year": 2013
        },
        {
            "authors": [
                "S. Chen",
                "J. Liu",
                "Y. Wang",
                "W. Zhang",
                "Z. Chi"
            ],
            "title": "Synchronous double-channel recurrent network for aspect-opinion pair extraction",
            "venue": "ACL, 2020, pp. 6515\u20136524.",
            "year": 2020
        },
        {
            "authors": [
                "H. Peng",
                "L. Xu",
                "L. Bing",
                "F. Huang",
                "W. Lu",
                "L. Si"
            ],
            "title": "Knowing what, how and why: A near complete solution for aspect-based sentiment analysis",
            "venue": "AAAI, 2020, pp. 8600\u20138607.",
            "year": 2020
        },
        {
            "authors": [
                "H. Zhao",
                "L. Huang",
                "R. Zhang",
                "Q. Lu",
                "H. Xue"
            ],
            "title": "Spanmlt: A span-based multi-task learning framework for pair-wise aspect and opinion terms extraction",
            "venue": "ACL, 2020, pp. 3239\u20133248.",
            "year": 2020
        },
        {
            "authors": [
                "J. Zhou",
                "J.X. Huang",
                "Q. Chen",
                "Q.V. Hu",
                "T. Wang",
                "L. He"
            ],
            "title": "Deep learning for aspect-level sentiment classification: Survey, vision, and challenges",
            "venue": "IEEE Access, vol. 7, pp. 78 454\u201378 483, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "H. Liu",
                "I. Chatterjee",
                "M. Zhou",
                "X.S. Lu",
                "A. Abusorrah"
            ],
            "title": "Aspect-based sentiment analysis: A survey of deep learning methods",
            "venue": "IEEE Trans. Comput. Soc. Syst., vol. 7, no. 6, pp. 1358\u20131375, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "S. Poria",
                "D. Hazarika",
                "N. Majumder",
                "R. Mihalcea"
            ],
            "title": "Beneath the tip of the iceberg: Current challenges and new directions in sentiment analysis research",
            "venue": "IEEE Trans. Affect. Comput., 2020.",
            "year": 2020
        },
        {
            "authors": [
                "J. Devlin",
                "M. Chang",
                "K. Lee",
                "K. Toutanova"
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "NAACL-HLT, 2019, pp. 4171\u20134186.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Liu",
                "M. Ott",
                "N. Goyal",
                "J. Du",
                "M. Joshi",
                "D. Chen",
                "O. Levy",
                "M. Lewis",
                "L. Zettlemoyer",
                "V. Stoyanov"
            ],
            "title": "Roberta: A robustly optimized BERT pretraining approach",
            "venue": "CoRR, vol. abs/1907.11692, 2019.",
            "year": 1907
        },
        {
            "authors": [
                "X. Li",
                "L. Bing",
                "W. Zhang",
                "W. Lam"
            ],
            "title": "Exploiting BERT for end-to-end aspect-based sentiment analysis",
            "venue": "W-NUT, 2019, pp. 34\u201341.",
            "year": 2019
        },
        {
            "authors": [
                "S.J. Pan",
                "Q. Yang"
            ],
            "title": "A survey on transfer learning",
            "venue": "IEEE Trans. Knowl. Data Eng., vol. 22, no. 10, pp. 1345\u2013 1359, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "S. Ruder",
                "I. Vulic",
                "A. S\u00f8gaard"
            ],
            "title": "A survey of crosslingual word embedding models",
            "venue": "J. Artif. Intell. Res., vol. 65, pp. 569\u2013631, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Yin",
                "F. Wei",
                "L. Dong",
                "K. Xu",
                "M. Zhang",
                "M. Zhou"
            ],
            "title": "Unsupervised word and dependency path embeddings for aspect term extraction",
            "venue": "IJCAI, 2016, pp. 2979\u20132985.",
            "year": 2016
        },
        {
            "authors": [
                "W. Wang",
                "S.J. Pan",
                "D. Dahlmeier",
                "X. Xiao"
            ],
            "title": "Recursive neural conditional random fields for aspect-based sentiment analysis",
            "venue": "EMNLP, 2016, pp. 616\u2013626.",
            "year": 2016
        },
        {
            "authors": [
                "X. Li",
                "W. Lam"
            ],
            "title": "Deep multi-task learning for aspect term extraction with memory interaction",
            "venue": "EMNLP, 2017, pp. 2886\u20132892.",
            "year": 2017
        },
        {
            "authors": [
                "W. Wang",
                "S.J. Pan",
                "D. Dahlmeier",
                "X. Xiao"
            ],
            "title": "Coupled multi-layer attentions for co-extraction of aspect and opinion terms",
            "venue": "AAAI, 2017, pp. 3316\u20133322.",
            "year": 2017
        },
        {
            "authors": [
                "X. Li",
                "L. Bing",
                "P. Li",
                "W. Lam",
                "Z. Yang"
            ],
            "title": "Aspect term extraction with history attention and selective transformation",
            "venue": "IJCAI, 2018, pp. 4194\u20134200.",
            "year": 2018
        },
        {
            "authors": [
                "H. Xu",
                "B. Liu",
                "L. Shu",
                "P.S. Yu"
            ],
            "title": "Double embeddings and cnn-based sequence labeling for aspect extraction",
            "venue": "ACL, 2018, pp. 592\u2013598.",
            "year": 2018
        },
        {
            "authors": [
                "D. Ma",
                "S. Li",
                "F. Wu",
                "X. Xie",
                "H. Wang"
            ],
            "title": "Exploring sequence-to-sequence learning in aspect term extraction",
            "venue": "ACL, 2019, pp. 3538\u20133547.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Yang",
                "K. Li",
                "X. Quan",
                "W. Shen",
                "Q. Su"
            ],
            "title": "Constituency lattice encoding for aspect term extraction",
            "venue": "COLING, 2020, pp. 844\u2013855.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Yin",
                "C. Wang",
                "M. Zhang"
            ],
            "title": "PoD: Positional dependency-based word embedding for aspect term extraction",
            "venue": "COLING, 2020, pp. 1714\u20131719.",
            "year": 2020
        },
        {
            "authors": [
                "K. Li",
                "C. Chen",
                "X. Quan",
                "Q. Ling",
                "Y. Song"
            ],
            "title": "Conditional augmentation for aspect term extraction via masked sequence-to-sequence generation",
            "venue": "ACL, 2020, pp. 7056\u20137066.",
            "year": 2020
        },
        {
            "authors": [
                "Z. Chen",
                "T. Qian"
            ],
            "title": "Enhancing aspect term extraction with soft prototypes",
            "venue": "EMNLP, 2020, pp. 2107\u2013 2117.",
            "year": 2020
        },
        {
            "authors": [
                "Q. Wang",
                "Z. Wen",
                "Q. Zhao",
                "M. Yang",
                "R. Xu"
            ],
            "title": "Progressive self-training with discriminator for aspect term extraction",
            "venue": "EMNLP, 2021, pp. 257\u2013268.",
            "year": 2021
        },
        {
            "authors": [
                "R. He",
                "W.S. Lee",
                "H.T. Ng",
                "D. Dahlmeier"
            ],
            "title": "An unsupervised neural attention model for aspect extraction",
            "venue": "ACL, 2017, pp. 388\u2013397.",
            "year": 2017
        },
        {
            "authors": [
                "L. Luo",
                "X. Ao",
                "Y. Song",
                "J. Li",
                "X. Yang",
                "Q. He",
                "D. Yu"
            ],
            "title": "Unsupervised neural aspect extraction with sememes",
            "venue": "IJCAI, 2019, pp. 5123\u20135129.",
            "year": 2019
        },
        {
            "authors": [
                "M. Liao",
                "J. Li",
                "H. Zhang",
                "L. Wang",
                "X. Wu",
                "K.-F. Wong"
            ],
            "title": "Coupling global and local context for unsupervised aspect extraction",
            "venue": "EMNLP-IJCNLP, 2019, pp. 4579\u20134589.",
            "year": 2019
        },
        {
            "authors": [
                "X. Zhou",
                "X. Wan",
                "J. Xiao"
            ],
            "title": "Representation learning for aspect category detection in online reviews",
            "venue": "AAAI, 2015, pp. 417\u2013424.",
            "year": 2015
        },
        {
            "authors": [
                "S. Movahedi",
                "E. Ghadery",
                "H. Faili",
                "A. Shakery"
            ],
            "title": "Aspect category detection via topic-attention network",
            "venue": "CoRR, vol. abs/1901.01183, 2019.",
            "year": 1901
        },
        {
            "authors": [
                "E. Ghadery",
                "S. Movahedi",
                "M.J. Sabet",
                "H. Faili",
                "A. Shakery"
            ],
            "title": "LICD: A language-independent approach for aspect category detection",
            "venue": "ECIR, 2019, pp. 575\u2013589.",
            "year": 2019
        },
        {
            "authors": [
                "M. Hu",
                "S. Zhao",
                "H. Guo",
                "C. Xue",
                "H. Gao",
                "T. Gao",
                "R. Cheng",
                "Z. Su"
            ],
            "title": "Multi-label few-shot learning for aspect category detection",
            "venue": "ACL-IJCNLP, 2021, pp. 6330\u20136340.",
            "year": 2021
        },
        {
            "authors": [
                "S. Tulkens",
                "A. van Cranenburgh"
            ],
            "title": "Embarrassingly simple unsupervised aspect extraction",
            "venue": "ACL, 2020, pp. 3182\u20133187.",
            "year": 2020
        },
        {
            "authors": [
                "T. Shi",
                "L. Li",
                "P. Wang",
                "C.K. Reddy"
            ],
            "title": "A simple and 17 effective self-supervised contrastive learning framework for aspect detection",
            "venue": "AAAI, 2021, pp. 13 815\u2013 13 824.",
            "year": 2021
        },
        {
            "authors": [
                "J. Yu",
                "J. Jiang",
                "R. Xia"
            ],
            "title": "Global inference for aspect and opinion terms co-extraction based on multi-task neural networks",
            "venue": "IEEE ACM Trans. Audio Speech Lang. Process., vol. 27, no. 1, pp. 168\u2013177, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "M. Wu",
                "W. Wang",
                "S.J. Pan"
            ],
            "title": "Deep weighted maxsat for aspect-based opinion extraction",
            "venue": "EMNLP, 2020, pp. 5618\u20135628.",
            "year": 2020
        },
        {
            "authors": [
                "Z. Fan",
                "Z. Wu",
                "X. Dai",
                "S. Huang",
                "J. Chen"
            ],
            "title": "Targetoriented opinion words extraction with target-fused neural sequence labeling",
            "venue": "NAACL-HLT, 2019, pp. 2509\u20132518.",
            "year": 2019
        },
        {
            "authors": [
                "H. Wan",
                "Y. Yang",
                "J. Du",
                "Y. Liu",
                "K. Qi",
                "J.Z. Pan"
            ],
            "title": "Target-aspect-sentiment joint detection for aspectbased sentiment analysis",
            "venue": "AAAI, 2020, pp. 9122\u2013 9129.",
            "year": 2020
        },
        {
            "authors": [
                "A.P.B. Veyseh",
                "N. Nouri",
                "F. Dernoncourt",
                "D. Dou",
                "T.H. Nguyen"
            ],
            "title": "Introducing syntactic structures into target opinion word extraction with deep learning",
            "venue": "EMNLP, 2020, pp. 8947\u20138956.",
            "year": 2020
        },
        {
            "authors": [
                "S. Mensah",
                "K. Sun",
                "N. Aletras"
            ],
            "title": "An empirical study on leveraging position embeddings for targetoriented opinion words extraction",
            "venue": "EMNLP, 2021, pp. 9174\u20139179.",
            "year": 2021
        },
        {
            "authors": [
                "L. Dong",
                "F. Wei",
                "C. Tan",
                "D. Tang",
                "M. Zhou",
                "K. Xu"
            ],
            "title": "Adaptive recursive neural network for targetdependent twitter sentiment classification",
            "venue": "ACL, 2014, pp. 49\u201354.",
            "year": 2014
        },
        {
            "authors": [
                "D. Vo",
                "Y. Zhang"
            ],
            "title": "Target-dependent twitter sentiment classification with rich automatic features",
            "venue": "IJCAI, 2015, pp. 1347\u20131353.",
            "year": 2015
        },
        {
            "authors": [
                "D. Tang",
                "B. Qin",
                "X. Feng",
                "T. Liu"
            ],
            "title": "Effective lstms for target-dependent sentiment classification",
            "venue": "COLING, 2016, pp. 3298\u20133307.",
            "year": 2016
        },
        {
            "authors": [
                "Y. Wang",
                "M. Huang",
                "X. Zhu",
                "L. Zhao"
            ],
            "title": "Attentionbased LSTM for aspect-level sentiment classification",
            "venue": "EMNLP, 2016, pp. 606\u2013615.",
            "year": 2016
        },
        {
            "authors": [
                "D. Tang",
                "B. Qin",
                "T. Liu"
            ],
            "title": "Aspect level sentiment classification with deep memory network",
            "venue": "EMNLP, 2016, pp. 214\u2013224.",
            "year": 2016
        },
        {
            "authors": [
                "D. Ma",
                "S. Li",
                "X. Zhang",
                "H. Wang"
            ],
            "title": "Interactive attention networks for aspect-level sentiment classification",
            "venue": "IJCAI, 2017, pp. 4068\u20134074.",
            "year": 2017
        },
        {
            "authors": [
                "J. Liu",
                "Y. Zhang"
            ],
            "title": "Attention modeling for targeted sentiment",
            "venue": "EACL, 2017, pp. 572\u2013577.",
            "year": 2017
        },
        {
            "authors": [
                "P. Chen",
                "Z. Sun",
                "L. Bing",
                "W. Yang"
            ],
            "title": "Recurrent attention network on memory for aspect sentiment analysis",
            "venue": "EMNLP, 2017, pp. 452\u2013461.",
            "year": 2017
        },
        {
            "authors": [
                "J. Cheng",
                "S. Zhao",
                "J. Zhang",
                "I. King",
                "X. Zhang",
                "H. Wang"
            ],
            "title": "Aspect-level sentiment classification with HEAT (hierarchical attention) network",
            "venue": "ACM CIKM, 2017, pp. 97\u2013106.",
            "year": 2017
        },
        {
            "authors": [
                "W. Xue",
                "T. Li"
            ],
            "title": "Aspect based sentiment analysis with gated convolutional networks",
            "venue": "ACL, 2018, pp. 2514\u20132523.",
            "year": 2018
        },
        {
            "authors": [
                "X. Li",
                "L. Bing",
                "W. Lam",
                "B. Shi"
            ],
            "title": "Transformation networks for target-oriented sentiment classification",
            "venue": "ACL, 2018, pp. 946\u2013956.",
            "year": 2018
        },
        {
            "authors": [
                "F. Fan",
                "Y. Feng",
                "D. Zhao"
            ],
            "title": "Multi-grained attention network for aspect-level sentiment classification",
            "venue": "EMNLP, 2018, pp. 3433\u20133442.",
            "year": 2018
        },
        {
            "authors": [
                "C. Zhang",
                "Q. Li",
                "D. Song"
            ],
            "title": "Syntax-aware aspect-level sentiment classification with proximityweighted convolution network",
            "venue": "SIGIR, 2019, pp. 1145\u20131148.",
            "year": 2019
        },
        {
            "authors": [
                "H. Xu",
                "B. Liu",
                "L. Shu",
                "P. Yu"
            ],
            "title": "BERT post-training for review reading comprehension and aspect-based sentiment analysis",
            "venue": "NAACL-HLT, 2019, pp. 2324\u2013 2335.",
            "year": 2019
        },
        {
            "authors": [
                "Z. Chen",
                "T. Qian"
            ],
            "title": "Transfer capsule network for aspect level sentiment classification",
            "venue": "ACL, 2019, pp. 547\u2013556.",
            "year": 2019
        },
        {
            "authors": [
                "C. Du",
                "H. Sun",
                "J. Wang",
                "Q. Qi",
                "J. Liao",
                "T. Xu",
                "M. Liu"
            ],
            "title": "Capsule network with interactive attention for aspect-level sentiment classification",
            "venue": "EMNLP- IJCNLP, 2019, pp. 5489\u20135498.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Liang",
                "F. Meng",
                "J. Zhang",
                "J. Xu",
                "Y. Chen",
                "J. Zhou"
            ],
            "title": "A novel aspect-guided deep transition model for aspect based sentiment analysis",
            "venue": "EMNLP-IJCNLP, 2019, pp. 5569\u20135580.",
            "year": 2019
        },
        {
            "authors": [
                "C. Zhang",
                "Q. Li",
                "D. Song"
            ],
            "title": "Aspect-based sentiment classification with aspect-specific graph convolutional networks",
            "venue": "EMNLP-IJCNLP, 2019, pp. 4568\u20134578.",
            "year": 2019
        },
        {
            "authors": [
                "K. Sun",
                "R. Zhang",
                "S. Mensah",
                "Y. Mao",
                "X. Liu"
            ],
            "title": "Aspect-level sentiment analysis via convolution over dependency tree",
            "venue": "EMNLP-IJCNLP, 2019, pp. 5678\u2013 5687.",
            "year": 2019
        },
        {
            "authors": [
                "H. Tang",
                "D. Ji",
                "C. Li",
                "Q. Zhou"
            ],
            "title": "Dependency graph enhanced dual-transformer structure for aspect-based sentiment classification",
            "venue": "ACL, 2020, pp. 6578\u20136588.",
            "year": 2020
        },
        {
            "authors": [
                "C. Chen",
                "Z. Teng",
                "Y. Zhang"
            ],
            "title": "Inducing targetspecific latent structures for aspect sentiment classification",
            "venue": "EMNLP, 2020, pp. 5596\u20135607.",
            "year": 2020
        },
        {
            "authors": [
                "L. Xu",
                "L. Bing",
                "W. Lu",
                "F. Huang"
            ],
            "title": "Aspect sentiment classification with aspect-specific opinion spans",
            "venue": "EMNLP, 2020, pp. 3561\u20133567.",
            "year": 2020
        },
        {
            "authors": [
                "X. Hou",
                "P. Qi",
                "G. Wang",
                "R. Ying",
                "J. Huang",
                "X. He",
                "B. Zhou"
            ],
            "title": "Graph ensemble learning over multiple dependency trees for aspect-level sentiment classification",
            "venue": "NAACL-HLT, 2021, pp. 2884\u20132894.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Tian",
                "G. Chen",
                "Y. Song"
            ],
            "title": "Aspect-based sentiment analysis with type-aware graph convolutional networks and layer ensemble",
            "venue": "NAACL-HLT, 2021, pp. 2910\u20132922.",
            "year": 2021
        },
        {
            "authors": [
                "R. Li",
                "H. Chen",
                "F. Feng",
                "Z. Ma",
                "X. Wang",
                "E.H. Hovy"
            ],
            "title": "Dual graph convolutional networks for aspectbased sentiment analysis",
            "venue": "ACL-IJCNLP, 2021, pp. 6319\u20136329.",
            "year": 2021
        },
        {
            "authors": [
                "B. Wang",
                "T. Shen",
                "G. Long",
                "T. Zhou",
                "Y. Chang"
            ],
            "title": "Eliminating sentiment bias for aspect-level sentiment classification with unsupervised opinion extraction",
            "venue": "Findings of EMNLP, 2021, pp. 3002\u20133012.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Zhou",
                "L. Liao",
                "Y. Gao",
                "Z. Jie",
                "W. Lu"
            ],
            "title": "To be closer: Learning to link up aspects with opinions",
            "venue": "EMNLP, 2021, pp. 3899\u20133909.",
            "year": 2021
        },
        {
            "authors": [
                "Z. Wu",
                "C. Ying",
                "F. Zhao",
                "Z. Fan",
                "X. Dai",
                "R. Xia"
            ],
            "title": "Grid tagging scheme for aspect-oriented fine-grained opinion extraction",
            "venue": "Findings of EMNLP, 2020, pp. 2576\u20132585. 18",
            "year": 2020
        },
        {
            "authors": [
                "L. Gao",
                "Y. Wang",
                "T. Liu",
                "J. Wang",
                "L. Zhang",
                "J. Liao"
            ],
            "title": "Question-driven span labeling model for aspect-opinion pair extraction",
            "venue": "AAAI, 2021, pp. 12 875\u201312 883.",
            "year": 2021
        },
        {
            "authors": [
                "S. Wu",
                "H. Fei",
                "Y. Ren",
                "D. Ji",
                "J. Li"
            ],
            "title": "Learn from syntax: Improving pair-wise aspect and opinion terms extraction with rich syntactic knowledge",
            "venue": "IJCAI, 2021, pp. 3957\u20133963.",
            "year": 2021
        },
        {
            "authors": [
                "M. Zhang",
                "Y. Zhang",
                "D. Vo"
            ],
            "title": "Neural networks for open domain targeted sentiment",
            "venue": "EMNLP, 2015, pp. 612\u2013621.",
            "year": 2015
        },
        {
            "authors": [
                "F. Wang",
                "M. Lan",
                "W. Wang"
            ],
            "title": "Towards a onestop solution to both aspect extraction and sentiment analysis tasks with neural multi-task learning",
            "venue": "IJCNN, 2018, pp. 1\u20138.",
            "year": 2018
        },
        {
            "authors": [
                "X. Li",
                "L. Bing",
                "P. Li",
                "W. Lam"
            ],
            "title": "A unified model for opinion target extraction and target sentiment prediction",
            "venue": "AAAI, 2019, pp. 6714\u20136721.",
            "year": 2019
        },
        {
            "authors": [
                "H. Luo",
                "T. Li",
                "B. Liu",
                "J. Zhang"
            ],
            "title": "DOER: dual crossshared RNN for aspect term-polarity co-extraction",
            "venue": "ACL, 2019, pp. 591\u2013601.",
            "year": 2019
        },
        {
            "authors": [
                "R. He",
                "W.S. Lee",
                "H.T. Ng",
                "D. Dahlmeier"
            ],
            "title": "An interactive multi-task learning network for end-to-end aspect-based sentiment analysis",
            "venue": "ACL, 2019, pp. 504\u2013515.",
            "year": 2019
        },
        {
            "authors": [
                "M. Hu",
                "Y. Peng",
                "Z. Huang",
                "D. Li",
                "Y. Lv"
            ],
            "title": "Opendomain targeted sentiment analysis via span-based extraction and classification",
            "venue": "ACL, 2019, pp. 537\u2013 546.",
            "year": 2019
        },
        {
            "authors": [
                "Z. Chen",
                "T. Qian"
            ],
            "title": "Relation-aware collaborative learning for unified aspect-based sentiment analysis",
            "venue": "ACL, 2020, pp. 3685\u20133694.",
            "year": 2020
        },
        {
            "authors": [
                "H. Luo",
                "L. Ji",
                "T. Li",
                "D. Jiang",
                "N. Duan"
            ],
            "title": "GRACE: Gradient harmonized and cascaded labeling for aspect-based sentiment analysis",
            "venue": "Findings of EMNLP, 2020, pp. 54\u201364.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Liang",
                "F. Meng",
                "J. Zhang",
                "Y. Chen",
                "J. Xu",
                "J. Zhou"
            ],
            "title": "An iterative multi-knowledge transfer network for aspect-based sentiment analysis",
            "venue": "Findings of EMNLP, 2021, pp. 1768\u20131780.",
            "year": 2021
        },
        {
            "authors": [
                "G. Yu",
                "J. Li",
                "L. Luo",
                "Y. Meng",
                "X. Ao",
                "Q. He"
            ],
            "title": "Self question-answering: Aspect-based sentiment analysis by role flipped machine reading comprehension",
            "venue": "Findings of EMNLP, 2021, pp. 1331\u20131342.",
            "year": 2021
        },
        {
            "authors": [
                "M. Schmitt",
                "S. Steinheber",
                "K. Schreiber",
                "B. Roth"
            ],
            "title": "Joint aspect and polarity classification for aspectbased sentiment analysis with end-to-end neural networks",
            "venue": "EMNLP, 2018, pp. 1109\u20131114.",
            "year": 2018
        },
        {
            "authors": [
                "H. Cai",
                "Y. Tu",
                "X. Zhou",
                "J. Yu",
                "R. Xia"
            ],
            "title": "Aspectcategory based sentiment analysis with hierarchical graph convolutional network",
            "venue": "COLING, 2020, pp. 833\u2013843.",
            "year": 2020
        },
        {
            "authors": [
                "J. Liu",
                "Z. Teng",
                "L. Cui",
                "H. Liu",
                "Y. Zhang"
            ],
            "title": "Solving aspect category sentiment analysis as a text generation task",
            "venue": "EMNLP, 2021, pp. 4406\u20134416.",
            "year": 2021
        },
        {
            "authors": [
                "L. Xu",
                "H. Li",
                "W. Lu",
                "L. Bing"
            ],
            "title": "Position-aware tagging for aspect sentiment triplet extraction",
            "venue": "EMNLP, 2020, pp. 2339\u20132349.",
            "year": 2020
        },
        {
            "authors": [
                "C. Zhang",
                "Q. Li",
                "D. Song",
                "B. Wang"
            ],
            "title": "A multi-task learning framework for opinion triplet extraction",
            "venue": "Findings of EMNLP, 2020, pp. 819\u2013828.",
            "year": 2020
        },
        {
            "authors": [
                "S. Chen",
                "Y. Wang",
                "J. Liu",
                "Y. Wang"
            ],
            "title": "Bidirectional machine reading comprehension for aspect sentiment triplet extraction",
            "venue": "AAAI, 2021, pp. 12 666\u201312 674.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Mao",
                "Y. Shen",
                "C. Yu",
                "L. Cai"
            ],
            "title": "A joint training dual-mrc framework for aspect based sentiment analysis",
            "venue": "AAAI, 2021, pp. 13 543\u201313 551.",
            "year": 2021
        },
        {
            "authors": [
                "W. Zhang",
                "X. Li",
                "Y. Deng",
                "L. Bing",
                "W. Lam"
            ],
            "title": "Towards generative aspect-based sentiment analysis",
            "venue": "ACL-IJCNLP, 2021, pp. 504\u2013510.",
            "year": 2021
        },
        {
            "authors": [
                "H. Yan",
                "J. Dai",
                "T. Ji",
                "X. Qiu",
                "Z. Zhang"
            ],
            "title": "A unified generative framework for aspect-based sentiment analysis",
            "venue": "ACL-IJCNLP, 2021, pp. 2416\u20132429.",
            "year": 2021
        },
        {
            "authors": [
                "L. Xu",
                "Y.K. Chia",
                "L. Bing"
            ],
            "title": "Learning span-level interactions for aspect sentiment triplet extraction",
            "venue": "ACL-IJCNLP, 2021, pp. 4755\u20134766.",
            "year": 2021
        },
        {
            "authors": [
                "H. Fei",
                "Y. Ren",
                "Y. Zhang",
                "D. Ji"
            ],
            "title": "Nonautoregressive encoder-decoder neural framework for end-toend aspect-based sentiment triplet extraction",
            "venue": "IEEE Trans. Neural Networks Learn. Syst., 2021.",
            "year": 2021
        },
        {
            "authors": [
                "R. Mukherjee",
                "T. Nayak",
                "Y. Butala",
                "S. Bhattacharya",
                "P. Goyal"
            ],
            "title": "PASTE: A tagging-free decoding framework using pointer networks for aspect sentiment triplet extraction",
            "venue": "EMNLP, 2021, pp. 9279\u20139291.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Lu",
                "Q. Liu",
                "D. Dai",
                "X. Xiao",
                "H. Lin",
                "X. Han",
                "L. Sun",
                "H. Wu"
            ],
            "title": "Unified structure generation for universal information extraction",
            "venue": "ACL, 2022, pp. 5755\u2013 5772.",
            "year": 2022
        },
        {
            "authors": [
                "C. Wu",
                "Q. Xiong",
                "H. Yi",
                "Y. Yu",
                "Q. Zhu",
                "M. Gao",
                "J. Chen"
            ],
            "title": "Multiple-element joint detection for aspectbased sentiment analysis",
            "venue": "Knowl. Based Syst., vol. 223, p. 107073, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "H. Cai",
                "R. Xia",
                "J. Yu"
            ],
            "title": "Aspect-category-opinionsentiment quadruple extraction with implicit aspects and opinions",
            "venue": "ACL-IJCNLP, 2021, pp. 340\u2013350.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Mao",
                "Y. Shen",
                "J. Yang",
                "X. Zhu",
                "L. Cai"
            ],
            "title": "Seq2path: Generating sentiment tuples as paths of a tree",
            "venue": "Findings of ACL, 2022, pp. 2215\u20132225.",
            "year": 2022
        },
        {
            "authors": [
                "X. Bao",
                "Z. Wang",
                "X. Jiang",
                "R. Xiao",
                "S. Li"
            ],
            "title": "Aspectbased sentiment analysis with opinion tree generation",
            "venue": "IJCAI, 2022, pp. 4044\u20134050.",
            "year": 2022
        },
        {
            "authors": [
                "T. Sun",
                "X. Liu",
                "X. Qiu",
                "X. Huang"
            ],
            "title": "Paradigm shift in natural language processing",
            "venue": "CoRR, vol. abs/2109.12575, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Kim"
            ],
            "title": "Convolutional neural networks for sentence classification",
            "venue": "EMNLP, 2014, pp. 1746\u20131751.",
            "year": 2014
        },
        {
            "authors": [
                "S. Hochreiter",
                "J. Schmidhuber"
            ],
            "title": "Long short-term memory",
            "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u2013 1780, 1997.",
            "year": 1997
        },
        {
            "authors": [
                "A. Vaswani",
                "N. Shazeer",
                "N. Parmar",
                "J. Uszkoreit",
                "L. Jones",
                "A.N. Gomez",
                "\u0141. Kaiser",
                "I. Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "NeurIPS, 2017, pp. 5998\u20136008.",
            "year": 2017
        },
        {
            "authors": [
                "J.D. Lafferty",
                "A. McCallum",
                "F.C.N. Pereira"
            ],
            "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
            "venue": "ICML, 2001, pp. 282\u2013289.",
            "year": 2001
        },
        {
            "authors": [
                "E.F.T.K. Sang",
                "J. Veenstra"
            ],
            "title": "Representing text chunks",
            "venue": "EACL, 1999, pp. 173\u2013179.",
            "year": 1999
        },
        {
            "authors": [
                "D. Chen"
            ],
            "title": "Neural reading comprehension and beyond",
            "venue": "Ph.D. dissertation, Stanford University, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "I. Sutskever",
                "O. Vinyals",
                "Q.V. Le"
            ],
            "title": "Sequence to 19 sequence learning with neural networks",
            "venue": "NeurIPS, 2014, pp. 3104\u20133112.",
            "year": 2014
        },
        {
            "authors": [
                "M. Pontiki",
                "D. Galanis",
                "J. Pavlopoulos",
                "H. Papageorgiou",
                "I. Androutsopoulos",
                "S. Manandhar"
            ],
            "title": "Semeval-2014 task 4: Aspect based sentiment analysis",
            "venue": "SemEval@COLING, P. Nakov and T. Zesch, Eds., 2014, pp. 27\u201335.",
            "year": 2014
        },
        {
            "authors": [
                "M. Pontiki",
                "D. Galanis",
                "H. Papageorgiou",
                "S. Manandhar",
                "I. Androutsopoulos"
            ],
            "title": "Semeval-2015 task 12: Aspect based sentiment analysis",
            "venue": "SemEval@NAACL-HLT, D. M. Cer, D. Jurgens, P. Nakov, and T. Zesch, Eds., 2015, pp. 486\u2013495.",
            "year": 2015
        },
        {
            "authors": [
                "M. Pontiki",
                "D. Galanis",
                "H. Papageorgiou",
                "I. Androutsopoulos",
                "S. Manandhar",
                "M. Al-Smadi",
                "M. Al-Ayyoub",
                "Y. Zhao",
                "B. Qin",
                "O.D. Clercq",
                "V. Hoste",
                "M. Apidianaki",
                "X. Tannier",
                "N.V. Loukachevitch",
                "E.V. Kotelnikov",
                "N. Bel",
                "S.M.J. Zafra",
                "G. Eryigit"
            ],
            "title": "Semeval- 2016 task 5: Aspect based sentiment analysis",
            "venue": "SemEval@NAACL-HLT, 2016, pp. 19\u201330.",
            "year": 2016
        },
        {
            "authors": [
                "J. Wang",
                "C. Sun",
                "S. Li",
                "X. Liu",
                "L. Si",
                "M. Zhang",
                "G. Zhou"
            ],
            "title": "Aspect sentiment classification towards question-answering with reinforced bidirectional attention network",
            "venue": "ACL, 2019, pp. 3548\u20133557.",
            "year": 2019
        },
        {
            "authors": [
                "Q. Jiang",
                "L. Chen",
                "R. Xu",
                "X. Ao",
                "M. Yang"
            ],
            "title": "A challenge dataset and effective models for aspectbased sentiment analysis",
            "venue": "EMNLP-IJCNLP, 2019, pp. 6280\u20136285.",
            "year": 2019
        },
        {
            "authors": [
                "X. Xing",
                "Z. Jin",
                "D. Jin",
                "B. Wang",
                "Q. Zhang",
                "X. Huang"
            ],
            "title": "Tasty burgers, soggy fries: Probing aspect robustness in aspect-based sentiment analysis",
            "venue": "EMNLP, 2020, pp. 3594\u20133605.",
            "year": 2020
        },
        {
            "authors": [
                "J. Bu",
                "L. Ren",
                "S. Zheng",
                "Y. Yang",
                "J. Wang",
                "F. Zhang",
                "W. Wu"
            ],
            "title": "ASAP: A chinese review dataset towards aspect category sentiment analysis and rating prediction",
            "venue": "NAACL-HLT, 2021, pp. 2069\u20132079.",
            "year": 2021
        },
        {
            "authors": [
                "L. Zhang",
                "S. Wang",
                "B. Liu"
            ],
            "title": "Deep learning for sentiment analysis: A survey",
            "venue": "Wiley Interdiscip. Rev. Data Min. Knowl. Discov., vol. 8, no. 4, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "K. Schouten",
                "O. van der Weijde",
                "F. Frasincar",
                "R. Dekker"
            ],
            "title": "Supervised and unsupervised aspect category detection for sentiment analysis with cooccurrence data",
            "venue": "IEEE Trans. Cybern., vol. 48, no. 4, pp. 1263\u20131275, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "W. Wang",
                "S.J. Pan"
            ],
            "title": "Recursive neural structural correspondence network for cross-domain aspect and opinion co-extraction",
            "venue": "ACL, 2018, pp. 2171\u20132181.",
            "year": 2018
        },
        {
            "authors": [
                "Z. Wu",
                "F. Zhao",
                "X. Dai",
                "S. Huang",
                "J. Chen"
            ],
            "title": "Latent opinions transfer network for target-oriented opinion words extraction",
            "venue": "AAAI, 2020, pp. 9298\u20139305.",
            "year": 2020
        },
        {
            "authors": [
                "T.N. Kipf",
                "M. Welling"
            ],
            "title": "Semi-supervised classification with graph convolutional networks",
            "venue": "ICLR, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "Y. Tay",
                "L.A. Tuan",
                "S.C. Hui"
            ],
            "title": "Learning to attend via word-aspect associative fusion for aspect-based sentiment analysis",
            "venue": "AAAI, 2018, pp. 5956\u20135963.",
            "year": 2018
        },
        {
            "authors": [
                "B. Xing",
                "L. Liao",
                "D. Song",
                "J. Wang",
                "F. Zhang",
                "Z. Wang",
                "H. Huang"
            ],
            "title": "Earlier attention? aspect-aware LSTM for aspect-based sentiment analysis",
            "venue": "IJCAI, 2019, pp. 5313\u20135319.",
            "year": 2019
        },
        {
            "authors": [
                "R. He",
                "W.S. Lee",
                "H.T. Ng",
                "D. Dahlmeier"
            ],
            "title": "Effective attention modeling for aspect-level sentiment classification",
            "venue": "COLING, 2018, pp. 1121\u20131131.",
            "year": 2018
        },
        {
            "authors": [
                "M. Zhang",
                "Y. Zhang",
                "D. Vo"
            ],
            "title": "Gated neural networks for targeted sentiment analysis",
            "venue": "AAAI, 2016, pp. 3087\u20133093.",
            "year": 2016
        },
        {
            "authors": [
                "C. Sun",
                "L. Huang",
                "X. Qiu"
            ],
            "title": "Utilizing BERT for aspect-based sentiment analysis via constructing auxiliary sentence",
            "venue": "NAACL-HLT, 2019, pp. 380\u2013385.",
            "year": 2019
        },
        {
            "authors": [
                "Z. Wu",
                "D.C. Ong"
            ],
            "title": "Context-guided BERT for targeted aspect-based sentiment analysis",
            "venue": "AAAI, 2021, pp. 14 094\u201314 102.",
            "year": 2021
        },
        {
            "authors": [
                "J. Dai",
                "H. Yan",
                "T. Sun",
                "P. Liu",
                "X. Qiu"
            ],
            "title": "Does syntax matter? A strong baseline for aspect-based sentiment analysis with roberta",
            "venue": "NAACL-HLT, 2021, pp. 1816\u20131829.",
            "year": 2021
        },
        {
            "authors": [
                "C. Brun",
                "D.N. Popa",
                "C. Roux"
            ],
            "title": "XRCE: hybrid classification for aspect-based sentiment analysis",
            "venue": "SemEval@COLING, 2014, pp. 838\u2013842.",
            "year": 2014
        },
        {
            "authors": [
                "S. Kiritchenko",
                "X. Zhu",
                "C. Cherry",
                "S. Mohammad"
            ],
            "title": "Nrc-canada-2014: Detecting aspects and sentiment in customer reviews",
            "venue": "SemEval@COLING, 2014, pp. 437\u2013442.",
            "year": 2014
        },
        {
            "authors": [
                "J. Zhou",
                "G. Cui",
                "S. Hu",
                "Z. Zhang",
                "C. Yang",
                "Z. Liu",
                "L. Wang",
                "C. Li",
                "M. Sun"
            ],
            "title": "Graph neural networks: A review of methods and applications",
            "venue": "AI Open, vol. 1, pp. 57\u201381, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "B. Huang",
                "K. Carley"
            ],
            "title": "Syntax-aware aspect level sentiment classification with graph attention networks",
            "venue": "EMNLP-IJCNLP, 2019, pp. 5469\u20135477.",
            "year": 2019
        },
        {
            "authors": [
                "M. Zhang",
                "T. Qian"
            ],
            "title": "Convolution over hierarchical syntactic and lexical graphs for aspect level sentiment analysis",
            "venue": "EMNLP, 2020, pp. 3540\u20133549.",
            "year": 2020
        },
        {
            "authors": [
                "K. Wang",
                "W. Shen",
                "Y. Yang",
                "X. Quan",
                "R. Wang"
            ],
            "title": "Relational graph attention network for aspect-based sentiment analysis",
            "venue": "ACL, 2020, pp. 3229\u20133238.",
            "year": 2020
        },
        {
            "authors": [
                "S. Ruder",
                "P. Ghaffari",
                "J.G. Breslin"
            ],
            "title": "A hierarchical model of reviews for aspect-based sentiment analysis",
            "venue": "EMNLP, 2016, pp. 999\u20131005.",
            "year": 2016
        },
        {
            "authors": [
                "X. Chen",
                "C. Sun",
                "J. Wang",
                "S. Li",
                "L. Si",
                "M. Zhang",
                "G. Zhou"
            ],
            "title": "Aspect sentiment classification with document-level sentiment preference modeling",
            "venue": "ACL, 2020, pp. 3667\u20133677.",
            "year": 2020
        },
        {
            "authors": [
                "W. Zhang",
                "Y. Deng",
                "X. Li",
                "L. Bing",
                "W. Lam"
            ],
            "title": "Aspect-based sentiment analysis in question answering forums",
            "venue": "Findings of EMNLP, 2021, pp. 4582\u2013 4591.",
            "year": 2021
        },
        {
            "authors": [
                "M. Hu",
                "S. Zhao",
                "L. Zhang",
                "K. Cai",
                "Z. Su",
                "R. Cheng",
                "X. Shen"
            ],
            "title": "CAN: constrained attention networks for multi-aspect sentiment analysis",
            "venue": "EMNLP-IJCNLP, 2019, pp. 4600\u20134609.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Ma",
                "H. Peng",
                "E. Cambria"
            ],
            "title": "Targeted aspectbased sentiment analysis via embedding commonsense knowledge into an attentive LSTM",
            "venue": "AAAI, 2018, pp. 5876\u20135883.",
            "year": 2018
        },
        {
            "authors": [
                "Z. Dai",
                "C. Peng",
                "H. Chen",
                "Y. Ding"
            ],
            "title": "A multi-task incremental learning framework with category name embedding for aspect-category sentiment analysis",
            "venue": "EMNLP, 2020, pp. 6955\u20136965.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Li",
                "Z. Yang",
                "C. Yin",
                "X. Pan",
                "L. Cui",
                "Q. Huang",
                "T. Wei"
            ],
            "title": "A joint model for aspect-category sentiment analysis with shared sentiment prediction layer",
            "venue": "CCL, 2020, pp. 388\u2013400. 20",
            "year": 2020
        },
        {
            "authors": [
                "T.-W. Hsu",
                "C.-C. Chen",
                "H.-H. Huang",
                "H.-H. Chen"
            ],
            "title": "Semantics-preserved data augmentation for aspectbased sentiment analysis",
            "venue": "EMNLP, 2021, pp. 4417\u2013 4422.",
            "year": 2021
        },
        {
            "authors": [
                "C. Brun",
                "V. Nikoulina"
            ],
            "title": "Aspect based sentiment analysis into the wild",
            "venue": "WASSA@EMNLP, 2018, pp. 116\u2013122.",
            "year": 2018
        },
        {
            "authors": [
                "T. Mikolov",
                "I. Sutskever",
                "K. Chen",
                "G.S. Corrado",
                "J. Dean"
            ],
            "title": "Distributed representations of words and phrases and their compositionality",
            "venue": "NeurIPS, 2013, pp. 3111\u20133119.",
            "year": 2013
        },
        {
            "authors": [
                "J. Pennington",
                "R. Socher",
                "C. Manning"
            ],
            "title": "GloVe: Global vectors for word representation",
            "venue": "EMNLP, 2014, pp. 1532\u20131543.",
            "year": 2014
        },
        {
            "authors": [
                "Y. Song",
                "J. Wang",
                "T. Jiang",
                "Z. Liu",
                "Y. Rao"
            ],
            "title": "Attentional encoder network for targeted sentiment classification",
            "venue": "arXiv, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "A. Rietzler",
                "S. Stabinger",
                "P. Opitz",
                "S. Engl"
            ],
            "title": "Adapt or get left behind: Domain adaptation through BERT language model finetuning for aspect-target sentiment classification",
            "venue": "LREC, 2020, pp. 4933\u20134941.",
            "year": 2020
        },
        {
            "authors": [
                "M. Lewis",
                "Y. Liu",
                "N. Goyal",
                "M. Ghazvininejad",
                "A. Mohamed",
                "O. Levy",
                "V. Stoyanov",
                "L. Zettlemoyer"
            ],
            "title": "BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
            "venue": "ACL, 2020, pp. 7871\u20137880.",
            "year": 2020
        },
        {
            "authors": [
                "C. Raffel",
                "N. Shazeer",
                "A. Roberts",
                "K. Lee",
                "S. Narang",
                "M. Matena",
                "Y. Zhou",
                "W. Li",
                "P.J. Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-totext transformer",
            "venue": "J. Mach. Learn. Res., vol. 21, pp. 140:1\u2013140:67, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Z. Wu",
                "Y. Chen",
                "B. Kao",
                "Q. Liu"
            ],
            "title": "Perturbed masking: Parameter-free probing for analyzing and interpreting BERT",
            "venue": "ACL, 2020, pp. 4166\u20134176.",
            "year": 2020
        },
        {
            "authors": [
                "X. Qiu",
                "T. Sun",
                "Y. Xu",
                "Y. Shao",
                "N. Dai",
                "X. Huang"
            ],
            "title": "Pre-trained models for natural language processing: A survey",
            "venue": "CoRR, vol. abs/2003.08271, 2020.",
            "year": 2003
        },
        {
            "authors": [
                "J. Blitzer",
                "R. McDonald",
                "F. Pereira"
            ],
            "title": "Domain adaptation with structural correspondence learning",
            "venue": "EMNLP, 2006, pp. 120\u2013128.",
            "year": 2006
        },
        {
            "authors": [
                "N. Jakob",
                "I. Gurevych"
            ],
            "title": "Extracting opinion targets in a single and cross-domain setting with conditional random fields",
            "venue": "EMNLP, 2010, pp. 1035\u20131045.",
            "year": 2010
        },
        {
            "authors": [
                "M. Chernyshevich"
            ],
            "title": "IHS R&D Belarus: Cross-domain extraction of product features using CRF",
            "venue": "SemEval, 2014, pp. 309\u2013313.",
            "year": 2014
        },
        {
            "authors": [
                "W. Wang",
                "S.J. Pan"
            ],
            "title": "Syntactically meaningful and transferable recursive neural networks for aspect and opinion extraction",
            "venue": "Computational Linguistics, vol. 45, no. 4, pp. 705\u2013736, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "M. Yang",
                "W. Yin",
                "Q. Qu",
                "W. Tu",
                "Y. Shen",
                "X. Chen"
            ],
            "title": "Neural attentive network for cross-domain aspectlevel sentiment classification",
            "venue": "IEEE Trans. Affect. Comput., 2019.",
            "year": 2019
        },
        {
            "authors": [
                "K. Zhang",
                "Q. Liu",
                "H. Qian",
                "B. Xiang",
                "Q. Cui",
                "J. Zhou",
                "E. Chen"
            ],
            "title": "Eatn: An efficient adaptive transfer network for aspect-level sentiment analysis",
            "venue": "IEEE TKDE, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "W. Wang",
                "S.J. Pan"
            ],
            "title": "Transferable interactive memory network for domain adaptation in fine-grained opinion extraction",
            "venue": "AAAI, 2019, pp. 7192\u20137199.",
            "year": 2019
        },
        {
            "authors": [
                "Z. Li",
                "X. Li",
                "Y. Wei",
                "L. Bing",
                "Y. Zhang",
                "Q. Yang"
            ],
            "title": "Transferable end-to-end aspect-based sentiment analysis with selective adversarial learning",
            "venue": "EMNLP-IJCNLP, 2019, pp. 4590\u20134600.",
            "year": 2019
        },
        {
            "authors": [
                "Z. Chen",
                "T. Qian"
            ],
            "title": "Bridge-based active domain adaptation for aspect term extraction",
            "venue": "ACL- IJCNLP, 2021, pp. 317\u2013327.",
            "year": 2021
        },
        {
            "authors": [
                "T. Liang",
                "W. Wang",
                "F. Lv"
            ],
            "title": "Weakly supervised domain adaptation for aspect extraction via multilevel interaction transfer",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J. Yu",
                "C. Gong",
                "R. Xia"
            ],
            "title": "Cross-domain review generation for aspect-based sentiment analysis",
            "venue": "Findings of ACL-IJCNLP, 2021, pp. 4767\u20134777.",
            "year": 2021
        },
        {
            "authors": [
                "J. Li",
                "J. Yu",
                "R. Xia"
            ],
            "title": "Generative cross-domain data augmentation for aspect and opinion co-extraction",
            "venue": "NAACL, 2022, pp. 4219\u20134229.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Ding",
                "J. Yu",
                "J. Jiang"
            ],
            "title": "Recurrent neural networks with auxiliary labels for cross-domain opinion target extraction",
            "venue": "AAAI, 2017, pp. 3436\u20133442.",
            "year": 2017
        },
        {
            "authors": [
                "F. Li",
                "S.J. Pan",
                "O. Jin",
                "Q. Yang",
                "X. Zhu"
            ],
            "title": "Crossdomain co-extraction of sentiment and topic lexicons",
            "venue": "ACL, 2012, pp. 410\u2013419.",
            "year": 2012
        },
        {
            "authors": [
                "C. Gong",
                "J. Yu",
                "R. Xia"
            ],
            "title": "Unified feature and instance based domain adaptation for aspect-based sentiment analysis",
            "venue": "EMNLP, 2020, pp. 7035\u20137045.",
            "year": 2020
        },
        {
            "authors": [
                "O. Pereg",
                "D. Korat",
                "M. Wasserblat"
            ],
            "title": "Syntactically aware cross-domain aspect and opinion terms extraction",
            "venue": "COLING, 2020, pp. 1772\u20131777.",
            "year": 2020
        },
        {
            "authors": [
                "H. Xu",
                "B. Liu",
                "L. Shu",
                "P. Yu"
            ],
            "title": "DomBERT: Domainoriented language model for aspect-based sentiment analysis",
            "venue": "Findings of EMNLP, 2020, pp. 1725\u20131731.",
            "year": 2020
        },
        {
            "authors": [
                "X. Zhou",
                "X. Wan",
                "J. Xiao"
            ],
            "title": "Clopinionminer: Opinion target extraction in a cross-language scenario",
            "venue": "IEEE ACM Trans. Audio Speech Lang. Process., vol. 23, no. 4, pp. 619\u2013630, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "R. Klinger",
                "P. Cimiano"
            ],
            "title": "Instance selection improves cross-lingual model training for fine-grained sentiment analysis",
            "venue": "CoNLL, 2015, pp. 153\u2013163.",
            "year": 2015
        },
        {
            "authors": [
                "W. Wang",
                "S.J. Pan"
            ],
            "title": "Transition-based adversarial network for cross-lingual aspect extraction",
            "venue": "IJCAI, 2018, pp. 4475\u20134481.",
            "year": 2018
        },
        {
            "authors": [
                "S. Jebbara",
                "P. Cimiano"
            ],
            "title": "Zero-shot cross-lingual opinion target extraction",
            "venue": "NAACL, 2019, pp. 2486\u2013 2495.",
            "year": 2019
        },
        {
            "authors": [
                "P. Lambert"
            ],
            "title": "Aspect-level cross-lingual sentiment classification with constrained SMT",
            "venue": "ACL, 2015, pp. 781\u2013787.",
            "year": 2015
        },
        {
            "authors": [
                "J. Barnes",
                "P. Lambert",
                "T. Badia"
            ],
            "title": "Exploring distributional representations and machine translation for aspect-based cross-lingual sentiment classification",
            "venue": "COLING, 2016, pp. 1613\u20131623.",
            "year": 2016
        },
        {
            "authors": [
                "M.S. Akhtar",
                "P. Sawant",
                "S. Sen",
                "A. Ekbal",
                "P. Bhattacharyya"
            ],
            "title": "Solving data sparsity for aspect based sentiment analysis using cross-linguality and multilinguality",
            "venue": "NAACL, 2018, pp. 572\u2013582.",
            "year": 2018
        },
        {
            "authors": [
                "X. Li",
                "L. Bing",
                "W. Zhang",
                "Z. Li",
                "W. Lam"
            ],
            "title": "Unsupervised cross-lingual adaptation for sequence tagging and beyond",
            "venue": "CoRR, vol. abs/2010.12405, 2020.",
            "year": 2010
        },
        {
            "authors": [
                "W. Zhang",
                "R. He",
                "H. Peng",
                "L. Bing",
                "W. Lam"
            ],
            "title": "Cross-lingual aspect-based sentiment analysis with 21 aspect term code-switching",
            "venue": "EMNLP, 2021, pp. 9220\u20139230.",
            "year": 2021
        },
        {
            "authors": [
                "C. Dyer",
                "V. Chahuneau",
                "N.A. Smith"
            ],
            "title": "A simple, fast, and effective reparameterization of IBM model 2",
            "venue": "NAACL, 2013, pp. 644\u2013648.",
            "year": 2013
        },
        {
            "authors": [
                "A. Conneau",
                "K. Khandelwal",
                "N. Goyal",
                "V. Chaudhary",
                "G. Wenzek",
                "F. Guzm\u00e1n",
                "E. Grave",
                "M. Ott",
                "L. Zettlemoyer",
                "V. Stoyanov"
            ],
            "title": "Unsupervised cross-lingual representation learning at scale",
            "venue": "ACL, 2020, pp. 8440\u20138451.",
            "year": 2020
        },
        {
            "authors": [
                "K. K",
                "Z. Wang",
                "S. Mayhew",
                "D. Roth"
            ],
            "title": "Crosslingual ability of multilingual BERT: an empirical study",
            "venue": "ICLR, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "J. Wang",
                "J. Wang",
                "C. Sun",
                "S. Li",
                "X. Liu",
                "L. Si",
                "M. Zhang",
                "G. Zhou"
            ],
            "title": "Sentiment classification in customer service dialogue with topic-aware multi-task learning",
            "venue": "AAAI, 2020, pp. 9177\u20139184.",
            "year": 2020
        },
        {
            "authors": [
                "J. Yu",
                "J. Wang",
                "R. Xia",
                "J. Li"
            ],
            "title": "Targeted multimodal sentiment classification based on coarse-to-fine grained image-target matching",
            "venue": "IJCAI, 2022, pp. 4482\u20134488.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Ling",
                "J. Yu",
                "R. Xia"
            ],
            "title": "Vision-language pretraining for multimodal aspect-based sentiment analysis",
            "venue": "ACL, 2022, pp. 2149\u20132159.",
            "year": 2022
        },
        {
            "authors": [
                "H. Wu",
                "S. Cheng",
                "J. Wang",
                "S. Li",
                "L. Chi"
            ],
            "title": "Multimodal aspect extraction with region-aware alignment network",
            "venue": "NLPCC, 2020, pp. 145\u2013156.",
            "year": 2020
        },
        {
            "authors": [
                "J. Zhou",
                "J. Zhao",
                "J.X. Huang",
                "Q.V. Hu",
                "L. He"
            ],
            "title": "MASAD: A large-scale dataset for multimodal aspect-based sentiment analysis",
            "venue": "Neurocomputing, vol. 455, pp. 47\u201358, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "N. Xu",
                "W. Mao",
                "G. Chen"
            ],
            "title": "Multi-interactive memory network for aspect based multimodal sentiment analysis",
            "venue": "AAAI, 2019, pp. 371\u2013378.",
            "year": 2019
        },
        {
            "authors": [
                "J. Yu",
                "J. Jiang"
            ],
            "title": "Adapting BERT for target-oriented multimodal sentiment classification",
            "venue": "IJCAI, 2019, pp. 5408\u20135414.",
            "year": 2019
        },
        {
            "authors": [
                "J. Yu",
                "J. Jiang",
                "R. Xia"
            ],
            "title": "Entity-sensitive attention and fusion network for entity-level multimodal sentiment classification",
            "venue": "IEEE ACM Trans. Audio Speech Lang. Process., vol. 28, pp. 429\u2013439, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Z. Khan",
                "Y. Fu"
            ],
            "title": "Exploiting BERT for multimodal target sentiment classification through input space translation",
            "venue": "MM, 2021, pp. 3034\u20133042.",
            "year": 2021
        },
        {
            "authors": [
                "X. Ju",
                "D. Zhang",
                "R. Xiao",
                "J. Li",
                "S. Li",
                "M. Zhang",
                "G. Zhou"
            ],
            "title": "Joint multi-modal aspect-sentiment analysis with auxiliary cross-modal relation detection",
            "venue": "EMNLP, 2021, pp. 4395\u20134405.",
            "year": 2021
        },
        {
            "authors": [
                "M. Delange",
                "R. Aljundi",
                "M. Masana",
                "S. Parisot",
                "X. Jia",
                "A. Leonardis",
                "G. Slabaugh",
                "T. Tuytelaars"
            ],
            "title": "A continual learning survey: Defying forgetting in classification tasks",
            "venue": "IEEE TPAMI, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Z. Chen",
                "N. Ma",
                "B. Liu"
            ],
            "title": "Lifelong learning for sentiment classification",
            "venue": "ACL, 2015, pp. 750\u2013756.",
            "year": 2015
        },
        {
            "authors": [
                "S. Wang",
                "G. Lv",
                "S. Mazumder",
                "G. Fei",
                "B. Liu"
            ],
            "title": "Lifelong learning memory networks for aspect sentiment classification",
            "venue": "IEEE BigData, 2018, pp. 861\u2013870.",
            "year": 2018
        },
        {
            "authors": [
                "B. Geng",
                "M. Yang",
                "F. Yuan",
                "S. Wang",
                "X. Ao",
                "R. Xu"
            ],
            "title": "Iterative network pruning with uncertainty regularization for lifelong sentiment classification",
            "venue": "SIGIR, 2021, pp. 1229\u20131238.",
            "year": 2021
        },
        {
            "authors": [
                "H. Wang",
                "S. Wang",
                "S. Mazumder",
                "B. Liu",
                "Y. Yang",
                "T. Li"
            ],
            "title": "Bayes-enhanced lifelong attention networks for sentiment classification",
            "venue": "COLING, 2020, pp. 580\u2013 591.",
            "year": 2020
        },
        {
            "authors": [
                "Z. Ke",
                "B. Liu",
                "H. Xu",
                "L. Shu"
            ],
            "title": "CLASSIC: continual and contrastive learning of aspect sentiment classification tasks",
            "venue": "EMNLP, 2021, pp. 6871\u20136883.",
            "year": 2021
        },
        {
            "authors": [
                "Z. Ke",
                "H. Xu",
                "B. Liu"
            ],
            "title": "Adapting BERT for continual learning of a sequence of aspect sentiment classification tasks",
            "venue": "NAACL-HLT, 2021, pp. 4746\u2013 4755.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014Aspect-Based Sentiment Analysis, Sentiment Analysis, Opinion Mining, Pre-trained Language Models\nF"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "D ISCOVERING and understanding opinions from on-line user-generated content is crucial for widespread applications. For example, analyzing customer sentiments and opinions from reviews in E-commerce platforms helps improve the product or service, and make better marketing campaigns. Given the massive amount of textual content, it is intractable to manually digest the opinion information. Therefore, designing an automatic computational framework for analyzing opinions hidden behind the unstructured texts is necessary, resulting in the emergence of the research field sentiment analysis and opinion mining [1].\nConventional sentiment analysis studies mainly perform prediction at the sentence or document level [2, 3, 4], identifying the overall sentiment towards the whole sentence or document. To make the prediction, it is assumed that a single sentiment is conveyed towards the single topic in the given text, which may not be the case in practice. Under this circumstance, the need for recognizing more fine-grained aspect-level opinions and sentiments, dubbed as AspectBased Sentiment Analysis (ABSA), has received increasing attention in the past decade [5, 6]. In the ABSA problem, the concerned target on which the sentiment is expressed shifts from an entire sentence or document to an entity or a certain aspect of an entity. For instance, an entity can be a specific product in the E-commerce domain, and its property or characteristics such as the price and size are the aspects of it. Since an entity can also be regarded as a special \u201cgeneral\u201d\n\u2022 Wenxuan Zhang, Xin Li and Lidong Bing are with DAMO Academy, Alibaba Group. E-mail: {saike.zwx, xinting.lx, l.bing}@alibaba-inc.com \u2022 Yang Deng and Wai Lam are with The Chinese Univerisity of Hong Kong. E-mail: {ydeng, wlam}@se.cuhk.edu.hk\nThis work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.\naspect, we collectively refer to an entity and its aspect as \u201caspect\u201d in this paper. ABSA is thus the process of building a comprehensive opinion summary at the aspect level, which provides useful fine-grained sentiment information for downstream applications.\nGenerally, the main research line of ABSA involves the identification of various aspect-level sentiment elements, namely, aspect terms, aspect categories, opinion terms and sentiment polarities [7]. As shown in Fig. 1, given a sentence \u201cThe pizza is delicious.\u201d, the corresponding sentiment elements are \u201cpizza\u201d, \u201cfood\u201d, \u201cdelicious\u201d, and \u201cpositive\u201d respectively, where \u201cpizza\u201d and \u201cdelicious\u201d are explicitly expressed, \u201cfood\u201d and \u201cpositive\u201d belong to the pre-defined category and sentiment sets. Early works in ABSA begin with identifying each single sentiment element separately. For instance, the aspect term extraction (ATE) task [8] aims to extract all mentioned aspect terms in the given text; while the aspect sentiment classification task [9] predicts the sentiment polarity for a specific aspect within a sentence. In this paper, we refer to these tasks as Single ABSA tasks.\nHowever, finding a single sentiment element is still far from satisfactory for understanding more complete aspectlevel opinion, which requires not only the extraction of multiple sentiment elements but also the recognition of the correspondence and dependency between them. To this end, several new ABSA tasks [10, 11, 12, 7] together with corresponding benchmark datasets have been introduced in recent years to facilitate the study on the joint prediction of multiple sentiment elements. These tasks are referred to as Compound ABSA tasks, in contrast to Single ABSA tasks involving a single sentiment element only. For example, the aspect-opinion pair (AOPE) extraction task [11, 13] requires extracting the aspect and its associated opinion term in a compound form, i.e., extracting (pizza, delicious) pair from the previous example sentence. It thus provides a clearer\nar X\niv :2\n20 3.\n01 05\n4v 2\n[ cs\n.C L\n] 6\nN ov\n2 02\n2\n2 ABSA System Aspect Term pizza Aspect Category food Opinion Term delicious Sentiment Polarity POS The pizza is delicious.\nFig. 1. An example of the four key sentiment elements of ABSA.\nunderstanding of what the mentioned opinion target and its associated opinion expression are. Following some pioneering works, a wide variety of frameworks have been proposed to tackle different compound ABSA tasks for enabling aspect-level opinion mining in different scenarios. However, a systematic review of various ABSA tasks, especially recent progress on compound ABSA tasks, is lacking in the existing surveys [1, 5, 14, 6, 15, 16], which we aim to fill through this survey paper.\nAside from designing specific models for different tasks, the advent of pre-trained language models (PLMs) such as BERT [17] and RoBERTa [18] has brought substantial improvements on a wide range of ABSA tasks in recent years. With PLMs as the backbone, the generalization capability and the robustness of ABSA models have been significantly improved. For example, Li et al. [19] show that using a simple linear classification layer stacked on top of BERT can achieve more competitive performance than previous specifically designed neural models for the Endto-End ABSA task. Although constructing ABSA models based on PLMs has become ubiquitous nowadays, they are not discussed in the existing surveys [6, 15, 16] due to their recency of publication. Therefore, in this paper, we provide an in-depth analysis of existing PLM-based ABSA models by discussing both their advances and limitations.\nTo conduct ABSA in practical settings, we also review the recent works tackling the cross-domain and cross-lingual ABSA problem. Current ABSA models that achieved satisfactory performance in various tasks often hold a common assumption: the training and testing data come from the same distribution (e.g., the same domain or the same language). When the distribution of data changes, re-training the ABSA model is often needed to guarantee the performance. However, it is usually expensive or even impossible to collect additional large volume of labeled data, especially for the ABSA task requiring aspect-level annotations. In this case, adapting the trained model to unseen domains, i.e., cross-domain transfer [20] or unseen languages, i.e., cross-lingual transfer [21], provides an alternative solution for building ABSA systems well generalizing to different domains and languages.\nThere have been other surveys and reviews about ABSA. Existing surveys of general sentiment analysis research [1, 16] discuss the ABSA problem, but they do not provide a detailed description of the recent advances and challenges. The earliest ABSA survey by Schouten and Frasincar [5] comprehensively introduces the ABSA studies before 2016, but it mainly focuses on the non-neural ABSA methods.\nZhou et al. [14], Liu et al. [15], and Nazir et al. [6] introduce the deep learning based ABSA models. However, their discussions are only limited to single ABSA tasks with a few pioneering works on the End-to-End ABSA task. A comprehensive review of all ABSA tasks, the impacts of PLMs for the ABSA problem, as well as recent progress on the cross-domain/lingual transfer are not covered.\nOverall, the main goal of this survey paper is to systematically review the advances and challenges of the ABSA problem from a modern perspective. More specifically, we provide a new taxonomy for ABSA which organizes various ABSA studies from the axes of concerned sentiment elements, with an emphasis on the compound ABSA tasks studied in recent years. Along this direction, we discuss and summarize all kinds of methods proposed for each task. Besides, we investigate the potentials and limitations of exploiting pre-trained language models for the ABSA problem. We also summarize the research efforts on crossdomain and cross-lingual ABSA. Finally, we discuss some emerging trends and open challenges, aiming to shed light on potential future directions in this area. We maintain a GitHub repository to collect useful resources such as the paper list discussed in this survey, links to tools and datasets: https://github.com/IsakZhang/ABSA-Survey."
        },
        {
            "heading": "2 BACKGROUND",
            "text": ""
        },
        {
            "heading": "2.1 Four Sentiment Elements of ABSA",
            "text": "According to Liu [1], the general sentiment analysis problem consists of two key components: target and sentiment. For ABSA, the target can be described with either an aspect category c or an aspect term a, while the sentiment involves a detailed opinion expression - the opinion term o, and a general sentiment orientation - the sentiment polarity p. These four sentiment elements constitute the main line of ABSA research:\n\u2022 aspect category c defines a unique aspect of an entity and is supposed to fall into a category set C, predefined for each specific domain of interest. For example, food and service can be aspect categories for the restaurant domain. \u2022 aspect term a is the opinion target which explicitly appears in the given text, e.g., \u201cpizza\u201d in the sentence \u201cThe pizza is delicious.\u201d When the target is implicitly expressed (e.g., \u201cIt is overpriced!\u201d), we can denote the aspect term as a special one named \u201cnull\u201d. \u2022 opinion term o is the expression given by the opinion holder to express his/her sentiment towards the target. For instance, \u201cdelicious\u201d is the opinion term in the running example \u201cThe pizza is delicious\u201d. \u2022 sentiment polarity p describes the orientation of the sentiment over an aspect category or an aspect term, which usually belongs to positive, negative, and neutral.\nNote that in the literature, the terminologies of ABSA studies are often used interchangeably, but sometimes they have different meanings according to the context. For example, \u201copinion target\u201d, \u201ctarget\u201d, \u201caspect\u201d, \u201centity\u201d are usually used to refer to the target on which the opinion is expressed. However, they can be either an aspect category or an aspect\n3 ABSA Single ABSA Tasks Aspect Term Extraction (ATE) Supervised ATE BiLSTM [8], WDEmbed [22], RNCRF [23], MIN [24], CMLA [25], HAST [26], DE-CNN [27], Seq2Seq4ATE [28], CL-BERT [29], PoD [30] Semi-supervised ATE Conditional Augment [31], SoftProto [32], Self-Traning [33] Unsupervised ATE ABAE [34], AE-CSA [35], LCC+GBC [36] Aspect Category Detection (ACD) Supervised ACD RepLearn [37], TAN [38], LICD [39], Proto-AWATT[40] Unsupervised ACD CAt [41], SSCL [42] Opinion Term Extraction (OTE) Aspect Opinion Co-Extraction (AOCE) RNCRF [23], CMLA [25], MIN [24], HAST [26], GMTCMLA [43], DeepWMaxSAT [44] Target-oriented Opinion Word Extraction (TOWE) IOG [45], LOTN [46], ONG [47], PositionEmbed [48] Aspect Sentiment Classification (ASC) AdaRNN [49], Target-dep [50], TC-LSTM [51], ATAE-LSTM [52], MemNet [53], IAN [54], BILSTM-ATT-G [55], RAM [56], HEAT [57], GCAE [58], TNet [59], MGAN [60], PWCN [61], BERT-pair [62], TransCap [63], IACapsNet [64], AGDT [65], ASGCN [66], CDT [67], DGEDT [68], kumaGCN [69], MCRF-SA [70], GraphMerge [71], TGCN [72], DualGCN [73], SARL [74], ACLT [75]\nCompound ABSA Tasks\nPair Extraction\nAspect-Opinion Pair Extraction (AOPE) SpanMlt [13], SDRN [11], GTS [76], QDSL [77], SynFue [78]\nEnd-to-End ABSA (E2E-ABSA)\nNN-CRF [79], MNN [80], E2E-TBSA [81], DOER [82], IMN [83], SPAN [84], RACL [85], GRACE [86], IMKTN [87], RF-MRC [88]\nAspect Category Sentiment Analysis (ACSA) AddOneDim [89], Hier-GCN [90], BART-generation [91]\nTriplet Extraction\nAspect Sentiment Triplet Extraction (ASTE)\nTwoStage [12], JET [92], GTS [76], OTE-MTL [93], BMRC [94] Dual-MRC [95], GAS [96], Gen-ABSA [97], Span-ASTE [98], NAG-ASTE [99], PASTE [100], UIE [101]\nAspect-Category-Sentiment Detection (ACSD) TAS-BERT [46], MEJD [102], GAS [96], Paraphrase [7]\nQuad Extraction Aspect SentimentQuad Prediction (ASQP) Extract-Classify-ACOS [103], Paraphrase [7], Seq2Path [104], Seq2Tree [105]\nFig. 2. Taxonomy of ABSA tasks, with representative methods of each task.\nterm depending on the context. This may cause unnecessary confusion and often makes the literature review incomplete. In this survey, we adopt the most commonly accepted terminologies while also ensuring that similar concepts are clearly distinguishable. Therefore, as defined above, we would use \u201caspect term\u201d and \u201caspect category\u201d to differentiate different formats of the aspect, and only use \u201ctarget\u201d or \u201caspect\u201d as a general expression for describing an opinion target."
        },
        {
            "heading": "2.2 ABSA Definition",
            "text": "With the four key sentiment elements defined in the last section, we can give a definition of ABSA from the perspective of concerned sentiment elements:\nAspect-based sentiment analysis (ABSA) is the problem to identify sentiment elements of interest for a concerned text item1, either a single sentiment element, or multiple elements with the dependency relation between them.\nWe can thus organize various ABSA studies according to the sentiment elements involved. Depending on whether the desired output is a single sentiment element or multiple coupled elements, we can categorize ABSA tasks into single ABSA tasks and compound ABSA tasks, e.g., aspect term\n1. In most ABSA benchmark datasets, a sentence is treated as the text item, we thus use \u201csentence\u201d to refer to a concerned text in this paper. However, the reader should be aware that the described methods naturally handle texts with any length.\nextraction (ATE) is a single ABSA task that aims to extract all aspect terms a given a sentence, while aspect-opinion pair extraction (AOPE) task is a compound ABSA task since it extracts all (a, o) pairs. From this perspective, we present a new taxonomy for ABSA which systematically organizes existing works from the axes of concerned sentiment elements. We present an overview of different ABSA tasks and representative methods for each task in Fig. 2.\nIn the light of the above definition, we describe single ABSA tasks in Sec 3 and compound ABSA tasks in Sec 4. For each task, we describe what the sentiment elements in the input and output are, what its relation with other tasks is, what the existing solutions are, especially recent progress achieving state-of-the-art performance, as well as general observations and conclusions from previous studies."
        },
        {
            "heading": "2.3 Modeling Paradigms",
            "text": "Before describing specific ABSA tasks and their solutions, we introduce several mainstream natural language processing (NLP) modeling paradigms that are commonly employed for ABSA tasks, including Sequence-level Classification (SeqClass), Token-level Classification (TokenClass), Machine Reading Comprehension (MRC), and Sequence-toSequence modeling (Seq2Seq). Each paradigm denotes a general computational framework for handling a specific input and output format. Therefore, by formulating a task as the specific format, the same paradigm can be used to solve\n4\nmultiple tasks [106]. Besides these four unified paradigms which tackle the task in an end-to-end fashion, some complicated ABSA tasks can be solved by the Pipeline (Pipeline) paradigm which pipes multiple models to make the final prediction. In Table 1, we present representative studies of different modeling paradigms for each task.\nWe denote a dataset corresponding to a certain ABSA task as D = {Xi, Yi}|D|i=1, where Xi and Yi are the input and the ground-truth label of the i-th data instance2 respectively. We then use such a notation to describe each paradigm.\n2.3.1 Sequence-level Classification (SeqClass) For the sequence-level classification, a model typically first feeds the input text X into an encoder Enc(\u00b7) to extract the task-specific features, followed by a classifier CLS(\u00b7) to predict the label Y :\nY = CLS(Enc(X)), (1)\nwhere Y can be represented as one-hot or multi-hot vectors (for single-label and multi-label classification, respectively). In the era of deep learning, the encoder Enc(\u00b7) could be convolutional networks [107], recurrent networks [108], or Transformers [109] for extracting contextual features. In some cases, the input text X may contain multiple parts, e.g., for the aspect sentiment classification task, both the sentence and a specific aspect are treated as the input. Then the encoder needs to not only extract useful features, but also capture the interactions between the inputs. The classifier CLS(\u00b7) is usually implemented as a multi-layer perceptron with a pooling layer to make the classification.\n2.3.2 Token-level Classification (TokenClass) In contrast to the sequence-level classification that assigns the label to the whole input text, token-level classification (also referred to as sequence labeling or sequence tagging) assigns a label to each token in the input text. It also first encodes the input text into contextualized features with an encoder Enc(\u00b7), while employs a decoder Dec(\u00b7) to predict the labels y1, ..., yn for each token x1, ..., xn in the input X :\ny1, ..., yn = Dec(Enc(x1, ..., xn)), (2)\n2. Since most ABSA tasks are studied in the supervised setting, we mainly describe the modeling paradigm with the supervised learning framework in this section. We also discuss unsupervised settings and methods for specific tasks in later sections.\nwhere Dec(\u00b7) can be implemented as either a multi-layer perceptron with a softmax layer, or conditional random fields (CRF) [110]. Different tagging schemes can also be used, e.g., the BIOES tagging scheme (B-beginning, I-inside, O-outside, E-ending, S-singleton) [111].\n2.3.3 Machine Reading Comprehension (MRC) The MRC paradigm [112] extracts continuous text spans from the input text X conditioned on a given query Xq . Therefore, ABSA methods with the MRC paradigm need to construct a task-specific query for the corresponding task, i.e., a query denoting what is the desired information. For example, Xq can be constructed as \u201cWhat are the aspect terms?\u201d in the ATE task. The original text, as well as the constructed query can then be used as the input to a MRC model to extract the text spans of aspect terms. It produces the result through predicting the starting position ys and the ending position ye of the text span:\nys, ye = CLS(Enc(X,Xq)), (3)\nwhere there are typically two linear classifiers, stacked on top of an encoder Enc(\u00b7), for predicting the starting and the ending positions, respectively.\n2.3.4 Sequence-to-Sequence (Seq2Seq) The sequence-to-sequence (Seq2Seq) framework takes an input sequence X = {x1, ..., xn} as input and aims to generate an output sequence Y = {y1, ..., ym}. A classical NLP application with such a paradigm is the machine translation task [113]. It is also used for solving ABSA tasks, e.g., directly generating the label sequence or desired sentiment elements given the input sentence. Taking the ATE task as an example, X can be \u201cThe fish dish is fresh\u201d, and Y can be \u201cfish dish\u201d in the natural language form. It typically adopts an encoder-decoder model such as Transformer [109]:\ny1, ..., ym = Dec(Enc(x1, ..., xn)), (4)\nwhere the encoder Enc(\u00b7) encodes contextualized features of the input, the decoder Dec(\u00b7) generates a token at each step, based on the encoded input and the previous output.\n2.3.5 Pipeline Method (Pipeline) The pipeline method is usually used for tackling compound ABSA tasks due to their complexities. As the name suggests,\n5\nit sequentially pipes multiple models with possibly different modeling paradigms to obtain the final result. The prediction of the former model is used as the input for the latter model, until the final output is produced. For example, the aforementioned AOPE problem aims to extract all (aspect term, opinion term) pairs. Therefore, one straightforward pipeline-style solution is to first use an ATE model to extract potential aspect terms, then employ another model to identify the corresponding opinion terms for each predicted aspect term. The valid predictions can then be organized as (aspect term, opinion term) pairs as the final results.\nCompared with unified paradigms described in the previous sections that solve the original problem in an end-toend manner, the pipeline method is usually easier to implement, since solutions to each sub-problem often already exist. However, it suffers from the error propagation issue, i.e., the errors produced by early models would propagate to the later models and affect the final overall performance. Return to the above example, if the ATE model produces wrong predictions, the final pair extraction results would be incorrect no matter how accurate the second model is. Given the imperfect performance of even simple ABSA tasks, pipeline methods often perform poorly on compound ABSA tasks, especially the complex ones. This often serves as the main motivation to design a unified model to handle the compound ABSA tasks in recent years, as we will discuss in the later sections."
        },
        {
            "heading": "2.4 Datasets & Evaluations",
            "text": "Annotated datasets play an essential role in the development of ABSA methods. This section presents some commonly used datasets and the corresponding evaluation metrics. An overview of each dataset with its language, data domain, annotated sentiment elements, and URL is summarized in Table 2.\nDatasets provided by SemEval-2014 [114], SemEval2015 [115], and SemEval-2016 [116] shared tasks are the most widely used benchmarks in the literature. User reviews from two domains, namely laptops and restaurants, are collected and annotated by the workshop organizers. These datasets contain annotations of aspect categories, aspect terms, and sentiment polarities (although not all of them contain all\ninformation), and thus can be directly used for many ABSA tasks such as aspect term extraction or aspect sentiment classification. However, the annotation of opinion terms is lacking, which is later compensated by the dataset provided in [45] for conducting the target-oriented opinion word extraction (TOWE) task. With these annotations available, Xu et al. [92] organize them together (with minor refinements) to obtain the ASTE-Data-V2 dataset, which contains (aspect term, opinion term, sentiment polarity) triplets for each sample sentence3 . Most recently, two datasets including ACOS [103] and ABSA-QUAD [7] for the sentiment quad prediction task have been introduced, where each data instance of them contains the annotations of the four sentiment elements in the quad format.\nBesides datasets used for tackling various ABSA tasks, datasets with other special focus have also been introduced. Jiang et al. [118] present a Multi-Aspect Multi-Sentiment (MAMS) dataset where each sentence in MAMS contains at least two aspects with different sentiment polarities, thus making the dataset more challenging. Wang et al. [117] propose a Chinese dataset for conducting the ASC task in the QA-style reviews (ASC-QA). Xing et al. [119] construct an Aspect Robustness Test Set (ARTS), based on the SemEval-2014 dataset, to probe the robustness of ABSA models. Recently, Bu et al. [120] release a large-scale Chinese dataset called ASAP (stands for Aspect category Sentiment Analysis and rating Prediction). Each sentence in ASAP is annotated with sentiment polarities of 18 pre-defined aspect categories and the overall rating, so it can also be useful to study the relationship between the coarse-grained and finegrained sentiment analysis task.\nRegarding the evaluation metrics, exact-match evaluation is widely adopted for various tasks and datasets: a prediction is correct if and only if all the predicted elements are the same as the human annotations. Then the typical classification metrics such as accuracy, precision, recall, and F1 scores can be calculated accordingly and used to compare among different methods.\n3. This dataset is based on the one used in Peng et al. [12], which they refer to as ASTE-Data-V1. It is further refined with some missing annotations, and the newly obtained dataset is thus called \u201cV2\u201d.\n6"
        },
        {
            "heading": "3 SINGLE ABSA TASKS",
            "text": "We first discuss single ABSA tasks in this section, whose target is to predict a single sentiment element only. As introduced in Sec 2.1, there are four tasks (corresponding to four sentiment elements), namely the aspect term extraction (ATE), aspect category detection (ACD), aspect sentiment classification (ASC), and opinion term extraction (OTE). A detailed taxonomy and representative methods are listed in the top branch of Fig. 2."
        },
        {
            "heading": "3.1 Aspect Term Extraction (ATE)",
            "text": "Aspect term extraction is a fundamental task of ABSA, aiming to extract explicit aspect expressions on which users express opinions in the given text. For example, two aspect terms \u201cpizza\u201d and \u201cservice\u201d are supposed to be extracted for the example sentence \u201cThe pizza is delicious, but the service is terrible.\u201d in Table 3. According to the availability of labeled data, ATE methods can be categorized into three types: supervised, semi-supervised, and unsupervised methods.\nSupervised ATE problem is often formulated as a tokenlevel classification (i.e., TokenClass) task since the desired aspect terms are usually single words or phrases appeared in the sentence. Therefore, sequence labeling methods based on CRF [22], RNN [8], and CNN [27] have been proposed. Since ATE requires domain-specific knowledge to identify aspects in a given domain, many research efforts are dedicated to improving word representation learning. Yin et al. [22] utilize the dependency path to link words in the embedding space for learning word representations. DE-CNN model proposed by Xu et al. [27] employs a dual embedding mechanism, including general-purpose and domain-specific embeddings. Xu et al. [62] further post-train BERT on domain-specific data to obtain better word representations. Yin et al. [30] design a positional dependency-based word embedding (POD) to consider both dependency relations and positional context. Specific network designs have been also proposed, e.g., modeling the relation between an aspect and its corresponding opinion expression [23, 25, 24, 26], and transforming the task to a Seq2Seq problem to capture the overall meaning of the entire sentence to predict the aspect with richer contextual information [28].\nAlthough achieving satisfactory results (e.g., around 80% F1 scores on benchmark datasets), supervised ATE methods require a large amount of labeled data, especially when training very sophisticated neural models. This motivates the recent trend on semi-supervised ATE studies. Given a set of labeled ATE data, as well as a (comparatively) large unlabeled dataset (e.g., plain review sentences), data augmentation is an effective solution to produce more pseudo-labeled data for training the ATE model. Various augmentation strategies have been proposed, such as masked sequenceto-sequence generation [31], soft prototype generation [32], and progressive self-training [33].\nUnsupervised ATE task aims to extract aspect terms without any labeled data and has been extensively studied in the literature [121]. In the context of neural network based methods, He et al. [34] present an autoencoder model named Attention-based Aspect Extraction (ABAE) that deemphasizes the irrelevant words to improve the coherence of extracted aspects. Following this direction, Luo et al.\n[35] leverage sememes to enhance lexical semantics when constructing the sentence representation. Liao et al. [36] propose a neural model to couple both local and global context (LCC+GBC) to discover aspect words. Tulkens and van Cranenburgh [41] propose a simple solution named CAt where they only use a POS tagger and in-domain word embeddings to extract the aspect terms: the POS tagger first extracts nouns as the candidate aspect, a contrastive attention mechanism is then employed to select aspects. Shi et al. [42] formulate the problem as a self-supervised contrastive learning task to learn better aspect representations."
        },
        {
            "heading": "3.2 Aspect Category Detection (ACD)",
            "text": "Aspect category detection is to identify the discussed aspect categories for a given sentence, where the categories belong to a pre-defined category set and is often domainspecific [114]. As shown in Table 3, an ACD method should predict the category food and service given the sentence. Compared with the ATE task, ACD can be beneficial from two perspectives: Firstly, ATE predicts individual aspect terms while the predicted category of ACD can be regarded as an aggregated prediction, which is more concise to present the opinion target. Secondly, ACD can identify the opinion targets even when they are not explicitly mentioned. For example, given a sentence \u201cIt is very overpriced and not tasty\u201d, ACD can detect two aspect categories price and food, whereas ATE is not applicable to such a case.\nACD can be classified into supervised ACD and unsupervised ACD, depending on whether the annotated data is available. The supervised ACD task is usually formulated as a multi-label classification (i.e., SeqClass) problem, treating each aspect category as a label. An early work named RepLearn [37] trains the word embedding on a noisy labeled dataset and obtains hybrid features through different feed-forward networks. A logistic regression model is then trained with such features to make the prediction. Later methods further leverage different characteristics of the task to improve the performance, e.g., using attention mechanism to attend to different parts of the text for different categories [38], considering the word-word co-occurrence patterns [122], and measuring the text matching between the sentence and a set of representative words in each specific category to predict whether a category exists [39].\nTo tackle the ACD task in an unsupervised manner, it is often decomposed into two steps: (1) extracting candidate aspect terms, and (2) mapping or clustering the aspect terms to aspect categories in a pre-defined category set [34], e.g., clustering \u201cpizza\u201d and \u201cpasta\u201d to the aspect category food. The first step is essentially the same as tackling the unsupervised ATE problem. For the second step, the most straightforward solution is to manually assign a label for each detected aspect cluster from the first step as the aspect category [34, 35], but it is time-consuming and may lead to errors when the detected aspects are noisy. In CAt [41], the cosine similarity between the sentence vector and the category vector is computed to assign the category label. Most recently, Shi et al. [42] propose a high-resolution selective mapping strategy to improve the mapping accuracy.\n7"
        },
        {
            "heading": "3.3 Opinion Term Extraction (OTE)",
            "text": "Opinion term extraction (OTE) is the task to identify opinion expressions towards an aspect. Since the opinion term and aspect term always co-occur, solely extracting the opinion term without considering its associated aspect is meaningless. Therefore, depending on whether the aspect term appears in the input or output, OTE can be roughly divided into two tasks: 1) aspect opinion co-extraction (AOCE) and 2) target-oriented opinion words extraction (TOWE).\nAspect opinion co-extraction (AOCE) attempts to predict the aspect and opinion terms together. For the running example in Table 3, the target output of AOCE is thus two aspect terms \u201cpizza\u201d and \u201cservice\u201d, as well as two opinion terms \u201cdelicious\u201d and \u201cterrible\u201d. Note that although two sentiment elements are involved here, AOCE is still a single ABSA task since the dependency relation between the two sentiment elements (e.g., \u201cdelicious\u201d is used to describe \u201cpizza\u201d) is not considered4. Very often, it is formulated as a TokenClass problem with either two label sets to extract aspect and opinion terms separately [43], or a unified label set (e.g., {B-A, I-A, B-O, I-O, N} denotes the beginning (B) or inside (I) of an aspect (-A) or opinion term (-O), or none of them (N)) to extract both sentiment elements simultaneously [123, 44]. Considering the close relationship between the aspect and opinion, the main research question of AOCE is how to model such a dependency relation. Various models have been developed to capture the aspect-opinion dependency, including dependency-tree based models [22, 23], attention-based models [25, 24, 26], and the models considering the syntactic structures to explicitly constrain the prediction [43, 44].\nOn the other hand, TOWE aims to extract the corresponding opinion terms given a specific aspect term within the text [45]. As shown in Table 3, an aspect of interest (e.g., \u201cpizza\u201d) is assumed to be given with the sentence, then a TOWE model aims to predict the corresponding opinion\n4. In fact, we can directly call this task as \u201cOTE\u201d if we neglect the prediction of aspect terms. But purely extracting opinion expressions without their targets is meaningless, leading to the AOCE task.\nterm (e.g., \u201cdelicious\u201d). TOWE is also often formulated as a TokenClass problem towards the input sentence, whereas the main research problem becomes how to model the aspect-specific representation in the input sentence to extract the corresponding opinions. Fan et al. [45] propose a neural model to incorporate the aspect information via an inward-outward LSTM to generate the aspect-fused context. Later methods manage to enhance the accuracy of extraction from several aspects: Wu et al. [124] utilize the general sentiment analysis dataset to transfer the latent opinion knowledge for tackling TOWE. Veyseh et al. [47] leverage the syntactic structures such as the dependency tree-based distance to the aspect to help identify the opinion terms. Mensah et al. [48] empirically evaluate the importance of positional embeddings based on various text encoders and find that BiLSTM-based methods have an inductive bias appropriate for the TOWE task, and using a GCN [125] to explicitly consider the structure information only brings minor performance gains."
        },
        {
            "heading": "3.4 Aspect Sentiment Classification (ASC)",
            "text": "Aspect sentiment classification (ASC), also called aspectbased/-targeted/-level sentiment classification, aims to predict the sentiment polarity for a specific aspect within a sentence. Generally, the aspect can be instantiated as either aspect term or aspect category, yielding two ASC problems: aspect term-based sentiment classification and aspect category-based sentiment classification. Regardless of some subtle differences (e.g., the given aspect term comes from the sentence, then its position information can be exploited), the main research question underlying these two settings is the same: how to appropriately exploit the connection between the aspect (term/category) and the sentence context to classify the sentiment. In fact, some works consider these two subtasks at the same time and tackle them seamlessly with the same model [52, 126, 58, 127]. Therefore, we do not specifically differentiate these two subtasks in this section and use \u201caspect\u201d to refer to either an aspect term or an aspect category.\n8 Deep learning based ASC has attracted a lot of interest and a variety of neural network based models have been proposed and brought large performance improvements [50, 14, 15]. To model the interaction between the aspect and sentence context, pioneering neural models such as TC-LSTM [51] employ relatively simple strategies such as concatenation to fuse the aspect information with the sentence context. Based on the intuition that different parts of the sentence play different roles for a specific aspect, the attention mechanism is widely employed to obtain aspectspecific representations [52, 126, 55, 57, 128, 70]. A repre-\nsentative work is Attention-based LSTM with Aspect Embedding (ATAE-LSTM) model proposed by Wang et al. [52] which appends the aspect embedding to each word vector of the input sentence for computing the attention weight, and an aspect-specific sentence embedding can be computed accordingly to classify the sentiment. The following methods design more complicated attention mechanisms to learn better aspect-specific representations, for instance, IAN [54] interactively learns attention in the aspect and sentence, and generate the representations for them separately. Apart from the LSTM network, other network structures have also been explored for supporting the attention mechanism, including the CNN-based network [58, 59], memory network [53, 56], and the gated network [129, 58]. Recently, pre-trained language models have become the mainstream building block for the ASC task [130, 62, 131, 132]. For example, Sun et al. [130] transform the ASC task as a sentence pair classification problem by constructing an auxiliary sentence, which can better utilize the sentence pair modeling ability of BERT.\nAnother line of the ASC research explicitly models the syntactic structure of the sentence to make the prediction, since the structural relation between the aspect and its associated opinion often indicates the sentiment orientation. In fact, earlier machine learning based ASC systems already take mined syntactic trees as features for the classification [133, 134]. However, as the dependency parsing itself is a challenging NLP task, ASC methods with inaccurate parsers did not show clear advantages than other methods [132]. Thanks to the improvements from the neural network based dependency parsing in recent years, more accurate parse trees have brought significant improvements for the dependency-based ASC model. For example, Sun et al. [67] and Zhang et al. [66] employ the graph neural network (GNN) [135] to model the dependency tree for exploiting the syntactical information and word dependencies. Following this direction, a variety of GNN-based methods have been proposed to explicitly leverage the syntactic information [136, 68, 137, 138, 69, 73, 71, 72, 75]. Besides the syntactic structure inside the sentence, other structural information has also been considered. Ruder et al. [139] model the relation between multiple review sentences, with the assumption that they build and elaborate upon each other and thus their sentiments are also related. Similarly, Chen et al. [140] consider the document-level sentiment preference to fully utilize the information in the existing data to improve the ASC performance.\nAOPE\nOTE\nATE\nASC\nACD\nE2E-ABSA\nACSA\nASTE\nACSD\nASQP\nOpinion Term\nAspect Term\nSentiment Polarity\nAspect Category\nFig. 3. The relations between the four sentiment elements, single ABSA tasks, and compound ABSA tasks."
        },
        {
            "heading": "4 COMPOUND ABSA TASKS",
            "text": "We then describe compound ABSA tasks whose target involves multiple sentiment elements. A detailed task taxonomy and representative methods are shown in the bottom branch of Fig. 2. Very often, these tasks can be treated as integrated tasks of the aforementioned single ABSA tasks. However, the goal of these compound tasks is not only the extraction of multiple sentiment elements, but also coupling them by predicting the elements in the pair (i.e., two elements), triplet (i.e., three elements), or even quad (i.e., four elements) format. Fig. 3 shows the relation between these tasks. Considering the inter-related dependency of the four sentiment elements, providing an integrated solution is a promising direction. Many research efforts have been made recently, which we systematically review in this section."
        },
        {
            "heading": "4.1 Aspect-Opinion Pair Extraction (AOPE)",
            "text": "As discussed in Sec 3.3, studies of the aspect opinion coextraction (AOCE) task often found that the extraction of each element can mutually reinforce each other. However, the output of the AOCE task contains two separate sets: an aspect set and an opinion set. The corresponding pairwise relation is neglected. This motivates the task of aspectopinion pair extraction (AOPE), aiming to extract the aspect and opinion terms in pairs so as to provide a clear picture of what the opinion target is and what the corresponding opinion expression is [13, 11].\nTo tackle AOPE, one can adopt the pipeline approach to decouple it into several subtasks and pipe them together to obtain the aspect-opinion pairs. One solution is to first conduct the AOCE task for obtaining the aspect and opinion sets, then employ a classification model to pair the potential aspect and opinion terms, i.e., classify whether an aspectopinion pair is valid. An alternative method is to first extract the aspect (i.e., the ATE task), then identify the corresponding opinion terms for each predicted aspect term (i.e., the TOWE task). Gao et al. [77] take the second approach with the MRC paradigm where they first use an MRC model to extract all aspect terms, then for each extracted aspect term, a query is constructed for another MRC model to identify the text span of the corresponding opinion term.\nEfforts have also been made to tackle AOPE in a unified manner, for alleviating the potential error propagation of the pipeline approach. Wu et al. [76] propose a grid tagging scheme (GTS): for each word pair, the model predicts\n9\nwhether they belong to the same aspect, the same opinion, the aspect-opinion pair, or none of the above. Then the original pair extraction task is transformed into a unified TokenClass problem. Zhao et al. [13] treat the problem as a joint term and relation extraction, and design a spanbased multi-task learning (SpanMlt) framework to jointly extract the aspect/opinion terms and the pair relation: a span generator is first used to enumerate all possible spans, then two output scorers assign the term label and evaluate the pairwise relations. Similarly, Chen et al. [11] propose a model containing two channels to extract aspect/opinion terms, and the relations respectively. Two synchronization mechanisms are further designed to enable the information interaction between two channels. More recently, syntactic and linguistic knowledge is also considered for improving the extraction performance [78]."
        },
        {
            "heading": "4.2 End-to-End ABSA (E2E-ABSA)",
            "text": "Given a sentence, End-to-End ABSA is the task of extracting the aspect term and its corresponding sentiment polarity simultaneously, i.e., extracting the (a, p) pairs5. It can be naturally broken down into two sub-tasks, namely ATE and ASC [84], and an intuitive pipeline method is to conduct them sequentially. However, detecting the aspect boundary and classifying the sentiment polarity can often reinforce each other. Taking the sentence \u201cI like pizza\u201d as an example, the context information \u201clike\u201d indicates a positive sentiment and also implies that the following word \u201cpizza\u201d is the opinion target. Inspired by such an observation, many methods have been proposed to tackle the problem in an end-to-end manner.\nThese end-to-end methods can be generally divided into two types [10, 79], as shown in Table 4. The first \u201cjoint\u201d method exploits the relation between two subtasks via training them jointly within a multi-task learning framework [82, 83, 85, 86, 87]. Two label sets including the aspect boundary label (the first row) and the sentiment label (the second row) are adopted to predict the two types of sentiment elements. Then the final prediction is derived from the combination of the outputs of two subtasks. Another type of method dismisses the boundary of these two subtasks and employs a \u201cunified\u201d (also called collapsed) tagging scheme to denote both sentiment elements in the tag of each token [80, 81, 19]. As shown in the last row of Table 4, the tag for each token now contains two parts of information: the first\n5. Since extracting mentioned aspects and classifying their sentiments lies in the core of ABSA problem [1], it is often directly referred to as the \u201cABSA problem\u201d. In recent years, to differentiate this task from the general ABSA problem (consisting of multiple tasks), it is called end-toend ABSA [19, 83] or unified ABSA [85, 96]. Following this convention, we thus take the name E2E-ABSA here to denote this task.\npart {B, I, E, S, O} denotes the boundary of the aspect (Bbeginning, I-inside, O-outside, E-ending, S-singleton) [111], the second part {POS, NEG, NEU} is the sentiment polarity of the corresponding token. For instance, B-NEG refers to the beginning of an aspect whose sentiment is negative. By using a collapsed label scheme, the E2E-ABSA task can be tackled with the TokenClass paradigm via a standard sequence tagger [19].\nWhichever type of method is adopted, some ideas are often shared and appear frequently in different models. For example, considering the relation between the aspect boundary and sentiment polarity has shown to be an important factor [81]. As opinion terms provide indicative clues for the appearance of aspect terms and the orientation of the sentiment, opinion term extraction is often treated as an auxiliary task [81, 83, 87, 88]. For example, the relation-aware collaborative learning (RACL) framework [85] explicitly models the interactive relation of three tasks with a relation propagation mechanism to coordinate these tasks. Liang et al. [87] further design a routing algorithm to improve the knowledge transfer between these tasks. Document-level sentiment information is also used to equip the model with coarse-grained sentiment knowledge, so as to better classify the sentiment polarity [83, 87].\nRegarding these three types of methods for tackling E2EABSA (i.e., pipeline, joint, and unified method), it is still unclear which one is the most suitable. Early works such as [10] found that the pipeline method performs better, but Li et al. [81] show that using a tailor-made neural model with the unified tagging scheme gives the best performance. Later, Li et al. [19] further verify that using a simple linear layer stacked on top of the pre-trained BERT model with the unified tagging scheme can achieve promising results, without complicated model design. More recently, research works based on either pipeline [95, 94], unified [141], or the joint method [87] all achieve good performance, i.e., around 70% F1 scores on benchmark datasets. This makes the comparison between different types of methods unclear and needs further exploration."
        },
        {
            "heading": "4.3 Aspect Category Sentiment Analysis (ACSA)",
            "text": "Aspect category sentiment analysis (ACSA) aims to jointly detect the discussed aspect categories and their corresponding sentiment polarities. For example, an ACSA model is expected to predict two category-sentiment pairs (food, POS) and (service, NEG) for the example in Table 3. Though ACSA is similar to the E2E-ABSA task (only the format of the aspect is different), the results of ACSA can be provided regardless of whether the aspect is implicit or explicitly mentioned in the sentence, thus ACSA is widely used in industries [120].\n10\nThe most straightforward method to tackle ACSA is the pipeline approach: first detecting the mentioned aspect categories (i.e., the ACD task), then predicting the sentiment polarities for those detected categories (i.e., the ASC task). However, the detection of a subset of the aspect categories appearing in the sentence is nontrivial, as discussed in Sec 3.2. The errors from the first step would severely limit the performance of the overall pair prediction. Moreover, the relations between these two steps are ignored, which is found to be important for both tasks [142]. In fact, performing these two tasks in the multi-task learning framework has shown to be beneficial for each separate task [143, 142, 144].\nIn essence, the ACD task is a multi-label classification problem (treating each category as a label), and the ASC task is a multi-class classification problem (where each sentiment polarity is a class) for each detected aspect category. As shown in Fig. 4, existing methods of tackling ACSA in a unified manner can be roughly categorized into four types: (1) Cartesian product, (2) add-one-dimension, (3) hierarchy classification, and (4) Seq2Seq modeling. The Cartesian product method enumerates all possible combinations of category-sentiment pairs by a Cartesian product. Then a classifier takes both the sentence and a specific categorysentiment pair as input, the prediction is thus a binary value, indicating whether such a pair holds [46]. However, it generates the training set several times larger than the original one, greatly increasing the computation cost. An alternative solution is to add one extra dimension to the prediction of the aspect category. Previously, for each aspect category, we predict its sentiment polarity, which normally has three possibilities: positive, negative, and neutral. Schmitt et al. [89] add one more dimension called \u201cN/A\u201d denoting whether the category appears in the sentence or not, thus handling the ACSA in a unified way.\nCai et al. [90] propose a hierarchical GCN-based method named Hier-GCN: a lower-level GCN first captures the relations between categories, then a higher-level GCN is used to capture the relations between categories and categoryoriented sentiments. Finally, an integration module takes the interactive features as inputs to perform the hierarchy prediction. Similarly, Li et al. [145] utilize a shared sentiment prediction layer to share the sentiment knowledge between different aspect categories to alleviate the data deficiency issue. Recently, Liu et al. [91] adopts the Seq2Seq modeling paradigm to tackle the ACSA problem. Based on a pretrained generative model, they use natural language sen-\ntences to represent the desired output (see Fig. 4(4)) which outperforms previous classification type models. Moreover, the experimental results suggest that such paradigm can better utilize the pre-trained knowledge and have large advantages in few-shot and zero-shot settings."
        },
        {
            "heading": "4.4 Aspect Sentiment Triplet Extraction (ASTE)",
            "text": "The aspect sentiment triplet extraction (ASTE) task attempts to extract (a, o, p) triplets from the given sentence, which tells what the opinion target is, how its sentiment orientation is, and why such a sentiment is expressed (through the opinion term) [12]. Therefore, a model which can predict opinion triplets shows more complete sentiment information, compared with previous models working for individual tasks. The ASTE task has attracted lots of attention in recent years. A variety of frameworks with different paradigms have been proposed for the ASTE task, we show some representative works of each paradigm in Fig. 5.\nPeng et al. [12] first introduce the ASTE task and propose a two-stage pipeline method for extracting the triplets. As shown in Fig. 5(a), two sequence tagging models are first performed to extract aspects with their sentiments, and the opinion terms respectively. In the second stage, a classifier is utilized to find the valid aspect-opinion pairs from the predicted aspects and opinions and finally construct the triplet prediction. To better exploit the relations of multiple sentiment elements, many unified methods have been proposed. Zhang et al. [93] present a multi-task learning framework including aspect term extraction, opinion term extraction, and sentiment dependency parsing tasks. Then heuristic rules are applied to produce the sentiment triplets from the predictions of those subtasks. Another potential direction is to design unified tagging schemes to extract the triplet in one-shot [92, 76]: JET model proposed by Xu et al. [92] utilizes a position-aware tagging scheme which extends the previous unified tagging scheme of the E2E-ABSA task [81] with the position information of the opinion term, as depicted in Fig. 5(b). Similarly, Wu et al. [76] extend the grid tagging scheme (GTS) for the AOPE task described in Sec 4.1 to also make a prediction on the sentiment polarity. Since those methods rely on the interactions between word pairs, they may not perform well when the aspect terms or the opinion terms are multi-word expressions. Motivated by this observation, Xu et al. [98] propose a span-level interaction model which explicitly considers the interactions\n11\nbetween the whole spans of aspects and those of opinions to improve the performance.\nOther modeling paradigms such as MRC (see Fig. 5(c)) [94, 95] and Seq2Seq modeling (see Fig. 5(d)) [97, 96, 100, 99] have also been employed for tackling ASTE. Mao et al. [95] transform the original problem as two MRC tasks by designing specific queries: the first MRC model is used to extract the aspect terms, the second MRC model then predicts the corresponding opinion term and sentiment polarity. Chen et al. [94] take a similar approach while they use a bidirectional MRC framework: one predicts aspect term then opinion term, another first predicts the opinion then the aspect. Seq2Seq modeling provides an elegant solution to make the triplet prediction in one shot. Zhang et al. [96] transform the original task as a text generation problem and propose two modeling paradigms including the annotation style and extraction style for predicting the sentiment triplets. Yan et al. [97] and Hsu et al. [146] take the sentence as input and treat the pointer indices as the target. Then to predict the aspect term (or the opinion term), the target becomes predicting the starting index and ending index of the term. Fei et al. [99] presents a non-autoregressive decoding (NAG-ASTE) method which models the ASTE task as an unordered triplet set prediction problem."
        },
        {
            "heading": "4.5 Aspect-Category-Sentiment Detection (ACSD)",
            "text": "Although the aspect category and aspect term can both serve as the opinion target when analyzing aspect-level sentiment, the sentiment often depends on both of them [147]. To capture such a dual dependence, Wan et al. [46] propose to detect all (aspect category, aspect term, sentiment polarity) triplets for a given sentence6. They separate the joint prediction task into two subtasks on the basis of (aspect category, sentiment polarity) pairs, whose idea is similar to the \u201cCartesian Product\u201d for the ACSA task\n6. The authors call \u201caspect category\u201d and \u201caspect term\u201d as \u201caspect\u201d and \u201ctarget\u201d respectively in the original paper. Here we use unified terminologies to refer to those sentiment elements.\ndescribed in Sec 4.3. Therefore, given a sentence with a specific combination of the aspect category and sentiment, the remaining problems are: whether any aspect terms exist for such a combination, and if so, what the aspect terms are? The former one can be formulated as a binary SeqClass task, and the latter becomes a conditional TokenClass problem. For example, given the sentence \u201cThe pizza is delicious\u201d with (food, POS) pair, the first subtask would predict that this combination exists and the sequence labeling model should extract \u201cpizza\u201d as the corresponding aspect term. Then a triplet (food, POS, pizza) can be output as a prediction. However, when receiving the same sentence with the (service, POS) pair as input, the first subtask is supposed to predict this combination does not exist. The overall training objective can be the combined loss of these two subtasks.\nFollowing this direction, Wu et al. [102] propose a model called MEJD which handles the task by using the sentence and a specific aspect category as input, then the remaining problems becomes: (1) predicting the sentiment polarity for the given category (i.e., a SeqClass problem), and (2) extract the corresponding aspect terms if exist (i.e., a TokenClass problem). Since a specific aspect category may not always exist in the concerned sentence, MEJD adds an extra dimension \u201cN/A\u201d in the SeqClass task, sharing the similar idea of the \u201cadd-one-dimension\u201d method [89] introduced in Sec 4.3. Therefore, when the classification model outputs \u201cN/A\u201d, it shows that there is no triplet related to the category in the input. Moreover, a GCN with an attention mechanism is employed in MEJD to capture the dependency between the aspect and the context.\nAs the number of predefined aspect categories for a specific domain is generally small, the aforementioned methods can decompose the original ACSD task by combining the sentence with each category as the input. Instead, Zhang et al. [96] tackle the problem in a Seq2Seq manner where they append the desired sentiment elements in the original sentence and treat it as the target sequence for a generation\n12\nmodel to learn the mapping relation. Zhang et al. [7] further design a Paraphrase model which constructs a natural language sentence containing all the sentiment elements as the target sequence for the sequence-to-sequence learning."
        },
        {
            "heading": "4.6 Aspect Sentiment Quad Prediction (ASQP)",
            "text": "The primary motivation of various compound ABSA tasks discussed above is to capture more detailed aspect-level sentiment information, either in the format of pair extraction (e.g., AOPE) or triplet extraction (e.g., ASTE). Although they can be useful under different scenarios, a model which can predict the four sentiment elements in one shot is supposed to provide the most complete aspect-level sentiment structure. This leads to the aspect sentiment quad prediction (ASQP) task proposed recently7 [103, 7], aiming to predict all the four sentiment elements in the quadruplet form given a text item. Returning to the example in Table 3, two sentiment quads are expected: (food, pizza, POS, delicious) and (service, service, NEG, terrible).\nCai et al. [103] study the ASQP task with an emphasis on the implicit aspects or opinions. The authors argue that implicit aspects or opinions appear frequently in real-world scenarios, and use \u201cnull\u201d to denote them in the sentiment quads. They introduce two new datasets with sentiment quad annotations and construct a series of Pipeline baselines by combining existing models to benchmark the task. Zhang et al. [7] propose a Paraphrase modeling strategy to predict the sentiment quads in an end-to-end manner. By combining the annotated sentiment elements with a prebuilt template and using the obtained natural language sentence as the target sequence, they transform the original quad prediction task to a text generation problem and tackle it via the Seq2Seq modeling paradigm. Therefore, the label semantics (i.e., the meaning of the sentiment elements) can be fully exploited. Following this direction, later methods further formulate the task as generating opinion trees [104, 105] or structured schema [101].\nCompared to other ABSA tasks, ASQP is the most complete and also the most challenging task. The main difficulty lies in the accurate coupling of different sentiment elements. Given the importance of the it and the potential large improvement space (e.g., the current best-performing models only achieve about 40% F1 scores on benchmark datasets), we expect to see more related studies in the future."
        },
        {
            "heading": "5 ABSA WITH PRE-TRAINED LANGUAGE MODELS",
            "text": "Conventional neural ABSA models usually couple the pretrained word embeddings, such as Word2Vec [148] and GloVe [149], with a well-designed task-specific neural architecture. Despite their effectiveness compared with early feature-based models, the improvement from these models gradually reached a bottleneck. One reason is that the context-independent word embeddings are insufficient for capturing the complex sentiment dependencies in the sentence. Besides, the sizes of existing ABSA datasets do not support the training of very complicated architecture. In recent years, pre-trained language models (PLMs) such\n7. It is also called Aspect-Category-Opinion-Sentiment (ACOS) quadruple extraction task in [103].\nas BERT [17] and RoBERTa [18] have brought substantial improvements on a wide range of NLP tasks. Naturally, they are also introduced for further improving the performance of the ABSA problem.\nThe initial works [62, 150, 151] do not spend too much effort on task-specific model designs, but simply introduce the contextualized embeddings from PLMs as the replacement of word embeddings. Given the rich knowledge learned in the pre-training stage, simply utilizing such contextualized embeddings already brings in a large performance gain. For instance, Li et al. [19] investigate the usage of stacking several standard prediction layers on top of a PLM for the E2E-ABSA task. They find that using the simplest linear classification layer with a PLM can outperform previous carefully-designed neural ABSA models. Similarly, simply concatenating the given sentence and a concerned aspect as the input to PLMs and utilizing the sentence-level output (e.g., representations corresponding to the [CLS] token for BERT) establishes new state-of-the-art results of the ASC task [62]. Moreover, the authors show that further posttraining the model on the domain and task related data can capture better domain- and task-specific knowledge, thus leading to better performance.\nHowever, simply adopting PLMs as the context-aware embedding layer might be insufficient. From the perspective of ABSA tasks, complicated tasks often require not only the identification of the sequence- or token-level labels but also the dependency relations between them, it thus needs more designs to make full use of the contextualized embeddings from PLMs. From the perspective of PLMs, the rich knowledge learned in the pre-training stage might not be sufficiently induced and utilized for the concerned ABSA tasks. To this end, many efforts have been made on better adapting the PLMs for different downstream ABSA tasks. An early attempt is Sun et al. [130] where they transform the ASC as a sentence pair classification task. Motivated by the observation that BERT has an advantage in tackling sentence pair classification problems such as question answering, they construct an auxiliary sentence for each aspect and feed the original sentence and the constructed sentence to BERT, achieving much better performance than previous works. Following similar intuition, Gao et al. [77], Chen et al. [94], Mao et al. [95] solve the AOPE task and ASTE task via the MRC modeling paradigm. By decomposing the original task as a series of MRC processes, the pairwise relation is thus naturally captured via the query-answer matching. Another line of work is to utilize the pre-trained generative models such as BART [152] and T5 [153] to solve various ABSA tasks [96, 97, 7, 91]. By transforming the original task to a Seq2Seq problem, the label semantics (i.e., the meaning of the desired sentiment elements) can be appropriately incorporated.\nIn addition to serving as the backbone of ABSA models, PLMs can benefit tackling ABSA tasks from other aspects. For example, the language modeling task used in the pretraining stage of PLMs often brings in the capability of performing generative data augmentation. Li et al. [31] employ PLMs as a conditional text generator and design a maskthen-predict strategy to generate the augmented training sentences for the ATE task. Hsu et al. [146] do not borrow the external linguistic resources but utilize the PLMs to achieve\n13\nsemantic-preserved augmentation in a generative manner, obtaining clear improvements over the baseline method on a series of ABSA tasks. Another interesting but largely ignored role of PLMs is to provide better dependency trees for various ABSA models, e.g., methods discussed in Sec 3.4. Explicitly utilizing the semantic relation can be beneficial for many ABSA tasks, but their performance heavily depends on the accuracy of the adopted dependency tree [133, 134, 132]. As the first attempt, Wu et al. [154] discover the dependency parse tree from PLMs with a tailor-made probing method and feed the obtained tree into a dependency-based ABSA model, achieving better ASC results than the model using the tree from the off-the-shelf parsers. Following them, Dai et al. [132] fine-tune the PLMs with the ASC data to inject the sentiment knowledge. The sentiment-oriented dependency tree is then induced from the fine-tuned PLMs, which further improves the performance of several state-of-the-art dependency-based models.\nSo far, the common viewpoint in the NLP community is that PLMs are capable of accurately reflecting the semantic meanings of input words [155]. However, the contextualized embeddings obtained via the self-attention mechanism that captures full word dependencies within the sentence are presumably redundant for the ABSA tasks. In fact, the superiority of the works [29, 138, 71], which explicitly guide the further transformation of PLM representations with meaningful structure, to those using \u201c[CLS]\u201d representation for predictions indirectly suggests the existence of such redundancy. How to consolidate meaningful and sparse structure with PLMs or refine the intrinsic fully-connected self-attention for obtaining ABSA-related representations in a more efficient way deserves more attention and research efforts. On the other hand, there is still room for improving the robustness of PLM-based ABSA models. Particularly, as observed in Xing et al. [119], even though the PLM-based models significantly outperform the previous neural models on the adversarial examples, it still suffers from more than 25% performance drop on the simplest ASC task. We believe that exploiting PLMs for truly understanding the aspectlevel sentiment, e.g., being robust to the reversed opinion and sentiment negation, instead of learning the spurious correlations between the aspect and sentiment labels is the future challenge for building PLM-based ABSA models. But there is still a long way to realize such kind of intelligence."
        },
        {
            "heading": "6 TRANSFERABLE ABSA",
            "text": ""
        },
        {
            "heading": "6.1 Cross-domain ABSA",
            "text": "The supervised ABSA models within a single domain have been well developed. However, in real-world scenarios which involve texts from multiple or even unseen domains, it is likely that these models fail to obtain satisfactory predictions. The major reason is that the aspects referring to the opinion target from different domains are usually of great difference, and the models may not have prior knowledge about the frequently-used terms in the unseen domains. A straightforward solution is to create labeled data for these domains and re-train additional in-domain models. Considering that the ABSA tasks require fine-grained annotations, it is often expensive or even impossible to\ncollect sufficient amount of labeled data. To enable the crossdomain ABSA predictions at lower cost, domain adaptation techniques [156, 20] are employed to provide alternative solutions for well generalizing the ABSA systems to other domains. Roughly speaking, the majority of these works can be separated into two groups: feature-based transfer and data-based transfer.\nThe core idea of feature-based transfer is to learn domain-independent representations for the ABSA tasks. Jakob and Gurevych [157] and Chernyshevich [158] instantiate this idea by introducing rich syntactic features, which are invariant across domains, into a CRF tagger for the cross-domain ATE task. Wang and Pan [123, 159] design a dependency edge prediction task to enforce the learning of syntactic-aware representations, with the aim of reducing the domain shift at the word level. Other auxiliary tasks, such as domain classification [160, 161], aspect-opinion interaction prediction [162] and opinion term detection [163], are also integrated to better align the representations in different domains. Different from the above studies, Chen and Qian [164] simply aggregate the syntactic roles for each word and regard the syntactic embedding as the bridge between the source domain and target domain, which considerably improves the efficacy of domain adaptation. Liang et al. [165] assume the availability of sentence-level aspect category annotations in the target domain and propose an interaction transfer network to capture the domaininvariant category-term correlations.\nCompared to feature-based transfer, data-based transfer aims to adjust the distribution of the training data to better generalize the ABSA model to the target domain [166, 167]. Ding et al. [168] employ high-precision syntactic patterns together with some domain-independent opinion terms to create pseudo-labeled data in the target domain. The pseudolabeled target data is then augmented to the source domain training set for building cross-domain ABSA models. Li et al. [169] build target-domain pseudo-labeled data in a similar way and perform re-weighting on the source domain training instances based on the pseudo-labeled data. Instead of producing supervision signals on the unlabeled targetdomain data, Yu et al. [166] develop an aspect-constrained (opinion-constrained) masked language model, which takes the source domain labeled reviews as input and perform inplace aspect term (opinion term) conversion from the source domain to the target domain as the silver training data.\nIn addition, Gong et al. [170] propose to couple a tokenlevel instance re-weighting strategy with domain-invariant representation learning from auxiliary tasks to consolidate feature-based transfer and data-based transfer for better domain adaptation for cross-domain E2E-ABSA. Pereg et al. [171] and Rietzler et al. [151] regard the embeddings from PLMs as the features for the ABSA predictions across different domains and obtain reasonable results, showing that the PLMs pre-trained on large-scale corpus are already able to provide good domain-independent representations. Xu et al. [172] further strengthen the domain specificity of PLMs by continually pre-training BERT with the unlabeled texts from multiple relevant domains, drastically improving the domain generalization capability of BERT on the E2EABSA task. Such advances suggest that the consolidation of feature-based transfer and data-based transfer is a bet-\n14\nter way for cross-domain ABSA and language model pretraining can be introduced as a plug-and-play component to further enhance the domain adaptation performance."
        },
        {
            "heading": "6.2 Cross-lingual ABSA",
            "text": "The majority of existing ABSA works are conducted on the resource-rich language (mostly in English), while opinions are often expressed in different languages in practice. However, annotating labeled data for each language can be time-consuming, which motivates the task of cross-lingual ABSA (XABSA). Due to the difficulty of cross-lingual transfer, most XABSA studies are conducted on simple ABSA tasks such as cross-lingual aspect term extraction (XATE) [173, 174, 175, 176], cross-lingual aspect sentiment classification (XASC) [177, 178, 179], and cross-lingual End-to-End ABSA [180, 181].\nTo realize the cross-lingual transfer, the key problem is to obtain the language-specific knowledge in the target language. Early methods typically rely on translation systems to obtain such knowledge. The sentence is first translated from the source to the target language with an off-the-shelf translation system. The label is then similarly projected from the source to target, either directly or with word alignment tools such as FastAlign [182] since some ABSA tasks (e.g., XATE) require token-level annotations. Therefore, an ABSA model can be trained with the obtained (pseudo-)labeled target language data. Because the performance of this type of method heavily relies on the quality of the translation and label projection, many techniques have been proposed to improve the data quality, including the co-training strategy [173], instance selection [174], or constrained SMT [177].\nCross-lingual word embeddings pre-trained on large parallel bilingual corpus have also been used for XABSA. By sharing a common vector space, the model can be used in a language-agnostic manner [178, 179]. For example, Wang and Pan [175] utilize a transition-based mechanism to tackle the XATE task by aligning representations in different languages into a shared space through an adversarial network. Jebbara and Cimiano [176] consider the zero-shot ATE task with two types of cross-lingual word embeddings. Especially, they find that transferring from multiple source languages can largely improve the performance.\nRecently, inspired by the success of exploiting monolingual PLMs, utilizing multilingual PLMs (mPLMs) such as multilingual BERT [17] and XLM-RoBERTa [183] to tackle cross-lingual NLP tasks has become a common practice. Typically, a PLM is first pre-trained on a large volume of multilingual corpus, then fine-tuned on the source language data for learning task-specific knowledge. Finally, it can be directly used to conduct inference on the target language testing data (called zero-shot transfer). Thanks to the language knowledge obtained in the pre-training stage, zeroshot transfer has shown to be an effective method for many cross-lingual NLP tasks [184, 185]. However, the language knowledge learned in the pre-training step may be insufficient for the XABSA problem. As compensation, utilizing the translated (pseudo-)labeled target language data can equip the model with richer target language knowledge. For example, Li et al. [180] propose a warm-up mechanism to distill the knowledge from the translated data in each language to enhance the performance. Zhang et al. [181] point\nout the importance of the translated target language data and propose an alignment-free label projection method to obtain high-quality pseudo-labeled target data. They show that even fine-tuning the mPLMs on such data can establish a strong baseline for the XABSA task.\nCompared with the monolingual ABSA problem, the XABSA problem is relatively underexplored. Although mPLMs are widely used for various cross-lingual NLP tasks nowadays, exploring their usage in the XABSA can be tricky since language-specific knowledge plays an essential role in any ABSA task. Therefore, it calls for better adaption strategies of the mPLMs to inject the model with richer target language knowledge. On the other hand, existing studies mainly focus on relatively easier ABSA tasks, exploring the cross-lingual transfer for more difficult compound ABSA tasks can be challenging while useful in practice."
        },
        {
            "heading": "7 CHALLENGES AND FUTURE DIRECTIONS",
            "text": "Over the last decade, we have seen great progress on the ABSA problem, either new tasks or novel methods. Despite the progress, there remains challenges for building more intelligent and powerful ABSA systems. In this section, we discuss some challenges, as well as potential directions which we hope can help advance the ABSA study."
        },
        {
            "heading": "7.1 Quest for Larger and More Challenging Datasets",
            "text": "As discussed in Sec 2.4, most existing ABSA datasets are derived from the SemEval shared challenges [114, 115, 116] with additional data processing and annotations for specific tasks. However, the relatively small size of data (e.g., hundreds of sentences) makes it difficult to clearly compare different models, especially for PLM-based models having millions of parameters. Currently, it is a common practice to train a model with different random seeds (often five or ten) and report the model performance with averaged scores across different runs, but it would be better to introduce larger datasets for more fair and reliable comparisons. Besides, although existing datasets provide valuable test beds for comparing different methods, there remains a great need for proposing more challenging datasets to satisfy the realworld scenarios. For example, datasets containing reviews from multiples domains or multiple languages can help evaluate multi-domain and multi-lingual ABSA systems. Moreover, since user opinions can be expressed in any kind of format, we also expect datasets collected from different opinion-sharing platforms such as question-answering platforms [141] or customer service dialogs [186]."
        },
        {
            "heading": "7.2 Multimodal ABSA",
            "text": "Most existing ABSA works focus on analyzing opinionated texts such as customer reviews or tweets. However, users often share their opinions with other modalities such as images. Since contents in different modalities are often closely related, exploiting such multimodal information can help better analyze users\u2019 sentiments towards different aspects [187, 188]. Recent studies on multimodal ABSA mainly concentrate on simple ABSA tasks such as multimodal ATE [189, 190] and multimodal ASC [191, 192, 193, 194]. To align the information from different modalities, the text\n15\nand image are often first encoded to feature representations, then some interaction networks are designed to fuse the information for making the final prediction. More recently, inspired by the success of the E2E-ABSA task in a single modal (i.e., based on texts only), Ju et al. [195] study the task of multimodal E2E-ABSA, aiming to capture the connection between its two subtasks in the multimodal scenario. They present a multimodal joint learning method with auxiliary cross-modal relation detection to obtain all aspect term and sentiment polarity pairs. Despite these initial attempts, there remain some promising directions: from the perspective of the task, handling more complicated multimodal ABSA tasks should be considered; from the perspective of the method, more advanced multimodal techniques should be proposed for fusing the multimodal opinion information, e.g., constructing models based on the multimodal PLMs. We believe multimodal ABSA would receive more attention given its increasing popularity in real-world applications."
        },
        {
            "heading": "7.3 Unified Model for Multiple Tasks",
            "text": "During the introduction of various ABSA tasks, we can notice that some ideas and model designs appear from time to time. Indeed, solutions to one ABSA task can be easily borrowed to tackle another similar task since these tasks are often closely related. This naturally posts a question: can we build a unified model that can tackle multiple (if not all) ABSA tasks at the same time? If so, there is no need to design specific models for every task. It can be also useful in practice because we may not want to change the model architecture and re-train it every time we have some new data with different types of opinion annotations. In Sec 2.3, we show that different tasks can be tackled via the same model if they can be formulated as the same modeling paradigm. Several recent studies demonstrate some initial attempts along this direction. They either transform the task into the MRC paradigm by designing task-specific queries for the MRC model [77, 94, 95], or the Seq2Seq paradigm by directly generating the target sentiment elements in the natural language form [96, 97, 7]. Beyond solving multiple tasks with the same architecture, Zhang et al. [7] further found that the task-specific knowledge could be easily transferred across different ABSA tasks (called cross-task transfer) if they are under the same modeling paradigm. We expect more research efforts would appear to enable more practically useful ABSA systems."
        },
        {
            "heading": "7.4 Lifelong ABSA",
            "text": "Lifelong learning, also referred to as continual learning, aims at accumulating knowledge learned from previous tasks and adapting it for helping future learning during a sequence of tasks [196]. Chen et al. [197] first study the sentiment analysis from the perspective of lifelong learning and propose the lifelong sentiment classification problem which requires a model to tackle a series of sentiment classification tasks. Wang et al. [198] impose the idea of lifelong learning into the ASC task with memory networks. Recent studies begin to investigate the catastrophic forgetting issue during the sequential learning [199, 200, 201, 202], instead of simply studying it as an extension of cross-domain sentiment analysis for knowledge accumulation. However, existing\nstudies mainly focus on domain incremental learning for the ASC task [201, 202], where all tasks sharing the same fixed label classes (e.g., positive, negative, and neutral) and no task information is required. To develop more advanced lifelong ABSA systems, it inevitably requires studying the incremental learning of the class and task. For instance, the classes of aspect categories vary in different applications, which calls for methods that can adapt to the changing categories. Besides, cross-task transfer [7] has been shown to be effective in transferring knowledge learned from lowlevel ABSA tasks to high-level ABSA tasks. Therefore, it is also worth exploring lifelong learning across different types of ABSA tasks."
        },
        {
            "heading": "8 CONCLUSIONS",
            "text": "This survey aims to provide a comprehensive review of the aspect-based sentiment analysis problem, including its various tasks, methods, current challenges, and potential directions. We first set up the background of ABSA research with the four sentiment elements of ABSA, the definition, common modeling paradigms, and existing resources. Then we describe each ABSA task with their corresponding solutions in detail, with an emphasis on the recent advances of the compound ABSA tasks. Meanwhile, we categorize existing studies from the sentiment elements involved and summarize representative methods of different modeling paradigms for each task, which provides a clear picture of current progress. We further discuss the utilization of pretrained language models for the ABSA problem, which has brought large improvements to a wide variety of ABSA tasks. We investigate the advantages they have, as well as their limitations. Besides, we review advances of crossdomain and cross-lingual ABSA, which can lead to more practical ABSA systems. Finally, we discuss some current challenges and promising future directions for this field."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "The work described in this paper is substantially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14200719)."
        }
    ],
    "title": "A Survey on Aspect-Based Sentiment Analysis: Tasks, Methods, and Challenges",
    "year": 2022
}