{
    "abstractText": "The proposed designs of many auxiliary long-lived particle (LLP) detectors at the LHC call for the instrumentation of a large surface area inside the detector volume, in order to reliably reconstruct tracks and LLP decay vertices. Taking the CODEX-b detector as an example, we provide a proof-of-concept optimization analysis that demonstrates the required instrumented surface area can be substantially reduced for many LLP models, while only marginally affecting the LLP signal efficiency. This optimization permits a significant reduction in cost and installation time, and may also inform the installation order for modular detector elements. We derive a branch-andbound based optimization algorithm that permits highly computationally efficient determination of optimal detector configurations, subject to any specified LLP vertex and track reconstruction requirements. We outline the features of a newly-developed generalized simulation framework, for the computation of LLP signal efficiencies across a range of LLP models and detector geometries. 1 ar X iv :2 21 1. 08 45 0v 1 [ he pph ] 1 5 N ov 2 02 2",
    "authors": [
        {
            "affiliations": [],
            "name": "Thomas Gorordo"
        },
        {
            "affiliations": [],
            "name": "Simon Knapen"
        },
        {
            "affiliations": [],
            "name": "Benjamin Nachman"
        },
        {
            "affiliations": [],
            "name": "Dean J. Robinson"
        }
    ],
    "id": "SP:e36bbb69b373cde19b48cd0230dcefb4c7a911ee",
    "references": [
        {
            "authors": [
                "J. Beacham"
            ],
            "title": "Physics Beyond Colliders at CERN: Beyond the Standard Model Working Group Report",
            "venue": "J. Phys. G",
            "year": 2020
        },
        {
            "authors": [
                "J. Alimena"
            ],
            "title": "Searching for long-lived particles beyond the Standard Model at the Large Hadron Collider",
            "venue": "J. Phys. G",
            "year": 2020
        },
        {
            "authors": [
                "T. Bose"
            ],
            "title": "Report of the Topical Group on Physics Beyond the Standard Model at Energy Frontier for Snowmass",
            "venue": "Snowmass Summer Study,",
            "year": 2021
        },
        {
            "authors": [
                "D. Curtin"
            ],
            "title": "Long-Lived Particles at the Energy Frontier: The MATHUSLA",
            "venue": "Physics Case, Rept. Prog. Phys",
            "year": 2019
        },
        {
            "authors": [
                "L. Lee",
                "C. Ohm",
                "A. Soffer",
                "T.-T. Yu"
            ],
            "title": "Collider Searches for Long-Lived Particles Beyond the Standard Model",
            "venue": "Prog. Part. Nucl. Phys. 106 ",
            "year": 2019
        },
        {
            "authors": [
                "G. Aielli"
            ],
            "title": "Expression of interest for the CODEX-b detector",
            "venue": "Eur. Phys. J. C",
            "year": 2020
        },
        {
            "authors": [
                "V.V. Gligorov",
                "S. Knapen",
                "M. Papucci",
                "D.J. Robinson"
            ],
            "title": "Searching for Long-lived Particles: A Compact Detector for Exotics at LHCb",
            "venue": "Phys. Rev. D97 ",
            "year": 2018
        },
        {
            "authors": [
                "C. Fanelli"
            ],
            "title": "Design of detectors at the electron ion collider with artificial intelligence",
            "venue": "JINST 17 ",
            "year": 2022
        },
        {
            "authors": [
                "S. Shirobokov",
                "V. Belavin",
                "M. Kagan"
            ],
            "title": "A",
            "venue": "Ustyuzhanin and A. G. Baydin, Black-Box Optimization with Local Generative Surrogates",
            "year": 2002
        },
        {
            "authors": [
                "MATHUSLA collaboration",
                "C. Alpigiani"
            ],
            "title": "Recent Progress and Next Steps for the MATHUSLA LLP Detector",
            "venue": "Snowmass Summer Study,",
            "year": 2022
        },
        {
            "authors": [
                "V.V. Gligorov",
                "S. Knapen",
                "B. Nachman",
                "M. Papucci",
                "D.J. Robinson"
            ],
            "title": "Leveraging the ALICE/L3 cavern for long-lived particle searches",
            "venue": "Phys. Rev. D 99 ",
            "year": 2019
        },
        {
            "authors": [
                "J. Pinfold"
            ],
            "title": "MoEDAL-MAPP",
            "venue": "an LHC Dedicated Detector Search Facility, in 2022 Snowmass Summer Study, 9",
            "year": 2022
        },
        {
            "authors": [
                "M. Bauer",
                "O. Brandt"
            ],
            "title": "L",
            "venue": "Lee and C. Ohm, ANUBIS: Proposal to search for long-lived neutral particles in CERN service shafts",
            "year": 1909
        },
        {
            "authors": [
                "B. Dey",
                "J. Lee"
            ],
            "title": "V",
            "venue": "Coco and C.-S. Moon, Background studies for the CODEX-b experiment: measurements and simulation",
            "year": 1912
        },
        {
            "authors": [
                "T. Sj\u00f6strand",
                "S. Ask",
                "J.R. Christiansen",
                "R. Corke",
                "N. Desai"
            ],
            "title": "P",
            "venue": "Ilten et al., An introduction to PYTHIA 8.2, Comput. Phys. Commun. 191 ",
            "year": 2015
        },
        {
            "authors": [
                "R.M. Schabinger",
                "J.D. Wells"
            ],
            "title": "A Minimal spontaneously broken hidden sector and its impact on Higgs boson physics at the large hadron collider",
            "venue": "Phys. Rev. D 72 ",
            "year": 2005
        },
        {
            "authors": [
                "S. Gopalakrishna",
                "S. Jung",
                "J.D. Wells"
            ],
            "title": "Higgs boson decays to four fermions through an abelian hidden sector",
            "venue": "Phys. Rev. D 78 ",
            "year": 2008
        },
        {
            "authors": [
                "D. Curtin",
                "R. Essig",
                "S. Gori",
                "J. Shelton"
            ],
            "title": "Illuminating Dark Photons with High-Energy Colliders",
            "venue": "JHEP 02 ",
            "year": 2015
        },
        {
            "authors": [
                "P. Meade",
                "M. Papucci",
                "T. Volansky"
            ],
            "title": "Dark Matter Sees The Light",
            "venue": "JHEP 12 ",
            "year": 2009
        },
        {
            "authors": [
                "M. Buschmann",
                "J. Kopp",
                "J. Liu",
                "P.A.N. Machado"
            ],
            "title": "Lepton Jets from Radiating Dark Matter",
            "venue": "JHEP 07 ",
            "year": 2015
        },
        {
            "authors": [
                "R.S. Willey",
                "H.L. Yu"
            ],
            "title": "Decays K\u00b1 \u2192 \u03c0\u00b1l+l\u2212 and limits on the mass of the neutral higgs boson",
            "venue": "Phys. Rev. D 26 ",
            "year": 1982
        },
        {
            "authors": [
                "R. Chivukula",
                "A.V. Manohar"
            ],
            "title": "Limits on a light higgs boson",
            "venue": "Physics Letters B 207 ",
            "year": 1988
        },
        {
            "authors": [
                "B. Grinstein",
                "L.J. Hall",
                "L. Randall"
            ],
            "title": "Do B meson decays exclude a light Higgs",
            "venue": "Phys. Lett. B211 ",
            "year": 1988
        },
        {
            "authors": [
                "M.W. Winkler"
            ],
            "title": "Decay and detection of a light scalar boson mixing with the Higgs boson",
            "venue": "Phys. Rev. D99 ",
            "year": 2019
        },
        {
            "authors": [
                "Y. Gershtein"
            ],
            "title": "S",
            "venue": "Knapen and D. Redigolo, Probing naturally light singlets with a displaced vertex trigger",
            "year": 2012
        },
        {
            "authors": [
                "J. Beacham"
            ],
            "title": "Physics Beyond Colliders at CERN: Beyond the Standard Model Working Group Report",
            "venue": "J. Phys. G",
            "year": 2020
        },
        {
            "authors": [
                "J. Alimena"
            ],
            "title": "Searching for long-lived particles beyond the Standard Model at the Large Hadron Collider",
            "venue": "J. Phys. G",
            "year": 2020
        },
        {
            "authors": [
                "T. Bose"
            ],
            "title": "Report of the Topical Group on Physics Beyond the Standard Model at Energy Frontier for Snowmass",
            "venue": "Snowmass Summer Study,",
            "year": 2021
        },
        {
            "authors": [
                "D. Curtin"
            ],
            "title": "Long-Lived Particles at the Energy Frontier: The MATHUSLA",
            "venue": "Physics Case, Rept. Prog. Phys",
            "year": 2019
        },
        {
            "authors": [
                "L. Lee",
                "C. Ohm",
                "A. Soffer",
                "T.-T. Yu"
            ],
            "title": "Collider Searches for Long-Lived Particles Beyond the Standard Model",
            "venue": "Prog. Part. Nucl. Phys. 106 ",
            "year": 2019
        },
        {
            "authors": [
                "G. Aielli"
            ],
            "title": "Expression of interest for the CODEX-b detector",
            "venue": "Eur. Phys. J. C",
            "year": 2020
        },
        {
            "authors": [
                "V.V. Gligorov",
                "S. Knapen",
                "M. Papucci",
                "D.J. Robinson"
            ],
            "title": "Searching for Long-lived Particles: A Compact Detector for Exotics at LHCb",
            "venue": "Phys. Rev. D97 ",
            "year": 2018
        },
        {
            "authors": [
                "C. Fanelli"
            ],
            "title": "Design of detectors at the electron ion collider with artificial intelligence",
            "venue": "JINST 17 ",
            "year": 2022
        },
        {
            "authors": [
                "S. Shirobokov",
                "V. Belavin",
                "M. Kagan"
            ],
            "title": "A",
            "venue": "Ustyuzhanin and A. G. Baydin, Black-Box Optimization with Local Generative Surrogates",
            "year": 2002
        },
        {
            "authors": [
                "MATHUSLA collaboration",
                "C. Alpigiani"
            ],
            "title": "Recent Progress and Next Steps for the MATHUSLA LLP Detector",
            "venue": "Snowmass Summer Study,",
            "year": 2022
        },
        {
            "authors": [
                "V.V. Gligorov",
                "S. Knapen",
                "B. Nachman",
                "M. Papucci",
                "D.J. Robinson"
            ],
            "title": "Leveraging the ALICE/L3 cavern for long-lived particle searches",
            "venue": "Phys. Rev. D 99 ",
            "year": 2019
        },
        {
            "authors": [
                "J. Pinfold"
            ],
            "title": "MoEDAL-MAPP",
            "venue": "an LHC Dedicated Detector Search Facility, in 2022 Snowmass Summer Study, 9",
            "year": 2022
        },
        {
            "authors": [
                "M. Bauer",
                "O. Brandt"
            ],
            "title": "L",
            "venue": "Lee and C. Ohm, ANUBIS: Proposal to search for long-lived neutral particles in CERN service shafts",
            "year": 1909
        },
        {
            "authors": [
                "B. Dey",
                "J. Lee"
            ],
            "title": "V",
            "venue": "Coco and C.-S. Moon, Background studies for the CODEX-b experiment: measurements and simulation",
            "year": 1912
        },
        {
            "authors": [
                "T. Sj\u00f6strand",
                "S. Ask",
                "J.R. Christiansen",
                "R. Corke",
                "N. Desai"
            ],
            "title": "P",
            "venue": "Ilten et al., An introduction to PYTHIA 8.2, Comput. Phys. Commun. 191 ",
            "year": 2015
        },
        {
            "authors": [
                "R.M. Schabinger",
                "J.D. Wells"
            ],
            "title": "A Minimal spontaneously broken hidden sector and its impact on Higgs boson physics at the large hadron collider",
            "venue": "Phys. Rev. D 72 ",
            "year": 2005
        },
        {
            "authors": [
                "S. Gopalakrishna",
                "S. Jung",
                "J.D. Wells"
            ],
            "title": "Higgs boson decays to four fermions through an abelian hidden sector",
            "venue": "Phys. Rev. D 78 ",
            "year": 2008
        },
        {
            "authors": [
                "D. Curtin",
                "R. Essig",
                "S. Gori",
                "J. Shelton"
            ],
            "title": "Illuminating Dark Photons with High-Energy Colliders",
            "venue": "JHEP 02 ",
            "year": 2015
        },
        {
            "authors": [
                "P. Meade",
                "M. Papucci",
                "T. Volansky"
            ],
            "title": "Dark Matter Sees The Light",
            "venue": "JHEP 12 ",
            "year": 2009
        },
        {
            "authors": [
                "M. Buschmann",
                "J. Kopp",
                "J. Liu",
                "P.A.N. Machado"
            ],
            "title": "Lepton Jets from Radiating Dark Matter",
            "venue": "JHEP 07 ",
            "year": 2015
        },
        {
            "authors": [
                "R.S. Willey",
                "H.L. Yu"
            ],
            "title": "Decays K\u00b1 \u2192 \u03c0\u00b1l+l\u2212 and limits on the mass of the neutral higgs boson",
            "venue": "Phys. Rev. D 26 ",
            "year": 1982
        },
        {
            "authors": [
                "R. Chivukula",
                "A.V. Manohar"
            ],
            "title": "Limits on a light higgs boson",
            "venue": "Physics Letters B 207 ",
            "year": 1988
        },
        {
            "authors": [
                "B. Grinstein",
                "L.J. Hall",
                "L. Randall"
            ],
            "title": "Do B meson decays exclude a light Higgs",
            "venue": "Phys. Lett. B211 ",
            "year": 1988
        },
        {
            "authors": [
                "M.W. Winkler"
            ],
            "title": "Decay and detection of a light scalar boson mixing with the Higgs boson",
            "venue": "Phys. Rev. D99 ",
            "year": 2019
        },
        {
            "authors": [
                "Y. Gershtein"
            ],
            "title": "S",
            "venue": "Knapen and D. Redigolo, Probing naturally light singlets with a displaced vertex trigger",
            "year": 2012
        }
    ],
    "sections": [
        {
            "text": "The proposed designs of many auxiliary long-lived particle (LLP) detectors at the LHC call for\nthe instrumentation of a large surface area inside the detector volume, in order to reliably recon-\nstruct tracks and LLP decay vertices. Taking the CODEX-b detector as an example, we provide a\nproof-of-concept optimization analysis that demonstrates the required instrumented surface area\ncan be substantially reduced for many LLP models, while only marginally affecting the LLP sig-\nnal efficiency. This optimization permits a significant reduction in cost and installation time, and\nmay also inform the installation order for modular detector elements. We derive a branch-and-\nbound based optimization algorithm that permits highly computationally efficient determination\nof optimal detector configurations, subject to any specified LLP vertex and track reconstruction\nrequirements. We outline the features of a newly-developed generalized simulation framework, for\nthe computation of LLP signal efficiencies across a range of LLP models and detector geometries.\nar X\niv :2\n21 1.\n08 45\n0v 1\n[ he\npph\n] 1\n5 N\nov 2\nCONTENTS\nI. Introduction 3\nII. Detector and reconstruction requirements 6\nA. Baseline configuration and components 7 B. Design drivers and detector configurations 7\nIII. Benchmark topologies and simplified models 11\nA. h\u2192 A\u2032A\u2032, A\u2032 \u2192 2e 13 B. b\u2192 sS, S \u2192 2e 14 C. b\u2192 sS, S \u2192 4\u03c0 14\nIV. Estimators and optimization 15\nA. General approach 15 B. Hit-weight estimator 17 C. Optimized results 20\nV. Summary 22\nAcknowledgments 25\nA. Optimization algorithm 25\n1. Optimized partial orderings 25 2. Branch and bound algorithm 29 3. Any-single-hit estimator 31\nB. Simulation framework 32\n1. Geometry kernel 33 2. HepyMC module and processors 33 3. Code flow 36 4. Normalization of efficiencies 36 5. Example scripts 38\nReferences 42\nI. INTRODUCTION\nThe LHC program is scheduled for ongoing upgrades and data collection until at least 2038. Given the relatively large luminosity that was already collected and correspondingly tightening limits, a potential discovery is increasingly likely to come from rare but striking classes of events, such as those involving new, long-lived particles (LLPs) [35\u201340]. Compelling signatures along these lines are displaced decays-in-flight of exotic LLPs, which can generically arise in any theory containing a hierarchy of scales, technically natural small parameters, or loop-suppressed interactions. LLPs are therefore ubiquitous in BSM scenarios [41\u201343].\nMost relatively heavy\u2014heavier than \u223c 10 GeV\u2014LLPs may be searched for effectively at ATLAS, CMS and LHCb, even for relatively long lifetimes. Searches for lighter LLPs are, however, often prohibitive, mainly because of large irreducible backgrounds and the corresponding trigger challenges. An alternate LLP search paradigm therefore comprises looking for displaced decays-in-flight of exotic LLPs, in a displaced auxiliary detector that is well shielded (usually by a combination of passive and active shielding) from the interaction point (IP). While such \u2018auxiliary\u2019 detectors would always have (much) smaller angular coverage than ATLAS and CMS, their increased shielding can more than compensate for their lower signal efficiency, by suppressing the backgrounds to negligible levels. The detector transverse location moreover determines the type of physics to which one is sensitive: forward-located detectors are better suited for LLPs produced at low partonic center-ofmass energy [44], while more transverse detectors are required to find many types of LLPs produced in e.g. Higgs decays or via other heavy mediators.\nTypically, the design of transverse auxiliary LLP detectors requires not only a sizeable fiducial volume but also a moderately large to large amount of instrumented surface area therein, for two reasons: Firstly, in order to reliably reconstruct LLP decay vertices, each track should pass through multiple tracking stations to yield a large sample of hits. The vertex resolution moreover typically degrades as the distance between the vertex and the first hit tracking station increases [45]. Resistive plate chambers (RPCs) are a compelling choice for the tracking technology, because of their relatively high hit (and timing) resolution. Secondly, a more-or-less hermetic detector configuration is desirable to veto hard-cosmic, soft-cavern, and soft shielding-sourced backgrounds. However, since this type of background\nrejection coverage requires only a high detection efficiency and not the high hit resolution needed for track reconstruction of LLP decay products, substantially cheaper technologies can be considered, such as scintillators.\nThe baseline configuration for the CODEX-b detector, proposed to be installed near LHCb\u2019s Interaction Point 8 (IP8) [43, 45], comprises \u223c 5100 m2 of instrumented surface area in the form of triplets or sextets of RPC layers. This configuration has already been shown in previous studies [43, 45] to have: (i) a competitive sensitivity to a wide range BSM LLP scenarios, exceeding or complementary to the sensitivity of other existing or proposed detectors; (ii) an achievable zero background environment, critical to LLP detection, and accessible, well-equipped experimental location in the DELPHI/UXA cavern; (iii) the ability to tag events within the LHCb detector, independently from the LHCb physics program; (iv) a compact size, on detector scales, and consequently an expected modest cost. A smaller proof-of-concept demonstrator detector, \u201cCODEX-\u03b2\u201d, is proposed for operation during Run 3 of the LHC [43].\nFor this class of LLP detector, the required amount of RPC panels are the predominant driver of forecasted costs and installation time. It is therefore imperative to understand methods to optimize the design geometry of an LLP detector, in order to minimize or substantially reduce the required amount of tracking versus background rejection surfaces, while maximizing (or preserving) the physics potential and sensitivity to LLP signals. Because of the broad range of BSM scenarios that may be probed by this class of auxiliary detector [41, 43], the optimization of the LLP detector geometry involves in turn the consideration of a broad range of well-motivated signal morphologies. These include many different signatures and kinematics, such that particle reconstruction requirements, efficiencies and acceptances vary widely. For instance, the expected boost and rapidity distribution, as well as decay products of the LLP varies significantly between LLPs produced in Higgs decays versus hadron decays. Further, in many well-motivated benchmark scenarios, the LLP may decay to various final states involving missing energy, photons, or a high multiplicity of softer tracks. These more complex decay morphologies can be much more challenging to detect and/or reconstruct.\nIn this work, we demonstrate the capabilities of a newly-developed versatile LLP simulation framework, that enables an understanding of the subtle interplay between the signal sensitivities of the detector geometry and the ability to probe underlying LLP models. Using\nthis framework, we simulate the response of variation in the detector geometry to different simulated BSM LLP production channels. In Appendix A, we derive in detail a \u2018branchand-bound\u2019-type optimization algorithm, that permits highly computationally efficient determination of optimal detector configurations. These allow us to best allocate and arrange tracking stations in a particular LLP detector volume for any specified total instrumented surface area. Importantly, the computation time for this algorithm scales approximately linearly with the number of tracking panels, rather than the exponential scaling one may naively expect from a brute-force method. (Details of the LLP simulation framework itself are provided in Appendix B.) This algorithmic approach may be contrasted with other detector optimization techniques that use an automated optimizer, either combinatorial or gradient-based as appropriate; see e.g. Refs [46\u201348].\nAs a proof of concept, we apply this algorithm to a mildly simplified representation of the CODEX-b detector, in order to demonstrate its capacity to identify optimized configurations. We show that good sensitivity over the space of LLP scenarios can be attained for CODEX-b while reducing the amount of RPC layers by an O(1) fraction, correspondingly substantially reducing the forecast cost as well as construction and installation times. For example, for many LLP portals, we find optimized configurations in which the amount of tracking surface can be reduced by 50% or more compared to baseline designs, while maintaining 80% or more relative efficiency for LLP detection compared to the CODEX-b baseline configuration.\nThis framework and optimization approach may be used in the future to develop realistic optimized detector designs that incorporate buildability and other engineering constraints. Furthermore, it may be exploited to further improve background rejection and reduce shielding costs: a subject of future work. Apart from the baseline location proposed in Refs. [43, 45], the framework may also be utilized to consider the sensitivities for other proposed CODEX-b locations near IP8, and optimize their geometries. We do not consider other such locations in this work, however. Similar applications are possible for other proposed auxiliary transverse LLP experiments, such as MATHUSLA [49], AL3X [50], MAPP [51] and ANUBIS [52]."
        },
        {
            "heading": "II. DETECTOR AND RECONSTRUCTION REQUIREMENTS",
            "text": "In an LLP (or any) detector, the number of LLP events Ne = \u03b5L (\u03c3 \u00d7 Br), where L is the integrated luminosity, \u03c3 \u00d7 Br is the LLP production cross-section times branching ratio, and \u03b5 is the total signal efficiency (or just \u2018total efficiency\u2019), which is sensitive to the LLP boost and rapidity distribution, the LLP decay topology, the detector geometry, and reconstruction of the LLP decay vertex. The total efficiency itself may be factored into a volumetric efficiency \u03b5v, dependent only on the fiducial volume of the detector and the LLP proper decay length (c\u03c4), and a reconstruction efficiency \u03b5r, that encodes the ability of the detector to reconstruct LLP decay tracks and the decay vertex. Explicitly, on an event-by-event basis, one may define\n\u03b5(~x, ~pi, \u03b2\u03b3) \u2261 \u03b5v(~x, \u03b2\u03b3)\u00d7 \u03b5r(~x, ~pi) (1)\nwhere ~x is the vertex position, \u03b2\u03b3 the boost of the LLP and the ~pi the momentum vectors of the decay products. The efficiency \u03b5v(~x, \u03b2\u03b3) represents the probability, under a particular normalization (see Appendix B 4), that the decay occurred within the fiducial volume. \u03b5r(~x, ~pi) is the probability that the decay was reconstructed, and depends directly on the detector configuration. The estimator of the total efficiency \u03b5 is the sample average of the weights in Eq. (1), \u3008\u03b5(~x, ~pi, \u03b2\u03b3)\u3009. For a fixed volume, the reconstruction efficiency\u2014and its relative ratio to a baseline configuration\u2014is the key figure of merit in comparing the performance of various detector configurations. It is defined as the ratio of the total efficiency estimator over the estimator for the efficiency of a perfect detector, in other words\n\u03b5r \u2261 \u3008\u03b5(~x, ~pi, \u03b2\u03b3)\u3009 / \u3008\u03b5v(~x, \u03b2\u03b3)\u3009 . (2)\nBefore turning to a study of these efficiencies, we first present the requirements, specifications and performance for LLP reconstruction at CODEX-b, keeping in mind that these may be adapted to other LLP search environments.\nA. Baseline configuration and components\nThe proposed CODEX-b detector technology follows the design for RPCs to be used in the ATLAS phase II muon chambers (see Ref. [53] for technical descriptions). The RPC tracking stations we consider will therefore be composed of 2 \u00d7 1 m2 triplet RPC layers.1 Based on evolving technical specifications for the CODEX-\u03b2 demonstrator detector (see also Ref. [43]), we assume the minimum detector element for the full CODEX-b detector will be a 2\u00d7 2 m2 square \u201cpanel\u201d, containing two 2\u00d7 1 m2 RPC triplet layers.\nThe baseline proposal for the CODEX-b detector comprises a 10 \u00d7 10 \u00d7 10 m3 fiducial volume located at x = 26 to 36 m (transverse), y = \u22127 to 3 m (vertical) and z = 5 to 15 m (forward) with respect to IP8. The baseline detector design studied in Refs. [43, 45], instrumented each of the six faces of this cubic detector volume with a sextet of RPC tracking layers, along with five triplets of RPC layers in the interior, uniformly spaced 1.67 m apart. To adapt to the 2 \u00d7 1 m2 element paradigm, we adjust the number of internal faces to four, as shown in Fig. 1a. This corresponds to 400 2 \u00d7 2 m2 RPC triplet panels, such that the total RPC surface area required for this design is \u223c 4800 m2. The baseline design has the advantage of being relatively simple to simulate. In Refs. [43, 45] it was shown that this design is capable of reconstructing many LLP decay scenarios while achieving an O(1) tracking and reconstruction efficiency.\nB. Design drivers and detector configurations\nThe design of the CODEX-b (or any LLP) detector is informed by four main considera-\ntions:\n(i) Acceptance: Because the decay products of less boosted LLPs or high multiplicity multibody decays may be distributed over wide opening angles, instrumentation is therefore required not just towards the back-end (x = 36 m) face of the detector, but also on the top, bottom and side faces of the cubic fiducial volume.\nIn the baseline design in Fig. 1a, all six external faces were instrumented equally, in order to ensure such softer LLP decays are captured. However, even in this case, typically one does not expect as high a density of hits on e.g. the z = 5 m face versus, say, the\n1 The more precise measurement for each chamber is 1.88\u00d7 1.03 m2, which we approximate by 2\u00d7 1 m2 in this work for simplicity and clarity.\nz = 15 m face [43], and it is unclear what fraction of the the z = 15 m face needs to have a sextet layer for optimal sensitivity. Because of this, we consider a maximal \u201cenvelope\u201d detector configuration, shown in Fig. 1b, in which the x = 36 m, z = 15 m and y = \u22127 m external faces have sextet panels, while z = 5 m and y = 3 m are triplet only. We stress that this configuration is merely meant as a maximal baseline, on which we shall apply our optimization procedure, and it is not intended per se as a realistic configuration for the eventual CODEX-b detector.\n(ii) Vertex resolution: Since no magnetic field will be available in CODEX-b, a good vertex resolution is essential to demonstrate the detection of an LLP. A mentioned above, one important parameter in this respect is the distance from the LLP decay vertex to the first hit tracking layer [45]. For the baseline design, this motivated the inclusion of the five additional RPC triplet planes distributed uniformly in the interior of the fiducial volume along the x direction, in order to achieve vertex resolutions at or below a centimeter.\nFor acceptable vertex reconstruction, in the study of the baseline design it was required that: each track is composed of at least 6 hits; and these hits are resolvable from hits gen-\nerated by other tracks in the LLP decay. In practice, assuming a O(cm) spatial resolution for the RPC layers, the second condition implies a separation of & 2 cm between resolvable hits on any given layer. We retain this requirement in our optimization analyses. Because, however, the present study is intended as a proof of concept for our optimization framework, and because we wish to preserve intuitive insight into its results, we make some simplifying assumptions concerning the vertex reconstruction: First, we assume the track six-hit condition is satisfied simply provided a track passes through two RPC triplet stations (in practice, the RPCs are obviously not perfectly efficient, requiring a correction). Second, we do not impose constraints on the distance from the LLP decay vertex to the first hit layer, nor direct minimal requirements on the vertex resolution. In other words, in this study we optimize for multitrack reconstruction efficiency, but not for vertex resolution. Resolution requirements can, however, be straightforwardly imposed for a realistic optimization analysis of a full engineering design, that also incorporates buildability and other engineering constraints.\nIn the baseline design, vertical internal layers lying in the x-y plane were not considered, even though these may (or may not) more effectively provide coverage of the interior of the fiducial volume in light of these reconstruction requirements. To examine their role under optimization, we include in the maximal envelope detector configuration an additional four internal faces uniformly spaced along the z axis, containing RPC triplets, as shown in Fig. 1b. The envelope configuration thus requires 450 panels, corresponding to a total RPC surface area of \u223c 5400 m2.\n(iii) Track momentum threshold: Because of the expected challenges in reconstructing multiply rescattered low momentum tracks, a minimum track threshold of 600 MeV was assumed for the baseline design study (for muon and pions to remain well inside the minimal ionizing particle regime through multiple detector elements; electrons instead undergo significant rescattering via Bremsstrahlung, though the same threshold was assumed). For soft LLPs produced from (heavy) hadron decays, Ref. [45], demonstrated that an O(1) reconstruction efficiency is maintained with a minimum track momentum threshold around 600 MeV or lower.\nThis threshold is the predominant driver of losses in reconstruction efficiency for e.g. the b \u2192 sS benchmark portal (see Sec. III B, below). At the same time, backgrounds produced by low energy (secondary) neutron scattering happen to grow significantly below this\nthreshold [43]. In this study, we retain the 600 MeV track momentum threshold as a track reconstruction constraint.\n(iv) Backgrounds: Tracking stations on the front (x = 26 m) face are needed to reject backgrounds from charged particles\u2014primarily muons\u2014emanating from the detector shielding. For these stations, hit resolution is less important than detection efficiency, which may permit the use alternative technologies, such as scintillator planes: this requires further study by the CODEX-b collaboration, outside the scope of this work. In all designs, we therefore always include a sextet of RPC layers on the front face, but we do not include this face\u2014amounting to 50 2\u00d72 m panels\u2014when comparing the relative amount of instrumented surface for signal sensitivity. That is, the instrumentation ratio for a configuration, with n panels,\nrn = (n\u2212 50)/(Nbase \u2212 50) (3)\nwhere Nbase = 400, is the number of panels in the baseline configuration.\nSoft backgrounds from cavern-based sources [54] and low-momentum neutral particles generated in the shielding also motivate a hermetic coverage of the fiducial detector volume, in order to reject such backgrounds. However, because once again detection efficiency is more crucial than hit resolution for the purpose of rejecting backgrounds, one may contemplate replacing the RPC triplet panels with scintillator on the outer faces of the detector, wherever the former would provide subleading sensitivity to LLP signal detection and reconstruction. Characterizing the degree to which this would degrade the signal sensitivity is a main goal of the present work.\nIn order to provide an intuitive comparison with the baseline design shown in Fig. 1a, as we discuss the performance of the model benchmarks chosen below we also consider the performance of a \u2018heuristic\u2019 configuration as shown in Fig. 1c. This configuration is an informed guess for an optimized configuration, that attempts to place instrumented surface only in locations expected to experience higher numbers of hits, while likely still satisfying the reconstruction requirements: It has a sextet only on the back x = 36 m face (and the x = 26 m face) with rn ' 0.51, and many entire or partial faces have been removed. As we shall see below in Sec. III, this configuration nonetheless has a relative reconstruction efficiency ranging from 50\u201380% of the baseline performance, depending on the LLP production and decay portals. We will find, however, that optimized configurations\nsubject to the above reconstruction requirements will outperform the heuristic configuration."
        },
        {
            "heading": "III. BENCHMARK TOPOLOGIES AND SIMPLIFIED MODELS",
            "text": "Exotic LLPs may generically arise in any theory containing a hierarchy of scales, technically natural small parameters, or loop-order suppressions, and are therefore ubiquitous in BSM scenarios [41, 43]. An analogous SM phenomenology is found in the long-lived K0L, \u03c0 \u00b1, neutron and muon. In this optimization study, we will limit ourselves to three combinations of simple LLP production and decay portals, that act as good representatives for the typical event topologies that may be searched for at LLP detectors such as CODEX-b. These are summarized in Table I, and discussed in more detail in the following subsections. Also indicated in Table I are other commonly considered LLP models, in particular axion-like particles (ALPs) and heavy neutral leptons (HNLs), whose typical production and decay morphologies are analogous to the considered models. For each production and decay portal, we survey several LLP masses in the 0.5\u201310 GeV range, leading to a total of eleven benchmark models.\nWe simulate total efficiencies and vertex reconstruction efficiencies using MC samples generated with Pythia 8 [55], that are then reweighted and further processed with a dedicated, generalized LLP simulation framework: The details of this simulation framework are discussed in Appendix B. Our benchmark models will also serve as good examples of use cases that determine its required specifications and capabilities. In Fig. 2 we show the total efficiencies for the various benchmarks, as a function of c\u03c4 . The displayed efficiencies are from raw simulation, without fitting to the known linear powerlaw behavior in the long-lifetime regime, and we include the uncertainties from MC statistics.\nAn important takeaway is that the impact of the reconstruction requirements is roughly a constant factor across broad ranges of c\u03c4 . (This is expected in the long lifetime regime, in which the LLP characteristic displacement from the IP, \u03b2\u03b3c\u03c4 , is much greater than the detector displacement. This is because the volumetric efficiency, which is determined by probability of decay, scales as e\u2212|~x|/\u03b2\u03b3c\u03c4/\u03b2\u03b3c\u03c4 ' 1/\u03b2\u03b3c\u03c4 for |~x| \u03b2\u03b3c\u03c4 . In this regime the dependence on c\u03c4 therefore drops out in Eq. (2).) As a result, even though the LLP lifetime is formally an independent parameter of the benchmark models, we shall hereafter discuss and compute vertex reconstruction efficiencies by marginalizing over a c\u03c4 range close to the\n\ud835\udf00\npeak, at which the kinematic distributions should well represent those for the entire longlifetime regime. We do not optimize for LLPs in the short-lifetime regime, where existing LHC experiments are expected to set the most competitive limits.\nA. h \u2192 A\u2032A\u2032, A\u2032 \u2192 2e\nThe first topology comprises LLP pair production at high boost, with each LLP decaying to two tracks. In particular, we consider LLP production via an exotic decay of the SM Higgs, in which it decays to a pair of highly-boosted GeV scale LLPs. There are many ways of realizing this scenario in a specific model: generally the branching ratio of the Higgs decay to the LLPs is independent from the LLP lifetime. For concreteness, we take the example of a dark photon (A\u2032) LLP [56\u201358], treating Br[h \u2192 A\u2032A\u2032], the LLP lifetime, c\u03c4 , and its mass, mA\u2032 , as independent parameters. In the \u223c GeV mass range we consider, the A\u2032 itself predominantly decays to pairs of charged leptons or hadrons [59, 60]: for simplicity we consider A\u2032 decays just to electron positron pairs.\nIn Table II we show the vertex reconstruction efficiencies for various benchmarks involving different LLP masses and production or decay portals. The vertex reconstruction efficiencies for this portal are in the \u03b5r(base), \u03b5r(env) \u223c 80\u201390% range for both the baseline and maximal envelope configurations, with the main reductions arising from & 2 cm hit separation criterion. For the heuristic configuration, the efficiency lies in the \u03b5r(heur) \u223c 70- 80% range even though this configuration features approximately half the amount of RPC surface area. This observation raises the prospect, which we confirm below, that a more systematic optimization procedure might permit further reductions in the instrumentation ratio, rn < rn(heur), while maintaining competitive sensitivities.\nB. b \u2192 sS, S \u2192 2e\nThe second topology comprises LLP production at moderate boost, with the LLP decaying to two tracks. In particular, we consider an exotic b-hadron decay, b\u2192 sS. The particle S is a light, new singlet added to the SM through the Higgs-mixing portal\nL \u2283 \u00b5SH\u2020H with \u00b5 mW . (4)\nIn this scenario, one benefits from the huge b-hadron production cross section, and very narrow decay width of the Bu,d,s or \u039bb ground state hadrons. In the mass basis, S has a Yukawa coupling to all SM fermions proportional to their masses and the mixing angle, sin \u03b8, between S and the SM Higgs. The B \u2192 XsS inclusive decay, as a representative of all inclusive b-hadron decays, occurs through an electroweak penguin with branching ratio Br[B \u2192 Xs\u03c6] \u2248 6.2 \u00d7 sin2 \u03b8 [61\u201363]. The lifetime is inversely proportional to sin2 \u03b8 and depends further on a somewhat complicated function of mS: See [64, 65] for the most recent calculation. In the \u223c GeV mass range we consider, the branching ratios for S decays to pairs of charged leptons or hadrons are O(1) (See e.g. [66]). Once again for simplicity we consider S decays just to electron positron pairs.\nFrom Table II, the vertex reconstruction efficiencies for this portal lie in the \u03b5r(base), \u03b5r(env) \u223c 50\u201390% range for the baseline and full nominal configurations, with the main reductions arising from the 600 MeV track momentum threshold. For the heuristic configuration, the efficiency lies in the \u223c 50% range, such that for some lighter mass points, the heuristic configuration performs comparably to the other two, while for heavier LLPs the sensitivity roughly scales with the instrumentation ratio rn(heur).\nC. b \u2192 sS, S \u2192 4\u03c0\nBeyond the case that the LLP decays to two (or more) tracks, a wide range of LLP scenarios may involve decays of the LLP to tracks plus missing energy. For instance, HNLs may feature decays to final states containing neutrinos. Other LLPs may feature decays to long-lived neutral hadrons, such as the K0L, that may escape the detector, or to neutral hadrons with rapid electromagnetic decays, such as the \u03c00, that cannot be reconstructed without a calorimeter (which we assume the LLP detector does not have). To capture this\ntopology, we also consider the above Higgs mixing portal, but with S decaying to \u03c0+\u03c0\u2212\u03c00\u03c00, in which the \u03c00 is treated as missing energy.\nCompared to the b \u2192 sS, S \u2192 2e mass benchmarks, the typically softer tracks in the S \u2192 4\u03c0 decay mode lead to further reductions in the vertex reconstruction efficiency, because of the 600 MeV track momentum threshold. We see in Table II, however, that just as for the S \u2192 2e mode, for the lighter 1 GeV mass point, the heuristic configuration performs comparably to the other two, while for heavier LLPs the sensitivity begins to scale as rn(heur)."
        },
        {
            "heading": "IV. ESTIMATORS AND OPTIMIZATION",
            "text": "A. General approach\nIn order to optimize the detector geometry, one seeks to understand which subset(s) of RPC panels exhibit greater or lesser sensitivity to our selection of LLP benchmark models. In particular, for the maximal envelope configuration \u03a3 = {pi}i\u2264N of N total panels, the goal is to understand which subset of size n \u2264 N panels, \u03c3(n) \u2282 \u03a3, maximizes the relative vertex reconstruction efficiency\n\u03c1n,b = \u03b5r,b(\u03c3 (n))/\u03b5r,b(base) , (5)\nin which \u03b5r,b(\u03c3) denotes the tracking reconstruction efficiency of configuration \u03c3 for benchmark b, as shown in Table II. We use here and hereafter a superscript in parentheses to denote the size\u2014the cardinality\u2014of a subset, so that |\u03c3(n)| = n. In general, the optimal configuration of size n, denoted \u03c3 (n) opt, will be sensitive to the particular benchmark under consideration. We will therefore seek maximizations that perform best on average over the space of selected benchmarks, assuming a uniform prior for the weight of any LLP benchmark versus the others. That is, we will seek \u03c3 (n) opt that optimizes the objective function\nf = \u2211\nb \u03c1n,b. 2\nIf the objective function, f , were additive\u2014i.e. f(\u03c3 \u222a \u03c3\u2032) = f(\u03c3) + f(\u03c3\u2032)\u2014then the optimization problem is a special case of the classic \u201c0/1-knapsack problem\u201d, whose solution can be found straightforwardly from a \u201cgreedy\u201d algorithm. This proceeds as: (i) determine\n2 In principle, the optimization problem may also admit relative panel weights, that encode some relative\n\u201ccost\u201d between different panels, and require their sum to be bounded. The following discussion may be straightforwardly generalized to include such considerations.\nis \u03c3 (1) opt = {p3}. The two-panel subset, however, with highest efficiency \u03c3 (2) opt = {p1, p2} 6\u2283 \u03c3 (1) opt.\nthe values of f({pi}) for each individual panel; (ii) order the panels accordingly from highest to lowest value; and (iii) take the n highest-valued panels. Put a different way, sorting by individual objective function values yields an ordering of panels {pzi} such that f({pz1}) \u2265 f({pz2}) \u2265 ....f({pzN}), and the solutions are \u03c3 (n) opt = {pzi}i\u2264n for each n \u2264 N .\nFor the tracking and vertex reconstruction requirements we consider in Sec. II B, however, the objective function, f = \u2211\nb \u03c1n,b, is manifestly nonadditive, because of the effects\nof hit-hit correlations in each event. This can be seen for the example shown in Fig. 3: \u03b5r({p1}) = \u03b5r({p2}) = 0, but \u03b5r({p1, p2}) = 2/3 (assuming in this example only three hits per track\u2014i.e. passing through one triplet\u2014is required for reconstruction). An additional complication arises when we consider that decay events may often saturate the minimum six hits per track requirement (corresponding to hitting at least two RPC triplet layers). As shown in Fig. 4, a minimum number of hits per track can lead to degeneracies in the vertex reconstruction efficiency among various configurations, such that the presence of some panels are anticorrelated with others, for a fixed cardinality n. Put in other words, such requirements can lead to panel-panel redundancies. As a result, the class of problem we must consider involves an objective function for which the difference\nf(\u03c3 \u222a \u03c3\u2032)\u2212 f(\u03c3)\u2212 f(\u03c3\u2032) , (6)\nmay take positive or negative values, because of the effect of hit-hit correlations and panelpanel redundancies, respectively: The set function f is neither superadditive nor subadditive.\nBecause of the hit-hit correlations and panel-panel redundancies in each event, in general for n\u2032 < n, the globally optimal subset \u03c3 (n\u2032) opt need not be a subset of the globally optimal \u03c3 (n) opt. (See Fig. 3 for a trivial example.) We are interested here, however, in a prioritization\nof the RPC panels by which one may incrementally install an experiment, and thus further panels may be added but not removed from a given configuration. We therefore seek an optimized ordering of configurations, in which each \u03c3 (n) opt contains \u03c3 (n\u2032) opt for n \u2032 \u2264 n, i.e.\n\u2205 = \u03c3(0)opt \u2282 \u03c3 (n1) opt \u2282 . . . \u2282 \u03c3 (nk) opt \u2282 . . . \u03c3 (nK\u22121) opt \u2282 \u03c3 (nK=N) opt = \u03a3 . (7)\nThis nesting requirement means that it may not be possible to construct a globally optimal \u03c3 (n) opt for every n \u2264 N , i.e. such that Eq. (7) is satisfied, because it may happen that one would need to remove panels from the existing n-panel configuration and add others elsewhere to achieve a global optimum for n+ 1 (see e.g. Fig. 3). Instead, we will show that it is possible to construct an optimized ordering for a (large) subset of cardinalities nk \u2264 N , such that nk+1\u2212 nk \u223c few, and interpolate for nk \u2264 n \u2264 nk+1: we refer to this as an optimized partial ordering.\nB. Hit-weight estimator\nBefore proceeding to the implementation of a systematic optimization, it is useful to first gain intuition for the general behavior and anticipated characteristics of \u03c1n[b] as a function\nof n. To this end, one can examine the ordering determined by the number of (weighted) hits on each of the RPC panels belonging to the maximal envelope configuration. Specifically, for benchmark b, the cumulative hit weight for the ith panel\nhi,b = \u2211 \u03b1\u2208Ei,b w\u03b1,b , (8)\nwhere Ei,b is the set of events in benchmark b whose tracks hit the ith panel (possibly multiple times, if multiple tracks hit the panel), subject to the pertinent track reconstruction requirements, and w\u03b1,b is the weight of the event to which the track belongs. The latter is defined as the event MC generation weight multiplied by the LLP decay probability (see also App. B 4). Normalizing over the space of panels and imposing a uniform prior over the set of benchmarks, leads to the benchmark-averaged hit weight\nh\u0302i = 1\nnB \u2211 b [ hi,b /\u2211 j hj,b ] , (9)\nwhere nB is the number of benchmarks. This weight may be thought of as the relative probability that a panel is hit by an LLP decay track. Sorting the panels from highest to lowest h\u0302i produces an ordering. Although it neglects the effects of hit-hit correlations from either the same track or another track in the same event, one might expect this ordering to act as a naive estimator for an optimized ordering, whose efficiency provides a lower bound for the latter. Further, one might expect an estimator based on hit weight to be correlated with decreasing distance from the LLP vertex (and thus correlated with good LLP vertex resolution) as the farther a panel is located from a vertex, the fewer tracks are expected to pass through it per event.\nIn Fig. 5 we show the relative reconstruction efficiencies \u03c1n for the various different benchmarks listed in Table II, as a function of the number of RPC panels selected from the maximal envelope configuration, with ordering determined by h\u0302i. Of particular importance is the relatively steep negative curvature for some of the benchmark models: the relative vertex reconstruction efficiency for these rises quite quickly as a function of the number of panels. This demonstrates that for some benchmarks, there exist configurations with high values for \u03c1n for relatively low n, even using this naive hit weight estimator. These orderings achieve a comparable or slightly higher relative efficiency versus that of the heuristic configuration\nof Fig. 1c, whose relative efficiency is shown by colored single data points in Fig. 5.\nIn Fig. 6 we show the configuration corresponding to the first n = 150 panels of this ordering (rn = 0.29), which can achieve up to 80% relative efficiency for the h \u2192 A\u2032A\u2032 benchmarks, and 25\u201375% relative efficiencies for b \u2192 sS. (This somewhat arbitrary choice for the number of panels combined with uncertainties from MC statistics leads to some planes being partially filled by disconnected panels.) It is notable that: (i) this configuration suggests predominant sensitivity is generated mainly by the z-y planes, with some minimal contributions on the z = 15 m face, and; (ii) little to no role appears to be played by the external faces other than x = 36 m. The selection of the z-y internal faces comports with the expectation above that this estimator is correlated with minimizing distance of the first hit tracking layer from the LLP decay vertex. We also show the configuration with the first n = 250 panels of this ordering (rn = 0.71), which can achieve up to 90% relative efficiency for the h\u2192 A\u2032A\u2032 benchmarks, and 40\u201390% relative efficiencies for b\u2192 sS. In this configuration, while the z = 15 m face and internal x-y faces now play a role, notably the top and bottom y = \u22127 m and y = 3 m faces contribute little.\nIn contrast to most of the benchmarks, in Fig. 5 \u03c1n is almost linear in n for the b\u2192 sS,\nS \u2192 2e model with mS = 4 GeV. To understand whether this can be improved, in Fig. 7 we show the relative reconstruction efficiencies using the ordering determined only by the hit weights for this mS = 4 GeV benchmark. While there is a mild improvement in the curvature of the mS = 4 GeV band, the h \u2192 A\u2032A\u2032 curves become flatter, as do the lighter b \u2192 sS benchmarks. Figures 5 and 7 taken together suggest that there may exist configurations that can further optimize the efficiencies across the entire space of benchmarks.\nC. Optimized results\nIn Appendix A we present the formal construction of a branch-and-bound algorithm to solve this optimization problem. It optimizes the increase in the objective function value per panel, in order to generate an optimized partial ordering. It makes use of a simpleto-compute bounding estimator of the objective function\u2014the \u201cany-single-hit\u201d estimator, defined in Sec. A 3. As explained therein, this bounding estimator has the required algebraic properties to permit the computational complexity of this algorithm to scale approximately linearly in the number of panels N , rather than the exponential scaling O(2N) of a brute force approach.\nIn Fig. 8 we show the relative vertex reconstruction efficiencies for the partially optimized ordering generated by this branch-and-bound approach, using the any-single-hit estimator applied to the objective determined by the b \u2192 sS, S \u2192 2e model. This ob-\njective is averaged over the four mass benchmarks mS = 0.5, 1.0, 2.5 and 4.0 GeV, i.e. \u03c1n = \u2211\nmS=0.5,1.0,2.5,4.0GeV \u03c1n,mS [B \u2192 sS(2e)]. We find the partially optimized ordering con-\ntains 218 groupings (see App. A 1) of one or two panels for a total of 271 panels selected from the maximal envelope configuration, and one very large final grouping of 179 panels, for which the optimizer does not specify a preferred ordering. This behavior is not unexpected, as there are 50 panels on the x = 25 m that receive zero hits, and many of the 75 panels on the z = 5, 7 and 9 m faces receive significantly fewer hits than others in the envelope configuration, along with some on the y = 3 and \u22127 m faces. To further order this last grouping, we apply the naive benchmark-averaged hit weight estimator on the space of events whose LLP vertices are not reconstructed by these first 271 panels.\nThe subsequent result for each of the eleven benchmarks, shown by dark bands, is compared to the corresponding relative vertex reconstruction efficiencies generated by the naive benchmark-averaged hit weight estimator in Fig. 5, shown by light bands. For all eight of the b \u2192 sS, S \u2192 2e and h \u2192 A\u2032A\u2032, A\u2032 \u2192 2e benchmarks, the optimized orderings outperform the naive hit weight estimator, by up to \u223c 20% in the small n regime. We see that for e.g. 150 panels (rn = 0.29), the relative vertex reconstruction efficiency \u03c1n is 60\u201380% for\nb\u2192 sS portals, and 80\u201390% for h\u2192 A\u2032A\u2032. The optimized orderings also achieve a notably higher efficiency than the heuristic configuration of Fig. 1c. The increase in performance for the h \u2192 A\u2032A\u2032 portals occurs even though the tracks for these benchmarks are typically harder and more collinear than the b \u2192 sS, S \u2192 2e benchmarks on which the optimizer objective was defined. This is not unexpected since the b\u2192 sS, S \u2192 2e benchmarks would demand a more widely distributed configuration of panels to achieve significant sensitivity, and thereby capture the more boosted A\u2032 decays. For the three b \u2192 sS, S \u2192 4\u03c0 benchmarks, by contrast, the performance between naive and optimized approaches is similar: the distribution of tracks for these benchmarks are broader and softer compared to the b\u2192 sS, S \u2192 2e benchmarks, so that further gains in sensitivity would likely require prioritizing an even more widely distributed configuration of panels.\nIn Fig. 9 we show the panel configurations for n = 150 and 250 panels, corresponding to the partially optimized ordering generated by the branch-and-bound approach. Intrinsic uncertainties from the MC statistics lead to some \u201cnoise\u201d in the selected panels. With fewer panels, the optimizer favors mainly panels on the external faces, as expected from the requirement that each track hits at least two RPC triplets. Where double RPC triplets on an external face are not available, such as on the y = 3 m face, the optimizer selects instead panels on the adjacent edge of the internal faces. As more panels are added, the optimizer further fills out these adjacent edges on the internal faces, leading to a partial ring-like configuration on the internal faces."
        },
        {
            "heading": "V. SUMMARY",
            "text": "In this work we demonstrated that compared to prior baseline hermetic RPC configurations for CODEX-b-like detectors, a significant optimization of the amount of required RPC-instrumented tracking surface is achievable. We showed that nested optimal detector configurations, which optimize the LLP decay vertex reconstruction efficiency as a function of the number of tracking station elements\u2014here, the number of RPC triplet panels, can be efficiently computed using an \u201cany-single-hit\u201d bounding estimator combined with a branch-and-bound optimization method. The computation time is approximately linear in the number of panels, rather than exponential. This is achievable because of the set-theoretic properties of the bounding estimator, that permit identification of a directed\npath(s) through the powerset configuration of panels on which the objective function\u2014the vertex reconstruction efficiency\u2014is maximized and nondecreasing.\nWithin this approach, we considered the vertex reconstruction efficiency for a range of different LLP production and decay portals, chosen to provide good coverage over the space of typical LLP production and decay morphologies and kinematics: In particular the bhadron production portals b\u2192 sS, S \u2192 2e and b\u2192 sS, S \u2192 4\u03c0, and the Higgs production portal h\u2192 A\u2032A\u2032, A\u2032 \u2192 2e, for scalar LLP S and vector LLP A\u2032, respectively, over a range of different LLP masses. For e.g. 150 panels (rn = 0.29), the optimized configurations achieve the relative vertex reconstruction efficiencies of 60\u201380% for b\u2192 sS portals, and 80\u201390% for h\u2192 A\u2032A\u2032 compared to the baseline configuration, which has 400 panels.\nThe branch-and-bound optimized configurations attained higher relative efficiencies compared to a more naive optimization approach, that ordered the panels based simply on the LLP track hit weight density. The latter is, however, even faster to compute. We found it typically provides a good proxy for characterizing the degree of optimization that is possible in any given LLP model, though it preferred a notably different configuration of tracking layers, in particular by selecting panels on internal faces earlier in the ordering. (It thus appears correlated with minimization of the distance of first-hit tracking layer from the LLP decay vertex.) However, the branch-and-bound optimizer is capable of deducing configurations that better avoid redundancies in hit-hit correlations among different panels, and thus\nshould be preferably used for optimization of LLP detectors. Although we used a simplified set of track vertex reconstruction criteria in this study, future applications of this algorithm may straightforwardly incorporate more realistic requirements for track reconstruction and LLP vertex resolution. One may thus implement a realistic optimization analysis for a future technical design report that incorporates buildability and other engineering constraints.\nUnderpinning this analysis was the development of a new, generalized simulation framework for the computation of LLP vertex reconstruction efficiencies across a wide range of LLP portals and different detector geometries. Beyond enabling the optimization of detector geometries for LLP detection, the combined simulation and optimization framework may also be adapted to study background rejection, in order to optimize the amount of require passive and active shielding in a realized LLP detector. This will be a subject of future work.\nACKNOWLEDGMENTS\nWe thank the membership of the CODEX-b collaboration for discussions, and in particular Juliette Alimena, Xabier Cid Vidal, Vladimir Gligorov, Phillip Ilten, Daniel Johnson, Titus Momba\u0308cher, Emilio Rodr\u0301\u0131guez Ferna\u0301ndez, and Michele Papucci for comments on the manuscript. SK thanks the Aspen Center for Physics (supported by the NSF Grant PHY1607611) for its hospitality, where part of this work was performed. This work was supported by the Laboratory Directed Research and Development Program of Lawrence Berkeley National Laboratory under U.S. Department of Energy Contract No. DE-AC02-05CH11231. SK, BN and DJR are supported by the Office of High Energy Physics of the U.S. Department of Energy under Contract No. DE-AC02-05CH11231.\nAppendix A: Optimization algorithm\n1. Optimized partial orderings\nIn principle, one needs \u2018merely\u2019 to compute the objective function over the power set of \u03a3, in order to determine \u03c3 (n) opt for any given n. For the case that N is of the order of several hundred panels, however, this becomes exponentially hard and computationally prohibitive. Further, because of the nesting requirement (7), it is not sufficient to identify the globally\noptimal subset of any particular cardinality. Rather, we seek to identify a sequence of \u03c3 (n) opt such that f(\u03c3 (n) opt) has the maximal possible negative curvature in n.\nTo understand the character of this optimization problem, suppose the entire set of panels can be partitioned into a set of K disjoint subsets\u2014\u201cgroupings\u201d\u2014of panels, \u03a3 = \u03b4\u03c31 \u222a . . . \u222a \u03b4\u03c3K . We use the notation for the ith grouping \u201c\u03b4\u03c3i\u201d to emphasize that a grouping is a (typically small) set of panels to be added to a larger configuration. We further define \u03a3k = \u03b4\u03c31 \u222a . . . \u222a \u03b4\u03c3k for each k \u2264 K, with \u03a30 = \u2205, the empty set. With respect to the conditional objective function\nf(\u03b4\u03c3|\u03c3) \u2261 f(\u03b4\u03c3 \u222a \u03c3)\u2212 f(\u03c3) , (A1)\nwhich describes the contribution to the objective from adding \u03b4\u03c3 to a given configuration of panels \u03c3, the total objective may always be expanded as a series of positive semidefinite partial sums\nf(\u03a3) = f(\u03b4\u03c31) + f(\u03b4\u03c32|\u03b4\u03c31) + f(\u03b4\u03c33|\u03b4\u03c31 \u222a \u03b4\u03c32) + . . .+ f(\u03b4\u03c3K |\u03b4\u03c31 \u222a . . . \u222a \u03b4\u03c3K\u22121)\n= f(\u03b4\u03c31|\u03a30) + f(\u03b4\u03c32|\u03a31) + f(\u03b4\u03c33|\u03a32) + . . .+ f(\u03b4\u03c3K |\u03a3K\u22121) . (A2)\nIf it is the case that the partition has the property\nf(\u03b4\u03c3i|\u03a3k) \u2265 f(\u03b4\u03c3i|\u03a3k+1) (A3)\nfor all i, and \u03b4\u03c3k+1 satisfies\nf(\u03b4\u03c3k+1|\u03a3k) = max i f(\u03b4\u03c3i|\u03a3k) , (A4)\nthen each partial sum is bounded above by the previous one, because f(\u03b4\u03c3k|\u03a3k\u22121) = maxi f(\u03b4\u03c3i|\u03a3k\u22121) \u2265 f(\u03b4\u03c3k+1|\u03a3k\u22121) \u2265 f(\u03b4\u03c3k+1|\u03a3k). The maximization ensures that f(\u03a3k) has maximal negative semidefinite curvature over n, in the strict sense that one has maximized each term only on the set of the {\u03b4\u03c3i} in the partition. Thus one may identify \u03a3k = \u03c3 (nk) opt in the case that Eqs. (A3) and (A4) hold. Note, however, for a given \u03a3 k possible degenerate choices may arise for \u03b4\u03c3k+1, so that multiple optimized orderings may exist: the solution to the optimization problem is not unique.\nThe correspondence in Eq. (A2) makes it clear that we can consider \u03b4\u03c3k satisfying\nEqs. (A3) and (A4) as defining the step between \u03c3 (nk\u22121) opt and \u03c3 (nk) opt . In this sense, let us represent the elements of the power set \u03c3 \u2208 P(\u03a3) as the vertices of a graph connected by edges, which each represent the addition or subtraction of a single panel between two adjacent vertices (see Fig. 10). We consider only the two directed graphs, in which the edges in a path between two vertices are all additive or are all subtractive (since we seek an ordering that represents a prioritization of the panels, not a set of rearrangements). We refer to these as the \u201cupwards\u201d and \u201cdownwards\u201d graph, respectively. Each vertex is weighted by the objective f . Each panel-wise expansion of the objective\nf(\u03a3) = f({p1}) + f({p2}|{p1}) + f({p3}|{p1, p2}) + . . .+ f({pn}|\u03a3 \\ {pn}), , (A5)\nis specified by a path through the directed graph, that includes panels in the order they appear in the sequence {pi}. Each term of Eq. (A5) encodes the incremental changes in the objective along this path. A grouping \u03b4\u03c3k = {pnk\u22121+1, . . . , pnk} is the subpath between the vertex \u03c3 (nk\u22121) opt and vertex \u03c3 (nk) opt .\nopt amounts to finding the vertex in the nth\nlayer with the largest objective.\nIn the graph picture, the optimization problem can now be constructed on the space of\npossible paths through the directed graph, rather than in terms of the choice of subsets of \u03a3. In the directed graph, there are 2N possible paths, so that it remains equally impractical to consider all possibilities iteratively as to compute f on all vertices. Instead, we shall seek a method of recursively constructing a path through the graph, that identifies \u03b4\u03c3k satisfying the conditions (A3) and (A4), and hence the optimized partial ordering \u03c3 (nk) opt .\nTo begin, we observe that \u03c3 (n1) opt and \u03c3 (nK\u22121) opt are (relatively) easy to construct, up to\ndegeneracy, using the following two methods, respectively:\n(I) Starting from the empty configuration \u2205 = \u03c3(0)opt, progress through the upwards\ndirected graph computing the vertex weights, and identify the set of vertices of the smallest cardinality with f(\u03c3) > f(\u2205)(= 0). Of these, choose the vertex with the largest objective weight to be \u03c3 (n1) opt . For the LLP decay vertex reconstruction requirements considered here (see Sec. II B) this choice will often be just two panels, in which case n1 = 2. However, an objective with more complex reconstruction requirements, such as more than two tracks per LLP decay vertex, might select n1 > 2. By construction, all \u03c3 such that |\u03c3| \u2264 n1 must satisfy\nf(\u03c3 (n1) opt ) \u2265 f(\u03c3) . (A6)\n(II) Starting from the full configuration \u03a3 = \u03c3 (nK) opt (under our convention nK = N) and\nmoving through the downwards directed graph, find the configuration of smallest cardinality whose objective is the same as the total f(\u03c3) = f(\u03c3 (nK) opt ), and choose this to be \u03c3 (nK\u22121) opt . Any vertex \u03c3 on the path from \u03a3 to this configuration must satisfy,\nf(\u03c3 (nK\u22121) opt )/nK\u22121 \u2265 f(\u03c3)/|\u03c3| , (A7)\nas must all \u03c3 such that |\u03c3| \u2265 nK\u22121. That is, \u03c3(nK\u22121)opt has the maximal objective per panel among all configurations of size nK\u22121 or higher.\nIt is important to remember that neither \u03c3 (n1) opt nor \u03c3 (nK\u22121) opt constructed in this manner are\nguaranteed to be unique, though one must require \u03c3 (nK\u22121) opt \u2283 \u03c3 (n1) opt . Iteratively applying the algorithms (I) and (II) starting with \u03c3 (n1) opt and \u03c3 (nK\u22121) opt , however, does not identify \u03c3 (n2) opt and \u03c3 (nK\u22122) opt . The reason for this is that although a directed subgraph from \u03c3 (n1) opt to \u03c3 (nK\u22121) opt has the same topology as the full graph (they are hypercubes of dimension nK\u22121 \u2212 n1 and N , respectively) one cannot guarantee from (I) and (II) alone that \u03c3 (n2) opt and \u03c3 (nK\u22122) opt lie within\nthis subgraph. Instead, we seek a method to identify a subset of the \u03c3 (nk) opt that is sparsely distributed over the entire graph, which we denote as \u03c3 (nki )\nopt . This may then be applied\nrecursively to generate a larger and more refined set of \u03c3 (nk) opt and hence an optimized partial ordering. The advantage of this method will be that it scales computationally as O(K2J), where J is the typical step size of adjacent nki \u2212 nki+1 \u223c few, rather than as O(2N).\n2. Branch and bound algorithm\nTo proceed further, we seek a means to traverse the upwards directed graph from a given \u03c3 (nk) opt , and efficiently identify which of its child vertices cannot themselves have children that may be a candidate \u03c3 (nk\u2032 ) opt , nk\u2032 > nk. This allows us to prune away paths, leaving a small subset on which we evaluate the objective directly to identify \u03c3 (nk\u2032 ) opt . To do this, we note from Eq. (A7), that f(\u03c3 (nk) opt )/nk should similarly be maximal for all |\u03c3| = nk. Thus we expect that a candidate \u03c3 (nk\u2032 ) opt should lie on the directed path on which the objective per panel is maximal. This leads us to propose the following test function for a vertex \u03c3 \u2283 \u03c3(nk)opt , namely the maximal objective per panel of the children of \u03c3,\nT (\u03c3) = max \u03c3\u2032\u2282\u03c3\nf(\u03c3\u2032)\n|\u03c3\u2032| = max \u03b4\u03c3:\u03b4\u03c3\u2229\u03c3=\u2205 f(\u03c3) + f(\u03b4\u03c3|\u03c3) |\u03b4\u03c3|+ |\u03c3| . (A8)\nIf T (\u03c3) < f(\u03c3\u0304)/|\u03c3\u0304| for some other \u03c3\u0304 \u2287 \u03a3k for which the objective is known, then all paths from \u03c3 upwards can be pruned.\nIn practice T is too expensive to compute, as it requires computation of 2N\u2212|\u03c3| terms. We therefore require a bounding estimator of the objective, f\u0304(\u03c3) > f(\u03c3), from which we can construct a bounding estimator of T that can be quickly computed with only linear complexity in N \u2212 |\u03c3|. In particular, provided f\u0304 has the conditional subadditive property that\nf\u0304(\u03b4\u03c3 \u222a \u03b4\u03c3\u2032|\u03c3) \u2264 f\u0304(\u03b4\u03c3|\u03c3) + f\u0304(\u03b4\u03c3\u2032|\u03c3) , (A9)\nthen\nB(\u03c3) = max \u03b4\u03c3:\u03b4\u03c3\u2229\u03c3=\u2205\nf(\u03c3) + \u2211\np\u2208\u03b4\u03c3 f\u0304({p}|\u03c3) |\u03b4\u03c3|+ |\u03c3| \u2265 T (\u03c3) . (A10)\nThat is, B(\u03c3) bounds the test function, but requires only the computation of N \u2212|\u03c3| terms, and is therefore far faster to compute. It is then sufficient to test whether B(\u03c3) < f(\u03c3\u0304)/|\u03c3\u0304|\nfor \u03c3\u0304 \u2287 \u03c3 whose objective is known, in order to eliminate all paths upwards from \u03c3.\nThe algorithm to construct \u03c3 (nk\u2032 ) opt from \u03c3 (nk) opt then proceeds as follows. Define a set of\nvertices \u03a3eval, initially containing just \u03c3 (nk) opt , whose objective f(\u03c3 (nk) opt ) is known. Starting from \u03c3 (nk) opt :\n(1) Compute the objective for the immediate child vertices, i.e., f(\u03c3), for each \u03c3 = {p} \u222a\n\u03c3 (nk) opt with p 6\u2208 \u03c3 (nk) opt . Assign each \u03c3 to \u03a3eval.\n(2) For each such \u03c3:\n(i) Compute f\u0304({p}|\u03c3) for all p 6\u2208 \u03c3 and sort them from highest to lowest value, to\ngenerate the ordered sequence {f\u0304({pi}|\u03c3)}i.\n(ii) One may then compute the maximand in Eq. (A10)\nf(\u03c3) + \u2211n\ni=1 f\u0304({pi}|\u03c3) n+ |\u03c3| , (A11)\nas a function of n until a local maximum is reached.\n(iii) This local maximum is used as a proxy for the true maximum, and thus the value\nof B(\u03c3). If B(\u03c3) < f(\u03c3\u2032)/|\u03c3\u2032| for some other \u03c3\u2032 \u2208 \u03a3eval whose objective has been computed in step (1), then discard \u03c3 and all paths in the directed graph that pass through it, and remove \u03c3 from \u03a3eval.\n(3) For each remaining child vertex \u03c3, recursively apply this algorithm starting from\nstep (1). Each recursion branch terminates when all immediate children are discarded by the bound.\n(4) Once all branches terminate, the \u03c3 \u2208 \u03a3eval with the largest f(\u03c3)/|\u03c3| is identified as\n\u03c3 (nk\u2032 ) opt .\nHaving determined an initial sparse set, \u03c3 (nki )\nopt one may then further recursively apply\nthis entire algorithm to each restricted directed subgraph from \u03c3 (nki ) opt to \u03c3 (nki+1 ) opt \u2014a hypercube of dimension nki+1 \u2212 nki that is self-similar to the full graph\u2014in order to determine a finer-spaced set of optimized configurations. Once restricted to this subgraph, the possible members of \u03a3eval at each step is correspondingly more restricted, leading to less restrictive bounds. Thus one may identify optimized configurations excluded in a prior pass. This recursion terminates once no further optimized configurations are identified, typically when nk+1 \u2212 nk \u223c few. This procedure generates the partially optimized ordering \u03c3(nk)opt , for a set of nk \u2264 N .\n3. Any-single-hit estimator\nWe now seek to identify a bounding estimator that obeys Eq. (A9). For the kinds of objectives and reconstruction criteria we consider, the objective function f may be decomposed into two pieces. First, a \u201creconstruction map\u201d, that determines the set of events that is reconstructible by a configuration, g : \u03c3 7\u2192 g(\u03c3). This map must be positive semi-definite, i.e. g(\u03c3) \u2286 g(\u03c3\u2032) for \u03c3 \u2286 \u03c3\u2032. Second, an outer measure \u03c1 that determines the vertex reconstruction efficiency of \u03c3, i.e. \u03c1 : g(\u03c3) 7\u2192 f(\u03c3). In the composition f = \u03c1 \u25e6 g, \u03c1 is additive (strictly, countably additive) by construction, and independent of reconstruction criteria.\nIn this language, the negative nonadditivity (f(\u03c3 \u222a \u03c3\u2032) < f(\u03c3) + f(\u03c3\u2032)) of the objective from panel-panel redundancies arises because for two disjoint configurations \u03c3 and \u03c3\u2032, the intersection g(\u03c3)\u2229g(\u03c3\u2032) can nonetheless be non-empty. Similarly, the positive nonadditivity (f(\u03c3\u222a\u03c3\u2032) > f(\u03c3)+f(\u03c3\u2032)) from hit-hit correlations arises because the set of events in g(\u03c3\u222a\u03c3\u2032)\nbut not in g(\u03c3) \u222a g(\u03c3\u2032)\u2014i.e. g(\u03c3 \u222a \u03c3\u2032) \\ [ g(\u03c3) \u222a g(\u03c3\u2032) ] \u2014can be nonempty. See Figure 11.\nThe LLP vertex reconstruction map g requires at least two LLP decay tracks to hit at least two RPC triplets, with a minimum track momentum threshold and hit separation (see Sec. II B). These requirements can be subsumed by looser ones: in particular the loosest,\nnontrivial such requirement is simply for at least one track in an event to have at least one hit. We define the corresponding \u201cany-single-hit\u201d reconstruction map by g\u0304. Though g\u0304 does not permit actual reconstruction of the LLP decay vertex, it must be the case that\ng\u0304(\u03c3) \u2287 g(\u03c3) , (A12)\nfor any \u03c3. Thus defining the any-single-hit estimator f\u0304 = \u03c1 \u25e6 g\u0304, then\nf(\u03c3) = \u03c1(g(\u03c3)) \u2264 \u03c1(g\u0304(\u03c3)) = f\u0304(\u03c3) , (A13)\ni.e. f\u0304 bounds f . This estimator is clearly positive semi-definite. Moreover, g\u0304 is additive by construction,\ng\u0304(\u03c3 \u222a \u03c3\u2032) = g\u0304(\u03c3) \u222a g\u0304(\u03c3\u2032) . (A14)\nIt follows that g\u0304(\u03b4\u03c3 \u222a \u03b4\u03c3\u2032 \u222a \u03c3) \u222a g\u0304(\u03c3) = g\u0304(\u03b4\u03c3 \u222a \u03c3 \u222a \u03b4\u03c3\u2032 \u222a \u03c3) = g\u0304(\u03b4\u03c3 \u222a \u03c3) \u222a g\u0304(\u03b4\u03c3\u2032 \u222a \u03c3), so that f\u0304(\u03b4\u03c3 \u222a \u03b4\u03c3\u2032 \u222a \u03c3) + f\u0304(\u03c3) = f\u0304(\u03b4\u03c3 \u222a \u03c3) + f\u0304(\u03b4\u03c3\u2032 \u222a \u03c3), and hence Eq. (A9) is satisfied at the boundary of the inequality, providing a relatively tight upper bound for Eq. (A10).\nAppendix B: Simulation framework\nThe Python3 simulation framework is structured into two main components: First, a \u201chepgk\u201d module, implementing a small Constructive Solid Geometry (CSG) geometry kernel for representing the detector volume, and a simple vertex representation of tracking surfaces, as well as the core geometry optimizer classes which derive from the pybnb [67] library. Second, a \u201chepymc\u201d module, that contains pythonic convenience wrapper derived from the HepMC3 [68] python bindings, and so-called processor classes, designed to reweight LLP events, implement particular LLP decay morphologies, and compute track hits on geometric elements, as required. In this Appendix we summarize the general features, functionalities and approximate code flow of this framework.\n1. Geometry kernel\nUsing the hepgk module, one can initialize different detector element geometries, including arbitrary fiducial decay volumes and tracking stations. Volumes are implemented at a fundamental level as unions, intersections or complements of convex polytopes that are fully specified by a list of their vertices. Additional sets of vertices encoding 2-dimensional polygons embedded in three dimensions are used to specify tracking station panel elements.\nSeveral predefined geometries are implemented, that admit definitions in terms of the center, symmetry axes, and pertinent dimensions. For instance, the inputs for the (approximate polyhedral) Cylinder object are its center, axis, radius, and length. Other predefined polyhedra include Cube, Tetrahedron and RectangularPrism. The user may also easily define additional preset geometries.\nAs an example, the nominal CODEX-b volume is implemented as\nimport hepgk as gk\nCODEXb_center, CODEXb_sidelen = [31e3, -2e3, 10e3], 10e3 CODEXb_geometry = gk.Cube(CODEXb_center, CODEXb_sidelen)\n2. HepyMC module and processors\nThe hepymc module is built around the general Processor class. This abstract class takes lists of events and then processes or modifies them in some way, returning the processed list to an output HepMC file. More precisely, the processor returns an iterable generator which iterates over the events of the processed list. The user can create a specific processor by initializing a class that inherits from the parent Processor class. The currently available set of processor classes, their corresponding functionalities, and schematic inputs and outputs are shown in Table III.\nFor example, the VolumeFilter processor selects all events from an EventRecord that contain\nan LLP that decays within a specified hepgk detector geometry. It is implemented as\nfrom .processor import *\nclass VolumeFilter(Processor):\ndef __init__(self, geometry, target=lambda p: p.pid == 999999,\\\nprecheck=lambda e: True, pstcheck=lambda e: True):\nself.geometry = geometry self.is_target = target super().__init__(self.identity, precheck=precheck,\npstcheck=lambda e: pstcheck(e) and self.in_vol(e))\ndef identity(self, event):\nyield event\ndef in_vol(self, event):\nreturn any(self.geometry.contains(p.end_vertex.position.vec3())\\\nfor p in event.particles if self.is_target(p)\\\nand p.end_vertex.position is not None)\nTo initialize the VolumeFilter, one must specify a geometry that determines the fiducial volume, as well as a target selection function for the decaying LLP particle of interest (by default assigned a particle identification (PID) number of 999999). One can also pass additional so-called precheck or pstcheck condition checking functions, discussed briefly further below. Initialization is completed by defining an action for the superclass: In this case, it is the identity generator method, which simply yields an unmodified event. The parent Processor class only performs action for events that pass the selections based on precheck and pstcheck functions, which filter input and output events of the action, respectively. Here, the default pstcheck uses the in_vol method which contains all the machinery of this processor, checking if an LLP decay vertex (whose location has typically has been set by a Rescaler preprocessor) is contained in the specified geometry. Processor classes may also augment events or add events (that is, clone events from one to many) by implementing a less trivial action generator function. For example, the Flipper class applies a Z2 reflection about a user-specified plane. from .processor import * from .. import event as ev, particle as pt import numpy as np\nclass Flipper(Processor):\ndef __init__self, axis=\u2019z\u2019, orientation=1, check_orientation=True,\nprecheck=lambda e: True, pstcheck=lambda e: True):\nself.axis = {a : i for i, a in enumerate([\u2019x\u2019, \u2019y\u2019, \u2019z\u2019, \u2019t\u2019])}[axis] self.orientation = np.sign(orientation) self._should_check = check_orientation self.PIDs = PIDs super().__init__(self.flip, precheck=precheck,\npstcheck=lambda e: self.oriented(e) and pstcheck(e))\ndef factor(self):\nreturn 2\ndef oriented(self, event):\nllps = [p for p in event.particles if p.pid==999999] return any([self.orientation * llp.momentum[self.axis] >= 0 for llp in\nllps]) if self._should_check else True\ndef flip(self, event):\nyield ev.Event(event) yield ev.Event(event).reflect(self.axis)\nwhere the reflect method for events is provided by a wrapped HepMC3 binding.\n3. Code flow\nIn this framework, detector analysis proceeds via the following code flow:\n(1) An initial HepMC sample is generated by Pythia8, (usually) containing undecayed\nLLPs, and stored.\n(2) A detector geometry is initialized, specifying both a fiducial decay volume and tracking\nelements.\n(3) These events are then passed through:\n(i) a units processor, to ensure correct conventions\n(ii) if needed, a splitter processor, to partition events with multiple LLP-producing\ndecays into separate events with a single LLP-producing decay each (an example is an event containing BB\u0304 pairs, that are both decayed to an LLP by Pythia8).\n(iii) a rotation and reflection processor, to boost the statistical sample in the detector\nacceptance\n(iv) a reweighting processor, that assigns an event weight for the displacement of the\nLLP decay-in-flight, according to an exponential with characteristic displacement \u03b2\u03b3c\u03c4\n(v) the fiducial volume processor, that selects events with LLP decays in the detector (vi) a decayer processor, that appends various desired final states to the LLP decay\n(such as e+e\u2212 or 4\u03c0 etc)\n(vii) a tracking processor, which appends track hit information on the various tracking\nelements to the HepMC record.\n4. Normalization of efficiencies\nFor an MC sample ofNs weighted events, indexed by i with MC generation weight wi, each containing a single LLP, the total efficiency (see definition at the beginning Sec II) for the\nLLP decay vertex and kinematics to satisfy the acceptance and reconstruction requirements has the simple estimator\n\u03b5 = 1\nNs Ns\u2211 i=1 wiri,dec . (B1)\nHere ri,dec is a reweighting factor for the LLP decay to fall in the detector acceptance (and the vertex be reconstructed): the ratio of the probability of decay (and reconstruction), pi,dec, to the sampling probability, ps, which is usually taken to be generated according to a log-uniform distribution in the LLP decay vertex displacement. I.e., ri,dec = pi,dec/ps. The numerator is zero if the LLP does not decay in the detector acceptance (and/or is not reconstructed). Often the LLPs are generated in unweighted events, i.e. wi = 1; but in cases that, e.g., a long tail in the pT of the LLP parent must be sampled efficiently, the MC generation may assign a nontrivial weight.\nFurther, because LLPs may be multiply produced in some portals, such as in h\u2192 A\u2032A\u2032, in computing the efficiency for these portals one must be careful with combinatorics and correlations. In this scenario, we are interested in computing the efficiency for at least one LLP to decay and be reconstructed by the detector in any given event. For an MC sample of Ns weighted events containing ni LLPs in the ith event, the ith (estimator of the) reweighting\nfactor for detecting at least one LLP decay then becomes ri,dec = [1\u2212 \u220fni \u03b1=1(1\u2212 p\u03b1,rec)]/ps. Because typically p\u03b1,rec 1 in the detectors we consider, then to a good approximation this may be linearized over the LLPs, so that\n\u03b5 ' 1 Ns Ns\u2211 i=1 wi ni\u2211 \u03b1=1 r\u03b1,dec , (B2)\nwith r\u03b1,dec = p\u03b1,rec/ps.\nFor convenience and parallelization, our MC samples are partitioned into subsamples. Thus to properly normalize the LLP event weights in order to compute the total efficiency, one must keep track of the total Ns across multiple subsamples: if Ns is different in each subsample, then the average of the ratios is not the ratio of the sum. Further, techniques that boost the MC statistics, such as discrete azimuthal rotation or reflection classes, modify the effective number of events in the simulation (sub)sample, and therefore require a careful accounting. The framework addresses both of these effects by allowing creation of a special final event in each output HepMC (sub)sample, that keeps track of the number\nof discarded events, Ndisc, that did not pass selection by whatever processor was applied to the (sub)sample. The sum of the Ndisc plus the total number of events in set of subsamples determines the denominator in the fiducial efficiency.\nAs an example, we may consider the normalization logic for the PhiStatBoost processor, that implements an enhancement of the MC statistics through discrete azimuthal angular rotations. In the particular case of the CODEX-b detector, one observes that the detector acceptance subtends quite a small angle around the beamline, just less than 2\u03c0/16 radians. Thus, if one takes an initial MC sample and applies discrete azimuthal rotations in steps of 2\u03c0/16, keeping all events with at least one LLP in the detector azimuthal angular acceptance at each step, one may significantly boost the statistics by approximately a factor of 16. The processor logic works as follows:\n(1) Compute the minimal azimuthal angular wedge of size 2\u03c0/n, i.e. for maximal possible\ninteger n, covering the detector acceptance (for CODEX-b, n = 16). Then, for each event:\n(2) Extract the LLP momenta (there may be more than one, in the case of e.g. h\u2192 A\u2032A\u2032). (3) If at least one LLP momentum lies in the detector angular acceptance, write the event\nto the output sample. Otherwise increment the discarded events Ndisc by one.\n(4) Rotate the entire event by 2\u03c0/16 and return to step (3), until n rotations have been\nperformed.\nExamining a particular sample of 50000 h\u2192 A\u2032A\u2032 events as an example, we naively expect only 50000/16\u00d72 = 6250 events to contain at least one LLP inside the detector acceptance. Applying the PhiStatBoost processor, one finds it is processed into a sample of Ns = 99343 events with at least one LLP inside the detector acceptance, and Ndisc = 700657; the total is 16\u00d750000. Thus we see the statistics is boosted by approximately a factor of 99343/6250 ' 15.9 as expected. Combining M such rotated subsamples, the total efficiency estimator becomes\n\u03b5 = ( M\u2211 m=1 Nm,s\u2211 i=1 wm,irm,i,dec )/[ M\u2211 m=1 Nm,s +Nm,disc ] . (B3)\n5. Example scripts\nHere we show an example script that implements the PhiStatBoost processor, taking a HepMC subsample file (or list of files) and geometry as a command line argument, and\nwriting the final normalization event in the output HepMC.\nmport os, sys, argparse as ap, pickle as pk, itertools as it from glob import glob from tqdm import tqdm sys.path.append(\u2019./\u2019) sys.path.append(\u2019../\u2019) import hepymc as mc, hepymc.processors as proc\nfrom pyHepMC3 import HepMC3 as hmc3 hmc3.Setup.set_print_warnings(False)\nif __name__ == \"__main__\":\nparser = ap.ArgumentParser(description=\"Provide statistics boost by rotational\nsymmetry in \u2018.hepmc\u2018 files\")\nparser.add_argument(\"geometry\", metavar=\"GEO_FILE\",\nhelp=\"Pickled \u2018.pk\u2018 geometry file to base phi angle on.\", type=str)\nparser.add_argument(\"files\", metavar=\"EVENT_FILE\",\nhelp=\"Event \u2018.hepmc\u2018 file(s) to process.\", nargs=\u2019+\u2019, type=str, default=[])\nparser.add_argument(\"-v\", \"--verbose\", help=\"Indicate status.\",\naction=\"store_true\")\nparser.add_argument(\"-l\", \"--logged\", help=\"Indicate for logged use.\",\naction=\"store_true\")\nargs = parser.parse_args()\nwith open(args.geometry, \u2019rb\u2019) as f:\ngeometry, _ = pk.load(f)\nrotbst = proc.PhiStatBoost(geometry, check_orientation=True)\nfilenames = [name for fn in args.files for name in glob(fn)] for fn in tqdm(filenames, desc=\"File\", unit=\"files\",\nleave=args.logged, disable=not args.verbose):\nwith mc.open(fn, \u2019r\u2019, version=3) as f:\nevents = list(tqdm(f, desc=\"Loading\", unit=\"events\", leave=args.logged,\ndisable=not args.verbose, postfix=os.path.basename(fn)))\nevents = list(rotbst(tqdm(events, desc=\"Boosting\", unit=\"events\",\nleave=args.logged,\ndisable=not args.verbose, postfix=\"x\" +\nstr(rotbst.factor))))\nfo = os.path.splitext(fn)[0] + \".ROTBSTx\" + str(rotbst.factor) + \".hepmc3\" with mc.open(fo, \u2019w\u2019, version=3) as f:\nf.write_events(tqdm(events, desc=\"Writing\", unit=\"events\",\nleave=args.logged, disable=not args.verbose, postfix=os.path.basename(fo)))\npre_discarded_count = proc.bin_events(tqdm(rotbst.prediscarded,\ndesc=\"Accounting pre-discarded\", leave=args.logged, disable=not\nargs.verbose),\nfactor=lambda e: rotbst.factor)\ndiscarded_count = proc.bin_events(it.chain(pre_discarded_count,\ntqdm(rotbst.pstdiscarded, desc=\"Accounting post-discarded\",\nunit=\"events\",\nleave=args.logged, disable=not\nargs.verbose)))\nf.write_events(tqdm(discarded_count,\ndesc=\"Writing Normalization\", unit=\"events\", leave=args.logged, disable=not args.verbose))\nrotbst.reset() del events\nIn addition, we show a script that implements the Rescaler processor. This processor computes the reweighting factors for LLP decays with respect to a log uniform sampling distribution in the LLP displacement l = 10uc\u03c4 , for a range of log(c\u03c4/mm) values. These are passed as a list to the command line \"--ctaus\" argument. The range of the log uniform sampling distribution exponent u is defined by arguments \"--minlog\" and \"--maxlog\", The number of sampling \u201cthrows\u201d for each LLP is determined by the argument \"--num\"; a clone of the event is created for each throw and written to output HepMC. #!/usr/bin/python3 import os, sys, argparse as ap, itertools as it from glob import glob from tqdm import tqdm sys.path.append(\u2019./\u2019) sys.path.append(\u2019../\u2019) import hepymc as mc, hepymc.processors as proc\nfrom pyHepMC3 import HepMC3 as hmc3 hmc3.Setup.set_print_warnings(False)\nif __name__ == \"__main__\":\nparser = ap.ArgumentParser(description=\"Rescale target particle lifetimes in\n\u2018.hepmc\u2018 files\")\nparser.add_argument(\"files\", metavar=\"EVENT_FILE\",\nhelp=\"Event \u2018.hepmc\u2018 file(s) to process.\",\nnargs=\u2019+\u2019, type=str, default=[])\nparser.add_argument(\"-t\", \"--ctaus\", help=\"Log lifetimes (in event current\nunits) to scale to.\",\nnargs=\u2019+\u2019, type=float, default=[])\nparser.add_argument(\"--linthrow\", help=\"Interpret log ctaus as linear lifetimes\nin the event current units.\",\naction=\"store_false\")\nparser.add_argument(\"-n\", \"--num\", help=\"Number of repeated throws to make per\nevent.\",\ntype=int, default=1)\nparser.add_argument(\"-p\",\"--pids\", help=\"PDG PIDs to target, default=[999999].\",\nnargs=\u2019+\u2019, type=int, default=[999999])\nparser.add_argument(\"--minlog\", help=\"Min log throw.\", type=float, default=-3.) parser.add_argument(\"--maxlog\", help=\"Max log throw.\", type=float, default=11.) parser.add_argument(\"--noprop\", help=\"Don\u2019t propagate rescaling to children.\",\naction=\"store_true\")\nparser.add_argument(\"-v\", \"--verbose\", help=\"Indicate status.\",\naction=\"store_true\")\nparser.add_argument(\"-l\", \"--logged\", help=\"Indicate for logged use.\",\naction=\"store_true\")\nargs = parser.parse_args()\nrescale = proc.Rescaler(args.ctaus, logt=args.linthrow, PID=args.pids,\nnum=args.num,\nminlogthrow=args.minlog, maxlogthrow=args.maxlog, prop=not args.noprop)\nfilenames = [name for fn in args.files for name in glob(fn)] for fn in tqdm(filenames, desc=\"File\", unit=\"files\",\nleave=args.logged, disable=not args.verbose):\nwith mc.open(fn, \u2019r\u2019, version=3) as f:\nevents = list(tqdm(f, desc=\"Loading\", unit=\"events\", leave=args.logged,\ndisable=not args.verbose, postfix=os.path.basename(fn)))\nevents = list(rescale(tqdm(events, desc=\"Rescaling\", unit=\"events\",\nleave=args.logged,\ndisable=not args.verbose)))\nfo = os.path.splitext(fn)[0] + \".RESCALED\" + str(args.ctaus).strip() +\n\".hepmc3\"\nwith mc.open(fo, \u2019w\u2019, version=3) as f:\nf.write_events(tqdm(events, desc=\"Writing\", unit=\"events\",\nleave=args.logged, disable=not args.verbose, postfix=os.path.basename(fo)))\npre_discarded_count = proc.bin_events(tqdm(rescale.prediscarded,\ndesc=\"Accounting pre-discarded\",\nleave=args.logged, disable=not\nargs.verbose))\ndiscarded_count = proc.bin_events(it.chain(pre_discarded_count,\ntqdm(rescale.pstdiscarded, desc=\"Accounting post-discarded\",\nunit=\"events\",\nleave=args.logged, disable=not\nargs.verbose)))\nf.write_events(tqdm(discarded_count,\ndesc=\"Writing Normalization\", unit=\"events\", leave=args.logged, disable=not args.verbose))\nrescale.reset() del events\n[1] J. Beacham et al., Physics Beyond Colliders at CERN: Beyond the Standard Model Working\nGroup Report, J. Phys. G 47 (2020) 010501, [1901.09966].\n[2] J. Alimena et al., Searching for long-lived particles beyond the Standard Model at the Large\nHadron Collider, J. Phys. G 47 (2020) 090501, [1903.04497].\n[3] P. Agrawal et al., Feebly-Interacting Particles:FIPs 2020 Workshop Report, 2102.12143.\n[4] M. Artuso, R. H. Bernstein and A. A. Petrov, Report of the Frontier For Rare Processes and\nPrecision Measurements, 2210.04765.\n[5] T. Bose et al., Report of the Topical Group on Physics Beyond the Standard Model at Energy\nFrontier for Snowmass 2021, in 2022 Snowmass Summer Study, 9, 2022. 2209.13128.\n[6] S. Gori et al., Dark Sector Physics at High-Intensity Experiments, 2209.04671.\n[7] D. Curtin et al., Long-Lived Particles at the Energy Frontier: The MATHUSLA Physics\nCase, Rept. Prog. Phys. 82 (2019) 116201, [1806.07396].\n[8] L. Lee, C. Ohm, A. Soffer and T.-T. Yu, Collider Searches for Long-Lived Particles Beyond\nthe Standard Model, Prog. Part. Nucl. Phys. 106 (2019) 210\u2013255, [1810.12602].\n[9] G. Aielli et al., Expression of interest for the CODEX-b detector, Eur. Phys. J. C 80 (2020)\n1177, [1911.00481].\n[10] J. L. Feng et al., The Forward Physics Facility at the High-Luminosity LHC, 2203.05090.\n[11] V. V. Gligorov, S. Knapen, M. Papucci and D. J. Robinson, Searching for Long-lived\nParticles: A Compact Detector for Exotics at LHCb, Phys. Rev. D97 (2018) 015023,\n[1708.09395].\n[12] C. Fanelli, Design of detectors at the electron ion collider with artificial intelligence, JINST\n17 (2022) C04038, [2203.04530].\n[13] Mode: Machine-learning optimized design of experiments,\nhttps://mode-collaboration.github.io/.\n[14] S. Shirobokov, V. Belavin, M. Kagan, A. Ustyuzhanin and A. G. Baydin, Black-Box\nOptimization with Local Generative Surrogates, 2002.04632.\n[15] MATHUSLA collaboration, C. Alpigiani et al., Recent Progress and Next Steps for the\nMATHUSLA LLP Detector, in 2022 Snowmass Summer Study, 3, 2022. 2203.08126.\n[16] V. V. Gligorov, S. Knapen, B. Nachman, M. Papucci and D. J. Robinson, Leveraging the\nALICE/L3 cavern for long-lived particle searches, Phys. Rev. D 99 (2019) 015023,\n[1810.03636].\n[17] J. Pinfold, MoEDAL-MAPP, an LHC Dedicated Detector Search Facility, in 2022 Snowmass\nSummer Study, 9, 2022. 2209.03988.\n[18] M. Bauer, O. Brandt, L. Lee and C. Ohm, ANUBIS: Proposal to search for long-lived neutral\nparticles in CERN service shafts, 1909.13022.\n[19] ATLAS collaboration, Technical Design Report for the Phase-II Upgrade of the ATLAS\nMuon Spectrometer, Tech. Rep. CERN-LHCC-2017-017. ATLAS-TDR-026, CERN, Geneva,\nSep, 2017.\n[20] B. Dey, J. Lee, V. Coco and C.-S. Moon, Background studies for the CODEX-b experiment:\nmeasurements and simulation, 1912.03846.\n[21] T. Sjo\u0308strand, S. Ask, J. R. Christiansen, R. Corke, N. Desai, P. Ilten et al., An introduction\nto PYTHIA 8.2, Comput. Phys. Commun. 191 (2015) 159\u2013177, [1410.3012].\n[22] R. M. Schabinger and J. D. Wells, A Minimal spontaneously broken hidden sector and its\nimpact on Higgs boson physics at the large hadron collider, Phys. Rev. D 72 (2005) 093007,\n[hep-ph/0509209].\n[23] S. Gopalakrishna, S. Jung and J. D. Wells, Higgs boson decays to four fermions through an\nabelian hidden sector, Phys. Rev. D 78 (2008) 055002, [0801.3456].\n[24] D. Curtin, R. Essig, S. Gori and J. Shelton, Illuminating Dark Photons with High-Energy\nColliders, JHEP 02 (2015) 157, [1412.0018].\n[25] P. Meade, M. Papucci and T. Volansky, Dark Matter Sees The Light, JHEP 12 (2009) 052,\n[0901.2925].\n[26] M. Buschmann, J. Kopp, J. Liu and P. A. N. Machado, Lepton Jets from Radiating Dark\nMatter, JHEP 07 (2015) 045, [1505.07459].\n[27] R. S. Willey and H. L. Yu, Decays K\u00b1 \u2192 \u03c0\u00b1l+l\u2212 and limits on the mass of the neutral higgs\nboson, Phys. Rev. D 26 (Dec, 1982) 3287\u20133289.\n[28] R. Chivukula and A. V. Manohar, Limits on a light higgs boson, Physics Letters B 207\n(1988) 86 \u2013 90.\n[29] B. Grinstein, L. J. Hall and L. Randall, Do B meson decays exclude a light Higgs?, Phys.\nLett. B211 (1988) 363\u2013369.\n[30] M. W. Winkler, Decay and detection of a light scalar boson mixing with the Higgs boson,\nPhys. Rev. D99 (2019) 015018, [1809.01876].\n[31] Y. Gershtein, S. Knapen and D. Redigolo, Probing naturally light singlets with a displaced\nvertex trigger, 2012.07864.\n[32] J. F. Gunion, H. E. Haber, G. L. Kane and S. Dawson, The Higgs Hunter\u2019s Guide, vol. 80.\n2000.\n[33] pybnb: A parallel branch-and-bound engine for python, https://pypi.org/project/pybnb/.\n[34] Hepmc3 event record library, http://hepmc.web.cern.ch/hepmc/.\n[35] J. Beacham et al., Physics Beyond Colliders at CERN: Beyond the Standard Model Working\nGroup Report, J. Phys. G 47 (2020) 010501, [1901.09966].\n[36] J. Alimena et al., Searching for long-lived particles beyond the Standard Model at the Large\nHadron Collider, J. Phys. G 47 (2020) 090501, [1903.04497].\n[37] P. Agrawal et al., Feebly-Interacting Particles:FIPs 2020 Workshop Report, 2102.12143.\n[38] M. Artuso, R. H. Bernstein and A. A. Petrov, Report of the Frontier For Rare Processes and\nPrecision Measurements, 2210.04765.\n[39] T. Bose et al., Report of the Topical Group on Physics Beyond the Standard Model at Energy\nFrontier for Snowmass 2021, in 2022 Snowmass Summer Study, 9, 2022. 2209.13128.\n[40] S. Gori et al., Dark Sector Physics at High-Intensity Experiments, 2209.04671.\n[41] D. Curtin et al., Long-Lived Particles at the Energy Frontier: The MATHUSLA Physics\nCase, Rept. Prog. Phys. 82 (2019) 116201, [1806.07396].\n[42] L. Lee, C. Ohm, A. Soffer and T.-T. Yu, Collider Searches for Long-Lived Particles Beyond\nthe Standard Model, Prog. Part. Nucl. Phys. 106 (2019) 210\u2013255, [1810.12602].\n[43] G. Aielli et al., Expression of interest for the CODEX-b detector, Eur. Phys. J. C 80 (2020)\n1177, [1911.00481].\n[44] J. L. Feng et al., The Forward Physics Facility at the High-Luminosity LHC, 2203.05090.\n[45] V. V. Gligorov, S. Knapen, M. Papucci and D. J. Robinson, Searching for Long-lived\nParticles: A Compact Detector for Exotics at LHCb, Phys. Rev. D97 (2018) 015023,\n[1708.09395].\n[46] C. Fanelli, Design of detectors at the electron ion collider with artificial intelligence, JINST\n17 (2022) C04038, [2203.04530].\n[47] Mode: Machine-learning optimized design of experiments,\nhttps://mode-collaboration.github.io/.\n[48] S. Shirobokov, V. Belavin, M. Kagan, A. Ustyuzhanin and A. G. Baydin, Black-Box\nOptimization with Local Generative Surrogates, 2002.04632.\n[49] MATHUSLA collaboration, C. Alpigiani et al., Recent Progress and Next Steps for the\nMATHUSLA LLP Detector, in 2022 Snowmass Summer Study, 3, 2022. 2203.08126.\n[50] V. V. Gligorov, S. Knapen, B. Nachman, M. Papucci and D. J. Robinson, Leveraging the\nALICE/L3 cavern for long-lived particle searches, Phys. Rev. D 99 (2019) 015023,\n[1810.03636].\n[51] J. Pinfold, MoEDAL-MAPP, an LHC Dedicated Detector Search Facility, in 2022 Snowmass\nSummer Study, 9, 2022. 2209.03988.\n[52] M. Bauer, O. Brandt, L. Lee and C. Ohm, ANUBIS: Proposal to search for long-lived neutral\nparticles in CERN service shafts, 1909.13022.\n[53] ATLAS collaboration, Technical Design Report for the Phase-II Upgrade of the ATLAS\nMuon Spectrometer, Tech. Rep. CERN-LHCC-2017-017. ATLAS-TDR-026, CERN, Geneva,\nSep, 2017.\n[54] B. Dey, J. Lee, V. Coco and C.-S. Moon, Background studies for the CODEX-b experiment:\nmeasurements and simulation, 1912.03846.\n[55] T. Sjo\u0308strand, S. Ask, J. R. Christiansen, R. Corke, N. Desai, P. Ilten et al., An introduction\nto PYTHIA 8.2, Comput. Phys. Commun. 191 (2015) 159\u2013177, [1410.3012].\n[56] R. M. Schabinger and J. D. Wells, A Minimal spontaneously broken hidden sector and its\nimpact on Higgs boson physics at the large hadron collider, Phys. Rev. D 72 (2005) 093007,\n[hep-ph/0509209].\n[57] S. Gopalakrishna, S. Jung and J. D. Wells, Higgs boson decays to four fermions through an\nabelian hidden sector, Phys. Rev. D 78 (2008) 055002, [0801.3456].\n[58] D. Curtin, R. Essig, S. Gori and J. Shelton, Illuminating Dark Photons with High-Energy\nColliders, JHEP 02 (2015) 157, [1412.0018].\n[59] P. Meade, M. Papucci and T. Volansky, Dark Matter Sees The Light, JHEP 12 (2009) 052,\n[0901.2925].\n[60] M. Buschmann, J. Kopp, J. Liu and P. A. N. Machado, Lepton Jets from Radiating Dark\nMatter, JHEP 07 (2015) 045, [1505.07459].\n[61] R. S. Willey and H. L. Yu, Decays K\u00b1 \u2192 \u03c0\u00b1l+l\u2212 and limits on the mass of the neutral higgs\nboson, Phys. Rev. D 26 (Dec, 1982) 3287\u20133289.\n[62] R. Chivukula and A. V. Manohar, Limits on a light higgs boson, Physics Letters B 207\n(1988) 86 \u2013 90.\n[63] B. Grinstein, L. J. Hall and L. Randall, Do B meson decays exclude a light Higgs?, Phys.\nLett. B211 (1988) 363\u2013369.\n[64] M. W. Winkler, Decay and detection of a light scalar boson mixing with the Higgs boson,\nPhys. Rev. D99 (2019) 015018, [1809.01876].\n[65] Y. Gershtein, S. Knapen and D. Redigolo, Probing naturally light singlets with a displaced\nvertex trigger, 2012.07864.\n[66] J. F. Gunion, H. E. Haber, G. L. Kane and S. Dawson, The Higgs Hunter\u2019s Guide, vol. 80.\n2000.\n[67] pybnb: A parallel branch-and-bound engine for python, https://pypi.org/project/pybnb/.\n[68] Hepmc3 event record library, http://hepmc.web.cern.ch/hepmc/."
        }
    ],
    "title": "Geometry Optimization for Long-lived Particle Detectors",
    "year": 2022
}