{
    "abstractText": "Reinforcement learning is applied to the development of control strategies in order to reduce skin friction drag in a fully developed turbulent channel flow at a low Reynolds number. Motivated by the so-called opposition control (Choi et al., J. Fluid Mech., vol. 253, 1993, pp. 509\u2013543), in which a control input is applied so as to cancel the wall-normal velocity fluctuation on a detection plane at a certain distance from the wall, we consider wall blowing and suction as a control input, and its spatial distribution is determined by the instantaneous streamwise and wall-normal velocity fluctuations at distance 15 wall units above the wall. A deep neural network is used to express the nonlinear relationship between the sensing information and the control input, and it is trained so as to maximize the expected long-term reward, i.e. drag reduction. When only the wall-normal velocity fluctuation is measured and a linear network is used, the present framework reproduces successfully the optimal linear weight for the opposition control reported in a previous study (Chung & Talha, Phys. Fluids, vol. 23, 2011, 025102). In contrast, when a nonlinear network is used, more complex control strategies based on the instantaneous streamwise and wall-normal velocity fluctuations are obtained. Specifically, the obtained control strategies switch abruptly between strong wall blowing and suction for downwelling of a high-speed fluid towards the wall and upwelling of a low-speed fluid away from the wall, respectively. Extracting key features from the obtained policies allows us to develop novel control strategies leading to drag reduction rates as high as 37 %, which is higher than the 23 % achieved by the conventional opposition control at the same Reynolds number. Finding such an effective and nonlinear control policy is quite difficult by relying solely on human insights. The present results indicate that reinforcement learning can be a novel framework for the development of effective control strategies through systematic learning based on a large number of trials.",
    "authors": [
        {
            "affiliations": [],
            "name": "Takahiro Sonoda"
        },
        {
            "affiliations": [],
            "name": "Zhuchen Liu"
        },
        {
            "affiliations": [],
            "name": "Toshitaka Itoh"
        },
        {
            "affiliations": [],
            "name": "Yosuke Hasegawa"
        }
    ],
    "id": "SP:395e23563beb78b54c7dd3191dd1b9ede8533439",
    "references": [
        {
            "authors": [
                "G. BEINTEMA",
                "A. CORBETTA",
                "L. BIFERALE",
                "F. TOSCHI"
            ],
            "title": "Controlling Rayleigh\u2013B\u00e9nard convection",
            "year": 2020
        },
        {
            "authors": [
                "H. CHOI",
                "P. MOIN",
                "J. KIM"
            ],
            "title": "Active turbulence control for drag reduction in wall-bounded flows",
            "year": 1994
        },
        {
            "authors": [
                "H. CHOI",
                "R. TEMAM",
                "P. MOIN",
                "J. KIM"
            ],
            "title": "Feedback control for unsteady flow and its application",
            "venue": "J. Fluid Mech",
            "year": 1993
        },
        {
            "authors": [
                "K. 26091\u201326098. FUKAGATA",
                "K. IWAMOTO",
                "N. KASAGI"
            ],
            "title": "Contribution of Reynolds stress distribution to the skin",
            "year": 2002
        },
        {
            "authors": [
                "H. GHRAIEB",
                "J. VIQUERAT",
                "A. LARCHER",
                "P. MELIGA",
                "E. HACHEM"
            ],
            "title": "Single-step deep reinforcement learning for open-loop control of laminar and turbulent flows",
            "venue": "Phys. Rev. Fluids",
            "year": 2021
        },
        {
            "authors": [
                "E. HACHEM",
                "H. GHRAIEB",
                "J. VIQUERAT",
                "A. LARCHER",
                "P. MELIGA"
            ],
            "title": "Deep reinforcement learning for the control of conjugate heat transfer",
            "venue": "J. Comput",
            "year": 2021
        },
        {
            "authors": [
                "E.P. HAMMOND",
                "T.R. BEWLEY",
                "P. MOIN"
            ],
            "title": "Observed mechanisms for turbulence attenuation and enhancement in opposition-controlled wall-bounded flows",
            "venue": "Phys. Fluids",
            "year": 1998
        },
        {
            "authors": [
                "HAN",
                "B.-Z",
                "HUANG",
                "W.-X"
            ],
            "title": "Active control for drag reduction of turbulent channel flow based on convolutional neural networks",
            "venue": "Phys. Fluids",
            "year": 2020
        },
        {
            "authors": [
                "Y. HASEGAWA",
                "N. KASAGI"
            ],
            "title": "Dissimilar control of momentum and heat transfer in a fully developed turbulent channel flow",
            "venue": "J. Fluid Mech",
            "year": 2011
        },
        {
            "authors": [
                "A.D. JAGTAP",
                "K. KAWAGUCHI",
                "G.E. KARNIADAKIS"
            ],
            "title": "Adaptive activation functions accelerate convergence in deep and physics-informed neural networks",
            "venue": "J. Comput",
            "year": 2020
        },
        {
            "authors": [
                "J. JIM\u00c9NEZ",
                "P. MOIN"
            ],
            "title": "The minimal flow unit in near-wall turbulence",
            "venue": "J. Fluid Mech",
            "year": 1991
        },
        {
            "authors": [
                "W.J. JUNG",
                "N. MANGIAVACCHI",
                "R. AKHAVAN"
            ],
            "title": "Suppression of turbulence in wall-bounded flows by high-frequency spanwise oscillations",
            "venue": "Phys. Fluids A",
            "year": 1992
        },
        {
            "authors": [
                "T. KAJISHIMA",
                "K. TAIRA"
            ],
            "title": "Computational Fluid Dynamics: Incompressible Turbulent Flows",
            "year": 2016
        },
        {
            "authors": [
                "Y. KAMETANI",
                "K. FUKAGATA"
            ],
            "title": "Direct numerical simulation of spatially developing turbulent boundary layers with uniform blowing or suction",
            "venue": "J. Fluid Mech",
            "year": 2011
        },
        {
            "authors": [
                "J. KIM",
                "P. MOIN"
            ],
            "title": "Application of a fractional-step method to incompressible Navier\u2013Stokes equations",
            "venue": "J. Comput. Phys",
            "year": 1985
        },
        {
            "authors": [
                "K.C. KIM",
                "R.J. ADRIAN"
            ],
            "title": "Very large-scale motion in the outer layer",
            "venue": "Phys. Fluids",
            "year": 1999
        },
        {
            "authors": [
                "J. KOBER",
                "J.A. BAGNELL",
                "J. PETERS"
            ],
            "title": "Reinforcement learning in robotics: a survey",
            "venue": "Intl J. Robot. Res",
            "year": 2013
        },
        {
            "authors": [
                "H. KOIZUMI",
                "S. TSUTSUMI",
                "E. SHIMA"
            ],
            "title": "Feedback control of Karman vortex shedding from a cylinder using deep reinforcement learning",
            "year": 2018
        },
        {
            "authors": [
                "S. LAIZET",
                "E. LAMBALLAIS"
            ],
            "title": "High-order compact schemes for incompressible flows: a simple and efficient method with quasi-spectral accuracy",
            "venue": "J. Comput. Phys",
            "year": 2009
        },
        {
            "authors": [
                "S. LAIZET",
                "N. LI"
            ],
            "title": "Incompact3d: a powerful tool to tackle turbulence problems with up to O(105) computational cores",
            "venue": "Intl J. Numer. Meth. Fluids",
            "year": 2011
        },
        {
            "authors": [
                "C. LEE",
                "J. KIM",
                "D. BABCOCK",
                "R. GOODMAN"
            ],
            "title": "Application of neural networks to turbulence control for drag reduction",
            "venue": "Phys. Fluids",
            "year": 1997
        },
        {
            "authors": [
                "LEE C",
                "KIM J",
                "CHOI H"
            ],
            "title": "Suboptimal control of turbulent channel flow for drag reduction",
            "venue": "J. Fluid Mech",
            "year": 1998
        },
        {
            "authors": [
                "X.Y. LEE",
                "A. BALU",
                "D. STOECKLEIN",
                "B. GANAPATHYSUBRAMANIAN",
                "S. SARKAR"
            ],
            "title": "A case study of deep reinforcement learning for engineering design: application to microfluidic devices for flow sculpting",
            "venue": "J. Mech. Des",
            "year": 2021
        },
        {
            "authors": [
                "R. LI",
                "Y. ZHANG",
                "H. CHEN"
            ],
            "title": "Learning the aerodynamic design of supercritical airfoils through deep reinforcement learning",
            "venue": "AIAA J",
            "year": 2021
        },
        {
            "authors": [
                "B.K. LIEU",
                "R. MARREF",
                "M.R. JOVANOVI\u0106"
            ],
            "title": "Controlling the onset of turbulence by streamwise travelling waves. Part 2. Direct numerical simulation",
            "venue": "J. Fluid Mech",
            "year": 2010
        },
        {
            "authors": [
                "T.P. LILLICRAP",
                "J.J. HUNT",
                "A. PRITZEL",
                "N. HEESS",
                "T. EREZ",
                "Y. TASSA",
                "D. SILVER",
                "D. WIERSTRA"
            ],
            "title": "Continuous control with deep reinforcement learning",
            "venue": "In 4th International Conference on Learning Representations,",
            "year": 2016
        },
        {
            "authors": [
                "H. MAMORI",
                "K. IWAMOTO",
                "A. MURATA"
            ],
            "title": "Effect of the parameters of traveling waves created by blowing and suction on the relaminarization phenomena in fully developed turbulent channel flow",
            "venue": "Phys. Fluids",
            "year": 2014
        },
        {
            "authors": [
                "T. MIN",
                "S.M. KANG",
                "J.L. SPEYER",
                "J. KIM"
            ],
            "title": "Sustained sub-laminar drag in a fully developed channel flow",
            "venue": "J. Fluid Mech",
            "year": 2006
        },
        {
            "authors": [
                "G. NOVATI",
                "S. VERMA",
                "D. ALEXEEV",
                "D. ROSSINELLI",
                "W.M. VAN REES",
                "P. KOUMOUTSAKOS"
            ],
            "title": "Synchronisation through learning for two self-propelled swimmers",
            "venue": "Bioinspir. Biomim",
            "year": 2018
        },
        {
            "authors": [
                "R. PARIS",
                "S. BENEDDINE",
                "J. DANDOIS"
            ],
            "title": "Robust flow control and optimal sensor placement using deep reinforcement learning",
            "venue": "J. Fluid Mech",
            "year": 2021
        },
        {
            "authors": [
                "J. PARK",
                "H. CHOI"
            ],
            "title": "Machine-learning-based feedback control for drag reduction in a turbulent channel flow",
            "venue": "J. Fluid Mech",
            "year": 2020
        },
        {
            "authors": [
                "S. QIN",
                "S. WANG",
                "L. WANG",
                "C. WANG",
                "G. SUN",
                "Y. ZHONG"
            ],
            "title": "Multi-objective optimization of cascade blade profile based on reinforcement learning",
            "venue": "Appl. Sci",
            "year": 2021
        },
        {
            "authors": [
                "M. QUADRIO",
                "P. RICCO"
            ],
            "title": "Critical assessment of turbulent drag reduction through spanwise wall oscillations",
            "venue": "J. Fluid Mech",
            "year": 2004
        },
        {
            "authors": [
                "J. RABAULT",
                "M. KUCHTA",
                "A. JENSEN",
                "U. REGLADE",
                "N. CERARDI"
            ],
            "title": "Artificial neural networks trained through deep reinforcement learning discover control strategies for active flow control",
            "venue": "J. Fluid Mech",
            "year": 2019
        },
        {
            "authors": [
                "J. RABAULT",
                "A. KUHNLE"
            ],
            "title": "Accelerating deep reinforcement learning strategies of flow control through a multi-environment approach",
            "venue": "Phys. Fluids",
            "year": 2019
        },
        {
            "authors": [
                "J. RABAULT",
                "F. REN",
                "W. ZHANG",
                "H. TANG",
                "H. XU"
            ],
            "title": "Deep reinforcement learning in fluid mechanics: a promising method for both active flow control and shape optimization",
            "venue": "J. Hydrodyn",
            "year": 2020
        },
        {
            "authors": [
                "F. REN",
                "J. RABAULT",
                "H. TANG"
            ],
            "title": "Applying deep reinforcement learning to active flow control in weakly turbulent conditions",
            "venue": "Phys. Fluids",
            "year": 2021
        },
        {
            "authors": [
                "D SILVER"
            ],
            "title": "Mastering the game of Go with deep neural networks and tree search",
            "venue": "Nature",
            "year": 2016
        },
        {
            "authors": [
                "Y. SUMITANI",
                "N. KASAGI"
            ],
            "title": "Direct numerical simulation of turbulent transport with uniform wall injection and suction",
            "venue": "AIAA J",
            "year": 1995
        },
        {
            "authors": [
                "R.S. SUTTON",
                "A.G. BARTO"
            ],
            "title": "Reinforcement Learning: An Introduction",
            "year": 2018
        },
        {
            "authors": [
                "T. SUZUKI",
                "Y. HASEGAWA"
            ],
            "title": "Estimation of turbulent channel flow at Re\u03c4 = 100 based on the wall measurement using a simple sequential approach",
            "venue": "J. Fluid Mech",
            "year": 2017
        },
        {
            "authors": [
                "H. TANG",
                "J. RABAULT",
                "A. KUHNLE",
                "Y. WANG",
                "T. WANG"
            ],
            "title": "Robust active flow control over a range of Reynolds numbers using an artificial neural network trained through deep reinforcement learning",
            "venue": "Phys. Fluids",
            "year": 2020
        },
        {
            "authors": [
                "M. TOKAREV",
                "E. PALKIN",
                "R. MULLYADZHANOV"
            ],
            "title": "Deep reinforcement learning control of cylinder flow using rotary oscillations at low",
            "venue": "Reynolds number. Energies",
            "year": 2020
        },
        {
            "authors": [
                "S. VERMA",
                "G. NOVATI",
                "P. KOUMOUTSAKOS"
            ],
            "title": "Efficient collective swimming by harnessing vortices through deep reinforcement learning",
            "venue": "Proc. Natl Acad. Sci. USA",
            "year": 2018
        },
        {
            "authors": [
                "J. VIQUERAT",
                "J. RABAULT",
                "A. KUHNLE",
                "H. GHRAIEB",
                "A. LARCHER",
                "E. HACHEM"
            ],
            "title": "Direct shape optimization through deep reinforcement learning",
            "venue": "J. Comput",
            "year": 2021
        },
        {
            "authors": [
                "Q. WANG",
                "R. HU",
                "P. BLONIGAN"
            ],
            "title": "Least squares shadowing sensitivity analysis of chaotic limit cycle oscillations",
            "venue": "J. Comput",
            "year": 2014
        },
        {
            "authors": [
                "H. XU",
                "W. ZHANG",
                "J. DENG",
                "J. RABAULT"
            ],
            "title": "Active flow control with rotating cylinders by an artificial neural network trained by deep reinforcement learning",
            "venue": "J. Hydrodyn",
            "year": 2020
        },
        {
            "authors": [
                "A. YAMAMOTO",
                "Y. HASEGAWA",
                "N. KASAGI"
            ],
            "title": "Optimal control of dissimilar heat and momentum transfer in a fully developed turbulent channel flow",
            "venue": "J. Fluid Mech",
            "year": 2013
        },
        {
            "authors": [
                "L. YAN",
                "X. CHANG",
                "R. TIAN",
                "N. WANG",
                "L. ZHANG",
                "W. LIU"
            ],
            "title": "A numerical simulation method for bionic fish self-propelled swimming under control based on deep reinforcement learning",
            "venue": "Proc. Inst. Mech. Engng C",
            "year": 2020
        },
        {
            "authors": [
                "X. YAN",
                "J. ZHU",
                "M. KUANG",
                "X. WANG"
            ],
            "title": "Aerodynamic shape optimization using a novel optimizer based on machine learning techniques",
            "venue": "Aerosp. Sci. Technol",
            "year": 2019
        },
        {
            "authors": [
                "Y. ZHU",
                "F.B. TIAN",
                "J. YOUNG",
                "J.C. LIAO",
                "J. LAI"
            ],
            "title": "A numerical study of fish adaption behaviors in complex environments with a deep reinforcement learning and immersed boundary-lattice Boltzmann method",
            "venue": "Sci. Rep",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "J. Fluid Mech. (2023), vol. 960, A30, doi:10.1017/jfm.2023.147\nReinforcement learning of control strategies for reducing skin friction drag in a fully developed turbulent channel flow\nTakahiro Sonoda1, Zhuchen Liu1, Toshitaka Itoh1 and Yosuke Hasegawa1,\u2020 1Institute of Industrial Science, The University of Tokyo, Komaba 4-6-1, Tokyo 153-8505, Japan\n(Received 30 March 2022; revised 3 January 2023; accepted 12 February 2023)\nReinforcement learning is applied to the development of control strategies in order to reduce skin friction drag in a fully developed turbulent channel flow at a low Reynolds number. Motivated by the so-called opposition control (Choi et al., J. Fluid Mech., vol. 253, 1993, pp. 509\u2013543), in which a control input is applied so as to cancel the wall-normal velocity fluctuation on a detection plane at a certain distance from the wall, we consider wall blowing and suction as a control input, and its spatial distribution is determined by the instantaneous streamwise and wall-normal velocity fluctuations at distance 15 wall units above the wall. A deep neural network is used to express the nonlinear relationship between the sensing information and the control input, and it is trained so as to maximize the expected long-term reward, i.e. drag reduction. When only the wall-normal velocity fluctuation is measured and a linear network is used, the present framework reproduces successfully the optimal linear weight for the opposition control reported in a previous study (Chung & Talha, Phys. Fluids, vol. 23, 2011, 025102). In contrast, when a nonlinear network is used, more complex control strategies based on the instantaneous streamwise and wall-normal velocity fluctuations are obtained. Specifically, the obtained control strategies switch abruptly between strong wall blowing and suction for downwelling of a high-speed fluid towards the wall and upwelling of a low-speed fluid away from the wall, respectively. Extracting key features from the obtained policies allows us to develop novel control strategies leading to drag reduction rates as high as 37 %, which is higher than the 23 % achieved by the conventional opposition control at the same Reynolds number. Finding such an effective and nonlinear control policy is quite difficult by relying solely on human insights. The present results indicate that reinforcement learning can be a novel framework for the development of effective control strategies through systematic learning based on a large number of trials.\nKey words: boundary layer control, drag reduction, machine learning\n\u2020 Email address for correspondence: ysk@iis.u-tokyo.ac.jp\n\u00a9 The Author(s), 2023. Published by Cambridge University Press. This is an Open Access article, distributed under the terms of the Creative Commons Attribution licence (https://creativecommons. org/licenses/by/4.0), which permits unrestricted re-use, distribution, and reproduction in any medium, provided the original work is properly cited. 960 A30-1\nht tp\ns: //\ndo i.o\nrg /1\n0. 10\n17 /jf\nm .2\n02 3.\n14 7\nPu bl\nis he\nd on\nlin e\nby C\nam br\nid ge\nU ni\nve rs\nity P\nre ss"
        },
        {
            "heading": "1. Introduction",
            "text": "Turbulent flows are ubiquitous in our daily life and determine the performances and the energy efficiencies of various thermo-fluids devices (Brunton & Noack 2015). In most engineering flows, turbulence is bounded by a solid surface, and their interaction plays a crucial role in generation and maintenance of near-wall turbulence, and associated momentum and heat transport between fluid and solid. Even over simple geometries such as a smooth flat wall, however, turbulence exhibits complex behaviour due to its nonlinear and multiscale nature, so that prediction and control of turbulent flow remain challenging.\nIn this study, we consider the control of a fully developed turbulent channel flow, which is one of the canonical flow configurations. Since near-wall turbulence is responsible for the increase in wall skin friction, a tremendous amount of effort has been devoted to reducing the skin friction drag. In general, flow control strategies can be categorized into passive and active schemes (Gad-el Hak 1996). The passive scheme does not require power input for control, and its typical example is a riblet surface (Dean & Bhushan 2010). In contrast, the active scheme requires additional power input for control, and it can be further classified into predetermined and feedback controls. The former applies a control input with a predetermined spatio-temporal distribution regardless of an instantaneous flow state. This makes a control system simple since no sensing of a flow field is required. Despite its simplicity, it is known that the predetermined control achieves relatively high drag reduction rates, and various control modes, such as spanwise wall oscillation (Jung, Mangiavacchi & Akhavan 1992; Quadrio & Ricco 2004), streamwise travelling wave of wall blowing and suction (Min et al. 2006; Lieu, Marref & Jovanovic\u0301 2010; Mamori, Iwamoto & Murata 2014), and uniform wall blowing (Sumitani & Kasagi 1995; Kametani & Fukagata 2011), have been proposed.\nIn contrast, the feedback control determines a control input based on a sensor signal obtained from an instantaneous flow field, therefore it enables a more flexible control. Meanwhile, due to the large degrees of freedom of the flow state and also the control input, optimizing a feedback control law is quite challenging. Therefore, existing control strategies have often been developed based on researchers\u2019 physical insights. A typical example of a feedback control is the so-called opposition control (Choi, Moin & Kim 1994; Hammond, Bewley & Moin 1998; Chung & Talha 2011), where local wall blowing and suction is applied so as to cancel the wall-normal velocity fluctuation at a certain height from the wall. The sensing plane is called a detection plane, and its optimal height has been reported as y+ = 15 in a wall unit (Hammond et al. 1998). The relationship between the wall-normal velocity on the detection plane and the control input has been assumed commonly to be linear a priori, and its optimal weight coefficient was found to be approximately unity (Choi et al. 1994; Chung & Talha 2011). It should be noted that optimization of these parameters in the control algorithm has mostly been done through trial and error, and such an approach is quite inefficient even for a simple control algorithm where the relationship between the sensor signal and the control input is assumed to be linear.\nThere also exists another approach to develop efficient feedback control laws. Optimal control theory is a powerful tool to optimize a control input with large degrees of freedom by explicitly leveraging mathematical models of a flow system such as Navier\u2013Stokes equations and mass conservation. Specifically, the spatio-temporal distribution of a control input is determined so as to minimize a prescribed cost functional. The cost functional can be defined within a certain time horizon, so that the future flow dynamics is taken into consideration in the optimization procedures. Optimal control theory was applied\n960 A30-2\nht tp\ns: //\ndo i.o\nrg /1\n0. 10\n17 /jf\nm .2\n02 3.\n14 7\nPu bl\nis he\nd on\nlin e\nby C\nam br\nid ge\nU ni\nve rs\nity P\nre ss\nsuccessfully to a low-Reynolds-number turbulent channel flow by Bewley, Moin & Temam (2001), and it was demonstrated that the flow can be relaminarized. One of the major drawbacks in optimal control theory is that it requires expensive iterations of forward and adjoint simulations within the time horizon in order to determine the optimal control input. By assuming a vanishingly small time horizon, suboptimal control theory (Lee, Kim & Choi 1998; Hasegawa & Kasagi 2011) provides an analytical expression of the control input without solving adjoint equations, but its control performance is not as high as that achieved by optimal control theory, suggesting the importance of considering future flow dynamics in determining the control input. Another issue is that there exists a severe limitation in the length of the time horizon employed in optimal control theory due to inherent instability of adjoint equations (Wang, Hu & Blonigan 2014). Specifically, the maximum time horizon is approximately 100 in a wall unit (Bewley et al. 2001; Yamamoto, Hasegawa & Kasagi 2013), which is quite short considering the time scale of wall turbulence. In particular, this limitation becomes critical at higher Reynolds numbers where large-scale structures play important roles in the dynamics of wall turbulence (Kim & Adrian 1999).\nIn recent years, much attention has been paid to reinforcement learning as a new framework for developing efficient control strategies in various fields, such as robot control (Kober, Bagnell & Peters 2013) and games (Silver et al. 2016). In reinforcement learning, an agent decides its action based on a current state. As a consequence, the agent receives a reward from an environment. By repeating this interaction with the environment, the agent learns an efficient policy, which dictates the relationship between the state and the action, so as to maximize the total expected future reward. In this way, the policy can be optimized from a long-term perspective. In addition, by combining reinforcement learning and deep neural networks, deep reinforcement learning (Sutton & Barto 2018) can deal naturally with a complex nonlinear relationship between sensor signals and a control input. We note that there already exist several studies applying machine learning techniques for reducing skin friction drag in a turbulent channel flow. For example, Lee et al. (1997) first applied neural networks to design a controller to suppress a certain physical quantity of interest in the short term, i.e. after one time step. More recently, Han & Huang (2020) and Park & Choi (2020) applied convolutional neural networks to predict the wall-normal velocity fluctuation at the detection plane to reproduce the opposition control (Choi et al. 1994) based on wall measurements only. However, the reinforcement learning distinguishes itself from those other machine learning techniques in the sense that it provides a framework to develop novel control strategies that are effective in the long term. It is therefore no surprise that reinforcement learning is gaining more and more attention for its applications to fluid mechanics. Recent attempts and achievements are summarized in several comprehensive review articles (Rabault et al. 2020; Garnier et al. 2021).\nPrevious studies cover a variety of purposes, such as drag reduction (Koizumi, Tsutsumi & Shima 2018; Rabault et al. 2019; Rabault & Kuhnle 2019; Fan et al. 2020; Tang et al. 2020; Tokarev, Palkin & Mullyadzhanov 2020; Xu et al. 2020; Ghraieb et al. 2021; Paris, Beneddine & Dandois 2021; Ren, Rabault & Tang 2021), control of heat transfer (Beintema et al. 2020; Hachem et al. 2021), optimization of microfluidics (Dressler et al. 2018; Lee et al. 2021), optimization of artificial swimmers (Novati et al. 2018; Verma, Novati & Koumoutsakos 2018; Yan et al. 2020; Zhu et al. 2021) and shape optimization (Yan et al. 2019; Li, Zhang & Chen 2021; Viquerat et al. 2021; Qin et al. 2021). In terms of drag reduction considered in the present study, Rabault et al. (2019) considered control of a two-dimensional flow around a cylinder at a low Reynolds number. They assumed\n960 A30-3\nht tp\ns: //\ndo i.o\nrg /1\n0. 10\n17 /jf\nm .2\n02 3.\n14 7\nPu bl\nis he\nd on\nlin e\nby C\nam br\nid ge\nU ni\nve rs\nity P\nre ss\nwall blowing and suction from two local slits over the cylinder, and demonstrated that a control policy obtained by reinforcement learning achieves 8 % drag reduction. Tang et al. (2020) discussed Reynolds number effects on the control performance at different Reynolds numbers, namely 100, 200, 300 and 400, and also showed the possibility of applying the obtained control policy to unseen Reynolds numbers. Paris et al. (2021) optimized the arrangement of sensors employed for controlling two-dimensional laminar flow behind a cylinder. Their sparsity-seeking algorithm allows us to reduce a number of sensors down to five without sacrificing the control performance. Ghraieb et al. (2021) proposed a degenerated version of reinforcement learning so that it does not require the information of the state as an input. This allows us to find effective open-loop control policies for both laminar and turbulent flows around an aerofoil and a cylinder. Fan et al. (2020) demonstrated experimentally that reinforcement learning can find effective rotation modes of small cylinders around a primal stationary cylinder for its drag reduction. As shown above, most previous studies consider relatively simple flow fields such as a two-dimensional laminar flow around a blunt object. Also, their control inputs are wall blowing and suction from slots at two or four prescribed locations, or rotation/vibrations of one or two cylinders, so that the degrees of freedom for a control input are commonly limited. Therefore, it remains an open question whether reinforcement learning can be applicable to turbulence control with a control input having large degrees of freedom.\nTo the best of the authors\u2019 knowledge, this is the first study applying reinforcement learning to control of a fully developed turbulent channel flow for reducing skin friction drag. As is often the case with wall turbulence control, we consider wall blowing and suction as a control input, which is defined at each computational grid point on the wall. This makes the degrees of freedom of the control input quite large (O(104)), compared with those assumed in the existing applications of reinforcement learning. This paper is organized as follows. After introducing our problem setting in \u00a7 2, we explain the framework of the present reinforcement learning in detail in \u00a7 3. Then we present new control policies obtained in the present study, and their control results in \u00a7 4. In \u00a7 5, we discuss further how the unique features of the present control policies lead to high control performances. Finally, we summarize the present study in \u00a7 6."
        },
        {
            "heading": "2. Problem setting",
            "text": "2.1. Governing equations and boundary conditions We consider a fully developed turbulent channel flow with wall blowing and suction as a control input, as shown in figure 1. The coordinate systems are set so that x, y and z correspond to the streamwise, wall-normal and spanwise directions, respectively. The corresponding velocity components are denoted by u, v and w. Time is expressed by t. The origin of the coordinates is placed on the bottom wall as shown in figure 1. Unless stated otherwise, we consider only the bottom half of the channel due to the symmetry of the system. The governing equations of the fluid flow are the following incompressible Navier\u2013Stokes and continuity equations:\n\u2202ui \u2202t + uj \u2202ui \u2202xj = \u2212 \u2202p \u2202xi + 1 Re \u22022ui \u2202xj \u2202xj , (2.1)\n\u2202ui \u2202xi = 0, (2.2) where p is the static pressure. Throughout this paper, all variables without a superscript are non-dimensionalized by the channel half-width h\u2217 and the bulk mean velocity U\u2217b , while\n960 A30-4\nht tp\ns: //\ndo i.o\nrg /1\n0. 10\n17 /jf\nm .2\n02 3.\n14 7\nPu bl\nis he\nd on\nlin e\nby C\nam br\nid ge\nU ni\nve rs\nity P\nre ss\na variable with an asterisk indicates a dimensional value. A constant flow rate condition is imposed, so that the bulk Reynolds number is Reb \u2261 2U\u2217bh\u2217/\u03bd\u2217 = 4646.72, where \u03bd\u2217 is the kinematic viscosity of the fluid. The corresponding friction Reynolds number in the uncontrolled flow is Re\u03c4 \u2261 u\u2217\u03c4 h\u2217/\u03bd\u2217 \u2248 150. Here, the friction velocity is defined as u\u2217\u03c4 = \u221a \u03c4 \u2217w/\u03c1\u2217, where \u03c1\u2217 is the fluid density, and \u03c4 \u2217w is the space\u2013time average of the wall\nfriction. Periodic boundary conditions are imposed in the streamwise and spanwise directions. As for the wall-normal direction, we impose no-slip conditions for the tangential velocity components on the wall, while wall blowing and suction with zero-net-mass flux is applied as a control input: ui(x, 0, z, t) = \u03c6(x, z, t) \u03b4i2. (2.3) Here, \u03c6(x, z, t) indicates the space\u2013time distribution of wall blowing and suction at the bottom wall (y = 0), and \u03b4ij is the Kronecker delta. Wall blowing and suction is also imposed at the top wall, and its space\u2013time distribution is determined based on the same control policy as that used for the bottom wall, so that the resulting flow is always statistically symmetric with respect to the channel centre. The objective of the present study is to find an effective strategy to determine the distributions of \u03c6(x, z, t) for drag reduction.\nIn reinforcement learning, a control policy (control law) is learned on a trial-and-error basis requiring a large number of simulations. In order to reduce the computational cost for the training, we introduce the minimal channel (Jim\u00e9nez & Moin 1991), which has the minimum domain size to maintain turbulence. Accordingly, the streamwise, wall-normal and spanwise domain sizes are set to be (Lx, Ly, Lz) = (2.67, 2.0, 0.8). Once a control policy is obtained in the minimal channel, it is assessed in a larger domain with (Lx, Ly, Lz) = (2.5\u03c0, 2.0, \u03c0). Hereafter, the latter larger domain is referred to as a full channel.\n2.2. Numerical methodologies The governing equations (2.1) and (2.2) are discretized in space by a pseudo-spectral method (Boyd 2001). Specifically, Fourier expansions are adopted in the streamwise and spanwise directions, while Chebyshev polynomials are used in the wall-normal direction. For the minimal channel, the number of modes used in each direction is (Nx, Ny, Nz) = (16, 65, 16), whilst they are set to be (Nx, Ny, Nz) = (64, 65, 64) for the full channel.\n960 A30-5\nht tp\ns: //\ndo i.o\nrg /1\n0. 10\n17 /jf\nm .2\n02 3.\n14 7\nPu bl\nis he\nd on\nlin e\nby C\nam br\nid ge\nU ni\nve rs\nity P\nre ss\nThe 3/2 rule is applied to eliminate aliasing errors, and therefore the number of grid points in the physical space is 1.5 times the number of modes employed in each direction.\nAs for the time advancement, a fractional step method (Kim & Moin 1985) is applied to decouple the pressure term from (2.1). The second-order Adams\u2013Bashforth method is used for the advection term. For viscous terms, we employ the Euler implicit method, since the Crank\u2013Nicolson method sometimes leads to numerical instability due to its slightly narrower stability region (Kajishima & Taira 2016). This is reasonable considering that the reinforcement learning is a trial-and-error process, which can lead to unstable control policies, especially during the early stage of learning. Hence it is more advantageous to prioritize stability over accuracy during the training, and then to verify the resulting control performances by the obtained control policies with higher-order schemes afterwards. Indeed, in the present study, a time-advancement scheme hardly affects the evaluation of the skin friction drag for a given control policy, as shown in Appendix D, since we commonly use a relatively small time step in both training and evaluation phases.\nSpecifically, the time step is set to be t+ = 0.06 and 0.03 for the minimal and full channels, respectively. The superscript + denotes a quantity scaled by the viscous scale in the uncontrolled flow throughout this paper. The above setting of the time step ensures that the Courant number is less than unity even with wall blowing and suction. The present numerical scheme has already been validated and applied successfully to control and estimation problems in previous studies (Yamamoto et al. 2013; Suzuki & Hasegawa 2017).\n3. Reinforcement learning\n3.1. Outline Reinforcement learning is a problem where an agent (learner) learns the optimal policy that maximizes a long-term total reward through trial and error. Specifically, an agent receives a state s from an environment (control target) and decides an action a based on a policy \u03bc(a|s). By executing the action against the environment, the state changes from s to s\u2032, and a resulting instantaneous reward r is obtained. Then s\u2032 and r are fed back to the agent and the policy is updated. With the new policy, the next action a\u2032 under the new state s\u2032 is determined. By repeating the above interaction with the environment, the agent learns the optimal policy. If the next state s\u2032 and the instantaneous reward r depend only on the previous state s and the action a, then this process is called the Markov decision process, which is the basis of the reinforcement learning (Sutton & Barto 2018).\nIn the current flow control problem, the environment is a fully developed turbulent channel flow, whereas the state is sensing a signal from the instantaneous flow field, and the action corresponds to the control input, i.e. wall blowing and suction. The instantaneous reward r(t) is the friction coefficient Cf (t) with a negative sign, since the reward is defined to be maximized, while the wall friction should be minimized in the present study. Specifically, it is defined as\nr(t) = \u2212Cf (t), (3.1) where\nCf (t) = \u03c4w1 2\u03c1U 2 b . (3.2)\nHere, \u03c4w is the spatial mean of the wall shear stress over the entire wall, and therefore both r and Cf are functions of time as written explicitly in (3.1) and (3.2). It should be noted\n960 A30-6\nht tp\ns: //\ndo i.o\nrg /1\n0. 10\n17 /jf\nm .2\n02 3.\n14 7\nPu bl\nis he\nd on\nlin e\nby C\nam br\nid ge\nU ni\nve rs\nity P\nre ss\nthat there is no unique way to define the reward. For example, the inverse of the friction coefficient, i.e. r(t) = 1/Cf (t), could be another choice. The major difference from the present choice, (3.1), is that the reward increases more rapidly when Cf becomes smaller. It was confirmed, however, that there is no significant difference in the final outcome. More detailed comparisons in the resulting control policies and their performances between the two rewards can be found in Appendix A.\nOur objective is to find an efficient control policy that describes the relationship between the flow state and the action for maximizing the future total reward. In the present study, we use the deep deterministic policy gradient (DDPG) algorithm (Lillicrap et al. 2016), which is a framework to optimize a deterministic policy. Specifically, this algorithm consists of two neural networks, called an actor and a critic, as shown in figure 2. The input of the actor is the state s, while its output is the action a. Therefore, the actor dictates a control policy \u03bc(a|s), and it has to be optimized. For this purpose, another network, i.e. a critic, is introduced. The inputs of the critic are the current state s and the action a. The critic outputs the estimation of an action value function Q\u03bc(s, a), i.e. the expected total future reward when a certain action a is taken under a certain state s. It should be noted that during the training, although the instantaneous reward, i.e. instantaneous wall friction, is obtained at every time step from simulation, we generally do not know Q\u03bc(s, a), since it is determined by the equilibrium state after the current control policy \u03bc is applied continuously to the flow field. The role of the critic network is to estimate Q\u03bc(s, a) from past states, actions and resulting rewards.\nAs for training the networks, the actor is first trained so as to maximize the expected total reward Q\u03bc(s, a) while fixing the critic network. Then the critic is optimized so that the resulting Q\u03bc(s, a) minimizes the following squared residual of the Bellman equation:\nLcritic = { r(s, a) + \u03b3 Q\u03bc(s\u2032, a\u2032) \u2212 Q\u03bc(s, a)}2 . (3.3)\nAs shown in figure 2, the two networks are coupled and trained alternatively, so that both of them will be optimized after a number of trials. Here, \u03b3 is the time discount rate. If it is set to be small, then the agent searches for a control policy yielding a short-term benefit. In contrast, when \u03b3 approaches unity, the policy is optimized from a longer-term perspective. Meanwhile, it is also known that when it is set too large, the agent tends to select no action to avoid failure, i.e. drag increase. In this study, \u03b3 is set to be 0.99, which is the same as the value used commonly in previous studies (Fan et al. 2020; Paris et al. 2021).\n960 A30-7\nht tp\ns: //\ndo i.o\nrg /1\n0. 10\n17 /jf\nm .2\n02 3.\n14 7\nPu bl\nis he\nd on\nlin e\nby C\nam br\nid ge\nU ni\nve rs\nity P\nre ss\n3.2. State, action and network setting Ideally, the velocity field throughout the entire domain should be defined as the state, and wall blowing and suction imposed at each grid point should be considered as the action. In such a case, however, the degrees of freedom of the state and the action become quite large, so that network training will be difficult. Meanwhile, considering the homogeneity of the current flow configuration in the streamwise and spanwise directions, wall blowing and suction could be decided based on the local information of the flow field. For example, the opposition control (Choi et al. 1994), which is one of the well-known control strategies, applies local wall blowing and suction so as to cancel the wall-normal velocity fluctuation above the wall. Belus et al. (2019) also introduce the idea of the translational invariance to the control of a one-dimensional falling liquid film based on reinforcement learning. They demonstrate that exploiting the locality of the flow system effectively accelerates network training. Hence, in the present study, we also assume that a local control input can be decided based solely on the velocity information above the location where the control is applied. Specifically, we set the detection plane height to y+d = 15, which is found to be optimal for the opposition control in previous studies (Hammond et al. 1998; Chung & Talha 2011). We note that we have conducted additional configuration where the state is defined as the velocity information at multiple locations above the wall. It was found that the resultant control performance is not improved significantly from that obtained in the present configuration with a single sensing location, and the largest weight was confirmed at approximately y+ = 15 (see Appendix B). Hence the present study focuses on a control with the single detection plane located at y+d = 15 from the wall.\nAs a first step, we consider the simplest linear actor, defined as\na \u2261 \u03c6(x, z, t) = \u03b1 v\u2032(x, yd, z, t) + \u03b2 + N. (3.4) Here, the prime indicates the deviation from the spatial mean, so that v\u2032 = v \u2212 v\u0304. Throughout this study, the velocity fluctuation used in the state is defined as the deviation from its spatial mean in the x and z directions at each instant, and \u03b1 and \u03b2 are constants to be optimized. In order to enhance the robustness of the training, a random noise N with zero mean and standard deviation 0.1 in a wall unit is added. Throughout the present study, the same magnitude of N is used in all the cases. In the present flow configuration, where periodicity is imposed in the streamwise and spanwise directions, v\u0304 is null, and therefore v\u2032 = v. We also note that the same values of \u03b1 and \u03b2 are used for all locations on the wall. In addition, a net mass flux from each wall is assumed to be zero, so that \u03b2 is zero. Eventually, the above problem reduces to optimizing the single parameter \u03b1 in the actor. This configuration will be referred to as Case Li00, as shown in table 1. For this control algorithm (3.4), the previous study (Chung & Talha 2011) reported that the optimal value of \u03b1 is approximately unity. The purpose of revisiting this configuration is to assess whether the present reinforcement learning can reproduce the opposition control, find the optimal value of \u03b1, and achieve a drag reduction similar to that reported in the previous study. We also note that the output of the actor is clipped to \u22121 \u2264 \u03c6+ \u2264 1 before applying it to the flow simulation in order to avoid a large magnitude of the control input.\nConsidering that the skin friction drag is related directly to the Reynolds shear stress \u2212u\u2032v\u2032 (Fukagata, Iwamoto & Kasagi 2002), the streamwise velocity fluctuation u\u2032 would also be worth considering in addition to v\u2032. Hence, for the rest of the cases shown in table 1, both u\u2032 and v\u2032 at y+d = 15 are considered as the state. The actor network has 1 layer and 8 nodes, as shown in figure 3(a). We have changed the size of the actor network and found that further increases in the numbers of layers and nodes do not improve the resultant\n960 A30-8\nht tp\ns: //\ndo i.o\nrg /1\n0. 10\n17 /jf\nm .2\n02 3.\n14 7\nPu bl\nis he\nd on\nlin e\nby C\nam br\nid ge\nU ni\nve rs\nity P\nre ss\ncontrol performance (see Appendix C). The mathematical expression of the present actor network is\na \u2261 \u03c6(x, z, t) = tanh [\u03c3 {u\u2032(x, yd, z)\u03b111 + v\u2032(x, yd, z)\u03b112 + \u03b21} \u00b7 \u03b12 + \u03b22] + N, (3.5) where \u03b111, \u03b112, \u03b21 and \u03b12 are vectors having the same dimension as the number of the nodes, while \u03b22 is a scalar quantity. As for the activation function \u03c3 , we consider rectified linear unit (ReLU), sigmoid, leaky ReLU and hyperbolic tangent, which are referred to respectively as Cases R18, S18, LR18 and T18 as listed in table 1. The last two digits in each case name represent the numbers of layers and nodes employed in the actor. We also note that a hyperbolic tangent is used for the activation function of the output layer in order to map the range of the control input into \u2016\u03c6+\u2016 < 1.0.\nThe network structure of the critic is shown schematically in figure 3(b). It consists of two layers with 8 and 16 nodes for the first and second layers for the state, and another one-layer network with 16 nodes for the action. Then the two networks are integrated by an additional two layers with 64 nodes, and the final output is the action value function Q\u03bc(s, a). ReLU is used for the activation function.\n960 A30-9\nht tp\ns: //\ndo i.o\nrg /1\n0. 10\n17 /jf\nm .2\n02 3.\n14 7\nPu bl\nis he\nd on\nlin e\nby C\nam br\nid ge\nU ni\nve rs\nity P\nre ss\nIn order to take into account the cost for applying the control, we extend the reward as\nr = \u2212Cf \u2212 d (\u03c6 +)2\n2 . (3.6)\nThe second term of (3.6) represents the cost of control, and d is a weight coefficient that determines the balance between the wall friction and the cost of applying the control. In the present study, d is changed systematically from 0 to 0.1, which cases are referred to as R18, R18D1, R18D2 and R18D3 (see table 1).\nWe note that the current reward ((3.1) or (3.6)) is defined based on the global quantities, which are averaged in the homogeneous directions x and z, whereas the control policy is defined locally as (3.5). Another option would be to define the reward locally as well. Belus et al. (2019) assessed carefully these two possibilities and concluded that to define both the control policy and the reward locally is more effective in training the network for their one-dimensional liquid film problem. In the present problem, however, we found that the training becomes unstable when the local reward is used. The reason for the instability is unclear, but we speculate as follows. In the case of wall turbulence, it is not difficult to achieve local drag reduction by applying strong wall blowing. However, it is highly possible that this will cause large drag increase afterwards (downstream). Therefore, using a local reward may not be effective for evaluating the global drag reduction effect in the present case. Consequently, the reward is defined globally throughout this study. It would also be interesting to include the spanwise velocity fluctuation w\u2032 to the state. However, we found that it does not contribute to further improvement of the resultant policy (not shown here). It is in contrast to Choi et al. (1994), where it is shown that the opposition control based on w\u2032 is most effective in reducing the skin friction drag. In their case, however, the control input is also a spanwise velocity on the wall, while the present study considers wall blowing and suction as a control input. Hence which quantities should be included in the state could depend on flow and control configurations.\n3.3. Learning procedures Figure 4 shows the general outline of the present learning procedures. The two networks, i.e. actor and critic, are trained in parallel with flow simulation within a fixed time interval, which is called an episode. In the present study, the episode duration is set to be T+ = 600, and the flow simulation is repeated within the same interval, i.e. t \u2208 [0, T]. In each episode, the flow simulation is started from the identical initial field at t = 0, which is a fully developed uncontrolled flow. For t > 0, the control input \u03c6 is applied from the two walls in accordance with the control policy \u03bc(a|s). We set the episode duration as T+ = 600, so that the period covers the entire process in which the initial uncontrolled flow transits to another fully developed flow with the applied control. If the episode length is too short, then the flow does not converge to a fully developed state, so that the obtained policy is effective for only the initial transient after the onset of the control. Meanwhile, if the episode length becomes longer, then the obtained policy is more biased to the fully developed state under the control, and therefore might not be effective for the initial transient. According to our experience, the episode duration should be determined so that it covers the entire procedures for the initial uncontrolled flow to converge to another fully developed state after the onset of a control. Of course, the transient period should generally depend on a control policy and also a flow condition, therefore the optimal episode duration has to be found by trial and error. The number of training episodes is set to be 100. We tested additional training with different initial conditions and also with\n960 A30-10\nht tp\ns: //\ndo i.o\nrg /1\n0. 10\n17 /jf\nm .2\n02 3.\n14 7\nPu bl\nis he\nd on\nlin e\nby C\nam br\nid ge\nU ni\nve rs\nity P\nre ss\ntwice the number of episodes in several cases, and confirmed that the resulting control policies presented in this study are hardly affected by them. We also note that in the present study, the control policy generally converges and exhibits similar features in the last 10\u201320 episodes within the total 100 episodes.\nWithin each episode, the agent interacts consecutively with the flow by applying a control, and receives the instantaneous reward (3.1) or (3.6). Based on each interaction, the networks of the actor and the critic are updated. The Adam optimizer is used for both the networks, whereas the learning rate is set to be 0.001 and 0.002 for the actor and the critic, respectively. The buffer size is set to be 5 000 000, while the batch size is 64. In the present study, the networks are trained every short interval t+update = 0.6. Accordingly, the control input is also recalculated from the updated policy at the same time interval. Within the time interval, the control input is interpolated linearly (see figure 4). Ideally, a smaller time interval is better, since there will be more chances to update the networks. Meanwhile, it is known that a short training interval often causes numerical instability (Rabault et al. 2019; Fan et al. 2020). Our preliminary simulation results indicate that t+update = 0.6 leads to the best control performance.\nThe detailed numerical conditions of the present flow simulations, and also the network configurations used in the present reinforcement learning, are summarized in tables 2 and 3, respectively. The wall clock time needed for the training of 100 episodes, i.e. for running direct numerical simulations of 100 cases within t+ = 600 in the minimal channel, with a single core of Intel Xeon Gold 6132 (2.6 GHz) is approximately one day. Most computational costs are for performing flow simulations, and the other costs such as updating the network parameters are quite minor. We also note that in the present study, the training of the networks is always conducted in the minimal channel, so that we do not apply transfer learning, where the network is first trained in the minimal channel or in the fully channel with a coarser mesh, and then fine tuning is performed in the full channel\n960 A30-11\nht tp\ns: //\ndo i.o\nrg /1\n0. 10\n17 /jf\nm .2\n02 3.\n14 7\nPu bl\nis he\nd on\nlin e\nby C\nam br\nid ge\nU ni\nve rs\nity P\nre ss\nwith a higher resolution. A few trials suggest that training in the full-size channel makes the training procedures much slower and sometimes unsuccessful, whereas successful cases result in policies similar to those obtained in the minimal channel. Hence the present reinforcement learning can successfully extract essential features of the effective control policies from the minimal channel."
        },
        {
            "heading": "4. Results of reinforcement learning",
            "text": "4.1. Linear policy: revisiting the opposition control As a first step, we consider Case Li00, where only the wall-normal velocity fluctuation v\u2032 at the detection plane at y+d = 15 is used as a state, and the policy dictating the relationship between the state and the control input is linear, as described in (3.4). The time traces of the instantaneous Cf for different episodes are shown in figure 5. The line colour changes from green to blue as the number of episodes increases. For comparison, we also plot the temporal evolution of Cf for the uncontrolled and opposition control cases, with black and red lines, respectively. It can be seen that Cf is reduced successfully as the training proceeds, and eventually converges to a value similar to that obtained by the opposition control.\nIn figure 6, the time average \u3008Cf \u3009 of the instantaneous friction coefficient is shown, where the bracket \u3008\u00b7\u3009 indicates the time average within the final period 500 \u2264 t+ \u2264 600 in each episode. It can be seen that \u3008Cf \u3009 decreases for the first ten episodes, then converges to the value obtained by the opposition control. Figure 7 shows the policy, i.e. the control input versus the state, obtained at the end of each episode. The line colour changes from green to blue with increasing episode number. As described by (3.4), the relationship between the state v\u2032 and the control input \u03c6 is linear, and the maximum absolute value of \u03c6+ is clipped to unity. The red line corresponds to the case (\u03b1, \u03b2) = (\u22121.0, 0) in (3.4), which was found to be optimal for the opposition control in Chung & Talha (2011). It can\n960 A30-12\nht tp\ns: //\ndo i.o\nrg /1\n0. 10\n17 /jf\nm .2\n02 3.\n14 7\nPu bl\nis he\nd on\nlin e\nby C\nam br\nid ge\nU ni\nve rs\nity P\nre ss\nbe seen that the present policy reproduces the opposition control with the optimal values (\u03b1, \u03b2) = (\u22121.0, 0) quite well, while the present policy has a slightly steeper slope. This is probably attributed to the fact that the magnitude of \u03c6 is clipped in the present policy. From the above results, we validate that the present reinforcement learning successfully finds the optimal linear control policy that has been reported in the previous studies (Choi et al. 1994; Chung & Talha 2011).\n4.2. Nonlinear control policies In this subsection, we present the results obtained by nonlinear policies, where a hidden layer and a nonlinear activation function are added to the actor network as listed in table 1.\n960 A30-13\nht tp\ns: //\ndo i.o\nrg /1\n0. 10\n17 /jf\nm .2\n02 3.\n14 7\nPu bl\nis he\nd on\nlin e\nby C\nam br\nid ge\nU ni\nve rs\nity P\nre ss\n4.2.1. Obtained policies Figure 8 shows \u3008Cf \u3009 as a function of the episode number for Cases R18, S18, LR18 and T18 using different activation functions. For all the cases, \u3008Cf \u3009 reduces from the uncontrolled value with increasing episode number, and eventually converges to a value similar to or even smaller than that achieved by the opposition control. In particular, higher drag reduction rates than that of the opposition control can be confirmed clearly in Cases R18, LR18 and S18.\nThe policy obtained at the best episode where the maximum drag reduction rate is achieved in each case is shown in figures 9(b\u2013e), where the control input \u03c6 is plotted as a function of the state (u\u2032, v\u2032) at y+d = 15. Red and blue correspond to wall blowing and suction, respectively. For reference, we also plot the policy of the opposition control defined by (3.4) with (\u03b1, \u03b2) = (\u22121.0, 0) in figure 9(a). In this case, the control input 960 A30-14\nht tp\ns: //\ndo i.o\nrg /1\n0. 10\n17 /jf\nm .2\n02 3.\n14 7\nPu bl\nis he\nd on\nlin e\nby C\nam br\nid ge\nU ni\nve rs\nity P\nre ss\ndepends on only v\u2032, so that the colour contours are horizontal, and the control input \u03c6 depends linearly on the state v\u2032.\nIn contrast, the present nonlinear control policies shown in figures 9(b\u2013e) obviously depend on not only v\u2032, but also u\u2032. In addition, the control input switches rapidly between wall blowing and suction, depending drastically on the state, i.e. u\u2032 and v\u2032 at y+d = 15. Specifically, for Cases R18 and T18 shown in figures 9(b) and 9(e), respectively, the boundary between wall blowing and suction is inclined, so that wall blowing is applied when a high-speed fluid (u\u2032 > 0) approaches the wall (v\u2032 < 0), while wall suction is applied for upwelling (v\u2032 > 0) of low-momentum fluid (u\u2032 < 0). On the other hand, for Cases S18 and LR18 shown in figures 9(c) and 9(d), respectively, the boundary between wall blowing and suction is almost vertical, so that the control input depends mostly on the streamwise velocity fluctuation u\u2032 only. It should be emphasized that such complex nonlinear relationships between the state and the control input can be obtained first by introducing the neural network for the actor. The joint probability density function (p.d.f.) of u\u2032 and v\u2032 at y+d = 15 for the uncontrolled flow is plotted in figure 9( f ). It can be confirmed that the joint p.d.f. fits roughly in the plot range, and the boundaries between wall blowing and suction obtained in all the cases cross the central part of the joint p.d.f.\n960 A30-15\nht tp\ns: //\ndo i.o\nrg /1\n0. 10\n17 /jf\nm .2\n02 3.\n14 7\nPu bl\nis he\nd on\nlin e\nby C\nam br\nid ge\nU ni\nve rs\nity P\nre ss\n4.2.2. Control performances of obtained policies As mentioned in \u00a7 3.3, the present control policies are obtained through iterative training within the fixed episode period T+ = 600. In order to evaluate their control performances, here we note that the exploration noise N in the control policy (3.5) is introduced only during the training process, while it is hereafter turned off in the evaluation of the obtained policies. The time evolutions of the instantaneous Cf for the obtained policies are shown in figure 10. It can be seen that all the nonlinear policies obtained in the present study achieve drag reduction rates higher than that achieved by the opposition control. In particular, relaminarization can be confirmed in Cases R18 and T18. However, it should be noted that these policies may not always be optimal, since the control performance of each policy could depend on an initial condition, especially for the minimal channel considered here. Indeed, when we apply the present policies to another initial condition, the resultant drag reduction rates are commonly larger than that obtained by the opposition control, while the relaminarization is not always confirmed (not shown here). Due to the small domain size of the minimal channel, the turbulent flow becomes intermittent even in the uncontrolled flow (Jim\u00e9nez & Moin 1991), therefore it is difficult to distinguish whether relaminarization is caused by the applied control or the intermittency of the flow.\nIn order to evaluate the control performances of the obtained policies, we apply them to the full channel. The results are shown in figure 11. Although relaminarization is no longer achieved in the full channel, it can be seen that the present control policies still outperform the opposition control. We regard the initial period T+ = 3000 after the onset of the control shown in figure 11 as a transient period. Then the skin friction drag is further averaged over another period T+ = 4000 to obtain the value at an equilibrium state. Throughout this study, the same criterion is used for the evaluation of the skin friction drag in the full channel. The resulting drag reduction rates achieved by Cases R18, S18, LR18 and T18 are respectively 31 %, 35 %, 35 % and 27 %, while that of the opposition control remains 23 %.\nIn summary, it is demonstrated that the control policies obtained in the minimal channel still work in the full channel, and the present reinforcement learning successfully finds control policies more efficient than the existing opposition control. We also note that Cases R18, S18 and LR18 lead to similar drag reduction rates, so we do not make\n960 A30-16\nht tp\ns: //\ndo i.o\nrg /1\n0. 10\n17 /jf\nm .2\n02 3.\n14 7\nPu bl\nis he\nd on\nlin e\nby C\nam br\nid ge\nU ni\nve rs\nity P\nre ss\nparticular statements about which case is the best. Rather, we consider that the common features found from these obtained policies shown in figures 9(b\u2013e) \u2013 such as the strong dependency of the control input on both u\u2032 and v\u2032, and the rapid switch from wall blowing and suction \u2013 are more important. In the following, taking the optimal policy obtained in Case R18 as the default, we investigate further how each feature contributes to the resulting drag reduction effects.\nBefore closing this subsection, we also briefly address the generality of the present results. The nonlinear policies shown in figures 9(b\u2013e) commonly exhibit a rapid switch between wall blowing and suction, which may cause numerical oscillations and affect the resultant control performances, especially when a pseudo-spectral method is used. Therefore, we have also assessed the obtained policies in the same flow configurations with another code based on a finite difference method. We found that the resultant drag reduction rates are hardly affected by changing the numerical scheme. The detailed comparisons between the two numerical schemes are summarized in Appendix D.\n4.3. Effects of the control cost Here, we assess the impacts of the weight d for the control cost in the reward (3.6) by comparing Cases R18, R18D1, R18D2 and R18D3. Figure 12 shows the average drag reduction rates during the final 20 episodes after the flow reaches an equilibrium state for each case in the minimal channel. Specifically, 38 %, 38 %, 10 % and 7 % of drag reduction are obtained in Cases R18, R18D1, R18D2 and R18D3, respectively. We also note that these values change to 31 %, 23 %, 7 % and 14 % in the full channel, respectively. From the above results, it can be confirmed that the resulting drag reduction rate decreases with increasing the weight d for the control cost. This suggests that the control cost is properly reflected in the learning process of the present reinforcement learning.\nThe obtained policy at the final episode in each case is shown in figures 13(b\u2013e) together with that of the opposition control in figure 13(a). Specifically, in Case R18D1, where d is relatively small, the obtained policy shown in figure 13(c) is similar to that in Case R18 shown in figure 13(b), where no control cost is taken into account. It should also be noted, however, that the control input in Case R18D1 almost vanishes in the central region of figure 13(c). This indicates that when the cost for the control is relatively small, the\n960 A30-17\nht tp\ns: //\ndo i.o\nrg /1\n0. 10\n17 /jf\nm .2\n02 3.\n14 7\nPu bl\nis he\nd on\nlin e\nby C\nam br\nid ge\nU ni\nve rs\nity P\nre ss\nobtained policy avoids applying the control when the streamwise and wall-normal velocity fluctuations are relatively small. This is reasonable, since larger velocity fluctuations should have larger contributions to the momentum transfer in the near-wall region. When the cost for the control becomes larger in Cases R18D2 and R18D3, it can be seen that the obtained control policies shown in figures 13(d,e) tend to be similar to the opposition control shown in figure 13(a). From these results, the opposition control can be considered optimal when the weight for the cost of the control becomes large.\n960 A30-18\nht tp\ns: //\ndo i.o\nrg /1\n0. 10\n17 /jf\nm .2\n02 3.\n14 7\nPu bl\nis he\nd on\nlin e\nby C\nam br\nid ge\nU ni\nve rs\nity P\nre ss\nBefore closing this section, we summarize the power consumptions for applying the controls in Cases R18, R18D1, R18D2 and R18D3. The conservative estimate of the control power input for applying wall blowing and suction at the bottom wall can be given by the formula (Hasegawa & Kasagi 2011)\n\u03a0 = \u2329 S1pwvw + 12 S2v3w \u232a , (4.1)\nwhere the bracket indicates the average in the x and z directions, and also time. vw (= \u03c6) denotes the wall blowing and suction at the bottom wall, and pw is the wall pressure. Since the energy recovery from the flow is unrealistic, we introduce switching functions S1 and S2 to make sure that a local negative value is discarded. Namely, S1 = 1 when pwvw > 0, and S1 = 0 when pwvw \u2264 0. Similarly, S2 = 1 when vw > 0, while S2 = 0 when vw \u2264 0. As a result, it is found that the ratios of the control power input to the pumping power for driving the uncontrolled flow are 0.53 %, 0.30 %, 0.24 % and 0.14 % for Cases R18, R18D1, R18D2 and R18D3, respectively. These values are approximately two orders of magnitude smaller than the obtained drag reduction rates reported above. Hence the power consumptions for applying the present controls are negligible."
        },
        {
            "heading": "5. Feature analyses of obtained policies and control inputs",
            "text": "5.1. Effects of the rate of change from wall blowing to suction The unique features of the control policies obtained in the present reinforcement learning are the rapid switches between wall blowing and suction, and their dependency on the streamwise and wall-normal velocity fluctuations at the detection plane y+d = 15, as shown in figure 9. In this subsection, we clarify how each feature affects the resulting drag reduction rate, and leads to a drag reduction higher than that obtained by the opposition control. For this purpose, we extract their features, systematically change parameters characterizing them, and evaluate the resulting control performances. We note that all results presented in this subsection are obtained in the full channel.\n5.1.1. u\u2032-based control We first consider the policies obtained in Cases S18 and LR18 shown in figures 9(c,d). Both of these policies depend mainly on u\u2032. In order to clarify how the rate of change from wall blowing to suction in u\u2032-based control affects the control performance, we consider the policies\n\u03c6+ = \u23a7\u23aa\u23a8 \u23aa\u23a9 \u03b1uu\u2032+|y+=15 (\u22121 \u2264 \u03b1uu\u2032+ \u2264 1), \u22121 (\u03b1uu\u2032+ < \u22121), 1 (\u03b1uu\u2032+ > 1),\n(5.1)\nwhere \u03b1u is a parameter controlling the rate of change from wall blowing to suction, changed systematically from 0.01 to \u221e in the present study. As in the previous cases, \u03c6+ is constrained from \u22121.0 to 1.0. The corresponding policies in the u\u2032\u2013v\u2032 plane and the obtained drag reduction rates are summarized in table 4.\nIn Case U3, where the slope from wall blowing to suction is moderate, i.e. \u03b1u = 0.1, 20 % drag reduction rate is achieved, and this value is similar to that obtained by the opposition control. When the rate of change from wall blowing to suction becomes steeper, i.e. \u03b1u > 0.1, the drag reduction rate increases further. From this result, it can be concluded that the sharp change from wall blowing to suction is effective in the u\u2032-based control.\n960 A30-19\nht tp\ns: //\ndo i.o\nrg /1\n0. 10\n17 /jf\nm .2\n02 3.\n14 7\nPu bl\nis he\nd on\nlin e\nby C\nam br\nid ge\nU ni\nve rs\nity P\nre ss\n5.1.2. v\u2032-based control Here, we consider the effects of the rate of change from wall blowing to suction in v\u2032-based control. In this case, the policy depends on the wall-normal velocity fluctuation v\u2032 only, and it can be expressed as\n\u03c6+ = \u23a7\u23aa\u23a8 \u23aa\u23a9 \u03b1vv \u2032+|y+=15 (\u22121 \u2264 \u03b1vv\u2032+ \u2264 1),\n\u22121 (\u03b1vv\u2032+ < \u22121), 1 (\u03b1vv\u2032+ > 1).\n(5.2)\nAgain, \u03b1v determines the slope from wall blowing to suction. It should be noted that when \u03b1v = \u22121.0, it corresponds to the opposition control. The results are summarized in table 5.\nIn contrast to u\u2032-based control summarized in table 4, the opposite trend can be seen. Namely, the drag reduction rate is reduced as \u03b1v increases. It should be noted that Chung & Talha (2011) conducted a parametric survey changing \u03b1v from \u22120.1 to \u22121.0, and reported that \u03b1v = \u22121.0 is optimal within the range. Hence we do not repeat these cases here. The current results indicate that the further decrease of \u03b1v from \u22121.0 does not improve the control performance.\n960 A30-20\nht tp\ns: //\ndo i.o\nrg /1\n0. 10\n17 /jf\nm .2\n02 3.\n14 7\nPu bl\nis he\nd on\nlin e\nby C\nam br\nid ge\nU ni\nve rs\nity P\nre ss\n5.1.3. u\u2032v\u2032-based control Next, we consider the effects of the rate of change from wall blowing to suction for a policy that depends on both u\u2032 and v\u2032. In this case, the considered policies are expressed as\n\u03c6+ = \u23a7\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a8 \u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a9 \u03b1uv ( u\u2032+|y+=15 2 \u2212 v\u2032+|y+=15 ) ( \u22121 \u2264 \u03b1uv ( u\u2032+ 2 \u2212 v\u2032+ ) \u2264 1 ) , \u22121 ( \u03b1uv ( u\u2032+ 2 \u2212 v\u2032+ ) < \u22121 ) , 1 (\n\u03b1uv\n( u\u2032+\n2 \u2212 v\u2032+\n) > 1 ) ,\n(5.3)\nwhere \u03b1uv is the rate of the change from wall blowing to suction. As summarized in table 6, the contours representing the control policies have the same inclination angle, which is taken from the policy obtained by the reinforcement learning in Case R18 shown in figure 9(b). It is found that the resultant drag reduction rate increases with increasing \u03b1uv . This trend is similar to that of u\u2032-based control, but opposite to v\u2032-based control. From these results, we could conclude that the rapid switch between wall blowing and suction is effective for a policy depending on u\u2032, and such a policy can outperform the existing opposition control, which is based on v\u2032 only.\n960 A30-21\nht tp\ns: //\ndo i.o\nrg /1\n0. 10\n17 /jf\nm .2\n02 3.\n14 7\nPu bl\nis he\nd on\nlin e\nby C\nam br\nid ge\nU ni\nve rs\nity P\nre ss\n5.1.4. Effects of the inclination of the boundary between wall blowing and suction Finally, we investigate the effects of the inclination angle of the boundary between wall blowing and suction. Specifically, the policies considered here can be expressed by\n\u03c6+ = {\u22121 (0 > u\u2032+|y+=15 \u2212 v\u2032+|y+=15) ,\n1 ( 0 \u2264 u\u2032+|y+=15 \u2212 v\u2032+|y+=15 ) ,\n(5.4)\nwhere controls the inclination angle of the boundary and is changed systematically as shown in table 7.\nIt is interesting to note that the resultant drag reduction rate increases with increasing , i.e. the inclination angle. In Case IA5, where the control policy depends only on u\u2032, i.e. = \u221e, the drag reduction rate becomes maximum. This policy is quite similar to those obtained in Cases S18 and LR18 shown in figures 9(c) and 9(d), respectively. It can also be seen that the drag reduction rates almost saturate when is larger than 0.25. Therefore, the control policies obtained in figures 9(b) and 9(e) can also be considered nearly optimal. Considering that all the policies shown in figures 9(b\u2013e) are obtained through training in the minimal channel, we can conclude that the reinforcement learning can successfully find the effective control policies that can be transferable to the full channel.\n5.2. Spatio-temporal distribution of control inputs It is of interest to investigate the spatio-temporal distribution of wall blowing and suction determined by the policy obtained from the current reinforcement learning and how it results in a drag reduction rate higher than that obtained by the conventional opposition control. The instantaneous flow fields as well as the control inputs at t+ = 0.6 and 20.4 after the onset of the control in Case R18 are shown in figures 14(a) and 14(b), respectively.\n960 A30-22\nht tp\ns: //\ndo i.o\nrg /1\n0. 10\n17 /jf\nm .2\n02 3.\n14 7\nPu bl\nis he\nd on\nlin e\nby C\nam br\nid ge\nU ni\nve rs\nity P\nre ss\nIt can be seen that the control input switches rapidly from wall blowing to suction, i.e. \u03c6 = 1.0 and \u22121.0, consistent with the policy shown in figure 9(b). Just after the onset of the control, at t+ = 0.6, the control input is elongated in the streamwise direction, reflecting instantaneous near-wall streaky structures (see figure 14a). Interestingly, as time passes, the control input transits to a coherent wave-like input as shown in figure 14(b), which is almost uniform in the spanwise direction, and its streamwise wavelength is equal to the streamwise domain size.\nWe also note that similar wave-like control inputs can be generated when policies with a rapid change from wall blowing to suction are applied. In order to extract a coherent component from the control input, we define the spanwise average of the instantaneous control input \u03c6 on each wall as\n\u03c6\u0303(x, t) = 1 Lz \u222b Lz 0 \u03c6(x, z, t) dz. (5.5)\nThe spanwise-averaged control inputs in Cases R18, U6 and V4 as functions of t and x are shown in figures 15(a), 15(b) and 15(c), respectively. We note that the corresponding drag reduction rates for Case R18, U6 and V4 are 31 %, 37 % and 9 %, respectively.\nIn Case R18, the wall blowing and suction switches at a high frequency, while its wave nodes move slowly upstream (see figure 15a). In contrast, when the control policy of Case U6 is applied, a downstream travelling wave can be confirmed, as shown in figure 15(b),\n960 A30-23\nht tp\ns: //\ndo i.o\nrg /1\n0. 10\n17 /jf\nm .2\n02 3.\n14 7\nPu bl\nis he\nd on\nlin e\nby C\nam br\nid ge\nU ni\nve rs\nity P\nre ss\nwhile a standing-wave-like control input can be confirmed in Case V4 (see figure 15c). Since all three policies switch rapidly from strong wall blowing to suction depending on the state u\u2032 and v\u2032 at the detection plane y+d = 15, such an abrupt change of the control input causes a strong perturbation at the detection plane. This in turn determines the control\n960 A30-24\nht tp\ns: //\ndo i.o\nrg /1\n0. 10\n17 /jf\nm .2\n02 3.\n14 7\nPu bl\nis he\nd on\nlin e\nby C\nam br\nid ge\nU ni\nve rs\nity P\nre ss\ninput in the next time step. Such feedback between the control input and the flow state at the detection plane should yield the wave-like coherent control inputs observed here. Indeed, the time period of switching from wall blowing to suction in Cases R18 and V4 is equal to the time step for updating the control input, i.e. t+update = 0.6.\nIt has been reported that drag reduction can be achieved by applying a travelling-wave-like control input. For example, Min et al. (2006) showed that sub-laminar\n960 A30-25\nht tp\ns: //\ndo i.o\nrg /1\n0. 10\n17 /jf\nm .2\n02 3.\n14 7\nPu bl\nis he\nd on\nlin e\nby C\nam br\nid ge\nU ni\nve rs\nity P\nre ss\ndrag can be achieved by an upstream travelling wave of wall blowing and suction when its phase speed is approximately three times the bulk mean velocity. As mentioned before, the wave node obtained in Case R18 shown in figure 15(a) travels in the upstream direction, and its velocity normalized by the bulk mean velocity is approximately 3.34. Although the travelling speed of the wave node in Case R18 agrees well with the value reported previously, the spanwise-averaged control input obtained in Case R18 switches wall blowing and suction at a higher frequency, so it is essentially different from the upstream travelling waves considered in the previous study.\nMeanwhile, relaminarization is also caused by a downstream travelling wave when the phase speed is larger than 1.5 times the bulk mean velocity (Lieu et al. 2010; Mamori et al. 2014). However, the phase speed of the downstream wave obtained in Case U6 shown figure 15(b) is much faster, i.e. approximately 28 when it is normalized by the bulk mean velocity. In order to clarify whether the coherent wave-like inputs observed in figures 15(a\u2013c) alone lead to drag reduction or not, we conduct additional simulations where only the coherent control inputs \u2013 which are uniform in the spanwise direction and depend only on the streamwise direction and the time \u2013 are applied. It should be noted that in these cases, the applied controls are no longer feedback controls, but predetermined controls, since the control inputs are determined a priori, and do not depend on the instantaneous flow state. The results are shown in figure 16. It is found that applying solely the coherent wave-like control inputs hardly leads to drag reduction. This indicates that the present controls are feedback controls, and applying a control input based on the instantaneous flow state is essential to yield the drag reduction effects.\nBased on the above findings, we investigate further the effects of the local control input. In figures 17(a,b), we show the spatial distributions of the instantaneous control inputs (wall blowing and suction) on the bottom wall at t+ = 0 and 30.0 in Case R18. Similarly, the instantaneous streamwise velocity fluctuations on the detection plane y+d = 15 at t+ = 0 and 30.0 are shown in figures 17(c,d), while the wall-normal velocity fluctuations on y+d = 15 at t+ = 0 and 30.0 are presented in figures 17(e, f ). It can be seen that the control input at the onset of the control, i.e. t+ = 0, shown in figure 17(a), agrees well with that of the streamwise velocity fluctuation shown in figure 17(c). This is because the optimal policy in Case R18 applies wall blowing and suction based on the streamwise velocity fluctuation at the detection plane, especially when its magnitude is large, i.e. |u\u2032+| > 2 (see figure 9b). In contrast, after some time has passed since the onset of the control, the streamwise velocity fluctuation at the detection plane is suppressed due to the control applied up to that point. Consequently, the control input shown at t+ = 30.0 in figure 17(b) correlates negatively with the wall-normal velocity fluctuation at y+d = 15 shown in figure 17( f ). This indicates that the applied control input on the wall propagates immediately in the wall-normal direction due to the incompressibility of the fluid, and the feedback of the wall-normal velocity fluctuation on the detection plane causes the coherent travelling-wave-like control input. As discussed already, however, such a coherent control input alone does not cause drag reduction effects. A closer look at figure 17(b) reveals that small-scale fluctuations in the control input are superimposed on the coherent input. These small-scale fluctuations coincide with regions with relatively large streamwise velocity fluctuation in figure 17(d), and should play essential roles in reducing skin friction drag.\nFigure 18 shows more detailed comparison between the local control inputs applied in Case R18 and the opposition control. Note that the same instantaneous uncontrolled flow field is used, and only the control inputs are different in the two figures. White surfaces\n960 A30-26\nht tp\ns: //\ndo i.o\nrg /1\n0. 10\n17 /jf\nm .2\n02 3.\n14 7\nPu bl\nis he\nd on\nlin e\nby C\nam br\nid ge\nU ni\nve rs\nity P\nre ss\ncorresponds to vortex cores, with black vectors representing the local fluid motions. Red and blue contours correspond to high- and low-speed regions. The detection plane at y+d = 15 is expressed by yellow, while the applied control input is shown by red vectors. It can be seen that the local control inputs in Case R18 and the opposition control are quite different, especially below the low-speed region. In the case of the opposition control, strong wall blowing is applied below the low-speed region (see figure 18b) so as to cancel the downwelling motion towards the bottom wall induced by the upper streamwise vortex. In contrast, wall suction is applied at the same region in Case R18 shown in figure 18(a), since its control policy depends mostly on the streamwise velocity fluctuation at the detection plane. It can be considered that applying strong wall suction below low-speed streaks prevents their lift-up, and therefore stabilizes the flow in a long-term perspective."
        },
        {
            "heading": "6. Conclusions",
            "text": "In this study, reinforcement learning is first applied to obtain effective control strategies using wall blowing and suction for reducing skin friction drag in a fully developed turbulent channel flow. The present framework is based on the deep deterministic policy gradient (DDPG) algorithm (Lillicrap et al. 2016), where the actor network dictating a control policy reads the flow state and outputs the action, i.e. the control input, while the critic network estimates the expected total future reward, i.e. a long-term drag reduction rate, when a certain action is taken under a certain flow state. The two networks are trained simultaneously through a number of trials in direct numerical simulation.\nWe first considered a simple policy where the local wall blowing and suction is linearly related to the wall-normal velocity fluctuation at the detection plane y+d = 15. It is found that the current reinforcement learning successfully finds the optimal weight coefficient reported in the previous study (Chung & Talha 2011). Next, we extended the above framework by adding the streamwise velocity fluctuation as well as the wall-normal velocity fluctuation as the state, and also including nonlinear activation functions in the actor network. It is demonstrated that the obtained policies lead to drag reduction rates as high as 37 %, which is higher than the 23 % achieved by the existing opposition control. The obtained control policies are characterized by a sharp change from wall blowing to\n960 A30-27\nht tp\ns: //\ndo i.o\nrg /1\n0. 10\n17 /jf\nm .2\n02 3.\n14 7\nPu bl\nis he\nd on\nlin e\nby C\nam br\nid ge\nU ni\nve rs\nity P\nre ss\nsuction depending on the streamwise and wall-normal velocity fluctuations at the detection plane. Further detailed analyses indicate that such a control policy with a rapid switch between wall blowing and suction is particularly effective when a control policy depends on the streamwise velocity fluctuation at the detection plane.\nIt should be emphasized that finding such an effective and highly nonlinear control policy is quite difficult by relying solely on researchers\u2019 insights, and it becomes possible by a systematic learning framework leveraged by neural networks. One of great advantages in the reinforcement learning is that it can learn not only from successes, but also from failures through numerous trials. In the flow control community, effective control laws have often been sought by human through trial and error. Reinforcement learning has a potential to replace such human efforts to explore effective control policies. Although we are still in the process of developing newly emerging methodologies, based on the obtained control policies, it is expected that we will be able to gain a deeper understanding of flow physics and new control guidelines. The unique control policies obtained in the present study would also contribute to these purposes. In the current study, we consider only the streamwise and wall-normal velocity fluctuations at a certain distance from the wall as a state, and there is a possibility that more effective control strategies could be found by extending the state in space and/or time. Meanwhile, our preliminary results suggest that the learning becomes more difficult when the network size becomes larger (see Appendix C). Establishing effective learning methodologies is obviously crucial. In the present study, we employ the DDPG algorithm, while some existing studies successfully applied the proximal policy optimization algorithm (Belus et al. 2019) to different flow problems (Rabault et al. 2019; Rabault & Kuhnle 2019; Tang et al. 2020; Tokarev et al. 2020; Xu et al. 2020; Ghraieb et al. 2021; Paris et al. 2021; Ren et al. 2021). Even for the current DDPG, enormous efforts are needed to validate various training parameters and network hyperparameters, and to clarify the optimal configuration. In particular, the network structures of the actor and the critic should have significant impacts on the training results. Since such verification is difficult to complete by a single group, it should be conducted by collaboration among multiple groups across countries. For this, we open the source code used in the present reinforcement learning (https://github.com/YSKLABSHARE/RL-turbulence-control).\nThe current study considers only a single low Reynolds number, and the applicability of the current approach to higher Reynolds numbers needs to be investigated. Considering that the obtained policies in the minimal channel work well in the full-size channel as well in the present study, transfer learning over Reynolds numbers, which combines pre-training at lower Reynolds numbers and then fine tuning at higher Reynolds numbers, could also be an interesting option. We also note that approaches of treating a system as a black box, as typified by reinforcement learning, should generally have wide applicability to experimental studies (Fan et al. 2020). In particular, if the state, action and reward can be measured and evaluated online, then the training becomes much faster and more effective than that in simulation. The above issues should be explored further in future studies.\nFunding. This work was partially supported by JSPS KAKENHI grant nos JP20H02063 and JP21H05007.\nDeclaration of interests. The authors report no conflict of interest.\nAuthor ORCIDs. Yosuke Hasegawa https://orcid.org/0000-0002-1878-972X.\n960 A30-28\nht tp\ns: //\ndo i.o\nrg /1\n0. 10\n17 /jf\nm .2\n02 3.\n14 7\nPu bl\nis he\nd on\nlin e\nby C\nam br\nid ge\nU ni\nve rs\nity P\nre ss"
        },
        {
            "heading": "Appendix A. Performances with different rewards",
            "text": "There are numerous ways to define the reward. In this appendix, we consider the following reward as an alternative to (3.1) used in the present study:\nr(t) = 1 Cf (t) . (A1)\nThis new reward also increases with decreasing Cf , but its increasing rate becomes larger with decreasing Cf . The learning curves with the two rewards in Case R18 are compared in figure 19(a). We can confirm a similar or slightly larger reduction of Cf with the new reward. The resultant optimal policy obtained with the new reward is shown in figure 19(b), and it is closer to u\u2032-based control. Nonetheless, the essential features of the optimal policies obtained with the present and new definitions ((3.1) and (A1)) are quite similar (compare figure 19b with figure 9b). Namely, wall blowing and suction switch rapidly, and it depends on not only the wall-normal velocity fluctuation, but the streamwise velocity fluctuation at the detection plane. Therefore, the effects of different definitions of the reward are minor."
        },
        {
            "heading": "Appendix B. Control policy based on flow states at different locations",
            "text": "The present results indicate that control policies based on the streamwise velocity fluctuation are generally more effective than those based on the wall-normal velocity fluctuation only. Meanwhile, all the policies considered in the present study are based on the flow state at a single location y+d = 15 from the wall. Here, we show one example where the state is extended to multiple locations from the wall.\nSpecifically, we use the streamwise velocity fluctuation u\u2032 at 10 different locations from the wall, i.e. y+d,i = 5, 10, 15, 20, 25, 30, 34, 41, 44 and 51. In order to make the problem simple, we consider the following linear control policy:\na \u2261 \u03c6(x, z, t) = tanh { 10\u2211\ni=1 \u03b1i u\u2032(x, yd,i, z) + \u03b2\n} + N, (B1)\nwhere \u03b1i (i = 1, . . . , 10) and \u03b2 are linear weights and a bias to be optimized, while N is a zero-mean random noise, the standard deviation of which is 0.1 in the wall unit.\n960 A30-29\nht tp\ns: //\ndo i.o\nrg /1\n0. 10\n17 /jf\nm .2\n02 3.\n14 7\nPu bl\nis he\nd on\nlin e\nby C\nam br\nid ge\nU ni\nve rs\nity P\nre ss\nFigure 20 shows the distribution of the weights \u03b1i for different distances from the wall at the end of the episode where the maximum drag reduction is achieved. It can be seen that the weights y+d = 15\u201320 become maximum. When this policy is applied in the full channel, the drag reduction rate 36 % is achieved. When the same linear activation function is used for a single detection plane at y+d = 15, the resulting drag reduction rate reduces to 29 % (see Appendix E). Hence the flow information at multiple y locations is certainly useful for constructing effective control policies. Meanwhile, even if we consider a single detection plane y+d , by leveraging a nonlinear activation function, we can achieve a 31 % drag reduction rate in Case R18. This suggests that the single detection plane at y+ = 15 already contains considerable information to characterize and control near-wall turbulence, at least for the present low Reynolds number. Furthermore, we also check the control performance when we consider the single detection plane at y+d = 20 in Case R18, since the corresponding weight is the largest in figure 20. The obtained drag reduction rate is approximately 28 %, which is also less than the 31 % obtained with the single detection plane at y+d = 15. From these results, we conclude that y+d = 15 is the optimal for a single detection plane, and further increase of the number of detection planes will not improve significantly the control performance."
        },
        {
            "heading": "Appendix C. Effects of the numbers of layers and nodes employed in the actor",
            "text": "Here, we summarize some results with different numbers of layers and nodes used for the actor. The obtained control policies and resulting drag reduction rates for all the cases considered are summarized in table 8. Except for the numbers of layers and nodes in the actor, the other settings such as an activation function in the actor, the hyperparameters in the critic and learning procedures are the same as for Case R18 in table 1. The drag reduction rates listed in table 8 are obtained by averaging the final 20 episodes during the training after the flow fields converge to equilibrium states.\nIt can be seen that the obtained control policies are qualitatively similar in Cases R14, R18 and R24. Among them, Case R18 results in the highest drag reduction rate. In Case R28, where the actor has the most complex network among all the cases considered, drag reduction is not achieved. It is still unclear why the policies do not converge when the\n960 A30-30\nht tp\ns: //\ndo i.o\nrg /1\n0. 10\n17 /jf\nm .2\n02 3.\n14 7\nPu bl\nis he\nd on\nlin e\nby C\nam br\nid ge\nU ni\nve rs\nity P\nre ss\nnetwork becomes more complex. It may be attributed to the difficulties in training large networks. In summary, Case R18 with 1 layer and 8 nodes is found to be suitable for the present problem setting."
        },
        {
            "heading": "Appendix D. Dependency of control performance on numerical schemes",
            "text": "As shown in figure 9, effective policies obtained in the present study commonly show a rapid change from wall blowing to suction depending on the flow state at y+d = 15. This may cause unphysical oscillations and affect the resulting drag reduction rate. In particular, such numerical effects could appear more strongly in a spectral method employed in the present study due to the Gibbs phenomena. Therefore, we conduct additional simulations with a finite difference method in order to confirm the universality of the present results. Note that we use the same policies obtained from the spectral method, and their control performances in the full channel are evaluated by another code.\nSpecifically, we use an open-source flow solver called Incompact3d (Laizet & Lamballais 2009; Laizet & Li 2011), which is based on the sixth-order compact finite difference scheme. Time integration is conducted by using the second-order Crank\u2013Nicolson scheme for the wall-normal diffusion term, whereas the third-order Adams\u2013Bashforth scheme is applied for the other terms. The friction Reynolds number and the domain size are set to Re\u03c4 \u2248 150 and (Lx, Ly, Lz) = (2.5\u03c0, 2, \u03c0), respectively. These are the same as used for the full channel in the present study. The number of grids in each direction is set to (Nx, Ny, Nz) = (128, 129, 96), resulting in the spatial resolutions x+ = 9.2, y+ = 0.83\u20136.6 and z+ = 4.9.\nTable 9 shows the comparisons of the drag reduction rates obtained by the present pseudo-spectral and finite difference methods for typical control policies, i.e. Cases R18, U3, U6 and V4, together with the results of the opposition control. We note that a rapid\n960 A30-31\nht tp\ns: //\ndo i.o\nrg /1\n0. 10\n17 /jf\nm .2\n02 3.\n14 7\nPu bl\nis he\nd on\nlin e\nby C\nam br\nid ge\nU ni\nve rs\nity P\nre ss\nswitch from wall blowing to suction exists in Cases R18, U6 and V4, while it changes smoothly in the rest of the cases. It can be seen that the impacts of the employed numerical schemes on the resultant drag reduction rates generally remain minor, so that the control performances obtained by the preset policies can be considered universal."
        },
        {
            "heading": "Appendix E. Influences of a nonlinear activation function in the actor network",
            "text": "Here, we investigate the effects of a nonlinear activation function used in the actor network. Specifically, we simply change the activation function ReLU used in Case R18 to a linear function, whereas the other learning conditions and network parameters are kept exactly the same. The new case with the linear activation function is referred to as Case Li18. The best policy obtained in Case Li18 is shown in figure 21. It can be seen that the resulting policy is qualitatively similar to those obtained with the nonlinear activation function in Case R18 shown in figure 9(b). In Case Li18, however, the change from wall blowing to suction is smoother than that obtained in Case R18. The best policy obtained in Case Li18 is then applied to the full-size channel, and the resulting drag reduction rate\n960 A30-32\nht tp\ns: //\ndo i.o\nrg /1\n0. 10\n17 /jf\nm .2\n02 3.\n14 7\nPu bl\nis he\nd on\nlin e\nby C\nam br\nid ge\nU ni\nve rs\nity P\nre ss\nis 29 %, which is slightly smaller than the 31 % achieved by Case R18. Hence we could conclude that nonlinearity in the actor is not critical in the present problem. Meanwhile, as shown in tables 4 and 6, a steeper change of the control input leads to a higher control performance, when the control input depends on the streamwise velocity fluctuation. The present results suggest that the nonlinear activation function used in the actor helps to approach this limit. Finally, in the present study, we clip the maximal absolute value of the control input. Such constraints in the control input are common in not only numerical simulations, but also experiments, and introduce additional nonlinearity in the control policy. One of the advantages in the reinforcement learning is that the nonlinearity in the control policy can be handled in a straightforward manner. We also note that there are attempts (Jagtap, Kawaguchi & Karniadakis 2020) to introduce adaptive parameters in the activation function itself, so that, in principle, the value for the clipping could also be learned and optimized."
        }
    ],
    "title": "Reinforcement learning of control strategies for reducing skin friction drag in a fully developed turbulent channel flow",
    "year": 2023
}