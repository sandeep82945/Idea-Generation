{
    "abstractText": "3D reconstruction plays an increasingly important role in modern photogrammetric systems. Conventional satellite or aerial-based remote sensing (RS) platforms can provide the necessary data sources for the 3D reconstruction of large-scale landforms and cities. Even with low-altitude UAVs (Unmanned Aerial Vehicles), 3D reconstruction in complicated situations, such as urban canyons and indoor scenes, is challenging due to the frequent tracking failures between camera frames and high data collection costs. Recently, spherical images have been extensively exploited due to the capability of recording surrounding environments from one camera exposure. Classical 3D reconstruction pipelines, however, cannot be used for spherical images. Besides, there exist few software packages for 3D reconstruction of spherical images. Based on the imaging geometry of spherical cameras, this study investigates the algorithms for the relative orientation using spherical correspondences, absolute orientation using 3D correspondences between scene and spherical points, and the cost functions for BA (bundle adjustment) optimization. In addition, an incremental SfM (Structure from Motion) workflow has been proposed for spherical images using the above-mentioned algorithms. The proposed solution is finally verified by using three spherical datasets captured by both consumer-grade and professional spherical cameras. The results demonstrate that the proposed SfM workflow can achieve the successful 3D reconstruction of complex scenes and provide useful clues for the implementation in open-source software packages. The source code of the designed SfM workflow would be made publicly available1.",
    "authors": [
        {
            "affiliations": [],
            "name": "San Jiang"
        },
        {
            "affiliations": [],
            "name": "Kan You"
        },
        {
            "affiliations": [],
            "name": "Yaxin Li"
        },
        {
            "affiliations": [],
            "name": "Duojie Weng"
        },
        {
            "affiliations": [],
            "name": "Wu Chen"
        }
    ],
    "id": "SP:5dcd931f0130c8ed6db22a7db72d147b5d3c093d",
    "references": [
        {
            "authors": [
                "T. Chuang",
                "N. Perng"
            ],
            "title": "Rectified feature matching for spherical panoramic images. Photogrammetric Engineering & Remote Sensing",
            "venue": "ACM Computing Surveys (CSUR). Fangi, G.,",
            "year": 2018
        },
        {
            "authors": [
                "G. Fangi",
                "R. Pierdicca",
                "M. Sturari",
                "E. Malinverni"
            ],
            "title": "International Archives of the Photogrammetry, Remote Sensing & Spatial Information Sciences, Athens, Greece",
            "year": 2013
        },
        {
            "authors": [
                "S. 381-395. Gao",
                "K. Yang",
                "H. Shi",
                "K. Wang",
                "J. Bai"
            ],
            "title": "Review on Panoramic Imaging and Its Applications in Scene Understanding",
            "venue": "arXiv preprint arXiv:2205.05570",
            "year": 2022
        },
        {
            "authors": [
                "H. Guan",
                "W.A. Smith"
            ],
            "title": "BRISKS: Binary features for spherical images on a geodesic grid",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Jhan",
                "J.-P",
                "N. Kerle",
                "Rau",
                "J.-Y"
            ],
            "title": "Integrating UAV and ground panoramic images for point cloud analysis of damaged building. IEEE geoscience and remote sensing letters",
            "venue": "ISPRS Journal of Photogrammetry and Remote Sensing",
            "year": 2022
        },
        {
            "authors": [
                "C. Jiang",
                "W. Jiang"
            ],
            "title": "panoramic sensor models for precise 3d measurements",
            "venue": "Photogrammetric Engineering & Remote Sensing",
            "year": 2020
        },
        {
            "authors": [
                "S. Jiang",
                "W. Jiang"
            ],
            "title": "UAV-BASED OBLIQUE PHOTOGRAMMETRY FOR 3D RECONSTRUCTION OF TRANSMISSION LINE: PRACTICES AND APPLICATIONS, International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences",
            "venue": "Copernicus Publications,",
            "year": 2019
        },
        {
            "authors": [
                "W. Jiang",
                "L. Wang"
            ],
            "title": "Outdoor Data Acquisition and Offsite Visual Inspection of Transmission Line",
            "venue": "Remote Sensing",
            "year": 2022
        },
        {
            "authors": [
                "S. Jiang",
                "Q. Li",
                "W. jiang",
                "W. Chen"
            ],
            "title": "Parallel Structure From Motion for UAV Images via Weighted Connected Dominating Set",
            "venue": "IEEE Transactions on Geoscience and Remote Sensing",
            "year": 2022
        },
        {
            "authors": [
                "G. Li",
                "X. Lu",
                "B. Lin",
                "L. Zhou",
                "G. Lv"
            ],
            "title": "Automatic Positioning of Street Objects Based on Self-Adaptive Constrained Line of Bearing from Street-View Images",
            "venue": "ISPRS International Journal of Geo-Information",
            "year": 2022
        },
        {
            "authors": [
                "C. Mei",
                "P. Rives"
            ],
            "title": "Distinctive image features from scale-invariant keypoints. International journal of computer vision",
            "venue": "Earth Observations and Remote Sensing 16,",
            "year": 2004
        },
        {
            "authors": [
                "B. Micusik",
                "J. Kosecka"
            ],
            "title": "Piecewise planar city 3D modeling from street view panoramic sequences, 2009",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
            "year": 2009
        },
        {
            "authors": [
                "A. Pagani",
                "D. Stricker"
            ],
            "title": "Structure from motion using full spherical panoramic cameras, 2011",
            "venue": "IEEE International Conference on Computer Vision Workshops (ICCV Workshops)",
            "year": 2011
        },
        {
            "authors": [
                "J.L. Sch\u00f6nberger",
                "E. Zheng",
                "Frahm",
                "J.-M",
                "M. Pollefeys"
            ],
            "title": "omnidirectional camera calibration and structure from motion, Fourth IEEE International Conference on Computer Vision Systems (ICVS'06)",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2016
        },
        {
            "authors": [
                "H. Taira",
                "Y. Inoue",
                "A. Torii",
                "M. Okutomi"
            ],
            "title": "Robust feature matching for distorted projection by spherical cameras",
            "venue": "European Conference on Computer Vision. Springer,",
            "year": 2015
        },
        {
            "authors": [
                "A. Torii",
                "M. Havlena",
                "T. Pajdla"
            ],
            "title": "From google street view to 3d city models, 2009 IEEE 12th international conference on computer vision workshops",
            "venue": "ICCV Workshops",
            "year": 2009
        },
        {
            "authors": [
                "M. Gerke",
                "N. Kerle",
                "G. Vosselman"
            ],
            "title": "Proceedings of the sixth workshop on omnidirectional vision, camera networks and nonclassical cameras",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "year": 1991
        },
        {
            "authors": [
                "Y. Wang",
                "S. Cai",
                "Li",
                "S.-J",
                "Y. Liu",
                "Y. Guo",
                "T. Li",
                "Cheng",
                "M.-M"
            ],
            "title": "3D point clouds from very high resolution oblique airborne images. ISPRS journal of photogrammetry and remote sensing",
            "year": 2018
        },
        {
            "authors": [
                "C. Wu"
            ],
            "title": "SiftGPU: A GPU implementation of scale invariant feature transform",
            "venue": "ISPRS Journal of Photogrammetry and Remote Sensing",
            "year": 2007
        },
        {
            "authors": [
                "X. Zhang",
                "P. Zhao",
                "Q. Hu",
                "M. Ai",
                "D. Hu",
                "J. Li"
            ],
            "title": "A UAV-based panoramic oblique photogrammetry (POP) approach using spherical projection",
            "venue": "ISPRS Journal of Photogrammetry and Remote Sensing",
            "year": 2020
        },
        {
            "authors": [
                "Z. Zhang",
                "H. Rebecq",
                "C. Forster",
                "D. Scaramuzza"
            ],
            "title": "Benefit of large field-of-view cameras for visual odometry, 2016",
            "venue": "IEEE International Conference on Robotics and Automation (ICRA)",
            "year": 2016
        }
    ],
    "sections": [
        {
            "text": "systems. Conventional satellite or aerial-based remote sensing (RS) platforms can provide the necessary data sources for the 3D reconstruction of large-scale landforms and cities. Even with low-altitude UAVs (Unmanned Aerial Vehicles), 3D reconstruction in complicated situations, such as urban canyons and indoor scenes, is challenging due to the frequent tracking failures between camera frames and high data collection costs. Recently, spherical images have been extensively exploited due to the capability of recording surrounding environments from one camera exposure. Classical 3D reconstruction pipelines, however, cannot be used for spherical images. Besides, there exist few software packages for 3D reconstruction of spherical images. Based on the imaging geometry of spherical cameras, this study investigates the algorithms for the relative orientation using spherical correspondences, absolute orientation using 3D correspondences between scene and spherical points, and the cost functions for BA (bundle adjustment) optimization. In addition, an incremental SfM (Structure from Motion) workflow has been proposed for spherical images using the above-mentioned algorithms. The proposed solution is finally verified by using three spherical datasets captured by both consumer-grade and professional spherical cameras. The results demonstrate that the proposed SfM workflow can achieve the successful 3D reconstruction of complex scenes and provide useful clues for the implementation in open-source software packages. The source code of the designed SfM workflow would be made publicly available1.\nKeywords: spherical image; 3D reconstruction; structure from motion; image matching; equirectangular projection"
        },
        {
            "heading": "1. Introduction",
            "text": "3D reconstruction plays an increasingly critical role in modern photogrammetric systems,\nwhich has been widely utilized for building modeling (Xiong et al., 2015), emergency response (Vetrivel et al., 2015), transmission corridor inspection (Jiang and Jiang, 2019; Jiang et al., 2017), etc. 3D reconstruction can be implemented by using data sources from varying sensors, such as optical cameras and laser scanners. Due to the low economic costs and the mature of image processing techniques, perspective cameras are the most popular sensors for 3D modeling, which have been equipped with different remote sensing (RS) platforms that range from highaltitude satellites (Yu et al., 2021) to low-altitude UAVs (Unmanned Aerial Vehicles) (Jiang et al., 2020; Jiang et al., 2022a).\n1 https://github.com/json87/SphereSfM\nExcept for 3D reconstruction of large-scale terrains and urban buildings, recent years have\nwitness the increasing demand for 3D modeling of fine-scale targets, e.g., building facades and urban streets. Although flexible data acquisition can be achieved through UAV-based nap-ofthe-object or optimized views photogrammetry (Li et al., 2023), the observation ability of aerial RS platforms is still unsatisfactory in complex urban environments. To cope with this situation, mobile mapping systems (MMS) are further exploited in the urban environments. However, perspective cameras cannot adapt well to the characteristics (da Silveira et al., 2022) of the data acquisition in the complex environments mainly because of their limited FOV (Field of View). This can be explained from two aspects. On the one hand, the trajectory of ground vehicles would be seriously restricted by street structures, while the trajectory can be adjusted flexibly for aerial RS platforms. This would cause sudden viewpoint changes at street turning points and frequent track failure between camera frames (Ji et al., 2020). On the other hand, the image observation regions are expanded from the single direction in the aerial RS platforms to the full direction in the ground RS platforms, which requires more image recording at each camera exposure position and increases the acquisition time consumptions (Zhang et al., 2016). Thus, efficient imaging techniques are required for 3D reconstruction.\nRecently, spherical cameras that have 360 and 180 degrees FOV in horizontal and vertical\ndirections are featured as recording full surrounding environments from one camera exposure. Figure 1 presents an illustration of the comparison between spherical and perspective images. It is clearly shown that spherical images can cover the whole scene from each camera record when compared with perspective images with very limited FOV. In addition, consumer-grade spherical cameras with low costs are becoming more and more popular, e.g., the Insta360 and Ricoh Theta (Gao et al., 2022). The capability and popularity of spherical cameras facilitate data acquisition in urban scenes and promote their usage for varying applications, including but not limited to underwater collision detection (Li et al., 2022), damaged building evaluation (Jhan et al., 2022), and urban 3D modeling (Fangi et al., 2018). Spherical images have become one of the most popular RS data sources for 3D reconstruction of urban scenes.\nCompared with perspective images, there is a major difference for spherical images in the\ncontext of 3D reconstruction, i.e., the camera imaging model (Pagani and Stricker, 2011). In contrast to the 2D plane imaging of 3D points in the perspective projection, spherical camera imaging model projects 3D points onto the 3D sphere. For the image storage, the 3D sphere are then flattened onto the 2D image plane, which causes serious geometric distortions (Jiang et al., 2023). The difference of camera imaging model requires extra consideration for 3D modeling. In the literature, some research has been designed and reported to achieve 3D modeling from spherical images. To alleviate the distortion in feature matching, both image rectification and redesigned algorithms have been proposed (Chuang and Perng, 2018; Guan and Smith, 2017; Taira et al., 2015; Wang et al., 2018; Zhao et al., 2015). Similar to feature matching for oblique images, Wang et al. (2018) proposed converting spherical images into the cubic-map format that consists of six perspective images and casts feature matching from spherical images to the traditional perspective images. Considering few distortions exist in the sphere equator, Taira et al. (2015) rotated spherical images and detected local features from regions near the equator,\nwhich can be seen as a semi-global rectification method when compared with the cubic-map conversion. By using local rectification, Chuang and Perng (2018) proposed reprojecting the local image patches of keypoints onto the corresponding tangent planes and calculating feature descriptors from the rectified image patches. In contrast to the rectification-based methods, other researchers focus on redesigning the algorithms, such as the SPHORB (Zhao et al., 2015) and BRISKS (Guan and Smith, 2017).\nTo achieve the 3D reconstruction of spherical images, other attempts have also been made\nfrom earlier two-view or multi-view image orientation (Torii et al., 2005) to recent large-scale 3D modeling based on Structure from Motion (SfM) (Jhan et al., 2022; Zhang et al., 2020). In the work of Torii et al. (2005), both two and three-view geometry were presented, which establishes the basic geometry for 3D modeling of spherical images. As the pioneer work, (Fangi, 2007; Fangi and Nardinocchi, 2013; Pagani and Stricker, 2011) proposed the concept of spherical photogrammetry (SP) in the field of photogrammetry and remote sensing and established the workflow for 3D modeling of cultural heritage documentation. By using Google Street Views, (Micusik and Kosecka, 2009; Torii et al., 2009) implemented 3D modeling of urban streets and verified the usage of spherical images for urban street reconstruction. By investigating the use of spherical images in Structure from Motion, Pagani and Stricker (2011) designed error models for the relative and absolute orientation of spherical images. All these above-mentioned work has promoted the development of 3D reconstruction of spherical images.\nHowever, there are only fewer open-source and commercial software packages that are\ndesigned for 3D reconstruction of spherical images when compared with perspective images. As far as we know, the satisfied software packages include MicMac, OpenMVG, Pix4dMapper, and Agisoft Metashape (Jiang et al., 2023). With the popularity of spherical cameras, such as consumer-grade Insta360, Ricoh Theta, and the development of image processing technology, the demand for 3D reconstruction of spherical images would increase dramatically. Therefore, this study aims to give an implementation of 3D reconstruction workflow for spherical images based on an incremental SfM (Structure from Motion) engine. The major contributions of this study include: (1) we present the basic camera imaging model for spherical cameras and give an insight analysis of key techniques for SfM-based 3D reconstruction of spherical images; (2) we implement a 3D reconstruction workflow based on an incremental SfM engine for spherical images, including the modules of feature matching, image orientation, cubic-map conversion, and dense matching; and (3) we verify the validation of the proposed SfM workflow by using spherical images recorded by both consumer-grade and professional cameras.\nThis paper is organized as follows. Section 2 presents the spherical image representation\nand camera model. Section 3 presents the key techniques for SfM-based 3D reconstruction. Section 4 proposes a 3D reconstruction pipeline for spherical images through incremental SfMbased image orientation and cubic-map-based dense matching, which is followed by the tests presented in Section 5. Finally, Section 6 concludes the work and future studies."
        },
        {
            "heading": "2. Spherical camera model",
            "text": "Camera model is the core to establish the projection relationship between 3D points in the\nobject space and corresponding points in the image plane. It is the core module to implement 3D reconstruction by using spherical images. In this subsection, the commonly utilized image representation and camera model would be presented."
        },
        {
            "heading": "2.1. Image representation",
            "text": "Spherical images can record the full surrounding environments from one camera exposure.\nAccording to the purpose of different usage, there are three formats for image representation of spherical images, as illustrated in Figure 2. The first one is the direct spherical representation, in which 3D points in the object space are projected onto 3D points on the sphere, as presented\nin Figure 2(a). Spherical representation has the advantages for panoramic navigation that has been extensively adopted in well-known street-view navigation services, e.g., the Google and Baidu street-view maps. To facilitate image processing and hardware storage, equirectangular representation is the second format that is implemented through the equirectangular projection of 3D spherical images to 2D planar images, as shown in Figure 2(b). Compared to perspective images, equirectangular images can be processed directly by using existing algorithms, e.g., SIFT for feature extraction and matching (Pagani and Stricker, 2011). However, because of the projection from 3D sphere to 2D plane, serious geometric distortions are introduced to the regions that are far from the sphere equator in the equirectangular representation. To alleviate the distortions, cubic-map representation is the third image representation that converts each spherical image into six perspective images, as shown in Figure 2(c). Since the normal format, equirectangular representation (ERP) has been extensively adopted for spherical images and used in open-source and commercial software packages, including OpenMVG, Pix4dMapper, and Agisoft Metashape. Therefore, this study focuses on the equirectangular representation of spherical image in the following sections."
        },
        {
            "heading": "2.2. Camera model",
            "text": "In the literature, there are three major camera models, i.e., the unified camera model,\ngeneral camera model, and multi-camera model. In the unified camera model (Mei and Rives, 2007), environment light rays intersect into a single point, i.e., the projection center of the mirror, as shown in Figure 3(a). The unified camera model obeys the theoretical projection that can model the real-world imaging errors. On the contrary, the general camera model (Scaramuzza et al., 2006) uses the Taylor polynomial function to fit the theoretical projection, which can adapt to varying spherical cameras. For the multi-camera rig, the multi-camera model has been designed to establish the projection of multi-camera sensors, which can be implemented by an individual camera model or a unit sphere camera model (Ji et al., 2014). The individual camera model is rigorous in formulating the imaging system as shown in Figure 3(b); the unit sphere camera model simplifies camera projection by using a straightforward formula as shown in Figure 3(c), which has also been used in the open-source and commercial software packages, e.g., OpenMVG and Pix4dMapper.\nRecently, the widely used spherical images are often collected by using a combination of\nmultiple fisheye camera lenses. For example, the consumer-grade camera Insta360 and the professional-grade camera Ladybug use the combined panoramic cameras with 2 and 6 fisheye lenses, respectively. According to the comparative analysis of the camera models (Ji et al., 2014) and considering the versatility of the SfM workflow, this study adopts the unit sphere camera model to establish the intrinsic imaging model of spherical cameras. The intrinsic parameters \ud835\udc3e of a spherical camera include the focal length \ud835\udc53 and the principal point (\ud835\udc50\ud835\udc65 , \ud835\udc50\ud835\udc66). For an unit sphere camera model with the radius \ud835\udc5f = 1, the focal length of the spherical camera is \ud835\udc53 = 1; the principal point coordinates are \ud835\udc50\ud835\udc65 = \ud835\udc4a/2 and \ud835\udc50\ud835\udc66 = \ud835\udc3b/2, in which \ud835\udc4a and \ud835\udc3b indicate the image width and height, respectively."
        },
        {
            "heading": "3. 3D reconstruction workflow of spherical images",
            "text": "The workflow for 3D reconstruction of spherical images is designed as shown in Figure 4.\nThe inputs are spherical images stored in equirectangular representation, and the outputs are dense point clouds, which can be further processed for textured models. In the workflow, there are three major components, i.e., image matching, image orientation, and dense matching. The key techniques involved in the SfM-based workflow are described as follows."
        },
        {
            "heading": "3.1. Camera imaging model",
            "text": "Camera imaging model establishes the geometric transformation between 3D points in the\nobject space and 2D points in the ERP image plane. The camera imaging model of spherical images is presented in Figure 5, in which Figure 5(a) presents the transformation between one 3D point \ud835\udc43 in the object space and its corresponding 3D point \ud835\udc5d on the sphere, and Figure 5(b) shows the transformation between the 3D point \ud835\udc5d and its corresponding 2D point on the ERP image plane. For the spherical image in Figure 5(a), there are two coordinate systems, i.e., the spherical geographic coordinate system \ud835\udc42 \u2212 \ud835\udc5f\ud835\udf03\ud835\udf11 and spherical Cartesian coordinate system \ud835\udc42 \u2212 \ud835\udc4b\ud835\udc4c\ud835\udc4d . In the spherical geographic coordinate system, the coordinate of point \ud835\udc5d is represented by using the longitude \ud835\udf03 and latitude \ud835\udf11; in the spherical Cartesian coordinate system, the coordinate of point \ud835\udc5d is presented by three coordinate terms (\ud835\udc65, \ud835\udc66, \ud835\udc67).\nSuppose that the line \ud835\udc3f shown by the dashed line in Figure 5(a) is the intersection of the\nEquatorial plane and the geodesic plane that passes point \ud835\udc5d. The longitude \ud835\udf03 and latitude \ud835\udf11 are defined as the intersection angle between \ud835\udc3f and \ud835\udc4d axis and the intersection angle between \ud835\udc3f and \ud835\udc42\ud835\udc43, respectively; 3D point \ud835\udc43 are projected onto 3D sphere point \ud835\udc5d = (\ud835\udc65, \ud835\udc66, \ud835\udc67)\ud835\udc47 . The\ntransformation between the coordinate system \ud835\udc42 \u2212 \ud835\udc5f\ud835\udf03\ud835\udf11 and \ud835\udc42 \u2212 \ud835\udc4b\ud835\udc4c\ud835\udc4d can be expressed using Equation (1), in which the sphere radius \ud835\udc5f is set as one for the unit sphere camera model.\ncos( )sin( )\nsin( )\ncos( )cos( )\nx\ny\nz\n \n\n \n        = \u2212           \n(1)\nAccording to the relationship between the 3D spherical geographic coordinate system and\nthe 2D ERP image plane coordinate system as illustrated in Figure 5(b), their transformation can be expressed by using Equation (2)\n*2\n*\nx x\ny y\nI c\nW\nc I\nH\n \n \n\u2212      =  \u2212      \n(2)\nwhere \ud835\udc3c\ud835\udc65 and \ud835\udc3c\ud835\udc66 are the image coordinates in the ERP image plane; \ud835\udc50\ud835\udc65 and \ud835\udc50\ud835\udc66 are the coordinates of the origin \ud835\udc42 of the ERP image plane; \ud835\udc4a and \ud835\udc3b are the image width and image height. Equations (1) and (2) establish the transformation between the 2D image plane coordinate system and 3D spherical coordinate system. Suppose that the pose of one spherical image is represented by the rotation matrix \ud835\udc45 and the translation vector \ud835\udc47 with respect to the world coordinate system \ud835\udc42 \u2212 \ud835\udc4b\ud835\udc4a\ud835\udc4c\ud835\udc4a\ud835\udc4d\ud835\udc4a, one 3D point \ud835\udc43\ud835\udc4a in \ud835\udc42 \u2212 \ud835\udc4b\ud835\udc4a\ud835\udc4c\ud835\udc4a\ud835\udc4d\ud835\udc4a can be transformed into the 3D spherical point \ud835\udc5d in \ud835\udc42 \u2212 \ud835\udc4b\ud835\udc4c\ud835\udc4d by using Equation (3). Thus, Equations (1), (2), and (3) consist of the imaging model of spherical cameras.\n*\n/\nWP R P T\np P P\n= +  =\n(3)"
        },
        {
            "heading": "3.2. Cost functions for bundle adjustment",
            "text": "Cost functions are used to measure the residuals during refining the initial estimation of\nunknown parameters. For spherical image orientation, there are two kinds of cost functions. The first one is used to measure the transformation error \ud835\udc36\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc60 between two 3D spherical points \ud835\udc5d1 = (\ud835\udc651, \ud835\udc661, \ud835\udc671) \ud835\udc47 and \ud835\udc5d2 = (\ud835\udc652, \ud835\udc662, \ud835\udc672) \ud835\udc47 under the estimated transformation parameters, which is mainly utilized for the relative pose estimation based on spherical 3D correspondences. In this study, the cost function \ud835\udc36\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc60 is expressed according to Equation (4)\n( ) 2\n2 1 2 2\n1 2\nT\ntrans\np Ep c\nEp Ep = + (4)\nwhere \ud835\udc38 indicates the estimated transformation between two spherical images; \u2016\u2219\u2016represents the vector length. The second one is used to measure the reprojection error \ud835\udc36\ud835\udc5f\ud835\udc5d\ud835\udc5f\ud835\udc57 from 3D scene points \ud835\udc4b\ud835\udc56 to camera \ud835\udc36\ud835\udc57, which would be used in the PnP (Perspective-n-Point) based absolute pose estimation as well as the local and global BA optimization. The cost function \ud835\udc36\ud835\udc5f\ud835\udc5d\ud835\udc5f\ud835\udc57 is represented by Equation (5)\n( ),rprj j i ijc P C X x= \u2212 (5) where \ud835\udc43(\ud835\udc36\ud835\udc57, \ud835\udc4b\ud835\udc56) represents the reprojection point \ud835\udc65\ud835\udc56\ud835\udc57 \u2032 = (\ud835\udc3c\ud835\udc65 , \ud835\udc3c\ud835\udc66) from 3D point \ud835\udc4b\ud835\udc56 to camera \ud835\udc36\ud835\udc57, which is calculated by using Equations (1)-(3); \ud835\udc65\ud835\udc56\ud835\udc57 is the image observation corresponding to 3D point \ud835\udc4b\ud835\udc56. These two cost functions would be used for parameter optimization."
        },
        {
            "heading": "3.3. Relative orientation using spherical correspondences",
            "text": "Relative orientation recovers the relative rotation \ud835\udc45 and translation \ud835\udc47 of two images by\nusing the coplanar constraint of corresponding rays. Similar to the plane perspective imaging, spherical imaging still maintains the colinear equation, in which the projection center, image point and object point are collinear. Thus, the coplanar constraint is also appliable to spherical images. Suppose that the camera intrinsic parameters \ud835\udc3e are known, the relative orientation parameters can be expressed as the essential matrix \ud835\udc38. For two corresponding rays \ud835\udc5d1 and \ud835\udc5d2, they satisfy the coplanar constraint as shown in Equation (6)\n2 1=0 Tp Ep (6)\nwhere \ud835\udc5d1 and \ud835\udc5d2 are the spherical coordinates of two corresponding image point \ud835\udc651 and \ud835\udc652, which are calculated according to the Equations (1) and (2). The geometrical meaning of the essential matrix \ud835\udc38 = [\ud835\udc47]\u00d7\ud835\udc45 is illustrated in Figure 6. When \ud835\udc5d1 and \ud835\udc5d2 are true corresponding points, the vectors \ud835\udc45\ud835\udc5d1, \ud835\udc5d2 and \ud835\udc47 are coplanar. In other words, \ud835\udc5d2 lies on the circular plane composed of the vector \ud835\udc45\ud835\udc5d1 and \ud835\udc47 (the normal vector of the circular plane is ?\u20d7? ). In this study, the 8-point algorithms (Hartley and Zisserman, 2003) is adopted to solve the \ud835\udc38 matrix, in which eight corresponding points form eight linear equations, and the linear system is solved through SVD (Singular Value Decomposition) (Umeyama, 1991).\nDue to outliers in initial corresponding matches, the hypothesis-verify framework based\non RANSAC (Fischler and Bolles, 1981) has been utilized for robust estimation, which depends on the error metric \ud835\udc52 and error threshold \ud835\udc52\ud835\udc5d. Different from the point-to-line perpendicular distance for perspective images, the corresponding ray \ud835\udc5d2 of \ud835\udc5d1 in the left image \ud835\udc421 lies on the circular plane that is defined by the normal vector ?\u20d7? and the projection center \ud835\udc422. Thus, this study adopts the vector-to-plane geodesic angular error metric (Pagani and Stricker, 2011), which is formulated according to Equation (7)\n( )( )1 2 1sin Te abs p Ep\u2212= (7) where \ud835\udc4e\ud835\udc4f\ud835\udc60(\u2219) indicates the absolute value. On the other hand, the unit of the error threshold \ud835\udc52\ud835\udc5d in image pixels should be converted to spherical angles that are used in the metric \ud835\udc52. In this study, the conversion is achieved based on Equation (8)\n( ) 2 max , a pe e W H  = (8)\nwhere 2\ud835\udf0b max(\ud835\udc4a,\ud835\udc3b)\u2044 indicates the scale factor of these two metrics; \ud835\udc52\ud835\udc4e is the error threshold in spherical angles. Thus, for an estimated matrix \ud835\udc38, the corresponding points \ud835\udc5d1 and \ud835\udc5d2 are labeled as one inlier if their angular error \ud835\udc52 < \ud835\udc52\ud835\udc4e.\nAfter the robust estimation based on RANSAC, four possible solutions can be obtained by\nthe decomposition of the essential matrix \ud835\udc38, and the cheirality check is then conducted to select the right solution. Instead of restricting the depth of resumed 3D scene points to be positive for perspective images, a consistent direction between 3D scene points \ud835\udc43 and 3D spherical points \ud835\udc5d is checked in the cheirality check. This constraint is formulated by \ud835\udc5d\ud835\udc47(\ud835\udc45\ud835\udc43 + \ud835\udc47) > 0. The solution with the largest number of consistent 3D scene points is selected as the initial \ud835\udc45 and \ud835\udc47, which are then refined in BA optimization with the cost function in Equation (4)."
        },
        {
            "heading": "3.4. Absolute orientation using 3D correspondences",
            "text": "The relative pose and 3D scene points can be recovered based on the relative orientation\nand the triangulation of spherical correspondences. In perspective imaging, the absolute pose of newly added images can be directly computed based on the 2D-3D correspondences using the PnP (Perspective-n-Point) algorithm. Since the colinear relationship among the projection center, image points and scene points is still satisfied, the PnP can also be utilized for absolute orientation, which is implemented by using the collinear constraint of 3D spherical point \ud835\udc5d and 3D scene point \ud835\udc43, as presented in Equation (9), in which \ud835\udc45 and \ud835\udc47 are the rotation matrix and translation vector of the spherical image. In this study, an initial solution is calculated by using three correspondences based on the P3P algorithm (Gao et al., 2003).\n  ( )=0p RP T  + (9)\nTo cope with outliers, the RANSAC algorithm is adopted for robust estimation. In contrast\nto the pixel distance residuals for perspective images, the angular error metric \ud835\udc52 is used to calculate the reprojection error as shown by Equation (10)\n( )( )( )1cos Te abs p RP T\u2212= + (10) where \ud835\udc45\ud835\udc43 + \ud835\udc47 indicates the projection point of the 3D scene point \ud835\udc43 in the camera system; \ud835\udc52 is the angle between \ud835\udc5d and \ud835\udc45\ud835\udc43 + \ud835\udc47. Similar to relative pose estimation, the error threshold \ud835\udc52\ud835\udc4e is also calculated by using Equation (8), and the correspondences are labeled as inliers if their error \ud835\udc52 < \ud835\udc52\ud835\udc4e. After obtaining the initial solution, BA optimization is then executed by using the cost function represented in Equation (5), which minimizes the reprojection error."
        },
        {
            "heading": "4. Implementation of the 3D reconstruction workflow",
            "text": "In the SfM-based workflow, the used key techniques are described in Section 3, and three\ncomponents, i.e., image matching, image orientation, and dense matching are designed and implemented as illustrated in Figure 4.\nImage matching is used to establish reliable matching points of two-view images, which\nis the basis of SfM-based 3D reconstruction. The pipeline of image matching includes three major steps, including feature extraction, feature matching and outlier removal. (1) Feature extraction. Although geometric distortions are introduced in the ERP projection,\nthis study uses the classic SIFT algorithm (Lowe, 2004) for feature extraction considering two main reasons. On the one hand, a majority of spherical images are recorded by fixing the camera roll (around Z axis) and pitch (around X axis) angles, such as the ground MMS system, which ensures that the consistency of geometric structures near the equator of the ERP images; on the other hand, SIFT is invariant to rotation and scale as well as has a high tolerance to the changes of imaging viewpoints and illuminations. In this implementation, the high-performance open-source library SIFTGPU (Wu, 2007) with default parameter settings has been used for feature extraction.\n(2) Feature matching. Before feature matching, effective matching pair selection can reduce\nthe high computational costs of exhaustive matching and avoid introducing false matches. Considering that most spherical cameras have built-in GNSS (Global Navigation Satellite System) sensors that can record the recording locations of images and at the same time, spherical images are usually acquired sequentially, this study uses the spatial constraint and sequential constraint to select matching pairs and guide feature matching. For feature matching, the SIFTGPU library is used by setting the ratio test and the maximum distance threshold with the values of 0.8 and 0.7, respectively.\n(3) Outlier removal. SIFT uses the local image patches around keypoints to calculate feature\ndescriptors, which would inevitably introduce outliers in the initial matches. Based on RANSAC-based essential matrix estimation as presented in Section 3.3, this study uses the angular error metric \ud835\udc52 between a 3D spherical point and its corresponding circular plane to eliminate outliers and obtain refined matches. In the \ud835\udc38 matrix estimation based on the RANSAC, the error threshold \ud835\udc52\ud835\udc5d is set as 4 pixels.\nImage orientation is implemented based on an incremental SfM engine by using reliable\nmatches, which iteratively solves and optimizes image poses and 3D points. The main steps of the workflow include seed image reconstruction, absolute orientation of the next-best image, and local or global BA optimization. (1) Seed image reconstruction. The initial seed images construct the basic model for the entire\nincremental SfM reconstruction. Seed image selection should consider both the number of matches and the intersection angle of images. The main steps include: (1) sort images in\nthe descending order according to the number of matches; (2) select the first image in the ordered sequence as the first image \ud835\udc3c\ud835\udc53\ud835\udc56\ud835\udc5f\ud835\udc60\ud835\udc61 in the seed images; (3) sort associated images of \ud835\udc3c\ud835\udc53\ud835\udc56\ud835\udc5f\ud835\udc60\ud835\udc61 in the descending order according to the number of matches; (4) select the first image from the ordered associated image as the second image \ud835\udc3c\ud835\udc60\ud835\udc52\ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc51 in the seed images; (5) conduct the relative orientation of these two seed images using spherical correspondences according to Section 3.3; (6) Iteratively execute steps (2)-(5) until two seed images are found, which satisfy the threshold of the number of matches \ud835\udc41\ud835\udc56\ud835\udc5b\ud835\udc59\ud835\udc56\ud835\udc52\ud835\udc5f and the intersection angle \ud835\udc34\ud835\udc61\ud835\udc5f\ud835\udc56. In this study, \ud835\udc41\ud835\udc56\ud835\udc5b\ud835\udc59\ud835\udc56\ud835\udc52\ud835\udc5f > 100, \ud835\udc34\ud835\udc61\ud835\udc5f\ud835\udc56 > 16 \u00b0. After the basic model construction by using seed images \ud835\udc3c\ud835\udc53\ud835\udc56\ud835\udc5f\ud835\udc60\ud835\udc61 and \ud835\udc3c\ud835\udc60\ud835\udc52\ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc51 , the global BA optimization is performed according to the cost function represented by Equation (5).\n(2) Absolute orientation of the next-best image. The next-best image represents the most\nrobust candidate image that can be registered to the reconstructed model. The selection of the next-best image is based on the number of observed 3D points and the image plane distribution of corresponding 2D feature points: (1) for all unoriented images, build the mapping relationships between feature points and reconstructed 3D points; (2) filter the images whose match number \ud835\udc41\ud835\udc5c\ud835\udc4f\ud835\udc60 < 30 and calculate the important value \ud835\udc46\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52 based on the image plane distribution of corresponding 2D feature points as described in the work of Schonberger and Frahm (2016); (3) sort remaining images in the descending order of \ud835\udc46\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52, and the candidate image with the highest score is the next-best image; (4) using the RANSAC-based absolute pose estimation as presented in Section 3.4, the rotation \ud835\udc45 and position \ud835\udc47 of the next-best image are calculated, which are then refined in BA optimization according to cost function in Equation (5); (5) the inliers of the next-best image are triangulated to resume more 3D scene points.\n(3) Local or global BA optimization. After the successful adding of the new next-best image,\nlocal or global BA optimization is executed according to two conditions: (1) the number of newly added images \ud835\udc41\ud835\udc56_\ud835\udc4e\ud835\udc51\ud835\udc51 > 3, local BA optimization is executed to refine only the poses of newly added images and their associated 3D points; (2) the number of newly added images \ud835\udc41\ud835\udc56_\ud835\udc4e\ud835\udc51\ud835\udc51 or the number of newly added 3D points \ud835\udc41\ud835\udc5d_\ud835\udc4e\ud835\udc51\ud835\udc51 is greater than a given threshold, global BA optimization is executed to optimize all images and 3D points. In this paper, the thresholds of \ud835\udc41\ud835\udc56_\ud835\udc4e\ud835\udc51\ud835\udc51 and \ud835\udc41\ud835\udc5d_\ud835\udc4e\ud835\udc51\ud835\udc51 are set to 10% of the number of reconstructed images and 3D points. According to the cost function in Equation (5), the goal is to minimize the reprojection error of 3D points\n( ) ,\n2\n1 1\nmin , j i\nn m\nij j i ij C X\ni j P C X x = = \u2212 (11)\nwhere \u2016\u2219\u2016 represents the vector L2 norm; \ud835\udf0c\ud835\udc56\ud835\udc57 is the visibility indicator of a 3D point \ud835\udc4b\ud835\udc56 in the image \ud835\udc36\ud835\udc57. When \ud835\udc4b\ud835\udc56 is visible in image \ud835\udc36\ud835\udc57, \ud835\udf0c\ud835\udc56\ud835\udc57 = 1; otherwise, \ud835\udf0c\ud835\udc56\ud835\udc57 = 0.\nDense matching is used to generate point clouds from the result of SfM reconstruction.\nTo make full use of existing dense matching algorithm, this study first converts the SfM results of spherical images into the format in the cubic-map image representation. (1) Cubic-map image generation. Assuming that the internal orientation matrix of the cubic-\nmap image \ud835\udc3c is represented by \ud835\udc3e\ud835\udc43, and its rotation matrix to the coordinate system of the spherical image is \ud835\udc45\ud835\udc43\ud835\udc46, the cubic-map image is generated with the following steps: (1) for an image point \ud835\udc65 \u2208 \ud835\udc3c , calculate its image coordinate and then perform homogeneous normalization by using \ud835\udc62 = \u220f\ud835\udc3e\ud835\udc43 \u22121\ud835\udc65, in which \u220f represents homogeneous normalization; (2) convert the homogeneous normalized coordinate \ud835\udc62 to the spherical coordinate system and obtain the corresponding spherical Cartesian coordinate \ud835\udc62\u2032 = \ud835\udc45\ud835\udc43\ud835\udc46 \ud835\udc47 \ud835\udc62; (3) according to Equations (1) and (2), calculate the pixel coordinate \ud835\udc65\u2032 in the ERP image for \ud835\udc62\u2032 , and linearly interpolate the gray value for the cubic-map image point \ud835\udc65. Through the steps (1)-(3), the cubic -map image is generated.\n(2) Pose update. Considering that the projection center of the cubic-map image coincides with\nthat of the spherical image, the rotation matrix and translation vector should be updated by using the relative rotation matrix \ud835\udc45\ud835\udc43\ud835\udc46, the pose of the generated cubic-map image can be obtained by using Equation (12).\n( ) P PS T\nP P\nR R R\nT R R T\n=  = \u2212 \u2212\n(12)\n(3) Dense matching. After the above pose transformation, this study uses a classic multi-view\nstereo (MVS) matching algorithm to generate dense point clouds. In the experiments, the PatchMatch (Sch\u00f6nberger et al., 2016) dense matching algorithm has been used."
        },
        {
            "heading": "5. Experiments and results",
            "text": "In the experiments, three spherical datasets are used for the performance evaluation of the\nproposed 3D reconstruction pipeline and comparison with other open-source and commercial software packages. First, we analyze the performance of the 3D reconstruction pipeline in terms of SIFT-based feature matching, SfM-based image orientation, and cubic-map converted dense matching. Second, three software packages that support spherical images are compared with the proposed 3D reconstruction, including the open-source package OpenMVG (Moulon et al., 2016) and commercial packages Pix4Dmapper (Pix4dMapper, 2022) and AgiSoft Metashape (Metashape, 2022). In this test, the proposed pipeline and OpenMVG are implemented by using the C++ programming language, and all experiments are conducted in a Windows desktop PC configured with 32 GB memory, an Intel Core i7-8700K 3.7GHz CPU (Central Processing Unit), and an NVIDIA GeForce GTX 1050Ti GPU (Graph Processing Unit)."
        },
        {
            "heading": "5.1. Test sites and datasets",
            "text": "In this study, three spherical datasets are used to evaluate the performance of the proposed\n3D reconstruction pipeline for spherical images. The detailed information for data acquisition and spherical images is presented in Table 1. The description of each dataset is listed as follows:\n\u26ab The first dataset is collected from a campus site, which contains a parterre surrounded\nby teaching buildings, as illustrated in Figure 8(a). For image acquisition, a Garmin VIRB 360 camera with two fisheye sensors is utilized, which stores recorded spherical images in the equirectangular representation format. The data acquisition campaign is conducted around the central parterre, and there are a total number of 37 images collected from this test site, whose resolution is 5640 by 2820 pixels.\n\u26ab The second dataset locates within a building dooryard that covers from its rooftop to\nthe inner aisles, as shown in Figure 8(b). There are some parterres on the rooftop, and the inner aisles connect different layers. For data acquisition, the same Garmin VIRB 360 camera as used in dataset 1 has been adopted. To accelerate data acquisition in the complex buildings, the camera has been handheld instead of ground fixed in\ndataset 1, and there are 279 spherical images collected from these test sites, which cover whole inner aisles.\n\u26ab The third dataset is collected from an MMS system that is mounted on a moving car."
        },
        {
            "heading": "5.2. Results of feature detection and matching",
            "text": "Feature detection and matching are implemented by using the traditional SIFT algorithm.\nIn this test, the GPU-based open-source library SIFTGPU has been used with default parameter configurations. For feature detection, the maximum number of detected features for each image is set as 8192, in which features with a larger scale would be retained. Figure 9 illustrates feature detection results for one image in datasets 1 and 2, as presented in Figure 9(a) and Figure 9(b), respectively. The detected features are rendered as yellow circles. We can see that the detected features can cover the whole image, especially for building facades that could provide stable features for images with varying viewpoints. In addition, for the low-texture sky regions, there are only a small fraction of detected features that are near the boundaries of clouds, which helps to reduce the introduction of false matches in feature matching.\nBefore feature matching, match pair selection is conducted to select image pairs that are\nspatially overlapped due to two reasons. On the one hand, it can filter unnecessary image pairs and decrease the time costs in feature matching; on the other hand, it can avoid the introduction of false matches that arise from non-overlapped image pairs. In practice, the strategy for match selection depends on the characteristics of data acquisition. For dataset 1, the exhaustive match pair selection strategy is used since images are recorded around a parterre. For datasets 2 and 3, the spatial and sequential constrained strategies are used as images are recorded with nonregular or corridor-like strategies, in which the maximum distance is set as 20 m in the spatial constraint, and the overlap image number is set as 10 in the sequential constraint. Figure 10 shows the match pair selection results, in which match pairs are presented using TCN graph.\nIt is shown that enough connections can be established between images, and there are 157, 4941, and 14836 match pairs selected for the three datasets.\nFeature matching is then executed and guided by the selected match pairs, in which the\ndistance ratio between the first and second closest descriptors and maximum distance between any two descriptors are set as 0.8 and 0.7, respectively. For outlier removal, cross-checking is used to reject false matches in the L2-norm-based initial feature matching, and essential matrixbased epipolar geometry is further used to remove remaining false matches, which is estimated through RANSAC with an error threshold configured as 4 pixels.\n(a) dataset 1 \u2013 initial match (149)\n(b) dataset 1 \u2013 refined match (116)\n(c) dataset 2 \u2013 initial match (291)\n(d) dataset 2 \u2013 refined match (202)\nFor visual analysis, Figure 11 illustrates the examples of feature matching for three image\npairs in the three datasets. Corresponding feature points are linked by using green lines; lines representing true matches are almost parallel; on the contrary, lines of false matches intersect others. It is shown that enough matches can be obtained from the used SIFT feature descriptors, and a majority of feature matches locate on building facades. For the three image pairs, there are a total number of 149, 291, and 373 initial matches. Due to repetitive patterns, false matches exist obviously in initial matches, especially for datasets 1 and 2. By using the essential matrixbased outlier removal strategy, these false matches can be filtered obviously. Finally, 116, 202, and 324 matches are retained for the three image pairs.\nFigure 12 illustrates the weight matrix of feature matching for the three datasets. The value\nin each cell indicates the number ratio between the feature matches of the corresponding match pair and the largest feature matches of all match pairs, which reflects the connection strength. We can see that all images have a strong connection with their neighboring images since larger values locate on the diagonal cells. In addition, extra connections have been established with other spatially closed images, which can be observed from the non-diagonal cells. In a word, all images are stringed into the image connection network."
        },
        {
            "heading": "5.3. Results of image orientation and dense matching",
            "text": "Image orientation is achieved based on an incremental SfM pipeline that aims to resume\ncamera poses and sparse 3D points by using established two-view feature matches. In this test,\nthree metrics are used to evaluate the performance of SfM-based image orientation, including efficiency, completeness, and precision. The metric efficiency indicates the time costs consumed in image orientation; the metric completeness indicates the number of resumed camera poses and 3D points; the metric precision represents the orientation accuracy, which is quantified by using the reprojection error in BA optimization.\nTable 2 presents the statistical result of image orientation for the three datasets. We can see\nthat all images can be successfully registered based on the proposed SfM solution, and there are 3044, 38995, and 290262 points generated for the three datasets, respectively. For the metric precision, the reprojection errors of the three datasets are almost consistent, which are 0.786, 0.798, and 0.581 pixels, respectively. Due to the usage of an incremental SfM solution, the time consumption increases dramatically with the increasing image numbers. For the three datasets, 0.25 min, 12.16 min, and 140.50 min are consumed in image orientation.\nFor visual analysis, Figure 13 presents the sparse point clouds generated from datasets 2\nand 3, in which images are rendered as blue rectangles. For dataset 2 as shown in Figure 13(a), spherical images are captured from four floors that are connected by inner stairs as illustrated by the top left spherical image. Noticeably, this test site includes building roofs, inner corridors, and underground tunnels. Due to the full FOV of special images, a reliable image connection can be established in this complex test site, which ensures the success of image orientation. For dataset 3 as shown in Figure 13(b), the length of the entire trajectory is about 8 kilometers, and all images are connected in image orientation. The successful image orientation of these two sites demonstrates the validation of the proposed solution for spherical images.\nInstead of revising existing or redesigning new algorithms, the proposed solution converts\nspherical images into perspective images according to the cubic-map representation. Existing dense matching algorithms can be directly applied to the generated perspective images. In the proposed 3D reconstruction pipeline of spherical images, dense point clouds are generated based on the PatchMatch-based stereo matching algorithm.\n(a)\nFigure 14 presents the dense point clouds generated from dataset 2. About 4.5 million 3D\npoints are reconstructed from this test site. We can see that the overall structure of this test site has been successfully reconstructed, including building facades, underground corridors, and inner facilities, which are verified by the subfigures in Figure 14(a), Figure 14(b), and Figure 14(c). Especially for inner facilities, we can observe the overall layout and detailed structure of stairs and handrails, as presented in Figure 14(c). In addition, there is only one trajectory during data acquisition in underground corridors. The detailed structures of these regions verify the advantages of spherical images on 3D reconstruction in the indoor environment."
        },
        {
            "heading": "5.4. Comparison with other software packages",
            "text": "Three software packages that support spherical images are evaluated and compared with\nthe proposed 3D reconstruction pipeline. The selected software packages consist of one opensource software OpenMVG, and two commercial software Metashape and Pix4Dmapper. In this test, the performance of image orientation would be evaluated for two main reasons. On the one hand, it determines the success of 3D reconstruction; on the other hand, some software packages do not provide dense matching modules, e.g., OpenMVG. Table 3 presents detailed information for these three software packages.\nTable 4 presents the statistical results of image orientation for the three datasets. It is shown\nthat OpenMVG fails to reconstruct all three datasets. For datasets 1 and 2, the initialization of two seed images failed; for dataset 3, the subsequent SfM reconstruction failed. Compared with OpenMVG, the robustness of the other two commercial software packages Metashape and Pix4Dmapper is higher. Pix4Dmapper can register a majority of images for datasets 1 and 2 with the highest precision although the failure in dataset 3, whose precision is 0.359 pixels and 0.341 pixels for datasets 1 and 2, respectively. Metashape can register all images in the three datasets with the highest efficiency, which are 0.16 min, 1.20 min, and 8.90 min, respectively. The main reason is the usage of cluster-based parallel ISfM. Considering the proposed solution, we can see that it can register all images with higher precision when compared with Metashape; its efficiency, however, is lower, which are 0.25 min, 12.16 min, and 140.50 min, respectively. The main reason is the drawback of the used incremental SfM solution, which can be solved by using a divide-and-conquer strategy, such as our previous work (Jiang et al., 2022b). Thus, the comparison results reveal that 3D reconstruction of spherical images is an unsolved issue in both open-source and commercial software packages, and more attention should be paid to promoting the development of well-designed algorithms and toolkits.\nTable 4. The statistical results of image orientation for the three datasets in terms of efficiency, completeness, and precision. The values in the bracket indicate the number of registered images in the image orientation result.\nMetric Method Dataset 1 Dataset 2 Dataset 3\nEfficiency (min)\nOpenMVG \u2014 \u2014 \u2014 Metashape 0.16 1.20 8.90 Pix4Dmapper 0.88 9.22 \u2014 Ours 0.25 12.16 140.50\nPrecision (pixel)\nOpenMVG \u2014 \u2014 \u2014 Metashape 5.260 4.450 0.958 Pix4Dmapper 0.359 0.341 \u2014 Ours 0.786 0.798 0.581\nCompleteness\nOpenMVG \u2014 \u2014 \u2014\nMetashape 5,540 (37) 43,120 (279) 295,390 (1,937) Pix4Dmapper 8,485 (36) 64,135 (238) \u2014\nOurs 3,044 (37) 38,995 (279) 290,262 (1,937)"
        },
        {
            "heading": "6. Conclusions",
            "text": "Spherical images can record all surrounding environments by using one camera exposure.\nIn contrast to perspective images with limited FOV, spherical images can cover the whole scene and have been increasingly used for 3D modeling in street-view and indoor environments. In this paper, we give a review of 3D reconstruction of spherical images according to the classical\nprocessing pipeline, i.e., feature detection and matching, image orientation to resume camera poses, and dense matching to generate point clouds. For feature detection and matching, the serious geometric distortions caused by equirectangular projection pose difficulties in feature description. Both traditional and newly designed algorithms to cope with distortions have been reviewed, as well as the deep learning-based methods. For image orientation, we reviewed the SfM-based offline methods and SLAM-based online methods and conducted a statistic of opensource and commercial software packages. For dense matching, we reviewed depth prediction from single images, traditional methods for cubic-map images, and recent deep learning CNN for two-view and multi-view stereo matching.\nAccording to the differences between perspective and spherical images, we also proposed\na 3D reconstruction pipeline through incremental SfM-based image orientation and cubic-map conversion-based dense matching. By using real spherical images recorded by both consumergrade and professional spherical cameras, the validation of the proposed pipeline has been evaluated and compared with open-source and commercial software packages. The test results demonstrate that spherical images can be promising data sources for 3D reconstruction in complex environments, e.g., urban canyons and indoor corridors. In addition, the comparison with other software packages verified that the proposed pipeline is robust and effective for 3D reconstruction of spherical images, which provides clues to guide further research."
        },
        {
            "heading": "Acknowledgment",
            "text": "This research was funded by the National Natural Science Foundation of China under\nGrant [number 42001413] and the Hong Kong Scholars Program under Grant [number 2021\u2043 114]."
        },
        {
            "heading": "9, 278.",
            "text": "Jiang, S., Jiang, W., Wang, L., 2022a. Unmanned Aerial Vehicle-Based Photogrammetric 3D\nMapping: A survey of techniques, applications, and challenges. IEEE Geoscience and Remote\nSensing Magazine 10, 135-171.\nJiang, S., Li, Q., jiang, W., Chen, W., 2022b. Parallel Structure From Motion for UAV Images via\nWeighted Connected Dominating Set. IEEE Transactions on Geoscience and Remote Sensing"
        },
        {
            "heading": "60, 1-13.",
            "text": "Jiang, S., Li, Y., Weng, D., You, K., Chen, W., 2023. 3D reconstruction of spherical images: A\nreview of techniques, applications, and prospects. arXiv preprint arXiv:2302.04495.\nLi, G., Lu, X., Lin, B., Zhou, L., Lv, G., 2022. Automatic Positioning of Street Objects Based on\nSelf-Adaptive Constrained Line of Bearing from Street-View Images. ISPRS International\nJournal of Geo-Information 11, 253.\nLi, Q., Huang, H., Yu, W., Jiang, S., 2023. Optimized Views Photogrammetry: Precision\nAnalysis and a Large-Scale Case Study in Qingdao. IEEE Journal of Selected Topics in Applied\nEarth Observations and Remote Sensing 16, 1144-1159.\nLowe, D.G., 2004. Distinctive image features from scale-invariant keypoints. International\njournal of computer vision 60, 91-110.\nMei, C., Rives, P., 2007. Single view point omnidirectional camera calibration from planar grids,\nProceedings 2007 IEEE International Conference on Robotics and Automation. IEEE, pp. 3945-\n3950.\nMetashape, A., 2022. Agisoft Metashape.\nMicusik, B., Kosecka, J., 2009. Piecewise planar city 3D modeling from street view panoramic\nsequences, 2009 IEEE Conference on Computer Vision and Pattern Recognition. IEEE, pp. 2906-\n2912.\nMoulon, P., Monasse, P., Perrot, R., Marlet, R., 2016. Openmvg: Open multiple view geometry,\nInternational Workshop on Reproducible Research in Pattern Recognition. Springer, pp. 60-74.\nPagani, A., Stricker, D., 2011. Structure from motion using full spherical panoramic cameras,\n2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops). IEEE,\npp. 375-382.\nPix4dMapper, 2022. Pix4dMapper.\nScaramuzza, D., Martinelli, A., Siegwart, R., 2006. A flexible technique for accurate\nomnidirectional camera calibration and structure from motion, Fourth IEEE International\nConference on Computer Vision Systems (ICVS'06). IEEE, pp. 45-45.\nSchonberger, J.L., Frahm, J.-M., 2016. Structure-from-motion revisited, Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, pp. 4104-4113.\nSch\u00f6nberger, J.L., Zheng, E., Frahm, J.-M., Pollefeys, M., 2016. Pixelwise view selection for\nunstructured multi-view stereo, European Conference on Computer Vision. Springer, pp. 501-\n518.\nTaira, H., Inoue, Y., Torii, A., Okutomi, M., 2015. Robust feature matching for distorted\nprojection by spherical cameras. IPSJ Transactions on Computer Vision and Applications 7, 84-\n88.\nTorii, A., Havlena, M., Pajdla, T., 2009. From google street view to 3d city models, 2009 IEEE\n12th international conference on computer vision workshops, ICCV Workshops. IEEE, pp.\n2188-2195.\nTorii, A., Imiya, A., Ohnishi, N., 2005. Two-and three-view geometry for spherical cameras,\nProceedings of the sixth workshop on omnidirectional vision, camera networks and non-\nclassical cameras. Citeseer, pp. 81-88.\nUmeyama, S., 1991. Least-squares estimation of transformation parameters between two point\npatterns. IEEE Transactions on Pattern Analysis and Machine Intelligence 13, 376-380.\nVetrivel, A., Gerke, M., Kerle, N., Vosselman, G., 2015. Identification of damage in buildings\nbased on gaps in 3D point clouds from very high resolution oblique airborne images. ISPRS\njournal of photogrammetry and remote sensing 105, 61-78.\nWang, Y., Cai, S., Li, S.-J., Liu, Y., Guo, Y., Li, T., Cheng, M.-M., 2018. Cubemapslam: A\npiecewise-pinhole monocular fisheye slam system, Asian Conference on Computer Vision.\nSpringer, pp. 34-49.\nWu, C., 2007. SiftGPU: A GPU implementation of scale invariant feature transform (SIFT).\nXiong, B., Jancosek, M., Elberink, S.O., Vosselman, G., 2015. Flexible building primitives for 3D\nbuilding modeling. ISPRS Journal of Photogrammetry and Remote Sensing 101, 275-290.\nYu, D., Ji, S., Liu, J., Wei, S., 2021. Automatic 3D building reconstruction from multi-view aerial\nimages with deep learning. ISPRS Journal of Photogrammetry and Remote Sensing 171, 155-\n170.\nZhang, X., Zhao, P., Hu, Q., Ai, M., Hu, D., Li, J., 2020. A UAV-based panoramic oblique\nphotogrammetry (POP) approach using spherical projection. ISPRS Journal of\nPhotogrammetry and Remote Sensing 159, 198-219.\nZhang, Z., Rebecq, H., Forster, C., Scaramuzza, D., 2016. Benefit of large field-of-view cameras\nfor visual odometry, 2016 IEEE International Conference on Robotics and Automation (ICRA).\nIEEE, pp. 801-808.\nZhao, Q., Feng, W., Wan, L., Zhang, J., 2015. SPHORB: A fast and robust binary feature on the\nsphere. International journal of computer vision 113, 143-159."
        }
    ],
    "title": "3D Reconstruction of Spherical Images based on Incremental Structure from Motion",
    "year": 2023
}