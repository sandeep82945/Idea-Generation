{
    "abstractText": "Pre-trained multi-modal vision-language models (VLMs) are becoming increasingly popular due to their exceptional performance on downstream vision applications, particularly in the fewand zero-shot settings. However, selecting the best-performing VLM for some downstream applications is non-trivial, as it is dataset and task-dependent. Meanwhile, the exhaustive evaluation of all available VLMs on a novel application is not only time and computationally demanding but also necessitates the collection of a labeled dataset for evaluation. As the number of open-source VLM variants increases, there is a need for an efficient model selection strategy that does not require access to a curated evaluation dataset. This paper proposes a novel task and benchmark for efficiently evaluating VLMs\u2019 zero-shot performance on downstream applications without access to the downstream task dataset. Specifically, we introduce a new task LOVM: LanguageOnly Vision Model Selection, where methods are expected to perform both model selection and performance prediction based solely on a text description of the desired downstream application. We then introduced an extensive LOVM benchmark consisting of ground-truth evaluations of 35 pre-trained VLMs and 23 datasets, where methods are expected to rank the pre-trained VLMs and predict their zero-shot performance.",
    "authors": [
        {
            "affiliations": [],
            "name": "Orr Zohar"
        },
        {
            "affiliations": [],
            "name": "Shih-Cheng Huang"
        },
        {
            "affiliations": [],
            "name": "Kuan-Chieh Wang"
        },
        {
            "affiliations": [],
            "name": "Serena Yeung"
        }
    ],
    "id": "SP:e3e33e8bee83f0b8323dee270e81f2b6f315f107",
    "references": [
        {
            "authors": [
                "Andrea Agostinelli",
                "Michal P\u00e1ndy",
                "Jasper Uijlings",
                "Thomas Mensink",
                "Vittorio Ferrari"
            ],
            "title": "How stable are transferability metrics evaluations",
            "venue": "Computer Vision \u2013 ECCV",
            "year": 2022
        },
        {
            "authors": [
                "Hangbo Bao",
                "Wenhui Wang",
                "Li Dong",
                "Qiang Liu",
                "Owais Khan Mohammed",
                "Kriti Aggarwal",
                "Subhojit Som",
                "Songhao Piao",
                "Furu Wei"
            ],
            "title": "VLMo: Unified vision-language pre-training with mixture-ofmodality-experts",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Jiefeng Chen",
                "Frederick Liu",
                "Besim Avci",
                "Xi Wu",
                "Yingyu Liang",
                "Somesh Jha"
            ],
            "title": "Detecting errors and estimating accuracy on unlabeled data with self-training ensembles",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Gong Cheng",
                "Junwei Han",
                "Xiaoqiang Lu"
            ],
            "title": "Remote sensing image scene classification: Benchmark and state of the art",
            "venue": "Proceedings of the IEEE,",
            "year": 2017
        },
        {
            "authors": [
                "M. Cimpoi",
                "S. Maji",
                "I. Kokkinos",
                "S. Mohamed",
                "A. Vedaldi"
            ],
            "title": "Describing textures in the wild",
            "venue": "In Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2014
        },
        {
            "authors": [
                "Adam Coates",
                "Andrew Ng",
                "Honglak Lee"
            ],
            "title": "An analysis of single-layer networks in unsupervised feature learning",
            "venue": "In Proceedings of the fourteenth international conference on artificial intelligence and statistics,",
            "year": 2011
        },
        {
            "authors": [
                "Yin Cui",
                "Zeqi Gu",
                "Dhruv Mahajan",
                "Laurens van der Maaten",
                "Serge Belongie",
                "Ser-Nam Lim"
            ],
            "title": "Measuring dataset granularity, 2019",
            "venue": "URL https://arxiv.org/abs/1912.10154",
            "year": 1912
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "IEEE conference on computer vision and pattern recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Weijian Deng",
                "Liang Zheng"
            ],
            "title": "Are labels always necessary for classifier accuracy evaluation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "Nan Ding",
                "Xi Chen",
                "Tomer Levinboim",
                "Soravit Changpinyo",
                "Radu Soricut"
            ],
            "title": "Pactran: Pac-bayesian metrics for estimating the transferability of pretrained models to classification tasks",
            "year": 2022
        },
        {
            "authors": [
                "Will Cukierski"
            ],
            "title": "Challenges in representation learning: Facial expression recognition",
            "year": 2013
        },
        {
            "authors": [
                "Kawin Ethayarajh",
                "Yejin Choi",
                "Swabha Swayamdipta"
            ],
            "title": "Understanding dataset difficulty with V-usable information, 2021",
            "venue": "URL https://arxiv.org/abs/2110.08420",
            "year": 2021
        },
        {
            "authors": [
                "M. Everingham",
                "L. Van Gool",
                "C.K.I. Williams",
                "J. Winn",
                "A. Zisserman"
            ],
            "title": "The PASCAL Visual Object Classes Challenge",
            "year": 2007
        },
        {
            "authors": [
                "Sabri Eyuboglu",
                "Maya Varma",
                "Khaled Kamal Saab",
                "Jean-Benoit Delbrouck",
                "Christopher Lee-Messer",
                "Jared Dunnmon",
                "James Zou",
                "Christopher Re"
            ],
            "title": "Domino: Discovering systematic errors with cross-modal embeddings",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Alex Fang",
                "Gabriel Ilharco",
                "Mitchell Wortsman",
                "Yuhao Wan",
                "Vaishaal Shankar",
                "Achal Dave",
                "Ludwig Schmidt"
            ],
            "title": "Data determines distributional robustness in contrastive language image pretraining (clip), 2022",
            "venue": "URL https://arxiv.org/abs/2205.01397",
            "year": 2022
        },
        {
            "authors": [
                "Leo Feng",
                "Mohamed Osama Ahmed",
                "Hossein Hajimirsadeghi",
                "Amir Abdi"
            ],
            "title": "Towards better selective classification, 2022",
            "venue": "URL https://arxiv.org/abs/2206.09034",
            "year": 2022
        },
        {
            "authors": [
                "Ronald A Fisher"
            ],
            "title": "The use of multiple measurements in taxonomic problems",
            "venue": "Annals of eugenics,",
            "year": 1936
        },
        {
            "authors": [
                "Andreas Geiger",
                "Philip Lenz",
                "Christoph Stiller",
                "Raquel Urtasun"
            ],
            "title": "Vision meets robotics: The kitti dataset",
            "venue": "International Journal of Robotics Research (IJRR),",
            "year": 2013
        },
        {
            "authors": [
                "Patrick Helber",
                "Benjamin Bischke",
                "Andreas Dengel",
                "Damian Borth"
            ],
            "title": "Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification",
            "venue": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,",
            "year": 2019
        },
        {
            "authors": [
                "Saachi Jain",
                "Hannah Lawrence",
                "Ankur Moitra",
                "Aleksander Madry"
            ],
            "title": "Distilling model failures as directions in latent space, 2022",
            "venue": "URL https://arxiv.org/abs/2206.14754",
            "year": 2022
        },
        {
            "authors": [
                "Chao Jia",
                "Yinfei Yang",
                "Ye Xia",
                "Yi-Ting Chen",
                "Zarana Parekh",
                "Hieu Pham",
                "Quoc V. Le",
                "Yun-Hsuan Sung",
                "Zhen Li",
                "Tom Duerig"
            ],
            "title": "Scaling up visual and vision-language representation learning with noisy text",
            "venue": "supervision. CoRR,",
            "year": 2021
        },
        {
            "authors": [
                "Yiding Jiang",
                "Vaishnavh Nagarajan",
                "Christina Baek",
                "J Zico Kolter"
            ],
            "title": "Assessing generalization of SGD via disagreement",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Justin Johnson",
                "Bharath Hariharan",
                "Laurens Van Der Maaten",
                "Li Fei-Fei",
                "C Lawrence Zitnick",
                "Ross Girshick"
            ],
            "title": "Clevr: A diagnostic dataset for compositional language and elementary visual reasoning",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Jonathan Krause",
                "Michael Stark",
                "Jia Deng",
                "Li Fei-Fei"
            ],
            "title": "3d object representations for fine-grained categorization",
            "venue": "In 4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13), Sydney, Australia,",
            "year": 2013
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny",
            "year": 2009
        },
        {
            "authors": [
                "Yann LeCun",
                "Corinna Cortes",
                "CJ Burges"
            ],
            "title": "Mnist handwritten digit database",
            "venue": "ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist,",
            "year": 2010
        },
        {
            "authors": [
                "Weixin Liang",
                "Yuhui Zhang",
                "Yongchan Kwon",
                "Serena Yeung",
                "James Zou"
            ],
            "title": "Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning",
            "venue": "In NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Yuval Netzer",
                "Tao Wang",
                "Adam Coates",
                "Alessandro Bissacco",
                "Bo Wu",
                "Andrew Y. Ng"
            ],
            "title": "Reading digits in natural images with unsupervised feature learning",
            "venue": "In NIPS Workshop on Deep Learning and Unsupervised Feature Learning,",
            "year": 2011
        },
        {
            "authors": [
                "Maria-Elena Nilsback",
                "Andrew Zisserman"
            ],
            "title": "Automated flower classification over a large number of classes",
            "venue": "In Indian Conference on Computer Vision, Graphics and Image Processing,",
            "year": 2008
        },
        {
            "authors": [
                "Michal P\u00e1ndy",
                "Andrea Agostinelli",
                "Jasper Uijlings",
                "Vittorio Ferrari",
                "Thomas Mensink"
            ],
            "title": "Transferability estimation using bhattacharyya class separability",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Omkar M. Parkhi",
                "Andrea Vedaldi",
                "Andrew Zisserman",
                "C.V. Jawahar"
            ],
            "title": "Cats and dogs",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2012
        },
        {
            "authors": [
                "Emmanouil Platanios",
                "Hoifung Poon",
                "Tom M Mitchell",
                "Eric J Horvitz"
            ],
            "title": "Estimating accuracy from unlabeled data: A probabilistic logic approach",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Emmanouil Antonios Platanios",
                "Avinava Dubey",
                "Tom Mitchell"
            ],
            "title": "Estimating accuracy from unlabeled data: A bayesian approach",
            "venue": "Proceedings of The 33rd International Conference on Machine Learning,",
            "year": 2016
        },
        {
            "authors": [
                "Michal P\u00e1ndy",
                "Andrea Agostinelli",
                "Jasper Uijlings",
                "Vittorio Ferrari",
                "Thomas Mensink"
            ],
            "title": "Transferability estimation using bhattacharyya class separability",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark",
                "Gretchen Krueger",
                "Ilya Sutskever"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "CoRR, abs/2103.00020,",
            "year": 2021
        },
        {
            "authors": [
                "Olivier Risser-Maroix",
                "Benjamin Chamand"
            ],
            "title": "What can we learn by predicting accuracy",
            "venue": "In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),",
            "year": 2023
        },
        {
            "authors": [
                "Peter J. Rousseeuw"
            ],
            "title": "Silhouettes: A graphical aid to the interpretation and validation of cluster analysis",
            "venue": "Journal of Computational and Applied Mathematics,",
            "year": 1987
        },
        {
            "authors": [
                "Florian Scheidegger",
                "Roxana Istrate",
                "Giovanni Mariani",
                "Luca Benini",
                "Costas Bekas",
                "Cristiano Malossi"
            ],
            "title": "Efficient image dataset classification difficulty estimation for predicting deep-learning accuracy, 2018",
            "venue": "URL https://arxiv.org/abs/1803.09588",
            "year": 2018
        },
        {
            "authors": [
                "Johannes Stallkamp",
                "Marc Schlipsing",
                "Jan Salmen",
                "Christian Igel"
            ],
            "title": "The german traffic sign recognition benchmark: a multi-class classification competition",
            "venue": "In The 2011 international joint conference on neural networks,",
            "year": 2011
        },
        {
            "authors": [
                "Xiaoxiao Sun",
                "Yunzhong Hou",
                "Weijian Deng",
                "Hongdong Li",
                "Liang Zheng"
            ],
            "title": "Ranking models in unlabeled new environments",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar",
                "Aurelien Rodriguez",
                "Armand Joulin",
                "Edouard Grave",
                "Guillaume Lample"
            ],
            "title": "Llama: Open and efficient foundation language models, 2023",
            "venue": "URL https://arxiv.org/abs/2302.13971",
            "year": 2023
        },
        {
            "authors": [
                "Thomas Unterthiner",
                "Daniel Keysers",
                "Sylvain Gelly",
                "Olivier Bousquet",
                "Ilya Tolstikhin"
            ],
            "title": "Predicting neural network accuracy from weights, 2020",
            "venue": "URL https://arxiv.org/abs/2002",
            "year": 2002
        },
        {
            "authors": [
                "Chris van der Lee",
                "Thiago Castro Ferreira",
                "Chris Emmery",
                "Travis Wiltshire",
                "Emiel Krahmer"
            ],
            "title": "Neural data-to-text generation based on small datasets: Comparing the added value of two semisupervised learning approaches on top of a large language model, 2022",
            "venue": "URL https://arxiv. org/abs/2207.06839",
            "year": 2022
        },
        {
            "authors": [
                "Bastiaan S Veeling",
                "Jasper Linmans",
                "Jim Winkens",
                "Taco Cohen",
                "Max Welling"
            ],
            "title": "Rotation equivariant CNNs for digital pathology",
            "year": 2018
        },
        {
            "authors": [
                "Wenhui Wang",
                "Hangbo Bao",
                "Li Dong",
                "Johan Bjorck",
                "Zhiliang Peng",
                "Qiang Liu",
                "Kriti Aggarwal",
                "Owais Khan Mohammed",
                "Saksham Singhal",
                "Subhojit Som",
                "Furu Wei"
            ],
            "title": "Image as a foreign language: BEiT pretraining for vision and vision-language tasks",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Lauren J. Wong",
                "Sean McPherson",
                "Alan J. Michaels"
            ],
            "title": "Assessing the value of transfer learning metrics for rf domain adaptation, 2022",
            "year": 2022
        },
        {
            "authors": [
                "J. Xiao",
                "J. Hays",
                "K.A. Ehinger",
                "A. Oliva",
                "A. Torralba"
            ],
            "title": "Sun database: Large-scale scene recognition from abbey to zoo",
            "venue": "In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition,",
            "year": 2010
        },
        {
            "authors": [
                "Xiaohua Zhai",
                "Joan Puigcerver",
                "Alexander Kolesnikov",
                "Pierre Ruyssen",
                "Carlos Riquelme",
                "Mario Lucic",
                "Josip Djolonga",
                "Andre Susano Pinto",
                "Maxim Neumann",
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Olivier Bachem",
                "Michael Tschannen",
                "Marcin Michalski",
                "Olivier Bousquet",
                "Sylvain Gelly",
                "Neil Houlsby"
            ],
            "title": "A large-scale study of representation learning with the visual task adaptation benchmark, 2020",
            "year": 2020
        },
        {
            "authors": [
                "Yuhui Zhang",
                "Jeff Z HaoChen",
                "Shih-Cheng Huang",
                "Kuan-Chieh Wang",
                "James Zou",
                "Serena Yeung"
            ],
            "title": "Diagnosing and rectifying vision models using language",
            "venue": "arXiv preprint arXiv:2302.04269,",
            "year": 2023
        },
        {
            "authors": [
                "Flowers102 [Nilsback",
                "Zisserman"
            ],
            "title": "102 classification natural image CIFAR100 [Krizhevsky et al., 2009] 100 classification natural image GTSRB",
            "venue": "[Stallkamp et al.,",
            "year": 2008
        },
        {
            "authors": [
                "Fisher criterion",
                "\u03c6fisher"
            ],
            "title": "The Fisher criterion [Fisher, 1936] has been widely used as a dataset granularity measure and has recently been shown to be effective for classification by Cui et al",
            "year": 2019
        },
        {
            "authors": [
                "logit Feng"
            ],
            "title": "2022] to determine dataset difficulty. Image Embedding Distance. Another common approach is estimating the difference in distribution between the test and train sets Scheidegger et al",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Advancements in artificial intelligence (AI) have permeated diverse sectors, but applications in areas such as medicine or those with long-tail distributions often struggle to collect the sizable training datasets required for the standard supervised learning framework. Pre-trained vision-language models (VLMs) offer a promising solution, demonstrating robust performance on diverse downstream vision tasks without the necessity of large-scale labeled datasets [Radford et al., 2021, Jia et al., 2021]. However, the performance of VLMs can vary substantially across different tasks and domains, which undermines the reliance solely on benchmark dataset performance for effective VLM selection. Consequently, users aiming to select a VLM for custom downstream applications frequently face a predicament: the lack of established performance rankings for these specific, non-conventional tasks.\nAs the number of pre-trained VLMs increases (see Fig. 1, [Ilharco et al., 2021] and App. Tab. 4), the challenge of model selection escalates. Exhaustive evaluation of all available VLMs on a novel application is not only time and computationally demanding but also necessitates the collection of a labeled dataset for evaluation. However, many users lack the resources or technical proficiency to collect and label an evaluation dataset and subsequently evaluate all available VLMs. Consequently, the development of methods that efficiently select the most suitable model for a given task without relying on access to the downstream task dataset has become critically important.\nPreprint. Under review.\nar X\niv :2\n30 6.\n08 89\n3v 1\n[ cs\n.C V\n] 1\n5 Ju\nRecent studies have demonstrated that text embeddings from VLMs can be used as a proxy for their corresponding image embeddings in various downstream tasks, including classification and error slice discovery [Zhang et al., 2023, Eyuboglu et al., 2022, Jain et al., 2022]. Specifically, although Liang et al. [2022] has shown that there exists a modality gap between text and image embeddings generated from VLMs, the geometry of this modality gap permits cross-modality transferability. This phenomenon allows text to serve as a proxy to corresponding images and vice versa. Therefore we aim to explore the utilization of cross-modality transferability to estimate VLM performance on a novel vision task using text alone.\nHerein, we propose a novel problem setting - LanguageOnly VLM selection (LOVM) as a novel model selection task. In the LOVM task, methods are expected to select the optimal VLM and predict its expected performance given only a text description of a downstream vision task/application, (see Fig. 2). Importantly, LOVM eliminate the need to gather, organize, and annotate custom datasets, thereby greatly simplifying the model selection process for downstream users. To facilitate the development of LOVM methods in the future, we collected a large dataset of ground-truth evaluations of 35 pre-trained VLMs on 23 datasets. We then introduce the appropriate evaluation protocol and method quality metrics to allow the evaluation and comparison of LOVM methods in the future.\nTo show that such a challenging task is possible, we provide simple baselines that utilize readily-available large language models to generate \u2018text datasets\u2019 for a given vision task. By utilizing the cross-modality transferability phenomenon, we show how simple baselines can be derived by utilizing the cross-modality transferability phe-\nnomenon. Our results show that text prompting may be an effective means of estimating zero-shot performance, showing that such a challenging task is possible while providing a baseline for future research.\nThe contributions of this study can be summarized as follows:\n\u2022 We propose a novel problem setting, LOVM: Language-Only VLM selection and performance prediction. LOVM methods are expected to perform both model selection and performance prediction using only a text description of the desired zero-shot application.\n\u2022 We provide a benchmark consisting of 35 pre-trained VLMs and 23 datasets. We evaluated all dataset-VLM combinations and reported their corresponding performance, which is used as the ground truth when training and evaluating LOVM methods. We also introduce the corresponding evaluation metrics and protocols.\n\u2022 In developing the LOVM baselines, we introduce several novel methodological contributions, such as using LLM models to generate text proxies for images. Our text-based methods outperform simple baselines - such as ImageNet benchmarking, showing the promise of the direction of LOVM.\n\u2022 By analyzing text-based score trends, we draw insights into VLM behavior and shed light on why ResNet-based models perform better on datasets with low visual diversity.\nOur code and dataset are available at https://github.com/orrzohar/LOVM"
        },
        {
            "heading": "2 Language-Only Vision Model Selection",
            "text": "In order to train and evaluate LOVM methods, we need the ground-truth (GT) zero-shot performance, i.e., image-based evaluation of many VLMs (differing by architecture and pre-training) on many tasks and datasets. Once collected, we can develop and evaluate LOVM methods. An ideal LOVM method should be able to select the best performing VLM for a downstream vision task and estimate the performance directly from text embeddings, eliminating the cost of image-based model selection. The VLM, dataset selection criterion, and dataset collection procedure are detailed in Sec. 2.1. Finally, the evaluation protocol of LOVM methods is described in Sec. 2.2. For a discussion on why we only evaluate zero-shot performace, see App. Sec. D.\nBackground. We first recap how VLMs are used as in zero-shot vision tasks. Given a pre-trained VLM v, along with an image X \u2208 X or text Y \u2208 Y input, we can obtain their L2-normalized embeddings x or y from the image encoder fx : X 7\u2192 Rn or the text encoder fy : Y 7\u2192 Rn, where n is the dimension of the shared multi-modal embedding space. To use a model v on a particular task, one encodes the class prompts, Y c for class c using the model\u2019s text encoder, producing the class embeddings yc = fy(Y c). To produce the final class prediction, one calculates the cosine similarity of an image embedding with all the corresponding text embeddings to predict the class logits.\nTask Definition In the LOVM task, for any downstream application/dataset d, methods are given a set of pre-trained VLMs, V = {v0, v1, ..} \u2208 V , a text description of the downstream task Yd (e.g., classification) and a list of the desired class names Y cd ,\u2200c \u2208 Cd where Cd is the number of classes in task d. Given this, LOVM methods are expected to rank and predict the accuracy of the set of models (see Fig. 3, i):\npv,d = fLOVM(v, {Y cd } Cd c=1, Yd), \u2200 v \u2208 V , (1)\nwhere pv,d \u2208 R is the relative/absolute performance of model v on dataset d."
        },
        {
            "heading": "2.1 Data Collection",
            "text": "To train and evaluate LOVM methods, we need the zero-shot ground-truth performance of many VLM models on many downstream datasets. We, therefore, selected 35 VLMs and 23 Datasets and then performed image-based evaluations of each model on all the datasets - a total of 805 evaluations using the same prompting strategies discussed by Radford et al. [2021], See Fig. 3, iii. These ground truth zero-shot image-based model rankings and accuracies constitute the bulk of our benchmark. The proposed LOVM benchmark consists of the aforementioned evaluation tables as well as the per-dataset prompting templates, class names, and domain descriptions.\nSelected Datasets. The proposed LOVM benchmark utilizes a heterogeneous assortment of 23 datasets. These datasets exhibit variability in the number of classes, their target tasks, and corresponding domains. The benchmark encompasses a comprehensive range of tasks such as classification, scene understanding, geolocalization, and object counting, rendering it extensively applicable across many applications. Further, the datasets span diverse domains, including natural, satellite, text, and medical images (See Tab. 1). To ensure maximal compatibility, we have opted for tasks that permit the utilization of the same VLM architecture, precluding any requisite alterations or additional training. This approach necessitated the exclusion of tasks such as segmentation and object detection, which mandate additional training modules, introducing extraneous noise during the evaluation of VLM performance.\nVLM Model Candidates. We utilize the open-clip library [Ilharco et al., 2021], a diverse collection of pre-trained VLMs spanning various architectures, including but not limited to CLIP and CoCa models, and utilizing encoders such as ResNet, ConvNext, and ViT. These models have undergone pre-training on various datasets, such as WIT [Radford et al., 2021], LAION 400m, and LAION 2b [Schuhmann et al., 2022], with different hyperparameters. From the 87 models currently available, we have carefully selected 35 for our study. A comprehensive list of all models used in this benchmark can be found in the App. Tab. 4. We avoided incorporating additional multi-modal models, such as BEIT[Wang et al., 2023] and VLMO [Bao et al., 2022], as these models utilize a shared text-image encoder and, therefore, cannot be evaluated on the same datasets as CoCa and CLIP. Utilizing models from the open-clip library ensures maximum compatibility and reproducibility in our work. Currently, CLIP models comprise a significant portion of VLMs employed in practice."
        },
        {
            "heading": "2.2 LOVM Evaluation Protocol",
            "text": "On our benchmark, methods are expected to rank 35 pre-trained multi-modal models that differ in architecture and pre-training datasets on 23 target datasets, and compare these rankings to the ground-truth rankings (see Fig. 3 (ii)) and report the performance for each of the 23 datasets as well as their averaged values.\nModel Ranking. When evaluating model ranking on a particular dataset, one has access to the performance of all the models on all the datasets besides the one being evaluated. We use the following metrics:\n\u2022 Top-5 Recall (R5) \u2013 We used R5 to evaluate a LOVM method\u2019s model ranking capability. It is defined as the ratio of correctly identified models.\n\u2022 Kendall\u2019s Rank Correlation (\u03c4 ) \u2013 We used \u03c4 to evaluate a LOVM method\u2019s model selection capability and give s fine-grained picture of how well the method ranked the high-performing models and is defined as Kendall\u2019s rank over the top-5 selected models.\nPerformance Prediction. When evaluating a model\u2019s prediction on a dataset, the GT performance of that model on all datasets and the performance of all models on that dataset are held out.\n\u2022 Mean Absolute Error (L1) \u2013 We used L1 to evaluate a LOVM method\u2019s performance prediction capability. Specifically, we compute the L1 loss of all models\u2019 predicted vs. actual mean per-class recall/top-1 accuracy."
        },
        {
            "heading": "3 LOVM Baselines",
            "text": "The assessment of model performance in traditional supervised methods often relies on benchmark dataset performance. Given that most pre-trained vision-language models (VLMs) are evaluated on ImageNet, it is convenient to utilize it as a baseline for comparison (This is our ImageNet Benchmark baseline). Alternatively, a large language model could generate many probable image captions, which could be encoded using the different VLMs text encoder, producing the corresponding text embeddings. Treating these embeddings as image-proxies, one can calculate different widelyaccepted scores (see Sec. 3.2) and fit a linear regression model to predict performance or rank VLMs. Specifically, from every VLM-dataset combination, one extracts these scores and then fits the model:\npv,d = w \u00b7 sv,d + b, (2) siv,d = f i feat(v,TextGen({Y cd } Cd c=1, Yd)), (3)\nwhere pv,d \u2208 R is the relative/absolute performance of model v on dataset d, w, b are the weights and bias of the linear model. siv, d is the i-th element in the score vector, sv,t = [s 1 v, d, s 2 v, d, ...]\nT , produced by the corresponding feature/score function f ifeat. The function TextGen is a function that generates text given the class names, {Y cd } Cd c=1 and task description Yd of the desired task/dataset d. We discuss the different scores, siv, d, in Sec. 3.2 and the TextGen function in Sec. 3.1. To evaluate model rankings on a dataset, we hold out the data for that particular dataset and fit a linear model on all the other datasets. Meanwhile, to evaluate the performance prediction of some model on a particular dataset, we hold out the data for that dataset and model and fit a linear model on the remaining combinations. We refer to the baselines by the combination of scores used in the linear model."
        },
        {
            "heading": "3.1 Text Data Generation",
            "text": "The impressive progress in large language models (LLMs) [OpenAI, 2023, Touvron et al., 2023] has rendered the generation of potential - and realistic - \u2018image captions\u2019 practically limitless, thus rendering text data generation remarkably attainable. In our study, we employ GPT-3.5, tasked to produce two distinct text-based datasets, each corresponding to a given vision task. These generated datasets serve as the foundation for extracting essential features for our task.\nCaptions Dataset. To generate the captions dataset, Dcap, we prompt an LLM to generate realistic - but confusing - captions for images containing the user-provided classes in the user-provided domain. We extracted the dataset description and class names from each dataset and prompted the LLM:\nGenerate long and confusing image captions for the {domain} domain, which will be used to evaluate a Vision-Language Model\u2019s {task} performance. Generate 50 captions for {classname}:\nWhere we assume the user supplies the target domain and task description. For examples of different dataset\u2019s domain and task, see Tab. 3.\nSynonyms Dataset. Prior studies have already leveraged synonyms to evaluate LLMs [van der Lee et al., 2022]. For example, if an LVM has seen many instances of the class \u2018chair\u2019 referenced as a \u2018chair\u2019, \u2018seat\u2019, etc., we expect these embeddings to be closely located in the shared embedding space. To evaluate this aspect of the VLM using text, we prompt an LLM to generate a list of semantically similar/synonyms for every object class. For example, for the class \u201cchair\u201d, we get: \u201cseat\u201d, \u201cbench\u201d, \u201carmchair\u201d, and \u201cfurniture\u201d. We prompt the LLM with the following:\nPlease list the superclasses/synonyms for {classname}. For example: chair: [furniture, seat, bench, armchair, sofa] {classname}:\nWe collect the results from this prompt to form the synonyms dataset, Dsyn."
        },
        {
            "heading": "3.2 Text-Derived Scores",
            "text": "There are many widely reported metrics for model transferability, dataset difficulty, and dataset granularity scores developed on image embeddings. We extract different commonly used features/metrics from the text dataset embeddings and calculate their text-only counterparts.\nText Classification Scores (C). We use the generated captions dataset as image proxies and evaluate the resulting model performance. Specifically, we replace the images with the generated image captions and evaluate each model\u2019s text top-1 accuracy, text-acc1 and f1-score text-f1.\nDataset Granularity Scores (G). Cui et al. [2019] introduced the use of two widely used dataset granularity measures for image classification, Fisher criterion [Fisher, 1936], \u03d5fisher and Silhouette score [Rousseeuw, 1987], \u03c6sil, and their normalization constant, Class Dispersion score, \u03c1disp. The Fisher criterion measures the degree of similarity of the classes or the extent of their separation. The Silhouette score is a well-established metric used to quantify the tightness of the same-class samples to the separation of different-class samples. The Class Dispersion score quantifies the degree of same-class tightness or data cone radius.\nRecently, van der Lee et al. [2022] has shown that synonym consistency can be used in large language models to correlate the degree of familiarity of the model with a particular concept. Using the Synonym dataset, we compare the cosine similarity between the text embedding of each class and its corresponding synonyms. A high Synonym Consistency score, \u03b3syn, between the class and its corresponding synonyms indicates that the model is aware of the semantic meaning of the class.\nFor detailed definitions of these metrics, see App. Sec. B."
        },
        {
            "heading": "4 Experiments and Results",
            "text": "In Sec. 4.1, we evaluate the model selection capabilities of the proposed baselins on the LOVM benchmark. In Sec. 4.2, we evaluate the proposed baselines performance prediction cababilities. We then analyse score trends and draw insights in Sec. 4.3."
        },
        {
            "heading": "4.1 Model Selection",
            "text": "A core aspect of this benchmark is model selection, as it allows the user to quickly and easily select the optimal model for the desired downstream task. From Tab. 2, we can see that, when predicting/ranking by the models mean per-class recall, the (C+G)-baseline can achieve a top-5 recall of 0.261, indicating that, on average, more than one model is correctly ranked as a top-5 performing model. Meanwhile, the INB-baseline had a R5 of 0.505. Combining the text and ImageNet scores, the (INB+G)-baseline achieves the highest accuracy of 0.539, a \u223c 15% improvement over the INB-baseline. When studying Kendall\u2019s rank correlation, the (G+C)-, INB-, and (INB+G)-baselines achieve a \u03c4 of 0.101, 0.186, and\n0.214, respectively. The (INB+G)-baseline had the highest rank correlation with \u223c 6% improvement over the INB-baseline. Similar results can be seen when predicting the top-1 accuracy. The consistent improvement of the baselines over the INB-baseline indicates the utility of both text-based and benchmark features. Interestingly, C-score (or the text-acc1) appears to be more influential in predicting/ranking model\u2019s the top-1 accuracy than the mean per-class recall.\n4.2 Performance Prediction\nBased on Tab. 2, it is clear that the granularity scores (G) are instrumental to predicting a model\u2019s top-1 accuracy and mean per-class recall. The G-baseline approach can achieve an average L1 error of 0.145 and 0.141 for predicting the mean-perclass recall and top-1 accuracy, respectively. Adding any other scores does not lead to an improvement in performance prediction. The INB-baseline, which uses Imagenet performance as prediction, leads to a much higher L1 error of 0.228 and 0.220 compared to the text-base baselines (text-based performance estimation outperformed INBbaselines by \u223c 36%). Finally, adding the ImageNet benchmark score to the text features in the Unified baseline did not im-\nprove the L1 compared to the text-only baseline. This is expected as the imagenet performance cannot be used to predict the performance on a different dataset. Fig. 4 shows the predicted vs. ground-truth accuracy. Our approach had a R2 score (or coefficient of determination) of = 0.55, showing significant room for improvement in accuracy prediction."
        },
        {
            "heading": "4.3 Insights into VLM Behavior",
            "text": "In this section, we visualize the dependence of the text-derived features on the pre-training datasets and model architectures while averaging them across the different datasets (see Fig. 5).\nModel Size. From studying Fig. 5, we can we can identify a clear trend of Fisher criterion and Silhouette score improving with model size, while Class Dispersion score and Synonym Consistency score degrade with model size. Silhouette score quantifies the degree of inter-class overlap or the degree of overlap between different classes in the embedding space. As the model size of the visual encoder increases, the embeddings from different classes become more and more orthogonal, decreasing the inter-class overlap. Fisher criterion quantifies the degree of granularity a model perceives the target datasets to be. As model size decreases, Fisher criterion decreases, or the degree of perceived granularity increases. Class Dispersion score quantifies the degree of intra-class dispersion, or how similar embeddings of the same class are. Specifically, as we increase model size, Class Dispersion score decreases, and therefore the class embeddings become more varied, effectively expanding the class cone radius. Synonym Consistency score quantified the closeness of a class to its synonyms and behaved similarly to Fisher criterion.\nPre-training Dataset. When studying the effect of pre-training dataset size, it is clear that there is a positive correlation between pre-training dataset size and all of the metrics when comparing models of the same size. As the pre-training dataset increases, the intra-class simularity increases more rapidly than the inter-class simularity, hence effectively different classes are more seperated. Specifically, Fisher criterion and Silhouette score increase, or the degree of perceived granularity decreases, and embeddings from different classes become less orthogonal, increasing the inter-class overlap. As the pre-training dataset size increases, Class Dispersion score increases and the intra-class dispersion is more condensed, leading to a smaller effective radius of a class dataset cone. Interestingly, larger models are more affected by the increase in dataset size (as seen by the large slope of ViT-L compared\nscores quantifying intra-class similarity. ResNet ( ) and ConvNext (\u00d7 ) based models are grouped separately to evaluate their effect on the score trends.\nto ViT-B) - which could explain previous works\u2019 observation that larger models benefit more when trained on larger datasets [Fang et al., 2022].\nModel Architecture. Pre-training datasets and model architectures significantly influence each other. ResNets and ViTs, for instance, consistently demonstrated differing behaviors and appeared to reside at distinct points on the class separation-dispersion trade-off curve. In particular, ResNets displayed lower Class Dispersion score and Silhouette score, indicating challenges in encoding instances of the same class within the feature space compared to ViTs. This may account for ResNets\u2019 superior performance on datasets with low visual variation, like MNIST; as the visual variation is relatively low, we would not expect the Class Dispersion score to be the limiting factor in model performance, making them less affected by this aspect of the dataset. Intriguingly, ConvNEXT models exhibited characteristics more in line with ViT-base models than ResNet-based ones. What leads to variation between WIT and L400m remains unclear, necessitating further investigation."
        },
        {
            "heading": "5 Related Work",
            "text": "Vision-Language Models. The field of vision-language models (VLMs) has witnessed significant progress in recent years, particularly with the introduction of contrastive pre-trained VLMs such as CLIP [Radford et al., 2021]. These models leverage large-scale datasets of aligned image-caption pairs to obtain shared embedding spaces that capture rich visual and textual features. The learned image and text encoders from these VLMs have demonstrated impressive feature extraction capabilities and even set state-of-the-art zero-shot performances. However, the performance of VLMs can vary significantly across different datasets, especially when there exists a domain, content, or distribution shift [Fang et al., 2022]. As the number of model architectures & pre-training datasets [Ilharco et al., 2021, Schuhmann et al., 2022] increase, it is challenging to select a pre-trained VLM, as good performance on existing benchmarks does not always translate to the downstream task. Therefore, there is a need to develop strategies that can estimate VLM performance on a new task without requiring an exhaustive evaluation of these models using the target dataset.\nText as a Proxy For Images. While these VLMs aim to project representations from different modalities into a shared embedding space, Liang et al. [2022] found that corresponding image and text pairs don\u2019t completely overlap in the embedding space. Instead, a \u201cmodality gap\u201d exists between the image embeddings and text embeddings sub-space. Subsequently, Zhang et al. [2023] has found that this gap can be approximated as an orthogonal constant between true pairs of image and text and is, therefore, parallel to the decision boundaries for a given modality. This suggests that cross-modality transferability - using one modality as input to the other\u2019s classifier - is possible for these contrastively pre-trained VLMs. Severak studies have demonstrated the utility of the cross-modality transferability\nphenomenon in different tasks. For instance, Domino leveraged the cross-modal embeddings to identify error slices and generate natural language descriptions of the error slices [Eyuboglu et al., 2022]. Similarly, Jain et al. [2022] used these embeddings to discover model failure directions in the multi-modal embedding space. Meanwhile, Zhang et al. [2023] proposed the DrML, which diagnoses and rectifies vision classifiers using natural language inputs. In this study, we also use text as a proxy for images, but for the novel task of ranking and estimating VLM performance.\nUnsupervised Model Selection. Unsupervised model selection was recently introduced by Sun et al. [2021], to select the best model for a new target domain without utilizing labeled data. Their work only considered domain (and not content) shifts and proposed constructing a proxy dataset that captures/closely approximates this shift. This proxy dataset is constructed by minimizing different dataset statistics using several labeled datasets. Evaluating models on this proxy set performs well for model selection/ranking. However, such a strategy is limiting in the setting of evaluating VLMs - the size of these models and their pre-training datasets makes it too computationally expensive to achieve the desired goal of evaluating model performance on any downstream task.\nUnsupervised Accuracy Estimation. Unsupervised or label-free accuracy estimation aims to estimate classifier model performance with only access to the unlabeled test set of a new task. Platanios et al. [2017, 2016] proposed strategies to apply probabilistic modeling approaches, such as the probabilistic logic or Bayesian modeling, to analyze and aggregate predictions from multiple classifiers. Other works approach this task by fitting models on feature statistics of the target dataset [Risser-Maroix and Chamand, 2023]. Some studies evaluated model agreement, where many classifiers are used on the target dataset, and the degree of agreement was correlated with model performance [Chen et al., 2021, Jiang et al., 2022] Other approaches for unsupervised accuracy estimation include training a neural network on the weight distribution statistics [Unterthiner et al., 2020] or composing a meta-dataset with available datasets, such that the meta-dataset matched some target dataset statistics [Deng and Zheng, 2021]. Some have attempted to craft embedding-based scores, trying to quantify the separability of clusters in the embeddings spaces [P\u00e1ndy et al., 2022, Ding et al., 2022]. All these methods assume access to the unlabeled dataset of the target task. Instead, our method only requires text descriptions of the novel task to estimate the model\u2019s performance."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this work, we introduce a new problem setting and task LOVM, which aims to select the bestperforming VLMs for a downstream vision task by only using its textual description. To demonstrate the feasibility of such a task, we show how large language models, in combination with the crossmodal transferability phenomenon, can be leveraged for such a task. We exhaustively test these methods on the proposed LOVM benchmark, consisting of 35 VLMs and 23 benchmark datasets. Our findings validate the viability of our proposed LOVM task, with unified (both text scores and ImageNet benchmarking) baselines outperforming the ImageNet benchmarking baseline. This suggests that text-based model selection methods (i.e., LOVM methods) provide additional benefits to baseline selection based on a model\u2019s performance on ImageNet. Furthermore, we found that the granularity-based scores influence performance prediction and modal ranking more greatly. These findings bolster the research direction of developing methods for VLM selections using only text.\nOur proposed LOVM benchmark aims to foster this research direction. We see two promising avenues for future research: (i) improving text-based classification correlation with ground-truth accuracy by either text generation, evaluation metrics, or cross-modal transferability, and (ii) introducing new granularity and transferability scores to the text-only paradigm. Namely, we anticipate the development of methods improving over our proposed baselines presented in Tab. 2. Our work aims to facilitate future research in this area and provide a more accurate and reliable means of comparing pre-trained VLMs, accelerating their utilization in downstream applications.\nFor a discussion about broader and potential negative societal impacts please see App. Sec E.\nAcknowledgments. We gratefully acknowledge the computational credits provided by Google Cloud Platform through Stanford\u2019s HAI Institute for Human-Centered Artificial Intelligence. We also thank the Knight-Hennessy Scholars Foundation for generously funding Orr Zohar."
        },
        {
            "heading": "Table of Contents",
            "text": "A LOVM Benchmark Details 14\nA.1 LOVM Benchmark - Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 A.2 LOVM Benchmark - Vision-Language Models . . . . . . . . . . . . . . . . . . 14 A.3 LOVM Benchmark - Ground-Truth Model Ranking . . . . . . . . . . . . . . . 15\nB Baseline Details 15 B.1 Prompting Templates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 B.2 Text-Derived Scores . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 B.3 Text Dataset Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 B.4 Text Classification and Noise Corruption . . . . . . . . . . . . . . . . . . . . . 19\nC Additional Results 19 C.1 LOVM Per-Dataset Breakdown . . . . . . . . . . . . . . . . . . . . . . . . . . 19 C.2 Ablation Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 C.3 Raw Model Ranking Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 C.4 Domain Shift Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\nD Limitations 26\nE Broader Impacts 28"
        },
        {
            "heading": "A LOVM Benchmark Details",
            "text": "We evaluated 35 on 23, a total of 805 evaluations. This constituted the bulk of our compute with a total of 4 days on an nvidia V100 instance. Evaluations were carried out using the CLIP_benchmark repository (https://github.com/LAION-AI/CLIP_benchmark)."
        },
        {
            "heading": "A.1 LOVM Benchmark - Datasets",
            "text": "The proposed LOVM benchmark comprises of 23 datasets, which were selected to maximize diversity. Specifically, these datasets vary in the number of classes (2 to 1000), their target tasks, and domains. The benchmark encompasses a comprehensive range of tasks such as classification, scene understanding, geolocalization, object counting, and more, with the goal of rendering it extensively applicable across many applications. Further, the datasets span diverse domains, including natural, satellite, text, and medical images (See Tab. 3 for a comprehensive account of the datasets and their source). To ensure maximal compatibility, we have opted for tasks that permit the utilization of the same VLM architecture, precluding any requisite alterations or additional training. This approach necessitated the exclusion of tasks such as segmentation and object detection, which mandate additional training modules, introducing extraneous noise while evaluating VLM performance. However, it is worth noting that previous transferability works have shown that these approaches may generalize to more complex applications such as semantic segmentation [P\u00e1ndy et al., 2022, Agostinelli et al., 2022]."
        },
        {
            "heading": "A.2 LOVM Benchmark - Vision-Language Models",
            "text": "Tab. 4 presents a list of models and their corresponding pre-training datasets used in the LOVM benchmark. We utilize the open-clip library [Ilharco et al., 2021], a diverse collection of pre-trained VLMs spanning various architectures, including but not limited to CLIP and CoCa models, and utilizing encoders such as ResNet, ConvNext, and ViT. These models have undergone pre-training on various datasets, such as WIT [Radford et al., 2021], LAION 400m, and LAION 2b [Schuhmann et al., 2022], with different hyperparameters. From the 87 models currently available, we have carefully\nselected 35 for our study. A comprehensive list of all models used in this benchmark can be found in Tab. 4. We avoided incorporating additional multi-modal models, such as BEIT[Wang et al., 2023] and VLMO [Bao et al., 2022], as these models utilize a shared text-image encoder and, therefore, cannot be evaluated on the same datasets as CoCa and CLIP. Using models from the open-clip library ensures maximum compatibility and reproducibility in our work. Currently, CLIP models comprise a significant portion of VLMs employed in practice. Tab. 4 includes 35 entries, each identified by an ID number. The first four columns indicate the ID number, model name, model abbreviation, and pre-training dataset name. The fifth column shows the abbreviation of the pre-training dataset name. The models listed in the table include ResNet (RN50, RN101, etc.) and Vision Transformer (ViT) with different sizes (B/32, B/16, L/14, etc.), and the pre-training datasets include OpenAI\u2019s WIT dataset and two variants of LAION (L400m and L2b) datasets."
        },
        {
            "heading": "A.3 LOVM Benchmark - Ground-Truth Model Ranking",
            "text": "To evaluate the validity and generalizability of the LOVM benchmark, we first present the groundtruth model ranking over all datasets to show that the model order is not constant across the datasets. We organized the benchmarks from natural image classification (Fig. 6, left) to non-natural image / non-classification benchmarks (Fig. 6, right). As depicted in Fig. 6, the distribution exhibits a non-uniform pattern, indicating the utility of LOVM methods and the importance of VLM selection methods in general. Interestingly, ranking variations are more significant on the non-natural image / non-classification benchmarks. This exemplifies the need for LOVM methods to contend with content shift (i.e., changing what classes are in the target domain) and domain/task shift."
        },
        {
            "heading": "B Baseline Details",
            "text": "Fig. 7 shows an overview of our baselines. We first describe the prompting protocol used in Sec B.1. We then give an in-detail description of the scores used in the study in Sec. B.2. In Sec. B.3, we give additional details for the text dataset generation. Finally, in Sec. B.4, we describe how we use noise to corrupt the text caption dataset when calculating text-acc1 and text-f1 scores."
        },
        {
            "heading": "B.1 Prompting Templates",
            "text": "We use the same prompting strategy introduced by Radford et al. [2021] to generate the model zero-shot weights (see Fig. 10 for examples of templates from different datasets). Specifically, for every class c, we used the reported templates to produce the text prompts Y c and encoded these prompts using the VLM text encoder, fy , to produce the text embeddings for class c, y\u0302c:\ny\u0302c = fy(Y c).\nWe then normalized each separate prompt by its L2 norm and averaged the resulting vector to produce y\u0304c, or the unnormalized zero-shot weight of class c:\ny\u0304c = 1\nN N\u2211 j=1 ycj ||ycj ||2 ,\nwhere y\u0304c is then normalized again to produce the final zero-shot classification weight of class c,\nyc = y\u0304c\n||y\u0304c||2 . (4)"
        },
        {
            "heading": "B.2 Text-Derived Scores",
            "text": "We define the six scores we derived for model selection and performance prediction. The Text top-1 accuracy score and Text f1-score is used to estimate the VLMs\u2019 performance on a vision task using text as a proxy, while the Fisher criterion and Silhouette score are used to understand the VLM\u2019s capability to separate samples from different classes in the target task (inter-class similarity. To estimate dataset granularity, we use Class Dispersion score. Finally, the Synonym Consistency score allows us to evaluate the degree of content shift between the VLMs\u2019 pre-training and target dataset (intra-class similarity).\nText Classification scores. We use the generated captions dataset (see Sec. 3.1) as a proxy for images and evaluate the resulting model performance. Specifically, we use the VLM text encoder to generate text-derived multi-modal embeddings. We then corrupt these embeddings with Gaussian noise to approximate image-instance variation (see Sec. B.4)and calculate their cosine similarity with the class prompt embeddings - derived using the same prompt ensembling strategies proposed by Radford et al. [2021] (see Fig. 7). We then calculate the text top-1 accuracy (text-acc1) and text f1-score (text-f1).\nFisher criterion, \u03d5fisher. The Fisher criterion [Fisher, 1936] has been widely used as a dataset granularity measure and has recently been shown to be effective for classification by Cui et al. [2019]. The Fisher score measures the degree of similarity of the classes or the extent of their separation. In VLMs, The quality of the class separation can be evaluated using text by assessing how close the\ndifferent (text-derived) class prompt embeddings are. We introduce the concept of Fisher criterion, a score that quantifies how close the class prompt embeddings are to each other (see Fig. 7):\n\u03d5fisher = 1\nC C\u2211 j=1 maxc,c\u0338=j [ \u03b8(yj ,yc) ] , (5)\nwhere yc is the class prompt embedding derived using the prompt ensembling strategies proposed in Radford et al. [2021] for class c (see Sec. B.1), \u03b8(\u00b7, \u00b7) is a function that calculates the cosine similarity between two vectors, and C is the number of classes.\nSilhouette score, \u03c6sil. The silhouette score [Rousseeuw, 1987] is a well-established score that has been used to quantify the tightness of the same-class samples to the separation of different-class samples [Scheidegger et al., 2018, Cui et al., 2019]. Inspired by this score, we introduce the text-based Silhouette score, \u03c6sil, which measures the separation of different-class samples in the caption dataset Dcap. To do so, we evaluate the average cosine similarity of captions to the nearest other class by:\n\u03c6sil = 1\nC C\u2211 j=1 maxc,c \u0338=j\n[ 1\nN N\u2211 k=1 \u03b8(Dcap[j]k,y c)\n] , (6)\nwhere yc is the class prompt embedding derived using the prompt ensembling strategies proposed in Radford et al. [2021] for class c (see Sec. B.1), \u03b8(\u00b7, \u00b7) is a function that calculates the cosine similarity between two vectors, and C is the number of classes. Dcap[j]k representing sample k of class j in the caption dataset Dcap, and there is a total of N such samples in for each class.\n.\nClass Dispersion score, \u03c1disp. The Class Dispersion score is used as the normalization constant to generate the Fisher and Silhouette scores, and it quantifies the degree of same-class tightness or data cone radius (see Fig. 7).\n\u03c1disp = 1\nCN C\u2211 c=1 N\u2211 k=1 \u03b8(Dcap[c]k,y c), (7)\nwhere yc is the class prompt embedding derived using the prompt ensembling strategies proposed in Radford et al. [2021] for class c (see Sec. B.1), \u03b8(\u00b7, \u00b7) is a function that calculates the cosine similarity between two vectors, and C is the number of classes. Dcap[c]k representing sample k of class c in the caption dataset Dcap, and there is a total of N such samples in for each class.\nSynonym Consistency score, \u03b3syn. Synonym consistency has been shown in large language models to correlate with the degree of familiarity of the model with a particular concept [van der Lee et al., 2022]. Using the Synonym dataset, we compare the cosine similarity between the text embedding of each class and its corresponding synonyms. A high cosine similarity between the class and its corresponding synonyms/supercategories indicates that the model is aware of the semantic meaning of the class and is defined as:\n\u03b3syn = 1\nCN C\u2211 c=1 N\u2211 k=1 \u03b8(Dsyn[c]k,y c), (8)\nwhere yc is the class prompt embedding derived using the prompt ensembling strategies proposed in Radford et al. [2021] for class c (see Sec. B.1), \u03b8(\u00b7, \u00b7) is a function that calculates the cosine similarity between two vectors, and C is the number of classes. Dsyn[c]k representing sample k of class c in the synonym dataset Dsyn, and there is a total of N such samples in for each class."
        },
        {
            "heading": "B.3 Text Dataset Generation",
            "text": "To generate the Captions dataset, we used a large language model to generate realistic (but confusing) image captions. It was necessary to request confusing image captions to get sufficient variation in the image captions. We used OpenAI\u2019s \u2018gpt-3.5-turbo-0301\u2019 model with a temperature of 1. For the synonym dataset, we reduced the temperature to 0.1 and only requested the synonyms themselves. We then used the prompting templates with the synonym in place of the original class name to generate Dsyn."
        },
        {
            "heading": "B.4 Text Classification and Noise Corruption",
            "text": "In this work, we introduce the use of Gaussian noise to corrupt text-derived multi-modal embeddings to approximate image-instance variation. The corrupted embeddings are then used to calculate the text top-1 accuracy and text-f1 score, which serves as a proxy for evaluating the performance of a vision model. The scores are derived from the Captions dataset, a collection of complex but probable image captions generated using a large language model for images containing the user-provided classes in the user-provided domain and for the user-provided task. For more, see Sec. 3.1.\nTo evaluate the effectiveness of the text top-1 accuracy, we systematically increase the level of noise corruption and plot the text top-1 accuracy against the ground-truth top-1 accuracy (See Fig. 8). We quantify this correlation via the R2 score or the degree of explained variance. As we do not fit a linear model to the predicted vs. ground-truth predictions, R2 ranges from 1 (perfect linear fit) to \u2212\u221e, where non of the variance is explained. The results show that, without noise corruption, the text top-1 accuracy is too high and frequently saturates without any corruption. However, as the noise level increases to 0.1, the text top-1 accuracy progressively improves until a better linear correlation can be seen. This indicates that increasing noise corruption can better approximate image-instance variation and improve the correlation of the text top-1 accuracy. Beyond 0.1, however, the correlation between the text top-1 accuracy and top-1 accuracy progressively worsens. This shows that while noise corruption helps improve the correlation of the text top-1 accuracy, there is a limit beyond which further noise corruption degrades its effectiveness."
        },
        {
            "heading": "C Additional Results",
            "text": ""
        },
        {
            "heading": "C.1 LOVM Per-Dataset Breakdown",
            "text": "Here, we show the per-dataset breakdown of our main results. In Tab. 5, we show our model ranking and performance prediction results for top-1 accuracy. In Tab. 6, we show our model ranking and performance prediction results for mean per-class recall."
        },
        {
            "heading": "C.2 Ablation Experiments",
            "text": "To understand the utility of each of our extracted scores, we exhaustively ablated their effect on top-1 accuracy model ranking and performance prediction (Tab. 7 and Tab. 8), and mean per-class recall model ranking and performance prediction (Tab. 9 and Tab. 10). Specifically, we ablated each score\u2019s impact on the resulting model\u2019s performance. As can be seen, using more than \u223c 3 features at a time seldom improves performance. Future work can investigate the use of more sophisticated models that may be able to utilize more scores in predicting model ranking and performance. Specifically, for ranking models, text classification and scores quantifying intra-class similarity (Class Dispersion score & Synonym Consistency score) were the most dominant, while for performance prediction, granularity scores quantifying both inter- and inta- class similarity was the most important. This\nWe ablate the model ranking performance to understand each extracted score\u2019s effect on ranking models. The text classification scores and scores quantifying intra-class similarity were the most consequential in predicting model ranking. Specifically, in ranking models, the text-f1 score, Class Dispersion score (\u03c1disp), and Synonym Consistency score (\u03b3syn) where the most dominant (Tab. 7 rows 8 & 11, Tab. 9 row 38). Overall, it seems like the text classification excelled at fine-grained ranking (as quantified by \u03c4 ), while the inter-class granularity scores improved the coarse ranking prediction (as quantified by R5. Meanwhile, granularity scores quantifying inter- and intra- class similarity were the most dominant for performance prediction. Specifically, Class Dispersion score (\u03c1disp), Synonym Consistency score (\u03b3syn), and Silhouette score (\u03c6sil) were the most influential (Tab. 8 rows 26 & 41, Tab. 10 row 60). INB does not aid performance prediction, indicating that getting a course estimation of dataset difficulty dominates performance prediction."
        },
        {
            "heading": "C.3 Raw Model Ranking Details",
            "text": "To illustrate the model ranking of the naive (ImageNet Benchmark) baseline to some text-based approaches, we visualize the raw ranking prediction of each method. We sort the datasets from natural image classification (Fig. 9 left) to non-natural image / non-classification benchmarks (Fig. 9 right). Here, the evident failure of the ImageNet Benchmark baseline to capture dataset-specific changes in ranking is apparent. As the benchmark approach ranks models by their ImageNet performance, the model ranking is constant for all datasets. Meanwhile, integrating the text features produces a ranking distribution with a discernible positive correlation between the ground truth and the predicted model ranking. The unified approach also captures more significant ranking variation in the non-natural image / non-classification benchmarks."
        },
        {
            "heading": "C.4 Domain Shift Experiment",
            "text": "An obvious obstacle to text-based model performance prediction methods is the difficulty in describing distribution shifts. For example, VLMs evaluated on ImageNet and ImageNet-v2 will get the same text-predicted accuracy while the actual performance differs. Meanwhile, for some domain shifts - like ImageNet and ImageNet sketch - this shift can be described via text. We want to evaluate how capable text-only methods are at estimating the dataset difficulty under such shifts and compare them to well-accepted image-based approaches.\nDataset Description Similarity. We extract each dataset\u2019s description from either the abstract or introduction section of the original manuscript. Subsequently, we extract the text embeddings for all dataset descriptions using a pre-trained CLIP model. We then compute the cosine similarity\nbetween the descriptions of the downstream datasets to the original pre-training dataset to quantify how different the two datasets are.\nPrompt Embedding Similarity. We use the cosine similarity between dataset-specific and generic class prompts to evaluate domain shift. Specifically, based on the original list of class prompts from CLIP, we pick the ones that best describe our target dataset. For instance, for the ImageNet-sketch dataset, we selected prompts such as \u201cA sketch of a {c}\u201d or \u201cA doodle of a {c}\u201d. Then, we use the text encoder from a pre-trained CLIP model to extract embeddings from dataset-specific and generic class prompts and compute the cosine similarity between each pair. We use the mean cosine similarity to measure how similar the target dataset is to the pre-training dataset. The dataset-specific prompts can be found in the following subsection.\nImage-Text Embedding. We wanted to compare with widely used dataset difficulty approaches. One common approach is to use the confidence of a model\u2019s prediction to determine a dataset\u2019s difficulty. To do so, we first n images from the target dataset and extract image embeddings for each of the n images. This simulates the scenario where we only have access to n images from the target dataset to estimate model performance, where n is much smaller than the dataset size. Then, we embed the class prompt into text embeddings and compute the prediction logits between each image embedding and class embeddings. Lastly, we compute the entropy score Ethayarajh et al. [2021] and max prediction logit Feng et al. [2022] to determine dataset difficulty.\nImage Embedding Distance. Another common approach is estimating the difference in distribution between the test and train sets Scheidegger et al. [2018]. We, therefore, use the distance between the\ntarget and pre-training image embeddings to quantify dataset difficulty. Similar to the image-text embedding approach, we first sample n images from the target dataset to extract image embedding using a pre-trained CLIP image encoder. Additionally, we sample m images from the pre-training dataset to extract image embeddings. We only sample m examples since these VLMs are typically pre-trained on an internet-scale dataset, which makes it challenging to embed and compute distance measures on the entire pre-training dataset. We then compute the L2 distances between the target and pre-training datasets. We use max, min & mean L2 to quantify dataset difficulty.\nDatasets. To evaluate the feasibility and effectiveness of each, we use the following variants of ImageNet. Each dataset captures a different distribution shift from the original ImageNet:\n\u2022 ImageNet: The original ImageNet dataset. \u2022 ImageNet Version 2 (ImageNet-v2): A new test set for ImageNet sampled a decade later. \u2022 ImageNet Sketch (ImageNet-s): A ImageNet test set dataset with sketch-like iamges. \u2022 ImageNet Rendition (ImageNet-r): contains art, cartoons, deviantart, graffiti, embroidery,\ngraphics, origami, paintings, patterns, plastic objects, plush objects, sculptures, sketches, tattoos, toys, and video game renditions of ImageNet classes.\n\u2022 ImageNet Adversarial (ImageNet-a): A real-world distribution shift ImageNet dataset with changes in image style, blurriness, camera operation, and geographic location.\nWe extract descriptions of each dataset from either the abstract or induction section of their original manuscript. The description used for each dataset is as shown here:\n\u2022 LAION400m: \u201ca dataset with CLIP-filtered 400 million image-text pairs.\u201d \u2022 ImageNet: \u201ca benchmark in object category classification and detection on hundreds of\nobject categories.\u201d\n\u2022 ImageNet Version 2: \u201cthree test sets with 10,000 new images each. Importantly, these test sets were sampled after a decade of progress on the original ImageNet dataset.\u201d\n\u2022 ImageNet Adversarial: \u201creal-world distribution shift datasets consisting of changes in image style, image blurriness, geographic locations.\u201d\n\u2022 ImageNet Rendition: \u201cart, cartoons, DeviantArt, graffiti, embroidery, graphics, origami, paintings, patterns, plastic objects, plush objects, sculptures, sketches, tattoos, toys, and video game renditions of ImageNet classes.\u201d\n\u2022 ImageNet Sketch: \u201ca new dataset consisting of sketch-like images, that matches the ImageNet classification validation set in categories and scale\u201d\nThe dataset-specific prompts used for the prompt embedding distance metric are listed in Fig. 10.\nEvaluation. We use Kendall\u2019s rank correlation (\u03c4 ) to evaluate our method\u2019s ability to rank the datasets in terms of their difficulties. Since image-text embedding and ImageNet embedding distance require sampling from the target dataset, we run our evaluation 1,000 times with different samples and compute the average metric. We also compute the standard deviations of the 1,000 run to estimate the variability of random samples.\nWe show the results of using our strategies to estimate domain shift in Tab 11. Based on the results, it is clear that none of the current methods can capture the dataset difficulty. Furthermore, the variability based on the standard deviation makes our results heavily dependent on the samples drawn from the target dataset, again suggesting these approaches\u2019 limitations."
        },
        {
            "heading": "D Limitations",
            "text": "Our study, while extensive, is not without limitations. Primarily, our focus rests on zero-shot tasks due to the nature of the LOVM\u2019s design. The framework\u2019s primary aim is to determine the best model for a given task when there is no access to the downstream task dataset. Under these circumstances, fine-tuning or linear probing is not viable, as they require access to labeled or unlabeled images from the downstream task dataset. If such data were available, the more straightforward approach would\nbe to address the conventional transferability problem as detailed in prior works. The ideal scenario we envision for using LOVM is one where a user with minimal technical expertise seeks to conduct a vision task. In this situation, the user can utilize a LOVM method to discern the most suitable model and the relevant classes, enabling them to deploy the model without needing to delve into technical nuances. However, if one possesses data for fine-tuning, conducting a direct evaluation on this small dataset is likely the most accurate course of action. This constraint stems from the fact that LOVM methods cannot make differential predictions without access to the fine-tuning data. Predicting the performance after fine-tuning or linear probing would heavily depend on the correlation between the results pre and post- fine-tuning/linear probing, a scenario we aim to avoid in the design of LOVM. However, previous work has shown some correlation exists, so there may be some transferability to fine-tuned/linear probed models [Wong et al., 2022].\nSecondly, as discussed in Sec. C.4, even datasets bearing identical content may encounter a domain shift. Such shifts can be clearly explained in some cases, such as when comparing ImageNetregular/rendition/sketch, but in others, the shift may be more elusive. For instance, when comparing ImageNet to ImageNet-a, or when class distribution shifts occur, identifying the source of the shift becomes challenging. In these scenarios, LOVM methods might struggle to accurately predict the performance of a VLM, though model selection might be marginally affected.\nFinally, while the utility of text-only solutions described in Sec. C.4 warrants continued investigation, it may be necessary to incorporate unlabeled test images to gauge domain shifts. Combining LOVM methods with these image-based evaluations remains a promising area of ongoing research."
        },
        {
            "heading": "E Broader Impacts",
            "text": "Our work simplifies selecting vision-language models (VLMs) for specific tasks, increasing the accessibility of artificial intelligence (AI) applications. However, this accessibility may be a doubleedged sword. On the one hand, it could democratize AI applications, allowing smaller entities or independent researchers to utilize AI technologies more effectively. On the other hand, this easy access might also enable malicious entities to deploy harmful applications more readily, posing risks to sectors such as information security and personal privacy.\nMoreover, despite our methodology\u2019s efficiencies, it carries the risk of sub-optimal model selection due to inherent limitations. Inaccuracies could lead to inefficient resource allocation or inferior performance in real-world applications, particularly in high-stakes fields such as healthcare or autonomous driving. Overall, while our work contributes to the efficiency and accessibility of AI applications, it highlights the need for vigilance and continuous refinement to mitigate potential negative impacts."
        },
        {
            "heading": "Checklist",
            "text": "1. For all authors... (a) Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s\ncontributions and scope? [Yes] (b) Did you describe the limitations of your work? [Yes] See Sec. D (c) Did you discuss any potential negative societal impacts of your work? [Yes] See Sec. E (d) Have you read the ethics review guidelines and ensured that your paper conforms to\nthem? [Yes] 2. If you are including theoretical results...\n(a) Did you state the full set of assumptions of all theoretical results? [N/A] (b) Did you include complete proofs of all theoretical results? [N/A]\n3. If you ran experiments (e.g. for benchmarks)... (a) Did you include the code, data, and instructions needed to reproduce the main exper-\nimental results (either in the supplemental material or as a URL)? [Yes] See Supplemental Material for code repository to reproduce all of our results, has the dataset, and even has an evaulation script for future works.\n(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] See Sec. 3 and Sec. B\n(c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [N/A]\n(d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] see Sec. A\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] open-clip, and yes,\nsee Sec. 2.2 and throughout the manuscript (b) Did you mention the license of the assets? [N/A] it is open-source and free to use (c) Did you include any new assets either in the supplemental material or as a URL? [Yes]\nSee codebase in supplemental, will be made open-source (d) Did you discuss whether and how consent was obtained from people whose data you\u2019re\nusing/curating? [N/A] (e) Did you discuss whether the data you are using/curating contains personally identifiable\ninformation or offensive content? [N/A] 5. If you used crowdsourcing or conducted research with human subjects...\n(a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A]\n(b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]\n(c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]"
        }
    ],
    "title": "LOVM: Language-Only Vision Model Selection",
    "year": 2023
}