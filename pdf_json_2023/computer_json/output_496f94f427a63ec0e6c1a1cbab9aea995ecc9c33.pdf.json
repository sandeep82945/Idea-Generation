{
    "abstractText": "Speaker recognition achieved great progress recently, however, it is not easy or efficient to further improve its performance via traditional solutions: collecting more data and designing new neural networks. Aiming at the fundamental challenge of speech data, i.e. low information density, multimodal learning can mitigate this challenge by introducing richer and more discriminative information as input for identity recognition. Specifically, since the face image is more discriminative than the speech for identity recognition, we conduct multimodal learning by introducing a face recognition model (teacher) to transfer discriminative knowledge to a speaker recognition model (student) during training. However, this knowledge transfer via distillation is not trivial because the big domain gap between face and speech can easily lead to overfitting. In this work, we introduce a multimodal learning framework, VGSR (Vision-Guided Speaker Recognition). Specifically, we propose a MKD (Marginbased Knowledge Distillation) strategy for cross-modality distillation by introducing a loose constrain to align the teacher and student, greatly reducing overfitting. Our MKD strategy can easily adapt to various existing knowledge distillation methods. In addition, we propose a QAW (Qualitybased Adaptive Weights) module to weight input samples via quantified data quality, leading to a robust model training. Experimental results on the VoxCeleb1 and CN-Celeb datasets show our proposed strategies can effectively improve the accuracy of speaker recognition by a margin of 10% \u223c 15%, and our methods are very robust to different noises.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yufeng Jin"
        },
        {
            "affiliations": [],
            "name": "Guosheng Hu"
        },
        {
            "affiliations": [],
            "name": "Haonan Chen"
        },
        {
            "affiliations": [],
            "name": "Duoqian Miao"
        },
        {
            "affiliations": [],
            "name": "Liang Hu"
        },
        {
            "affiliations": [],
            "name": "Cairong Zhao"
        }
    ],
    "id": "SP:b2d875b09db697e4f14b5b0635bd6c2bd439722f",
    "references": [
        {
            "authors": [
                "D. Cai",
                "W. Wang",
                "M. Li"
            ],
            "title": "Incorporating Visual Information in Audio Based Self-Supervised Speaker Recognition",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 30: 1422\u20131435.",
            "year": 2022
        },
        {
            "authors": [
                "J.S. Chung",
                "J. Huh",
                "S. Mun",
                "M. Lee",
                "H.S. Heo",
                "S. Choe",
                "C. Ham",
                "S. Jung",
                "B.-J. Lee",
                "I. Han"
            ],
            "title": "In defence of metric learning for speaker recognition",
            "venue": "Interspeech.",
            "year": 2020
        },
        {
            "authors": [
                "J.S. Chung",
                "A. Nagrani",
                "A. Zisserman"
            ],
            "title": "VoxCeleb2: Deep Speaker Recognition",
            "venue": "Interspeech.",
            "year": 2018
        },
        {
            "authors": [
                "N. Dehak",
                "P.J. Kenny",
                "R. Dehak",
                "P. Dumouchel",
                "P. Ouellet"
            ],
            "title": "Front-end factor analysis for speaker verification",
            "venue": "IEEE Transactions on Audio, Speech, and Language Processing, 19(4): 788\u2013798.",
            "year": 2010
        },
        {
            "authors": [
                "B. Desplanques",
                "J. Thienpondt",
                "K. Demuynck"
            ],
            "title": "ECAPA-TDNN: Emphasized Channel Attention, propagation and aggregation in TDNN based speaker verification",
            "venue": "Interspeech 2020, 3830\u20133834.",
            "year": 2020
        },
        {
            "authors": [
                "A. Duarte",
                "F. Roldan",
                "M. Tubau",
                "J. Escur",
                "S. Pascual",
                "A. Salvador",
                "E. Mohedano",
                "K. McGuinness",
                "J. Torres",
                "X. Giro-i Nieto"
            ],
            "title": "Wav2Pix: Speech-conditioned Face Generation Using Generative Adversarial Networks",
            "venue": "ICASSP 2019 - 2019 IEEE International Conference on",
            "year": 2019
        },
        {
            "authors": [
                "Y. Fan",
                "J. Kang",
                "L. Li",
                "K. Li",
                "H. Chen",
                "S. Cheng",
                "P. Zhang",
                "Z. Zhou",
                "Y. Cai",
                "D. Wang"
            ],
            "title": "CN-Celeb: A Challenging Chinese Speaker Recognition Dataset",
            "venue": "ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 7604\u20137608.",
            "year": 2020
        },
        {
            "authors": [
                "J. Gou",
                "B. Yu",
                "S.J. Maybank",
                "D. Tao"
            ],
            "title": "Knowledge distillation: A survey",
            "venue": "International Journal of Computer Vision (IJCV), 129(6): 1789\u20131819.",
            "year": 2021
        },
        {
            "authors": [
                "S. Gupta",
                "J. Hoffman",
                "J. Malik"
            ],
            "title": "Cross modal distillation for supervision transfer",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2827\u20132836.",
            "year": 2016
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 770\u2013778.",
            "year": 2016
        },
        {
            "authors": [
                "G. Hinton",
                "O. Vinyals",
                "J Dean"
            ],
            "title": "Distilling the knowledge in a neural network",
            "venue": "arXiv preprint arXiv:1503.02531,",
            "year": 2015
        },
        {
            "authors": [
                "N. Inoue"
            ],
            "title": "Teacher-assisted mini-batch sampling for blind distillation using metric learning",
            "venue": "ICASSP 20212021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 4160\u20134164. IEEE.",
            "year": 2021
        },
        {
            "authors": [
                "M. Kamachi",
                "H. Hill",
                "K. Lander",
                "E. Vatikiotis-Bateson"
            ],
            "title": "Putting the Face to the Voice\u2019: Matching Identity across Modality",
            "venue": "Current Biology, 13(19): 1709\u20131714.",
            "year": 2003
        },
        {
            "authors": [
                "M. Kim",
                "A.K. Jain",
                "X. Liu"
            ],
            "title": "AdaFace: Quality Adaptive Margin for Face Recognition",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).",
            "year": 2022
        },
        {
            "authors": [
                "L. Liu",
                "Q. Huang",
                "S. Lin",
                "H. Xie",
                "B. Wang",
                "X. Chang",
                "X. Liang"
            ],
            "title": "Exploring inter-channel correlation for diversity-preserved knowledge distillation",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 8271\u20138280.",
            "year": 2021
        },
        {
            "authors": [
                "A. Nagrani",
                "S. Albanie",
                "A. Zisserman"
            ],
            "title": "Seeing Voices and Hearing Faces: Cross-Modal Biometric Matching",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
            "year": 2018
        },
        {
            "authors": [
                "A. Nagrani",
                "J.S. Chung",
                "A. Zisserman"
            ],
            "title": "VoxCeleb: a large-scale speaker identification dataset",
            "venue": "Interspeech.",
            "year": 2017
        },
        {
            "authors": [
                "T.-H. Oh",
                "T. Dekel",
                "C. Kim",
                "I. Mosseri",
                "W.T. Freeman",
                "M. Rubinstein",
                "W. Matusik"
            ],
            "title": "Speech2Face: Learning the Face Behind a Voice",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).",
            "year": 2019
        },
        {
            "authors": [
                "K. Okabe",
                "T. Koshinaka",
                "K. Shinoda"
            ],
            "title": "Attentive statistics pooling for deep speaker embedding",
            "venue": "Interspeech.",
            "year": 2018
        },
        {
            "authors": [
                "W. Park",
                "D. Kim",
                "Y. Lu",
                "M. Cho"
            ],
            "title": "Relational knowledge distillation",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 3967\u20133976.",
            "year": 2019
        },
        {
            "authors": [
                "N. Passalis",
                "A. Tefas"
            ],
            "title": "Probabilistic knowledge transfer for deep representation learning",
            "venue": "CoRR, abs/1803.10837, 1(2): 5.",
            "year": 2018
        },
        {
            "authors": [
                "B. Peng",
                "X. Jin",
                "J. Liu",
                "D. Li",
                "Y. Wu",
                "Y. Liu",
                "S. Zhou",
                "Z. Zhang"
            ],
            "title": "Correlation congruence for knowledge distillation",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 5007\u20135016.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Qian",
                "Z. Chen",
                "S. Wang"
            ],
            "title": "Audio-visual deep neural network for robust person verification",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29: 1079\u20131092.",
            "year": 2021
        },
        {
            "authors": [
                "A. Romero",
                "N. Ballas",
                "S.E. Kahou",
                "A. Chassang",
                "Y. Bengio"
            ],
            "title": "FitNets: Hints for Thin Deep Nets",
            "venue": "International Conference on Learning Representation (ICLR).",
            "year": 2015
        },
        {
            "authors": [
                "L. Sar\u0131",
                "K. Singh",
                "J. Zhou",
                "L. Torresani",
                "N. Singhal",
                "Y. Saraf"
            ],
            "title": "A multi-view approach to audio-visual speaker verification",
            "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 6194\u20136198. IEEE.",
            "year": 2021
        },
        {
            "authors": [
                "F. Schroff",
                "D. Kalenichenko",
                "J. Philbin"
            ],
            "title": "Facenet: A unified embedding for face recognition and clustering",
            "venue": "In",
            "year": 2015
        },
        {
            "authors": [
                "H.M.J. Smith",
                "A.K. Dunn",
                "T. Baguley",
                "P.C. Stacey"
            ],
            "title": "Concordant Cues in Faces and Voices: Testing the Backup Signal Hypothesis",
            "venue": "Evolutionary Psychology, 14(1): 1474704916630317.",
            "year": 2016
        },
        {
            "authors": [
                "D. Snyder",
                "G. Chen",
                "D. Povey"
            ],
            "title": "MUSAN: A Music, Speech, and Noise Corpus",
            "venue": "ArXiv:1510.08484v1, .",
            "year": 2015
        },
        {
            "authors": [
                "D. Snyder",
                "D. Garcia-Romero",
                "G. Sell",
                "D. Povey",
                "S. Khudanpur"
            ],
            "title": "X-vectors: Robust dnn embeddings for speaker recognition",
            "venue": "2018 IEEE international conference on acoustics, speech and signal processing (ICASSP), 5329\u20135333. IEEE.",
            "year": 2018
        },
        {
            "authors": [
                "R. Tao",
                "R.K. Das",
                "H. Li"
            ],
            "title": "Audio-visual Speaker Recognition with a Cross-modal Discriminative Network",
            "venue": "Interspeech.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Tian",
                "D. Krishnan",
                "P. Isola"
            ],
            "title": "Contrastive Representation Distillation",
            "venue": "International Conference on Learning Representations.",
            "year": 2019
        },
        {
            "authors": [
                "F. Tung",
                "G. Mori"
            ],
            "title": "Similarity-preserving knowledge distillation",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 1365\u20131374.",
            "year": 2019
        },
        {
            "authors": [
                "Q. Wang",
                "P. Zhang",
                "H. Xiong",
                "J. Zhao"
            ],
            "title": "Face",
            "venue": "evoLVe: A High-Performance Face Recognition Library. arXiv preprint arXiv:2107.08621.",
            "year": 2021
        },
        {
            "authors": [
                "S. Zagoruyko",
                "N. Komodakis"
            ],
            "title": "Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer",
            "venue": "International Conference on Learning Representation (ICLR).",
            "year": 2017
        },
        {
            "authors": [
                "L. Zhang",
                "Z. Chen",
                "Y. Qian"
            ],
            "title": "Knowledge Distillation from Multi-Modality to Single-Modality for Person Verification",
            "venue": "Proc. Interspeech 2021, 1897\u20131901.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "Introduction",
            "text": "A wealth of information is contained in the speaker\u2019s voice which can be abstracted into different properties, such as gender, age, tone, etc. These properties can contribute to identity recognition, i.e. speaker recognition. Speaker recognition is widely applied in the real world. For example, in some smart audio devices, personalized configurations can be loaded by recognizing the speaker. In addition, it has important applications in security systems, investigation, forensics, etc. I-vector (Dehak et al. 2010) is a traditional speaker recognition method. With the popularity of\n*Corresponding author Copyright \u00a9 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\ndeep learning, neural networks (Nagrani, Chung, and Zisserman 2017; Snyder et al. 2018; Okabe, Koshinaka, and Shinoda 2018) started to become the mainstream and achieved promising progress. However, speaker recognition is fundamentally difficult, the voices of two speakers are likely very similar, since the population (the number of speakers) can be very large and the information density of speech is very low. The intuitive solution is to collect bigger data and design more effective neural networks, however, this solution maybe not very efficient and does not approach the fundamental difficulty: the low information density of speech data.\nMultimodal learning mitigates the challenges of speaker recognition at their root by introducing richer and more discriminative information as input for speaker recognition. For identity recognition, face images clearly convey much more identity-related information than voice, and faces are more widely applied than speech for identity recognition. It inspires researchers to use both face and speech for identity recognition (Tao, Das, and Li 2020; Sar\u0131 et al. 2021; Qian, Chen, and Wang 2021). Their experimental results show that the multimodal speaker recognition is much stronger than that using speech only as input. However, their multimodal speaker recognition assumes face and speech are always available during the training and test periods. Actually, many applications have only speech input and do not have face images as input, e.g. most smart speakers do not have cameras.\nMultimodal learning motivates us to ask a question: can we use multimodal learning for the scenarios of speech input only? This question triggers us to bridge the multimodal and speech only identity recognition by introducing a new setting: training data with both annotated speech and face data and test set with speech data only. Existing works (Inoue 2021; Cai, Wang, and Li 2022) have a similar multimodal setting, but their training data is unlabelled.\nWe think our aforementioned setting is technically feasible. This feasibility results from (1) technical soundness and (2) available data. For (1), we assume the two modalities (face and speech) are highly correlated for identity recognition. In this way, the feature space of the networks trained separately by these two modalities is also correlated. Based on this assumption, the stronger modality (face) can transfer the knowledge in feature space to the weaker modality\n(speech) during the training, making the student network more discriminative than that without teacher (face model) supervision. During the test period, the speech network only, which already learns the discriminative knowledge from the teacher, can conduct robust identity recognition. This process is illustrated in Fig. 1. Then we explain the soundness of the assumption where face and speech are very correlated for identity recognition. Biological studies (Kamachi et al. 2003; Smith et al. 2016) show that human speech is correlated with facial appearance, and some attributes (such as gender, age, race, and some hormone levels) influence both appearance and voice. The success of voice-face crossmodality matching (Nagrani, Albanie, and Zisserman 2018) and speech-to-face generation (Duarte et al. 2019; Oh et al. 2019) tasks also demonstrates that this correlation can be learned by neural networks. For (2), fortunately, a multimodal recognition dataset (Chung, Nagrani, and Zisserman 2018) with aligned face and speech data is available. With this dataset, we can investigate our solutions under the aforementioned setting.\nTo achieve the knowledge transfer from a stronger modality (face) to a weaker one (speech) during training, we introduce knowledge distillation. Knowledge distillation is a technique to supervise the training of a student network by a stronger teacher model. Knowledge distillation is well studied and has widely been applied to speech analysis, computer vision, etc. From the source of knowledge, knowledge distillation can be categorized as feature-based, relationbased, and response-based methods (Gou et al. 2021). Feature-based methods (Romero et al. 2015) conduct distillation using the output from the last or intermediate layers of the neural network. Relation-based methods (Tung and Mori 2019; Park et al. 2019; Peng et al. 2019) capture the relationship between different samples or different layers as knowledge. Response-based methods (Hinton et al. 2015) use the output of the last layer of networks, i.e. logits for\nknowledge transfer. However, we empirically find that simply applying existing knowledge distillation methods to our cross-modal teacher-student learning cannot achieve desirable performance. We think two possible reasons lead to this degraded performance: (1) the big domain gap in latent space between two modalities and (2) the quality of input data (face and speech) not well aligned. Specifically, for (1), most existing knowledge distillation methods minimize the difference (l1 or l2 norm) between the teacher and student, forcing the student to behave exactly the same as the teacher. However, the domain gap between two modalities (face and speech) is clearly big and forcing them to be exactly the same can easily lead to overfitting. For (2), empirically, if the data quality of two streams (face and speech) does not match, e.g. a very blurry face supervises an audio sequence of good quality, the performance of distillation will be degraded.\nIn this paper, we propose a VGSR (Vision-Guided Speaker Recognition) method which can improve the accuracy and generalizability of speaker recognition. Specifically, we propose a distillation strategy, MKD (Marginbased Knowledge Distillation), which introduces a loose constraint with a margin between two modalities instead of forcing them to be exactly the same. Our MKD can facilitate the student to learn the discriminative features while avoiding overfitting to learn irrelevant features. Furthermore, our MKD can easily adapt to many mainstream distillation loss functions. Specifically, we reformulate the existing featurebased distillation (Romero et al. 2015), relation-based distillation (Tung and Mori 2019), and response-based distillation (Hinton et al. 2015) loss functions to adapt them to our cross-modality knowledge transfer. As aforementioned, the quality of input data can greatly affect the distillation performance. Thus, we propose a weighting method, QAW (Quality-based Adaptive Weights), which quantifies the data quality by l2 norm and then weights the samples using the quality scores for distillation, greatly improve the model robustness.\nOur method is trained on the multimodal dataset VoxCeleb2 (Chung, Nagrani, and Zisserman 2018) and evaluated on the speech dataset in VoxCeleb1 (Nagrani, Chung, and Zisserman 2017) and CN-Celeb (Fan et al. 2020). The results show that the VGSR method can effectively improve the performance of speaker recognition (around 10% \u223c 15%). In addition, since our method is quality-aware, and less affected by low quality samples, showing promising model robustness, e.g. robustness against noises.\nOur contributions can be summarized as:\n\u2022 We introduce a practical and technically feasible setting to the society: annotated face and speech data for training, and speech data only for test. Based on this setting, we propose a cross-modality learning method VGSR, leading to the promising accuracy and model robustness for speaker recognition.\n\u2022 Due to the big domain gap of different modalities, the existing knowledge distillation methods do not work well for cross-modality knowledge transfer. We propose a distillation strategy, MKD, which introduces a loose align-\nment between the teacher and student, to effectively avoid overfitting caused by the modality domain gap. Our MKD can work in a plug-and-play way, easily adapting to existing distillation loss functions.\n\u2022 We propose a quality-aware sample weighting method, QAW, which can improve the robustness of our method, effectively avoiding the negative effect caused by low quality samples.\n\u2022 We conduct extensive experiments on VoxCeleb1, VoxCeleb2, and CN-Celeb, the results show that our method can effectively improve the accuracy of speaker recognition, and the robustness against noises."
        },
        {
            "heading": "Related Work",
            "text": ""
        },
        {
            "heading": "Speaker Recognition",
            "text": "Speaker recognition is the task of voice-based biometric identification, which plays an important role in smart voice assistants. I-vector (Dehak et al. 2010) is a traditional speaker recognition method.\nRecently, the deep neural network has achieved better performance in speaker recognition than the traditional method. Snyder et al. (Snyder et al. 2018) proposed X-Vector, an early speaker recognition model for deep learning. Chung et al. (Chung, Nagrani, and Zisserman 2018) propose the VoxCeleb2 dataset, which is one of the most commonly used speaker recognition datasets, and establish a new benchmark using the ResNet model (He et al. 2016). Okabe et al. (Okabe, Koshinaka, and Shinoda 2018) propose the attentive statistics pooling method, which does attention weighting and adds statistics when pooling. Chung et al. (Chung et al. 2020) compare various loss functions in speaker recognition tasks and proposes angular prototypical loss to achieve the best results. Desplanques et al. (Desplanques, Thienpondt, and Demuynck 2020) propose a powerful speaker recognition model ECAPA-TDNN.\nSome researchers also try to combine face images and audio for audio-visual speaker recognition. With the addition of visual modalities, promising recognition performance can be achieved. Sar et al. (Sar\u0131 et al. 2021) propose to learn joint audio-visual embeddings and perform cross-modal verification. Qian et al. (Qian, Chen, and Wang 2021) proposes and compares multiple multimodal fusion methods. However, this type of method may encounter the problem that the face image cannot be acquired when it is used in practice. Unlike them, we use speech input only during test."
        },
        {
            "heading": "Knowledge Distillation",
            "text": "Knowledge distillation is a technique that uses a highperformance teacher model to guide student model training and is often used for model compression. As for the type of knowledge, Gou et al. (Gou et al. 2021) classifies knowledge distillation into three categories: feature-based, relation-based, and response-based. Feature-based methods use features from the last or intermediate layers to distill. Zagoruyko and Komodakis (Zagoruyko and Komodakis 2017) propose the attention transfer method, which encouraging the student model to learn the spatial attention distribution of the teacher model. Relation-based methods use\nrelationships between samples. Tung and Mori (Tung and Mori 2019) propose the similarity preserving method, encouraging the similarity of activations between samples and samples of the teacher model and the student model to be consistent. Response-based methods use the output of the last layer of the model, the logits, for distillation. The earliest knowledge distillation method (Hinton et al. 2015) is achieved by minimizing the kl divergence between the logits of the teacher model and the student model.\nAs for cross-modal knowledge distillation, most methods operate between two very similar modalities. Gupta et al. (Gupta, Hoffman, and Malik 2016) propose a method to use a model of RGB images as a teacher model to guide the training of depth and optical flow image models. Tian et al. (Tian, Krishnan, and Isola 2019) proposes contrastive representation distillation, which implements cross-modal distillation from RGB images to depth images. Some methods that distill between disparate modalities usually use the teacher modality to provide labels for the student model in an unsupervised setting. Inoue (Inoue 2021) uses the face recognition model to provide positive and negative pairs for unlabeled speech data, and uses metric learning to train the speaker recognition model. Unlike them which use an unsupervised setting, in this work, we explore a supervised crossmodal distillation. Zhang et al. (Zhang, Chen, and Qian 2021) use a multi-modal teacher of face and speech to guide the training of single-modal student, and find that the gap between the speech and the teacher system is large, making it difficult to improve the performance. This paper focuses on solving problems such as large modal gaps."
        },
        {
            "heading": "Method",
            "text": "Our method uses a dual stream of visual and speech inputs in the training phase to transfer knowledge from the visual modality to the speech modality. In the test phase, the speech model only conducts identity recognition."
        },
        {
            "heading": "Algorithmic Overview",
            "text": "The overall structure of our proposed method VGSR (Vision-Guided Speaker Recognition) is shown in Fig. 2. We have two networks, teacher (face encoder) and student (audio encoder). The teacher network transfers the knowledge to student via the proposed Margin-based Knowledge Distillation (MKD). To make the MKD more robust, the Qualitybased Adaptive Weight (QAW) is used to weight the samples based on data quality to avoid the negative effects of low-quality data. In practice, we find the performance is not satisfying if we directly use face features from a pre-trained face recognition model because of the big domain gap between face and speech. We use a projection head, a threelayer MLP, to narrow down the domain gap.\nFormally, we have a face image IT , subscript T means teacher. The image is encoded by a pre-trained face encoder Encoderf and we obtain the features ET as shown in Eq. (1). In order to extract speech-related features, the original features ET are further fine-tuned through the projection head, a three-layer MLP . To avoid overfitting by finetuning, the original features ET and the fine-tuned features\nare mixed in a certain ratio \u03b1, to obtain the final teacher features FT , as shown in Eq. (2).\nET = Encoderf (IT ) (1)\nFT = \u03b1 \u00b7 ET + (1\u2212 \u03b1) \u00b7MLP (ET ) (2) As for the speech branch, the input speech is encoded by the audio encoder to obtain the student feature FS . The total loss is the combination of the identity loss supervised with label information in speaker recognition and the distillation loss MKD, and the distillation loss has an adaptive sample weight w generated by the QAW module,\nL = Lidentity(FS , label) +w \u00b7 Ldistillation(FS , FT ) (3)\nWe detail MKD and QAW in the next section."
        },
        {
            "heading": "Margin-Based Knowledge Distillation (MKD)",
            "text": "Empirically, we cannot achieve desirable performance if we simply use the existing knowledge distillation method. After extensive experiments and analysis, we realize the existing knowledge distillation methods use l1 or l2 loss to force the student to learn to be exact to the teacher. It works well if the domain gap between teacher and student is relatively small. However, in our task, the cross-modality domain gap between face and speech is very big. The existing knowledge distillation methods tend to cause overfitting. Motivated by metric learning (Schroff, Kalenichenko, and Philbin 2015) which usually uses a margin to separate positive and negative pairs, we introduce a margin m to relatively loosely align the teacher and student. In this way, we do not push\nthe student forward to be exactly the same as the teacher. Instead, we use a margin m to bound the maximal similarity between face and speech. This margin brings a mechanism that can potentially ask the student to learn the discriminative information from the teacher and effectively avoid overfitting.\nThe existing distillation methods can simply categorized as feature-based, relation-based, and response-based according to Gou et al. (Gou et al. 2021). Our Margin-based knowledge distillation (MKD) can easily adapted to these mainstream distillation methods. Then we formulate MKD for different distillation methods.\nFeature-based. Feature-based knowledge distillation aligns the teacher and student using the output from the last or intermediate layers of the model. We empirically find the cosine similarity works better than l2 similarity for featurebased distillation. Cosine similarity requires angular similarity rather than numerical equality.\nOur MKD introduces a hyperparameter m to construct the distillation loss over cosine similarity,\nLfea = \u230a m\u2212 FT \u00b7 FS\n\u2225FT \u2225 \u00b7 \u2225FS\u2225\n\u230b (4)\nwhere \u230a\u230b means cut down to 0, equivalent to max(\u00b7, 0). Clearly, when the similarity between the two modalities reaches m, the loss becomes 0.\nRelation-based. Relation-based knowledge distillation usually exploits the relationship between different samples. The typical method is to maintain the similarity between samples, which helps to learn the structural features in the teacher model. Specifically, after obtaining the output feature F \u2208 Rb\u00d7c, where b is the batch size and c is the num-\nber of channels of the feature map, we calculate the cosine similarity between every two samples, and a b\u00d7 b similarity matrix can be obtained, as shown in Eq. (5).\nFn = F/ \u2225F\u2225 ; G = Fn \u00b7 FTn (5) Then the goal of the distillation loss is to let the student model learn the similarity of the teacher model. Our MKD introduces a hyperparameter m to achieve,\nLrel = \u230a (GT \u2212GS)2 \u2212m \u230b (6)\nClearly, the loss becomes 0 when the difference of two similarity matrices from two modalities is smaller than m.\nResponse-based. Response-based knowledge distillation uses logits to align the teacher and student. Logits L is the vector obtained after passing through the classification layer. The logits can introduce very high-level information used by teacher to supervise the student.\nAgain, we introduce a margin m for relaxation to achieve,\nLres = \u230a (LT \u2212 LS)2 \u2212m \u230b (7)"
        },
        {
            "heading": "Quality-Based Adaptive Weight (QAW)",
            "text": "In our practice of cross-modal distillation, we find the quality of data can greatly affect the performance. It is not hard to understand this. For example, if the quality of the face image is poor (e.g blurry faces) and the audio data is very high, the teacher (face model) potentially misleads the student. It motivates us to weight the samples by data quality.\nThere exist many specialized models which can quantify data quality, however, these models are usually computationally expensive. Kim et al. (Kim, Jain, and Liu 2022) prove that there is a high correlation between feature norm and input sample quality. Therefore, we use this simple and effective way to quantify the data quality. In this work, we quantify data quality by using l2 norm of the features \u2225zi\u2225, and normalizing it to remove the effect of numerical size,\nQi = \u2225zi\u2225 \u2212 \u00b5z\n\u03c3z (8)\nwhere \u00b5z and \u03c3z are the mean and standard deviation of all \u2225zi\u2225 in a batch.\nThe final sample weight is determined by the difference of data quality over two modalities,\n\u2206Q = QT \u2212QS (9)\nwi = e\u2206Qi\u2211N j=0 e \u2206Qj (10)\nwhere N is the batch size."
        },
        {
            "heading": "Experiments",
            "text": ""
        },
        {
            "heading": "Implementation Details",
            "text": "Input. During training, we use randomly cropped 2 seconds speech segments, without any other data enhancements. 40- dimensional filter-banks (Fbank) with a window of width 25ms and step 10ms are used as the input. For the visual modality, we take 1 frame from each video, crop out the face\npart, then align it, and finally scale it to 112\u00d7112 size as the input of the network.\nDatasets. Our model is trained on the VoxCeleb2 (Chung, Nagrani, and Zisserman 2018) dataset and we do the evaluation on the VoxCeleb1 (Nagrani, Chung, and Zisserman 2017) dataset. Both datasets are collected from Youtube. VoxCeleb1 dataset contains 1,251 speakers and over 100,000 utterances, only has data for audio modality. Its original test set contains 37,720 randomly selected pairs, and the hard-set test set contains 552,536 pairs of the same race and gender. VoxCeleb2 dataset contains over a million utterances from over 6,000 speakers and provides both audio and visual modalities. There are no common speakers in the two datasets.\nTo test the generalizability across datasets, tests were also performed on the CN-Celeb (Fan et al. 2020) dataset. The CN-Celeb dataset contains 11 genres of interviews, singing, movies, etc., in Chinese language. It is very different from the training dataset VoxCeleb, which can effectively test the generalization. Its test set contains 18,849 utterances from 200 speakers and provides 3,484,292 test pairs, which can largely eliminate chance.\nModel. For the speaker recognition model, we use XVector (Snyder et al. 2018), VGGM (Nagrani, Chung, and Zisserman 2017), ECAPA-TDNN (Desplanques, Thienpondt, and Demuynck 2020) and ResNet34 (with ASP (Okabe, Koshinaka, and Shinoda 2018) for aggregate temporal frames). These models are representatives of the most commonly used and advanced models in speaker recognition. The loss function is a combination of angular prototypical loss (Chung et al. 2020) and cross-entropy loss. The teacher model is a pretrained face recognition model IR-50 taken from (Wang et al. 2021).\nOptimization. We use the Adam optimizer with an initial learning rate of 1e-3 decreasing by 25% every 3 epochs and a weight decay of 5e-5. Each batch has 100 speakers, and each speaker has 2 audio utterances. The network is trained for 36 epochs, on an Nvidia RTX 3090 GPU, it takes about 9 hours to train X-Vector and about 2 days to train ResNet34.\nEvaluation. Ten 4-second temporal crops are sampled from each test segment for evaluation, and we calculate the distance between all possible pairs (10 \u00d7 10 = 100), and use the mean distance as the score. This is the same as (Chung, Nagrani, and Zisserman 2018; Chung et al. 2020). We report two most commonly used evaluation metrics in speaker recognition: the Equal Error Rate (EER) and the minimum value of Cdet. EER is the rate at which both acceptance and rejection errors are equal. And Cdet can be calculated by\nCdet = Cmiss\u00d7Pmiss\u00d7Ptar+Cfa\u00d7Pfa\u00d7(1\u2212Ptar) (11)\nwhere we assume a prior target probability Ptar of 0.01 and equal weights of 1.0 between misses Cmiss and false alarms Cfa.\nTo evaluate the robustness under noise, we use the musan dataset (Snyder, Chen, and Povey 2015) to augment our test set. For each piece of test audio, we mix a piece of noise. We set different noise levels based on the decibel gap between\nthe original speech and the noisy speech. The larger the \u2206db, the smaller the noise.\n\u2206db = dbaudio \u2212 dbnoise (12)"
        },
        {
            "heading": "Baseline Results",
            "text": "We first compare the performance of popular speaker recognition methods: X-Vector (Snyder et al. 2018), VGGM (Nagrani, Chung, and Zisserman 2017), ECAPA (Desplanques, Thienpondt, and Demuynck 2020), and ResNet34 (Chung et al. 2020), the input is unified as a 40-dimensional fbank without data augmentation, and the results are shown in Table 2. ResNet34 model achieves the best performance.\nWe then compare the student (speech model) with the teacher (face model). The ResNet34 model is used for student model which achieve best performance in speech. Since the VoxCeleb1 does not have visual modality data which is needed by the teacher, this test was performed on the test set of the VoxCeleb2 dataset. The results in Table 3 show that the pre-trained face recognition model without fine-tuning greatly outperforms the audio model, justifying our assumption that the teacher (face model) is much stronger than the student (speech model)."
        },
        {
            "heading": "Comparisons with State-of-the-Art",
            "text": "Note that not all distillation methods can be used for our task. For example, since the spatial locations of visual and speech modalities are not correlated, methods using spatial locations (Zagoruyko and Komodakis 2017) for distillation cannot be applied. Apart from these methods, we compare with some very popular distillation methods (Hinton et al. 2015; Passalis and Tefas 2018; Tung and Mori 2019; Liu et al. 2021) in Table 1. Results show that these methods cannot effectively improve the performance due to the overfitting. Clearly our MKD family can greatly improve the performance by introducing a loose distillation which can effectively reduce overfitting.\nApart from ResNet-34, we also test our method using a light-weight X-Vector (Snyder et al. 2018) model in Table 4. Since ICKD (Liu et al. 2021) method cannot work with XVector, we only compare with KD (Hinton et al. 2015), PKD (Passalis and Tefas 2018) and SP (Tung and Mori 2019) methods. Results show our method can achieve the greatest performance gains."
        },
        {
            "heading": "Comparisons with Cross-Dataset Settings",
            "text": "To evaluate the cross-dataset generalizability, the ResNet34 model is trained on VoxCeleb2 and tested on CN-Celeb. These two datasets are very different in terms of video scenes and languages. Since the CN-Celeb dataset is collected from a variety of actual scenarios such as interviews, singing, vlog, etc., it is more difficult and has a higher error rate compared to VoxCeleb as shown in Table 5. Compared to the baseline, our proposed approach VGSR effectively improves the cross-dataset generalizability."
        },
        {
            "heading": "Ablation Study",
            "text": "In this section, we conduct abation study to verify the effectiveness of different components of our methods. We perform this study using ResNet34 on VoxCeleb1.\nEffect of projection head hyperparameter \u03b1. The face embedding from a pretrained face model might have a big discrepancy against speech embedding. We propose a learnable projection head to reduce this discrepancy with the expectation that the feature from projection head can more easily align with the speech feature. This feature itself can degrade the face recognition performance of the original feature. Thus, we introduce a hyperparameter \u03b1 in Eq. (2) to balance these two features. From Table 6, if \u03b1 is too small, it means we mainly rely on the projected feature, the face recognition performance might be degraded greatly; If \u03b1 is too large, we mainly use the original face embedding, which is very far from speech feature. It will cause difficulty for teacher-student distillation. Neither is the best choice, choosing an intermediate value of 0.6 gives the best results.\nEffect of distillation hyperparameter m. The margin m controls the degree of similarity that the teacher and student should achieve. We use feature-based distillation for this experiment. From Table 7, if m is too large, the constraints are too loose and the supervision is weak; If the m is too small, it may lead to over-fitting. The best performance is achieved by cos(30\u25e6).\nEffect of MKD and QAW. We conduct evaluations on the original test set of VoxCeleb1 and its noisy version by adding various noises from musan dataset. As shown in Table 1, different types of knowledge can effectively\nbe improved, and the feature-based knowledge can achieve the biggest improvements. In addition, the performance achieved by using QAW is further improved, especially in the case of noises."
        },
        {
            "heading": "Visualization of Features",
            "text": "To understand the features we learned, we visualize the features extracted by the face model, its projection head, and the speech model after applying our MKD and QAW. To achieve these features, we use 200 samples randomly selected from the test set. We conduct dimensionality reduction by t-SNE on these features. From Fig. 3, we can see that there is a large domain gap between the two modalities of face (red) and speech (blue), which is difficult to align them directly. After the mapping of the projection head (green), the distance between face and speech features is reduced, making the teacher-student distillation easier. In addition, the features extracted by the projection head and speech model do not completely overlap because the MKD strategy uses a margin to avoid overfitting. Moreover, based on the annotation of gender (dark), it can be seen that the features mapped by the projection head well indicate gender, which is a shared attribute in face and speech, justifying our assumption that face and speech are highly correlated in feature space."
        },
        {
            "heading": "Conclusion",
            "text": "In this paper, we propose the VGSR method that utilizes a more discriminative face recognition model as a teacher to guide the student (speech) to improve the performance of speaker recognition. To achieve a promising cross-modal distillation performance between vision-speech modalities, we propose the MKD distillation method and a qualityaware weighting strategy, QAW. Experiments show that our method can effectively transfer discriminative knowledge from face to speech. We hope our cross-modality knowledge transfer strategies can introduce insights into other multimodal learning tasks."
        },
        {
            "heading": "Acknowledgments",
            "text": "This work was supported by National Natural Science Fund of China (62076184, 61976158, 61976160, 62076182, 62276190), in part by Shanghai Innovation Action Project of Science and Technology (20511100700) and Shanghai Natural Science Foundation (22ZR1466700), in part by Fundamental Research Funds for the Central Universities and State Key Laboratory of Integrated Services Networks (Xidian University)."
        }
    ],
    "title": "Cross-Modal Distillation for Speaker Recognition",
    "year": 2023
}