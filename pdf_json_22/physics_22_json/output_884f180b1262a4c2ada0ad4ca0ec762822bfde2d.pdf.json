{
    "abstractText": "The results of the Anomalous Diffusion Challenge (AnDi Challenge) [30] have shown that machine learning methods can outperform classical statistical methodology at the characterization of anomalous diffusion in both the inference of the anomalous diffusion exponent \u03b1 associated with each trajectory (Task 1), and the determination of the underlying diffusive regime which produced such trajectories (Task 2). Furthermore, of the five teams that finished in the top three across both tasks of the AnDi challenge, three of those teams used recurrent neural networks (RNNs). While RNNs, like the long short-term memory (LSTM) network, are effective at learning long-term dependencies in sequential data, their key disadvantage is that they must be trained sequentially. In order to facilitate training with larger data sets, by training in parallel, we propose a new transformer based neural network architecture for the characterization of anomalous diffusion. Our new architecture, the Convolutional Transformer (ConvTransformer) uses a bi-layered convolutional neural network to extract features from our diffusive trajectories that can be thought of as being words in a sentence. These features are then fed to two transformer encoding blocks that perform either regression (Task 1) or classification (Task 2). To our knowledge, this is the first time transformers have been used for characterizing anomalous diffusion. Moreover, this may be the first time that a transformer encoding block has been used with a convolutional neural network and without the need for a transformer decoding block or positional encoding. Apart from being able to train in parallel, we show that the ConvTransformer is able to outperform the previous state of the art at determining the underlying diffusive regime (Task2) in short trajectories (length 10-50 steps), which are the most important for experimental researchers.",
    "authors": [
        {
            "affiliations": [],
            "name": "Nicol\u00e1s Firbas"
        },
        {
            "affiliations": [],
            "name": "\u00d2scar Garibo-i-Orts"
        },
        {
            "affiliations": [],
            "name": "Miguel \u00c1ngel Garcia-March"
        },
        {
            "affiliations": [],
            "name": "J. Alberto Conejero"
        }
    ],
    "id": "SP:9e142f09070c869d195cfc9cfb650e397868f988",
    "references": [
        {
            "authors": [
                "A. Argun",
                "G. Volpe",
                "S. Bo"
            ],
            "title": "Classification, inference and segmentation of anomalous diffusion with recurrent neural networks",
            "venue": "J. Phys. A Math. Theor., 54,",
            "year": 2021
        },
        {
            "authors": [
                "S. Bai",
                "J.Z. Kolter",
                "V. Koltun"
            ],
            "title": "An empirical evaluation of generic convolutional and recurrent networks for sequence modeling",
            "venue": "arXiv preprint arXiv:1803.01271,",
            "year": 2018
        },
        {
            "authors": [
                "S. Bo",
                "F. Schmidt",
                "R. Eichhorn",
                "G. Volpe"
            ],
            "title": "Measurement of anomalous diffusion using recurrent neural networks",
            "venue": "Phys. Rev. E, 100(1):010102,",
            "year": 2019
        },
        {
            "authors": [
                "I. Bronstein",
                "Y. Israel",
                "E. Kepten",
                "S. Mai",
                "Y. Shav-Tal",
                "E. Barkai",
                "Y. Garini"
            ],
            "title": "Transient anomalous diffusion of telomeres in the nucleus of mammalian cells",
            "venue": "Phys. Rev. Lett., 103:018102, Jul",
            "year": 2009
        },
        {
            "authors": [
                "R. Brown"
            ],
            "title": "A brief account of microscopical observations made in the months of june, july and august 1827, on the particles contained in the pollen of plants; and on the general existence of active molecules in organic and inorganic bodies",
            "venue": "Philosoph. Mag., 4(21):161\u2013173,",
            "year": 1828
        },
        {
            "authors": [
                "A. Bunde"
            ],
            "title": "Fractals in science with a MS-DOS program Diskette",
            "venue": "Springer,",
            "year": 1994
        },
        {
            "authors": [
                "K. Clark",
                "U. Khandelwal",
                "O. Levy",
                "C.D. Manning"
            ],
            "title": "What does BERT look at? an analysis of BERT\u2019s attention, 2019",
            "year": 2019
        },
        {
            "authors": [
                "P. Dosset",
                "P. Rassam",
                "L. Fernandez",
                "C. Espenel",
                "E. Rubinstein",
                "E. Margeat",
                "P.-E. Milhiet"
            ],
            "title": "Automatic detection of diffusion modes within biological membranes using back-propagation neural network",
            "venue": "BMC Bioinform., 17(1):1\u201312,",
            "year": 2016
        },
        {
            "authors": [
                "\u00d2. Garibo-i Orts",
                "A. Baeza-Bosca",
                "M.A. Garcia-March",
                "J.A. Conejero"
            ],
            "title": "Efficient recurrent neural network methods for anomalously diffusing single particle short and noisy trajectories",
            "venue": "J. Phys. A Math. Theor., 54(50):504002, nov",
            "year": 2021
        },
        {
            "authors": [
                "A. Gentili",
                "G. Volpe"
            ],
            "title": "Characterization of anomalous diffusion classical statistics powered by deep learning (CONDOR)",
            "venue": "Journal of Physics A: Mathematical and Theoretical, 54(31):314003, jul",
            "year": 2021
        },
        {
            "authors": [
                "N. Granik",
                "L.E. Weiss",
                "E. Nehme",
                "M. Levin",
                "M. Chein",
                "E. Perlson",
                "Y. Roichman",
                "Y. Shechtman"
            ],
            "title": "Single-particle diffusion characterization by deep learning",
            "venue": "Biophys. J., 117(2):185\u2013192,",
            "year": 2019
        },
        {
            "authors": [
                "J. Guo",
                "K. Han",
                "H. Wu",
                "C. Xu",
                "Y. Tang",
                "Y. Wang"
            ],
            "title": "CMT: Convolutional neural networks meet vision transformers, 2021",
            "year": 2021
        },
        {
            "authors": [
                "S. Hochreiter",
                "J. Schmidhuber"
            ],
            "title": "Long short-term memory",
            "venue": "Neural Comput., 9(8):1735\u20131780,",
            "year": 1997
        },
        {
            "authors": [
                "J. Janczura",
                "P. Kowalek",
                "H. Loch-Olszewska",
                "J. Szwabi\u0144ski",
                "A. Weron"
            ],
            "title": "Classification of particle trajectories in living cells: Machine learning versus statistical testing hypothesis for fractional anomalous diffusion",
            "venue": "Phys. Rev. E, 102(3), Sep",
            "year": 2020
        },
        {
            "authors": [
                "J. Klafter",
                "G. Zumofen"
            ],
            "title": "L\u00e9vy statistics in a hamiltonian system",
            "venue": "Phys. Rev. E, 49:4873\u20134877, Jun",
            "year": 1994
        },
        {
            "authors": [
                "P. Kowalek",
                "H. Loch-Olszewska",
                "J. Szwabi\u0144ski"
            ],
            "title": "Classification of diffusion modes in single-particle tracking data: Feature-based versus deep-learning approach",
            "venue": "Phys. Rev. E, 100(3):032410,",
            "year": 2019
        },
        {
            "authors": [
                "D. Li",
                "Q. Yao",
                "Z. Huang"
            ],
            "title": "WaveNet-based deep neural networks for the characterization of anomalous diffusion (WADNet)",
            "venue": "J. Phys. A: Math. Theor., 54(40):404003, sep",
            "year": 2021
        },
        {
            "authors": [
                "S.C. Lim",
                "S.V. Muniandy"
            ],
            "title": "Self-similar gaussian processes for modeling anomalous diffusion",
            "venue": "Phys. Rev. E, 66:021114, Aug",
            "year": 2002
        },
        {
            "authors": [
                "Zhouyong Liu",
                "Shun Luo",
                "Wubin Li",
                "Jingben Lu",
                "Yufan Wu",
                "Chunguo Li",
                "Luxi Yang"
            ],
            "title": "Convtransformer: A convolutional transformer network for video frame synthesis",
            "year": 2011
        },
        {
            "authors": [
                "H. Loch-Olszewska",
                "J. Szwabi\u0144ski"
            ],
            "title": "Impact of feature choice on machine learning classification of fractional anomalous diffusion",
            "venue": "Entropy, 22(12),",
            "year": 2020
        },
        {
            "authors": [
                "A.F. dos Santos Maike",
                "L. Menon Junior"
            ],
            "title": "Random diffusivity models for scaled brownian motion",
            "venue": "Chaos, Solitons & Fractals,",
            "year": 2021
        },
        {
            "authors": [
                "B.B. Mandelbrot",
                "J.W. Van Ness"
            ],
            "title": "Fractional Brownian motions, fractional noises and applications",
            "venue": "SIAM Review, 10(4):422\u2013437,",
            "year": 1968
        },
        {
            "authors": [
                "C. Manzo",
                "M.F. Garcia-Parajo"
            ],
            "title": "A review of progress in single particle tracking: from methods to biophysical insights",
            "venue": "Rep. Prog. Phys., 78(12):124601,",
            "year": 2015
        },
        {
            "authors": [
                "P. Massignan",
                "C. Manzo",
                "J.A. Torreno-Pina",
                "M.F. Gar\u0107\u0131a-Parajo",
                "M. Lewenstein",
                "G.J. Lapeyre"
            ],
            "title": "Nonergodic subdiffusion from Brownian motion in an inhomogeneous medium",
            "venue": "Phys. Rev. Lett., 112:150603, Apr",
            "year": 2014
        },
        {
            "authors": [
                "B. Mehus Sunde"
            ],
            "title": "Early stopping for Pytorch, 2020. [Online; accessed 6-Nov-2020",
            "year": 2020
        },
        {
            "authors": [
                "R. Metzler",
                "J.-H. Jeon",
                "A.G. Cherstvy",
                "E. Barkai"
            ],
            "title": "Anomalous diffusion models and their properties: non-stationarity, non-ergodicity, and ageing at the centenary of single particle tracking",
            "venue": "Phys. Chem. Chem. Phys., 16(44):24128\u201324164,",
            "year": 2014
        },
        {
            "authors": [
                "G. Mu\u00f1oz-Gil",
                "M.A. Garcia-March",
                "C. Manzo",
                "J.D. Mart\u0301\u0131n-Guerrero",
                "M. Lewenstein"
            ],
            "title": "Single trajectory characterization via machine learning",
            "venue": "New J. Phys.,",
            "year": 2020
        },
        {
            "authors": [
                "G. Mu\u00f1oz-Gil",
                "G. Volpe",
                "M.A. Gar\u0107\u0131a-March",
                "R. Metzler",
                "M. Lewenstein",
                "c. Manzo"
            ],
            "title": "The Anomalous Diffusion challenge: objective comparison of methods to decode anomalous diffusion",
            "venue": "Emerging Topics in Artificial Intelligence (ETAI) 2021,",
            "year": 2021
        },
        {
            "authors": [
                "G. Mu\u00f1oz-Gil",
                "B. Requena",
                "G. Volpe",
                "M.A. Garcia-March",
                "C. Manzo"
            ],
            "title": "Andichallenge/andi datasets: Challenge 2020 release",
            "venue": "May",
            "year": 2021
        },
        {
            "authors": [
                "G. Mu\u00f1oz-Gil",
                "G. Volpe",
                "M.A. Garcia-March",
                "E. Aghion",
                "A. Argun",
                "C.B. Hong",
                "T. Bland",
                "S. Bo",
                "J.A.N. Conejero",
                "Firbas"
            ],
            "title": "Objective comparison of methods to decode anomalous diffusion",
            "venue": "Nature Communications,",
            "year": 2021
        },
        {
            "authors": [
                "N. Nagaya",
                "N. Mizumoto",
                "M.S. Abe",
                "S. Dobata",
                "R. Sato",
                "R. Fujisawa"
            ],
            "title": "Anomalous diffusion on the servosphere: A potential tool for detecting inherent organismal movement patterns",
            "venue": "PLOS ONE, 12(6):1\u201315, 06",
            "year": 2017
        },
        {
            "authors": [
                "J. Perrin"
            ],
            "title": "Movement brownien et realite molec",
            "venue": "Ann. Chim. Phys., 18:1\u2013114,",
            "year": 1909
        },
        {
            "authors": [
                "Yoav Sagi",
                "Miri Brook",
                "Ido Almog",
                "Nir Davidson"
            ],
            "title": "Observation of anomalous diffusion and fractional self-similarity in one dimension",
            "venue": "Phys. Rev. Lett.,",
            "year": 2012
        },
        {
            "authors": [
                "H. Scher",
                "E.W. Montroll"
            ],
            "title": "Anomalous transit-time dispersion in amorphous solids",
            "venue": "Phys. Rev. B, 12:2455\u20132477, Sep",
            "year": 1975
        },
        {
            "authors": [
                "S.L. Smith",
                "P.J. Kindermans",
                "C. Ying",
                "Q.V. Le"
            ],
            "title": "Don\u2019t decay the learning rate, increase the batch size, 2018",
            "year": 2018
        },
        {
            "authors": [
                "D. Szarek"
            ],
            "title": "Neural network-based anomalous diffusion parameter estimation approaches for gaussian processes",
            "venue": "Int. J. Adv. Eng. Sci. Appl. Math., 13(2-3):257\u2013269,",
            "year": 2021
        },
        {
            "authors": [
                "H.T. S\u00f8rensen",
                "S. Sabroe",
                "K.J. Rothman",
                "M. Gillman",
                "F.H. Steffensen",
                "P. Fischer",
                "T.I.A. Serensen"
            ],
            "title": "Birth Weight and Length as Predictors for Adult Height",
            "venue": "Amer. J. Epidem., 149(8):726\u2013729, 04",
            "year": 1999
        },
        {
            "authors": [
                "A. Vaswani",
                "N. Shazeer",
                "N. Parmar",
                "J. Uszkoreit",
                "L. Jones",
                "A.N. Gomez",
                "L. Kaiser",
                "I. Polosukhin"
            ],
            "title": "Attention is all you need, 2017",
            "year": 2017
        },
        {
            "authors": [
                "O. Vilk",
                "E. Aghion",
                "T. Avgar",
                "C. Beta",
                "O. Nagel",
                "A. Sabri",
                "R. Sarfati",
                "D.K. Schwartz",
                "M. Weiss",
                "D. Krapf",
                "R. Nathan",
                "R. Metzler",
                "M. Assaf"
            ],
            "title": "Unravelling the origins of anomalous diffusion: from molecules to migrating storks, 2021",
            "year": 2021
        },
        {
            "authors": [
                "T. Wolf",
                "L. Debut",
                "V. Sanh",
                "J. Chaumond",
                "C. Delangue",
                "A. Moi",
                "P. Cistac",
                "T. Rault",
                "R. Louf",
                "M. Funtowicz",
                "J. Brew"
            ],
            "title": "Huggingface\u2019s transformers: State-of-the-art natural language processing",
            "venue": "CoRR, abs/1910.03771,",
            "year": 2019
        },
        {
            "authors": [
                "Y. Wu",
                "M. Schuster",
                "Z. Chen",
                "Quoc V. Le",
                "M. Norouzi",
                "W. Macherey",
                "M. Krikun",
                "Y. Cao",
                "Q. Gao",
                "K. Macherey"
            ],
            "title": "Google\u2019s neural machine translation system: Bridging the gap between human and machine translation",
            "venue": "arXiv preprint arXiv:1609.08144,",
            "year": 2016
        }
    ],
    "sections": [
        {
            "text": "The results of the Anomalous Diffusion Challenge (AnDi Challenge) [30] have shown that machine learning methods can outperform classical statistical methodology at the characterization of anomalous diffusion in both the inference of the anomalous diffusion exponent \u03b1 associated with each trajectory (Task 1), and the determination of the underlying diffusive regime which produced such trajectories (Task 2). Furthermore, of the five teams that finished in the top three across both tasks of the AnDi challenge, three of those teams used recurrent neural networks (RNNs). While RNNs, like the long short-term memory (LSTM) network, are effective at learning long-term dependencies in sequential data, their key disadvantage is that they must be trained sequentially. In order to facilitate training with larger data sets, by training in parallel, we propose a new transformer based neural network architecture for the characterization of anomalous diffusion. Our new architecture, the Convolutional Transformer (ConvTransformer) uses a bi-layered convolutional neural network to extract features from our diffusive trajectories that can be thought of as being words in a sentence. These features are then fed to two transformer encoding blocks that perform either regression (Task 1) or classification (Task 2). To our knowledge, this is the first time transformers have been used for characterizing anomalous diffusion. Moreover, this may be the first time that a transformer encoding block has been used with a convolutional neural network and without the need for a transformer decoding block or positional encoding. Apart from being able to train in parallel, we show that the ConvTransformer is able to outperform the previous state of the art at determining the underlying diffusive regime (Task2) in short trajectories (length 10-50 steps), which are the most important for experimental researchers.\nKeywords: anomalous diffusion, machine learning, recurrent neural networks, convolutional networks, transformers, attention\nar X\niv :2\n21 0.\n04 95\n9v 1\n[ cs"
        },
        {
            "heading": "1. Introduction",
            "text": "It could be said that the study of diffusion began in 1827 when Brown first observed the motion, which now carries his namesake, of pollen from Clarkia pulchella suspended in water [5]. This movement results from small particles being bombarded by the molecules of the liquid in which they are suspended, as was first conjectured by Einstein and later verified by Perrin [32]. Though Brown never managed to explain the movement he observed, we now know that Brownian motion is a kind of normal diffusion.\nTo describe diffusion, we can consider the following analogy: Let us imagine a\nparticle being an ant, or some other diminutive explorer, we can then think of mean squared displacement (MSD), which can be written as \u3008x2\u3009, as the portion of the system that it has explored. For normal diffusion such as Brownian motion, the relation between the portion of explored region and time is linear, \u3008x2\u3009 \u223c t. As time progresses, the expected value of distance explored by our ant (MSD) will remain constant. In contrast to normal diffusion, anomalous diffusion is characterized by \u3008x2\u3009 \u223c t\u03b1, \u03b1 6= 1. Anomalous diffusion can be further subdivided into super-diffusion and sub-diffusion, when \u03b1 > 1 or \u03b1 < 1, respectively. To continue using the analogy of our ant, an intuitive example of sub-diffusion would be diffusion on a fractal. In this case, it is easy to see how, as time progresses and our ant ventures into zones of increasing complexity, its movement will in turn be slowed. Thus the relationship of space explored and time will be \u3008x2\u3009 \u223c t\u03b1, \u03b1 < 1. Conversely, if we give our ant wings and have it randomly take flight at random times ti sampled from t \u2212\u03c3\u22121 with flight times positively correlated to the wait time, then for \u03c3 \u2208 (0, 2) we would have a super-diffusive Le\u0301vy flight trajectory.\nSince the discovery of Brownian motion, many systems have shown diffusive\nbehavior that deviates from the normal one, where MSD scales linearly with time. These systems can range from the atomic scale to complex organisms such as birds. Examples of such diffusive systems include ultra-cold atoms [33], telomeres in the nuclei of cells [4], moisture transport in cement-based materials, the free movement of arthropods [31], and the migration patterns of birds [39]. Anomalous diffusive patterns can even be observed in signals that are not directly related to movement, such as heartbeat intervals and DNA [6, pg. 49-89]. The interdisciplinary scope of anomalous diffusion highlights the need for modeling frameworks that are able to quickly and accurately characterize diffusion in real-life scenarios, where data is often limited and noisy.\nDespite the importance of anomalous diffusion in many fields of study [23],\ndetection and characterization remain difficult to this day. Traditionally, mean squared displacement (MSD(t) \u223c t\u03b1) and its anomalous diffusion exponent \u03b1 have been used to characterize diffusion. In practice, computation of MSD is often challenging as we often work with a limited number of points in the trajectories, which may be short and/or noisy, highlighting a need for a robust method for real-world conditions. The problem with using \u03b1 to characterize anomalous diffusion is that trajectories often have the same anomalous diffusion exponent while having different underlying diffusive regimes. An\nexample would be the motion messenger RNA (mRNA) in a living E. coli cell. The individual trajectories of the mRNA share roughly the same \u03b1 despite their trajectories being quite distinct [26].\nBeing able to classify trajectories based on their underlying diffusive regime is\nuseful because it can shed light on the underlying behavior of the particles undergoing diffusion. This could be more important for experimental researchers, which may be more concerned with how a particle moves not necessarily how much it has moved. In this vein, the AnDi (Anomalous Diffusion) Challenge organizers identified the following five diffusive models [28]: the continuous-time random walk (CTRW) [34], fractional Brownian motion (FBM) [22], the Le\u0301vy walk (LW) [15], annealed transient motion (ATTM) [24], and scaled Brownian motion (SBM) [18], with which to classify trajectories. This information is not meant to supplant traditional MSD-based analysis, rather, it is meant to give us additional information about the underlying stochastic process behind the trajectory. For example, for a particular exponent \u03b1, one may not have access to an ensemble of homogeneous trajectories. Moreover, one cannot assure that all measured trajectories have the same behavior and can therefore be associated with the same anomalous exponent \u03b1. In these cases, it may be possible to explain the behavior of the diffusing particles by using what we know about five models mentioned above.\nThe first applications of machine learning methods to the study of diffusion aimed\nto discriminate among confined, anomalous, normal qualitatively, and directed motion [8, 16]. These ML models did not extract quantitative information nor determining did they determine the underlying physical model. At first long short-term memory (LSTM) recurrent neural networks [13] were considered for the analysis of anomalous diffusion trajectories from experimental data in [3]. Later, Mun\u0303oz-Gil et al. [27] computed the distances between consecutive positions in raw trajectories and normalized them by dividing by the standard deviation. Then, their cumulative sums fed random forest algorithms that permit to infer the anomalous exponent \u03b1 and to classify the trajectory in one of these models, CTRW, FBM, or LW. Random forests and gradient boosting methods were already considered for the study of fractional anomalous diffusion of single-particle trajectories in [14,20].\nThe results of the AnDi Challenge [30] showed that machine learning (ML)\nalgorithms outperform traditional statistical techniques in the inference of the anomalous diffusion exponent (Task 1) and in the classification of the underlying diffusion model (Task 2), across one, two, and three dimensions. Some of the most successful techniques consisted of: a couple of convolutional layers combined with some bidirectional LSTM layers and a final dense layer [9], two LSTM layers of decreasing size with a final dense layer [1], a WaveNet encoder with LSTM layers [17] , or the extraction of classical statistics features combined with three deep feed-forward neural networks [10].\nAs we can see, the best performing methods from the AnDi Challenge were either\nentirely based on LSTMs recurrent neural networks or incorporated them as part of a larger architecture. For many years, LSTM have been one of the most successful techniques in natural language processing (NLP) and time series analysis. As a matter of fact, Google Translate algorithm is a stack of just seven large LSTM layers [41]. However, since the landmark paper Attention is All You Need [38], transformers have become the dominant architecture in NLP, where they have surpassed previous models based on convolutions and recurrent neural networks [40]. Inspired by the transformers\u2019 success and by drawing a parallel between the sequential nature of language and the diffusion of a single particle, we propose a new architecture combining convolutional layers with transformers, that we will call a convolutional transformer: the Convolutional Transformer (ConvTransformer)"
        },
        {
            "heading": "1.1. The Convolutional Transformer",
            "text": "The ConvTransformers method has been applied to both the inference the anomalous diffusion exponent \u03b1 (Task 1) and the determination of the underlying diffusion model (Task 2). As the name suggests, the ConvTransformer uses two convolutional layers followed by a transformer encoding block. However, unlike the transformer in [38], our method uses only two transformer encoding blocks in sequence without a transformer decoding block or positional encoding. The convolutional layers behave as an encoder extracting both linear and non-linear features from the trajectory while retaining spatiotemporal awareness eliminating the need for positional encoding. These features are then passed to the transformer encoding layers where attention is performed upon them. The ConvTransformer structure can be intuitively understood if we consider a single trajectory is akin to a sentence. In this analogy the CNNs are used to create pseudo-words, which are the features produced by the CNNs. Finally, we perform attention twice on the pseudo-words with our transformer encoder, which allows us to determine which features are the most important, and from there we are able to obtain either our \u03b1 or the underlying diffusive regime model.\nThe ConvTransformer does not require positional encoding because the CNN\nkernel moves across the trajectory to create the features. As the CNN kernel moves along the trajectory, it learns positional information, negating the need for positional encoding prior to the transformer encoding block. This was assessed by testing the ConvTransformer on the Task 2 with five-fold validation on a training set of size 50K (32K for training, 8K for validation, and 10K for testing), using the same set of hyperparameters, with and without the trigonometric encoding scheme used in Vaswani et al. 2017 [38]. A five-fold validation of models trained with and without positional encoding showed showed that model classification accuracy decreased with positional encoding to a mean 72.39% and standard deviation of 4.86 from 75.66% and standard deviation of 1.54. Thus, positional encoding did not improve ConvTransformer performance, and it was omitted from the model.\nIn Figure 1, we show a diagram detailing the structure of the ConvTransformer.\nAs was previously said, the ConvTransformer uses two convolutional layers, one which scales our trajectory up to 20 features, and a second one takes those 20 features and outputs 64 features. This structure allows the CNN to learn lower-level features first and then refine those features in the subsequent layer. Both convolutional layers use a kernel size of 3, a stride of 1, and are followed by a rectified linear unit function (ReLU), and a dropout with a 5% probability of setting a learned parameter to 0 to avoid over-fitting. At the end of the convolutional block, we do a pooling with kernel size 2, which cuts the length of our output in half. This helps conserve video memory (VRAM), optimizing resource consumption and democratizing the model as it can run in consumer-grade hardware. The transformer encoding block follows the basic structure of the transformer encoding block from [38]. It uses wide multi-headed attention with 16 attention heads. The attention mechanism is then followed by layer normalization and dropout. This output feeds two linear layers separated by a ReLU, which ultimately goes to another dropout. This transformer encoding block then feeds into another transformer encoding block. This output of this final transformer encoding block then goes to a max function, which gets the largest value of the output tensor by column. Finally, the output of the ConvTransformer feeds a linear layer that outputs size one for Task 1 or size five for each of the categories in Task 2."
        },
        {
            "heading": "2. Methods",
            "text": ""
        },
        {
            "heading": "2.1. Generation of Data sets",
            "text": "All of the data sets used to train and test our models were generated with the Python 3 package provided by the AnDi Challenge [30]. The code was made freely available by the AnDi Challenge organizers at: https://github.com/AnDiChallenge/ANDI_datasets."
        },
        {
            "heading": "2.2. Hyper-Parameter Selection",
            "text": "In order to select the hyper-parameters, we chose a relatively small dataset to train all permutations of these hyper-parameters using five-fold validation to ensure that model performance for a given hyper-parameter set was reliable across runs. The hyperparameters sets were assessed using a data set with 50K trajectories of lengths [10, 1000] across both Task 1 and Task 2, which was broken up into 32K training, 8K validation, and 10K testing. Then the sets of tested hyper-parameters were evaluated as per the five-fold validation, and the hyper-parameters were chosen for the final model, except the Learning Rate (LR), which has to be scaled up with respect to training set size and batch size [35]. In the selection process, model performance and feasibility were assessed to ensure that model training could take place on our hardware within a reasonable amount of time. Different sets of hyper-parameters were tested for both Task 1 and Task 2 but, in the end, we found that the same hyper-parameters work well for both tasks. These final hyper-parameters can be found in Table 1.\nIn order to generalize learning rate to larger training data sets, we used the results\nfrom [35] to relate the noise scale (g) during training to batch size (B), training size (N), and learning rate ( ), as shown in Equation 1.\ng \u2248 \u00b7 N B\n(1)\nDuring the training process, we found that an LR of 0.01 worked well across both\ntasks. Using Equation 1, this would give us an equivalent LR of 2.133 \u00d7 10\u22125 when training with 1.35\u00d7 106 trajectories. We used this value as a baseline and we ended up setting an LR of 2.133 \u00d7 10\u22124 for training the final model on a larger data set, as can be seen in Table 1.\nOur testing revealed that using smaller batches and more heads improved\nperformance. However, decreasing the batch size greatly increases the running time. Ideally, we would have used 32 or more heads. However, we were constrained by our equipment\u2019s 8GB video memory (GTX 1070). Thus, with the above set of hyperparameters, we strove to attain a good balance of speed and performance with our hardware constraints."
        },
        {
            "heading": "2.3. Model Training",
            "text": "All model training was conducted in Python 3.8.5 using Pytorch in Jupyter Notebook 6.0.3. For simplicity, we trained all our models for Task 1 and Task 2 on data sets of size 2 million using the same split as before: 75% of the data for training and 25% for testing, with the training set further broken as 90% for training and 10% for validation to be used by Early Stopping to halt the training [25]. Thus, the final training sets were broken up into: 1.35\u00d7106 million trajectories for training, 1.50\u00d7105 for validation, and 5\u00d7 105 for testing.\nFor Task 1, we trained 12 models, each model corresponding to a batch of trajectory\nlengths: [10,20], [21,30], [31,40], [41,50], [51,100], [101,200], [201,300], [301,400], [401,500], [501,600], [601,800], and [801,1000]. All datasets are of size 2 \u00d7 106, with the aforementioned training/test/validation split ratio. By default, the andi-datasets package [29] generates trajectories with anomalous exponent \u03b1 \u2208 [0.05, 2) in intervals of 0.05. This means that 39 different alpha values can be generated, and there are five diffusion models for a total of 195 different kinds of trajectories. After generating data sets of size 2\u00d7106 would ensure that each of the 195 combinations of diffusion model and \u03b1 has a representative sample size of about 105. Naturally, this is lower after splitting the data sets into training, test, and validation. However, data sets of this size were a good compromise between model performance and training time on our hardware.\nIn order to accelerate the training of the 12 models, we reduced the patience of\nour early stopping function from ten, used in hyper-parameter selection, to five while maintaining the number of epochs at 100. Additionally, we conducted the training so that models inherit the parameter state of a previously trained model. This has two advantages: Firstly, it indirectly exposes the model to more unique trajectories, as the model will inherit a parameter state that was trained on a different data set, thus reducing overfitting. Secondly, it jump-starts the training of each model with a parameter state that was trained on longer trajectories, which should contain relevant information for classifying shorter trajectories.\nTo implement this training scheme, we trained the first ConvTransformer on the\neasiest dataset, trajectories of length [801\u22121000], as can be inferred from our testing and\nthe results in the AnDi Challenge [30]. Then the parameter state of this model is used as the starting parameters state for the next model, which will be trained on trajectories of lengths [601 \u2212 800] and so forth until the final model is trained on trajectories of length [10, 20]. Once we have completed the first training pass through, we loop back to the top and repeat the process, with model [801, 1000] from round two inheriting the parameter state of model [10, 20] from the first training round. Finally, the round two models are tested on every testing data set, and the best models at each trajectory length are selected. This final selection process resulted in 11 models as the models trained on [401, 500], [601 \u2212 800], and [501 \u2212 600] outperformed other models at their native trained trajectory lengths. Thus, our compiled model for Task 1 consists of 11 models, each of which is in charge of certain trajectory lengths.\nFinally, for Task 2, a single model was used across all trajectory lengths (10 to\n1000) as we were to improve upon the state art while maintaining parsimony, as we show in Figure 3. The transformer was trained using 100 epochs, patience of 10, and a single data set with trajectory lengths [10, 1000]."
        },
        {
            "heading": "3. Results",
            "text": "We use the AnDi Interactive Tool\u2021 extensively in our testing in order to be able to assess our model\u2019s performance against the current state of the art. However, in order to gain further insight into model\u2019s performance under different combinations of trajectory type (ATTM, CTRW, LW, FBM, SBM), trajectory length, anomalous diffusion exponent (\u03b1), and signal to noise ratio (SNR), defined as SNR= \u03c3disp/\u03c3noise, where \u03c3disp is the standard deviation of the displacements and \u03c3noise is the standard deviation of the Gaussian white noise. We generated datasets for all of the permutations seen in Table 2.\nWe have used the performance of our model on these data sets to make the figures\nin the following sections. In order to improve model comparability we will use the following metrics of performance:\n\u2021 http://andi-challenge.org/interactive-tool/\n\u2022 The Mean Average Error (MAE) is defined as\n1\nN N\u2211 j=1 |\u03b1j,pred \u2212 \u03b1j,true|, (2)\nwhere \u03b1\u00b7,pred and \u03b1\u00b7,true are the predicted and true \u03b1 values respectively.\n\u2022 The F1-Score is the harmonic mean of precision and recall and it is defined as True Pos.\nTrue Pos. + 1 2 (False Pos. + False Neg.).\n(3)\nFor our purposes, we have used the micro averaged F1-score, that is biased by class frequencies, as it has been considered in the AnDi Challenge.\nUsing the AnDi Challenge interactive tool, we can see how the ConvTransformer\nwould have performed in the AnDi Challenge in Table 3. Overall the ConvTransformer would have placed in the middle of the top ten of the AnDi Challenge. However, ConvTransformer shines in classifying short trajectories (Task 2). Here, it outperforms the top three models in length trajectories [10, 50]. Impressively, the ConvTransformer manages these results by training on a single data set that is small when compared to the training set used by team UPV-MAT, described in [9], whose model came within the margin of error of ours at the classification of short trajectories in one dimension. Team UPV-MAT\u2019s model was trained using 4\u00d7 106 trajectories [9], while we used only 1.35\u00d7 106. This is noteworthy because we observed that during the period of patience, after the final saved parameter state, the ConvTransformer continues to converge to a training loss of zero. This indicates that the network is not fully saturated. Thus, it is possible that given a larger training set, the ConvTransformer would learn, which should lead to an increase in performance."
        },
        {
            "heading": "3.1. Regression of the anomalous diffusion exponent (Task 1)",
            "text": "We first show the performance of our model with the AnDi Interactive tool, see Figure 2 .The ConvTransformer, as well as the top performers in the AnDi Challenge seen in Table 3, had the most difficulty inferring the \u03b1 of ATTM and SBM diffusive regimes, with ATTM being far more problematic. This makes sense if we consider the way ATTM trajectories are generated. The displacements of particles undergoing ATTM are distributed BM(D, t,\u2206t), where BM generates a Brownian motion trajectory of length t sampled at times \u2206t, with diffusivity coefficient D. Additionally, in ATTM D is re-sampled every t \u223c D\u03c3/\u03b1. This means that every time t a particle in ATTM will change diffusive regime in a manner that may obscure \u03b1.\nSimilar to ATTM, SBM also experiences changes in D, the diffusivity coefficient.\nHowever, in SBM D(t) = D\u03c8(t) [21]. On the surface it would appear as though a gradual change in diffusivity should not appear to pose as much difficulty as the regime shifts in ATTM. However, it may have contributed to the difficulty of inferring \u03b1 in SBM trajectories.\nConvTransformer performance scales as expected with trajectory length, see Figure\n3. That is to say, as trajectory length increases, the model performance also improves. Notably, performance scaled less erratically than in other models, such as the best performing model in Task 1 [9]. Interestingly, noise does not affect model accuracy as heavily at shorter trajectory lengths, and the performance difference between trajectories with respect to the SNR appears to stabilize after trajectories of length \u223c 200.\nIn Figure 4, we can see the performance breakdown by the underlying diffusive\nregime. These plots shed more light on the performance issues when regressing \u03b1 for ATTM. From Figure 4, it is evident that most of the difficulty with regressing \u03b1 in ATTM appears to occur in heavy sub-diffusive trajectories at \u03b1 \u2248 0.1, regardless of noise. There, we can see a roughly bi-modal distribution with two clusters at \u03b1 \u2248 0.4 and \u03b1 \u2248 0.9, with the more significant peak at about 0.9, as shown by the median value line.\nAdditionally, the ConvTransformer shows similar confusion patterns in the\nregression task for the SBM model, where it confuses highly super-diffusive trajectories with, roughly, normal diffusion (Figure 4). In both of these cases, the regime shift in ATTM and the change in D in SBM could be making heavy anomalous diffusion (both super and sub-diffusion) appear as though it was normal diffusion. However, these effects could also be an artifact of the training data since all diffusive regimes can exhibit normal diffusion, so there will be more trajectories with \u03b1 = 1 than either super or sub-diffusion, or a combination of both effects.\nFigure 5 takes a closer look at model performance by trajectory length and type.\nOnce again, most trajectories, with the exception of the ones generated by the SBM model, perform very similarly at SNR 1 and SNR 2, which shows notably worse performance at SNR 1 on all trajectory lengths. It can be verified via the AnDi Interactive Tool that the same effect occurs in the top three models (UPV-MAT, HNU,\neduN) shown in Table 3, where SBM performance is the most sensitive to additional noise in the trajectory.\nWhen regressing the anomalous diffusion exponent, the sensitivity of machine\nlearning models to added noise in SBM trajectories warrants further study. Recently, Szarek [36] has encountered a similar lack in resiliency to noise using an RNN-based model, like UPV-Mat, HNU, and eduN models. It appears that the difficulty in working with SBM is an inherent characteristic of SBM trajectories, as opposed to the neural network architecture used for inference of \u03b1. This is further substantiated by our transformer-based method encountering the same problem.\nIn terms of model performance for different values of \u03b1, the ConvTransformer\nperform best at \u03b1 \u2248 .9 (Figure 6). However, for long trajectories, those with 200 or more points, the model performs best roughly between \u03b1 \u2208 [0.25, 0.5]. The latter scenario seems to be closer to the truth if we examine the model performance at various levels of \u03b1 by trajectory type as in Figure 7. Our ConvTransformer performs best roughly in the middle of the domain of \u03b1 of each trajectory type, with an adequate, though not optimal, performance at \u03b1 \u2248 1 across all trajectory types (Figure 7). Part of the reason\nperformance is overestimated, at \u03b1 \u2248 1, when pooling the trajectory types may be that CTRW and SBM perform best at \u03b1 \u2248 1, and these two types of diffusion can be super and sub-diffusive. Thus, they have more testing points and skew the pooled values in Figure 6."
        },
        {
            "heading": "3.2. Classification of trajectories according to the anomalous diffusion generating model (Task 2)",
            "text": "ConvTransformer performance in the classification of trajectories according to the anomalous diffusion generating model (Task 2) presents results in the average overall, with respect to the ten best models of the AnDi Challenge [30]. However, as we mentioned earlier, our ConvTransformer shines in short trajectories. As with the inference of \u03b1 (Task 1), ATTM trajectories proved to be the most difficult to work with. These trajectories were most often confused with SBM (Figures 8 and 10). As we have said, this may be because both models have changes in the diffusivity coefficient D. If we imagine a short ATTM trajectory, where D only changes a few times, the\ndiffusivity coefficient can increase with time (D \u223c \u03a6(t)) in a way that ATTM could mimic SBM.\nNoise affects ConvTransformer the least at both short and long trajectory lengths\n(Figure 9). Performance at the lower noise data improves faster with respect to trajectory length. The most significant difference between the two curves, in Figure 9, occurs at trajectories of length \u223c 200, after which the SNR 1 curve converges towards SNR 2. This indicates that longer trajectories are most helpful when dealing with noisy trajectories that are roughly 200 to 600 dispersals in length.\nWhen looking at F1-score by the underlying diffusion model, we can see that\nConvTransformer performance varies significantly across our five diffusive regimes\n(Figure 11). That being said, unlike Task 1, performance change with respect to noise remains fairly constant across the different kinds of diffusion. The outlier to this behavior is short CTRW trajectories at SNR 1 (Figure 11). In this case, model performance is better for the shortest trajectories and then drops off before resuming the expected convergence behavior of F1-Score with respect to the trajectory length. The cause of this artifact in the F1-Score is that at SNR 1 and short trajectory lengths ([10, 50]), the ConvTransformer is inclined to classify the other diffusive regimes, with the exception of LW, as CTRW (Figure 10). It is noteworthy that the ConvTransformer can make the distinction between LW and CTRW as LW can be considered a special case of CTRW [30].\nIn terms of ConvTransformer performance in classification (Task 2) with regards\nto \u03b1, we can see that ConvTransformer performs better at a value of \u03b1 \u2248 0.5 and at the higher-end \u03b1 \u2265 1.5, with an apparent plateauing behavior at the upper end of the \u03b1 domain in longer trajectories with lower noise (Figure 12). In Figure 13 we again look at F1-Score as a function of \u03b1. However, this time we look at the relationship in terms of the underlying diffusive model. Most diffusive models retain the relationship seen in Figure 12, within their respective domains of \u03b1. However, CTRW and LW deviate from this behavior. Both CTRW and LW appear to have a more linear relationship between F1-Score and \u03b1, with CTRW performing best at low values of \u03b1 and LW performing best at higher values of \u03b1. This relationship strength (F1-Score \u223c \u03b1) appears to be exacerbated by noise."
        },
        {
            "heading": "4. Conclusions",
            "text": "The primary purpose of this paper was to introduce our new architecture, the ConvTransformer, for the analysis of anomalous diffusion trajectories. To the best of our knowledge, this is the first transformer based architecture to characterize anomalous\ndiffusion. Indeed it is only recently that anyone else has produced a convolutional transformer (for computer vision) [12, 19], with the development of their models being concurrent with ours. However, our ConvTransformer stands out in that it does not use positional encoding and only uses the transformer encoding block from [38]. As such, it is simpler and easier to implement while still providing state-of-the-art results in trajectory classification (Task 2) in short and noisy trajectories.\nInspired by the success of transformers in NLP we set out to replace the recurrent\nbidirectional LSTM part of the architecture in [9] by transformers. We have improved the classification of short trajectories accuracy with a model that is trained pretty fast since it can be trained in parallel. When we first started working on this model, there was no native support in PyTorch for transformers. However, when writing this manuscript, transformer encoders and decoders are natively supported. As such, we expect further improvements as ease of implementation and optimized code will lead to more accessibility. This should lead to improved iterations of the model and a finer hyperparametrization tuning. Additionally, the increased optimization and access to newer hardware should increase our ConvTransformers performance for improved usability in experimental research.\nApart from the direct practical implementation of our model in experimental\nresearch, going forwards, we would also like to focus on model interpretability. One of the issues plaguing deep learning is the black box effect. When looking at models, we are often only interested in what we can predict or characterize and tend to overlook what we can learn from parameter weighting. Traditionally, parameter weight would allow us to see simple relation between the input features and our desired prediction. For example, birth weight is a strong predictor of adult height [37]. Furthermore, with traditional models like regression, parameter selection leads to discarding information which also informs us about the features that are not relevant to our subject of study. With the rise of deep learning models, we are no longer looking at features, but rather we ingest the data directly and allow our models to discern these features for themselves. With the exception of deep learning models that use feature engineering, as we saw with group UCL and their CONDOR model [10]. The naive approach to modeling brought about by ML means that we not only lose all information about features, but we also do not know what features are important.\nAs we know from Clark et al. [7], in the context of NLP, transformer attention\nheads tend to focus on specific aspects of syntax. For instance, some attention heads may focus entirely on the next token, while others may attend almost entirely to the periods or breaks in a sentence. Following this logic, it is highly likely that some of our ConvTransformer attention heads are specializing on specific features of the trajectories. Hence, a transformer based architecture could be used to determine what trajectory features are important. In this manner, we could recover some model interpretability, and learn from machine learn models in a similar way to how we have traditionally learned from regression."
        }
    ],
    "title": "Characterization of anomalous diffusion through convolutional transformers",
    "year": 2022
}