{
    "abstractText": "Digging out the latent information from large-scale incomplete matrices is a key issue with challenges. The Latent Factor Analysis (LFA) model has been investigated in depth to an alyze the latent information. Recently, Swarm Intelligence-related LFA models have been proposed and adopted widely to improve the optimization process of LFA with high efficiency, i.e., the Particle Swarm Optimization (PSO)-LFA model. However, the hyper-parameters of the PSO-LFA model have to tune manually, which is inconvenient for widely adoption and limits the learning rate as a fixed value. To address this issue, we propose an Adam-enhanced Hierarchical PSO-LFA model, which refines the latent factors with a sequential Adam-adjusting hyper-parameters PSO algorithm. First, we design the Adam incremental vector for a particle and construct the Adam-enhanced evolution process for particles. Second, we refine all the latent factors of the target matrix sequentially with our proposed Adam-enhanced PSO\u2019s process. The experimental results on four real datasets demonstrate that our proposed model achieves higher prediction accuracy with its peers. Keywords\u2014High-dimensional and Incomplete (HDI) Matrix, Latent Factor Analysis (LFA) model, Adam algorithm, Particle Swarm Optimization (PSO) algorithm.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jia Chen"
        },
        {
            "affiliations": [],
            "name": "Renyu Zhang"
        },
        {
            "affiliations": [],
            "name": "Yuanyi Liu"
        }
    ],
    "id": "SP:8d782c3a1639d00600748002b35cab170745da67",
    "references": [
        {
            "authors": [
                "X. Luo",
                "Z. Liu",
                "L. Jin",
                "Y. Zhou",
                "M. Zhou"
            ],
            "title": "Symmetric Nonnegative Matrix Factorization-Based Community Detection Models and Their Convergence Analysis",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 33, no. 3, pp. 1203-1215, March 2022.",
            "year": 2022
        },
        {
            "authors": [
                "D. Wu",
                "X. Luo",
                "M. Shang",
                "Y. He",
                "G. Wang",
                "X. Wu"
            ],
            "title": "A Data-Characteristic-Aware Latent Factor Model for Web Services QoS Prediction",
            "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 34, no. 6, pp. 2525-2538, June 2022.",
            "year": 2022
        },
        {
            "authors": [
                "D. Wu",
                "Q. He",
                "X. Luo",
                "M. Shang",
                "Y. He",
                "G. Wang"
            ],
            "title": "A Posterior-Neighborhood-Regularized Latent Factor Model for Highly Accurate Web Service QoS Prediction",
            "venue": "IEEE Transactions on Services Computing, vol. 15, no. 2, pp. 793-805, 1 March 2022.",
            "year": 2022
        },
        {
            "authors": [
                "X. Luo",
                "Y. Zhou",
                "ZG. Liu",
                "MC. Zhou"
            ],
            "title": "Fast and Accurate Non-negative Latent Factor Analysis on High-dimensional and Sparse Matrices in Recommender Systems",
            "venue": "IEEE Transactions on Knowledge and Data Engineering, doi: 10.1109/TKDE.2021.3125252.",
            "year": 2021
        },
        {
            "authors": [
                "X. Luo",
                "H. Wu",
                "ZC. Li"
            ],
            "title": "NeuLFT: A Novel Approach to Nonlinear Canonical Polyadic Decomposition on High-Dimensional Incomplete Tensors",
            "venue": "IEEE Transactions on Knowledge and Data Engineering, doi: 10.1109/TKDE.2022.3176466.",
            "year": 2022
        },
        {
            "authors": [
                "X. Luo",
                "H. Wu",
                "Z. Wang",
                "JJ. Wang",
                "DY. Meng"
            ],
            "title": "A Novel Approach to Large-Scale Dynamically Weighted Directed Network Representation",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, doi: 10.1109/TPAMI.2021.3132503.",
            "year": 2021
        },
        {
            "authors": [
                "X. Shi",
                "Q. He",
                "X. Luo",
                "Y. Bai",
                "M. Shang"
            ],
            "title": "Large-Scale and Scalable Latent Factor Analysis via Distributed Alternative Stochastic Gradient Descent for Recommender Systems",
            "venue": "IEEE Transactions on Big Data, vol. 8, no. 2, pp. 420-431, 1 April 2022.",
            "year": 2022
        },
        {
            "authors": [
                "D. Wu",
                "M. Shang",
                "X. Luo",
                "Z. Wang"
            ],
            "title": "An L1-and-L2-Norm-Oriented Latent Factor Model for Recommender Systems",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems, doi: 10.1109/TNNLS.2021.3071392.",
            "year": 2021
        },
        {
            "authors": [
                "J. Chen",
                "Y. Yuan",
                "R. Tao",
                "T.R.J. Chen",
                "X. Luo"
            ],
            "title": "Hyper-Parameter-Evolutionary Latent Factor Analysis for High-Dimensional and Sparse Data from Recommender Systems",
            "venue": "Neurocomputing, vol. 421, pp. 316-328, Jan. 2021.",
            "year": 2021
        },
        {
            "authors": [
                "D. Chen",
                "S. Li",
                "D. Wu",
                "X. Luo"
            ],
            "title": "New Disturbance Rejection Constraint for Redundant Robot Manipulators: An Optimization Perspective",
            "venue": "IEEE Transactions on Industrial Informatics, vol. 16, no. 4, pp. 2221-2232, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "H. Wu",
                "X. Luo",
                "MC. Zhou"
            ],
            "title": "Discovering Hidden Pattern in Large-scale Dynamically Weighted Directed Network via Latent Factorization of Tensors",
            "venue": "Proc. of the 17th IEEE Int. Conf. on Automation Science and Engineering (CASE 2021) (Regular), doi: 10.1109/CASE49439.2021.9551506.",
            "year": 2021
        },
        {
            "authors": [
                "X. Luo",
                "Z. You",
                "S. Li",
                "Y. Xia",
                "H. Leung"
            ],
            "title": "Improving Network Topology-Based Protein Interactome Mapping via Collaborative Filtering",
            "venue": "Knowledge- Based Systems, vol. 90, pp. 23-32, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "X. Luo",
                "MC. Zhou",
                "S. Li",
                "Y. Xia",
                "ZH. You",
                "Q. Zhu",
                "H. Leung"
            ],
            "title": "Incorporation of Efficient Second-order Solvers into Latent Factor Models for Accurate Prediction of Missing QoS Data",
            "venue": "IEEE Transactions on Cybernetics, vol. 48, no. 4, pp. 1216-1228, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "X. Luo",
                "H. Wu",
                "H. Yuan",
                "M. Zhou"
            ],
            "title": "Temporal pattern-aware QoS prediction via biased non-negative latent factorization of tensors",
            "venue": "IEEE Transactions on Cybernetics, vol. 50, no. 5, pp. 1798-1809, 2019.",
            "year": 1809
        },
        {
            "authors": [
                "Q. Deng",
                "Q. Kang",
                "L. Zhang",
                "M. Zhou"
            ],
            "title": "An Objective Space-based Population Generation to Accelerate Evolutionary Algorithms for Large-scale Manyobjective Optimization",
            "venue": "IEEE Transactions on Evolutionary Computation, doi: 10.1109/TEVC.2022.3166815, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "X. Luo",
                "M. Shang",
                "S. Li"
            ],
            "title": "Efficient extraction of non-negative latent factors from high-dimensional and sparse matrices in industrial applications",
            "venue": "IEEE 16th International Conference on Data Mining, vol. 51, pp. 311-319, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "D. Wu",
                "X. Luo"
            ],
            "title": "Robust Latent Factor Analysis for Precise Representation of High-dimensional and Sparse Data",
            "venue": "IEEE/CAA Journal of Automatica Sinica, vol. 8, no. 4, pp. 796-805, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "H. Wu",
                "X. Luo",
                "MC. Zhou"
            ],
            "title": "Advancing non-negative latent factorization of tensors with diversified regularizations",
            "venue": "IEEE Transactions on Services Computing, vol. 15, no. 3, pp. 1334-1344, 1 May-June 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Z. Liu",
                "X. Luo",
                "Z. Wang"
            ],
            "title": "Convergence analysis of single latent factor-dependent, nonnegative, and multiplicative update-based nonnegative latent factor models",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 32, no. 4, pp. 1737-1749, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "M. Shang",
                "Y. Yuan",
                "X. Luo",
                "MC. Zhou"
            ],
            "title": "An \u03b1-\u03b2-divergence-generalized recommender for highly accurate predictions of missing user preferences",
            "venue": "IEEE Transactions on Cybernetics, vol. 52, no. 8, pp. 8006-8018, Aug. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "X. Luo",
                "Y. Yuan",
                "S. Chen",
                "N. Zeng",
                "Z. Wang"
            ],
            "title": "Position-Transitional Particle Swarm Optimization-incorporated Latent Factor Analysis",
            "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 34, no. 8, pp. 3958-3970, 1 Aug. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "L. Xin",
                "Y. Yuan",
                "M. Zhou",
                "Z. Liu",
                "M. Shang"
            ],
            "title": "Non-negative latent factor model based on \u03b2-divergence for recommender systems",
            "venue": "IEEE Transactions on Systems, Man, and Cybernetics: Systems, vol. 51, no. 8, pp. 4612-4623, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "J. Chen",
                "X. Luo",
                "M. Zhou"
            ],
            "title": "Hierarchical Particle Swarm Optimization-Incorporated Latent Factor Analysis for Large-Scale Incomplete Matrices",
            "venue": "IEEE Transactions on Big Data, doi: 10.1109/TBDATA.2021.3090905.",
            "year": 2021
        },
        {
            "authors": [
                "J. Chen",
                "R. Wang",
                "D. Wu",
                "X. Luo"
            ],
            "title": "A Differential Evolution-Enhanced Position-Transitional Approach to Latent Factor Analysis",
            "venue": "IEEE Transactions on Emerging Topics in Computational Intelligence, 2022, doi: 10.1109/TETCI.2022.3186673.",
            "year": 2022
        },
        {
            "authors": [
                "X. Luo",
                "Z. Liu",
                "S. Li",
                "M. Shang",
                "Z. Wang"
            ],
            "title": "A Fast Non-Negative Latent Factor Model Based on Generalized Momentum Method",
            "venue": "IEEE Transactions on Systems, Man, and Cybernetics: Systems, vol. 51, no. 1, pp. 610-620, Jan. 2021.",
            "year": 2021
        },
        {
            "authors": [
                "X. Luo",
                "M. Zhou",
                "S. Li",
                "M. Shang"
            ],
            "title": "An inherently non\u2010negative latent factor model for high\u2010dimensional and sparse matrices from industrial applications",
            "venue": "IEEE Transactions on Industrial Informatics,",
            "year": 2018
        },
        {
            "authors": [
                "X. Luo",
                "Z. Wang",
                "M. Shang"
            ],
            "title": "An Instance-Frequency-Weighted Regularization Scheme for Non-Negative Latent Factor Analysis on High- Dimensional and Sparse Data",
            "venue": "IEEE Transactions on Systems,"
        },
        {
            "authors": [
                "H. Wu",
                "X. Luo",
                "M. Zhou",
                "M.J. Rawa",
                "K. Sedraoui",
                "A. Albeshri"
            ],
            "title": "A PID-incorporated Latent Factorization of Tensors Approach to Dynamically Weighted Directed Network Analysis",
            "venue": "IEEE/CAA Journal of Automatica Sinica, vol. 9, no. 3, pp. 533-546, March 2022.",
            "year": 2022
        },
        {
            "authors": [
                "D. Wu",
                "Y. He",
                "X. Luo",
                "MC. Zhou"
            ],
            "title": "A latent factor analysis-based approach to online sparse streaming feature selection",
            "venue": "IEEE Transactions on Systems, Man, and Cybernetics: Systems, vol. 52, no. 11, pp. 6744-6758, Nov. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "H. Wu",
                "X. Luo"
            ],
            "title": "Instance-Frequency-Weighted Regularized, Nonnegative and Adaptive Latent Factorization of Tensors for Dynamic QoS Analysis",
            "venue": "Proc. of the 2021 IEEE Int. Conf. on Web Services, doi: 10.1109/ICWS53863.2021.00077.",
            "year": 2021
        },
        {
            "authors": [
                "H. Wu",
                "X. Luo",
                "MC. Zhou"
            ],
            "title": "Neural Latent Factorization of Tensors for Dynamically Weighted Directed Networks Analysis",
            "venue": "Proc. of the 2021 IEEE Int. Conf. on Systems, Man, and Cybernetics (SMC2021), doi: 10.1109/SMC52423.2021.9659145.",
            "year": 2021
        },
        {
            "authors": [
                "J.-A Konstan",
                "B.-N. Miller",
                "D. Maltz",
                "J. L Herlocker",
                "L.-R Gordon",
                "J. Riedl"
            ],
            "title": "Grouplens: applying collaborative filtering to usenet news",
            "venue": "Communications of the ACM, vol. 40, no. 3, pp. 77-87, 1997.",
            "year": 1997
        },
        {
            "authors": [
                "J. Mohsen",
                "E. Martin"
            ],
            "title": "A Matrix Factorization Technique with Trust Propagation for Recommendation in Social Networks",
            "venue": "Proc. of the 4th ACM Conf. on Recommender Systems, Barcelona, Spain, 2010, pp. 135-142.",
            "year": 2010
        },
        {
            "authors": [
                "H. Ma",
                "I. King",
                "M.-R Lyu"
            ],
            "title": "Learning to Recommend with Social Trust Ensemble",
            "venue": "Proc. of the 32nd Int. ACM SIGIR Conf. on Research and Development in Information Retrieval, Boston, MA, USA, Jul. 2009, pp. 203-210.",
            "year": 2009
        },
        {
            "authors": [
                "P. Massa",
                "P. Avesani"
            ],
            "title": "Trust-aware Recommender Systems",
            "venue": "Proc. of the 1st ACM Conf. on Recommender Systems, Minneapolis, MN, USA, Oct. 2007, pp. 17-24.",
            "year": 2007
        },
        {
            "authors": [
                "Y. Liu",
                "J. Chen",
                "D. Wu"
            ],
            "title": "An Adam-adjusting-antennae BAS Algorithm for Latent Factor Analysis Refinement",
            "venue": "arXiv e-prints,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Factor Analysis (LFA) model has been investigated in depth to an alyze the latent information. Recently, Swarm Intelligence-related LFA models have been proposed and adopted widely to improve the optimization process of LFA with high efficiency, i.e., the Particle Swarm Optimization (PSO)-LFA model. However, the hyper-parameters of the PSO-LFA model have to tune manually, which is inconvenient for widely adoption and limits the learning rate as a fixed value. To address this issue, we propose an Adam-enhanced Hierarchical PSO-LFA model, which refines the latent factors with a sequential Adam-adjusting hyper-parameters PSO algorithm. First, we design the Adam incremental vector for a particle and construct the Adam-enhanced evolution process for particles. Second, we refine all the latent factors of the target matrix sequentially with our proposed Adam-enhanced PSO\u2019s process. The experimental results on four real datasets demonstrate that our proposed model achieves higher prediction accuracy with its peers.\nKeywords\u2014High-dimensional and Incomplete (HDI) Matrix, Latent Factor Analysis (LFA) model, Adam algorithm, Particle\nSwarm Optimization (PSO) algorithm.\nI. INTRODUCTION\nWith the smart applications and services becoming more complex and more widespread, there have generated and accumulated massive of High-Dimensional and Incomplete (HDI) data [1-7]. These data involve extremely useful latent information for analyzing the features of real entities and their relationships. Digging out the latent information from the HDI data is a challenging and interesting issue, which can help the smart applications and services provide better recommendation to the customers.\nTo analyze the HDI data effectively, the Latent Factor Analysis (LFA) model has been widely investigated [8-11]. The LFA model focuses on analyzing the latent factors of involved entities, which can represent the relationships between two entities and the latent characteristics of entities. The input data of an LFA model is generally an HDI matrix. The LFA model maps the target HDI matrix into a low-dimensional latent factor space, builds a low rank approximate matrix based on the already known HDI data, and resolves the unknown data in the approximate matrix via the iterative optimization process [12-17].\nThe iterative optimization process is quite important in LFA analysis, which is widely used together with optimization algorithms, such as the SGD, Adam, AdaGrad and AdaDelta etc. [18-21]. Among them, SGD is the most classical algorithm with high prediction accuracy. However, SGD algorithm needs to tune learning rate value for various datasets. To adjust the learning rate adaptively, the Adam, AdaGrad and AdaDelta algorithms are adopted recently. Although they obtain relatively high accuracy w/o tunning learning rate manually, much running time is needed to adjust the learning rate in each optimization iteration.\nTo further improve the optimization process, Luo et al.[22-25] introduce the Swarm Intelligence algorithms into the LFA model analysis. In this field, they have proposed a series of state-of-the-art models, i.e., the Particle Swarm Optimization (PSO)LFA models, the Differential Evolution (DE)-LFA models, and the Beetle Antennae Search (BAS)-LFA models. Among them, the Hierarchical Position-transitional Particle Swarm Optimization-based LFA (HPL) model is an outstanding one, which optimizes the latent factors with two innovative PSO algorithms sequentially [24]. First, the Position-transitional Particle Swarm Optimization-based LFA (PLFA) model is adopted to optimize latent factors fast with adjusting the SGD\u2019s learning rate adaptively [22]. Second, the Mini-Batch PSO (MPSO) algorithm is adopted to refine the latent factors, which can achieve higher accuracy with acceptable additional time cost. However, the HPL model has to tune PSO\u2019s hyper-parameters for refining the latent factors of various datasets. That causes two problems. One is consuming lots of tunning time for various datasets, The other is the hyper-parameters can\u2019t be changed during the optimization process.\nTo resolve these two problems, we propose an Adam-enhanced HPL (ADHPL) model to refine the latent factors with adjusting the PSO\u2019s hyper-parameters adaptively. The proposed model first constructs a new gradient vector with the particle\u2019s velocity increment. Then, it adopts Adam to construct the first and second moment gradient vectors and update each particle with Adam algorithm. Last, the ADHPL model refines the latent factors with Adam-adjusting PSO algorithm. Our main contributions are listed as follows:\na) An ADHPL model. We incorporate the Adam-enhanced PSO evolution algorithm into the PSO\u2019s refinement process of HPL. Thereby, the proposed model refines all the latent factors w/o presetting the hyper-parameters. Compared with HPL, the proposed model obtains higher prediction accuracy with almost the same time cost.\nCorresponding author: Yuanyi Liu.\nb) An Adam-enhanced PSO\u2019s evolution algorithm. The classical PSO model updates each particle\u2019s velocity and position iteratively based on its velocity increment. We replace it with the designed Adam incremental vector and construct the Adamenhanced evolution process for particles. The designed vector is based on the particle\u2019s velocity increment vector. Thereby, the proposed parametric adaptive evolution algorithm can simulate the evolution scheme of PSO effectively.\nSection II describes the related definitions and algorithms. Section III proposes our innovative ADHPL model. Section IV describes experimental data and results. Finally, Section V concludes this paper and plans for the future work.\nII. PRELIMINARIES\nFirst, we summarize the mainly adopted symbols in Table I. Then we review the working mechanisms of an SGD-based LFA model, an ADAM algorithm and an HPL model."
        },
        {
            "heading": "A. An LFA Model for HiDS Matrices",
            "text": "Let Z|U|\u00d7|I| represents an HiDS matrix, which denotes the relationships between two large entity sets U and I. Each element zu,i\u2208Z represents a specific relationship value between two specific entities u\u2208U and i\u2208I. Besides, Zun denotes the unknown entity set of Z while Zkn denotes the known entity set. For an HiDS matrix, the number of entities in Zkn is much smaller than in Zun, |Zkn|\u226a|Zun|.\nTo predict Zun, an LFA model constructs a F-rank approximate matrix Z\u0302=PQT, where P|U|\u00d7F and Q|I|\u00d7F denote the Fdimensional latent factor matrices for U and I, respectively. Then, an objective function estimating the Euclidean distance between Z and Z\u0302 is given as [26-32], which is usually combined with linear biases for P and Q, and an L2-norm-based regularization item. The objective function is formulated as:\n( )  =\n = =\n  = \u2212 \u2212 \u2212  \n \n  + + + +  \n \n \n  \n,\n,\n2\n, , , 1\n2 2 2 2\n, , 1 1\n1 , , ,\n2\n. 2\nu i kn\nu i kn\nF\nu i u f f i u i z Z f\nF F\nu f f i u i z Z f f\n\u03b5 P Q z p q b c\n\u03bb p q b c\nb c\n(1)\nwhere bu and ci denote the specific bias vector for U and I, \u03bb is regularization constant, respectively. Because (1) is non-convex and not analytically solvable, an LFA model commonly adopts SGD to optimize its latent factor set {P, Q, b, c} with the preset gradient iteratively.\nRecently, the PSO-related LFA models becomes popular, which incorporates the classical PSO algorithm into LFA\u2019s optimization process. Among them, the HPL model is an outstanding one. Thereby, we select it as the basic model."
        },
        {
            "heading": "A. An HPL Model",
            "text": "An HPL model has a two-layer structure. The first layer adopts a PLFA model to pre-train latent factors, which adjusting SGD\u2019s learning rate by a P2SO algorithm. Then the second layer refines latent factors with a Mini-batch PSO (MPSO) algorithm. It separates all the latent factors into (|U|+|I|) mini-batch groups and optimize each group with PSO algorithm sequentially.\nFor each group [pu,bu], \u2200u\u2208|U|, the MPSO algorithm constructs a swarm consisting of S particles. Each particle\u2019s initial position vector x u\ns(0) consists of a vector nearby or\nZ, zu,i An HiDS matrix consists of the relationship value for two entity sets U and I, and each element in Z.\nZkn, Zun Known and unknown entry set in Z.\nZ\u0302, z\u0302u,i The approximation matrix of Z, and each element in Z\u0302. \u2207\u03b5u,i(\u2219) A partial derivative operator of the objective function to a specific relationship element between the u-th and the i-th entities.\n\u0393, \u039b Training dataset and testing dataset of Z.\n\u03b5() Objective function estimating the distance between Z and Z\u0302. \u03b7, \u03bb Learning rate and regularization constant for a LFA model.\nequal to [pu,bu]. Each particle\u2019s position x u s(\u03c4) and velocity vectors v u s(\u03c4) at the \u03c4-th iteration are represented as:\n  = =    ( ) ( ), ( ) , ( ) ( ), ( ) ,u u u u u u s s s s s s \u03c4 \u03c4 b \u03c4 t \u03c4 b \u03c4x p v p (2)\nwhere p u s (\u03c4), b u s (\u03c4) denote the velocity of pu, bu at the \u03c4-th iteration for the s-th particle, ( ) u s \u03c4p and ( )u s b \u03c4 denote their corresponding velocities. Then, the MPSO algorithm updates each [pu, bu] with traditional PSO and obtain the next iteration value. Velocity v u\ns(\u03c4+1) at the (\u03c4+1)-th iteration is updated as:\n( ) ( )\n( ) ( )\n+ =  + \u2212\n+ \u2212\n   = + \u2212   \n + \u2212  \n1 1\n2 2\n1 1\n2 2\n( 1) ( ) ( ) ( )\n( ) ( )\n( ), ( ) ( ) ( ), ( )\n( ) ( ), ( ) ,\nu u u u s s s s\nu\nu s\nu u u u u s s s s s\nu u\nu s s\n\u03c4 \u03c9 \u03c4 \u03b3 rd \u03c4 \u03c4\n\u03b3 rd \u03c4 \u03c4\n\u03c9 \u03c4 b \u03c4 \u03b3 rd \u03c4 \u03c4 b \u03c4\n\u03b3 rd \u03c4 \u03c4 b \u03c4\nv v h x\ng x\np h p\ng p\n(3)\nwhere \u03b31 and \u03b32 are the two acceleration coefficients. rd1 and rd2 are two random parameters between [0, 1], respectively. Then, the position x u\ns(\u03c4+1) of each [pu, bu] is updated as:\n+ = + +( 1) ( ) ( 1),u u u s s s \u03c4 \u03c4 \u03c4x x v (4)\nTo measure the accuracy of all the updated positions, the fitness function is constructed as:\n( ) ( ) ( )  + = \u2212 \u2212 \u2212 + + ,\n2 2 2\n1 ,\n1 ( 1) .\n2 2 u i kn\nu s u i u i u i u u z Z\n\u03bb F x \u03c4 z q b c bp p (5)\nWith (5), each particle\u2019s updated fitness value is compared with its historical best one. The comparison formula at the (\u03c4+1)th iteration is given as:\n( ) ( ) + +  + = \n\n1 1 ( 1), if ( 1) ( ) ,\n( 1) ( ), otherwise,\nu u u\ns s su s u\ns\n\u03c4 F \u03c4 F \u03c4 \u03c4\n\u03c4\nx x h h\nh (6)\nThen, the global best position g\u0306u(\u03c4+1) can be updated as:\n( )  + + = + 1 ( 1) ( 1) arg min ( 1) . u s\nu u\ns \u03c4 \u03c4 F \u03c4 h g h (7)\nWith the above evolution process, the total |U| [pu, bu] are refined sequentially to resolve the pre-mature problem in the PSO-LFA model. After all the |U| mini-batch [pu, bu] are refined, the |I| mini-batch [qi, ci] are refined sequentially. The HPL model achieves the balance of prediction accuracy and running efficiency."
        },
        {
            "heading": "B. An Adam Algorithm",
            "text": "The working mechanism of Adam is to adjust the instant learning rate at each iteration by calculating the ratio of moving averages of the gradient \u2207\u03b5(\u03c4) and the squared moving averages of the gradient \u2207\u03b5(\u03c4)2. Specifically, at the (\u03c4+1)-th iteration, the moving averages m(\u03c4+1) and the squared moving averages v(\u03c4+1) are calculated as:\n + = + \u2212   + = + \u2212  1 1 2 2 2 ( 1) ( ) (1 ) ( ), ( 1) ( ) (1 ) ( ), m \u03c4 \u03b2 m \u03c4 \u03b2 \u03b5 \u03c4 v \u03c4 \u03b2 v \u03c4 \u03b2 \u03b5 \u03c4 (8)\nwhere \u03b21 and \u03b22 is the exponential decay rate parameter of m and v at the previous iteration, respectively. At the first iteration, m(0) and v(0) are initialized to 0 according to [18]. Besides, \u03b21 and \u03b22 are commonly set as 0.9 and 0.999 as in [18]. Then the corrected parameters m\u0302(\u03c4+1) and v\u0302(\u03c4+1) are formulated as:\n + = + \u2212  + = + \u2212 1 2 \u02c6 ( 1) ( 1) / (1 ), \u02c6( 1) ( 1) / (1 ), m \u03c4 m \u03c4 \u03b2 v \u03c4 v \u03c4 \u03b2 (9)\nWith the updated m\u0302(\u03c4+1) and v\u0302(\u03c4+1), the partial derivative operator \u2207\u03b5(\u03c4+1) can be updated as:\n + =  + + +\u02c6 \u02c6( 1) ( 1) / ( ( 1) ), a a \u03b5 \u03c4 \u03b1 m \u03c4 v \u03c4 \u03c8 (10)\nwhere \u03b1 denotes the step-size setting, \u03c8 denotes a parameter avoiding denominator becoming zero. Then, each parameter in the objective function updates with \u2207\u03b5 iteratively.\nThe above Adam algorithm has been widely adopted in deep learning, graph learning and HiDS matrices analysis. The Adam algorithm improves the accuracy w/o tunning the learning rate manually. Thereby, we incorporate Adam into an HPL model. The proposed ADHPL model does not only adjusts the hyper-parameters automatically, but also promotes the prediction accuracy.\nIII. METHODOLOGY\nAn ADHPL model first reconstructs each particle\u2019s velocity updating formula with Adam, then apply the Adam-MPSO algorithm to refine the latent factors in the HiDS matrices. The specific working mechanism is introduced in detail as described the next two sub-sections."
        },
        {
            "heading": "A. Construct the Adam incremental vector for a particle",
            "text": "The first job is to construct a (F+1)-dimensional gradient vector \u2207\u03b5(\u03c4), which is the base incremental element for the Adamadjusting velocity. To do so, we adopt the combination of the global best position ( )u \u03c4g and each particle\u2019s best position\n( )u s \u03c4h in (3) as the (F+1)-dimensional gradient vector \u2207\u03b5(\u03c4). The \u2207\u03b5(\u03c4) can be represented as:\n( ) ( )\n = \u2212 + \u2212\n = \u2212  \n + \u2212  \n1 2\n1\n2\n( ) ( ( ) ( )) ( ( ) ( ))\n( ) ( ), ( )\n( ) ( ), ( ) .\nu u u u\ns s s\nu u u\ns s\nu u u\ns s\n\u03b5 \u03c4 rd \u03c4 \u03c4 rd \u03c4 \u03c4\nrd \u03c4 \u03c4 b \u03c4\nrd \u03c4 \u03c4 b \u03c4\nh x g x\nh p\ng p\n(11)\nNote that the constructed gradient vector \u2207\u03b5(\u03c4) equals to the incremental velocity vector between vu s(\u03c4+1) and v u\ns(\u03c4) in (3). Thereby, the \u2207\u03b5(\u03c4) can also represent the particle\u2019s velocity evolution during two adjacent iterations. Next, we build the first and second moment moving average parameters gradient vectors m(\u03c4+1) and v(\u03c4+1). The formulas can be reformulated from (8) as:\n( )\n( )\n( )\n( )\n + = + \u2212    \u2212  = + \u2212  + \u2212    + = + \u2212     \u2212  = + \u2212  + \u2212   1 1 1 1 1 2 2 2 2 , 2 1 2 2 2 ( 1) ( ) (1 ) ( ) ( ) ( ) ( ) (1 ) , ( ) ( ) ( 1) ( ) (1 ) ( ) ( ) ( ) ( ) (1 ) . ( ) ( ) u u s u u s s u i u u s u u s s \u03c4 \u03b2 \u03c4 \u03b2 \u03b5 \u03c4 rd \u03c4 \u03c4 \u03b2 \u03c4 \u03b2 rd \u03c4 \u03c4 \u03c4 \u03b2 \u03c4 \u03b2 \u03b5 \u03c4 rd \u03c4 \u03c4 \u03b2 \u03c4 \u03b2 rd \u03c4 \u03c4 m m g x m h x v v g x v h x\n(12)\nThe corrected parameters m\u0302(\u03c4+1) and v\u0302(\u03c4+1) are given as:\n( ) ( )( )\n( ) ( )( )\n + = + \u2212 =  \u2212 +  =  \u2212  + \u2212 + \u2212  + = + \u2212 =  \u2212 +  =  \u2212   + \u2212 + \u2212  1 1 1 1 1 1 2 2 2 2 2 2 2 2 1 2 \u02c6 ( 1) ( 1) / (1 ) ( ) / (1 ) ( ) ( ) / (1 ) ( ) ( ) ( ) ( ) , \u02c6 ( 1) ( 1) / (1 ) ( ) / (1 ) ( ) ( ) / (1 ) ( ) ( ) ( ) ( ) . u u u u s s u u u u s s \u03c4 \u03c4 \u03b2 \u03b2 \u03c4 \u03b2 \u03b5 \u03c4 \u03b2 \u03c4 \u03b2 rd \u03c4 \u03c4 rd \u03c4 \u03c4 \u03c4 \u03c4 \u03b2 \u03b2 \u03c4 \u03b2 \u03b5 \u03c4 \u03b2 \u03c4 \u03b2 rd \u03c4 \u03c4 rd \u03c4 \u03c4 m m m m g x h x v v v v g x h x\n(13)\nBy adding (13) into (10), we reformulate the gradient \u2207\u03b5(\u03c4+1) in the next iteration with m(\u03c4), v(\u03c4) and \u2207\u03b5(\u03c4) as:\n( ) ( )( )\n( ) ( )( )\n \u2212 +  + = \n \u2212 + +\n  \u2212 + \u2212 + \u2212 =\n \u2212 + \u2212 + \u2212 +\n1 1\n2\n2 2\n1 1 1 2\n2\n2 2 1 2\n( ) / (1 ) ( ) ( 1)\n( ) / (1 ) ( )\n( ) / (1 ) ( ) ( ) ( ) ( ) .\n( ) / (1 ) ( ) ( ) ( ) ( )\nu u u u\ns s\nu u u u\ns s\n\u03b2 \u03c4 \u03b2 \u03b5 \u03c4 \u03b5 \u03c4 \u03b1\n\u03b2 \u03c4 \u03b2 \u03b5 \u03c4 \u03c8\n\u03b1 \u03b2 \u03c4 \u03b2 rd \u03c4 \u03c4 rd \u03c4 \u03c4\n\u03b2 \u03c4 \u03b2 rd \u03c4 \u03c4 rd \u03c4 \u03c4 \u03c8\nm\nv\nm g x h x\nv g x h x\n(14)\nWith the above (F+1)-dimensional gradient vector \u2207\u03b5(\u03c4) and \u2207\u03b5(\u03c4+1) consists of particle swarm\u2019s global best position and each particle\u2019s best position, we can update the particles\u2019 velocity and positions with Adam. Besides, the constructed incremental vector uses the PSO\u2019s original incremental elements, which can remain the PSO\u2019s characteristics."
        },
        {
            "heading": "B. Refine the latent factors in MPSO with Adam",
            "text": "We construct ADHPL model by updating all the particles\u2019 positions with Adam in the MPSO algorithm. All the preset hyper-parameters are substituted with the Adam incremental vector. First, we construct and initialize the particle swarms for each mini-batch group [pu, bu], \u2200u\u2208|U| and [qi,ci], \u2200i\u2208|I| consisting of S particles. For a specific s-th particle, the velocity v u s (0) and position x u\ns(0) are both initialized as a vector nearby the vector [pu, bu]. Besides, the first and second moment moving average parameters m(0) and v(0) are preset. Second, we set velocity v u\ns(\u03c4+1) as \u2207\u03b5(\u03c4+1), which substitutes the formula in (3). We represent the new formula as:\n + =  + = + +   ( 1) ( 1) ( 1), ( 1) ,u u u s s s \u03c4 \u03b5 \u03c4 \u03c4 b \u03c4v p (15)\nWith the updated velocity v u s(\u03c4+1), the new position of the s-th particle x u s(\u03c4+1) is updated as:\n( ) ( )( )\n( ) ( )( )\n+ = + + = + +\n+ \u2212 + \u2212 \u2212\n= +\n+ \u2212 + \u2212 + \u2212\n1 1 2\n1\n2 2\n1 2\n2\n( 1) ( ) ( 1) ( ) ( 1)\n( ) ( ) ( ) ( ) ( )\n1 [ ( ), ( )] .\n( ) ( ) ( ) ( ) ( )\n1\nu u u u s s s s\nu u u u\ns s u u s s\nu u u u\ns s\n\u03c4 \u03c4 \u03c4 \u03c4 \u03b5 \u03c4\n\u03b1\u03b2 \u03c4 rd \u03c4 \u03c4 rd \u03c4 \u03c4\n\u03b2 p \u03c4 b \u03c4\n\u03b2 \u03c4 rd \u03c4 \u03c4 rd \u03c4 \u03c4 \u03c8\n\u03b2\nx x v x\nm g x h x\nv g x h x\n(16)\nBased on the above update scheme, each [pu, bu] is updated with Adam until reaches the convergence conditions. After refining all the |U| [pu, bu], the ADHPL model continues refining all the |I| [qi,ci] until reaches the convergence conditions."
        },
        {
            "heading": "C. Algorithm Design and Analysis",
            "text": "With the above latent factor refining process, we can construct an ADHPL model. It can be seen as a sequential Adam-PSO refining process for all the latent factors in the target HDI matrix. The workflow of the ADHPL model is depicted in Fig. 1.\nFirst, all the pre-trained latent factors are grouped by rows and columns. Thereby, all the latent factors are separated as the |U| row groups and |I| column groups. Each row group contains a latent factor pair [pu, bu], and each column group contains a latent factor pair [qi, ci].\nSecond, for each [pu, bu], \u2200u\u2264|U|, we perform the Adam-PSO to refine the latent factors while fixing the others. Each refined latent factor is transferred back to P, b.\nThird, for each [qi, ci], \u2200i\u2264|I|, we perform the Adam-PSO to refine the latent factors while fixing the others. Each refined latent factor is transferred back to Q, c.\nFinally, after all the groups in {P, Q, b, c} have been refined on training dataset once, calculate RMSE value on validation dataset. If the model converges on validation dataset, stop the refinement process and evaluate its RMSE value on testing dataset, otherwise continue refinement.\nBased on the above steps, the proposed ADHPL model can refine all the latent factors in a target HiDS matrix sequentially with Adam-adjusting PSO algorithm.\nIV. EXPERIMENTAL RESULTS AND ANALYSIS"
        },
        {
            "heading": "A. General Settings",
            "text": "Evaluation Protocol. To compare our proposed model\u2019s prediction accuracy and efficiency with some state-of-the-art models, we choose RMSE to measure the prediction accuracy. We choose the CPU running time cost to measure the models\u2019 efficiency. We perform all the experiments on a workstation that has a 32GB RAM and a 3.91 GHz Xeon CPU. The formulas of RMSE are given as:\n( ) , 2 , ,\n1 \u02c6( ) .\nu i u i u ir\nRMSE r r  = \u2212  \nwhere \u039b represents the testing set disjoint with the training and validation sets.\nDataset. We adopt four datasets ML10M[33], Flixster[34],\nFig.1 Flow Chart of the ADHPL Model\nDouban[35] and ExtEpinion[36] to perform our models. We divide each dataset as training, validation and testing sub-datasets, which are 70%, 10% and 20%, respectively.\nModel Settings. We adopt the SGD-based LFA, Adam-based LFA, PLFA with biases, A2BAS-PLFA[37], HPL and SGDEPLFA models as the comparison models. The dimension of LF space f is set as 20. The regularization coefficient is preset the same value for all the involved models. All the other hyper-parameters are set as the original settings."
        },
        {
            "heading": "B. Performance comparison",
            "text": "The lowest RMSE value and Friedman rank of all the involved models on four datasets are given in Table II. The time cost is given in Table III."
        },
        {
            "heading": "1) Comparison of prediction accuracy",
            "text": "From Table II, we summarize the ADHPL model achieves relatively higher prediction accuracy compared with its peers. Two specific conclusions can be summarized as follows:\na) The Adam-HPL\u2019s prediction accuracy evaluated by RMSE is the highest of all the involved models, except for the SGDEPLFA. For instance, the ADHPL\u2019s RMSE on ExtEpinion equals to 0.5279, which is 0.25% lower than SGDE-PLFA\u2019s 0.5292, 1.16% lower than HPL\u2019s 0.5341, and even much lower than other models. The ADHPL\u2019s RMSE on Flixster equals to 0.8599, which is 0.13% lower than SGDE-PLFA\u2019s 0.8610, 0.19% lower than A2BAS-PLFA\u2019s 0.8615, and much lower than other models.\nOne exception is the SGDE-PLFA model. Our proposed ADHPL outperforms two datasets compared with SGDE-PLFA, performs equally well on one dataset, and performs worse on one dataset. For instance, on ML10M, the ADHPL\u2019s RMSE value equals to 0.7840, which is 0.04% higher than the SGDE-PLFA\u2019s 0.7837, while this value is lower than all the others. Besides, on Douban, the ADHPL performs as well as SGDE-PLFA, while they all outperform the others.\nb) The ADHPL\u2019s prediction accuracy gain is statistically significant compared with its peers. The Win/Loss results are recorded in the next-to-last row of Table II, from which we can observe that the ADHPL and SGDE-PLFA both\nML10M 874 891 157 115 280 249 235\nExtEpinion 1152 4331 369 1462 700 438 426\nFlixster 451 3393 91 331 212 222 252\nDouban 1193 2064 340 298 541 476 482\nwin the others on all the datasets. The Friedman test results are recorded in the last row of Table II. Both ADHPL and SGDEPLFA\u2019s F-rank value equal to 1.5, which is much less than other models."
        },
        {
            "heading": "2) Comparison of computational efficiency",
            "text": "From Table III, we summarize three conclusions of the ADHPL model\u2019s time cost:\na) It costs much less than the SGD-based LFA and Adam-based LFA. For instance, the ADHPL costs only 482s on Douban, which is 59.6% lower than SGD-based LFA\u2019s 1193s, 76.65% lower than Adam-based LFA\u2019s 2064s.\nb) It costs about the same amount of time with other Swarm Intelligence-related LFA models, i.e., A2BAS-PLFA, HPL and SGDE-PLFA. Furthermore, they all cost considerably more time than PLFA model. The reason is that they all refine the latent factors after the PLFA model convergences. However, the extra time of about a hundred seconds is worthy for promoting the prediction accuracy.\nAccording to the above analysis, we conclude that the ADHPL model obtains superiority in terms of prediction accuracy, while convergences relatively fast.\nV. CONCLUSIONS\nThis paper focuses on refining the latent factors with an Adam-enhanced HPL model. In this model, we design a base gradient with particle\u2019s velocity increment. With the proposed gradient, we propose an Adam-PSO algorithm, which can remain the characteristics of PSO w/o tunning PSO\u2019s hyper-parameters. Using this algorithm, we construct the ADHPL model to refine the latent factors sequentially with row and column sub-groups. The experimental results from four industrial datasets verify its high prediction accuracy. In future, we plan to investigate an ensemble model with various Swarm Intelligence-related LFA models, which can dynamically schedule these models considering various situations or datasets."
        }
    ],
    "year": 2023
}