{
    "abstractText": "Robots and artificial agents that interact with humans should be able to do so without bias and inequity, but facial perception systems have notoriously been found to work more poorly for certain groups of people than others. In our work, we aim to build a system that can perceive humans in a more transparent and inclusive manner. Specifically, we focus on dynamic expressions on the human face, which are difficult to collect for a broad set of people due to privacy concerns and the fact that faces are inherently identifiable. Furthermore, datasets collected from the Internet are not necessarily representative of the general population. We address this problem by offering a Sim2Real approach in which we use a suite of 3D simulated human models that enables us to create an auditable synthetic dataset covering 1) underrepresented facial expressions, outside of the six basic emotions, such as confusion; 2) ethnic or gender minority groups; and 3) a wide range of viewing angles that a robot may encounter a human in the real world. By augmenting a small dynamic emotional expression dataset containing 123 samples with a synthetic dataset containing 4536 samples, we achieved an improvement in accuracy of 15% on our own dataset and 11% on an external benchmark dataset, compared to the performance of the same model architecture without synthetic training data. We also show that this additional step improves accuracy specifically for racial minorities when the architecture\u2019s feature extraction weights are trained from scratch.",
    "authors": [
        {
            "affiliations": [],
            "name": "Saba Akhyani"
        },
        {
            "affiliations": [],
            "name": "Mehryar Abbasi"
        },
        {
            "affiliations": [],
            "name": "Mo Chen"
        },
        {
            "affiliations": [],
            "name": "Angelica Lim"
        }
    ],
    "id": "SP:0cc898e2e20128edbe409d4d444b7343bdd69414",
    "references": [
        {
            "authors": [
                "I. Abbasnejad"
            ],
            "title": "Using synthetic data to improve facial expression analysis with 3d convolutional networks",
            "venue": "ICCVW, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "T. Baltru\u0161aitis"
            ],
            "title": "Openface: An open source facial behavior analysis toolkit",
            "venue": "WACV, 2016, pp. 1\u201310.",
            "year": 2016
        },
        {
            "authors": [
                "L.F. Barrett"
            ],
            "title": "Emotional expressions reconsidered: Challenges to inferring emotion from human facial movements",
            "venue": "Psychological Science in the Public Interest, vol. 20, no. 1, pp. 1\u201368, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "M. Bastioni"
            ],
            "title": "Ideas and methods for modeling 3d human figures: the principal algorithms used by makehuman and their implementation in a new approach to parametric modeling",
            "venue": "Proceedings of the 1st Bangalore Annual Compute Conference, 01 2008, p. 10.",
            "year": 2008
        },
        {
            "authors": [
                "J. Buolamwini",
                "T. Gebru"
            ],
            "title": "Gender shades: Intersectional accuracy disparities in commercial gender classification",
            "venue": "FAT, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "J. Carreira",
                "A. Zisserman"
            ],
            "title": "Quo vadis, action recognition? a new model and the kinetics dataset",
            "venue": "CVPR 2017.",
            "year": 2017
        },
        {
            "authors": [
                "H.A. Dau"
            ],
            "title": "The UCR time series archive",
            "venue": "CoRR, vol. abs/1810.07758, 2018.",
            "year": 1810
        },
        {
            "authors": [
                "F. Dornaika",
                "B. Raducanu"
            ],
            "title": "Efficient facial expression recognition for human robot interaction",
            "venue": "IWANN, 2007, pp. 700\u2013708.",
            "year": 2007
        },
        {
            "authors": [
                "P. Ekman",
                "E.L. Rosenberg"
            ],
            "title": "What the face reveals: Basic and applied studies of spontaneous expression using the Facial Action Coding System (FACS)",
            "year": 1997
        },
        {
            "authors": [
                "H.I. Fawaz"
            ],
            "title": "Inceptiontime: Finding alexnet for time series classification",
            "venue": "Data Mining and Knowledge Discovery, vol. 34, no. 6, pp. 1936\u20131962, 2020.",
            "year": 1936
        },
        {
            "authors": [
                "T. Gerig"
            ],
            "title": "Morphable face models-an open framework",
            "venue": "FG 2018.",
            "year": 2018
        },
        {
            "authors": [
                "M. Gilbert"
            ],
            "title": "Facshuman a software to create experimental material by modeling 3d facial expression",
            "venue": "IVA 2018.",
            "year": 2018
        },
        {
            "authors": [
                "A. Gupta"
            ],
            "title": "Synthetic data for text localisation in natural images",
            "venue": "CVPR, 2016, pp. 2315\u20132324. 4https://www.unrealengine.com",
            "year": 2016
        },
        {
            "authors": [
                "J. Han"
            ],
            "title": "Improving face detection performance with 3d-rendered synthetic data",
            "venue": "arXiv:1812.07363, 12 2018.",
            "year": 1812
        },
        {
            "authors": [
                "M. Hucko"
            ],
            "title": "Confusion detection dataset of mouse and eye movements",
            "venue": "UMAP, July 2020.",
            "year": 2020
        },
        {
            "authors": [
                "R.E. Jack",
                "P.G. Schyns"
            ],
            "title": "Toward a social psychophysics of face communication",
            "venue": "Annual Review of Psychology, vol. 68, no. 1, pp. 269\u2013297, Jan. 2017.",
            "year": 2017
        },
        {
            "authors": [
                "M. Jangid"
            ],
            "title": "Video-based facial expression recognition using a deep learning approach",
            "venue": "Advances in Computer Communication and Computational Sciences, 2019, pp. 653\u2013660.",
            "year": 2019
        },
        {
            "authors": [
                "M.I. Jordan",
                "T.M. Mitchell"
            ],
            "title": "Machine learning: Trends, perspectives, and prospects",
            "venue": "Science, no. 6245, pp. 255\u2013260, July 2015.",
            "year": 2015
        },
        {
            "authors": [
                "B.-K. Kim"
            ],
            "title": "Fusing aligned and non-aligned face information for automatic affect recognition in the wild: a deep learning approach",
            "venue": "CVPR Workshop 2016.",
            "year": 2016
        },
        {
            "authors": [
                "A. Kortylewski"
            ],
            "title": "Empirically analyzing the effect of dataset biases on deep face recognition systems",
            "venue": "CVPR Workshops, 2018, pp. 2093\u20132102.",
            "year": 2018
        },
        {
            "authors": [
                "P. Lucey"
            ],
            "title": "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression",
            "venue": "CVPR workshops, 2010, pp. 94\u2013101.",
            "year": 2010
        },
        {
            "authors": [
                "D. McColl"
            ],
            "title": "A survey of autonomous human affect detection methods for social robots engaged in natural HRI",
            "venue": "JINT, vol. 82, no. 1, pp. 101\u2013133, Aug. 2015.",
            "year": 2015
        },
        {
            "authors": [
                "F. Mueller"
            ],
            "title": "Ganerated hands for real-time 3d hand tracking from monocular rgb",
            "venue": "CVPR 2018.",
            "year": 2018
        },
        {
            "authors": [
                "N. Naik",
                "M.A. Mehta"
            ],
            "title": "An improved method to recognize handover-face gesture based facial emotion using convolutional neural network",
            "venue": "CONECCT, July 2020.",
            "year": 2020
        },
        {
            "authors": [
                "B. Nojavanasghari"
            ],
            "title": "Hand2face: Automatic synthesis and recognition of hand over face occlusions",
            "venue": "ACII, oct 2017, pp. 209\u2013215.",
            "year": 2017
        },
        {
            "authors": [
                "M. Pantic"
            ],
            "title": "Web-based database for facial expression analysis",
            "venue": "ICME, 2005, pp. 5\u2013pp.",
            "year": 2005
        },
        {
            "authors": [
                "L. Rhue"
            ],
            "title": "Racial influence on automated perceptions of emotions",
            "venue": "SSRN Electronic Journal, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "J.A. Russell"
            ],
            "title": "A circumplex model of affect.",
            "venue": "Journal of Personality and Social Psychology,",
            "year": 1980
        },
        {
            "authors": [
                "G. Saheb Jam"
            ],
            "title": "Developing a data-driven categorical taxonomy of emotional expressions in real world human robot interactions",
            "venue": "HRI, 2021, p. 479\u2013483.",
            "year": 2021
        },
        {
            "authors": [
                "F. Schroff"
            ],
            "title": "FaceNet: A Unified Embedding for Face Recognition and Clustering",
            "venue": "arXiv:1503.03832 [cs], June 2015.",
            "year": 2015
        },
        {
            "authors": [
                "G. Sharma",
                "A. Dhall"
            ],
            "title": "A survey on automatic multimodal emotion recognition in the wild",
            "venue": "Advances in Data Science: Methodologies and Applications, Aug. 2020, pp. 35\u201364.",
            "year": 2020
        },
        {
            "authors": [
                "S.D. Sims",
                "C. Conati"
            ],
            "title": "A neural architecture for detecting user confusion in eye-tracking data",
            "venue": "ICMI, Oct. 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Tang"
            ],
            "title": "Deep learning using support vector machines",
            "venue": "CoRR, abs/1306.0239, vol. 2, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "A. Toisoul"
            ],
            "title": "Estimation of continuous valence and arousal levels from faces in naturalistic conditions",
            "venue": "Nature Machine Intelligence, vol. 3, no. 1, pp. 42\u201350, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "T. Tommasi"
            ],
            "title": "A deeper look at dataset bias",
            "venue": "Domain adaptation in computer vision applications, 2017, pp. 37\u201355.",
            "year": 2017
        },
        {
            "authors": [
                "Z. Wang"
            ],
            "title": "Time series classification from scratch with deep neural networks: A strong baseline",
            "venue": "IJCNN, May 2017.",
            "year": 2017
        },
        {
            "authors": [
                "F.I. Yasser"
            ],
            "title": "Detection of confusion behavior using a facial expression based on different classification algorithms",
            "venue": "ETJ, vol. 39, no. 2A, pp. 316\u2013325, Feb. 2021.",
            "year": 2021
        },
        {
            "authors": [
                "B.-K. Yi"
            ],
            "title": "Efficient retrieval of similar time sequences under time warping",
            "venue": "ICDE 1998.",
            "year": 1998
        },
        {
            "authors": [
                "Z. Yu",
                "C. Zhang"
            ],
            "title": "Image based static facial expression recognition with multiple deep network learning",
            "venue": "ICMI, 2015, p. 435\u2013442.",
            "year": 2015
        },
        {
            "authors": [
                "K. Zhang"
            ],
            "title": "Joint face detection and alignment using multitask cascaded convolutional networks",
            "venue": "IEEE Signal Processing Letters, vol. 23, no. 10, pp. 1499\u20131503, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "G. Zhao"
            ],
            "title": "Facial expression recognition from near-infrared videos",
            "venue": "Image and Vision Computing, vol. 29, no. 9, pp. 607\u2013619, 2011.",
            "year": 2011
        }
    ],
    "sections": [
        {
            "text": "I. INTRODUCTION\nThere has been an increasing interest in using robots in everyday social environments like hospitals, retail stores, and homes. As a result, it has become essential for robots to communicate and interact socially for various humanrobot interaction (HRI) applications and understand people\u2019s nonverbal expressions. For instance, imagine a restaurant service robot seeing a look of confusion wash over your face as you look at the menu. It detects your hesitation and proactively offers assistance: \u201cDo you have any questions I can answer?\u201d This simple scenario plays out between humans each day all over the world, but robots are still far from capable of performing this kind of proactive assistance in a robust manner. Several major challenges prevent such systems from being deployed in the real world.\nFirstly, a recent review has argued that state-of-the-art facial emotion classifiers cannot be applied effectively to human emotion analysis in the wild [3]. One underlying reason is that in HRI, as stated in a review by [22], \u201cthere are a wide range of possible affective levels expressed by\nAll authors are associated with Simon Fraser University, Burnaby, Canada {sakhyani,mabbasib,mochen,angelica}@sfu.ca. This work was supported by the Huawei-SFU Visual Computing Joint Lab and the Natural Sciences and Engineering Research Council of Canada (NSERC), RGPIN/06908-2019.\npeople in human-robot interaction that the robot needs to understand in order to participate in a natural bi-directional social interaction with humans.\u201d In other words, real-world interactions comprise a rich and subtle set of expressions, while most datasets focus on collecting the prototypical set of emotions [3] of happiness, sadness, anger, surprise, fear, disgust, and neutral [17], [34], [40], [19], Specifically, to the best of our knowledge, there is no public video dataset containing confusion [38] nor any comparison benchmark [15] for these dynamic facial expressions [16], [8]. Strategies are needed to effectively create video datasets for the many underrepresented emotional expression categories that actually occur in the wild with robots, such as the 28 social signals identified in Saheb Jam et al. [30], including confused, worried, skeptical, and so on. Indeed, prototypical sadness or fear were not seen in these real-world humanrobot interactions, and we do not focus on these emotions in this paper.\nSecondly, social robots should also have the ability to evaluate human affective expressions fairly, without discriminating against underrepresented groups. A recent survey on automatic multi-modal emotion recognition in the wild shows that inclusivity of all ethnicities remains a challenge in facial expression recognition systems and should be further investigated [32]. According to [27], [5], racial bias is apparent in current machine learning methods in general, especially those involving the face. One major cause of this bias is that major facial expression recognition (FER) datasets are underrepresentative of genders and nonCaucasian backgrounds [36]. As faces are inherently identifiable, ethical and privacy concerns arise when requiring real humans to provide their data [18], yet anonymizing the face can remove important facial features. Data collection to reduce bias in face-related algorithms is, therefore, a major ethical challenge.\nFinally, mobile robots can potentially view humans from various viewing angles and in varied lighting conditions. Collecting and labeling large amounts of naturalistic videos for facial expression recognition is challenging due to several reasons. First, creating datasets for spontaneous (rather than posed) user affective states is very time-consuming [33]. Additionally, facial emotional expressions are difficult to label due to the subjective nature of annotation, compared to those in domains where deep learning has been most successful, such as in object recognition. Thus, data augmentation techniques are expected to be particularly useful to help facial expression recognition succeed in the wild.\nIn this paper, we employ a simulation to reality (Sim2Real)\nar X\niv :2\n20 8.\n07 47\n2v 1\n[ cs\n.C V\n] 1\n5 A\nug 2\n02 2\napproach to address the previously mentioned challenges. Sim2Real approaches have performed well in different domains such as hand tracking [23] and text detection [13]. For face recognition and facial feature detection, studies have attempted to address some of these problems by creating static, synthetic image datasets [11], [1], [14], [20]. We extend these approaches into the video emotion recognition domain by i) identifying facial movements and social signals of the desired dynamic emotional expressions from real data, ii) converting the identified social signals into renderable animations, iii) generating virtual human models of various and specific ethnicities, iv) rendering the animations into videos of all virtual human models from multiple viewing angles, v) pretraining a dynamic emotion classifier model on the synthetic videos, vi) retraining and testing the model on the real dataset.\nWe generate an inclusive synthetic facial expression dataset from virtual humans that specifically incorporates new expressions outside of the prototypical set of six basic emotions, underrepresented ethnicities, and varied camera angles. As a proof-of-concept, we detect the dynamic social signal of confusion. In order to evaluate the model, anger and disgust were chosen from the 6 basic emotions as challenging emotion expression confounders due to their similarity to confusion. Characteristics of these three emotions overlap, and expressions can be easily mistaken for one another, as illustrated in the Circumplex Model of Affect (CMA) shown in Fig. 1. The circumplex model is a graphical representation of affective states on the 2-D plane of arousal (vertical) and valence (horizontal).\nOur contributions are as follows:\n1) We propose the first video dataset of the understudied emotional expression of confusion (as well as nearby social signals \u201cdisgust\u201d and \u201canger\u201d as shown in the circumplex model in Fig. 1). We first gather a real dataset, then augment this limited data using Sim2Real to produce a much larger synthetic dataset. Our dataset also addresses the critical issue of racial bias, which is apparent in existing real-world data, by comprising faces of underrepresented ethnicities, including Black,\nAsian and Hispanic individuals. 2) We explore the effect of adding synthetic data on\nimproving fairness using a CNN+Time Series Classification (TSC) network architecture. Our experiments demonstrate that: 1) training on a combination of realworld data and a randomly selected portion of synthetic data (changing every epoch) achieves the highest performance and 2) fine-tuning on a pre-trained CNN with unfrozen face feature extraction weights decreases racial bias.\nThe Sim2Real approach would allow us to create even larger synthetic datasets in future, because of the flexibility to be applied to any desired emotion or ethnicity. This is feasible due to the fact that facial movements (action units) associated with any emotion can be extracted either automatically using OpenFace [2] or by manual observation. Additionally, the wide modification range of the MakeHuman toolkit [4] allows for the generation of several human models to incorporate other ethnicities."
        },
        {
            "heading": "II. METHODOLOGY",
            "text": "The methodology behind our Sim2Real approach is explained in this section. An overview of our dataset generation and preparation, as well as an overview of our deep-learningbased dynamic facial expression recognition model, is provided. The synthetic data pretraining step of the Sim2real approach is explored in Section III."
        },
        {
            "heading": "A. Dataset Generation",
            "text": "In this section, we describe the collection of in-thewild emotionally expressive videos and the generation of synthetic videos using a suite of simulated humans. We focus this study on confusion, a dynamic social signal which is underrepresented in datasets and lacks examples on the web [15], yet is common in HRI [30].\n1) Collection of in-the-wild confusion, anger, and disgust videos: Confusion is an affective state conveyed through varied and multiple expressions, as are both anger and disgust. Some of these expressions (such as frowning) are common between the three, resulting in some expressions being easily mistaken as another emotion, possibly due to these three emotions appearing very close together in the CMA [29]. We therefore focus on making a dataset for these three emotions.\nWe collected short video clips (1-3s) from YouTube.com and Giphy.com using search tags such as \u201cangry\u201d, \u201cconfused\u201d, and \u201cdisgust\u201d reactions to gather human facial expressions of our desired social signals, as shown in Fig. 2. This search resulted in 153 clips. Each video was then labeled for the conveying facial expression, by two annotators identifying with Canadian culture (inter-rater agreement kappa score=.88), and low confidence videos were discarded.\nWe created a multi-ethnicity dataset of real human videos expressing the three social signals of confusion (41 videos), anger (41 videos), and disgust (41 videos). The final dataset contains 123 videos, of which 26 are of non-Caucasian individuals.\n2) Generation of augmented dynamic social signal video dataset: The task of creating desired social signals videos is made possible using the MakeHuman toolkit [4] and the FACSHuman plugin [12]. MakeHuman toolkit is an open-source and free 3D computer graphics toolset designed for prototyping human-like models. FACSHuman offers the possibility of manipulating the Action Units (AU) presented in the Facial Action Coding System (FACS) [9] on the 3D models created in the MakeHuman software. This manipulation of AUs is a key component of our Sim2Real process. FACSHuman enabled us to generate social signal animations that can be rendered into videos or frames on a selected human virtual model from any viewing angle. We created a script plugin that used this capability to render 4536 synthetic videos from the combination of 24 human virtual models, 21 social signals (facial movement animations), and 9 viewing angles.\nCreation of a suite of simulated humans: The overarching vision of this work is to create a large, auditable suite of human models to represent people from many different backgrounds. As a first step, we create 24 simulated human adult models balanced on gender and four different ethnicities (Caucasian, Black, Asian, and Hispanic). The MassProduce plugin within the MakeHuman application was then used to create several randomly generated human models of multiple ethnicities and different ages and skin colors. Out of all those generated human-like models, we selected 24 models for our study based on the realism of action unit manipulation on the model (8 samples are shown in Fig. 3). We chose to use 3D models as we hope to eventually use them in HRI simulators, to create expressive virtual humans with facial expressions. These models are provided on Github1 so that researchers can also import the 24 virtual humans with 21 dynamic expressions (7 social signals per emotion), to replay them in front of their virtual robot.\n1https://github.com/sabaak95/confusionDetection\nMultiple social signals per emotion: We used the FACSHuman software to create 21 different social signal animations, 7 for each emotional class. These animations convey multiple variations of social cues of confusion, anger, and disgust. These 21 social signals were manually animated over 25 frames and were created via inspection of the in-the-wild real human dataset. For example, Fig. 4 shows the AUs that were used to create dynamic confusion social signals. Varied AU combinations and sequences were used to animate the 21 social signals, validated by an annotator with Canadian culture. An example is a side-eye movement confusion state made by a timed sequence of the following AUs: AU61, AU62, AU61. While future work should automatically perform the animation creation process from video data, the manual animation creation step in this study allows us to validate the Sim2Real portion given human-level feature extraction, enabling us to identify specific underlying social signals for each emotion (7 for each emotion), which are now available for use in our 3D models. The dataset is available for download.2\nMultiple viewing angles: As robots may view a human from varied angles, it is important that our generated dataset incorporate varied perspectives. Our dataset was therefore expanded by creating videos of the same facial gesture from 9 viewing angles, to make our network invariant to the face viewing angle, as shown in Fig. 5. The camera movement included horizontal rotations of \u221240,\u221220, 0, 20, 40 degrees and vertical rotations of \u221230, 15, 0, 15, 30 degrees. The\n2https://www.rosielab.ca/datasets/ confusion-in-the-wild\nnine combinations of (Hrotation, Vrotation)={(\u221240,\u221230), (\u221220,\u221215), (0, 0), (20, 15), (40, 30), (40,\u221230), (20,\u221215), (20,\u221215), (40,\u221230)} were selected as our viewing angles in degrees, as shown in Fig. 5."
        },
        {
            "heading": "B. Data preparation",
            "text": "In order to refine the data and remove any unimportant or unrelated information in the images, we used Multi-task CNN (MTCNN) to detect and crop the faces before feeding frames to our network [41] and resized images to 160 \u00d7 160. Additional transforms were also applied to the images randomly on each epoch, including cropping, perspective, affine transform, horizontal flip, and color transforms (shown in Fig. 6).We ensured that the same transformations were applied to all the frames from the same video."
        },
        {
            "heading": "C. Model Architecture",
            "text": "We developed a basic framework for our video classification problem to test the Sim2Real strategy. Existing work can extract valuable frame-based facial features from a face image, such as FaceNet [31] and OpenFace [2]; one of these models can be used to first extract each frames\u2019 facial features. After frame-based feature extraction, we model the problem as Time Series Classification. Sections II-C.1, IIC.2 are dedicated for further exposition on our selections for this architecture.\n1) Facial Feature Extraction Network: We used the pretrained FaceNet [31] architecture as our facial feature extractor. FaceNet uses an InceptionResnetV1 architecture trained on the VGGFace2 dataset. Each video is given to FaceNet frame by frame, and the output feature arrays are concatenated together across the time dimension to create a multivariate time series array.\n2) Time Series Classifier Network: Further processing of this output requires a time series classification algorithm. K-Nearest Neighbor (KNN) algorithm with Dynamic Time Warping (DTW) [39] metric is one of the earliest techniques for this task that is still used, specially when working with relatively small datasets. Many machine learning algorithms have also been applied to this problem, such as ResNet and FCN [37]. However, we opted to use InceptionTime [10] as our classifier because it has proven to be a versatile and promising machine learning solution for many Time Series Classification tasks, based on its performance results on the UCR [7] benchmark collection datasets.\n3) Proposed and Baseline Architectures: Our proposed DNN structure is the combination of FaceNet [31] and InceptionTime128 [10]. This structure is shown in Fig. 7. This model is referred to here as FN+INC25 or FN+INC64. The two numbers of 25 and 64 indicate the required number of frames for the video input. Videos shorter than the indicated number are padded to the required input length, and the longer videos are cropped. This procedure is explained in Section III. We included two instances of these models in our experiments. In one instance, the FaceNet weights are frozen. In the main instance, the FaceNet weights are not frozen, and are tuned alongside the InceptionTime weights. We hypothesized that the addition of synthetic data would allow us to tailor the FaceNet weights in favor of this dataset.\nThe second DNN architecture included in our experiments is I3D [6], an advanced video classification method that applies combined temporal and spatial processing using 3D convolutional layers. This addition enabled us to evaluate the problem using a model not already affected by previous facial information knowledge.\nFinally, we included a KNN classifier architecture applicable to the small real dataset. This model uses FaceNet [31] for frame facial feature extraction and a KNN with DTW [39]\nmetric as the video classifier. We propose this baseline model, FN+KNN, for comparison."
        },
        {
            "heading": "III. EXPERIMENTS AND RESULTS",
            "text": "In this section, we evaluate our network on the created real dataset. We performed several experiments varying the architecture, input length, and the use of only synthetic, synthetic plus real, or only real training data. We used 5- fold cross-validation on the real dataset to compare the performance of different approaches, with one fold consisting primarily of expressions by non-Caucasian individuals.\nWe explored three training strategies for our experiments. In the first strategy, the algorithms are only trained on the small real dataset. The baseline KNN model was tested under this strategy. In the second strategy, the networks are first trained on synthetic data, then fine-tuned on the real dataset. The second strategy was developed to add and assess the addition of synthetic data. Third, the strategy was to combine the real training data set with the synthetic dataset and pass them to the network alongside each other. This strategy was designed to explore if a higher performance could be achieved by creating ratioed synthetic and real data training. Instead of combining the whole synthetic dataset with the real data, we trained the network with one-fourth of the synthetic dataset. In the next test, the ratio of the synthetic dataset was set to half. Finally, in the last test, the ratio of the synthetic dataset was set to one, meaning the whole synthetic dataset was included.\nOne important factor in our training and testing processes is setting the input video length to a fixed number of frames L. Input videos shorter than the set length were looped until they reached length L. The way we dealt with longer videos differed depending on the training phase. In the training phase, we randomly selected L consecutive frames from the lengthier videos. For a video with N frames, frames n to n+L\u2212 1 are cropped. The n is selected randomly on each epoch between 0 and N \u2212L. However, in the testing phase, we only selected the middle L frames as the representative sequence in each video. In our experiment, we set L to two values: 25 and 64. We selected 25 because the number of frames in our synthetic videos was 25. The choice of 64 was reliant on two factors. First, 64 was long enough to include a majority of the input video while small enough to keep computational cost and time consumption adequate. Second, the I3D network used in some of our experiments was designed for 64 frame inputs. In the following subsections, we elaborate on the latter two training strategies: (i) fine-tuning the synthetic trained network on real data and combined synthetic. (ii) Combined synthetic and real data training."
        },
        {
            "heading": "A. Fine-tuning the synthetic trained network on real data",
            "text": "In this experiment, the model was first trained on the synthetic dataset alone. The simulated human models were randomly divided into two sets of 19 and 5 models. All of the generated videos using simulated human models in the larger set were used for training, and those in the smaller set were used for validation. The respective numbers for the\nvideos in the training and validation data were 3591 and 945. The training was done over 20 epochs with the learning rate of 10\u22124 and the categorical cross-entropy loss function. The batch size was set to 8. After the training on synthetic data, we fine-tuned the model on the four selected training folds of the real dataset, over 50 epochs. The model is tested on the remaining single test fold. This operation is repeated 5 times each time a new fold is selected as the test fold. Learning rates and parameters were chosen empirically.\nThe results averaged over all 5 runs for these experiments are shown in Table I. FZ specifies the instances where FaceNet weights were frozen to treat FaceNet purely as a feature extraction network, with weights of the rest of the network updated during training. Table I also includes experiments in which the synthetic data training step was skipped to highlight its effect. Additionally, we compared our methods with the baseline FN+KNN classifier applied only to the real data. Our results show that the models trained on synthetic data outperformed their counterparts only trained on the real data. The unfrozen FN+IN25 model achieved an\n89% accuracy on the real data when trained on synthetic data. In the frozen weights instances, the inception models perform similarly to the KNN models when trained only on the real data. However, the addition of synthetic data training improved the FZ model accuracy up to 83% in the case of FN(FZ)+INC64. This addition also significantly impacted the I3D model, and its accuracy of 83% outperforms all models not influenced by the synthetic data. Interestingly, this model even outperforms the FZ models trained on the synthetic data. This is quite impressive because I3D was designed for video action recognition tasks. Unlike the other models, the I3D had no prior information about the facial features.\nIn Table II we explored the effect of unfreezing the FaceNet weights and the addition of synthetic data on the non-Caucasian data fold. We created one test fold which included 25 videos of the underrepresented ethnicities. The FN(FZ)+IN models trained without synthetic data are highlighted as the base models in this table. This table shows that the addition of synthetic data combined with unfreezing of the pre-trained weights has the highest impact on the correct classification of the non-Caucasian data samples (24% increase). The addition of synthetic data alone has a limited beneficial impact; it can not alter the dataset bias effect of the original dataset on which the FaceNet was trained."
        },
        {
            "heading": "B. Combining synthetic and real data for training",
            "text": "We designed another experiment to investigate how the accuracy changes with the addition of synthetic data. A portion of the synthetic data was randomly selected and combined with the real training data to create a new data set. The model was trained on this new training data and tested only on the real test data sample. We applied this training strategy to our most satisfactory model, input length 25 FaceNet + InceptionTime. In our first experiment, we set the ratio of selected synthetic data portion to 0.25. This ratio was doubled in the next experiment and doubled again in the last one. In each epoch bratio\u00d7 24c human-like models were randomly selected. For every selected human-like model, out of the nine videos of that human-like model expressing a specific expression from multiple angles, only one was chosen randomly. This selection method means that only bratio\u00d7 24c \u00d7 21 synthetic videos are used in the models training alongside the real data in that epoch. This number equals 126 for the ratio of 0.25, roughly equal to the number of real training videos. The selected human-like models and viewing angles were refreshed at the start of each epoch.\nThe results for these experiments are shown in Table III. These results show that doubling the synthetic data ratio from 0.5 to 1 increases the model\u2019s performance. However, this does not apply to the change from 0.25 to 0.5. In the case of FaceNet+Inception64, the synthetic ratio of 0.25 results in the highest performing network. This model achieved a 94% accuracy, which shows an 18% increase over the performance of the same model trained without the synthetic data. The combined confusion matrix of all the folds for this highest performing network is shown in Fig. 8."
        },
        {
            "heading": "C. Evaluating the Sim2Real approach on GIFGIF dataset",
            "text": "To evaluate the generalization of our Sim2Real approach and model, we selected an external dataset for validation, GIFGIF [28].3 As previously noted, there are currently no video datasets with confusion samples [38]. GIFGIF [28] has video-level annotations and contains 2 of our emotions of interest (\u201canger\u201d and \u201cdisgust\u201d).\nThis dataset is a collection of 3,858 cropped short videos with annotation scores for 17 emotions. We used GIFGIF API to get the first 400 highest-ranking videos for the \u201cdisgust\u201d emotion. These videos were filtered down to 75 based on the following criteria: 1) contains a human face reaction video, 2) must not hold a higher score in other categories. Similarly, we chose the top 75 \u201canger\u201d videos. The Arousal-Valence distribution of all these 150 samples is displayed in Fig. 9. The Arousal-Valence values are extracted using Emonet [35]. Fig. 9 suggests that this evaluation dataset is severely challenging.\nFor this evaluation, no additional training was performed.\n3We also considered AffWild, EmoReact, ElderReact, but their data did not contain anger or confusion, or their annotation schemes were not directly comparable with our data (e.g., frame-based). CK+ [21], Oulu-Casia [42], and MMI [26] were also not selected since they are all acted/posed and we focus on in-the-wild interactions.\nWe tested the two FN+IN25 models presented in Table I on this dataset. One model was trained on our real dataset, another model was pretrained on the synthetic dataset then trained on the real dataset. The GIFGIF dataset was used as a test dataset for these models. The results for this experiment are shown in Table IV. The FN+IN25 model pretrained on synthetic data achieved a 75% accuracy. Out of 150 videos, this model misclassified 7 disgust videos and 2 anger as confusion. The same model without synthetic pretraining achieved a 64% accuracy and misclassified 14 disgust and 4 anger videos as confusion. Therefore, without any additional transfer learning, we showed that our Sim2Real approach improved FN+INC25 performance on this dynamic FER task by 11%."
        },
        {
            "heading": "IV. DISCUSSION AND LIMITATIONS",
            "text": "In this section, we elaborate on insights that we found while doing experiments and after analyzing the results. Our experiments showed that additional synthetic data is similar to have an extensive dataset, and the generalization of the final model is increased.\nAn interesting finding in our experiments was that all the models with frozen FaceNet weights performed worse than their counterparts with unfrozen weights or even I3D. This was especially the case when considering non-Caucasian samples, which was an unexpected result because FaceNet was trained on a vast face recognition dataset. This shows\nthat although the FaceNet feature embedding performs well on facial recognition tasks, it may not be entirely related to the facial changes of a specific emotional expression. However, more research is needed to investigate these hypotheses.\nAnother interesting point was the misclassification of specific samples that were revealed after we looked deeper into the results. These samples were classified wrongly even in our best model with an average accuracy of 94%, Fig. 10 show three examples of the eight wrongly classified videos from all folds. From left to right, each column corresponds to the first, middle, and last frame. The incorrect classification of the first video might be related to the minimal movement of the face. The generated synthetic dataset that we used lacks fully static samples. Additionally, the main concept behind the proposed model was the focus on dynamic movements. The incorrect prediction of the second video relates to its head movement. The dynamic movement of the expression is done over frames involved with head movement. This adds fluctuation to the inception model\u2019s multivariant timeseries input that may not relate to the emotion. The OpenFace algorithm [2] uses perspective transform to make all of the input images have a frontal face view. The addition of this step may help deal with these types of videos. However, we believe the ultimate solution is in designing a model that can predict from shorter video snippets inputs. Poor prediction of the third video relates to the movement of hand midway through the video. Face occlusion is a challenge in FER, and even though new studies focus on reducing or removing the occlusions [25], [24], their advancement has been minimal.\nAn interesting notion observed in the annotation of the real dataset was that annotators had trouble distinguishing between disgust and confusion in some cases. However, when the audio was played alongside the video, this confusion was resolved. This could mean that the next step for more inclusive and accurate facial expression recognition systems could incorporate audiovisual data processing.\nOne of the main components of this work was the generation of synthetic data. The MakeHuman application limitation highly affects this component. More advanced applications can be used for this task to generate more\nrealistic synthetic datasets, and to explore other variations including age, non-binary gender, or conditions impacting facial development. Another point worth mentioning is that while we understand that the relatively small size of our dataset (synthetically generated dataset plus the real human dataset gathered from YouTube and Giphy) might be a limiting factor, this is sufficient to illustrate the proposed approach as a proof-of-concept."
        },
        {
            "heading": "V. CONCLUSIONS AND FUTURE WORK",
            "text": "We showed that our Sim2Real approach improves FN+INC64 performance on our dynamic FER task by 11- 18%, up to 94% on our internal test dataset, and up to 75% on a previously unseen dataset, compared to the performance of the same model architecture without synthetic training data. This performance was achieved by mixed synthetic and real data training. Additionally, it was shown that the proposed FN+INC model along with our Sim2Real approach is less sensitive to dataset ethnicity bias. This study was a first step towards emotion recognition in the wild, and future work can explore applying our approach and test the trained classifiers to data gathered from real-world HRI scenarios [30]. Another notion that can be explored in the future is to observe the effects of replacing the face feature extractor model with a static FER feature extractor. An increased number of simulated human models may improve the overall accuracy, especially if they can be made with photo-realistic 3D model generating engines such as MetaHuman creator from Unreal Engine4. Automated animation generation from real videos can also be the next step for this study."
        }
    ],
    "title": "Towards Inclusive HRI: Using Sim2Real to Address Underrepresentation in Emotion Expression Recognition",
    "year": 2022
}