{
    "abstractText": "Scene Graph Generation is a critical enabler of environmental comprehension for autonomous robotic systems. Most of existing methods, however, are often thwarted by the intricate dynamics of background complexity, which limits their ability to fully decode the inherent topological information of the environment. Additionally, the wealth of contextual information encapsulated within depth cues is often left untapped, rendering existing approaches less effective. To address these shortcomings, we present STDG, an avant-garde DepthGuided One-Stage Scene Graph Generation methodology. The innovative architecture of STDG is a triad of custom-built modules: The Depth Guided HHA Representation Generation Module, the Depth Guided Semi-Teaching Network Learning Module, and the Depth Guided Scene Graph Generation Module. This trifecta of modules synergistically harnesses depth information, covering all aspects from depth signal generation and depth feature utilization, to the final scene graph prediction. Importantly, this is achieved without imposing additional computational burden during the inference phase. Experimental results confirm that our method significantly enhances the performance of one-stage scene graph generation baselines.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xukun Zhou"
        },
        {
            "affiliations": [],
            "name": "Zhenbo Song"
        },
        {
            "affiliations": [],
            "name": "Jun He"
        },
        {
            "affiliations": [],
            "name": "Hongyan Liu"
        },
        {
            "affiliations": [],
            "name": "Zhaoxin Fan"
        }
    ],
    "id": "SP:da7f033d1195498a7841515db6fabbed08c41b98",
    "references": [
        {
            "authors": [
                "J. Johnson",
                "R. Krishna",
                "M. Stark",
                "L.-J. Li",
                "D. Shamma",
                "M. Bernstein",
                "L. Fei-Fei"
            ],
            "title": "Image retrieval using scene graphs",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 3668\u20133678.",
            "year": 2015
        },
        {
            "authors": [
                "F. Amodeo",
                "F. Caballero",
                "N. D\u0131\u0301az-Rodr\u0131\u0301guez",
                "L. Merino"
            ],
            "title": "Ogsgg: ontology-guided scene graph generation\u2014a case study in transfer learning for telepresence robotics",
            "venue": "IEEE Access, vol. 10, pp. 132 564\u2013 132 583, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Q. Wu",
                "D. Teney",
                "P. Wang",
                "C. Shen",
                "A. Dick",
                "A. Van Den Hengel"
            ],
            "title": "Visual question answering: A survey of methods and datasets",
            "venue": "Computer Vision and Image Understanding, vol. 163, pp. 21\u201340, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "T. Qian",
                "J. Chen",
                "S. Chen",
                "B. Wu",
                "Y.-G. Jiang"
            ],
            "title": "Scene graph refinement network for visual question answering",
            "venue": "IEEE Transactions on Multimedia, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "M. Hildebrandt",
                "H. Li",
                "R. Koner",
                "V. Tresp",
                "S. G\u00fcnnemann"
            ],
            "title": "Scene graph reasoning for visual question answering",
            "venue": "arXiv preprint arXiv:2007.01072, 2020.",
            "year": 2007
        },
        {
            "authors": [
                "S.-Y. Yu",
                "A.V. Malawade",
                "D. Muthirayan",
                "P.P. Khargonekar",
                "M.A. Al Faruque"
            ],
            "title": "Scene-graph augmented data-driven risk assessment of autonomous vehicle decisions",
            "venue": "IEEE Transactions on Intelligent Transportation Systems, vol. 23, no. 7, pp. 7941\u20137951, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A.V. Malawade",
                "S.-Y. Yu",
                "B. Hsu",
                "D. Muthirayan",
                "P.P. Khargonekar",
                "M.A. Al Faruque"
            ],
            "title": "Spatiotemporal scene-graph embedding for autonomous vehicle collision prediction",
            "venue": "IEEE Internet of Things Journal, vol. 9, no. 12, pp. 9379\u20139388, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "S. Amiri",
                "K. Chandan",
                "S. Zhang"
            ],
            "title": "Reasoning with scene graphs for robot planning under partial observability",
            "venue": "IEEE Robotics and Automation Letters, vol. 7, no. 2, pp. 5560\u20135567, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "G. Zhu",
                "L. Zhang",
                "Y. Jiang",
                "Y. Dang",
                "H. Hou",
                "P. Shen",
                "M. Feng",
                "X. Zhao",
                "Q. Miao",
                "S.A.A. Shah"
            ],
            "title": "Scene graph generation: A comprehensive survey",
            "venue": "arXiv preprint arXiv:2201.00443, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J. Fan",
                "P. Zheng",
                "S. Li"
            ],
            "title": "Vision-based holistic scene understanding towards proactive human\u2013robot collaboration",
            "venue": "Robotics and Computer-Integrated Manufacturing, vol. 75, p. 102304, 2022. [Online]. Available: https://www.sciencedirect.com/science/article/pii/ S0736584521001848",
            "year": 2022
        },
        {
            "authors": [
                "J. Yang",
                "Y.Z. Ang",
                "Z. Guo",
                "K. Zhou",
                "W. Zhang",
                "Z. Liu"
            ],
            "title": "Panoptic scene graph generation",
            "venue": "European Conference on Computer Vision. Springer, 2022, pp. 178\u2013196.",
            "year": 2022
        },
        {
            "authors": [
                "W. Li",
                "H. Zhang",
                "Q. Bai",
                "G. Zhao",
                "N. Jiang",
                "X. Yuan"
            ],
            "title": "Ppdl: Predicate probability distribution based loss for unbiased scene graph generation",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2022, pp. 19 447\u201319 456.",
            "year": 2022
        },
        {
            "authors": [
                "X. Lin",
                "C. Ding",
                "Y. Zhan",
                "Z. Li",
                "D. Tao"
            ],
            "title": "Hl-net: Heterophily learning network for scene graph generation",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2022, pp. 19 476\u201319 485.",
            "year": 2022
        },
        {
            "authors": [
                "X. Dong",
                "T. Gan",
                "X. Song",
                "J. Wu",
                "Y. Cheng",
                "L. Nie"
            ],
            "title": "Stacked hybrid-attention and group collaborative learning for unbiased scene graph generation",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 19 427\u201319 436.",
            "year": 2022
        },
        {
            "authors": [
                "L. Li",
                "L. Chen",
                "Y. Huang",
                "Z. Zhang",
                "S. Zhang",
                "J. Xiao"
            ],
            "title": "The devil is in the labels: Noisy label correction for robust scene graph generation",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 18 869\u201318 878.",
            "year": 2022
        },
        {
            "authors": [
                "A. Newell",
                "J. Deng"
            ],
            "title": "Pixels to graphs by associative embedding",
            "venue": "Advances in neural information processing systems, vol. 30, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "H. Liu",
                "N. Yan",
                "M. Mortazavi",
                "B. Bhanu"
            ],
            "title": "Fully convolutional scene graph generation",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 11 546\u201311 556.",
            "year": 2021
        },
        {
            "authors": [
                "G. Adaimi",
                "D. Mizrahi",
                "A. Alahi"
            ],
            "title": "Composite relationship fields with transformers for scene graph generation",
            "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2023, pp. 52\u201364.",
            "year": 2023
        },
        {
            "authors": [
                "A. Dosovitskiy",
                "L. Beyer",
                "A. Kolesnikov",
                "D. Weissenborn",
                "X. Zhai",
                "T. Unterthiner",
                "M. Dehghani",
                "M. Minderer",
                "G. Heigold",
                "S. Gelly",
                "J. Uszkoreit",
                "N. Houlsby"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "2021.",
            "year": 2021
        },
        {
            "authors": [
                "T.-Y. Lin",
                "P. Dollar",
                "R. Girshick",
                "K. He",
                "B. Hariharan",
                "S. Belongie"
            ],
            "title": "Feature pyramid networks for object detection",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.",
            "year": 2017
        },
        {
            "authors": [
                "N. Carion",
                "F. Massa",
                "G. Synnaeve",
                "N. Usunier",
                "A. Kirillov",
                "S. Zagoruyko"
            ],
            "title": "End-to-end object detection with transformers",
            "venue": "European conference on computer vision. Springer, 2020, pp. 213\u2013 229.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Li",
                "W. Ouyang",
                "B. Zhou",
                "K. Wang",
                "X. Wang"
            ],
            "title": "Scene graph generation from objects, phrases and region captions",
            "venue": "Proceedings of the IEEE international conference on computer vision, 2017, pp. 1261\u20131270.",
            "year": 2017
        },
        {
            "authors": [
                "J. Yang",
                "J. Lu",
                "S. Lee",
                "D. Batra",
                "D. Parikh"
            ],
            "title": "Graph r-cnn for scene graph generation",
            "venue": "Proceedings of the European conference on computer vision (ECCV), 2018, pp. 670\u2013685.",
            "year": 2018
        },
        {
            "authors": [
                "K. Tang",
                "Y. Niu",
                "J. Huang",
                "J. Shi",
                "H. Zhang"
            ],
            "title": "Unbiased scene graph generation from biased training",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 3716\u20133725.",
            "year": 2020
        },
        {
            "authors": [
                "J. Gu",
                "H. Zhao",
                "Z. Lin",
                "S. Li",
                "J. Cai",
                "M. Ling"
            ],
            "title": "Scene graph generation with external knowledge and image reconstruction",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Cong",
                "W. Liao",
                "H. Ackermann",
                "B. Rosenhahn",
                "M.Y. Yang"
            ],
            "title": "Spatial-temporal transformer for dynamic scene graph generation",
            "venue": "Proceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 16 372\u201316 382.",
            "year": 2021
        },
        {
            "authors": [
                "J. Ji",
                "R. Krishna",
                "L. Fei-Fei",
                "J.C. Niebles"
            ],
            "title": "Action genome: Actions as compositions of spatio-temporal scene graphs",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 10 236\u201310 247.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Teng",
                "L. Wang",
                "Z. Li",
                "G. Wu"
            ],
            "title": "Target adaptive context aggregation for video scene graph generation",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 13 688\u201313 697.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Lu",
                "C. Chang",
                "H. Rai",
                "G. Yu",
                "M. Volkovs"
            ],
            "title": "Multi-view scene graph generation in videos",
            "venue": "International Challenge on Activity Recognition (ActivityNet) CVPR 2021 Workshop, vol. 3, 2021, p. 2.",
            "year": 2021
        },
        {
            "authors": [
                "T. Chen",
                "W. Yu",
                "R. Chen",
                "L. Lin"
            ],
            "title": "Knowledge-embedded routing network for scene graph generation",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 6163\u20136171.",
            "year": 2019
        },
        {
            "authors": [
                "M.-J. Chiou",
                "H. Ding",
                "H. Yan",
                "C. Wang",
                "R. Zimmermann",
                "J. Feng"
            ],
            "title": "Recovering the unbiased scene graphs from the biased ones",
            "venue": "Proceedings of the 29th ACM International Conference on Multimedia, 2021, pp. 1581\u20131590.",
            "year": 2021
        },
        {
            "authors": [
                "A. Badki",
                "A. Troccoli",
                "K. Kim",
                "J. Kautz",
                "P. Sen",
                "O. Gallo"
            ],
            "title": "Bi3d: Stereo depth estimation via binary classifications",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 1600\u20131608.",
            "year": 2020
        },
        {
            "authors": [
                "C. Zhao",
                "Q. Sun",
                "C. Zhang",
                "Y. Tang",
                "F. Qian"
            ],
            "title": "Monocular depth estimation based on deep learning: An overview",
            "venue": "Science China Technological Sciences, vol. 63, no. 9, pp. 1612\u20131627, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "J. Gautier",
                "O. Le Meur",
                "C. Guillemot"
            ],
            "title": "Depth-based image completion for view synthesis",
            "venue": "2011 3DTV Conference: The True Vision-Capture, Transmission and Display of 3D Video (3DTV-CON). IEEE, 2011, pp. 1\u20134.",
            "year": 2011
        },
        {
            "authors": [
                "M. Ding",
                "Y. Huo",
                "H. Yi",
                "Z. Wang",
                "J. Shi",
                "Z. Lu",
                "P. Luo"
            ],
            "title": "Learning depth-guided convolutions for monocular 3d object detection",
            "venue": "Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition workshops, 2020, pp. 1000\u20131001.",
            "year": 2020
        },
        {
            "authors": [
                "K. Lasinger",
                "R. Ranftl",
                "K. Schindler",
                "V. Koltun"
            ],
            "title": "Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer",
            "venue": "CoRR, vol. abs/1907.01341, 2019. [Online]. Available: http://arxiv.org/abs/1907.01341",
            "year": 1907
        },
        {
            "authors": [
                "S. Gupta",
                "R. Girshick",
                "P. Arbel\u00e1ez",
                "J. Malik"
            ],
            "title": "Learning rich features from rgb-d images for object detection and segmentation",
            "venue": "Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VII 13. Springer, 2014, pp. 345\u2013360.",
            "year": 2014
        },
        {
            "authors": [
                "J. Li",
                "C. Xu",
                "Z. Chen",
                "S. Bian",
                "L. Yang",
                "C. Lu"
            ],
            "title": "Hybrik: A hybrid analytical-neural inverse kinematics solution for 3d human pose and shape estimation",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021, pp. 3383\u20133393.",
            "year": 2021
        },
        {
            "authors": [
                "H. Zhang",
                "Y. Tian",
                "X. Zhou",
                "W. Ouyang",
                "Y. Liu",
                "L. Wang",
                "Z. Sun"
            ],
            "title": "Pymaf: 3d human pose and shape regression with pyramidal mesh alignment feedback loop",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 11 446\u2013 11 456.",
            "year": 2021
        },
        {
            "authors": [
                "K. Duan",
                "S. Bai",
                "L. Xie",
                "H. Qi",
                "Q. Huang",
                "Q. Tian"
            ],
            "title": "Centernet: Keypoint triplets for object detection",
            "venue": "Proceedings of the IEEE/CVF international conference on computer vision, 2019, pp. 6569\u20136578.",
            "year": 2019
        },
        {
            "authors": [
                "K. Simonyan",
                "A. Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "arXiv preprint arXiv:1409.1556, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "Y. Lu",
                "H. Rai",
                "J. Chang",
                "B. Knyazev",
                "G. Yu",
                "S. Shekhar",
                "G.W. Taylor",
                "M. Volkovs"
            ],
            "title": "Context-aware scene graph generation with seq2seq transformers",
            "venue": "Proceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 15 931\u201315 941.",
            "year": 2021
        },
        {
            "authors": [
                "R. Li",
                "S. Zhang",
                "B. Wan",
                "X. He"
            ],
            "title": "Bipartite graph network with adaptive message passing for unbiased scene graph generation",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 11 109\u201311 119.",
            "year": 2021
        },
        {
            "authors": [
                "S.-T. Kim",
                "H.J. Lee"
            ],
            "title": "Lightweight stacked hourglass network for human pose estimation",
            "venue": "Applied Sciences, vol. 10, no. 18, p. 6497, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "J. Wang",
                "K. Sun",
                "T. Cheng",
                "B. Jiang",
                "C. Deng",
                "Y. Zhao",
                "D. Liu",
                "Y. Mu",
                "M. Tan",
                "X. Wang",
                "W. Liu",
                "B. Xiao"
            ],
            "title": "Deep high-resolution representation learning for visual recognition",
            "venue": "2020.",
            "year": 2020
        },
        {
            "authors": [
                "Z. Liu",
                "Y. Lin",
                "Y. Cao",
                "H. Hu",
                "Y. Wei",
                "Z. Zhang",
                "S. Lin",
                "B. Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "Proceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 10 012\u201310 022.",
            "year": 2021
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770\u2013778.",
            "year": 2016
        }
    ],
    "sections": [
        {
            "text": "I. INTRODUCTION\nScene graph generation, a task first introduced by Johnson et al. [1], transforms an input image into a structured scene graph. This graph encapsulates the objects, attributes, and relationships within the scene, offering a detailed representation of the environment. The utility of scene graph generation spans across multiple domains. It bolsters scene understanding [2], aids in visual question answering [3], [4], [5], and supports advancements in autonomous driving [6], [7]. The application of scene graphs is even particularly significant in the domain of robotics. In particular, in robotic navigation, the scene graphs enhance decision-making [8], facilitate the execution of complex tasks [9], and enable interaction with environmental objects [10]. Hence, scene graph generation is a crucial tool in the enhancement of robotic capabilities.\nExisting scene graph generation methodologies predominantly fall into two categories: two-stage approaches and one-stage approaches. The two-stage approach first employs an object detection network to identify and locate objects within an image [11], [12], [13]. Once objects are identified, the information is used as ground truth input for a separate relationship prediction network [14], [15]. This network predicts the relationships between the various objects. Despite\n1Renmin University of China {xukun zhou,hejun, fanzhaoxin}@ruc.edu.cn\n2Nanjing University of Science and Technology, China songzb@njust.edu.cn\n3Tsinghua University, China hyliu@tsinghua.edu.cn 4Psyche AI Inc, China fanzhaoxin@psyai.net\nits effectiveness, this approach requires training two separate networks, leading to significant time and computational costs. In contrast, the one-stage approach consolidates the object detection and relationship prediction tasks into a single network. Newell et al. [16] first propose this concept, which is later expanded by Liu et al. [17] into the Fully Convolutional Scene Graph Generation (FCSGG) method. This method models relationships as integral distribution maps on the image. Adaimi et al. [18] further advance this approach by optimizing the matching relationships between different objects, thereby enabling the generation of more accurate scene graphs while maintaining computational efficiency.\nDespite advancements in one-stage scene graph generation, these methods continue to face substantial challenges. Two key difficulties lie in the ability to filter out complex backgrounds from RGB images and the need for a deeper understanding of scene topology. The rich color information in RGB images often complicates accurate modeling of object relationships, particularly for simpler models. One intuitive solution that emerges to address these challenges is the use of depth information. Depth data, unaffected by the influence of color, provides a potential pathway to more accurate scene graph generation. Not only does depth data provide an alternative to the color complexities of RGB images, but it also delivers rich 3D information. This 3D information could significantly enhance the network\u2019s understanding of the scene\u2019s topology, providing a more direct approach to modeling objects, attributes, and relationships. Moreover, depth information could potentially allow the network to bypass complications such as occlusion, which are common when predicting relationships within 2D images. By considering the depth of different objects, the network could gain a better understanding of which objects are in front of others, improving the accuracy of its predictions. This insight into the potential use of depth information prompts the question: Can depth information be leveraged to guide one-stage scene graph generation?\nMotivated by this, we propose a novel Depth-Guided OneStage Scene Graph Generation method, termed as STDG. The innovation of STDG lies in the use of predicted depth as a robust guiding signal to enhance scene graph generation performance, with negligible additional computational cost. In STDG, we first introduce a Depth Guided HHA Representation Generation Module. In this module, we use an off-the-shelf depth predictor to estimate depth information from monocular images, which is then transformed into a novel HHA structure. This HHA structure is capable of better reflecting the topology of the scene and facilitates easier\nar X\niv :2\n30 9.\n08 17\n9v 1\n[ cs\n.C V\n] 1\n5 Se\np 20\n23\nfeature learning by the network. Subsequently, we train a Depth Guided Semi-Teaching Network Learning Module. Within this module, we first train a teacher network for scene graph generation using the HHA. We then utilize this teacher network to \u201dteach\u201d a student network on how to learn to predict scene graphs from images (as opposed to from HHAs). During this \u201dteaching\u201d process, we only \u201dinherit\u201d key aspects of the teacher that are most beneficial for scene graph generation, making this a semi-teaching network. Finally, we adopt a novel Depth Guided Scene Graph Generation Module as prediction heads to simultaneously predict all information of the scene graph. Upon completion of the training phase, only the student network is required for inference, eliminating the need for further depth prediction. This results in a fast and lightweight inference solution that enhances operational efficiency.\nTo demonstrate the effectiveness of STDG, we conduct experiments on the well-known VG-100k dataset [19]. Experimental results demonstrate that our method significantly improves the performance of one-stage scene graph generation."
        },
        {
            "heading": "II. RELATED WORK",
            "text": ""
        },
        {
            "heading": "A. Two-stage Scene Graph Generation",
            "text": "Scene graph generation traditionally adopts a two-stage approach. First, an object detection model, such as the Feature Pyramid Networks (FPN) [20] or DEtection TRansformer (DETR) [21], is trained to identify the positions and categories of objects in an image. After detecting the objects, a complete graph is assumed to exist, representing their relationships. This graph is then processed by a relation classification network to determine the type of each relationship [22], [23], [24], [25].Efforts to refine this approach can be categorized into two main areas. First, several works have proposed methods for enhancing the extraction of visual information. For instance, the use of global information to strengthen object representation has been suggested [26], [27], [28], [29]. Furthermore, there has been advocacy for improving the convolutional network\u2019s focus on region sizes to propagate information between different objects [30], [27], [26]. The second area of focus has been the mitigation of the long-tail effect during the prediction process. This involves addressing overfitting to the dataset distribution during training. Tang et al. [24] suggested enhancing the relationship representation by adjusting the average relationship probability distribution, while Chiou et al. [31] proposed dynamically adjusting the loss for different relationships during training to enhance the learning of rare samples. Although traditional two-stage approaches to scene graph generation have shown promising results, they typically suffer from slower processing speeds and the potential accumulation of errors due to the necessity of two separate network stages. To circumvent these shortcomings, this paper explores the more efficient one-stage method."
        },
        {
            "heading": "B. One-stage Scene Graph Generation",
            "text": "As previously discussed, two-stage methods for scene graph generation are burdened by slower processing speeds and the potential for error accumulation. To address these issues, one-stage methods have been introduced. One-stage methods primarily focus on relationship modeling without accounting for an object\u2019s position. Pioneering work by Newell et al. [16] employed dual heatmaps to independently predict the positions of objects and relationships. This was followed by feature extraction corresponding to the identified relationship location, and computation of their interactions. FCSGG [17] expanded on this concept, initially considering human keypoints matching for scene graph object handling. Subsequent methodologies, such as CoRF[18], further developed this idea, utilizing a random field as the relationship classifier. Despite these advancements, one-stage methods remain in their infancy and are still under active exploration. Furthermore, we observe that current one-stage methods are heavily influenced by complex color backgrounds, failing to adequately perceive scene topology and 3D layout. To address these challenges, this paper proposes a novel depthguided one-stage scene graph generation method."
        },
        {
            "heading": "C. Depth Estimator and Utlization",
            "text": "Predominantly, there are two methods to estimate depth information in the real world: stereo depth estimation and monocular depth estimation. Stereo depth estimation, such as the method proposed by Badki et al.[32], utilizes a dual-camera setup to capture a single scene, computing depth by comparing the disparity between the two captured images. On the other hand, monocular depth estimation, like the approach introduced by Zhao et al.[33], leverages a single camera and machine learning techniques to infer depth information from visual cues in a single image. Depth information has found extensive application across various domains, significantly enhancing performance. For instance, Gautier et al.[34] used depth for improved object detection, while Ding et al.[35] utilized depth information for better scene understanding in autonomous driving. Inspired by these developments, we propose to integrate depth information into scene graph generation to filter out unnecessary background information in a depth guided semi-teaching way. Given that scene graph generation tasks provide only a single image for each scene, we chose to employ a monocular depth estimator to generate pseudo labels, following the methodology of monocular depth estimation studies. To the best of our knowledge, we are the first to propose a depthguided scene graph generation method."
        },
        {
            "heading": "III. METHOD",
            "text": ""
        },
        {
            "heading": "A. Overview",
            "text": "The task of one-stage scene graph generation is defined as follows. Given an image I, a network G is used to generate detections for n objects and relationships between objects within m relationship categories. Mathematically, we express this as:\nG(I) = (Detn,Relm) (1)\nwhere Detn represents the detection results of n objects and Relm denotes the relationships between objects, categorized into m kinds of relationships.\nFig.1 illustrates the pipeline of our proposed method, termed STDG. At the heart of our approach is a depth-guided method. Firstly, we introduce a Depth Guided HHA Representation Generation Module. In this module, an off-theshelf depth predictor is used to estimate depth information from monocular images, which is transformed into a novel HHA structure. This HHA structure better reflects the scene\u2019s topology and facilitates easier feature learning by the network. Next, we train a Depth Guided Semi-Teaching Network Learning Module. Within this module, we initially train a teacher network for scene graph generation using the HHA. This teacher network is then utilized to instruct a student network on how to learn to predict scene graphs from images, as opposed to from HHAs. During this teaching process, we only inherit key aspects of the teacher that are most beneficial for scene graph generation, creating a semi-teaching network. Finally, we adopt a novel Depth Guided Scene Graph Generation Module as prediction heads to simultaneously predict all information of the scene graph. Upon completion of the training phase, only the student network is used for inference, eliminating the need for further depth prediction. This results in a fast and lightweight inference solution that enhances operational efficiency. In the following sections, we provide detailed descriptions of the three modules we propose: the Depth Guided HHA Representation Generation Module, the Depth Guided Semi-\nTeaching Network Learning Module, and the Depth Guided Scene Graph Generation Module."
        },
        {
            "heading": "B. Depth Guided HHA Representation Generation Module",
            "text": "As mentioned earlier, due to the complex information contained within an RGB image, one-stage scene graph generation models may struggle to effectively learn scene topology from it. To tackle this issue, we propose to utlize depth information. In our work, We first adopt MiDas[36] as our monocular depth estimator to predict depth maps, which subsequently serve as our guidance signal. Once the depth map is obtained, the most straightforward idea is to directly extract deep feature from it to enhance the main scene graph generation network. However, the utility of monocularly predicted depth maps is hampered by two main factors. Firstly, these predictions inherently suffer from inaccuracies due to the ambiguity of depth perception from a single viewpoint, which can lead to erroneous estimations of the spatial relations between objects. Secondly, the extraction\nof meaningful features from depth maps is a nontrivial task due to their high-dimensional nature and the complex interdependencies between depth values across the image.\nTo this end, inspired by the methodology outlined in Gupta et al.[37], we propose to use the HHA format (Horizontal disparity, Height above ground, and the Angle the pixel\u2019s local surface normal makes with the inferred gravity direction). However, the generation of HHA images requires camera parameters. It is nearly impossible to obtain accurate camera parameters from existing scene graph generation datasets. To circumvent this issue, we introduce a simplified approach, drawing on common methods found in contemporary human pose estimation models [38], [39]. Specifically, we standardize the use of the image center as the camera origin and set a fixed focal length, despite the inherent limitations of this method. As a result, we can express our depth information, I\u0302, as follows: I\u0302 = (Ihori, Iheight, Iangle).\nFig.2 shows an illustration of the proposed the HHA presentation. The HHA format provides a more intuitive understanding of the scene topology compared to raw depth maps, thereby offering a more effective guidance signal for scene graph generation."
        },
        {
            "heading": "C. Depth Guided Semi-Teaching Network Learning Module",
            "text": "For one-stage scene graph generation, we follow the definition provided by FCSGG [17]. This approach necessitates a single network to simultaneously perform object detection and relation prediction. We adopt the methodology of CenterNet [40] to predict the center point and corresponding offset, o, in both object detection branch and relation prediction branch.\nTo effectively leverage depth information, we propose a novel teacher-student model. This model enables the RGBbased network (student network) to learn depth information from the depth-based network (depth teacher). In order to circumvent the potential impact of missing information on model performance, we use depth information to supervise the intermediate offset, o, without imposing any additional constraints at the image feature level. This strategy is referred to as our semi-teacher module.\nThe depth teacher is developed using a deformation neural network. Here, the offset location serves as the teaching intermediate variable, denoted as oli, j for the offset at location (i, j) of convolution kernel l. The depth teacher is initially trained with HHA (Horizontal disparity, Height above ground, Angle with gravity) representation as input. The HHA encoding effectively leverages depth information, enabling the depth teacher to more robustly understand the topological aspects of the scene. The output from this phase is the offset variable oli, j and can be represented mathematically as:\nGdepth(HHA) = oli, j (2)\nGiven that the HHA representation is also 3-channel like the original RGB image, the depth teacher network can share the exact same network architecture with the student network. This enhances consistency for the subsequent semi-teaching process.\nThe predicted offset is then regarded as the transferred knowledge, used to train the student network, which takes the original image as input, aiming to predict all parameters of a scene graph."
        },
        {
            "heading": "D. Depth Guided Scene Graph Generation Module",
            "text": "Once the depth teacher network is fully trained, we proceed to train the student network. The student network takes the original image as input and aims to output the complete set of parameters for a scene graph. During this training process, the offset label is provided by the depth teacher as a pseudolabel, while the ground truth directly supervises the rest of the parameters.\nObject Detection. In our object detection module, our foundation is the structural framework of CenterNet [40]. Each object is characterized by its location, classification, and size. The object locations and classifications are jointly represented using a single heatmap, denoted as h \u2208 RC\u00d7W\u00d7H , where C signifies the number of object classes. The sizes of objects are represented as s \u2208 R2\u00d7W\u00d7H . To effectively incorporate depth information into the object detection process, we introduce two deformable convolutional layers positioned between the visual backbone and the prediction layer. The object detection loss, denoted as Lossdet , is formulated as the sum of the squared differences between the ground truth and the predicted values of the object heatmap (h) and object size (s):\nLossdet = (hgt \u2212hpred)2 +(sgt \u2212 spred)2 (3)\nRelation Classification. For the scene graph generation, we use the CoRF method as outlined in Adaimi et al.[18]. This method allows us to predict multiple relations at a single location simultaneously. We represent a relation, denoted as r, in the scene graph as rpi, j = [\u03b1,xob j,yob j,xsub j,ysub j,ssub j,sob j], where \u03b1 signifies the confidence level of relation p at location (i, j) for sub j and ob j. The x and y correspond to the coordinates of the subject and object, while ssub j and sob j denote one-ninth of the minimal width or height for the subject and object, respectively. The relation can be represented as r \u2208 R7\u00d7W\u00d7H , where W and H are the width and height of the scene. The loss function for relation prediction, denoted as Lossrel , is formulated as follows:\nLossrel = (rgt \u2212 rpred)2 (4)\nSemi-Supervised Learning. The semi-supervision loss, denoted as Losssemi, is calculated by comparing the predicted offsets of the student network and the depth teacher:\nLosssemi = W\n\u2211 w=0\nH\n\u2211 h=0 (ow,hi, j \u2212 o\u0302 w,h i, j ) 2 (5)\nHere, o and o\u0302 represent the offsets generated by the student model and the depth teacher, respectively. Note, in both detection branch and relation prediction branch, there exist offset\nOverall Loss. Finally, the overall loss for training the student network with an RGB image is calculated as follows:\nLoss = Lossdet +Lossrel +Losssemi \u2217mode (6)\nHere, the mode is set to 0 when training the depth model and 1 otherwise. This ensures that the semi-supervision loss is only incorporated when training the student model, reinforcing the learned depth information."
        },
        {
            "heading": "IV. EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "A. Experiments Settings",
            "text": "Our experiments use the VG-100k dataset, a subset of VG150K. This dataset comprises 108,000 images, 150 object categories, and 50 predicate categories. To validate our model, we utilize two backbone architectures: Resnet50 and Swin-Transformer. The representation of relations is based on the composite relation field method used for one-stage relation classification. During the training process, we set the backbone\u2019s learning rate to 5e-5 and the prediction part of the network to 5e-4. We apply gradient clipping at a rate of 5e-5. Both the deep network Gdepth and the network G are trained for 60 epochs, with the last epoch results serving as our evaluation metrics. Evaluation is achieved through three aspects: Scene Graph Relationship Prediction, Scene Graph Predication Classification (PredCls), and Scene Graph Classification (SGCls). We utilize Recall@K as our primary metric, considering the VG-100k dataset\u2019s partially annotated nature. Additionally, we report non-graph constraints (ngRecall@K) results and mean-Recall results to accommodate\nmultiple possible relationships and the dataset\u2019s long-tail data."
        },
        {
            "heading": "B. Performance Analysis & Comparison",
            "text": "Our model\u2019s results are presented in three categories: recall performance, mean recall performance, and zero-shot recall performance, showcasing the superiority of our proposed semi-teaching depth-guided method.\nThe recall performance (Table.I) illustrates our model\u2019s high inference speed, comparable to CoRF, and an approximate 1-unit increase in recall in our fastest mode. Our approach surpasses CoRF in terms of processing speed by 1.6 images per second and enhances SGDet\u2019s Recall50 by 1.2. Despite a slight performance dip in PreCls compared to the Transformer-enhanced CoRF model, substantial improvements are observed in SGCls and SGDet, alongside a computation speed increase of 3.6 images per second.\nIn mean recall performance (Table.II), our model outperforms CoRF with the same backbone, presenting a maximum recall improvement of 1.4 in PredCLS. Despite a 1.1 decrease in ng-recall in PredCLS, other aspects did not drop by more than 0.3, and SGDet\u2019s recall even improved by 0.1 compared to CoRF+T. For zero-shot recall performance, our model outperforms CoRF with ResNet50 as the backbone, improving ng recall50 in PredCLS by 1.0. Experiments with\nSwin Transformer enhanced recall by 0.3/0.6 in SGCLS, with a minor recall decrease of 0.2/0.1 in SGDet. Our model complexity analysis (Table.IV) reveals that our SDTG model has significantly fewer parameters compared to CoRF, making it more appealing for applications with lower GPU and CPU performance demands.\nOverall, our proposed semi-teaching depth-guided method demonstrates superior performance in multiple aspects, thereby affirming its effectiveness in improving recall, mean recall, and zero-shot recall performances, and reducing model complexity."
        },
        {
            "heading": "C. Ablation Study",
            "text": "In this section, we investigate the effectiveness of each proposed module. We choose ResNet50 as the backbone. Results are shown as Table.V.\nEffectiveness of Depth Guided Semi-Teaching Network Learning Module. To investigate the effectiveness of depth guidance, we conduct an ablation study in which both modules were trained simultaneously while allowing the depth module to supervise the RGB scene graph generation model. The outcomes of this experiment are presented in the first row of Table.V. The results indicate a substantial performance decline when the depth model is co-trained with the RGB model, thereby underscoring the crucial role of pretraining the depth extraction model in enhancing the overall effectiveness of the scene graph generation process. We also investigate the difference between fully-teach and semi-teach, it can be seen from Table.V that our semi-teach scheme achieves much better performance.\nEffectiveness of Depth Guided Scene Graph Generation Module. To substantiate the effectiveness of depth guidance in both object detection and relation extraction, we conducted dedicated experiments: one with guidance training solely for object detection, and another exclusively for relation classification. As demonstrated in the second section of Table.V, the absence of guidance in the object detection module enhances performance in PredCLS, underlining the model\u2019s proficiency in learning relation distributions. Conversely, without guidance in the relation classification module, the model exhibits improved performance in the SGCls task, signifying its strength in object detection. Optimal performance in SGDet tasks \u2014 a holistic metric for scene graph prediction that integrates both relation prediction and object detection \u2014 is only achieved when guidance is simultaneously employed in both object detection and relation classification. This underscores the importance of coordinated guidance in these two domains for superior scene graph prediction.\nEffectiveness of Depth Guided HHA Representation Generation Module. In order to verify the effectiveness of our approach to depth information processing using HHA data, we conducte an additional set of ablation studies. We experiment with raw depth information, horizontal disparity, and a combination of horizontal disparity with height above ground. As indicated in the fourth row of Table.V, the SDTG model reaps tangible benefits from the enhancement of depth information using HHA features. Conversely, the utilization of solely horizontal disparity or height above ground information does not yield the superior performance associated with HHA data, underscoring the significant role HHA data plays in optimizing our model."
        },
        {
            "heading": "V. CONCLUSION",
            "text": "In this work, we introduce the Depth-Guided One-Stage Scene Graph Generation (STDG) methodology, addressing existing challenges in scene graph generation for autonomous robotic systems. STDG\u2019s innovative architecture comprising three modules leverages depth information from generation to prediction without extra computational demand. Experimental results confirm its significant performance enhancement in one-stage scene graph generation."
        }
    ],
    "title": "STDG: Semi-Teacher-Student Training Paradigram for Depth-guided One-stage Scene Graph Generation",
    "year": 2023
}