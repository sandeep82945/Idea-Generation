{
    "abstractText": "In this paper, we present SAFER, a novel system for emotion recognition from facial expressions. It employs state-of-the-art deep learning techniques to extract various features from facial images and incorporates contextual information, such as background and location type, to enhance its performance. The system has been designed to operate in an open-world setting, meaning it can adapt to unseen and varied facial expressions, making it suitable for real-world applications. An extensive evaluation of SAFER against existing works in the field demonstrates improved performance, achieving an accuracy of 91.4% on the CAER-S dataset. Additionally, the study investigates the effect of novelty such as face masks during the Covid-19 pandemic on facial emotion recognition and critically examines the limitations of mainstream facial expressions datasets. To address these limitations, a novel dataset for facial emotion recognition is proposed. The proposed dataset and the system are expected to be useful for various applications such as human-computer interaction, security, and surveillance.",
    "authors": [
        {
            "affiliations": [],
            "name": "Mijanur Palasha"
        },
        {
            "affiliations": [],
            "name": "Bharat Bhargava"
        }
    ],
    "id": "SP:d64667798ebd7826ea6b02c4fd1a06cdfaf3d250",
    "references": [
        {
            "authors": [
                "R. Cowie",
                "E. Douglas-Cowie",
                "N. Tsapatsoulis",
                "G. Votsis",
                "S. Kollias",
                "W. Fellenz",
                "J.G. Taylor"
            ],
            "title": "Emotion recognition in human-computer interaction",
            "venue": "IEEE Signal processing magazine 18 (1) ",
            "year": 2001
        },
        {
            "authors": [
                "C. Clavel",
                "I. Vasilescu",
                "L. Devillers",
                "G. Richard",
                "T. Ehrette"
            ],
            "title": "Fear-type emotion recognition for future audio-based surveillance systems",
            "venue": "Speech Communication 50 (6) ",
            "year": 2008
        },
        {
            "authors": [
                "B. Li",
                "D. Dimitriadis",
                "A. Stolcke"
            ],
            "title": "Acoustic and lexical sentiment analysis for customer service calls",
            "venue": "in: ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE",
            "year": 2019
        },
        {
            "authors": [
                "M. Ali",
                "F. Al Machot",
                "A.H. Mosa",
                "K. Kyamakya"
            ],
            "title": "A novel eeg-based emotion recognition approach for e-healthcare applications",
            "venue": "in: Proceedings of the 31st Annual ACM Symposium on Applied Computing",
            "year": 2016
        },
        {
            "authors": [
                "K. Patel",
                "D. Mehta",
                "C. Mistry",
                "R. Gupta",
                "S. Tanwar",
                "N. Kumar",
                "M. Alazab"
            ],
            "title": "Facial sentiment analysis using ai techniques: state-of-theart",
            "venue": "taxonomies, and challenges, IEEE Access 8 ",
            "year": 2020
        },
        {
            "authors": [
                "H.A. Elfenbein"
            ],
            "title": "N",
            "venue": "Ambady, On the universality and cultural specificity 32 of emotion recognition: a meta-analysis., Psychological bulletin 128 (2) ",
            "year": 2002
        },
        {
            "authors": [
                "A. Adadi",
                "M. Berrada"
            ],
            "title": "Peeking inside the black-box: a survey on explainable artificial intelligence (xai)",
            "venue": "IEEE access 6 ",
            "year": 2018
        },
        {
            "authors": [
                "J. Jayalekshmi",
                "T. Mathew"
            ],
            "title": "Facial expression recognition and emotion classification system for sentiment analysis",
            "venue": "in: 2017 International Conference on Networks & Advances in Computational Technologies (NetACT), IEEE",
            "year": 2017
        },
        {
            "authors": [
                "H. Li",
                "Z. Lin",
                "X. Shen",
                "J. Brandt",
                "G. Hua"
            ],
            "title": "A convolutional neural network cascade for face detection",
            "venue": "in: Proceedings of the IEEE conference on computer vision and pattern recognition",
            "year": 2015
        },
        {
            "authors": [
                "N.B. Kar",
                "K.S. Babu",
                "A.K. Sangaiah",
                "S. Bakshi"
            ],
            "title": "Face expression recognition system based on ripplet transform type ii and least square svm",
            "venue": "Multimedia Tools and Applications 78 (4) ",
            "year": 2019
        },
        {
            "authors": [
                "H.M. Shah",
                "A. Dinesh",
                "T.S. Sharmila"
            ],
            "title": "Analysis of facial landmark features to determine the best subset for finding face orientation",
            "venue": "in: 2019 International Conference on Computational Intelligence in Data Science (ICCIDS), IEEE",
            "year": 2019
        },
        {
            "authors": [
                "V. Bazarevsky",
                "Y. Kartynnik",
                "A. Vakunov",
                "K. Raveendran",
                "M. Grundmann"
            ],
            "title": "Blazeface: Sub-millisecond neural face detection on mobile gpus",
            "venue": "arXiv preprint arXiv:1907.05047 ",
            "year": 2019
        },
        {
            "authors": [
                "R.S. Jadhav",
                "P. Ghadekar"
            ],
            "title": "Content based facial emotion recognition model using machine learning algorithm",
            "venue": "in: 2018 International Conference on Advanced Computation and Telecommunication (ICACAT), IEEE",
            "year": 2018
        },
        {
            "authors": [
                "Y. Gan",
                "J. Chen",
                "L. Xu"
            ],
            "title": "Facial expression recognition boosted by soft label with a diverse ensemble",
            "venue": "Pattern Recognition Letters 125 ",
            "year": 2019
        },
        {
            "authors": [
                "P. Dhankhar"
            ],
            "title": "Resnet-50 and vgg-16 for recognizing facial emotions",
            "venue": "International Journal of Innovations in Engineering and Technology (IJIET) 13 (4) ",
            "year": 2019
        },
        {
            "authors": [
                "A.P. Fard",
                "M.H. Mahoor"
            ],
            "title": "Ad-corre: Adaptive correlation-based loss for facial expression recognition in the wild",
            "venue": "IEEE Access 10 ",
            "year": 2022
        },
        {
            "authors": [
                "A.H. Farzaneh",
                "X. Qi"
            ],
            "title": "Facial expression recognition in the wild via deep attentive center loss",
            "venue": "in: Proceedings of the IEEE/CVF winter conference on applications of computer vision",
            "year": 2021
        },
        {
            "authors": [
                "Y. Li",
                "J. Zeng",
                "S. Shan",
                "X. Chen"
            ],
            "title": "Occlusion aware facial expression recognition using cnn with attention mechanism",
            "venue": "IEEE Transactions on Image Processing 28 (5) ",
            "year": 2018
        },
        {
            "authors": [
                "K. Wang",
                "X. Peng",
                "J. Yang",
                "D. Meng",
                "Y. Qiao"
            ],
            "title": "Region attention networks for pose and occlusion robust facial expression recognition",
            "venue": "IEEE Transactions on Image Processing 29 ",
            "year": 2020
        },
        {
            "authors": [
                "J. She",
                "Y. Hu",
                "H. Shi",
                "J. Wang",
                "Q. Shen",
                "T. Mei"
            ],
            "title": "Dive into ambiguity: Latent distribution mining and pairwise uncertainty estimation for facial expression recognition",
            "venue": "in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2021
        },
        {
            "authors": [
                "S. Datta",
                "D. Sen",
                "R. Balasubramanian"
            ],
            "title": "Integrating geometric and textural features for facial emotion classification using svm frameworks",
            "venue": "in: Proceedings of International Conference on Computer Vision and Image Processing, Springer",
            "year": 2017
        },
        {
            "authors": [
                "A.R. Kurup",
                "M. Ajith",
                "M.M. Ram\u00f3n"
            ],
            "title": "Semi-supervised facial expression recognition using reduced spatial features and deep belief networks",
            "venue": "Neurocomputing 367 ",
            "year": 2019
        },
        {
            "authors": [
                "P. Marks"
            ],
            "title": "Can the biases in facial recognition be fixed; also",
            "venue": "should they?, Communications of the ACM 64 (3) ",
            "year": 2021
        },
        {
            "authors": [
                "J. Buolamwini",
                "T. Gebru"
            ],
            "title": "Gender shades: Intersectional accuracy disparities in commercial gender classification",
            "venue": "in: Conference on fairness, accountability and transparency, PMLR",
            "year": 2018
        },
        {
            "authors": [
                "D. Zeng",
                "Z. Lin",
                "X. Yan",
                "Y. Liu",
                "F. Wang",
                "B. Tang"
            ],
            "title": "Face2exp: Combating data biases for facial expression recognition",
            "venue": "in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2022
        },
        {
            "authors": [
                "B. Zhou",
                "A. Lapedriza",
                "A. Khosla",
                "A. Oliva",
                "A. Torralba"
            ],
            "title": "Places: A 10 million image database for scene recognition",
            "venue": "IEEE transactions on pattern analysis and machine intelligence 40 (6) ",
            "year": 2017
        },
        {
            "authors": [
                "A. Mollahosseini",
                "B. Hasani",
                "M.H. Mahoor"
            ],
            "title": "Affectnet: A database for facial expression",
            "venue": "valence, and arousal computing in the wild, IEEE Transactions on Affective Computing 10 (1) ",
            "year": 2017
        },
        {
            "authors": [
                "S. Li",
                "W. Deng",
                "J. Du"
            ],
            "title": "Reliable crowdsourcing and deep localitypreserving learning for expression recognition in the wild",
            "venue": "in: Proceedings of the IEEE conference on computer vision and pattern recognition",
            "year": 2017
        },
        {
            "authors": [
                "P. Lucey",
                "J.F. Cohn",
                "T. Kanade",
                "J. Saragih",
                "Z. Ambadar",
                "I. Matthews"
            ],
            "title": "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression",
            "venue": "in: 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops",
            "year": 2010
        },
        {
            "authors": [
                "R. Kosti",
                "J.M. Alvarez",
                "A. Recasens",
                "A. Lapedriza"
            ],
            "title": "Context based emotion recognition using emotic dataset",
            "venue": "IEEE transactions on pattern analysis and machine intelligence 42 (11) ",
            "year": 2019
        },
        {
            "authors": [
                "J. Lee",
                "S. Kim",
                "S. Kim",
                "J. Park",
                "K. Sohn"
            ],
            "title": "Context-aware emotion recognition networks",
            "venue": "in: Proceedings of the IEEE/CVF International Conference on Computer Vision",
            "year": 2019
        },
        {
            "authors": [
                "H. Gunes",
                "M. Piccardi"
            ],
            "title": "Bi-modal emotion recognition from expressive 36 face and body gestures",
            "venue": "Journal of Network and Computer Applications 30 (4) ",
            "year": 2007
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren"
            ],
            "title": "J",
            "venue": "Sun, Deep residual learning for image recognition ",
            "year": 2015
        },
        {
            "authors": [
                "J. Deng",
                "W. Dong",
                "R. Socher",
                "L.-J. Li",
                "K. Li",
                "L. Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "in: 2009 IEEE conference on computer vision and pattern recognition, Ieee",
            "year": 2009
        },
        {
            "authors": [
                "J. Lee",
                "S. Kim",
                "S. Kim",
                "J. Park",
                "K. Sohn"
            ],
            "title": "Context-aware emotion recognition networks",
            "venue": "in: Proceedings of the IEEE/CVF International Conference on Computer Vision",
            "year": 2019
        },
        {
            "authors": [
                "W. Li",
                "X. Dong",
                "Y. Wang"
            ],
            "title": "Human emotion recognition with relational region-level analysis",
            "venue": "IEEE Transactions on Affective Computing ",
            "year": 2021
        },
        {
            "authors": [
                "G. Wen",
                "Z. Hou",
                "H. Li",
                "D. Li",
                "L. Jiang",
                "E. Xun"
            ],
            "title": "Ensemble of deep neural networks with probability-based fusion for facial expression recognition",
            "venue": "Cognitive Computation 9 (5) ",
            "year": 2017
        },
        {
            "authors": [
                "A. Renda",
                "M. Barsacchi",
                "A. Bechini",
                "F. Marcelloni"
            ],
            "title": "Comparing ensemble strategies for deep learning: An application to facial expression recognition",
            "venue": "Expert Systems with Applications 136 ",
            "year": 2019
        },
        {
            "authors": [
                "K. Wang",
                "X. Peng",
                "J. Yang",
                "S. Lu",
                "Y. Qiao"
            ],
            "title": "Suppressing uncertainties for large-scale facial expression recognition",
            "venue": "in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition",
            "year": 2020
        },
        {
            "authors": [
                "J. Chakraborty",
                "S. Majumder",
                "T. Menzies"
            ],
            "title": "Bias in machine learning software: why? how? what to do",
            "venue": "in: Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "In this paper, we present SAFER, a novel system for emotion recognition from facial expressions. It employs state-of-the-art deep learning techniques to extract various features from facial images and incorporates contextual information, such as background and location type, to enhance its performance. The system has been designed to operate in an open-world setting, meaning it can adapt to unseen and varied facial expressions, making it suitable for real-world applications. An extensive evaluation of SAFER against existing works in the field demonstrates improved performance, achieving an accuracy of 91.4% on the CAER-S dataset. Additionally, the study investigates the effect of novelty such as face masks during the Covid-19 pandemic on facial emotion recognition and critically examines the limitations of mainstream facial expressions datasets. To address these limitations, a novel dataset for facial emotion recognition is proposed. The proposed dataset and the system are expected to be useful for various applications such as human-computer interaction, security, and surveillance. Keywords: Facial Expression Recognition, Emotion Recognition, Deep\n\u2217Corresponding author Email addresses: mpalash@purdue.edu (Mijanur Palash), bbshail@purdue.edu\n(Bharat Bhargava)\nPreprint submitted to Open-World AI June 19, 2023\nar X\niv :2\n30 6.\n09 37\n2v 1\n[ cs\n.C V\n] 1\n4 Ju\nn 20\nLearning, Covid-19, Contextual information, Open-world AI"
        },
        {
            "heading": "1. Introduction",
            "text": "Human emotion recognition (ER) has gained significant research interest in recent years, particularly in the field of Artificial Intelligence (AI). This is due in part to the growing demand for online and remote learning systems as a result of the Covid-19 pandemic, where ER can play a crucial role in maintaining a positive and engaging learning environment by tracking the emotional status of students. Additionally, ER has a wide range of applications in domains such as human-computer interactions [1], law enforcement and surveillance [2], interactive gaming, consumer behavior analysis, customer service [3], and health care [4], among others.\nFacial emotion recognition (FER) is a widely adopted approach for ER, which primarily relies on the analysis of facial expressions to infer emotional states. Researchers have traditionally categorized basic emotions into seven distinct categories, including anger, happiness, sadness, disgust, fear, contempt, and surprise [6]. Figure 1 illustrate some common facial expressions associated with each of these emotions. The universality of facial expressions across different cultures, as demonstrated in this well-known study at [7], has greatly facilitated the development of FER systems. Additionally, the presence of micro-expressions, which are involuntary facial actions indicative of concealed emotions, also play a critical role in FER.\nThe Facial Action Coding System (FACS) is a widely accepted method for describing the movements of various facial muscles associated with different\nemotions. FACS breaks down facial expressions into individual components of muscle movement, referred to as Action Units (AU). Table 1 lists some common AU activities associated with different emotions.\nSimilarly, it is important to note that the situational context surrounding an individual also plays a significant role in shaping their emotional state. For instance, a person working in a sweaty coal mine is more likely to exhibit unhappiness as compared to someone walking in a park with their dog. However, in some situations, place type alone may not be sufficient. For example, in a stadium, depending on whether the team wins or loses, some people may be happy and some may be unhappy. Figure 2(a) and 2(b) illustrate this point, where without the background, the facial expression may be misleading. Therefore, the ability to extract and incorporate situational information, such as scene background and location type, can greatly enhance the accuracy of FER systems.\nHowever, during the Covid-19 pandemic, the widespread use of face masks has presented a unique challenge for FER, as masks obscure facial expressions and result in a loss of important information. This can lead to a significant decrease in performance for models trained on datasets without masked subjects, with accuracy drops of up to 29% observed in masked test sets. In this context, situational knowledge becomes critical, and special datasets and models that can deal with masked subjects are needed.\nMoreover, the field of explainable AI has gained significant momentum in recent years [9], as the lack of transparency and interpretability of black-box deep learning systems has become a major concern. However, current ER works do not focus on this issue, and only classify the emotional status of\nthe subject without providing any explanation for their decision. To address this, we propose the use of facial data, scene background, and place type, to create situational knowledge that can explain the results of our emotion recognition system, SAFER.\nThe main contributions of this work are:\n1. The development of a novel multi-stream emotion recognition system,\nSAFER, that utilizes deep learning methods to classify emotions from facial expressions, scene background, and location type.\n2. Evaluation of the proposed system on several benchmark datasets, and\na comparison of our results with other recent works. Additionally, an ablation study is conducted to evaluate the effects of each stream in the learning process.\n3. A discussion of the drawbacks of existing FER datasets, including issues\nrelated to data quality, imbalanced class distribution, and racial and gender bias.\n4. A novel dataset for FER called DeFi, which includes masked subjects,\nto enable researchers to investigate the effect of masks on emotion recognition.\nOverall, this work represents a significant step forward in the field of explainable artificial intelligence, by providing a transparent and reliable system for emotion recognition that utilizes multiple streams of data and context, and by addressing the challenges posed by the use of face masks. This work can be considered as a step towards open world AI, where AI systems can adapt to the changing scenario and can provide explanations for their predictions."
        },
        {
            "heading": "2. Background and Related Work",
            "text": "Emotions are a natural response to environmental stimuli. Understanding human emotions is crucial for effective communication. Human perception and judgment of situations or individuals are heavily influenced by their emotional state, which can impact activities such as driving a car, learning in a classroom, or interactions with law enforcement, among others.\nIn the field of facial expression recognition (FER), it is crucial to accurately detect and segment the facial region from the surrounding image. Various techniques have been proposed for this task, such as the Viola-Jones method [10, 11], the combination of PCA and Viola-Jones [12], and the Haar Cascades method [13]. These approaches have demonstrated effectiveness in accurately localizing the facial region for further processing in FER systems. A recent technique proposed by Bazarevsky et al. [14] is capable of detecting six landmarks on a face and can handle the detection of multiple faces within an image.\nConvolutional neural networks (CNNs) are a widely-utilized deep learning architecture in image analysis. The convolution operation, which applies repeated filters to an input, results in a map of activations known as a feature map. This highlights the locations and strength of features detected in the input, allowing CNNs to identify important parts of an image that differentiate between classes. In a previous study [15], the authors employed a CNN to detect emotions from facial expressions in the FER-2013 dataset [16].\nGan et al. [17] achieved improved accuracy on the FER-2013 dataset using ensemble CNN and a novel label level perturbation strategy. In [18], authors used the ensemble method and transfer learning with VGG16 and ResNet-50.\nFard et al. [19] proposed adaptive correlation-based loss for facial expression in the wild. Similarly, Farzaneh et al. [20] proposed an deep attentive center loss for facial expression recognition. Both works use variations of Deep Metric Learning (DML) for emotion recognition. Li et al. [21] proposed CNN with attention for occlusion aware facial expression recognition. Wang et al. [22] used region attention networks for more pose and occlusion robust recognition. She et al. [23] used latent distribution mining and pairwise uncertainty estimation for facial emotion recognition.\nSupport Vector Machine (SVM) is another type of machine learning model. It tries to identify the largest margin plane between the classes. The SVM is also popular in facial emotion recognition due to its lightweight architecture compared to CNN. Authors at [24] used SVM for emotion classification. Their method achieved 91.8% test accuracy on the CK+ dataset. Deep Belief Network (DBN) is also used in emotion recognition. Authors in [25] reported 98% accuracy on the CK+ dataset using the DBN technique.\nBias in machine learning is an important challenge [26, 27]. Buolamwini et. al. [27] showed that machine learning algorithms can discriminate based on classes like race and gender. Zeng et al. [28] showed the bias in the class distribution of FER training datasets. He proposed a circuit feedback mechanism to tackle the issue.\nThe Places Database [29] has a massive collection with 10 million labeled scene photographs from around the world. Additionally, it offers various pretrained CNNs (Places-CNNs) for scene classification, which can be used for scene category and attribute identification."
        },
        {
            "heading": "3. Datasets",
            "text": "In the field of facial emotion recognition, various datasets have been used to evaluate the performance of different algorithms. We utilized various datasets including FER-2013 [16], AffectNet [30] and RAF-DB [31] for our experiments.\nThe AffectNet is a comprehensive dataset of facial expressions that were sourced from the Internet via the use of 1,200 keywords related to emotions. This database comprises over one million facial images, with 440,000 of them being manually annotated. The images in this dataset are classified into eight distinct emotion categories and also include valence-arousal annotations.\nThe FER2013 includes 28709 training images, 3589 validation images, and 3589 test images that are classified into seven different emotion classes. These images are posed and there is an observable class imbalance issue within the dataset, as the \u2018Disgust\u2019 class only comprises 700 images, while some other classes have around 5000 images. This presents a challenge for accurate emotion recognition using this dataset.\nThe CAER-S dataset is a collection of nearly 70,000 facial expressions from 79 different TV shows. These data are manually annotated into six distinct emotion categories.\nThe RAF-DB is a dataset containing 29,672 facial expression images. These images have been collected from the internet and manually labeled by 40 annotators, and are classified into 7 classes of basic emotions and 12 classes of compound emotions. The images in this database vary greatly in terms of subject age, gender, and ethnicity.\nThe DeFi is a new dataset proposed in this work. It contains 21,000\nimages that have been collected in both posed and wild settings. These images have been classified into seven basic emotion classes. Additionally, the dataset includes data that can be used to train an emotion recognition model to identify emotions even when the user is wearing a face mask. The specifics of this dataset will be discussed in Section 7.\nA summary of all these datasets is presented in table 2."
        },
        {
            "heading": "4. Our method: SAFER",
            "text": "In this section, we present our proposed emotion recognition system SAFER. Figure 3 shows a high-level diagram of the system and its components."
        },
        {
            "heading": "4.1. Input",
            "text": "The input of this system is an RGB image that contains the face and the background. The image can be a still photo, a frame from video footage, or a live video feed for continuous monitoring."
        },
        {
            "heading": "4.2. Face Detection",
            "text": "The SAFER system requires the separation of the facial area from the rest of the input image. Face detection is accomplished through the use of the Blazeface technique, as described by Bazarevsky et al. [14]. This method can detect six landmarks on the face and can handle the detection of multiple faces in an image."
        },
        {
            "heading": "4.3. Face Feature Extraction",
            "text": "This module consists of three components: the AU feature generator, the visible feature generator, and the deep feature extractor. These three feature\nsets are combined to produce the face feature set Ff . The different parts of this module are depicted in Figure 6."
        },
        {
            "heading": "4.3.1. Action Unit (AU) Feature Set",
            "text": "From the face mesh (figure 5) generated using Blazeface [14], we identify 12 key AU centers in the face based on a set of predefined rules outlined in Table 3. The centers are determined by selecting the closest landmark positions, which simplifies the process without sacrificing accuracy. Finally, we calculate the distances between all AU points to form our AU feature set."
        },
        {
            "heading": "4.3.2. Visible Feature Set",
            "text": "From the face mesh generated using Blazeface [14], we create a group of features shown in the Table 4. They capture various aspects of the face including width, height, distance, and angle of different facial parts. Our approach leverages the observation that changes in facial expression, such as when shouting or laughing, often result in alterations to specific facial features. For example, when smiling or laughing, the mouth tends to open, leading to increased lip width. Conversely, expressions of surprise often result in increased eye width and height. These features are thus valuable for use in facial emotion recognition."
        },
        {
            "heading": "4.3.3. Deep Feature Set",
            "text": "Deep features are the values we obtain from the output of the deep fea-\nture extractor as shown in figure 6. For the feature extractor we experiment with two different CNN model types.\n\u2022 Regular CNN: In the regular CNN setup, we use three convolutional\nlayers and one fully connected layer. Input images of different resolutions are converted to 226\u00d7 226 . All convolutions have a filter size of 2\u00d72 and a stride of 1. The use of a stride of 1 in convolution was found to be more effective in our experiments as it permits all spatial downsamplings to occur in the downsampling layers. The downsampling layers employ the 2\u00d7 2 Max-Pooling technique.\n\u2022 Transfer learning from ResNet-50: In this configuration, the Trans-\nfer Learning approach is utilized for deep feature extraction. This approach addresses the challenge of an insufficient number of training data points by transferring knowledge from a model that has been trained on a large dataset with similar properties to the smaller dataset in question. In this study, the ResNet-50 model [36] trained on the ImageNet [37] database is employed. ResNet-50 is a deep neural network with 50 layers, which can mitigate the vanishing and exploding gradient problems that are commonly encountered in substantially deeper neural networks through the utilization of a deep residual learning framework and residual mapping technique."
        },
        {
            "heading": "4.4. Background Feature Extraction",
            "text": "We first remove the subject body and face from the scene to extract background. The extracted background is then processed through a deep feature extraction network as depicted in Figure 7. The network consists of three convolutional layers and one fully connected layer, with all convolutional layers utilizing a filter size of 2\u00d7 2 and a stride of 1. The downsampling layers\nemploy the 2 \u00d7 2 Max-Pooling technique. The result of this network is the Background Feature Set (Fb)."
        },
        {
            "heading": "4.5. Place Feature Set",
            "text": "In this step, we remove the subject body and face from the scene. We use pre-trained AlexNet provided with the Places dataset and pass the scene through it. Deep features are collected from AlexNet after the final maxpooling operation to produce the Location Feature Set (Fl). We also collect final place categories such as \u2018classroom\u2019 and attributes such as \u2018no horizon\u2019 and \u2018enclosed area\u2019 for explanation generation."
        },
        {
            "heading": "4.6. Detection Model",
            "text": "The final feature set (F ) is a concatenation of the face feature set (Fp), background feature set (Fb) and place feature set (Fl). We use two FC layers (figure 3) for the final classification. Cross-entropy loss is used for the loss function."
        },
        {
            "heading": "5. Experimental Results",
            "text": "In this section, we present the experimental evaluation of SAFER on various facial emotion recognition datasets. Our model is trained on the benchmark datasets listed in Table 2, and its performance is compared to recent approaches in the literature. The experiments were conducted on a server PC that had 20 cores with a 2.6 GHz Intel Xeon CPU and 96 GB of memory, as well as three NVIDIA TESLA GPUs with 24 GB of memory each. To facilitate accelerated computing, we used Python multiprocessing and mixed precision libraries. The datasets were split into training, validation, and test sets in an 80:10:10 ratio. All images were resized to 224 \u00d7 224 pixels, and dataset augmentation was performed using cropping, rotation, brightness, and contrast adjustments. We employed an adaptive learning rate that started at 1e\u22125 and a batch size of 32\nWe use test accuracy as our performance criteria. The test accuracy is\ngiven by the following equation:\nAccuracy = #Nc #Nt\nWhere #Nc indicates the number of items correctly predicted and #Nt\nindicates the total number of items in the test dataset."
        },
        {
            "heading": "5.1. Performance of SAFER on Benchmark Datasets",
            "text": "The results of SAFER with some facial emotion recognition benchmark datasets are shown in the table 5. Our results are comparable with the stateof-the-art works in all datasets. For the FABO dataset, we outperform the accuracy reported by various recent works (table 7). For the CK+ dataset,\nwe find the best-reported result to be 98.57% accuracy as in [25], our accuracy of 98.5% is comparable to it.\nBesides, we report the results of several recent works on the CAER-S dataset in table 6. We see SAFER outperforms closest result reported by Li. et al. [39] by 7.8%.\nIn FER-2013 dataset (table 7), our model outperforms results from [19] and [21]. Similarly, table 8 shows results from several recent works on the AffectNet dataset. Here, our accuracy is 63.7%. Which is close to the accuracy values reported by [28] and [20]. In [28], author provided a effective solution to the class imbalance issue widespread in most of the FER datasets including AffectNet. In AffectNet dataset \u2018Happiness\u2019 class has 146,198 samples while \u2018Disgust\u2019 has only 5,264 samples. For this reason our model gives lower accuracy in \u2018Disgust\u2019 class. However, our work is orthogonal to [28] and both can be implemented together. Similarly, [20] uses Deep Metric Learning (DML) method with modified loss functions. They argued that using softmax loss can not provide proper discrimination between classes due to inter-class similarity and intra-class variations. Hence they use sparse center loss in adition to the softmax as the final objective function. This work is also orthogonal to our work as this loss function can be used with our method too.\nFrom table 6 and 8, it is clear that our model offers greater improvement in the CAER-S dataset than AffectNet. This is due to the presence of less class imbalance and more contextual information in the CAER-S dataset.\nFigure 8 shows the confusion matrices of our model for the FER-2013 and CAER-S datasets. In the figure, we list actual emotion labels along the\nvertical direction and predicted ones along the horizontal direction. An entry \u2018Cij\u2019 in row \u2018i\u2019 and column \u2018j\u2019 represents the number of samples who has the true label of row \u2018i\u2019, and the predicted label of column \u2018j\u2019.\nFrom the confusion matrix of FER-2013, we can see some classes such as \u2018Happiness\u2019 and \u2018Surprise\u2019 are better recognizable while \u2018Disgust\u2019 and \u2018Fear\u2019 classes are not. But we do not see a similar pattern in the CAER-S dataset. One possible reason is the unequal distribution of samples in FER2013 classes. For example, in FER-2013 the \u2018Disgust\u2019 class has only 436 training samples while the \u201dHappiness\u201d class has 7215 training samples. But in CAER-S all the emotion classes have an equal number of samples (7001 samples). Hence, our model results in similar accuracy in all the emotion classes of the CAER-S dataset.\nMany of the emotion classes share some of the facial expression with each other. For example, we see lots of \u2018Happy\u2019 samples are wrongly classified as \u2018Neutral\u2019, \u2018Disgust\u2019 as \u2018Anger\u2019 etc. This happens due to the close correlation between these emotion classes and increases the complexity of the classification task."
        },
        {
            "heading": "5.2. Explanation Generation",
            "text": "Besides determining emotion we also provide a guideline to generate an explanation for that result. We can provide human explainable reasoning by creating an idea of the situation around the subject. Individual modules tell us what information is available from the face and background. For instance, the subject in figure 9 red bounding box has a smiling face and colorful vibrant background. By extracting age, gender, location type and location attributes, we can create our situational knowledge which further enhances this reasoning. In the case of figure 9, place category output is a day care play room. By combining all these a human understandable explanation of happiness class for the subject can be constructed as \u201dthe subject is a child in a playroom and smiling, has a happy facial expression\u201d.\nThis is an early effort for explainable emotion classification using multiple data modalities as per our knowledge. However, our explanation generation\nstill requires further work. Therefore, we are planning to explore explanation generation in a more detailed manner in our future works."
        },
        {
            "heading": "5.3. Ablation Study",
            "text": "We show the resulting outputs from various ablation experiments in table 9. We compare the accuracy of our model on AffectNet and CAER-S datasets in several combinations. The combinations are: face feature set Fp only, face feature set Fp + background feature set Fb, face feature set Fp + place feature set Fl and all three feature sets combined.\nFrom the table, we can see that in the AffectNet dataset, operating on face data alone results in an accuracy of 61.9%. Adding background and place feature sets does not improve accuracy significantly. But for the CAERS dataset, we see the addition of these two extra feature sets result in a noticeable increase in accuracy. Our understanding is that the AffectNet dataset has only face images with a paltry background. They offer very little information for the background and place streams to extract. However, CAER-S is designed for context-based emotion recognition. Hence, we have lots of background information in the samples. That is why the contribution of background and place streams are significant for CAER-S.\nWe also analyse the choice of the deep feature extractor. In this case, pre-trained ResNet-50 performs better in our experiments than the regular CNN extractor discussed in section 4.3.3. We also notice that adding an AU feature set and visible feature has a positive effect on the accuracy of the face stream alone. If we remove them and use only the deep features from the face using ResNet-50, the accuracy drops to a lower value.\nThus when contextual information is available, adding background or\nplace features improves accuracy and the best accuracy result is achieved when we use all available feature sets. However, if no contextual information is available then these two modules fail to offer meaningful contributions and our model does not provide the best results."
        },
        {
            "heading": "6. Issues of Facial Expression Based Emotion Recognition",
            "text": ""
        },
        {
            "heading": "6.1. Covid Mask Issue",
            "text": "During the global Covid-19 pandemic, people used to wear a face mask [43].\nFace mask covers a good part of our face including the nose, lips, mouth and chin. This unexpected situation is a novelty and creates a challenge for facial-based emotion recognition, as the absence of these facial features can result in the loss of important cues for recognition. To examine the impact of masks on facial emotion recognition, a masked section has been included in our proposed DeFi dataset.\nTesting on this mask dataset with a regular dataset-trained model gave us only 38.58% accuracy. This highlights the need for a specialized dataset for masked subjects as models trained on regular face images are not effective at classifying emotions from partially visible faces.\nWe further trained and evaluated our model using the masked section of our proposed DeFi dataset. The results showed an increased accuracy of approximately 58%, representing a 29% improvement compared to the results from testing the regular dataset-trained model. Despite this improvement, the accuracy still remains lower compared to the results obtained from training and testing on unmasked data.\nWe further analyze this result with the feature maps from different convolution layers of the ResNet-50 of our architecture. The selected feature maps from layer 10, 20, 30 and 40 are shown in Figure 10. A visual inspection of these maps reveals that the lips and nose regions are highlighted in most of them. This highlights the fact that these facial parts are crucial for emotion classification, as feature maps highlight the most important parts of the image. As such, covering these important facial features through the use of masks significantly hinders the ability of facial expression-based models.\nOne potential approach to address the issue of face masks affecting facial expression-based emotion recognition is to employ multi-modal emotion detection, which leverages multiple modalities, such as facial expression, posture, and gait, to recognize emotions. However, these multi-modal systems tend to be more complex compared to single-modality facial expression-based systems, and also require diverse types of datasets that are not typically available in most FER datasets, such as posture and gait information. Thus, multi-modal emotion recognition is left as a future work and not addressed in this current study."
        },
        {
            "heading": "6.2. Dataset Bias",
            "text": "A good learnt model is dependent on a good dataset. However, without proper care a dataset can lack proper diversity. The model trained on it then performs worse when it encounters minority subjects of the dataset. For example, keyword searching in Google with \u201dangry face\u201d resulted in 59 acceptable images in the first 100 images. Of these images, only 9 are women while 50 are men. This pattern holds for other generic keywords such as \u201dsad people\u201d or \u201dhappy human\u201d. Preparing a dataset by collecting results from generic keywords instead of more specific ones such as \u201dblack woman sad\u201d has a higher chance of bias against women and people of color. A similar situation is also applicable to the volunteer choice for creating an acted dataset. Without the careful selection of people from various genders and ethnic backgrounds, dataset bias can be easily incorporated into the model. Table 10 shows the percentage of images with male subjects for different gender-neutral emotion-related keywords for the first 100 results listed by Google. As we can see, the number of male and female images is not equally represented in the results. For anger-related searches, men have a\nhigher percentage of images while in sadness-related searches women tend to appear in larger numbers. We also show the number of males in the first 100 images in the FER-2013 dataset for the \u201dAngry\u201d, \u201dFear\u201d, \u201dHappy\u201d and \u201dSad\u201d categories and report similar and unequal male/female representations.\nBesides gender and race, many of the existing FER datasets are biased toward majority classes. Some of the classes have a larger number of samples and hence models tend to show favor towards them. For example, in the AffectNet dataset, the \u2018Happy\u2019 class has more than 130k samples and the \u2018Disgust\u2019 and \u2018Contempt\u2019 classes have only a few thousand samples each."
        },
        {
            "heading": "6.3. Quality Concern of Current Datasets",
            "text": "In table 2 we listed some widely used datasets for facial emotion recognition. By having a closer look at them, we found certain issues we can work on resolving. Some of the images from CAER-S, Emotic and FER-2013 are shown in figure 11. The main issues we found in these datasets are:\n\u2022 Noise issue: Some of the images are not relevant and contain no face.\nWe suspect this is due to the Google image scrapping. In our search, we find that Google includes some irrelevant images in the search results. We need to clean them manually. Examples of this type of image are shown in the first row of figure 11.\n\u2022 Quality issue: Some of the images are cropped and lack parts of the\nface/ full face as shown in the second row of figure 11. As we can see from figure 10, we need all those parts of the face for successful recognition.\n\u2022 Confusing annotation: These are the images where annotations do not\nmatch with the image. This is certainly an issue for those images where there is a lack of consensus among human annotators. In some of the datasets where they used frames from video clips of movies and dramas, they annotated all the frames for a clip as a certain class, even though emotion changes from neutral to apex to back to neutral again. Using a larger number of annotators per image and removing confusing images can resolve this issue. All the images on the third row of 11 are annotated at angry. The first three are from a popular TV series and those particular situations can be best termed neutral conversations.\n\u2022 Number of items in classes: Another issue we found is having a large\ndifference in the number of data points per class. For example, in FER-2013 the \u2018Angry\u2019 class has 3995 images while the \u2018Disgust\u2019 class has only 436 images. A good dataset should have an equal number of samples in each of the classes."
        },
        {
            "heading": "7. Proposed DeFi Dataset",
            "text": "Recent works have proposed several algorithmic methods to deal with the above-mentioned bias in the datasets [28, 19, 44]. However, these methods do not mitigate the problem completely. Hence in this work, we intend\nto propose a new dataset where samples are carefully chosen to be more inclusive. This is an orthogonal approach to the algorithmic solutions and they can work together.\nWe first start with improving the existing datasets according to the issues outlined above. We have 8 volunteers, 4 males and 4 females, all aged 18- 30 years and university students, checking the datasets for any irrelevant images similar to the first row in the figure 11. They also annotate the images individually. From these annotations, we keep images with at least 4 annotators agreeing on the label. We call it the 80% consensus method. After these steps, approximately 7.5% images are removed from the original FER-2013 dataset. From FABO dataset videos, we extract the frames at a 10 FPS rate. As the author outlined, subjects in this dataset go from neutral to apex emotion state and then back to neutral. Our volunteers collect the\nimages showing apex emotion states from the extracted frames. We again follow 80% consensus among the annotators for selecting an image in this step.\nBesides working on existing datasets we record video clips showing 7 emotional states from a group of 10 volunteers. Due to the Covid 19 pandemic, we do not meet the participants in person, rather they are recruited over social media platforms. These volunteers are aged 18-30 and from both genders with diverse racial and ethnic backgrounds. All are university students. Each participant records video clips of their best impression of these emotions and provides us with the clips. We extract the video frames as 10 FPS and perform the 80% consensus check by the annotators.\nTo increase diversity in the dataset and reduce bias, we include people from multiple geographic and ethnic backgrounds by collecting images from Google searches using keywords such as \u201dblack man happy face\u201d, \u201dIndian woman sad face\u201d, \u201dAsian man angry face\u201d etc. Collected images from these Google searches are checked for irrelevant images and tested for 80% consensus in their annotations. By similar keyword searching on YouTube and other streaming platforms we collect relevant video clips from which we generate dataset images and labels.\nBy aggregating these images from various sources we create our new dataset DeFi. We believe it is free of irrelevant images, more accurately annotated, more balanced in the number of samples in each class and represents people with a more diverse background. Each class in DeFi dataset has 7000+ images with 10% images kept aside for the test set. Each image is 224\u00d7 224 resolution.\nFor all the images in DeFi, we create another dataset that imitates the face with the mask. This can be useful for other researchers who want to work with a dataset with masked people. To the best of our knowledge, any such dataset does not exist yet. The complete dataset will be publicly available at the provided web location [45].\nWe test SAFER in this new dataset. For both types of feature extractor\nof SAFER the accuracy is shown in the table 11."
        },
        {
            "heading": "8. Conclusions and Future Work",
            "text": "In conclusion, this paper presents SAFER, a novel system for emotion recognition from facial expressions that leverages action units, facial features, and state-of-the-art deep learning techniques. The system showed improved performance on benchmark datasets and demonstrated the positive effect of background and place features on recognition accuracy. Furthermore, the impact of mask-wearing during the Covid-19 pandemic on facial expressionbased emotion recognition was analyzed and a novel dataset was proposed to address this novelty situation. In future work, we aim to investigate multi-modal techniques for novelty detection and mitigation in facial emotion recognition. These efforts promise to advance the state of the art in this field and have the potential to improve the robustness and effectiveness of facial emotion recognition systems."
        }
    ],
    "title": "SAFER: Situation Aware Facial Emotion Recognition",
    "year": 2023
}