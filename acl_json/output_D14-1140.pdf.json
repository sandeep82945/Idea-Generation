{
    "abstractText": "We introduce a reinforcement learningbased approach to simultaneous machine translation\u2014producing a translation while receiving input words\u2014 between languages with drastically different word orders: from verb-final languages (e.g., German) to verb-medial languages (English). In traditional machine translation, a translator must \u201cwait\u201d for source material to appear before translation begins. We remove this bottleneck by predicting the final verb in advance. We use reinforcement learning to learn when to trust predictions about unseen, future portions of the sentence. We also introduce an evaluation metric to measure expeditiousness and quality. We show that our new translation model outperforms batch and monotone translation strategies.",
    "authors": [
        {
            "affiliations": [],
            "name": "Alvin C. Grissom"
        }
    ],
    "id": "SP:c90260ab2a3dadacc90b0d5938d569c7e2992a3c",
    "references": [
        {
            "authors": [
                "Pieter Abbeel",
                "Andrew Y. Ng."
            ],
            "title": "Apprenticeship learning via inverse reinforcement learning",
            "venue": "Proceedings of the International Conference of Machine Learning.",
            "year": 2004
        },
        {
            "authors": [
                "Satanjeev Banerjee",
                "Alon Lavie."
            ],
            "title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
            "venue": "Proceedings of the Association for Computational Linguistics. Association for Com-",
            "year": 2005
        },
        {
            "authors": [
                "Koby Crammer",
                "Alex Kulesza",
                "Mark Dredze."
            ],
            "title": "Adaptive regularization of weight vectors",
            "venue": "Machine Learning, 91(2):155\u2013187.",
            "year": 2013
        },
        {
            "authors": [
                "Hal Daum\u00e9 III",
                "John Langford",
                "Daniel Marcu."
            ],
            "title": "Search-based structured prediction",
            "venue": "Machine Learning Journal (MLJ).",
            "year": 2009
        },
        {
            "authors": [
                "Fernanda Ferreira."
            ],
            "title": "Syntax in language production: An approach using tree-adjoining grammars",
            "venue": "Aspects of language production, pages 291\u2013330.",
            "year": 2000
        },
        {
            "authors": [
                "Christian F\u00fcgen."
            ],
            "title": "A system for simultaneous translation of lectures and speeches",
            "venue": "Ph.D. thesis, KIT-Bibliothek.",
            "year": 2008
        },
        {
            "authors": [
                "Tomoki Fujita",
                "Graham Neubig",
                "Sakriani Sakti",
                "Tomoki Toda",
                "Satoshi Nakamura."
            ],
            "title": "Simple, lexicalized choice of translation timing for simultaneous speech translation",
            "venue": "INTERSPEECH.",
            "year": 2013
        },
        {
            "authors": [
                "Francesca Gaiba."
            ],
            "title": "The origins of simultaneous interpretation: The Nuremberg Trial",
            "venue": "University of Ottawa Press.",
            "year": 1998
        },
        {
            "authors": [
                "Kenneth Heafield",
                "Ivan Pouzyrevsky",
                "Jonathan H. Clark",
                "Philipp Koehn."
            ],
            "title": "Scalable modified Kneser-Ney language model estimation",
            "venue": "Proceedings of the Association for Computational Linguistics.",
            "year": 2013
        },
        {
            "authors": [
                "Reinhard Kneser",
                "Hermann Ney."
            ],
            "title": "Improved backing-off for n-gram language modeling",
            "venue": "Acoustics, Speech, and Signal Processing, 1995. ICASSP-95., 1995 International Conference on. IEEE.",
            "year": 1995
        },
        {
            "authors": [
                "Philipp Koehn"
            ],
            "title": "German-english parallel corpus \u201cde-news",
            "year": 2000
        },
        {
            "authors": [
                "John Langford",
                "Bianca Zadrozny."
            ],
            "title": "Relating reinforcement learning performance to classification performance",
            "venue": "Proceedings of the International Conference of Machine Learning.",
            "year": 2005
        },
        {
            "authors": [
                "Shigeki Matsubara",
                "Keiichi Iwashima",
                "Nobuo Kawaguchi",
                "Katsuhiko Toyama",
                "Yasuyoshi Inagaki."
            ],
            "title": "Simultaneous Japanese-English interpretation based on early predition of English verb",
            "venue": "Symposium on Natural Language",
            "year": 2000
        },
        {
            "authors": [
                "Hideki Mima",
                "Hitoshi Iida",
                "Osamu Furuse."
            ],
            "title": "Simultaneous interpretation utilizing example-based incremental transfer",
            "venue": "Proceedings of the 17th international conference on Computational linguistics-Volume 2, pages 855\u2013",
            "year": 1998
        },
        {
            "authors": [
                "Hideki Mima",
                "Hitoshi Iida",
                "Osamu Furuse."
            ],
            "title": "Simultaneous interpretation utilizing example-based incremental transfer",
            "venue": "Proceedings of the Association for Computational Linguistics.",
            "year": 1998
        },
        {
            "authors": [
                "Shota Momma",
                "Robert Slevc",
                "Colin Phillips"
            ],
            "title": "The timing of verb selection in english active and passive sentences",
            "year": 2014
        },
        {
            "authors": [
                "Franz Josef Och",
                "Hermann Ney."
            ],
            "title": "A systematic comparison of various statistical alignment models",
            "venue": "Computational Linguistics, 29(1):19\u201351.",
            "year": 2003
        },
        {
            "authors": [
                "Yusuke Oda",
                "Graham Neubig",
                "Sakriani Sakti",
                "Tomoki Toda",
                "Satoshi Nakamura."
            ],
            "title": "Optimizing segmentation strategies for simultaneous speech translation",
            "venue": "Proceedings of the Association for Computational Linguistics, June.",
            "year": 2014
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "Wei-Jing Zhu."
            ],
            "title": "BLEU: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the Association for Computational Linguistics.",
            "year": 2002
        },
        {
            "authors": [
                "Adam Pauls",
                "Dan Klein."
            ],
            "title": "Faster and smaller n-gram language models",
            "venue": "Proceedings of the Association for Computational Linguistics.",
            "year": 2011
        },
        {
            "authors": [
                "Brock Pytlik",
                "David Yarowsky."
            ],
            "title": "Machine translation for languages lacking bitext via multilingual gloss transduction",
            "venue": "5th Conference of the Association for Machine Translation in the Americas (AMTA), August.",
            "year": 2006
        },
        {
            "authors": [
                "Uwe Quasthoff",
                "Matthias Richter",
                "Christian Biemann."
            ],
            "title": "Corpus portal for search in monolingual corpora",
            "venue": "International Language Resources and Evaluation, pages 1799\u20131802.",
            "year": 2006
        },
        {
            "authors": [
                "Siegfried Ramler",
                "Paul Berry."
            ],
            "title": "Nuremberg and Beyond: The Memoirs of Siegfried Ramler from 20th Century Europe to Hawai\u2019i",
            "venue": "Booklines Hawaii Limited.",
            "year": 2009
        },
        {
            "authors": [
                "Koichiro Ryu",
                "Shigeki Matsubara",
                "Yasuyoshi Inagaki."
            ],
            "title": "Simultaneous english-japanese spoken language translation based on incremental dependency parsing and transfer",
            "venue": "Proceedings of the Association for Computational Lin-",
            "year": 2006
        },
        {
            "authors": [
                "Koichiro Ryu",
                "Shigeki Matsubara",
                "Yasuyoshi Inagaki."
            ],
            "title": "Alignment-based translation unit for simultaneous japanese-english spoken dialogue translation",
            "venue": "Innovations in Intelligent Machines\u20132, pages 33\u201344. Springer.",
            "year": 2012
        },
        {
            "authors": [
                "Akiko Sakamoto",
                "Nayuko Watanabe",
                "Satoshi Kamatani",
                "Kazuo Sumita"
            ],
            "title": "Development of a simultaneous interpretation system for faceto-face services and its evaluation experiment in real situation",
            "year": 2013
        },
        {
            "authors": [
                "Baskaran Sankaran",
                "Ajeet Grewal",
                "Anoop Sarkar."
            ],
            "title": "Incremental decoding for phrasebased statistical machine translation",
            "venue": "Proceedings of the Joint Fifth Workshop on Statistical Machine Translation.",
            "year": 2010
        },
        {
            "authors": [
                "H Schriefers",
                "E Teruel",
                "RM Meinshausen."
            ],
            "title": "Producing simple sentences: Results from picture\u2013word interference experiments",
            "venue": "Journal of Memory and Language, 39(4):609\u2013632.",
            "year": 1998
        },
        {
            "authors": [
                "Hiroaki Shimizu",
                "Graham Neubig",
                "Sakriani Sakti",
                "Tomoki Toda",
                "Satoshi Nakamura."
            ],
            "title": "Collection of a simultaneous translation corpus for comparative analysis",
            "venue": "International Language Resources and Evaluation.",
            "year": 2014
        },
        {
            "authors": [
                "Matthew Snover",
                "Bonnie Dorr",
                "Richard Schwartz",
                "Linnea Micciulla",
                "John Makhoul."
            ],
            "title": "A study of translation edit rate with targeted human annotation",
            "venue": "In Proceedings of Association for Machine Translation in the Americas.",
            "year": 2006
        },
        {
            "authors": [
                "Umar Syed",
                "Michael Bowling",
                "Robert E. Schapire."
            ],
            "title": "Apprenticeship learning using linear programming",
            "venue": "Proceedings of the International Conference of Machine Learning.",
            "year": 2008
        },
        {
            "authors": [
                "Christoph Tillmann",
                "Stephan Vogel",
                "Hermann Ney",
                "Alex Zubiaga."
            ],
            "title": "A dp-based search using monotone alignments in statistical translation",
            "venue": "Proceedings of the Association for Computational Linguistics.",
            "year": 1997
        },
        {
            "authors": [
                "Kristina Toutanova",
                "Dan Klein",
                "Christopher D Manning",
                "Yoram Singer."
            ],
            "title": "Feature-rich part-of-speech tagging with a cyclic dependency network",
            "venue": "Conference of the North American Chapter of the Association for Computational",
            "year": 2003
        },
        {
            "authors": [
                "Stephan Vogel",
                "Hermann Ney",
                "Christoph Tillmann."
            ],
            "title": "HMM-based word alignment in statistical translation",
            "venue": "Proceedings of the 16th International Conference on Computational Linguistics (COLING).",
            "year": 1996
        },
        {
            "authors": [
                "Wolfgang Wahlster."
            ],
            "title": "Verbmobil: foundations of speech-to-speech translation",
            "venue": "Springer.",
            "year": 2000
        }
    ],
    "sections": [
        {
            "text": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1342\u20131352, October 25-29, 2014, Doha, Qatar. c\u00a92014 Association for Computational Linguistics"
        },
        {
            "heading": "1 Introduction",
            "text": "We introduce a simultaneous machine translation (MT) system that predicts unseen verbs and uses reinforcement learning to learn when to trust these predictions and when to wait for more input.\nSimultaneous translation is producing a partial translation of a sentence before the input sentence is complete, and is often used in important diplomatic settings. One of the first noted uses of human simultaneous interpretation was the Nuremberg trials after the Second World War. Siegfried Ramler (2009), the Austrian-American who organized the translation teams, describes the linguistic predictions\nand circumlocutions that translators would use to achieve a tradeoff between translation latency and accuracy. The audio recording technology used by those interpreters sowed the seeds of technology-assisted interpretation at the United Nations (Gaiba, 1998).\nPerforming real-time translation is especially difficult when information that comes early in the target language (the language you\u2019re translating to) comes late in the source language (the language you\u2019re translating from). A common example is when translating from a verb-final (sov) language (e.g., German or Japanese) to a verb-medial (svo) language, (e.g., English). In the example in Figure 1, for instance, the main verb of the sentence (in bold) appears at the end of the German sentence. An offline (or \u201cbatch\u201d) translation system waits until the end of the sentence before translating anything. While this is a reasonable approach, it has obvious limitations. Real-time, interactive scenarios\u2014such as online multilingual video conferences or diplomatic meetings\u2014require comprehensible partial interpretations before a sentence ends. Thus, a significant goal in interpretation is to make the translation as expeditious as possible.\nWe present three components for an sov-tosvo simultaneous mt system: a reinforcement learning framework that uses predictions to create expeditious translations (Section 2), a system to predict how a sentence will end (e.g., predicting the main verb; Section 4), and a metric that balances quality and expeditiousness (Section 3). We combine these components in a framework that learns when to begin translating sections of a sentence (Section 5).\nSection 6 combines this framework with a\n1342\ntranslation system that produces simultaneous translations. We show that our data-driven system can successfully predict unseen parts of the sentence, learn when to trust them, and outperform strong baselines (Section 7).\nWhile some prior research has approached the problem of simultaneous translation\u2014we review these systems in more detail in Section 8\u2014 no current model learns when to definitively begin translating chunks of an incomplete sentence. Finally, in Section 9, we discuss the limitations of our system: it only uses the most frequent source language verbs, it only applies to sentences with a single main verb, and it uses an idealized translation system. However, these limitations are not insurmountable; we describe how a more robust system can be assembled from these components."
        },
        {
            "heading": "2 Decision Process for Simultaneous Translation",
            "text": "Human interpreters learn strategies for their profession with experience and practice. As words in the source language are observed, a translator\u2014human or machine\u2014must decide whether and how to translate, while, for certain language pairs, simultaneously predicting future words. We would like our system to do the same. To this end, we model simultaneous mt as a Markov decision process (mdp) and use reinforcement learning to effectively combine predicting, waiting, and translating into a coherent strategy."
        },
        {
            "heading": "2.1 States: What is, what is to come",
            "text": "The state st represents the current view of the world given that we have seen t words of a source language sentence.1 The state contains information both about what is known and what is predicted based on what is known.\n1We use t to evoke a discrete version of time. We only allow actions after observing a complete source word.\nTo compare the system to a human translator in a decision-making process, the state is akin to the translator\u2019s cognitive state. At any given time, we have knowledge (observations) and beliefs (predictions) with varying degrees of certainty: that is, the state contains the revealed words x1:t of a sentence; the state also contains predictions about the remainder of the sentence: we predict the next word in the sentence and the final verb.\nMore formally, we have a prediction at time t of the next source language word that will appear, n(t)t+1, and for the final verb, v\n(t). For example, given the partial observation \u201cich bin mit dem\u201d, the state might contain a prediction that the next word, n(t)t+1, will be \u201cZug\u201d and that the final verb v(t) will be \u201cgefahren\u201d.\nWe discuss the mechanics of next-word and verb prediction further in Section 4; for now, consider these black boxes which, after observing every new source word xt, make predictions of future words in the source language. This representation of the state allows for a richer set of actions, described below, permitting simultaneous translations that outpace the source language input2 by predicting the future."
        },
        {
            "heading": "2.2 Actions: What our system can do",
            "text": "Given observed and hypothesized input, our simultaneous translation system must decide when to translate them. This is expressed in the form of four actions: our system can commit to a partial translation, predict the next word and use it to update the translation, predict the verb and use it to update the translation, or wait for more words.\nWe discuss each of these actions in turn before describing how they come together to incrementally translate an entire sentence:\nWait Waiting is the simplest action. It produces no output and allows the system to receive more input, biding its time, so that when it does choose to translate, the translation is based on more information.\nCommit Committing produces translation output: given the observed source sentence, produce the best translation possible.\n2Throughout, \u201cinput\u201d refers to source language input, and \u201coutput\u201d refers to target language translation.\nNext Word The next word action takes a prediction of the next source word and produces an updated translation based on that prediction, i.e., appending the predicted word to the source sentence and translating the new sentence.\nVerb Our system can also predict the source sentence\u2019s final verb (the last word in the sentence). When our system takes the verb action, it uses its verb prediction to update the translation using the prediction, by placing it at the end of the source sentence.\nWe can recreate a traditional batch translation system (interpreted temporally) by a sequence of wait actions until all input is observed, followed by a commit to the complete translation. Our system can commit to partial translations if it is confident, but producing a good translation early in the sentence often depends on missing information."
        },
        {
            "heading": "2.3 Translation Process",
            "text": "Having described the state, its components, and the possible actions at a state, we present the process in its entirety. In Figure 2, after each German word is received, the system arrives at a new state, which consists of the source input, target translation so far, and predictions of the unseen words. The translation system\nmust then take an action given information about the current state. The action will result in receiving and translating more source words, transitioning the system to the next state. In the example, for the first few source-language words, the translator lacks the confidence to produce any output due to insufficient information at the state. However, after State 3, the state shows high confidence in the predicted verb \u201cgefahren\u201d. Combined with the German input it has observed, the system is sufficiently confident to act on that prediction to produce English translation."
        },
        {
            "heading": "2.4 Consensus Translations",
            "text": "Three straightforward actions\u2014commit, next word, and verb\u2014all produce translations. These rely black box access to a translation (discussed in detail in Section 6): that is, given a source language sentence fragment, the translation system produces a target language sentence fragment.\nBecause these actions can happen more than once in a sentence, we must form a single consensus translation from all of the translations that we might have seen. If we have only one translation or if translations are identical, forming the consensus translation is trivial. But how should we resolve conflicting translations?\nAny time our system chooses an action that\nproduces output, the observed input (plus extra predictions in the case of next-word or verb), is passed into the translation system. That system then produces a complete translation of its input fragment.\nAny new words\u2014i.e., words whose target index is greater than the length of any previous translation\u2014are appended to the previous translation.3 Table 1 shows an example of forming these consensus translations.\nNow that we have defined how states evolve based on our system\u2019s actions, we need to know how to select which actions to take. Eventually, we will formalize this as a learned policy (Section 5) that maps from states to actions. First, however, we need to define a reward that measures how \u201cgood\u201d an action is."
        },
        {
            "heading": "3 Objective: What is a good simultaneous translation?",
            "text": "Good simultaneous translations must optimize two objectives that are often at odds, i.e., producing translations that are, in the end, accurate, and producing them in pieces that are presented expeditiously. While there are existing automated metrics for assessing translation quality (Papineni et al., 2002; Banerjee and Lavie, 2005; Snover et al., 2006), these must be modified to find the necessary compromise between translation quality and expeditiousness. That is, a good metric for simultaneous translation must achieve a balance between translating chunks early and translating accurately. All else being equal, maximizing either goal in isolation is trivial: for accurate translations, use a batch system and wait until the sentence is complete, translating it all at once; for a maximally expeditious translation, create monotone translations, translating each word as it appears, as in Tillmann et al. (1997) and Pytlik and Yarowsky (2006). The former is not simultaneous at all; the latter is mere word-for-word replacement and results in awkward, often unintelligible translations of distant language pairs.\nOnce we have predictions, we have an expanded array of possibilities, however. On one extreme, we can imagine a psychic translator\u2014\n3Using constrained decoding to enforce consistent translation prefixes would complicate our method but is an appealing extension.\none that can completely translate an imagined sentence after one word is uttered\u2014as an unobtainable system. On the other extreme is a standard batch translator, which waits until it has access to the utterer\u2019s complete sentence before translating anything.\nAgain, we argue that a system can improve on this by predicting unseen parts of the sentence to find a better tradeoff between these conflicting goals. However, to evaluate and optimize such a system, we must measure where a system falls on the continuum of accuracy versus expeditiousness.\nConsider partial translations in a twodimensional space, with time (quantized by the number of source words seen) increasing from left to right on the x axis and the bleu score (including brevity penalty against the reference length) on the y axis. At each point in time, the system may add to the consensus translation, changing the precision (Figure 3). Like an roc curve, a good system will be high and to the left, optimizing the area under the curve: the ideal system would produce points as high as possible immediately. A translation which is, in the end, accurate, but which is less expeditious, would accrue its score more slowly but outperform a similarly expeditious system which nevertheless translates poorly.\nAn idealized psychic system achieves this, claiming all of the area under the curve, as it would have a perfect translation instantly, having no need of even waiting for future input.4 A batch system has only a narrow (but tall) sliver to the right, since it translates nothing until all of the words are observed.\nFormally, let Q be the score function for a partial translation, x the sequentially revealed source words x1, x2, . . . , xT from time step 1 to T , and y the partial translations y1, y2, . . . , yT , where T is the length of the source language input. Each incremental translation yt has a bleu-n score with respect to a reference r. We apply the usual bleu brevity penalty to all the incremental translations (initially empty) to\n4One could reasonably argue that this is not ideal: a fluid conversation requires the prosody and timing between source and target to match exactly. Thus, a psychic system would provide too much information too quickly, making information exchange unnatural. However, we take the information-centric approach: more information faster is better.\nobtain latency-bleu (lbleu),\nQ(x,y) = 1 T \u2211 t bleu(yt, r) (1)\n+ T \u00b7 bleu(yT , r) The lbleu score is a word-by-word integral across the input source sentence. As each source word is observed, the system receives a reward based on the bleu score of the partial translation. lbleu, then, represents the sum of these T rewards at each point in the sentence. The score of a simultaneous translation is the sum of the scores of all individual segments that contribute to the overall translation.\nWe multiply the final bleu score by T to ensure good final translations in learned systems\nto compensate for the implicit bias toward low latency.5"
        },
        {
            "heading": "4 Predicting Verbs and Next Words",
            "text": "The next and verb actions depend on predictions of the sentence\u2019s next word and final verb; this section describes our process for predicting verbs and next words given a partial source language sentence.\nThe prediction of the next word in the source language sentence is modeled with a left-toright language model. This is (na\u0308\u0131vely) analogous to how a human translator might use his own \u201clanguage model\u201d to guess upcoming words to gain some speed by completing, for example, collocations before they are uttered. We use a simple bigram language model for next-word prediction. We use Heafield et al. (2013).\nFor verb prediction, we use a generative model that combines the prior probability of a particular verb v, p(v), with the likelihood of the source context at time t given that verb (namely, p(x1:t | v)), as estimated by a smoothed Kneser-Ney language model (Kneser and Ney, 1995). We use Pauls and Klein (2011). The prior probability p(v) is estimated by simple relative frequency estimation. The context, x1:t, consists of all words observed. We model p(x1:t | v) with verb-specific n-gram language models. The predicted verb v(t) at time t is then:\narg max v p(v) t\u220f i=1 p(xi | v, xi\u2212n+1:i\u22121) (2)\n5One could replace T with a parameter, \u03b2, to bias towards different kinds of simultaneous translations. As \u03b2 \u2192\u221e, we recover batch translation.\nwhere xi\u2212n+1:i\u22121 is the n\u22121-gram context. To narrow the search space, we consider only the 100 most frequent final verbs, where a \u201cfinal verb\u201d is defined as the sentence-final sequence of verbs and particles as detected by a German part-of-speech tagger (Toutanova et al., 2003).6"
        },
        {
            "heading": "5 Learning a Policy",
            "text": "We have a framework (states and actions) for simultaneous machine translation and a metric for assessing simultaneous translations. We now describe the use of reinforcement learning to learn a policy, a mapping from states to actions, to maximize lbleu reward.\nWe use imitation learning (Abbeel and Ng, 2004; Syed et al., 2008): given an optimal sequence of actions, learn a generalized policy that maps states to actions. This can be viewed as a cost-sensitive classification (Langford and Zadrozny, 2005): a state is represented as a feature vector, the loss corresponds to the quality of the action, and the output of the classifier is the action that should be taken in that state.\nIn this section, we explain each of these components: generating an optimal policy, representing states through features, and learning a policy that can generalize to new sentences."
        },
        {
            "heading": "5.1 Optimal Policies",
            "text": "Because we will eventually learn policies via a classifier, we must provide training examples to our classifier. These training examples come from an oracle policy \u03c0\u2217 that demonstrates the optimal sequence\u2014i.e., with maximal lbleu score\u2014of actions for each sequence. Using dynamic programming, we can determine such actions for a fixed translation model.7 From this oracle policy, we generate training examples for a supervised classifier. State st is represented as a tuple of the observed words x1:t, predicted verb v(t), and the predicted word n(t)t+1. We represent the state to a classifier as a feature vector \u03c6(x1:t, n (t) t+1, v (t)).\n6This has the obvious disadvantage of ignoring morphology and occasionally creating duplicates of common verbs that have may be associated with multiple particles; nevertheless, it provides a straightforward verb to predict.\n7This is possible for the limited class of consensus translation schemes discussed in Section 2.4."
        },
        {
            "heading": "5.2 Feature Representation",
            "text": "We want a feature representation that will allow a classifier to generalize beyond the specific examples on which it is trained. We use several general classes of features: features that describe the input, features that describe the possible translations, and features that describe the quality of the predictions.\nInput We include both a bag of words representation of the input sentence as well as the most recent word and bigram to model word-specific effects. We also use a feature that encodes the length of the source sentence.\nPrediction We include the identity of the predicted verb and next word as well as their respective probabilities under the language models that generate the predictions. If the model is confident in the prediction, the classifier can learn to more so trust the predictions.\nTranslation In addition to the state, we include features derived from the possible actions the system might take. This includes a bag of words representation of the target sentence, the score of the translation (decreasing the score is undesirable), the score of the current consensus translation, and the difference between the current and potential translation scores."
        },
        {
            "heading": "5.3 Policy Learning",
            "text": "Our goal is to learn a classifier that can accurately mimic the oracle\u2019s choices on previously unseen data. However, at test time, when we run the learned policy classifier, the learned policy\u2019s state distribution may deviate from the optimal policy\u2019s state distribution due to imperfect imitation, arriving in states not on the oracle\u2019s path. To address this, we use searn (Daume\u0301 III et al., 2009), an iterative imitation learning algorithm. We learn from the optimal policy in the first iteration, as in standard supervised learning; in the following iterations, we run an interpolated policy\n\u03c0k+1 = \u03c0k + (1\u2212 )\u03c0\u2217, (3)\nwith k as the iteration number and the mixing probability. We collect examples by asking the policy to label states on its path. The interpolated policy will execute the optimal action with probability 1\u2212 and the learned\npolicy\u2019s action with probability . In the first iteration, we have \u03c00 = \u03c0\u2217.\nMixing in the learned policy allows the learned policy to slowly change from the oracle policy. As it trains on these no-longer-perfect state trajectories, the state distribution at test time will be more consistent with the states used in training.\nsearn learns the policy by training a costsensitive classifier. Besides providing the optimal action, the oracle must also assign a cost to an action\nC(at,x) \u2261 Q(x, \u03c0\u2217(xt))\u2212Q(x, at(xt)), (4)\nwhere at(xt) represents the translation outcome of taking action at. The cost is the regret of not taking the optimal action."
        },
        {
            "heading": "6 Translation System",
            "text": "The focus of this work is to show that given an effective batch translation system and predictions, we can learn a policy that will turn this into a simultaneous translation system. Thus, to separate translation errors from policy errors, we perform experiments with a nearly optimal translation system we call an omniscient translator.\nMore realistic translation systems will naturally lower the objective function, often in ways that make it difficult to show that we can effectively predict the verbs in verb-final source languages. For instance, German to English translation systems often drop the verb; thus, predicting a verb that will be ignored by the translation system will not be effective.\nThe omniscient translator translates a source sentence correctly once it has been fed the appropriate source words as input. There are two edge cases: empty input yields an empty output, while a complete, correct source sentence returns the correct, complete translation. Intermediate cases\u2014where the input is either incomplete or incorrect\u2014require using an alignment. The omniscient translator assumes as input a reference translation r, a partial source language input x1:t and a corresponding partial output y. In addition, the omniscient translator assumes access to an alignment between r and x. In practice, we use the hmm aligner (Vogel et al., 1996; Och and Ney, 2003).\nWe first consider incomplete but correct inputs. Let y = \u03c4(x1:t) be the translator\u2019s output given a partial source input x1:t with translation y. Then, \u03c4(x1:t) produces all target words yj if there is a source word xi in the input aligned to those words\u2014i.e., (i, j) \u2208 ax,y\u2014and all preceding target words can be translated. (That translations must be contiguous is a natural requirement for human recipients of translations). In the case where yj is unaligned, the closest aligned target word to yj that has a corresponding alignment entry is aligned to xi; then, if xi is present in the input, yj appears in the output. Thus, our omniscient translation system will always produce the correct output given the correct input.\nHowever, our learned policy can make wrong predictions, which can produce partial translations y that do not match the reference. In this event, an incorrect source word x\u0303i produces incorrect target words y\u0303j , for all j : (i, j) \u2208 ax,y. These y\u0303j are sampled from the ibm Model 1 lexical probability table multiplied by the source language model y\u0303j \u223c Mult(\u03b8x\u0303i)pLM (x\u0303).\n8 Thus, even if we predict the correct verb using a next word action, it will be in the wrong position and thus generate a translation from the lexical probabilities. Since translations based on Model 1 probabilities are generally inaccurate, the omniscient translator will do very well when given correct input but will produce very poor translations otherwise."
        },
        {
            "heading": "7 Experiments",
            "text": "In this section, we describe our experimental framework and results from our experiments. From aligned data, we derive an omniscient translator. We use monolingual data in the source language to train the verb predictor and the next word predictor. From these features, we compute an optimal policy from which we train a learned policy."
        },
        {
            "heading": "7.1 Data sets",
            "text": "For translation model and policy training, we use data from the German-English Parallel \u201cdenews\u201d corpus of radio broadcast news (Koehn, 2000), which we lower-cased and stripped of\n8If a policy chooses an incorrect unaligned word, it has no effect on the output. Alignments are positionspecific, so \u201cwrong\u201d refers to position and type.\npunctuation. A total of 48, 601 sentence pairs are randomly selected for building our system. Of these, we use 70% (34, 528 pairs) for training word alignments.\nFor training the translation policy, we restrict ourselves to sentences that end with one of the 100 most frequent verbs (see Section 4). This results in a data set of 4401 training sentences and 1832 test sentences from the de-news data. We did this to narrow the search space (from thousands of possible, but mostly very infrequent, verbs).\nWe used 1 million words of news text from the Leipzig Wortschatz (Quasthoff et al., 2006) German corpus to train 5-gram language models to predict a verb from the 100 most frequent verbs.\nFor next-word prediction, we use the 18, 345 most frequent German bigrams from the training set to provide a set of candidates in a language model trained on the same set. We use frequent bigrams to reduce the computational cost of finding the completion probability of the next word."
        },
        {
            "heading": "7.2 Training Policies",
            "text": "In each iteration of searn, we learn a multi-class classifier to implement the policy. The specific learning algorithm we use is arow (Crammer et al., 2013). In the complete version of searn, the cost of each action is calculated as the highest expected reward starting at the current state minus the actual roll-out reward. However, computing the full roll-out reward is computationally very expensive. We thus use a surrogate binary cost: if the predicted action is the same as the optimal action, the cost is 0; otherwise, the cost is 1. We then run searn for five iterations. Results on the development data indicate that continuing for more iterations yields no benefit."
        },
        {
            "heading": "7.3 Policy Rewards on Test Set",
            "text": "In Figure 4, we show performance of the optimal policy vis-a\u0300-vis the learned policy, as well as the two baseline policies: the batch policy and the monotone policy. The x-axis is the percentage of the source sentence seen by the model, and the y-axis is a smoothed average of the reward as a function of the percentage of the sentence revealed. The monotone policy\u2019s performance is close to the optimal policy for\nthe first half of the sentence, as German and English have similar word order, though they diverge toward the end. Our learned policy outperforms the monotone policy toward the end and of course outperforms the batch policy throughout the sentence.\nFigure 5 shows counts of actions taken by each policy. The batch policy always commits at the end. The monotone policy commits at each position. Our learned policy has an action distribution similar to that of the optimal policy, but is slightly more cautious."
        },
        {
            "heading": "7.4 What Policies Do",
            "text": "Figure 6 shows a policy that, predicting incorrectly, still produces sensible output. The policy correctly intuits that the person discussed\nis Angela Merkel, who was the environmental minister at the time, but the policy uses an incorrectly predicted verb. Because of our poor translation model (Section 6), it renders this word as \u201cshown\u201d, which is a poor translation. However, it is still comprehensible, and the overall policy is similar to what a human would do: intuit the subject of the sentence from early clues and use a more general verb to stand in for a more specific one."
        },
        {
            "heading": "8 Related Work",
            "text": "Just as mt was revolutionized by statistical learning, we suspect that simultaneous mt will similarly benefit from this paradigm, both from a systematic system for simultaneous translation and from a framework for learning how to incorporate predictions.\nSimultaneous translation has been dominated by rule and parse-based approaches (Mima et al., 1998a; Ryu et al., 2006). In contrast, although Verbmobil (Wahlster, 2000) performs incremental translation using a statistical mt module, its incremental decisionmaking module is rule-based. Other recent approaches in speech-based systems focus on waiting until a pause to translate (Sakamoto et al., 2013) or using word alignments (Ryu et al., 2012) between languages to determine optimal translation units.\nUnlike our work, which focuses on prediction and learning, previous strategies for dealing with sov-to-svo translation use rule-based methods (Mima et al., 1998b) (for instance, passivization) to buy time for the translator to\nhear more information in a spoken context\u2014or use phrase table and reordering probabilities to decide where to translate with less delay (Fujita et al., 2013). Oda et al. (2014) is the most similar to our work on the translation side. They frame word segmentation as an optimization problem, using a greedy search and dynamic programming to find segmentation strategies that maximize an evaluation measure. However, unlike our work, the direction of translation was from from svo to svo, obviating the need for verb prediction. Simultaneous translation is more straightforward for languages with compatible word orders, such as English and Spanish (Fu\u0308gen, 2008).\nTo our knowledge, the only attempt to specifically predict verbs or any late-occurring terms (Matsubara et al., 2000) uses pattern matching on what would today be considered a small data set to predict English verbs for Japanese to English simultaneous mt.\nIncorporating verb predictions into the translation process is a significant component of our framework, though n-gram models strongly prefer highly frequent verbs. Verb prediction might be improved by applying the insights from psycholinguistics. Ferreira (2000) argues that verb lemmas are required early in sentence production\u2014prior to the first noun phrase argument\u2014and that multiple possible syntactic hypotheses are maintained in parallel as the sentence is produced. Schriefers et al. (1998) argues that, in simple German sentences, noninitial verbs do not need lemma planning at all. Momma et al. (2014), investigating these prior claims, argues that the abstract relationship between the internal arguments and verbs triggers selective verb planning."
        },
        {
            "heading": "9 Conclusion and Future Work",
            "text": "Creating an effective simultaneous translation system for sov to svo languages requires not only translating partial sentences, but also effectively predicting a sentence\u2019s verb. Both elements of the system require substantial refinement before they are usable in a real-world system.\nReplacing our idealized translation system is the most challenging and most important next step. Supporting multiple translation hypotheses and incremental decoding (Sankaran\net al., 2010) would improve both the efficiency and effectiveness of our system. Using data from human translators (Shimizu et al., 2014) could also add richer strategies for simultaneous translation: passive constructions, reordering, etc.\nVerb prediction also can be substantially improved both in its scope in the system and how we predict verbs. Verb-final languages also often place verbs at the end of clauses, and also predicting these verbs would improve simultaneous translation, enabling its effective application to a wider range of sentences. Instead predicting an exact verb early (which is very difficult), predicting a semantically close or a more general verb might yield interpretable translations.\nA natural next step is expanding this work to other languages, such as Japanese, which not only has sov word order but also requires tokenization and morphological analysis, perhaps requiring sub-word prediction."
        },
        {
            "heading": "Acknowledgments",
            "text": "We thank the anonymous reviewers, as well as Yusuke Miyao, Naho Orita, Doug Oard, and Sudha Rao for their insightful comments. This work was supported by NSF Grant IIS-1320538. Boyd-Graber is also partially supported by NSF Grant CCF-1018625. Daume\u0301 III and He are also partially supported by NSF Grant IIS0964681. Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the view of the sponsor."
        }
    ],
    "title": "Don\u2019t Until the Final Verb Wait: Reinforcement Learning for Simultaneous Machine Translation",
    "year": 2014
}