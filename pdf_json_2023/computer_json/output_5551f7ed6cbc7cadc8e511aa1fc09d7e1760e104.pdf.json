{
    "abstractText": "Anderson mixing (AM) is a classical method that can accelerate fixed-point iterations by exploring historical information. Despite the successful application of AM in scientific computing, the theoretical properties of AM are still under exploration. In this paper, we study the restarted version of the Type-I and Type-II AM methods, i.e., restarted AM. With a multi-step analysis, we give a unified convergence analysis for the two types of restarted AM and justify that the restarted Type-II AM can locally improve the convergence rate of the fixed-point iteration. Furthermore, we propose an adaptive mixing strategy by estimating the spectrum of the Jacobian matrix. If the Jacobian matrix is symmetric, we develop the short-term recurrence forms of restarted AM to reduce the memory cost. Finally, experimental results on various problems validate our theoretical findings.",
    "authors": [
        {
            "affiliations": [],
            "name": "Fuchao Wei"
        },
        {
            "affiliations": [],
            "name": "Chenglong Bao"
        },
        {
            "affiliations": [],
            "name": "Yang Liu"
        },
        {
            "affiliations": [],
            "name": "Guangwen Yang"
        }
    ],
    "id": "SP:1b6b1ccfa0a537f9c1feeda1d6ddd531d6bd31dc",
    "references": [
        {
            "authors": [
                "Donald G. Anderson"
            ],
            "title": "Iterative procedures for nonlinear integral equations",
            "venue": "J. Assoc. Comput. Mach.,",
            "year": 1965
        },
        {
            "authors": [
                "Donald G. Anderson"
            ],
            "title": "Comments on \u201cAnderson acceleration, mixing and extrapolation",
            "venue": "Numer. Algorithms,",
            "year": 2019
        },
        {
            "authors": [
                "Akash Arora",
                "David C. Morse",
                "Frank S. Bates",
                "Kevin D. Dorfman"
            ],
            "title": "Accelerating selfconsistent field theory of block polymers in a variable unit cell",
            "venue": "J. Chem. Phys.,",
            "year": 2017
        },
        {
            "authors": [
                "Rajendra Bhatia"
            ],
            "title": "Matrix analysis",
            "venue": "Grad. Texts in Math., 169. Springer-Verlag, New York,",
            "year": 1997
        },
        {
            "authors": [
                "Wei Bian",
                "Xiaojun Chen"
            ],
            "title": "Anderson acceleration for nonsmooth fixed point problems",
            "venue": "SIAM J. Numer. Anal.,",
            "year": 2022
        },
        {
            "authors": [
                "Wei Bian",
                "Xiaojun Chen",
                "C.T. Kelley"
            ],
            "title": "Anderson acceleration for a class of nonsmooth fixed-point problems",
            "venue": "SIAM J. Sci. Comput.,",
            "year": 2021
        },
        {
            "authors": [
                "Claude Brezinski",
                "Michela Redivo-Zaglia",
                "Yousef Saad"
            ],
            "title": "Shanks sequence transformations and Anderson acceleration",
            "venue": "SIAM Rev.,",
            "year": 2018
        },
        {
            "authors": [
                "Claude Brezinski",
                "Stefano Cipolla",
                "Michela Redivo-Zaglia",
                "Yousef Saad"
            ],
            "title": "Shanks and Anderson-type acceleration techniques for systems of nonlinear equations",
            "venue": "IMA J. Numer. Anal.,",
            "year": 2022
        },
        {
            "authors": [
                "Chih-Chung Chang",
                "Chih-Jen Lin"
            ],
            "title": "LIBSVM: A library for support vector machines",
            "venue": "ACM Trans. Intell. Syst. Technol.,",
            "year": 2011
        },
        {
            "authors": [
                "Xiaojun Chen",
                "C.T. Kelley"
            ],
            "title": "Convergence of the EDIIS algorithm for nonlinear equations",
            "venue": "SIAM J. Sci. Comput.,",
            "year": 2019
        },
        {
            "authors": [
                "Arthur I. Cohen"
            ],
            "title": "Rate of convergence of several conjugate gradient algorithms",
            "venue": "SIAM J. Numer. Anal.,",
            "year": 1972
        },
        {
            "authors": [
                "Yu-Hong Dai",
                "William W. Hager",
                "Klaus Schittkowski",
                "Hongchao Zhang"
            ],
            "title": "The cyclic Barzilai- Borwein method for unconstrained optimization",
            "venue": "IMA J. Numer. Anal.,",
            "year": 2006
        },
        {
            "authors": [
                "Hans De Sterck",
                "Yunhui He"
            ],
            "title": "On the asymptotic linear convergence speed of Anderson acceleration, Nesterov acceleration, and nonlinear GMRES",
            "venue": "SIAM J. Sci. Comput.,",
            "year": 2021
        },
        {
            "authors": [
                "Hans De Sterck",
                "Yunhui He"
            ],
            "title": "Linear asymptotic convergence of Anderson acceleration: fixedpoint analysis",
            "venue": "SIAM J. Matrix Anal. Appl.,",
            "year": 2022
        },
        {
            "authors": [
                "Hans De Sterck",
                "Yunhui He",
                "Oliver A"
            ],
            "title": "Krzysik. Anderson acceleration as a Krylov method with application to asymptotic convergence analysis",
            "year": 2023
        },
        {
            "authors": [
                "Stanley C. Eisenstat",
                "Howard C. Elman",
                "Martin H. Schultz"
            ],
            "title": "Variational iterative methods for nonsymmetric systems of linear equations",
            "venue": "SIAM J. Numer. Anal.,",
            "year": 1983
        },
        {
            "authors": [
                "Mark Embree"
            ],
            "title": "The tortoise and the hare restart GMRES",
            "venue": "SIAM Rev.,",
            "year": 2003
        },
        {
            "authors": [
                "Claire Evans",
                "Sara Pollock",
                "Leo G. Rebholz",
                "Mengying Xiao"
            ],
            "title": "A proof that Anderson acceleration improves the convergence rate in linearly converging fixed-point methods (but not in those converging quadratically)",
            "venue": "SIAM J. Numer. Anal.,",
            "year": 2020
        },
        {
            "authors": [
                "Haw-ren Fang",
                "Yousef Saad"
            ],
            "title": "Two classes of multisecant methods for nonlinear acceleration",
            "venue": "Numer. Linear Algebra Appl.,",
            "year": 2009
        },
        {
            "authors": [
                "Anqi Fu",
                "Junzi Zhang",
                "Stephen Boyd"
            ],
            "title": "Anderson accelerated Douglas-Rachford splitting",
            "venue": "SIAM J. Sci. Comput.,",
            "year": 2020
        },
        {
            "authors": [
                "Michael Garstka",
                "Mark Cannon",
                "Paul Goulart"
            ],
            "title": "Safeguarded Anderson acceleration for parametric nonexpansive operators",
            "venue": "In European Control Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Gene H. Golub",
                "Charles F. Van Loan"
            ],
            "title": "Matrix computations",
            "year": 2013
        },
        {
            "authors": [
                "Anne Greenbaum",
                "Zden\u011bk"
            ],
            "title": "Strako\u0161. Matrices that generate the same Krylov residual spaces. In Recent advances in iterative methods",
            "venue": "IMA Vol. Math. Appl.,",
            "year": 1994
        },
        {
            "authors": [
                "Huan He",
                "Shifan Zhao",
                "Yuanzhe Xi",
                "Joyce Ho",
                "Yousef Saad"
            ],
            "title": "GDA-AM: On the effectiveness of solving min-imax optimization via Anderson mixing",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Magnus R. Hestenes",
                "Eduard Stiefel"
            ],
            "title": "Methods of conjugate gradients for solving linear systems",
            "venue": "J. Research Nat. Bur. Standards,",
            "year": 1953
        },
        {
            "authors": [
                "Nicholas J. Higham",
                "Nata\u0161a"
            ],
            "title": "Strabi\u0107. Anderson acceleration of the alternating projections method for computing the nearest correlation matrix",
            "venue": "Numer. Algorithms,",
            "year": 2016
        },
        {
            "authors": [
                "Nguyenho Ho",
                "Sarah D. Olson",
                "Homer F. Walker"
            ],
            "title": "Accelerating the Uzawa algorithm",
            "venue": "SIAM J. Sci. Comput.,",
            "year": 2017
        },
        {
            "authors": [
                "C.T. Kelley"
            ],
            "title": "Numerical methods for nonlinear equations",
            "venue": "Acta Numer.,",
            "year": 2018
        },
        {
            "authors": [
                "Melanie L. Lenard"
            ],
            "title": "Convergence conditions for restarted conjugate gradient methods with inaccurate line",
            "venue": "searches. Math. Programming,",
            "year": 1976
        },
        {
            "authors": [
                "Vien V. Mai",
                "Mikael Johansson"
            ],
            "title": "Anderson acceleration of proximal gradient methods",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Thomas A. Manteuffel",
                "James S. Otto"
            ],
            "title": "On the roots of the orthogonal polynomials and residual polynomials associated with a conjugate gradient method",
            "venue": "Numer. Linear Algebra Appl.,",
            "year": 1994
        },
        {
            "authors": [
                "Yurii Nesterov"
            ],
            "title": "Lectures on convex optimization",
            "year": 2018
        },
        {
            "authors": [
                "Sara Pollock",
                "Leo G. Rebholz"
            ],
            "title": "Anderson acceleration for contractive and noncontractive operators",
            "venue": "IMA J. Numer. Anal.,",
            "year": 2021
        },
        {
            "authors": [
                "Sara Pollock",
                "Leo G. Rebholz",
                "Mengying Xiao"
            ],
            "title": "Anderson-accelerated convergence of Picard iterations for incompressible Navier-Stokes equations",
            "venue": "SIAM J. Numer. Anal.,",
            "year": 2019
        },
        {
            "authors": [
                "Florian A. Potra",
                "Hans Engler"
            ],
            "title": "A characterization of the behavior of the Anderson acceleration on linear problems",
            "venue": "Linear Algebra Appl.,",
            "year": 2013
        },
        {
            "authors": [
                "Phanisri P. Pratapa",
                "Phanish Suryanarayana",
                "John E. Pask"
            ],
            "title": "Anderson acceleration of the Jacobi iterative method: an efficient alternative to Krylov methods for large, sparse linear systems",
            "venue": "J. Comput. Phys.,",
            "year": 2016
        },
        {
            "authors": [
                "Peter Pulay"
            ],
            "title": "Convergence acceleration of iterative sequences. The case of SCF iteration",
            "venue": "Chem. Phys. Lett.,",
            "year": 1980
        },
        {
            "authors": [
                "Peter Pulay"
            ],
            "title": "Improved SCF convergence acceleration",
            "venue": "J. Comput. Chem.,",
            "year": 1982
        },
        {
            "authors": [
                "Thorsten Rohwedder",
                "Reinhold Schneider"
            ],
            "title": "An analysis for the DIIS acceleration method used in quantum chemistry calculations",
            "venue": "J. Math. Chem.,",
            "year": 2011
        },
        {
            "authors": [
                "Youcef Saad",
                "Martin H. Schultz"
            ],
            "title": "GMRES: a generalized minimal residual algorithm for solving nonsymmetric linear systems",
            "venue": "SIAM J. Sci. Statist. Comput.,",
            "year": 1986
        },
        {
            "authors": [
                "Yousef Saad"
            ],
            "title": "Variations on Arnoldi\u2019s method for computing eigenelements of large unsymmetric matrices",
            "venue": "Linear Algebra Appl.,",
            "year": 1980
        },
        {
            "authors": [
                "Yousef Saad"
            ],
            "title": "Krylov subspace methods for solving large unsymmetric linear systems",
            "venue": "Math. Comp.,",
            "year": 1981
        },
        {
            "authors": [
                "Yousef Saad"
            ],
            "title": "Iterative methods for sparse linear systems",
            "venue": "SIAM, Philadelphia, PA, second edition,",
            "year": 2003
        },
        {
            "authors": [
                "Yousef Saad"
            ],
            "title": "Numerical methods for large eigenvalue problems",
            "venue": "SIAM, Philadelphia,",
            "year": 2011
        },
        {
            "authors": [
                "Damien Scieur",
                "Alexandre d\u2019Aspremont",
                "Francis Bach"
            ],
            "title": "Regularized nonlinear acceleration",
            "venue": "Math. Program.,",
            "year": 2020
        },
        {
            "authors": [
                "Ke Sun",
                "Yafei Wang",
                "Yi Liu",
                "Yingnan Zhao",
                "Bo Pan",
                "Shangling Jui",
                "Bei Jiang",
                "Linglong Kong"
            ],
            "title": "Damped Anderson mixing for deep reinforcement learning: Acceleration, convergence, and stabilization",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Alex Toth",
                "C.T. Kelley"
            ],
            "title": "Convergence analysis for Anderson acceleration",
            "venue": "SIAM J. Numer. Anal.,",
            "year": 2015
        },
        {
            "authors": [
                "Alex Toth",
                "J. Austin Ellis",
                "Tom Evans",
                "Steven Hamilton",
                "C.T. Kelley",
                "Roger Pawlowski",
                "Stuart Slattery"
            ],
            "title": "Local improvement results for Anderson acceleration with inaccurate function evaluations",
            "venue": "SIAM J. Sci. Comput.,",
            "year": 2017
        },
        {
            "authors": [
                "Homer F. Walker",
                "Peng Ni"
            ],
            "title": "Anderson acceleration for fixed-point iterations",
            "venue": "SIAM J. Numer. Anal.,",
            "year": 2011
        },
        {
            "authors": [
                "Fuchao Wei",
                "Chenglong Bao",
                "Yang Liu"
            ],
            "title": "Stochastic Anderson mixing for nonconvex stochastic optimization",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Fuchao Wei",
                "Chenglong Bao",
                "Yang Liu"
            ],
            "title": "A class of short-term recurrence Anderson mixing methods and their applications",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Fuchao Wei",
                "Chenglong Bao",
                "Yang Liu",
                "Guangwen Yang"
            ],
            "title": "A variant of Anderson mixing with minimal memory size",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Yunan Yang"
            ],
            "title": "Anderson acceleration for seismic inversion",
            "venue": "Geophysics, 86(1):R99\u2013R108,",
            "year": 2021
        },
        {
            "authors": [
                "Junzi Zhang",
                "Brendan O\u2019Donoghue",
                "Stephen Boyd"
            ],
            "title": "Globally convergent type-I Anderson acceleration for nonsmooth fixed-point iterations",
            "venue": "SIAM J. Optim.,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "Keywords: Anderson mixing, fixed-point iteration, Krylov subspace methods, nonlinear equations, linear equations, unconstrained optimization"
        },
        {
            "heading": "1 Introduction",
            "text": "Anderson mixing (AM) [1], also known as Anderson acceleration [49], or Pulay mixing, DIIS method in quantum chemistry [37, 38, 39], is a classical extrapolation method for accelerating fixed-point iterations [7] and has wide applications in scientific computing [3, 36, 27, 34, 53]. Consider a fixedpoint problem\nx = g(x), (1.1)\nwhere x \u2208 Rd and g : Rd \u2192 Rd. The conventional fixed-point iteration\nxk+1 = g(xk), k = 0, 1, . . . , (1.2)\nconverges if g is contractive. To accelerate the convergence of (1.2), AM generates each iterate by the extrapolation of historical steps. Specifically, let mk \u2265 0 be the size of the used historical sequences at the k-th iteration. AM obtains xk+1 via\nxk+1 = (1\u2212 \u03b2k) mk\u2211 j=0 \u03b1 (j) k xk\u2212mk+j + \u03b2k mk\u2211 j=0 \u03b1 (j) k g(xk\u2212mk+j), (1.3)\nar X\niv :2\n30 7.\n02 06\n2v 1\n[ m\nat h.\nN A\n] 5\nJ ul\nwhere \u03b2k > 0 is the mixing parameter, and the extrapolation coefficients {\u03b1(j)k } mk j=0 are determined by solving a constrained least squares problem:\nmin {\u03b1(j)k } mk j=0 \u2225\u2225\u2225\u2225\u2225\u2225 mk\u2211 j=0 \u03b1 (j) k (g(xk\u2212mk+j)\u2212 xk\u2212mk+j) \u2225\u2225\u2225\u2225\u2225\u2225 2 s.t. mk\u2211 j=0 \u03b1 (j) k = 1. (1.4)\nThere are several approaches to choosing mk. For example, the full-memory AM chooses mk = k, i.e., using the whole historical sequences for one extrapolation; the limited-memory AM sets mk = min{m, k}, where m \u2265 1 is a constant integer.\nFor solving systems of equations where the fixed-point iterations are slow in convergence, AM is a practical alternative to Newton\u2019s method when handling the Jacobian matrices is difficult [28, 8]. It has been recognized that AM is a multisecant quasi-Newton method that implicitly updates the approximation of the inverse Jacobian matrix to satisfy multisecant equations [19]. Also, another type of AM called Type-I AM was introduced in [19]. Different from the original AM (also called Type-II AM), the Type-I AM directly approximates the Jacobian matrix. Both types of AM have been adapted to solve various fixed-point problems [26, 30, 54, 20, 46].\nMotivated by the promising numerical performance in many applications, the theoretical analysis of AM methods has become an important topic. For solving linear systems, it turns out that both types of full-memory AM methods are closely related to Krylov subspace methods [49]. However, for solving nonlinear problems, the theoretical properties of AM are still vague. For the Type-II AM, the known results in [47, 48, 10, 6] show that the limited-memory version has a local linear convergence rate that is no worse than that of the fixed-point iteration. Recent works [18, 33] further point out that the potential improvement of AM over fixed-point iterations depends on the quality of extrapolation, which is determined during iterations. For the Type-I AM, whether similar results hold remains unclear. It is worth noting that these theoretical results of the limited-memory AM follow the conventional one-step analysis, which may only have a partial assessment of the efficacy of AM, as also commented by Anderson in his review [2]. A fixed-point analysis in [14] reveals the continuity and differentiability properties of the Type-II AM iterations, but the convergence still lacks theoretical quantification. Besides, some new variants of AM have been developed and analyzed in different settings, e.g., see [45, 13, 50, 5, 52].\nIn this paper, we apply a multi-step analysis to investigate the long-term convergence behaviour of AM for solving nonlinear fixed-point problems. We focus on the restarted version of AM, i.e., restarted AM, where the method clears the historical information and restarts when some restarting condition holds. Restart is a common approach to improving the stability and robustness of AM [19, 54, 24, 21]. Compared with the limited-memory AM, the restarted AM has the benefit that it is more amenable to extending the relationship between AM methods and Krylov subspace methods to nonlinear problems. Based on such a relationship, we establish the convergence properties of both types of restarted AM methods which explain the efficacy of AM in practice. Furthermore, by investigating the properties of restarted AM, we obtain an efficient procedure to estimate the eigenvalues of the Jacobian matrix that is beneficial for choosing the mixing parameters; for problems with symmetric Jacobian matrices, we derive the short-term recurrence forms of AM. We highlight our main contributions as follows.\n1. We formulate the restarted Type-I and Type-II AM methods with certain restarting conditions and give a unified convergence analysis for both methods. Our multi-step analysis justifies that the restarted Type-II AM method can locally improve the convergence rate of the fixed-point iteration.\n2. We propose an adaptive mixing strategy that adaptively chooses the mixing parameters by estimating the eigenvalues of the Jacobian matrix. The eigenvalue estimation procedure originates from the projection method for eigenvalue problems and can be efficiently implemented using historical information. We also discuss the related theoretical properties.\n3. We show that the restarted AM methods can be simplified to have short-term recurrences if the Jacobian matrix is symmetric, which can reduce the memory cost. We give the convergence analysis of the short-term recurrence methods and develop the corresponding adaptive mixing strategy.\nNotations. The operator \u2206 denotes the forward difference, e.g., \u2206xk = xk+1 \u2212 xk. h\u2032 is the Jacobian of a function h : Rd \u2192 Rd. For every matrix A, range(A) is the subspace spanned by the columns of A; Kk(A, v) := span{v,Av, . . . , Ak\u22121v} is the k-th Krylov subspace generated by A and a vector v; S(A) := (A + AT)/2 is the symmetric part of A; \u03c3(A) is the spectrum of A; \u2225A\u22252 is the spectral norm of A; \u2225x\u2225A := (xTAx)1/2 is the A-norm if A is symmetric positive definite (SPD). Pk denotes the space of polynomials of degree not exceeding k."
        },
        {
            "heading": "2 Two types of Anderson mixing methods",
            "text": "We re-interpret each iteration of the Type-I/Type-II AM method as a two-step procedure following [50]. Define rk = g(xk) \u2212 xk to be the residual at xk. The historical sequences are stored as two matrices Xk, Rk \u2208 Rd\u00d7mk (mk \u2265 1):\nXk = (\u2206xk\u2212mk ,\u2206xk\u2212mk+1, . . . ,\u2206xk\u22121), Rk = (\u2206rk\u2212mk ,\u2206rk\u2212mk+1, . . . ,\u2206rk\u22121). (2.1)\nBoth Type-I and Type-II AM obtain xk+1 via a projection step and a mixing step:\nx\u0304k = xk \u2212Xk\u0393k, (Projection step) r\u0304k = rk \u2212Rk\u0393k,\nxk+1 = x\u0304k + \u03b2kr\u0304k, (Mixing step)\n(2.2)\nwhere \u03b2k > 0 is the mixing parameter. For convenience, let Zk := Xk for the Type-I AM and Zk := Rk for the Type-II AM, then \u0393k is determined by the condition\nr\u0304k \u22a5 range(Zk). (2.3) Assume ZTk Rk is nonsingular. From (2.2), xk+1 = xk + \u03b2krk \u2212 (Xk + \u03b2kRk) \u0393k. With the solution \u0393k from (2.3), we obtain\nxk+1 = xk +Gkrk, where Gk = \u03b2kI \u2212 (Xk + \u03b2kRk)(ZTk Rk)\u22121ZTk . (2.4)\nFor the Type-I AM, Gk satisfies Gk = J \u22121 k , where Jk solves minJ \u2225J \u2212 \u03b2\u22121k I\u2225F s.t. JXk = \u2212Rk; For the Type-II AM, Gk solves minG \u2225G\u2212 \u03b2kI\u2225F s.t. GRk = \u2212Xk. Hence, both methods can be viewed as multisecant quasi-Newton methods [19]. Remark 2.1. For the Type-II method, the condition (2.3) is equivalent to \u0393k = argmin\u0393\u2208Rmk \u2225rk \u2212 Rk\u0393\u22252. Let \u0393k = (\u0393(1)k , . . . ,\u0393 (mk) k )\nT \u2208 Rmk . The extrapolation coefficients {\u03b1(j)k } can be obtained from \u0393k: \u03b1 (0) k = \u0393 (1) k , \u03b1 (j) k = \u0393 (j+1) k \u2212 \u0393 (j) k (j = 1, . . . ,mk \u2212 1), \u03b1 (mk) k = 1 \u2212 \u0393\n(mk) k . Then rk \u2212 Rk\u0393k =\u2211mk\nj=0 \u03b1 (j) k rk\u2212mk+j . The above formulation of Type-II AM is equivalent to that given by (1.3) and\n(1.4)."
        },
        {
            "heading": "3 Restarted Anderson mixing",
            "text": "Initialized with m0 = 0, the restarted AM sets mk = mk\u22121+1 if no restart occurs and sets mk = 0 if a restarting condition is satisfied, similar to the restarted GMRES [40]. Thus, the restarting conditions are critical for the method. To define the restarting conditions, we first construct modified historical sequences. Such modification does not alter the iterates but is essential for the following analysis."
        },
        {
            "heading": "3.1 The AM update with modified historical sequences",
            "text": "Consider the nontrivial case that mk > 0. Note that the Gk in (2.4) does not change if we replace Xk, Rk by Pk := XkS \u22121 k , Qk := RkS \u22121 k , where Sk \u2208 Rmk\u00d7mk is nonsingular. So we can choose some suitable transformation Sk to reformulate the AM update. We construct the modified historical sequences Pk = (pk\u2212mk+1, . . . , pk), Qk = (qk\u2212mk+1, . . . , qk) in a recursive way. Let Vk := Pk for the Type-I AM and Vk := Qk for the Type-II AM. Assume that det(ZTj Rj) \u0338= 0 for j = k\u2212mk+1, . . . , k. The AM update with modified historical sequences consists of the following two steps.\nStep 1: Modified vector pair. If mk = 1, then pk = \u2206xk\u22121, qk = \u2206rk\u22121. If mk \u2265 2, we set the vector pair pk, qk as\npk = \u2206xk\u22121 \u2212 Pk\u22121\u03b6k, qk = \u2206rk\u22121 \u2212Qk\u22121\u03b6k, (3.1)\nwhere \u03b6k = (\u03b6 (1) k , . . . , \u03b6 (mk\u22121) k ) T is determined by qk \u22a5 range(Vk\u22121). Step 2: AM update. We obtain xk+1 via\nx\u0304k = xk \u2212 Pk\u0393k, r\u0304k = rk \u2212Qk\u0393k, xk+1 = x\u0304k + \u03b2kr\u0304k, (3.2)\nwhere \u0393k = (\u0393 (1) k , . . . ,\u0393 (mk) k ) T is determined by r\u0304k \u22a5 range(Vk). It can be verified by induction that the above process produces the same iterates as (2.4). To facilitate the analysis, we give explicit procedures to obtain \u03b6k and \u0393k. Let Zk = (zk\u2212mk+1, . . . , zk), Vk = (vk\u2212mk+1, . . . , vk). We first describe the procedure to compute \u03b6k and qk. Define q 0 k = \u2206rk\u22121. For j = 1, 2, . . . ,mk \u2212 1, the procedure computes \u03b6 (j) k and the intermediate vector qjk sequentially:\n\u03b6 (j) k =\nvTk\u2212mk+jq j\u22121 k\nvTk\u2212mk+jqk\u2212mk+j , qjk = q j\u22121 k \u2212 qk\u2212mk+j\u03b6 (j) k . (3.3)\nThen qk = q mk\u22121 k . Next, \u0393k and r\u0304k can be computed similarly. Define r 0 k = rk. For j = 1, 2, . . . ,mk, the \u0393 (j) k and the intermediate vector r j k are computed sequentially:\n\u0393 (j) k =\nvTk\u2212mk+jr j\u22121 k\nvTk\u2212mk+jqk\u2212mk+j , rjk = r j\u22121 k \u2212 qk\u2212mk+j\u0393 (j) k . (3.4)\nThen r\u0304k = r mk k . Procedures (3.3) and (3.4) are reminiscent of the modified Gram-Schmidt orthogonalization process that is recommended for the implementation of Type-II AM [2]. The next proposition shows the correctness of the above procedures.\nProposition 3.1. Suppose that det(ZTj Rj) \u0338= 0 for j = k\u2212mk +1, . . . , k. Then the procedures (3.3) and (3.4) are well defined, and the following properties hold:\n1. Xk = PkSk, Rk = QkSk, where Sk is unit upper triangular;\n2. V Tk Qk is lower triangular;\n3. r\u0304k \u22a5 range(Vk).\nThe scheme (3.2) produces the same {xj}k+1j=k\u2212mk+1 as the original AM update (2.4).\nThe proof is given in Appendix A.1. It is worth noting that our formulation of the restarted AM focuses on theoretical analysis. Better implementations are needed in some specific scenarios, e.g., parallel computing."
        },
        {
            "heading": "3.2 Restarting conditions",
            "text": "Let \u03c4 \u2208 (0, 1), \u03b7 > 0, and m \u2208 (0, d] is an integer. Following [52], the restart criterion is related to the following conditions:\nmk \u2264 m, (3.5) |vTk qk| \u2265 \u03c4 |vTk\u2212mk+1qk\u2212mk+1|, (3.6) \u2225rk\u22252 \u2264 \u03b7\u2225rk\u2212mk\u22252. (3.7)\nIf any condition in (3.5)-(3.7) is violated during the iteration, set mk = 0 and restart the method. Details of the restarted AM are given in Algorithm 1. Next, we explain the rationale behind the above three conditions.\nThe first condition (3.5) limits the size of the historical sequences, which plays an important role in bounding the accumulated high-order errors in the convergence analysis. The second condition (3.6) ensures the nonsingularity of V Tk Qk as long as v T k\u2212mk+1qk\u2212mk+1 \u0338= 0. This is because V T k Qk is lower triangular and the diagonal elements {vTj qj}kj=k\u2212mk+1 are nonzero due to (3.6). Also, (3.6) controls the condition number of V Tk Qk by the following lower bound:\n|vTk\u2212mk+1qk\u2212mk+1| |vTk qk| = |eT1 V Tk Qke1| |eTmkV Tk Qkemk | \u2264 \u2225V Tk Qk\u22252\u2225\n( V Tk Qk )\u22121 \u22252, (3.8) where ej denotes the j-th column of the identity matrix Imk . Thus, a too-small |vTk qk| can cause numerical instability and we have to restart the AM method. The third condition (3.7) is to control the growth degree of the residuals, which avoids the problematic behaviour of AM and can be seen as a safeguard condition. Moreover, as shown in our proof, the conditions (3.5)-(3.7) can lead to the boundedness of the extrapolation coefficients, which is a critical assumption in [47]."
        },
        {
            "heading": "4 Convergence analysis",
            "text": "In this section, we give a unified convergence analysis for the restarted AM methods described in Algorithm 1. We first recall the relationship between AM methods and the Krylov subspace methods for solving linear systems. Let xAk and x G k denote the k-th iterate of Arnoldi\u2019s method [42] and the k-th iterate of GMRES [40], respectively. We summarize the results if (1.1) is linear.\nAlgorithm 1 Restarted Anderson mixing for solving the fixed-point problem (1.1). The Type-I method: vj := pj (j \u2265 1); the Type-II method: vj := qj (j \u2265 1). Input: x0 \u2208 Rd, \u03b2k > 0,m \u2208 Z+, \u03c4 \u2208 (0, 1), \u03b7 > 0 Output: x \u2208 Rd 1: m0 = 0 2: for k = 0, 1, . . . , until convergence do 3: rk = g(xk)\u2212 xk 4: if mk > m or \u2225rk\u22252 > \u03b7\u2225rk\u2212mk\u22252 then 5: mk = 0 6: end if 7: if mk > 0 then 8: pk = xk \u2212 xk\u22121, qk = rk \u2212 rk\u22121 9: for j = 1, . . . ,mk \u2212 1 do\n10: \u03b6 = ( vTk\u2212mk+jqk ) / ( vTk\u2212mk+jqk\u2212mk+j ) 11: pk = pk \u2212 pk\u2212mk+j\u03b6, qk = qk \u2212 qk\u2212mk+j\u03b6 12: end for 13: if |vTk qk| < \u03c4 |vTk\u2212mk+1qk\u2212mk+1| then 14: mk = 0 15: end if 16: end if 17: x\u0304k = xk, r\u0304k = rk 18: for j = 1, . . . ,mk do\n19: \u03b3 = ( vTk\u2212mk+j r\u0304k ) / ( vTk\u2212mk+jqk\u2212mk+j ) 20: x\u0304k = x\u0304k \u2212 pk\u2212mk+j\u03b3, r\u0304k = r\u0304k \u2212 qk\u2212mk+j\u03b3 21: end for 22: xk+1 = x\u0304k + \u03b2kr\u0304k 23: mk+1 = mk + 1 24: end for 25: return xk\nProposition 4.1. Consider the fixed-point problem (1.1) with g(x) = (I \u2212A)x+ b, where A \u2208 Rd\u00d7d is nonsingular and b \u2208 Rd. Let {xk} be the sequence generated by the full-memory Type-I/Type-II AM method with nonzero mixing parameters. If det(ZTj Rj) \u0338= 0 for j = 1, . . . , k, then the following relations hold:\n1. Rk = \u2212AXk, range(Xk) = Kk(A, r0);\n2. for the Type-I AM method, x\u0304k = x A k provided that x0 = x A 0 ;\n3. for the Type-II AM method, x\u0304k = x G k provided that x0 = x G 0 .\nFurthermore, if A is positive definite and rj \u0338= 0, j = 0, . . . , k, then det(ZTj Rj) \u0338= 0, j = 1, . . . , k; the constructions of the modified historical sequences Pk and Qk are well-defined, and\nQk = \u2212APk, range(Pk) = range(Xk) = Kk(A, r0). (4.1)\nWe give the proof in Appendix B.1. Properties 1-3 are known results [49]. Proposition 3.1 and Proposition 4.1 establish the relationship between the restarted AM and Krylov subspace methods in the linear case.\nNow, we study the convergence properties of the restarted AM for solving nonlinear problems. Rewriting the fixed-point problem (1.1) as h(x) := x\u2212 g(x) = 0, we make the following assumptions on h:\nAssumption 4.2. (i) There exists x\u2217 such that h(x\u2217) = 0; (ii) h is Lipschitz continuously differentiable in a neighbourhood of x\u2217; (iii) The Jacobian h\u2032(x\u2217) is positive definite, i.e., all the eigenvalues of S(h\u2032(x\u2217)) are positive.\nFrom Assumption 4.2, there exist positive constants \u03c1\u0302, \u03ba\u0302, \u00b5, and L such that for all x \u2208 B\u03c1\u0302(x\u2217) := {z \u2208 Rd|\u2225z \u2212 x\u2217\u22252 \u2264 \u03c1\u0302}, the following relations hold:\n\u00b5\u2225y\u22252 \u2264 \u2225h\u2032(x)y\u22252 \u2264 L\u2225y\u22252, \u2200y \u2208 Rd; (4.2) \u00b5\u2225y\u222522 \u2264 yTh\u2032(x)y \u2264 L\u2225y\u222522, \u2200y \u2208 Rd; (4.3) \u2225h(x)\u2212 h\u2032(x\u2217)(x\u2212 x\u2217)\u22252 \u2264 1\n2 \u03ba\u0302\u2225x\u2212 x\u2217\u222522. (4.4)\nInspired by the proofs of the restarted conjugate gradient methods [11, 29] and the cyclic BarzilaiBorwein method [12], we establish the convergence properties of the restarted AM methods from their properties in the linear problems. To achieve this goal, we first introduce the local linear model of h around x\u2217: h\u0302(x) = h\u2032(x\u2217)(x\u2212 x\u2217), (4.5) which deviates from h(x) by at most a second-order term 12 \u03ba\u0302\u2225x\u2212x\u2217\u222522 in B\u03c1\u0302(x\u2217) from (4.4). Then we construct two sequences of iterates {xk} and {x\u0302k}, which are associated with solving h(x) = 0 and h\u0302(x) = 0, respectively.\nDefinition 4.3. Let the mixing parameters {\u03b2k} satisfy \u03b2 \u2264 |\u03b2k| \u2264 \u03b2\u2032 for positive constants \u03b2 and \u03b2\u2032. The sequences {xk} and {x\u0302k} are generated by two processes:\n(i) Process I: Solve the fixed-point problem (1.1) with the restarted Type-I/Type-II AM method (see Algorithm 1), and the resulting sequence is {xk}.\n(ii) Process II: In each interval between two successive restarts in Process I, apply the full-memory Type-I/Type-II AM with modified historical sequences to solve the linear system h\u0302(x) = 0. Specifically, let mk and \u03b2k be the same ones in Process I and define r\u0302k = \u2212h\u0302(x\u0302k). The iterates are given as follows:\nx\u0302k = xk, and x\u0302k+1 = x\u0302k + \u03b2kr\u0302k, if mk = 0; x\u0302k+1 = x\u0302k + \u03b2kr\u0302k \u2212 ( P\u0302k + \u03b2kQ\u0302k ) \u0393\u0302k, if mk > 0,\n(4.6)\nwhere \u0393\u0302k is chosen such that r\u0302k \u2212 Q\u0302k\u0393\u0302k \u22a5 range(V\u0302k). Here P\u0302k = (p\u0302k\u2212mk+1, . . . , p\u0302k) and Q\u0302k = (q\u0302k\u2212mk+1, . . . , q\u0302k) are the modified historical sequences. Let V\u0302k = P\u0302k if the Type-I method is used in Process I, and V\u0302k = Q\u0302k if the Type-II method is used in Process I. Then, p\u0302k = \u2206x\u0302k\u22121, q\u0302k = \u2206r\u0302k\u22121, if mk = 1; p\u0302k = \u2206x\u0302k\u22121 \u2212 P\u0302k\u22121\u03b6\u0302k, q\u0302k = \u2206r\u0302k\u22121 \u2212 Q\u0302k\u22121\u03b6\u0302k, if mk \u2265 2, where \u03b6\u0302k is chosen such that q\u0302k \u22a5 range(V\u0302k\u22121).\nThe next lemma compares the outputs of the above two processes.\nLemma 4.4. Suppose that Assumption 4.2 holds for the fixed-point problem (1.1). For the sequences {xk} and {x\u0302k} in Definition 4.3, if x0 is sufficiently close to x\u2217 and \u2225h(xj)\u22252 \u2264 \u03b70\u2225h(x0)\u22252, j = 0, . . . , k, where \u03b70 > 0 is a constant, then\n\u2225rk \u2212 r\u0302k\u22252 = \u03ba\u0302 \u00b7 O(\u2225xk\u2212mk \u2212 x\u2217\u222522), (4.7) \u2225xk+1 \u2212 x\u0302k+1\u22252 = \u03ba\u0302 \u00b7 O(\u2225xk\u2212mk \u2212 x\u2217\u222522). (4.8)\nThe proof is given in Appendix B.2 due to space limitations. Since Process II is closely related to Krylov subspace methods from Proposition 4.1, Lemma 4.4 extends this relationship to the nonlinear case. When certain assumptions hold, \u2225xk \u2212 x\u0302k\u22252 is bounded by a second-order term. Intuitively, we can obtain the convergence of {xk} for nonlinear problems from the convergence of {x\u0302k} for the corresponding linear problems. If {x\u0302k} converges linearly (not quadratically), it is expected that {xk} has a similar convergence rate to {x\u0302k} provided that x0 is sufficiently close to x\u2217.\nTheorem 4.5. Suppose that Assumption 4.2 holds for the fixed-point problem (1.1). Let {xk} and {rk} denote the iterates and residuals of the restarted AM, A := I \u2212 g\u2032(x\u2217), \u03b8k := \u2225I \u2212 \u03b2kA\u22252, and \u03b70 > 0 is a constant. We assume \u03b2j \u2208 [\u03b2, \u03b2\u2032] (j \u2265 0) for some positive constants \u03b2 and \u03b2\u2032. The following results hold.\n1. For the Type-I AM, let \u03c0k be the orthogonal projector onto Kmk(A, rk\u2212mk) and Ak := \u03c0kA\u03c0k. Ak|Kmk (A,rk\u2212mk ) denotes the restriction of Ak to Kmk(A, rk\u2212mk). If \u2225rj\u22252 \u2264 \u03b70\u2225r0\u22252 (0 \u2264 j \u2264 k) and x0 is sufficiently close to x \u2217, then\n\u2225xk+1 \u2212 x\u2217\u22252 \u2264 \u03b8k \u221a\n1 + \u03b32k\u03ba 2 k minp\u2208Pmk\np(0)=1\n\u2225p(A)(xk\u2212mk \u2212 x\u2217)\u22252 + \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u222522), (4.9)\nwhere \u03b3k = \u2225\u03c0kA(I \u2212 \u03c0k)\u22252 \u2264 L, and \u03bak = \u2225(Ak|Kmk (A,rk\u2212mk )) \u22121\u22252 \u2264 1/\u00b5.\n2. For the Type-II AM, if \u2225rj\u22252 \u2264 \u03b70\u2225r0\u22252 (0 \u2264 j \u2264 k + 1) and x0 is sufficiently close to x\u2217, then\n\u2225rk+1\u22252 \u2264 \u03b8k min p\u2208Pmk p(0)=1 \u2225p(A)rk\u2212mk\u22252 + \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u222522). (4.10)\nAlternatively, letting \u03b8 \u2208 [( 1 \u2212 \u00b52\nL2\n)1/2 , 1 ) be a constant, if \u03b8j = \u2225I \u2212 \u03b2jA\u22252 \u2264 \u03b8 (j \u2265 0) and x0 is\nsufficiently close to x\u2217, then (4.10) holds. 3. For either method, if the aforementioned assumptions hold and mk = d, then \u2225xk+1 \u2212 x\u2217\u22252 = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u222522), namely, (d+ 1)-step quadratic convergence.\nThe proof is given in Appendix B.3, which is based on Lemma 4.4, Proposition 4.1, and the convergence properties of Krylov subspace methods. Results (4.9) and (4.10) characterize the longterm convergence behaviours of both restarted AM methods for solving nonlinear equations h(x) = 0, where h satisfies Assumption 4.2. Remark 4.6. The assumption that x0 is sufficiently close to x \u2217 is common for the local analysis of an iterative method [47, 28, 8]. Similar to [12], since an explicit bound for \u2225x0 \u2212 x\u2217\u22252 is rather cumbersome and not very useful in practice, we omit it here for conciseness. Besides, we do not assume g to be contractive here. The critical point is the positive definiteness of the Jacobian h\u2032(x\u2217), without which there is no convergence guarantee even for solving linear systems [49, 35, 17]. Remark 4.7. Ifmk is large and x0 is sufficiently close to x \u2217, the convergence rates of both restarted AM methods are dominated by the minimization problems in (4.9) and (4.10), which have been extensively\nstudied in the context of Krylov subspace methods [42, 16, 23, 43]. For j \u2265 0, define uj = xj \u2212 x\u2217 for the Type-I method, and uj = rj for the Type-II method. Note that \u2225\u2225I \u2212 \u00b5 L2 A \u2225\u2225 2 \u2264 \u03b8 := ( 1 \u2212 \u00b52 L2 )1/2 (see Lemma B.1 in Appendix B.3). Choosing p(A) = ( I \u2212 \u00b5 L2 A )mk , it follows that\nmin p\u2208Pmk p(0)=1 \u2225p(A)uk\u2212mk\u22252 \u2264 minp\u2208Pmk p(0)=1 \u2225p(A)\u22252\u2225uk\u2212mk\u22252 \u2264 \u03b8mk \u2225uk\u2212mk\u22252 . (4.11)\nWith more properties about A, we may choose other polynomials to sharpen the upper bound in (4.11). We give a refined result in Remark 6.3 when A is symmetric.\nNow, we consider the case that the fixed-point map g is a contraction. Specifically, we make the following assumptions on g, which are similar to those in [47, 18].\nAssumption 4.8. The fixed-point map g : Rd \u2192 Rd has a fixed point x\u2217. In the local region B\u03c1\u0302(x\u2217) := {z \u2208 Rd|\u2225z \u2212 x\u2217\u22252 \u2264 \u03c1\u0302} for some constant \u03c1\u0302 > 0, g is Lipschitz continuously differentiable, and there are constants \u03ba \u2208 (0, 1) and \u03ba\u0302 > 0 such that\n\u2022 \u2225g(y)\u2212 g(x)\u22252 \u2264 \u03ba\u2225y \u2212 x\u22252 for every x, y \u2208 B\u03c1\u0302(x\u2217);\n\u2022 \u2225g\u2032(y)\u2212 g\u2032(x)\u22252 \u2264 \u03ba\u0302\u2225y \u2212 x\u22252 for every x, y \u2208 B\u03c1\u0302(x\u2217).\nIn fact, we show Assumption 4.8 is a sufficient condition for Assumption 4.2.\nLemma 4.9. Suppose Assumption 4.8 holds for the fixed-point problem (1.1). Let h(x) := x\u2212 g(x). Then h satisfies Assumption 4.2. In B\u03c1\u0302(x\u2217), the Lipschitz constant of h\u2032 is \u03ba\u0302; for (4.2) and (4.3), the constants are \u00b5 = 1\u2212 \u03ba, L = 1 + \u03ba.\nThe proof is given in Appendix B.4. Based on Lemma 4.9 and Theorem 4.5, we obtain the following corollary for the Type-II AM.\nCorollary 4.10. Suppose that Assumption 4.8 holds for the fixed-point problem (1.1). Let {xk} and {rk} denote the iterates and residuals of the restarted Type-II AM with \u03b2k = 1 (k \u2265 0). If x0 is sufficiently close to x\u2217, then\n\u2225rk+1\u22252 \u2264 \u03ba min p\u2208Pmk p(0)=1 \u2225p(A)rk\u2212mk\u22252 + \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u222522), (4.12)\nwhere A := I \u2212 g\u2032(x\u2217). If mk = d, then \u2225xk+1 \u2212 x\u2217\u22252 = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u222522).\nRemark 4.11. The R-linear convergence of the limited-memory Type-II AM has been established in [47]: Under Assumption 4.8 and assuming that \u2211mk\nj=0 |\u03b1 (j) k | is bounded, it is proved that for \u03ba\u0303 \u2208 (\u03ba, 1),\nif x0 is sufficiently close to x \u2217, then \u2225rk\u22252 \u2264 \u03ba\u0303k\u2225r0\u22252. (4.13) However, as noted by Anderson [2], (4.13) does not show the advantage of AM over the fixed-point iteration (1.2) since the latter converges Q-linearly with Q-factor \u03ba. In [18], an improved bound is obtained:\n\u2225rk+1\u22252 \u2264 sk(1\u2212 \u03b2k + \u03ba\u03b2k)\u2225rk\u22252 + m\u2211 j=0 O(\u2225rk\u2212j\u222522), (4.14)\nwhere k \u2265 m and sk := \u2225r\u0304k\u22252/\u2225rk\u22252. If \u03b2k = 1, (4.14) improves (4.13) since sk \u2264 1. However, the quality of extrapolation, namely sk, is difficult to estimate in advance. The recent analysis in [33]\nrefines the higher-order terms in (4.14), but leaves the issue about sk unaddressed. For the restarted Type-II AM, Corollary 4.10 shows that its convergence rate is dominated by the first term on the righthand side of (4.12). Using p(A) = (I\u2212A)mk , (4.12) leads to \u2225rk+1\u22252 \u2264 \u03bamk+1\u2225rk\u2212mk\u22252+\u03ba\u0302O(\u2225xk\u2212mk\u2212 x\u2217\u222522) that is comparable to the fixed-point iteration (1.2). Nonetheless, due to the optimality, the polynomial that minimizes \u2225p(A)rk\u2212mk\u22252 corresponds to the mk-step GMRES iterations and can often provide a much better bound than (I \u2212A)mk [40, 23], which justifies the acceleration by TypeII AM in practice. Therefore, our multi-step analysis provides a better assessment of the efficacy of Type-II AM than previous works.\nRemark 4.12. Though the numerical experiments in [14] suggest that the limited-memory AM can converge faster than the restarted AM with the same m, the theoretical properties of the limitedmemory AM are much more vague, even in the linear case [15]. We leave the analysis for the limited-memory AM as our future work."
        },
        {
            "heading": "5 Adaptive mixing strategy",
            "text": "As shown in Theorem 4.5, the choice of \u03b2k directly affects the factor \u03b8k in (4.9) and (4.10). If g is not contractive, a proper \u03b2k is required to ensure the numerical performance of AM [19, 2]. However, tuning \u03b2k with a grid search can be costly in practice. In this section, we explore the properties of restarted AM to develop an efficient procedure to estimate the eigenvalues of h\u2032(x\u2217), based on which we can choose \u03b2k adaptively.\nWe start from the linear case to better explain how to estimate the eigenvalues. Let g(x) = (I \u2212 A)x+ b in the fixed-point problem (1.1), where A \u2208 Rd\u00d7d is positive definite and b \u2208 Rd. Then h(x) = Ax\u2212 b. Using the historical information in the restarted AM, we apply a projection method [44] to estimate the spectrum of A:\nu \u2208 range(Qk), (A\u2212 \u03bbI)u \u22a5 range(Vk), (5.1)\nwhere u \u2208 Rd is an approximate eigenvector of A sought in range(Qk), and \u03bb \u2208 R is an eigenvalue estimate. The orthogonality condition in (5.1) is known as the Petrov-Galerkin condition. Let u = Qky, y \u2208 Rmk . Then (5.1) leads to\nV Tk AQky = \u03bbV T k Qky. (5.2)\nNext, we describe how to solve the generalized eigenvalue problem (5.2) using the properties of restarted AM.\nAt the (k + 1)-th iteration, suppose that mk+1 \u2265 2. As will be shown in Proposition 5.3, there is an upper Hessenberg matrix Hk \u2208 Rmk\u00d7mk such that\nAQk = QkHk + qk+1 \u00b7 h(mk+1)k eTmk , (5.3)\nwhere h (mk+1) k \u2208 R and emk is the mk-th column of Imk . Since qTk+1Vk = 0 from the construction of qk+1 (cf. Proposition 3.1), it follows from (5.3) that V T k AQk = V T k QkHk, which together with (5.2) yields that V Tk QkHky = \u03bbV T k Qky. Noting that det(V T k Qk) \u0338= 0 if the restarted AM does not reach the exact solution, we find (5.2) is reduced to\nHky = \u03bby, (5.4)\nwhich can be solved by efficient numerical algorithms [22] using O(m3k) flops.\nRemark 5.1. From Proposition 4.1, range(Qk) = AKmk(A, rk\u2212mk). For the Type-I method, (5.1) is an oblique projection method; for the Type-II method, (5.1) can be viewed as the Arnoldi\u2019s method [41] based on ATA-norm. It is expected that with larger mk, the eigenvalue estimates are closer to the exact eigenvalues of A.\nNow, we describe the construction of Hk in Definition 5.2 and show the role of Hk in Proposition 5.3.\nDefinition 5.2. Consider applying the restarted AM to solve the fixed-point problem (1.1). At the (k + 1)-th iteration, suppose that mk+1 \u2265 2. Define the unreduced upper Hessenberg matrix H\u0304k = (H T k , h (mk+1) k emk)\nT \u2208 R(mk+1)\u00d7mk , where h(mk+1)k \u2208 R, and emk is the mk-th column of Imk . The Hk \u2208 Rmk\u00d7mk is defined as Hk = hk if mk = 1 and Hk = ( H\u0304k\u22121, hk ) if mk \u2265 2. Define \u03d5k = \u0393k + \u03b6k+1, \u0393 [mk\u22121] k = (\u0393 (1) k , . . . ,\u0393 (mk\u22121) k ) T. The hk \u2208 Rmk in Hk is constructed as follows:\nhk = 1\n1\u2212 \u0393k\n( 1\n\u03b2k\u22121 \u2212 1 \u03b2k \u03d5k\n) , if mk = 1;\nhk = 1\n1\u2212 \u0393(mk)k\n( 1\n\u03b2k\u22121\n( \u03d5k\u22121 1 ) \u2212 1 \u03b2k \u03d5k \u2212 H\u0304k\u22121 ( \u03d5k\u22121 \u2212 \u0393[mk\u22121]k )) , if mk \u2265 2.\n(5.5)\nThe construction of h (mk+1) k is\nh (mk+1) k = \u2212\n1\n\u03b2k(1\u2212 \u0393(mk)k ) , for mk \u2265 1. (5.6)\nProposition 5.3. Let g(x) = (I \u2212 A)x + b in the fixed-point problem (1.1), where A \u2208 Rd\u00d7d is positive definite and b \u2208 Rd. For the restarted Type-I/Type-II AM method, if mk+1 \u2265 2 at the (k + 1)-th iteration, then with the notations defined in Definition 5.2, we have\nAPk = Pk+1H\u0304k = PkHk + pk+1 \u00b7 h(mk+1)k eTmk . (5.7)\nThe proof is given in Appendix C.1. Since Qk+1 = \u2212APk+1 from Proposition 4.1, the relation (5.3) holds as a result of (5.7).\nRemark 5.4. Definition 5.2 suggests that Hk can be economically constructed by manipulating the coefficients in the restarted AM. Thus we can efficiently solve the problem (5.1) without any additional matrix-vector product.\nNext, consider the nonlinear case. We can still construct Hk by Definition 5.2. Let A := h \u2032(x\u2217). Since g is nonlinear, the relation (5.3) does not exactly hold in general, which can make the eigenvalues of Hk different from those computed by solving (5.2). Nonetheless, similar to the proof of Lemma 4.4, we consider an auxiliary process using restarted AM to solve the linearized problem h\u0302(x) = 0, where a Hessenberg matrix H\u0302k can be constructed as well. By comparing Hk and H\u0302k, we show that the eigenvalues of Hk can still approximate the eigenvalues of A.\nLemma 5.5. Suppose that Assumption 4.2 holds for the fixed-point problem (1.1). For the Process I in Definition 4.3, assume that there are positive constants \u03b70, \u03c40 such that \u2225h(xj)\u22252 \u2264 \u03b70\u2225h(x0)\u22252 (0 \u2264 j \u2264 k + 1) and |1\u2212 \u0393(mj)j | \u2265 \u03c40 (1 \u2264 j \u2264 k); Hk is defined by Definition 5.2. For the Process II in Definition 4.3, the upper Hessenberg matrix H\u0302k \u2208 Rmk\u00d7mk is defined correspondingly (by replacing \u0393k, \u03b6k+1 with \u0393\u0302k, \u03b6\u0302k+1 in Definition 5.2). Then for x0 sufficiently close to x \u2217, we have\n\u2225Hk\u22252 = O(1), \u2225Hk \u2212 H\u0302k\u22252 = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u22252). (5.8)\nThe proof can be found in Appendix C.2. It suggests that Hk is a perturbation of H\u0302k. Since h\u0302(x) is linear, the eigenvalues of H\u0302k exactly solve V\u0302 T k AQ\u0302ky = \u03bb\u0302V\u0302 T k Q\u0302ky, thus approximating the eigenvalues of A. Next, we compare \u03c3(Hk) and \u03c3(H\u0302k) using the perturbation theory.\nTheorem 5.6. Under the same assumptions of Lemma 5.5, let \u03bb be an eigenvalue of Hk. Then for x0 sufficiently close to x \u2217, we have\nmin \u03bb\u0302\u2208\u03c3(H\u0302k)\n|\u03bb\u0302\u2212 \u03bb| = \u03ba\u03021/mkO(\u2225xk\u2212mk \u2212 x\u2217\u2225 1/mk 2 ). (5.9)"
        },
        {
            "heading": "If further assuming H\u0302k is diagonalizable, i.e., there is a nonsingular matrix Mk \u2208 Rmk\u00d7mk such that",
            "text": "H\u0302k = MkD\u0302kM \u22121 k , where D\u0302k is diagonal, then\nmin \u03bb\u0302\u2208\u03c3(H\u0302k)\n|\u03bb\u0302\u2212 \u03bb| = \u2225Mk\u22252\u2225M\u22121k \u22252\u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u22252). (5.10)\nProof. (5.9) follows from Lemma 5.5 and [4, Theorem VIII.1.1], and (5.10) is a consequence of Lemma 5.5 and Bauer-Fike theorem [22, Theorem 7.2.2].\nSince H\u0302k is unavailable in practice, Theorem 5.6 suggests that we can use the eigenvalues of Hk to roughly estimate the eigenvalues of A. Then, suppose mk \u2265 2 and \u03bb\u0303 is the eigenvalue of Hk\u22121 of the largest absolute value. We set the mixing parameter at the k-th iteration as\n\u03b2k = 2\n|\u03bb\u0303| . (5.11)\nWe call such a way to choose \u03b2k as the adaptive mixing strategy since \u03b2k is chosen adaptively. Usually, the extreme eigenvalues can be quickly estimated, so we only need to run the eigenvalue estimation procedure for a few steps."
        },
        {
            "heading": "6 Short-term recurrence methods",
            "text": "For solving high-dimensional problems, the memory cost of AM can be prohibitive when mk is large. In this section, we show that if the Jacobian matrix is symmetric, the restarted AM methods can have short-term recurrence forms which address the memory issue while maintaining fast convergence. Since Assumption 4.2 assumes that h\u2032(x\u2217) is positive definite, the symmetry of h\u2032(x\u2217) motivates us to consider solving SPD linear systems first.\nProposition 6.1. Let g(x) = (I \u2212A)x+ b in the fixed-point problem (1.1), where A \u2208 Rd\u00d7d is SPD and b \u2208 Rd. For the full-memory Type-I/Type-II AM with modified historical sequences, we have\n\u03b6 (j) k = 0, for j \u2264 k \u2212 3 (k \u2265 4), and \u0393 (j) k = 0, for j \u2264 k \u2212 2 (k \u2265 3), (6.1)\nif the algorithm has not found the exact solution.\nProof. Since A is SPD, by Proposition 4.1, the procedures (3.3) and (3.4) are well defined during the iterations. Proposition 3.1 also suggests that\nqk = \u2206rk\u22121 \u2212Qk\u22121\u03b6k \u22a5 range(Vk\u22121), r\u0304k = rk \u2212Qk\u0393k \u22a5 range(Vk). (6.2)\nHence,\n\u03b6k = (V T k\u22121Qk\u22121) \u22121V Tk\u22121\u2206rk\u22121, \u0393k = (V T k Qk) \u22121V Tk rk. (6.3)\nSince A is symmetric, it follows that V Tk Qk is diagonal for either type of the AM methods. Note that rk = r\u0304k\u22121 \u2212 \u03b2k\u22121Ar\u0304k\u22121 and range(AVk\u22122) \u2286 range(Vk\u22121) due to (4.1) in Proposition 4.1. Hence\nV Tk\u22122rk = V T k\u22122r\u0304k\u22121 \u2212 \u03b2k\u22121(AVk\u22122)Tr\u0304k\u22121 = 0, (6.4)\nas a consequence of (6.2). So the first (k \u2212 2) elements of V Tk rk are zeros. Thus, \u0393 (j) k = 0, j \u2264 k \u2212 2, for k \u2265 3. Also, (6.4) yields that V Tk\u22123\u2206rk\u22121 = V T k\u22123rk \u2212 V Tk\u22123rk\u22121 = 0, (6.5)\nwhich infers that the first (k \u2212 3) elements of V Tk\u22121\u2206rk\u22121 are zeros. Thus, \u03b6 (j) k = 0, j \u2264 k \u2212 3, for k \u2265 4.\nProposition 6.1 suggests that for solving SPD linear systems, we only need to maintain the most recent two vector pairs, and there is no loss of historical information. Specifically, suppose k \u2265 3 and define {vj} as that in Section 3.1. The procedure has short-term recurrences and is described as follows.\nStep 1: Modified vector pair. At the beginning of the k-th iteration, pk, qk are obtained from \u2206xk\u22121,\u2206rk\u22121 and (pk\u22122, pk\u22121), (qk\u22122, qk\u22121):\npk = \u2206xk\u22121 \u2212 (pk\u22122, pk\u22121)\u03b6k, qk = \u2206rk\u22121 \u2212 (qk\u22122, qk\u22121)\u03b6k, (6.6) where \u03b6k \u2208 R2 is chosen such that qk \u22a5 span{vk\u22122, vk\u22121}.\nStep 2: AM update. The next step is the ordinary AM update:\nx\u0304k = xk \u2212 (pk\u22121, pk)\u0393k, r\u0304k = rk \u2212 (qk\u22121, qk)\u0393k, xk+1 = x\u0304k + \u03b2kr\u0304k, (6.7) where \u0393k \u2208 R2 is chosen such that r\u0304k \u22a5 span{vk\u22121, vk}.\nWe call it short-term recurrence AM (ST-AM). The Type-II ST-AM has been proposed in [51]. Combining the ST-AM update with the restarting conditions (3.5)-(3.7), we obtain the restarted ST-AM methods, as shown in Algorithm 2.\nWe establish the convergence properties in the nonlinear case.\nTheorem 6.2. For the fixed-point problem (1.1), suppose that g\u2032(x) is symmetric and Assumption 4.2 holds. Let {xk} and {rk} denote the iterates and residuals of the restarted ST-AM, A := I \u2212 g\u2032(x\u2217), \u03b8k := \u2225I \u2212 \u03b2kA\u22252, and \u03b8 \u2208 [L\u2212\u00b5 L+\u00b5 , 1 ) is a constant. For k = 0, 1, . . . , \u03b2k is chosen such that \u03b8k \u2264 \u03b8."
        },
        {
            "heading": "The following results hold.",
            "text": "1. For the Type-I method, if x0 is sufficiently close to x \u2217, then\n\u2225xk+1 \u2212 x\u2217\u2225A \u2264 2\u03b8k (\u221a\nL/\u00b5\u2212 1\u221a L/\u00b5+ 1 )mk \u2225xk\u2212mk \u2212 x\u2217\u2225A + \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u222522). (6.8)\n2. For the Type-II method, if x0 is sufficiently close to x \u2217, then \u2225rk+1\u22252 \u2264 2\u03b8k (\u221a\nL/\u00b5\u2212 1\u221a L/\u00b5+ 1 )mk \u2225rk\u2212mk\u22252 + \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u222522). (6.9)\n3. For either method, if the aforementioned assumptions hold and mk = d, then \u2225xk+1 \u2212 x\u2217\u22252 = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u222522), namely (d+ 1)-step quadratic convergence.\nAlgorithm 2 Restarted ST-AM. g is the fixed-point map with symmetric Jacobian g\u2032. The Type-I method: vj := pj (j \u2265 1); the Type-II method: vj := qj (j \u2265 1). Input: x0 \u2208 Rd, \u03b2k > 0,m \u2208 Z+, \u03c4 \u2208 (0, 1), \u03b7 > 0 Output: x \u2208 Rd 1: m0 = 0 2: for k = 0, 1, . . . , until convergence do 3: rk = g(xk)\u2212 xk 4: if mk > m or \u2225rk\u22252 > \u03b7\u2225rk\u2212mk\u22252 then 5: mk = 0 6: end if 7: if mk > 0 then 8: pk = xk \u2212 xk\u22121, qk = rk \u2212 rk\u22121 9: for j = max{1,mk \u2212 2}, . . . ,mk \u2212 1 do\n10: \u03b6 = ( vTk\u2212mk+jqk ) / ( vTk\u2212mk+jqk\u2212mk+j ) 11: pk = pk \u2212 pk\u2212mk+j\u03b6, qk = qk \u2212 qk\u2212mk+j\u03b6 12: end for 13: if |vTk qk| < \u03c4 |vTk\u2212mk+1qk\u2212mk+1| then 14: mk = 0 15: end if 16: end if 17: x\u0304k = xk, r\u0304k = rk 18: for j = max{1,mk \u2212 1}, . . . ,mk do 19: \u03b3 = ( vTk\u2212mk+j r\u0304k ) / ( vTk\u2212mk+jqk\u2212mk+j\n) 20: x\u0304k = x\u0304k \u2212 pk\u2212mk+j\u03b3, r\u0304k = r\u0304k \u2212 qk\u2212mk+j\u03b3 21: end for 22: xk+1 = x\u0304k + \u03b2kr\u0304k 23: mk+1 = mk + 1 24: end for 25: return xk\nWe give the proof in Appendix D.1. Theorem 6.2 shows that the asymptotic convergence rates of both types of restarted ST-AM methods are optimal with respect to the condition number (see [32, Section 2.1.4]), thus significantly improving the convergence rate of the fixed-point iteration. The theorem also suggests that the restarted ST-AM methods are applicable for solving large-scale unconstrained optimization problems since the Hessian matrices are naturally symmetric.\nRemark 6.3. When A is symmetric, the convergence bound (4.9) for the restarted Type-I AM can be refined to\n\u2225xk+1 \u2212 x\u2217\u2225A \u2264 \u03b8k min p\u2208Pmk p(0)=1 \u2225p(A)(xk\u2212mk \u2212 x\u2217)\u2225A + \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u222522). (6.10)\nUsing Chebyshev polynomials for the minimization problems in (6.10) and (4.10) [43, Theorem 6.29], we can establish the same convergence rates as (6.8) and (6.9) for the restarted Type-I AM and the restarted Type-II AM, respectively. On the other side, the convergence results in [47, 18] shown by (4.13) and (4.14) in Remark 4.11 cannot provide such refined results and underestimate the efficacy of AM.\nIn practice, we can also choose the mixing parameters {\u03b2k} with simplified computation by exploring the symmetry of the Jacobian.\nProposition 6.4. Let g(x) = (I \u2212A)x+ b in the fixed-point problem (1.1), where A \u2208 Rd\u00d7d is SPD and b \u2208 Rd. For the restarted Type-I/Type-II ST-AM method, if mk+1 \u2265 2 at the (k+1)-th iteration, then\nApk = t (mk\u22121) k pk\u22121 + t (mk) k pk + t (mk+1) k pk+1, (6.11)\nwhere pk\u2212mk := 0 \u2208 Rd, and the coefficients are given by\nt (mk\u22121) k =\n\u03d5k\u22121\n\u03b2k\u22121(1\u2212 \u0393(mk)k ) ,\nt (mk) k =\n1\n1\u2212 \u0393(mk)k\n( 1\n\u03b2k\u22121 \u2212 \u03d5k \u03b2k\n) ,\nt (mk+1) k = \u2212\n1\n\u03b2k(1\u2212 \u0393(mk)k ) ,\n(6.12)\nwhere \u03d5k := 0 if mk = 0, and \u03d5k := \u0393 (mk) k + \u03b6 (mk) k+1 = \u0393 (mk) k+1 = vTk rk+1 vTk qk if mk \u2265 1. Thus there exists a tridiagonal matrix T\u0304k \u2208 R(mk+1)\u00d7mk such that\nAPk = Pk+1T\u0304k = PkTk + pk+1 \u00b7 t(mk+1)k eTmk , (6.13)\nwhere Tk is obtained from T\u0304k by deleting its last row, and emk is the mk-th column of Imk .\nProof. Note that\n\u0393k + \u03b6k+1 = (V T k Qk) \u22121V Tk rk + (V T k Qk) \u22121V Tk \u2206rk = (V T k Qk) \u22121V Tk rk+1.\nSince V Tk Qk is diagonal due to symmetry, and V T k\u22121rk+1 = 0, it follows that\n\u0393k + \u03b6k+1 = (0, . . . , 0, vTk rk+1\nvTk qk )T = \u0393 [mk] k+1 ,\nwhere \u0393 [mk] k+1 is the subvector of the first mk elements of \u0393k+1. Then the formula (6.12) follows from Definition 5.2 and Proposition 5.3.\nThen, following the derivation in Section 5, the projection method (5.1) to estimate the eigenvalues of h\u2032(x\u2217) is reduced to solving the eigenvalues of Tk.\nTheorem 6.5. For the fixed-point problem (1.1), suppose that g\u2032(x) is symmetric and Assumption 4.2 holds. For the Process I in Definition 4.3, replace the restarted AM by the restarted ST-AM, and assume that there are positive constants \u03b70, \u03c40 such that \u2225h(xj)\u22252 \u2264 \u03b70\u2225h(x0)\u22252 (0 \u2264 j \u2264 k + 1), |1\u2212\u0393(mj)j | \u2265 \u03c40 (1 \u2264 j \u2264 k); Tk is the tridiagonal matrix as defined in Proposition 6.4, and \u03bb \u2208 \u03c3(Tk). For the Process II in Definition 4.3, the tridiagonal matrix T\u0302k is defined correspondingly. For x0 sufficiently close to x\u2217, we have\nmin \u03bb\u0302\u2208\u03c3(T\u0302k)\n|\u03bb\u0302\u2212 \u03bb| = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u22252). (6.14)\nThe proof is given in Appendix D.2. Since h\u0302(x) is linear and h\u2032(x\u2217) is symmetric, using the eigenvalues of T\u0302k to approximate the eigenvalues of h\n\u2032(x\u2217) is equivalent to a Lanczos method [22]. For the Type-I method, the Lanczos method is A-norm based; for the Type-II method, the Lanczos method is A2-norm based. Thus the eigenvalue \u03bb\u0302 \u2208 \u03c3(T\u0302k) is known as the generalized Ritz value [31]. Theorem 6.5 indicates that \u03c3(Tk) is close to \u03c3(T\u0302k) when \u2225xk\u2212mk \u2212 x\u2217\u22252 is small.\nAt the k-th iteration, where mk \u2265 2, let \u00b5\u0303 be the eigenvalue of Tk\u22121 of the smallest absolute value, and let L\u0303 be the eigenvalue of Tk\u22121 of the largest absolute value. We use |\u00b5\u0303| and |L\u0303| as the estimates of \u00b5 and L. Then we set\n\u03b2k = 2\n|\u00b5\u0303|+ |L\u0303| (6.15)\nas an estimate of the optimal value 2/(\u00b5+ L)."
        },
        {
            "heading": "7 Numerical experiments",
            "text": "In this section, we validate our theoretical findings by solving three nonlinear problems: (I) the modified Bratu problem; (II) the Chandrasekhar H-equation; (III) the regularized logistic regression. Additional experimental results can be found in Appendix E. Let AM-I and AM-II denote the restarted Type-I and restarted Type-II AM. AM-I(m) and AM-II(m) denote the Type-I AM and Type-II AM with mk = min{m, k}. ST-AM-I and ST-AM-II are abbreviations of the restarted Type-I and restarted Type-II ST-AM. Since this work focuses on the theoretical properties of AM, we used the iteration number as the evaluation metric of convergence in the experiments."
        },
        {
            "heading": "7.1 Modified Bratu problem",
            "text": "To verify Theorem 4.5 and Theorem 6.2, we considered solving the modified Bratu problem introduced in [19]:\nuxx + uyy + \u03b1ux + \u03bbe u = 0, (7.1)\nwhere u is a function of (x, y) \u2208 D = [0, 1]2, and \u03b1, \u03bb \u2208 R are constants. The boundary condition is u(x, y) \u2261 0 for (x, y) \u2208 \u2202D. The equation was discretized using centered differences on a 200 \u00d7 200 grid. The resulting problem is a system of nonlinear equations: F (U) = 0, where U \u2208 R200\u00d7200 and F : R200\u00d7200 \u2192 R200\u00d7200. Following [19], we set \u03bb = 1 and initialized U with 0. The Picard iteration is Uk+1 = Uk + \u03b2rk, where rk = F (Uk) is the residual. For the restarted AM and ST-AM, we set \u03c4 = 10\u221232, m = 1000, and \u03b7 = \u221e since a large mk is beneficial for solving this problem."
        },
        {
            "heading": "7.1.1 Nonsymmetric Jacobian",
            "text": "We set \u03b1 = 20 so that the Jacobian F \u2032(U) is not symmetric. For the Picard iteration, we tuned \u03b2k and set it as 6 \u00d7 10\u22126. We applied the adaptive mixing strategy (5.11) for AM-I and AM-II with \u03b20 = 1.\nAs shown in Figure 1, both AM-I and AM-II converge much faster than the Picard iteration. In fact, to achieve \u2225F\u22252 \u2264 10\u22126, AM-I uses 500 iterations, and AM-II uses 497 iterations, where no restart occurs in either method. Hence, the results verify Theorem 4.5 and suggest that AM methods significantly accelerate the Picard iteration when mk is large in solving this problem. Also, observe that AM-I and AM-II diverge in the initial stage due to the inappropriate choice \u03b20 = 1. Nonetheless,\nfrom Figure 1, we see the \u03b2k is quickly adjusted to the optimal value \u03b2 = 6 \u00d7 10\u22126 based on the eigenvalue estimates. Thus we only need to compute the eigenvalue estimates within a few steps and keep \u03b2k unchanged in the later iterations."
        },
        {
            "heading": "7.1.2 Symmetric Jacobian",
            "text": "We set \u03b1 = 0 so that the Jacobian F \u2032(U) is symmetric. We compared the restarted ST-AM methods with the Picard iteration, the limited-memory AM, and the full-memory AM. By a grid search in {1 \u00d7 10\u22126, 2 \u00d7 10\u22126, . . . , 1 \u00d7 10\u22125}, we chose \u03b2 = 6 \u00d7 10\u22126 for the Picard iteration. Then, we set \u03b2k = 6 \u00d7 10\u22126 for AM-I(2), AM-II(2), AM-I(\u221e), and AM-II(\u221e). We applied the adaptive mixing strategy (6.15) for ST-AM-I and ST-AM-II with \u03b20 = 1.\nThe results in Figure 2 show the convergence of each method and the choices of \u03b2k in ST-AMI/ST-AM-II. Observe that AM-I(2) and AM-II(2) perform similarly to the Picard iteration, which is\nreasonable since m = 2 is too small. On the other hand, ST-AM-I and ST-AM-II exhibit significantly faster convergence rates as predicted by Theorem 6.2 (no restart occurs in either method, i.e., mk = k). We also find that the \u03b2k of ST-AM-I/ST-AM-II quickly converges to 6.19 \u00d7 10\u22126. So the curve of ST-AM-I/ST-AM-II roughly coincides with that of the full-memory AM-I/AM-II in the early stage. However, due to the loss of orthogonality, ST-AM-I and ST-AM-II require more iterations to achieve \u2225F\u22252 \u2264 10\u22126 than the full-memory methods."
        },
        {
            "heading": "7.2 Chandrasekhar H-equation",
            "text": "To check the effect of the restarting conditions (3.5)-(3.7), we applied the restarted AM to solve the Chandrasekhar H-equation considered in [47, 10]:\nF(H)(\u00b5) = H(\u00b5)\u2212 ( 1\u2212 \u03c9\n2 \u222b 1 0 \u00b5H(\u03bd)d\u03bd \u00b5+ \u03bd )\u22121 = 0, (7.2)\nwhere \u03c9 \u2208 [0, 1] is a constant and the unknown is a continuously differentiable function H defined in [0, 1]. Following [47], we discretized the equation with the composite midpoint rule. The resulting equation is\nhi = G(h)i := 1\u2212 \u03c9 2N N\u2211 j=1 \u00b5ih j \u00b5i + \u00b5j \u22121 . (7.3) Here hi is the i-th component of h \u2208 RN , G(h)i is the i-th component of G(h) \u2208 RN , and \u00b5i = (i\u2212 1/2)/N for 1 \u2264 i \u2264 N . Define F (h) = h\u2212G(h) = 0, and rk = G(hk)\u2212 hk is the residual at hk. We set N = 500 and considered \u03c9 = 0.5, 0.99, 1. The initial point was h0 = (1, 1, . . . , 1)\nT. Since the fixed-point operator G is nonexpansive and the Picard iteration hk+1 = G(hk) = hk + rk converges in this case, we set \u03b2k = 1 for both restarted AM methods. (The hk here has no relation with that in Section 5.)\nWe studied the convergence of restarted AM with different settings of m, \u03c4, and \u03b7. Table 1 tabulates the results. This problem is hard to solve when \u03c9 approaches 1. For the easy case \u03c9 = 0.5, the restarting conditions have neglectable effects on the convergence. However, for \u03c9 = 0.99 and especially for \u03c9 = 1, the restarting conditions are critical, which help avoid the divergence of the\niterations. It is preferable to use a smallm in this problem. By comparing the casem = 100, \u03c4 = 10\u221215 with the case m = 100, \u03c4 = 10\u221232, we find that using (3.6) to control the condition number of V Tk Qk is necessary. Also, setting \u03b7 = 1 in (3.7) is helpful for AM-I."
        },
        {
            "heading": "7.3 Regularized logistic regression",
            "text": "To validate the effectiveness of ST-AM-I and ST-AM-II for solving unconstrained optimization problems, we considered solving the regularized logistic regression:\nmin x\u2208Rd\nf(x) := 1\nT T\u2211 i=1 log(1 + exp(\u2212yixT\u03bei)) + w 2 \u2225x\u222522, (7.4)\nwhere \u03bei \u2208 Rd is the i-th input data sample and yi = \u00b11 is the corresponding label. We used the \u201cmadelon\u201d dataset from LIBSVM [9], which contains 2000 data samples (T = 2000) and 500 features (d = 500). We considered w = 0.01. The compared methods were gradient descent (GD), Nesterov\u2019s method [32, Scheme 2.2.22], and the limited-memory AM methods with m = 2 and m = 20. For GD, we tuned the step size and set it as 1.6. For AM methods, we also set \u03b2k = 1.6. Let x\n\u2217 denote the minimizer. We used the smallest and the largest Ritz values of \u22072f(x\u2217) to approximate \u00b5 and L, which are required for Nesterov\u2019s method. For the restarted ST-AM methods, we set m = 40, \u03c4 = 1\u00d7 10\u221215, \u03b7 = \u221e and applied the adaptive mixing strategy (6.15) with \u03b20 = 1 to choose \u03b2k.\nFigure 3 shows the convergence of each method and the choices of \u03b2k in ST-AM-I and ST-AMII. Like AM-I(2) and AM-II(2), the ST-AM methods only use two vector pairs for the AM update. However, they have improved convergence, and the convergence rates are close to those of AM-I(20) and AM-II(20). Also, the mixing parameters for restarted ST-AM methods do not need to be tuned manually. With the convergence of xk to the minimizer x\n\u2217, the \u03b2k is adjusted to 2/(\u00b5+ L). In Table 2, we show the effects of the restarting conditions on ST-AM-I and ST-AM-II. We only consider m and \u03c4 in (3.5) and (3.6) since both methods converge in solving this problem. The results suggest that well-chosen m and \u03c4 can lead to improved convergence."
        },
        {
            "heading": "8 Conclusions",
            "text": "In this paper, we study the restarted AM methods formulated with modified historical sequences and certain restarting conditions. Using a multi-step analysis, we extend the relationship between AM and Krylov subspace methods to nonlinear fixed-point problems. We prove that under reasonable assumptions, the long-term convergence behaviour of the restarted Type-I/Type-II AM is dominated by a minimization problem that also appears in the theoretical analysis of Krylov subspace methods. The convergence analysis provides a new assessment of the efficacy of AM in practice and justifies the potential local improvement of restarted Type-II AM over the fixed-point iteration. As a by-product of the restarted AM, the eigenvalues of the Jacobian can be efficiently estimated, based on which we can choose the mixing parameter adaptively. When the Jacobian is symmetric, we derive the short-term recurrence variants of restarted AM methods and the simplified eigenvalue estimation procedure. The short-term recurrence AM methods are memory-efficient and can significantly accelerate the fixed-point iterations. The experiments validate our theoretical results and the restarting conditions."
        },
        {
            "heading": "A Proofs of Section 3",
            "text": ""
        },
        {
            "heading": "A.1 Proof of Proposition 3.1",
            "text": "Proof. We prove the results by induction. If mk = 1, then pk = \u2206xk\u22121, qk = \u2206rk\u22121. The Property 1 and Property 2 hold, and v T k qk = zTk \u2206rk\u22121 \u0338= 0. Then by (3.4), r\u0304k \u22a5 vk. Hence, (3.2) produces the same iterate as (2.4). Formk > 1, suppose that from the (k\u2212mk+1)-th iteration to the (k\u22121)-th iteration, Properties 1- 3 hold, and (3.2) produces the same iterates as (2.4). Then, at the k-th iteration, we first prove that qjk \u22a5 span{vk\u2212mk+1, . . . , vk\u2212mk+j}, j = 1, . . . ,mk \u2212 1, by induction.\nFor j = 1, since vTk\u2212mk+1qk\u2212mk+1 \u0338= 0, it follows that q 1 k \u22a5 vk\u2212mk+1 due to (3.3). Consider 1 < j \u2264 mk \u2212 1. Due to the inductive hypothesis, 0 \u0338= det(ZTk\u22121Rk\u22121) = det(STk\u22121V Tk\u22121Qk\u22121Sk\u22121). It follows that det(V Tk\u22121Qk\u22121) \u0338= 0. So the diagonal element vTk\u2212mk+jqk\u2212mk+j \u0338= 0, which together with (3.3) implies qjk \u22a5 vk\u2212mk+j . Also, both q j\u22121 k and qk\u2212mk+j are orthogonal to span{vk\u2212mk+1, . . . , vk\u2212mk+j\u22121} by the inductive hypotheses. Thus, qjk \u22a5 span{vk\u2212mk+1, . . . , vk\u2212mk+j\u22121}. We complete the induction. Consequently, we have qk = q mk\u22121 k \u22a5 span{vk\u2212mk+1, . . . , vk\u22121} = range(Vk\u22121). With the inductive hypothesis that V Tk\u22121Qk\u22121 is lower triangular, it follows that V T k Qk is lower triangular, namely the Property 2. We prove that qk \u0338= 0. Note that qk = \u2206rk\u22121 \u2212Qk\u22121\u03b6k. If qk = 0, then \u2206rk\u22121 \u2208 range(Qk\u22121) = range(Rk\u22121), which is impossible since Rk has full column rank due to det(Z T k Rk) \u0338= 0. Hence qk \u0338= 0 and Rk = QkSk, where Sk is unit upper triangular. Since pk = \u2206xk\u22121 \u2212 Pk\u22121\u03b6k, we also have Xk = PkSk. So the Property 1 holds.\nNext, we prove that rjk \u22a5 span{vk\u2212mk+1, . . . , vk\u2212mk+j}, j = 1, . . . ,mk, by induction. As Properties 1-2 hold at the k-th iteration, we have 0 \u0338= det(ZTk Rk) = det(STk V Tk QkSk), which implies that det(V Tk Qk) \u0338= 0. Hence vTk\u2212mk+jqk\u2212mk+j \u0338= 0 for j = 1, . . . ,mk. Then we have r 1 k \u22a5 vk\u2212mk+1 due to (3.4). Consider 1 < j \u2264 mk. Due to vTk\u2212mk+jqk\u2212mk+j \u0338= 0 and (3.4), r j k \u22a5 vk\u2212mk+j . Also, by the inductive hypotheses, both rj\u22121k and qk\u2212mk+j are orthogonal to span{vk\u2212mk+1, . . . , vk\u2212mk+j\u22121}. It follows that rjk \u22a5 span{vk\u2212mk+1, . . . , vk\u2212mk+j\u22121}. Thus, we complete the induction. It yields that r\u0304k = r mk k \u22a5 range(Vk), namely the Property 3.\nFinally, the complete update of (3.2) is xk+1 = xk +Gkrk, where\nGk = \u03b2kI \u2212 (Pk + \u03b2kQk)(V Tk Qk)\u22121V Tk . (A.1)\nHere, we use Property 3 which implies \u0393k = (V T k Qk) \u22121V Tk rk. Then with Pk = XkS \u22121 k and Qk = RkS \u22121 k , the equivalent form of (A.1) is Gk = \u03b2kI\u2212(Xk+\u03b2kRk)(ZTk Rk)\u22121ZTk , which is the original AM update (2.4). So (3.2) produces the same iterate as (2.4). As a result, we complete the induction."
        },
        {
            "heading": "B Proofs of Section 4",
            "text": ""
        },
        {
            "heading": "B.1 Proof of Proposition 4.1",
            "text": "Proof. 1. The Properties 1-3 are known results [49]. We give the proof here for completeness. The definition of g suggests that the residual rk = g(xk)\u2212xk = b\u2212Axk for k \u2265 0 and Rk = \u2212AXk for k \u2265 1. Recall that each \u0393j is determined by solving\nr\u0304j = rj \u2212Rj\u0393j \u22a5 range(Zj), (B.1)\nwhere 1 \u2264 j \u2264 k and k \u2265 1. The condition det(ZTj Rj) \u0338= 0 ensures that \u0393j is uniquely determined. Thus the AM updates are well defined.\nSince A is nonsingular and Rk = \u2212AXk, it follows that rank(Xk) = rank(Rk). Then, due to det(ZTk Rk) \u0338= 0, we have rank(Zk) = rank(Rk) = k. So rank(Xk) = k. We first prove range(Xk) = Kk(A, r0) by induction.\nFirst, \u2206x0 = \u03b20r0 since x1 = x0 + \u03b20r0. If k = 1, then the proof is complete. Suppose that k > 1 and range(Xk\u22121) = Kk\u22121(A, r0). Define ek \u2208 Rk to be the vector with all elements being ones. From the AM update (2.4), we have\n\u2206xk\u22121 = \u03b2k\u22121rk\u22121 \u2212 (Xk\u22121 + \u03b2k\u22121Rk\u22121)\u0393k\u22121 = \u03b2k\u22121(b\u2212Axk\u22121)\u2212 (Xk\u22121 \u2212 \u03b2k\u22121AXk\u22121)\u0393k\u22121 = \u03b2k\u22121b\u2212 \u03b2k\u22121A(x0 +\u2206x0 + \u00b7 \u00b7 \u00b7+\u2206xk\u22122)\u2212 (Xk\u22121 \u2212 \u03b2k\u22121AXk\u22121)\u0393k\u22121 = \u03b2k\u22121r0 \u2212 \u03b2k\u22121AXk\u22121ek\u22121 \u2212 (Xk\u22121 \u2212 \u03b2k\u22121AXk\u22121)\u0393k\u22121.\nSince range(Xk\u22121) = Kk\u22121(A, r0), we have range(AXk\u22121) \u2286 Kk(A, r0). Also, noting that r0 \u2208 Kk\u22121(A, r0), we have \u2206xk\u22121 \u2208 Kk(A, r0). Thus, range(Xk) \u2286 Kk(A, r0). Since rank(Xk) = k, it follows that range(Xk) = Kk(A, r0), thus completing the induction.\nSince rk = b\u2212 Axk = b\u2212 A(x0 +Xkek) = r0 \u2212 AXkek, it follows that rk \u2212 Rk\u0393 = rk + AXk\u0393 = r0 \u2212 AXkek + AXk\u0393 = r0 \u2212 AXk\u0393\u0303, where \u0393\u0303 = ek \u2212 \u0393, for \u2200 \u0393 \u2208 Rk. So \u0393k solves (B.1) for j = k if and only if \u0393\u0303k = e k \u2212 \u0393k solves\nr0 \u2212AXk\u0393\u0303k \u22a5 range(Zk). (B.2)\nSince range(Xk) = Kk(A, r0), the condition (B.2) is equivalent to r0 \u2212Az \u22a5 range(Zk) s.t. z \u2208 Kk(A, r0). (B.3)\nHere range(Zk) = Kk(A, r0) for the Type-I method, and range(Zk) = AKk(A, r0) for the Type-II method. Since the initializations are identical, the conditions (B.3) for Type-I and Type-II methods are the Petrov-Galerkin conditions for the Arnoldi\u2019s method and GMRES, respectively. Due to the nonsingularity of ZTk Rk, the solution of (B.2) is also unique. Therefore, we have\nx\u0304k = xk \u2212Xk\u0393k = xk \u2212Xk(ek \u2212 \u0393\u0303k) = x0 +Xk\u0393\u0303k = xAk , for the Type-I method, and x\u0304k = x G k for the Type-II method.\n2. Consider the case that A is positive definite, and the algorithm has not found the exact solution, i.e. rj \u0338= 0 for j = 0, . . . , k. We prove the result by induction.\nIf k = 1, then \u2206x0 = \u03b20r0, and \u2206r0 = \u2212\u03b20Ar0. Hence ZT1 R1 = \u2206xT0 \u2206r0 = \u2212\u03b220rT0 Ar0 for the Type-I method; ZT1 R1 = \u2206r T 0 \u2206r0 = \u03b2 2 0r T 0 A\nTAr0 for the Type-II method. Since r0 \u0338= 0 and A is positive definite, it follows that det(ZT1 R1) \u0338= 0.\nFor k > 1, suppose that det(ZTk\u22121Rk\u22121) \u0338= 0. It indicates rank(Rk\u22121) = k\u2212 1, thus rank(Xk\u22121) = k \u2212 1. We prove det(ZTk Rk) \u0338= 0 by contradiction.\nIf det(ZTk Rk) = 0, then there exists a nonzero y \u2208 Rk such that ZTk Rky = 0. Then yTZTk Rky = 0. Note that ZTk Rk = X T k Rk = \u2212XTk AXk for the Type-I method, and ZTk Rk = RTkRk = XTk ATAXk for the Type-II method. Since A is positive definite, we have Xky = 0, which implies that Xk is rank deficient. As Xk\u22121 has full column rank, it yields \u2206xk\u22121 = \u2212Xk\u22121\u0393k\u22121 + \u03b2k\u22121r\u0304k\u22121 \u2208 range(Xk\u22121). Hence r\u0304k\u22121 \u2208 range(Xk\u22121). So r\u0304k\u22121 = Xk\u22121\u03be for some \u03be \u2208 Rk\u22121. Since det(ZTk\u22121Rk\u22121) \u0338= 0, the condition r\u0304k\u22121 = rk\u22121 \u2212Rk\u22121\u0393k\u22121 \u22a5 Zk\u22121 has a unique solution. Thus\n0 = r\u0304Tk\u22121Zk\u22121\u03be = \u03be TXTk\u22121Zk\u22121\u03be. (B.4)\nFor the Type-I method, XTk\u22121Zk\u22121 = X T k\u22121Xk\u22121; for the Type-II method, X T k\u22121Zk\u22121 = X T k\u22121Rk\u22121 = \u2212XTk\u22121AXk\u22121. Since Xk\u22121 has full column rank and A is positive definite, it follows from (B.4) that \u03be = 0 for both cases, which yields r\u0304k\u22121 = 0. However, it is impossible because when r\u0304k\u22121 = 0, we have xk = x\u0304k\u22121 and rk = r\u0304k\u22121 = 0, which contradicts the assumption that rk \u0338= 0. Therefore, det(ZTk Rk) \u0338= 0. We complete the induction.\n3. Since det(ZTj Rj) \u0338= 0, j = 1, . . . , k, it follows from Proposition 3.1 that the constructions of the modified historical sequences Pk and Qk are well defined. The Property 1 in Proposition 3.1 further yields the relation (4.1)."
        },
        {
            "heading": "B.2 Proof of Lemma 4.4",
            "text": "Proof. The proof follows the technique in [52]. Besides (4.7) and (4.8), we shall also prove the following relations.\nxk \u2208 B\u03c1\u0302(x\u2217), (B.5) |\u03b6(j)k | = O(1), |\u03b6\u0302 (j) k \u2212 \u03b6 (j) k | = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u22252), (B.6) \u2225pk\u22252 = O(\u2225xk\u2212mk \u2212 x\u2217\u22252), \u2225qk\u22252 = O(\u2225xk\u2212mk \u2212 x\u2217\u22252), (B.7) \u2225pk \u2212 p\u0302k\u22252 = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u222522), \u2225qk \u2212 q\u0302k\u22252 = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u222522), (B.8) |\u0393(j)k | = O(1), |\u0393\u0302 (j) k \u2212 \u0393 (j) k | = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u22252), (B.9) \u2225x\u0304k \u2212 \u00af\u0302xk\u22252 = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u222522), \u2225r\u0304k \u2212 \u00af\u0302rk\u22252 = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u222522), (B.10)\nwhere j = 0, . . . ,mk. Here, for convenience, we define \u03b6\u0302 (0) k = \u03b6 (0) k = \u03b6\u0302 (mk) k = \u03b6 (mk) k = 0, \u0393\u0302 (0) k = \u0393 (0) k = 0; when mk = 0, define p\u0302k = q\u0302k = pk = qk = 0, x\u0304k = xk, r\u0304k = rk, and \u00af\u0302xk = x\u0302k, \u00af\u0302rk = r\u0302k; when mk > 0, \u00af\u0302xk = x\u0302k \u2212 P\u0302k\u0393\u0302k, \u00af\u0302rk = r\u0302k \u2212 Q\u0302k\u0393\u0302k. Then, the two processes to generate {xk} and {x\u0302k} are\nxk+1 = x\u0304k + \u03b2kr\u0304k, and x\u0302k+1 = \u00af\u0302xk + \u03b2k \u00af\u0302rk.\nWe first prove (B.5). Due to (4.2), we have the following relation:\n\u00b5\u2225xk \u2212 x\u2217\u22252 \u2264 \u2225rk\u22252 = \u2225h(xk)\u2212 h(x\u2217)\u22252 \u2264 L\u2225xk \u2212 x\u2217\u22252. (B.11)\nChoose \u2225x0 \u2212 x\u2217\u22252 \u2264 \u00b5\u03c1\u0302\u03b70L . With the condition \u2225rk\u22252 \u2264 \u03b70\u2225r0\u22252, we obtain\n\u2225xk \u2212 x\u2217\u22252 \u2264 1\n\u00b5 \u2225rk\u22252 \u2264 \u03b70 \u00b5 \u2225r0\u22252 \u2264 \u03b70L \u00b5 \u2225x0 \u2212 x\u2217\u22252 \u2264 \u03b70L \u00b5 \u00b7 \u00b5\u03c1\u0302 \u03b70L = \u03c1\u0302, (B.12)\nnamely (B.5). The (B.12) also implies we can choose sufficiently small \u2225x0\u2212x\u2217\u22252 to ensure \u2225xk\u2212mk \u2212 x\u2217\u22252 \u2264 \u03b70L\u00b5 \u2225x0\u2212x\u2217\u22252 is sufficiently small. Then, we prove (4.7), (4.8), and (B.6)-(B.10) by induction.\nFor k = 0, the relations (B.6)-(B.9) clearly hold. Besides, due to (4.4), we have \u2225r0 \u2212 r\u03020\u22252 \u2264 1 2 \u03ba\u0302\u2225x0 \u2212 x\u2217\u222522, namely (4.7). Since x0 = x\u03020, the (B.10) also holds. Then (4.8) follows from\n\u2225x1 \u2212 x\u03021\u22252 = \u2225x0 + \u03b20r0 \u2212 (x\u03020 + \u03b20r\u03020)\u22252 = \u03b20\u2225r0 \u2212 r\u03020\u22252 \u2264 \u03b20\u03ba\u0302 2 \u2225x0 \u2212 x\u2217\u222522.\nSuppose that k \u2265 1, and as an inductive hypothesis, the relations (4.7), (4.8), and (B.6)-(B.10) hold for i = 0, . . . , k \u2212 1. Consider the k-th iteration.\nIf mk = 0, i.e., a restarting condition is met at the beginning of the k-th iteration, then x\u0302k = xk. The same as the case that k = 0, (4.7), (4.8), and (B.6)-(B.10) hold.\nConsider the nontrivial case that mk > 0. Due to (3.7), we have\n\u2225xj \u2212 x\u2217\u22252 \u2264 1\n\u00b5 \u2225rj\u22252 \u2264\n\u03b7 \u00b5 \u2225rk\u2212mk\u22252 \u2264 \u03b7L \u00b5 \u2225xk\u2212mk \u2212 x\u2217\u22252, j = k \u2212mk + 1, . . . , k.\nTherefore,\n\u2225xj \u2212 x\u2217\u22252 = O(\u2225xk\u2212mk \u2212 x\u2217\u22252), j = k \u2212mk, . . . , k. (B.13) Since xk \u2208 B\u03c1\u0302(x\u2217), it follows that\n\u2225rk \u2212 r\u0302k\u22252 = \u2225h(xk)\u2212 h\u0302(x\u0302k)\u22252 \u2264 \u2225h(xk)\u2212 h\u0302(xk)\u22252 + \u2225h\u0302(xk)\u2212 h\u0302(x\u0302k)\u22252 = \u2225h(xk)\u2212 h\u2032(x\u2217)(xk \u2212 x\u2217)\u22252 + \u2225h\u2032(x\u2217)(xk \u2212 x\u0302k)\u22252 \u2264 1\n2 \u03ba\u0302\u2225xk \u2212 x\u2217\u222522 + L\u2225xk \u2212 x\u0302k\u22252 = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u222522), (B.14)\nwhere the second inequality is due to (4.4) and (4.2), and the last equality is due to (B.13) and the inductive hypothesis (4.8). Thus, the relation (4.7) holds.\nSince the condition (3.6) holds, we have |vTk qk| \u2265 \u03c4 |vTk\u2212mk+1qk\u2212mk+1|. We discuss the TypeI method and the Type-II method separately. Using the fact that the (k \u2212 mk)-th iteration is xk\u2212mk+1 = xk\u2212mk + \u03b2k\u2212mkrk\u2212mk , we have that\n|vTk qk| \u2265 \u03c4 |pTk\u2212mk+1qk\u2212mk+1| = \u03c4 |\u2206x T k\u2212mk\u2206rk\u2212mk |\n= \u03c4 \u2223\u2223\u2223\u2223\u2206xTk\u2212mk \u222b 1 0 h\u2032(xk\u2212mk + t\u2206xk\u2212mk)\u2206xk\u2212mkdt \u2223\u2223\u2223\u2223 \u2265 \u03c4\u00b5\u2225\u2206xk\u2212mk\u222522 = \u03c4\u00b5\u03b22k\u2212mk\u2225rk\u2212mk\u2225 2 2 \u2265 \u03c4\u00b53\u03b22k\u2212mk\u2225xk\u2212mk \u2212 x \u2217\u222522,\nfor the Type-I method, where the second inequality is due to (4.3) and the third inequality is due to (B.11). For the Type-II method,\n|vTk qk| \u2265 \u03c4 |qTk\u2212mk+1qk\u2212mk+1| = \u03c4\u2225\u2206rk\u2212mk\u2225 2 2 \u2265 \u03c4\u00b52\u2225\u2206xk\u2212mk\u222522\n= \u03c4\u00b52\u03b22k\u2212mk\u2225rk\u2212mk\u2225 2 2 \u2265 \u03c4\u00b54\u03b22k\u2212mk\u2225xk\u2212mk \u2212 x \u2217\u222522.\nThen, define \u03ba = \u03c4\u00b53\u03b22 for the Type-I method, and \u03ba = \u03c4\u00b54\u03b22 for the Type-II method. Since no restart has occurred in the last mk iterations, we have\n|vTi qi|2 \u2265 \u03ba\u2225xk\u2212mk \u2212 x\u2217\u222522, for i = k \u2212mk + 1, . . . , k. (B.15)\nNow, we prove (B.6). We shall prove an auxiliary relation:\n\u2225qjk\u22252 = O(\u2225xk\u2212mk \u2212 x\u2217\u22252), \u2225q j k \u2212 q\u0302 j k\u22252 = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u222522), (B.16)\nfor j = 0, . . . ,mk \u2212 1. We conduct the proof by induction. For j = 0, (B.6) holds due to \u03b6\n(0) k = \u03b6\u0302 (0) k = 0. Since q 0 k = \u2206rk\u22121, q\u0302 0 k = \u2206r\u0302k\u22121, it follows that\n\u2225q0k\u22252 \u2264 \u2225rk\u22252 + \u2225rk\u22121\u22252 \u2264 2\u03b7\u2225rk\u2212mk\u22252 = O(\u2225xk\u2212mk \u2212 x\u2217\u22252),\nwhich is due to (3.7) and (B.11). Also, from (B.14) and (4.7), we have\n\u2225q0k \u2212 q\u03020k\u22252 \u2264 \u2225rk \u2212 r\u0302k\u22252 + \u2225rk\u22121 \u2212 r\u0302k\u22121\u22252 = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u222522).\nHence, the (B.6) and (B.16) hold when j = 0. Suppose that j \u2265 1, and (B.6) and (B.16) hold for \u2113 = 0, . . . , j \u2212 1. Consider the j-th step in (3.3). Due to (B.15) and the inductive hypotheses (B.7) and (B.16), we obtain\n|\u03b6(j)k | \u2264 \u2225vk\u2212mk+j\u22252\u2225q j\u22121 k \u22252\n\u03ba\u2225xk\u2212mk \u2212 x\u2217\u222522 = O(\u2225xk\u2212mk \u2212 x\u2217\u222522) \u03ba\u2225xk\u2212mk \u2212 x\u2217\u222522 = O(1). (B.17)\nNext, if vTk\u2212mk+jq j\u22121 k \u0338= 0, then\n|\u03b6(j)k \u2212 \u03b6\u0302 (j) k | = |\u03b6(j)k | \u00b7 \u2223\u2223\u2223\u2223\u22231\u2212 \u03b6\u0302 (j) k\n\u03b6 (j) k\n\u2223\u2223\u2223\u2223\u2223 = |\u03b6(j)k | \u00b7 \u2223\u2223\u2223\u2223\u22231\u2212 v\u0302Tk\u2212mk+j q\u0302 j\u22121 k\nvTk\u2212mk+jq j\u22121 k\n\u00b7 vTk\u2212mk+jqk\u2212mk+j\nv\u0302Tk\u2212mk+j q\u0302k\u2212mk+j \u2223\u2223\u2223\u2223\u2223 = |\u03b6(j)k | \u00b7 |a(1\u2212 b) + b| \u2264 |\u03b6 (j) k | \u00b7 (|a|+ |b|+ |ab|), (B.18)\nwhere a := 1\u2212 v\u0302 T k\u2212mk+j\nq\u0302j\u22121k\nvTk\u2212mk+j qj\u22121k\nand b := 1\u2212 v T k\u2212mk+j qk\u2212mk+j\nv\u0302Tk\u2212mk+j q\u0302k\u2212mk+j\n. We have\n|\u03b6(j)k | \u00b7 |a| = \u2223\u2223\u2223\u2223\u2223vTk\u2212mk+jq j\u22121 k \u2212 v\u0302Tk\u2212mk+j q\u0302 j\u22121 k\nvTk\u2212mk+jqk\u2212mk+j \u2223\u2223\u2223\u2223\u2223 . (B.19) From (B.7), (B.8), and (B.16), we obtain\n|vTk\u2212mk+j(q j\u22121 k \u2212 q\u0302 j\u22121 k )| \u2264 \u2225vk\u2212mk+j\u22252\u2225q j\u22121 k \u2212 q\u0302 j\u22121 k \u22252 = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u222532),\nand\n|(vk\u2212mk+j \u2212 v\u0302k\u2212mk+j)Tq\u0302 j\u22121 k |\n\u2264 |(vk\u2212mk+j \u2212 v\u0302k\u2212mk+j)Tq j\u22121 k |+ |(vk\u2212mk+j \u2212 v\u0302k\u2212mk+j)T(q j\u22121 k \u2212 q\u0302 j\u22121 k )| \u2264 \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u222532) + \u03ba\u03022O(\u2225xk\u2212mk \u2212 x\u2217\u222542) = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u222532).\nThen, it follows that\n|vTk\u2212mk+jq j\u22121 k \u2212 v\u0302Tk\u2212mk+j q\u0302 j\u22121 k | \u2264 |vTk\u2212mk+j(q j\u22121 k \u2212 q\u0302 j\u22121 k )|\n+ |(vk\u2212mk+j \u2212 v\u0302k\u2212mk+j)Tq\u0302 j\u22121 k | = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u222532). (B.20)\nCombining (B.19), (B.20), and (B.15) yields\n|\u03b6(j)k | \u00b7 |a| \u2264 \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u222532) \u03ba\u2225xk\u2212mk \u2212 x\u2217\u222522 = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u22252). (B.21)\nSimilar to (B.20), the following bound holds:\n|vTk\u2212mk+jqk\u2212mk+j \u2212 v\u0302 T k\u2212mk+j q\u0302k\u2212mk+j | = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x \u2217\u222532). (B.22)\nBesides,\n|v\u0302Tk\u2212mk+j q\u0302k\u2212mk+j | \u2265 |vTk\u2212mk+jqk\u2212mk+j | \u2212 |v T k\u2212mk+jqk\u2212mk+j \u2212 v\u0302 T k\u2212mk+j q\u0302k\u2212mk+j | \u2265 \u03ba\u2225xk\u2212mk \u2212 x\u2217\u222522 \u2212 \u03ba\u0302c1\u2225xk\u2212mk \u2212 x\u2217\u222532 \u2265 1\n2 \u03ba\u2225xk\u2212mk \u2212 x\u2217\u222522, (B.23)\nwhere the existence of c1 is guaranteed by (B.22), and the last inequality holds if \u2225xk\u2212mk\u2212x\u2217\u22252 \u2264 \u03ba2\u03ba\u0302c1 , which can be obtained by choosing \u2225x0 \u2212 x\u2217\u22252 \u2264 \u00b5\u03ba2\u03ba\u0302\u03b70Lc1 since \u2225xk\u2212mk \u2212 x\n\u2217\u22252 \u2264 \u03b70L\u00b5 \u2225x0 \u2212 x\u2217\u22252 by (B.12). From (B.22) and (B.23), it follows that\n|b| = \u2223\u2223\u2223\u2223\u2223 v\u0302Tk\u2212mk+j q\u0302k\u2212mk+j \u2212 vTk\u2212mk+jqk\u2212mk+jv\u0302Tk\u2212mk+j q\u0302k\u2212mk+j \u2223\u2223\u2223\u2223\u2223 = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u22252). (B.24) As a result, by (B.21), (B.24), (B.17), and (B.18), we obtain\n|\u03b6(j)k \u2212 \u03b6\u0302 (j) k | = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u22252).\nNow consider the case that vTk\u2212mk+jq j\u22121 k = 0. It is clear that \u03b6 (j) k = 0. Then\n|\u03b6(j)k \u2212 \u03b6\u0302 (j) k | = \u2223\u2223\u2223\u2223\u2223 v\u0302Tk\u2212mk+j q\u0302 j\u22121 k\nv\u0302Tk\u2212mk+j q\u0302k\u2212mk+j \u2223\u2223\u2223\u2223\u2223 \u2264 \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u222532)1 2\u03ba\u2225xk\u2212mk \u2212 x\u2217\u222522 = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u22252).\nTherefore, (B.6) holds for \u2113 = j. Next, we obtain\n\u2225qjk\u22252 \u2264 \u2225q j\u22121 k \u22252 + \u2225qk\u2212mk+j\u22252|\u03b6 (j) k | = O(\u2225xk\u2212mk \u2212 x\u2217\u22252), (B.25)\nwhich is due to (B.16), (B.7), (B.17), and j < mk \u2264 m. Also, from (B.8), (B.6), (B.7), and j \u2264 mk\u22121, it follows that\n\u2225qk\u2212mk+j\u03b6 (j) k \u2212 q\u0302k\u2212mk+j \u03b6\u0302 (j) k \u22252 \u2264 \u2225(qk\u2212mk+j \u2212 q\u0302k\u2212mk+j)\u03b6 (j) k \u22252\n+ \u2225(q\u0302k\u2212mk+j \u2212 qk\u2212mk+j)(\u03b6 (j) k \u2212 \u03b6\u0302 (j) k )\u22252 + \u2225qk\u2212mk+j(\u03b6 (j) k \u2212 \u03b6\u0302 (j) k )\u22252 = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u222522), (B.26)\nwhich together with (3.3) further yields that\n\u2225qjk \u2212 q\u0302 j k\u22252 \u2264 \u2225q j\u22121 k \u2212 q\u0302 j\u22121 k \u22252 + \u2225qk\u2212mk+j\u03b6 (j) k \u2212 q\u0302k\u2212mk+j \u03b6\u0302 (j) k \u22252 = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u222522).\nThen, (B.16) holds for \u2113 = j, thus completing the induction. Since (B.16) holds for j = mk \u2212 1, and qk = qmk\u22121k , we know \u2225qk\u22252 = O(\u2225xk\u2212mk \u2212 x\u2217\u22252) and \u2225qk\u2212q\u0302k\u22252 = \u03ba\u0302O(\u2225xk\u2212mk\u2212x\u2217\u222522). If mk = 1, then pk = \u2206xk\u22121. So \u2225pk\u22252 \u2264 \u2225xk\u2212x\u2217\u22252+\u2225xk\u22121\u2212x\u2217\u22252 = O(\u2225xk\u2212mk \u2212 x\u2217\u22252) and \u2225pk \u2212 p\u0302k\u22252 \u2264 \u2225xk \u2212 x\u0302k\u22252 + \u2225xk\u22121 \u2212 x\u0302k\u22121\u22252 = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u222522). Consider mk \u2265 2. Since\n\u2225pk\u22252 = \u2225\u2206xk\u22121 \u2212 Pk\u22121\u03b6k\u22252 \u2264 \u2225xk \u2212 x\u2217\u22252 + \u2225xk\u22121 \u2212 x\u2217\u22252 + mk\u22121\u2211 j=1 \u2225pk\u2212mk+j\u03b6 (j) k \u22252,\nit follows that \u2225pk\u22252 = O(\u2225xk\u2212mk \u2212 x\u2217\u22252). Also, similar to (B.26), we have\n\u2225pk\u2212mk+j\u03b6 (j) k \u2212 p\u0302k\u2212mk+j \u03b6\u0302 (j) k \u22252 = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u222522),\nwhich further yields\n\u2225Pk\u22121\u03b6k \u2212 P\u0302k\u22121\u03b6\u0302k\u22252 \u2264 mk\u22121\u2211 j=1 \u2225pk\u2212mk+j\u03b6 (j) k \u2212 p\u0302k\u2212mk+j \u03b6\u0302 (j) k \u22252 = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u222522).\nThen, with \u2225pk \u2212 p\u0302k\u22252 \u2264 \u2225xk \u2212 x\u0302k\u22252 + \u2225xk\u22121 \u2212 x\u0302k\u22121\u22252 + \u2225Pk\u22121\u03b6k \u2212 P\u0302k\u22121\u03b6\u0302k\u22252, we obtain \u2225pk \u2212 p\u0302k\u22252 = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u222522). Hence, (B.7) and (B.8) hold.\nNow, we prove (B.9), following a similar way of proving (B.6). The concerned auxiliary relation is\n\u2225rjk\u22252 = O(\u2225xk\u2212mk \u2212 x\u2217\u22252), \u2225r j k \u2212 r\u0302 j k\u22252 = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u222522), (B.27)\nfor j = 0, . . . ,mk. We still conduct the proof by induction.\nFor j = 0, (B.9) holds due to \u0393 (0) k = \u0393\u0302 (0) k = 0. Since r 0 k = rk, r\u0302 0 k = r\u0302k, we have \u2225r0k\u22252 \u2264 \u03b7\u2225rk\u2212mk\u22252 \u2264 \u03b7L\u2225xk\u2212mk \u2212 x\u2217\u22252, and \u2225r0k \u2212 r\u03020k\u22252 = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u222522). Suppose that j \u2265 1, and (B.9) and (B.27) hold for \u2113 = 0, . . . , j \u2212 1. Consider the j-th step in (3.4). With (B.15), we have\n|\u0393(j)k | \u2264 \u2225vk\u2212mk+j\u22252\u2225r j\u22121 k \u22252\n\u03ba\u2225xk\u2212mk \u2212 x\u2217\u222522 = O(\u2225xk\u2212mk \u2212 x\u2217\u222522) \u03ba\u2225xk\u2212mk \u2212 x\u2217\u222522 = O(1). (B.28)\nNext, if vTk\u2212mk+jr j\u22121 k \u0338= 0, then\n|\u0393(j)k \u2212 \u0393\u0302 (j) k | = |\u0393 (j) k | \u00b7 |a1(1\u2212 b1) + b1| \u2264 |\u0393 (j) k | \u00b7 (|a1|+ |b1|+ |a1b1|), (B.29)\nwhere a1 := 1 \u2212 v\u0302Tk\u2212mk+j\nr\u0302j\u22121k\nvTk\u2212mk+j rj\u22121k\nand b1 := 1 \u2212 vTk\u2212mk+j qk\u2212mk+j\nv\u0302Tk\u2212mk+j q\u0302k\u2212mk+j\n. With (B.27), (B.8), and (B.7), it follows\nthat\n|vTk\u2212mk+jr j\u22121 k \u2212 v\u0302Tk\u2212mk+j r\u0302 j\u22121 k | \u2264 |vTk\u2212mk+j(r j\u22121 k \u2212 r\u0302 j\u22121 k )|\n+ |(vk\u2212mk+j \u2212 v\u0302k\u2212mk+j)Tr j\u22121 k |+ |(vk\u2212mk+j \u2212 v\u0302k\u2212mk+j)T(r\u0302 j\u22121 k \u2212 r j\u22121 k )|\n= \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u222532). (B.30) Then with (B.30) and (B.15), we obtain\n|\u0393(j)k | \u00b7 |a1| = \u2223\u2223\u2223\u2223\u2223vTk\u2212mk+jr j\u22121 k \u2212 v\u0302Tk\u2212mk+j r\u0302 j\u22121 k\nvTk\u2212mk+jqk\u2212mk+j \u2223\u2223\u2223\u2223\u2223 \u2264 \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u22252). (B.31) For the bound of |b1|, note that we have obtained (B.24) and also have already proved (B.7) and (B.8) for the k-th iteration. Thus, |b1| = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u22252), which together with (B.31), (B.28), and (B.29) yields |\u0393(j)k \u2212 \u0393\u0302 (j) k | = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u22252). On the other side, if vTk\u2212mk+jr j\u22121 k = 0, then \u0393 (j) k = 0. Hence\n|\u0393(j)k \u2212 \u0393\u0302 (j) k | = \u2223\u2223\u2223\u2223\u2223 v\u0302Tk\u2212mk+j r\u0302 j\u22121 k\nv\u0302Tk\u2212mk+j q\u0302k\u2212mk+j \u2223\u2223\u2223\u2223\u2223 \u2264 \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u222532)1 2\u03ba\u2225xk\u2212mk \u2212 x\u2217\u222522 = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u22252).\nTherefore (B.9) holds for \u2113 = j. Next, we obtain\n\u2225rjk\u22252 \u2264 \u2225r j\u22121 k \u22252 + \u2225qk\u2212mk+j\u22252|\u0393 (j) k | = O(\u2225xk\u2212mk \u2212 x\u2217\u22252)\ndue to (B.27), (B.7), (B.28), and j \u2264 mk \u2264 m. By (B.8), (B.9), and (B.7), we have \u2225qk\u2212mk+j\u0393 (j) k \u2212 q\u0302k\u2212mk+j\u0393\u0302 (j) k \u22252 \u2264 \u2225(qk\u2212mk+j \u2212 q\u0302k\u2212mk+j)\u0393 (j) k \u22252\n+ \u2225(q\u0302k\u2212mk+j \u2212 qk\u2212mk+j)(\u0393 (j) k \u2212 \u0393\u0302 (j) k )\u22252 + \u2225qk\u2212mk+j(\u0393 (j) k \u2212 \u0393\u0302 (j) k )\u22252\n= \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u222522), (B.32) which yields that\n\u2225rjk \u2212 r\u0302 j k\u22252 \u2264 \u2225r j\u22121 k \u2212 r\u0302 j\u22121 k \u22252 + \u2225qk\u2212mk+j\u0393 (j) k \u2212 q\u0302k\u2212mk+j\u0393\u0302 (j) k \u22252 = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u222522).\nThen, (B.27) holds for \u2113 = j, thus completing the induction. Since (B.27) holds for j = mk, and r\u0304k = r mk k , we obtain \u2225r\u0304k\u22252 = O(\u2225xk\u2212mk\u2212x\u2217\u22252) and \u2225r\u0304k\u2212\u00af\u0302rk\u22252 = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u222522). Moreover, similar to (B.32), we have \u2225pk\u2212mk+j\u0393 (j) k \u2212 p\u0302k\u2212mk+j\u0393\u0302 (j) k \u22252 = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u222522),\nwhich further yields\n\u2225Pk\u0393k \u2212 P\u0302k\u0393\u0302k\u22252 \u2264 mk\u2211 j=1 \u2225pk\u2212mk+j\u0393 (j) k \u2212 p\u0302k\u2212mk+j\u0393\u0302 (j) k \u22252 = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u222522).\nThen, from \u2225x\u0304k \u2212 \u00af\u0302xk\u22252 \u2264 \u2225xk \u2212 x\u0302k\u22252 + \u2225Pk\u0393k \u2212 P\u0302k\u0393\u0302k\u22252, we obtain \u2225x\u0304k \u2212 \u00af\u0302xk\u22252 = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u222522). Hence, (B.10) holds.\nFinally, since xk+1 = x\u0304k + \u03b2kr\u0304k, it follows that\n\u2225xk+1 \u2212 x\u0302k+1\u22252 = \u2225(x\u0304k \u2212 \u00af\u0302xk) + \u03b2k(r\u0304k \u2212 \u00af\u0302rk)\u22252 = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u222522), where the second equality is due to (B.10) and the fact that \u03b2k is bounded.\nAs a result, we complete the induction. Thus, (4.7) and (4.8) are proved."
        },
        {
            "heading": "B.3 Proof of Theorem 4.5",
            "text": "Let \u03bbmin(\u00b7) and \u03bbmax(\u00b7) denote the smallest eigenvalue and the largest eigenvalue of a real symmetric matrix. We first give a lemma.\nLemma B.1. Suppose that A \u2208 Rd\u00d7d is positive definite with \u03bbmin(S(A)) \u2265 \u00b5 and \u2225A\u22252 \u2264 L, where \u00b5,L > 0. Then for a constant \u03b8 \u2208 [( 1\u2212 \u00b52\nL2\n)1/2 , 1 ) , there exist positive constants \u03b2, \u03b2\u2032 such that when\n\u03b2k \u2208 [\u03b2, \u03b2\u2032], the inequality \u2225I \u2212 \u03b2kA\u22252 \u2264 \u03b8 holds. If \u03b8 = ( 1 \u2212 \u00b52\nL2\n)1/2 , then \u2225I \u2212 \u03b2kA\u22252 \u2264 \u03b8 when\n\u03b2k = \u00b5/L 2.\nProof. Since (I \u2212 \u03b2kA)T(I \u2212 \u03b2kA) = I \u2212 \u03b2k(A+AT) + \u03b22kATA, it follows from Weyl\u2019s inequalities [4, Theorem III.2.1] that\n\u2225I \u2212 \u03b2kA\u222522 \u2264 \u03bbmax ( I \u2212 \u03b2k(A+AT) ) + \u03bbmax(\u03b2 2 kA TA)\n\u2264 1\u2212 \u03b2k\u03bbmin(A+AT) + \u03b22k\u2225A\u222522 \u2264 1\u2212 2\u03b2k\u00b5+ \u03b22kL2. (B.33)\nThus, to ensure \u2225I \u2212 \u03b2kA\u22252 \u2264 \u03b8, it suffices to require that\n1\u2212 2\u03b2k\u00b5+ \u03b22kL2 \u2264 \u03b82. (B.34)\nSince \u03b8 \u2208 [( 1\u2212 \u00b52\nL2\n)1/2 , 1 ) , solving (B.34) yields that \u03b2k \u2208 [\u03b2, \u03b2\u2032], where\n\u03b2 = \u00b5\u2212\n( \u00b52 \u2212 L2(1\u2212 \u03b82) )1/2 L2 , \u03b2\u2032 = \u00b5+ ( \u00b52 \u2212 L2(1\u2212 \u03b82) )1/2 L2 . (B.35)\nIf \u03b8 = ( 1\u2212 \u00b52\nL2\n)1/2 , then \u03b2k = \u00b5/L 2.\nNow, we give the proof of Theorem 4.5.\nProof. Let the notations be the same as those in the proof of Lemma 4.4. 1. For the Type-I method, let xAk and r A k denote the mk-th iterate and residual of Arnoldi\u2019s method applied to solve h\u0302(x) = 0, with the starting point xk\u2212mk . Due to Proposition 4.1, we have \u00af\u0302xk = x A k . Then, according to the known convergence of Arnoldi\u2019s method [42, Corollary 2.1 and Proposition 4.1],\n\u2225\u00af\u0302xk \u2212 x\u2217\u22252 = \u2225xAk \u2212 x\u2217\u22252 \u2264 \u221a\n1 + \u03b32k\u03ba 2 k minp\u2208Pmk\np(0)=1\n\u2225p(A)(xk\u2212mk \u2212 x\u2217)\u22252. (B.36)\nSince x\u0302k+1 = \u00af\u0302xk + \u03b2k \u00af\u0302rk, it follows that x\u0302k+1 \u2212 x\u2217 = (I \u2212 \u03b2kA)(\u00af\u0302xk \u2212 x\u2217). Hence \u2225x\u0302k+1 \u2212 x\u2217\u22252 \u2264 \u03b8k\u2225\u00af\u0302xk \u2212 x\u2217\u22252, which along with (B.36) and Lemma 4.4 yields (4.9).\nLet \u03c3min(\u00b7) denote the smallest singular value. Choose Uk \u2208 Rd\u00d7mk that satisfies range(Uk) = Kmk(A, rk\u2212mk) and UTk Uk = I. Then \u03c0k = UkUTk . Since \u03c0k and I \u2212 \u03c0k are orthogonal projectors, it follows that \u03b3k = \u2225\u03c0kA(I \u2212 \u03c0k)\u22252 \u2264 \u2225A\u22252 \u2264 L. For the restriction Ak|Kmk (A,rk\u2212mk ), we have\n\u03c3min(Ak|Kmk (A,rk\u2212mk )) = min\ny\u2208Rmk \u2225y\u22252=1 \u2225AkUky\u22252 = min y\u2208Rmk \u2225y\u22252=1 \u2225UkUTk AUky\u22252 = \u03c3min(UTk AUk).\nSince \u03c3min(U T k AUk) \u2265 \u03bbmin(S(UTk AUk)) = \u03bbmin(UTk S(A)Uk) \u2265 \u03bbmin(S(A)) \u2265 \u00b5, where the first inequality is due to Fan-Hoffman theorem [4, Proposition III.5.1], and the second inequality is due to [4, Corollary III.1.5], it follows that \u03bak = \u2225(Ak|Kmk (A,rk\u2212mk ))\n\u22121\u22252 = 1\u03c3min(Ak|Kmk (A,rk\u2212mk )) \u2264 1/\u00b5.\n2. For the Type-II method, let xGk and r G k denote the mk-th iterate and residual of GMRES\napplied to solve h\u0302(x) = 0, with the starting point xk\u2212mk . We have \u00af\u0302xk = x G k due to Proposition 4.1. It follows from the property of GMRES [40] that\n\u2225\u00af\u0302rk\u22252 = \u2225rGk \u22252 = min p\u2208Pmk p(0)=1 \u2225p(A)r\u0302k\u2212mk\u22252 \u2264 minp\u2208Pmk p(0)=1 \u2225p(A)rk\u2212mk\u22252 + \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u222522), (B.37)\nwhere the inequality is due to (4.7) and \u2225p(A)\u22252 \u2264 1 when p(A) = (I \u2212 \u00b5L2A)mk (see Lemma B.1). Since x\u0302k+1 = \u00af\u0302xk + \u03b2k \u00af\u0302rk, it follows that r\u0302k+1 = (I \u2212 \u03b2kA)\u00af\u0302rk. Hence \u2225r\u0302k+1\u22252 \u2264 \u03b8k\u2225\u00af\u0302rk\u22252, which along with (B.37), Lemma 4.4, and \u03b8k \u2264 1 + \u03b2\u2032L yields (4.10).\nIf \u03b8j \u2264 \u03b8 < 1 (ensured by Lemma B.1) for j = 0, . . . ,max{k \u2212 1, 0}, then \u2225rj\u22252 \u2264 \u2225r0\u22252, j = 0, . . . , k, (B.38)\nwhen \u2225x0 \u2212 x\u2217\u22252 is sufficiently small. We prove it by induction. For k = 0, (B.38) is clear. Suppose that k \u2265 0, and as an inductive hypothesis, (B.38) holds for k. We establish the result for k + 1. Since\n\u2225A(x\u0302k+1 \u2212 x\u2217)\u22252 = \u2225r\u0302k+1\u22252 \u2264 \u03b8k min p\u2208Pmk p(0)=1 \u2225p(A)r\u0302k\u2212mk\u22252 \u2264 \u03b8k\u2225A(xk\u2212mk \u2212 x\u2217)\u22252,\nit follows that\n\u2225A(xk+1 \u2212 x\u2217)\u22252 \u2264 \u2225A(x\u0302k+1 \u2212 x\u2217)\u22252 + \u2225A(xk+1 \u2212 x\u0302k+1)\u22252 \u2264 \u03b8\u2225A(xk\u2212mk \u2212 x\u2217)\u22252 + L\u2225xk+1 \u2212 x\u0302k+1\u22252.\nFrom Lemma 4.4, \u2225xk+1 \u2212 x\u0302k+1\u22252 = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u222522). So there exists a constant c > 0 such that \u2225xk+1\u2212 x\u0302k+1\u22252 \u2264 \u03ba\u0302c\u2225A(xk\u2212mk \u2212x\u2217)\u222522. Hence, if xk\u2212mk is chosen such that \u2225A(xk\u2212mk \u2212x\u2217)\u22252 \u2264 1\u2212\u03b82L\u03ba\u0302c , it yields \u2225A(xk+1 \u2212 x\u2217)\u22252 \u2264 1+\u03b82 \u2225A(xk\u2212mk \u2212 x\u2217)\u22252. Thus, \u2225xk+1 \u2212 x\u2217\u22252 \u2264 1+\u03b82 L\u00b5\u2225xk\u2212mk \u2212 x\u2217\u22252, which indicates xk+1 \u2208 B\u03c1\u0302(x\u2217) for \u2225xk\u2212mk \u2212 x\u2217\u22252 \u2264 2\u00b5\u03c1\u0302L(1+\u03b8) . Then due to (4.4), we have \u2225rk+1\u22252 \u2264 \u2225A(xk+1 \u2212 x\u2217)\u22252 + 12 \u03ba\u0302\u2225xk+1 \u2212 x\u2217\u222522. Therefore,\n\u2225rk+1\u22252 \u2264 1 + \u03b8 2 \u2225A(xk\u2212mk \u2212 x\u2217)\u22252 + 1 2 \u03ba\u0302\u2225xk+1 \u2212 x\u2217\u222522\n\u2264 1 + \u03b8 2\n( \u2225rk\u2212mk\u22252 + \u03ba\u0302\n2 \u2225xk\u2212mk \u2212 x\u2217\u222522\n) + 1\n2 \u03ba\u0302\n( 1 + \u03b8\n2\nL \u00b5 \u2225xk\u2212mk \u2212 x\u2217\u22252 )2 \u2264 \u03b8\u2032\u2225rk\u2212mk\u22252 + \u03ba\u0302c\u2032\u2225rk\u2212mk\u222522,\nwhere \u03b8\u2032 := 1+\u03b82 , c \u2032 > 0 is a constant, and the last inequality is due to (B.11). So by choosing \u2225xk\u2212mk \u2212 x\u2217\u22252 \u2264 1\u2212\u03b8 \u2032 2\u03ba\u0302c\u2032L , it follows that \u2225rk\u2212mk\u22252 \u2264 L\u2225xk\u2212mk \u2212 x\u2217\u22252 \u2264 1\u2212\u03b8 \u2032\n2\u03ba\u0302c\u2032 . Then \u2225rk+1\u22252 \u2264 1+\u03b8\u2032\n2 \u2225rk\u2212mk\u22252 < \u2225rk\u2212mk\u22252 \u2264 \u2225r0\u22252. Since \u2225xk\u2212mk \u2212 x\u2217\u22252 \u2264 1\u00b5\u2225rk\u2212mk\u22252 \u2264 1\u00b5\u2225r0\u22252 \u2264 L\u00b5\u2225x0 \u2212 x\u2217\u22252, the requirement that \u2225xk\u2212mk \u2212 x\u2217\u22252 \u2264 \u03c1 for some constant \u03c1 > 0 can be induced from \u2225x0 \u2212 x\u2217\u22252 \u2264 \u00b5\u03c1L . Hence, we complete the induction. Then (4.10) holds if \u03b8j \u2264 \u03b8 (j \u2265 0) and x0 is sufficiently close to x\u2217.\n3. If mk = d, then the Process II obtains the exact solution of h\u0302(x) = 0, i.e. x\u0302k+1 = x \u2217. Therefore\n\u2225xk+1 \u2212 x\u2217\u22252 = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u222522)."
        },
        {
            "heading": "B.4 Proof of Lemma 4.9",
            "text": "Proof. Since h\u2032(x) = I \u2212 g\u2032(x), it follows that for every x, y \u2208 B\u03c1\u0302(x\u2217), \u2225h\u2032(x)\u2212 h\u2032(y)\u22252 = \u2225g\u2032(x)\u2212 g\u2032(y)\u22252 \u2264 \u03ba\u0302\u2225x\u2212 y\u22252,\nwhich implies that h(x) is Lipschitz continuously differentiable in B\u03c1\u0302(x\u2217) and the Lipschitz constant of h\u2032(x) is \u03ba\u0302.\nDue to \u2225I \u2212 h\u2032(x)\u22252 = \u2225g\u2032(x)\u22252 \u2264 \u03ba < 1, we have \u2225h\u2032(x)\u22252 \u2264 \u2225I\u22252 + \u2225I \u2212 h\u2032(x)\u22252 \u2264 1 + \u03ba, and 1\n\u03c3min(h\u2032(x)) = \u2225h\u2032(x)\u22121\u22252 = \u2225\n( I \u2212 g\u2032(x) )\u22121 \u22252 \u2264 1 1\u2212 \u2225g\u2032(x)\u22252 \u2264 1 1\u2212 \u03ba,\nwhere \u03c3min(\u00b7) denotes the smallest singular value. Thus, \u03c3min(h\u2032(x)) \u2265 1 \u2212 \u03ba. The (4.2) holds for \u00b5 = 1\u2212 \u03ba and L = 1 + \u03ba. Note that\u2225\u2225I \u2212 S(h\u2032(x))\u2225\u2225\n2 \u2264 1 2 (\u2225I \u2212 h\u2032(x)\u22252 + \u2225I \u2212 h\u2032(x)T\u22252) = \u03ba. (B.39)\nLet \u03bb be an arbitrary eigenvalue of S(h\u2032(x)). Since S(h\u2032(x)) is symmetric, it follows from (B.39) that |1\u2212 \u03bb| \u2264 \u03ba, which yields 0 < 1\u2212 \u03ba \u2264 \u03bb \u2264 1 + \u03ba. Thus (4.3) also holds for \u00b5 = 1\u2212 \u03ba and L = 1 + \u03ba.\nTherefore, Assumption 4.2 is satisfied."
        },
        {
            "heading": "C Proofs of Section 5",
            "text": ""
        },
        {
            "heading": "C.1 Proof of Proposition 5.3",
            "text": "Proof. Since vTj qj \u0338= 0 for j = k\u2212mk+1, . . . , k, the procedures (3.3) and (3.4) are well defined. First, by construction, we have\npk+1 = \u2206xk \u2212 Pk\u03b6k+1 = \u2212Pk\u0393k + \u03b2kr\u0304k \u2212 Pk\u03b6k+1 = \u03b2kr\u0304k \u2212 Pk\u03d5k = \u03b2k(rk \u2212Qk\u0393k)\u2212 Pk\u03d5k = \u03b2k(I \u2212 \u03b2k\u22121A)r\u0304k\u22121 \u2212 \u03b2kQk\u0393k \u2212 Pk\u03d5k\n= \u03b2k(I \u2212 \u03b2k\u22121A) pk + Pk\u22121\u03d5k\u22121\n\u03b2k\u22121 + \u03b2kAPk\u0393k \u2212 Pk\u03d5k. (C.1)\nHere, for brevity, we define Pk = 0 \u2208 Rd, \u03d5k = 0, \u0393[0]k+1 = 0, \u03b6k+1 = 0, if mk = 0. Correspondingly, x\u0304k = xk, r\u0304k = rk, if mk = 0. We prove (5.7) by induction.\nIf mk = 1, it follows from (C.1) that\npk+1 = \u03b2k(I \u2212 \u03b2k\u22121A) pk\n\u03b2k\u22121 + \u03b2kApk\u0393k \u2212 pk\u03d5k.\nIt follows that Apk = 1\n1\u2212\u0393k\n( 1\n\u03b2k\u22121 \u2212 1\u03b2k\u03d5k ) pk \u2212 1(1\u2212\u0393k)\u03b2k pk+1, namely (5.7).\nFor mk \u2265 2, the inductive hypothesis is APk\u22121 = PkH\u0304k\u22121. With (C.1), we have\npk+1 = \u03b2k \u03b2k\u22121 (pk + Pk\u22121\u03d5k\u22121)\u2212 \u03b2kA(pk + Pk\u22121\u03d5k\u22121) + \u03b2kAPk\u0393k \u2212 Pk\u03d5k\n= Pk ( \u03b2k \u03b2k\u22121 ( \u03d5k\u22121 1 ) \u2212 \u03b2kH\u0304k\u22121 ( \u03d5k\u22121 \u2212 \u0393[mk\u22121]k ) \u2212 \u03d5k ) \u2212 \u03b2kApk ( 1\u2212 \u0393(mk)k ) .\nHence, by rearrangement, we obtain (5.7), thus completing the induction.\nSuppose that mk \u2265 1. We prove 1 \u2212 \u0393(mk)k \u0338= 0 by contradiction. If \u0393 (mk) k = 1, then r\u0304k =\nrk\u2212Qk\u0393k = rk\u2212Qk\u22121\u0393[mk\u22121]k \u2212 qk = rk\u2212Qk\u22121\u0393 [mk\u22121] k \u2212 (\u2206rk\u22121\u2212Qk\u22121\u03b6k) = rk\u22121\u2212Qk\u22121 ( \u0393 [mk\u22121] k \u2212\n\u03b6k ) \u22a5 range(Vk). Hence r\u0304k = r\u0304k\u22121 due to r\u0304k \u22a5 range(Vk\u22121). For the Type-I method, vk = pk = \u03b2k\u22121r\u0304k\u22121 \u2212 Pk\u22121\u03d5k\u22121, so 0 = r\u0304Tk\u22121pk = \u03b2k\u22121r\u0304Tk\u22121r\u0304k\u22121, which indicates r\u0304k\u22121 = 0. For the TypeII method, vk = qk = \u2212Apk = \u2212\u03b2k\u22121Ar\u0304k\u22121 \u2212 Qk\u22121\u03d5k\u22121, so 0 = r\u0304Tk\u22121qk = \u2212\u03b2k\u22121r\u0304Tk\u22121Ar\u0304k\u22121, which indicates r\u0304k\u22121 = 0 since A is positive definite. However, r\u0304k\u22121 = 0 yields that rk = (I\u2212\u03b2k\u22121A)r\u0304k\u22121 = 0, which is impossible because the algorithm has not found the exact solution. As a result, 1\u2212\u0393(mk)k \u0338= 0. Thus (5.5) and (5.6) are well defined."
        },
        {
            "heading": "C.2 Proof of Lemma 5.5",
            "text": "Proof. From (B.9) in the proof of Lemma 4.4 and the assumption |1 \u2212 \u0393(mk)k | \u2265 \u03c40, with sufficiently small \u2225x0 \u2212 x\u2217\u22252, we can ensure |\u0393(mk)k \u2212 \u0393\u0302 (mk) k | = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u22252) and |1\u2212 \u0393\u0302\n(mk) k | \u2265 12\u03c40. Thus\u2223\u2223\u2223\u2223\u2223 11\u2212 \u0393(mk)k \u2212 1 1\u2212 \u0393\u0302(mk)k \u2223\u2223\u2223\u2223\u2223 = |\u0393 (mk) k \u2212 \u0393\u0302 (mk) k | |(1\u2212 \u0393(mk)k )(1\u2212 \u0393\u0302 (mk) k )| = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u22252). (C.2)\nWe prove (5.8) by induction. The same as hk, h (mk+1) k , Hk, H\u0304k, \u03d5k in Process I, the notations\nh\u0302k, h\u0302 (mk+1) k , H\u0302k, \u00af\u0302 Hk, \u03d5\u0302k are defined for Process II, correspondingly.\nIf mk = 1, then\u2223\u2223\u2223hk \u2212 h\u0302k\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223 11\u2212 \u0393k ( 1 \u03b2k\u22121 \u2212 1 \u03b2k \u03d5k ) \u2212 1 1\u2212 \u0393\u0302k ( 1 \u03b2k\u22121 \u2212 1 \u03b2k \u03d5\u0302k )\u2223\u2223\u2223\u2223 \u2264 \u2223\u2223\u2223\u2223\u2223 11\u2212 \u0393k \u00b7 \u03d5\u0302k \u2212 \u03d5k\u03b2k \u2223\u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223( 11\u2212 \u0393k \u2212 11\u2212 \u0393\u0302k ) \u00b7 ( 1 \u03b2k\u22121 \u2212 1 \u03b2k \u03d5\u0302k\n)\u2223\u2223\u2223\u2223 = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u22252), (C.3)\nbecause of (C.2), and (B.6), (B.9) in the proof of Lemma 4.4. Also, \u2225Hk\u22252 = |hk| = O(1). Suppose that mk \u2265 2, and as an inductive hypothesis, \u2225Hk\u22121 \u2212 H\u0302k\u22121\u22252 = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u22252), \u2225Hk\u22121\u22252 = O(1). First, due to (C.2), we have\u2223\u2223\u2223h(mk)k\u22121 \u2212 h\u0302(mk)k\u22121 \u2223\u2223\u2223 = 1\u03b2k\u22121 \u2223\u2223\u2223\u2223\u2223 11\u2212 \u0393(mk\u22121)k\u22121 \u2212 1 1\u2212 \u0393\u0302(mk\u22121)k\u22121\n\u2223\u2223\u2223\u2223\u2223 = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u22252). Also, |h(mk)k\u22121 | \u2264 1\u03b2\u03c40 , and mk \u2264 m. Thus for H\u0304k\u22121 and \u00af\u0302 Hk\u22121, we have that\n\u2225H\u0304k\u22121 \u2212 \u00af\u0302Hk\u22121\u22252 = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u22252), \u2225H\u0304k\u22121\u22252 = O(1). (C.4) As a result, \u2225\u2225\u2225H\u0304k\u22121(\u03d5k\u22121 \u2212 \u0393[mk\u22121]k )\u2212 \u00af\u0302Hk\u22121(\u03d5\u0302k\u22121 \u2212 \u0393\u0302[mk\u22121]k )\u2225\u2225\u2225\n2 \u2264 \u2225\u2225\u2225H\u0304k\u22121 (\u03d5k\u22121 \u2212 \u0393[mk\u22121]k \u2212 (\u03d5\u0302k\u22121 \u2212 \u0393\u0302[mk\u22121]k ))\u2225\u2225\u2225\n2 + \u2225\u2225\u2225(H\u0304k\u22121 \u2212 \u00af\u0302Hk\u22121)(\u03d5\u0302k\u22121 \u2212 \u0393\u0302[mk\u22121]k )\u2225\u2225\u2225\n2\n\u2264 \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u22252),\nand \u2225H\u0304k\u22121(\u03d5k\u22121 \u2212 \u0393[mk\u22121]k )\u22252 = O(1). Besides,\u2225\u2225\u2225\u2225 1\u03b2k\u22121 ( \u03d5k\u22121 1 ) \u2212 1 \u03b2k \u03d5k \u2212 ( 1 \u03b2k\u22121 ( \u03d5\u0302k\u22121 1 ) \u2212 1 \u03b2k \u03d5\u0302k )\u2225\u2225\u2225\u2225 2\n= \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u22252),\u2225\u2225\u2225\u2225 1\u03b2k\u22121 ( \u03d5k\u22121 1 ) \u2212 1 \u03b2k \u03d5k \u2225\u2225\u2225\u2225 2 = O(1).\nTherefore, \u2225(1\u2212\u0393(mk)k )hk \u2212 (1\u2212 \u0393\u0302 (mk) k )h\u0302k\u22252 = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u22252), \u2225(1\u2212\u0393 (mk) k )hk\u22252 = O(1). Hence,\n\u2225hk \u2212 h\u0302k\u22252 \u2264 \u2225\u2225\u2225\u2225\u2225 11\u2212 \u0393(mk)k ( (1\u2212 \u0393(mk)k )hk \u2212 (1\u2212 \u0393\u0302 (mk) k )h\u0302k )\u2225\u2225\u2225\u2225\u2225 2\n+ \u2225\u2225\u2225\u2225\u2225 (\n1 1\u2212 \u0393(mk)k \u2212 1 1\u2212 \u0393\u0302(mk)k\n) \u00b7 ( 1\u2212 \u0393\u0302(mk)k ) h\u0302k \u2225\u2225\u2225\u2225\u2225 2 = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u22252),\nand \u2225hk\u22252 = O(1), which together with (C.4) and mk \u2264 m implies that (5.8) holds. Thus we complete the induction."
        },
        {
            "heading": "D Proofs of Section 6",
            "text": ""
        },
        {
            "heading": "D.1 Proof of Theorem 6.2",
            "text": "Proof. Consider the two processes defined in Definition 4.3. Here, we replace the restarted AMmethod by the restarted ST-AM method. Note that the restarted ST-AM is obtained from the restarted AM by setting \u03b6 (j) k = 0 for j \u2264 k \u2212 3, and \u0393 (j) k = 0 for j \u2264 k \u2212 2. Similar to Lemma 4.4, it can be proved that \u2225rk \u2212 r\u0302k\u22252 = \u03ba\u0302 \u00b7 O(\u2225xk\u2212mk \u2212 x\u2217\u222522), \u2225xk+1 \u2212 x\u0302k+1\u22252 = \u03ba\u0302 \u00b7 O(\u2225xk\u2212mk \u2212 x\u2217\u222522), (D.1)\nprovided that there exists a constant \u03b70 > 0 such that\n\u2225rj\u22252 \u2264 \u03b70\u2225r0\u22252, j = 0, . . . , k, (D.2)\nand x0 \u2208 B\u03c1\u0302(x\u2217) is sufficiently close to x\u2217. Since \u03b8k = \u2225I \u2212 \u03b2kA\u22252 \u2264 \u03b8, there are positive constants \u03b2, \u03b2\u2032 such that \u03b2 \u2264 \u03b2k \u2264 \u03b2\u2032. In fact, by\nchoosing \u03b2k \u2208 [ 1\u2212\u03b8 \u00b5 , 1+\u03b8 L ] , we can ensure \u03b8k \u2264 max{|1\u2212 \u03b2kL|, |1\u2212 \u03b2k\u00b5|} \u2264 \u03b8 < 1. We give the proof of the Type-I method here. For the restarted Type-I ST-AM method, if x0 is sufficiently close to x \u2217, then\n\u2225xj \u2212 x\u2217\u2225A \u2264 \u2225x0 \u2212 x\u2217\u2225A, j = 0, . . . , k. (D.3)\nWe prove (D.2) and (D.3) hold for the Type-I method by induction. For k = 0, (D.2) and (D.3) hold. Suppose that for k \u2265 0, the results hold for k. We establish the results for k + 1. Let xAk and rAk denote the mk-th iterate and residual of Arnoldi\u2019s method applied to solve h\u0302(x) = 0, with the starting point xk\u2212mk . Due to Proposition 6.1 and Proposition 4.1, we have \u00af\u0302xk = x A k . Hence\n\u2225x\u0302k+1 \u2212 x\u2217\u2225A \u2264 \u03b8k\u2225\u00af\u0302xk \u2212 x\u2217\u2225A = \u03b8k\u2225xAk \u2212 x\u2217\u2225A = \u03b8k min\np\u2208Pmk p(0)=1\n\u2225p(A)(xk\u2212mk \u2212 x\u2217)\u2225A \u2264 \u03b8k\u2225xk\u2212mk \u2212 x\u2217\u2225A. (D.4)\nHere, we use the fact that \u2225I \u2212 \u03b2kA\u2225A = \u2225I \u2212 \u03b2kA\u22252. From (D.1), it follows that \u2225xk+1 \u2212 x\u0302k+1\u2225A = \u03ba\u0302O(\u2225xk\u2212mk \u2212x\u2217\u22252A). Then, there is a constant c1 > 0 such that \u2225xk+1\u2212 x\u0302k+1\u2225A \u2264 \u03ba\u0302c1\u2225xk\u2212mk \u2212x\u2217\u22252A. With (D.4), we have\n\u2225xk+1 \u2212 x\u2217\u2225A \u2264 \u03b8\u2225xk\u2212mk \u2212 x\u2217\u2225A + \u03ba\u0302c1\u2225xk\u2212mk \u2212 x\u2217\u22252A. (D.5) Then \u2225xk+1 \u2212 x\u2217\u2225A \u2264 1+\u03b82 \u2225xk\u2212mk \u2212 x\u2217\u2225A provided \u2225xk\u2212mk \u2212 x\u2217\u2225A \u2264 1\u2212\u03b82\u03ba\u0302c1 , which can be satisfied by choosing \u2225x0 \u2212 x\u2217\u22252 \u2264 1\u2212\u03b82\u221aL\u03ba\u0302c1 , since by the inductive hypothesis, \u2225xk\u2212mk \u2212 x\n\u2217\u2225A \u2264 \u2225x0 \u2212 x\u2217\u2225A \u2264\u221a L\u2225x0 \u2212 x\u2217\u22252. Thus, \u2225xk+1 \u2212 x\u2217\u2225A < \u2225xk\u2212mk \u2212 x\u2217\u2225A \u2264 \u2225x0 \u2212 x\u2217\u2225A, namely (D.3) for k + 1. Also, \u2225xk+1\u2212x\u2217\u22252 \u2264 1\u221a\u00b5\u2225xk+1\u2212x\u2217\u2225A \u2264 1\u221a\u00b5\u2225x0\u2212x\u2217\u2225A \u2264 \u221a L\u221a \u00b5\u2225x0\u2212x\u2217\u22252. So we can impose \u2225x0\u2212x\u2217\u22252 \u2264 \u221a \u00b5\u03c1\u0302\u221a L to ensure xk+1 \u2208 B\u03c1\u0302(x\u2217), which further yields that \u2225rk+1\u22252 \u2264 L\u2225xk+1 \u2212 x\u2217\u22252 \u2264 L \u221a L\u221a \u00b5 \u2225x0 \u2212 x\u2217\u22252 \u2264 L \u221a L\n\u00b5 \u221a \u00b5 \u2225r0\u22252, namely (D.2) for k + 1, and \u03b70 = L\n\u221a L\n\u00b5 \u221a \u00b5 . Hence, we complete the induction.\nSince A is SPD, we can use the Chebyshev polynomial to obtain\nmin p\u2208Pmk p(0)=1 \u2225p(A)\u22252 \u2264 min p\u2208Pmk p(0)=1 max \u03bb\u2208[\u00b5,L]\n|p(\u03bb)| \u2264 2 (\u221a\nL/\u00b5\u2212 1\u221a L/\u00b5+ 1\n)mk , (D.6)\nwhich is a classical result [43, Section 6.11.3]. Note that \u2225p(A)(xk\u2212mk \u2212 x\u2217)\u2225A \u2264 \u2225p(A)\u2225A\u2225xk\u2212mk \u2212 x\u2217\u2225A = \u2225p(A)\u22252\u2225xk\u2212mk \u2212 x\u2217\u2225A. Thus, by choosing x0 sufficiently close to x\u2217, (6.8) holds as a result of (D.4), (D.6), and (D.1).\nFor the Type-II method, since \u03b8k \u2264 \u03b8 < 1, the bound (4.10) can be established following the similar approach to proving Theorem 4.5. With (D.6), the bound (6.9) holds."
        },
        {
            "heading": "D.2 Proof of Theorem 6.5",
            "text": "Proof. The same as t (mk+1) k , Tk in Process I, the notations t\u0302 (mk+1) k , T\u0302k are defined for Process II, correspondingly. In this case, the tridiagonal matrix T\u0302k can be diagonalized. Let A := h \u2032(x\u2217). Then AQ\u0302k = Q\u0302kT\u0302k + t\u0302 (mk+1) k q\u0302k+1e T mk . Hence\nV\u0302 Tk AQ\u0302k = V\u0302 T k Q\u0302kT\u0302k,\ndue to V\u0302 Tk q\u0302k+1 = 0. Thus T\u0302k = (V\u0302 T k Q\u0302k) \u22121V\u0302 Tk AQ\u0302k. Here, V\u0302 T k Q\u0302k and V\u0302 T k AQ\u0302k are symmetric for both types of ST-AM methods. Define W\u0302k = \u2212V\u0302 Tk Q\u0302k for the Type-I method, and W\u0302k = V\u0302 Tk Q\u0302k for the Type-II method. Then\nW\u0302 1/2 k T\u0302kW\u0302 \u22121/2 k = \u2213W\u0302 \u22121/2 k (V\u0302 T k AQ\u0302k)W\u0302 \u22121/2 k , (D.7)\nwhere the sign is \u201c\u2212\u201d for the Type-I method, and \u201c+\u201d for the Type-II method. The right side in (D.7) is symmetric, so there exists an orthonormal matrix U\u0302k \u2208 Rmk\u00d7mk such that\nT\u0302k = W\u0302 \u22121/2 k U\u0302 T k D\u0302kU\u0302kW\u0302 1/2 k , (D.8)\nwhere D\u0302k is a diagonal matrix formed by the eigenvalues of T\u0302k. Also, similar to the proof of Lemma 4.4, the relations (B.23), (B.7), and (B.8) also hold for the ST-AM methods. Note that V\u0302 Tk Q\u0302k is diagonal. We have\n\u2225V\u0302 Tk Q\u0302k\u22252\u2225(V\u0302 Tk Q\u0302k)\u22121\u22252 = maxk\u2212mk+1\u2264i\u2264k{|v\u0302Ti q\u0302i|} mink\u2212mk+1\u2264j\u2264k{|v\u0302Tj q\u0302j |} = O(1).\nThus \u2225W\u0302 1/2k \u22252\u2225W\u0302 \u22121/2 k \u22252 = O(1). Also, similar to Lemma 5.5, we have \u2225Tk \u2212 T\u0302k\u22252 = \u03ba\u0302O(\u2225xk\u2212mk \u2212 x\u2217\u22252). Hence, the result (6.14) follows from Bauer-Fike theorem."
        },
        {
            "heading": "E Additional experimental results",
            "text": ""
        },
        {
            "heading": "E.1 Solving linear systems",
            "text": "To verify the theoretical properties of the AM and ST-AM methods for solving linear systems, we considered solving\nAx = b, (E.1)\nwhere A \u2208 Rd\u00d7d, b \u2208 Rd, the residual is defined as rk = b \u2212 Axk at xk. The fixed-point iteration is the Richardson\u2019s iteration xk+1 = xk + \u03b2rk, where \u03b2 was chosen to ensure linear convergence. For the restarted AM and restarted ST-AM, the restarting conditions were disabled since (E.1) is linear. AM-I and AM-II used (5.11) with \u03b20 = 1 to choose \u03b2k; ST-AM-I and ST-AM-II used (6.15) with \u03b20 = 1 to choose \u03b2k.\nE.1.1 Nonsymmetric linear system\nThe matrix A \u2208 R100\u00d7100 was randomly generated from Gaussian distribution and was further modified by making all the eigenvalues have positive real parts.\nThe results are shown in Figure 4 and Figure 5. The convergence behaviours of \u2225r\u0304k\u22252/\u2225r0\u22252 and \u2225rk\u22252/\u2225r0\u22252 verify Proposition 3.1, Proposition 4.1 and Theorem 4.5. The eigenvalue estimates well approximate the exact eigenvalues of A, which justifies the adaptive mixing strategy."
        },
        {
            "heading": "E.1.2 SPD linear system",
            "text": "We first generated a matrix B \u2208 R100\u00d7100 from Gaussian distribution, then chose A = BTB. In this case, the conjugate gradient (CG) method [25] and the conjugate residual (CR) method [43, Algorithm 6.20], which have short-term recurrences, are equivalent to Arnoldi\u2019s method and GMRES, respectively.\nThe results in Figure 6 verify the properties of the ST-AM methods. We see the intermediate residuals {r\u0304k} of ST-AM-I/ST-AM-II match the residuals {rk} of the CG/CR method during the\nfirst 30 iterations. However, the equivalence cannot exactly hold in the later iterations due to the loss of global orthogonality in finite arithmetic. Nonetheless, the convergence of ST-AM-I/ST-AM-II is comparable to that of CG/CR. Figure 7 shows that the eigenvalue estimates from ST-AM well approximate the exact eigenvalues of A.\nE.2 Additional results of solving the modified Bratu problems\nWe provide details about the eigenvalue estimates and show the effect of \u03b2k on the convergence."
        },
        {
            "heading": "E.2.1 Nonsymmetric Jacobian",
            "text": "To verify Theorem 5.6, we compared the eigenvalue estimates with the Ritz values of F \u2032(U\u2217) where F (U\u2217) = 0. The Ritz values were obtained from the k-step Arnoldi\u2019s method [41] (denoted by\nArnoldi(k)). Figure 8 indicates that the extreme Ritz values are well approximated, which accounts for the proper choices of \u03b2k.\nWe also tested AM-I and AM-II with fixed \u03b2k. Figure 9 shows that the choice of \u03b2k can largely affect the convergence behaviours of both methods, and the adaptive mixing strategy performs well. It is worth noting that for the Picard iteration, choosing \u03b2 from {10\u22125, . . . , 10\u22122} causes divergence, which suggests \u03b8k > 1 in (4.9) and (4.10) when \u03b2k \u2208 {10\u22125, . . . , 10\u22122}. Nevertheless, the residual norms of the restarted AM methods can still converge since the minimization problems in (4.9) and (4.10) dominate the convergence when mk is large."
        },
        {
            "heading": "E.2.2 Symmetric Jacobian",
            "text": "Figure 10 shows the eigenvalue estimates computed by ST-AM-I/ST-AM-II and the Ritz values of F \u2032(U\u2217) computed by the k-step symmetric Lanczos method [22] (denoted by Lanczos(k)), where\nF (U\u2217) = 0. It is observed that the eigenvalue estimates well approximate the Ritz values, which verifies Theorem 6.5.\nE.3 Additional results of solving the Chandrasekhar H-equation\nFigure 11 shows the Ritz values of F \u2032(h\u2217) and the eigenvalue estimates, where h\u2217 is the solution. We computed the Ritz values of F \u2032(h\u2217) by applying 500 steps of Arnoldi\u2019s method to G\u2032(h\u2217). It is observed in Figure 11 that most Ritz values are nearly 1, which accounts for the efficiency of the simple Picard iteration for solving this problem. Since the eigenvalues form 3 clusters, we also computed three eigenvalue estimates by AM-I/AM-II (\u03b7 = \u221e,m = 100, \u03c4 = 10\u221215, and \u03b2k \u2261 1). We find the eigenvalue estimates still roughly match the Ritz values in the cases \u03c9 = 0.5 and \u03c9 = 0.99. For \u03c9 = 1, the Jacobian F \u2032(h\u2217) is singular, so the error in estimating the eigenvalue zero is large.\nE.4 Additional results of solving the regularized logistic regression\nFigure 12 shows the eigenvalue estimates computed by the eigenvalue estimation procedure of STAM-I/ST-AM-II at the last iteration. The comparison with the Ritz values of \u22072f(x\u2217) indicates that the extreme eigenvalues are well approximated."
        }
    ],
    "title": "Convergence Analysis for Restarted Anderson Mixing and Beyond",
    "year": 2023
}