{
    "authors": [
        {
            "affiliations": [],
            "name": "Omid Abbasi"
        },
        {
            "affiliations": [],
            "name": "Nadine Steingr\u00e4ber"
        },
        {
            "affiliations": [],
            "name": "Nikos Chalas"
        },
        {
            "affiliations": [],
            "name": "Daniel S. Kluger"
        },
        {
            "affiliations": [],
            "name": "Joachim Gross"
        }
    ],
    "id": "SP:6ac6a82f2c02e6fb79e44247d824d6344ed73b54",
    "references": [
        {
            "authors": [
                "S.C. Levinson"
            ],
            "title": "Turn-taking in Human Communication--Origins and Implications for Language Processing",
            "venue": "Trends Cogn Sci (Regul Ed) 20,",
            "year": 2016
        },
        {
            "authors": [
                "K.J. Friston",
                "C.D. Frith"
            ],
            "title": "Active inference, communication and hermeneutics",
            "venue": "Cortex 68,",
            "year": 2015
        },
        {
            "authors": [
                "G. Hickok"
            ],
            "title": "Computational neuroanatomy of speech production",
            "venue": "Nat. Rev. Neurosci. 13,",
            "year": 2012
        },
        {
            "authors": [
                "J Gross"
            ],
            "title": "The neural basis of intermittent motor control in humans",
            "venue": "Proc Natl Acad Sci USA",
            "year": 2002
        },
        {
            "authors": [
                "M.J. Pickering",
                "S. Garrod"
            ],
            "title": "An integrated theory of language production and comprehension",
            "venue": "Behav. Brain Sci",
            "year": 2013
        },
        {
            "authors": [
                "J.F. Houde",
                "S.S. Nagarajan",
                "K. Sekihara",
                "M.M. Merzenich"
            ],
            "title": "Modulation of the auditory cortex during speech: an MEG study",
            "venue": "J. Cogn. Neurosci",
            "year": 2002
        },
        {
            "authors": [
                "G. Curio",
                "G. Neuloh",
                "J. Numminen",
                "V. Jousm\u00e4ki",
                "R. Hari"
            ],
            "title": "Speaking modifies voice-evoked activity in the human auditory cortex",
            "venue": "Hum. Brain Mapp",
            "year": 2000
        },
        {
            "authors": [
                "X. Tian",
                "D. Poeppel"
            ],
            "title": "Dynamics of self-monitoring and error detection in speech production: evidence from mental imagery and MEG",
            "venue": "J. Cogn. Neurosci",
            "year": 2015
        },
        {
            "authors": [
                "J.F. Houde",
                "S.S. Nagarajan"
            ],
            "title": "Speech production as state feedback control",
            "venue": "Front. Hum. Neurosci. 5,",
            "year": 2011
        },
        {
            "authors": [
                "N Ding"
            ],
            "title": "Temporal modulations in speech and music",
            "venue": "Neurosci. Biobehav. Rev",
            "year": 2017
        },
        {
            "authors": [
                "L.H. Arnal",
                "Giraud",
                "A.-L"
            ],
            "title": "Cortical oscillations and sensory predictions",
            "venue": "Trends Cogn Sci (Regul Ed)",
            "year": 2012
        },
        {
            "authors": [
                "H. Park",
                "R.A.A. Ince",
                "P.G. Schyns",
                "G. Thut",
                "J. Gross"
            ],
            "title": "Frontal top-down signals increase coupling of auditory low-frequency oscillations to continuous speech in human listeners",
            "venue": "Curr. Biol",
            "year": 2015
        },
        {
            "authors": [
                "Wang",
                "X.-J"
            ],
            "title": "Neurophysiological and computational principles of cortical rhythms in cognition",
            "venue": "Physiol. Rev",
            "year": 2010
        },
        {
            "authors": [
                "Bastos",
                "A. M"
            ],
            "title": "Visual areas exert feedforward and feedback influences through distinct frequency channels",
            "venue": "Neuron 85,",
            "year": 2015
        },
        {
            "authors": [
                "J. Gross"
            ],
            "title": "Let the Rhythm Guide You: Non-invasive Tracking of Cortical Communication Channels",
            "venue": "Neuron 89,",
            "year": 2016
        },
        {
            "authors": [
                "G Michalareas"
            ],
            "title": "Alpha-Beta and Gamma Rhythms Subserve Feedback and Feedforward Influences among Human Visual Cortical Areas",
            "venue": "Neuron 89,",
            "year": 2016
        },
        {
            "authors": [
                "Giraud",
                "A.-L",
                "D. Poeppel"
            ],
            "title": "Cortical oscillations and speech processing: emerging computational principles and operations",
            "venue": "Nat. Neurosci",
            "year": 2012
        },
        {
            "authors": [
                "J Gross"
            ],
            "title": "Speech rhythms and multiplexed oscillatory sensory coding in the human brain",
            "venue": "PLoS Biol. 11,",
            "year": 2013
        },
        {
            "authors": [
                "C.E. Schroeder",
                "P. Lakatos"
            ],
            "title": "Low-frequency neuronal oscillations as instruments of sensory selection",
            "venue": "Trends Neurosci",
            "year": 2009
        },
        {
            "authors": [
                "N.A. Busch",
                "J. Dubois",
                "R. VanRullen"
            ],
            "title": "The phase of ongoing EEG oscillations predicts visual perception",
            "venue": "J. Neurosci",
            "year": 2009
        },
        {
            "authors": [
                "N. Ding",
                "J.Z. Simon"
            ],
            "title": "Neural coding of continuous speech in auditory cortex during monaural and dichotic listening",
            "venue": "J. Neurophysiol",
            "year": 2012
        },
        {
            "authors": [
                "P Lakatos"
            ],
            "title": "The spectrotemporal filter mechanism of auditory selective attention",
            "venue": "Neuron 77,",
            "year": 2013
        },
        {
            "authors": [
                "J.E. Peelle",
                "J. Gross",
                "M.H. Davis"
            ],
            "title": "Phase-locked responses to speech in human auditory cortex are enhanced during comprehension",
            "venue": "Cereb. Cortex",
            "year": 2013
        },
        {
            "authors": [
                "E. Zion-Golumbic",
                "C.E. Schroeder"
            ],
            "title": "Attention modulates \u201cspeech-tracking\u201d at a cocktail party",
            "venue": "Trends Cogn Sci (Regul Ed)",
            "year": 2012
        },
        {
            "authors": [
                "L.Y. Ganushchak",
                "I.K. Christoffels",
                "N.O. Schiller"
            ],
            "title": "The use of electroencephalography in language production research: a review",
            "venue": "Front. Psychol",
            "year": 2011
        },
        {
            "authors": [
                "J. Gehrig",
                "M. Wibral",
                "C. Arnold",
                "C.A. Kell"
            ],
            "title": "Setting up the speech production network: how oscillations contribute to lateralized information routing",
            "venue": "Front. Psychol",
            "year": 2012
        },
        {
            "authors": [
                "A. Llorens",
                "A. Tr\u00e9buchon",
                "C. Li\u00e9geois-Chauvel",
                "Alario",
                "F.-X"
            ],
            "title": "Intra-cranial recordings of brain activity during language production",
            "venue": "Front. Psychol",
            "year": 2011
        },
        {
            "authors": [
                "C.J. Price"
            ],
            "title": "The anatomy of language: a review of 100 fMRI studies",
            "venue": "https://doi.org/10.1101/2022.11.17.516860 doi: bioRxiv preprint 29 Acad. Sci. 1191,",
            "year": 2009
        },
        {
            "authors": [
                "A. Borovsky",
                "A.P. Saygin",
                "E. Bates",
                "N. Dronkers"
            ],
            "title": "Lesion correlates of conversational speech production deficits",
            "venue": "Neuropsychologia 45,",
            "year": 2007
        },
        {
            "authors": [
                "Ri\u00e8s",
                "S. K"
            ],
            "title": "Spatiotemporal dynamics of word retrieval in speech production revealed by cortical high-frequency band activity",
            "venue": "Proc Natl Acad Sci USA 114,",
            "year": 2017
        },
        {
            "authors": [
                "N. Janssen",
                "Meij",
                "M. van der",
                "P.J. L\u00f3pez-P\u00e9rez",
                "H.A. Barber"
            ],
            "title": "Exploring the temporal dynamics of speech production with EEG and group",
            "venue": "ICA. Sci. Rep. 10,",
            "year": 2020
        },
        {
            "authors": [
                "M. Liljestr\u00f6m",
                "J. Kujala",
                "C. Stevenson",
                "R. Salmelin"
            ],
            "title": "Dynamic reconfiguration of the language network preceding onset of speech in picture naming",
            "venue": "Hum. Brain Mapp",
            "year": 2015
        },
        {
            "authors": [
                "A. Fairs",
                "A. Michelas",
                "S. Dufour",
                "K. Strijkers"
            ],
            "title": "The Same Ultra-Rapid Parallel Brain Dynamics Underpin the Production and Perception of Speech",
            "venue": "Cereb. Cortex Commun. 2,",
            "year": 2021
        },
        {
            "authors": [
                "T. Saarinen",
                "H. Laaksonen",
                "T. Parviainen",
                "R. Salmelin"
            ],
            "title": "Motor cortex dynamics in visuomotor production of speech and non-speech mouth movements",
            "venue": "Cereb. Cortex 16,",
            "year": 2006
        },
        {
            "authors": [
                "I Ruspantini"
            ],
            "title": "Corticomuscular coherence is tuned to the spontaneous rhythmicity of speech",
            "venue": "Hz. J. Neurosci",
            "year": 2012
        },
        {
            "authors": [
                "A.M. Alexandrou",
                "T. Saarinen",
                "S. M\u00e4kel\u00e4",
                "J. Kujala",
                "R. Salmelin"
            ],
            "title": "The right hemisphere is highlighted in connected natural speech production and perception",
            "venue": "Neuroimage 152,",
            "year": 2017
        },
        {
            "authors": [
                "A. P\u00e9rez",
                "M. Carreiras",
                "J.A. Du\u00f1abeitia"
            ],
            "title": "Brain-to-brain entrainment: EEG interbrain synchronization while speaking and listening",
            "venue": "Sci. Rep",
            "year": 2017
        },
        {
            "authors": [
                "A P\u00e9rez"
            ],
            "title": "Joint entrainment to the speech envelope during speaking and listening cannot completely explain brain-to-brain synchronisation",
            "year": 2021
        },
        {
            "authors": [
                "M Bourguignon"
            ],
            "title": "Neocortical activity tracks the hierarchical linguistic structures of selfproduced speech during reading aloud",
            "venue": "Neuroimage 216,",
            "year": 2020
        },
        {
            "authors": [
                "O. Abbasi",
                "N. Steingr\u00e4ber",
                "J. Gross"
            ],
            "title": "Correcting MEG artifacts caused by overt speech",
            "venue": "Front. Neurosci",
            "year": 2021
        },
        {
            "authors": [
                "J Gross"
            ],
            "title": "Comparison of undirected frequency-domain connectivity measures for cerebroperipheral analysis",
            "venue": "Neuroimage 245,",
            "year": 2021
        },
        {
            "authors": [
                "Ince",
                "R.A. A"
            ],
            "title": "A statistical framework for neuroimaging data analysis based on mutual information estimated via a gaussian copula",
            "venue": "Hum. Brain Mapp",
            "year": 2017
        },
        {
            "authors": [
                "N. Ding",
                "J.Z. Simon"
            ],
            "title": "Adaptive temporal encoding leads to a background-insensitive cortical representation of speech",
            "venue": "J. Neurosci",
            "year": 2013
        },
        {
            "authors": [
                "T.M. Elliott",
                "F.E. Theunissen"
            ],
            "title": "The modulation transfer function for speech intelligibility",
            "venue": "PLoS Comput. Biol. 5,",
            "year": 2009
        },
        {
            "authors": [
                "M Schaum"
            ],
            "title": "Right inferior frontal gyrus implements motor inhibitory control via beta-band oscillations in humans",
            "venue": "eLife 10,",
            "year": 2021
        },
        {
            "authors": [
                "H. Park",
                "R.A.A. Ince",
                "P.G. Schyns",
                "G. Thut",
                "J. Gross"
            ],
            "title": "Representational interactions during audiovisual speech entrainment: Redundancy in left posterior superior temporal gyrus and synergy in left motor cortex",
            "venue": "PLoS Biol. 16,",
            "year": 2018
        },
        {
            "authors": [
                "A. Keitel",
                "J. Gross",
                "C. Kayser"
            ],
            "title": "Perceptually relevant speech tracking in auditory and motor cortex reflects distinct linguistic features",
            "venue": "PLoS Biol. 16,",
            "year": 2018
        },
        {
            "authors": [
                "M Bourguignon"
            ],
            "title": "The pace of prosodic phrasing couples the listener\u2019s cortex to the reader\u2019s voice",
            "venue": "Hum. Brain Mapp",
            "year": 2013
        },
        {
            "authors": [
                "H. Luo",
                "D. Poeppel"
            ],
            "title": "Phase patterns of neuronal responses reliably discriminate speech in human auditory cortex",
            "venue": "Neuron 54,",
            "year": 2007
        },
        {
            "authors": [
                "Cogan",
                "G. B"
            ],
            "title": "Sensory-motor transformations for speech occur bilaterally",
            "venue": "Nature 507,",
            "year": 2014
        },
        {
            "authors": [
                "M Vander Ghinst"
            ],
            "title": "Left Superior Temporal Gyrus Is Coupled to Attended Speech in a Cocktail-Party Auditory Scene",
            "venue": "J. Neurosci",
            "year": 2016
        },
        {
            "authors": [
                "A.J. Simmonds",
                "R. Leech",
                "C. Collins",
                "O. Redjep",
                "R.J.S. Wise"
            ],
            "title": "Sensory-motor integration during speech production localizes to both left and right plana temporale",
            "venue": "J. Neurosci",
            "year": 2014
        },
        {
            "authors": [
                "V. Piai",
                "X. Zheng"
            ],
            "title": "Speaking waves: Neuronal oscillations in language production",
            "venue": "in vol",
            "year": 2019
        },
        {
            "authors": [
                "G. Hickok"
            ],
            "title": "The cortical organization of speech processing: feedback control and predictive coding the context of a dual-stream model",
            "venue": "J. Commun. Disord",
            "year": 2012
        },
        {
            "authors": [
                "J.F. Houde",
                "E.F. Chang"
            ],
            "title": "The cortical computations underlying feedback control in vocal production",
            "venue": "Curr. Opin. Neurobiol",
            "year": 2015
        },
        {
            "authors": [
                "L.H. Arnal",
                "V. Wyart",
                "Giraud",
                "A.-L"
            ],
            "title": "Transitions in neural oscillations reflect prediction errors generated in audiovisual speech",
            "venue": "Nat. Neurosci",
            "year": 2011
        },
        {
            "authors": [
                "L. Fontolan",
                "B. Morillon",
                "C. Liegeois-Chauvel",
                "Giraud",
                "A.-L"
            ],
            "title": "The contribution of frequencyspecific activity to hierarchical information processing in the human auditory cortex",
            "venue": "Nat. Commun",
            "year": 2014
        },
        {
            "authors": [
                "W Sedley"
            ],
            "title": "Neural signatures of perceptual inference",
            "venue": "eLife 5,",
            "year": 2016
        },
        {
            "authors": [
                "Z.C. Chao",
                "K. Takaura",
                "L. Wang",
                "N. Fujii",
                "S. Dehaene"
            ],
            "title": "Large-Scale Cortical Networks for Hierarchical Prediction and Prediction Error in the Primate Brain",
            "venue": "Neuron 100,",
            "year": 2018
        },
        {
            "authors": [
                "H.G. Yi",
                "M.K. Leonard",
                "E.F. Chang"
            ],
            "title": "The encoding of speech sounds in the superior temporal gyrus",
            "venue": "Neuron 102,",
            "year": 2019
        },
        {
            "authors": [
                "O. Abbasi",
                "J. Gross"
            ],
            "title": "Beta-band oscillations play an essential role in motor-auditory interactions",
            "venue": "Hum. Brain Mapp",
            "year": 2020
        },
        {
            "authors": [
                "B. Morillon",
                "S. Baillet"
            ],
            "title": "Motor origin of temporal predictions in auditory attention",
            "venue": "Proc Natl Acad Sci USA 114,",
            "year": 2017
        },
        {
            "authors": [
                "H. Park",
                "G. Thut",
                "J. Gross"
            ],
            "title": "Predictive entrainment of natural speech through two fronto-motor top-down channels",
            "venue": "Lang. Cogn. Neurosci",
            "year": 2018
        },
        {
            "authors": [
                "A. Strau\u00df",
                "M. W\u00f6stmann",
                "J. Obleser"
            ],
            "title": "Cortical alpha oscillations as a tool for auditory selective inhibition",
            "venue": "Front. Hum. Neurosci. 8,",
            "year": 2014
        },
        {
            "authors": [
                "M. W\u00f6stmann",
                "Lim",
                "S.-J",
                "J. Obleser"
            ],
            "title": "The human neural alpha response to speech is a proxy of attentional control",
            "venue": "Cereb. Cortex",
            "year": 2017
        },
        {
            "authors": [
                "P. Lakatos",
                "J. Gross",
                "G. Thut"
            ],
            "title": "A new unifying account of the roles of neuronal entrainment",
            "venue": "Curr. Biol. 29,",
            "year": 2019
        },
        {
            "authors": [
                "D. Moser",
                "J.M. Baker",
                "C.E. Sanchez",
                "C. Rorden",
                "J. Fridriksson"
            ],
            "title": "Temporal order processing of syllables in the left parietal lobe",
            "venue": "J. Neurosci",
            "year": 2009
        },
        {
            "authors": [
                "G. Hickok",
                "J. Houde",
                "F. Rong"
            ],
            "title": "Sensorimotor integration in speech processing: computational basis and neural organization",
            "venue": "Neuron 69,",
            "year": 2011
        },
        {
            "authors": [
                "M. Floegel",
                "S. Fuchs",
                "C.A. Kell"
            ],
            "title": "Differential contributions of the two cerebral hemispheres to temporal and spectral speech feedback control",
            "venue": "Nat. Commun",
            "year": 2020
        },
        {
            "authors": [
                "B Ishkhanyan"
            ],
            "title": "Anterior and posterior left inferior frontal gyrus contribute to the implementation of grammatical determiners during language production",
            "venue": "Front. Psychol. 11,",
            "year": 2020
        },
        {
            "authors": [
                "G.A. Castellucci",
                "C.K. Kovach",
                "M.A. Howard",
                "J.D.W. Greenlee",
                "M.A. Long"
            ],
            "title": "A speech planning network for interactive language use",
            "year": 2022
        },
        {
            "authors": [
                "D Saur"
            ],
            "title": "Ventral and dorsal pathways for language",
            "venue": "Proc Natl Acad Sci USA 105,",
            "year": 2008
        },
        {
            "authors": [
                "A.U. Turken",
                "N.F. Dronkers"
            ],
            "title": "The neural architecture of the language comprehension network: converging evidence from lesion and connectivity analyses",
            "venue": "Front. Syst. Neurosci",
            "year": 2011
        },
        {
            "authors": [
                "Cope",
                "T. E"
            ],
            "title": "Evidence for causal top-down frontal contributions to predictive processes in speech perception",
            "venue": "Nat. Commun",
            "year": 2017
        },
        {
            "authors": [
                "M.H. Martikainen",
                "K. Kaneko",
                "R. Hari"
            ],
            "title": "Suppressed responses to self-triggered sounds in the human auditory cortex",
            "venue": "Cereb. Cortex 15,",
            "year": 2005
        },
        {
            "authors": [
                "T. Fujioka",
                "L.J. Trainor",
                "E.W. Large",
                "B. Ross"
            ],
            "title": "Internalized timing of isochronous sounds is represented in neuromagnetic \u03b2 oscillations",
            "venue": "J. Neurosci",
            "year": 2012
        },
        {
            "authors": [
                "E. Biau",
                "S.A. Kotz"
            ],
            "title": "Lower beta: A central coordinator of temporal prediction in multimodal speech",
            "venue": "Front. Hum. Neurosci. 12,",
            "year": 2018
        },
        {
            "authors": [
                "D. Poeppel",
                "M.F. Assaneo"
            ],
            "title": "Speech rhythms and their neural foundations",
            "venue": "Nat. Rev. Neurosci",
            "year": 2020
        },
        {
            "authors": [
                "C. Chandrasekaran",
                "A. Trubanova",
                "S. Stillittano",
                "A. Caplier",
                "A.A. Ghazanfar"
            ],
            "title": "The natural statistics of audiovisual speech",
            "venue": "PLoS Comput. Biol. 5,",
            "year": 2009
        },
        {
            "authors": [
                "R. Oostenveld",
                "P. Fries",
                "E. Maris",
                "Schoffelen",
                "J.-M"
            ],
            "title": "FieldTrip: Open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data",
            "venue": "Comput. Intell. Neurosci",
            "year": 2011
        },
        {
            "authors": [
                "J Gross"
            ],
            "title": "Good practice for conducting and reporting MEG research",
            "venue": "Neuroimage 65,",
            "year": 2013
        },
        {
            "authors": [
                "A. Stolk",
                "A. Todorovic",
                "Schoffelen",
                "J.-M",
                "R. Oostenveld"
            ],
            "title": "Online and offline tools for head movement compensation in MEG",
            "venue": "Neuroimage 68,",
            "year": 2013
        },
        {
            "authors": [
                "O. Abbasi",
                "J. Hirschmann",
                "G. Schmitz",
                "A. Schnitzler",
                "M. Butz"
            ],
            "title": "Rejecting deep brain stimulation",
            "venue": "J. Neurosci. Methods 268,",
            "year": 2016
        },
        {
            "authors": [
                "G. Nolte"
            ],
            "title": "The magnetic lead field theorem in the quasi-static approximation and its use for magnetoencephalography forward calculation in realistic volume conductors",
            "venue": "Phys. Med. Biol",
            "year": 2003
        },
        {
            "authors": [
                "E. Maris",
                "R. Oostenveld"
            ],
            "title": "Nonparametric statistical testing of EEG- and MEG-data",
            "venue": "J. Neurosci. Methods 164,",
            "year": 2007
        },
        {
            "authors": [
                "N. Jenkinson",
                "P. Brown"
            ],
            "title": "New insights into the relationship between dopamine, beta oscillations and motor function",
            "venue": "Trends Neurosci",
            "year": 2011
        },
        {
            "authors": [
                "A.K. Engel",
                "P. Fries"
            ],
            "title": "Beta-band oscillations--signalling the status",
            "venue": "quo? Curr. Opin. Neurobiol",
            "year": 2010
        }
    ],
    "sections": [
        {
            "text": "Speech production and perception are fundamental processes of human cognition that both rely on an internal forward model that is still poorly understood. Here, we study this forward model by using Magnetoencephalography (MEG) to comprehensively map connectivity of regional brain activity within the brain and to the speech envelope during continuous speaking and listening. Our results reveal a partly shared neural substrate for both processes but also a dissociation in space, delay and frequency. Neural activity in motor and frontal areas is coupled to succeeding speech in delta band (1-3 Hz), whereas coupling in the theta range follows speech in temporal areas during speaking. Neural connectivity results showed a separation of bottom-up and top-down signalling in distinct frequency bands during speaking. Here, we show that frequency-specific connectivity channels for bottom-up and top-down signalling support continuous speaking and listening in a way that is consistent with the predictive coding framework.\nIntroduction\nTurn-taking during a fluent dialogue is often highly efficient and fast. During a natural conversation, the distribution of gaps between turns peaks at 200 ms, which is surprising because producing a single word in a primed picture naming task takes about 600 ms 1. Levinson suggests that this efficiency is only possible due to predictive comprehension where speech production is planned before the interlocutor finishes their turn. With a similar reference to prediction, Friston and Frith described human communication as two dynamic systems that are coupled via sensory information and operate according to principles of active inference by minimising prediction errors 2. Here, predictions are computed in a generative (forward) model that is likely implemented in a cerebello-thalamo-cortical loop 3,4. In this theory of communication, speech production is controlled by the speaker's forward model that allows, for example, adjustments of speech volume, speed, or articulation based on the proprioceptive and auditory feedback. In the listener\u2019s brain, the listener\u2019s forward model will generate predictions about the timing and content of upcoming speech and these predictions are constantly updated by and compared to incoming sensory information. This model implies the involvement of the forward model in both speech perception and speech production, and therefore a partially shared neural substrate for these two cognitive processes 5. Consistent with this model, previous studies have shown that auditory-evoked brain activity from selfproduced speech is reduced compared to passive listening 6\u20138. Self-produced speech allows speakers to accurately predict the expected auditory input which is used to suppress the expected sensory consequences of the own motor act (known as sensory attenuation). Interestingly, this suppression is absent when a speaker hears their own voice after its acoustic properties have been changed in real-time 9. In this case, the acoustic sensory input to the speaker's auditory system deviates from the predicted input. This prediction error is reflected in auditory cortex activity that resembles the activity in response to passively perceived speech. However, in the human brain, this predictive machinery has to operate in the presence of significant delays at various stages 3,4: Proprioceptive and other sensory information reach the brain with delays determined by conduction velocities and distance from the brain. Moreover, internal processing requires time, as does the execution of corrective motor commands. These delays might necessitate a discontinuous, rhythmic operation of the internal forward model. Such a rhythmic mode has been demonstrated in the visuomotor system. Here, slow precision movements are adjusted at a rate of about 6-9 Hz from within a cerebellothalamo-cortical loop 4. Similarly, a preferred frequency is evident in speech production. Syllables are typically produced at a rate of about 5 per second which coincides with the modulation rate of the speech envelope 10. This typical rate therefore establishes a communication channel between interlocutors who can expect to receive syllables at this particular rate. This alignment5 facilitates prediction as it establishes a temporal coupling between the speaker\u2019s and the listener\u2019s forward models. Pertaining to the neural substrates of such forward models, predictions in the human brain have been related to brain rhythms 11,12. Brain rhythms are rhythmic fluctuations in neuronal activity that can be recorded non-invasively with Electroencephalography (EEG) or Magnetoencephalography (MEG) and have been implicated in cognitive processes such as attention, perception, and memory 13,14. More\nspecifically, studies in the visual domain demonstrate that predictions are mostly communicated in low frequency bands (alpha, beta) whereas prediction errors are communicated in higher gamma frequencies 15,16,17. In the auditory domain, brain rhythms have been studied during continuous speech perception. A consistent finding in these studies is that frequency-specific brain activity as measured with MEG or EEG becomes temporally aligned to the partly rhythmic amplitude variations in continuous speech (e.g. modulation rate). The alignment is thought to be initiated by acoustic edges in the speech waveform (such as onsets or, alternatively, peaks in vowel energy) that lead to a phase resetting of ongoing oscillations in the auditory cortex 18. As a result of this phase resetting, brain activity will be temporally aligned to the quasi-rhythmic structure in speech. In MEG data this is reflected in significant cross-correlation or coherence between the speech envelope and brain activity in the auditory cortex 19. How is this alignment related to predictions in the brain\u2019s forward model? We know that low-frequency brain oscillations represent cyclic excitability changes in underlying neuronal populations 14,20. Sensory stimuli arriving at an optimal phase of an ongoing oscillation (corresponding to high excitability) can be preferentially processed while stimuli arriving at the opposite phase (low excitability) might even remain undetected 21. In the context of continuous speech processing, the alignment is therefore thought to result in a rhythmic sampling or discretization of the continuous input stream into segments for further processing. The listener's brain tunes into the speaker's brain and the syllable rate resonates with the listener\u2019s sensory system and feeds the forward model rhythmically with new information. Indeed, evidence suggests that this temporal alignment facilitates predictions by the forward model. Temporal alignment appears to be stronger for intelligible and attended speech as previously shown by our group 19 as well as other groups 22\u201325. Our previous MEG study 12 could directly confirm a significantly stronger causal influence of higher-order brain areas (left inferior frontal gyrus and left motor areas) to auditory cortex for intelligible compared to unintelligible speech. Taken together these studies suggest that brain rhythms orchestrate internal prediction processes related to the processing of continuous speech. This relatively new field of speech-brain coupling has almost exclusively focused on speech processing in the listener\u2019s brain and largely omitted speech production in the speaker\u2019s brain. Therefore, the speech production system generating the continuous speech stimuli used in speech-brain entrainment studies has so far not been integrated into a coherent methodological and conceptual framework. Consequently, we know very little about the role of brain oscillations in speech production and their relationship to quasirhythmic components in speech and how it differs from speech perception. Here, we use the experimental and analytical approach of speech-brain coupling to study speech production and directly compare it to speech perception. Specifically, with speech-brain coupling we refer to experimental paradigms that use continuous speech (instead of single words) and analytic techniques that specifically capture temporal alignment of brain activity to rhythmic speech components (e.g. syllable rate). The aim is to unravel the dynamic network of speech production with a focus on how it engages in the quasi-rhythmic components of speech - such as syllable production. Our study builds on previous research demonstrating that speech production is supported by a distributed network of brain areas including prefrontal, motor, somatosensory, auditory, and associative regions 3,26\u2013\n28. Our knowledge about this network relies largely on fMRI 29 and lesion studies 30. Recently, invasive 28,31 and non-invasive 26,32 electrophysiological studies have contributed to the field by capitalising on their superior temporal resolution. However, invasive recordings provide only limited spatial coverage from selected recording sites in patients. MEG/EEG could potentially overcome this limitation but it suffers from movement- and muscle-related contamination of signals. Therefore, non-invasive electrophysiological studies have so far largely focused on single word or syllable production 6,7,33,34 or the preparatory phase of speech production 27,35. So far, only very few studies have studied the functional link between brain oscillations and speech production. Ruspantini and colleagues 36 instructed participants to perform a syllable repetition task at various frequencies. They found significant coherence between MEG activity from the motor cortex and EMG activity from lip muscles, peaking at about 2-3 Hz. While this study was based on silently voiced speech, Alexandrou and co-workers used spontaneously produced overt speech 37. They recorded muscle activity with EMG and relied on careful data cleaning that included independent component analysis (ICA) to remove artefacts related to mouth movements. Finally, they analysed the difference of cortical power in specific frequency bands between experimental tasks such as speech production of continuous speech versus syllable repetition. This statistical contrast revealed bilateral modulation of oscillatory power in a high frequency band (60-90 Hz) in bilateral temporal, frontal, and parietal brain areas. This study nicely demonstrates the feasibility of studying natural speech production with MEG. It also provides further evidence that brain oscillations are sensitive markers of the engagement of brain areas in speech production tasks. Even fewer studies have specifically investigated speech-brain coupling during continuous speech production. One EEG study nicely demonstrated this temporal coupling in a hyperscanning experiment but did not quantify the effects of speech artefacts on the results and did not identify the relevant brain areas 38. In a follow-up study the same group showed that the envelope of self-produced speech was maximally coupled to the speaker\u2019s EEG signals at negative lags of 25 ms (i.e. EEG signal preceding the speech envelope) 39. For listening, maximum coupling was observed at about 100 ms consistent with other studies 19. Bourguignon and colleagues studied speech-brain coupling during reading with MEG 40. They reported significant speech-brain coupling in the speaker at frequencies corresponding to phrases (< 1Hz), words (2-4 Hz), and syllables (4-8 Hz). Recently, we characterised the effects of head movement on the recorded MEG data during speech production 41. In the same study, we also suggested an artefact rejection approach, based on regression analysis and signal space projection (SSP), to remove the induced head movement artefacts from the recorded MEG data and make the current study feasible. In summary, the current study was designed to identify the neural correlates of predictive forward models during continuous speaking and listening and address the following questions: First, where and in which frequencies is brain activity significantly coupled to the speech envelope during speaking? Second, where and in which frequencies does brain activity either lead or lag speech envelope during speaking? Third, how is speech-brain coupling different between speaking and listening? Fourth, how is the difference between speaking and listening supported by differential brain connectivity?\nIn this study, we aimed to address these questions by recording MEG during performing two different experimental conditions: i.e. speaking and listening. We recorded MEG data from 30 participants while they answered seven questions (60 seconds each; speaking condition) as well as listened to audiorecordings of their own voice from the previous speaking session (listening condition; see Methods for details). We quantified speech-brain coupling using multivariate delayed mutual information (MI) on multitaper spectral estimates which has been shown to have high sensitivity and specificity compared to various other methods 42. We performed our analysis in several sequential steps. First, we describe the brain areas showing significant coupling to the speech envelope during continuous speech together with a spectrum of frequencies that supports this coupling. Next, we dissociate cortical areas where brain activity is significantly coupled to speech envelope with either positive or negative lag, respectively. We then turn to the statistical comparison of speaking and listening conditions to identify similarities and differences in the cortical speech production and perception networks. Our next step was to identify the brain network and directed connectivities within this network that support the cognitive operations in listening and speaking. Hence, we directly tested frequency-specific communication channels for topdown and bottom-up signalling during speech production and perception using multivariate Granger causality. Our speech-brain coupling results separated the temporal areas following the speech envelope (in the theta range) from frontal and motor areas preceding the speech envelope (in the delta range). Moreover, our connectivity results indicated that the feedback signals, connecting higher areas such as motor to STG, represent predictions and are communicated via slow rhythms (below 40 Hz) whereas feedforward signals (reverse direction) possibly represent prediction errors and are communicated via faster rhythms (above 40Hz).\nResults\nOur first step in the analysis was to identify the cortical networks involved in speech production and perception. We like to note that our description of the network of speech production and listening refers specifically to speech-brain coupling, i.e. we describe the network of areas where neural activity (during speaking or listening) is significantly coupled to the speech envelope. Our approach (except for the analysis of spectral power and Granger causality) is insensitive to other brain areas that are engaged in speaking- or listening-related neural processes that are not synchronous to the speech envelope."
        },
        {
            "heading": "The speech production network",
            "text": "Our analysis operates on a cortical parcellation with 230 areas 43. For each parcel, the beamformer-derived time-series of all voxels in the parcel and all the three source orientations are subjected to singular value decomposition (SVD) and the three strongest components are used for multivariate mutual information (MI) analysis 44 with the speech envelope after transformation into the frequency domain (see Figure 1 as well as Methods for details). In other words, neural activity in each parcel is represented by the three timeseries explaining most of the variance and we quantify the degree of synchronisation to the speech envelope with multivariate MI. To capture neural processes that precede or follow speech we compute MI for 52 equally spaced delays between speech envelope and neural activity ranging from -1 to 1 s and for\nfrequencies between 1 Hz and 10 Hz. Note that we focused on coupling in the frequency range up to 10 Hz because previous studies showed that speech envelope frequencies in this range are important for comprehension 45,46.\nFigure 2 shows the statistical contrast of speech-brain coupling during speaking compared to 95th percentile of surrogate data for each parcel and participant. The statistical analysis was conducted for each frequency and delay (and FDR-corrected across these dimensions). The figure shows the sum of significant t-values across time and frequency for each significant parcel. Coupling to the speech envelope can be seen in an extended network of brain areas centred around auditory and motor areas and extending to inferior temporal, inferior frontal, and parietal areas in both hemispheres. Maxima are observed in superior temporal and ventral motor and premotor areas that are known to support speech production\n34,43,44 (see Suppl. Figure 1 for speech-brain coupling during speech perception condition). Before embarking on a more detailed analysis of the dynamic neural pattern that we can resolve with MEG, we aimed to assess the localisation accuracy of our MEG results. Comprehensive comparison with an automated fMRI meta-analysis of speech production networks from neurosynth.org shows very good correspondence with our results. In fact, our speech production network resembled the neurosynth speech production network robustly and significantly more than the neurosynth speech perception network which is surprising given the close resemblance of the speech production and speech perception networks (see Suppl. Figure 2).\nNext, we aimed to distinguish the brain areas where coupling to the speech envelope is stronger at negative lags (brain activity precedes speech envelope) versus areas where coupling is stronger at positive lags (brain activity follows speech envelope). Negative lags reflect processes of motor/speech planning and motor commands activating articulators whereas positive lags reflect sensory processing of self-generated speech. The negative lag coupling results beyond the primary motor cortex support the role of the internal forward model in predicting motor programs and the sensory consequences of expected speech output. Instead, positive lag coupling may represent prediction error computation which is defined by the mismatch between prior expectation (prediction) and real auditory input. Figure 3 shows the relative change between summed significant t-values for positive and negative delays (Figure 3a). Blue colours indicate areas with higher t-values at negative compared to positive lags whereas red colours indicate stronger effects for positive lags. Our results show a clear temporal gradient from the frontal and motor cortex where coupling to the speech envelope is stronger at negative lags preceding speech to temporal and temporo-parietal areas following speech. So far the analysis has focused exclusively on the spatial distribution of brain areas synchronous to the envelope of self-produced speech. However, the speech-brain coupling shown in Figure 3a could be supported by different frequency bands. Therefore, we computed the averaged significant t-values across positive (red) and negative (blue) delays, for each frequency bin, and for L4 and LSTG (Figure 3b) parcels representing motor and temporal areas, respectively. The result suggests a dissociation where delta speech-brain coupling dominates at negative lags (synchronous activity preceding speech in frontal and\nmotor areas) while theta speech-brain coupling dominates at positive lags (synchronous activity following speech in parietal and temporal areas).\nNext, we aimed to refine the results shown in Figure 3 by mapping the optimal delay of speech-brain coupling for each brain area. This analysis was again based on the statistical map across frequencies and delays. We computed for each area the relative change of speech-brain coupling to the global speechbrain coupling (averaged across all brain areas) for each delay and frequency (see Methods). Figure 4 shows the lag (between -1 s and 1s) where this relative change averaged across frequencies is maximal. This procedure was selected to enhance local deviations from the general global profile and it clearly dissociates frontal and motor areas at negative lags from mostly temporal areas at positive lags."
        },
        {
            "heading": "Speaking versus listening",
            "text": "Having characterised speech-brain coupling for speaking, we comprehensively analysed similarities and differences in speech-brain coupling between speaking and listening conditions. First, to quantify similarities we computed the correlation of delayed MI (with delays ranging from -1s to 1s) for listening and speaking conditions for each parcel and participant. Figure 5a shows group-level cortical maps of these correlations at 2 Hz (represents delta frequency band) and 6 Hz (represents theta frequency band). Across the cortex, the highest similarity between conditions in how multivariate speech-brain MI depends on delay can be seen at delta band in superior temporal and inferior motor areas bilaterally. Figure 5b also demonstrates the average of Fisher Z-transformed correlation of lagged MI across all parcels and delays at frequencies from 1 to 10 Hz. These results confirm that the highest similarities between conditions is found at 2 Hz.\nTurning to differences between both conditions, we show several results that are based on a single statistical analysis (dependent-samples t-test of speaking versus listening) across delays (-1s to 1s) and frequencies (1-10 Hz) while correcting for multiple comparisons across frequencies, delays, and parcels (see Methods). As a result, we obtained t-values for each parcel, frequency, and delay. First, we distinguish between brain areas that show (across all delays) stronger or weaker speech-brain coupling during speaking compared to listening, respectively. Figure 6 shows the sum of significant positive (Figure 6a) and negative (Figure 6b) t-values across all delays and frequencies. Significantly stronger coupling to the speech envelope during speaking was localised within left and right motor areas as well as left inferior frontal and superior temporal areas. We are cautious regarding the interpretation of the orbitofrontal and anterior temporal coupling which is likely due to residual speaking-artefacts (such as jaw movements) since this coupling is strongest in atlas parcels closest to the articulators. In contrast, stronger speechbrain coupling during listening compared to speaking was observed in bilateral auditory, inferior frontal, temporal, (pre-) motor, and temporo-parietal areas. Comparing panels a and b seems to indicate a left lateralisation to frontal areas stronger for speaking than listening and a right lateralisation in temporal cortex for listening compared to speaking. Therefore, we performed statistical analysis to detect whether there is significant speech-brain coupling lateralisation for both the speaking and listening condition. Our results demonstrate that there is no significant lateralisation for the speaking condition. However, we observed significantly stronger coupling in the right compared to the left temporal area for the listening condition (see Suppl. Figure 3). Moreover, both maps (Figure 6a and 6b) show some overlap especially in motor cortex, superior temporal, and inferior frontal areas. This is not a contradiction but merely indicates the existence of significant negative and positive t-values in the same anatomical area at different delays or frequencies (More detailed comparisons of speech-brain coupling in speaking versus listening at delta (2 Hz) and theta (5 Hz) for different delays (from -600 to 600 ms) can be seen in Suppl. Figure 4). This is indeed supported by Figure 6c that shows the frequency-delay map obtained from averaging all significant t-values across all brain areas. The figure illustrates the spectral-delay profile for speech-brain coupling comparing speaking to listening. In general, speech-brain coupling is stronger for speaking than listening at negative lags (brain activity preceding speech envelope) and stronger for listening than speaking at positive lags. This is expected because speaking requires preparatory processes by using the internal forward model in order to predict motor programs and the sensory consequences of expected speech output that occur at negative lags. Similarly, motor commands from primary motor cortex activating articulators precede the speech envelope during speaking. Also, self-generated speech leads to attenuated auditory processing compared to a passive listening condition 6,7. This reversal (from positive to negative before and after 0 ms) is prominent in the delta band (1-3 Hz). However, the strongest dominance of speaking over listening can be seen in the theta frequency band (around 5 Hz) at a negative lag just before 0 ms. Figure 6d shows the spatial maps of t-values at 5 Hz and 0 ms and reveals leftlateralized dominance of speech-brain coupling in speaking compared to listening in (pre-)motor, left frontal, and temporal brain areas. Since the theta frequency band corresponds to the syllable rate 10 this\nresult indicates that syllable-related activity in these areas is more precisely time-locked to the speech envelope in speaking compared to listening.\nDirected information flow in speaking and listening\nHaving established the multivariate, frequency-specific speech-brain coupling in speaking and listening we next investigated the hallmarks of predictive forward models during continuous speaking and listening. Specifically, we tested predictive coding models that have been established in vision and postulate the communication of top-down and bottom-up signals in low versus high frequency bands, respectively. To this end, we computed the connectivity of the left superior temporal gyrus (LSTG; depicted in Suppl. Figure 5) as a central node in the cortical speech network to all other cortical parcels using multivariate nonparametric Granger causality (mGC) 15,47. We opted for multivariate Granger causality, first, to be consistent with our multivariate speech-brain coupling analysis and, second, to obtain a more accurate estimate of connectivity by using three-dimensional representations of each parcel activity that capture more signal variance compared to more traditional one-dimensional representations. Computation of mGC between STG and each cortical parcel resulted in two mGC spectra for each pair reflecting both directions (A->B and B->A). Next, we computed the directed asymmetry index (DAI) for\neach pair of spectra 15. DAI is the relative difference between both directions and therefore captures the predominant direction of mGC between two parcels (see \u2019Methods\u2019 for more information). Positive DAI reflects dominant directionality from a parcel towards STG whereas negative DAI reflects the opposite directionality (from STG to another parcel). First, we used group statistics to identify brain areas where DAI values in specific frequency bands in the speaking condition differed significantly from zero. We performed group statistics for the following canonical frequency bands: Delta (0-4 Hz), theta (4-8 Hz), alpha (8-12 Hz), beta (12-30 Hz), gamma (30- 60 Hz), high gamma (60-90 Hz). In Figure 7a and b, colour codes t-values. Blue colour represents the flow of information from STG to other cortical parcels and the red colour represents the opposite direction. Clustering statistics revealed significant connectivity during speaking from motor cortex (including precentral gyrus and superior/middle frontal gyrus) to STG in lower frequency bands (e.g. theta) and in the opposite direction in high gamma frequencies (Figure 7a). This striking reversal indicates a dissociation of bottom-up and top-down information flow during speaking in distinct frequency bands. Topdown signalling is predominantly communicated in lower frequency bands while bottom-up signalling relies on high-frequency bands (see Suppl. Fig. 6 for all frequency bands). The listening condition shows an overall similar pattern compared to speaking including the directionality reversal between theta and gamma frequencies (Figure 7b). However, both the top-down effects in the theta band and the bottom-up effects in the gamma band are less pronounced in listening compared to speaking. This is confirmed by a direct comparison of DAI connectivity between speaking and listening. Figure 7c shows significant connectivity differences between these two conditions in the theta band and illustrates significantly stronger top-down signals from frontal/motor cortex to STG in speaking compared to listening and significantly stronger coupling from STG to anterior temporal cortex in listening compared to speaking. Interestingly, our connectivity results illustrate that the top-down effects from different areas to left STG during speaking occur in distinct low frequency bands (see Suppl. Fig. 7 for all frequency bands). Figure 7d indicates that the strongest top-down connectivity from the motor cortex to STG occurs at theta (4-8 Hz) and beta (12-30 Hz) frequencies, while from the left parietal to STG at alpha frequencies (8-13 Hz). Finally, although there is stronger bottom-up connectivity from STG to motor areas in the high gamma band during speaking (compared to listening, Figure 7d), no significant difference was observed.\nThe general pattern of top-down connectivity in low frequencies and bottom-up connectivity in high frequencies bands is consistent across multiple connections in Figure 7. Therefore, we conducted a dedicated analysis and specifically contrasted the connectivity patterns from cortical parcels to left STG in low-frequency ranges (0-30 Hz) versus high-frequency ranges (60-90 Hz). We statistically compared the DAI values of all the cortical parcels in low-frequency ranges versus high-frequency ranges for both the speaking and listening conditions. As we expected according to Figure 7, the motor areas showed the strongest significant difference in the speaking condition, confirming the connectivity from frontal and motor cortex to left STG in low frequencies and the opposite direction in high frequencies (Figure 8).\nFinally, we aimed to connect the two main parts of the analysis, namely the speech-brain and brain-brain connectivities. Specifically, we investigated the relationship between connectivity patterns from all other cortical parcels to STG and the speech-STG coupling. Therefore, we correlated the top-down connectivity indices and MI values (between STG and speech) across participants. We computed correlations separately for each parcel and for different frequency bands for both speaking and listening conditions. This analysis revealed significant negative correlations between top-down beta connectivity from bilateral motor areas as well as left frontal area and the speech-STG coupling in theta band (at 130 ms lag) for speaking condition (Figure 9a; p < 0.05). However, this correlation was not observed in the listening condition nor with speech-STG coupling at 0 ms lag (Figure 9b-c). Moreover, we conducted the correlation analysis between top-down beta connectivity and speech-STG coupling in each frequency bin in the theta frequency range (at 130 ms lag) which revealed a striking pattern. For low theta frequencies, the negative correlation of top-down beta connectivity with speech-STG coupling is significant in parietal and motor areas (Figure 9d). As frequency in the theta band increases this effect shifts to frontal areas. This transition is most evident in inferior and middle frontal areas where significant clusters are absent in low theta frequencies but present in high theta frequencies (while a superior frontal cluster is largely independent of frequency). Finally, we further divided the top-down connectivity into low and high beta bands to test which frequency range would be more involved in motor-auditory area interaction during the speaking condition. The main intention of us separating these two frequency channels was to see if there is a distinct frequency channel connecting the motor to auditory areas in speech production. We observed that top-down low beta (12-20 Hz) connectivity originating from bilateral motor areas are negatively correlated with low theta speechSTG coupling (Figure 9e: left panel). However, top-down beta (20-30 Hz) connectivity originating from the left frontal area is negatively correlated with speech-STG coupling in the high theta band (Figure 9e: right\npanel). These results further support the importance of beta band activity and connectivity in motor cortex 48,49. Due to previous results in the literature our primary hypothesis was related to the beta band. However, an exploratory analysis across frequency bands revealed significant negative correlation between speechSTG coupling in theta range and top-down delta as well as theta connectivity from mainly left occipital and parietal areas (see Suppl. Figure 8).\nDiscussion\nIn this study, we aimed to identify the neural correlates of predictive forward models during continuous speaking and listening. Since participants listened to their own previously produced speech they were likely able to predict some aspects of its content (although both measurements were separated by several days). Still, we opted for this design because it was the only way to guarantee that the same stimulus material was used in both conditions such that differences in both conditions could not have been caused by differences in low-level stimulus features. First, we quantified speech-brain coupling using multivariate delayed mutual information (MI) on multitaper spectral estimates - a method that was recently demonstrated to perform well on simulated and real data 42. Our findings revealed significant coupling between an extended network of brain areas centred around auditory areas and the speech envelope during continuous speech. These coupling results dissociate frontal and motor areas preceding the speech temporal envelope (in the delta range) from mostly temporal areas following the speech envelope (in the theta range). Furthermore, significantly stronger theta-range speech-brain coupling was observed for speaking (compared to listening) in left and right motor areas and left inferior frontal and superior temporal areas. In contrast, stronger delta-range speech-brain coupling during listening (compared to speaking) was observed in bilateral auditory, inferior frontal, temporal, (pre-) motor, and temporo-parietal areas. Moreover, we investigated the directed coupling using multivariate Granger causality within the involved area during speech production and perception. The results of directed coupling analysis indicate significant connectivity from the motor area to STG in lower frequency bands (up to beta) during speaking and in the opposite direction in high gamma frequencies. Notably, the detected beta-range top-down connectivity in the speaking condition was negatively correlated with the speech-brain coupling within the left STG in the theta band.\nSpeech-brain coupling in speaking and listening\nSpeech-brain coupling represents the alignment of quasi-rhythmic speech components with rhythmic modulations of cortical neural activity which facilitate speech processing 19. Since previous studies demonstrated that speech envelope frequencies below 10 Hz are important for comprehension, in the first part of our study, we focused on coupling in the frequency range up to 10 Hz 45,46. We observed significant speech-brain coupling in frequencies below 10 Hz revealing the contribution of a network of brain areas centred on bilateral auditory areas in both speech production and speech perception conditions. This coupling was previously reported in several studies 19,37,40,50,51. The detected speech-brain coupling was observed bilaterally in both speech production and perception. This finding is in agreement with previous studies indicating bilateral cortical areas are involved in speech perception 52. Ghinst and colleagues revealed that both the left STG and right supratemporal auditory cortex tracked speech envelopes at different frequencies 53. Another study by Gross and colleagues suggested that theta phase entrainment to speech envelope was significant in bilateral auditory areas 19. The same pattern of activation was observed in Alexandrou and colleagues' report on speech production and perception. Specifically, they found that power modulation occurs in the frontocentral, parietal, and temporoparietal areas during speech\nproduction. However, speech perception is primarily mediated by the temporo-parietal and temporal cortices with a pronounced right hemispheric component 37. Contrary to previous studies, they demonstrated that speech perception and production share cortical representations mainly in the right hemisphere instead of just the left. Interestingly, we also found that bilaterally, inferior and superior temporal motor areas of both conditions shared the largest similarity between them. This clearly indicates that these areas engage in speaking and listening in a similar manner. Moreover, the stronger similarity in the right hemisphere could support the hypothesis that the right hemisphere processes speech meaning in both modalities 37. Additionally, we found that underlying neural activity during speech production differed significantly from that during speech perception. We observed stronger coupling between left and right motor areas as well as left inferior frontal and superior temporal areas and speech envelope during speaking compared to listening. These results are unsurprising, as speech production requires fine voluntary control of phonation and articulation 36,54. The beta-band power decrease in widespread bilateral motor areas during continuous speech, presented in our study (see Suppl. Figure 9) as well as earlier studies 37,54,55, also indicates that motor areas are actively involved in speech production. A further observation we made was that the speech-brain coupling in the motor areas is stronger at negative lags indicating synchronous activity preceding speech during speaking condition. This coupling at negative lag most likely represents motor signals controlling speech articulators. However, the detected negative lag coupling localized beyond the primary motor cortex supports the role of the internal forward model in predicting motor programs and the sensory consequences of expected speech output before they are actually generated 40,56,57. Furthermore, another recent study demonstrated that speech-brain coupling increased when reading aloud as compared to listening, providing further evidence that the brain generates sensorimotor representations of self-produced speech 40. As a result of this internal monitoring system, cortical responses in the auditory cortex are attenuated because speakers can accurately predict the sensory consequences of what they say before they utter it 6\u20138. Therefore, a smaller cortical response may be associated with weaker speechbrain coupling in the auditory cortex. The results of our experiments support this hypothesis, as speechbrain coupling in the listening condition was stronger than in the speaking condition in the bilateral auditory cortices.\nDirected information flow in speaking and listening\nOur multivariate connectivity analysis provides the first confirmation of frequency-specific communication channels for top-down and bottom-up signalling during continuous speaking and listening in noninvasive recordings of healthy participants. Our results indicate that the implementation of predictive coding via distinct frequency channels that has been demonstrated in the visual domain largely generalises to both continuous speaking and listening in the auditory-motor domain. The model posits that signalling along cortical hierarchies reflect distinct processes: Feedback signals represent predictions and are communicated via slow rhythms (below 40 Hz) whereas feedforward signals represent prediction errors and are communicated via faster rhythms (above 40Hz) 15,17 13,58 16.\nOnly few studies have specifically tested this model in the auditory domain and none during continuous speaking. Previously, highly interesting early work by Fontolan and colleagues demonstrated in invasive recordings distinct frequency channels for feedforward and feedback communication between two hierarchically different auditory areas in three epilepsy patients 59. Also using invasive recordings of epilepsy patients listening to tones, Sedley and colleagues showed that prediction errors are represented in gamma power while predictions are represented in lower frequency beta power 60. Similar results were obtained in an ECoG study in monkeys 61. Our multivariate connectivity results significantly extend these reports to non-invasive recordings with whole-brain coverage in our healthy participants that each engage in a listening and a speaking task. Our focus on directed connectivity to and from left STG, a central node in the auditory hierarchy for extracting meaningful linguistic information from speech input 62, allows the investigation of feedforward and feedback processing during speaking and listening. Indeed, during speaking and listening, multivariate Granger causality from frontal, parietal, and motor cortices to left STG was significantly stronger in frequencies below 30 Hz, while the opposite direction is significantly stronger in frequencies above 30 Hz. This striking reversal of directionality from low to high frequency bands confirms in the auditory domain the results established in the visual domain mentioned above - namely the selective communication of feedforward and feedback information in frequency-specific channels. These findings are consistent with the \u2018predictive coding\u2019 framework that would interpret the top-down signals as predictions and bottom-up signals as prediction errors. Further research is needed to clarify if the spatial resolution of MEG with naturalistic tasks as ours is sufficient to reconstruct more detailed functional hierarchies in the auditory domain in a way consistent with previous demonstrations in the visual domain based on strong, highcontrast stimuli 17. It is however interesting to note that top-down effects from different areas to left STG during speaking occur in distinct low frequency bands (Figure 7d and Suppl. Fig 6). Motor cortex shows strongest topdown signals at theta (4-8 Hz) and beta (12-30 Hz) frequencies, left inferior frontal cortex at delta (<4 Hz), and left parietal cortex at alpha frequencies (8-13 Hz). Theta-band connectivity from the motor cortex likely conveys the timing of syllables (that are produced at theta rate) during speaking to the temporal cortex. Indeed, during listening, when the motor cortex does not produce these syllable patterns, this connectivity is reduced. In contrast, beta band signals from the motor cortex to the left STG are similarly strong in both conditions. These signals have previously been linked to the process of updating sensory predictions and have been observed during various auditory tasks 60,61,63,64 including continuous speech perception 65. Our correlation results (discussed in detail in the following section) also suggest that top-down beta oscillations are related to prediction maintenance by carrying speech-related information to the auditory areas. For gamma frequencies, DAI reverses sign to become negative, indicating the dominance of feedforward signals from STG to motor cortex in this frequency band (Figure 7d). Both feedforward and feedback signals between motor and auditory areas are stronger during speaking compared to listening, indicating the stronger coupling of both systems when speech is self-generated. This is consistent with the predictive model since self-generated speech leads to estimations of predicted sensory input that are communicated to auditory areas through low frequency channels. In turn, the auditory input is used to update these\nestimations through feedforward signals in the high gamma band. However, since we have not controlled or modified our stimulus material in a way that leads to specific changes in predictions, we can not assign more precise computational roles to our reported connectivities. A very similar connectivity pattern indicating feedforward signalling at gamma frequencies and feedback signals at low frequencies is evident for the connection from left parietal cortex to left STG. However, compared to the motor cortex, top-down effects use a different low-frequency band, namely alpha. Importantly, alpha oscillations in the auditory system have been linked to attentional control 66,67 and might serve as a multi-dimensional filter of sensory inputs across space, time, and frequency dimensions 68. Our connectivity results suggest that these filters in early auditory areas are controlled and modulated by the parietal cortex with its unique anatomical and functional location at the interface between auditory and articulatory speech areas. Indeed, parietal areas are known to support sensorimotor integration 69,70, code predicted sensory consequences during speaking, and provide top-down signals to early auditory areas 71. The stronger top-down effects during speaking might indicate the selective inhibition of auditory signals that are predicted by the internal forward model. In fact, this \u2018sensory attenuation\u2019 (see also subsection below) has been shown to be reflected in alpha oscillations 72 in accordance with our connectivity results. Left inferior frontal gyrus (IFG) is crucial for language comprehension and production and contributes to the formation of a predictive model of language 73. In a TMS study, Ishkhanyan and colleagues targeted anterior and posterior parts of left IFG during an adjective-noun production task and demonstrated the importance of this area for grammatically and syntactically correct speech production 73. More recently, Castellucci and colleagues demonstrated the major role of IFG in speech planning and preparation 74. Saur and colleagues reported an anatomical connectivity between left IFG and auditory cortex 75,76. Similar to motor and parietal cortex, our results show that during speech production, IFG receives feedforward signals from the left STG in the high gamma band. Yet, another distinct frequency channel is used for feedback signals, namely the delta band (Suppl Figure 3). The importance of low-frequency top-down signals from left IFG to early auditory areas in the delta band has previously been shown for listening 12,77. It should be noted that with DAI we are contrasting the two directed connectivities between two areas (see Methods section) which makes our analysis specifically sensitive to asymmetries (i.e. one direction being significantly stronger than the opposite direction) and not directly comparable to studies investigating individual directions (e.g. as Gross et al 12 ). In summary, our connectivity results are in line with the theory of predictive coding and reveal the essential role of directional frequency-specific modulation in cortical information processing for speech production and perception. According to the predictive coding framework, predictions are updated by the linear accumulation of prediction errors over time which leads to a slower changing of prediction in comparison to prediction error 17,59. Thus, prediction error signals need to utilise a higher frequency channel for communication than prediction signals which is in agreement with our findings.\nSensory attenuation in speech production\nIn contrast to externally generated sounds, self-generated sounds exhibit smaller cortical responses in the auditory cortex because their sensory consequences can be accurately predicted (known as sensory\nattenuation) 78. This is also the case for speech: Previous studies have shown that self-produced speech results in reduced auditory-evoked brain activity compared to passive listening 6\u20138. We extend these findings by showing that speech-brain coupling is significantly weaker during speaking than during listening, especially at low frequencies. This is consistent with predictions derived from the sensory attenuation literature and the predictive speech model. According to this model, motor commands for speech production are not only sent to muscles controlling articulators but also communicated to cortical areas that infer the to be expected auditory input. Therefore, during speaking, expected sensory input will be attenuated in early auditory areas leading to a reduced coupling of auditory activity to the (selfgenerated) speech envelope. In our previous study, we found that neural responses to self-produced stimuli are modulated by betaband oscillations in the motor cortex 63. Additionally, we reported significant directional couplings in the beta range originating from motor cortices towards bilateral auditory areas. In the current study, we also tested whether top-down connectivity directly affected the cortical tracking of speech signals. Interestingly, we found a negative correlation between theta-range speech-brain coupling in the left auditory area and top-down beta connectivity originating from motor areas. This finding is consistent with prior studies showing the role of beta activity in updating predictions about the sensory consequences of the upcoming movements 60,79. In an MEG study, Morillon and colleagues also reported beta top-down connectivity from motor to auditory cortices is directly linked to temporal predictions 64. Interestingly, our correlation results showed that attenuation of speech-brain coupling, derived from top-down audio-motor connectivity in the low beta range (12-20 Hz) primarily occurs in the low theta band (4-6 Hz). These results show the important role of low beta oscillations in interactions between motor and auditory areas in speech production which are in line with previous study introducing low beta activity as a central coordinator of temporal predictions for auditory entrainment 80. On the other hand, according to previous studies, the maximum speech modulations fall in low theta band (around 4Hz) 10,81. Therefore, the decrease in low theta speech-brain coupling and its negative correlation with top-down low beta connectivity support our hypothesis regarding reflecting sensory attenuation phenomenon. In summary, using multivariate analysis we reveal the similarities and differences of speech tracking between listening and speaking across the cortex. We show that during speaking, brain activity is most strongly aligned to the speech envelope in auditory and motor areas. Our time-resolved MEG results further demonstrate that this alignment precedes the speech envelope in frontal and motor areas and follows the speech envelope in auditory areas. Indeed, in frontal and motor areas this speech-brain coupling is stronger for speaking than listening while the opposite is true for auditory areas. Detecting a negative lag coupling beyond the primary motor cortex supports the role of internal forward models in predicting speech programs and sensory consequences. More importantly, our directed connectivity analysis suggests that feedforward signals are mostly communicated in the gamma frequency band while feedback signals use low frequency channels with frequencies depending on the cortical origin of these signals. These findings are consistent with the \u2018predictive coding\u2019 framework that would interpret the top-down signals as predictions and bottom-up signals as prediction errors. This communication is also modulated by tasks such that feedback from the frontal-motor cortex is stronger during speaking\nthan listening. Additionally, the negative correlation between top-down beta connectivity originating from the motor areas and theta-range speech-brain coupling in the left auditory area supports the role of beta activity in predicting the sensory consequences of self-generated speech.\nMethods"
        },
        {
            "heading": "Participants",
            "text": "We recruited thirty native German-speaking participants (15 males, mean age 25.1 \u00b1 2.8 years, range 20\u2013 32 years) from a local participant pool. The study was approved by the local ethics committee and conducted in accordance with the Declaration of Helsinki. Prior written informed consent was obtained before the measurement and participants received monetary compensation after the experiment."
        },
        {
            "heading": "Recording",
            "text": "MEG, electromyogram (EMG), and speech signals were recorded simultaneously. A 275 whole-head sensor system (OMEGA 275, VSM Medtech Ltd., Vancouver, Canada) was used for all of the recordings with a sampling frequency of 1200 Hz, except the speech recording which had a sampling rate of 44.1 kHz. Audio data was captured with a microphone, which was placed at a distance of 155 cm from the participants\u2019 mouth, in order not to cause any artefacts through the microphone itself. Three pairs of EMG surface electrodes were placed after tactile inspection to find the correct location to capture muscle activity from the m. genioglossus, m. orbicularis oris, and m. zygomaticus major (for exact location see Figure 1 in Abbasi et al 41 ). One pair of electrodes was used for each muscle with about 1 cm between electrodes. A low-pass online filter with a 300Hz cut-off was applied to the recorded MEG and EMG data."
        },
        {
            "heading": "Paradigm",
            "text": "Participants were asked to sit relaxed while performing the given tasks and to keep their eyes focused on a white fixation cross. This study consisted of three separate recordings: i) speech production ii) speech production while perception was masked iii) speech perception. For the speech production recording, there were 7 trials for overt speech. Each trial consisted of a 60 second time period in which participants answered a given question such as \u2018What does a typical weekend look like for you?\u2019. A colour change of the fixation cross from white to blue indicated the beginning of the time period in which participants should speak and the end was marked by a colour change back to white. In the second recording, participants were asked to perform the same task as in the first recording while they heard white noise, leaving them unable to hear their own voice. The questions were different to the prior recording in order to prevent repetition and prefabricated answers. To keep emotional answers out of the way, questions covering neutral topics were chosen (Full list of questions: Supplementary Table 1). In the third recording session participants listened to audio-recordings of their own voice, which were collected in the first and second recordings. For this paper only conditions i) and iii) were used."
        },
        {
            "heading": "Preprocessing and data analysis",
            "text": "Prior to data analysis, MEG data were visually inspected. No jump artefacts or bad channels were detected. A discrete Fourier transform (DFT) filter was applied to eliminate 50 Hz line noise from the continuous MEG and EMG data. Moreover, EMG data was highpass-filtered at 20 Hz and rectified. Continuous head position and rotation were extracted from the fiducial coils placed at anatomical landmarks (nasion, left, and right ear canals). The wideband amplitude envelope of the speech signal was\ncomputed using the method presented in 82. Nine logarithmically spaced frequency bands between 100- 10000 Hz were constructed by bandpass filtering (third-order, Butterworth filters). Then, we computed the amplitude envelope for each frequency band as the absolute value of the Hilbert transform and downsampled them to 1200 Hz. Finally, we averaged them across bands and used the computed wideband amplitude envelope for all further analysis. MEG, EMG, speech envelope, and head movement signals were downsampled to 256 Hz and were segmented to 60s trials. In the preprocessing and data analysis steps, custom-made scripts in Matlab R2020 (The Mathworks, Natick, MA, USA) in combination with the Matlab-based FieldTrip toolbox 83 were used in accord with current MEG guidelines 84."
        },
        {
            "heading": "Artefact rejection",
            "text": "For removing the speech-related artefacts we used the pipeline presented in our recently published study 41. In a nutshell, the artefact rejection comprises four major steps: (i) head movement-related artefact was initially reduced by incorporating the head position time-series into the general linear model (GLM) using regression analysis 85. (ii) To further remove the residual artefact, singular value decomposition (SVD) was used to estimate the spatial subspace (components) containing the speech-related artefact from the MEG data. (iii) Artefactual components were detected via visual inspections and mutual information (MI) analysis and then removed from the single-trial data 86; (iv) finally, all remaining components were backtransformed to the sensor level."
        },
        {
            "heading": "Source localization",
            "text": "For source localisation we aligned individual T1-weighted anatomical MRI scans with the digitised head shapes using the iterative closest point algorithm. Then, we segmented the MRI scans and generated single-shell volume conductor models 87, and used this to create forward models. For group analyses, individual MRIs were linearly transformed to a MNI template provided by Fieldtrip. Next, the linearly constrained minimum variance (LCMV) algorithm was used to compute time-series for each voxel on a 5- mm grid. The time-series were extracted for each dipole orientation, resulting in three time-series per voxel. The reduced version of the HCP brain atlas was applied on the source space time-series in order to reduce the dimensionality of the data, resulting in 230 parcels 43. Finally, we extracted the first three components of a singular value decomposition (SVD) of time-series from all dipoles in this parcel, explaining most of the variance."
        },
        {
            "heading": "Mutual information",
            "text": "For each parcel the three SVD components were subjected to multitaper spectral analysis with +/- 2Hz spectral smoothing on 2s long windows with 50% overlap. We then estimated mutual information (MI) using Gaussian Copula MI between the speech envelope and complex-valued spectral estimates of all three time-series 44. Note, that due to the multivariate nature of the computation this resulted in a single MI value for the parcel that combines information across all three time-series. This computation was repeated for each parcel and 52 delays between speech envelope and neural activity ranging (equally spaced) from -1 to 1 s. For each delay the shifting of the speech envelope with respect to the MEG signal\nwas performed before computing the multitaper spectral estimate. In addition, we computed a surrogate distribution reflecting the expected MI values in the absence of true synchronisation by computing MI for 500 random delays between speech envelope and source activity (only delays longer than 3s were used). We implemented shifting with the matlab function circshift.m with wrapping around the edge of the timeseries."
        },
        {
            "heading": "Delay estimation",
            "text": "To estimate the optimal lag of speech-brain coupling during speaking (Figure 3) we used the t-values resulting from the comparison of speaking against the 95th percentile of surrogate data. The t-values had been computed across lags (-1s to 1s) and frequencies (1-10 Hz) and p-values had been FDR-corrected for multiple comparisons across lags and frequencies. First, we averaged the t-values across brain areas yielding the global speech-brain coupling signature across lags and frequencies. Next, we computed for each brain area the relative change of t-values compared to the global signature resulting in a matrix of relative changes across lag and frequency for each brain area. Finally, we averaged this matrix across frequencies and identified the lag where this averaged relative change was maximal. Please note that this procedure only captures the lag corresponding to the global peak across lags even if multiple peaks are present."
        },
        {
            "heading": "Connectivity analysis",
            "text": "We performed connectivity analysis by using a multivariate nonparametric Granger causality approach (mCG) 47. We computed the mCG to determine the directionality of functional coupling between STG (LA5 parcel in HCP atlas) and each cortical area, in pairwise steps, during speech production and perception. Initially, the source signals were divided into trials of four seconds, with 500 milliseconds overlap. We used the fast Fourier transform in combination with multitapers (2 Hz smoothing) to compute the cross-spectral density (CSD) matrix of the trials. Next, using a blockwise approach, we considered the first three SVD components of each parcel as a block and estimated the connectivity between STG and other parcels. Finally, we computed the directed influence asymmetry index (DAI) defined by 15 as\n\ud835\udc37\ud835\udc37\ud835\udc37\ud835\udc37\ud835\udc37\ud835\udc37 = [\ud835\udc5a\ud835\udc5a\ud835\udc5a\ud835\udc5a\ud835\udc5a\ud835\udc5a(\ud835\udc5d\ud835\udc5d\ud835\udc5d\ud835\udc5d\ud835\udc5d\ud835\udc5d\ud835\udc5d\ud835\udc5d\ud835\udc5d\ud835\udc5d\ud835\udc59\ud835\udc59 \u2192 \ud835\udc46\ud835\udc46\ud835\udc46\ud835\udc46\ud835\udc5a\ud835\udc5a) \u2212\ud835\udc5a\ud835\udc5a\ud835\udc5a\ud835\udc5a\ud835\udc5a\ud835\udc5a(\ud835\udc46\ud835\udc46\ud835\udc46\ud835\udc46\ud835\udc5a\ud835\udc5a \u2192 \ud835\udc5d\ud835\udc5d\ud835\udc5d\ud835\udc5d\ud835\udc5d\ud835\udc5d\ud835\udc5d\ud835\udc5d\ud835\udc5d\ud835\udc5d\ud835\udc59\ud835\udc59)] [\ud835\udc5a\ud835\udc5a\ud835\udc5a\ud835\udc5a\ud835\udc5a\ud835\udc5a(\ud835\udc5d\ud835\udc5d\ud835\udc5d\ud835\udc5d\ud835\udc5d\ud835\udc5d\ud835\udc5d\ud835\udc5d\ud835\udc5d\ud835\udc5d\ud835\udc59\ud835\udc59 \u2192 \ud835\udc46\ud835\udc46\ud835\udc46\ud835\udc46\ud835\udc5a\ud835\udc5a) + \ud835\udc5a\ud835\udc5a\ud835\udc5a\ud835\udc5a\ud835\udc5a\ud835\udc5a(\ud835\udc46\ud835\udc46\ud835\udc46\ud835\udc46\ud835\udc5a\ud835\udc5a \u2192 \ud835\udc5d\ud835\udc5d\ud835\udc5d\ud835\udc5d\ud835\udc5d\ud835\udc5d\ud835\udc5d\ud835\udc5d\ud835\udc5d\ud835\udc5d\ud835\udc59\ud835\udc59)]\nTherefore, a positive DAI for a given frequency indicates that the selected parcel conveys feedforward influences to STG in this frequency, and a negative DAI indicates feedback influences. Note that for the connectivity analysis, we used MEG data with 1200 Hz sampling rate without downsampling."
        },
        {
            "heading": "Statistical analysis",
            "text": "We determined significant normalised MI values as well as connectivity patterns (DAI values) in both speaking and listening conditions using non-parametric cluster-based permutation tests 88. First, we estimated the statistical contrast of speech-brain coupling during speaking compared to the 95th percentile of MI for each parcel and participant. Second, the normalised MI values in the speaking condition were contrasted with MI values in the listening condition at the group level. The statistical analysis was conducted for each frequency (in the range 1-10 Hz) and delay (in the range -1 to 1 s with 52 equally\nspaced delays) using a dependent-samples t-test. We used a cluster-based correction to account for multiple comparisons across frequencies, delays and parcels. We performed five thousand permutations and set the critical alpha value at 0.05. We performed the same procedures in order to detect significant connectivities between STG and other cortical parcels in both speaking and listening conditions."
        },
        {
            "heading": "Correlation analysis",
            "text": "We assessed whether there is a relationship between our connectivity results from all the parcels to LSTG and speech-STG coupling using non-parametric cluster-based permutation tests. First, we estimated the top-down connectivity values for each parcel and frequency band. Next, we computed speech-STG couplings for each frequency band. The correlation analysis was conducted for each frequency band and parcel using the Pearson method implemented in the ft_statfun_correlationT function in Fieldtrip. We used cluster-based correction to account for multiple comparisons across parcels. Our analysis was repeated for different frequency bands. Therefore, our results are not corrected across frequencies. We performed five thousand permutations and set the critical alpha value at 0.05.\nData availability\nAn example data is available at https://osf.io/9fq47/?view_only=e6bad57efb854f069474c2d6f93a00a6. Raw data, however, are protected by data privacy laws and cannot be made widely available, but may be made available upon reasonable request (subject to these privacy laws).\nCode availability\nThe code used for this study is available at https://osf.io/9fq47/?view_only=e6bad57efb854f069474c2d6f93a00a6.\nAcknowledgements We acknowledge support by the Interdisciplinary Center for Clinical Research (IZKF) of the medical faculty of M\u00fcnster (grant number Gro3/001/19). OA (EFRE-0400394) was supported by the EFRE. DSK (KL 3580/1-1) and JG (GR 2024/5-1; GR 2024/11-1;GR 2024/12 -1) were further supported by the DFG.\nBibliography 1. Levinson, S. C. Turn-taking in Human Communication--Origins and Implications for Language\nProcessing. Trends Cogn Sci (Regul Ed) 20, 6\u201314 (2016).\n2. Friston, K. J. & Frith, C. D. Active inference, communication and hermeneutics. Cortex 68, 129\u2013143\n(2015).\n3. Hickok, G. Computational neuroanatomy of speech production. Nat. Rev. Neurosci. 13, 135\u2013145\n(2012).\n4. Gross, J. et al. The neural basis of intermittent motor control in humans. Proc Natl Acad Sci USA\n99, 2299\u20132302 (2002).\n5. Pickering, M. J. & Garrod, S. An integrated theory of language production and comprehension.\nBehav. Brain Sci. 36, 329\u2013347 (2013).\n6. Houde, J. F., Nagarajan, S. S., Sekihara, K. & Merzenich, M. M. Modulation of the auditory cortex\nduring speech: an MEG study. J. Cogn. Neurosci. 14, 1125\u20131138 (2002).\n7. Curio, G., Neuloh, G., Numminen, J., Jousm\u00e4ki, V. & Hari, R. Speaking modifies voice-evoked\nactivity in the human auditory cortex. Hum. Brain Mapp. 9, 183\u2013191 (2000).\n8. Tian, X. & Poeppel, D. Dynamics of self-monitoring and error detection in speech production:\nevidence from mental imagery and MEG. J. Cogn. Neurosci. 27, 352\u2013364 (2015).\n9. Houde, J. F. & Nagarajan, S. S. Speech production as state feedback control. Front. Hum.\nNeurosci. 5, 82 (2011).\n10. Ding, N. et al. Temporal modulations in speech and music. Neurosci. Biobehav. Rev. 81, 181\u2013187\n(2017).\n11. Arnal, L. H. & Giraud, A.-L. Cortical oscillations and sensory predictions. Trends Cogn Sci (Regul\nEd) 16, 390\u2013398 (2012).\n12. Park, H., Ince, R. A. A., Schyns, P. G., Thut, G. & Gross, J. Frontal top-down signals increase\ncoupling of auditory low-frequency oscillations to continuous speech in human listeners. Curr. Biol.\n25, 1649\u20131653 (2015).\n13. Wang, X.-J. Neurophysiological and computational principles of cortical rhythms in cognition.\nPhysiol. Rev. 90, 1195\u20131268 (2010).\n14. Thut, G., Miniussi, C. & Gross, J. The functional importance of rhythmic activity in the brain. Curr.\nBiol. 22, R658-63 (2012).\n15. Bastos, A. M. et al. Visual areas exert feedforward and feedback influences through distinct\nfrequency channels. Neuron 85, 390\u2013401 (2015).\n16. Gross, J. Let the Rhythm Guide You: Non-invasive Tracking of Cortical Communication Channels.\nNeuron 89, 244\u2013247 (2016).\n17. Michalareas, G. et al. Alpha-Beta and Gamma Rhythms Subserve Feedback and Feedforward\nInfluences among Human Visual Cortical Areas. Neuron 89, 384\u2013397 (2016).\n18. Giraud, A.-L. & Poeppel, D. Cortical oscillations and speech processing: emerging computational\nprinciples and operations. Nat. Neurosci. 15, 511\u2013517 (2012).\n19. Gross, J. et al. Speech rhythms and multiplexed oscillatory sensory coding in the human brain.\nPLoS Biol. 11, e1001752 (2013).\n20. Schroeder, C. E. & Lakatos, P. Low-frequency neuronal oscillations as instruments of sensory\nselection. Trends Neurosci. 32, 9\u201318 (2009).\n21. Busch, N. A., Dubois, J. & VanRullen, R. The phase of ongoing EEG oscillations predicts visual\nperception. J. Neurosci. 29, 7869\u20137876 (2009).\n22. Ding, N. & Simon, J. Z. Neural coding of continuous speech in auditory cortex during monaural and\ndichotic listening. J. Neurophysiol. 107, 78\u201389 (2012).\n23. Lakatos, P. et al. The spectrotemporal filter mechanism of auditory selective attention. Neuron 77,\n750\u2013761 (2013).\n24. Peelle, J. E., Gross, J. & Davis, M. H. Phase-locked responses to speech in human auditory cortex\nare enhanced during comprehension. Cereb. Cortex 23, 1378\u20131387 (2013).\n25. Zion-Golumbic, E. & Schroeder, C. E. Attention modulates \u201cspeech-tracking\u201d at a cocktail party.\nTrends Cogn Sci (Regul Ed) 16, 363\u2013364 (2012).\n26. Ganushchak, L. Y., Christoffels, I. K. & Schiller, N. O. The use of electroencephalography in\nlanguage production research: a review. Front. Psychol. 2, 208 (2011).\n27. Gehrig, J., Wibral, M., Arnold, C. & Kell, C. A. Setting up the speech production network: how\noscillations contribute to lateralized information routing. Front. Psychol. 3, 169 (2012).\n28. Llorens, A., Tr\u00e9buchon, A., Li\u00e9geois-Chauvel, C. & Alario, F.-X. Intra-cranial recordings of brain\nactivity during language production. Front. Psychol. 2, 375 (2011).\n29. Price, C. J. The anatomy of language: a review of 100 fMRI studies published in 2009. Ann. N. Y.\nAcad. Sci. 1191, 62\u201388 (2010).\n30. Borovsky, A., Saygin, A. P., Bates, E. & Dronkers, N. Lesion correlates of conversational speech\nproduction deficits. Neuropsychologia 45, 2525\u20132533 (2007).\n31. Ri\u00e8s, S. K. et al. Spatiotemporal dynamics of word retrieval in speech production revealed by\ncortical high-frequency band activity. Proc Natl Acad Sci USA 114, E4530\u2013E4538 (2017).\n32. Janssen, N., Meij, M. van der, L\u00f3pez-P\u00e9rez, P. J. & Barber, H. A. Exploring the temporal dynamics\nof speech production with EEG and group ICA. Sci. Rep. 10, 3667 (2020).\n33. Liljestr\u00f6m, M., Kujala, J., Stevenson, C. & Salmelin, R. Dynamic reconfiguration of the language\nnetwork preceding onset of speech in picture naming. Hum. Brain Mapp. 36, 1202\u20131216 (2015).\n34. Fairs, A., Michelas, A., Dufour, S. & Strijkers, K. The Same Ultra-Rapid Parallel Brain Dynamics\nUnderpin the Production and Perception of Speech. Cereb. Cortex Commun. 2, tgab040 (2021).\n35. Saarinen, T., Laaksonen, H., Parviainen, T. & Salmelin, R. Motor cortex dynamics in visuomotor\nproduction of speech and non-speech mouth movements. Cereb. Cortex 16, 212\u2013222 (2006).\n36. Ruspantini, I. et al. Corticomuscular coherence is tuned to the spontaneous rhythmicity of speech at\n2-3 Hz. J. Neurosci. 32, 3786\u20133790 (2012).\n37. Alexandrou, A. M., Saarinen, T., M\u00e4kel\u00e4, S., Kujala, J. & Salmelin, R. The right hemisphere is\nhighlighted in connected natural speech production and perception. Neuroimage 152, 628\u2013638\n(2017).\n38. P\u00e9rez, A., Carreiras, M. & Du\u00f1abeitia, J. A. Brain-to-brain entrainment: EEG interbrain\nsynchronization while speaking and listening. Sci. Rep. 7, 4190 (2017).\n39. P\u00e9rez, A. et al. Joint entrainment to the speech envelope during speaking and listening cannot\ncompletely explain brain-to-brain synchronisation. (2021) doi:10.31234/osf.io/tzhn4.\n40. Bourguignon, M. et al. Neocortical activity tracks the hierarchical linguistic structures of self-\nproduced speech during reading aloud. Neuroimage 216, 116788 (2020).\n41. Abbasi, O., Steingr\u00e4ber, N. & Gross, J. Correcting MEG artifacts caused by overt speech. Front.\nNeurosci. 15, 682419 (2021).\n42. Gross, J. et al. Comparison of undirected frequency-domain connectivity measures for cerebro-\nperipheral analysis. Neuroimage 245, 118660 (2021).\n43. Tait, L., \u00d6zkan, A., Szul, M. J. & Zhang, J. Cortical source imaging of resting-state MEG with a high\nresolution atlas: An evaluation of methods. BioRxiv (2020) doi:10.1101/2020.01.12.903302.\n44. Ince, R. A. A. et al. A statistical framework for neuroimaging data analysis based on mutual\ninformation estimated via a gaussian copula. Hum. Brain Mapp. 38, 1541\u20131573 (2017).\n45. Ding, N. & Simon, J. Z. Adaptive temporal encoding leads to a background-insensitive cortical\nrepresentation of speech. J. Neurosci. 33, 5728\u20135735 (2013).\n46. Elliott, T. M. & Theunissen, F. E. The modulation transfer function for speech intelligibility. PLoS\nComput. Biol. 5, e1000302 (2009).\n47. Schaum, M. et al. Right inferior frontal gyrus implements motor inhibitory control via beta-band\noscillations in humans. eLife 10, (2021).\n48. Park, H., Ince, R. A. A., Schyns, P. G., Thut, G. & Gross, J. Representational interactions during\naudiovisual speech entrainment: Redundancy in left posterior superior temporal gyrus and synergy\nin left motor cortex. PLoS Biol. 16, e2006558 (2018).\n49. Keitel, A., Gross, J. & Kayser, C. Perceptually relevant speech tracking in auditory and motor cortex\nreflects distinct linguistic features. PLoS Biol. 16, e2004473 (2018).\n50. Bourguignon, M. et al. The pace of prosodic phrasing couples the listener\u2019s cortex to the reader\u2019s\nvoice. Hum. Brain Mapp. 34, 314\u2013326 (2013).\n51. Luo, H. & Poeppel, D. Phase patterns of neuronal responses reliably discriminate speech in human\nauditory cortex. Neuron 54, 1001\u20131010 (2007).\n52. Cogan, G. B. et al. Sensory-motor transformations for speech occur bilaterally. Nature 507, 94\u201398\n(2014).\n53. Vander Ghinst, M. et al. Left Superior Temporal Gyrus Is Coupled to Attended Speech in a\nCocktail-Party Auditory Scene. J. Neurosci. 36, 1596\u20131606 (2016).\n54. Simmonds, A. J., Leech, R., Collins, C., Redjep, O. & Wise, R. J. S. Sensory-motor integration\nduring speech production localizes to both left and right plana temporale. J. Neurosci. 34, 12963\u2013\n12972 (2014).\n55. Piai, V. & Zheng, X. Speaking waves: Neuronal oscillations in language production. in vol. 71 265\u2013\n302 (Elsevier, 2019).\n56. Hickok, G. The cortical organization of speech processing: feedback control and predictive coding\nthe context of a dual-stream model. J. Commun. Disord. 45, 393\u2013402 (2012).\n57. Houde, J. F. & Chang, E. F. The cortical computations underlying feedback control in vocal\nproduction. Curr. Opin. Neurobiol. 33, 174\u2013181 (2015).\n58. Arnal, L. H., Wyart, V. & Giraud, A.-L. Transitions in neural oscillations reflect prediction errors\ngenerated in audiovisual speech. Nat. Neurosci. 14, 797\u2013801 (2011).\n59. Fontolan, L., Morillon, B., Liegeois-Chauvel, C. & Giraud, A.-L. The contribution of frequency-\nspecific activity to hierarchical information processing in the human auditory cortex. Nat. Commun.\n5, 4694 (2014).\n60. Sedley, W. et al. Neural signatures of perceptual inference. eLife 5, e11476 (2016).\n61. Chao, Z. C., Takaura, K., Wang, L., Fujii, N. & Dehaene, S. Large-Scale Cortical Networks for\nHierarchical Prediction and Prediction Error in the Primate Brain. Neuron 100, 1252-1266.e3\n(2018).\n62. Yi, H. G., Leonard, M. K. & Chang, E. F. The encoding of speech sounds in the superior temporal\ngyrus. Neuron 102, 1096\u20131110 (2019).\n63. Abbasi, O. & Gross, J. Beta-band oscillations play an essential role in motor-auditory interactions.\nHum. Brain Mapp. 41, 656\u2013665 (2020).\n64. Morillon, B. & Baillet, S. Motor origin of temporal predictions in auditory attention. Proc Natl Acad\nSci USA 114, E8913\u2013E8921 (2017).\n65. Park, H., Thut, G. & Gross, J. Predictive entrainment of natural speech through two fronto-motor\ntop-down channels. Lang. Cogn. Neurosci. 35, 739\u2013751 (2018).\n66. Strau\u00df, A., W\u00f6stmann, M. & Obleser, J. Cortical alpha oscillations as a tool for auditory selective\ninhibition. Front. Hum. Neurosci. 8, 350 (2014).\n67. W\u00f6stmann, M., Lim, S.-J. & Obleser, J. The human neural alpha response to speech is a proxy of\nattentional control. Cereb. Cortex 27, 3307\u20133317 (2017).\n68. Lakatos, P., Gross, J. & Thut, G. A new unifying account of the roles of neuronal entrainment. Curr.\nBiol. 29, R890\u2013R905 (2019).\n69. Moser, D., Baker, J. M., Sanchez, C. E., Rorden, C. & Fridriksson, J. Temporal order processing of\nsyllables in the left parietal lobe. J. Neurosci. 29, 12568\u201312573 (2009).\n70. Hickok, G., Houde, J. & Rong, F. Sensorimotor integration in speech processing: computational\nbasis and neural organization. Neuron 69, 407\u2013422 (2011).\n71. Floegel, M., Fuchs, S. & Kell, C. A. Differential contributions of the two cerebral hemispheres to\ntemporal and spectral speech feedback control. Nat. Commun. 11, 2839 (2020).\n72. Cao, L., Thut, G. & Gross, J. The role of brain oscillations in predicting self-generated sounds.\nNeuroimage 147, 895\u2013903 (2017).\n73. Ishkhanyan, B. et al. Anterior and posterior left inferior frontal gyrus contribute to the\nimplementation of grammatical determiners during language production. Front. Psychol. 11, 685\n(2020).\n74. Castellucci, G. A., Kovach, C. K., Howard, M. A., Greenlee, J. D. W. & Long, M. A. A speech\nplanning network for interactive language use. Nature (2022) doi:10.1038/s41586-021-04270-z.\n75. Saur, D. et al. Ventral and dorsal pathways for language. Proc Natl Acad Sci USA 105, 18035\u2013\n18040 (2008).\n76. Turken, A. U. & Dronkers, N. F. The neural architecture of the language comprehension network:\nconverging evidence from lesion and connectivity analyses. Front. Syst. Neurosci. 5, 1 (2011).\n77. Cope, T. E. et al. Evidence for causal top-down frontal contributions to predictive processes in\nspeech perception. Nat. Commun. 8, 2154 (2017).\n78. Martikainen, M. H., Kaneko, K. & Hari, R. Suppressed responses to self-triggered sounds in the\nhuman auditory cortex. Cereb. Cortex 15, 299\u2013302 (2005).\n79. Fujioka, T., Trainor, L. J., Large, E. W. & Ross, B. Internalized timing of isochronous sounds is\nrepresented in neuromagnetic \u03b2 oscillations. J. Neurosci. 32, 1791\u20131802 (2012).\n80. Biau, E. & Kotz, S. A. Lower beta: A central coordinator of temporal prediction in multimodal\nspeech. Front. Hum. Neurosci. 12, 434 (2018).\n81. Poeppel, D. & Assaneo, M. F. Speech rhythms and their neural foundations. Nat. Rev. Neurosci.\n21, 322\u2013334 (2020).\n82. Chandrasekaran, C., Trubanova, A., Stillittano, S., Caplier, A. & Ghazanfar, A. A. The natural\nstatistics of audiovisual speech. PLoS Comput. Biol. 5, e1000436 (2009).\n83. Oostenveld, R., Fries, P., Maris, E. & Schoffelen, J.-M. FieldTrip: Open source software for\nadvanced analysis of MEG, EEG, and invasive electrophysiological data. Comput. Intell. Neurosci.\n2011, 156869 (2011).\n84. Gross, J. et al. Good practice for conducting and reporting MEG research. Neuroimage 65, 349\u2013\n363 (2013).\n85. Stolk, A., Todorovic, A., Schoffelen, J.-M. & Oostenveld, R. Online and offline tools for head\nmovement compensation in MEG. Neuroimage 68, 39\u201348 (2013).\n86. Abbasi, O., Hirschmann, J., Schmitz, G., Schnitzler, A. & Butz, M. Rejecting deep brain stimulation\nartefacts from MEG data using ICA and mutual information. J. Neurosci. Methods 268, 131\u2013141\n(2016).\n87. Nolte, G. The magnetic lead field theorem in the quasi-static approximation and its use for\nmagnetoencephalography forward calculation in realistic volume conductors. Phys. Med. Biol. 48,\n3637\u20133652 (2003).\n88. Maris, E. & Oostenveld, R. Nonparametric statistical testing of EEG- and MEG-data. J. Neurosci.\nMethods 164, 177\u2013190 (2007).\n89. Jenkinson, N. & Brown, P. New insights into the relationship between dopamine, beta oscillations\nand motor function. Trends Neurosci. 34, 611\u2013618 (2011).\n90. Engel, A. K. & Fries, P. Beta-band oscillations--signalling the status quo? Curr. Opin. Neurobiol. 20,\n156\u2013165 (2010)."
        }
    ],
    "title": "Oscillatory brain networks in continuous speaking and listening",
    "year": 2022
}