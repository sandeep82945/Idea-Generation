{
    "abstractText": "Recovering an outdoor environment\u2019s surface mesh is vital for an agricultural robot during task planning and remote visualization. Image-based dense 3D reconstruction is sensitive to large movements between adjacent frames and the quality of the estimated depth maps. Our proposed solution for these problems is based on a newly-designed panoramic stereo camera along with a hybrid novel software framework that consists of three fusion modules: disparity fusion, pose fusion, and volumetric fusion. The panoramic stereo camera with a pentagon shape consists of 5 stereo vision camera pairs to stream synchronized panoramic stereo images for the following three fusion modules. In the disparity fusion module, rectified stereo images produce the initial disparity maps using multiple stereo vision algorithms. Then, these initial disparity maps, along with the intensity images, are input into a disparity fusion network to produce refined disparity maps. Next, the refined disparity maps are converted into full-view (360\u25e6) point clouds or single-view (72\u25e6) point clouds for the pose fusion module. The pose fusion module adopts a two-stage global-coarse-to-local-fine strategy. In the first stage, each pair of full-view point clouds is registered by a global point cloud matching algorithm to estimate the transformation for a global pose graph\u2019s edge, which effectively implements loop closure. In the second stage, a local point cloud matching algorithm is used to match single-view point clouds in different nodes. Next, we locally refine the poses of all corresponding edges in the global pose graph using three proposed rules, thus constructing a refined pose graph. The refined pose graph is optimized to produce a global pose trajectory for volumetric fusion. In the volumetric fusion module, the global poses of all the nodes are used to integrate the single-view point clouds into the volume to produce the mesh of the whole garden. The proposed framework and its three fusion modules are tested on a real outdoor garden dataset to show the superiority of the performance. The whole pipeline takes about 4 minutes on a desktop computer to process the real garden dataset, which is available at: https://github.com/Canpu999/Trimbot-Wageningen-SLAM-Dataset.",
    "authors": [
        {
            "affiliations": [],
            "name": "Can Pua"
        },
        {
            "affiliations": [],
            "name": "Chuanyu Yanga"
        },
        {
            "affiliations": [],
            "name": "Jinnian Pua"
        },
        {
            "affiliations": [],
            "name": "Radim Tylecekb"
        },
        {
            "affiliations": [],
            "name": "Robert B. Fisherb"
        }
    ],
    "id": "SP:8a8ed0e9775f1eb7089feff6e4e8173d3d9dc565",
    "references": [
        {
            "authors": [
                "M. Ahmadi",
                "A.A. Naeini",
                "Z. Arjmandi",
                "Y. Zhang",
                "M.M. Sheikholeslami",
                "G. Sohn"
            ],
            "title": "Hdpv-slam: Hybrid depth-augmented panoramic visual 32ContextCapture\u2019s quality report: https://drive.google.com/file/d",
            "year": 2023
        },
        {
            "authors": [
                "M.S. Alam",
                "M. Alam",
                "M. Tufail",
                "M.U. Khan",
                "A. G\u00fcne\u015f",
                "B. Salah",
                "F.E. Nasir",
                "W. Saleem",
                "M.T. Khan"
            ],
            "title": "Tobset: A new tobacco crop and weeds image dataset and its utilization for vision-based spraying by agricultural robots",
            "venue": "Applied Sciences",
            "year": 2022
        },
        {
            "authors": [
                "S. Ao",
                "Q. Hu",
                "B. Yang",
                "A. Markham",
                "Y. Guo"
            ],
            "title": "Spinnet: Learning a general surface descriptor for 3d point cloud registration",
            "venue": "in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "D. Barath",
                "D. Mishkin",
                "I. Eichhardt",
                "I. Shipachev",
                "J. Matas"
            ],
            "title": "Efficient initial pose-graph generation for global sfm",
            "venue": "in: Proceedings of the First Author et al.: Preprint submitted to Elsevier",
            "year": 2021
        },
        {
            "authors": [
                "P.J. Besl",
                "N.D. McKay"
            ],
            "title": "Method for registration of 3-d shapes, in: Sensor fusion IV: control paradigms and data structures, Spie",
            "year": 1992
        },
        {
            "authors": [
                "C. Campos",
                "R. Elvira",
                "J.J.G. Rodr\u00edguez",
                "J.M. Montiel",
                "J.D. Tard\u00f3s"
            ],
            "title": "Orb-slam3: An accurate open-source library for visual, visual\u2013inertial, and multimap slam",
            "venue": "IEEE Transactions on Robotics",
            "year": 2021
        },
        {
            "authors": [
                "L. Carlone",
                "D.M. Rosen",
                "G. Calafiore",
                "J.J. Leonard",
                "F. Dellaert"
            ],
            "title": "2015a. Lagrangian duality in 3d slam: Verification techniques and optimal solutions",
            "venue": "in: 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),",
            "year": 2015
        },
        {
            "authors": [
                "L. Carlone",
                "R. Tron",
                "K. Daniilidis",
                "F. Dellaert"
            ],
            "title": "2015b. Initialization techniques for 3d slam: a survey on rotation estimation and its use in pose graph optimization",
            "venue": "IEEE international conference on robotics and automation (ICRA),",
            "year": 2015
        },
        {
            "authors": [
                "N. Chebrolu",
                "P. Lottes",
                "A. Schaefer",
                "W. Winterhalter",
                "W. Burgard",
                "C. Stachniss"
            ],
            "title": "Agricultural robot dataset for plant classification, localization and mapping on sugar beet fields",
            "venue": "The International Journal of Robotics Research",
            "year": 2017
        },
        {
            "authors": [
                "H. Chen",
                "W. Hu",
                "K. Yang",
                "J. Bai",
                "K. Wang"
            ],
            "title": "Panoramic annular slam with loop closure and global optimization",
            "venue": "Applied Optics",
            "year": 2021
        },
        {
            "authors": [
                "H. Chen",
                "K. Wang",
                "W. Hu",
                "K. Yang",
                "R. Cheng",
                "X. Huang",
                "J. Bai"
            ],
            "title": "Palvo: visual odometry based on panoramic annular lens",
            "venue": "Optics express",
            "year": 2019
        },
        {
            "authors": [
                "W. Chen",
                "G. Shang",
                "A. Ji",
                "C. Zhou",
                "X. Wang",
                "C. Xu",
                "Z. Li",
                "K. Hu"
            ],
            "title": "An overview on visual slam: From tradition to semantic",
            "venue": "Remote Sensing",
            "year": 2022
        },
        {
            "authors": [
                "M. Cordts",
                "M. Omran",
                "S. Ramos",
                "T. Rehfeld",
                "M. Enzweiler",
                "R. Benenson",
                "U. Franke",
                "S. Roth",
                "B. Schiele"
            ],
            "title": "The cityscapes dataset for semantic urban scene understanding",
            "venue": "in: Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "B. Curless",
                "M. Levoy"
            ],
            "title": "A volumetric method for building complex models from range images",
            "venue": "in: Proceedings of the 23rd annual conference on Computer graphics and interactive techniques,",
            "year": 1996
        },
        {
            "authors": [
                "A. Dosovitskiy",
                "P. Fischer",
                "E. Ilg",
                "P. Hausser",
                "C. Hazirbas",
                "V. Golkov",
                "P. Van Der Smagt",
                "D. Cremers",
                "T. Brox"
            ],
            "title": "Flownet: Learning optical flow with convolutional networks",
            "venue": "in: Proceedings of the IEEE international conference on computer vision,",
            "year": 2015
        },
        {
            "authors": [
                "J. Engel",
                "V. Koltun",
                "D. Cremers"
            ],
            "title": "Direct sparse odometry",
            "venue": "IEEE transactions on pattern analysis and machine intelligence",
            "year": 2017
        },
        {
            "authors": [
                "J. Engel",
                "J. St\u00fcckler",
                "D. Cremers"
            ],
            "title": "Large-scale direct slam with stereo cameras",
            "venue": "in: 2015 IEEE/RSJ international conference on intelligent robots and systems (IROS),",
            "year": 2015
        },
        {
            "authors": [
                "Y. Fan",
                "Q. Zhang",
                "Y. Tang",
                "S. Liu",
                "H. Han"
            ],
            "title": "Blitz-slam: A semantic slam in dynamic environments",
            "venue": "Pattern Recognition",
            "year": 2022
        },
        {
            "authors": [
                "M.A. Fischler",
                "R.C. Bolles"
            ],
            "title": "Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography",
            "venue": "Commun. ACM",
            "year": 1981
        },
        {
            "authors": [
                "A. Geiger",
                "P. Lenz",
                "C. Stiller",
                "R. Urtasun"
            ],
            "title": "Vision meets robotics: The kitti dataset",
            "venue": "The International Journal of Robotics Research",
            "year": 2013
        },
        {
            "authors": [
                "I. Goodfellow",
                "J. Pouget-Abadie",
                "M. Mirza",
                "B. Xu",
                "D. Warde-Farley",
                "S. Ozair",
                "A. Courville",
                "Y. Bengio"
            ],
            "title": "Generative adversarial nets, in: Advances in neural information processing",
            "year": 2014
        },
        {
            "authors": [
                "G. Grisetti",
                "R. K\u00fcmmerle",
                "C. Stachniss",
                "W. Burgard"
            ],
            "title": "A tutorial on graph-based slam",
            "venue": "IEEE Intelligent Transportation Systems Magazine",
            "year": 2010
        },
        {
            "authors": [
                "R. Grosso",
                "D. Zint"
            ],
            "title": "A parallel dual marching cubes approach to quad only surface reconstruction",
            "venue": "The Visual Computer",
            "year": 2022
        },
        {
            "authors": [
                "X. Gu",
                "C. Tang",
                "W. Yuan",
                "Z. Dai",
                "S. Zhu",
                "P. Tan"
            ],
            "title": "Rcp: Recurrent closest point for point cloud",
            "venue": "in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "H. Hirschmuller"
            ],
            "title": "Accurate and efficient stereo processing by semi-global matching and mutual information",
            "venue": "in: Computer Vision and Pattern Recognition,",
            "year": 2005
        },
        {
            "authors": [
                "D. Honegger",
                "T. Sattler",
                "M. Pollefeys"
            ],
            "title": "Embedded real-time multibaseline stereo",
            "venue": "IEEE International Conference on Robotics and Automation (ICRA),",
            "year": 2017
        },
        {
            "authors": [
                "N. Hu",
                "S. Wang",
                "X. Wang",
                "Y. Cai",
                "D. Su",
                "P. Nyamsuren",
                "Y. Qiao",
                "Y. Jiang",
                "B. Hai",
                "H. Wei"
            ],
            "title": "Lettucemot: A dataset of lettuce detection and tracking with re-identification of re-occurred plants for agricultural robots",
            "venue": "Frontiers in Plant Science",
            "year": 2022
        },
        {
            "authors": [
                "X. Huang",
                "S. Li",
                "Y. Zuo",
                "Y. Fang",
                "J. Zhang",
                "X. Zhao"
            ],
            "title": "Unsupervised point cloud registration by learning unified gaussian mixture models",
            "venue": "IEEE Robotics and Automation Letters ",
            "year": 2022
        },
        {
            "authors": [
                "X. Huang",
                "G. Mei",
                "J. Zhang",
                "R. Abbas"
            ],
            "title": "A comprehensive survey on point cloud registration",
            "year": 2021
        },
        {
            "authors": [
                "D.Q. Huynh"
            ],
            "title": "Metrics for 3d rotations: Comparison and analysis",
            "venue": "Journal of Mathematical Imaging and Vision",
            "year": 2009
        },
        {
            "authors": [
                "S. Izadi",
                "D. Kim",
                "O. Hilliges",
                "D. Molyneaux",
                "R. Newcombe",
                "P. Kohli",
                "J. Shotton",
                "S. Hodges",
                "D. Freeman",
                "A Davison"
            ],
            "title": "Kinectfusion: real-time 3d reconstruction and interaction using amoving depth camera",
            "venue": "in: Proceedings of the 24th annual ACM symposium on User interface software and technology,",
            "year": 2011
        },
        {
            "authors": [
                "S. Ji",
                "Z. Qin",
                "J. Shan",
                "M. Lu"
            ],
            "title": "Panoramic slam from a multiple fisheye camera rig",
            "venue": "ISPRS Journal of Photogrammetry and Remote Sensing",
            "year": 2020
        },
        {
            "authors": [
                "E.M.O. Junior",
                "D.R. Santos",
                "Miola",
                "G.A.R"
            ],
            "title": "A new variant of the icp algorithm for pairwise 3d point cloud registration",
            "venue": "American Academic Scientific Research Journal for Engineering, Technology, and Sciences",
            "year": 2022
        },
        {
            "authors": [
                "J. Kang",
                "Y. Zhang",
                "Z. Liu",
                "A. Sit",
                "G. Sohn"
            ],
            "title": "Rpv-slam: Rangeaugmented panoramic visual slam for mobile mapping system with panoramic camera and tilted lidar",
            "venue": "20th International Conference on Advanced Robotics (ICAR),",
            "year": 2021
        },
        {
            "authors": [
                "I.A. Kazerouni",
                "L. Fitzgerald",
                "G. Dooly",
                "D. Toal"
            ],
            "title": "A survey of stateof-the-art on visual slam",
            "venue": "Expert Systems",
            "year": 2022
        },
        {
            "authors": [
                "T. Lewiner",
                "H. Lopes",
                "A.W. Vieira",
                "G. Tavares"
            ],
            "title": "Efficient implementation of marching cubes\u2019 cases with topological guarantees",
            "venue": "Journal of graphics tools",
            "year": 2003
        },
        {
            "authors": [
                "W. Liu",
                "H. Wu",
                "G.S. Chirikjian"
            ],
            "title": "Lsg-cpd: Coherent point drift with local surface geometry for point cloud registration",
            "venue": "in: Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "W.E. Lorensen",
                "H.E. Cline"
            ],
            "title": "Marching cubes: A high resolution 3d surface construction algorithm",
            "venue": "ACM siggraph computer graphics",
            "year": 1987
        },
        {
            "authors": [
                "G. Marin",
                "P. Zanuttigh",
                "S. Mattoccia"
            ],
            "title": "Reliable fusion of tof and stereo depth driven by confidence measures",
            "venue": "in: European Conference on Computer Vision,",
            "year": 2016
        },
        {
            "authors": [
                "N. Mayer",
                "E. Ilg",
                "P. Hausser",
                "P. Fischer",
                "D. Cremers",
                "A. Dosovitskiy",
                "T. Brox"
            ],
            "title": "A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation",
            "venue": "in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2016
        },
        {
            "authors": [
                "M. Menze",
                "A. Geiger"
            ],
            "title": "Object scene flow for autonomous vehicles",
            "venue": "in: Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2015
        },
        {
            "authors": [
                "G. Moreira",
                "M. Marques",
                "J.P. Costeira"
            ],
            "title": "2021a. Fast pose graph optimization via krylov-schur and cholesky factorization",
            "venue": "in: Proceedings of the IEEE/CVFWinter Conference on Applications of Computer Vision,",
            "year": 1906
        },
        {
            "authors": [
                "G. Moreira",
                "M. Marques",
                "J.P. Costeira"
            ],
            "title": "Rotation averaging in a split second: A primal-dual method and a closed-form for cycle graphs",
            "venue": "in: Proceedings of the IEEE/CVF International Conference on Computer First Author et al.: Preprint submitted to Elsevier",
            "year": 2021
        },
        {
            "authors": [
                "R. Mur-Artal",
                "J.M.M. Montiel",
                "J.D. Tardos"
            ],
            "title": "Orb-slam: a versatile and accurate monocular slam system",
            "venue": "IEEE transactions on robotics",
            "year": 2015
        },
        {
            "authors": [
                "R. Mur-Artal",
                "J.D. Tard\u00f3s"
            ],
            "title": "Orb-slam2: An open-source slam system for monocular",
            "venue": "stereo, and rgb-d cameras. IEEE transactions on robotics",
            "year": 2017
        },
        {
            "authors": [
                "A. Myronenko",
                "X. Song"
            ],
            "title": "Point set registration: Coherent point drift. IEEE transactions on pattern analysis and machine intelligence",
            "year": 2010
        },
        {
            "authors": [
                "P.C. Ng",
                "S. Henikoff"
            ],
            "title": "Sift: Predicting amino acid changes that affect protein function",
            "venue": "Nucleic acids research",
            "year": 2003
        },
        {
            "authors": [
                "M. Poggi",
                "G. Agresti",
                "F. Tosi",
                "P. Zanuttigh",
                "S. Mattoccia"
            ],
            "title": "Confidence estimation for tof and stereo sensors and its application to depth data fusion",
            "venue": "IEEE Sensors Journal",
            "year": 2019
        },
        {
            "authors": [
                "M. Poggi",
                "S. Mattoccia"
            ],
            "title": "Deep stereo fusion: combining multiple disparity hypotheses with deep-learning",
            "venue": "in: 2016 Fourth International Conference on 3D Vision (3DV),",
            "year": 2016
        },
        {
            "authors": [
                "M. Poggi",
                "F. Tosi",
                "K. Batsos",
                "P. Mordohai",
                "S. Mattoccia"
            ],
            "title": "On the synergies between machine learning and binocular stereo for depth estimation from images: a survey",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "year": 2021
        },
        {
            "authors": [
                "R. Polvara",
                "S. Molina",
                "I. Hroob",
                "A. Papadimitriou",
                "T. Konstantinos",
                "D. Giakoumis",
                "S. Likothanassis",
                "D. Tzovaras",
                "G. Cielniak",
                "M. Hanheide"
            ],
            "title": "Blt dataset: acquisition of the agricultural bacchus long-term dataset with automated robot deployment",
            "venue": "Journal of Field Robotics, Agricultural Robots for Ag 4.0 Under Review",
            "year": 2022
        },
        {
            "authors": [
                "C. Pu",
                "R.B. Fisher"
            ],
            "title": "Udfnet: Unsupervised disparity fusion with adversarial networks",
            "venue": "IEEE International Conference on Image Processing (ICIP),",
            "year": 2019
        },
        {
            "authors": [
                "C. Pu",
                "N. Li",
                "R. Tylecek",
                "B. Fisher"
            ],
            "title": "Dugma: Dynamic uncertaintybased gaussian mixture alignment",
            "venue": "in: 2018 International Conference on 3D Vision (3DV),",
            "year": 2018
        },
        {
            "authors": [
                "C. Pu",
                "R. Song",
                "R. Tylecek",
                "N. Li",
                "R.B. Fisher"
            ],
            "title": "Sdf-man: Semi-supervised disparity fusion with multi-scale adversarial networks",
            "venue": "Remote Sensing",
            "year": 2019
        },
        {
            "authors": [
                "T. Qin",
                "P. Li",
                "S. Shen"
            ],
            "title": "Vins-mono: A robust and versatile monocular visual-inertial state estimator",
            "venue": "IEEE Transactions on Robotics",
            "year": 2018
        },
        {
            "authors": [
                "E. Rublee",
                "V. Rabaud",
                "K. Konolige",
                "G. Bradski"
            ],
            "title": "Orb: An efficient alternative to sift or surf",
            "venue": "in: 2011 International conference on computer vision,",
            "year": 2011
        },
        {
            "authors": [
                "E. Sandstr\u00f6m",
                "M.R. Oswald",
                "S. Kumar",
                "S. Weder",
                "F. Yu",
                "C. Sminchisescu",
                "L. VanGool"
            ],
            "title": "Learning onlinemulti-sensor depth fusion",
            "year": 2022
        },
        {
            "authors": [
                "T. Sattler",
                "R. Tylecek",
                "T. Brox",
                "M. Pollefeys",
                "R.B. Fisher"
            ],
            "title": "2017. 3d reconstruction meets semantics\u2013reconstruction challenge 2017",
            "venue": "in: ICCV Workshop,",
            "year": 2017
        },
        {
            "authors": [
                "J.L. Schonberger",
                "J.M. Frahm"
            ],
            "title": "Structure-from-motion revisited, in",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "A. Segal",
                "D. Haehnel",
                "S. Thrun"
            ],
            "title": "Generalized-icp., in: Robotics: science and systems, Seattle, WA",
            "year": 2009
        },
        {
            "authors": [
                "T. Shan",
                "B. Englot"
            ],
            "title": "Lego-loam: Lightweight and ground-optimized lidar odometry and mapping on variable terrain",
            "venue": "in: 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),",
            "year": 2018
        },
        {
            "authors": [
                "S. Sumikura",
                "M. Shibuya",
                "K. Sakurada"
            ],
            "title": "Openvslam: A versatile visual slam framework",
            "venue": "in: Proceedings of the 27th ACM International Conference on Multimedia,",
            "year": 2019
        },
        {
            "authors": [
                "R. Szeliski"
            ],
            "title": "Computer vision: algorithms and applications",
            "year": 2010
        },
        {
            "authors": [
                "R. Tylecek",
                "R.B. Fisher"
            ],
            "title": "Trimbot2020 dataset for garden navigation and bush trimming, in: Edinburgh DataVault. 10.7488/9f9de786-5e584bca-9279-f1d7ffddda41",
            "year": 2020
        },
        {
            "authors": [
                "D. Wang",
                "J. Wang",
                "Y. Tian",
                "K. Hu",
                "M. Xu"
            ],
            "title": "Pal-slam: a featurebased slam system for a panoramic annular lens",
            "venue": "Optics Express",
            "year": 2022
        },
        {
            "authors": [
                "T. Whelan",
                "R.F. Salas-Moreno",
                "B. Glocker",
                "A.J. Davison",
                "S. Leutenegger"
            ],
            "title": "Elasticfusion: Real-time dense slam and light source estimation",
            "venue": "The International Journal of Robotics Research",
            "year": 2016
        },
        {
            "authors": [
                "B. Wu",
                "J. Ma",
                "G. Chen",
                "P. An"
            ],
            "title": "Feature interactive representation for point cloud registration",
            "venue": "in: Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "X. Xu",
                "L. Zhang",
                "J. Yang",
                "C. Cao",
                "W. Wang",
                "Y. Ran",
                "Z. Tan",
                "M. Luo"
            ],
            "title": "A review of multi-sensor fusion slam systems based on 3d lidar",
            "venue": "Remote Sensing",
            "year": 2022
        },
        {
            "authors": [
                "F.S. Zakeri",
                "M. Sj\u00f6str\u00f6m",
                "J. Keinert"
            ],
            "title": "Guided optimization framework for the fusion of time-of-flight with stereo depth",
            "venue": "Journal of Electronic Imaging",
            "year": 2020
        },
        {
            "authors": [
                "A. Zeng",
                "S. Song",
                "M. Nie\u00dfner",
                "M. Fisher",
                "J. Xiao",
                "T. Funkhouser"
            ],
            "title": "3dmatch: Learning local geometric descriptors from rgb-d reconstructions",
            "venue": "in: Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "S. Zhang",
                "L. Zheng",
                "W. Tao"
            ],
            "title": "Survey and evaluation of rgb-d slam",
            "venue": "IEEE Access",
            "year": 2021
        },
        {
            "authors": [
                "Y. Zhang",
                "F. Huang"
            ],
            "title": "Panoramic visual slam technology for spherical images. Sensors",
            "year": 2021
        },
        {
            "authors": [
                "X. Zhao",
                "Q. Hu",
                "X. Zhang",
                "H. Wang"
            ],
            "title": "An orb-slam3 autonomous positioning and orientation approach using 360-degree panoramic video",
            "venue": "29th International Conference on Geoinformatics,",
            "year": 2022
        },
        {
            "authors": [
                "Q.Y. Zhou",
                "V. Koltun"
            ],
            "title": "Dense scene reconstruction with points of interest",
            "venue": "ACM Transactions on Graphics (ToG)",
            "year": 2013
        },
        {
            "authors": [
                "Q.Y. Zhou",
                "J. Park",
                "V. Koltun"
            ],
            "title": "Fast global registration",
            "venue": "in: European conference on computer vision,",
            "year": 2016
        },
        {
            "authors": [
                "Q.Y. Zhou",
                "J. Park",
                "V. Koltun"
            ],
            "title": "Open3d: A modern library for 3d data processing",
            "year": 2018
        },
        {
            "authors": [
                "X.X. Zhu",
                "Y. Yu",
                "P.F. Wang",
                "M.J. Lin",
                "H.R. Zhang",
                "Q.X. Cao"
            ],
            "title": "A visual slam system based on the panoramic camera",
            "venue": "IEEE International Conference on Real-time Computing and Robotics (RCAR), IEEE",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "ART ICLE INFO\nKeywords: 3D reconstruction stereo vision disparity fusion pose graph optimization point cloud registration volumetric fusion\nRecovering an outdoor environment\u2019s surface mesh is vital for an agricultural robot during task planning and remote visualization. Image-based dense 3D reconstruction is sensitive to large movements between adjacent frames and the quality of the estimated depth maps. Our proposed solution for these problems is based on a newly-designed panoramic stereo camera along with a hybrid novel software framework that consists of three fusion modules: disparity fusion, pose fusion, and volumetric fusion. The panoramic stereo camera with a pentagon shape consists of 5 stereo vision camera pairs to stream synchronized panoramic stereo images for the following three fusion modules. In the disparity fusion module, rectified stereo images produce the initial disparity maps using multiple stereo vision algorithms. Then, these initial disparity maps, along with the intensity images, are input into a disparity fusion network to produce refined disparity maps. Next, the refined disparity maps are converted into full-view (360\u25e6) point clouds or single-view (72\u25e6) point clouds for the pose fusion module. The pose fusion module adopts a two-stage global-coarse-to-local-fine strategy. In the first stage, each pair of full-view point clouds is registered by a global point cloud matching algorithm to estimate the transformation for a global pose graph\u2019s edge, which effectively implements loop closure. In the second stage, a local point cloud matching algorithm is used to match single-view point clouds in different nodes. Next, we locally refine the poses of all corresponding edges in the global pose graph using three proposed rules, thus constructing a refined pose graph. The refined pose graph is optimized to produce a global pose trajectory for volumetric fusion. In the volumetric fusion module, the global poses of all the nodes are used to integrate the single-view point clouds into the volume to produce the mesh of the whole garden. The proposed framework and its three fusion modules are tested on a real outdoor garden dataset to show the superiority of the performance. The whole pipeline takes about 4 minutes on a desktop computer to process the real garden dataset, which is available at: https://github.com/Canpu999/Trimbot-Wageningen-SLAM-Dataset."
        },
        {
            "heading": "1. Introduction",
            "text": "An economical but robust online 3D reconstruction approach for outdoor environments is vital for the remote visualization of the scene and robot task planning. Recovering the dense 3D structure (e.g. mesh) of an outdoor garden with only image input quickly and robustly is challenging because of lighting changes, texture similarity, shadow interference, limited computation and network resources, etc. Figure 1 (a) shows a real outdoor garden for our gardening robot Trimbot\u2019s1 navigation and plant pruning. In real applications, there are two big challenges2 for image-based dense 3D reconstruction with high fidelity: 1) Movement (rotation or translation) between adjacent frames is big because of e.g. a gardening robot\u2019s fast speed (1 m\u2215s translation or 90 deg\u2215s rotation), the temporal downsampling ratio of the frames3\n\u2217Corresponding author at: Shenzhen Amigaga Technology Co. Ltd. Email address: chuanyu.yang@amigaga.com (C. Yang) can.pu@amigaga.com (C. Pu); jipu0216@uni.sydney.edu.au (J. Pu); radim.tylecek@gmail.com (R. Tylecek); rbf@inf.ed.ac.uk (R.B. Fisher)\nORCID(s): 1Trimbot2020 project URL: http://trimbot2020.webhosting.rug.nl/ 2For more description about the challenges in the real world, please read the following file: https://github.com/Canpu999/ Trimbot-Wageningen-SLAM-Dataset/blob/main/Real-challenges.pdf\n3Because of the mobile network speed, we pick one out of every ten frames to transfer to the server for online 3D reconstruction.\n(1/10), and the image sensors\u2019 low frame rate (12 FPS); 2) Disparity maps4 from existing methods in real outdoor environments are not accurate, dense and robust enough because of texture similarity, lighting changes, and shadows.\nAccording to a recent survey (Chen et al., 2022), SLAM5 systems are classified by the input data source or the sensors used. Figure 2 shows the SLAM classification results. Given that the input data to our proposed framework is from a newly-designed panoramic stereo camera (see Figure 1 (b), Figure D.1 or Figure D.3) and there is no existing similar work as far as we know, we have defined a new branch in the pure visual SLAM class called \u2018Panoramic Stereo SLAM\u2019 with abbreviation \u2018PS-SLAM\u2019. Panoramic stereo SLAM is a class of pure visual SLAM methods with panoramic stereo images as input (e.g. our ring of stereo vision cameras to achieve 360\u25e6 perception). Our proposed framework belongs to PS-SLAM which is in the pure visual SLAM area.\n4In a stereo configuration, disparity and depth are interchangeable measures: dept\u210e = focal_lengt\u210e\u00d7baseline\u2215disparity. When input data is from a depth sensor like Lidar or time of flight sensor, the depth information can be converted into disparity information by using a constant baseline and focal length. Thus, in this paper we regard the two terms as the same and won\u2019t distinguish them.\n5The abbreviations in this manuscript are listed in Appendix A List of Abbreviations. And the frequently-used symbols in this manuscript are listed in Appendix B List of Symbols.\nFirst Author et al.: Preprint submitted to Elsevier Page 1 of 32\nar X\niv :2\n30 5.\n06 27\n8v 1\n[ cs\n.C V\n] 1\n0 M\nay 2\n02 3\nInstead of following the existingmain-stream pure visual SLAM technique [pose recovery by feature extraction and mapping (e.g. Campos et al. (2021); Schonberger and Frahm (2016)); pose recovery by minimizing the pixel-wise photometric error (e.g. Engel et al. (2017))], we start a new approach (slightly similar to the RGB-D SLAM algorithm Kinectfusion (Izadi et al., 2011)) to do pose recovery by using the point clouds rather than the features or pixel intensities (which are sensitive to illumination, scene appearance, and shadows). To guarantee point cloud quality outdoors, a new disparity fusion algorithm is first introduced into the SLAM pipeline, whose outputs are then improved by some practical techniques. To deal with fast motion or rotation of the robot, an innovative multi-stage pose trajectory estimation method with joint information (Algorithm 1) is developed based on loop closure (LC), view switching, and global & local information transition. The integration of multi-level fusion modules, various supporting algorithms and different\ninnovative strategies make the proposed hybrid framework unique and able to cope with the real challenges mentioned above, on which the traditional SLAM frameworks (e.g. Orbslam3 (Campos et al., 2021), Open3D reconstruction system (Zhou et al., 2018), and the commercial software \u2018ContextCapture\u2019) perform badly.\nMore specifically, to solve the two big challenges above in a real garden for the trimming robot, the TrimBot2020 project team designed a new hardware configuration called the \u2018panoramic stereo camera\u2019 along with a novel 3D reconstruction software framework containing three fusion modules to compute accurate disparity maps, estimate relative pose, and geometrically integrate the maps. Figure 1 (b) shows the panoramic stereo camera which is mounted on the TrimBot2020 robot, and which is primarily used for navigation and visual servoing when the vehicle is near to plants to be trimmed. The diagram in Figure 1 (b) shows the panoramic stereo camera with 5 stereo vision cameras\nFirst Author et al.: Preprint submitted to Elsevier Page 2 of 32\n(10 image sensors \u2018Cam0\u2019 - \u2018Cam9\u2019) arranged in a pentagon shape. The panoramic stereo camera streams the synchronized panoramic stereo images (see Figure 1 (c)) from the 10 image sensors (\u2018Cam0\u2019 - \u2018Cam9\u2019) for the following three modules to deal with. First, in the disparity fusion module, rectified stereo images are combined to compute the initial disparity maps by multiple stereo vision algorithms. Then the initial disparity maps along with the image information are input into a disparity fusion network to produce a refined disparity map. Next, the refined disparity map is converted into a full-view (360\u25e6) point cloud or a singleview (72\u25e6) point cloud for the pose fusion module (see Algorithm 1). In the first stage of pose fusion, each two 360\u25e6 local point clouds are registered by a global point cloud matching algorithm to get the corresponding transformation for the global pose graph\u2019s edge, which realizes loop closure (LC) essentially. The global pose graph is then optimized to produce a coarse global pose trajectory of the robot\u2019s path through the garden. In the second stage, a refined pose graph is computed based on the coarse global pose graph. Local point cloud registration along with the coarse global pose trajectory from the first stage jointly update all the available edges in the refined pose graph, which is then optimized to estimate an accurate global sensor pose trajectory. Lastly, in the volumetric fusion module, the global poses of all the nodes in the refined pose graph are used to integrate the corresponding depth maps or point clouds into a volume to produce the surface mesh of the whole garden, which could be used for task planning and the remote visualization. Figure 3 gives an overview of the whole fusion pipeline.\nIn conclusion, there are three major contributions which could be regarded as the foundation of the PS-SLAM approach. The major contributions are :\n(1) First real garden dataset (Figure 6) for future PSSLAM research, which contains the ground truth of the fully-dense depthmaps, the semantic maps, the global poses, the rectified stereo images, sparse Lidar scans, and the semantic 3D model;\n(2) First hybrid 3D dense reconstruction framework based on panoramic stereo images, which could be regarded as the initial baseline framework (Figure 3) for future PSSLAM research;\n(3) First two-stage full-view-to-single-view global-coarseto-local-fine pose trajectory estimationmethod (Algorithm 1), which is robust to fast or large transformations between adjacent frames.\nAdditionally, there are three notable minor contributions to solve the related problems or improve the related performance in this paper:\n1) Theoretical proof (Appendix C.1) that the Frobeniusnorm-based transformation difference loss function (Equation (4)) is a special case of the maximum likelihood loss function when applied to the pose graph optimization problem;\n2) Two practical strategies (in Section 3.1.2) with the theoretical proof (Appendix C.2) to improve the disparity fusion accuracy by setting the maximum distance of interest\n(denoted by \u2018Maximum Distance\u2019) and up-and-down resolution transformation (denoted by \u2018High Definition\u2019);\n3) Three rules (in Section 3.2.2) to optimize the edge set which constrains the pose graph\u2019s loss function, boosting the estimated pose\u2019s accuracy.\nThe remainder of this paper is structured as follows. Section 2 presents previous research about SLAM classification, influence factors, and the dataset. Section 3 presents the proposed multi-level fusion framework including the disparity fusion module, the pose fusion module, and the volumetric fusion module. Section 4 describes the real garden dataset and demonstrates the performance of the fusion framework including the disparity fusion module, the pose fusion module and the volumetric fusion module on the real garden dataset. Section 5 presents a discussion and summary of the work."
        },
        {
            "heading": "2. Related Works",
            "text": "This section reviews the existing SLAM classification and positioning the proposed new SLAM framework within it. Secondly, we analyse factors which influence the framework\u2019s performance. Lastly, the outdoor datasets used for visual SLAM are reviewed."
        },
        {
            "heading": "2.1. SLAM Classification",
            "text": "According to a recent survey (Chen et al., 2022), SLAM systems are classified by the input data source or the sensors used. Figure 2 shows the classification of different SLAM systems. SLAM systems have been divided into two main categories: Lidar SLAM (e.g. Lego-Loam (Shan and Englot, 2018)) and visual SLAM.Within the visual SLAM category, there are two sub-categories: semantic visual SLAM (e.g. Blitz-slam (Fan et al., 2022)) and traditional visual SLAM. RGB-D SLAM (e.g. Kinectfusion (Izadi et al., 2011), Elasticfusion (Whelan et al., 2016)), pure visual SLAM and visual-inertial SLAM (e.g. Vins-mono (Qin et al., 2018)) constitute the traditional visual SLAM family. Monocular SLAM with a single image sensor (e.g. Colmap (Schonberger and Frahm, 2016)), stereo SLAMwith a stereo vision camera (e.g. Stereo LSD-SLAM (Engel et al., 2015)), and our proposed framework with a panoramic stereo camera belong to the pure visual SLAM class.\nWhile the newly-designed panoramic stereo camera (a ring of 5 synchronized stereo vision cameras mounted in a pentagon shape) extends the category of stereo SLAM, there are significant differences which stem from the specifics of the panoramic arrangement of multiple cameras. Thus, we define a new traditional pure visual SLAM branch \u2018Panoramic Stereo SLAM\u2019 (PS-SLAM),which uses a panoramic stereo camera - a ring of synchronized stereo vision cameras - to input a 360\u25e6 view. Although there is a stereo panoramic vision system (Guo et al., 2022) that uses a stereo vision camera containing two panoramic vision sensors with a wide field of view (FOV) , that approach still largely follows the classic stereo SLAM concept with some improvements to the stereo SLAM framework. As far as we know, our\nFirst Author et al.: Preprint submitted to Elsevier Page 3 of 32\nproposed framework (Figure 3) is the first true PS-SLAM research.\nCompared with mainstream panoramic SLAM algorithms (Chen et al., 2021, 2019; Ji et al., 2020; Wang et al., 2022; Zhang and Huang, 2021; Zhao et al., 2022; Zhu et al., 2019), one difference between ours and theirs is that they cannot provide dense global depth information because there is only one monocular camera at each viewpoint inside their panoramic camera whereas in our case multiple stereo images from different perspectives create the panoramic image. The second difference is that they still follow the typical visual SLAM pipeline (e.g. SFM (Schonberger and Frahm, 2016), Orb-slam (Mur-Artal et al., 2015)): feature extraction and mapping, pose estimation by triangulation, loop closure detection, and global optimization, which makes them sensitive to lighting changes unlike our proposed method. Additionally, these algorithms only produce a sparse reconstruction of the scene based on matched feature points as compared to our dense reconstruction.\nMeanwhile, some researchers (Ahmadi et al., 2023; Kang et al., 2021) projected the point clouds from a Lidar scanner to the image plane of a panoramic image from a panoramic camera to form a panoramic RGB-D image first. Then, the panoramic RGB-D images were input into OpenVSLAM (Sumikura et al., 2019), an open-source thirdparty library containing commonly used visual SLAM algorithms (e.g. RGB-D SLAM algorithm in Orb-slam framework (Campos et al., 2021; Mur-Artal and Tard\u00f3s, 2017)). Compared with our solution, their Lidar sensor is expensive and does not produce a dense point cloud. In summary, the existing panoramic SLAM algorithms follow the traditional visual SLAM pipeline and ours has a different theoretical framework. Based on our newly-designed panoramic stereo camera, we provide an economic dense reconstruction solution which is robust to light changes and scene appearance.\nDifferent from previous frameworks (Chen et al., 2022; Kazerouni et al., 2022; Xu et al., 2022; Zhang et al., 2021), the proposed framework is designed specifically for a panoramic stereo camera set and concentrates on depth\nquality improvement under some challenging conditions (illumination changes, similar texture, etc.) and accurate global pose trajectory estimation while coping with the robot\u2019s fast motion or rotation. Thus, some new features are proposed to enhance the SLAM framework, such as the disparity fusion module, the field of view switching (360\u25e6 versus 72\u25e6), the novel multi-stage global pose trajectory estimation algorithm 1.\nTo summarize, the proposed framework is the first to do 3D dense reconstruction in the PS-SLAM research subfield, and so is a baseline to facilitate the progress of PSSLAM. Compared with the popular SLAM frameworks (e.g. OrbSLAM3 (Campos et al., 2021), Open3D reconstruction system (Zhou et al., 2018), and the commercial software \u2018ContextCapture\u2019), the integration of multi-level fusionmodules, various supporting algorithms, and different innovative strategies makes the proposed framework both unique and capable of performing well even given the two real challenges facing any real outdoor robot: depth data quality and fast robot motion."
        },
        {
            "heading": "2.2. Performance Factors",
            "text": ""
        },
        {
            "heading": "2.2.1. Depth Quality",
            "text": "Compared with Lidar scanners (expensive and their point cloud is sparse) and ToF sensors (sensitive to infrared light outdoors), etc., image-based depth estimation methods (e.g. stereo vision algorithms) are economical and produce dense depth map indoors and outdoors robustly. In our proposed framework, the stereo vision algorithms estimate the raw disparity maps and the disparity fusion algorithm is used to refine the raw disparity maps from the stereo vision algorithms to get a refined disparity map.\nThe most well-known classical stereo vision algorithm is the semi-global matching method (Hirschmuller, 2005), which conducts pixel-wise matching using mutual information with a global smoothness approximation. With the rise of deep neural networks, Flownet (Dosovitskiy et al., 2015) is the first to use an end-to-end convolutional neural network to estimate the disparity map between two images.\nFirst Author et al.: Preprint submitted to Elsevier Page 4 of 32\nA recent survey (Poggi et al., 2021) gives an overview of the latest progress of stereo vision algorithms. Although stereo vision algorithms have made huge progress recently, a single stereo vision algorithm still has different advantages and disadvantages, and fails to estimate disparity maps accurately at all pixels in all scenes. Disparity fusion is a good method for refining the initial raw disparity maps (from the same viewpoint) from several individual disparity estimation algorithms to estimate a more accurate and robust disparity map based on their complementary properties. The majority of classical disparity fusion methods (Marin et al., 2016; Poggi et al., 2019; Zakeri et al., 2020) share the same pipeline: estimate the disparity map and a confidence map from different sensors, and then use a specific fusion method to fuse the disparity maps using the confidence maps as weights. Because it is hard to estimate the confidence map and disparity distribution accurately, these classical methods have a lower precision compared with deep-learning-based methods (Pu and Fisher, 2019; Pu et al., 2019; Sandstr\u00f6m et al., 2022). To highlight, Sdf-man (Pu et al., 2019) is the first to input multiple initial disparity maps with auxiliary information (e.g. RGB, gradients) into the refiner network to produce a refined disparity map. It used a discriminator to classify the refined disparity map and the ground truth disparity map as real or fake to improve the refined disparity map\u2019s accuracy.\nIn existing SLAM system surveys (Chen et al., 2022; Kazerouni et al., 2022; Xu et al., 2022; Zhang et al., 2021), the emerging concept of \u2018disparity fusion\u2019 was not mentioned and we saw no mention of using a disparity fusion algorithm in the SLAM system to improve the 3D dense reconstruction accuracy. We are the first to encode the disparity fusion part in the front end of our proposed SLAM system based on Sdf-man (Pu et al., 2019). Although Sdf-man achieved state-of-art real-time performance in an outdoor garden, its error rate still lies at ~10 cm level. In this paper, we propose two new practical strategies (Section 3.1.2) along with a proof (Appendix C.2) to improve the disparity fusion accuracy, as demonstrated by experiments in Section 4.2. 2.2.2. Pose Accuracy\nCompared with image feature matching (e.g. SIFT (Ng and Henikoff, 2003), ORB (Rublee et al., 2011)) to estimate the 6D pose between different views, estimation based on point clouds can produce a more reliable and accurate result. Currently, there are three classes of point cloud matching algorithms to estimate the relative 6D pose. A recent survey (Huang et al., 2021) has an overview. The first class of algorithms (e.g. Gu et al. (2022); Junior et al. (2022); Segal et al. (2009)) is derived from ICP (Besl and McKay, 1992), which calculates the relative 6D pose between two point clouds by finding the closest corresponding points in two point clouds and minimizing their Euclidean distance. Exactly corresponding points seldom exist in the real cases, so the ICP-based methods have low accuracy (and initialization issues). The second class is feature-based algorithms (Ao et al., 2021; Wu et al., 2021; Zeng et al., 2017; Zhou et al.,\n2016). They extract local descriptors from two point clouds first and then do feature matching to estimate the relative 6D pose between the two point clouds. This class is sensitive to noisy and sparse point clouds which may lead to inaccurate local descriptors and could evenmake the algorithm collapse when the density is too sparse or the noise is too strong. The third class (Huang et al., 2022; Liu et al., 2021; Myronenko and Song, 2010; Pu et al., 2018) treats point cloud registration as a probability matching problem. They use probabilistic models to describe the geometric distribution of the two point clouds first and thenmaximize the likelihood of two probabilistic models overlap to calculate the relative 6D pose of the two point clouds. This class of algorithms aligns point clouds more accurately and robustly compared with the previous two classes, but is slow because of their computational complexity.\nPairwise point cloud registration algorithms only compute the relative 6D pose between two local segments within a whole pose trajectory, and do not guarantee estimation of the global optimum of the whole global pose trajectory. That is, registering point clouds sequentially produces a sensor pose trajectory which inherently drifts over time because of the accumulated error. Building an optimized pose graph (Barath et al., 2021; Grisetti et al., 2010; Mendes et al., 2016) could reduce the accumulated error and give an optimized global solution. Ordinarily, loop closure helps resolve this issue, but here the 360 degree point clouds allow many overlapping point sets. Based on the full-view and single-view point clouds, we are the first to develop a two-stage full-view-to-single-view global-coarse-to-localfine pose trajectory estimation method (Algorithm 1), which can cope well with the fast motion of the real robot outdoors."
        },
        {
            "heading": "2.2.3. Volumetric Fusion",
            "text": "With a range of depth maps and their corresponding global poses, volumetric fusion methods (Curless and Levoy, 1996; Zhou and Koltun, 2013) integrate the surface geometry information into a volume that represents the 3D space in the world coordinate system. Using volumetric integration to build the 3D model of the garden and the marching cube technique (Grosso and Zint, 2022; Lewiner et al., 2003) to extract the surfacemesh and its corresponding point cloud is a good option to remove outliers and noise, which could result in good quality when reconstructing the 3D garden model. We use an existing volumetric fusion technique (Section 3.3) for completeness and visualization purposes and do not claim a contribution for this part."
        },
        {
            "heading": "2.3. Dataset",
            "text": "There are multiple outdoor datasets for visual SLAM research, such as Kitti (Geiger et al., 2013; Menze and Geiger, 2015) and Cityscapes (Cordts et al., 2016) for autonomous driving in the city. Besides our dataset, some other datasets (e.g. Alam et al. (2022); Chebrolu et al. (2017); Hu et al. (2022); Polvara et al. (2022)) for agricultural robots have recently been announced. For example, LettuceMOT (Hu\nFirst Author et al.: Preprint submitted to Elsevier Page 5 of 32\net al., 2022) and TobSet (Alam et al., 2022) have only semantic information for lettuce, tobacco crop, weed detection and tracking. The Sugar Beets Dataset (Chebrolu et al., 2017) contains the data from an RGB-D sensor (Kinect v2), a 4-channel multi-spectral camera (JAI AD-130GE), two on-board Lidar scanners (Velodyne VLP-16 Puck) and two GPS sensors (Leica RTK GPS and Ublox GPS) as well as wheel encoders to facilitate the research relevant to plant classification, localization and mapping in a sugar beet field. The BLT Dataset6 (Polvara et al., 2022) contains the data from two RGB-D sensors (ZED), an IMU (RSX-UM7), a 2D Lidar scanner (SICKMRS1000) and a 3D Lidar scanner (Ouster OS1-16) for long-term mapping and localization in a vineyard.\nCompared with all the previous datasets, the obvious difference in our dataset is the inclusion of fully dense ground truth depth maps, a fully dense ground truth semantic 3D model and the synchronized joint panoramic stereo information (including RGB & intensity, fully-dense depth, semantic labels, sparse laser scan and global pose) for the first time. Our released dataset could facilitate multiple research topics in SLAM, including sensor calibration, depth estimation, semantic segmentation, pose estimation, and all types of SLAM frameworks (Lidar SLAM, semantic visual SLAM, and traditional visual SLAM). To the best of our knowledge, this is the first public dataset (Section 4.1) in the panoramic stereo SLAM domain."
        },
        {
            "heading": "3. Methodology",
            "text": "Figure 3 shows the proposed hybrid 3D dense reconstruction framework based on images from a panoramic stereo camera rig. Five calibrated binocular cameras inside the panoramic stereo camera (each with a FOV of 72\u25e6) stream five pairs of rectified stereo images from the related left and right cameras synchronously into the disparity fusionmodule. The stereo image pairs are fed into two separate stereo vision algorithms to compute disparity maps in the same view. The disparity maps are used by the disparity fusion algorithm to produce the refined disparity map, which can be converted into the corresponding full-view (FOV = 360\u25e6) or single-view (FOV = 72\u25e6) point cloud. The point clouds\u2019 outliers are removed by post-processing.\nIn the pose fusion module, the full-view point clouds from different frame times are first registered with each other by a global point cloud matching algorithm for a coarse global pose estimate, which becomes the corresponding edge\u2019s transformation in the global pose graph. Then, the global pose graph is optimized to produce a coarse global sensor pose trajectory. In the second pose fusion stage, the refined pose graph is initialized by the global pose graph. Each pose graph edge\u2019s transformation is an input into q local point cloud registration algorithm to obtain a more accurate global pose, which will be used to update each edge\u2019s transformation in the refined pose graph. Finally, the\n6BLT dataset: https://lcas.lincoln.ac.uk/wp/research/ data-sets-software/blt/\nrefined pose graph is optimized to output a more accurate global pose trajectory, which transforms each single-view point cloud (or depth map) into the global coordinate system in the volumetric fusion module to create a mesh of the whole garden.\nIn the following, Section 3.1 introduces the disparity fusion module including the stereo vision algorithms, the disparity fusion algorithm, and the post-processing step. Section 3.2 introduces the pose fusion module, including the global pose graph and refined pose graph. Section 3.3 introduces the volumetric fusion module."
        },
        {
            "heading": "3.1. Disparity Fusion Module",
            "text": ""
        },
        {
            "heading": "3.1.1. Disparity Estimation",
            "text": "In this stage, stereo vision algorithms with complementary properties estimate the initial disparity maps from the stereo images. Based on common sense and experience, classical stereo vision algorithms (e.g. Hirschmuller (2005)) perform better at the edges and small objects while the methods based on deep learning (e.g. Mayer et al. (2016)) perform better at other aspects (e.g. flat planes, close shots). We have chosen DispNet (Mayer et al., 2016) and Semiglobal matching (Hirschmuller, 2005) as suitable representatives to compute the initial disparity maps in our project, but other stereo vision algorithms can be used as well. In the following, the initial disparity maps and auxiliary information (intensity and gradient information) are fed into the disparity fusion network to get a refined disparity map."
        },
        {
            "heading": "3.1.2. Disparity Fusion",
            "text": "In order to obtain a more accurate disparity map robustly, fusing disparity maps from multiple sources is a good solution considering cost and performance, under the assumption that the initial disparity inputs are from the same viewpoint at the same time. Fusing multiple input disparity maps to get a refined disparity map output is called disparity fusion, and we base it on Sdf-man (Pu et al., 2019) with some small differences, motivated by a machine learning ensemble approach. As demonstrated in (Pu et al., 2019), the disparity fusion algorithm Sdf-man can refine the initial disparity inputs effectively and produce a more accurate disparity map robustly compared with its initial disparity inputs, even where the disparity inputs are inaccurate on their own.\nSimilar to the GAN approach, Sdf-man (Pu et al., 2019) consists of two adversarial networks (refiner and discriminator) to perform a mini-max two-player game strategy to make the refiner network output a more accurate disparity map. However, unlike standard GANs (Goodfellow et al., 2014), the input is the initial disparity maps plus intensity and gradient information rather than random noise, and its output is deterministic during inference.\nFor the sake of readability, we summarize the Sdfman (Pu et al., 2019) method; more background and details can be found in the original paper.\nThe refiner neural network R (which is similar to the generatorG in (Goodfellow et al., 2014)) is trained to output\nFirst Author et al.: Preprint submitted to Elsevier Page 6 of 32\na refined disparity map that is not classified as \u201cfake\u201d by the discriminator network D. The discriminator network D is trained simultaneously to conclude that the input disparity map from the ground truth is real and the input disparity map from the refiner network R is fake. With a minimax twoplayer game strategy, it leads the output distribution from the refiner to approximate the real disparity data distribution. The full system pipeline is shown in Figure 4.\nTo train the refiner network and discriminator network, the following loss function is used in a fully supervised way:\n(R,D) = 1LdL1 (R)\n+ 2Ldsm (R) + 3 M \u2211\ni=1 LdGAN (R,Di)\n(1)\nwhere 1, 2, 3 are the weight values of the different loss terms.M is the number of scale levels used.R represents the refiner network andD represents the discriminator network. LdL1 (R) is a gradient-based L1 distance training loss, which applies a bigger weight to the disparity information at the scene edges to avoid blurring at scene edges. Ldsm (R) is a gradient-based smoothness term, which is used to propagate more accurate disparity values from scene edges to other areas, assuming that the disparity values of neighboring pixels should be close if their image intensities are similar. LdGAN (R,Di) is a disparity relationship training loss, which assists the refiner in outputting a disparity map whose distribution is closer to the real distribution. Ld is the labelled data, which is fed in the supervised learning process.\nIn this paper, we updated the original method (Pu et al., 2019) to improve its performance in the real robot application with the following two practical strategies:\n(1) Maximum Distance strategy: The disparity fusion network does not require to output all the disparity information in the source stereo images because the mobile robot needsmore accurate depths of the nearby surroundings (rather than the remote scene). Thus, a maximum distance threshold max_dist constrains the output of the disparity fusion network rather than the maximum disparity threshold max_disp. More specifically, in the initial stages, at the end of the refiner network, it uses the function tanh to output an intermediate map w and uses the function initial_disp = max_disp\u22c5(w+1) 2 to convert the intermediate map w into the disparity map initial_disp. The intermediate map\u2019s size (width, height, channel) is identical to the disparity map\u2019s. In this paper, the difference is that we use a modified function new_disp = 2fbmax_dist\u22c5(w+1) to map the tanh output to the new disparity map new_disp where f and b are the focal length and baseline of the stereo vision camera. This strategy effectively reduces the disparity fusion error (see the experiments in Section 4.2). The theoretical proof can be found in Appendix C.2.\n(2)High Definition strategy: The initial disparity fusion network (Pu et al., 2019) outputs a disparity map with the same resolution as that of the stereo images, whichwill result in small details being lost in the fused disparity map. To produce a more detailed result in the fused disparity map, the new disparity fusion network is required to output an HD (High Definition) disparity map first. The ratio between\nFirst Author et al.: Preprint submitted to Elsevier Page 7 of 32\nthe HD width resolution and the initial width resolution is w and the ratio between the HD height resolution and the initial height resolution is \u210e. Then the HD disparity map is downsampled to the initial resolution (same as the input stereo images). The refiner and the discriminator from Sdf-man are able to adjust their networks adaptively to any resolution of images, as shown in Sdfman (Pu et al., 2019) (Figures 2,3). What is done differently here is: 1) upscaling the data input first and then inputing the upscaled data into the networks to train autonomously; 2) downscaling the disparity output from the refiner network to the resolution of the initial stereo images as the final result. The reason why the up-and-down resolution transformation strategy works is that Sdf-man will include more neurons in the refiner and\ndiscriminator network structure to capturemore small details autonomously when the input resolution becomes higher. Experiments in Section 4.2 demonstrate this is an effective strategy.\nAfter disparity fusion, a refined disparity map is produced registered to the left view in the stereo configuration. Given that there are still some outliers in the refined disparity map, the refined disparity map is converted into a local point cloud with the outliers removed in the next stage."
        },
        {
            "heading": "3.1.3. Post-processing",
            "text": "The disparity post-processing part consists of three steps: 1) converting the disparity map into a depth map; 2) converting the depth map into a local point cloud using the\nFirst Author et al.: Preprint submitted to Elsevier Page 8 of 32\ncamera calibration parameters; 3) removing the point cloud outliers.\nIn the first step, the refined disparity map is converted into the depth map using Equation (2).\ndept\u210e = focal_lengt\u210e \u00d7 baseline\ndisparity (2)\nIn the second step, the depth map is back-projected into a 3D point cloud using the camera intrinsic parameters (Szeliski, 2010). In Equation (3), (u, v) is the coordinate of the 2D point on the image plane and (X, Y ,Z) is the corresponding 3D point in the camera space. fx, fy are the focal lengths on the x, y axes and cx, cy are the coordinates of the principle point on the x, y axes. D is the depth value.\nD \u23a1 \u23a2\n\u23a2\n\u23a3\nu v 1 \u23a4 \u23a5 \u23a5\n\u23a6\n= \u23a1 \u23a2\n\u23a2\n\u23a3\nfx 0 cx 0 fy cy 0 0 1 \u23a4 \u23a5 \u23a5\n\u23a6\n\u23a1\n\u23a2\n\u23a2\n\u23a3\nX Y Z \u23a4 \u23a5 \u23a5\n\u23a6\n(3)\nIn the third step, any points that 1) have less than Np neighboring points in a given sphere with the radius radius or 2) are farther away from theirNn neighboring points than a threshold distance ratio dist_ratio (which is equal to the mean distance to theirNn neighboring points divided by the distance standard deviation to the Nn neighboring points) are treated as outliers and are removed.\nAfter post-processing stage, the local single-view (72\u25e6) and full-view (360\u25e6) point clouds in the current frame are produced and used in the following pose fusion module."
        },
        {
            "heading": "3.2. Pose Fusion Module",
            "text": ""
        },
        {
            "heading": "3.2.1. First Stage: Global Pose Graph",
            "text": "As a single-view point cloud (72\u25e6) has limited features for point cloud matching, we combine the local point clouds from five views (5 stereo vision cameras) at the same time to form a full-view (360\u25e6) point cloud for point cloud registration. This representation improves tracking robustness against fast or big transformations, by using the full-view (360\u25e6) point clouds for global registration. The full-view (360\u25e6) point cloudXmi , the single-view (72\n\u25e6) point cloudXsi and their corresponding global pose Pi in the world frame will constitute a pose graph node Vi. The global pose Pi is also the pose of the node Vi. Every pair of nodes Vi and Vj have an edge Eij containing a transformation matrix Tij that aligns their full-view point clouds Xmi and X m j . The nodes Vi and the edges Eij form a global pose graph G(V ,E). V is the set of nodes and E is the set of edges.\nEvery pair of full-view point clouds in different nodes are registered to get the corresponding edge\u2019s transformation matrix by using the feature-based fast global registration algorithm (Zhou et al., 2016), which essentially implements loop closure (LC). The global pose graph G(V ,E) is then optimized to produce a coarse global pose trajectory {P1, ..., Pn} by minimizing loss:\n(G(V ,E)) = argmin {P1,...,Pn}\u2208SE(3)n\n\u2211\n(i,j)\u2208E ||Tij\u2212PiP\u22121j || 2 F (4)\n||\u2219||F is the Frobenius norm, SE(3) is the special Euclidean group in 3 dimensions and n is the number of the pose graph nodes. The loss function represented by Equation (4) derives from the maximum likelihood estimation formula in (Moreira et al., 2021a) when setting the uncertainty of the translations to be identical to the rotations. We use the optimization method in (Moreira et al., 2021a) to minimize the loss function based on the implementation available at: https://github.com/gabmoreira/maks.\nThe derivation and proof of Equation (4) can be found in Appendix C.1. The same deduction and optimization methods can be applied to Equation (6) below."
        },
        {
            "heading": "3.2.2. Second Stage: Refined Pose Graph",
            "text": "The global pose graph has edges between every pair of nodes possibly, even if there is little or no overlap between their corresponding single-views. This can lead to local distortions. This stage optimizes the global pose graph by using the poses from overlapping views. The refined pose graph G\u0303(V\u0303 , E\u0303) is initialized by the global pose graph G(V ,E).\nSince the fast global registration algorithm (Zhou et al., 2016) is not accurate enough compared with local registration algorithms (e.g. GICP (Segal et al., 2009), DUGMA (Pu et al., 2018)), we input each edge\u2019s transformation matrix Tij (from Eij in the global pose graph G(V ,E)) into the local registration algorithm GICP (Segal et al., 2009) as a global initialization to align the corresponding singleview (72\u25e6) point clouds7 Xsi and X s j . The local registration algorithm GICP (Segal et al., 2009) outputs a new estimated transformation matrix T lij for the corresponding edge.\nThen, every pair of point clouds Xsi and X s j are transformed into the same coordinate system using the transformation matrix T lij . We calculate the number of the corresponding point pairs within a distance threshold (similar to finding corresponding closest point pairs in ICP). The overlap percentage of one point cloud after registration is equal to the number of the corresponding pairs divided by the number of the points in the point cloud. The overlap percentage of the pair of point clouds after registration is equal to the overlap percentage of the point cloud with the fewest points. Based on the registration results above and the coarse global pose trajectory from the first stage, the edges E\u0303ij in the refined pose graph are updated using the following three rules:\n(1) Prune: If the overlap percentage of the two point clouds after registration is lower than the threshold OLmin, prune the edge (remove the edge between the two nodes).\n(2) Update: If the overlap percentage of the two point clouds after registration is higher than threshold OLmax and if the transformation PiP\u22121j (whose 6D pose is denoted as the 6D vector v\u20d7P ) between the two nodes (Vi, Vj) is similar to the newly calculated transformation T lij (whose 6D pose is denoted as the 6D vector v\u20d7T ), update the edge.\n7When using the extrinsic transformation matrices to merge the point clouds from the five stereo vision cameras on the camera ring, the error from the extrinsic parameters will cause the full-view (360\u25e6) point cloud to be not as accurate as the single-view point cloud.\nFirst Author et al.: Preprint submitted to Elsevier Page 9 of 32\n(3) Keep: As for the \u2018else\u2019 case, keep but do not update the edge transformation.\nEquation (5) gives the precise logic for the three rules above:\nT\u0303ij =\n\u23a7\n\u23aa\n\u23a8\n\u23aa\n\u23a9\nNull < OLmin [rule1] T lij > OLmax & |v\u20d7P \u2212 v\u20d7T | < v\u20d7t\u210e [rule2] Tij else [rule3]\n(5)\nIn Equation (5), T\u0303ij is the transformation matrix of the edge E\u0303ij in the refined pose graph. is the overlap percentage of the two point clouds after registration. v\u20d7t\u210e is 6D pose threshold in vector format and |\u2219|means getting the absolute value of each element to form a new vector. Null denotes \"deleting this edge\". The two rules (Prune and Update) act on the edge set to constrain the loss function - Equation (6). An accurate constraint could give a more accurate global pose estimation, which is demonstrated by the ablation study in Section 4.3.1. After edge refinement, the refined pose graph G\u0303(V\u0303 , E\u0303) is optimized using Equation (6) to produce a more accurate global pose trajectory {P\u03031, ..., P\u0303n}. The refined accurate global pose P\u0303i of each node V\u0303i and their corresponding single-view point cloud (or depth map) will be used in the volumetric fusion process to construct the surface mesh of the whole garden.\n(G\u0303(V\u0303 , E\u0303)) = argmin {P\u03031,...,P\u0303n}\u2208SE(3)n\n\u2211\n(i,j)\u2208E\u0303\n||T\u0303ij\u2212P\u0303iP\u0303\u22121j || 2 F (6)\nTo conclude, we have proposed a two-stage full-view-tosingle-view global-coarse-to-local-fine pose trajectory estimation method in this subsection. We name this proposed method for pose trajectory estimation as \u2018Multi-stage Pose Trajectory Estimation with Joint Information (MPTEJI)\u2019. Algorithm 1 shows the pseudocode of the proposed algorithm MPTEJI, which gives a formal overview of the whole proposed method.\nAlgorithm 1 Multi-stage Pose Trajectory Estimation with Joint Information (MPTEJI) Input: single-view (72\u25e6) and full-view (360\u25e6) point clouds 1: procedure WITH FULL VIEW \u22b3 1st stage 2: global registration (feature-based) \u2192 Loop Closure 3: coarse pose graph G(V ,E) \u2190 Equation (4) 4: procedure WITH SINGLE VIEW \u22b3 2nd stage 5: pose graph inheritance \u2190 G(V ,E) 6: local registration (ICP-based)\u2190 Tij 7: edge refinement \u2190 Equation (5) 8: refined pose graph G\u0303(V\u0303 , E\u0303) \u2190 Equation (6)\nOutput: an accurate global pose trajectory {P\u03031, ..., P\u0303n}"
        },
        {
            "heading": "3.3. Volumetric Fusion Module",
            "text": "Fusing the range images (containing depth information) into a voxel-based volumetric scene representation is called volumetric fusion (Curless and Levoy, 1996). The refined accurate global pose trajectory {P\u03031, ..., P\u0303n} gives where to integrate the associated RGB-D range images projected from the single-view point clouds into a voxel-grid-based TSDF (Truncated Signed Distance Field) volume. The value of each voxel here represents the signed distance to the closest surface interface in the global space, which is in turn used to obtain the mesh of the reconstructed scene, using the marching cubes (Lorensen and Cline, 1987) algorithm.\nMore specifically, the single-view point clouds are projected back into the image planes to get the related depth maps first, creating again RGB-D images.We use the refined single-view point clouds to compute the depth maps rather than use the original depth maps from the disparity fusion (Section 3.1.2) directly because the single-view point clouds after the third step \u2018outlier removal\u2019 in the post-processing section (Section 3.1.3) are more accurate. Then the pairwise data (the RGB-D images and the corresponding global poses) are integrated into the global TSDF volume using the technique from Izadi et al. (2011); Zhou and Koltun (2013). Finally, we extract the surface mesh using marching cubes (Lewiner et al., 2003; Lorensen and Cline, 1987), based on a publicly available implementation8.\nThe volumetric fusion module produces a smooth and watertight 3D mesh of the reconstructed scene in the global coordinate system. The corresponding dense 3D point cloud of the reconstructed scene can be produced by extracting all the vertexes of the 3D mesh above. Simply put, the volumetric fusion performs like a weighted average filter in the 3D global space to reduce the noise and remove the outliers from multiple local segments by using the joint global information in the global coordinate system. That is the reason why we use the volumetric fusion to extract the mesh and the corresponding point cloud sequentially, rather than stitching the single-view point clouds together using their corresponding pose directly."
        },
        {
            "heading": "4. Experiments",
            "text": "All the experiments in this section are conducted on a machine with Intel Core i7-12700KF processor (12 cores, 20 threads, 25 MB cache, up to 5 GHz) and Nvidia GeForce GTX 1080 Ti. Section 4.1 gives the description of the real outdoor garden dataset we released and used in this paper. Section 4.2 evaluates the performance improvement of the disparity fusion module quantitatively compared with the initial disparity inputs (Hirschmuller, 2005; Mayer et al., 2016), the ground truth of DSF (Poggi and Mattoccia, 2016) and the initial version of Sdf-man (Pu et al., 2019). Section 4.3 evaluates the global pose trajectory\u2019s accuracy from\n8 https://github.com/qianyizh/ElasticReconstruction/tree/master/ Integrate\nFirst Author et al.: Preprint submitted to Elsevier Page 10 of 32\nthe pose fusion module quantitatively compared with ORBSLAM3 (Campos et al., 2021) and the \"reconstruction system\" in the latest version (0.15.1) of Open3D (Zhou et al., 2018). Section 4.4 gives a view of the reconstructed point cloud from the volumetric fusion module qualitatively and quantitatively compared with Open3D (Zhou et al., 2018)."
        },
        {
            "heading": "4.1. Dataset Description",
            "text": "Figure 5 shows the 3D model of the outdoor garden, the robot platform and the route path for collecting the raw data. All the data in our dataset were recorded within the same half day to avoid interference from vegetation growth. The raw data is from the \"test_around_garden\" bagfile9 in the Trimbot Garden 2017 dataset (Sattler et al., 2017; Tylecek and Fisher, 2020). The raw data was divided into two parts in the post-processing step: one for network training and one for testing. Figure 5 (c) shows the robot navigation path for the training and testing datasets. In Figure 5 (c), the \"route 1\" trajectory (black loop curve) around the whole garden is for the SLAM testing and the \"route 2\" trajectory10 (red curve) is for the network training (e.g. depth estimation, semantic segmentation, etc.).We use a robot (See Figure 5 b) equipped with a ring of 5 stereo vision cameras (for live operations), Velodyne Puck (VLP-16) Lidar sensor (for sparse lidar scans\n9https://www.research.ed.ac.uk/en/datasets/ trimbot2020-dataset-for-garden-navigation-and-bush-trimming.\n10All the scenes in \"route 1\" can be seen in \"route 2\".\ncollection), STIM300 IMU sensor and Topcon PS Series Robotic Total Station position tracking system (for groundtruth positions) to collect the raw images, sparse Lidar scans and the global pose of the robot. The raw stereo vision images are calibrated and rectified using the Kalibr package11. The sparse Lidar scan from the Velodyne Puck (VLP16) Lidar sensor is projected to the camera plane of each left camera in the 5 stereo settings. Robot navigation poses were recorded in the coordinate system of Topcon PS Series Robotic Total Station along with STIM300 IMU sensor first and then transformed into each image sensor\u2019s global pose. Structure-from-motion (Schonberger and Frahm, 2016) is used to refine each image sensor\u2019s pose subsequently. The 3D model of the whole garden is collected using Leica ScanStation P15 equipment and is semantically labelled manually. Figure 5 (a) shows the semantic 3D model of the whole garden. Using the semantic 3D model and each camera\u2019s pose in the garden, the dense depth map and semantic map are acquired by projecting the semantic 3D model into each camera\u2019s plane. More details about the data collection process can be found in Appendix D.\nFigure 6 shows frames from the new dataset \u201cThe TrimbotWageningen SLAMDataset\u201d, which is the augmentation of the Trimbot Garden 2017 dataset used in the semantic\n11https://github.com/ethz-asl/kalibr\nFirst Author et al.: Preprint submitted to Elsevier Page 11 of 32\nTable 1 Parameters of the Trimbot Wageningen SLAM Dataset\nParameter Name Parameter Value\nThe number of panoramic stereo camera rigs 1;\nThe number of stereo vision cameras 5;\nThe number of image sensors 10;\nThe number of panoramic frames - 360\u25e6 In the training subset: 68; In the test subset: 67;\nThe number of stereo vision frames - 72\u25e6 In the training subset: 340; In the test subset: 335;\nImage resolution 752 \u00d7 480 pixels (width \u00d7 height);\nThe mean relative pose between adjacent frames ([translation on x axis, translation on y axis, translation on z axis, roll, pitch, yaw]) [0.29 m, 0.21 m, 0.00 m, 9.04 deg, 0.97 deg, 1.16 deg];\nThe standard deviation of the relative pose between adjacent frames ([translation on x axis, translation on y axis, translation on z axis, roll, pitch, yaw]) [0.18 m, 0.18 m, 0.00 m, 13.32 deg, 0.76 deg, 0.89 deg];\nThe maximum translation value on each axis between adjacent frames"
        },
        {
            "heading": "X axis:0.47 m; Y axis: 0.67 m; Z axis: 0.02 m;",
            "text": "The maximum rotation value on each axis between adjacent frames X axis: 81.64 deg; Y axis: 3.21 deg; Z axis: 4.46 deg;\nData support RGB | intensity, dense depth, sparse lidar, semantics, pose, point cloud, calibration.\nreconstruction challenge of ICCV 2017 workshop \u201c3D Reconstruction meets Semantics\u201d (Sattler et al., 2017). In the new Trimbot Wageningen SLAM dataset, we release all the rectified images from the 10 image sensors (5 stereo vision cameras) ranging from cam_0 to cam_9 (See Figure 1b for the position of the 10 image sensors). The newly released sparse Lidar scan (from the onboard Lidar sensor - Velodyne Puck (VLP-16)), dense depth map and semantic map are in the coordinate system of each left image sensor (Cam 0, Cam 2, Cam 4, Cam 6, Cam 8). Each image sensor\u2019s global pose in the garden, their intrinsic parameters and distortion models are available in the new dataset. We subsample one out of every 10 frames from the initial raw data bagfile to form the new dataset. Table 1 lists the key dataset properties and their corresponding values in the Trimbot Wageningen SLAM Dataset. Figure 6 gives an overview of the new dataset. See the dataset website for more details: https://github.com/ Canpu999/Trimbot-Wageningen-SLAM-Dataset."
        },
        {
            "heading": "4.2. Disparity Fusion Module",
            "text": "In the disparity fusionmodule, we use the SGM (Hirschmuller, 2005) (withMatlab implementation12) andDispnet13 (Mayer et al., 2016) stereo vision algorithms to get the initial disparity maps. With the initial disparity maps and auxiliary\n12Matlab Implementation URL:https://www.mathworks.com/help/ vision/ref/disparitysgm.html\n13The authors of Dispnet (Mayer et al., 2016) were our project partners and they trained Dispnet on the project dataset to get their best performance.\ninformation (left intensity image and left gradient information), we train our supervised disparity fusion network on our outdoor real garden dataset - \"Trimbot Wageningen SLAM Dataset\". All 340 samples (\u2248 50%) in the training set are used to train and all 335 samples (\u2248 50%) in the test set are used to test. The initial supervised method (Pu et al., 2019) is named \"Sdfman-initial\" and the updated method using the two practical strategies presented in Section 3.1 is named \"Sdfman-star\". The parameter max_dist is set to 5 meters14. The parameters w and \u210e (resolution ratio between the HD and initial image width and height) are set to 2. Disparity fusion algorithms DSF (Poggi andMattoccia, 2016) and \"Sdfman-initial\" (Pu et al., 2019) are compared to the new method \"Sdfman-star\". Additionally, an ablation study is conducted by adding two internal comparison algorithms (\"Sdfman-max-dist\" and \"Sdfman-HR\"). \"Sdfmanmax-dist\" is an internal comparison algorithm that only applies the \"Maximum Distance\" strategy to \"Sdfmaninitial\". \"Sdfman-HR\" is an internal comparison algorithm that only applies the \"High Definition\" strategy to \"Sdfmaninitial\". Table 2 summarizes the algorithms\u2019 names with the corresponding strategies.\nWhen calculating the error of each algorithm, we omit the pixels whose ground truth depth exceeds the maximum\n14As the focal length and baseline are fixed, depth estimation is inversely proportional to its corresponding disparity value - see Equation (2). When the depth is 5 meters, the corresponding disparity is about 3 pixels. Estimated depth values larger than 5 meters (i.e. disparity value smaller than 3 pixels) have a larger error compared with depths closer than 5 meters.\nFirst Author et al.: Preprint submitted to Elsevier Page 12 of 32\ndistance threshold max_dist = 5 m. Table 3 shows the accuracy of each algorithm. We use meter (m) rather than\npixel disparity as the units for the error to give a more intuitive sense of the error magnitudes. With the initial input (Matlab SGM (Hirschmuller, 2005), Dispnet (Mayer et al., 2016)), DSF (Poggi and Mattoccia, 2016) reduces the mean absolute depth error from 0.40 m (Matlab SGM) and 0.24 m (Dispnet) to 0.18 m (DSF), which is larger than that of Sdfman-initial (0.09 m). Compared with Sdfmaninitial (0.09 m), Sdfman-max-dist, Sdfman-HR and Sdfmanstar are more accurate, which demonstrates that each of the proposed strategies contributes to improving the fusion accuracy. Algorithm Sdfman-star performs best with the mean absolute depth error (0.03m) and achieves this at 34.21\nFirst Author et al.: Preprint submitted to Elsevier Page 13 of 32\nframes per second. In the following experiments, we omit the two internal algorithms (Sdfman-max-dist and Sdfman-HR) because they are only used for the ablation study.\nFigure 7 (a) compares the mean absolute error of each frame\u2019s depth map in the test dataset from all the algorithms. The accuracy of Sdfman-star is better than the other algorithms at all frames, which shows the robustness of Sdfmanstar. Define parameter badX to be the percentage of pixels whose absolute depth errors in the depth map are bigger than X * 0.025 m (X is a positive number). Figure 7 (b) shows the percentage of badX pixels for different badX thresholds (values of X). Compared with the other algorithms, Sdfmanstar has fewer pixels whose absolute depth error is bigger than 0.025 m, 0.05 m, 0.075 m and 0.1 m respectively. More than 95% of the pixels (bad4) from Sdfman-star have an absolute depth error less than 0.1 m.\nFigure 8 shows one qualitative result from one image sensor (Cam 0). Compared with the other algorithms, Sdfman-star is more accurate globally and also preserves small details more vividly (e.g. object edges, the trees\u2019 trunks).\nIn the post-processing step, Np is set as 20 and radius is set as 0.05 m. Nn is set as 20 and dist_ratio is set as 1.5. Figure 9 shows one example of the point clouds from the depth maps after outlier removal. Compared with the ground truth, the remote objects (e.g. trunk) in the point clouds from Sdfman-star are noisy, which can be expected."
        },
        {
            "heading": "4.3. Pose Fusion Module",
            "text": "Section 4.3.1 presents results from an ablation study to show that the strategies proposed in Section 3.2 are effective. Section 4.3.2 compares Orbslam3 (Campos et al., 2021) and Open3D (Zhou et al., 2018) with the proposed pose fusion method.\nTwo methods are used to evaluate the 6D pose estimate accuracy. The first one uses the 6D pose vector [tx, ty, tz, r, p, y] ([translation on X axis, translation on Y axis, translation onZ axis, roll, pitch, yaw]). The unit for tx, ty, tz is meters and the unit for r, p, y is degrees. The absolute difference between the ground truth and the estimated 6D vector is a measure of the 6D pose\u2019s accuracy on each axis. The second method uses the rotation matrix and translation\nFirst Author et al.: Preprint submitted to Elsevier Page 14 of 32\nFirst Author et al.: Preprint submitted to Elsevier Page 15 of 32\nvector. The overall accuracy of the 6D poses is computed using Equation (7) and Equation (8) (Huynh, 2009)\nER = ||I \u2212RgtR\u22121est||F (7)\nEt = ||tgt \u2212 test||F (8)\nwhere || \u2219 ||F is the Frobenius norm. Rgt , tgt are the ground truth and Rest , test are the estimated values, respectively.\nEquation (7) does not have a physical unit although smaller is better and is a measure of better point cloud overlap. Equation (8) is the distance between the two coordinate systems\u2019 origins (the ground truth and estimated coordinate system) and its unit is meter. Both ways of the above methods evaluate the 6D pose\u2019s accuracy, although their error values and their error estimation methods are different.\nFirst Author et al.: Preprint submitted to Elsevier Page 16 of 32"
        },
        {
            "heading": "4.3.1. Ablation Study",
            "text": "We define six models to compare each proposed strategy\u2019s effectiveness. Table 4 lists the defined model names and the strategies. The model \"Ours\" is the proposed method in Section 3.2 with the point cloud input from Sdfman-star and is the baseline model, from which the other models are derived by changing only one strategy or factor. Model \"Ours-global\" disables the second-stage pose graph - the refined pose graph in Section 3.2.2. Model \"Ours-prune\" disables rule 1 - \"Prune\" and thus will not prune any edge, no matter what the edge\u2019s reliability is. Model \"Ours-update\" disables rule 2 - \"Update\" and thus will not update the transformation matrix of each edge. Model \"Ours-gtdepth\" replaces the input point clouds from Sdfman-star with the point clouds from the ground truth depth. Model \"Ourssingle-stereo\" only inputs the stereo images from the front stereo camera (which consists of image sensors Cam0 and Cam1) rather than the panoramic stereo images from the ring of synchronized stereo cameras.\nFor all models, the overlapping rate threshold OLmin = 0.33 and OLmax = 0.35. The 6D pose vector v\u20d7t\u210e is set to [0.4 m, 0.4 m, 0.4 m, 15\u25e6, 15\u25e6, 15\u25e6] ([translation on X axis, translation on Y axis, translation onZ axis, roll, pitch, yaw]). Figure 10 shows the 2D trajectories from all the models when looking downward from above at the whole garden.\nThe trajectory of Ours-gtdepth is closest to the ground truth. The trajectories of Ours and Ours-prune are similar and rank 2nd together. The remainingmodels performworse. In particular note that Ours-single-stereo did not work correctlyin the latter part of the global pose trajectory, with completely wrong pose estimates. All the models except Ours-single-stereo have similar performance on the rotation factor, but perform on the translation factor variously. Table 5 shows the overall performance of each model by using the metrics in Equation (7) and Equation (8). The running time of all the models are close (about 230 s) except Ours-global (210.86 s) and Ours-single-stereo (179.28 s). From the quantitative aspect, it is obvious that Ours-singlestereo fails compared with the other models. Thus, using the panoramic stereo images from the ring of synchronized stereo vision cameras in the proposed framework is vital to overcome the challenges of the fast or large transformations between adjacent frames when a real robot navigates in a\nreal outdoor environment. The reason is that the 360\u25e6 field of view makes the overlap between successive views high, which ensures the success of the global point cloudmatching in the first stage of the pose fusion - global coarse pose graph optimization - to avoid the possibility of the whole 3D reconstruction framework collapsing. This strongly supports our major contribution (3) because we are the first to combine the two-stage full-view-to-single-view globalcoarse-to-local-fine pose graph optimization with a ring of synchronized stereo vision cameras simultaneously to handle the robot\u2019s fast movement in the real world. The performance the other models (except Ours-single-stereo) on the rotation factor are similar (0.08 - 0.12) but the performance on the translation factor fluctuates (0.27 m - 0.52 m). The translation accuracy of the model \"Ours-global\" and \"Ours-update\" is much lower than that of the model \"Ours\", which demonstrates the two-stage from-coarse-to-fine pose graph optimization and rule 2 - \"Update\" are effective. The accuracy of the model \"Ours-prune\" is slightly worse than that of the model \"Ours\" on both rotation and translation factors, which shows that rule 1 - \"Prune\" is effective. Rule 1 \"Prune\" and rule 2 \"Update\" improve the performance because theymake the transformation of the edge set E\u0303 more accurate and reliable which improves the constraint encoded in the loss function (see Equation (6)). A more accurate constraint leads to to a more accurate pose estimate. The accuracy of model \"Ours-gtdepth\" is better than that of the model \"Ours\", which demonstrates that better recovery of the input point clouds leads to better pose fusion accuracy. Thus, one topic for future work is to continue improving the accuracy of the input point clouds.\nFirst Author et al.: Preprint submitted to Elsevier Page 17 of 32\nTable 5 Ablation study for pose fusion\nMetric Ours Ours-global Ours-prune Ours-update Ours-gtdepth Ours-single-stereo ER 0.11 0.08 0.12 0.08 0.08 0.98 ER 0.07 0.04 0.07 0.04 0.05 0.99 Et (m) 0.33 0.48 0.34 0.52 0.27 2.47 Et (m) 0.18 0.32 0.20 0.29 0.16 1.99 T ime (s) 233.24 210.86 233.14 233.15 230.41 179.28"
        },
        {
            "heading": "4.3.2. Comparison with Existing Methods",
            "text": "We compare themodel \"Ours\" with existing state-of-theart algorithms, represented by the RGBD SLAM algorithm in Orbslam3 (Campos et al., 2021) and the reconstruction system in Open3d (Zhou et al., 2018) as available online15. The depth maps that all the algorithms receive as input are the output from the Sdfman-star fusion algorithm. The parameter setting in model \"Ours\" is the same as that in Section 4.3.1. For Orbslam3, we set the number of features per image \"ORBextractor.nFeatures\" as 10000. The number of levels in the scale pyramid \"ORBextractor.nLevels\" is 15. The fast threshold \"ORBextractor.iniThFAST\" is 5 and \"ORBextractor.minThFAST\" is 3. The number of camera frames per second is 1. The rest of the parameters are the same as those in their released code. As the Orbslam3 framework does not support panoramic data, we input the RGB images and the corresponding depth maps from the image sensor \u2018Cam0\u2019 into the RGBD SLAM algorithm in the Orbslam3 framework. To make Orbslam3 work better on the difficult dataset \"Trimbot Wageningen SLAM Dataset\", we additionally provide the ground truth pose to Orbslam3 when the adjacent frames have a large rotation and Orbslam3 lost tracking (at all the corners of the trajectory). More specifically, at Frames 20, 31, 50, 54, 56, we provide the corresponding ground truth pose to Orbslam3. See Figure 11 (b) and Figure 11 (c) where both the rotation and translation error of Orbslam3 are equal to 0. Open3D failed to work if we only input the single-view point clouds. To make Open3D perform better, we modified its initial code to make it use our full-view and single-view point clouds. We also provide the comparison results under the same conditions in Appendix E.2 Fair Comparison With More Open-source Frameworks. Readers can test their own code on the \"Trimbot Wageningen SLAM dataset\".\nFigure 11 (a) shows the estimated global trajectory from GT, Orbslam3, Open3D, and Ours. Figure 11 (b) and (c) show the rotation and translation error at each frame time in the test dataset using Equation (7) and Equation (8). From Figure 11 we could see Open3D and Ours perform much more accurately and robustly than Orbslam3. Open3D and Ours have similar performance on the rotation and Ours performs more accurately than Open3D on the translation.\nIf we use the absolute difference between the ground truth and the estimated 6D vector to describe the 6D pose accuracy, Table 6 compares the performance of the algorithms\n15Orbslam3: https://github.com/UZ-SLAMLab/ORB_SLAM3 and Open3D: https://github.com/isl-org/Open3D\non each axis. Our approach\u2019s mean bias and the related standard deviation of the translation on the x, y, and z axis are generally smaller than those of Orbslam3 and Open3D. The rotation performance of Ours and Open3D on x, y, and z axis is more accurate and robust than that of Orbslam3. Our rotation performance on x, y, and z axis is similar to that of Open3D.\nTable 7 shows the overall performance of each algorithm using Equation (7) and Equation (8). Ours performs best, although it increases the running time slightly. Given the bad performance of Orbslam3 on the real outdoor garden dataset, we will omit Orbslam3 in the following text and compare Ours with Open3D in Section 4.4 \"Volumetric Fusion Module\" only."
        },
        {
            "heading": "4.4. Volumetric Fusion Module",
            "text": "In this part, we set the maximum depth for integrating as 5 meters. The size of TSDF (Truncated Signed Distance Field) cube is 10 meters. The length of each voxel is 0.01 m (1 cm). The truncation value for the signed distance function (SDF) is set to 0.06.\nFirst Author et al.: Preprint submitted to Elsevier Page 18 of 32\nFigure 12 shows the mesh of the reconstructed garden and its details at different sites. Figure 12 (a) shows the overview of the reconstructed whole garden. Figure 5 (a) shows the ground truth. Figure 12 (b) (c) (d) (e) show closeup views at sites 1, 2, 3, 4 in Figure 12 (a). The white blank areas in all the figures are regions that have not been scanned during driving. These regions did not have target plants for the trimming robot and thus were not scanned. From the details, the reconstructed scene is good enough for the remote visualization and coarse robot task planning. A video that shows the reconstructed garden is at: https: //youtu.be/zGxcj0_NXCA.\nIn the following, the reconstructed gardens from all the algorithms are compared to the ground truth 3D model of the whole garden in the same world coordinate system.\nThe evaluation method consists of estimating the mean and standard deviation of the minimum distance between each point of the reconstructed garden and its closest point in the ground truth garden model. Thus, this metric measures how close the reconstructed garden is, on average, with respect to the ground truth garden model.\nTable 8 shows the mean dist and standard deviation dist of the minimum distance between the corresponding closest points. The mean and standard deviation of the minimum distance\u2019s absolute bias on x, y, and z axis are (dx, dx), (dy, dy) and (dz, dz) respectively. The maximum of the minimum distance\u2019s absolute mean bias on all the three axes is dz (0.15 m) and the mean of the minimum distance dist is\nFirst Author et al.: Preprint submitted to Elsevier Page 19 of 32\n0.18 m, which are good enough for the user\u2019s remote visualization and robot global task planning16 on the reconstructed global model.\n16Our trimming robot did coarse global task planning on the reconstructed global model first. When the robot arrives at the specific location\nfor trimming, the robot arm will move the depth cameras on the robot arm to scan the target locally and build the accurate local 3D model with the precise pose update from the robot arm\u2019s joints.\nFirst Author et al.: Preprint submitted to Elsevier Page 20 of 32\nFigure 13 (a) (c) (e) show the reconstructed gardens from Open3D, Ours, and the ground truth garden model. Figure 13 (b) (d) (f) show the details on the same site in the real garden. Compared with ours, we could see Open3D fails to align the point clouds of the same tree, and makes it seem that there were two trees on that site.\nIn addition to the experiments above, there are two other experiments. In the first experiment, the proposed framework is successfully tested with scene appearance and sunlight change. More details can be found in Appendix E.1. The second experiment compares our proposal with a popular commercial software application \u2018ContextCapture\u2019 on the Trimbot Wageningen SLAM Dataset. The proposed approach again has better performance. For more details, see Appendix E.3."
        },
        {
            "heading": "5. Conclusion and Discussion",
            "text": "This paper presented an improved approach for recovering accurate outdoor 3D scene reconstruction, based on disparity fusion, pose fusion and volumetric fusion, and demonstrated its performance by reconstructing a real outdoor garden containing a variety of different natural and man-made structures. Avoiding the need for expensive and sparse Lidar scans, the proposed approach inputs the disparity maps from two different stereo vision algorithms into a disparity fusion network to produce accurate disparity maps, which is a cheap, accurate and robust solution to get higher quality depth data. The depth data is converted into point clouds, whose outliers are removed, and then input into the pose fusion module. The pose fusion module uses a two-stage from-global-coarse-to-local-fine pose graph optimization to estimate a more accurate global pose trajectory. More specifically, in the first stage, we use fast global point cloud registration (Zhou et al., 2016) and full-view (360\u25e6) point clouds to build a coarse global pose graph, which is robust to fast motion and big transformations between two consecutive frames. In the second stage, a local point cloud registration algorithm GICP (Segal et al., 2009) extended with three domain rules optimizes the refined pose graph, which produces a more accurate global pose trajectory. With the accurate global pose trajectory and the fused depth\nmaps, the mesh of the whole garden can be reconstructed by volumetric fusion, as demonstrated on a real outdoor dataset.\nThe key to a good 3D reconstruction of the real garden is the accurate depth map and global pose trajectory. In future work, more advanced disparity fusion networks will be explored to continue to improve the disparity accuracy. The accuracy of the 6D pose that registers the point clouds affects the accuracy of the edges in the pose graph, which in turn influence the optimized global pose trajectory. More advanced and faster global and local point cloud registration algorithms that are robust against strong noise and large occlusions will be explored to get more accurate initial 6D pose estimation. The research proposed in this paper is adapted for use by a robot in a garden environment, but it can be generalized to different outdoor application scenarios. However, this requires the related ground truth for the network training and performance evaluation. Expanding into related domains of robot applications is our priority for the near future.\nCRediT authorship contribution statement Can Pu: conceptualization, methodology, experiment, writing - original draft, funding acquisition.ChuanyuYang: methodology, experiment, review & editing, funding acquisition. Jinnian Pu: Experiment data analysis and visualization. Radim Tylecek: Experiment data analysis and visualization (work done while at University of Edinburgh), review & editing. Robert B. Fisher: helped with design of theory and experiments, review & editing, funding acquisition, supervision.\nDeclaration of Competing Interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper."
        },
        {
            "heading": "Acknowledgements",
            "text": "Before 2020, the research was funded by the TrimBot2020 project (Grant Agreement No. 688007, URL: http: //trimbot2020.webhosting.rug.nl/) from the EuropeanUnion Horizon 2020 programme. After 2020, the research funding is from Shenzhen Amigaga Technology Co. Ltd. by the Gagabot2022 project (Grant Agreement No. P987001), from the Human Resources and Social Security Administration of ShenzhenMunicipality by Overseas High-Caliber Personnel project (Grant NO. 202102222X, Grant NO. 202107124X) and from Human Resources Bureau of Shenzhen Baoan District by High-Level Talents in Shenzhen Baoan project (Grant No. 20210400X, Grant No. 20210402X). We thank Gabriel Moreira from Moreira et al. (2021a,b) for the help with pose graph optimization. We thank all the partners from TrimBot2020 consortium for their help when we did this work, and for their contributions to test garden design, robot and sensor design and construction, data collection,\nFirst Author et al.: Preprint submitted to Elsevier Page 21 of 32\nand ground truthing, as well as many other contributions to the TrimBot2020 project.\nFirst Author et al.: Preprint submitted to Elsevier Page 22 of 32"
        },
        {
            "heading": "Appendix A. List of Abbreviations",
            "text": "FOV Field Of View GAN Generative Adversarial Network HD High Definition ICP Iterative Closest Point LC Loop Closure MPTEJI Multi-stage Pose Trajectory Estimation with Joint Information ORB Oriented FAST and Rotated BRIEF PS-SLAM Panoramic Stereo SLAM SDF Signed Distance Function SFM Structure From Motion SIFT Scale Invariant Feature Transform SLAM Simultaneous Localization And Mapping TOF Time of Flight TSDF Truncated Signed Distance Field"
        },
        {
            "heading": "Appendix B. List of Symbols",
            "text": "max_dist The maximum distance threshold w The width resolution ratio between the HD\nand the initial image \u210e The height resolution ratio between the HD\nand the initial image Np The threshold number of the neighboring points in a sphere radius The radius of the sphere Nn The number of the points in the neighborhood dist_ratio The distance ratio to remove the points OLmin The minimum overlapping rate threshold OLmax The maximum overlapping rate threshold v\u20d7t\u210e 6D pose threshold in vector format"
        },
        {
            "heading": "Appendix C. Formula derivation",
            "text": "C.1. Loss Function In this subsection, we will prove that Equation (4) in this paper is equal to equation 4 in the paper (Moreira et al., 2021a) under the assumption that the uncertainty of the rotation is the same as the translation\u2019s. Although \u2018Equation 1\u2019 in the paper (Moreira et al., 2021a) is similar to our Equation (4), the authors (Moreira et al., 2021a) did not prove that the Frobenius-norm-based transformation difference loss function (Equation (4)) is a special case of the maximum likelihood loss function in the pose graph optimization, which is the motivation for this section.\nIn Equation (4), G(V ,E) is a connected pose graph with |V | = n poses (or vertices). The rigid transformation Tij (here, computed using a point cloud registration algorithm) from the it\u210e pose (denoted by Pi) to the jt\u210e pose (denoted by Pj) could be written as {R\u0303ij , t\u0303ij} for the edge (i, j) \u2208 E. E is the edge set. R\u0303ij and t\u0303ij are the corresponding relative rotation and translation estimates. The it\u210e pose Pi could be written as {Ri, ti}i=1,...,n. In the following, we will use blockmatrix notation to represent Equation (4).\nAs the transformation is rigid, thus: R\u0303ijR\u0303Tij = I, RiR T i = I, RjR T j = I. Let us write: Tij = [ R\u0303ij t\u0303ij 0 1 ] , Pi = [ Ri ti 0 1 ] , Pj = [ Rj tj 0 1 ] , P\u22121j = [ RTj \u2212R T j tj 0 1 ] ; Thus, Equation (4) can be transformed into:\nargmin \u2211\n(i,j)\u2208E\n\u2016 \u2016 \u2016 Tij \u2212 PiP\u22121j \u2016 \u2016 \u2016\n2\nF\n=argmin \u2211\n(i,j)\u2208E\n\u2016 \u2016 \u2016 \u2016 \u2016\n[ R\u0303ij \u2212 RiRTj t\u0303ij \u2212 ti + RiR T j tj\n0 0\n]\n\u2016 \u2016 \u2016 \u2016 \u2016\n2\nF\n=argmin \u2211\n(i,j)\u2208E (\u2016\u2016 \u2016 R\u0303ij \u2212 RiRTj \u2016 \u2016 \u2016\n2 F + \u2016\u2016 \u2016 t\u0303ij \u2212 ti + RiRTj tj \u2016 \u2016 \u2016 2 F )\n(C.1)\nAccording to the definition, Frobenius norm of a matrixA is defined as the square root of the sum of the absolute squares of its elements in thematrix, which is equal to the square root of the matrix trace of AAT . Additionally, tr(A) = tr(AT ). Expanding the term \u2016\u2016\n\u2016\nR\u0303ij \u2212 RiRTj \u2016 \u2016\n\u2016\n2 F in Equation (C.1):\n\u2016 \u2016 \u2016 R\u0303ij \u2212 RiRTj \u2016 \u2016 \u2016\n2\nF\n=tr{(R\u0303ij \u2212 RiRTj )(R\u0303ij \u2212 RiR T j ) T } =tr(R\u0303ijR\u0303Tij + RiR T j RjR T i \u2212 R\u0303ijRjR T i \u2212 RiR T j R\u0303 T ij) =tr(I + I) \u2212 tr(R\u0303ijRjRTi ) \u2212 tr{(R\u0303ijRjR T i ) T } =6 \u2212 2tr(R\u0303ijRjRTi ) (C.2)\nSubstitute the term \u2016\u2016 \u2016 R\u0303ij \u2212 RiRTj \u2016 \u2016 \u2016\n2 F in Equation (C.1) with\nEquation (C.2) and neglect the constant term:\nargmin \u2211\n(i,j)\u2208E\n\u2016 \u2016 \u2016 Tij \u2212 PiP\u22121j \u2016 \u2016 \u2016\n2\nF\n=argmin \u2211\n(i,j)\u2208E {\u2016\u2016 \u2016 t\u0303ij \u2212 ti + RiRTj tj \u2016 \u2016 \u2016\n2 F \u2212 2tr(R\u0303ijRjRTi )}\n(C.3)\nIn Equation (C.3) we minimize the loss function to get the estimated rotation and translation by maximizing its negative. Thus, divide the right part of the equal sign by the negative constant \u22122 2R, we get:\nargmin \u2211\n(i,j)\u2208E\n\u2016 \u2016 \u2016 Tij \u2212 PiP\u22121j \u2016 \u2016 \u2016\n2\nF\n=argmax{\u2212 1 2 2R \u2211 (i,j)\u2208E \u2016 \u2016 \u2016 t\u0303ij \u2212 ti + RiRTj tj \u2016 \u2016 \u2016 2 F\n+ 1 2R \u2211 (i,j)\u2208E tr(R\u0303ijRjRTi )}\n(C.4)\nCompare our term Equation (C.4) with the log-likelihood\nFirst Author et al.: Preprint submitted to Elsevier Page 23 of 32\nterm equation 4 in the paper (Moreira et al., 2021a), which is shown in the following Equation (C.5):\nlogL( |y)\n= \u2212 1 2 2t \u2211 (i,j)\u2208E \u2016 \u2016 \u2016 t\u0303ij \u2212 ti + RiRTj tj \u2016 \u2016 \u2016 2 F\n+ 1 2R \u2211 (i,j)\u2208E tr(R\u0303ijRjRTi )\n(C.5)\nWhen the noise level for the rotation and translation in the paper (Moreira et al., 2021a) is assumed to be equal (i.e. t = R), Equation (C.4) (in our paper) and Equation (C.5) (which is same with equation 4 in Moreira et al. (2021a)) are completely the same. Thus, we could use the optimization method17 in Moreira et al. (2021a) to optimize our error function. Finding the optimum rotation parameters first and then solving for the translation parameters turns it into a least-squares problem. To deduce the Equation (C.5), refer to Page 9 - 10 in (URL: https://drive.google.com/file/ d/1ML7mkLSIALm3x5DtID7ozHD3YL7S1iNC/view?usp=sharing) or the most related papers (Carlone et al., 2015a,b; Moreira et al., 2021a,b). To conclude, Equation (4) can be turned into a maximum likelihood estimation problem under the assumption of the proper noise level for rotation and translation. From another aspect, there is a more intuitive way to express the physical meaning of Equation (4). That is: estimate the pose of each node accurately, which in turn makes the existing relative pose measurements between different nodes closer to the post-calculated relative pose between different nodes based on their estimated global pose.\nC.2. Maximum Distance In the initial work Sdf-man (Pu et al., 2019), at the end of the refiner network (see Figure 2 on page 7 in Pu et al. (2019)) the method uses the function \u2018tanh\u2019 to output an intermediate map w and each value in w is in (\u22121, 1).\ninitial_disp = max_disp \u22c5 (w + 1) 2\n(C.6)\nThen it uses Equation (C.6) to convert the intermediate map w to the disparity map initial_disp. max_disp is the maximum disparity threshold. Converting the disparity map initial_disp into a depth map using Equation (2) gives Equation (C.7).\ninitial_dept\u210e = 2fb max_disp \u22c5 (w + 1)\n(C.7)\nf and b are the focal length value and baseline value of the stereo vision camera. As w ranges from -1 to 1, the initial_dept\u210e values will range from fbmax_disp to +\u221e.\nIn this paper, the difference is that we use a new Equation (C.8) to map the intermediate mapw to the new disparity map new_disp rather than Equation (C.6).\nnew_disp = 2fb max_dist \u22c5 (w + 1)\n(C.8)\n17The URL of the released code: https://github.com/gabmoreira/maks\nmax_dist is set as the maximum distance threshold of interest. Converting the new disparity map new_disp into the depth map format using Equation (2) gives Equation (C.9), whose value domain is (0, max_dist).\nnew_dept\u210e = max_dist \u22c5 (w + 1) 2\n(C.9)\nComparing the value domain of new_dept\u210e in Equation (C.9) and initial_dept\u210e Equation (C.7), the domain ( fbmax_disp ,+\u221e) of initial_dept\u210e is larger than the domain (0, max_dist) of new_dept\u210e considerably, though their definition domains are the same. Thus, as for noise with the same granularity in the input, new_dept\u210e from the proposed strategy will output a more robust and accurate result. By setting the maximum distance threshold of interest and the newmapping function Equation (C.8), we effectively narrow the value domain of the depth output to increase depth accuracy. This alternative approach has also been confirmed by the experiment results in Section 4.2."
        },
        {
            "heading": "Appendix D. More Details about Data Collection",
            "text": "As Figure 5 (b) shows, the trimming robot Trimbot18 navigates around the outdoor garden19 to collect the raw data. The top of the robot resembles a \u2018tower\u2019 (see Figure D.1), which consists of a prism retroreflector, a Velodyne VLP16, and the panoramic stereo camera (a ring of 5 stereo cameras).\nThe prism reflector on the robot is used to reflect the laser beam from Topcon PS Series Robotic Total Station\n18Trimbot\u2019s hardware was mainly developed by our Trimbot2020 consortium member Robert Bosch GmbH based on the Bosch Indigo lawn mower.\n19The garden was constructed by our Trimbot2020 consortium member Wageningen Research in Netherlands.\nFirst Author et al.: Preprint submitted to Elsevier Page 24 of 32\n(See Figure D.2), to estimate the robot\u2019s position in 3D space. According to the datasheet of the Topcon PS Series Robotic Total Station20, the distance measurement accuracy with the prism could be down to 1.5mm + 2ppm. To estimate the robot\u2019s orientation [roll, pitch, yaw], a STIM300 IMU sensor inside the trimming robot is used to record the acceleration and rotation rate measurements. According to the datasheet of STIM300 IMU sensor21, the gyroscope input range is \u00b1400deg\u2215sec and its angular random walk is 0.15deg\u2215 \u221a\n\u210er. The accelerometer range is \u00b110g and its velocity random walk is 0.06m\u2215s\u2215 \u221a\n\u210er. The in-run bias stability of the gyroscope and accelerometer is 0.5deg\u2215\u210er and, 0.05mg respectively. Based on the precise measurements from the STIM300 IMU sensor, the orientation of the robot is estimated in an offline post-processing step by strap-down integration. The 3D position [tx, ty, tz] from Topcon PS Series Robotic Total Station and the orientation (roll, pitc\u210e, yaw) are appended to constitute the 6-DoF pose of the robot. Then, by calibration, the 6-DoF pose of the robot is transformed to get the pose of each rigidly placed image sensor in the panoramic stereo camera. Finally, structure-from-motion (Schonberger and Frahm, 2016) is used to refine each image sensor\u2019s pose to form the ground truth pose of each image sensor, particularly to fix poses where the line of sight between the Topcon and prism was interrupted by obstacles.\nThe Velodyne VLP16 lidar sensor is mounted on top of the panoramic stereo camera to record a reference point cloud from the Trimbot robot\u2019s perspective. The lidar sensor has a 360\u25e6 horizontal field of view with an angle resolution spanning from 0.1\u25e6 to 0.4\u25e6, which corresponds to the rotation rate from 5 Hz to 20 Hz. In the Trimbot Wageningen SLAM Dataset, the angular resolution is set to 0.2\u25e6 and the rotation rate is set to 10Hz. The Velodyne VLP16 lidar\n20Topcon PS Series Robotic Total Station: https://drive.google.com/ file/d/1Z54jDM0fkqhNgYq1SBsZRFX-XG14alIC/view?usp=sharing\n21STIM300 IMU: https://drive.google.com/file/d/ 1PhFtSSABCs0msnu2Gwza0EhAg4mpUZar/view?usp=sharing\nsensor has 16 horizontal rays, which are distributed within a vertical field of view of \u00b115\u25e6. According to its datasheet22, its scanning range could be up to 100 m with an accuracy of \u00b13cm. Then the lidar point cloud is projected to the image planes of the 10 image sensors in the panoramic stereo camera to form sparse depth maps.\nThe panoramic stereo camera23 is built with tenMT9V024 CMOS image sensors fromONSemiconductors24. The housing of the panoramic stereo camera has a pentagon shape and is manufactured by 3D printing. Figure D.3 shows the\n22Velodyne VLP16: https://drive.google.com/file/d/ 1ZYrYHf7wqI5PjuuKpxO7SPzDfmb33b1Y/view?usp=sharing\n23The panoramic stereo camera hardware was developed by our Trimbot2020 consortium member ETH Z\u00fcrich based on their previous work (Honegger et al., 2017).\n24MT9V024 datasheet URL:https://github.com/ Canpu999/Trimbot-Wageningen-SLAM-Dataset/blob/main/\nImage-sensor-MT9V024-datasheet-ON_Semiconductor.pdf\nFirst Author et al.: Preprint submitted to Elsevier Page 25 of 32\npanoramic stereo camera from the top and side views. The image sensors have 752\u00d7480 (Horizontal\u00d7V ertical) pixels resolution with global shutter. The maximum frame rate of a single image sensor could be up to 60 FPS at full resolution. The five synchronized stereo vision cameras (10 image sensors) could only stream the synchronized panoramic stereo images (10 synchronized images) at 12 HZ because it is limited by the bandwidth of the panoramic stereo camera\u2019s data bus. The image sensor\u2019s operating temperature ranges from \u221240\u25e6C to +100\u25e6C ambient. The active imager size is 4.51mm(H) \u00d7 2.88mm(V ), whose diagonal size is 5.35 mm. The pixel size is 6.0 m \u00d7 6.0 m. We calibrated the image sensor using the Kalibr package25 by setting the camera model as pinhole and the distortion model as radialtangential. The calibration accuracy of image sensors in terms of a mean reprojection error was 0.00 pixels on the X and Y axis, with standard deviation ranging from 0.1 pixels to 0.2 pixels. Given that the image sensors\u2019 temperature and long operating time influences the robustness of the commercial hardware\u2019s imaging quality, which will possibly influence the calibration accuracy, more engineering robustness tests would be future work.\nThe 3D point cloud of the whole garden was collected by a Leica ScanStation P1526. The Leica ScanStation P15 measures the distance by a laser scanner and incorporates the corresponding RGB data from a color camera into the laser scanner\u2019s coordinate system. The laser scanner\u2019s working distance range varies from [0.4m, 40m]. The 3D position accuracy could be low to 3 mm at 40 meters and the linearity error is smaller than 1 mm. The angular accuracy is 8\u201d in the horizontal direction and 8\u201d in the vertical direction. Figure D.4 shows the garden\u2019s point clouds from different views with RGB data and colored height (different colors represent different heights). As the distance measurement and the RGB measurement are conducted sequentially, both measurements are not exactly synchronized. As a consequence, there may be wrong RGB values on moving objects\u2019 point clouds. Amoving leaf\u2019s point cloud (taken at one timestamp) will show the sky\u2019s color (taken at another timestamp) if the leaf moved at one timestamp (when the laser scanner was acquiring data) to another place at another timestamp (when the RGB sensor was acquiring data). Reflectance properties of glossy leaves can also result in color changes. Thus, in Figure D.4, the tree\u2019s point clouds show amixed color (green and sky-white). That is the reason why we did not include the point cloud with RGB data into the TrimbotWageningen SLAM Dataset."
        },
        {
            "heading": "Appendix E. Additional Qualitative Results",
            "text": "E.1. Robustness to Environment Change\nAs sunlight and the scene appearance outdoors will influence an image\u2019s appearance directly, thus we vary sunlight from different time periods and scene appearance from\n25https://github.com/ethz-asl/kalibr 26Leica ScanStation P15 Datasheet URL: https://github.com/\nCanpu999/Trimbot-Wageningen-SLAM-Dataset/blob/main/Leica_ScanStation_\nP15_datasheet.pdf\ndifferent places in the following experiment to demonstrate the robustness of the proposed framework. Figure E.1 shows the experimental setting for the robustness test. In Scene 1, the robot drove straight along route A under sunlight A. In Scene 2, the robot drove straightly along route B (whose travelled distance is same with that of route A) under the same sunlight A. In Scene 3, the robot drove straightly along the route A, but under the different sunlight B. Since factors other than scene appearance and lighting should not influence the image appearance directly, we classify the other factors as Uncontrolled Random Factors. Table E.1 gives the precise value of each factor, and the values of the uncontrolled random factors are from a weather website 27.\nFigure E.2 shows the three scenes and the related 3D reconstruction results in surface mesh format by the proposed framework. As acquiring ground truth requires considerable expense and effort by a big team, we only show the corresponding qualitative results. Compare the shadows, trees, posts and bushes in the example images (Figure E.2 a c e) with the corresponding shapes in the reconstructed 3D meshes (Figure E.2 b d f). The high quality and believable shapes, even with different lighting and viewpoint, demonstrate the high quality of the 3D reconstruction by our proposed framework. Further robustness tests against other factors (e.g. temperature, wind speed, long operating time, humidity, foggy or snowy weather) are future work.\nE.2. Fair Comparison with More Open-source Frameworks\nOrbslam3 (Campos et al., 2021) and Open3D (Zhou et al., 2018) needed hints in Section 4.3.2 to make them work. In this subsection, we will compare all the frameworks when using only one stereo vision camera (which consists of the image sensor Cam0 and Cam1) without any hints provided to any framework. We use the initial parameter setting of Open3D for a test (denoted by Open3D-initial). For Orbslam3, we test their RGBD SLAM algorithm (denoted by Orbslam3-rgbd), their stereo SLAM algorithm (denoted by Orbslam3-stereo), and their monocular SLAM algorithm (denoted by Orbslam3-monocular). For the parameter setting of each SLAM algorithm fromOrbslam3, please refer to the video at https://youtu.be/4luADtHNbuA. In this video28, we also show the performance of each algorithm from Orbslam3 on our Trimbot Wageningen SLAM dataset when having a random test29.\nFigure E.3 shows the global pose trajectory of each framework. The results of Orbslam3 are from the best trials among tens of random trials. Method \"Ours\" is the method proposed in this paper with panoramic stereo images as\n27https://tcktcktck.org/netherlands/gelderland/wageningen 28There are many false extracted features near the edge of the robot in the above video. We refined the data input of Orbslam3 by masking more areas near the robot and tuned the parameters to maximize Orbslam3\u2019s performance. The modified video (URL: https://youtu.be/9TaayXl4mJ4) shows the corresponding performance.\n29Due to the stochastic property of the algorithm RANSAC (Fischler and Bolles, 1981) in the framework Orbslam3, the results from Orbslam3 are different each time.\nFirst Author et al.: Preprint submitted to Elsevier Page 26 of 32\ninput. Method \"Ours-single-stereo\" denotes a method which is derived from the \"Ours\" method, but with the input from only one stereo camera. Figure E.3 shows that all of the algorithms sucessfully estimate poses initially, but eventually fail, except \"Ours\" when traversing the whole\ndataset. \"Ours-single-stereo\" seems to work better compared with the external counterparts, although it still fails at the latter part of the global pose trajectory. The trajectory of Orbslam3-monocular (cyan trajectory) has only one dot to\nFirst Author et al.: Preprint submitted to Elsevier Page 27 of 32\nLegend\nsunlight A\nroute A\nstart point A\nsunlight B\nroute B\nstart point B\nFigure E.1: The experiment setting in the robustness test\nTable E.1 Scene Definition with Different Condition Setting.\nControlled Variable Uncontrolled Random Factors\nName route sunlight time date temperature wind speed humidity dew point pressure\nScene 1 A A \u223c 15 \u2236 30 2017-05-17 22.5\u25e6C 12.7 km/h 76% 15.7\u25e6C 995.6Mb Scene 2 B A \u223c 15 \u2236 30 2017-05-17 22.5\u25e6C 12.7 km/h 76% 15.7\u25e6C 995.6Mb Scene 3 A B \u223c 11 \u2236 00 2018-06-27 20.5\u25e6C 12.7 km/h 75% 12.7\u25e6C 1003.4Mb\nshow at the starting point, near the coordinate (0 m, - 1 m ), because Orbslam3-monocular almost lost tracking every frame. Given the bad performance of each external algorithm, it is meaningless to calculate the corresponding quantitative results. It is also the reason why we needed to provide \"some extra help\" to the external counterparts to get results to compare with in Section 4.3.2.\nE.3. Comparison with Software \u2018ContextCapture\u2019 Above, we compared our proposed framework with the latest popular frameworks Orbslam3 (Campos et al., 2021) and Open3D (Zhou et al., 2018). The experimental results have shown our framework\u2019s performance superiority over the two open-source frameworks. Here, we compare the proposed framework with the commercial software called\n\"ContextCapture Center\" from Bentley30, which is known for its image-based 3D reconstruction and aerial photogrammetry. Because the commercial software ContextCapture\u2019s 3D reconstruction framework does not support 360\u25e6 imagebased 3D reconstruction, we could only input the RGB images from our image sensor Cam0 in ourWageningen SLAM Dataset. Figure E.4 shows the reconstructed meshes from two views. From the strange reconstruction results, we could easily see that the ContextCapture reconstruction system failed at our task. The reason is that the real robot\u2019s fast movement and large transformation between frames when navigating in the real outdoor garden lead ContextCapture to lose tracking (please read the acquisition report31 from\n30https://www.bentley.com/software/contextcapture/ 31ContextCapture\u2019s acquisition report: https://drive.google.com/file/\nd/1k_oUyolomh2LmJv3ATyrUnxA5sc--bxE/view?usp=sharing\nFirst Author et al.: Preprint submitted to Elsevier Page 28 of 32\n(a) Scene 1 (b) Reconstructed Model 1\n(c) Scene 2 (d) Reconstructed Model 2\n(e) Scene 3 (f) Reconstructed Model 3\nFigure E.2: Figure (a) (c) (e) shows the scenes and Figure (b) (d) (f) shows the reconstructed models.\nContextCapture). The quality report32 from the software ContextCapture reveals more reconstruction details about its failure."
        }
    ],
    "title": "A Multi-modal Garden Dataset and Hybrid 3D Dense Reconstruction Framework Based on Panoramic Stereo Images for a Trimming Robot",
    "year": 2023
}