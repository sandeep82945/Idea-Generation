{
    "abstractText": "Due to the immense potential of quantum computers and the significant computing overhead required in machine learning applications, the variational quantum classifier (VQC) has received a lot of interest recently for image classification. The performance of VQC is jeopardized by the noise in Noisy Intermediate-Scale Quantum (NISQ) computers, which is a significant hurdle. It is crucial to remember that large error rates occur in quantum algorithms due to quantum decoherence and imprecision of quantum gates. Previous studies have looked towards using ensemble learning in conventional computing to reduce quantum noise. We also point out that the simple average aggregation in classical ensemble learning may not work well for NISQ computers due to the unbalanced confidence distribution in VQC. Therefore, in this study, we suggest that ensemble quantum classifiers be optimized with plurality voting. On the MNIST dataset and IBM quantum computers, experiments are carried out. The results show that the suggested method can outperform state-of-the-art on twoand four-class classifications by up to 16.0% and 6.1% , respectively.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ruiyang Qin"
        },
        {
            "affiliations": [],
            "name": "Zhiding Liang"
        },
        {
            "affiliations": [],
            "name": "Jinglei Cheng"
        },
        {
            "affiliations": [],
            "name": "Peter Kogge"
        },
        {
            "affiliations": [],
            "name": "Yiyu Shi"
        }
    ],
    "id": "SP:62a505b55b94da95e2068dac126fba94522bc469",
    "references": [
        {
            "authors": [
                "T. Young",
                "D. Hazarika",
                "S. Poria",
                "E. Cambria"
            ],
            "title": "Recent trends in deep learning based natural language processing",
            "venue": "ieee Computational intelligenCe magazine, vol. 13, no. 3, pp. 55\u201375, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "H. Sak",
                "A.W. Senior",
                "F. Beaufays"
            ],
            "title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling",
            "venue": "2014.",
            "year": 2014
        },
        {
            "authors": [
                "R. Qin",
                "H. Luo",
                "Z. Fan",
                "Z. Ren"
            ],
            "title": "Ibert: Idiom cloze-style reading comprehension with attention",
            "venue": "arXiv preprint arXiv:2112.02994, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770\u2013778.",
            "year": 2016
        },
        {
            "authors": [
                "H. Luo",
                "R. Qin"
            ],
            "title": "Open-ended multi-modal relational reason for video question answering",
            "venue": "arXiv preprint arXiv:2012.00822, 2020.",
            "year": 2012
        },
        {
            "authors": [
                "O. Oktay",
                "J. Schlemper",
                "L.L. Folgoc",
                "M. Lee",
                "M. Heinrich",
                "K. Misawa",
                "K. Mori",
                "S. McDonagh",
                "N.Y. Hammerla",
                "B. Kainz"
            ],
            "title": "Attention u-net: Learning where to look for the pancreas",
            "venue": "arXiv preprint arXiv:1804.03999, 2018.",
            "year": 1804
        },
        {
            "authors": [
                "V. Havl\u0131\u0301\u010dek",
                "A.D. C\u00f3rcoles",
                "K. Temme",
                "A.W. Harrow",
                "A. Kandala",
                "J.M. Chow",
                "J.M. Gambetta"
            ],
            "title": "Supervised learning with quantumenhanced feature spaces",
            "venue": "Nature, vol. 567, no. 7747, pp. 209\u2013212, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "W. Jiang",
                "J. Xiong",
                "Y. Shi"
            ],
            "title": "A co-design framework of neural networks and quantum circuits towards quantum advantage",
            "venue": "Nature communications, vol. 12, no. 1, pp. 1\u201313, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S.Y.-C. Chen",
                "C.-H.H. Yang",
                "J. Qi",
                "P.-Y. Chen",
                "X. Ma",
                "H.-S. Goan"
            ],
            "title": "Variational quantum circuits for deep reinforcement learning",
            "venue": "IEEE Access, vol. 8, pp. 141 007\u2013141 024, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "A. Zlokapa",
                "A. Gheorghiu"
            ],
            "title": "A deep learning model for noise prediction on near-term quantum devices",
            "venue": "arXiv preprint arXiv:2005.10811, 2020.",
            "year": 2005
        },
        {
            "authors": [
                "H. Wang",
                "Y. Ding",
                "J. Gu",
                "Y. Lin",
                "D.Z. Pan",
                "F.T. Chong",
                "S. Han"
            ],
            "title": "Quantumnas: Noise-adaptive search for robust quantum circuits",
            "venue": "2022 IEEE International Symposium on High-Performance Computer Architecture (HPCA). IEEE, 2022, pp. 692\u2013708.",
            "year": 2022
        },
        {
            "authors": [
                "H. Wang",
                "J. Gu",
                "Y. Ding",
                "Z. Li",
                "F.T. Chong",
                "D.Z. Pan",
                "S. Han"
            ],
            "title": "Quantumnat: quantum noise-aware training with noise injection, quantization and normalization",
            "venue": "Proceedings of the 59th ACM/IEEE Design Automation Conference, 2022, pp. 1\u20136.",
            "year": 2022
        },
        {
            "authors": [
                "Z. Liang",
                "Z. Wang",
                "J. Yang",
                "L. Yang",
                "Y. Shi",
                "W. Jiang"
            ],
            "title": "Can noise on qubits be learned in quantum neural network? a case study on quantumflow",
            "venue": "2021 IEEE/ACM International Conference On Computer Aided Design (ICCAD). IEEE, 2021, pp. 1\u20137.",
            "year": 2021
        },
        {
            "authors": [
                "Z. Liang",
                "J. Cheng",
                "H. Ren",
                "H. Wang",
                "F. Hua",
                "Y. Ding",
                "F. Chong",
                "S. Han",
                "Y. Shi",
                "X. Qian"
            ],
            "title": "Pan: Pulse ansatz on nisq machines",
            "venue": "arXiv preprint arXiv:2208.01215, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Z. Liang",
                "H. Wang",
                "J. Cheng",
                "Y. Ding",
                "H. Ren",
                "X. Qian",
                "S. Han",
                "W. Jiang",
                "Y. Shi"
            ],
            "title": "Variational quantum pulse learning",
            "venue": "arXiv preprint arXiv:2203.17267, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J. Cheng",
                "H. Deng",
                "X. Qia"
            ],
            "title": "Accqoc: Accelerating quantum optimal control based pulse generation",
            "venue": "2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA). IEEE, 2020, pp. 543\u2013555.",
            "year": 2020
        },
        {
            "authors": [
                "A. Wu",
                "G. Li",
                "Y. Ding",
                "Y. Xie"
            ],
            "title": "Mitigating noise-induced gradient vanishing in variational quantum algorithm training",
            "venue": "arXiv preprint arXiv:2111.13209, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "L. Gentini",
                "A. Cuccoli",
                "S. Pirandola",
                "P. Verrucchi",
                "L. Banchi"
            ],
            "title": "Noiseresilient variational hybrid quantum-classical optimization",
            "venue": "Physical Review A, vol. 102, no. 5, p. 052414, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "R. LaRose",
                "B. Coyle"
            ],
            "title": "Robust data encodings for quantum classifiers",
            "venue": "Physical Review A, vol. 102, no. 3, p. 032420, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "S.M. Pillay",
                "I. Sinayskiy",
                "E. Jembere",
                "F. Petruccione"
            ],
            "title": "Implementing quantum-kernel-based classifiers in the nisq era",
            "venue": "Southern African Conference for Artificial Intelligence Research. Springer, 2021, pp. 257\u2013273.",
            "year": 2021
        },
        {
            "authors": [
                "D. Silver",
                "T. Patel",
                "D. Tiwari"
            ],
            "title": "Quilt: Effective multi-class classification on quantum computers using an ensemble of diverse quantum classifiers",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, vol. 36, no. 8, 2022, pp. 8324\u20138332.",
            "year": 2022
        },
        {
            "authors": [
                "G. Li",
                "Z. Song",
                "X. Wang"
            ],
            "title": "Vsql: Variational shadow quantum learning for classification",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, vol. 35, no. 9, 2021, pp. 8357\u20138365.",
            "year": 2021
        },
        {
            "authors": [
                "D. Maheshwari",
                "D. Sierra-Sosa",
                "B. Garcia-Zapirain"
            ],
            "title": "Variational quantum classifier for binary classification: Real vs synthetic dataset",
            "venue": "IEEE Access, vol. 10, pp. 3705\u20133715, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Z. Liang",
                "Z. Song",
                "J. Cheng",
                "Z. He",
                "J. Liu",
                "H. Wang",
                "R. Qin",
                "Y. Wang",
                "S. Han",
                "X. Qian"
            ],
            "title": "Hybrid gate-pulse model for variational quantum algorithms",
            "venue": "arXiv preprint arXiv:2212.00661, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "M. Schuld",
                "F. Petruccione"
            ],
            "title": "Quantum ensembles of quantum classifiers",
            "venue": "Scientific reports, vol. 8, no. 1, pp. 1\u201312, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "A. Macaluso",
                "L. Clissa",
                "S. Lodi",
                "C. Sartori"
            ],
            "title": "Quantum ensemble for classification",
            "venue": "arXiv preprint arXiv:2007.01028, 2020.",
            "year": 2007
        },
        {
            "authors": [
                "M.A. Ganaie",
                "M. Hu"
            ],
            "title": "Ensemble deep learning: A review",
            "venue": "arXiv preprint arXiv:2104.02395, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "C. Chen",
                "D. Dong",
                "B. Qi",
                "I.R. Petersen",
                "H. Rabitz"
            ],
            "title": "Quantum ensemble classification: A sampling-based learning control approach",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 28, no. 6, pp. 1345\u20131359, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "I.C. Araujo",
                "A.J. Da Silva"
            ],
            "title": "Quantum ensemble of trained classifiers",
            "venue": "2020 International Joint Conference on Neural Networks (IJCNN). IEEE, 2020, pp. 1\u20138.",
            "year": 2020
        },
        {
            "authors": [
                "E. Grant",
                "M. Benedetti",
                "S. Cao",
                "A. Hallam",
                "J. Lockhart",
                "V. Stojevic",
                "A.G. Green",
                "S. Severini"
            ],
            "title": "Hierarchical quantum classifiers",
            "venue": "npj Quantum Information, vol. 4, no. 1, pp. 1\u20138, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "S. Resch",
                "U.R. Karpuzcu"
            ],
            "title": "Benchmarking quantum computers and the impact of quantum noise",
            "venue": "ACM Computing Surveys (CSUR), vol. 54, no. 7, pp. 1\u201335, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A. Kandala",
                "A. Mezzacapo",
                "K. Temme",
                "M. Takita",
                "M. Brink",
                "J.M. Chow",
                "J.M. Gambetta"
            ],
            "title": "Hardware-efficient variational quantum eigensolver for small molecules and quantum magnets",
            "venue": "Nature, vol. 549, no. 7671, pp. 242\u2013246, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "B. Romera-Paredes",
                "M.S. Aung",
                "N. Bianchi-Berthouze"
            ],
            "title": "A one-vsone classifier ensemble with majority voting for activity recognition",
            "venue": "2013.",
            "year": 2013
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014Quantum Machine Learning, Variational Quantum Circuits, Quantum Ensemble Learning, Noise Mitigation\nI. INTRODUCTION\nIn the past decade, machine learning models have achieved consistent success in many practical applications such as natural language processing [1]\u2013[3], image classification [4], [5], and medical diagnosis [6]. However, as dataset size and model complexity increase, computation power in classical computing hardware has become the bottleneck. To alleviate the problem, various efforts have been made to leverage the power of quantum computers to speed up machine learning tasks, which have opend a research area known as quantum machine learning (QML). One of the most popular QML models is Variational Quantum Classifier (VQC) [7], which deploys parameterized quantum circuits and trains them on the target classification tasks [8], [9].\nNevertheless, despite the great potentials of quantum computing, current Noisy Intermediate-Scale Quantum (NISQ) computers are extremely sensitive to their surrounding environments and risk losing their quantum state due to quantum decoherence, which contributes to high quantum noise. And it is theoretically impossible to accurately predict or eliminate quantum noise [10] in NISQ computers. As a result, numerous studies have been done in the literature to reduce the noise by constructing noise-resistant circuits [11]\u2013[13] or control-\nling lower-level quantum hardware [14]\u2013[16]. Many of these methods are task-specific, i.e., they are developed with the implementation of certain quantum algorithms in mind.\nMore specifically, for VQC, current noise mitigation techniques include mitigating noise-induced gradient vanishment [17], adopting hybrid optimization [18], pre-processing data [19], and developing kernel matrix [20]. Recently, QUILT [21] was proposed for VQC tasks. It was inspired by the idea of ensemble learning, where a mixture of various models make a prediction collectively for greater accuracy. To generate the final prediction, it deploys several VQCs and averages their output confidence, which is defined as the possibilities\nar X\niv :2\n21 0.\n01 65\n6v 3\n[ qu\nan t-\nph ]\n1 7\nD ec\n2 02\nthat the prediction being correct. Although quantum noise is unpredictable, as will be shown in Section II, we note that the impact of wrong predictions is typically much more than that of correct predictions. As a result, utilizing averaged confidence may result in incorrect prediction.\nIn this paper, we present EQV, an Ensemble Quantum classifiers with plurality Voting, to address this issue. As illustrated in Fig. 1, EQV deploys numerous VQCs onto multiple quantum computers and integrates the outcoming results through plurality voting to produce the final prediction. If the number of quantum computers producing accurate predictions predominate, such an approach can successfully eliminate inaccurate predictions, even if they have very low confidence numbers. Experimental results on real-world quantum computers with MNIST dataset demonstrate that EQV can increase the accuracy by 16.0% and 6.1% over the state-ofthe-art method for two-class and four-class classification tasks respectively.\nThe rest of the paper is structured as follows. Section II provides background information and introduces the motivation for our work. The detailed framework of quantum ensemble learning for VQC is articulated in Section III. Section IV contains the experimental results as well as the concluding remarks."
        },
        {
            "heading": "II. BACKGROUND AND MOTIVATION",
            "text": ""
        },
        {
            "heading": "A. Background",
            "text": "The quantum bit (qubit) is the fundamental building block of quantum information. Similar to how a conventional bit may store either a 0 or 1, a qubit can also be used to store information. The states of a qubit can be represented by two vectors: |0\u3009 and |1\u3009. The linear combination of these two state vectors is superposition, which can be represented as\n|\u03c8\u3009 = \u03b1 |0\u3009+ \u03b2 |1\u3009 , (1)\nwhere \u03b1 and \u03b2 should satisfy |\u03b1|2 + |\u03b2|2 = 1. Besides superposition, qubits can also be entangled, which is impossible for classical bits. Using two-qubit gates such as CNOT gates to connect two qubits is a typical method for entangling qubits.\nA quantum circuit is a collection of various quantum gates that can perform quantum operations efficiently. Parameterized quantum circuit is a type of quantum circuit that can be parameterized to enable trainability by changing the angles of the rotation gates. We are able to obtain results that are analogous to those obtained from training a classical neural network if we design and train the variational quantum classifier. For instance, variational quantum classifiers (VQC) are adequate for solving image classification problems in an effective manner [22], [23].\nThe leading quantum computers in the NISQ era contain around one hundred qubits, but they are not advanced enough to achieve fault-tolerance nor large enough to demonstrate quantum supremacy on practical problems. The performance of these computers is restricted by their high quantum noise.\nTherefore, powerful QML applications such as quantum neural network [8], [24], which is believed to better simulate human neurons than classical neural networks, cannot be implemented."
        },
        {
            "heading": "B. Motivation and Related Work",
            "text": "The idea of quantum ensemble learning was brought up in 2017, where the ensemble corresponds to state preparation routines, and the quantum classifiers can be evaluated in parallel [25]. Schuld et al theoretically proves the feasibility of quantum ensemble learning. In another work, Macaluso et al [26] propose a quantum algorithm which takes the advantages of quantum superposition, entanglement and interference to build an ensemble classification. Macaluso et al [26] achieves quantum ensemble learning by using the bagging strategy [27], and it mainly discusses how to apply classical ensemble learning methods to quantum computing. Chen et al [28] combines quantum ensemble learning with supervised learning by recasting quantum ensemble classification as a supervised quantum learning problem, and using a sampling-based learning control to present quantum discrimination. There are also researchers working on building an exponentially larger ensemble size of classifiers, aiming to explore the scalability problem [29]. In hierarchical quantum classifiers [30], researchers use the combination of different quantum binary classifiers to complete classification tasks.\nMost recently, QUILT [21] deploys five core classifiers and N binary classifiers for N-class classification tasks. In QUILT, accuracy-based weights are used to aggregate outputs from core classifiers. The binary classifiers in QUILT are One-VsAll classifiers, which are trained to discriminate one class from other classes. In QUILT, binary classifiers are used to tell apart two outputs with lowest confidence numbers. For example, if the lowest confidence classes are one and five, the binary classifier will be adopted to make the final decision.\nWe conduct a toy experiment and present its results in Fig. 2 to demonstrate the distribution of \u201cimpact numbers\u201d for wrong and correct prediction. For an input image, two numbers are generated by multiple models to determine the\nclass of the input image. By comparing these two numbers, the larger numbers indicate the classes predicted by these models. To evaluate how the numbers will take effect in the average, we define an \u201cimpact factor\u201d for predictions. For predictions, the factor is defined as the difference between two confidence numbers generated by the models. When the confidence number are averaged in ensemble learning models, the factor number will determine the \u201cweight\u201d of this model. The larger the factor number is, the more impact this single model will have on the ensemble learning model. And in the experiment, we find out that if the predictions are wrong, as shown in Fig. 2, their impact factors are generally larger than the impact factors of correct predictions. For example, we have three binary classification models and each model will take an image of digit of 1 as its input. The model 1 generates confidence numbers of class 1 and class 0 as {0.1, 0.9}, the model 2 generates confidence numbers of class 1 and class 0 as {0.6, 0.4}, and the model 3 generates confidence numbers of class 1 and class 0 as {0.55, 0.45}. So the impact factors are 0.8 for model 1, 0.2 for model 2, and 0.1 for model 3. Model 2 and model 3 should generate the correct classification results. However, through simple averaging method, the ensembled model has confidence of 1 as 0.417 and confidence of 0 as 0.58. Therefore, the ensembled model will choose digit 0 as its final prediction class. Due to the large impact factor, model 1 governs the ensemble model\u2019s prediction. On the contrary, since we know the predictions of the three models as 0, 1, 1, we can make the final prediction as digit 1 because more models \u201cchoose\u201d digit 1 as the results.\nAs shown in Table I, different quantum computers have different error rates. The impact of quantum noise on quantum computers are also difficult to predict [31]. Running all quantum classifiers on the same quantum computer may lead to low performance if this quantum computer receives high quantum noise. To eliminate this bias introduced by quantum noise, we propose to run experiments on different quantum computers. We propose a quantum ensemble model in this paper that trains quantum classifiers across multiple quantum computers. Instead of simply averaging the outputs of these classifiers, we generate the final output using a voting strategy called plurality voting. As a result, our work avoids over-reliance on a single quantum computer and mitigates the biased impact of low-accuracy quantum classifiers."
        },
        {
            "heading": "III. ARCHITECTURE: EQV",
            "text": ""
        },
        {
            "heading": "A. Overview",
            "text": "As shown in Fig. 1, EQV uses many variational quantum classifiers (VQCs) to train on the same task. EQV makes copies of VQCs and deploys them onto several quantum computers. Each VQC will generate its own predictions. And the confidence of all VQCs\u2019 outputs are different. In this work, we use intermediate confidence to represent each VQC\u2019s output. As shown in Fig. 1, our voting strategy collects all intermediate confidence and generates the final output. Instead of averaging all intermediate confidence and adding it up for\nvoting, it uses a plurality voting approach to combine all inputs (intermediate confidence).\nWe show the behaviors of EQV using the two-class classification as an example. In this illustration, we use EQV to categorize the digits one and zero. Assuming there are five two-qubit quantum circuit\u2013based classifiers for quantum systems. The five quantum classifiers will provide their predictions given an image. Three quantum classifiers indicate the given image being a digit of one if they individually generate the confidence numbers as 0.57, 0.63, 0.38, 0.27, and 0.61. The digit one will be chosen as the final result based on plurality voting. But for QUILT aggregation technique, we will have the averaged confidence number of 0.492, resulting in the wrong prediction. In this case, voting is capable of making a reliable prediction without being affected by the two low-confidence outcomes (0.38 and 0.27)."
        },
        {
            "heading": "B. Ensemble the Outputs: Plurality Voting",
            "text": "Voting is a way to aggregate outputs from various VQCs. In a classification task, a classifier predicts a class label from a set of class labels. For example, a classifier predicts the input image has 0.58 as confidence number to be label1, then the predicted label of this image is label1. In voting, this prediction represents one classifier\u2019s vote. In the final output, the class label that obtains the most votes represents the final prediction. This voting strategy is called plurality voting. The final prediction does not need to receive more than 50% votes. For instance, there are three classifiers for classifying data into two classes (class 1 and class 2). Upon receiving an input, classifiers output confidence on class 1 (correct class): {0.6, 0.55, 0.1} and output confidence on class 2: {0.4, 0.45, 0.9}. Class 1 receives the most votes in the three classes. Thus, EQV makes accurate predictions. Again, the average level of confidence for class 1 is 0.4167, while for class 2 it is 0.5833. Using average as the aggregate method, the final prediction will be class 2, which is incorrect.\nIn contrast to the suggested plurality voting, existing quantum ensemble learning methods, as explained in Section II, use average aggregation, which adds all outputs and computes the average of the sum. This is similar to how classical machine learning works, where an ensemble output often averages the outcomes of numerous models trained to accomplish the same task. Because some properties may be captured better by one model than by another, the final result should be superior to any single output. However, quantum noise causes certain outputs to perform far worse than others. In QML, average output aggregation may reduce confidence.\nWe present an example below to demonstrate plurality voting. There are four labels to choose from. The correct label for a given image is c1. In this case, there are six incorrect predictions. When employing average aggregation, the final prediction may be incorrect since incorrect predictions have a bigger impact on the average number. Our voting strategy, on\nthe other hand, can yield the correct prediction.\nlabel = {c1, c3, c6, c9} dataset = {imgi, i = 1 \u223c 300}\nimgi : {c1 \u2217 3, c3 \u2217 2, c9 \u2217 2, c6 \u2217 2} V oting(imgi)\u21d2 c1\n(2)\nThis example demonstrates that even if more than half of quantum classifiers make incorrect predictions due to quantum noise, voting can still produce the correct prediction."
        },
        {
            "heading": "IV. EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "A. Experimental Setup",
            "text": "We undertake two experiments to evaluate EQV\u2019s performance: a two-class classification experiment and a fourclass classification experiment. The training set for two-class classification consists of 300 MNIST-2 images, and the test set consists of 30 MNIST-2 images. The two classes are made up of digits one and nine. The training set for four-class classification consists of 300 MNIST-4 images, and the test set consists of 30 MNIST-4 images. The four categories are as follows: digits one, four, seven and nine. A batch size of 100 is used. Three training subsets are established in this arrangement. We construct all quantum classifiers based on the quantum circuit shown as Fig. 3. We position rotating gates at the beginning of the quantum circuit. CNOT gates are put after the leftmost rotation gates, and more rotation gates may follow CNOT gates. If their CNOT gates or rotation gates are arranged differently, two quantum circuits with the same number of qubits can be considered variations of one another. However, a quantum circuit with three qubits cannot be a variation of a circuit with two qubits. Likewise, a quantum circuit lacking CNOT gates or rotation gates cannot be called a version of a quantum circuit containing both type gates. In Fig. 3, The block enclosed by the grey edges is known as an entangled unitary. No matter how many qubits a quantum circuit has, an entangled unitary should have rotation gates and CNOT gates entangle all qubits. To change an entangled unitary, we can position CNOT gates in different positions, thus entangling distinct qubits. (entangling qubit1 & qubit2 \u2192 entangling qubit1 & qubit3). For dataset, we split it into many batches. For example, if the batch size is 128, then 128 images will be convert into tensors which will be considered as an input. By reshaping, the input will be embedded into rotation gates\nAs shown in Fig. 1, Different variants are bundled together and deployed on a single quantum computer. However, the same variants will be deployed to different quantum computers. If three variants are constructed, three VQCs will be performed successively in a quantum computer. These variants\u2019 copies will be transferred to a different quantum computer. For example, if we have four variants, each with two copies, we will send four sets of VQCs to three separate quantum computers.\nWe run the experiments on five IBM quantum computers, ibmq lima, ibmq quito, ibmq belem, ibmq nairobi\nand imbq oslo, with their properties shown in Table I. Since our quantum circuits consist of four qubits, we do the majority of our work on quantum computers with five qubits. We also present tests on two quantum computers with seven qubits.\nVoting is used during the testing process. The test set is distributed to each VQC, and the outputs of all VQCs are gathered and forwarded to the voting system. Using a plurality voting approach, the final outcome is determined. Together, the final output and test set labels can be used to calculate the accuracy."
        },
        {
            "heading": "B. Experimental Results",
            "text": "The quantum circuits used in the two experiments mentioned in the previous section should have the same number of qubits. Another experiment is conducted to compare the performance of quantum classifiers with varying numbers of qubits. As shown in Table II, We use VQCs with two, four, and six qubits to conduct two-class classifications. A 2-qubit VQC, for example, has two entangled qubits, one CNOT gate followed by two rotation gates for each qubit. It can be viewed as a \u201chalf\u201d of the ansatz depicted in Fig. 3.\nWe use five quantum computers shown in Table II, the first three are five-qubit quantum computers, and the last two are seven-qubit. In two-qubit setting, the three five-qubit quantum computers are slightly better than the rest two. They are have around 65% of accuracy. In four-qubit setting, the five\nquantum computers in general have around 80% of accuracy, which is much higher than that in two-qubit setting. Only ibm oslo and ibm nairobi can perform six-qubit setting of experiment, their performance in average is slightly better than that in four-qubit setting.\nWhat we find from this experiment: \u2022 For classification tasks, the VQC with more qubits might\nachieve higher performance. \u2022 Four qubits can be used to handle two-class classification\nwith a comparatively high accuracy. EQV is evaluated on two-class and four-class classification. In both experiments, we use four-qubit VQCs. For each experiment, we set five different ensemble size for EQV. Ensemble size in our work can be seen as the number of VQCs. We evaluate EQV in ensemble size of 3, 5, 7, 9, and 11. For ensemble size of 3, there are three variants and they all have no copies. For ensemble size of 5, three variants are created, and randomly pick two of three to make one copy of each. For ensemble of 7, three variants are created as well, two of them will have one copy, and one of them will have two copies. Similar rules apply to ensemble size of 9 and 11.\nWe perform EQV based on three IBM quantum computers: ibmq lima, ibmq belem, and ibmq quito. As shown in Fig. 4, in two-class classification, ensemble size of 7 has the highest accuracy (87%). In four-class classification, ensemble size of\n11 has the highest accuracy (45%). We use ensemble size of 7 for two-class classification and ensemble size of 11 for fourclass classification to run single VQC experiment (One VQC on one quantum computer without ensemble and voting).\nThe performance of EQV shown in Fig. 5, where two-class classification has accuracy of 87% and four-class classification has accuracy of 45%. As Table III shown, while the performance of our work in real quantum computer is lower than simulation performance (0.91 for two-class classification and 0.71 for four-class classification), the performance of our work is still higher than QUILT [21] and One-Vs-One [33].\nAs shown in Table III, We compare the performance of EQV to that of the same ansatz on a single quantum computer. If we just run the model on the same quantum computers, the performance in two-class and four-class classification is no better than EQV, which runs the model on three quantum computers. However, due to quantum noise, the performance of EQV is worse than that of simulation. Furthermore, the accuracy drop from simulation to EQV is greater in four-class classification than in two-class classification. It demonstrates that the more qubits involved, the greater the probability of being influenced by quantum noise.\nIn two-class classification tasks shown as Fig. 5, the performance of our work (87%) is better than QUILT (71%)\nand better than One-Vs-One (28%). In four-class classification tasks shown as Fig. 5, the performance of our work (45.1%) is better than QUILT (39%) and better than One-Vs-One (21%)."
        },
        {
            "heading": "V. CONCLUSION AND DISCUSSION",
            "text": "We propose EQV, a quantum ensemble learning framework with the capability of noise mitigation. We use plurality voting strategy from classical ensemble learning to integrate the outputs from all sub-quantum classifiers. We study the impact of information loss based on this voting strategy. By deploying quantum classifiers into different quantum computers, combining with this voting strategy, our work achieves better performance in accuracy and stability than the state of art works under the same settings. On real NISQ machines, we are able to achieve up to 16.0% and 6.1% higher accuracy compared with state-of-the-art on two- and four-class classifications.\nIn quantum ensemble learning, we observe the unbalanced confidence distribution of correct and incorrect predictions provided by quantum classifiers from the distributions shown in Fig. 2. We go over instances when an unbalanced distribution prevents average aggregation from generating correct results. In addition to quantum noise, the unbalanced distribution may result from structure of quantum circuit and its data processing. In our work, we find that plurality voting can be utilized to address the unbalanced distribution.\nMultiple quantum classifiers are utilized to perform the same task in this study. Each quantum classifier can also be used to perform a subset of the overall task and incorporate the results into the output. Due to the differences in their underlying hardware and structural makeup, quantum classifiers differ from traditional machine learning classifiers. In addition to predictions, a quantum classifier\u2019s output may also include information regarding outputs other than prediction and accuracy, such as the effects of quantum noise. Although such information cannot be quantified, it will influence the result in other ways, such as the confidence distribution. In this manner, the outputs of quantum classifiers must be independently analyzed. And we propose to do so with EQV."
        }
    ],
    "title": "Improving Quantum Classifier Performance in NISQ Computers by Voting Strategy from Ensemble Learning",
    "year": 2022
}