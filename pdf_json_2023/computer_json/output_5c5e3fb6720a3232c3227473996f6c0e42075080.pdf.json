{
    "abstractText": "Rewriting incomplete and ambiguous utterances can improve dialogue models\u2019 understanding of the context and help them generate better results. However, the existing end-toend models will have the problem of too large search space, resulting in poor quality of rewriting results. We propose a 2-phase rewriting framework which first predicts the empty slots in the utterance that need to be completed, and then generate the part to be filled into each positions. Our framework is simple to implement, fast to run, and achieves the state-of-the-art results on several public rewriting datasets.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zitong Li"
        },
        {
            "affiliations": [],
            "name": "Jiawei Li"
        },
        {
            "affiliations": [],
            "name": "Haifeng Tang"
        },
        {
            "affiliations": [],
            "name": "Kenny Q. Zhu"
        },
        {
            "affiliations": [],
            "name": "Ruolan Yang"
        }
    ],
    "id": "SP:31896aedd2303cab68eea4d38e2fec47468c5b9e",
    "references": [
        {
            "authors": [
                "Ahmed Elgohary",
                "Denis Peskov",
                "Jordan BoydGraber."
            ],
            "title": "Can you unpack that? learning to rewrite questions-in-context",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
            "year": 2019
        },
        {
            "authors": [
                "Jiatao Gu",
                "Zhengdong Lu",
                "Hang Li",
                "Victor O.K. Li"
            ],
            "title": "Incorporating copying mechanism",
            "year": 2016
        },
        {
            "authors": [
                "Jie Hao",
                "Linfeng Song",
                "Liwei Wang",
                "Kun Xu",
                "Zhaopeng Tu",
                "Dong Yu."
            ],
            "title": "RAST: Domain-robust dialogue rewriting as sequence tagging",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4913\u20134924, Online",
            "year": 2021
        },
        {
            "authors": [
                "Jie Hao",
                "Linfeng Song",
                "Liwei Wang",
                "Kun Xu",
                "Zhaopeng Tu",
                "Dong Yu."
            ],
            "title": "RAST: domain-robust dialogue rewriting as sequence tagging",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Vir-",
            "year": 2021
        },
        {
            "authors": [
                "Shumpei Inoue",
                "Tsungwei Liu",
                "Nguyen Hong Son",
                "Minh-Tien Nguyen."
            ],
            "title": "Enhance incomplete utterance restoration by joint learning token extraction and text generation",
            "venue": "arXiv preprint arXiv:2204.03958.",
            "year": 2022
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Jin Lisa",
                "Song Linfeng",
                "Jin Lifeng",
                "Yu Dong",
                "Gildea1 Daniel"
            ],
            "title": "Hierarchical context tagging for utterance rewriting",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence",
            "year": 2022
        },
        {
            "authors": [
                "Qian Liu",
                "Bei Chen",
                "Jian-Guang Lou",
                "Bin Zhou",
                "Dongmei Zhang."
            ],
            "title": "Incomplete utterance rewriting as semantic segmentation",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2846\u20132857,",
            "year": 2020
        },
        {
            "authors": [
                "Scott Martin",
                "Shivani Poddar",
                "Kartikeya Upasani."
            ],
            "title": "MuDoCo: Corpus for multidomain coreference resolution and referring expression generation",
            "venue": "Proceedings of the 12th Language Resources and Evaluation Conference, pages 104\u2013111, Marseille,",
            "year": 2020
        },
        {
            "authors": [
                "Zhu Feng Pan",
                "Kun Bai",
                "Yan Wang",
                "Lianqiang Zhou",
                "Xiaojiang Liu"
            ],
            "title": "Improving open-domain dialogue systems via multi-turn incomplete utterance",
            "year": 2019
        },
        {
            "authors": [
                "Zhufeng Pan",
                "Kun Bai",
                "Yan Wang",
                "Lianqiang Zhou",
                "Xiaojiang Liu."
            ],
            "title": "Improving open-domain dialogue systems via multi-turn incomplete utterance restoration",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-",
            "year": 2019
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia,",
            "year": 2002
        },
        {
            "authors": [
                "Jun Quan",
                "Deyi Xiong",
                "Bonnie Webber",
                "Changjian Hu."
            ],
            "title": "GECOR: An end-to-end generative ellipsis and co-reference resolution model for taskoriented dialogue",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-",
            "year": 2019
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Michael Regan",
                "Pushpendre Rastogi",
                "Arpit Gupta",
                "Lambert Mathias."
            ],
            "title": "A dataset for resolving referring expressions in spoken dialogue via contextual query rewrites (cqr)",
            "venue": "arXiv preprint arXiv:1903.11783.",
            "year": 2019
        },
        {
            "authors": [
                "Abigail See",
                "Peter J. Liu",
                "Christopher D. Manning."
            ],
            "title": "Get to the point: Summarization with pointergenerator networks",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073\u2013",
            "year": 2017
        },
        {
            "authors": [
                "F\u00e1bio Souza",
                "Rodrigo Nogueira",
                "Roberto Lotufo."
            ],
            "title": "Portuguese named entity recognition using bert-crf",
            "venue": "arXiv preprint arXiv:1909.10649. 2740",
            "year": 2019
        },
        {
            "authors": [
                "Hui Su",
                "Xiaoyu Shen",
                "Rongzhi Zhang",
                "Fei Sun",
                "Pengwei Hu",
                "Cheng Niu",
                "Jie Zhou."
            ],
            "title": "Improving multi-turn dialogue modelling with utterance rewriter",
            "venue": "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Flo-",
            "year": 2019
        },
        {
            "authors": [
                "Hui Su",
                "Xiaoyu Shen",
                "Rongzhi Zhang",
                "Fei Sun",
                "Pengwei Hu",
                "Cheng Niu",
                "Jie Zhou."
            ],
            "title": "Improving multi-turn dialogue modelling with utterance ReWriter",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Bo-Hsiang Tseng",
                "Shruti Bhargava",
                "Jiarui Lu",
                "Joel Ruben Antony Moniz",
                "Dhivya Piraviperumal",
                "Lin Li",
                "Hong Yu."
            ],
            "title": "CREAD: Combined resolution of ellipses and anaphora in dialogues",
            "venue": "Proceedings of the 2021 Conference of the North American Chap-",
            "year": 2021
        },
        {
            "authors": [
                "Oriol Vinyals",
                "Meire Fortunato",
                "Navdeep Jaitly."
            ],
            "title": "Pointer networks",
            "venue": "Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc.",
            "year": 2015
        },
        {
            "authors": [
                "Yong Zhang",
                "Zhitao Li",
                "Jianzong Wang",
                "Ning Cheng",
                "Jing Xiao."
            ],
            "title": "Self-attention for incomplete utterance rewriting",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2022, Virtual and Singapore, 23-27 May 2022, pages",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 2731\u20132745 July 9-14, 2023 \u00a92023 Association for Computational Linguistics"
        },
        {
            "heading": "1 Introduction",
            "text": "In multi-turn dialogues, speakers naturally tend to make heavy use of references or omit complex discourses to save the efforts. Thus natural language understanding models usually need the dialogue history to understand the true meaning of the current utterance. The existence of such incomplete utterances increases the difficulty of modeling dialogues.\nThe sources of incompleteness of an utterance can be divided into two categories: coreference and ellipsis. The task for solving these two kinds of incompleteness is called Incomplete Utterance\n\u2217 The corresponding author.\nRewriting (IUR). As shown in Figure 1, the third utterance of this multi-turn dialogue is incomplete. If this utterance is taken out alone without a context, we will not be able to understand what \u201cone\u201d means and where to buy it. The fourth utterance is a rewriting of the third one. We can see that \u201cone\u201d in the third utterance is replaced by \u201cJ.K. Rowling\u2019s new book\u201d. In addition, the place adverbial \u201cfrom the book store in town\u201d is inserted after \u201cfor me\u201d. In today\u2019s industry strength dialogue systems and applications, due to stringent requirements on running time and maintenance cost, single-turn models are much more preferred than multi-turn models. If an incomplete single-turn utterance can be completed, it will be more understandable without the context, and the cost of downstream NLP tasks, such as intention extraction and response generation, will be reduced.\nFigure 1 shows that that all the words added in the rewritten utterance except \u201cfrom\u201d come from the context. Inspired by this, many early rewriting works used pointer networks (Vinyals et al., 2015) or sequence to sequence models with copy mechanism (Gu et al., 2016; See et al., 2017) to directly copy parts from the context into the target utterance. More recently, pre-trained language models such as T5 (Raffel et al., 2020) succeeds in many NLP tasks, and it appears that T5 is a plausible choice for utterance rewriting as well. However, IUR task is different from other generation tasks in that new parts typically only need to be added in one or two specific locations in the original utterance. That is, the changes to the utterance are localized. For example, a typical operation is adding modifiers before or after a noun. On the contrary, end-toend text generation models such as T5 may not preserve the syntactic structure of the input, which may cause the loss of important information and the introduction of wrong information into the output, which is illustrated as below (Two examples are generated by T5.).\n2731\n\u2022 Can you buy J.K. Rowling\u2019s new book? (Losing original structure)\n\u2022 Can you publish new book for me ? (Introducing wrong information)\nAnother problem of the end-to-end pre-trained models, which generate the rewritten utterances from scratch, is that they generally incur a large search space and are therefore not only imprecise but also inefficient. In order to solve the large search space issue, Hao et al. (2021a) treated utterance rewriting as a sequence tagging task. For each input word, they predict whether it should be deleted and the span that needs to be replaced with. Liu et al. (2020) formulated IUR as a syntactic segmentation task. They predict segmentation operations required on the utterance to be rewritten. However, they still did not take the important step of predicting the site of rewrite, particularly the position within the syntactic structure of the input utterance. If the model can learn the syntactic structure information in the target sentence, it can predict which part of the sentence needs to be modified, i.e., which words need to be replaced and where new words need to be inserted. After that, the model only needs to fill in these predicted positions. These two tasks are relatively simple to perform, and they collectively avoid the above problems. Our approach is based on the above intuition.\nIn order to effectively utilize the syntactic structure of the sentence to be rewritten, we divide the IUR task into two phases. The first phase is to predict which positions in the utterance need to be rewritten (including coreference and ellipsis). The second phase is to fill in the predicted positions. In the first phase, we use the sequence annotation method to predict the locations of coreference and ellipsis in the utterance. In the second phase, we take the utterances with blanks as input and directly predict the words required for the blank position. By seperating the original rewriting task into two relatively simple phases, our results show that our model performs the best among recent state-of-theart rewriting models 1.\nOur main contributions are as follows.\n\u2022 A two-phase framework for solving incomplete utterance rewriting task is proposed.\n1Complete code is available at https://github.com/ AutSky-JadeK/Locate-and-Fill.\nIt can complete the Incomplete Utterance Rewriting (IUR) task. (Section 2)\n\u2022 An algorithm for aligning the two sentences before and after rewriting based on the longest common subsequences (LCS) algorithm. We succinctly and efficiently generated two kinds of data which can be used for predicting the positions to be rewritten (the first phase) and filling the blanks (the second phase) respectively. (Section 2.1.2)\n\u2022 We have carried out experiments on 5 datasets, and the experimental results show that our two-phase framework achieves state-of-theart results. (Section 3)"
        },
        {
            "heading": "2 Approach",
            "text": "Our framework is divided into two phases: Locating positions to rewrite and Filling the blanks. Figure 2 is a brief schematic of the framework. Phase 1 can be done either by heuristic rules or by supervision. Phase 2 can be done with a seq2seq text generation model. We give the details of these phases next."
        },
        {
            "heading": "2.1 Locating Positions to Rewrite",
            "text": "We designed an unsupervised and a supervised method to locate positions to rewrite. The two methods are described below."
        },
        {
            "heading": "2.1.1 Unsupervised Rule-based Method",
            "text": "We first implement a rule-based method for the first phase of our problem, aiming at predicting the blanks automatically. We looked through thousands of complete utterance examples in Elgohary et al. (2019). Based on our observations and experience, we define six rules for generating two kinds\nof blanks which are used for resolving coreference and ellpisis in the second phase. The rules for generating blanks are summarized and explained below: Personal Pronouns: We replace all the personal pronouns (except the first- and second-person pronouns) and their corresponding possessive pronouns with [MASK_r]. This indicates that we will replace these pronouns with some specific noun phrases at second phase. Interrogatives: We insert [MASK_i] after the interrogative if the whole utterance only contains interrogatives such as what, how, why, when and so forth. [MASK_i] indicates that some additional text span shall be inserted at this location. That, This: The use of word like \u201cthis\u201d, \u201cthat\u201d, \u201cthese\u201d and \u201cthose\u201d are commonly used in colloquial language, which becomes a source of ambiguity. Therefore, we deal with the use of these pronouns in following ways:\n- Not followed by a noun phrase: In this case, we simply replace the word by [MASK_r].\n- Otherwise: We will insert [MASK_i] after the noun phrase.\nThe+Noun Phrase: We will insert [MASK_i] after the noun phrase. Other, Another, Else: If the utterance contains these words, it usually indicates that there are people/things additional to what have been mentioned before. Hence, we add a [MASK_i] at the head of the sentence. Before, After: We insert [MASK_i] after the sentence ended with \u201cbefore\u201d or \u201cafter\u201d, which is considered as an incompletion."
        },
        {
            "heading": "2.1.2 Supervised LCS-based Method",
            "text": "We also design an algorithm based on the Longest Common Subsequence (LCS) algorithm . The sentence to be rewritten X and after rewriting Y are aligned via a sequence labeling model. To obtain the common subsequence, LCS algorithm returns a matrix M which stores the LCS sequence for each step of the calculation. The value of Mi,j indicates the actual LCS length of sequences X[0, i] and Y [0, j] 2. When we trace back from the max value at the corner, the decreases of length show that the sentences have a common token.\n2https://en.wikipedia.org/wiki/Longest_common_ subsequence_problem\nCoreference and ellipsis towards original sentence are extracted through LCS trace back algorithm, which is further labeled as COR and ELL respectively. Given the tokenized original sentence X and ground truth Y as shown in Figure 3, the rules for labeling are specified as follows:\n\u2022 The labeling is proceeded from the bottom right to the top left corner of a LCS matrix. If the current tokens in Xi and Yj are equal, Xi matches part of the LCS and is labeled as O, then we go both up and left (shown in black). If not, we go up or left, depending on which cell has a higher number or lower index j, until we find next matched Xi\u2032 that satisfies Xi\u2032 = Yj\u2032 .\n\u2022 If traversed path from previously matched token pair to newly match pair is a straight up arrow, it indicates that token(s) in interval (Yj\u2032 , Yj)\n3 is (are) inserted at corresponding position i\u2032 in X to complete the original sentence. In this case, token Xi\u2032 is labeled as ELL(shown in orange).\n\u2022 If two matched pairs in the LCS matrix are joined by paths with corners, interval (Xi\u2032 , Xi) is replaced by (Yj\u2032 , Yj) during rewriting. As a result, coreferenced words are labeled as COR(shown in blue).\nThen, we input the pre-processed training data into the BERT-CRF (Souza et al., 2019) model, which is considered as a sequence annotation task. Using the method described in Section 2.1.2, we obtained the locations of coreferences and ellipses\n3(a, b) means an open interval excl. the endpoints a and b.\nof each utterance waiting to be rewritten. As shown in Figure 3, we use the BIO format to annotate the sequence. The starting position of the coreference is marked as B-COR (Begin-Coreference), while other positions of the coreference are marked as ICORs (Inside-Coreference). Ellipsis only appears in the middle of two tokens, so we mark the position of the latter token as B-ELL (Begin-Ellipse), which means that there should be missing words between this token and the previous token, and the subsequent model is required to fill in it."
        },
        {
            "heading": "2.2 Blanks Filling",
            "text": "Can you buy [MASK_r] for me ?\nCan you buy that novel for me [MASK_i] ?\nSub-sentence 1 Sub-sentence 2\nCan you buy [MASK_r] for me [MASK_i] ?\nCan you buy that novel for me ? Original Sentence\nSentence with Blanks\nSplit\nFigure 4: Split the sentence according to the number of blanks in the utterance.\nCan you buy [MASK_r] (that novel) for me [MASK_i] ( ) ?\nCan you buy [MASK_r] for me [MASK_i] ?\nCan you buy that novel for me ? Original Sentence\nSentence with Blanks\nAdd Hints\nSentence with Hints\nFigure 5: Add hints to blanks.\nWe use T5-small (Raffel et al., 2020) and bartbase (Lewis et al., 2020) as pre-trained language model (PLM) in phase 2. In this section, we will take T5 as an example to illustrate the process of blanks filling.\nWe use the T5 model to fill in blanks with two optimizations: adding hints and splitting the current utterance into sub-sentences. The latter can ensure that there is only one blank in the sentence to be filled in the T5 model. The two optimizations are shown in Figure 4 and Figure 5. We transfer the data of each multi-turn dialogue into the format shown in Figure 6, and fine-tune the T5 model.\nInput: I heard that J.K. Rowling's new book has been published. [SEP] Great. I'm going to the bookstore in town. [SEP] Can you buy <extra_id_0> (that book) for me? Output: - . 5RZOLQJ\u00b6V QHZ ERRN\nInput: I heard that J.K. Rowling's new book has been published. [SEP] Great. I'm going to the bookstore in town. [SEP] Can you buy that book for me <extra_id_0> ( ) ? Output: from the bookstore in town\nFigure 6: Format of fine-tune data of T5.\nAfter fine-tuning, we take the predicted results of\nBERT-CRF model in Section 2.1.2 as input to get the final blank filled results of T5 model. Finally, the outputs of T5 model are filled back into the blanks of the original sentence to get the rewritten utterance. The same is for the rule-based method. The blank prediction obtained from it is directly input into the same T5 model (the two optimization methods described in Figure 4 and Figure 5 will also be used) to obtain the output of T5."
        },
        {
            "heading": "3 Experiment",
            "text": "In this section, we will introduce our experiment setup and results."
        },
        {
            "heading": "3.1 Datasets",
            "text": "We tested the baseline and our framework on 3 public datasets in English and 2 in Chinese. The statistics are shown in Table 1. The examples are shown in Appendix.\nMuDoCo (Martin et al., 2020) has a lower rewriting rate, which makes the rule-based method less accurate in predicting the locations to be rewritten. CQR (Regan et al., 2019) contains imperative dialogues in life (between people or between people and intelligent agents). The sentence patterns are relatively simple, fixed and easy to understand. REWRITE (Su et al., 2019a) is a Chinese dataset, each dialogue of which contains 3 turns. It is collected from several popular Chinese social media platforms. The task is to complete the last turn. RES (Restoration-200k) (Pan et al., 2019a) is a large-scale Chinese dataset in which 200K multiturn conversations are collected and manually labeled with the explicit relations between an utterance and its context. Each dialogue is longer than REWRITE. CANARD (Elgohary et al., 2019) contains a series of English dialogues about a certain topic or person organized in the form of QA. It has the largest size and the longest context length. The sentence pattern in CANARD is complex, the\nunderstanding is difficult, and the rewriting degree is high."
        },
        {
            "heading": "3.2 Baselines",
            "text": "We choose the following strong baselines to compete with our framework. T5-small model and T5-base model 4 (Raffel et al., 2020). We directly take the context and the current utterance as inputs, use the training set to fine-tune the T5 model, and test its end-to-end output on the test set as the result of rewriting the utterance. BART-base model (Lewis et al., 2020). This is another pre-trained model we used. Its size is close to T5-small. Our model is tested based on these 2 PLMs. Rewritten U-shaped Network (RUN) (Liu et al., 2020). In this work, the authors regard the incomplete utterance rewriting task as a dialogue editing task, and propose a new model using syntactic segmentation to solve this task. Hierarchical Context Tagging (HCT)(Lisa et al., 2022). A method based on sequence tagging is proposed to solve the robustness problem in rewriting task. Rewriting as Sequence Tagging (RAST)(Hao et al., 2021b). The authors proposed a novel tagging-based approach that results in a significantly smaller search space than the existing methods on the incomplete utterance rewriting task."
        },
        {
            "heading": "3.3 Evaluation Metrics",
            "text": "We use the BLEUn score (Papineni et al., 2002) to measure the similarity between the generated rewritten utterance and the ground truth. Low order n-gram BLEUn score can measure precision, while high-order n-gram can measure the fluency of the sentence. We also use the ROUGEn score (Lin, 2004) to measure recall of rewritten utterance. Rewriting F-scoren (Pan et al., 2019b) is used to examine the words newly added to the current sentence. We calculte Rewriting F-score by comparing words added by the rewriting model with added words in ground truth. It is a widely accepted metric that can better measure the quality of rewriting. In addition to the automatic evaluation method, we also asked human annotators to conduct comparative tests on the rewriting results.\n4A fine-tuned version of T5-base is used: https:// huggingface.co/castorini/t5-base-canard."
        },
        {
            "heading": "3.4 Implementation Details",
            "text": "All of the models are running and evaluated on 2 Intel(R) Xeon(R) Silver 4210 CPU @ 2.20GHz with 4 NVIDIA GeForce RTX 2080 GPU and a 128GB RAM. Due to the memory constraints of our experimental environment, we adopt T5-small model in the second phase of our framework, and fine-tune it for 20 epochs. All experiments are repeated for 3 times and averaged."
        },
        {
            "heading": "3.5 Main Results",
            "text": "In the following section, \u201cOurs-T5\u201d represents our model based on T5-small in phase 2. \u201cOurs-BART\u201d is our framework based on BART-base in phase 2. \u201cOurs-rule\u201d is a variant of our method which uses the rule-based method in Section 2.1.1 to generate blanks in phase 1 and T5-small in phase 2. \u201cGoldT5\u201d is the result of directly inputting the sentence with the correct blanks into the T5-small model in phase 2. \u201cGold-BART\u201d is directly inputting the sentence with the correct blanks into the BARTbase model in phase 2.\nTable 2 shows the results of our framework and baselines on CQR and MuDoCo. Compared with CANARD, the two datasets are smaller in size and simpler in sentence structure. Our approach is significantly better than all baselines on all metrics. For Rewriting F-score, our method is 6.37 and 6.63 percentage points higher than the sub-optimal end-to-end T5-small model, respectively. This metric strongly shows that our method can introduce more new words provided in the ground truth (compared with the original sentence). Relatively larger advantages of our model compared with T5-small in BLEU and ROUGE show that our method based on blank prediction and filling can retain the structure of the original sentence to the greatest extent, so as to retain more correct same information when calculating these two metrics and comparing the two sequences. However, end-to-end T5 model generates the whole rewriting utterance directly, which may lose some information from the original sentence.\nThe last part of Table 2 shows the results of our framework and baselines on CANARD. Among the three datasets we used, samples in CANARD are the most difficult and the most complex. Our model is superior to other baseline methods in all the experimental metrics. Especially in BLEU score, our method is significantly better than all baselines. As for Rewriting F-score and ROUGE, we found that\nthe performance of end-to-end T5 model is close to our method. This is because the generative T5 model is very powerful and can generate fluent sentences. However, our 2-phase framework can better predict which positions in the current sentence should be rewritten, which can not be achieved by the end-to-end model. In the following analysis, we will further analyze this point.\nAn important reason why our framework is better than baselines on CQR and MuDoCo is that CQR mainly contains dialogues that users are asking agents for help. The positions and forms of words that can be added are relatively fixed, such as adding place adverbials. Samples in MuDoCo are basically imperative dialogues in daily life. It also has the same feature, which makes our model easier to learn. The results in Section 3.7 can also illustrate this point. The accuracy of the first phase of our framework is higher on CQR and MuDoCo.\nTable 3 shows the results of our framework and baselines on Chinese datasets REWRITE and RES. Due to the better performance of BART in Chinese texts, our model is mainly tested based on BARTbase rather than T5-small in these two datasets. These two PLMs have similar sizes. HCT, RUN and RAST perform well on these two datasets. Because these two datasets have few turns and simple contents, they have been studied a lot in previous work. However, their performance is not as good as that of BART-base. This shows the great potential\nof using PLMs directly in rewriting tasks. Compared with BART-base, our model has improved in BLEU score and ROUGE score. This shows that our method is also effective in Chinese. And when different PLMs are used as frameworks, the results can be improved.\nIn the Table 4, our model is compared with T5 model for end-to-end prediction. It can be observed that the word \u201cthere\u201d is not considered to be replaced by the end-to-end model, which is due to the fact that the position to be rewritten is not obviously predicted. Our two-phase framework can make up for this. The sequence annotation model indicates that \u201cthere\u201d is a part that needs to be replaced, so the T5 in the second phase can be predicted correctly. This is our advantage over the end-to-end model. More case studies are shown in appendix."
        },
        {
            "heading": "3.6 Human Evaluation",
            "text": "Table 5 shows the results of human evaluation on CANARD. For each pair of competing models, 50 pairs of rewriting results were randomly sampled from the testset for comparative testing. A total of 200 questions were randomly assigned to 5 human volunteers on average. Each person needs to choose the better one from the prediction results of the two models. As can be seen from the table, our method is significantly stronger than RUN, HCT, and the rule-based method in Section 2.1.1. When compared with the end-to-end T5-small model, our advantage is relatively small. After observing the feedback of human annotators, we find that the end-to-end model has the advantage of direct generation and can generate more complete and fluent sentences. Our method only generates the words needed in blank, which lacks a certain degree of sentence fluency. However, our 2-phase framework can accurately predict the positions that need to be rewritten in the current sentence, which is beyond the reach of the end-to-end model (see appendix for specific analysis). Taken together, our method should be even better."
        },
        {
            "heading": "3.7 Ablation Tests",
            "text": "Table 6 shows the results of end-to-end ablation test on CANARD. We can see that by replacing LCS algorithm with greedy algorithm, the experimental results have decreased to a certain extent, which shows the effectiveness of LCS algorithm. On the other hand, due to the diversity of experi-\nmental data, the matching algorithm can only approach the correct results, and can not guarantee the complete correctness. Greedy algorithm is also a substitute. Our greedy algorithm is described as follows.\nWe use 2 pointers to traverse the current utterance and ground truth utterance. The pointers point to the current word in each of the two utterances. If they cannot be matched, the pointer of the ground truth will advance to the next matching position and stop, and the scanned span will be marked as an \u201cellipsis\u201d. If no match can be made until the end, the pointer of the current occurrence moves forward one bit and adds the previous position to the span of \u201ccoreference\u201d.\nIf we remove the two optimizations of splitting sentences according to the number of blanks and adding hint from our framework, there will be more obvious decline. The reason is that splitting sentences can keep more syntactic information in sentences, and multiple blanks will make sentences look \u201cfull of loopholes\u201d. Adding hint will prompt the original words in the language model in phase 2, so as to provide more information. For example, if our hint is \u201che\u201d, the model will not tend to fill in a female name or other things here.\nTable 7 shows the F1-score of our LCS based algorithm and greedy based algorithm in predicting the locations that need to be rewritten (that is, the first phase in the 2-phase framework). They are trained and tested on the sequence annotation data generated by their own methods. We can see that the algorithm based on LCS has better effect."
        },
        {
            "heading": "3.8 Time Cost Evaluation",
            "text": "Table 7 shows the results of training and predicting time on CANARD. In Section 3.5, we found that\nour model has the least advantage over the endto-end T5-small model. Therefore, in this section, we compare their time consumption. In Table 7a, under the same configuration, we found that our method would take more time to fine-tune. This is understandable because although there are only 5571 samples in the testset of canard dataset, we will segment sentences according to the number of blanks. Even if there are sentences without any blanks, this optimization also leads to an increase in the number of samples to 6569. Interestingly, in the inference time, Table 7b shows that our model takes less time. This may be because our model does not need to generate a whole sentence, but only needs to fill in the blank, which is much shorter than a complete utterance. Due to the short time of BERTCRF, our method only takes 11.9% more time than the end-to-end T5 model, and the overall size of the model is almost the same as other training requirements. Therefore, we believe that even a small increase in results can illustrate the effectiveness of our method."
        },
        {
            "heading": "3.9 Comparison with ChatGPT",
            "text": "In this section, we will present the results of comparison with ChatGPT 5. Dialogue systems are useful in many tasks and scenarios. Rewriting utterances is particularly useful when a light-weight dialogue model which only takes the last utterance as input is desirable. This is exactly where very large models such as ChatGPT cannot help, not to mention the various woes of current ChatGPT such as the cost of deployment, slow inference speed, and privacy issues. Therefore, we believe that it is not fair to compare ChatGPT with the kind of rewriting technology that we are advocating in this\n5https://chat.openai.com/\npaper, and the latter still has its merits.\nThe scale of ChatGPT or is at least 3 orders of magnitude larger than the models we use in this paper, which means this is not a fair comparison. Nevertheless, we still conducted the following supplementary experiments on ChatGPT. The prompt we used is shown in Table 8.\nThe experimental results on 30 cases of CANARD is shown in Table 8. Some examples of the results are shown in Table 9. After repeated tries and with the best prompt we can find, ChatGPT is still worse than our method in terms of automatic evaluation metrics. However, by human evaluation, testers think that the rewriting results of ChatGPT are of higher quality (more fluent). This is no surprise given the tremendous parameter space of ChatGPT."
        },
        {
            "heading": "4 Related Work",
            "text": "Early work on rewriting often considers the problem as a standard text generation task, using pointer networks or sequence-to-sequence models with a copy mechanism (Su et al., 2019b; Elgohary et al.,\n2019; Quan et al., 2019) to fetch the relevant information in the context (Gu et al., 2016). Later, pre-trained models like T5 (Raffel et al., 2020) are fine-tuned with conversational query reformulation dataset to generate the rewritten utterance directly. Inoue et al. (2022) uses Picker which identifies the omitted tokens to optimize T5.In general, these generative approaches ignore the characteristic of IUR problem: rewritten utterances often share the same syntactic structure as the original incomplete utterances.\nGiven that coreference is a major source of incompleteness of an utterance, another common thought is to utilize a coreference resolution or corresponding feartures. Tseng et al. (2021) proposed a model which jointly learns coreference resolution and query rewrite with the GPT-2 architecture (Radford et al., 2019). By first predicting coreference links between the query and context, the performance of rewriting has improved while the incompleteness is induced by coreference. However, this does not work for utterances with ellipisis. Besides, the performance of the rewriting model is limited by the coreference resolution model.\nRecently, some of the work on incomplete utterance rewriting focuses on the \u201cactions\u201d we take to change the original incomplete utterance into a self-contained utterance (target utterance). Hao et al. (2021a) solves this problem with a sequencetagging model. For each word in the input utterance, the model will predict whether to delete it or not, meanwhile, the span of words which need to be inserted before the current word will be chosen from the context. Liu et al. (2020) formulated the problem as a syntactic segmentation task by predicting segmentation operations for the rewritten utterance. Zhang et al. (2022) extracts the coreference and omission relationship directly from the self-attention weight matrix of the transformer instead of word embeddings. Compared with these methods, our framework separates the two phases more thoroughly of predicting the rewriting position and filling in the blanks, and meanwhile,\nreduces the difficulty of the two phases with the divide and conquer method."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this work, we present a new 2-phase framework which includes locating positions to rewrite and filling the blanks for solving Incomplete Utterance Rewriting (IUR) task. We also propose an LCS based method to align the original incomplete sentence with the ground truth utterance to obtain the positions of coreference and ellipsis. Results show that our model performs the best in several metrics. We also recognize two directions for further research. First, as the performance of our 2-phase framework is often limited by the first phase, we will try to improve the accuracy of locating rewriting positions. Second, it will be useful to study the best way for applying our rewriting model to other downstream NLP tasks."
        },
        {
            "heading": "6 Limitations",
            "text": "Our framework is a two-phase process, which has its inherent defects, that is, the results of the second phase depend on the results of the phase 1. Because the sequence annotation algorithm in the first phase cannot achieve 100% accuracy, it will predict the wrong position that should be rewritten when the second phase is followed, which will further lead to the error of the final result.\nOn the other hand, T5 model is only used to predict the words that should be filled in blank, rather than generate the whole sentence, which may lead to the decline of the overall fluency of the sentence."
        },
        {
            "heading": "Acknowledgments",
            "text": "This work was generously supported by the CMB Credit Card Center & SJTU joint research grant, and Meituan-SJTU joint research grant."
        },
        {
            "heading": "A Examples of Datasets",
            "text": "The brief descriptions, statistics and samples of the datasets are shown in Table 10."
        },
        {
            "heading": "B Cases in CANARD",
            "text": "Table 11 shows some specific examples of rewriting using our model and other baselines. The examples of predicting results of our model, HCT, RUN and T5-small on CANARD dataset are shown from top to bottom. HCT tends to copy the predicted span directly from the context. From the first example, we can find that HCT predicts the correct position\nof coreference in the current sentence, but finds the wrong span. From the second example, we can see that RUN\u2019s edit based model duplicates the span from the context. Our model uses T5 to find the corresponding span from the context, which is significantly stronger than RUN and HCT.\nThe third example shows the shortcomings of our model. Compared with the end-to-end T5small model, the first step of our framework failed to predict the need to insert words between \u201cwrite\u201d and \u201creal\u201d, so the second step could not fill in the correct answer. This shows the inherent defect of the 2-step framework, that is, the result of the second step depends on that of the first step, and there is a certain gap."
        },
        {
            "heading": "C Cases in CQR and MuDoCo",
            "text": "As a supplement to case study, we provide more cases from CQR and MuDoCo here in Table 12.\nACL 2023 Responsible NLP Checklist"
        },
        {
            "heading": "A For every submission:",
            "text": "3 A1. Did you describe the limitations of your work?\n6\nA2. Did you discuss any potential risks of your work? Not applicable. Left blank.\n3 A3. Do the abstract and introduction summarize the paper\u2019s main claims? Abstract\n7 A4. Have you used AI writing assistants when working on this paper? I didn\u2019t use it.\nB 3 Did you use or create scientific artifacts? 3\n3 B1. Did you cite the creators of artifacts you used? 3.1, 3.2\n7 B2. Did you discuss the license or terms for use and / or distribution of any artifacts? They are completely public.\n7 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)? They are consistent with the intended use.\n7 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? They are completely anonymous.\n3 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? 3.1, Appendix\n3 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be. They are shown at the beginning of the section 3.\nC 3 Did you run computational experiments? 3\n3 C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used? 3.4\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.\n3 C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values? 3.4\n3 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run? 3.4\n3 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)? 3.2\nD 7 Did you use human annotators (e.g., crowdworkers) or research with human participants? Left blank.\nD1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.? No response.\nD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants\u2019 demographic (e.g., country of residence)? No response.\nD3. Did you discuss whether and how consent was obtained from people whose data you\u2019re using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used? No response.\nD4. Was the data collection protocol approved (or determined exempt) by an ethics review board? No response.\nD5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? No response."
        }
    ],
    "title": "Incomplete Utterance Rewriting by A Two-Phase Locate-and-Fill Regime",
    "year": 2023
}