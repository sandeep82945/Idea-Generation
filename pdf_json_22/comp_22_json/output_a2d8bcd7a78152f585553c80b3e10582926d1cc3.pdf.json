{
    "abstractText": "At present, the facial expression recognition model in video communication has problems such as weak network generalization ability and complex model structure, which leads to a large amount of computation. Firstly, the Inception architecture is adopted as a design philosophy. \u009de Visual Geometry Group Network (VGGNet) model is improved. Multiscale kernel convolutional layers are constructed to obtainmore expressive features. Secondly, the attentionmechanism is integrated into amultiscale feature fusion network to form a multiattention mechanism Convolutional Neural Network (CNN) model. Novel spatial and multichannel attention models are designed. \u009de e\u008eects of redundant information and noise are reduced. Finally, experiments are carried out on the Fer2013 dataset and the Extended Cohn-Kanade Dataset (CK+) to verify the detection accuracy of the model. \u009de results show that the Delivered Duty Unpaid (DDU) loss can be used for facial expression recognition in complex environments. After the attention module is added, the overall recognition accuracy of the network on Fer2013 and CK+has been improved to varying degrees. \u009de addition of the channel attention module has a more obvious e\u008eect on the recognition accuracy compared with the spatial attention module. \u009de addition of the attention module enables the network to increase the attention to error-prone samples. \u009de improved network model can better extract the key features of facial expressions, enhance the feature discrimination ability, and improve the recognition accuracy of error-prone expressions. \u009de accuracy rate of facial expression recognition with larger movements is over 98%. Facial expressions are an important way of communication between people, and online video has greatly limited this communication method. \u009de proposed CNN model based on multiscale feature fusion will e\u008eectively solve these network limitations and have an important and positive impact on future network information exchange.",
    "authors": [
        {
            "affiliations": [],
            "name": "Fulan Ye"
        }
    ],
    "id": "SP:970e95037ff1d3b53bf89e340fd6b32c8dff580a",
    "references": [
        {
            "authors": [
                "F.S. Tahir",
                "A.A. Abdulrahman",
                "Z. Hikmet",
                "anon"
            ],
            "title": "Novel face detection algorithm with a mask on neural network training",
            "venue": "International Journal of Nonlinear Analysis and Applications, vol. 13, no. 1, pp. 209\u2013215, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "S. Laith",
                "F.S. Tahir",
                "A.A. Abdulrahman"
            ],
            "title": "Effectiveness of new algorithms for facial recognition based on deep neural networks",
            "venue": "International Journal of Nonlinear Analysis and Applications, vol. 13, no. 1, pp. 2171\u20132178, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "C.-T. Lu",
                "C.-W. Su",
                "H.-L. Jiang",
                "Y.-Y. Lu"
            ],
            "title": "An interactive greeting system using convolutional neural networks for emotion recognition",
            "venue": "Entertainment Computing, vol. 40, Article ID 100452, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "E. Armingol",
                "A. Officer",
                "O. Harismendy",
                "N.E. Lewis"
            ],
            "title": "Deciphering cell-cell interactions and communication from gene expression",
            "venue": "Nature Reviews Genetics, vol. 22, no. 2, pp. 71\u201388, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "R.M. Cutri",
                "J. Mena"
            ],
            "title": "A critical reconceptualization of faculty readiness for online teaching",
            "venue": "Distance Education, vol. 41, no. 3, pp. 361\u2013380, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "G. Jeevan",
                "G.C. Zacharias",
                "M.S. Nair",
                "J. Rajan"
            ],
            "title": "An empirical study of the impact of masks on face recognition",
            "venue": "Pattern Recognition, vol. 122, Article ID 108308, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "H.N. Vu",
                "M.H. Nguyen",
                "C. Pham"
            ],
            "title": "Masked face recognition with convolutional neural networks and local binary patterns",
            "venue": "Applied Intelligence, vol. 52, no. 5, pp. 5497\u20135512, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "X. Yang",
                "W. Zhang"
            ],
            "title": "Heterogeneous face detection based on multi-task cascaded convolutional neural network",
            "venue": "IET Image Processing, vol. 16, no. 1, pp. 207\u2013215, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "W. Hariri"
            ],
            "title": "Efficient masked face recognition method during the covid-19 pandemic",
            "venue": "Signal, image and video processing, vol. 16, no. 3, pp. 605\u2013612, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "A. Jaiswal",
                "A.R. Babu",
                "M.Z. Zadeh",
                "D. Banerjee",
                "F. Makedon"
            ],
            "title": "A survey on contrastive self-supervised learning",
            "venue": "Technologies, vol. 9, no. 1, p. 2, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "J. Pfaff",
                "A. Filippov",
                "S. Liu"
            ],
            "title": "Intra prediction and mode coding in VVC",
            "venue": "IEEE Transactions on Circuits and Systems for Video Technology, vol. 31, no. 10, pp. 3834\u20133847, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S.T. Tuzimski"
            ],
            "title": "Szubartowski, \u201cApplication of d-SPE before SPE and HPLC-FLD to analyze bisphenols in human breast milk samples,\u201dMolecules",
            "venue": "vol. 26,",
            "year": 2021
        },
        {
            "authors": [
                "V. Muthukumar",
                "A. Narang",
                "V. Subramanian",
                "M. Belkin",
                "D. Hsu",
                "A. Sahai"
            ],
            "title": "Classification vs. regression in overparameterized regimes: does the loss function matter",
            "venue": "Journal of Machine Learning Research, vol. 22, no. 222, pp. 1\u201369, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "W. Sun",
                "H. Wang",
                "Q. Gu",
                "S. Rong",
                "L. Fan"
            ],
            "title": "Exact frequency estimation in the i.i.d. Noise via KL divergence of accumulated power",
            "venue": "IEEE Communications Letters, vol. 25, no. 8, pp. 2574\u20132578, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "G.W. Lindsay"
            ],
            "title": "Convolutional neural networks as a model of the visual system: Past, present, and future",
            "venue": "Journal of Cognitive Neuroscience, vol. 33, no. 10, pp. 2017\u20132031, 2021.",
            "year": 2017
        },
        {
            "authors": [
                "I. Cong",
                "S. Choi",
                "M.D. Lukin"
            ],
            "title": "Quantum convolutional neural networks",
            "venue": "Nature Physics, vol. 15, no. 12, pp. 1273\u2013 1278, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "D. Zou",
                "Y. Cao",
                "D. Zhou",
                "Q. Gu"
            ],
            "title": "Gradient descent optimizes over-parameterized deep ReLU networks",
            "venue": "Machine Learning, vol. 109, no. 3, pp. 467\u2013492, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "B. Subrahmanyeswara Rao"
            ],
            "title": "Accurate leukocoria predictor based on deep VGG-net CNN technique",
            "venue": "IET Image Processing, vol. 14, no. 10, pp. 2241\u20132248, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Z. Wang",
                "R.M. Liu",
                "Q.T. Huang"
            ],
            "title": "Inflated VGGNet-16 networks for human action recognition",
            "venue": "Journal of Beijing University of Chemical Technology (Natural Science Edition), vol. 47, no. 3, p. 114, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Sun",
                "Y. Weng",
                "B. Luo"
            ],
            "title": "Gesture recognition algorithm based on multi-scale feature fusion in RGB-D images",
            "venue": "IET Image Processing, vol. 14, no. 15, pp. 3662\u20133668, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "X. Wang",
                "S. Zhang",
                "S. Wang",
                "T. Fu",
                "H. Shi",
                "T. Mei"
            ],
            "title": "Misclassified vector guided softmax loss for face recognition",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 07, pp. 12241\u201312248, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "R. Zebari",
                "A. Abdulazeez",
                "D. Zeebaree",
                "D. Zebari",
                "J. Saeed"
            ],
            "title": "A comprehensive review of dimensionality reduction techniques for feature selection and feature extraction",
            "venue": "Journal of Computational Intelligence and Neuroscience 9 Applied Science and Technology Trends, vol. 1, no. 2, pp. 56\u201370, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "A. Singh",
                "S. Prakash",
                "A. Kumar",
                "D. Kumar"
            ],
            "title": "A proficient approach for face detection and recognition using machine learning and high-performance computing,\u201dConcurrency and Computation: Practice and Experience",
            "venue": "vol. 34,",
            "year": 2022
        },
        {
            "authors": [
                "G.P. Kusuma",
                "J. Jonathan",
                "A.P. Lim"
            ],
            "title": "Emotion recognition on fer-2013 face images using fine-tuned vgg-16",
            "venue": "Advances in Science, Technology and Engineering Systems Journal, vol. 5, no. 6, pp. 315\u2013322, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "M.J. Khan",
                "M.J. Khan",
                "A.M. Siddiqui",
                "K. Khurshid"
            ],
            "title": "An automated and efficient convolutional architecture for disguise-invariant face recognition using noise-based data augmentation and deep transfer learning",
            "venue": "Ce Visual Computer, vol. 38, no. 2, pp. 509\u2013523, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "B.K. Triwijoyoa",
                "A. Adila"
            ],
            "title": "Analysis of medical image resizing using bicubic interpolation algorithm",
            "venue": "Jurnal Ilmu Komputer, vol. 14, no. 2, pp. 20\u201329, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "W. Dias",
                "F. Andal\u00f3",
                "R. Padilha"
            ],
            "title": "Cross-dataset emotion recognition from facial expressions through convolutional neural networks",
            "venue": "Journal of Visual Communication and Image Representation, vol. 82, Article ID 103395, 2022. 10 Computational Intelligence and Neuroscience",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "Research Article",
            "text": ""
        },
        {
            "heading": "Emotion Recognition of Online Education Learners by",
            "text": ""
        },
        {
            "heading": "Convolutional Neural Networks",
            "text": "Fulan Ye\nSchool of Big Data, Fuzhou University of International Studies and Trade, Fuzhou 350202, Fujian, China\nCorrespondence should be addressed to Fulan Ye; y @fzfu.edu.cn\nReceived 13 April 2022; Revised 16 May 2022; Accepted 24 May 2022; Published 9 June 2022\nAcademic Editor: Arpit Bhardwaj\nCopyright \u00a9 2022 Fulan Ye. is is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.\nAt present, the facial expression recognition model in video communication has problems such as weak network generalization ability and complex model structure, which leads to a large amount of computation. Firstly, the Inception architecture is adopted as a design philosophy. e Visual Geometry Group Network (VGGNet) model is improved. Multiscale kernel convolutional layers are constructed to obtainmore expressive features. Secondly, the attentionmechanism is integrated into amultiscale feature fusion network to form a multiattention mechanism Convolutional Neural Network (CNN) model. Novel spatial and multichannel attention models are designed. e e ects of redundant information and noise are reduced. Finally, experiments are carried out on the Fer2013 dataset and the Extended Cohn-Kanade Dataset (CK+) to verify the detection accuracy of the model. e results show that the Delivered Duty Unpaid (DDU) loss can be used for facial expression recognition in complex environments. After the attention module is added, the overall recognition accuracy of the network on Fer2013 and CK+has been improved to varying degrees. e addition of the channel attention module has a more obvious e ect on the recognition accuracy compared with the spatial attention module. e addition of the attention module enables the network to increase the attention to error-prone samples. e improved network model can better extract the key features of facial expressions, enhance the feature discrimination ability, and improve the recognition accuracy of error-prone expressions. e accuracy rate of facial expression recognition with larger movements is over 98%. Facial expressions are an important way of communication between people, and online video has greatly limited this communication method. e proposed CNN model based on multiscale feature fusion will e ectively solve these network limitations and have an important and positive impact on future network information exchange."
        },
        {
            "heading": "1. Introduction",
            "text": "is study aims to improve the accuracy of facial recognition. Facial expressions can intuitively convey people\u2019s emotions and wish through nontext forms and are the main method of conveying emotional information and communicating interpersonal relationships between the two parties [1]. Psychologists have shown through investigation and research that, in the process of human emotion communication, the emotional information conveyed by facial expressions accounts for about 55%, the sound form accounts for 38%, and the language form conveys only 7% of the information [2]. Facial expressions can better re ect the authenticity of human inner emotional activities [3]. Expression is an irreplaceable nonverbal communication\nmethod in interpersonal communication, which plays a role in conveying emotional state and intention [4].\nAt present, due to the in uence of Coronavirus Disease 2019, the combination of online and o\u00a3ine classes has become a new development trend in teaching. e recognition of students\u2019 facial expressions can become an important technical means to assist teachers in classroom teaching and can also improve the quality of teaching. Education and teaching will also ourish towards meaningful and valuable health education [5]. e improved existing deep learning model structure is used as the entry point through the research and summary of the existing deep learning theory and network model. In view of the current network, with generalization ability being weak, the model structure is complex and causes a large amount of\nHindawi Computational Intelligence and Neuroscience Volume 2022, Article ID 4316812, 10 pages https://doi.org/10.1155/2022/4316812\ncalculation and other problems, and facial expression recognition problems have been studied [6].\nConvolutional Neural Networks (CNNs) are widely used in the \u00a6eld of facial expression recognition. Vu et al. designed a multimodel system, which used four di erent models for video expression recognition: audio model, static expression recognition model, dynamic expression recognition model, and 3D facial feature point classi\u00a6cation model. Sound Net is used in the audiomodel to extract audio features from the video. In the static expression recognition model, 1 InceptionNet and 3 Dense Nets are used to extract expression features from a single video frame. In the dynamic expression recognition model, the Visual Geometry Group (VGG) network combined with the Long Short-Term Memory (LSTM) network is used to extract the temporal features of the video. In the 3D model of face feature point classi\u00a6cation, the extracted face feature points are calculated by Euclidean distance as a feature for classi\u00a6cation [7]. Yang and Zhang have designed a Frame Focusing Attention Network (FAN) for video facial expression recognition, identifying some discriminative frames, and highlighting them in an end-to-end framework. e network is divided into feature embedding and frame attention modules. e feature embedding module is a deep CNN. It embeds face images into feature vectors. e frame attention module learns multiple attention weights. ese weights are used to adaptively aggregate feature vectors to form a single discriminative video representation [8]. Hariri has proposed a dual-modal fusion model. e model is divided into two parts: the face image and the audio model. In the face image model, four di erent CNNs are used for expression feature extraction, and the extracted features are input into bidirectional LSTM for temporal feature extraction [9]. In the audio model, two di erent methods are used to extract supplementary information from the audio. Finally, the fusion stage uses a grid search strategy to optimize the model\u2019s performance. ese listed works all have some problems, including more models, which consume a lot of time and resources for training and recognition, and the recognition rate is also low.\nBased on improving the facial recognition accuracy of current convolutional networks, this study presents a convolutional computational neural network model for the fusion of multiple large-scale information. First, the network structure is optimized and improved. e feature maps output by di erent convolutional layers are fused at multiple scales. Second, the loss function is improved. More e\u00aacient regularization strategies are introduced. More discriminative expression features are extracted. Aiming at the problems of randomness, noise, and insu\u00aacient discriminative features of facial expressions in the feature extraction process of CNN, a multiattention mechanism facial expression recognition algorithm is proposed. By adding a variety of improved attention modules to the network, the adaptive extraction of key feature information related to the expression recognition task is further enhanced to enhance the discriminability of expression features further, and improvements and innovations are made on top of this. e innovation is that, considering that the face information\nintercepted in the video is easily a ected by the external environment, the attention module is introduced, and the calculation amount is smaller than the existing model. A good recognition accuracy rate is achieved. ese innovations open the way for follow-up research. e overall architecture is shown in Figure 1."
        },
        {
            "heading": "2. Materials and Methods",
            "text": "2.1. Supervised Learning. Usually, machine learning is used to deal with computer vision optimization problems and can use the mapping function f. Its relevant parameter is \u03b8, as shown in the following equation:\nf\u03b8: X\u27f6 Y. (1)\nIn (1), X is the inlet gap, and Y is the outlet gap. As a visual recognition task in what is essentially a multiclass graph partitioning problem, the entry space is a set of twodimensional pixels. An exit space is a set of labels or targets. Problem object-speci\u00a6c \u00a6xed-type ensembles are dispatched to all entries [10]. Morphologically, the goal of a supervised learning system is to approximate the mapping function f. A predictionmodel is established based on the training dataset, and all inputs are related to a certain token [11]. Assuming that there are n samples, the corresponding training dataset is shown as follows:\nxi, yi( ), . . . , xn, yn( ){ }. (2)\nIn (2), (xi, yi) \u2208 X \u00d7 Y, i \u2208 1, . . . , n{ }, and the target of supervised learning is in every sample of the training dataset."
        },
        {
            "heading": "Conclusion",
            "text": ""
        },
        {
            "heading": "Research results",
            "text": ""
        },
        {
            "heading": "Face image preprocessing",
            "text": "In the space of function f, make the loss function R(y\u0302i, yi) be the smallest f\u2217 are looking for, as shown in the following equation:\nf\u2217 \u2248 argmin\ufe38 \ufe37\ufe37 \ufe38 f\u2208F\n1 n \u2211 n\ni 1 R f xi, \u03b8( ), yi( ). (3)\ne loss function R measures that the predicted labels y\u0302i f(xi, \u03b8) are inconsistent with the true labels yi. A typical supervised learning process is shown in Figure 2. e performance of the model is generally re ected in the accuracy rate. e accuracy rate is the percentage of reasonably divided data samples in all data analysis samples [12]. Here, however, the focus is on the performance of the predictive model on previously observed training data or its ability to generalize on the test dataset [13].\n2.2. Loss Function. In the classic multiclass image classi\u00a6cation problem, the mapping function f\u03b8: X\u27f6 Y is generally used. In each class, the input space is re ected as a likelihood distribution y\u0302i \u2208 Y. e probability distribution also represents the actual labels of the dataset yi. For the predicted labels y\u0302i of any sample in the given dataset and the corresponding actual labels yi, the parameter \u03b8 is optimized by the objective function such that the two probability distributions y\u0302i are similar to yi [14]. e method of measuring the dissimilarity between two probability distributions is called the Kullback-Leibler (KL) divergence [15]. KL divergence, also known as relative entropy, is shown as follows:\nKL yi\u2016y\u0302i( ) \u2211 k\nk 1 yiklog yik y\u0302ik . (4)\nIn the above equation, yik 1 if xi belongs to the k-th class and 0 otherwise. y\u0302ik represents the predicted probability of the input sample entering the k-th class. K is the number of classes. It can also be rewritten, as shown in the following equation:\nKL yi\u2016y\u0302i( ) \u2211 k yiklogyik\u2212\u2211 k yiklogy\u0302ik \u2212H yi( )+H yi,y\u0302i( ).\n(5)\nIn (5), H(yi, y\u0302i) is called cross-entropy or negative loglikelihood. e dissimilarity between the predicted label and the true label is measured as\nH yi, y\u0302i( ) \u2212\u2211 k yiklogy\u0302ik. (6)\nIn a supervised learning paradigm, minimize H(yi, y\u0302i) using real labels and model f\u2217 with no similarity between the predictions.\n2.3. CNN. e core technology of CNN is convolution \u00a6ltering. It performs local feature convolution with the input information, and the obtained feedback also manifests as local features [16]. An example of a small region of the input image being convolved is shown in Figure 3.\nIn the ith \u00a6lter f\u2217, i \u2208 1, 2, . . . , Nc{ }, and the response parameter is \u03b8fi which is convolved with the image patch pixel value X. e response is saved as the \u00a6lter response f(\u03b8fi, X). Likewise, the \u00a6lter is moved along the local extent of the input image to create a 2D \u00a6ltered output. Di erent \u00a6lters are also applied to the input image to build a convolutional layer withNc channels. e convolution output is passed through an activation function such as the recti\u00a6ed linear unit (ReLU) to extract the hidden nonlinear feature data [17]. e advantages of convolution operations in CNNs are twofold: parameter sharing and connection sparsity. Convolutional \u00a6lters or feature detectors are applied to di erent regions of the image; that is, all parts of the image share the parameters of the ith \u00a6lter. Furthermore, the output value of the \u00a6lter only depends on a small number of input values. is situation results in sparse connections between input and output. erefore, the designed CNN contains fewer parameters than the equivalent Deep Neural Network (DNN). Multiple convolutional layers are stacked sequentially, and a deep CNN is constructed. At the heart of this network is a deep CNN hierarchical manager that automatically learns complex image features, resulting in deep\nfeatures as high-level representations that encode the abstract semantics of the data. en, a trainable linear unit (fully connected layer) classi\u00a6es the resulting deep feature vectors using a speci\u00a6c loss function. A large corpus of labeled data is used to learn powerful visual features to enable deep CNNs to abstract well on real data. Convolutional networks learn facial features at many di erent levels of abstraction, from small edges to very complex features such as nose, eye, and mouth features. e deep features are mixed with fully connected linear units of the last convolutional layer. e loss function then classi\u00a6es the resulting deep features and makes predictions based on a \u00a6xed set of classes.\n2.4. Introduction toVGGNetModel. Visual Geometry Group Network (VGGNet) contains several di erent levels of network models. VGGNet16 is chosen as the backbone network and improved [18]. VGGNet16 is structurally improved to make the network more adaptable to the actual needs of facial expression recognition tasks [19]. e overall network structure is shown in Figure 4.\nIn Figure 4, in the improved model, multiscale feature fusion at di erent levels of network width and depth is successfully established without a ecting the depth of the network [20]. In VGGNet16, since the parameters are mainly gathered on several fully connected layers at the end of the original network, all connected layers in the original network are replaced by the global average pooling method layer. A direct connection is formed between the type of the expression label in the dataset and the output type by setting a fully connected layer with a node of 7. Finally, the design results enter the Softmax classi\u00a6er to obtain the best probability of each type and output the results [21].\nIn CNN, the number of parameters in the network is usually used as the evaluation index of the complexity of the network. e calculation of the parameter quantity in the network is shown as follows:\nS K2 \u00d7 I \u00d7 O. (7)\nIn the above equation, K is the size of the convolution kernel, and I and O are the numbers of input and output channels of the feature map, respectively.\n2.5. Multiscale Feature Fusion Strategy. e feature maps of Cony2 and Cony3 before the input pooling layer in the convolution module are extracted. ey are used as a branch feature map and VGGNet16 together with the feature map provided by the last layer of convolution module Cony5 for more scale features. Finally, the fused feature maps are integrated, and path dimensionality reduction is performed through a 1\u00d7 1 convolutional layer [22].\nIn this experiment, the stochastic gradient descent algorithm updates the network parameters. e momentum is set to 0.9, the batch scale is set to 32, and the initial learning rate is set to 0.01. Subsequent experiments use an exponential decay strategy to adjust the learning rate dynamically. e learning rate decay coe\u00aacient is set to 0.9."
        },
        {
            "heading": "2.6. Experimental Dataset",
            "text": "2.6.1. Fer2013 Dataset. e Fer2013 dataset is a public facial expression dataset provided by the 2013 Kaggle Facial Expression Recognition Challenge. e Fer2013 dataset contains di erent states of people. e face recognition accuracy on this dataset is 65\u00b1 5%. It is challenging to use this dataset for facial expression recognition.\n2.6.2. CK+ Dataset. e Extended Cohn-Kanade Dataset (CK+) was collected and proposed by Lucey in 2010 and extended based on Cohn-Kanade Dataset (CK). It is the most widely used facial expression dataset captured under controlled laboratory conditions at present [23]. In order to be compatible with the seven basic expressions in the Fer2013 dataset, contempt expressions with a small sample size were removed in this experiment, and 3 to 5 frames were intercepted from each image sequence as expression samples in this experiment [24]. Finally, the obtained data samples\nare randomly divided into a training set and a test set according to the ratio of 9 :1."
        },
        {
            "heading": "2.7. Face Image Preprocessing",
            "text": "2.7.1. Face Detection. In addition to the human face, the facial expression images in the given dataset also include nonface regions such as the background [25]. erefore, the input image is preprocessed before being fed into the CNN. e detected face regions are cropped and saved [15]. e valid images obtained after the face images in the dataset are preprocessed as shown in Figure 5.\nIn Figure 5, the existence of a face is determined by multiple stacking of face candidate windows of di erent scales to reduce the false detection rate of a face. e scale of the detected image is normalized by the bilinear interpolation algorithm. A grayscale image of 224\u00d7 224 pixels is output and saved [26].\n2.7.2. Data Enhancement. When the sample size is seriously insu\u00aacient, it is necessary to arti\u00a6cially increase the sample size. Data augmentation is used to augment the dataset samples. Without adding additional image samples, many completely new image samples are generated without changing the sample label category [27]. e data enhancement e ect is shown in Figure 6.\nIn Figure 6, in order to improve the anti-interference and reliability of the model, each pixel of the data is mirrored and expanded to three times the number of original samples. e resulting pixels are then angularly ipped. e rotation angle range is set to \u00b1100. After that, ip every \u00a6fty degrees. Next, the pixels are normalized. e pixel size is set to 224\u00d7 224, which enlarges the number of samples by 15 times.\n2.8. Facial Expression Recognition Algorithm Based on MultiattentionMechanism. In CNN, it is generally assumed that all position information on the feature map is equally important. But this does not necessarily give great results when extracting image features. is is because the content on each face image is not the same, and di erent tasks focus on the image content di erently, focusing on the extraction of facial expression features. erefore, di erent positions of the input image are given the same weight indiscriminately. is will not only extract the facial expression feature information but also extract a lot of redundant backgroundnoise information, which will eventually a ect the recognition accuracy. e attentionmechanism is introduced into thenetworkmodel so that the network suppresses redundant information and enhances important information when extracting features, making the expression recognition results more accurate. e attention mechanism originated from the study of the human visual system. Human vision quickly scans global information to obtain target areas that need to be focused on. is area is also known as the focus of attention. en, more attention resources are allocated to this area to obtain more detailed information about the relevant target. e above mechanism is often referred to as the attention mechanism,\nwhich is a signal processing mechanism unique to human vision. In the case of limited information processing resources, this mechanism can selectively focus on speci\u00a6c parts within the visual range, capture the most discriminative visual information, and improve the e\u00aaciency and accuracy of information processing.\nInspired by the human visual attention mechanism, many researchers have begun to try to introduce the attention mechanism into the neural network so that the computer can also strengthen the attention to the key information like humans. At present, the attention mechanism has been widely used in various types of deep learning tasks, such as natural language processing, image recognition, and speech recognition. In CNN, attention modules are usually generic and can be embedded into existing network architectures to obtain more discriminative features by assigning di erent weights to di erent regions of the feature map. According to the di erent forms of attention, attention is divided into soft attention and hard attention. After generating the attention weights, hard attention will set a part of the unquali\u00a6ed weights to 0 and no longer pay attention to this part of redundant information that is not related to the current task. Soft attention avoids \u00a6ltering data and calculates attention weights on all data.\n2.9. Channel Attention Mechanism. e input image is initially represented by three channels: Red (R), Green (G), and Blue (B). Each convolution kernel extracts di erent features from the input image and extracts a set of feature maps with the number of channels equal to the number of convolution kernels. e features of each channel represent the components of the image on di erent convolution kernels. e features of di erent channels have di erent degrees of in uence on key information. e channel attention mechanism is used to automatically acquire channel features that are more critical to the current task. e Squeeze-and-Excitation Networks (SENet) model mainly focuses on the feature channel perspective. e importance of each feature channel can be obtained, and then the learned features can be weighed by processing by using the interdependence between the explicit model feature channels. e SE module mainly includes two operations: squeeze and excitation. e SENet model structure is shown in Figure 7.\nIn Figure 7, convolution can only be applied to local space, so it is di\u00aacult to obtain su\u00aacient information to extract the correlation between channels. erefore, in the SEmodule, all input images are preprocessed by a pooling layer of global spatial average, and the representationof eachchannel isbased on theglobal spatial characteristics of eachchannel to establish a one-dimensional channel descriptor that contains the global receptive \u00a6eld information of each channel to some extent. Assuming that the input feature map is U RH\u00d7W\u00d7C, the channel descriptor z \u2208 R1\u00d71\u00d7C is obtained after the pooling layer. en, the c-th output of z is shown as follows:\nzc 1 H \u00d7W \u2211 H i 1 \u2211 W j 1 uc(i, j). (8)\nIn the above equation, uc is the output of the c-th convolution kernel after the feature map U undergoes a standard convolution. After compression, the global description features of each channel are obtained. en, the dependencies between di erent channels are obtained through the activation operation. A network layer with parameter w is used to generate weights for all feature channels. e speci\u00a6c process is as follows: Firstly, the compressed output will go through a fully connected layer with c/r nodes. r is a scaling parameter used to reduce the number of channels and the amount of computation. e ReLU function is used to add nonlinear transformations. en, a fully connected layer with the number of nodes c is used to restore the original dimension. Finally, after the sigmoid function is activated, the learned weights of each channel are obtained. In order to output the feature map adjusted by the SE module, the weight value output by the activation process is regarded as the importance of each feature channel. e weights of each channel are multiplied by the previous features to recalibrate the input features. e SE module is lightweight and consists of two fully connected layers and a global average pooling layer, which increases the sensitivity of the model to channel features while only increasing the number of parameters and computation, resulting in signi\u00a6cant performance improvements.\n2.10. Spatial Attention Mechanism. In order to \u00a6nd the spatial structure attention, the importance of each position in the graph is learned at the spatial structure level. e feature map generated by the \u00a6rst channel attention modeling is used as the entry feature map and the global mean pooling method according to the channel level and operation of the global max pooling method. Afterwards, these two feature maps are combined in series. A standard convolutional layer with a kernel length of 7\u00d7 7 is used to perform dimensionality reduction of the spatial structure channel. Sigmoid function activations are used to form a two-dimensional attention map. en, the spatial structure attention map and the feature map of the entrance are calculated by the Hadamard product to obtain the feature map of the \u00a6nal output. e structure of the spatial attention module in the model is shown in Figure 8.\nAssume that the input feature map is F. en, the calculation of the spatial attention map is shown as follows:\nMs(F) \u03c3 f 7\u00d77 FSavg;F S max[ ]( )( ), (9)"
        },
        {
            "heading": "F (W): 1\u00d71\u00d7CX: C\u00d7H\u00d7W",
            "text": "where f7\u00d77 indicates the convolution operation, the size of the convolution kernel is 7\u00d7 7, \u03c3 indicates the sigmoid activation function, and the dimensions of the intermediate feature maps are all H\u00d7W\u00d7 1."
        },
        {
            "heading": "3. Results and Discussion",
            "text": "3.1. Comparison of Loss Functions. On a mixed dataset consisting of CK+ and Fer2013, more extensive experiments are carried out to evaluate the properties of the provided loss functions. e Delivered Duty Unpaid (DDU) loss function is introduced. It has better properties compared to baseline loss functions (i.e., Softmax economic loss and center economic loss). Secondly, the resulting DDU loss is evaluated in the hybrid dataset and in the two large Fer datasets in a variety of better ways. Figure 9 shows the di erence between intra- and interclass distances under various loss functions; the smaller the intraclass distance deviation, the larger the interclass distance and the better the performance of the loss function.\nIn Figure 9, due to the increase of the c-like value, the contribution of the DDU loss also increases accordingly, and the spacings embedded in the voids are also larger and larger. Feature groups are usually compact and well segmented. When using the hyperparameter c, feature clusters are biased away from the set of other features. erefore, DDU loss can be used for facial expression recognition in complex environments. e DDU loss implicitly pushes the deep features of a class from other classes to the corresponding class centers in the embedding space. Under the joint supervision of Softmax loss and center loss, DDU loss has extremely uneven data distribution in the embedding space. is e ectively distinguishes feature clusters from the majority and minority classes.\n3.2. Validation of Attention Mechanism. e comparative experiments are carried out by adding di erent categories of attention modules to the multiscale feature fusion network model proposed above, which are as follows: (1)M-VGGNet, a multiscale feature fusion network model that does not introduce an attention mechanism; (2) MCA-VGGNet, where only the network model of channel attention (CA) mechanism is introduced; (3) MSA-VGGNet, a network model that only introduces the spatial attention (SA) mechanism; and (4) MCSA-VGGNet, where the spatial channel attention (CSA) mechanism is introduced into the network model. e comparative experimental results are shown in Figure 10.\nIn Figure 10, after adding the attention module, the overall recognition accuracy of the network on the Fer2013 and CK+datasets has been improved to varying degrees. Among them, the e ect of adding a channel attention module to the recognition accuracy is more obvious than that of the spatial attentionmodule. e experimental results showed that the superimposed use of two attention modules in MCSA-VGGNet can signi\u00a6cantly improve recognition accuracy. e network extracts the feature information more relevant to the facial expression recognition task under the\ncombined action of the channel and space dimensions so that the obtained expression features have stronger discriminability. e experimental results demonstrate the e ectiveness of the joint use of the two attention modules.\nIn order to test the enhancement e ect of the model on the recognition rate of various expressions after introducing the attention mechanism, three more obvious expressions are selected. Angry, happy, and sad are used as contrasting expressions. e comparison results on the two datasets are shown in Table 1.\nAccording to Table 1, the comparison of the recognition accuracy of various expressions on the two datasets by the model with di erent attention modules is drawn, as shown in Figure 11.\nIn Figure 11, after two attention modules are added to the network, the recognition accuracy of various expressions on the two datasets has been improved to a certain extent. is shows that the addition of the attention module enables the network to increase the attention to error-prone samples and extract more discriminative expression features, thereby improving the overall recognition accuracy of the model. However, expressions such as happy and angry, which have already achieved a high recognition rate, have no obvious improvement e ect. is is mainly because such expression features are highly recognizable, and the features are easy to be extracted, even if the attention mechanism is not introduced. e network can still extract enough discriminative features to classify such expressions correctly. e validation set of the Fer2013 model consists of 50 images obtained from the network. Each image varies in size and clarity. e expressions in the validation set cover the seven basic expression types in the dataset. 30 of them are\nrandomly selected as examples. e most obvious facial expressions are angry, sad, and happy, with ten pictures for each emotion. ese face images are input into the model trained by the Fer2013 training set for expression recognition, and the corresponding expression recognition results are obtained, respectively. e data in Figure 12(a) are the average number of 30 image recognitions. e CK+ validation set consists of 50 face expression images randomly intercepted from the dataset that did not participate in the training process. e selected face image also contains seven basic expression types, and the recognition results on this dataset are shown in Figure 12(b).\nIn Figure 12, the recognition rate of expressions with relatively exaggerated and large movements is quite high, and the accuracy rate can even reach more than 98%. e recognition rate of facial expressions that are not suitable for showing traces is not ideal, so the later technical research direction should be closer to such expressions."
        },
        {
            "heading": "4. Conclusions",
            "text": "Due to the rapid development of computers, people have put forward higher demands on the intelligence level of HCI. Realizing the correct recognition of facial expressions by computers in HCI has become a current research trend.,is study provides a facial expression recognition algorithm based on multiscale feature fusion technology, constructs a feature fusion network in dimensions, and provides a facial expression recognition algorithm based on a multiattention mechanism. Experiments are performed on the Fer2013 and CK+datasets. ,e experimental results show that the algorithm effectively improves the accuracy of model recognition. Although this study has achieved certain results, there are still shortcomings. ,ere is still a certain gap between the used dataset and the real scene, and the dataset should be closer to the real scene. Although the network structure of VGGNet has been improved to a certain extent, there are still many parameters in the improved network. ,e structure of the added attention module needs to be improved, and the ways of adding various attention mechanisms need to be explored. Additionally, fewer datasets are used. In the future, the latest datasets will be added, such as AffectNet, Ascertain, and Emoti. Network parameters will also be increased to explore more ways to add attention mechanisms, adding more datasets for model performance testing. ,is study expects optimizing the expression recognition technology further. Follow-up research will also be combined with the current development of Coronavirus Disease 2019, adding research on facial mask recognition."
        },
        {
            "heading": "Data Availability",
            "text": ",e data used to support this study are available from the corresponding author upon request."
        },
        {
            "heading": "Conflicts of Interest",
            "text": ",e author declares that there are no conflicts of interest."
        },
        {
            "heading": "Acknowledgments",
            "text": ",is work was supported by School Level Teaching Team, \u201c1 +X\u201d Python teaching team (Project no. TD2021002)."
        }
    ],
    "title": "Emotion Recognition of Online Education Learners by Convolutional Neural Networks",
    "year": 2022
}