{
    "abstractText": "We consider simple stochastic games G with energy-parity objectives, a combination of quantitative rewards with a qualitative parity condition. The Maximizer tries to avoid running out of energy while simultaneously satisfying a parity condition. We present an algorithm to approximate the value of a given configuration in 2-NEXPTIME. Moreover, \u03b5-optimal strategies for either player require at most O ( 2-EXP (|G|) \u00b7 log ( 1",
    "authors": [
        {
            "affiliations": [],
            "name": "Mohan Dantam"
        },
        {
            "affiliations": [],
            "name": "Richard Mayr"
        }
    ],
    "id": "SP:1b6fbc1f3151db6839c23392be7029aca647038e",
    "references": [
        {
            "authors": [
                "Rajeev Alur",
                "Thomas A Henzinger",
                "Orna Kupferman"
            ],
            "title": "Alternating-time temporal logic",
            "venue": "Journal of the ACM,",
            "year": 2002
        },
        {
            "authors": [
                "Patrick Billingsley"
            ],
            "title": "Probability and measure",
            "year": 2008
        },
        {
            "authors": [
                "T. Br\u00e1zdil",
                "V. Bro\u017eek",
                "K. Etessami",
                "A. Ku\u010dera"
            ],
            "title": "Approximating the Termination Value of One-Counter MDPs and Stochastic Games",
            "venue": "Information and Computation,",
            "year": 2013
        },
        {
            "authors": [
                "T. Br\u00e1zdil",
                "A. Ku\u010dera",
                "P. Novotn\u00fd"
            ],
            "title": "Optimizing the expected mean payoff in energy Markov decision processes",
            "venue": "In International Symposium on Automated Technology for Verification and Analysis (ATVA),",
            "year": 2016
        },
        {
            "authors": [
                "Tom\u00e1s Br\u00e1zdil",
                "V\u00e1clav Brozek",
                "Kousha Etessami"
            ],
            "title": "One-Counter Stochastic Games",
            "venue": "In Kamal Lodaya and Meena Mahajan, editors, IARCS Annual Conference on Foundations of Software Technology and Theoretical Computer Science (FSTTCS 2010),",
            "year": 2010
        },
        {
            "authors": [
                "Krishnendu Chatterjee",
                "Laurent Doyen"
            ],
            "title": "Energy parity games",
            "venue": "In International Colloquium on Automata, Languages and Programming (ICALP),",
            "year": 2010
        },
        {
            "authors": [
                "Krishnendu Chatterjee",
                "Laurent Doyen"
            ],
            "title": "Energy and mean-payoff parity Markov decision processes",
            "venue": "In International Symposium on Mathematical Foundations of Computer Science (MFCS),",
            "year": 2011
        },
        {
            "authors": [
                "Krishnendu Chatterjee",
                "Laurent Doyen"
            ],
            "title": "Games and Markov decision processes with mean-payoff parity and energy parity objectives",
            "venue": "In Mathematical and Engineering Methods in Computer Science (MEMICS),",
            "year": 2011
        },
        {
            "authors": [
                "Krishnendu Chatterjee",
                "Laurent Doyen",
                "Hugo Gimbert",
                "Youssouf Oualhadj"
            ],
            "title": "Perfectinformation stochastic mean-payoff parity games",
            "venue": "In International Conference on Foundations of Software Science and Computational Structures (FoSSaCS),",
            "year": 2014
        },
        {
            "authors": [
                "Krishnendu Chatterjee",
                "Thomas A. Henzinger",
                "Nir Piterman"
            ],
            "title": "Generalized parity games",
            "venue": "editor, International Conference on Foundations of Software Science and Computational Structures (FoSSaCS),",
            "year": 2007
        },
        {
            "authors": [
                "Krishnendu Chatterjee",
                "Marcin Jurdzi\u0144ski",
                "Thomas A. Henzinger"
            ],
            "title": "Simple stochastic parity games",
            "venue": "In Computer Science Logic (CSL), volume 2803 of LNCS,",
            "year": 2003
        },
        {
            "authors": [
                "Krishnendu Chatterjee",
                "Marcin Jurdzi\u0144ski",
                "Thomas A. Henzinger"
            ],
            "title": "Quantitative stochastic parity games",
            "venue": "In ACM-SIAM Symposium on Discrete Algorithms (SODA),",
            "year": 2004
        },
        {
            "authors": [
                "Anne Condon"
            ],
            "title": "The complexity of stochastic games",
            "venue": "Information and Computation,",
            "year": 1992
        },
        {
            "authors": [
                "Laure Daviaud",
                "Martin Jurdzi\u0144ski",
                "Ranko Lazi\u0107"
            ],
            "title": "A pseudo-quasi-polynomial algorithm for mean-payoff parity games",
            "venue": "In Logic in Computer Science (LICS),",
            "year": 2018
        },
        {
            "authors": [
                "David L. Dill"
            ],
            "title": "Trace theory for automatic hierarchical verification of speed-independent circuits, volume 24",
            "venue": "MIT press Cambridge,",
            "year": 1989
        },
        {
            "authors": [
                "Dean Gillette"
            ],
            "title": "Stochastic games with zero stop probabilities",
            "venue": "Contributions to the Theory of Games,",
            "year": 1957
        },
        {
            "authors": [
                "Hugo Gimbert",
                "Florian Horn"
            ],
            "title": "Solving simple stochastic tail games",
            "venue": "In ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 847\u2013862,",
            "year": 2010
        },
        {
            "authors": [
                "Hugo Gimbert",
                "Edon Kelmendi"
            ],
            "title": "Submixing and shift-invariant stochastic games. 2022",
            "year": 2022
        },
        {
            "authors": [
                "Hugo Gimbert",
                "Youssouf Oualhadj",
                "Soumya Paul"
            ],
            "title": "Computing optimal strategies for Markov decision processes with parity and positive-average conditions",
            "venue": "URL: https: //hal.science/hal-00559173/en/",
            "year": 2011
        },
        {
            "authors": [
                "Marcin Jurdzi\u0144ski"
            ],
            "title": "Deciding the winner in parity games is in UP \u2229 co-UP",
            "venue": "Information Processing Letters,",
            "year": 1998
        },
        {
            "authors": [
                "A. Maitra",
                "W. Sudderth"
            ],
            "title": "Stochastic games with Borel payoffs. In Stochastic Games and Applications, pages 367\u2013373",
            "venue": "Kluwer, Dordrecht,",
            "year": 2003
        },
        {
            "authors": [
                "Donald A. Martin"
            ],
            "title": "The determinacy of Blackwell games",
            "venue": "Journal of Symbolic Logic,",
            "year": 1998
        },
        {
            "authors": [
                "Richard Mayr",
                "Sven Schewe",
                "Patrick Totzke",
                "Dominik Wojtczak"
            ],
            "title": "MDPs with Energy-Parity Objectives",
            "venue": "In Logic in Computer Science (LICS). IEEE,",
            "year": 2017
        },
        {
            "authors": [
                "Richard Mayr",
                "Sven Schewe",
                "Patrick Totzke",
                "Dominik Wojtczak"
            ],
            "title": "Simple stochastic games with almost-sure energy-parity objectives are in NP and coNP",
            "venue": "In Proc. of Fossacs,",
            "year": 2021
        },
        {
            "authors": [
                "Jo\u00ebl Ouaknine",
                "James Worrell"
            ],
            "title": "On linear recurrence sequences and loop termination",
            "venue": "ACM SIGLOG News,",
            "year": 2015
        },
        {
            "authors": [
                "Jakob Piribauer"
            ],
            "title": "On non-classical stochastic shortest path problems",
            "venue": "PhD thesis, Technische Universita\u0308t Dresden, Germany,",
            "year": 2021
        },
        {
            "authors": [
                "Jakob Piribauer",
                "Christel Baier"
            ],
            "title": "On Skolem-Hardness and Saturation Points in Markov Decision Processes",
            "venue": "Proc. of ICALP, volume 168 of LIPIcs,",
            "year": 2020
        },
        {
            "authors": [
                "Jakob Piribauer",
                "Christel Baier"
            ],
            "title": "Positivity-hardness results on Markov decision processes. 2023",
            "year": 2023
        },
        {
            "authors": [
                "Amir Pnueli",
                "Roni Rosner"
            ],
            "title": "On the synthesis of a reactive module",
            "venue": "In Annual Symposium on Principles of Programming Languages (POPL),",
            "year": 1989
        },
        {
            "authors": [
                "Peter J. Ramadge",
                "W. Murray Wonham"
            ],
            "title": "Supervisory control of a class of discrete event processes",
            "venue": "SIAM journal on control and optimization,",
            "year": 1987
        },
        {
            "authors": [
                "W. Zielonka"
            ],
            "title": "Infinite games on finitely coloured graphs with applications to automata on infinite trees",
            "venue": "Theoretical Computer Science,",
            "year": 1998
        }
    ],
    "sections": [
        {
            "text": "We present an algorithm to approximate the value of a given configuration in 2-NEXPTIME. Moreover, \u03b5-optimal strategies for either player require at most O ( 2-EXP (|G|) \u00b7 log ( 1 \u03b5 )) memory modes.\n2012 ACM Subject Classification Computing methodologies \u2192 Stochastic games\nKeywords and phrases Energy-Parity Games, Simple Stochastic Games, Parity, Energy\nDigital Object Identifier 10.4230/LIPIcs.MFCS.2023.36\nRelated Version Full version of a paper presented at MFCS 2023.:\n1 Introduction\nBackground. Simple stochastic games (SSGs) are 2-player turn-based perfect information stochastic games played on finite graphs. They are also called competitive Markov decision processes [20], or 2 12 -player games [13, 12]. Introduced by Shapley [36] in 1953, they have since played a central role in the solution of many problems, e.g., synthesis of reactive systems [35, 34] and formal specification and verification [17, 18, 1]. Every state either belongs to one of the players (Maximizer or Minimizer) or is a random state. In each round of the game the player who owns the current state gets to choose the successor state along the game graph. For random states the successor is chosen according to a predefined distribution. Given a start state and strategies of Maximizer and Minimizer, this yields a distribution over induced infinite plays. We consider objectives O that are measurable subsets of the set of possible plays, and the players try to maximize (resp. minimize) the probability of O.\nMany different objectives for SSGs have been studied in the literature. Here we focus on parity, mean-payoff and energy objectives. We assign numeric rewards to transitions and priorities (aka colors), encoded by bounded non-negative numbers, to states. A play satisfies the (min-even) parity objective iff the minimal priority that appears infinitely often in a play is even. It subsumes all \u03c9-regular objectives, and in particular safety, liveness, fairness, etc. On finite SSGs, the parity objective can be seen as a special case of the mean-payoff objective which requires the limit average reward per transition along a play to be positive (or non-negative). Mean-payoff objectives in SSGs go back to a 1957 paper by Gillette [21] and have been widely studied, due to their relevance for efficient control. The energy objective [6] requires that the accumulated reward at any time in a play stays above some finite threshold. The intuition is that a controlled system has some finite initial energy level that must never become depleted. Since the accumulated reward is not bounded a-priori, this essentially turns a finite-state game into an infinite-state one.\nEnergy-parity. We consider SSGs with energy-parity objectives, where plays need to \u00a9 Mohan Dantam and Richard Mayr; licensed under Creative Commons License CC-BY 4.0\n48th International Symposium on Mathematical Foundations of Computer Science (MFCS 2023). Editors: J\u00e9r\u00f4me Leroux, Sylvain Lombardy, and David Peleg; Article No. 36; pp. 36:1\u201336:18\nLeibniz International Proceedings in Informatics Schloss Dagstuhl \u2013 Leibniz-Zentrum f\u00fcr Informatik, Dagstuhl Publishing, Germany\nar X\niv :2\n30 7.\n05 76\n2v 1\n[ cs\n.G T\n] 1\n1 Ju\nl 2 02\n3\nsatisfy both an energy and a parity objective. The parity objective specifies functional correctness, while the energy condition can encode efficiency or risk considerations, e.g., the system should not run out of energy since manually recharging would be costly or risky.\nPrevious work. Much work on combined objectives for stochastic systems is restricted to Markov decision processes (MDPs) [8, 9, 4, 28].\nFor (stochastic) games, the computational complexity of single objectives is often in NP \u2229 coNP, e.g., for parity or mean-payoff objectives [25]. Multi-objective games can be harder, e.g., satisfying two different parity objectives leads to coNP completeness [11].\nStochastic mean-payoff parity games can be solved in NP \u2229 coNP [10]. However, this does not imply a solution for stochastic energy-parity games, since, unlike in the non-stochastic case [7], there is no known reduction from energy-parity to mean-payoff parity in stochastic games. The reduction in [7] relies on the fact that Maximizer has a winning finite-memory strategy for energy-parity, which does not generally hold for stochastic games, or even MDPs [28]. For the same reason, the direct reduction from stochastic energy-parity to ordinary energy games proposed in [8, 9] does not work for general energy-parity but only for energy-B\u00fcchi; cf. [28].\nNon-stochastic energy-parity games can be solved in NP \u2229 coNP (and even in pseudoquasi-polynomial time [16]) and Maximizer strategies require only finite (but exponential) memory [7].\nStochastic energy-parity games have been studied in [29], where it was shown that the almost-sure problem is decidable and in NP \u2229 coNP. That is, given an initial configuration (control-state plus current energy level), does Maximizer have a strategy to ensure that energy-parity is satisfied with probability 1 against any Minimizer strategy? Unlike in many single-objective games, such an almost-surely winning Maximizer strategy (if it exists) requires infinite memory in general. This holds even in MDPs and for energy-coB\u00fcchi objectives [28].\nHowever, [29] did not address quantitative questions about energy-parity objectives, such as computing/approximating the value of a given configuration, or the decidability of exact questions like \u201cIs the value of this configuration \u2265 k ?\u201d for some constant k (e.g., k = 1/2).\nThe decidability of the latter type of exact question about the energy-parity value is open, but there are strong indications that it is very hard. In fact, even simpler subproblems are already at least as hard as the positivity problem for linear recurrence sequences, which in turn is at least as hard as the Skolem problem [19]. (The decidability of these problems has been open for decades; see [30] for an overview.) Given an SSG with an energy-parity objective, suppose we remove the parity condition (assume it is always true) and also suppose that Maximizer is passive (does not get to make any decisions). Then we obtain an MDP where the only active player (the Minimizer in the SSG) has a termination objective, i.e., to reach a configuration where the energy level is \u2264 0. Exact questions about the value of the termination objective in MDPs are already at least as hard as the positivity problem [31, Section 5.2.3] (see also [32, 33]). Thus exact questions about the energy-parity value in SSGs are also at least as hard as the positivity problem.\nOur contributions. Since exact questions about the energy-parity value in SSGs are positivity-hard, we consider the problem of computing approximations of the value. We present an algorithm that, given an SSG G and error \u03b5, computes \u03b5-close approximations of the energy-parity value of any given configuration in 2-NEXPTIME. Moreover, we show that \u03b5-optimal Maximizer (resp. Minimizer) strategies can be chosen as deterministic and using only finite memory with O ( 2-EXP (|G|) \u00b7 log ( 1 \u03b5 )) memory modes. One can understand the idea as a constructive upper bound on the accuracy with which the players need to remember\nthe current energy level in the game. (This is in contrast to the result in [28] that almostsurely winning Maximizer strategies require infinite memory in general.) Once the upper bound on Maximizer\u2019s memory for \u03b5-optimal strategies is established, one might attempt a reduction from energy-parity to mean-payoff parity, along similar lines as for non-stochastic games in [7]. However, instead we use a more direct reduction from energy-parity to parity in a derived SSG for our approximation algorithm.\n2 Preliminaries\nA probability distribution over a countable set S is a function f : S \u2192 [0, 1] with \u2211\ns\u2208S f(s) = 1. supp(f) def= {s| f(s) > 0} denotes the support of f and D(S) is the set of all probability distributions over S. Given an alphabet \u03a3, let \u03a3\u03c9 and \u03a3\u2217 (\u03a3+) denote the set of infinite and finite (non-empty) sequences over \u03a3, respectively. Elements of \u03a3\u03c9 or \u03a3\u2217 are called words.\nGames, MDPs and Markov chains. A Simple Stochastic Game (SSG) is a finite-state 2-player turn-based perfect-information stochastic game G = (S, (S2, S3, S#), E, P ) where the finite set of states S is partitioned into the states S2 of the player 2 (Maximizer), states S3 of player 3 (Minimizer), and chance vertices (aka random states) S#. Let E \u2286 S \u00d7 S be the transition relation. We write s\u2212\u2192s\u2032 if (s, s\u2032) \u2208 E and assume that Succ(s) def= {s\u2032 | sEs\u2032} =\u0338 \u2205 for every state s. The probability function P assigns each random state s \u2208 S# a distribution over its successor states, i.e., P (s) \u2208 D(Succ(s)). For ease of presentation, we extend the domain of P to S\u2217S# by P (\u03c1s)\ndef= P (s) for all \u03c1s \u2208 S+S#. An MDP is a game where one of the two players does not control any states. An MDP is maximizing (resp. minimizing) iff S3 = \u2205 (resp. S2 = \u2205). A Markov chain is a game with only random states, i.e., S2 = S3 = \u2205.\nStrategies. A play is an infinite sequence s0s1 . . . \u2208 S\u03c9 such that si\u2212\u2192si+1 for all i \u2265 0. A path is a finite prefix of a play. Let Plays (G) def= { \u03c1 = (qi)i\u2208N |qi\u2212\u2192qi+1 } denote the set of all possible plays. A strategy of the player 2 (3) is a function \u03c3 : S\u2217S2 \u2192 D(S) (\u03c0 : S\u2217S3 \u2192 D(S)) that assigns to every path ws \u2208 S\u2217S2 (\u2208 S\u2217S3) a probability distribution over the successors of s. If these distributions are always Dirac then the strategy is called deterministic (aka pure), otherwise it is called randomized (aka mixed). The set of all strategies of player 2 and 3 in G is denoted by \u03a3G and \u03a0G , respectively. A play/path s0s1 . . . is compatible with a pair of strategies (\u03c3, \u03c0) if si+1 \u2208 supp(\u03c3(s0 . . . si)) whenever si \u2208 S2 and si+1 \u2208 supp(\u03c0(s0 . . . si)) whenever si \u2208 S3.\nFinite-memory deterministic (FD) strategies are a subclass of strategies described by deterministic transducers T = (M, m0, upd, nxt) where M is a finite set of memory modes with initial mode m0, upd : M \u00d7 E 7\u2192 M updates the memory mode upon observing a transition and nxt : M \u00d7 S\u2299 7\u2192 S chooses the successor state based on the current memory mode and state. FD strategies without memory (|M| = 1) are called memoryless deterministic (MD). For deterministic strategies, there is no difference between public memory (observable by the other player) and private memory.\nMeasure. A game G with initial state s0 and strategies (\u03c3, \u03c0) yields a probability space (s0S\u03c9, Fs0 , PG\u03c3,\u03c0,s0) where Fs0 is the \u03c3-algebra generated by the cylinder sets s0s1 . . . snS \u03c9 for n \u2265 0. The probability measure PG\u03c3,\u03c0,s0 is first defined on the cylinder sets. For \u03c1 = s0 . . . sn, let PG\u03c3,\u03c0,s0(\u03c1)\ndef= 0 if \u03c1 is not compatible with \u03c3, \u03c0 and otherwise PG\u03c3,\u03c0,s0(\u03c1S \u03c9) def=\u220fn\u22121\ni=0 \u03c4(s0 . . . si)(si+1) where \u03c4 is \u03c3 or \u03c0 or P depending on whether si \u2208 S2 or S3 or S#, respectively. By Carath\u00e9odory\u2019s extension theorem [2], this defines a unique probability measure on the \u03c3-algebra.\nObjectives and Payoff functions. General objectives are defined by real-valued measurable functions. However, we only consider indicator functions of measurable sets.\nMFCS 2023\nHence our objectives can be described by measurable subsets O \u2286 S\u03c9 of plays. The payoff, under strategies (\u03c3, \u03c0), is the probability that plays belong to O.\nWe use the syntax and semantics of the LTL operators [14] F (eventually), G (always) and X (next) to specify some conditions on plays.\nReachability & Safety. A reachability objective is defined by a set of target states T \u2286 S. A play \u03c1 = s0s1 . . . belongs to F T iff \u2203i \u2208 N si \u2208 T . Similarly, \u03c1 belongs to F\u2264nT (resp. F\u2265nT ) iff \u2203i \u2264 n (resp. i \u2265 n) such that si \u2208 T . Dually, the safety objective G T consists of all plays which never leave T . We have G T = \u00acF\u00acT .\nParity. A parity objective is defined via bounded function Col : S \u2192 N that assigns non-negative priorities (aka colors) to states. Given an infinite play \u03c1 = s0s1 . . ., let Inf(\u03c1) denote the set of numbers that occur infinitely often in the sequence Col(s0)Col(s1) . . .. A play \u03c1 satisfies even parity w.r.t. Col iff the minimum of Inf(\u03c1) is even. Otherwise, \u03c1 satisfies odd parity. The objective even parity is denoted by EPAR(Col) and odd parity is denoted by OPAR(Col). Most of the time, we implicitly assume that the coloring function is known and just write EPAR and OPAR. Observe that, given any coloring Col, we have EPAR = OPAR and OPAR(Col) = EPAR(Col + 1) where Col + 1 is the function which adds 1 to the color of every state. This justifies to consider only one of the even/odd parity objectives, but, for the sake of clarity, we distinguish these objectives wherever necessary.\nEnergy/Reward/Counter based objectives. Let r : E \u2192 {\u2212R, . . . , 0, . . . , R} be a bounded function that assigns weights to transitions. Depending on context, the sum of these weights in a path can be viewed as energy, cost/reward or a counter. If s\u2212\u2192s\u2032 and r((s, s\u2032)) = c, we write s c\u2212\u2192 s\u2032. Let \u03c1 = s0 c0\u2212\u2192 s1 c1\u2212\u2192 . . . be a play. We say that \u03c1 satisfies\n1. the k-energy objective EN(k) iff ( k + \u2211n\u22121\ni=0 ci\n) > 0 for all n \u2265 0.\n2. the l-storage condition if l + \u2211n\u22121\ni=m ci \u2265 0 holds for every infix sm cm\u2212\u2192 sm+1 . . . sn of the\nplay. Let ST(k, l) denote the set of plays that satisfy both the k-energy and the l-storage condition. Let ST(k) def= \u22c3 l ST(k, l). Clearly, ST(k) \u2286 EN(k).\n3. k-Termination Term(k) iff there exists n \u2265 0 such that ( k + \u2211n\u22121\ni=0 ci\n) \u2264 0.\n4. Limit objective LimInf( z) iff ( lim infn\u2192\u221e \u2211n\u22121 i=0 ci ) z for \u2208 {<, \u2264, =, \u2265, >} and\nz \u2208 R \u222a {\u221e, \u2212\u221e} and similarly for LimSup( z). 5. Mean payoff MP( c) for some constant c \u2208 R iff ( lim infn\u2192\u221e 1n \u2211n\u22121 i=0 ci ) c. Observe that the objectives k-energy and k-termination are mutually exclusive and cover all of the plays. A different way to consider these objectives is to encode the energy level (the sum of the transition weights so far) into the state space and then consider the obtained infinite-state game with safety/reachability objective, respectively.\nAn objective O is called shift-invariant iff for all finite paths \u03c1 and plays \u03c1\u2032 \u2208 S\u03c9, we have \u03c1\u03c1\u2032 \u2208 O \u21d0\u21d2 \u03c1\u2032 \u2208 O. Parity and mean payoff objectives are shift-invariant, but energy and termination objectives are not. Objective O is called submixing iff for all sequences of finite nonempty words u0, v0, u1, v1 . . . we have u0v0u1v1 . . . \u2208 O =\u21d2 ((u0u1 . . . \u2208 O) \u2228 (v0v1 . . . \u2208 O)).\nDeterminacy. Given an objective O and a game G, state s has value (w.r.t to O) iff\nsup \u03c3\u2208\u03a3G inf \u03c0\u2208\u03a0G PG\u03c3,\u03c0,s(O) = inf \u03c0\u2208\u03a0G sup \u03c3\u2208\u03a3G PG\u03c3,\u03c0,s(O).\nIf s has value then valGO (s) denotes the value of s defined by the above equality. A game with an objective is called weakly determined if every state has value. Stochastic games with Borel objectives are weakly determined [26, 27]. Our objectives above are Borel, hence any boolean combination of them is also weakly determined. For \u03b5 > 0 and state s, a strategy 1. \u03c3 \u2208 \u03a3G is \u03b5-optimal (maximizing) iff PG\u03c3,\u03c0,s(O) \u2265 valGO (s) \u2212 \u03b5 for all \u03c0 \u2208 \u03a0G .\n2. \u03c0 \u2208 \u03a0G is \u03b5-optimal (minimizing) iff PG\u03c3,\u03c0,s(O) \u2264 valGO (s) + \u03b5 for all \u03c3 \u2208 \u03a3G . A 0-optimal strategy is called optimal. An MD strategy is called uniformly \u03b5-optimal (resp. uniformly optimal) if it is so from every start state. An optimal strategy for player 2 from state s is almost surely winning if valGO (s) = 1. By AS (O) we denote the set of states that have an almost surely winning strategy for objective O. For ease of presentation, we drop subscripts and superscripts wherever possible if they are clear from the context.\nEnergy-parity. We are concerned with approximating the value for the combined energy-parity objective EN(k) \u2229 EPAR and building \u03b5-optimal strategies.\nIn our constructions we use some auxiliary objectives. Following [29], these are defined as Gain def= LimInf(> \u2212\u221e) \u2229 EPAR and Loss def= Gain = LimInf(= \u2212\u221e) \u222a OPAR. \u25b6 Remark 1. For finite-state SSGs and the following objectives there exist optimal MD strategies for both players. Moreover, if the SSG is just a maximizing MDP then the set of states that are almost surely winning for Maximizer can be computed in polynomial time. 1. F T [15] 2. LimInf( \u2212 \u221e), LimInf( \u221e), LimSup( \u2212 \u221e), LimSup( \u221e), MP(> 0) [5, Prop. 1] 3. EPAR [37]\n3 The Main Result\nThe following theorem states our main result.\n\u25b6 Theorem 2. Let G = (S, (S2, S3, S#), E, P ) be an SSG with transition rewards in unary assigned by function r and colors assigned to states by function Col. For every state s \u2208 S, initial energy level i \u2265 0 and error margin \u03b5 > 0, one can compute 1. a rational number v\u2032 such that 0 \u2264 v\u2032 \u2212 valGEN(i) \u2229EPAR (s) \u2264 \u03b5 in 2-NEXPTIME. 1\n2. \u03b5-optimal FD strategies \u03c3\u03b5 and \u03c0\u03b5 for Maximizer and Minimizer, resp., in 2-NEXPTIME. These strategies use O ( 2-EXP (|G|) \u00b7 log ( 1 \u03b5 )) memory modes. For rewards in binary, the bounds above increase by one exponential.\nWe outline the main steps of the proof; details in the following sections. We begin with the observation that EN(i) \u2286 EN(j) for i \u2264 j, and thus for all states s we have valGEN(i) \u2229 EPAR (s) \u2264 val G EN(j) \u2229 EPAR (s) \u2264 1. So limn\u2192\u221e val G EN(n) \u2229 EPAR (s) exists. We define\nLvalG (s) def= lim n\u2192\u221e valGEN(n) \u2229 EPAR (s). (1)\nWe will see that LvalG (s) and valGGain (s) are in fact equal (a consequence of Lemma 9) and valGGain (s) can be computed in nondeterministic polynomial time (Theorem 5). Intuitively, for high energy levels, the precise energy level does not matter much for the value.\nThe main steps of the approximation algorithm are as follows. 1. Compute FD strategies \u03c3\u2217(s) that are optimal maximizing for the objective Gain starting\nfrom state s in G. Compute an MD strategy \u03c0\u2217 that is uniformly optimal minimizing for the objective Gain. Compute the value valGGain (s) for every s \u2208 S. See Section 4.\n2. Compute a natural number N such that for all s \u2208 S and all i \u2265 N we have\n0 \u2264 valGGain (s) \u2212 valGEN(i) \u2229 EPAR (s) \u2264 \u03b5.\nN will be doubly exponential. See Section 5.\n1 We write \u201ccomputing a number v\u2032 in 2-NEXPTIME\u201d as a shorthand for the property that questions like v\u2032 \u2264 c for constants c are decidable in 2-NEXPTIME.\nMFCS 2023\n3. Consider the finite-state parity game G\u2032 derived from G by encoding the energy level up-to N into the states, i.e., the states of G\u2032 are of the form (s, k) for s \u2208 S and 0 \u2264 k \u2264 N , and colors are inherited from s. Moreover, we add gadgets that ensure that states (s, N) at the upper end win with probability valGGain (s) and states (s, 0) at the lower end lose. By the previous item, valGGain (s) is \u03b5-close to valGEN(N) \u2229 EPAR (s). Thus, for k < N we can \u03b5-approximate the value v = valGEN(k) \u2229 EPAR (s) by v \u2032 def= valG \u2032\nEPAR ((s, k)). If k \u2265 N we can \u03b5-approximate v by v\u2032 def= valGGain (s). Moreover, we obtain \u03b5-optimal FD strategies \u03c3\u03b5 for Maximizer (resp. \u03c0\u03b5 for Minimizer) for EN(k) \u2229 EPAR in G. Let \u03c3\u0302 (resp. \u03c0\u0302) be optimal MD strategies for Maximizer (resp. Minimizer) for the objective EPAR in G\u2032. Then \u03c3\u03b5 plays as follows. While the current energy level j (k plus the sum of the rewards so far) stays < N , then, at any state s\u2032, play like \u03c3\u0302 at state (s\u2032, j) in G\u2032. Once the energy level reaches a value \u2265 N at some state s\u2032 for the first time, then play like \u03c3\u2217(s\u2032) forever. Similarly, \u03c0\u03b5 plays as follows. While the current energy level j (k plus the sum of the rewards so far) stays < N , then, at any state s\u2032, play like \u03c0\u0302 at state (s\u2032, j) in G\u2032. Once the energy level reaches a value \u2265 N (at any state) for the first time, then play like \u03c0\u2217 forever. See Section 6.\nAs a technical tool, we sometimes consider the dual of a game G (resp. the dual maximizing MDP of some minimizing MDP). Consider Gd def= ( S\u2032, (S\u20322, S\u20323, S\u2032#), E\u2032, P \u2032 ) with the complement objective EN(k) \u2229 EPAR = Term(k) \u222a OPAR, where Gd is simply the game with the roles of Maximizer and Minimizer reversed, i.e.,\nS\u2032 = S S\u20322 = S3 S\u20323 = S2 S\u2032# = S# E\u2032 = E P \u2032 = P\nHence \u03a3Gd = \u03a0G and \u03a0Gd = \u03a3G . It is easy to see that for any objective O and start state s 1. valGO (s) + valG d O (s) = 1. 2. \u03c3 is \u03b5-optimal maximizing for O in G iff it is \u03b5-optimal minimizing for O in Gd. 3. \u03c0 is \u03b5-optimal minimizing for O in G iff it is \u03b5-optimal maximizing for O in Gd. So approximating the value of EN(k) \u2229 EPAR in G can be reduced in linear time to approximating the value of Term(k) \u222a OPAR in Gd.\n4 Computing valGGain (s)\nGiven an SSG G = (S, (S2, S3, S#), E, P ) and a start state s, we will show how to compute valGGain (s) and the optimal strategies for both players.\nWe start with the case of maximizing MDPs. The following lemma summarizes some previous results ([29, Lemmas 30,16], [28, Lemma 26], [24, Proposition 4]).\n\u25b6 Lemma 3. Let M be a maximizing MDP. 1. LvalM (s) = valMGain (s) for all states s \u2208 S. 2. Optimal strategies for Gain in M exist and can be chosen FD, with O(exp(|M|O(1)))\nmemory modes, and exponential memory is also necessary. 3. For any state s \u2208 S, LvalM (s) is rational and can be computed in O(|M|8) deterministic\npolynomial time if rewards are in unary, and in NP and coNP if rewards are in binary.\nProof. Item 1 holds by [29, Lemma 30]. Towards Item 2, we follow the proof of [29, Lemma 16]. Since Gain = LimInf(> \u2212\u221e) \u2229 EPAR is shift-invariant, there exist optimal strategies by [22]. By [28, Theorem 18] and Item 1, an optimal strategy for Gain can be constructed as follows. Let A def= \u22c3 k\u2208N AS (ST(k) \u2229 EPAR) and B def= AS (LimInf(= \u221e) \u2229 EPAR) be the subsets of states from which there exist almost\nsurely winning strategies for the objectives ST(k)\u2229EPAR and LimInf(= \u221e)\u2229EPAR, respectively. By [28, Theorem 8], we can restrict the values k in the definition of A by some k\u2032 = O(|S| \u00b7R), i.e., A = \u22c3 k\u2264k\u2032 AS (ST(k) \u2229 EPAR). An optimal strategy \u03c3 for Gain works in two phases. First it plays an optimal strategy \u03c3R towards reaching the set A \u222a B, where \u03c3R can be chosen MD by Remark 1. Then, upon reaching A (resp. B), it plays an almost surely winning strategy \u03c3A for the objective ST(k) \u2229 EPAR (resp. \u03c3B for the objective LimInf(= \u221e) \u2229 EPAR). By [28, Theorem 8], the strategy \u03c3A requires O(k \u00b7 |S|) memory modes for a given k and thus at most O(|S|2 \u00b7 R), since we can assume that k \u2264 k\u2032. Towards the strategy \u03c3B , we first observe that in finite MDPs a strategy is almost-surely winning for LimInf(= \u221e) \u2229 EPAR iff it is almostsurely winning for MP(> 0) \u2229 EPAR. By [24, Proposition 4], there exist optimal deterministic strategies for MP(> 0) \u2229 EPAR that use exponential memory, i.e., O(exp(|M|O(1))) memory modes. The memory required for \u03c3B exceeds that of \u03c3R and \u03c3A (even when R is given in binary), and the one extra memory mode to record the switch from \u03c3R to \u03c3A (resp. \u03c3B) is negligible in comparison. Thus we can conclude that \u03c3 uses O(exp(|M|O(1))) memory modes. [24, Fig. 1 and Prop. 4] shows that exponential memory is necessary.\nTowards Item 3, let d def= |Col(S)| be the number of priorities in the parity condition. By [28, Lemma 26], for each s \u2208 S, LvalM (s) is rational and can be computed in deterministic time O\u0303(|E| \u00b7 d \u00b7 |S|4 \u00b7 R + d \u00b7 |S|3.5 \u00b7 (|P | + |r|)2) (and still in NP and coNP if R is given in binary). So LvalM (s) can be computed in O(|M|8) deterministic polynomial time if weights are given in unary, and in NP and coNP if weights are given in binary. \u25c0\nIn order to extend Lemma 3 from MDPs to games, we need the notion of derived MDPs, obtained by fixing the choices of one player according to some FD strategy. Given an SSG G = (S, (S2, S3, S#), E, P ) and a finite memory deterministic (FD) strategy \u03c0 for Minimizer (resp. \u03c3 for Maximizer) from a state s, described by (M, m0, upd, nxt), let G\u03c0 (resp. G\u03c3) be the maximizing (resp. minimizing) MDP with state space M \u00d7 S obtained by fixing Minimizer\u2019s (resp. Maximizer\u2019s) choices according to \u03c0 (resp. \u03c3).\n\u25b6 Lemma 4. For every SSG G, objective O and Minimizer (resp. Maximizer) FD strategy \u03c0 = (M, m0, upd, nxt) (resp. \u03c3), from state s we get valG \u03c3 O ((m0, s)) \u2264 valGO (s) \u2264 val G\u03c0 O ((m0, s)) and equality holds if \u03c0 (resp. \u03c3) is optimal from state s.\n\u25b6 Theorem 5. Consider an SSG G = (S, (S2, S3, S#), E, P ) with the Gain objective. 1. Optimal Minimizer strategies exist and can be chosen uniform MD. 2. valGGain (s) is rational and questions about it, i.e., valGGain (s) \u2264 c for constants c, are\ndecidable in NP. 3. Optimal Maximizer strategies exist and can be chosen FD, with O(exp(|G|O(1))) memory\nmodes. Moreover, exponential memory is also necessary.\nProof. Towards Item 1, observe that since both the objectives LimInf(= \u2212\u221e) and OPAR are shift-invariant and submixing, so is their union, i.e., Gain is shift-invariant and submixing. Hence, by [23, Theorem 1.1], an optimal MD strategy \u03c0\u2217s for Minimizer exists from any state s \u2208 S. Since S is finite and Gain is shift-invariant, we can also obtain a uniformly optimal MD strategy \u03c0\u2217, i.e., \u03c0\u2217 is optimal from every state.\nTowards Item 2, consider the maximizing MDP G\u03c0\u2217 obtained from G by fixing \u03c0\u2217 (cf. Definition 11). Since \u03c0\u2217 is MD, the states of G\u03c0\u2217 are the same as the states as G. Since \u03c0\u2217 is optimal for Minimizer from every state s, we obtain that valGGain (s) = val G\u03c0\u2217 Gain (s) for every state s by Lemma 4. By Lemma 3, the latter is rational and can be computed in polynomial time for weights in unary (resp. in NP and coNP for weights in binary). Thus, by guessing \u03c0\u2217, we can decide questions valGGain (s) \u2264 c in NP.\nMFCS 2023\nTowards Item 3, we again use the property that Gain is shift-invariant and submixing (see above). By [29, Theorem 6, Def. 24], optimal FD Maximizer strategies for Gain in an SSG require only |S3| \u00b7 \u2308log(|E|)\u2309 many extra bits of memory above the memory required for optimal Maximizer strategies in any derived MDP M where Minimizer\u2019s choices are fixed. Hence, by Lemma 3, one can obtain optimal FD Maximizer strategies in G that use at most 2|S3|\u00b7\u2308log(|E|)\u2309 \u00b7 O(exp(|M|O(1))) = O(exp(|G|O(1))) memory modes. The corresponding exponential lower bound on the memory holds already for MDPs by Lemma 3. \u25c0\n5 Computing the Upper Bound N\nWe show how to compute the upper bound N , up-to which Maximizer needs to remember the energy level, for any given error margin \u03b5 > 0. Similarly as in Section 4, we first solve the problem for maximizing MDPs and then extend the solution to SSGs."
        },
        {
            "heading": "5.1 Computing N for maximizing MDPs",
            "text": "Given a maximizing MDP M = (S, S2, S#, E, P ) and \u03b5 > 0, we will compute an N \u2208 N such that for all s \u2208 S and all j \u2265 N\n0 \u2264 valMTerm(j) \u222a OPAR (s) \u2212 valMLoss (s) \u2264 \u03b5.\nRecall that Loss = LimInf(= \u2212\u221e) \u222a OPAR. We now define the sets of states W0 def= AS (Loss), W1 def= AS (LimInf(= \u2212\u221e)) and W2\ndef= AS (OPAR). By Remark 1, there exist optimal MD strategies for LimInf(= \u2212\u221e) and OPAR. Since Loss is shift-invariant and submixing, there exists an optimal MD strategy for it by [23, Theorem 1.1].\n\u25b6 Lemma 6. For every state s in the MDP M we have 1. W1 \u222a W2 \u2286 W0 2. valF W0 (s) \u2264 valLoss (s) 3. val\nOPAR \u2229 F W2 (s) = 0\n4. for every initial energy level j \u2265 0\nval(Term(j) \u222a OPAR) \u2229 F W0 (s) = valF W0 (s) (2)\nvalLoss (s) \u2264 valTerm(j) \u222a OPAR (s) \u2264 valLoss (s) + sup \u03c3\nP\u03c3,s ( Term(j) \u2229 F W1 ) (3)\nProof. 1. This follows directly from the definitions of W0, W1, W2. 2. Let \u03c3\u2032 be an optimal MD strategy for F W0 from s and \u03c3\u2032\u2032 be an almost surely winning\nMD strategy for Loss from any state in W0. Let \u03c3 be the strategy that plays \u03c3\u2032 until reaching W0 and then switches to \u03c3\u2032\u2032. We have valLoss (s) \u2265 P\u03c3,s(Loss) \u2265 P\u03c3\u2032,s(F W0) = valF W0 (s). 3. For s \u2208 W2 the statement is obvious. So let s /\u2208 W2 and consider the modified MDP M\u2032 = ( S\u2032, S\u20322, S \u2032 #, E \u2032, P \u2032 )\nwhere all states in W2 are collapsed into a losing sink. I.e., S\u2032\ndef= (S \\ W2) \u228e {trap}, with trap a new random sink state having color 0 (thus losing for objective OPAR), E\u2032 contains all of (E \u2229 {(S \\ W2) \u00d7 (S \\ W2)} \u222a (trap, trap)) and all transitions to W2 are redirected to trap and P \u2032 is derived accordingly from P . Then valM \u2032\nOPAR (s\u0302) = valMOPAR \u2229 FW2 (s\u0302) for all states s\u0302 \u2208 S \\ W2. Towards a contradiction, assume that valM\nOPAR \u2229 FW2 (s) > 0. Hence valM\u2032OPAR (s) > 0. Then, by [22, Theorem 3.2], there\nexists a state s\u2032 \u2208 S\u2032 such that valM\u2032OPAR (s\u2032) = 1, and it is easy to see that s\u2032 =\u0338 trap and thus s\u2032 \u2208 S \\ W2. But this implies that valMOPAR (s\u2032) = 1 and thus s\u2032 \u2208 W2, a contradiction.\n4. Let O def= Term(j) \u222a OPAR. For Equation (2), the first inequality valO \u2229 FW0 (s) \u2264 valFW0 (s) is trivial, since O \u2229 F W0 \u2286 FW0. To show the reverse inequality, consider the strategy \u03c3 that first plays like an optimal MD strategy \u03c3\u2032 for the objective F W0 and after reaching W0 switches to an almost surely winning MD strategy \u03c3\u2032\u2032 for the objective Loss. Then valO \u2229 FW0 (s) \u2265 P\u03c3,s(O \u2229 F W0) \u2265 P\u03c3,s(Loss \u2229 F W0) = P\u03c3\u2032,s(F W0) = valFW0 (s), where the second inequality is due to LimInf(= \u2212\u221e) \u2286 Term(j). For Equation (3), the first inequality is again due to the fact that LimInf(= \u2212\u221e) \u2286 Term(j) for all j \u2265 0. Towards the second inequality of Equation (3) we have\nvalO (s) = sup\n\u03c3 P\u03c3,s(O)\n= sup \u03c3\n( P\u03c3,s (O \u2229 F W0) + P\u03c3,s ( O \u2229 F W0 )) (Law of total probability)\n\u2264 sup \u03c3 P\u03c3,s (O \u2229 F W0) + sup \u03c3\nP\u03c3,s ( O \u2229 F W0 ) (sup (f + g) \u2264 sup f + sup g)\n= sup \u03c3 P\u03c3,s (F W0) + sup \u03c3\nP\u03c3,s ( O \u2229 F W0 ) (Equation (2))\n\u2264 valLoss (s) + sup \u03c3\nP\u03c3,s ( O \u2229 F W0 ) (Item 2)\nWe can upper-bound the second summand above as follows.\nsup \u03c3 P\u03c3,s(O \u2229 F W0)\n= sup \u03c3\nP\u03c3,s ( (Term(j) \u222a OPAR) \u2229 F W0 ) \u2264 sup\n\u03c3 P\u03c3,s\n( Term(j) \u2229 F W0 ) + sup\n\u03c3 P\u03c3,s\n( OPAR \u2229 F W0 ) (Union bound)\n\u2264 sup \u03c3\nP\u03c3,s ( Term(j) \u2229 F W1 ) + sup\n\u03c3 P\u03c3,s\n( OPAR \u2229 F W2 ) (Item 1)\n= sup \u03c3\nP\u03c3,s ( Term(j) \u2229 F W1 ) (Item 3) \u25c0\nWe show that the term sup\u03c3 P\u03c3,s ( Term(j) \u2229 F W1 ) in Equation (3) can be made arbit-\nrarily small for large j. To this end, we use [3, Lemma 3.9] (adapted to our notation).\n\u25b6 Lemma 7. [3, Lemma 3.9 and Claim 6] Let M = (S, S2, S#, E, P ) be a maximizing finite MDP with rewards in unary and W1\ndef= AS (LimInf(= \u2212\u221e)). One can compute, in polynomial time, a rational constant c < 1, and an integer h \u2265 0 such that for all j \u2265 h and s \u2208 S\nsup \u03c3\nP\u03c3,s ( Term(j) \u2229 F W1 ) \u2264 c j\n1 \u2212 c .\nMoreover, 1/(1 \u2212 c) \u2208 O ( exp(|M|O(1)) ) and h \u2208 O(exp ( |M|O(1) ) ).\n\u25b6 Lemma 8. Consider a maximizing MDP M = (S, S2, S#, E, P ), \u03b5 > 0 and the constants c, h from Lemma 7. For rewards in unary and i \u2265 N we have valMTerm(i) \u222a OPAR (s) \u2212 valMLoss (s) \u2264 \u03b5 where N def= max (h, \u2308logc (\u03b5 \u00b7 (1 \u2212 c))\u2309) \u2208 O ( exp(|M|O(1)) \u00b7 log (1/\u03b5) ) .\nFor rewards in binary we have N \u2208 O ( exp(exp(|M|O(1))) \u00b7 log (1/\u03b5) ) , i.e., the size of N\nincreases by one exponential.\nProof sketch. For rewards in unary, the result follows from Lemma 6(Equation (3)) and Lemma 7. For rewards in binary, the constants increase by one exponential via encoding binary rewards into unary rewards in a modified MDP. \u25c0\nMFCS 2023"
        },
        {
            "heading": "5.2 Computing N for SSGs",
            "text": "In order to compute the bound N for an SSG G, we first consider bounds N(s) for individual states s and then take their maximum. Given a state s, we can use Theorem 5(Item 3) to obtain an optimal FD strategy (with O(exp(|G|O(1))) memory modes) \u03c3\u2217(s) = (M, m0, upd, nxt) for Maximizer from state s w.r.t. the Gain objective. Theorem 5(Item 1) yields a uniform MD strategy \u03c0\u2217 that is optimal for Minimizer from all states s w.r.t. the Gain objective.\n\u25b6 Lemma 9. Given an SSG G = (S, (S2, S3, S#), E, P ) and \u03b5 > 0, we can compute a number N \u2208 N such that for all i \u2265 N and states s \u2208 S we have\nvalGEN(i) \u2229 EPAR (s)\u2212\u03b5 \u2264 val G Gain (s)\u2212\u03b5 \u2264 inf \u03c0 PG\u03c3\u2217(s),\u03c0,s (EN(i) \u2229 EPAR) \u2264 val G EN(i) \u2229 EPAR (s) (4)\ni.e., \u03c3\u2217(s) is \u03b5-optimal for Maximizer for EN(i) \u2229 EPAR for all i \u2265 N . In particular, 0 \u2264 valGGain (s) \u2212 valGEN(i) \u2229 EPAR (s) \u2264 \u03b5.\nMoreover, \u03c0\u2217 is \u03b5-optimal for Minimizer from any state s for i \u2265 N .\nsup \u03c3 PG\u03c3,\u03c0\u2217,s (EN(i) \u2229 EPAR) \u2264 sup \u03c3 PG\u03c3,\u03c0\u2217,s (Gain) = valGGain (s) \u2264 valGEN(i) \u2229 EPAR (s) + \u03b5 (5)\nFor rewards in unary, N is doubly exponential, i.e., N \u2208 O ( exp(exp(|G|O(1))) \u00b7 log (1/\u03b5) ) and it can be computed in exponential time. For rewards in binary, the size of N and its computation time increase by one exponential, respectively.\nProof. Assume that rewards are in unary. The first inequality of (4) holds because EN(i) \u2229 EPAR \u2286 Gain for any i. The third inequality of (4) follows from the definition of the value. Towards the second inequality of (4), we consider the minimizing MDP M(s) def= G\u03c3\u2217(s) obtained by fixing the Maximizer strategy \u03c3\u2217(s). Since \u03c3\u2217(s) is optimal for Maximizer from state s wrt. the objective Gain, Lemma 4 yields that\nvalGGain (s) = val M(s) Gain ((m0, s)). (6)\nSince \u03c3\u2217(s) has O(exp(|G|O(1))) memory modes, the size of M(s) is exponential in |G| and M(s) can be computed in exponential time.\nNow we consider the dual maximizing MDP M(s)d and the objectives Term(i) \u222a OPAR and Loss. (Note that M(s)d has the same size as M(s).) From Lemma 8, we obtain a bound N(s) \u2208 N such that for all i \u2265 N(s)\n0 \u2264 valM(s) d Term(i) \u222a OPAR ((m0, s)) \u2212 val M(s)d Loss ((m0, s)) \u2264 \u03b5. (7)\nBy Lemma 8 and Lemma 7, N(s) is exponential in |M(s)d| and thus doubly exponential in |G|, i.e., N(s) \u2208 O ( exp(exp(|G|O(1))) \u00b7 log (1/\u03b5) ) . Moreover, N(s) can be computed in time polynomial in |M(s)d| and thus in time exponential in |G|. By duality, we can rewrite Equation (7) for M(s) as follows. For all i \u2265 N(s)\n0 \u2264 valM(s)Gain ((m0, s)) \u2212 val M(s) EN(i) \u2229 EPAR ((m0, s)) \u2264 \u03b5. (8)\nIn order to get a uniform upper bound that holds for all states, let N def= maxs\u2208S N(s). Since |S| is linear, we still have N \u2208 O ( exp(exp(|G|O(1))) \u00b7 log (1/\u03b5) ) and it can be computed in\nexponential time in |G|. Finally, we can show the second inequality of (4).\ninf \u03c0 PG\u03c3\u2217(s),\u03c0,s (EN(i) \u2229 EPAR)\n= inf \u03c0 PM(s)\u03c0,(m0,s) (EN(i) \u2229 EPAR) = valM(s)EN(i) \u2229 EPAR ((m0, s)) \u2265 valM(s)Gain ((m0, s)) \u2212 \u03b5 by i \u2265 N \u2265 N(s) and Equation (8) = valGGain (s) \u2212 \u03b5 by (6)\nThe first inequality of (5) holds because EN(i) \u2229 EPAR \u2286 Gain for any i. The equality in (5) holds by the optimality of \u03c0\u2217. The second inequality of (5) follows from the previously stated consequence of (4).\nFor rewards in binary, the sizes of the numbers N(s) (and hence N) and the time to compute it increase by one exponential by Lemma 8. \u25c0\n6 Unfolding the Game to Energy Level N\nGiven an SSG G = (S, (S2, S3, S#), E, P ) and error tolerance \u03b5 > 0, for each state s \u2208 S and energy level i \u2265 0, we want to compute a rational number v\u2032 which satisfies 0 \u2264 v\u2032\u2212valGEN(i) \u2229 EPAR (s) \u2264 \u03b5, and \u03b5-optimal FD strategies \u03c3\u03b5 and \u03c0\u03b5 for Maximizer and Minimizer, resp. We achieve this by constructing a finite-state parity game G\u2032 that closely approximates the original game G, as described in Section 3(Item 3).\nFor clarity, we explain the construction in two steps. In the first step, we consider a finite-state parity game G [N ]. (Unlike G\u2032, the game G [N ] is not actually constructed. It just serves as a part of the correctness proof.) G [N ] encodes the energy level up-to N + R (where R is the maximal transition reward) into the states, i.e., it has states of the form (s, k) with k \u2264 N + R. It imitates the original game G till energy level N + R, but at any state (s, i) with energy level i \u2265 N it jumps to a winning state with probability valGEN(i) \u2229 EPAR (s) and to a losing state with probability 1 \u2212 valGEN(i) \u2229 EPAR (s). (We need the margin up-to N + R, because transitions can have rewards > 1, so the level N might not be hit exactly.) Similarly, at states (s, 0) with energy level 0, we jump to a losing state. The coloring function in the new game G [N ] derives its colors from the colors in the original game G, i.e., all states (s, i) have the same color as s in G.\nBy construction of G [N ], for i \u2264 N , the EPAR value of (s, i) in G [N ] coincides with valGEN(i) \u2229 EPAR (s).\nIn the second step, since we do not know the exact values valGEN(i) \u2229 EPAR (s) for N + R \u2265 i > N , we approximate these by the slightly larger valGGain (s). I.e., we modify G [N ] by replacing the probability values valGEN(i) \u2229 EPAR (s) for the jumps to the winning state by valGGain (s). Let G\u2032 be the resulting finite-state parity game. It follows from Lemma 9 that 0 \u2264 valGGain (s) \u2212 valGEN(i) \u2229 EPAR (s) \u2264 \u03b5 for i \u2265 N and Lval G EN \u2229 EPAR (s) = valGGain (s). Thus G\u2032 \u03b5-over-approximates G [N ] and G, and we obtain the following lemma.\n\u25b6 Lemma 10. For all states s and all 0 \u2264 i \u2264 N\nvalG[N ]EPAR ((s, i)) = valGEN(i) \u2229 EPAR (s), and 0 \u2264 valG \u2032 EPAR ((s, i)) \u2212 val G[N ] EPAR ((s, i)) \u2264 \u03b5.\nNow we are ready to prove the main theorem.\nMFCS 2023\n\u25b6 Theorem 2. Let G = (S, (S2, S3, S#), E, P ) be an SSG with transition rewards in unary assigned by function r and colors assigned to states by function Col. For every state s \u2208 S, initial energy level i \u2265 0 and error margin \u03b5 > 0, one can compute 1. a rational number v\u2032 such that 0 \u2264 v\u2032 \u2212 valGEN(i) \u2229EPAR (s) \u2264 \u03b5 in 2-NEXPTIME. 2\n2. \u03b5-optimal FD strategies \u03c3\u03b5 and \u03c0\u03b5 for Maximizer and Minimizer, resp., in 2-NEXPTIME. These strategies use O ( 2-EXP (|G|) \u00b7 log ( 1 \u03b5 )) memory modes. For rewards in binary, the bounds above increase by one exponential.\nProof. For i > N we output v\u2032 = valGGain (s), which satisfies the condition by Lemma 9. For i \u2264 N we output v\u2032 = valG \u2032\nEPAR ((s, i)), which satisfies the condition by Lemma 10. By Theorem 5, the values valGGain (s) are rational for all states s. Therefore all probability values in G\u2032 are rational and thus the EPAR values of all states in G\u2032 are rational. Hence our numbers v\u2032 are always rational.\nBy Theorem 5, the values valGGain (s) for all states s \u2208 S can be computed in exponential time. By Lemma 9, N \u2208 O ( exp(exp(|G|O(1))) \u00b7 log (1/\u03b5) ) is doubly exponential. Therefore,\nwe can construct G\u2032 in O ( exp(exp(|G|O(1))) \u00b7 log (1/\u03b5) ) time and space. Questions about the parity values of states in G\u2032 can be decided in nondeterministic time polynomial in |G\u2032|. Thus the numbers v\u2032 are computed in 2-NEXPTIME.\nTowards Item 2, we construct \u03b5-optimal FD strategies \u03c3\u03b5 for Maximizer (resp. \u03c0\u03b5 for Minimizer) for EN(i) \u2229 EPAR in G. Let \u03c3\u0302 (resp. \u03c0\u0302) be optimal MD strategies for Maximizer (resp. Minimizer) for the objective EPAR in G\u2032, which exist by Remark 1. Since these strategies are MD, they can be guessed in nondeterministic time polynomial in the size |G\u2032|, and thus in O ( exp(exp(|G|O(1))) \u00b7 log (1/\u03b5) ) nondeterministic time.\nThen \u03c3\u03b5 plays as follows. While the current energy level j (i plus the sum of the rewards so far) stays < N , then, at any state s\u2032, play like \u03c3\u0302 at state (s\u2032, j) in G\u2032. Once the energy level reaches a value \u2265 N at some state s\u2032 for the first time, then play like \u03c3\u2217(s\u2032) forever. (Recall that \u03c3\u2217(s\u2032) is the optimal FD Maximizer strategy for Gain from state s\u2032 from Section 5.2.) \u03c3\u03b5 is \u03b5-optimal by Lemma 10 and Lemma 9. It needs to remember the energy level up-to N while simulating \u03c3\u0302. Moreover, \u03c3\u2217(s\u2032) needs O(exp(|G|O(1))) memory modes by Theorem 5. Finally, it needs to remember the switch from \u03c3\u0302 to \u03c3\u2217(s\u2032). Since N \u2208 O ( exp(exp(|G|O(1))) \u00b7 log (1/\u03b5)\n) dominates the rest, \u03c3\u03b5 uses O ( exp(exp(|G|O(1))) \u00b7 log (1/\u03b5) ) memory modes.\nSimilarly, \u03c0\u03b5 plays as follows. While the current energy level j stays < N , at any state s\u2032, play like \u03c0\u0302 at state (s\u2032, j) in G\u2032. Once the energy level reaches a value \u2265 N (at any state) for the first time, then play like \u03c0\u2217 forever (where \u03c0\u2217 is the uniform optimal MD Minimizer strategy for Gain from Section 5.2.) \u03c0\u03b5 is \u03b5-optimal by Lemma 10 and Lemma 9. While \u03c0\u2217 is MD and does not use any memory, \u03c0\u03b5 still needs to remember the energy level up-to N while simulating \u03c0\u0302, and thus it uses O ( exp(exp(|G|O(1))) \u00b7 log (1/\u03b5) ) memory modes.\nFor rewards in binary, all bounds increase by one exponential via an encoding of G into an exponentially larger but equivalent game with rewards in unary. \u25c0\nNo nontrivial lower bounds are known on the computational complexity of approximating valGEN(i) \u2229EPAR (s). However, even without the parity part, the problem appears to be hard. The best known algorithm for approximating the value of the energy objective (resp. the dual termination objective) runs in NEXPTIME for SSGs with rewards in unary [3].\nAs for lower bounds on the strategy complexity, \u03b5-optimal Maximizer strategies need at least an exponential number of memory modes (for any 0 < \u03b5 < 1) even in maximizing\n2 We write \u201ccomputing a number v\u2032 in 2-NEXPTIME\u201d as a shorthand for the property that questions like v\u2032 \u2264 c for constants c are decidable in 2-NEXPTIME.\nMDPs. This can easily be shown by extending the example in Lemma 3(Item 2) and [24,\nFig. 1 and Prop. 4] that shows the lower bound for the Gain objective. First loop in a state with an unfavorable color to accumulate a sufficiently large reward (depending on \u03b5) and then switch to the MDP in [24, Fig. 1 and Prop. 4] to play for Gain (since EN(i) \u2229 EPAR will be very close to Gain then). Even the latter part requires exponentially many memory modes.\n7 Conclusion & Extensions\nWe gave a procedure to compute \u03b5-approximations of the value of combined energy-parity objectives in SSGs. The decidability of questions about the exact values is open, but the problem is at least as hard as the positivity problem for linear recurrence sequences [31, Section 5.2.3]. Unlike almost surely winning Maximizer strategies which require infinite memory in general [28, 29], \u03b5-optimal strategies for either player require only finite memory with at most doubly exponentially many memory modes.\nAn interesting topic for further study is whether these results can be extended to other combined objectives where the parity part is replaced by something else, i.e., energy-X for some objective X (e.g., some other color-based condition like Rabin/Streett, or a quantitative objective about multi-dimensional transition rewards). While our proofs are not completely specific to parity, they do use many strong properties that parity satisfies.\nShift-invariance of EPAR is used in several places, e.g. in Lemma 6 (and thus its consequences) and for the correctness of the constructions in Section 6. We use the fact that EPAR goes well together with LimInf(> \u2212\u221e), i.e., the objective Gain = LimInf(> \u2212\u221e) \u2229 EPAR allows optimal FD strategies for Maximizer in MDPs; cf. Lemma 3. The submixing property of OPAR = EPAR is used in Theorem 5 to lift Lemma 3 from MDPs to SSGs.\nMFCS 2023\nReferences 1 Rajeev Alur, Thomas A Henzinger, and Orna Kupferman. Alternating-time temporal logic.\nJournal of the ACM, 49(5):672\u2013713, 2002. 2 Patrick Billingsley. Probability and measure. John Wiley & Sons, 2008. 3 T. Br\u00e1zdil, V. Bro\u017eek, K. Etessami, and A. Ku\u010dera. Approximating the Termination Value of\nOne-Counter MDPs and Stochastic Games. Information and Computation, 222:121\u2013138, 2013. URL: https://arxiv.org/abs/1104.4978.\n4 T. Br\u00e1zdil, A. Ku\u010dera, and P. Novotn\u00fd. Optimizing the expected mean payoff in energy Markov decision processes. In International Symposium on Automated Technology for Verification and Analysis (ATVA), volume 9938 of LNCS, pages 32\u201349, 2016.\n5 Tom\u00e1s Br\u00e1zdil, V\u00e1clav Brozek, and Kousha Etessami. One-Counter Stochastic Games. In Kamal Lodaya and Meena Mahajan, editors, IARCS Annual Conference on Foundations of Software Technology and Theoretical Computer Science (FSTTCS 2010), volume 8 of Leibniz International Proceedings in Informatics (LIPIcs), pages 108\u2013119, Dagstuhl, Germany, 2010. Schloss Dagstuhl\u2013Leibniz-Zentrum fuer Informatik. Full version at http://arxiv. org/abs/1009.5636. URL: http://drops.dagstuhl.de/opus/volltexte/2010/2857, doi: 10.4230/LIPIcs.FSTTCS.2010.108.\n6 Arindam Chakrabarti, Luca De Alfaro, Thomas A Henzinger, and Mari\u00eblle Stoelinga. Resource interfaces. In International Workshop on Embedded Software, pages 117\u2013133, 2003.\n7 Krishnendu Chatterjee and Laurent Doyen. Energy parity games. In International Colloquium on Automata, Languages and Programming (ICALP), volume 6199 of LNCS, pages 599\u2013610, 2010.\n8 Krishnendu Chatterjee and Laurent Doyen. Energy and mean-payoff parity Markov decision processes. In International Symposium on Mathematical Foundations of Computer Science (MFCS), volume 6907, pages 206\u2013218, 2011.\n9 Krishnendu Chatterjee and Laurent Doyen. Games and Markov decision processes with mean-payoff parity and energy parity objectives. In Mathematical and Engineering Methods in Computer Science (MEMICS), volume 7119 of LNCS, pages 37\u201346. Springer, 2011.\n10 Krishnendu Chatterjee, Laurent Doyen, Hugo Gimbert, and Youssouf Oualhadj. Perfectinformation stochastic mean-payoff parity games. In International Conference on Foundations of Software Science and Computational Structures (FoSSaCS), volume 8412 of LNCS, 2014."
        },
        {
            "heading": "11 Krishnendu Chatterjee, Thomas A. Henzinger, and Nir Piterman. Generalized parity",
            "text": "games. In Helmut Seidl, editor, International Conference on Foundations of Software Science and Computational Structures (FoSSaCS), volume 4423 of LNCS, pages 153\u2013167. Springer, 2007. URL: https://doi.org/10.1007/978-3-540-71389-0_12, doi:10.1007/ 978-3-540-71389-0\\_12."
        },
        {
            "heading": "12 Krishnendu Chatterjee, Marcin Jurdzi\u0144ski, and Thomas A. Henzinger. Simple stochastic parity",
            "text": "games. In Computer Science Logic (CSL), volume 2803 of LNCS, pages 100\u2013113. Springer, 2003."
        },
        {
            "heading": "13 Krishnendu Chatterjee, Marcin Jurdzi\u0144ski, and Thomas A. Henzinger. Quantitative stochastic",
            "text": "parity games. In ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 121\u2013130. SIAM, 2004.\n14 E.M. Clarke, O. Grumberg, and D. Peled. Model Checking. MIT Press, Dec. 1999. 15 Anne Condon. The complexity of stochastic games. Information and Computation, 96(2):203\u2013 224, 1992. doi:https://doi.org/10.1016/0890-5401(92)90048-K."
        },
        {
            "heading": "16 Laure Daviaud, Martin Jurdzi\u0144ski, and Ranko Lazi\u0107. A pseudo-quasi-polynomial algorithm",
            "text": "for mean-payoff parity games. In Logic in Computer Science (LICS), pages 325\u2013334, 2018. 17 Luca De Alfaro and Thomas A Henzinger. Interface automata. ACM SIGSOFT Software Engineering Notes, 26(5):109\u2013120, 2001. 18 David L. Dill. Trace theory for automatic hierarchical verification of speed-independent circuits,\nvolume 24. MIT press Cambridge, 1989."
        },
        {
            "heading": "19 Graham Everest, Alfred Jacobus van der Poorten, Igor Shparlinski, and Thomas Ward.",
            "text": "Recurrence sequences. ACM, 2003. 20 J. Filar and K. Vrieze. Competitive Markov Decision Processes. Springer, 1997. 21 Dean Gillette. Stochastic games with zero stop probabilities. Contributions to the Theory of Games, 3:179\u2013187, 1957. 22 Hugo Gimbert and Florian Horn. Solving simple stochastic tail games. In ACM-SIAM\nSymposium on Discrete Algorithms (SODA), pages 847\u2013862, 2010. URL: http://epubs.siam. org/doi/abs/10.1137/1.9781611973075.69.\n23 Hugo Gimbert and Edon Kelmendi. Submixing and shift-invariant stochastic games. 2022. URL: https://arxiv.org/abs/1401.6575. 24 Hugo Gimbert, Youssouf Oualhadj, and Soumya Paul. Computing optimal strategies for Markov decision processes with parity and positive-average conditions. 2011. URL: https: //hal.science/hal-00559173/en/. 25 Marcin Jurdzi\u0144ski. Deciding the winner in parity games is in UP \u2229 co-UP. Information Processing Letters, 68(3):119\u2013124, 1998. 26 A. Maitra and W. Sudderth. Stochastic games with Borel payoffs. In Stochastic Games and Applications, pages 367\u2013373. Kluwer, Dordrecht, 2003. 27 Donald A. Martin. The determinacy of Blackwell games. Journal of Symbolic Logic, 63(4):1565\u20131581, 1998. 28 Richard Mayr, Sven Schewe, Patrick Totzke, and Dominik Wojtczak. MDPs with Energy-Parity Objectives. In Logic in Computer Science (LICS). IEEE, 2017. URL: https://arxiv.org/ abs/1701.02546. 29 Richard Mayr, Sven Schewe, Patrick Totzke, and Dominik Wojtczak. Simple stochastic games with almost-sure energy-parity objectives are in NP and coNP. In Proc. of Fossacs, volume 12650 of LNCS, 2021. Extended version on arXiv. URL: https://arxiv.org/abs/2101.06989. 30 Jo\u00ebl Ouaknine and James Worrell. On linear recurrence sequences and loop termination. ACM SIGLOG News, 2(2):4\u201313, 2015. 31 Jakob Piribauer. On non-classical stochastic shortest path problems. PhD thesis, Technische Universit\u00e4t Dresden, Germany, 2021. URL: https://nbn-resolving.org/urn:nbn:de:bsz: 14-qucosa2-762812. 32 Jakob Piribauer and Christel Baier. On Skolem-Hardness and Saturation Points in Markov Decision Processes. In Artur Czumaj, Anuj Dawar, and Emanuela Merelli, editors, Proc. of ICALP, volume 168 of LIPIcs, pages 138:1\u2013138:17, Dagstuhl, Germany, 2020. Schloss Dagstuhl\u2013 Leibniz-Zentrum f\u00fcr Informatik. URL: https://drops.dagstuhl.de/opus/volltexte/2020/ 12545, doi:10.4230/LIPIcs.ICALP.2020.138. 33 Jakob Piribauer and Christel Baier. Positivity-hardness results on Markov decision processes. 2023. URL: https://arxiv.org/abs/2302.13675v1. 34 Amir Pnueli and Roni Rosner. On the synthesis of a reactive module. In Annual Symposium on Principles of Programming Languages (POPL), pages 179\u2013190, 1989. 35 Peter J. Ramadge and W. Murray Wonham. Supervisory control of a class of discrete event processes. SIAM journal on control and optimization, 25(1):206\u2013230, 1987. 36 Lloyd S. Shapley. Stochastic games. Proceedings of the national academy of sciences, 39(10):1095\u20131100, 1953. 37 W. Zielonka. Infinite games on finitely coloured graphs with applications to automata on infinite trees. Theoretical Computer Science, 200(1-2):135\u2013183, 1998.\nMFCS 2023\nA Appendix for Section 4\n\u25b6 Definition 11. Given an SSG G = (S, (S2, S3, S#), E, P ) and a finite memory deterministic (FD) strategy \u03c0 = (M, m0, upd, nxt) for Minimizer let G\u03c0 be the maximizing MDP with state space M \u00d7 S obtained by fixing Minimizer\u2019s choices according to \u03c0. The transition rules \u2212\u2192\u2032 in the derived MDP G\u03c0 are given as follows. 1. If s \u2208 S2 for every (s, s\u2032) \u2208 E, m \u2208 M, we have (m, s) \u2212\u2192\u2032 (upd(m, (s, s\u2032)), s\u2032), i.e.,\nMaximizer determines the successor state and Minimizer updates its memory according to the observed transition. 2. Similarly if s \u2208 S# for every (s, s\u2032) \u2208 E, m \u2208 M we have (m, s) \u2212\u2192\u2032 (upd(m, (s, s\u2032)), s\u2032) and P ((m, s))((upd(m, (s, s\u2032)), s\u2032)) = P (s)(s\u2032), i.e., transition probabilities are inherited and Minimizer\u2019s memory is updated according to the observed transition. 3. If s \u2208 S3 then (m, s) \u2212\u2192\u2032 (upd(m, (s, s\u2032)), s\u2032) where s\u2032 = nxt(m, s), i.e., Minimizer chooses the successor state according to the strategy \u03c0 and updates its memory accordingly. The reward of each transition is the same as the reward of the transition in G from which it is derived. Similarly for the priorities (aka coloring) of the states. The ownership of the vertices (m, s) in G\u03c0 is as follows. If s \u2208 S2 then (m, s) belongs to Maximizer. If s \u2208 S# then (m, s) is also a chance vertex. If s \u2208 S3 then (m, s) also becomes a chance vertex (with exactly one successor), since Minimizer\u2019s choice has been fixed.\nIn the dual case where a FD strategy \u03c3 for Maximizer is fixed, we obtain a minimizing MDP G\u03c3. The construction is the same as above, with the roles of Minimizer and Maximizer swapped.\nB Appendix for Section 5\n\u25b6 Lemma 8. Consider a maximizing MDP M = (S, S2, S#, E, P ), \u03b5 > 0 and the constants c, h from Lemma 7. For rewards in unary and i \u2265 N we have valMTerm(i) \u222a OPAR (s) \u2212 valMLoss (s) \u2264 \u03b5 where N def= max (h, \u2308logc (\u03b5 \u00b7 (1 \u2212 c))\u2309) \u2208 O ( exp(|M|O(1)) \u00b7 log (1/\u03b5) ) .\nFor rewards in binary we have N \u2208 O ( exp(exp(|M|O(1))) \u00b7 log (1/\u03b5) ) , i.e., the size of N\nincreases by one exponential.\nProof. By Lemma 6(Equation (3)) and Lemma 7, we have\nvalMTerm(i) \u222a OPAR (s) \u2212 valMLoss (s) \u2264 sup \u03c3\nP\u03c3,s ( Term(i) \u2229 F W1 ) \u2264 c i\n1 \u2212 c\nfor all i \u2265 h and s \u2208 S. To obtain a bound N \u2265 h with c N\n1\u2212c \u2264 \u03b5, it suffices to choose\nN def= max (h, \u2308logc (\u03b5 \u00b7 (1 \u2212 c))\u2309) .\nWe observe that logc (\u03b5 \u00b7 (1 \u2212 c)) = \u2212 ln (\u03b5 \u00b7 (1 \u2212 c)) \u00b7 (\u2212 ln(c)\u22121). However, \u2212 ln(c) = \u2212 ln(1 \u2212 (1 \u2212 c)) \u2265 (1 \u2212 c). Thus logc (\u03b5 \u00b7 (1 \u2212 c)) \u2264 ln ( 1 \u03b5 \u00b7 1 1\u2212c ) \u00b7 11\u2212c .\nFor rewards in unary, by Lemma 7, we have 1/(1 \u2212 c) \u2208 O ( exp(|M|O(1)) ) and h is only\nO(exp ( |M|O(1) ) ). Thus N \u2208 O ( exp(|M|O(1)) \u00b7 log (1/\u03b5) ) .\nNow consider the case where rewards are given in binary. Following the proof of [3, Lemma 3.9], the bounds are derived from the size of solutions of the constructed linear program. While the MDPs in [3] only consider unary rewards from {\u22121, 0, 1}, one can extend it to the case where the rewards come from the set {\u2212R, . . . , 0, . . . , R} in a natural way. This affects the complexity of the above computed constants and thereby size of N . More\nprecisely, the proof of Lemma 7 can be split into three steps. Firstly, given an MDP M construct a new \u201crising\u201d MDP M\u2032. Then from this derived M\u2032, construct a linear program. From the solutions of constructed LP, compute the required c and h. We evaluate the effect of having non-unary rewards in each of these steps.\nWhen rewards are given in unary, the resulting M\u2032 has overall size |M\u2032| \u2264 10|M|4. More exactly, |S\u2032| \u2264 10 \u2217 |S|3 \u2217 (|S| + |E|) and similarly for |E\u2032|. When the rewards are given in binary, the construction results in an additional R2 factor. So the resulting M\u2032 is pseudo-polynomially big when compared to M in our case.\nThe constructed LP (cf. [3, Fig.1]) has S\u2032 + 2 variables (zs for each state, x for the mean payoff and \u03be for converting the constraint x > 0 to x \u2265 \u03be). Moreover all variables can be assumed non-negative. The number of constraints is bounded by E\u2032 + 1. Furthermore all the constants appearing in the constraints are either constants in the original MDP M or 1 or 0.\nFinally, from an optimal solution of the LP (zs, x, \u03be) one can compute exp ( \u2212x2 2\u00b7(zmax+x+R)2 ) and to get c, then take a rational over-approximation and also take h as \u2308zmax\u2309 where zmax\ndef= maxs\u2208S\u2032 zs \u2212 mins\u2208S\u2032 zs. The only difference compared to the unary rewards case here is that the one step change of the submartingale is bounded by zmax + x + R instead of zmax + x + 1.\nFrom the complexity point of view, both the construction of the LP and the computation from its optimal solutions aren\u2019t affected by changes in the rewards, i.e., the previous bounds for c, h and N in terms of |M\u2032| still hold. In particular, c \u2208 O ( exp ( 1/2|M\u2032|O(1) )) ,\nh \u2208 O(exp ( |M\u2032|O(1) ) ) and thus N \u2208 O ( exp ( |M\u2032|O(1) ) \u00b7 log (1/\u03b5) ) by [3, Claim 6].\nWhile previously, M\u2032 is only polynomially larger than M, introducing binary rewards blows up the construction (cf. [3, Appendix A.2]). As a result we have that |M\u2032| \u2208 O ( 2|M|O(1) ) . Therefore N can be doubly exponential in the size of the original MDP M,\ni.e., N \u2208 O ( exp(exp(|M|O(1))) \u00b7 log (1/\u03b5) ) . \u25c0\nC Appendix for Section 6\n\u25b6 Definition 12 (Definition of G [N ]). We present formally the definition of the game G [N ], which unfolds the energy level in G till N\nG [N ] = (S [N ] , (S2 [N ] , S3 [N ] , S# [N ]) , E [N ] , P [N ])\nwhere 1. S [N ] def= S \u00d7 {0, . . . , N + R} \u228e {swin, slose}, the set of states is the tuple with the game\nstate and energy level until N + R as the maximum change in a single step is R and since we are only interested in energy levels \u2264 N , it suffices to consider till N + R. 2. S\u2299 [N ] def= S\u2299 \u00d7 {1, . . . , N}, both players control their respective states until energy level\nN . Every state with energy > N becomes a chance node. Consequently, 3. S# [N ]\ndef= S#\u00d7{1, . . . , N}\u222aS \u00d7{0, N + 1, . . . , N + R}\u222a{swin, slose}, since the Maximizer loses when the energy level becomes \u2264 0, we make these states as a chance vertex which go to a losing loop. 4. E [N ], P [N ] a. For 0 < i \u2264 N , (s, i) \u2212\u2192 (s\u2032, max(0, j)) iff s j\u2212i\u2212\u2192 s\u2032 \u2208 E, this is just simulating the\ntransitions of the game until energy level N and taking care of border cases. When energy drops below 0, we move to level 0 as there is no difference. When it shoots above N , it cannot go beyond N + R and thus the transition is well defined.\nb. If s \u2208 S# above, then the probability is carried over.\nMFCS 2023\nc. (s, 0) \u2212\u2192slose with probability 1. d. (s, N + k) \u2212\u2192swin with probability valGEN(N+k) \u2229 EPAR (s) and with remaining probability\nmoves to slose for 1 \u2264 k \u2264 R e. slose\u2212\u2192slose with probability 1. Similarly for swin."
        }
    ],
    "title": "Approximating the Value of Energy-Parity Objectives in Simple Stochastic Games",
    "year": 2023
}