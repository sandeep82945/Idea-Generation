{
    "abstractText": "9",
    "authors": [],
    "id": "SP:4f6982638272df85cfa1d44c361dc8ad0fe0c5ee",
    "references": [
        {
            "authors": [
                "Alejandro Lage-Castellanos",
                "Roberto Mulet",
                "Federico Ricci-Tersenghi",
                "Tommaso Rizzo"
            ],
            "title": "Replica cluster variational method: the replica symmetric solution for the 2d random bond ising model",
            "venue": "Journal of Physics A: Mathematical and Theoretical,",
            "year": 2013
        },
        {
            "authors": [
                "Tien-Yien Li"
            ],
            "title": "Numerical solution of multivariate polynomial systems by homotopy continuation methods",
            "venue": "Acta numerica,",
            "year": 1997
        },
        {
            "authors": [
                "Tien-Yien Li"
            ],
            "title": "Solving polynomial systems by the homotopy continuation method",
            "venue": "Handbook of Numerical Analysis,",
            "year": 2003
        },
        {
            "authors": [
                "Qiang Liu",
                "Alexander T Ihler"
            ],
            "title": "Bounding the partition function using H\u00f6lder\u2019s inequality",
            "venue": "In Proceedings of ICML,",
            "year": 2011
        },
        {
            "authors": [
                "Hans A Loeliger"
            ],
            "title": "An introduction to factor graphs",
            "venue": "IEEE Signal Processing Magazine,",
            "year": 2004
        },
        {
            "authors": [
                "Steffen L Lauritzen",
                "David J Spiegelhalter"
            ],
            "title": "Local computations with probabilities on graphical structures and their application to expert systems",
            "venue": "Journal of the Royal Statistical Society. Series B,",
            "year": 1988
        },
        {
            "authors": [
                "David JC MacKay"
            ],
            "title": "A conversation about the Bethe free energy and sumproduct",
            "venue": "Technical Report of MERL.,",
            "year": 2001
        },
        {
            "authors": [
                "David JC MacKay"
            ],
            "title": "Information Theory, Inference and Learning Algorithms",
            "year": 2003
        },
        {
            "authors": [
                "Talya Meltzer",
                "Amir Globerson",
                "Yair Weiss"
            ],
            "title": "Convergent message passing algorithms: a unifying view",
            "venue": "In Proceedings of UAI,",
            "year": 2009
        },
        {
            "authors": [
                "Ofer Meshi",
                "Ariel Jaimovich",
                "Amir Globerson",
                "Nir Friedman"
            ],
            "title": "Convexifying the Bethe free energy",
            "venue": "In Proceedings of UAI. AUAI Press,",
            "year": 2009
        },
        {
            "authors": [
                "Joris M Mooij",
                "Hilbert J Kappen"
            ],
            "title": "On the properties of the Bethe approximation and loopy belief propagation on binary networks",
            "venue": "Journal of Statistical Mechanics: Theory and Experiment,",
            "year": 2005
        },
        {
            "authors": [
                "Joris M Mooij",
                "Hilbert J Kappen"
            ],
            "title": "Sufficient conditions for convergence of the sum\u2013product algorithm",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 2007
        },
        {
            "authors": [
                "Joris M Mooij",
                "Hilbert J Kappen"
            ],
            "title": "Bounds on marginal probability distributions",
            "venue": "In Proceedings of NIPS,",
            "year": 2009
        },
        {
            "authors": [
                "Victorin Martin",
                "Jean-Marc Lasgouttes",
                "Cyril Furtlehner"
            ],
            "title": "The role of normalization in the belief propagation algorithm",
            "venue": "arXiv preprint arXiv:1101.4170,",
            "year": 2011
        },
        {
            "authors": [
                "Ernst W Mayr",
                "Albert R Meyer"
            ],
            "title": "The complexity of the word problems for commutative semigroups and polynomial ideals",
            "venue": "Advances in Mathematics,",
            "year": 1982
        },
        {
            "authors": [
                "H Michael M\u00f6ller",
                "Ferdinando Mora"
            ],
            "title": "Upper and lower bounds for the degree of Gr\u00f6bner bases",
            "venue": "EUROSAM",
            "year": 1984
        },
        {
            "authors": [
                "Marc Mezard",
                "Andrea Montanari"
            ],
            "title": "Information, Physics, and Computation",
            "year": 2009
        },
        {
            "authors": [
                "H Michael M\u00f6ller"
            ],
            "title": "Systems of algebraic equations solved by means of endomorphisms",
            "venue": "In International Symposium on Applied Algebra, Algebraic Algorithms, and Error-Correcting Codes,",
            "year": 1993
        },
        {
            "authors": [
                "Joris Marten Mooij"
            ],
            "title": "Understanding and Improving Belief Propagation",
            "venue": "PhD thesis, Radboud University Nijmegen,",
            "year": 2008
        },
        {
            "authors": [
                "Bernard Mourrain"
            ],
            "title": "Pythagore\u2019s dilemma, symbolic-numeric computation, and the border basis method",
            "venue": "In Symbolic-Numeric Computation,",
            "year": 2007
        },
        {
            "authors": [
                "Marc Mezard",
                "Giorgio Parisi",
                "Miguel Virasoro"
            ],
            "title": "Spin Glass Theory and Beyond: An Introduction to the Replica Method and Its Applications, volume 9",
            "venue": "World Scientific Publishing Co Inc,",
            "year": 1987
        },
        {
            "authors": [
                "H Michael M\u00f6ller",
                "Hans J Stetter"
            ],
            "title": "Multivariate polynomial equations with multiple zeros solved by matrix eigenproblems",
            "venue": "Numerische Mathematik,",
            "year": 1995
        },
        {
            "authors": [
                "Kevin P Murphy"
            ],
            "title": "Machine Learning: A Probabilistic Perspective",
            "venue": "MIT press,",
            "year": 2012
        },
        {
            "authors": [
                "Kevin P Murphy",
                "Yair Weiss",
                "Michael I Jordan"
            ],
            "title": "Loopy belief propagation for approximate inference: An empirical study",
            "venue": "In Proceedings of UAI,",
            "year": 1999
        },
        {
            "authors": [
                "Manfred Opper",
                "Ole Winther"
            ],
            "title": "Tractable approximations for probabilistic models: the adaptive Thouless-Anderson-Palmer mean field approach",
            "venue": "Physical Review Letters,",
            "year": 2001
        },
        {
            "authors": [
                "Payam Pakzad",
                "Venkat Anantharam"
            ],
            "title": "Belief propagation and statistical physics",
            "venue": "In Proceedings of Conference on Information Sciences and Systems,",
            "year": 2002
        },
        {
            "authors": [
                "Xaq Pitkow",
                "Yashar Ahmadian",
                "Ken D Miller"
            ],
            "title": "Learning unbelievable probabilities",
            "venue": "In Proceedings of NIPS,",
            "year": 2011
        },
        {
            "authors": [
                "Judea Pearl"
            ],
            "title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference",
            "year": 1988
        },
        {
            "authors": [
                "R Peierls"
            ],
            "title": "Statistical theory of superlattices with unequal concentrations of the components",
            "venue": "Proceedings of the Royal Society of London. Series A-Mathematical and Physical Sciences,",
            "year": 1936
        },
        {
            "authors": [
                "Wei Ping",
                "Alexander Ihler"
            ],
            "title": "Belief propagation in conditional rbms for structured prediction",
            "venue": "Proceedings of AISTATS,",
            "year": 2017
        },
        {
            "authors": [
                "F Pernkopf",
                "R Peharz",
                "S Tschiatschek"
            ],
            "title": "Introduction to Probabilistic Graphical Models. Academic Press",
            "venue": "Library in Signal Processing,",
            "year": 2014
        },
        {
            "authors": [
                "Siamak Ravanbakhsh",
                "Russell Greiner"
            ],
            "title": "Revisiting algebra and complexity of inference in graphical models",
            "venue": "arXiv preprint arXiv:1409.7410,",
            "year": 2014
        },
        {
            "authors": [
                "Siamak Ravanbakhsh",
                "Russell Greiner"
            ],
            "title": "Perturbed message passing for constraint satisfaction problems",
            "venue": "Journal of Machine Learning Research,",
            "year": 2015
        },
        {
            "authors": [
                "Bj\u00f6rn S R\u00fcffer",
                "Christopher M Kellett",
                "Peter M Dower",
                "Steven R Weller"
            ],
            "title": "Belief propagation as a dynamical system: The linear case and open problems",
            "venue": "IET Control Theory & Applications,",
            "year": 2010
        },
        {
            "authors": [
                "Dan Roth"
            ],
            "title": "On the hardness of approximate reasoning",
            "venue": "Artificial Intelligence,",
            "year": 1996
        },
        {
            "authors": [
                "Nicholas Ruozzi"
            ],
            "title": "The Bethe partition function of log-supermodular graphical models",
            "venue": "In Proceedings of NIPS,",
            "year": 2012
        },
        {
            "authors": [
                "Nicholas Ruozzi"
            ],
            "title": "Beyond log-supermodularity: lower bounds and the Bethe partition function",
            "venue": "In Proceedings of UAI,",
            "year": 2013
        },
        {
            "authors": [
                "Edward R. Scheinerman"
            ],
            "title": "Invitation to Dynamical Systems",
            "year": 2000
        },
        {
            "authors": [
                "Jinwoo Shin"
            ],
            "title": "Complexity of Bethe approximation",
            "venue": "In Proceedings of AISTATS, pages 1037\u20131045,",
            "year": 2012
        },
        {
            "authors": [
                "David Sontag",
                "Tommi S Jaakkola"
            ],
            "title": "New outer bounds on the marginal polytope",
            "venue": "In Proceedings of NIPS,",
            "year": 2008
        },
        {
            "authors": [
                "Alaa Saade",
                "Florent Krzakala",
                "Lenka Zdeborov\u00e1"
            ],
            "title": "Spectral clustering of graphs with the Bethe Hessian",
            "venue": "In Proceedings of NIPS,",
            "year": 2014
        },
        {
            "authors": [
                "Alaa Saade",
                "Florent Krzakala",
                "Lenka Zdeborov\u00e1"
            ],
            "title": "Spectral bounds for the ising ferromagnet on an arbitrary given graph",
            "venue": "Journal of Statistical Mechanics,",
            "year": 2017
        },
        {
            "authors": [
                "Charles Sutton",
                "Andrew McCallum"
            ],
            "title": "Improved dynamic schedules for belief propagation",
            "venue": "In Proceedings of UAI,",
            "year": 2007
        },
        {
            "authors": [
                "Christopher Srinivasa",
                "Siamak Ravanbakhsh",
                "Brendan Frey"
            ],
            "title": "Survey propagation beyond constraint satisfaction problems",
            "venue": "In Proceedings of AISTATS,",
            "year": 2016
        },
        {
            "authors": [
                "Gilbert Strang"
            ],
            "title": "Introduction to Linear Algebra",
            "year": 2016
        },
        {
            "authors": [
                "Andrew J Sommese",
                "Charles W Wampler"
            ],
            "title": "The Numerical Solution of Systems of Polynomials Arising in Engineering and Science, volume 99",
            "venue": "World Scientific,",
            "year": 2005
        },
        {
            "authors": [
                "Gerald Teschl"
            ],
            "title": "Ordinary Differential Equations and Dynamical Systems",
            "venue": "American Mathematical Society,",
            "year": 2012
        },
        {
            "authors": [
                "Sekhar C Tatikonda",
                "Michael I Jordan"
            ],
            "title": "Loopy belief propagation and Gibbs measures",
            "venue": "In Proceedings of UAI,",
            "year": 2002
        },
        {
            "authors": [
                "Nobuyuki Taga",
                "Shigeru Mase"
            ],
            "title": "Applications of Gibbs measure theory to loopy belief propagation algorithm",
            "venue": "In Mexican International Conference on Artificial Intelligence,",
            "year": 2006
        },
        {
            "authors": [
                "Peng Hui Tan",
                "Lars K Rasmussen"
            ],
            "title": "Belief propagation for coded multiuser detection",
            "venue": "In IEEE International Symposium on Information Theory, pages 1919\u20131923,",
            "year": 2006
        },
        {
            "authors": [
                "Jan Verschelde"
            ],
            "title": "Algorithm 795: Phcpack: A general-purpose solver for polynomial systems by homotopy continuation",
            "venue": "ACM Transactions on Mathematical Software (TOMS),",
            "year": 1999
        },
        {
            "authors": [
                "Andrew Viterbi"
            ],
            "title": "Error bounds for convolutional codes and an asymptotically optimum decoding algorithm",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 1967
        },
        {
            "authors": [
                "Pascal O Vontobel"
            ],
            "title": "Counting in graph covers: A combinatorial characterization of the bethe entropy function",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 2013
        },
        {
            "authors": [
                "Martin Wainwright"
            ],
            "title": "Stochastic processes on graphs with cycles: geometric and variational approaches",
            "venue": "PhD thesis, Massachusetts Institute of Technology,",
            "year": 2002
        },
        {
            "authors": [
                "Yusuke Watanabe"
            ],
            "title": "Discrete geometric analysis of message passing algorithm on graphs",
            "venue": "PhD thesis, The Graduate University for Advanced Studies,",
            "year": 2010
        },
        {
            "authors": [
                "Adrian Weller",
                "Justin Domke"
            ],
            "title": "Clamping improves TRW and mean field approximations",
            "venue": "In Artificial Intelligence and Statistics,",
            "year": 2016
        },
        {
            "authors": [
                "Yair Weiss"
            ],
            "title": "Correctness of local probability propagation in graphical models with loops",
            "venue": "Neural Computation,",
            "year": 2000
        },
        {
            "authors": [
                "Adrian Weller"
            ],
            "title": "Methods for Inference in Graphical Models",
            "venue": "PhD thesis, Columbia University,",
            "year": 2014
        },
        {
            "authors": [
                "Adrian Weller"
            ],
            "title": "Uprooting and rerooting graphical models",
            "venue": "In Proceedings of ICML,",
            "year": 2016
        },
        {
            "authors": [
                "Yusuke Watanabe",
                "Kenji Fukumizu"
            ],
            "title": "Graph zeta function in the Bethe free energy and loopy belief propagation",
            "venue": "In Proceedings of NIPS,",
            "year": 2017
        },
        {
            "authors": [
                "Eugene P Wigner"
            ],
            "title": "On the distribution of the roots of certain symmetric matrices",
            "venue": "The Annals of Mathematics,",
            "year": 1958
        },
        {
            "authors": [
                "Martin J Wainwright",
                "Michael I Jordan"
            ],
            "title": "Graphical models, exponential families, and variational inference",
            "venue": "Foundations and Trends in Machine Learning,",
            "year": 2008
        },
        {
            "authors": [
                "Adrian Weller",
                "Tony Jebara"
            ],
            "title": "Bethe bounds and approximating the global optimum",
            "venue": "In Proceedings of AISTATS,",
            "year": 2013
        },
        {
            "authors": [
                "Adrian Weller",
                "Tony Jebara"
            ],
            "title": "Approximating the Bethe partition function",
            "venue": "In Proceedings of UAI,",
            "year": 2014
        },
        {
            "authors": [
                "Adrian Weller",
                "Tony Jebara"
            ],
            "title": "Clamping variables and approximate inference",
            "venue": "In Proceedings of NIPS,",
            "year": 2014
        },
        {
            "authors": [
                "Martin J Wainwright",
                "Tommi Jaakkola",
                "Alan S Willsky"
            ],
            "title": "Tree-based reparameterization for approximate inference on loopy graphs",
            "venue": "In Proceedings of NIPS,",
            "year": 2002
        },
        {
            "authors": [
                "Martin J Wainwright",
                "Tommi S Jaakkola",
                "Alan S Willsky"
            ],
            "title": "Tree-based reparameterization framework for analysis of sum-product and related algorithms",
            "venue": "IEEE Transactions on information theory,",
            "year": 2003
        },
        {
            "authors": [
                "Martin J Wainwright",
                "Tommi S Jaakkola",
                "Alan S Willsky"
            ],
            "title": "Tree-reweighted belief propagation algorithms and approximate ML estimation by pseudomoment matching",
            "venue": "In Proceedings of AISTATS,",
            "year": 2003
        },
        {
            "authors": [
                "Martin J Wainwright",
                "Tommi S Jaakkola",
                "Alan S Willsky"
            ],
            "title": "A new class of upper bounds on the log partition function",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 2005
        },
        {
            "authors": [
                "Alan S Willsky",
                "Erik B Sudderth",
                "Martin J Wainwright"
            ],
            "title": "Loop series and Bethe variational bounds in attractive graphical models",
            "venue": "In Proceedings of NIPS,",
            "year": 2008
        },
        {
            "authors": [
                "Max Welling",
                "Yee Whye Teh"
            ],
            "title": "Belief optimization for binary networks: A stable alternative to loopy belief propagation",
            "venue": "In Proceedings of UAI,",
            "year": 2001
        },
        {
            "authors": [
                "Max Welling",
                "Yee Whye Teh"
            ],
            "title": "Approximate inference in Boltzmann machines",
            "venue": "Artificial Intelligence,",
            "year": 2003
        },
        {
            "authors": [
                "Adrian Weller",
                "Kui Tang",
                "Tony Jebara",
                "David Sontag"
            ],
            "title": "Understanding the Bethe approximation: When and how can it go wrong",
            "venue": "In Proceedings of UAI,",
            "year": 2014
        },
        {
            "authors": [
                "Henk Wymeersch"
            ],
            "title": "Iterative Receiver Design, volume 234",
            "year": 2007
        },
        {
            "authors": [
                "Jonathan S Yedidia",
                "William T Freeman",
                "Yair Weiss"
            ],
            "title": "Generalized belief propagation",
            "venue": "In Proceedings of NIPS,",
            "year": 2001
        },
        {
            "authors": [
                "Jonathan S Yedidia",
                "William T Freeman",
                "Yair Weiss"
            ],
            "title": "Constructing freeenergy approximations and generalized belief propagation algorithms",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 2005
        },
        {
            "authors": [
                "Allan Peter Young"
            ],
            "title": "Spin Glasses and Random Fields, volume 12",
            "venue": "World Scientific,",
            "year": 1998
        },
        {
            "authors": [
                "Alan L Yuille",
                "Anand Rangarajan"
            ],
            "title": "The concave-convex procedure",
            "venue": "Neural Computation,",
            "year": 2003
        },
        {
            "authors": [
                "Lenka Zdeborov\u00e1",
                "Florent Krzakala"
            ],
            "title": "Statistical physics of inference: Thresholds and algorithms",
            "venue": "Advances in Physics,",
            "year": 2016
        }
    ],
    "sections": [
        {
            "text": "PhD Thesis\nUnderstanding the Behavior of Belief Propagation\nConvergence Properties, Approximation Quality, and Solution Space Analysis\nconducted at the Signal Processing and Speech Communications Laboratory\nGraz University of Technology, Austria\nby Dipl.-Ing. Christian Knoll, BSc.\nSupervisors: Assoc.Prof. Dipl.-Ing. Dr.mont. Franz Pernkopf\nAssessors/Examiners: Assoc.Prof. Dipl.-Ing. Dr.mont. Franz Pernkopf\nDr. Adrian Weller\nGraz, November 8, 2019\nar X\niv :2\n20 9.\n05 46\n4v 1\n[ cs\n.A I]\n5 S\nep 2\n02 2\nStatutory Declaration\nI declare that I have authored this thesis independently, that I have not used other than the declared sources/resources, and that I have explicitly marked all material which has been quoted either literally or by content from the used sources. The text document uploaded to TUGRAZonline is identical to the present doctoral dissertation.\ndate (signature)\nUnderstanding the Behavior of Belief Propagation\nContents\nAbstract 9\nAcknowledgments 11\nNotational Conventions 13"
        },
        {
            "heading": "1 Introduction 19",
            "text": "1.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n1.2 Five Relevant PhD Theses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 1.3 Contribution and Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22"
        },
        {
            "heading": "2 Background 25",
            "text": "2.1 Probability Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n2.2 Graph Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 2.3 Probabilistic Graphical Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n2.3.1 Undirected Graphical Models (Markov Random Fields) . . . . . . . . . . 28\n2.4 Binary Pairwise Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n2.4.1 Model Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 2.4.2 Generality of Binary Pairwise Models . . . . . . . . . . . . . . . . . . . . 31 2.4.3 Exponential Representation and Model Parametrization . . . . . . . . . . 32 2.4.4 The Ising Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n2.5 Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n2.5.1 Exact Inference: Efficient Methods . . . . . . . . . . . . . . . . . . . . . . 36"
        },
        {
            "heading": "3 Approximate Inference: Belief Propagation 41",
            "text": "3.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n3.2 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 3.3 Variational Interpretation of BP \u2013 A Physics Perspective . . . . . . . . . . . . . . 45\n3.3.1 Energy Landscape of the Bethe Free Energy . . . . . . . . . . . . . . . . . 49 3.3.2 The Bethe Approximation and Belief Propagation . . . . . . . . . . . . . 49 3.3.3 Other Variational Approaches . . . . . . . . . . . . . . . . . . . . . . . . . 52\n3.4 Approximation Quality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 3.5 BP as a Dynamical System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\n3.5.1 Enhancing Belief Propagation\u2019s Properties . . . . . . . . . . . . . . . . . . 56 3.5.2 Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 3.5.3 Update . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 3.5.4 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\n3.6 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62"
        },
        {
            "heading": "4 Discrete-Time Dynamical Systems: Solution Space Analysis 63",
            "text": "4.1 Discrete Time Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\n4.2 Finding Fixed Points of a Discrete Time Map . . . . . . . . . . . . . . . . . . . . 64\n4.2.1 Systems of Polynomial Equations . . . . . . . . . . . . . . . . . . . . . . . 64\n4.3 Solving Polynomial Systems: A Comparison of Methods . . . . . . . . . . . . . . 65\n4.3.1 Numerical Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 4.3.2 Symbolic Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 4.3.3 Numerical Polynomial Homotopy Continuation (NPHC) Method . . . . . 70\n4.4 Stability Analysis of Fixed Points . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n4.4.1 Stability Analysis for a Nonlinear Discrete Time Map . . . . . . . . . . . 74\nNovember 8, 2019 \u2013 iii \u2013"
        },
        {
            "heading": "5 Solution Space of Belief Propagation: Number of Fixed Points and Their Stability 77",
            "text": "5.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\n5.2 Fixed Points of Belief Propagation . . . . . . . . . . . . . . . . . . . . . . . . . . 79\n5.2.1 Number of Fixed Points on Regular Ising Graphs . . . . . . . . . . . . . . 79\n5.3 Fixed Point Equations of Belief Propagation . . . . . . . . . . . . . . . . . . . . . 80\n5.3.1 Solving the Fixed Point Equations . . . . . . . . . . . . . . . . . . . . . . 81 5.3.2 Polyhedral Homotopy Method . . . . . . . . . . . . . . . . . . . . . . . . 82\n5.4 Stability of Fixed Points . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\n5.4.1 Reformulation of Belief Propagation for Binary Variables . . . . . . . . . 85 5.4.2 Linearization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86\n5.5 Selected Models with Ising Potentials . . . . . . . . . . . . . . . . . . . . . . . . . 87 5.6 Number of Fixed Points and Marginal Accuracy . . . . . . . . . . . . . . . . . . 88\n5.6.1 Evalutation Criteria . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88 5.6.2 Grid Graphs with Random Factors (Spin Glasses) . . . . . . . . . . . . . 89 5.6.3 Grid Graphs with Uniform Factors . . . . . . . . . . . . . . . . . . . . . . 90 5.6.4 Complete Graphs with Uniform Factors . . . . . . . . . . . . . . . . . . . 91 5.6.5 Fixed Point Evolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92 5.6.6 Runtime Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\n5.7 Empirical Stability Analysis of Belief Propagation on Ising Models . . . . . . . . 95\n5.7.1 Vanishing Local Field . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95 5.7.2 Non-Vanishing Local Field . . . . . . . . . . . . . . . . . . . . . . . . . . 95\n5.8 Theoretical Stability Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98 5.9 Application: Error-Correcting Codes . . . . . . . . . . . . . . . . . . . . . . . . . 101 5.10 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103"
        },
        {
            "heading": "6 Enhancing Belief Propagation: Self-Guided Belief Propagation 105",
            "text": "6.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\n6.2 Self-Guided Belief Propagation (SBP) . . . . . . . . . . . . . . . . . . . . . . . . 106\n6.2.1 Practical Considerations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107 6.2.2 Pseudocode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107\n6.3 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108\n6.3.1 Experimental Settings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109 6.3.2 Attractive Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110 6.3.3 General Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n6.4 Theoretical Analysis of Self-Guided Belief Propagation . . . . . . . . . . . . . . . 112\n6.4.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112 6.4.2 Properties of Self-Guided Belief Propagation . . . . . . . . . . . . . . . . 113\n6.5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116"
        },
        {
            "heading": "7 Understanding Belief Propagation: Accurate Marginals or Partition Function 119",
            "text": "7.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119\n7.2 Model Specifications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120\n7.2.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120 7.2.2 Patch Potential Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121\n7.3 Fixed Point Behavior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122\n7.3.1 Combination Of Fixed Points . . . . . . . . . . . . . . . . . . . . . . . . . 122 7.3.2 Solution Space of Patch Potential Models . . . . . . . . . . . . . . . . . . 123 7.3.3 Approximation Accuracy . . . . . . . . . . . . . . . . . . . . . . . . . . . 124 7.3.4 Comparison Of Marginal Accuracy and Partition Function Accuracy . . . 125\n7.4 Theoretical Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126\n7.4.1 Effective Field . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127 7.4.2 Definition of Region (II) . . . . . . . . . . . . . . . . . . . . . . . . . . . 127\n7.4.3 Properties Of Region (II) . . . . . . . . . . . . . . . . . . . . . . . . . . . 127 7.5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133"
        },
        {
            "heading": "8 Discussion and Open Questions 135",
            "text": ""
        },
        {
            "heading": "A Appendix 139",
            "text": "A.1 Proofs from Chapter 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139\nA.1.1 Proof of Lemma 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139 A.1.2 Proof of Theorem 9 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140\nA.2 Proofs from Chapter 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143 A.2.1 Proof of Theorem 11 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143 A.2.2 Proof of Corollary 15.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144 A.2.3 Proof of Theorem 16 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145 A.3 Pseudocode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146\nB Guide to Quotes 147\nUnderstanding the Behavior of Belief Propagation\nAbstract\nProbabilistic graphical models are a powerful concept for modeling high-dimensional distributions. Besides modeling distributions, probabilistic graphical models also provide an elegant framework for performing statistical inference; because of the high-dimensional nature, however, one must often use approximate methods for this purpose.\nBelief propagation performs approximate inference, is efficient, and looks back on a long success-story. Yet, in most cases, belief propagation lacks any performance and convergence guarantees. Many realistic problems are presented by graphical models with loops, however, in which case belief propagation is neither guaranteed to provide accurate estimates nor that it converges at all.\nThis thesis investigates how the model parameters influence the performance of belief propagation. We are particularly interested in their influence on (i) the number of fixed points, (ii) the convergence properties, and (iii) the approximation quality. For this purpose, we take a different perspective on belief propagation and realize that the fixed points define a set of polynomial equations \u2013 albeit a large one. Solving polynomial equations of this size is problematic; nonetheless, we present the numerical polynomial homotopy continuation method that is capable of solving the fixed point equations, the solutions of which are the fixed points of belief propagation.\nThe solutions to the fixed point equations give us knowledge of the whole solution space and serve as a stepping stone for analyzing belief propagation\u2019s properties. In particular, we observe a large variety of marginal accuracy across all fixed points. This, to some degree, explains the large discrepancy in the performance of belief propagation. Another important aspect of belief propagation\u2019s fixed points is their stability, that is if belief propagation can \u2013 at least in principle \u2013 converge to a given fixed point. Existing stability analyses were limited to models without local potentials for the lack of knowing the solution space. The capability to solve the fixed point equations thus allows us to extend the stability analysis to a wide range of models. In doing so, we obtain novel insights into how the model parameters and the model size affect the stability. In particular, we find that strong pairwise potentials degrade the performance, whereas strong local potentials enhance the performance.\nMoreover, our theoretical findings inspire a simple, yet powerful, modification of belief propagation. We present self-guided belief propagation that starts from a simple model (for which belief propagation obtains the exact solutions) and iteratively adapts it to the desired model. As the model is modified, self-guided belief propagation keeps track of the solution; this way, it improves upon standard belief propagation and it obtains the best possible solution for attractive models with unidirectional local potentials. For more general models, we empirically show that self-guided belief propagation maintains its favorable properties, converges more often, and is superior in terms of marginal accuracy.\nFinally, we question whether the global minimum of the Bethe free energy provides the most accurate marginals. This is a common conjecture that inspired a range of methods that aim to minimize the Bethe free energy. In the past, the studied models were either too simplistic or too complex as to the true nature of this relationship. Therefore, we must first introduce a novel class of models \u2013 termed patch potential models \u2013 which are simple enough so that we can compute all fixed points. Yet, patch potential models are complex enough to possess a rich and non-trivial solution space. A study of this solution space proves this conjecture wrong and, additionally, explains the nature of the difference between accurate marginals and good approximations of the free energy.\nNovember 8, 2019 \u2013 9 \u2013\nUnderstanding the Behavior of Belief Propagation\nAcknowledgments\nThere are a lot of people who influenced this thesis and made working on it much more enjoyable. Here I want to express my gratitude for all of you.\nFirst of all, I must thank my supervisor Franz Pernkopf for being such a great advisor. You somehow always managed to find time for stimulating discussions, both about research and about life in general. Most importantly, however, you not only granted me the freedom to pursue my own ideas but you also believed in me when I struggled to do so.\nThank you, Bernhard and Gernot for supervising my Master\u2019s thesis. It is because of you I had such a good experience back then which made me seriously consider pursuing a PhD at this lab.\nI want to thank all members of the SPSC I had the pleasure to meet during my time here \u2013 you are responsible for creating an awesome environment to work in. There are several people I want to thank in particular: Robert and Sebastian for the warm welcome I received when starting here, for the thought-provoking discussions, and for showing me how to survive in academia. Josef for being the best room-mate I can think of, for sharing the white-board with me, and for introducing me to space-exploration (as well as for discussing all kinds of research questions). Wolfgang for always having an open door and an open mind (and a good supply of sweets). Johannes for our endless discussions about research, teaching, and \u2013 most importantly \u2013 everything else. Thomas for making sure that our kitchen is always well-stocked with the essentials. Stefan, Erik, Jamilla, Michi, Martin T., Elmar, Johanna and Alex for regularly testing the capacity of our coffee-machine \u2013 you certainly enriched the time I spent standing in the queue. I am pretty sure I forgot to mention some of you \u2013 please forgive me and do not take this personally!\nLooking back at how I got here, I must thank those who probably shaped me the most during my whole life: A big thank you to my parents and my sister. Thanks for your support, for always being honest, and for listening to me whenever I need someone to talk to.\nFinally, I must thank my own small family. Thank you, Jakob and Hanna, for constantly reminding me that there are things in life far more important than my research. And, of course, I own my deepest gratitude to you, Kati! Thank you for your endless support; thank you for sometimes understanding me better than I do myself; and thank you for learning me how to appreciate and enjoy even the little success-stories in life.\nNovember 8, 2019 \u2013 11 \u2013\nUnderstanding the Behavior of Belief Propagation\nNotational Conventions\nGeneral Notation\nR\u2217+ positive real numbers, excluding zero Z\u2217+ positive integer numbers, excluding zero \u2207 gradient \u22072 Hessian D(P\u0303X||PX) Kullback Leibler divergence between P\u0303X and PX Re{\u00b7}, Im{\u00b7} real and imaginary part i imaginary unit | \u00b7 | magnitude or absolute value \u2295 exclusive disjunction sgn(\u00b7) sign function n (mod m) n modulo m\n1 indicator function log(\u00b7) natural logarithm || \u00b7 ||p lp-norm N (\u00b5, \u03c32) Gaussian distribution with mean \u00b5 and variance \u03c32 U(a, b) uniform distribution on (a, b)\nProbability\nX,Y, . . . random variables X ,Y, . . . range of a discrete random variable x, y, . . . value of random variable\nX set of random variables\nx values for a set of random variables (configuration) XN product space, range of X PX(x) probability distribution of X\nP\u0303X(x) approximation of the probability distribution of X\nP\u0303B pseudomarginals (set of all singleton and pairwise marginals) Zi, Zij normalization terms for approximated marginals PX|Y(x|y) conditional probability distribution of X given Y PX(x) Joint probability distribution of X E(x) expected value of X mi mean of Xi \u3008m\u3009 expected mean of X \u03c7ij correlation between Xi and Xj\nNovember 8, 2019 \u2013 13 \u2013\nContents\nGraphs\nG undirected graph X1, X2, . . . nodes X set of nodes\nN number of nodes\n(i, j) edge between Xi and Xj m number of the edge (i, j) under some ordering\nE set of edges\n\u2202(i) neighbors of Xi di degree of Xi d\u0302G average degree of G G\u2032 subgraph of G induced by X\u2032 A adjacency matrix\nGraphical Models\nU undirected graphical model Ci clique C(i,j,...) clique consisting of Xi, Xj , . . . C set of cliques\n\u03a6Ci(xCi) clique-potential \u03a8 set of potentials\n\u03a6Xi(xi), \u03a6(xi) local potential \u03a6Xi,Xj (xi, xj), \u03a6(xi, xj) pairwise potential Z partition function E(x) energy of the configuration x\nJij coupling \u03b8i local field \u03c3i spin \u03c3 configuration of spins\n\u03b2 inverse temperature\nH external magnetic field JP product of couplings along path P\n\u2013 14 \u2013 November 8, 2019\nContents\nDynamical Systems and Equation-Systems\nx state vector x(n),xn state vector x at time index n x\u25e6 fixed point of x\nU(x) -neighborhood of x F(\u00b7) discrete-time map B system matrix F \u2032(x) Jacobian matrix \u03bbi eigenvalue \u03bbmax eigenvalue with the largest magnitude\n\u039b ( A )\nset of all eigenvalues for the matrix A \u03c1 ( A ) spectral radius of the matrix A\nfi(x) function or polynomial equation ak polynomial coefficient K[x1, . . . , xn] polynomial ring F (x),F (x1, . . . , xn) system of polynomial equations V(F ) variety (set of solutions) of F (x)\nVR(F ),VR\u2217+(F ) variety of F (x) for the real and the positive real numbers Si convex hull of the exponent vectors of fi V (Si) volume of the polytope Si M(Si, Sj) mixed volume of Si and Sj \u03c9i lifting for the polynomial fi \u03c9i(a) lifting function for the exponent-vector a \u3008f1, . . . , fs\u3009 ideal of F dt total degree BKK Bernshtein-Kushnirenko-Khovanskii bound\nH(x, t) homotopy\nQ(x) start system\nt goes from 0 to 1 to deform the homotopy\n\u03b3 random complex number\nNovember 8, 2019 \u2013 15 \u2013\nContents\nBelief Propagation, Variational Approximations\n\u00b5nij(xj) message from Xi to Xj \u00b5n set of messages\nn iteration index \u03b1nij message normalization for \u00b5 n ij(xj) \u00b5\u25e6ij(xj) fixed point message \u00b5\u25e6 fixed point of belief propagation\nE ( P\u0303X )\naverage energy S ( P\u0303X ) entropy FH,FG ,FB Helmholtz-, Gibbs-, and Bethe- free energy EB(P\u0303B) average (Bethe) energy\nSB(P\u0303B) Bethe entropy M(G),M marginal polytope L(G),L local polytope ZB Bethe partition function FB\u2217,FBm,FB\u25e6 global minimum, local minimum, and stationary point of FB P\u0303 \u2217B, P\u0303 m B , P\u0303 \u25e6 B pseudomarginals at the global minimum, a local minimum, and a stationary point of FB S set of stable belief propagation fixed points M set of fixed points corresponding to local minima of FB T set of all fixed points\nEZ(m) error of the partition function approximation EP (m) error of the approximated marginals\nBP ( \u00b7 )\nmapping induced by belief propagation\nDP evaluation function for the pseudomarginals DZ evaluation function for the Bethe partition function rnij message-residual rn set of residuals\ndamping factor of belief propagation with damping\nF (\u00b5,\u03b1) fixed point equations of belief propagation \u03bdnij reparameterized messages \u03bd set of reparameterized messages\nhij cavity field P\u0303 TB weighted combination of all fixed points P\u0303MB weighted combination of all fixed points belonging to local minima P\u0303MAXB fixed point maximizing the partition function JA, JC , JC(G, \u03b8) critical values of coupling strength at the onset of phase-transitions \u03ba\u03b8 field-dependent scaling term for Jacobian matrix\n\u2013 16 \u2013 November 8, 2019\nContents\nError Correcting Codes\nerror-probability\nYi variable node fa factor node rnAi(yi) message from factor to variable qniA(yi) message from variable to factor \u03b1niA normalization term\nSelf-Guided Belief Propagation\n\u03b6k scaling term K number of models considered by self-guided belief propagation (length of {\u03b6k}) UK undirected graphical model for \u03b6k \u03a8k set of potentials for \u03b6k \u03a6k(xi, xj),\u03a6k(xi) pairwise and local potentials for \u03b6k \u00b5\u25e6k fixed point of BP for the model Uk c(\u03b6) solution path\nNBP maximum number of belief propagation iterations\nPatch Potential Models\nGi patch of the graph G Pm\u00b5 fraction of runs converging to the m th fixed point P\u0303 rB, P\u0303 q B fixed points with all marginals biased to one state P\u0303 pB state-preserving fixed point P\u0303 uB, P\u0303 v B, . . . all other fixed points \u03b8\u0303i effective field for Xi Qi(k, l) mismatch between P\u0303 k B and P\u0303 l B EP set of all boundary edges EC set of edges between variables that favor different states Nf number of flipped variables Nc number of state-preserving variables \u2206SB difference of the Bethe entropy between two fixed points\nNovember 8, 2019 \u2013 17 \u2013\nUnderstanding the Behavior of Belief Propagation\n1 Introduction\n\u201dUncertainty is an uncomfortable position. But certainty is an absurd one.\u201d\n\u2013 Voltaire"
        },
        {
            "heading": "1.1 Motivation",
            "text": "Every realistic domain, in accordance with the opening quote, contains some degree of uncertainty: not only is every human imperfect \u2013 with each of our decisions being subject to uncertainty \u2013 but also has every artificial system only access to partial information. When reasoning under such uncertainties, one must take all available information into account and infer the quantity of interest. This task of probabilistic reasoning is one of the central problems of statistical inference1 and, additionally, lies at the center of every decision-making process.\nMost inference-problems of practical relevance belong to the realm of multivariate statistics and involve many interacting variables. As the number of variables increases, the corresponding distributions become increasingly hard to grasp and thus require compact representations; particularly if one wishes to retain some interpretability. Fortunately, such a representation exists in the form of probabilistic graphical models; these models (as suggested by their name) rely on the proven capabilities of graphs2 and represent complex interactions in an intuitive and expressive way.\nWe will study two of the most fundamental problems of inference. These are: computing the marginal distribution and evaluating the partition function. Both problems suffer from the increased complexity when working with many variables. Assistance comes in the form of graphical models once again; not only do they represent the problems efficiently, but they also warrant efficient inference algorithms that exploit the graph structure and thus facilitate the task of inference. Note, however, that graphical models are not a panacea and while inference becomes tractable for some models (e.g., trees) it remains NP-hard for graphs with loops.\nBesides being of theoretical interest, such loopy graphs arise in a multitude of practical applications; ranging from statistical signal-, speech-, and image-processing, to statistical physics, medical diagnosis systems, and error-correcting codes. For all these important problems, exact inference methods, however, are destined to fail, which substantiates the need for efficient approximation methods.\nOne particularly prominent approximation method focuses on local interactions and infers the global model-behavior thereof. Consider, for example, a large group of friends that want to celebrate a party and need to find a suitable date. If all friends come together and discuss their\n1 One may argue that statistical inference provides the common ground for a myriad of scientific fields ranging from mathematics over empirical sciences to philosophy [EH16]. We comply with this perspective and provide a broad context to the rather specific topic of this thesis. 2 Graphs naturally emerge in a wide range of problems that include applications in social sciences, biology, physics, and communications. Graph theory constitutes a concept with many persuasive properties that warrant their wide-spread usage. First, graphs lend themselves for visual representations that reveal the underlying problem-structure, often hidden initially, to the human observer. Second, manipulations and computations on graphs are well established and provide an extensive tool-box to tackle a problem once formulated as graph.\nNovember 8, 2019 \u2013 19 \u2013\n1 Introduction\npreferences in one large negotiation, they would find an optimal date; but this seems overly complicated and impracticable. Instead, one could hope to find an acceptable date that works for most by negotiating in a distributed manner: assume that everybody discusses his preferences only with his closest friends (and that these sub-groups sufficiently overlap), then a reasonable agreement on the date will be found that will suit at least the majority of the group. Note how this focus on local interactions comes relatively intuitive when dealing with large, complex systems. The graphical model\u2019s structure is exploited precisely in such a way by belief propagation that performs local interactions \u2013 in the form of exchanging messages between neighboring nodes \u2013 to approximate the marginal distribution and the partition function.\nLet us briefly summarize the main strength of belief propagation (BP); it is the ability to efficiently perform approximate inference on models where (because of loops) exact inference becomes infeasible. BP often works remarkably well in this context, although theoretical results fail to explain BP\u2019s empirical success. Yet, portraying BP as an outright success-story would not correspond to the truth either \u2013 and indeed, while BP often performs approximate inference in a very efficient manner it completely fails to do so other times. For this lack of reliability, BP is sometimes confronted with skepticism. Instead of only hoping for a reasonable performance of BP, we would, ideally, like to have some performance guidelines established. Stating whether a given model is well-suited for the application of BP, such guidelines must focus on the following two aspects.\nFirst, one needs to understand the underlying reasons for the failure of BP. Although we still lack a rigorous understanding of why BP fails, it is empirically well-established that BP fails because: (i) multiple solutions exist with varying accuracy; (ii) one or all fixed points are unstable so that BP does not converge.\nSecond, one needs to derive performance guarantees that take the model specifications into account. If we want to better understand BP we will not only need to understand how the model specifications influence (i) the convergence properties and (ii) the approximation quality but also how both properties relate to each other. Note that both properties are directly related to each other for simple models (e.g., graphs with a single loop or small grid graphs), i.e., the better the approximation quality the faster BP converges [Wei00, Ihl07]. Such a relation, however, does not generalize to more complex models [WJ14a].\nTo summarize, it will be important to understand how specific models affect the performance of BP and if BP can be expected to perform well. This will also increase the reliability of BP.\nTo a large degree, we owe our current understanding of BP to concepts from statistical physics. As it turns out, there is a fundamental connection between many concepts in computer science and in physics (cf. [MM09,WT03,TJ02]). Most notably, the fixed points of BP are in a one-toone correspondence with the stationary points of the Bethe free energy.\nBut, although the Bethe free energy can provide many insights, it remains an intricate function that is hard to analyze. Also, most theoretical results on the Bethe free energy only hold for restricted model classes, where typically all variables sit on a regular grid and the model is specified by relatively few parameters. How those insights carry over to more general models is an open question.\nMoreover, the Bethe free energy fails to reveal whether a given fixed point is stable and under which conditions BP converges to it. The Bethe free energy also fails to explain why and how certain modifications of BP (e.g., scheduling or damping) often help to achieve convergence.\nThe quest for a better understanding of BP is thus ongoing, with the hope that new insights will recognize certain model classes for which it is safe to utilize BP, i.e., for which BP converges fast while maintaining the desired accuracy.\nThe overarching aim of this thesis is to extend the current understanding of BP, the reason for this being twofold: first, to theoretically understand for which problems and applications BP\n\u2013 20 \u2013 November 8, 2019\n1.2 Five Relevant PhD Theses\ncan be expected to perform well; and second, to utilize those theoretical insights and modify BP to enhance its capabilities.\nWe specifically address the question of how knowledge of the solution space can advance our current understanding of BP. As the complete set of solutions is generally not available, we must first develop a way to obtain all fixed points for a given model. We then start with the analysis of relatively small and simple models and, by successively building upon our obtained insights, extend our analysis to increasingly more complex models. Finally, we take a comprehensive view at the solution space and study the relation between the convergence properties, approximation quality, and the number of fixed points. This approach will provide several insights into the behavior of BP, extend the current theoretical understanding and open the door for practical considerations that enhance the performance of BP. In particular, our findings suggest a modification of BP that enforces convergence toward accurate fixed points, thus improving the approximation quality."
        },
        {
            "heading": "1.2 Five Relevant PhD Theses",
            "text": "As discussed, BP touches a diverse set of scientific fields. It is the common ground between all those fields that provides a solid foundation for the analysis of BP. The work presented in this thesis therefore builds upon a vast body of literature. The following five PhD theses cover a wide range of the recent developments and are highly relevant for the current thesis. Therefore, this section contains a brief overview of how they shaped the current thesis in particular.\nThe connection between exponential representations of distributions, information geometry, and approximate inference methods on probabilistic graphical models is revealed in the thesis of Martin Wainwright, submitted at the Massachusetts Institute of Technology in 2002 [Wai02]. His thesis introduces the concept of reparameterization. This concept casts BP as one specific instance in a more general class of message passing algorithms (termed tree-based reparameterization (TRP)). Moreover, TRP suggests solving a particular sequence of simpler sub-problems over spanning trees in the graph [WJW03a] in order to enhance the convergence properties. Additionally, important insights with respect to the marginal accuracy are obtained in the form of an exact expression for the marginal error, that \u2013 although being infeasible to evaluate in general \u2013 suggests computable bounds on the marginal error. The proposed bounds rely on the approximation of the log-partition function which advocates a close connection between both quantities; this connection nicely connects to the present work where we inspect this relationship in great detail in Chapter 7. Another powerful concept is the consideration of BP as an optimization problem (over the local polytope). In the conclusion, the author proposes tracing the evolution of the pseudomarginals while relaxing the marginal- to the local polytope with the prospect of practical and theoretical consequences; a similar evolution of the pseudomarginals lies at the core of Chapter 6.\nThe overarching aim of the thesis of Joris Marten Mooij, submitted at the Radboud Universiteit Nijmegen in 2008 [Moo08], is to understand and improve belief propagation, which closely resembles the aim of the current thesis. In a nutshell, his thesis investigates the relationship between accuracy, uniqueness, and convergence properties of belief propagation\u2019s fixed points. One particular insightful contribution was the consideration of belief propagation as a dynamical system. This led to conditions for stability and uniqueness of a fixed point, as well as insights into the relation between those properties. The analysis, however, was restricted to vanishing local potentials. We adhere to the spirit of considering belief propagation as a dynamical system and extend the analysis to models with arbitrary parameters in Chapter 5, providing novel theoretical insights.\nThe application of belief propagation to inference problems arising in the context of sensor\nNovember 8, 2019 \u2013 21 \u2013\n1 Introduction\nnetworks is the main motivation in the thesis of Alexander Ihler [Ihl05], submitted at the Massachusetts Institute of Technology in 2005. His thesis focused on studying the fundamental limitations of belief propagation. Studying the vulnerability of the approximation quality with respect to errors in the messages was one particularly important aspect with the prospect of validating whether approximating the messages is a viable option. Notably, this leads to a couple of interesting theoretical results on the accuracy of belief propagation in general, and the introduction of error-bounds on the marginal accuracy specifically. While these bounds are only valid for cycle-free graphs, they predict the performance of belief propagation reasonably well in the presence of loops as well. This is one of the few results that analyze the error in the marginals, while much of the literature focuses on the approximation error of the partition function. We agree with the author on the importance of assessing the marginal accuracy and will particularly focus on how the model parameters influence the marginal accuracy (cf. Chapter 5 and 7).\nThe thesis [Wel14] of Adrian Weller, submitted at the Columbia University in 2014, focuses on the variational interpretation of belief propagation and the Bethe approximation in particular. His work provides interesting insights into the differences between log-partition function estimates, singleton marginals, and pairwise marginals for both the (non-convex) Bethe approximation and convex variational approaches. In particular, it becomes evident that different approximation methods do not affect the marginals and the log-partition function in the same way; the accuracy of the log-partition function may for example remain the same whereas the accuracy of the marginals increases. This observations raises the question of how both quantities are related, an important question that will be the main focus of Chapter 7. Besides the theoretical relevance of the obtained insights, his thesis further proposes an approximation of the Bethe function, which extends preceding work [Shi12] by approximating the global minimum. This approximation method converges in polynomial runtime for attractive models and serves as an important comparison for our proposed method in Chapter 6.\nA direct relationship between belief propagation and graph geometry is established in the thesis of Yusuke Watanabe [Wat10], submitted at The Graduate University for Advanced Studies, SOKENDAI in 2010; this relationship provides novel insights into the behavior of belief propagation. In particular, the graph-zeta function relates the convergence properties of belief propagation to the shape of the Bethe free energy.3 Moreover, the results impose some general properties on the solution space. These properties are of relevance for belief propagation as well and, for example, demonstrate that the overall number of fixed points is always odd. Computing the number of fixed points is also of central interest throughout the current thesis, and Chapter 5 and 7 in particular."
        },
        {
            "heading": "1.3 Contribution and Outline",
            "text": "A large part of the contributions to this thesis has previously been published; a list of the corresponding publications is presented below. Several parts, however, have been significantly reworked and restructured in order to align nicely with the structure of the thesis.\n\u2022 [KRTP15] Christian Knoll, Michael Rath, Sebastian Tschiatschek, and Franz Pernkopf. Message scheduling methods for belief propagation. In Proceedings of ECML PKDD, pages 295\u2013310. Springer, 2015.\n\u2022 [KPMC16] Christian Knoll, Franz Pernkopf, Dhagash Mehta, and Tianran Chen. Fixed point solutions of belief propagation. In NIPS-Workshop: Advances in Approximate Bayesian Inference, 2016.\n3 In addition to demonstrating the wide range of fields that play an important role for our current understanding of belief propagation, the graph-zeta function also serves as a foundation for recent developments in spectral clustering [SKZ14].\n\u2013 22 \u2013 November 8, 2019\n1.3 Contribution and Outline\n\u2022 [KP17] Christian Knoll and Franz Pernkopf. On loopy belief propagation \u2013 local stability analysis for non-vanishing fields. In Proceedings of UAI, 2017.\n\u2022 [KMCP18] Christian Knoll, Dhagash Mehta, Tianran Chen, and Franz Pernkopf. Fixed points of belief propagation \u2013 an analysis via polynomial homotopy continuation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018.\n\u2022 [KKP18] Christian Knoll, Florian Kulmer, and Franz Pernkopf. Self-guided belief propagation\u2013 a homotopy continuation method. arXiv preprint arXiv:1812.01339, 2018.\n\u2022 [KP19] Christian Knoll and Franz Pernkopf. Belief propagation: Accurate marginals or accurate partition function \u2013 where is the difference? In Proceedings of UAI, 2019.\nIt is the very nature of most PhD theses to pinpoint and raise multiple questions before providing some \u2013 hopefully insightful \u2013 answers to them. As is often the case, this thesis tackles very specific problems in an already highly specialized field. One aspect, however, that widens the scope and that made working on this thesis particularly interesting is that belief propagation is applied in various scientific fields ranging from information theory and signal processing to statistical physics and artificial intelligence. Our study of belief propagation brings insights from all these fields together and benefits from a particularly large toolbox, fueled by such diverse inputs. While it is interesting to delve into each particular field and emerge oneself in all the associated details, it is nearly impossible to hide one\u2019s past. Therefore, and for the purpose of a consistent notation, we settle for the language used in the statistics and machine learning community, point at relations to associated scientific fields if appropriate, and draw from them if beneficial.\nWriting a coherent thesis, even after completing the research tasks, remains an extensive task that should ideally serve the interested reader. Therefore, this thesis \u2013 rather than being a conglomerate of the presented results \u2013 offers a thorough introduction and sticks to one coherent story. In doing so, we identify an inherent structure that lends itself to a segmentation into two major parts.\nThe first part (Chapter 2 - 4) provides all relevant background and serves as the preparation for the subsequent chapters.\nThe whole thesis resides in the context of probabilistic graphical models, which are introduced in Chapter 2. We briefly discuss the relevant background from probability- and graph-theory and introduce pairwise graphical models that are the main focus of this work. Moreover, we define the problem of inference with a particular focus on efficient exact methods.\nBelief propagation (BP) is introduced as a method of approximate inference in Chapter 3. After discussing some of the most serious issues of BP, we describe some of the underlying reasons for failure of BP and introduce alternative characterizations of BP. In particular, we introduce BP as a variational method and connect the properties of BP with the energy landscape of the Bethe free energy. Additionally, we cast BP as a dynamical system. It turns out that this characterizations provides a general framework, encompassing a wide range of approximate inference methods. Moreover, the consideration as a dynamical system suggests various ways to enhance the properties of BP.\nChapter 4 provides the reader with the most important background of dynamical system theory and algebraic geometry. While this chapter does not contain novel insights, it provides us with the relevant tools for analyzing BP in detail. In particular, the most prominent methods for solving system of equations are introduced and discussed.\nThe second part (Chapter 5-7) builds upon the knowledge developed so far, presents the major results and extends the current understanding of BP\u2019s behavior.\nNovember 8, 2019 \u2013 23 \u2013\n1 Introduction\nThe consideration of BP as a dynamical system provides the foundation of the results developed in Chapter 5. We apply tools from dynamical systems theory, analyze the solution space of BP, and gain new insights into the behavior of BP; the focus on the whole solution space instead of just a single fixed point reveals how the number of fixed points, the approximation quality of the individual fixed points, and the convergence properties are related to each other. One major issue is that, despite being conceptually straightforward, it is problematic to find the set of all fixed points. We present how the set of all BP fixed points can be computed by using the numerical polynomial homotopy continuation (NPHC) method. This allows us to assess and compare the accuracy of the individual BP fixed points and weighted combinations thereof. Moreover, the knowledge of all fixed points allows us to extend the local stability analysis \u2013 previously restricted to models with vanishing local potentials \u2013 to more general models. This generalization also explains the role of the local potentials and reveals how strong local potentials enhance the convergence properties.\nThe focus of Chapter 6 is to exploit the theoretical finding that local and pairwise potentials play an opposing role regarding the performance of BP. In this chapter we propose one way to account for this observation and to enhance the performance of BP. By modifying BP according to a homotopy continuation method we account for this observation and incorporate the pairwise potentials only gradually. This procedure is deterministic, converges to a uniquely defined fixed point, and thus resolves the dependence on a well-chosen initialization. Experiments on a wide range of models reveal that the proposed method increases the performance without increasing the computational burden; in particular we exemplify that the results are at least as accurate as BP, if BP converges, and that accurate results are often obtained, even if BP fails to converge. This empirical analysis is further supplemented with a theoretical analysis that proves optimality \u2013 i.e., the proposed method converges to the global minimum of the Bethe free energy that constitutes the fixed point with the most accurate marginals \u2013 for restricted models that have all local potentials favoring the same state.\nAll models considered so far adhere to the common conjecture that accuracy with respect to the marginals and with respect to the partition function are interchangeable. This conjecture suggests that the most accurate marginals of BP are to be found at the global minimum of the Bethe free energy. In Chapter 7 we aim to validate this assumption. We therefore introduce patch potential models that simplify the analysis significantly. The parameter space of patch potential models is split into multiple regions with fundamentally different properties. Elaborating on one specific region we first exemplify why there is no strict relationship between the accuracy of the marginals and the partition function and then provide sufficient and necessary conditions for a fixed point to be optimal with respect to approximating both.\nFinally, Chapter 8 concludes this thesis, summarizes the results obtained, and discusses the extent to which our understanding of BP has changed. Furthermore, the most pressing questions left unanswered are indicated, hence providing a pointer to potential future research directions.\n\u2013 24 \u2013 November 8, 2019\nUnderstanding the Behavior of Belief Propagation\n2 Background\n\u201dA mind is like a parachute. It doesn\u2019t work if it is not open.\u201d\n\u2013 Frank Zappa\nThis chapter introduces the necessary background and lays the foundation for the subsequent chapters. The focus of this thesis lies at problems that arise in the context of probabilistic graphical models. Accordingly, we begin with a brief introduction to probability theory, in Section 2.1, and to graph theory, in Section 2.2, before we finally unite both concepts in probabilistic graphical models, in Section 2.3. Section 2.4 discusses one specific class of probabilistic graphical models, undirected binary pairwise models, that are studied extensively throughout the thesis. We finally devote Section 2.5 to the tasks summarized under the notion of inference."
        },
        {
            "heading": "2.1 Probability Theory",
            "text": "We begin with a brief review of probability theory and introduce the most relevant notions. Note that we will restrict our focus to discrete random variables throughout this thesis. Let us therefore consider a discrete random variable X that maps from the sample space \u2126 to a discrete finite set X , i.e., X : \u2126\u2192 X .\nWe denote the probability mass function that assigns a probability to the generic value x \u2208 X by the shorthand notation PX(x) = PX(X = x). Likewise, let X and Y be two discrete random variables; then, with slight abuse of notation we define the joint distribution according to\nPX,Y(x, y) = PX,Y ((X = x) \u2229 (Y = y)) . (2.1)\nNow let us consider a set of N discrete random variables X = {X1, X2, . . . , XN} with the joint distribution PX(x) where the range of X is the product space XN = X1\u00d7 \u00b7 \u00b7 \u00b7 \u00d7XN . We further denote the configuration for a given set of random variables by x = {x1, x2, . . . , xN}. The marginal probability distribution for a subset Y \u2282 X is obtained by summing out all variables Xi that are not in Y according to\nPY(y) = \u2211\nXi\u2208{X\\Y} \u2211 xi\u2208Xi PX(x). (2.2)\n(Conditional) statistical independence between random variables is a particularly important property when dealing with probabilistic graphical models. We say that X is conditionally independent of Y given Z whenever PX,Y|Z(x,y|z) = PX|Z(x|z)PY|Z(y|z) holds for all configurations x \u2208 XN , y \u2208 YN , and z \u2208 ZN .\nDistributions are often characterized by their expectations. For a discrete random variable X we define its expectation under the distribution PX(x) according to\nE(x) = \u2211 x\u2208X x \u00b7 PX(x). (2.3)\nNovember 8, 2019 \u2013 25 \u2013\n2 Background"
        },
        {
            "heading": "2.2 Graph Theory",
            "text": "One of the most persuasive properties of probabilistic graphical models is the representation of joint probability distributions by using graphs. This section gives a self-contained introduction to the basics of graph theory. We refer the interested reader to one of the many available books on graph theory for a more in-depth treatment (e.g., [KV05,Die16]).\nLet us consider a graph G = (X,E) with a set of nodes (or vertices) X = {X1, . . . , XN} and a set of edges E. A graph is either directed or undirected and consists of directed or undirected edges respectively. We denote an undirected edge by (i, j) \u2208 E (or equivalently by (j, i)) if it joins two nodes Xi \u2208 X and Xj \u2208 X. Note that, for the remainder of this thesis, we will only consider undirected graphs and further restrict our focus to simple graphs \u2013 that come without parallel edges (that would join the same pair of nodes twice) and without self-loops (i.e., i 6= j).\nLet us consider an edge (i, j) \u2208 E, then Xj is a neighbor of Xi and vice versa; the set of neighbors for any variable Xi \u2208 X is defined by\n\u2202(i) = {Xj \u2208 X : (i, j) \u2208 E}. (2.4)\nThe degree of a node Xi is the number of incident edges and is consequently defined by the cardinality of the neighbor-set, i.e., by\ndi = |\u2202(i)|. (2.5)\nWe further denote the average degree of G by d\u0302G = 2|E|N . A path between two nodes Xi, Xk \u2208 X is a graph P = ({Xi, . . . Xk}, {(i, i+ 1), . . . , (k \u2212 1, k)}), where Xi and Xk are the end-nodes of P. A path goes from Xi to Xk if there is an edge-progression that connects those two nodes.\nDefinition 2.2.1 (Loop). A loop is a path from a node Xi back to the same node along a sequence of edges, i.e., a path P = ({Xi, Xi+1, . . . , Xk}, {(i, i + 1), . . . , (k \u2212 1, k)}) exists for k = i.\nDepending on the notational conventions, loops are sometimes also referred to as cycles or circuits. Note that every loop has to contain at least three edges, since we do not allow for self-loops.\nA graph is said to be connected if, for every pair of nodes Xi and Xj , a path exists that connects Xi to Xj . We only consider connected graphs in this thesis.\nIn the context of this thesis, there are certain graph types that are particularly relevant; most notably these include the following:\nDefinition 2.2.2 (Tree). A connected graph that does not contain any loops is a tree.\nDefinition 2.2.3 (Complete Graph). A complete graph G = (X,E) has every pair of nodes Xi and Xj connected by an edge (i, j); i.e., G is fully connected with E = {(i, j) : Xi, Xj \u2208 X, i 6= j}. It follows that every node has an equal degree di = N \u2212 1. See Figure 2.1 for an illustration of complete graphs of size one to four.\nDefinition 2.2.4 (Grid Graph). An m\u00d7n grid graph, or lattice graph, is a graph with N = mn nodes that has all edges aligned along the square lattice. See Figure 2.2 for an illustration of a two-dimensional grid graph.\nIt will often be necessary to consider only a part of the graph; we call this a subgraph.\nDefinition 2.2.5 (Subgraph). Let G = (X,E) be a graph and let X\u2032 \u2286 X. Then, the subgraph (or graph-component) G\u2032 = (X\u2032,E\u2032) is induced by X\u2032, where E\u2032 = {(i, j) \u2208 E : Xi, Xj \u2208 X\u2032}.\n\u2013 26 \u2013 November 8, 2019\n2.3 Probabilistic Graphical Models\nComplete subgraphs are of particular relevance and are often referred to as cliques.\nDefinition 2.2.6 (Clique). Let G\u2032 = (X\u2032,E\u2032) be a subgraph. If G\u2032 is complete we call it a clique and refer to it as Ci. If any adjacent node Xj \u2208 X\\X\u2032 : (i, j) \u2208 E, Xi \u2208 X\u2032 exists that, by adding, would render Ci not complete anymore, Ci is a maximal clique.\nSee Figure 2.1 that depicts the maximal cliques of size one to four. Note that any graph G = (X,E) is complete if and only if G is a maximal clique.\nDefinition 2.2.7 (Regular Graph). Let G = (X,E) be a graph where all nodes Xi \u2208 X have equal degree di = d. Then, we refer to G as a d-regular graph.\nIt follows that the average degree for a regular graph equals the degree of all individual nodes, i.e., d\u0302G = di.\nDefinition 2.2.8 (Bipartite Graph). A bipartite Graph G = (X,E) is a graph that decomposes into two disjoint subgraphs Y and Z where X = Y \u222a Z and Y \u2229 Z = \u00d8 so that every edge connects those two subgraphs; i.e., for all Yi \u2208 Y we have \u2202(Yi) \u2208 Z and vice versa.\nDefinition 2.2.9 (Adjacency Matrix). The adjacency matrix A represents the connections of a finite graph. A is a 0\u2212 1 matrix with rows and columns indexed by the set of nodes so that aij = 1 if and only if (i, j) \u2208 E. Note that A is symmetric for undirected graphs."
        },
        {
            "heading": "2.3 Probabilistic Graphical Models",
            "text": "Probabilistic graphical models provide a compact representation of joint distributions and are particularly well suited for representing distributions with many random variables. The straightforward specification of a distribution in N random variables requires one to define and store the probabilities of all |X |N configurations; considering the fact that typical problems often have\nNovember 8, 2019 \u2013 27 \u2013\n2 Background\nhundreds of random variables renders such an approach impracticable. The representation of the joint distribution by the means of a graph on the other hand is intuitive and comes with the advantage of being interpretable by humans. Moreover, and of even greater relevance, the graph exploits the statistical dependencies of the distribution and makes them explicit. This is required if confronted with (even moderately) high-dimensional joint distributions.\nProbabilistic graphical models come in different forms that represent the statistical dependencies in slightly different ways. These include the most prominent types such as factor graphs, Bayesian networks, and Markov random fields. We will, however, restrict our focus to Markov random fields (which are also termed undirected graphical models)."
        },
        {
            "heading": "2.3.1 Undirected Graphical Models (Markov Random Fields)",
            "text": "A probabilistic graphical model U = (G,\u03a8) consists of an undirected graph G = (X,E) and a set of K potentials (sometimes called clique potentials, or compatibility functions) \u03a8 that defines the joint distribution PX(x).\nThere is a certain elegance to graphical models; one that relates the structural properties of the graph to the properties of the associated joint distribution. Specifically, a one-to-one correspondence between the set of nodes X = {X1, . . . , XN} and the set of random variables4 holds. Additionally, each edge (i, j) represents the existence of a statistical dependency between Xi and Xj .\nThe correspondence between edges and statistical dependencies manifests itself into the global Markov property. Let U \u2282 X be a subset of nodes; we say that U separates two disjoint (sets of) nodes V and W (i.e., V \u2229W = \u00d8) if every path connecting the sets V and W contains at least one variable Ui \u2208 U. Removing the subgraph induced by U from G thus eliminates all paths between V and W. Let us assume that U separates V and W as discussed for G; then a joint distribution PX(x) satisfies the global Markov property with respect to G if\nPV,W|U(v,w|u) = PV|U(v|u)PW|U(w|u). (2.6)\nSee Figure 2.3 for a visualization of a graph that satisfies (2.6). The global Markov property entails strong restrictions on the factorization properties of the joint distribution. Specifically, according to the Hammersley-Clifford-Theorem [HC71], the joint distribution factorizes into a set of potentials \u03a8 = {\u03a6C1 , . . . ,\u03a6CK} specified over the set of cliques C = {C1, . . . , CK} of the graph. That is\nPX(x) = 1 Z \u220f Ci\u2208C \u03a6Ci(xCi), (2.7)\nwhere the potentials \u03a6Ci(xCi) only depend on the values of random variables belonging to the given clique, i.e., xCi = {xi : Xi \u2208 Ci}. Although the potentials need to be non-negative and may remind us of conditional probabilities, it is important to stress that this is not the case and that potentials do not necessarily have a specific probabilistic interpretation (cf. [KF09, Example 4.2]). The flexibility one has in assigning values to the potential function comes at a cost, however: the product over all potential functions may require a normalization function (partition function) Z to ensure that the product in (2.7) constitutes a valid distribution.5\nThe product in (2.7) can, in principle, consist of potentials specified only over the maxcliques of the graphs. This may, however, hide the factorization properties of the underlying problem [KF09, p.108] and it is thus often preferable to consider only smaller (sub)-cliques as\n4 We consider only random variables with a discrete alphabet X although the framework would generalize to continuous random variables with X = R as well. 5 The partition function is evaluated by computing the unnormalized sum over all states and is usually denoted by the letter Z to denote its origin in the German word Zustandssumme.\n\u2013 28 \u2013 November 8, 2019\n2.4 Binary Pairwise Models\ne.g., of size two for pairwise models. Pairwise models are particularly well-suited for a theoretical analysis because of their simplicity and will therefore be the main focus of this thesis. Because of their central role, we devote the following section to the introduction of binary pairwise models."
        },
        {
            "heading": "2.4 Binary Pairwise Models",
            "text": "Throughout the main part of this thesis, we will focus on one particular type of undirected models: these are binary pairwise graphical models. Binary pairwise models admit a relatively simple treatment while being rich enough to represent a wide class of problems. Indeed, most practical problems permit a representation as a binary pairwise graphical models, which highlights that the class is much less restrictive as it may seem at first.\nWe will first provide a formal introduction in Section 2.4.1 and then discuss both the limitations and the relevance of binary pairwise graphical models in Section 2.4.2. Finally, we introduce the exponential representation in Section 2.4.3 and clarify the relation to the Ising model in Section 2.4.4."
        },
        {
            "heading": "2.4.1 Model Description",
            "text": "Binary models are graphical models where every random variable Xi \u2208 X takes values from the binary alphabet, e.g., according to xi \u2208 X = {\u22121,+1}. Pairwise models have potential functions that consist of two random variables at most; i.e., potentials are only associated with cliques Ci \u2208 C of size |XCi | \u2264 2. We resort to an even finer-grained representation that separately specifies the potentials over the nodes and edges. Complying to this representation, we then\nNovember 8, 2019 \u2013 29 \u2013\n2 Background\ndefine the joint distribution according to\nPX(x) = 1 Z \u220f\n(i,j)\u2208E \u03a6Xi,Xj (xi, xj) \u220f Xi\u2208X \u03a6Xi(xi), (2.8)\nwhere we introduced two different potential-types. These are: the pairwise potentials \u03a6Xi,Xj (xi, xj) that act on the edges (i, j) \u2208 E and the local potentials \u03a6Xi(xi) that act on the nodes Xi \u2208 X. Note that each pairwise potential is only considered once as (i, j) = (j, i). We will prefer to denote the local and pairwise potentials by their shorthand notation \u03a6(xi) and \u03a6(xi, xj), unless the actual values of the associated random variables are of immediate relevance.\nNote that the factorization in (2.7) considers the set of all cliques (with varying sizes), whereas the factorization in (2.8) considers cliques of size two at most. Therefore, note that every connected pair of variables is also a clique (albeit possibly only a subset of a larger clique). This means that both forms become identical \u2013 even in the presence of larger cliques \u2013 whenever all higher-order potentials are trivial.6\nThe representation in (2.8) has yet another equivalent factorization\nPX(x) = 1 Z \u220f\nC(i,j)\u2208C \u03a6C(i,j)(xi, xj)\nthat incorporates all local potentials into the pairwise ones. Such a compact representation clearly reduces the overall number of potentials, but may obscure the underlying structure. This often makes the model much less intuitive to interpret. We will now discuss and compare the different ways of factorizing the joint distribution by means of the following example.\nExample 1 (Factorization into Potentials of Varying Size). Consider the undirected graphical model U = (G,\u03a8) depicted in Figure 2.4.\nWe consider a joint distribution that is specified according to (2.8) in terms of local potentials \u03a6(xi) and pairwise potentials \u03a6(xi, xj) so that\nPX(x)= 1\nZ\u03a6(x1, x2)\u03a6(x2, x3)\u03a6(x3, x1)\u03a6(x1)\u03a6(x2)\u03a6(x3). (2.9)\nAs discussed above, one possible factorization stems from incorporating the local potentials into the pairwise ones. The specific model in Figure 2.4 provides one obvious way of doing\n6 A trivial potential does not influence the values of the joint distribution. One way of making a potential trivial is by assigning identical values, as for example \u03a6C(xC) = 1, for all possible configurations xC .\n\u2013 30 \u2013 November 8, 2019\n2.4 Binary Pairwise Models\nso; therefore, let us define the clique potentials according to\n\u03a6C(i,j)(xi, xj) = \u03a6(xi, xj)\u03a6(xj) (2.10)\nso that\nPX(x) = 1\nZ\u03a6C(1,2)(x1, x2)\u03a6C(2,3)(x2, x3)\u03a6C(3,1)(x3, x1). (2.11)\nNote that (2.10) is just one possible way of constructing clique potentials from the local and the pairwise ones. One only has to make sure that every potential is considered exactly once, as the joint distribution in (2.11) would not conform with its original definition in (2.9) otherwise.\nFinally, it is always possible to factorize the joint distribution over the maximum cliques. Note that, in this example, G is already a maximum clique, which makes it possible to express the joint distribution by a single clique potential. This potential of the maximum clique incorporates all pairwise- and singleton- potentials according to\n\u03a6C(1,2,3)(x1, x2, x3) = \u03a6(x1, x2)\u03a6(x2, x3)\u03a6(x3, x1)\u03a6(x1)\u03a6(x2)\u03a6(x3) (2.12)\nso that\nPX(x) = 1\nZ\u03a6C(1,2,3)(x1, x2, x3). (2.13)\nThe above example highlights the influence of the particular factorization on the overall number of potentials. Higher-order cliques reduce the overall number of potentials but, simultaneously, tend to conceal the underlying structure of the joint distribution [KF09, Chapter 4.2]). Similarly, pairwise graphical models enforce one specific factorization and may thus hide the \u201ctrue\u201d factorization of the underlying distribution as well. Factor graphs, on the other hand, provide a flexible representation that makes this underlying factorization explicit. We will, however, except for the application to error-correcting codes, restrict ourselves to pairwise graphical models for the remainder of this thesis."
        },
        {
            "heading": "2.4.2 Generality of Binary Pairwise Models",
            "text": "The simplicity of binary pairwise models is compelling, although the exclusion of other models \u2013 not representable by binary pairwise models \u2013 may seem rather restrictive. Despite this impression, binary pairwise models provide a general framework. That is, most graphical models can be directly converted into a binary pairwise model [YFW01,Wei00].\nThe conversion from general undirected models to binary pairwise models becomes immediately apparent for models with strictly positive potentials, in which case, a simple reduction to the binary pairwise case exists [EG13]. Note, however, that such a conversion potentially scales up the problem immensely and is therefore not always ideal from a practical point of view [Mac01].\nFrom a theoretical point of view, however, it is beneficial to study binary pairwise models; these models are rich enough to exhibit complex behavior \u2013 thus demonstrating many interesting aspects, though they still admit a simplified treatment \u2013 thus avoiding many technical subtleties. Moreover, the study of binary pairwise models carries great relevance; not only because they are still far from being fully understood but also because theoretical insights carry over to more general models (by conversion of one model-class to the other).\nNovember 8, 2019 \u2013 31 \u2013\n2 Background\nBesides their generality, binary pairwise models are important in their own right as they arise in various applications."
        },
        {
            "heading": "2.4.3 Exponential Representation and Model Parametrization",
            "text": "So far we have not considered the actual specification of the potentials. We will now introduce one specific parameterization of binary pairwise models, show why it belongs to an exponential family, and define different model-classes with fundamentally different behavior. Besides, we will fix our naming convention and briefly point to alternative ones. Note that, because of its immediate connection, the terminology is often rooted in the physical interpretations of the quantities.\nMany relevant problems have a non-zero probability for all configurations x, such that the joint distribution factorizes into strictly positive potentials. This allows us to assign some energy E(x) to every configuration, so that the joint distribution from (2.8) can be expressed in its exponential form according to\nPX(x) = 1 Z \u00b7 e \u2212E(x). (2.14)\nFor the remainder of this thesis we shall use a minimal representation of binary pairwise models (cf. [WJ08, Section 3.3]) that defines the energy in terms of couplings Jij \u2208 R, that act on the edges (i, j) \u2208 E and local fields \u03b8i \u2208 R, that act on the nodes Xi \u2208 X. Note that we drop the subscripts and write Jij = J or \u03b8i = \u03b8 whenever the parameters are identical for all edges or for all nodes respectively.7\nLet the local and pairwise potentials of state xi \u2208 {\u22121,+1} be \u03a6(xi) = exp(\u03b8ixi) and \u03a6(xi, xj) = exp(Jijxixj). Then, if we plug these potentials into (2.8) we end up with a joint distribution that has its energy (cf. (2.14)) given by\nE(x) = \u2212 \u2211\n(i,j)\u2208E Jijxixj \u2212 \u2211 Xi\u2208X \u03b8ixi. (2.15)\nThe energy (and consequently the joint distribution) depends not only on x but on Jij and \u03b8i as well; we will, however, only make this dependence explicit by E(x, J, \u03b8) if of immediate relevance.\nIn the literature, one distinguishes two different types of interactions between variables: if a coupling is positive (Jij > 0) then the associated edge (i, j) is attractive; if a coupling is negative (Jij < 0) then the associated edge (i, j) is repulsive. In accordance with this naming-convention, we call a model U attractive if it contains only attractive edges (these models are also known as ferromagnetic models [MM09] or log-supermodular models [Ruo12]); we call it repulsive (or antiferromagnetic) if it contains only repulsive edges; and we call a model general if it contains both types of edges.\nThe particular representation of (2.15) is also known as Hopfield network (cf. [Hop82] and [Mac03, Chapter 42]) or Boltzmann machine (cf. [HS86,WT03] and [Mac03, Chapter 43]) in the machine learning community and as Ising model (cf. [Bru67] and [Mac03, Chapter 31]) in the physics literature. The Ising model has been studied for a long time in statistical physics. For its particular relevance, we will devote the subsequent section to the Ising model, discuss its most important properties, and provide a brief historical outline of its development.\n7 If all nodes have the same value \u03b8i = \u03b8 we will sometimes refer to \u03b8 as the external field.\n\u2013 32 \u2013 November 8, 2019\n2.4 Binary Pairwise Models"
        },
        {
            "heading": "2.4.4 The Ising Model",
            "text": "The study of pairwise models in the form of (2.14) actually dates back for more than a century.8 In the field of statistical physics, one studies the macroscopic behavior of systems with a large number of interacting components (e.g., atoms or molecules) in the thermodynamic limit of infinitely many components.\nOne concept of central relevance are phase transitions; these are points in the parameter space where the partition function becomes non-analytic. Phase transitions trigger fundamental changes in the system behavior, as for example the change from a fluid to gas. In computer science, we often encounter algorithms that have many \u201ccomponents\u201d interacting with each other (e.g., as in message passing algorithms); such algorithms often exhibit rapid performance-drops for certain points in the parameter space, which clearly remind us of phase transitions.\nOne extensively studied model in statistical physics is the Ising model: it consists of N atoms in a configuration \u03c3 = (\u03c31, . . . , \u03c3N ) with spins \u03c3i \u2208 {\u22121,+1} that lie on a d-dimensional lattice and that are magnetically coupled.9 As a model for magnetic bodies, the Ising model\u2019s phase transitions describe the change from the paramagnetic to the ferromagnetic region among others. The Ising model is one of the simplest models that is sufficiently rich to exhibit phase transitions; therein lies its appeal.\nThe interaction between two neighboring spins is specified by the coupling strength J that is either ferromagnetic (i.e., positive) or anti-ferromagnetic (i.e, negative). Two neighbors then energetically favor the same state in case of ferromagnetic interactions and the opposite state in case of anti-ferromagnetic interactions. The energy of a configuration \u03c3 is\nE(\u03c3) = \u2212\u03b2J \u2211\n(i,j)\u2208E \u03c3i\u03c3j +H N\u2211 i=1 \u03c3i, (2.16)\nwhere H is the external magnetic field and \u03b2 = 1kBT is the inverse temperature with kB being the Boltzmann constant.\nThe appreciation of the Ising model had its ups and downs. After being dumped as a too simplified model with no physical usefulness, it took some years before the Ising model received its well-deserved attention. We outline some of the cornerstones in this development below and refer the interested reader to the surprisingly exciting read [Bru67] for a review of the major events in the history of the Ising model.\nWilhelm Lenz proposed the Ising model, as a simplified model for the interactions inside magnetic bodies, with the aim of developing a better understanding of the underlying properties. One of his students, Ernst Ising, solved the one-dimensional case (by applying the transfermatrix method) in his thesis. The result, rather surprisingly, revealed the inexistence of phasetransitions in the one-dimensional case. Being unable to solve it in higher dimensions, he concluded that the Ising model is generally incapable of experiencing phase transitions. This conjecture, however, was in stark contrast to the ferromagnetic theory developed by Pierre Currie, which led to the disregard of the Ising model.\nFinally, this contradiction was resolved by Lars Onsanger who solved the Ising model on the two-dimensional grid in a mathematical \u201ctour-de-force\u201d (as the Onsanger solution is nowadays often referred to) and revealed the existence of phase transitions in two dimensions. Note that the exact solution of the Ising model is only known in these two cases and it remains an open\n8 Distributions of this form were first introduced by Ludwig Boltzmann and by Josiah Willard Gibbs and provide the foundation of statistical physics. In the physics literature, one often refers to distributions of the form (2.14) as Boltzmann-distributions or Gibbs-measures. 9 Here we use the standard notation for the Ising model, although the correspondence to the notation in this thesis becomes obvious by comparing (2.15) to (2.16).\nNovember 8, 2019 \u2013 33 \u2013\n2 Background\nproblem to compute the exact solution for higher-dimensional cases.\nThe classical Ising model, with its energy defined according to (2.16), provides a powerful generalization: the spin glass model has the (local) fields \u03b8i and the couplings Jij take potentially different values for all nodes and edges and has its energy defined according to\nE(\u03c3) = \u2212\u03b2 \u2211\n(i,j)\u2208E Jij\u03c3i\u03c3j + N\u2211 i=1 \u03b8i\u03c3i. (2.17)\nWhat makes these spin glass models so interesting is that we have a much poorer understanding of them as opposed to the classical Ising model. The lack of understanding is mainly because of frustrations [Ge\u030177]. These are configurations that have some pairs of random variables Xi, Xj energetically favor states that contradict the coupling of the associated edge (i, j) (cf. Example 4). To make this more precise we introduce the product of couplings along some cycle P, i.e.,\nJP = \u220f P Jij . (2.18)\nThen, a graph is frustrated whenever it contains a cycle for which JP equates to a negative number. For the classical Ising model where all couplings take the same value, the existence of frustrations depends only on the graph structure (i.e., if cycles of odd-length exist). For spin glasses, however, the existence of frustrations depends on both the graph structure and the parameters. This is one of the main reasons for studying spin glasses as frustrations can lead to a complex solution space with potentially many different solutions.\nSo far, we have seen that binary pairwise models constitute a very general class of graphical models and fit nicely into the framework of statistical physics. Although this emphasizes the relevance of binary pairwise models, we barely mentioned the purpose of introducing graphical models. We will now shift our focus to the problems we intend to solve and thus reveal the elegance and the advantages of graphical models."
        },
        {
            "heading": "2.5 Inference",
            "text": "The task of inference plays an important role in many scientific fields and it is a prerequisite for probabilistic reasoning. In a nutshell, inference deals with drawing statistical conclusions about a certain subset of random variables, given some (possibly noisy) observations [PPT14].\nApplications include, but are not limited to, computer vision and speech processing where the observations are corrupted versions of the image or the speech-signal and one is interested in finding the most probable explanation. Ideally, one would hope that the best explanation for the given observations matches the original image or signal. Another important application is found in the context of error-correcting codes, where the observation is the received codeword and one is interested in obtaining the sent codeword.\nTo make the task of inference more precise we consider a joint distribution PX(x) over a set of nodes X that consists of two disjoint subsets, i.e, X = Y \u222a O and Y \u2229 O = \u00d8. The observed variables are denoted by O and the unobserved ones are denoted by Y. Without loss of generality, we will restrict ourselves to problems without observed variables O = \u00d8 throughout this thesis. Now we present the following three central problems of probabilistic inference.\n\u2022 Maximum a posterior (MAP) inference estimates the mode of the distribution with the aim of identifying the joint assignment y\u2217 that maximizes the probability according to\n\u2013 34 \u2013 November 8, 2019\n2.5 Inference\ny\u2217 = arg maxy\u2208Y|Y| PY|O(y|o). Note that, strictly speaking, this is the most probable explanation (MPE) estimate, whereas the MAP estimate deals with the more general problem of maximizing the probability for any subset of Y; it is quite common, however, to neglect this distinction despite MPE inference being the easiest instance [KFD+07, Chapter 2.3]. We abide to this convention and, provided O = \u00d8, aim to obtain\nx\u2217 = arg max x\u2208XN PX(x). (2.19)\n\u2022 Marginal inference is the task of computing the marginal distribution for a subset of random variables Y \u2282 X, i.e., to compute PY(y) according to (2.2). Note that we will sometimes use the following shorthand notation for this double-summation where\nPY(y) = \u2211 X\\Y PX(x) = \u2211 O PX(x) = \u2211 Xi\u2208O \u2211 xi\u2208X PX(x). (2.20)\nWe will be particularly interested in the singleton marginals PXi(xi) for single random variables and the pairwise marginals PXi,Xj (xi, xj) for pairs of random variables.\nFor binary models it is often more convenient to work with the expectations, which are the mean mi (or magnetization) and the correlation \u03c7ij , instead of considering the singleton marginals PXi(xi) and the pairwise marginals PXi,Xj (xi, xj) explicitly, where\nmi = E(Xi) = PXi(Xi = 1)\u2212 PXi(Xi = \u22121), (2.21)\n\u03c7ij = E(XiXj). (2.22)\n\u2022 Another important problem is to evaluate the partition function\nZ = \u2211\nx\u2208XN \u220f Ci\u2208C \u03a6Ci(xCi), (2.23)\nwhich is the normalization coefficient of the joint distribution in (2.7).\nWe will primarily focus on the problems of computing marginal distributions and evaluating the partition function in this thesis. Note that these two problems are in fact closely related as the marginal distribution PXi(xi) equals the ratio between a partial partition function Z(Xi) and the partition function:\nPXi(xi) = \u2211 X\\Xi PX(x) = \u2211 X\\Xi 1 Z \u220f Ci\u2208C \u03a6Ci(xCi),\n= 1 Z \u2211 X\\Xi \u220f Ci\u2208C \u03a6Ci(xCi),\n=\n\u2211 X\\Xi \u220f Ci\u2208C\n\u03a6Ci(xCi)\u2211 x\u2208XN \u220f Ci\u2208C \u03a6Ci(xCi) = Z(Xi) Z . (2.24)\nThe computation of the marginals seems relatively straightforward according to (2.24). Yet, there is one fundamental problem that prohibits the application of (2.24) to practical problems, which essentially boils down to the overall number of variables involved. Even if the joint distribution PX(x) is known \u2013 neglecting the fact that the memory complexity of storing PX(x) is exponential in the number of variables \u2013 computing PXi(xi) is problematic. More specifically,\nNovember 8, 2019 \u2013 35 \u2013\n2 Background\nconsider a joint distribution PX(x) specified over N random variables with k = |X | states. Then, the sum in the numerator of (2.24) is evaluated N \u2212 1 times and goes over k terms each time; i.e., in total computing a marginal distribution would require the summation over kN\u22121 terms in total.\nThis drastically limits the problem size for which the marginals can be evaluated in practice. On the positive side, however, we have already encountered the compact representation of probabilistic graphical models. We will subsequently show how to utilize this representation and how this opens the door for efficient inference methods. Before considering arbitrary models, we focus on models with certain graph structures that lend themselves to particularly elegant ways of performing inference. Ideally one should aim to exploit this representation beyond that and extend it to the central problems of inference."
        },
        {
            "heading": "2.5.1 Exact Inference: Efficient Methods",
            "text": "Tree-Structured Graphs\nChains and trees posses Markov properties that, if exploited properly, give rise to efficient inference methods. The statistical dependencies imposed by tree-structured models admit inference methods that, instead of manipulating the joint distribution directly, recursively perform local computations; this reduces the computational complexity immensely. Various inference methods were independently introduced in different fields (cf. Section 3.1 for a brief overview) that all rely on the very same principle to perform efficient inference. The basic principle is to perform a set of local computations, often interpreted as messages between random variables; this also explains the term message passing algorithms that is often used to unite all those algorithms. The following example demonstrates the underlying principles and highlights the efficiency of message passing algorithms.\nExample 2 (Exact Inference on a Tree). Consider the undirected graphical model U = (G,\u03a8) depicted in Figure 2.5. First, we express the joint distribution as the product over all pairwise clique potentials according to (2.7) so that\nPX(x) = 1 Z \u220f Ci\u2208C \u03a6Ci(xCi)\n= 1\nZ\u03a6C(1,4)(x1, x4)\u03a6C(2,4)(x2, x4)\u03a6C(3,4)(x3, x4)\u03a6C(4,5)(x4, x5)\u00b7 \u03a6C(5,6)(x5, x6)\u03a6C(6,7)(x6, x7). (2.25)\nNote that the factorization in terms of pairwise cliques equals the factorization in terms of maximum cliques for tree-structured models for the lack of loops.\nSecond, we compute the singleton marginals for one specific random variable, e.g., X5, by summing over all other variables such that\nPX5(x5) = 1 Z \u2211 x1\u2208X \u00b7 \u00b7 \u00b7 \u2211 x4\u2208X \u2211 x6\u2208X \u2211 x7\u2208X . \u220f Ci\u2208C \u03a6Ci(xCi) (2.26)\nA closer look at the clique potentials reveals the benefits of reordering of the summations. The total amount of summations becomes much more manageable if we make use of the commutative and the distributive law and rewrite (2.26) according to\n\u2013 36 \u2013 November 8, 2019\n2.5 Inference\nPX5(x5) = (\u2211 x4\u2208X\n\u03a6C(4,5)(x4, x5)\u00b7(\u2211 x1\u2208X \u03a6C(1,4)(x1, x4) )\n\ufe38 \ufe37\ufe37 \ufe38 \u00b514(x4)\n(\u2211 x2\u2208X \u03a6C(2,4)(x2, x4) )\n\ufe38 \ufe37\ufe37 \ufe38 \u00b524(x4)\n(\u2211 x3\u2208X \u03a6C(3,4)(x3, x4) )\n\ufe38 \ufe37\ufe37 \ufe38 \u00b534(x4)\n)\n\ufe38 \ufe37\ufe37 \ufe38 \u00b545(x5)\n\u00b7\n(\u2211 x6\u2208X \u03a6C(5,6)(x5, x6) (\u2211 x7\u2208X \u03a6C(6,7)(x6, x7) )\n\ufe38 \ufe37\ufe37 \ufe38 \u00b576(x6)\n)\n\ufe38 \ufe37\ufe37 \ufe38 \u00b565(x5)\n. (2.27)\nNote that we introduced the powerful notion of messages in (2.27); messages \u00b5ij(xj) provide a compact notational convention and express the consequence of summing over the respective variable. We say that \u00b5ij(xj) is passed along the edge from Xi to Xj (see Figure 2.5).\nThe desired marginal distribution PX5(x5) is then given by the normalized product of all incoming messages. We consequently realize from (2.27) that\nPX5(x5) = 1\nZ \u00b545(x5)\u00b565(x5). (2.28)\nNote how the messages \u00b545(x5) and \u00b565(x5) incorporate all messages of the sub-trees rooted in X4 and X6. Consequently, knowledge of only the two messages \u00b545(x5) and \u00b565(x5) is sufficient to compute the marginal PX5(x5) according to (2.28).\nThe reordering of the summations reduces the overall amount of required summations notably. Comparison of (2.26) and (2.27) reveals that the computational complexity reduces from O(|X |N ) to O(|X |2).\nRemember how \u00b545(x5) accounted for all messages that come from the subgraph induced by X\u2032 = {X1, X2, X3, X4}. This suggests a recursive rule for computing the messages without the need for explicitly rearranging the summations. In particular, we can compute the message \u00b5ij(xj) by taking the product of all incoming messages, except the one from Xj , times the\nNovember 8, 2019 \u2013 37 \u2013\n2 Background\npairwise potential \u03a6C(i,j)(xi, xj) so that\n\u00b5ij(xj) = \u2211 xi\u2208X \u03a6C(i,j)(xi, xj) \u220f Xk\u2208{\u2202(i)\\Xj} \u00b5ki(xi). (2.29)\nThe message \u00b5ij(xj) is a vector over all states xj \u2208 X . One can interpret this message as the belief of node Xi about the relative probabilities that Xj is in state xj given all the information 10 that is available to Xi, except from Xj . The notion of messages has a further advantage: if we wish to compute the marginals PXi(xi) for multiple nodes in the graphical model we can, instead of performing the procedure multiple times, reuse the already computed messages. It suffices to pass the messages back and forth throughout the whole graph only once. For a chain this means propagating the messages along both directions.11 If we wish to apply this procedure to a tree we must first fix some ordering, i.e., pick one node as root, before then passing the messages upwards from the leaves and then pass the messages down again.\nThe representation of the summations in (2.27) in terms of messages captures an important property of tree-structured graphical models. For every Xi \u2208 X it holds that any pair of its neighbors {Xk, Xl} \u2208 \u2202(i) is conditionally independent given Xi, i.e.,\nPXk,Xl|Xi(xk, xl|xi) = PXk|Xi(xk|xi)PXl|Xi(xl|xi). (2.30)\nThese statistical independence statements further impose one important property on the graph. Namely, that removing the edge (i, k) creates two disconnected subgraphs G(i) = ( X(i),E(i) ) :\nXi \u2208 X(i) and G(k) = ( X(k),E(k) ) : Xk \u2208 X(k). Note that the existence of any alternative path between G(i) and G(k), i.e., the existence of loops, would violate (2.30).\nGraphs with Loops\nSo far we have witnessed the capabilities of message passing algorithms for chains and treestructured models; one might ask how this concept generalizes and if it is able to cope with loops. This is a critical question for two reasons: efficient inference methods are essential when confronted with large and loopy models; and many problems of practical relevance correspond to loopy models. Every general-purpose inference method must, therefore, be able to cope with loops.\nUnfortunately, this is not the case for message passing algorithms. Loopy graphs violate the independence statements in (2.30) and thus prevent a straightforward generalization of message passing to loopy graphs.\nAs discussed, excluding models with loops is not a viable option either. Nonetheless, exploiting the Markov properties seemed promising. Fortunately, we can have both. That is, an algorithm exists that is both capable of accounting for loops and capable of exploiting the Markov properties. These seemingly conflicting intentions are satisfied by modifying the graph and carefully constructing certain subgraphs until we finally end up with a tree again.\nThe failure of message passing is demonstrated on a loopy graph in Example 3. Subsequently, we show how the graph needs to be modified to allow for efficient inference.\n10 We are using the term information here only figuratively in the sense that all necessary summations for evaluating PXi(xi) are subsumed in \u00b5ji(xi). This does not adhere to the formal definition of entropy as an information-theoretic measure. 11 In the particular case of a chain the messages actually equal the Chapman Kolmogorov equations.\n\u2013 38 \u2013 November 8, 2019\n2.5 Inference\nExample 3 (Exact Inference on Loopy Graphs). We create a loopy graph by adding the edge (5, 7) to the model of Example 2 (cf. Figure 2.6 (a)). Let us focus on the subgraph induced by X\u2032 = {X5, X6, X7}. We first observe that the conditional independence statements of (2.30) are violated as\nPX5,X7|X6(x5, x7|x6) 6= PX5|X7(x5|x7)PX6|X7(x6|x7); (2.31)\nand second that information flows from X7 towards X5 (and the remainder of the graph) via two different paths: once via P1 = ({X5, X7}, {(7, 5)}) and once via P2 = ({X5, X6, X7}, {(7, 6), (6, 5)}). As a result, the message \u00b557(x7), if computed according to (2.29), incorporates the belief stemming from X7 via one path and erroneously propagates it back to X7 via the other path.\nThe graphical model needs to be tree-structured in order to satisfy the formal requirements for message passing to work; this, however, is not the case as seen in Figure 2.6(a). We can, however, form super-nodes, consisting of multiple variables, until no more loops are present in the graph to leverage the power of message passing. For our current example one can simply group the variables X5, X6, and X7 together into X5\u22126\u22127 and define the pairwise potential between X4 and this super-node according to\n\u03a6C(4,5\u22126\u22127)(x4, x5, x6, x7) = \u03a6C(4,5)(x4, x5)\u03a6C(5,6)(x5, x6)\u03a6C(6,7)(x6, x7)\u03a6C(5,7)(x5, x7).\n(2.32)\nThe resulting graph is depicted in Figure 2.6 (b). Now that we have created a tree-structured model again (cf. Figure 2.6(b)), we can simply perform message passing on this modified graph and compute the marginals accordingly. Note how the new node X5\u22126\u22127 has an increased number of states according to |X ||C5\u22126\u22127| = |X |3 now. Despite the exponential growth of the state space, grouping variables together has its merits. In particular, it suggests an optimal way of rearranging the variables in the computation of the marginals that maintains the Markov properties despite the existence of loops.\nWe have seen in Example 3 that performing inference on loopy graphs increases the computational complexity. This is an immediate consequence of considering higher-order cliques in the construction of the junction tree (i.e., the modified graph). In general \u2013 and for models with multiple intertwined loops in particular \u2013 however, there is not just one but many possibilities of grouping variables together. Consequently, one should construct the junction tree such that the largest clique is kept as small as possible. If one wants to keep the complexity in check, it is thus of utmost importance to carefully construct the junction.\nOne might ask whether a modified graph (i.e., a clique- or junction-tree) without loops always\nNovember 8, 2019 \u2013 39 \u2013\n2 Background\nexists \u2013 and how it is constructed. Indeed, for every graphical model such a junction tree exists and the construction of an optimal junction tree \u2013 with as small cliques as possible \u2013 involves three major steps. These steps define the junction tree algorithm [LS88] that we will review below. We do not give an exhaustive introduction with all the details but rather aim to provide an overview that highlights the overall idea of the required operations.\nFirst, note that every graph G has an associated junction tree if and only if G is triangulated [LS88]. A graph is triangulated (or chordal) if every cycle of length four or greater has a chord (i.e., an edge that joins two nodes of the cycle). Usually, a graph possesses multiple valid triangulated graphs, the choice of which has a major influence on the overall computational complexity [KF09, Section 10.4]. Finding an optimal, that is a minimal, triangulation is NP-hard on its own but various heuristics exist that find reasonably good triangulation. Most of these heuristics rely on a particular form of variable elimination, whereas it depends on the given graph which method performs best [KF09, Section 9.4.3.2].\nSecond, the triangulated graph provides the basis for the construction of the junction tree. The nodes in the junction tree correspond to maximal cliques in the triangulated graph; if a variable is present in two cliques, the cliques are joined by an edge in the junction tree. A triangulated graph usually admits multiple junction trees of varying sizes. The construction of the junction tree thus has a major influence on the overall efficiency. Typically, one assigns a weight to each edge that corresponds to the number of variables included in both cliques and subsequently constructs a maximum spanning tree [KF09, Section 10.4.2].\nFinally, once the junction tree is constructed an efficient inference method, for example message passing, can be applied to the junction tree to yield the marginal distributions over all cliques. The singleton marginals are then obtained by direct summation over a clique containing the desired variable.\nThe junction tree algorithm is straightforward in principle and provides a graph-based approach that exploits the factorization properties for loopy graphs. Although the junction tree significantly reduces the complexity of exact inference, it is still of limited practical use. This stems from working with the maximal cliques of the triangulated graph and the exponential growth of the state space with the clique-size. The applicability of the junction tree algorithm is consequently limited by the size of its largest clique [BKvdEvdG01].\nNonetheless, there are two reasons that justify the introduction of the junction tree: First, despite its limitation to problems with relatively small tree-width, the junction tree is efficient in some sense; in particular, no general exact inference method exists that is computationally more efficient than the junction tree [Bis06, Section 8.4.6]. The junction tree thus serves as the method of choice for estimating the ground truth when assessing approximate inference methods in the subsequent chapters. Second, the junction tree algorithm reveals how the inherent properties of loopy graphs increase the complexity of exact inference; this further emphasizes the need for efficient approximation methods.\nIn the subsequent chapter, we will explicitly focus on loopy graphs and show how to tackle the related complexity issues. We take different points of view on performing approximate inference, extend the underlying concept of message passing to more general graphs, and discuss how these methods cope with the existence of loops. Ultimately, this search for efficient methods culminates in a wide range of available approximate inference methods.\n\u2013 40 \u2013 November 8, 2019\nUnderstanding the Behavior of Belief Propagation\n3 Approximate Inference: Belief Propagation\n\u201dThe stars bend like slaves to laws not decreed for them by human intelligence,\nbut gleaned from them.\u201d \u2013 Ludwig E. Boltzmann\nOwing to belief propagation\u2019s specific relevance for this thesis, we present belief propagation (BP) in detail. This includes the classical definition as a message passing algorithm in Section 3.2, as well as the variational interpretation in Section 3.3 that highlights the connection to methods from statistical physics. Moreover, we cast BP as a dynamical system and elaborate on the resulting implications in Section 3.5. The perspective of dynamical systems gives rise to a whole group of message passing algorithms for approximate inference, where every particular instance comes with its own pitfalls and limitations. We briefly discuss some popular variants of BP and describe how each one enhances the performance over the standard implementation in Section 3.5.2- 3.5.4.\nMuch of this chapter summarizes established textbook knowledge (cf. [KF09,MWJ99,MM09]) although we review some of the most recent developments as well. The representation of BP as a dynamical system is rather obvious and has thus been considered multiple times (e.g., in [MK05, RKDW10, TR06]); here, we emphasize the added value of doing so and show how casting BP as a dynamical system provides a unifying framework. The presentation of BP variants in Section 3.5.3 contains results developed in collaboration with Michael Rath, Sebastian Tschiatschek, and Franz Pernkopf [KRTP15]."
        },
        {
            "heading": "3.1 Motivation",
            "text": "In Chapter 2 we have seen how exact inference methods suffer from the existence of loops. In fact, exact inference is NP-hard [Coo90], unless the probabilistic graphical model is appropriately restricted, which, however, would rule out many models of practical relevance. This highlights the need for efficient approximate inference methods. Even approximate inference, however, is NP-hard if a certain accuracy is required [Rot96, DL93]. Note, however, that this pessimistic statement does not render approximate inference completely useless, but only indicates that specific models do exist for which achieving the desired accuracy is intractable.\nIf one wants to perform approximate inference, one can choose from a wide range of different methods. Before one can make a well-informed choice and select an appropriate variant it is necessary to have a good understanding of a given method\u2019s capabilities and limitations. Hence, developing this understanding is of central importance. Knowledge of the limitations and failure modes further has the advantage of suggesting ways to improve upon.\nWe will focus on one specific class of approximate inference methods. That is the class of message passing algorithms. As discussed in Section 2.5.1 message passing algorithms efficiently perform exact inference on tree-structured models; the existence of loops, however, has a severe effect on message passing and impedes the straightforward generalization (cf. Example 3). Nonetheless, the elegance and simplicity of performing only local operations remain indisputable\nNovember 8, 2019 \u2013 41 \u2013\n3 Approximate Inference: Belief Propagation\nand it is tempting, therefore, to ignore the existence of loops, apply the same principles, and hope for a reasonable outcome.\nIndeed, various approximate inference methods work according to this very principle. In fact, because of their appealing simplicity, similar concepts were independently introduced multiple times in different fields: Judea Pearl introduced message passing algorithms in the machine learning- and statistics-community [Pea88] for tree-structured graphs, terming it belief propagation. He already advocated the extension to loopy graphs as an approximate method [Pea88, Section 4.4] \u2013 nowadays often referred to as loopy belief propagation to emphasize the approximate nature of the method.12 In the information theory community, message passing algorithms were first introduced in the PhD Thesis of Robert Gallager [Gal68] for decoding low-density-paritycheck codes. The capabilities of message passing \u2013 known as the sum-product-algorithm \u2013 were, however, largely overlooked until the success of Turbo codes [BGT93]. Relying on the very same principles, this renewed interest finally put the original work into perspective and ultimately led to its well-deserved recognition. Very similar concepts are also applied with success in the signal-processing community as in the Kalman filter [Kal60] for state estimation, or in the Viterbi algorithm [Vit67] for hidden Markov models. Probably the first explicit application of local operations for approximating the global behavior has to be attributed to the physics community: Hans Bethe and Rudolf Peierls introduced this concept to the field of statistical physics with the aim of understanding the behavior of large, otherwise incomprehensible, interacting systems [Bet35,Pei36]. This approximation is known as the Bethe-Peierls approximation or the Cavity method.13"
        },
        {
            "heading": "3.2 Preliminaries",
            "text": "Here in this section, we will finally define belief propagation. Let us consider a binary pairwise graphical model U = (G,\u03a8). First, we define messages that are passed along the edges, where the message from Xi to Xj is denoted by \u00b5 n ij(xj) with n \u2208 Z numbering the current iteration.14 The messages are updated according to the recursive rule:\n\u00b5n+1ij (xj) \u221d \u2211 xi\u2208X \u03a6(xi, xj)\u03a6(xi) \u220f Xk\u2208{\u2202(i)\\Xj} \u00b5nki(xi). (3.1)\nTo compute the messages, BP collects all messages sent to Xi, except from Xj and multiplies this product with the local potential \u03a6(xi) and the pairwise potential \u03a6(xi, xj). Finally, the sum over all states xi \u2208 X is sent. In practice, the messages require some form of normalization [IFW05]; we will normalize the messages by \u03b1nij \u2208 R\u2217+ so that \u2211 xj\u2208X \u00b5 n ij(xj) = 1, which gives the messages a probabilistic interpretation. Note that all messages are consequently restricted to \u00b5ij(xj) \u2208 [0, 1] and the application of the update rule (3.1) does not change this.\nLemma 1. Normalized messages, sent from node Xi to Xj over (i, j) \u2208 E, represent probabilities and remain so under successive application of the BP update equation \u2013 provided all messages are initialized to be positive.\nProof. Positive potentials in (3.1) guarantee that all messages remain positive at every iteration.\n12 We will overload the terminology and, for the sake of brevity, always refer to it as belief propagation; irrespective whether the graph contains loops or not. 13 Nowadays, the connections between the coding- and the artificial intelligence community [KFL01] as well as the connections to the physics community [YFW05] are well established and may seem rather obvious in hindsight. Looking at the original literature, however, it becomes clear that, because of the different formalism, these findings were indeed rather surprising at first. 14 Two neighboring nodes are joined by a single edge by definition. Note, that information has to flow into both directions and that two distinct messages are passed along opposing directions over every edge.\n\u2013 42 \u2013 November 8, 2019\n3.2 Preliminaries\nConsequently, a normalization term \u03b1nij exists so that \u2211 xj\u2208X \u00b5n+1ij (xj) = 1.\nThe update equation in (3.1) closely resembles the recursive definition of the messages in Section 2.5.1, except for one fundamental difference. For tree-structured graphs the recursive definition of the messages results from simply reorganizing the summations. Consequently, the messages converge once all messages were passed up and down the tree. For loopy graphs, however, such a reorganization is not possible \u2013 since the recursive definition of a message, say \u00b5ij(xj), contains the message \u00b5ji(xi) itself. For the very same reason, the notion of passing the messages up and down the tree is simply not possible anymore. Instead, one can neglect the existence of loops, update the message according to the recursive definition, accept the approximate nature of this approach, and hope for the messages to converge to the fixed point messages \u00b5\u25e6ij(xj).\nAfter convergence of BP, one can approximate the marginals similar as for the tree-structured models. Note that the marginals were defined over any possible subset of random variables in (2.2), though we will only consider the singleton marginals PXi(xi) and the pairwise marginals PXi,Xj (xi, xj) in this thesis. In general, the marginals over any subset of random variables are computed by the product of incoming messages times the associated potentials. Accordingly, we approximate the singleton and pairwise marginals by\nP\u0303Xi(xi) = 1\nZi \u03a6(xi) \u220f Xk\u2208\u2202(i) \u00b5\u25e6ki(xi), (3.2)\nP\u0303Xi,Xj (xi, xj) = 1\nZij \u03a6(xi)\u03a6(xj)\u03a6(xi, xj) \u220f Xk\u2208{\u2202(i)\\Xj} \u00b5\u25e6ki(xi) \u220f Xl\u2208{\u2202(j)\\Xi} \u00b5\u25e6lj(xj), (3.3)\nwhere Zi, Zij \u2208 R\u2217+ guarantee that all probabilities sum to one. We denote the set of all approximated singleton- and pairwise marginals by\nP\u0303B = {P\u0303Xi(xi), P\u0303Xi,Xj (xi, xj) : Xi \u2208 X, (i, j) \u2208 E}, (3.4)\nand refer to P\u0303B as the pseudomarginals. 15 This naming-convention should highlight the fact that, in general, BP only approximates the marginals. In fact, one may end up with pseudomarginals that do not correspond to any valid distribution at all.\nExample 4 (Unrealizable Pseudomarginals). Let us consider the pairwise graphical model in Figure 3.1 where X is a set of three binary random variables with xi \u2208 X = {\u22121, 1}.\nThe model is frustrated and contains two attractive edges with Jij = 1 (depicted by solid lines) and one repulsive edge with Jij = \u22121 (depicted by the dashed line).\nLet us specify the pairwise potentials so that\n\u03a6X1,X2(x1, x2) =\n[ exp(J) exp(\u2212J)\nexp(\u2212J) exp(J)\n] ,\n\u03a6X2,X3(x2, x3) =\n[ exp(J) exp(\u2212J)\nexp(\u2212J) exp(J)\n] ,\n\u03a6X3,X1(x3, x1) = [ exp(\u2212J) exp(J) exp(J) exp(\u2212J) ] .\n15 Note that the pseudomarginals are also often called beliefs, hence the name belief propagation.\nNovember 8, 2019 \u2013 43 \u2013\n3 Approximate Inference: Belief Propagation\nWe further set all local potentials to identical values and choose \u03b8 = 0 so that\n\u03a6Xi(xi) =\n[ exp(\u03b8)\nexp(\u2212\u03b8)\n] = [ 1 1 ] . (3.5)\nBelief propagation converges to a unique fixed point defined by \u00b5\u25e6ij(xj) = 0.5 for all xj \u2208 X . It is well-established that this particular fixed point exists and does not depend on the pairwise potentials if \u03b8 = 0 (cf. Section 5.7.1). The fact that these messages \u00b5\u25e6ij(xj) = 0.5 constitute a fixed point becomes immediately obvious if we plug in all potentials into the update equation (3.1).\nThe pairwise marginals are subsequently approximated by P\u0303Xi,Xj (xi, xj); in particular we have\nP\u0303X1,X2(x1, x2) = [ 0.44 0.06 0.06 0.44 ] , (3.6)\nP\u0303X2,X3(x2, x3) = [ 0.44 0.06 0.06 0.44 ] , (3.7)\nP\u0303X3,X1(x3, x1) = [ 0.06 0.44 0.44 0.06 ] . (3.8)\nNow we will proof by contradiction that the pairwise marginals from above are unrealizable; i.e., one cannot specify a joint distribution over three random variables that has its pairwise marginals correspond to (3.6) - (3.8). To make this more precise, note that by (3.6) and by the sum-rule we have P\u0303X(\u22121,+1,\u22121) + P\u0303X(\u22121,+1,+1) = 0.06, so that the joint probability for both configurations must satisfy P\u0303X(\u22121,+1,\u22121) ! \u2264 0.06 and P\u0303X(\u22121,+1,+1) ! \u2264 0.06.\nAccordingly, (3.6) - (3.8) reveal the joint probabilities for all configurations x \u2208 |X |N . In this example all joint probabilities must satisfy\nP\u0303X(x) ! \u2264 0.06. (3.9)\nThe total probability is thus bounded from above according to\u2211 x P\u0303X(x) \u2264 2N \u00b7 0.06 = 0.48, (3.10)\nwhich violates the fundamental rules of probability. Consequently, the pseudomarginals obtained by BP cannot belong to a valid joint distribution.\n\u2013 44 \u2013 November 8, 2019\n3.3 Variational Interpretation of BP \u2013 A Physics Perspective\nThe above example demonstrated why the pseudomarginals may fail to represent a valid distribution. Still, unrealizable pseudomarginals need not pose a serious problem. In fact, we usually resort to BP whenever evaluating the exact marginals is not possible, so that approximating the marginals reasonably well is often sufficient (even if they are unrealizable). The main motivation for considering BP was the hope for its efficient nature to carry over to loopy graphs. This is unfortunately not always the case and, in the presence of loops, BP \u2013 besides only approximating the marginals \u2013 may fail to converge altogether.\nNonetheless, despite all these shortcomings, BP often works surprisingly well, even for models that contain many loops. So far we have depicted BP as a pure heuristic without any theoretical motivation. This is unsatisfactory, and the situation was exactly like this for many years until a close relation to variational methods was revealed. The variational perspective of BP is appealing for several reasons: it relates to a similar problem, well studied in statistical physics; it provides a more lucid justification of BP; and it explains the approximate nature of BP. It is for all those reasons that the variational perspective spurred much of the research on BP in the last decade and significantly improved the theoretical understanding of BP."
        },
        {
            "heading": "3.3 Variational Interpretation of BP \u2013 A Physics Perspective",
            "text": "Ultimately, the aim of approximate inference is to approximate some unknown complex distribution in a way that simplifies answering probabilistic queries, as for example estimating the marginals. One particularly powerful way of approximating distributions is available in the form of variational methods. The variational free energy approach [JGJS99] was first formalized in the context of statistical mechanics by Richard Feynman [Fey72, Section 3.4]. Although variational methods originate from the physics literature, they are not limited to problems arising in this context and have become a reliable tool in the machine learning community. For our purpose, we focus on the Bethe free energy FB that bridges the gap from BP to variational methods.\nWe show how the Bethe free energy emerges quite naturally if we approximate PX(x) according to the variational free energy principle and follow the excellent deductive presentation of [YFW05] or [Mac03, Chapter 33]. An exhaustive treatment of variational representations in the context of probabilistic graphical models that elucidates how a variety of approximate inference methods can be understood in terms of their variational representations is presented in the excellent review article [WJ08]. Despite its origin, we refrain, however, from discussing the insights of specific relevance for problems in physics and refer the interested reader to some of the many well-written books [Geo11,Hua63] for a more in-depth treatment of the Bethe approximation from the physics perspective.\nWhen approximating a joint distribution PX(x), we first introduce a trial distribution P\u0303X(x). The aim is to manipulate the trial distribution so that the mismatch between both distributions is reduced. Therefore, we quantify the mismatch by the Kullback-Leibler divergence, defined according to\nD(P\u0303X||PX) = \u2211\nx\u2208XN P\u0303X(x)log\nP\u0303X(x) PX(x) , (3.11)\nwhere, with slight abuse of notation, log(\u00b7) corresponds to the natural logarithm. Note that the Kullback-Leibler is a non-symmetric function and not a proper distance measure. It does, however, satisfy the Gibbs inequality, i.e., it satisfies D(P\u0303X||PX) \u2265 0 with equality if and only if P\u0303X(x) = PX(x). 16\n16 Different proofs can be found in the literature for this property of the Kullback-Leibler divergence that, e.g., rely on the log-sum or the Jensen\u2019s inequality [MM09, p.7].\nNovember 8, 2019 \u2013 45 \u2013\n3 Approximate Inference: Belief Propagation\nLet PX(x) belong to an exponential family, defined by (2.14), and let P\u0303X(x) be an arbitrary trial distribution. Then we can rewrite the Kullback-Leibler divergence according to\nD(P\u0303X||PX) = \u2211\nx\u2208XN P\u0303X(x)logP\u0303X(x) + \u2211 x\u2208XN P\u0303X(x)E ( x ) + \u2211 x\u2208XN P\u0303X(x)logZ. (3.12)\nOverloading our notation, we further define the average energy E ( P\u0303X )\n= EX(E(x)) and the entropy S ( P\u0303X ) = EX(\u2212log(P\u0303X(x))) so that\nE ( P\u0303X ) = \u2211\nx\u2208XN P\u0303X(x)E(x) (3.13)\nS ( P\u0303X ) = \u2212 \u2211\nx\u2208XN P\u0303X(x)logP\u0303X(x). (3.14)\nWe can now plug the average energy and the entropy into the definition of the Kullback-Leibler divergence and express (3.12) in a simplified way according to\nD(P\u0303X||PX) = logZ + E ( P\u0303X ) \u2212 S ( P\u0303X ) . (3.15)\nLet us define two important quantities, namely the Helmholtz free energy (or the negative logpartition function)\nFH = \u2212logZ, (3.16)\nand the Gibbs free energy, which is given by FG(P\u0303X) = E ( P\u0303X ) \u2212 S ( P\u0303X ) . (3.17)\nThen, we can finally rearrange terms, plug the Gibbs free energy into (3.15) and express it in a sensible way that suggests how to estimate FH (and thus the partition function Z.\nFG(P\u0303X) = FH +D(P\u0303X||PX). (3.18)\nThe properties of the Kullback-Leibler divergence imply that FG \u2265 FH with equality if and only if P\u0303X(x) = PX(x). This is a pleasant formalism that opens the door for variational approaches; in particular minimizing FG(P\u0303X) provides FH and additionally, at its minimum, where the Kullback-Leibler divergence vanishes, the trial distribution converges to the exact one PX(x). More formally, for the set of all valid trial distribution P\u0303X(x) over XN we have\nPX(x) = arg min P\u0303X(x)\nFG(P\u0303X). (3.19)\nNote that this constitutes the central problems of inference, i.e., evaluating the partition function and computing marginal distributions (cf. Section 2.5). The formulation as an optimization problem provides an elegant way of solving those inference problems but \u2013 in its current form \u2013 is of limited relevance. Evaluation of the Gibbs free energy alone becomes infeasible with increasing model size as both terms in the Gibbs free energy require a summation over exponentially many terms (cf.(3.13) and (3.14)). The importance of (3.19), however, lies in the fact that a whole family of tractable approximation methods results from restricting the possible choices of trial distributions.\nOne popular approximation method that follows this idea is the mean field method. In its simplest form, one assumes that the trial distribution is defined over independent random variables. Consequently, the joint distribution factorizes into the product of the singleton marginals\n\u2013 46 \u2013 November 8, 2019\n3.3 Variational Interpretation of BP \u2013 A Physics Perspective\naccording to P\u0303X(x) = \u220f Xi\u2208X P\u0303Xi(xi). (3.20)\nRestricting ourselves to trial distributions of the form (3.20), the minimization of the Gibbs free energy suddenly becomes tractable. The minimizer of (3.19) then approximates the marginals. Some problems exist for which the mean field approximation even becomes asymptotically exact, as for example for infinite size Ising grid graphs with d\u0302G =\u221e [MM09, p.80]. The approximation tends to require a high average degree to work well. In general, however, this is not the case and the mean field method often provides poor approximations. This is mainly because of statistical dependencies that are present in PX(x), which the mean field method fails to account for.\nIt is an obvious next step to enhance the approximation quality by accounting for the correlations between pairs of random variables. One prevalent way comes in the form of the Bethe approximation that is of particular relevance for this work. Not only does the Bethe free energy FB restrict the class of trial distributions but it inherently approximates the Gibbs free energy as well. The latter is a result of the average energy EB(P\u0303B) and the Bethe entropy SB(P\u0303B) that are evaluated over the pseudomarginals P\u0303B instead of the joint distribution according to\nEB(P\u0303B) =\u2212 \u2211\nXm\u2208{Xi\u2208X}\u222a{Xi,Xj :(i,j)\u2208E}\nP\u0303Xm(xm) \u00b7 ln \u03a6Xm(xm)\n=\u2212 \u2211 Xi\u2208X \u2211 xi\u2208X P\u0303Xi(xi) ln \u03a6(xi)\u2212 \u2211 (i,j)\u2208E \u2211 xi,xj\u2208X 2 P\u0303Xi,Xj (xi, xj) ln \u03a6(xi, xj) (3.21)\nSB(P\u0303B) =\u2212 \u2211\n(i,j)\u2208E \u2211 xi,xj\u2208X 2 P\u0303Xi,Xj (xi, xj) ln P\u0303Xi,Xj (xi, xj)\n+ \u2211 Xi\u2208X ( di \u2212 1 ) \u2211 xi\u2208X 2 P\u0303Xi(xi) ln P\u0303Xi(xi). (3.22)\nThis subsequently defines the Bethe free energy in accordance with (3.17) so that\nFB(P\u0303B) =EB(P\u0303B)\u2212 SB(P\u0303B) (3.23)\n= \u2211\n(i,j)\u2208E \u2211 xi,xj P\u0303Xi,Xj (xi, xj) ln P\u0303Xi,Xj (xi, xj) \u03a6(xi, xj) \u2212 \u2211 Xi \u2211 xi P\u0303Xi(xi) ln \u03a6(xi)\n\u2212 \u2211 Xi ( di \u2212 1 )\u2211 xi P\u0303Xi(xi) ln P\u0303Xi(xi). (3.24)\nIn comparison to the Gibbs free energy, the Bethe free energy reduces the computational burden drastically; solely because FB is evaluated over the pseudomarginals, i.e., the singleton and pairwise marginals, instead of the full joint distribution. Just as the minimum of FG provides the partition function and the marginals, one would hope that \u2013 as FB approximate FG \u2013 minimizing FB will provide approximations to the partition function and the marginals.\nIn order to minimize FG efficiently, we had to restrict the choice of trial distributions. A similar restriction is required for minimizing FB; the most obvious class of trial distributions are realizable pseudomarginals that adhere to the sum-rule of probability. This constraints the pseudomarginals so that they correspond to some valid distribution PX(x); we refer to this set as the marginal polytope.\nNovember 8, 2019 \u2013 47 \u2013\n3 Approximate Inference: Belief Propagation\nDefinition 3.3.1 (Marginal Polytope). The marginal polytope is the set of all pseudomarginals that are jointly realizable by a valid joint distribution PX(x); i.e.,\nM(G) = { P\u0303Xi(xi), P\u0303Xi,Xj (xi, xj) : P\u0303Xi(xi) = \u2211 X\\Xi PX(x) : Xi \u2208 X,\nP\u0303Xi,Xj (xi, xj) = \u2211\nX\\{Xi,Xj}\nPX(x) : (i, j) \u2208 E } . (3.25)\nNote that the number of constraints in the marginal polytope depends on the structure of the graph as the singleton marginals and the pairwise marginals are only defined over the set of all nodes and the set of all edges respectively. We will refrain from making the dependence on the graph explicit, however, and only refer to the marginal polytope as M.\nThe constrained minimization of the Bethe free energy, i.e., minMFB(P\u0303B), seems like a standard optimization problem. It is, but the huge amount of constraints in M impedes the optimization in practice [Mur12, Section 22.3]. This issue is usually dealt with by relaxing the set of constraints. Accordingly we may relax the requirement from globally realizable marginals to locally consistent marginals. This makes the dependence on PX(x) superfluous and only requires two properties to be satisfied: all singleton and pairwise marginals have to be properly normalized, and the singleton marginals must be consistent with the pairwise marginals. These constraints then define the local polytope L.\nDefinition 3.3.2 (Local Polytope). The local polytope is the set of all pseudomarginals that are locally consistent, i.e.,\nL(G) = { P\u0303Xi(xi), P\u0303Xi,Xj (xi, xj) : \u2211 xi\u2208X P\u0303Xi(xi) = 1,\nP\u0303Xi(xi) = \u2211 xj\u2208X P\u0303Xi,Xj (xi, xj) } (3.26)\nAs for the marginal polytope, we will also refrain from making the dependence on the graph structure explicit and only refer to the local polytope by L. Further note that L contains fewer constraints than M and thus provides an outer bound on the marginal polytope (with equality for tree-structured models (cf. [WJ08, Prop. 4.1]).\nIt is worth pointing out a few differences that set the Bethe approximation apart from other variational methods. On the one hand, we would like to stress that the Bethe approximation approximates the marginals well in many cases. On the other hand, there are some conceptional shortcomings: First, the minimization must be performed over the local polytope; the obtained marginals are consequently only locally consistent and, in general, do not belong to a valid joint distribution. Second, except for certain sub-classes of graphical models, the Bethe free energy does not upper bound (nor lower bound) the original objective. This seems to fundamentally jeopardize the main assumption that the minimum of FB(P\u0303B) should be somewhat close to the minimum of FG(P\u0303X).\nTo summarize, the Bethe approximation has the potential to work well and may thus serve as an efficient approximation method but, at the same time, its properties indicate that performance guarantees will often be hard to come by. This highlights the need for developing a good understanding of the Bethe approximation and its shortcomings. The Bethe approximation is well-studied in statistical physics. Note, however, that the models studied in physics differ from the ones studied in computer science in one important aspect. Whereas physicists usually study the behavior in the thermodynamic limit, i.e., for graphs of infinite size,17 in computer science,\n17 This allows one to take limits and admits an analytical treatment in some cases.\n\u2013 48 \u2013 November 8, 2019\n3.3 Variational Interpretation of BP \u2013 A Physics Perspective\nwe are confronted with models of finite size. Although we expect a similar behavior to a certain degree, we will see that finite-size effects will often have a notable effect on the properties of FB."
        },
        {
            "heading": "3.3.1 Energy Landscape of the Bethe Free Energy",
            "text": "Let us now focus on the practical aspects of minimizing the Bethe free energy directly. In particular, we will take a closer look at how to approximate the marginals and the partition function in this setting. Similar as the partition function corresponds to the Helmholtz free energy (cf. (3.16)), we introduce the Bethe partition function ZB that corresponds to the Bethe free energy according to\nZB = exp(\u2212FB). (3.27)\nJust as FB approximates FH, ZB approximates Z. Note that there is literally no difference between both quantities and instead of minimizing FB one could equivalently maximize ZB. We stick to the notion of minimizing the Bethe free energy, primarily for historical reasons.\nLet us recap the two key-ingredients of the Bethe approximation again: we must replace the Gibbs free energy by the Bethe free energy and replace the constraints of the marginal polytope by the ones of the local polytope. This gives us the global minimum of the constrained Bethe free energy according to\nFB\u2217 = min L FB(P\u0303B), (3.28)\nwhere the pseudomarginals are specified by the associated minimizer\nP\u0303 \u2217B = arg min L FB(P\u0303B). (3.29)\nAlthough the relaxation to L reduces the complexity, it concurrently alters the energy landscape with drastic consequences. As opposed to the convex Gibbs free energy, the Bethe free energy is generally a non-convex function (cf. Example 5).\nWe will denote all stationary points of the constrained Bethe free energy by FB\u25e6 \u2208 { FB : \u2207FB(P\u0303B) = 0 } . (3.30)\nFinally, local minima of the constrained Bethe free energy are particularly relevant. Let us express the Hessian of the Bethe free energy by \u22072FB(P\u0303B). Then we denote local minima explicitly by\nFBm \u2208 { FB : \u2207FB(P\u0303B) = 0,\u22072FB(P\u0303B) is positive definite } . (3.31)\nNote, that the different types of stationary points consequently satisfy FB\u2217 \u2286 {FBm} \u2286 {FB\u25e6}."
        },
        {
            "heading": "3.3.2 The Bethe Approximation and Belief Propagation",
            "text": "One might ask where the added value of the variational approach lies exactly? The main reason is the immediate connection between the stationary points of the Bethe free energy and the fixed points of BP. The rich history of studying the Bethe free energy thus benefits our understanding of BP directly, as the theoretical properties carry over through this relationship.\nNovember 8, 2019 \u2013 49 \u2013\n3 Approximate Inference: Belief Propagation\nExplicit Connection\nThere is a fundamental connection between the stationary points of the Bethe free energy and the BP fixed points \u00b5\u25e6 (and the associated pseudomarginals P\u0303 \u25e6B). In a nutshell, the stationary points are in a one-to-one correspondence with the fixed points. We take advantage of the fact that FB is defined over the pseudomarginals (cf. (3.28)) and express the relation between the stationary points FB\u25e6 and the pseudomarginals according to\nFB\u25e6 = FB(P\u0303 \u25e6B). (3.32)\nThis correspondence becomes apparent when specifying the minimization of FB explicitly and including the constraints of the local polytope as a Lagrangian. Taking the derivatives and setting them to zero then yields the BP update equations [YFW01]. Consequently, at stationary points of FB (where all partial derivatives are zero), all BP messages remain unaffected by the update rule (which constitutes a BP fixed point). In fact, every fixed point of BP is an interior stationary point of the constrained Bethe free energy [YFW05]. Note that the nature of this correspondence further reveals a subtle, yet important, detail of BP: updating a message forces one gradient at a time to zero while keeping all other variables fixed, which albeit often going downwards is not a gradient descent step per se. The variational principle, however, suggests that we should specifically consider minima of FB; luckily, the BP updates still tend to proceed in a sensible way towards minima of FB [Hes03,AM00].\nImplications\nStable fixed points of BP (see. Section. 4.4 for a thorough discussion on stability) are of particular relevance in practice. Let us index all stable fixed points P\u0303 (s) B by s = 1, . . . , S. Every stable fixed point (s) then has an associated local minimum FB(s), an associated partition function Z(s)B , and associated pseudomarginals P\u0303 (s) B ; we denote the set of all S stable fixed points by\nS = {( Z(1)B , P\u0303 (1) B ) , . . . , ( Z(S)B , P\u0303 (S) B )} . (3.33)\nLikewise, we consider the set of all fixed points that constitute minima of the Bethe free energy M = {( Z1B, P\u0303 1B ) , . . . , ( ZMB , P\u0303MB )} , (3.34)\nand the total set of fixed points T = {( Z1B, P\u0303 1B ) , . . . , ( ZTB , P\u0303 TB )} . (3.35)\nFurther note that all stable fixed points of BP must be minima of FB. In the presence of frustrated cycles, however, minima may be unstable as well [Hes03, MWJ99]. The different types of fixed points thus relate to each other according to S \u2286M \u2286 T.\nOne important question is to understand under which conditions a fixed point is unique, i.e., when |T| = 1. It seems appealing to utilize the connection between BP and FB for that purpose. Besides for very simple models, however, conditions for convexity of FB are hard to come by, and established conditions are often far from necessary [PA02,Hes04]. A good overview of different conditions for convexity of FB is presented in [MK07].\nBesides these results for uniqueness, not many results aim to characterize the expected number of fixed points. One work that pursues this direction is [WF09]. Note that \u2013 at least as long as some proper form of message normalization is used [MLF11] \u2013 the number of fixed points is always finite (cf. Theorem 7 and Lemma 2 in [WF09]). Another important insight is that the number of fixed points is always odd.\n\u2013 50 \u2013 November 8, 2019\n3.3 Variational Interpretation of BP \u2013 A Physics Perspective\nWe will later characterize the number of fixed points and asses their stability in Chapter 5 and in Chapter 7 for a range of models. As of now, we will show the correspondence between stationary points of FB and fixed points of BP for one exemplary model.\nExample 5 (Energy Landscape of FB for an Attractive Ising Model). Let us take a closer look at the relationship between the Bethe free energy and BP by means of an attractive Ising model, specified on an infinite-size two-dimensional grid graph. Let all variables have identical local potentials, specified by \u03b8i = \u03b8 > 0, and all edges have identical pairwise potentials, specified by Jij = J > 0. We illustrate a slice of the Bethe- and the Gibbs- free energy along the marginals of one variable PXi(xi) for a model with weak couplings (Figure 3.2) and for a model with strong couplings (Figure 3.3) Note that the Bethe free energy upper bounds the Gibbs free energy for attractive models (cf. Section 3.4).\nIf the couplings are sufficiently small, FB is convex and BP has a unique and stable fixed point that has the whole message-space as a region of attraction (cf. Section 5.6). BP will thus always converge independently of its initialization.\nNovember 8, 2019 \u2013 51 \u2013\n3 Approximate Inference: Belief Propagation\nLet us now consider a model with strong couplings. Note how FB becomes non-convex and exhibits multiple stationary points that correspond to BP fixed points. We will later show in Chapter 5 that both local minima are stable fixed points and that the message initialization will determine to which fixed point BP converges. This has a major influence on the overall quality of the approximated quantities as the difference to the exact marginals PXi(xi) (i.e., the values minimizing the Gibbs free energy) and thus the accuracy varies considerably between different fixed points."
        },
        {
            "heading": "3.3.3 Other Variational Approaches",
            "text": "The one-to-one correspondence between stationary points of FB and fixed points of BP led to an improved understanding of BP and paved the way for methods that minimize the Bethe free energy directly [WT01, WT03, YR03]. The minimization of FB, however, remains non-trivial and usually requires additional considerations to render it viable.\nThe consideration of the Bethe free energy opens the door for provable convergent algorithms. Belief optimization [WT01] minimizes FB by following the negative gradient and is guaranteed to converge. The minimization takes place along the edges of the local polytope, which guarantees that the marginalization constraints in (3.26) are satisfied throughout. Alternatively, one can decompose the non-convex FB into a convex and a concave problem (this decomposition is in general not unique) and optimize both objectives in an alternating fashion. This procedure, known as the constrained convex-concave procedure [YR03],, is guaranteed to obtain a stationary point of FB. Although both methods converge to stationary points of FB, two major limitations remain: first, the approximation quality may vary considerably between different stationary points and only obtaining some (local) extremum may yield sub-optimal solutions; and second, despite the appeal of convergence guarantees, run-time guarantees are often just as important in practice.\nTo counteract some of these problems, one can also relax the Bethe free energy and come up with well-behaved surrogates that can be minimized efficiently. Convex surrogates seem to be specifically well-suited for this task and have thus received considerable attention. One can, for example, upper bound the objective by a convex function as in tree-reweighted belief propagation (TRW) [WJW03b, WJW05, Kol06] that enforces a concave entropy-term as a combination of tree-entropies. Note, however, that there are multiple ways of coming up with convex surrogates [MGW09, GJ07, HS08]. One comprehensive overview that unifies many different approaches in terms of the chosen counting number is presented in [MJGF09]. Overall these convex versions are well-behaved and can be optimized efficiently. A trade-off between convergence-properties and accuracy, however, persists and \u2013 if it can be minimized \u2013 the Bethe approximation often outperforms its convex surrogates in terms of accuracy [MJGF09,WJ14a].\nThis observation led to a renewed focus on trying to minimize the Bethe free energy efficiently (i.e., in polynomial run-time). The efficient minimization of FB becomes possible if we impose certain properties on the graphical model (in terms of its structure or parameters) and consider -approximating the stationary points. In particular, this includes sparse models [Shi12], where a projection scheme in the minimization task allows for a fully polynomial-time approximation on graphs with max(di) = O(logN). For attractive models (not necessarily sparse), this algorithm is further improved in [WJ14a] so that it obtains the global minimum of the Bethe free energy. If both properties are fulfilled, i.e., for locally tree-like attractive models the Bethe approximation is exact and can be optimized efficiently [DM10].\nFrom a completely different point of view, the approximation quality can also be enhanced by better approximations of FG . The concept of the Bethe approximation, that only accounts for the singleton- and pairwise marginals, generalizes to the Kikuchi method that accounts for the\n\u2013 52 \u2013 November 8, 2019\n3.4 Approximation Quality\nmarginals of larger cliques as well. The same principle \u2013 i.e., considering larger cliques \u2013 generalizes BP as well; this method is accordingly termed as generalized belief propagation [YFW01]. Both the accuracy and the convergence properties improve by adopting the computations to larger cliques. Although larger cliques inevitably increase the computational complexity, the principles of generalized belief propagation are flexible enough to allow moving freely along this trade-off.\nGauge transformations [CC06,CCT08] are somewhat similar in that they work with the exact partition function Z directly while simplifying its estimation. This is achieved by expressing the exact partition function via a loop-series expansion of ZB. Interestingly, all terms in the loop-series correspond to fixed points of BP, which suggests one way of computing them. The number of terms in the expansion, however, may be large for models with many loops so that computing all terms is often not an option. Nonetheless, gauge transformations provide a principled approach to improve upon BP by at least accounting for some terms in the loop-series.\nTo conclude this section, a variety of variational approaches are available that aim to find the sweet spot between accuracy and complexity when performing approximate inference. Most of the above methods build upon the fundamental principles of the Bethe approximation, that \u2013 although providing a considerable simplification over the Gibbs free energy \u2013 remains problematic to minimize in practice."
        },
        {
            "heading": "3.4 Approximation Quality",
            "text": "So far we have already discussed various approximate inference methods. Different methods will perform differently and, depending on the given model, achieve varying accuracy. If we want to evaluate and compare approximate inference methods, it is important to measure the accuracy of the approximation. Having said that, approximate inference methods are usually applied to problems that forbid the computation of the exact solution. This prohibits measuring the accuracy by comparison to the exact solution and highlights the need for some qualitative measures of the expected performance for a given problem. Essentially, one would like to provide model-specific performance guarantees and bounds on the approximation error. Few bounds on the approximation quality are, however, established and some bounds require considerable computational resources on their own, not to mention that most bounds are often relatively loose.\nIn this section, we outline how the error in the approximation of the partition function and the marginals will be measured throughout the thesis. Moreover, we will discuss some of the available error-bounds for both quantities.\nPartition Function\nThe error of the partition function is usually evaluated in terms of the relative error between the log-partition functions (i.e., the negative value of the Bethe free energy) according to\nEZ(m) = |logZmB \u2212 logZ| logZ = |FG \u2212FBm| \u2212FG , (3.36)\nwhere ZmB = ZB(P\u0303mB ) is the Bethe partition function of the mth fixed point [GMK07]. Note that (3.36) quantifies the error in the partition function and in the free energy.\nExisting bounds on the partition function usually combine an upper bound [WJW05, JJ97] with some lower bound as e.g., the naive mean field [WJ08]. Other bounds are based on the loop-series expansions [WSW08] or the non-backtracking operator [SKZ14].\nNovember 8, 2019 \u2013 53 \u2013\n3 Approximate Inference: Belief Propagation\nOne intriguing detail of the Bethe approximation is that it only approximates the partition function but neither provides an upper nor a lower bound of it. For the important class of attractive models, however, the Bethe partition function does indeed lower bound the partition function, i.e., ZB < Z [Ruo12]. This has the important consequence that minimizing FB is always optimal with respect to the approximation quality of the partition function for attractive models, since arg minZmB (EZ(m)) = exp(\u2212minLFB(P\u0303 m B )).\nAn alternative, arguably more intuitive, proof that reveals how the Bethe partition function lower bounds the true partition function relies on the concept of clamping [WJ14b]. The main idea is to condition (i.e., to clamp) on a variable taking one specific value at a time and to evaluate the partition function as a sum over all sub-partition functions. This brings the advantage of working directly on the Bethe free energy and obviates the need for relying on additional concepts such as graph covers [Von13,Ruo13] or loop-series expansions [WSW08]. Moreover, for attractive models clamping always improves the approximation quality of the partition function over BP. This is particularly true if selecting the clamped variables wisely, as for example according to some heuristics [Wel16]. Clamping also improves the upper and lower bounds of Z as it can be utilized to improve the partition function estimates of TRW and the mean field method [WD16].\nClosely related to BP, but non-iterative, is the mini-bucket elimination scheme [DR03]. Minibucket elimination also provides upper and lower bounds on the partition function and offers a trade-off between the accuracy of the bound and computational efficiency based on the cliquesize considered. One can further generalize this concept based on Ho\u0308lders inequality [LI11] and efficiently compute bounds with good quality, where the improvement on the estimated bounds is because of incorporating the concept of TRW. The method of gauge transformations further generalized this concept and allows one to obtain tighter bounds on the partition function with similar computational effort [ACSW18].\nMarginals\nWe measure the error of the singleton marginals by the mean squared error (MSE), where the error of the approximated marginals at the mth fixed point is given by\nEP (m) = 1\nN \u2211 Xi\u2208X ||PXi(xi)\u2212 P\u0303mXi(xi)||22 (3.37)\n= 2\nN \u2211 Xi\u2208X ( PXi(xi = 1)\u2212 P\u0303mXi(xi = 1) )2 . (3.38)\nThe simplification in (3.38) is because of symmetry properties for binary random variables. Note that one can replace the squared l2-norm in (3.37) by any other norm if desired; in particular the l\u221e-norm is considered by some authors to measure the worst-case error.\nThe expected mean describes the response of the system to the field \u03b8 [MM09] according to\n\u3008m\u3009 = E(m) = 1 N \u2211 Xi\u2208X mi = 1 N \u2211 Xi\u2208X E(Xi), (3.39)\nwhere we parameterize binary random variables by their mean (cf. (2.21)). Note that the difference between the expected mean of the exact marginals \u3008m\u3009 and of the approximated marginals \u3008m\u0303\u3009 is identical to the sum over all marginal errors\n\u3008m\u3009 \u2212 \u3008m\u0303\u3009 = 2 N \u2211 Xi\u2208X PXi(xi = 1)\u2212 P\u0303Xi(xi = 1). (3.40)\n\u2013 54 \u2013 November 8, 2019\n3.5 BP as a Dynamical System\nNote that we consider binary random variables with X = {\u22121, 1}; if we would consider X \u2032 = {0, 1} instead, the expected mean would change according to \u3008m\u2032\u3009 = 1N \u2211N i=1 PXi(xi = 1) = 1 2(\u3008m\u3009+ 1).\nSome methods quantify and bound the approximation error of the marginals instead of EZ(m) by computing a confidence interval (i.e., an upper- and lower bound) on the exact marginals.18 Bound propagation [LK03] computes bounds for small clusters of nodes; similar to BP, these bounds are then propagated throughout the graph and provide bounds on the singleton marginals after convergence. The computational complexity is determined by the tree-width (similar as for the Junction tree algorithm), thus limiting the applicability to models with few loops.\nSimilar in principle, but more efficient in terms of computational complexity, is the recursive propagation of the bounds over sub-trees of the model [MK09]. The computational complexity is determined by the support of the random variables. This renders the method applicable to models with high connectivity as well.\nFrom a completely different perspective, TRP provides bounds on the approximation error as well [WJW03a]. TRP considers spanning trees to approximate the marginals, which suggests computing bounds on the marginals over spanning trees as well. Regarding the computational complexity, there is one particularly intricate ingredient involved in bounding the error over the spanning trees, with the problem being the requirement for the log-partition function that cannot be computed exactly for complex models.\nSome of these issues are resolved by computing the bounds over self-avoiding walk trees instead [Ihl07]. If we compare the quality of the bounds we observe that the proposed methods in [MK09] and [WJW03a] are of similar quality. The computation over self-avoiding walk trees impacts the quality of the obtained bounds: for models with weak interactions (where BP converges fast) the tightest bounds are provided by [Ihl07]; for models with strong interactions (where BP performs worse) the confidence intervals become wider than the ones provided by [WJW03a].\nAll those methods aim to estimate the expected performance of BP in terms of the marginal error and while this often estimates the performance well, all presented methods have one particular problem in common. Increasing the coupling strength degrades the quality of the estimated bounds."
        },
        {
            "heading": "3.5 BP as a Dynamical System",
            "text": "It is often convenient to consider BP as a dynamical system, i.e., as a discrete time map (see Definition 4.1.1 for a formal definition). Let \u00b5n = {\u00b5nij(xj), \u00b5nji(xi) : (i, j) \u2208 E} be the set of all messages at iteration n, then we denote the mapping induced by the update equations of BP in (3.1) according to\n\u00b5n+1 = BP(\u00b5n). (3.41)\nIn accordance with the notion of fixed point messages \u00b5\u25e6ij(xj) that remain unaffected under the application of BP, we refer to the set of all fixed point messages by \u00b5\u25e6. We will often refer to \u00b5\u25e6 simply as fixed point and write\n\u00b5\u25e6 = BP\u25e6(\u00b5), (3.42)\nwhere BP\u25e6 updates the messages until convergence.\n18 Note that upper and lower bounds of the partition function can also be related to upper and lower bounds of the marginals [WJ14a].\nNovember 8, 2019 \u2013 55 \u2013\n3 Approximate Inference: Belief Propagation\nRemember that BP is neither guaranteed to provide accurate marginals nor guaranteed to converge. Failure of BP (to provide accurate results) can be attributed to the existence of multiple (stable) fixed points or the existence of unstable fixed points (for which the messages oscillate far away from any fixed point [Wei00, MK07, IFW05]). Thus, if we want to make BP more robust, we should modify BP such that it converges towards an accurate stable fixed point. Potential modifications include variants that minimize FB directly or minimize alternative objectives as e.g., convex surrogates (cf. Section 3.3.3). From a completely different perspective, one can modify the update procedure directly \u2013 ignoring the effect on the variational function \u2013 so that the overall performance improves. In particular, the consideration of BP as a dynamical system comes with a flexible formalism that suggests multiple ways of modifying and enhancing BP."
        },
        {
            "heading": "3.5.1 Enhancing Belief Propagation\u2019s Properties",
            "text": "BP, as presented in Section 3.1, is only one specific instance in a broad class of algorithms that fit under the umbrella of more general message passing methods. We will see that, although similar in nature, the choice of the respective message passing algorithm is not a mere formality but influences the behavior significantly \u2013 and is therefore of great practical relevance. The family of message passing algorithms is defined by a set of key-ingredients, where all message passing algorithms proceed along with the following three steps:\n\u2022 Initialization: Each message \u00b5ij(xj) \u2208 \u00b5 is assigned an initial value \u00b51ij(xj).\n\u2022 Update: An update function is defined that acts on the set of messages F : \u00b5n \u2192 \u00b5n+1. This update function usually consists of one function per message fij : \u00b5 n \u2192 \u00b5n+1ij (xj).\n\u2022 Evaluation: An evaluation or decision function maps the set of messages to the values of interest. In particular, we are interested in approximating the marginals (by the pseudomarginals) and in approximating the partition function (by the Bethe partition function); thus we consider evaluation functions for the pseudomarginals DP : \u00b5 \u2192 P\u0303B and for the Bethe partition function DZ : \u00b5 \u2192 ZB.\nThis rather general formulation of message passing algorithms grants a certain degree of freedom in specifying an algorithm. Together, those aspects define the overall behavior of BP and every aspect may be tuned individually to enhance its performance. Not a single best algorithm exists though and, depending on the application, one or the other variant may perform better.\nWe will now discuss some established ways of enhancing BP\u2019s performance. We particularly focus on how all those variants result from tuning some of the key-step mentioned above. There is an undeniable elegance in providing such a unifying view; even more important though is the fact that this unifying view also suggests novel ways of enhancing the performance of BP. We will also highlight how many contributions of the later chapters conform to this view as well."
        },
        {
            "heading": "3.5.2 Initialization",
            "text": "The choice of the initialization is no formality but, in case of multiple fixed points, determines which fixed point a given algorithm will converge to and as such it plays an important role in analyzing BPs behavior. Although only practical for problems of modest size, one can for example explore the solution space exhaustively by using a sufficiently wide range of initial values.\nNote that BP often performs significantly better then worst-case analyses suggest. It seems, that for some reason, a wide range of initial message values converges to a good fixed point. This further raises the question of how one should initialize message passing algorithms. One common\n\u2013 56 \u2013 November 8, 2019\n3.5 BP as a Dynamical System\nchoice, that often works well, is to initialize all messages to the same value \u00b51ij(xj) = 1 |X | ; it remains unclear, however, where the advantages of uniform initialization stem from. In general, it would be desirable to identify initial message values that are optimal for certain problemclasses. We will later pursue this idea and provide a heuristic that obviates the need for choosing initial values and guides BP towards accurate fixed points (cf. self-guided belief propagation in Chapter 6)."
        },
        {
            "heading": "3.5.3 Update",
            "text": "The choice of the update function is one of the key-aspects of message passing algorithms and has an enormous influence on the overall behavior of a particular algorithm. In particular, it is altering the update function that often leads to algorithms with favorable properties. The search for enhanced message passing algorithms thus spurred the development of alternative update functions, which ultimately led to the proposal of various algorithms. We will discuss some successful variants and group these modifications into two different categories.\nFirst, instead of applying the update function F to all messages at once, one can update only a single message at each iteration. Effectively, this procedure determines a schedule according to which the messages are updated; we consequently term this concept scheduling. Note that scheduling preserves all update functions and only changes the order in which they are applied.\nOn the other hand, one can, of course, try to improve the convergence properties by altering the update functions themselves. Although one can think of numerous ways to alter the update functions we intend to consider variants that perform at least as good as plain BP. In particular, this includes two variants that will be considered in this thesis; these are: damping and selfguided belief propagation.\nScheduling\nIt is widely accepted that scheduling the messages and updating them asynchronously enhances BP. In particular, this helps to achieve convergence more often and in fewer iterations. Up until now, we considered BP without scheduling where all messages are updated according to\n\u00b5n+1 = BP(\u00b5n) = ( f1(\u00b5 n), . . . , fm(\u00b5 n), . . . , f2|E|(\u00b5 n) ) . (3.43)\nNote that we imposed some ordering o : (i, j) \u2192 m on the messages and refer to the update functions accordingly, i.e., fm computes the m\nth message. There are 2|E| messages in total as they are passed along both directions of the edges.\nFor an asynchronous version of BP only a single message \u00b5ij(xj) = \u00b5m(x) is updated per iteration according to\n\u00b5(n+1) = ( \u00b5n1 (x), . . . , fm(\u00b5 n), . . . , \u00b5n2|E|(x) ) . (3.44)\nThe update-order, i.e., which message to compute, has a major influence on the overall performance. One common choice is a fixed order, either according to a round-robin scheme or a random ordering. In both cases, the sorting of the messages follows a particular ordering and the messages are selected for an update according to\nm = n (mod 2|E|). (3.45)\nSuch a fixed update-order already improves the performance of BP significantly, yet it leaves considerable room for improvement.\nNovember 8, 2019 \u2013 57 \u2013\n3 Approximate Inference: Belief Propagation\nOne popular way of scheduling the messages relies on the fact that, for a tree, BP is guaranteed to converge and to provide the exact marginals. Tree based reparameterization (TRP) [WJW02] computes a set of spanning trees so that the union of all spanning trees includes all edges of the original graph. Then, one applies BP to one tree at a time. One says that this calibrates the marginal along the spanning tree, i.e., the marginals would be exact if the graph would contain no more edges. Of course, performing BP on another spanning tree will alter some of the marginals again, but at least TRP enforces consistency through parts of the graph. The choice and the order of the spanning trees impact the overall performance of TRP and can be tailored in a model-specific way. Intuitively, one can argue that the focus on spanning trees enhances the overall convergence properties by enforcing a brisk exchange of information in the graph.\nAll scheduling methods discussed so far rely on a fixed update-order that may or may not depend on the graph. Instead of carefully selecting an update-order, it is, however, much more flexible to resort to an adaptive update-order, where the current message values determine which message to update next. This obviates the need for carefully tuning the update-order to the current graph and further improves the convergence properties because of the inherent flexibility in adaptive methods.\nThe most basic adaptive scheduling method, residual belief propagation (RBP) [EMK06], already improves the convergence properties a lot. RBP updates the messages along the least calibrated edge, i.e., for which the message varies the most. The underlying assumption is that strongly varying messages contain more information and are thus of predominant importance for the convergence of BP. To make this more precise we introduce the residual rij to measure the message-distances between two successive iterations 19\nrnij = ||\u00b5n+1ij (xj)\u2212 \u00b5nij(xj)||\u221e = max\nxj |\u00b5n+1ij (xj)\u2212 \u00b5nij(xj)|. (3.46)\nLet rn = {rnij : (i, j) \u2208 E} be the set of all residuals at iteration n, then the index that maximizes the residual\nm = arg max m\nrn (3.47)\nidentifies the message that is selected for an update. Overall this significantly reduces the number of message updates until convergence and the overall convergence time. Computing all residuals, however, poses some computational overhead. We can reduce this overhead and further reduce the convergence time without degrading the accuracy much by computing an upper bound on the residual [SM07].\nAlthough adaptive scheduling increases the number of models for which BP converges, some models remain problematic. Closer inspection reveals that only small parts of the graph fail to converge, whereas large parts (almost) converge in a couple of iterations. RBP thus updates the same subset of messages ad infinitum. This observation inspired the design of scheduling methods that detect such problematic behavior, specifically counteract the oscillations, and enforce convergence [KRTP15].\nNoise injection belief propagation (NIBP) applies RBP and actively checks for oscillations: if RBP converges no modifications are undertaken; if, however, oscillations occur, NIBP injects Gaussian noise to the selected message. The underlying intuition is that the injected noise\n19 Ultimately one is interested in the distance to the fixed point, if it exists, lim n\u2192\u221e \u00b5nij(xj). However, since\nlim n\u2192\u221e\n\u00b5nij(xj) is not known, the time variation of the messages offers a valid surrogate (cf. [EMK06]).\n\u2013 58 \u2013 November 8, 2019\n3.5 BP as a Dynamical System\nwill propagate from the most influential part of the graph (i.e., the message selected by RBP) throughout the whole graph and, by introducing a relevant change to the overall model, lead to convergence. Let \u00b5ij(xj) be the message selected according to (3.47); if this message oscillates, i.e., if \u00b5nij(xj) = \u00b5 n\u2212l ij (xj) for any l \u2208 R, Gaussian noise is added so that\n\u00b5n+1ij (xj) = fij(\u00b5 n) +N (0, \u03c32). (3.48)\nAlternatively, instead of actively detecting oscillations, one can question the underlying assumption of RBP. Messages, despite varying significantly, often take identical values repeatedly; arguably, these messages carry not much information. Weight decay belief propagation (WBP) penalizes this behavior and dampens the residual of messages that were already updated. Consequently, WBP increases the relevance of the remaining messages and thus further refines the parameterization of the overall graph. More precisely, WBP divides the residual of a messages by the number of times this message has already been scheduled and reformulates (3.47) according to\nm = arg max m rnij\u2211n n\u2032=1 1 n\u2032 ij , (3.49)\nwhere the indicator function 1n \u2032 ij = 1 if and only if r n\u2032 ij = arg maxm r n\u2032 . Detailed pseudocode for NIBP and WBP is presented in Appendix A.3.\nWhen comparing the convergence properties of the discussed scheduling methods on grid graphs with random Ising potentials, a clear picture emerges. We present one exemplary experiment in Figure 3.4 and refer to the paper [KRTP15] for more exhaustive experiments.\nAlthough scheduling with a round-robin schedule, ABP (black), improves the convergence properties over BP without scheduling, ABP does not perform as well as more sophisticated methods. In particular, adaptive scheduling enhances the convergence properties notably, demonstrating its conceptual advantage. Evaluating the overall convergence behavior, NIBP (orange) converges for more models than any other method. WBP (blue) still converges more often than RBP (green), although it requires more iterations to converge. Damping of the residual slows the convergence as messages that require multiple updates to converge will now be selected less frequently.\nNovember 8, 2019 \u2013 59 \u2013\n3 Approximate Inference: Belief Propagation\nAlthough the positive influence of adaptive scheduling methods on the convergence properties is indisputable, the influence on the marginal accuracy is less obvious. In practice, however, we are not only interested in good convergence properties but require accurate marginals as well. A comprehensive comparison of different scheduling methods in terms of marginal accuracy is presented in [KRTP15]. We summarize the major insights here: considering only graphs for which a simple round-robin schedule (i.e., ABP) achieves convergence, ABP approximates the marginals well for all models, whereas RBP and NIBP fail to do so. It may seem that advanced scheduling methods pay the price for improving the convergence properties and obtain less accurate marginals. It is thus particularly remarkable that WBP maintains the marginal accuracy of ABP although it converges for a wide range of models. Of all considered methods, WBP is thus superior.\nAlternative Update Functions\nWhen the BP messages oscillate (often just between two values) applying a damping term has proven to be a successful strategy to improve the convergence properties [MWJ99]. That is, one replaces the messages with a weighted average of the last messages so that\nBPD(\u00b5) = (1\u2212 )BP(\u00b5) + \u00b5, (3.50)\nwhere \u2208 [0, 1). Remember that we intend to change the update function without changing the fixed points. Complying with our requirement, fixed points of BPD(\u00b5) must, therefore, be fixed points of BP without damping as well. This is the case as fixed points of BP must satisfy \u00b5\u25e6 = BP(\u00b5\u25e6), which holds per definition so that\nBPD(\u00b5\u25e6) = (1\u2212 )BP(\u00b5\u25e6) + \u00b5\u25e6 = BP(\u00b5\u25e6). (3.51)\nNote that, although damping does not affect the fixed points, the local stability of the individual fixed points may change (cf. Section 5.4.2).\nIn Chapter 6 we will propose self-guided belief propagation that relies on an alternative update function. While damping modifies the update function in a static manner, SBP modifies it in a more flexible, time-dependent, way. The underlying idea is to consider the potentials as a function of the iteration \u03a8(n), which enhances the performance across all areas (i.e., convergence properties and accuracy)."
        },
        {
            "heading": "3.5.4 Evaluation",
            "text": "BP often performs better than suggested by worst-case analyses [MM09, Section 22.3]. This observation comes not too surprising, especially when considering the existence of multiple fixed points. We would indeed be rather surprised if all fixed points have similar accuracy. Rather than a disadvantage, the difference in the accuracy can be seen as a chance; modified evaluation functions may exploit this difference and select or combine fixed points to enhance the approximation quality. Certainly, modified evaluation functions are not a remedy to all problems. But modified evaluation functions are an important, flexible, concept to account for the existence of multiple fixed points and to mitigate some of the problems arising in loopy models.\nHere we discuss two popular modifications that take advantage of the variability in accuracy across multiple fixed points. Taking the set of all fixed points T (or a subset thereof) into account, we modify the evaluation function DP : {\u00b5\u25e6} \u2192 P\u0303B in two ways. Either to select a single fixed point or to compute a weighted combination of multiple fixed points. Note that we will consider and evaluate both modifications in Chapter 5 and theoretically analyze them in\n\u2013 60 \u2013 November 8, 2019\n3.5 BP as a Dynamical System\nChapter 7. The modifications of DP become relevant in the presence of multiple fixed points. BP may converge to any fixed point as it does, in general, not favor a particular one over the other. Even if all fixed points are available, however, it remains unclear how one should select the fixed point that approximates the marginals best.\nVariational methods minimize their objective and aim to find the global minimum, although \u2013 for non-convex FB \u2013 this cannot be done efficiently. Given a set of fixed points, we should thus select the fixed point that minimizes FB (and maximizes ZB), which is the global minimum of FB. We denote the associated pseudomarginals by\nP\u0303MAXB = arg max P\u0303B\u2208T ZB(P\u0303B) = arg min L FB(P\u0303B). (3.52)\nNote that the complete set of fixed points T is generally not available and the maximization can thus only be performed over the set of available fixed points.\nRecently an interest emerged in methods that obtain and combine multiple fixed points [BYGRS12, BMZ05, SRF16]. The combination of all fixed points M lies at the heart of the replica symmetry breaking (RSB) assumption. This assumption emerged in statistical physics from the study of spin glass models and describes how the exact marginals decompose into a weighted sum of different contributions, which turn out to correspond to minima of FB.20 If FB is non-convex, the RSB theory describes the decomposition of the exact solution into a convex combination of marginals that are weighted by their associated partition function so that\nPXi(xi) = 1\u2211 mZmB M\u2211 m=1 ZmB P\u0303mXi(xi). (3.53)\nThis representation can be attributed to [MPV87] and, rather than a theorem, it is a set of postulates. One underlying assumption is that the system does exhibit multiple fixed points (unique fixed points would falsely imply exact marginals otherwise); an accessible introduction to the RSB theory and all underlying assumptions can be found in [MM09, Chapter 19]. Despite its non-rigorous flavor, (3.53) has been verified for a wide range of problems (e.g., random SAT problems and spin glasses). In particular, many state-of-the-art solvers for combinatorial problems rely on the RSB theory [RG15]. Further note that, even if the local polytope is a strict outer bound on the marginal polytope, a convex combination of P\u0303mXi(xi) (that correspond to edge points of L [YFW05]) may consequently end up on an edge point of M (which it must if it is exact).\nThe RSB theory states how to form the exact solution from pseudomarginals that were obtained by efficient approximate inference methods. This seems to contradict all results on the computational complexity of exact inference. Closer inspection resolves this apparent contradiction: the number of fixed points may grow exponentially with the model-size so that obtaining all fixed points is just as problematic as computing the marginals exactly. Indeed, this is one of the reasons that limit the practical applicability of the RSB assumption.\nFor the special case of constrained satisfaction problems, survey propagation [BMZ05] is one efficient way to evaluate (3.53). The extension to more general models, however, remains somewhat elusive [SRF16,RG14].\nNonetheless, besides being relevant from a theoretical point of view, we will also identify certain models for which only relatively few fixed points exist, which substantiates the practical relevance of the RSB assumption as well.\n20 In physics one deals with the decomposition of the Gibbs measure (i.e., the joint distribution) into a weighted combination of Bethe measures (that correspond to BP fixed points).\nNovember 8, 2019 \u2013 61 \u2013\n3 Approximate Inference: Belief Propagation"
        },
        {
            "heading": "3.6 Conclusion",
            "text": "This chapter introduced BP as a message passing algorithm for approximate inference. We further presented two alternative perspectives that hone our understanding of BP. Namely, we discussed (i) the variational interpretation that relates all fixed points of BP to stationary points of the Bethe free energy and (ii) the consideration as dynamical systems that suggest various ways to enhance BP. Both concepts arguably provided insights into the solution space of BP, explained the behavior of BP, and thus benefited our understanding of BP. Besides discussing the properties of BP from different perspectives, one major purpose of the current chapter was to provide an introduction for the second part of this thesis (i.e., Chapter 5 - 7). Our discussion is far from being exhaustive and left many questions open. Below we list a couple of open questions worth pursuing that will be addressed in the remainder of the thesis and that will provide interesting insights into the nature of BP.\nOne of the most pressing needs when considering the variational interpretation is a thorough analysis of the energy landscape of the Bethe free energy (specifically for problems where FB is non-convex). Since every local minimum serves as a potential fixed point it would be of great relevance to characterize the solution space and to obtain all possible fixed points.\nWhile the knowledge of all fixed points is valuable in its own, this does not reveal all properties of relevance for BP. In particular, the energy landscape does not explain the convergence properties and it remains an open, albeit important, question under which conditions one can expect BP to converge.\nFurthermore, we defined the set of all stable fixed points S in Section 3.3.2. Whether this set is available is questionable as the full set of fixed points is generally not available. It is consequently an important subject to come up with message passing algorithms that tend to converge towards the most accurate fixed point.\nFollowing the discussion in Section 3.4, it is crucial to assign some qualitative measurements to the fixed points after convergence of BP. This is particularly important as the exact solution is not available in practice. It remains an ongoing research topic, however, to determine useful accuracy-bounds that can be evaluated efficiently. Moreover, it is not obvious at all how different measurements, e.g, bounds on the marginal accuracy or the partition function, relate to each other.\n\u2013 62 \u2013 November 8, 2019\nUnderstanding the Behavior of Belief Propagation\n4 Discrete-Time Dynamical Systems:\nSolution Space Analysis\n\u201dInside a broken clock Splashing the wine\nWith all the rain dogs Taxi, we\u2019d rather walk...\u201d\n\u2013 Tom Waits\nThis chapter provides a brief, high-level, introduction to (nonlinear) dynamical systems and further introduces tools from computational mathematics, required in the subsequent chapters. Already in Section 3.5, we have witnessed some of the benefits that result from considering BP as a nonlinear dynamical system. Framing BP as discrete time map allows us to draw from the rich history of dynamical systems (cf. [Tes12,Sch00]) and will be particularly beneficial in studying BP\u2019s convergence properties. Before we discuss how to analyze the convergence properties of a discrete-time map, a proper definition of the terminology is required. Subsequently, we express the essential steps in analyzing discrete-time maps, present multiple ways of doing so, and compare the individual advantages.\nWe begin this chapter with the definition of discrete time maps in Section 4.1. We then illustrate how the problem of computing the fixed points reduces to solving a polynomial system in Section 4.2, before we discuss and compare different approaches for solving polynomial systems in Section 4.3. Finally, we introduce the notion of stability and show how to assess the stability of a fixed point in Section 4.4."
        },
        {
            "heading": "4.1 Discrete Time Map",
            "text": "Let us first introduce the notation of a discrete time map now:\nDefinition 4.1.1 (Discrete Time Map). A discrete time map is defined by an n-dimensional state vector x \u2208 Rn and a function F : Rn \u2192 Rn that acts on the state vector according to x(n+1) = F ( x(n) ) , where n \u2208 Z\u2217+ is the time index. Note that the function F is a composition of n functions fi : Rn \u2192 R. We will denote the value of the state vector at time n by xn = x(n) so that\nxn+1 = F (xn) . (4.1)\nA discrete-time map is further said to be nonlinear if some (or all) functions fi(x) are nonlinear.\nOne of the overarching goals in the field of dynamical systems is to understand how the system responds and evolves for given initial conditions in order to gain knowledge about the global behavior. In particular, this knowledge can be gained by first obtaining all fixed points of a map, for which the state vector x remains unaffected by the application of (4.1). More formally,\nNovember 8, 2019 \u2013 63 \u2013\n4 Discrete-Time Dynamical Systems: Solution Space Analysis\na fixed point of the map F is a state vector x\u25e6 \u2208 Rn that maps to itself so that\nx\u25e6 = F(x\u25e6). (4.2)\nThe problem of obtaining all fixed points is formally introduced in Section 4.2 before we compare different approaches in Section 4.3. The knowledge of all fixed points alone, however, does not provide sufficient knowledge about the global behavior of a system and is only part of the whole picture. According to the definition in (4.2), fixed points are not affected by (4.1), though it is not revealed if and under which conditions x will converge to a specific fixed point. We are, however, interested in determining which fixed points can be obtained through the repetitive application of (4.1), i.e., whether a fixed point is stable (cf. Section 4.4)."
        },
        {
            "heading": "4.2 Finding Fixed Points of a Discrete Time Map",
            "text": "Fixed points of a discrete-time map are state vectors that remain unaffected by the map; the fixed points consequently correspond to the solutions of the fixed point equations x\u25e6\u2212F(x\u25e6) = 0 per definition. Thus, computing the fixed points boils down to solving a system of polynomial equations, i.e., of nonlinear algebraic equations. However, albeit straightforward in principle, solving the fixed point equations is by no means straightforward to do in practice; in particular if confronted with a nonlinear system.\nSystems of polynomial equations arise quite naturally in many engineering applications and it is a classical and important problem in computational mathematics to solve them. But, despite their relevance and the long research-history, it is still an active field of research. Today, solving systems of polynomial equations brings together many different branches of mathematics (such as topology, numerical mathematics, geometry, and algebra) that are joined under the term algebraic geometry. However, the body of the corresponding literature is vast and \u2013 as different fields play together \u2013 requires deep knowledge in many subjects. Moreover, the topic is unfortunately mainly studied from a theoretical point of view, which often prevents the application to practical problems and poses a notable hurdle for non-experts.\nAlthough many approaches for solving polynomial systems seem promising at first, it is a daunting task to understand their properties and capabilities enough as to select an appropriate method, well-suited for the task at hand. Indeed, the ability to deal with large systems of polynomial equations was a major contribution and a prerequisite for many results that are obtained in Chapter 5. Although it is possible to read those sections and to understand the ultimate insights, this thesis should be as self-contained as possible. Therefore we give a brief (and far from complete) summary of the most prominent ways for solving polynomial systems subsequently. In particular, we will discuss numerical methods (Section 4.3.1), symbolical methods (Section 4.3.2), and numerical polynomial homotopy continuation methods (Section 4.3.3)."
        },
        {
            "heading": "4.2.1 Systems of Polynomial Equations",
            "text": "Before we delve into the subtleties that come with the problem of solving systems of polynomial equations and before we compare some fundamentally different approaches, we have to provide some definitions first. Note that our notation is adapted from [CLO92], where a comprehensive overview is provided and the underlying concepts for solving polynomial systems are described in great detail.\n\u2013 64 \u2013 November 8, 2019\n4.3 Solving Polynomial Systems: A Comparison of Methods\nDefinition 4.2.1 (Polynomial). A polynomial f in the variables x = {x1, . . . , xn}, and with coefficients a1, . . . , aK in Cn, is a function of the form\nf(x1, . . . , xn) = K\u2211 k=0 akx k1 1 \u00b7 \u00b7 \u00b7xknn . (4.3)\nWe say that the monomial xk = \u220fn i=1 x ki i is of degree |k| = \u2211n i=1 ki and that f is of degree\nmax 0\u2264k\u2264K\n|k|.\nOne fundamental concept in commutative algebra is the notion of a polynomial ring K[x1, . . . , xn]. It consists of all possible polynomials (constructed as in (4.3)) in the variables {x1, . . . , xn} with coefficients ai \u2208 K (see [CLO92, Appendix A] for a more formal definition).\nWe will only work with polynomials that have their variables defined over the complex numbers, i.e., x \u2208 Cn in this thesis. Note that we are actually only interested in polynomials with real variables, i.e., with x \u2208 Rn. Some methods do, however, require the definition over complex variables. Moreover, numerical inaccuracies may introduce a non-zero, albeit negligible, imaginary part to an originally real solution. Considering only x \u2208 Rn leads to the failure of obtaining these solutions. Also, considering the complex domain does no harm as we can later easily choose the real solutions.\nNow let us consider a system of polynomial equations that we intend to solve:\nDefinition 4.2.2 (Polynomial System). A polynomial system, or set of polynomial equations, F consists of s polynomials f1, . . . , fs in the variables {x1, . . . , xn} and is a function of the form\nF (x1, . . . , xn) =  f1(x1, . . . , xn) ...\nfs(x1, . . . , xn).\n(4.4)\nWe are interested in obtaining the set of solutions for which all equations equate to zero. This defines a variety, a classical object in algebraic geometry.\nDefinition 4.2.3 (Set of Solutions (Variety)). A variety, or the set of solutions, V(F ) \u2282 Cn, is the set of variables x for which all equations equate to zero, i.e.,\nV(F ) = {x \u2208 Cn : fi(x) = 0 for all fi(x) \u2208 F (x)}. (4.5)\nNote that the variety is defined over the complex numbers as well; we further define the set of solutions over the real and the strictly positive real numbers VR\u2217+(F ) \u2282 VR(F ) \u2282 V(F )."
        },
        {
            "heading": "4.3 Solving Polynomial Systems: A Comparison of Methods",
            "text": "We are now well prepared to finally present how systems of polynomial equations are actually solved. A great variety of approaches have been developed over the years; we present some of the most well-known approaches and introduce the underlying concepts to highlight some of their advantages and disadvantages. It shall be noted that this is by no means an exhaustive overview. Instead of discussing all methods in full detail, which would be beyond the scope of this thesis, we rather aim to outline the underlying concepts for each method.\nEvery method comes with its specific properties, that have to be considered and should ideally be aligned with the requirements of a given problem; in particular, one should answer the following questions before selecting one particular method to solve the polynomial system (cf. [SW05, p.68]):\nNovember 8, 2019 \u2013 65 \u2013\n4 Discrete-Time Dynamical Systems: Solution Space Analysis\n\u2022 Do we require the set of all solutions V(F ), or just a single possible solution?\n\u2022 What are the specifications of the problem under consideration (sparsity, problem size, etc.) and which methods are well-suited to exploit those properties?\n\u2022 Is the method already available in some software-package?\n\u2022 Are there any hyper-parameters that influence the efficiency of the method and how much experience and effort is required in adapting the parameters to the problem under consideration?\nThe subsequently presented methods belong to \u2013 and are the more prominent examples of \u2013 three fundamentally different approaches: numerical solvers, symbolic methods, and numerical polynomial homotopy continuation (NPHC) methods. We will evaluate and compare the different approaches with respect to the above-mentioned properties. Note that although the most relevant properties for the subsequent chapters will be pointed out, it would be presumptuous to speak of an all-encompassing list of advantages and disadvantages."
        },
        {
            "heading": "4.3.1 Numerical Methods",
            "text": "One basic and well-established method for solving systems of nonlinear equations is the NewtonRaphson method (often just called Newton\u2019s method) which is an iterative solver that progressively refines an initial guess to reach a solution (see [CLO05, pp.30] for a brief introduction, or some classical books on numerical analysis, e.g. [BF11, Chapter 2.3]).\nThe underlying idea is to approximate the considered system by a first-order Taylor approximation in an initial guess, and to successively refine this guess by proceeding along a series of first-order Taylor approximations.\nNewton\u2019s method is a powerful tool that often converges quickly to the solution, particularly if one has a good initial guess. Theoretical results on the convergence rate of such iterative solver are relatively well established (cf. [BF11, Chapter 2.4]) and have two particularly relevant implications for our work: First, an initial guess, sufficiently close to a solution, is required or, otherwise, the iterative solver may diverge of even exhibit chaotic behavior. Second, many different initializations are required to obtain multiple solutions, and it consequently remains problematic to obtain the full set of solutions with these methods."
        },
        {
            "heading": "4.3.2 Symbolic Methods",
            "text": "From a completely different point of view, symbolic methods [CLO92, CLO05] (e.g. Gro\u0308bner basis method, Border basis method, Wu\u2019s method, and method of sparse resultant) rely on symbolic manipulation of the polynomial system and successive elimination of variables to obtain a simpler but equivalent form. In a sense, these methods generalize the Gaussian elimination method from linear systems into the nonlinear settings.\nBefore we discuss the details of symbolic methods, we need to introduce some fundamental notions from commutative algebra. One particularly relevant concept is the notion of an ideal.\nDefinition 4.3.1 (Ideal). Consider a polynomial system F (x1, . . . , xn) in a polynomial ring K[x1, . . . , xn] and some other polynomial system H(x1, . . . , xn) in the same polynomial ring. The ideal \u3008f1, . . . , fs\u3009 is closed under addition and multiplication and defines the set of all polynomial systems that satisfy\n\u3008f1, . . . , fs\u3009 = { s\u2211 i=1 hifi : H \u2208 K[x1, . . . , xn] } . (4.6)\nWe say that the ideal \u3008f1, . . . , fs\u3009 is generated by the basis F (x).\n\u2013 66 \u2013 November 8, 2019\n4.3 Solving Polynomial Systems: A Comparison of Methods\nThe above definition of the ideal implies that any element of the ideal equates to zero if F (x1, . . . , xn) = 0 (cf. [CLO92, pp.30]). This is an immediate consequence of (4.6) and it has important implications for obtaining the variety V(F ). Not that, in fact, the Hilbert\u2019s basis theorem states that every ideal \u3008f1, . . . , fs\u3009 is generated by a finite set of equations. This implies that the variety of the ideal\nV(\u3008f1, . . . , fs\u3009) = { x \u2208 K[x] : f(x) = 0 for all f(x) \u2208 \u3008f1, . . . , fs\u3009 } must equal the variety of its generator, i.e., V(F ) = V(\u3008f1, . . . , fs\u3009) (cf. [CLO92, Proposition 9]). This has one particularly important consequence, namely that two polynomial systems F (x) and G(x) that generate the same ideal, i.e., for which \u3008f1, . . . , fs\u3009 = \u3008g1, . . . , gs\u3009, will also have the same variety, i.e., V(F ) = V(G).\nNote that every ideal has not just one but many different generators; this is a crucial prerequisite for the concept of symbolic methods. Moreover, this allows us to draw an analogy to linear algebra: in this analogy, the ideal corresponds to some subspace (that is closed under addition and multiplication) that is spanned by some vectors (that correspond to the polynomials in the generator).\nSymbolic methods take a fundamentally different approach to solving systems of polynomial equations F (x) than numerical methods. Whereas numerical methods aim to estimate the solution directly, symbolic methods focus on computing an alternative generator G(x) with \u201cgood\u201d properties. The overarching aim is to construct G(x) so that both generators have the same ideal \u3008g1, . . . , gs\u3009 = \u3008f1, . . . , fs\u3009 \u2013 and thus identical solutions, and to concurrently impose certain properties on G(x) that facilitate computing the set of solutions. In particular, we hope that G(x) consists of equations that have a low degree and are in an upper triangle form (to take up the analogy with linear algebra, the aim is to find a more intuitive description of the subspace, e.g., by orthonormal vectors).\nGro\u0308bner Basis\nOne particularly successful way to construct a good basis is to compute the (reduced) Gro\u0308bner basis. Together with the Buchberger algorithm that computes such a basis, Gro\u0308bner bases were introduced in the Thesis of Bruno Buchberger [Buc65,Buc06].\nGro\u0308bner bases tend to consist of equations of low degree that are \u2013 if possible - in upper triangle form. A formal and more comprehensive treatment of the Gro\u0308bner basis method can be found in many textbooks (e.g., in [CLO92]); here we shall be content with illustrating the underlying concepts exemplary.\nExample 6 (Gro\u0308bner Basis for Solving System of Equations). Let us solve a simple polynomial system F (x) with three variables in three equations by utilizing the Gro\u0308bner basis method. Therefore, consider\nf1(x) = x 2 1 + x 2 2 \u2212 1 (4.7) f2(x) = (x1 \u2212 1)2 + x22 \u2212 1 (4.8) f3(x) = x 2 1 + x 2 2 + x 2 3 \u2212 2. (4.9)\nWe then compute the Gro\u0308bner basis using Wolfram Mathematica and obtain an alternative basis according to\ng1(x) = x 2 3 \u2212 1 (4.10) g2(x) = 4x 2 2 \u2212 3 (4.11) g3(x) = 2x1 \u2212 1. (4.12)\nNovember 8, 2019 \u2013 67 \u2013\n4 Discrete-Time Dynamical Systems: Solution Space Analysis\nNote that all equations gi(x) \u2208 G(x) depend only on a single variable and have low degree, thus satisfying all desired requirements. One can consequently obtain all four solutions from G(x) in a straight-forward manner; these are (x1, x2, x3) = ( 1 2 ,\u00b1 \u221a 3 2 ,\u00b11).\nThe Gro\u0308bner basis method obviously obtains the full set of solutions (as compared to only a single one by numerical methods). Furthermore, methods that obtain the Gro\u0308bner basis are particularly attractive for solving systems of polynomial equations as many algorithms are available and included in many software packages.\nThe application of the Gro\u0308bner basis method to large-scale problems with many variables demands a problem-dependent adaption of the algorithm, however, that requires good knowledge of algorithmic details. Even more problematic is the fact that Gro\u0308bner bases are unstable under small changes in the coefficients, which limits its application to problems with rational coefficients. We exemplify this numerical instability in Example 7.\nExample 7 (Instability of Gro\u0308bner Basis). The Gro\u0308bner Basis is numerically unstable with respect to small changes in the parameters. Let us exemplify this by the following system of equation F (x) (inspired by [KR05, Example 6.4.1]): we consider two ellipses defined by the following two polynomials in two variables\nf1(x) = 1\n4 x21 + x 2 2 \u2212 1 (4.13)\nf2(x) = x 2 1 +\n1 4 x22 \u2212 1. (4.14)\nAgain, the Gro\u0308bner basis provides a well-behaved system of polynomial equations\ng1(x) = 5x 2 1 \u2212 4 (4.15) g2(x) = 5x 2 2 \u2212 4, (4.16)\nthat yields all four solutions (x1, x2) = (\u00b1 2\u221a5 ,\u00b1 2\u221a 5 ) immediately.\nNow, assume we rotate the ellipses slightly by changing the underlying system of equations to F\u0303 (x) with\nf\u03031(x) = 1\n4 x21 + x 2 2 \u2212 1 + 0.001x1x2 (4.17)\nf\u03032(x) = x 2 1 +\n1 4 x22 \u2212 1 + 0.001x1x2. (4.18)\nNote that this minor rotation shifts the four solutions change only slightly (cf. Figure 4.1); in fact, the solutions to F\u0303 (x) reside within a radius of 5 \u00b7 10\u22124 of the solutions to F (x). On the other hand, however, a dramatic change occurs in the corresponding Gro\u0308bner basis, both in the coefficients and in the monomials:\ng\u03031(x) = 1250x 3 2 \u2212 1000x2 + x1 (4.19) g\u03032(x) = x 4 2 \u2212 1.6x22 + 0.64. (4.20)\nNot only do the Gro\u0308bner bases G(x) and G\u0303(x) differ in their structure, but \u2013 even more problematic \u2013 computation of V(G\u0303) now suffers from numerical issues. The large coefficients in g\u03031(x) introduce an extreme sensitivity with respect to numerical accuracy and thus render the computation of the solutions by backward substitution highly problematic.\n\u2013 68 \u2013 November 8, 2019\n4.3 Solving Polynomial Systems: A Comparison of Methods\nAccording to the implicit function theorem, the solutions of F\u0303 (x) will change only slightly if the coefficients are modified by an infinitesimally small value; the Gro\u0308bner basis, however, may experience a dramatic shift. In fact, the result will also differ depending on the representation (decimal, or fraction of two integers) of the coefficients in F\u0303 (x). This poses a drastic problem for any polynomial system with non-rational coefficients as the representation of the coefficients will have a major influence on the final result.\nOther Symbolic Methods\nThe limitation of the Gro\u0308bner basis approach to problems with rational coefficients and the numerical instabilities pose a considerable problem for many applications. Consequently, to extend the applicability of symbolic methods, much attention was focused on circumventing these issues and extending the range of solvable polynomial systems.\nAll symbolic methods adhere to one common characteristic; that is the search for an alternative well-behaved generator. There are various ways of obtaining this generator. One promising approach relates the computation of the generator (non-linear in the polynomial ring) to a similar problem in the quotient ring. The consideration in the quotient ring renders the problem a linear one that can be solved efficiently with well-established methods from linear algebra, e.g., by an eigendecomposition. This fruitful connection was first recognized in the work of Winfried Auzinger and Hans J. Stetter.21 Their work already revealed many expedient properties that come with working on the quotient ring [AS88,AS89]. This was only a first step, however, one that avoided the actual construction of the quotient ring [Ste04]. Nonetheless, the underlying ideas proofed to be worth further pursuing and inspired a range of results. The inherent connection between the representation in the quotient ring and the problem of solving algebraic equations was first made explicit in [Mo\u0308l93]. This connection finally led to the first working algorithm, the Mo\u0308ller-Stetter method [MS95]. An excellent overview of the underlying concepts and the initial developments can be found in [Ste04].\nAfter laying out the foundation, many promising advances were proposed, often encompassed under the name of Border basis. This development is still ongoing and the advances are scattered over many publications. We refer to some of the excellent comprehensive papers available that provide a good overview of the recent advances and may serve as a good starting point for the\n21 Stetter who was already about to retire found interest in the topic and prolonged his active research time by another 10 years; the results of which are summarized in his book [Ste04].\nNovember 8, 2019 \u2013 69 \u2013\n4 Discrete-Time Dynamical Systems: Solution Space Analysis\ninterested reader. The beneficial properties of Border basis are outlined and discussed in the tutorial paper [Mou07]; likewise [KKR05] summarizes the major properties and provides some accessible examples that show how to compute the Border basis. In a nutshell, the Border basis method combines the advantages of the Gro\u0308bner basis method, while avoiding some of the most relevant disadvantages (e.g., numerical instabilities). Unfortunately, however, much of the results remain of academic nature and have not yet found their way into existing software packages, thus severely limiting the potential application to practical problems.\nWe would like to stress that the toolbox of symbolic methods is of course not limited to Gro\u0308bner- and Border basis but contains many more methods, all with their own set of advantages and drawbacks. Nonetheless, we hope that this section provides a gentle introduction to some of the more established methods; a more detailed treatment of symbolic methods is available in a number of books (and the references therein) [CLO92,CLO05,KR00,KR05,DE05].\nAs a final statement, we conclude by stressing the overall drawback of symbolic methods: it is often expensive to apply these methods to problems of high degree. This is particularly critical as the sparsity in polynomial systems cannot be utilized in a straightforward manner so that even small \u2013 but high dimensional \u2013 systems suffer from this drawback."
        },
        {
            "heading": "4.3.3 Numerical Polynomial Homotopy Continuation (NPHC) Method",
            "text": "Another important approach for solving a system of polynomial equations is the numerical polynomial homotopy continuation (NPHC) method [Li03,SW05]. The NPHC method performs multiple stages in order to compute the solutions of the target system F (x).\nFirst, the target system is inspected and a root count is computed that provides an upper bound on the number of solutions. Second, a closely related start system Q(x) is created that is trivial to solve and has precisely as many solutions as suggested by the root count. Third, the start system is continuously deformed into the target system. Finally, with appropriate construction, the trivial solutions of the start system also vary continuously under this deformation forming solution paths that connect to the desired solutions of the target system.\nFor instance, one basic form of a homotopy is given by\nH(x, t) = (1\u2212 t)Q(x) + \u03b3tF (x) = 0, (4.21)\nfor t \u2208 (0, 1] and with \u03b3 \u2208 C. Clearly, at t = 0 the homotopy reduces to the start system Q(x) and at t = 1 it reduces to the target system F (x). As t varies continuously from 0 to 1, the homotopy represents a deformation from the start system to the target system and the NPHC method tracks the solutions.\nExample 8 (NPHC method for Solving an Equation). Although the NPHC method is particularly suited for large systems of equations with many variables, we consider one minimalistic example for illustrative purposes. Let us consider the following target system consisting of a single equation f(x) in a single complex variable x\nF (x) = x2 + ix\u2212 2 = 0. (4.22)\nThe root count for (4.22) is straightforward to estimate and just looking at the system reveals the existence of two solutions. This consequently requires a start systems with two solutions as well; one of the simplest system of equations with two solutions is for example given by\nQ(x) = (x\u2212 1)(x+ 1) = 0. (4.23)\nWe can immediately recognize both initial solutions of the start system xs = (\u22121,+1).\n\u2013 70 \u2013 November 8, 2019\n4.3 Solving Polynomial Systems: A Comparison of Methods\nThe homotopy H(x, t) is finally given by\nH(x, t) = (1\u2212 t)(x\u2212 1)(x+ 1) + \u03b3t(x2 + ix\u2212 2) = 0, (4.24)\nwhere \u03b3 = exp(i\u03b8) with a random angle \u03b8 \u2208 [0, 2\u03c0). A couple of solution paths, that emerge for different values of \u03b8, are illustrated in Figure 4.2. One can see that the angle \u03b8 influences the shape of the solution path. Every chosen value \u03b8 has a well-behaved solution path that connects the initial solutions of the start system (xs,0, xs,1) to the desired solutions of the target system (xt,0, xt,1). Using a predictor-corrector method to proceed along the solution path, the NPHC method then obtains both solutions of the target system xt = ( \u221a 7\u2212i 2 ,\u2212 \u221a 7\u2212i 2 ).\nAlthough the NPHC method seemingly provides a simple method to obtain the desired solutions we have spared some of the subtleties that may arise; there are two ingredients in particular that require careful attention and were not mentioned in detail so far. These are the path-tracking algorithm that keeps track of the solution as t increases, the computation of the root count, and the creation of the start system.\nTracking the Solution\nBecause of its importance, the process of tracking the solution paths with increasing t is central to the NPHC method. It is thus important to consider robust and proficient path-tracking algorithms. We will present the key-steps of such path-tracking algorithms in the context of the NPHC method, closely following [SW05, Section 2.3], and refer to [AG03] for a more exhaustive description of path-tracking in general.\nA generic approach must proceed along the solution paths, defined by the continuous homotopy H(x, t) with known initial values H(x, 0) = 0, using a predictor-correction method. Such an iterative algorithm proceeds according to the following three steps. In the prediction step, one fits a function to the current values and predicts the values of x at the next step. This can be done either by extrapolating a linear (or higher-order) function based on the last couple of points, or by linearizing in the last point and proceeding along the tangent direction. Note that the latter approach is well known as Euler\u2019s method.\nIn the correction step, one corrects the predicted value, using a numerical method as e.g., Newton\u2019s method. Despite its shortcomings discussed before, Newton\u2019s method is well suited for the correction step as the prediction step will estimate a value close enough for Newton\u2019s\nNovember 8, 2019 \u2013 71 \u2013\n4 Discrete-Time Dynamical Systems: Solution Space Analysis\nmethod to converge. In general, the accuracy of the prediction will increase with a reduced step-size; if necessary, it is thus always possible to enforce convergence by adaptive reduction of the step-size.\nBefore repeating the above procedure and predicting the next value again, the step-size should ideally adapt according to the current correction step. If the preceding prediction was very accurate it is often safe to assume that the step-size can be enlarged, whereas it is preferable to reduce the step-size otherwise. Overall, the step-size significantly impacts the overall performance; a step-size too small introduces unnecessary many iterations and a step-size too large may lead to failure in the correction step.\nThese steps, prediction, correction, and step-size adaption are then repeated until the solutions of the target system are obtained for t = 1.\nAn important prerequisite for path-tracking algorithms is the existence of a well-behaved path. Fortunately, the introduction of some generic hyper-parameter, as for example the random complex number \u03b3 in (4.21), produces such well-behaved paths with probability one [SW05, Lemma 7.1.3].22\nStart System\nAnother crucial detail in the NPHC method is the computation of the root-count and the creation of the start system. There are various ways of computing this bound that differ in their complexity.\nLoose bounds are often straightforward to compute but may lead to \u2013 potentially many \u2013 solutions in the start system that do not correspond to any solution in the target system. Still, one has to track all emerging solution paths, before finally recognizing the superfluous ones that diverge as the homotopy resembles the target system. Besides unnecessarily wasting computational resources this may have even more serious implications; a root count too large may render solving a problem infeasible because of the sheer amount of paths that need to be tracked.\nVarious methods have been proposed to bound the number of solutions (cf. [SW05, Section 8.1]) with more sophisticated methods generally reducing the number of paths to be tracked. Nonetheless, it is safe to assume that the computational effort required for estimating the bound increases with aiming for a tighter bound. Moreover, closely related to the estimation of the root count, the creation of the start system becomes more intricate as well.\nFor some problems, the consideration of more sophisticated methods scales nicely with the problem and the reduction in the number of paths outweighs the additional burden of creating an appropriate start-system. For other problems, however, estimating a more accurate root count requires significantly more computational resources than the second task, of tracking all paths to their solutions, altogether.\nThe optimal choice of method \u2013 that introduces as few paths as possible and spends as few resources for the creation of the start system as needed \u2013 is thus vital for the overall performance of the NPHC method. Unfortunately, general guidelines do not exist and choosing a reasonable method for a given problem is usually based on experience.\n22 The main argument of the proof is that for all possible values of \u03b3 \u2208 C, except for a finite amount of distinct values, the solution paths are well-behaved. Picking the value \u03b3 at random will therefore result in a wellbehaved path with probability one.\n\u2013 72 \u2013 November 8, 2019\n4.4 Stability Analysis of Fixed Points\nProperties of the NPHC method\nThe NPHC method takes the structure and the sparsity of the target system F (x) into account. If the number of solutions is small, a well-chosen start system will also have only a few solutions thus implicitly exploiting the sparsity of the equation system.\nThe fundamental concept is that every solution of the target system has one corresponding solution residing in the start system. Looking at (4.21) it becomes apparent that, as t goes from 0 to 1, every solution path is completely independent of all others. This property admits parallel implementations that track all solution paths independently after the creation of the start system. The NPHC method is thus, in principle, suited for solving much larger systems than symbolic methods that cannot utilize parallelism in such a way.\nMoreover, the NPHC method is implemented and available in a couple of software packages that often have interfaces with established software packages as, e.g., Maple, Matlab, or Python. Existing software packages, only naming a few, include PHCpack [Ver99], bertini [BHSW], and Hom4PS-3 [CLL14]. The existence of such established software packages makes the NPHC method attractive from a practical perspective and lowers the entrance hurdle.\nOne fundamental disadvantage of the NPHC method is the need for working in the complex domain, even though only the real solutions are of interest. Although the real solutions are contained in the set of complex solutions, working in the complex domain may lead to some problems: On the one hand, the NPHC method may fail if some of the solutions are not zerodimensional, i.e., the solutions are continua instead of points. Restricting ourselves to the real line, these solutions could be zero-dimensional; yet the NPHC method will inevitably fail for its need to work in the complex domain. On the other hand, from a more practical perspective, the consideration of the complex field may increase the computational requirements significantly. This is most obvious if considering a problem with only a few real solutions but a huge amount of complex solutions that induce an equally huge amount of solution paths to be tracked. As it is not possible to determine the complex solutions upfront, all solution paths have to be tracked before neglecting most of them only after the termination of the NPHC method."
        },
        {
            "heading": "4.4 Stability Analysis of Fixed Points",
            "text": "As discussed at the beginning of this chapter, the knowledge of the fixed points alone is not sufficient in getting an adequate understanding of the considered map. All fixed points remain constant under repeated application of the considered map per definition, but may be different in their nature. To account for that difference we classify the fixed points as stable or unstable, corresponding to the long-term behavior of the map.\nWe start by getting some intuition behind the meaning of stable and unstable fixed points. Therefore, consider a dynamical system description of how a ball moves through a surface with valleys and peaks. By neglecting all kinds of forces that occur in real-world, except for gravity, we get a particularly simple model: along the slope, the ball will always accelerate downwards and decelerate upwards. Fixed points are consequently flat points on the surface with zero slope. Assume that ball sits on the top of a peak; then, moving the ball slightly will throw it out of equilibrium as it starts rolling towards another fixed point: we refer to the peak as an unstable fixed point. If, on the other hand, the ball resides at the bottom of a valley, the ball will return back to the fixed point after moving it slightly: we refer to the valley as a stable fixed point.\nIn analogy, the stability of a fixed point can be defined by the trajectory that the state vector describes after setting the state vector to some value close to a fixed point and whether repeated iteration of the map brings it back to the considered fixed point or not.\nLet us make this notion of stability more precise now: A fixed point x\u25e6 is (asymptotic) stable if a neighborhood U(x\u25e6) exists such that any x1 \u2208 U(x\u25e6) converges to x\u25e6; i.e., for every (small)\nNovember 8, 2019 \u2013 73 \u2013\n4 Discrete-Time Dynamical Systems: Solution Space Analysis\nvalue of > 0 there is a maximum number of iterations N so that xn is -close to x\u25e6 for n > N . A fixed point x\u25e6 is unstable if x diverges from the fixed point for some x1 \u2208 U(x\u25e6).23"
        },
        {
            "heading": "4.4.1 Stability Analysis for a Nonlinear Discrete Time Map",
            "text": "We are now going to discuss a simple, yet powerful, method to assess the stability of a fixed point x\u25e6 under a specific discrete-time map F(\u00b7). Therefore, we only require some tools from linear algebra. Note that we can express every linear map as a linear system of equations, and write it in matrix form so that F(x) = Bx where the ith equation is given by\nfi(x) = n\u2211 k=0 ai,kxk. (4.25)\nThis representation already suggests how to assess the stability of a fixed point. First of all, according to (4.2) a fixed point remains unaffected under multiplication by B. Consequently, a fixed point x\u25e6 is stable if repeated application of B brings the state vector x 6= x\u25e6 to the fixed point; accordingly, we call a fixed stable whenever\nx\u25e6 = Bnx (4.26)\nfor all values n > N with n,N \u2208 Z. Taking a closer look at (4.26) reveals how to assess the stability: the effect of repeated multiplication byB directly relates to the eigenvalues [Str16, Chapter 6]. It is a fundamental property of linear algebra that a state vector converges to x\u25e6 if all eigenvalues \u03bbi have a magnitude strictly smaller than one. Accordingly, we conclude that the linear system F(x) exhibits a stable fixed point if its associated system-matrix B has all eigenvalues |\u03bbi| < 1.\nThe above analysis crucially relies on the matrix representation of the discrete-time map. Since non-linear maps do not permit this matrix representation it is not directly possible to generalize the stability analysis. Despite this limitation, we may just approximate the nonlinear map F(\u00b7) by a linear one in every fixed point and analyze the linearized system instead. Fortunately, the Hartman-Grobman theorem [Har60,Gro59] (see [Tes12, pp.264] for a proof) allows us to do just that. More precisely, the theorem permits the simplification of treating the nonlinear system as a linear one in its fixed points (at least in most cases).\nA one-dimensional system is linearized by taking the derivative; a multivariate system is linearized by computing all first-order partial derivatives and collecting them in the Jacobian according to\nF \u2032(x) =  \u2202f1 \u2202x1 \u00b7 \u00b7 \u00b7 \u2202f1\u2202xn ... . . . ...\n\u2202fs \u2202x1 \u00b7 \u00b7 \u00b7 \u2202fs\u2202xn  (4.27) Then, in accordance with linear system theory, we inspect the eigenvalues of the Jacobian.\nTherefore, let us denote the spectrum of the Jacobian, i.e., the set of all eigenvalues by \u039b ( F \u2032(x) ) = {\u03bb1, . . . , \u03bbn}, (4.28)\n23 Note that we do not discuss marginally stable fixed points, i.e., fixed points around which the state vector does neither converge nor diverge. Nonlinear systems require a more involved analysis as considered here if marginally stable fixed points are of relevance.\n\u2013 74 \u2013 November 8, 2019\n4.4 Stability Analysis of Fixed Points\nand the spectral radius, i.e., the maximum magnitude of all eigenvalue by \u03c1 ( F \u2032(x) ) = max\n1\u2264i\u2264n |\u03bbi|. (4.29)\nThis means that \u2013 whenever \u03c1 ( F \u2032(x) ) 6= 1 \u2013 it is possible for nonlinear systems to infer the stability of a fixed point by looking at the spectral radius of the linearized system. A fixed point x\u25e6 is stable if all eigenvalues of the Jacobian have absolute value strictly smaller than one and lie inside the unit circle, i.e., if\n\u03c1 ( F \u2032(x\u25e6) ) < 1. (4.30)\nThe system F(\u00b7) consequently converges to x\u25e6 if initialized sufficiently close enough. A fixed point is unstable with respect to F(\u00b7) if at least one eigenvalue exists outside the unit circle so that \u03c1 ( F \u2032(x\u25e6) ) > 1.\nThe concept of linearization now allows us to assess the stability of a given fixed point. Note that this still excludes any discussion about the region of attraction; just because a fixed point is stable under a given map does not imply that every possible initialization will converge to the fixed point. Nonetheless, the notion of local stability has its merits and tells us whether a map can, in principle, converge to a fixed point (if it is stable) or if it can never converge to a fixed point (if it is unstable).\nNovember 8, 2019 \u2013 75 \u2013\nUnderstanding the Behavior of Belief Propagation\n5 Solution Space of Belief Propagation:\nNumber of Fixed Points and Their Stability\n\u201d Travel makes one modest. You see what a tiny place you occupy in the world.\u201d\n\u2013 Gustave Flaubert\nIn this chapter, we represent BP as a dynamical system and analyze its solution space in order to gain a deeper understanding of BP\u2019s properties. Choosing from the variety of methods presented in Chapter 4 for solving the fixed point equations of BP, the NPHC method proves to be capable of providing the full set of fixed points. This is the first time that this is achieved for finite-size models with non-vanishing local potentials, which reveals a fundamental connection between the Bethe free energy and the accuracy of the marginals. The notion of a local stability analysis relies on the tools presented in Section 4.2 and was first applied to BP \u2013 although only for models without local potentials \u2013 in [MK05]. The NPHC method makes the full set of fixed points available and consequently admits an extension of the local stability analysis to more general models. This extension has far-reaching implications. For example, it was a common conjecture that strong local potentials positively influence the stability; we finally prove this assumption true. The empirical results further inspire the derivation of theoretical results that explain the influence of the potentials and the graph size on the convergence properties.\nWe begin this chapter with a brief summary of existing results on the number of fixed points in Section 5.2. In Section 5.3 we show how to formulate the fixed point equations for BP and how to use the NPHC method for solving them. We then reparameterize the messages and discuss the subtleties of applying a stability analysis to BP in Section 5.4. Finally, we specify a range of models in Section 5.5, before we compute all fixed points and evaluate their accuracy in Section 5.6 as well as their stability in Section 5.7. We then follow our empirical observations and analyze how the model-size and the potentials affect the stability in Section 5.8. In Section 5.9 we compute all fixed points and discuss the limitations of BP in the context of error-correcting codes.\nLarge portions of this chapter have been previously published but were considerably modified to fit into one coherent chapter: The computation of the full set of fixed points and the evaluation of the accuracy have been published in [KMCP18] and in [KPMC16]. Shifting the focus away from symbolic methods was triggered by a fruitful discussion with Michael Kerber; the application of polyhedral homotopy methods finally proved to be successful with the aid of Dhagash Metha and Tianran Chan. In particular the interpretation of the results and the assessment of the accuracy of the BP fixed points are due to the present author. The local stability analysis is a result of joint work with Franz Pernkopf and is published in [KP17]."
        },
        {
            "heading": "5.1 Motivation",
            "text": "The previous chapters introduced BP as an efficient method for approximate inference. For arbitrary models with many loops, however, neither guarantees for convergence nor bounds on\nNovember 8, 2019 \u2013 77 \u2013\n5 Solution Space of Belief Propagation: Number of Fixed Points and Their Stability\nthe approximation error are established. It is precisely the existence of many loops, however, that renders exact inference intractable and requires the utilization of approximate inference methods (as for example BP). A better understanding of BP\u2019s capabilities and limitations would therefore be of great relevance for loopy models. In this chapter, we aim to analyze BP on a range of models in great detail; we further aim to establish theoretical properties on the performance of BP and explain why BP often works surprisingly well on dense and loopy graphs, but other times fails to converge or give accurate results.\nIf multiple fixed points exist, the expected behavior of BP depends on the number of fixed points and the individual fixed point\u2019s behavior. It is thus quite natural to strive for answers to the following questions:\n\u2022 Can we specify model classes \u2013 by restricting, either individually or jointly, the structure and the parameters of probabilistic graphical models \u2013 for which a unique fixed point exists?\n\u2022 Which models admit at least one fixed point to which BP converges for any choice of initial message values (or just if initialized close enough)? Does uniqueness of a fixed point imply that BP converges?\nThese questions have obviously been addressed before: sufficient conditions for uniqueness of fixed points were proposed by accounting for both the potentials as well as the graph structure [Hes04, MK07]. A direct relationship exists between accuracy and convergence rate for graphs with a single loop [Wei00] and for small grid graphs [Ihl07]. In contrast, graphs do exist that feature surprisingly accurate fixed points although BP fails to converge [WJ14a]. We shall thus gain a better understanding of the behavior of BP by developing a precise relation among the number of fixed points (and conditions for uniqueness), the approximation accuracy, and the convergence properties.\nThe consideration of BP as a discrete-time map suggests one way to compute the solution space of BP; the analysis of which reveals deep insights into the behavior of BP and (at least partially) answers the questions raised above. Applying the tools prepared in Chapter 4, one has to perform the following three steps for a given model: (i) obtain the set of all possible fixed points; (ii) assess and compare the accuracy for all fixed points; and (iii) analyze the stability of all fixed points.\nFirst, we must obtain all fixed points: BP is unsuited for this task of obtaining all fixed points as it provides only a single fixed point and, obviously, does not provide unstable fixed points. If the unstable fixed points correspond to local minima of the Bethe free energy they can be obtained by methods that minimize FB directly (cf. Section 3.3.3). Nonetheless, those methods fail to obtain fixed points that correspond to local maxima and are not even guaranteed to obtain all local minima. In order to find all fixed points we reformulate the fixed point equations as a system of polynomial equations that we solve directly.\nSecond, after all fixed points are obtained we assess their accuracy in a straightforward manner; this is possible since we keep the models small enough to admit the application of exact inference methods for comparison.\nThird, the concept of local stability is well established for discrete-time maps in general, as well as for BP in particular (see [MK05] for the special case of binary pairwise models without local potentials). The major obstacle that prohibits the stability analysis for more general models is the need for all fixed points, which are not known in general. We have already obtained all fixed points in the first step, so that we can analyze the stability of all fixed points by linearization of BP now.\nFinally, we consolidate the insights of all three steps and shed some light onto the relation between the number of fixed points, the approximation accuracy, and the convergence properties. In particular, we will provide novel insights and find answers to the following questions:\n\u2013 78 \u2013 November 8, 2019\n5.2 Fixed Points of Belief Propagation\n\u2022 Convergence properties and accuracy relate to each other for small models. Does this generalize to models of arbitrary size as well?\n\u2022 Does BP favor a particular fixed point if multiple fixed points are present? Is the most accurate fixed point always stable?\n\u2022 What can be said about the accuracy if multiple fixed points exist? Do accurate fixed points still exist or does the existence of multiple fixed points imply failure of BP to provide accurate results? Can we enhance the approximation quality by a (suitable) combination of multiple fixed points?\n\u2022 Under which circumstances does damping help to enforce convergence of BP?\n\u2022 What is the influence of the model parameters: How does the graph structure (number of variables and connectivity) influence the number of fixed points and their stability for finite-size graphs? How do the local potentials influence the accuracy and the stability of fixed points? \u2013 It is a common conjecture (cf. [MK05]) that models with weak local potentials perform worst; why does the presence of strong local potentials enhance the performance of BP?"
        },
        {
            "heading": "5.2 Fixed Points of Belief Propagation",
            "text": "Let us recall the iterative message update equations as discussed in Section 3.1:\n\u00b5n+1ij (xj) = \u03b1 n ij \u2211 xi\u2208X \u03a6Xi,Xj (xi, xj)\u03a6Xi(xi) \u220f Xk\u2208{\u2202(i)\\Xj} \u00b5nki(xi). (5.1)\nThe collection of all update equations \u00b5n+1 = BP(\u00b5n) defines a discrete-time map. We can thus rely on the standard recipe for analyzing discrete-time maps and analyze the solution space induced by (5.1) as discussed in Chapter 4. We begin with computing and counting the fixed points \u00b5\u25e6 for some reasonable form of normalization.\nOne alternative way to compute and count the fixed points is to utilize the correspondence to stationary points of the Bethe free energy and to study this energy landscape instead. Several methods (cf. Section 3.3.3) are available that minimize FB and, in the presence of multiple BP fixed points (i.e., for non-convex FB), either converge to a local or the global minimum. None of these methods, however, is guaranteed to capture all fixed points. Moreover, every method that relies on minimizing FB inevitably fails to account for (unstable) fixed points that correspond to local maxima of FB.24"
        },
        {
            "heading": "5.2.1 Number of Fixed Points on Regular Ising Graphs",
            "text": "Remember that the Ising model exhibits critical regions in the parameter space (so-called phase transitions) where the behavior of the model changes abruptly, unless it is specified on a path graph [Geo11, Chapter 12]. In the physics literature, one computes these phase transitions for infinite-size, or at least very large, models. Smaller graphs render the computation of the\n24 The trivial (paramagnetic) solution \u00b5\u25e6ij(xj) = 1 |X| = 1 2 for all \u00b5ij(xj) \u2208 \u00b5 is known in the special case of models with \u03b8 = 0 and, depending on the coupling strength, it is either a local minimum or a local maximum of FB [MK05]. The extension to models with \u03b8i 6= 0, however, is not straightforward and the complete set of all stationary points is not known in general.\nNovember 8, 2019 \u2013 79 \u2013\n5 Solution Space of Belief Propagation: Number of Fixed Points and Their Stability\nphase transitions much more intricate.25 If, however, all nodes have equal degree (and unitary potentials Jij = J and \u03b8i = \u03b8), the phase transitions can be computed for small graphs as well. Therefore, one replaces the finite-size graph by an infinite-size graph with identical properties, a so-called Cayley tree.26 Then, the Cayley tree provides a way to compute the phase transitions of the underlying graph in an analytical fashion [TM06].\nLet us partition the parameter space (\u03b8, J) into three distinct regions (F ), (P ), and (AF ). This terminology complies with the naming convention in statistical physics where the regions are referred to as ferromagnetic (F ), paramagnetic (P ), and antiferromagnetic (AF ). We can then, in accordance with [Geo11, Section 12.2], define the phase transitions partitioning the parameter space. Therefore, let us first introduce the shorthand notation w = tanh |J |; we further introduce the following function\np(J, d) =  d atanh \u221a d\u00b7w\u22121 d/w\u22121 \u2212 atanh \u221a d\u22121/w d\u2212w if J > arcoth(d) d atanh \u221a d\u00b7w\u22121 d/w\u22121 + atanh \u221a d\u22121/w d\u2212w if J < arcoth(d)\n0 else.\n(5.2)\nFinally, the regions are specified according to\n(J, \u03b8) \u2208 (F ) if J > 0, J > arcoth(d) and |\u03b8| \u2264 p(J, d), (5.3) (J, \u03b8) \u2208 (AF ) if J < 0, J < \u2212 arcoth(d) and |\u03b8| < p(J, d), (5.4) (J, \u03b8) \u2208 (P ) if (J, \u03b8) /\u2208 (F ) and (J, \u03b8) /\u2208 (AF ). (5.5)\nFor attractive models with positive couplings (J > 0) BP converges to a unique fixed point inside (P ). This fixed point becomes unstable and two additional fixed points emerge inside (F ) [YFW05,MM09]. For repulsive models with negative couplings (J < 0) BP only converges inside (P ) and not inside (AF ) [MK05].\nThe phase transitions of the complete graph will be computed according to (5.3)-(5.5). The phase transitions of the grid graph can only be numerically estimated and are defined by sudden changes in the number of fixed points. We will later discuss the specific influence of the graph-size on the phase transitions in Section 5.8."
        },
        {
            "heading": "5.3 Fixed Point Equations of Belief Propagation",
            "text": "Now, let us consider the update equations (5.1) as a nonlinear discrete-time map (cf. Section 3.5). We will adhere to the notation of Chapter 4 and Definition 4.1.1 in particular and consequently formulate the fixed point equations for the update rule of BP. Then, we aim to solve the fixed point equations directly; this will yield the set of all BP fixed points.\nWe did not consider a particular form of message normalization so far, i.e., how to choose \u03b1ij . The normalization, however, affects the update rule and must therefore be defined explicitly before attempting to solve the fixed point equations; we define the normalization terms \u03b1ij so that the messages along every edge sum up to one, i.e., \u03b1nij = \u2211 xj\u2208X \u00b5 n ij(xj)\u2212 1.\nLet us recall the definition of fixed point messages from (4.2). Then, the update equations\n25 Note that true phase transitions, for which the partial derivatives of FB vanish can only exist for infinite-size graphs [Bin87]; with slight abuse of notation we refer to the finite-size manifestations of phase transitions as phase transitions as well. 26 A Cayley tree is an infinite tree without loops that captures the interactions of a cyclic finite-size graph.\n\u2013 80 \u2013 November 8, 2019\n5.3 Fixed Point Equations of Belief Propagation\ndefine the following set of polynomial equations F (\u00b5,\u03b1) = \u00b5\u25e6 \u2212 BP(\u00b5\u25e6) = 0 so that\nF (\u00b5,\u03b1) =  \u00b5nij(Xj = +1)\u2212 \u03b1nij \u2211 xi\u2208X \u03a6Xi,Xj (xi,+1)\u03a6Xi(xi) \u220f Xk\u2208{\u2202(i)\\Xj} \u00b5nki(xi) \u00b5nij(Xj = \u22121)\u2212 \u03b1nij \u2211 xi\u2208X \u03a6Xi,Xj (xi,\u22121)\u03a6Xi(xi) \u220f Xk\u2208{\u2202(i)\\Xj} \u00b5nki(xi)\n\u00b5nij(Xj = +1) + \u00b5 n ij(Xj = \u22121)\u2212 1.\n(5.6)\nThis system of polynomial equations consist of S equations (f1(\u00b5,\u03b1), . . . , fs(\u00b5,\u03b1)), where\nS = 2|E| \u00b7 (|X |+ 1). (5.7)\nIt is advantageous to consider this polynomial system defined over the complex numbers instead of the real numbers. Else, we would restrict ourselves and rule out several methods for solving systems of polynomial equations that rely on the definition over the complex numbers (cf. Section 4.3.3). Let us now define the set of solutions over the complex numbers, without accounting for multiplicity, according to\nV(F ) = {(\u00b5,\u03b1) \u2208 C : fi(\u00b5,\u03b1) = 0 for all fi \u2208 F }. (5.8)\nWe are particularly interested in the set of solutions over strictly positive real numbers VR\u2217+(F ) \u2286 V(F ). Note that VR\u2217+ is of specific relevance as it directly relates to the fixed points of BP.\nTheorem 1 (Fixed Points of BP). Let (\u00b5,\u03b1) be some set of messages and normalization terms. Then, (\u00b5,\u03b1) is a fixed point of BP, if and only if (\u00b5,\u03b1) \u2208 VR\u2217+(F ).\nProof. First, we show that every (\u00b5,\u03b1) \u2208 VR\u2217+(F ) characterizes a fixed point of BP. All messages are positive by definition and are normalized according to (5.6) so that they represent probabilities (cf. Lemma 1). Furthermore, it follows from (5.6) that \u00b5 \u2212 BP(\u00b5) = 0, which constitutes a fixed point.\nConversely, consider some fixed point messages with its corresponding normalization coefficients (\u00b5\u25e6,\u03b1\u25e6), it then follows by definition that BP(\u00b5\u25e6) = \u00b5\u25e6 and consequently F (\u00b5,\u03b1) = 0.\nThe set of solutions over the strictly positive real number thus directly corresponds to the set of all fixed points according to T = {P\u0303B = DP (\u00b5\u25e6),ZB = DZ(\u00b5\u25e6) : (\u00b5\u25e6,\u03b1\u25e6) \u2208 VR\u2217+(F )}.\nCorollary 1.1. Consider a graph with strictly positive potentials \u03a6(xi, xj) and \u03a6(xi) as e.g., with Ising potentials. Then, the solution set VR\u2217+(F ) is nonempty.\nProof. For non-negative potentials the average energy is bounded from below and FB has at least one minimum [YFW05, Theorem 4]. Minima of the constrained FB correspond to BP fixed point solutions, the existence of which implies non-emptiness of VR\u2217+(F ) by Theorem 1."
        },
        {
            "heading": "5.3.1 Solving the Fixed Point Equations",
            "text": "Solving systems of nonlinear polynomial equations is a classical problem in computational mathematics and a great variety of methods have been developed such as iterative solvers (cf. Section 4.3.1), symbolic methods (cf. Section 4.3.2), and homotopy methods (cf. Section 4.3.3). We now briefly review the applicability of these methods for the purpose of computing the set of all solutions.\nThe main disadvantage of iterative solvers is the need for an already known initial guess in the vicinity of a solution. Moreover, it is difficult, to obtain the full set of solutions with these\nNovember 8, 2019 \u2013 81 \u2013\n5 Solution Space of Belief Propagation: Number of Fixed Points and Their Stability\nmethods; this is of particular relevance as we are specifically interested in the entire set of the (positive) real solutions VR\u2217+(F ).\nSymbolic methods, on the other hand, are capable of obtaining the entire solution set. In particular, the Gro\u0308bner basis method is available in a number of software-packages and has seen substantial development in the past several decades. As discussed in Section 4.3.2, the main drawbacks are: the limitation to rational coefficients (cf. Example 7), a worst-case complexity that is double exponential in the number of variables [MM82,MM84], and the limited scalability in parallel computations. Altogether, this limits the application of symbolic methods to smaller systems; in fact the Gro\u0308bner basis method did not converge for any of our considered models.\nThe NPHC method obtains the entire solution set by deforming a start system that is trivial to solve to the target system that we intend to solve. Although maybe not as established as the Gro\u0308bner basis method, multiple software packages are available for the NPHC method as well. One crucial ingredient of the NPHC method is that a number of independent solution paths is tracked under the deformation of the homotopy. This suggests to track all solution paths independently, which makes the approach pleasantly parallelizeable; this is essential in dealing with large polynomial systems. Even though only positive real solutions are of interest in this work, the NPHC method requires us to extend the domain to the field of the complex numbers in order to guarantee the emergence of smooth solution paths. Obviously a notable computational overhead is introduced in doing so; especially if only few of the solutions fall onto the real line. Various different forms of homotopies are available that differ in the complexity of bounding the number of solutions. As discussed in Section 4.3.3 the optimal choice depends strongly on the particular target system and is rarely known up-front.\nAt large, the NPHC method is the most promising approach for a couple of reasons: it obtains all isolated nonzero complex solutions27 that must include all BP fixed points VR\u2217+(F ); it is readily available in software packages; and it has a level of parallel scalability that can deal with models of relevant size"
        },
        {
            "heading": "5.3.2 Polyhedral Homotopy Method",
            "text": "Let us briefly recall one of the most basic forms of homotopies from Section 4.3.3:\nH(x, t) = (1\u2212 t)Q(x) + \u03b3tF (x) = 0. (5.9)\nAlternatively, one can construct more advanced \u201dnonlinear\u201d homotopies where the parameter t appears in nonlinear form, in order to reduce the computational costs. Among a great variety of polynomial homotopy constructions, the polyhedral homotopy method, developed by B. Huber and B. Sturmfels [HS95], is particularly suited for the systems of polynomial equations considered.\nIn applying the NPHC method to solve (5.6), the choice of Q(\u00b5,\u03b1) (the trivial system of equations that the target system is deformed into) plays an important role in the overall efficiency of the approach since different choices of Q(\u00b5,\u03b1) may induce a vastly different number of solution paths one has to track. The crucial part is to come up with a good upper bound on the number of solutions and to create an appropriate start system. Once this is solved, the desired solutions are simply obtained by tracking all independent solution paths.\nNote that in each equation of (5.6) only few of the monomials are present, i.e., the update equations of BP imply a sparse system of equations [HS95]. In our experiments, we observed that despite the rather high total degree28 dt [SW05, pp.118], each equation in (5.6) contains only\n27 Here, \u201cnonzero complex solutions\u201d refer to complex solutions of a system of polynomial equations where each variable is nonzero. A solution is considered to be isolated if it has no degree of freedom, i.e., there is an open set containing it but no other solutions. 28 The total degree of a system of polynomial equations is the product of the degrees of each equation. It is a basic fact in algebraic geometry that the total number of isolated complex solutions a polynomial system has\n\u2013 82 \u2013 November 8, 2019\n5.3 Fixed Point Equations of Belief Propagation\nrelatively few of the monomials. Such sparse systems usually benefit from resorting to more involved method homotopy methods that take the structure of the system into account and consequently provide tighter bounds on the number of solutions. The number of solution paths one has to track when using the polyhedral homotopy method for solving a system of polynomial equations is given by the so-called Bernstein-Kushnirenko-Khovanskii (BKK) bound : fixing the list of monomials that appear in the polynomial system, it is an important yet surprising fact in algebraic geometry that for almost all choices of the coefficients (in the probabilistic sense), the number of isolated nonzero complex solutions is a fixed number which only depends on the list of monomials. This number is known as the BKK bound [Ber75, Kus76, Kho78]. Intermediate steps in the determination of the BKK bound are reused to create an appropriate start system. Using the fully parallel implementation Hom4PS-3 [CLL14] of the polyhedral homotopy method, we compute the BKK bound, that is tight in all our experiments, and obtain all isolated positive solutions.\nThe polyhedral homotopy method exploits the structure of (5.6), but also requires some subtle steps. Rather than presenting all technical details we present an illustrative example to explain the underlying principles. For more details we refer the reader to the excellent overview papers [Li97,Li03,CL15] or to [SW05, Section 8.5.4] and the references therein.\nExample 9 (Polyhedral Homotopy). The essential steps in solving polynomial system with the polyhedral homotopy method are: first, to compute a root count based on mixed volume computations [Li03, Section 3]; second, to come up with an easy to solve start system [Li03, Section 4]; and finally, to solve the start system and track the solution paths to the target system [Li03, Section 1].\n(i) Root Count: Consider the example system (taken from [SW05, p.142]) with two unknown variables x = {x1, x2} and with 4 solutions.\nF(x) =\n{ 1 + ax1 + bx 2 1x 2 2\n1 + cx1 + dx2 + ex1x 2 2.\n(5.10)\nThe total degree of this system of equations is dt = 4 \u00b73 = 12, which serves as an upper bound on the actual number of solutions. If the system of equations is sparse, the BKK bound serves as a much tighter bound.\nEvery equation fi \u2208 F(x) has an associated polytope Si which is the convex hull of the exponent vectors for all monomials of fi. For f1 the polytope is\nS1 = {(0, 0)(1, 0)(2, 2)}, (5.11)\nwhich has a graphical representation in Figure 5.1 (a). Similar for f2 the polytope, shown in Figure 5.1 (b), is\nS2 = {(0, 0)(1, 0)(0, 1)(1, 2)}. (5.12)\nSome important operation on polytopes are the computation of the Minkowski sum S1 + S2 = {s1 + s2 : s1 \u2208 S1, s2 \u2208 S2} and the computation of volumes, denoted by V (Si). Note that computing the BKK bound is a viable thing to do in any dimension; therefore we refer to V (Si) as volume although the polytopes of this example only lie in the two-dimensional space. The computation of the mixed volume M(S1, S2) is a combinatorial problem that is especially comprehensible in the case of two equations where\nM(S1, S2) = V (S1 + S2)\u2212 V (S1)\u2212 V (S2). (5.13)\nis bounded by its total degree (i.e., Bezout bound). Therefore the total degree serves as a crude measure of the complexity of the polynomial system.\nNovember 8, 2019 \u2013 83 \u2013\n5 Solution Space of Belief Propagation: Number of Fixed Points and Their Stability\nFor a generalization to higher dimensions see [SW05, p.140]. The polytope of S1 + S2 is illustrated in Figure 5.1 (c); the mixed volume is accordingly obtained by subtracting V (S1) and V (S2) from V (S1 + S2), which equals the sum of all gray areas (known as mixed cells).\nIt is straightforward to see that each parallelogram has volume equal to 2; the BKK bound therefore equals M(S1, S2) = 4 and thus provides a tight bound on the number of solutions.\n(ii) Start System: The BKK bound does provide a tight bound on the number of solutions but does not immediately reveal the initial solutions of an appropriate start system Q(x) with qi(x) = \u2211 a\u2208Si ci,ax\na, where xa = xa11 \u00b7 xa22 and ci,a are random coefficients. However, the mixed volume computation can also be accomplished by introducing a lifting\n\u03c9i = {\u03c9i(a) : a \u2208 Si}\nfor each fi. Thereby, we increase the dimension of the polytope Si to S\u0302i by adding one component to each exponent-vector a. This component is obtained by the lifting function \u03c9i(a). In our example we choose the lifting values \u03c91 = {0, 0, 0} and \u03c92 = {0, 1, 1, 3}; these values are obtained by the inner products \u03c91(a) = (0, 0)\u25e6 (a1, a2) and \u03c92(a) = (1, 1)\u25e6 (a1, a2). The polytopes are lifted accordingly so that\nS\u03021 = {(0, 0, 0)(0, 1, 0)(2, 2, 0)}, S\u03022 = {(0, 0, 0)(1, 0, 1)(0, 1, 1)(1, 2, 3)},\nS\u03021 + S\u03022 = {(0, 0, 0)(0, 1, 0)(2, 2, 0)(0, 2, 1)(0, 1, 1)(3, 2, 1)(3, 4, 3)}.\nThen the faces in the lower hull of S\u03021 + S\u03022 correspond to cells shown in Figure 5.1, which is known as a fine mixed subdivision.\nThese liftings, together with the random coefficients ci,a, now form the homotopy Q\u0302(x, t) with q\u0302i = \u2211 a\u2208Si ci,ax at\u03c9i(a) such that\nQ\u0302(x, t) =\n{ 1 + c1,1x1 + c1,22x 2 1x 2 2\n1 + c2,1x1t+ c2,2x2t+ c2,3x1x 2 2t\n3. (5.14)\nBy closer inspection, however, it is still not possible to identify the starting points because q\u03022(x, t = 0) = 1. This problem can be resolved according to [HS95, Lemma 3.1]: i.e., initial values are obtained by solving a binomial system for every cell that contributes to the mixed volume computation (i.e., for every gray cell in Figure 5.1). One can then increase t and obtain the solutions of the start system by tracking the solution paths to Q\u0302(x, t = 1) = Q(x).\n(iii) Target System: Finally we have all 4 solutions to Q(x). Now what remains is to construct a linear homotopy according to (4.21) and to increase t, starting at t = 0. At t = 1 the homotopy reduces to F(x) and provides the desired solutions of the target system.\n\u2013 84 \u2013 November 8, 2019\n5.4 Stability of Fixed Points"
        },
        {
            "heading": "5.4 Stability of Fixed Points",
            "text": "We proceed according to the usual procedure in dynamical systems (cf. Chapter 4) and, after computing the set of all fixed points, asses their (local) stability. A fixed point is locally stable if a neighborhood exists such that messages inside this neighborhood (i.e., messages sufficiently close to the fixed point), converge to the fixed point under the considered map [Tes12, pp.170].\nNote that the Bethe free energy does not fully characterize the stability of a given fixed point (cf. Section 3.3.2): although stable fixed points are local minima of FB, local minima must not be stable [Hes03]. A general way of investigating the stability of fixed points is available by the method of Lyapunov [Sch00, pp.93], though it is sufficient for all graphs considered in this work to restrict our analysis to linearization (the indirect method of Lyapunov) as introduced in Section 4.4.\nThis approach is well-established in the dynamical systems literature and it may consequently seem rather surprising that such a stability analysis has not been considered so far for BP. The main difficulty, however, is not the stability analysis as such, but the prerequisite of estimating all fixed points. Indeed, a local stability analysis has been performed for Ising models with vanishing local potentials [MK05], for which all fixed points are known. We can, however, rely on the NPHC method, as discussed in the preceding section, to solve the fixed point equations and to obtain the set of all fixed points. Subsequently, it is evident how to analyze the stability by computing the Jacobian and thus linearizing BP in every fixed point."
        },
        {
            "heading": "5.4.1 Reformulation of Belief Propagation for Binary Variables",
            "text": "Before we investigate the stability of all BP fixed points, we introduce an alternative parameterization of the update equations for the particular case of binary variables. This parameterization reduces the number of variables and eases some calculations without changing the properties of BP. Therefore, we express both messages along the same edge as a single message (cf. [MK05,KP17]) defined by\n\u03bdnij = atanh ( \u00b5nij(Xj = 1)\u2212 \u00b5nij(Xj = \u22121) ) . (5.15)\nThen, the update rule in (3.1) can be rewritten according to\ntanh(\u03bdn+1ij ) = tanh(Jij) tanh(h n ij), (5.16)\nwhere the cavity field hnij acts on Xi, while neglecting the incoming message from Xj , according to\nhnij = \u03b8i + \u2211\nXk\u2208{\u2202(i)\\Xj}\n\u03bdnki. (5.17)\nConsidering (5.16) and (5.17) (cf. [OW01]) it becomes evident that, for binary pairwise models, BP corresponds to the so-called cavity method [MPV87]. Its name stems from the fact that we essentially dig a cavity into the model by removing Xj . We then express the remaining field that acts Xi by the cavity field hij .\nThe marginals, or the mean, can then \u2013 similar as for BP \u2013 be computed by all incoming\nNovember 8, 2019 \u2013 85 \u2013\n5 Solution Space of Belief Propagation: Number of Fixed Points and Their Stability\nmessages according to mi = tanh ( hij + \u03bdji ) = \u03b8i +\n\u2211 Xk\u2208\u2202(i) \u03bdki. (5.18)\nIn accordance with the consideration of BP as a dynamical system, we will denote the set of all re-parameterized messages by \u03bd and denote the mapping induced by BP as \u03bdn+1 = BP(\u03bdn). Keeping the notation consistent, we say that BP converged to a fixed point\n\u03bd\u25e6 = BP(\u03bd\u25e6), (5.19)\nif successive messages remain unchanged under BP. If, however, BP fails to converge, one can try to achieve convergence by one of the many modifications discussed in Section 3.5.1. One of these modifications that lends itself handsomely for our stability analysis is BP with damping.\nLet us briefly state the update equations of BP with damping in terms of the reformulated messages again: BPD(\u03bdn) = (1 \u2212 )BP(\u03bdn) + \u03bdn, where \u2208 [0, 1) is the damping factor. Remember that, although the fixed points of BP with damping are fixed points of BP without damping as well (cf. (3.51)), the stability of the fixed points may change nonetheless."
        },
        {
            "heading": "5.4.2 Linearization",
            "text": "In order to assess the stability of the fixed points \u03bd\u25e6 we approximate BP (\u00b7) by a linear function in its fixed point(s) and analyze the behavior of the linearized system. This is done by taking the partial derivatives of all messages, i.e., by analyzing the Jacobian matrix F \u2032(\u03bd\u25e6) with its elements defined as\nF \u2032(\u03bd\u25e6)mn = \u2202\u03bd\u25e6ij \u2202\u03bd\u25e6kl , (5.20)\nwhere \u2013 given some ordering \u2013 (i, j) and (k, l) are the mth and the nth edge. For binary pairwise models parameterized as in Section 5.4.1, the Jacobian is given as follows: without loss of generality29 we consider only messages where (i, j) \u2208 E and (k, l) \u2208 E such that\nF \u2032(\u03bd\u25e6)mn =  tanh(Jij)(1\u2212tanh2(hij)) 1\u2212tanh2(Jij) tanh2(hij)\nif i= l and k\u2208{\u2202(i)\\Xj} 0 else.\n(5.21)\nLet us briefly recap the properties of the eigenvalues with respect to the stability of the BP fixed points from Section 4.4. A fixed point \u03bd\u25e6 is locally stable if all eigenvalues have absolute value strictly smaller than one, i.e., if \u03c1 ( F \u2032(\u03bd\u25e6) ) < 1 and BP converges if initialized sufficiently close enough. A fixed point is unstable with respect to BP if at least one eigenvalue exists outside the unit circle such that \u03c1 ( F \u2032(\u03bd\u25e6) ) > 1. For \u03c1 ( F \u2032(\u03bd\u25e6) ) = 1 stability of the nonlinear system cannot be inferred by just looking at the linear system.\nIn addition to damping, there are many other variants of BP available that have the same set of solutions V \u2217R+ [WJW03a]; in particular, this includes different scheduling methods. Analyzing the stability of the fixed points under these variants, however, becomes problematic \u2013 mainly because the update function changes with time. Moreover, the fact that many messages are not\n29 If all combinations of m and n are considered we would effectively consider all n(n\u22121) 2\npossible edges. All rows and columns that correspond to a message \u03bdvw, where (v, w) /\u2208 E include only zero-values, however, and can therefore be neglected without changing the eigenvalues.\n\u2013 86 \u2013 November 8, 2019\n5.5 Selected Models with Ising Potentials\n1 Re{\u03bb}\nIm{\u03bb}\nBP stable\nBPD stable unstable\n( F \u2032(\u03bd\u25e6) ) . BP is stable if all eigenvalues lie inside the\nupdated, introduces eigenvalues with \u03bbi = 1, which renders the stability analysis by linearization impossible.\nTherefore, of all the modified versions of BP, we will restrict our attention to damping. The application of damping modifies the eigenvalue spectrum according to\n\u039b ( F \u2032D(\u03bd\u25e6) ) = \u039b ( F \u2032(\u03bd\u25e6) ) \u00b7 (1\u2212 ) + . (5.22)\nNote that all eigenvalues are reduced by a factor (1 \u2212 ) and experience a shift by along the real axis, i.e., Re { \u039b ( F \u2032D(\u03bd\u25e6) )} = + Re { (1\u2212 )\u039b ( F \u2032(\u03bd\u25e6) )} . A fixed point \u03bd\u25e6 is thus locally stable under BPD (\u00b7) if\nRe { \u039b ( F \u2032(\u03bd\u25e6) )} < 1. (5.23)\nThe correspondence between stability and the eigenvalue spectrum is summarized and visualized in Figure 5.2. A fixed point is stable under BP if all eigenvalues lie inside the unit circle (depicted by the blue area), under BP with damping if all eigenvalues have a real part strictly smaller than one (depicted by the gray area), and are unstable else (depicted by the green area).\nNote that the properties of the Bethe Hessian (and thus the stability of the fixed points) further relate to some concepts from graph-theory; in particular to the Ihara zeta function [WF09] and \u2013 for models with vanishing local fields, i.e., where \u03b8 = 0 \u2013 to the non-backtracking matrix (known as the Hashimoto-matrix) [SKZ14,SKZ17]. Note that the latter connection is insightful by any means but is only valid in a well-behaved region where all eigenvalues of the non-backtracking matrix are inside the unit circle (cf. [SKZ17, Theorem 2.1])."
        },
        {
            "heading": "5.5 Selected Models with Ising Potentials",
            "text": "Finally, we will now consider a range of probabilistic graphical models, apply the NPHC method to the fixed point equations (5.6), obtain all fixed points by first finding all isolated non-zero complex solutions, and analyze the stability of all fixed points.\nWe first evaluate and compare the accuracy of all fixed points obtained by NPHC; details for our evaluation criteria are presented in Section 5.6.1. Note that we will already anticipate some results from the later stability analysis and separately evaluate the accuracy of stable and unstable fixed points. However, here we rather ask if a given fixed point is stable, instead of why it is stable. Furthermore, we present the evolution of the fixed points over the parameter space in Section 5.6.5 and present some implications on the accuracy to better understand for which parameters BP can be expected to provide good results.\nThe capability of NPHC to obtain all fixed points, subsequently allows for a thorough stability\nNovember 8, 2019 \u2013 87 \u2013\n5 Solution Space of Belief Propagation: Number of Fixed Points and Their Stability\n(a) (b) (c)\nFigure 5.3: Considered Ising graphs: (a) grid-graph; (b) complete graph; (c) grid-graph with periodic boundary conditions.\nanalysis in Section 5.7. Our empirical observations further inspire some theoretical investigations on Ising models with unitary parameters in Section 5.8. There we show why convergence properties degrade with growing graph size and why strong local potentials help to achieve convergence. Finally, the performance of BP-decoding for error-correcting codes is analyzed in terms of the solution space in Section 5.9.\nConsidered Graphs\nWe consider different realizations of the Ising model with attractive, repulsive, and mixed interactions on a range of graphs. These graphs include complete graphs, grid-graphs, and grid-graphs with periodic boundary conditions (see Figure 5.3).\nFor the complete graph each pair of nodes is connected by an edge; it follows by definition that this is a regular graph, i.e., all variables Xi \u2208 X have equal degree di = N \u2212 1. Because of this, we can construct a Cayley tree to determine the phase-transitions.\nThe grid-graph has all edges aligned along the two-dimensional square lattice and, for finitesize graphs, contains variables of varying degrees. We additionally consider grid-graphs with periodic boundary conditions, where nodes on the boundary are joined by edges so that all variables have equal degree again."
        },
        {
            "heading": "5.6 Number of Fixed Points and Marginal Accuracy",
            "text": "This section provides an exhaustive solution space analysis of BP in terms of analyzing the properties for all fixed points. In order to obtain the set of all fixed points we have to solve the system of fixed point equations (5.6) first. The systems considered in this work are simply too large to be solved with symbolic methods or even with the NPHC method based on a linear homotopy. The BKK bound, however, takes into account the sparsity of the system F (\u00b5,\u03b1), induced by the graph structure, and reduces the number of solution paths to be tracked so that the problem can be solved in practice. We present a detailed runtime analysis in Section 5.6.6. Note that both the structure of (5.6) and the number of complex solutions in V (F) remain the same if the graph structure is kept constant [CM17, CMN16]; depending on the potentials, however, the number of solutions in V \u2217R+(F) may change."
        },
        {
            "heading": "5.6.1 Evalutation Criteria",
            "text": "We evaluate all fixed points in terms of their marginal accuracy and compare them with marginals obtained by an implementation of BP without damping. Evaluation of the marginal\n\u2013 88 \u2013 November 8, 2019\n5.6 Number of Fixed Points and Marginal Accuracy\naccuracy requires the availability of the exact marginals; these are obtained by the junction tree algorithm that we can still resort to because of the limited size of the considered models.\nIn particular, we evaluate the correctness of the approximated marginals in terms of their accuracy (cf. (3.38)). The expected mean (cf. (3.39)) provides an alternative way of evaluating the marginals and is particularly well suited for illustrative purposes. We will therefore not only compare different fixed points in terms of the MSE but also visualize the averaged mean of the exact solution \u3008m\u3009 as well as of all BP fixed points \u3008m\u0303\u3009. Remember that \u3008m\u3009 \u2212 \u3008m\u0303\u3009 equals the average of all marginal errors (cf. (3.39)) and is thus well-suited for visually comparing the marginal accuracy of different fixed points.\nWe will not only compare different fixed points but also evaluate the effect of altering the evaluation function. A combination of properly weighted marginals may improve the marginal accuracy considerable but is often problematic in practice for the lack of methods that obtain more than a single fixed point. The NPHC method, however, computes the set of all solutions VR\u2217+(F ) and \u2013 by Theorem 1 \u2013 yields the set of all fixed points T. This is the foundation for evaluating the marginal accuracy of different combinations.\nTo combine the marginals, we first need to compute the pseudomarginals P\u0303B and the Bethe partition function ZB for all fixed points. Then, we utilize the obtained values of the Bethe partition function ZB together with the pseudomarginals P\u0303B and modify the evaluation function DP in three different ways.\nFirst, all fixed points are weighted by their partition function and are combined so that\nP\u0303 TB = 1\u2211\nZB\u2208T ZB \u2211 (P\u0303B ,ZB)\u2208T P\u0303B \u00b7 ZB. (5.24)\nSecond, we combine all fixed points that correspond to local minima of the Bethe partition function in the same way so that\nP\u0303MB = 1\u2211\nZB\u2208M ZB \u2211 (P\u0303B ,ZB)\u2208M P\u0303B \u00b7 ZB. (5.25)\nNote that these combinations are not just some heuristics. In particular the latter one reminds us of how the exact solution is expected to decompose according to the RSB assumption.\nFinally, we aim to select one specific fixed point and hope that accurate marginals are to be found at the maximum of the Bethe partition function (i.e., the global minimum of the Bethe free energy). The fixed point maximizing ZB is consequently selected and evaluated as well; more formally it is given by\nP\u0303MAXB = arg max P\u0303B\u2208T ZB(P\u0303B). (5.26)"
        },
        {
            "heading": "5.6.2 Grid Graphs with Random Factors (Spin Glasses)",
            "text": "Consider a grid graph of size N = 3 \u00d7 3 with randomly distributed parameters. All pairwise and local potentials are sampled uniformly; i.e., (Jij , \u03b8i) \u223c U(\u2212K,K). The larger the support of the uniform distribution is, the more difficult the task of inference becomes; we choose K = 3, which is large enough to make BP fail to converge sometimes (cf. [SM07]).\nAccording to (5.7) the system of equations consists of M = 72 equations in 72 unknowns. More specifically, (5.6) consists of 24 linear (i.e., normalization constraints), 40 quadratic, and 8 cubic equations; the total degree bounds the number of solutions by dt = 1\n24 \u00b7240 \u00b738 = 7.2 \u00b71015. Tracking such an amount of solution paths is not feasible in practice, even with a parallel\nNovember 8, 2019 \u2013 89 \u2013\n5 Solution Space of Belief Propagation: Number of Fixed Points and Their Stability\nimplementation of the NPHC method. The system of equations in (5.6), however, is sparse. We can exploit this sparsity that is induced by the graph structure if we consider the BKK bound and reduce the computational complexity. The number of complex solutions for this graph is bounded by BKK = 608. After creating a suitable start system the problem is straightforward to solve with the NPHC method. It actually turns out that the BKK bound is tight for all graphs considered.\nIn particular we evaluate 100 grid graphs with random factors: on 99 graphs BP converged after at most 104 iterations. Although the grid graph has multiple loops and the constrained FB is not necessarily convex [Hes04, Corr.2], we observe that for all 100 graphs NPHC obtains a unique positive real solution that corresponds to a unique BP fixed point."
        },
        {
            "heading": "5.6.3 Grid Graphs with Uniform Factors",
            "text": "We further analyze BP on grid graphs of size N = 3 \u00d7 3 with constant potentials among all nodes and edges; i.e., we specify the couplings for all edges by Jij = J and specify the fields for all nodes by \u03b8i = \u03b8. We apply BP and NPHC for 1681 graphs in the parameter region (J, \u03b8) \u2208 {\u22122,\u22121.9, . . . , 1.9, 2} and illustrate the size of the solution set in Figure 5.4.\nThe number of solutions in V \u2217R+(F) is presented in Figure 5.4 (a). In the well-behaved region (P ) of the parameter space a unique fixed point exists, whereas three fixed points exist in (F ) and (AF ) \u2013 this is in accordance with statistical mechanics [MM09, p.43].30 Interestingly, we observe a close relation between the onset of phase transitions and the increase in the number of real solutions in Figure 5.4 (b). Most of these solutions, however, correspond to negative message values that violate Lemma 1 and are not feasible.\nBP converges to some fixed point on all 1681 graphs within at most 104 iterations. This raises a couple of questions: What is the approximation error of BP if it converges to the most accurate fixed point? Or, speaking in terms of free energies, how large is the gap between the global minimum of the constrained FB and the minimum of the Gibbs free energy? To answer this question we evaluate the correctness of the approximated marginals by computing the MSE between the exact and the approximated marginals according to (3.38). The results are presented in Table 5.1. Averaged over all graphs we can see that BP does not necessarily converge to the most accurate fixed point. For the NPHC method we present the MSE for the fixed point with the lowest MSE; this highlights the existence of fixed points, which give more accurate approximations than BP. Looking at all parameter regions separately we can see that BP does converge to the global optimum in (P ), as well as in (F ). In the antiferromagnetic\n30 Note that the graph under consideration is of finite size and thus di varies among the nodes. As a consequence the partitioning according to (5.3) - (5.5), is only an approximation.\n\u2013 90 \u2013 November 8, 2019\n5.6 Number of Fixed Points and Marginal Accuracy\nregion (AF ), BP converges to a fixed point that does not necessarily give the best possible approximation.\nIf we consider regions with multiple fixed point solutions (i.e., (F ) and (AF )) it becomes obvious that the fixed point maximizing the partition function (i.e., P\u0303MAXB ) is not necessarily the best one; this is especially surprising as BP obtains the best possible fixed point solution inside (F ).\nFor \u03b8 = 0 it turns out that initializing all messages to the same value \u00b51ij(xj) = \u00b5 1 = 1|X | will result in a fixed point where P\u0303Xi(xi) = 0.5 (and mi = 0) for all Xi \u2208 X. Although this fixed point is identical to the exact one, it may be unstable (cf. Figure 5.6).\nInspired by these observations, one should not only consider the fixed point maximizing ZB (i.e., P\u0303MAXB ), but rather obtain multiple fixed points by NPHC and combine them. Indeed, especially inside region (F ) a combination of all fixed point solutions according to (3.53), i.e., P\u0303 TB increases the accuracy of the approximation. If we combine the fixed points at local minima only (which are all stable in this specific case), i.e., P\u0303MB , the accuracy increases even more and gives the most accurate approximation over the entire parameter space."
        },
        {
            "heading": "5.6.4 Complete Graphs with Uniform Factors",
            "text": "We consider a complete graph with N = 4 binary random variables and illustrate the number of solutions in Figure 5.5. The system of equations (5.6) consists of 36 equations in 36 unknowns and has its number of solutions bounded by the total degree dt = 1\n12 \u00b7224 = 16.8 \u00b7106. Similar as for the grid graph, a much tighter bound of BKK = 120 is provided by the BKK bound. Among all four nodes, we apply unitary factors, specified by Jij = J and \u03b8i = \u03b8. This type of graph is particularly interesting because one can derive exact conditions where phase transitions occur (cf. Section 5.2.1).\nFor (J, \u03b8) \u2208 (P ), BP has a unique fixed point, which is a stable attractor in the whole message space [MK05]. In (F ) three fixed points satisfy (5.6), one of which is unstable and a local minimum of ZB. Both other fixed points are local maxima of ZB and BP converges to one of them.\nFor repulsive models, BP only converges inside (P ) and not inside (AF ) as shown in Figure 5.5 (a). We can see two interesting effects: first, in Figure 5.5 (c) the number of real solutions increases at the onset of phase transitions; second, even though the convergence of BP breaks down at the phase transition, a unique fixed point exists inside (AF ) (Figure 5.5 (b)) that gives an accurate approximation (cf. Table 5.2).\nSimilar as in Section 5.6.3, we asses the MSE of the marginals obtained by NPHC and BP; the results \u2013 averaged over all graphs, and for each distinct region \u2013 are presented in Table 5.2. Indeed, inside region (AF ) the marginals obtained by NPHC give a much better approximation than BP does.\nNovember 8, 2019 \u2013 91 \u2013\n5 Solution Space of Belief Propagation: Number of Fixed Points and Their Stability\nFurthermore, note that the fixed point maximizing ZB does not always give the best approximation. For \u03b8 = 0, if all messages are initialized to the same value, BP obtains the exact marginals (cf. Section 5.6.3). If multiple fixed points exist, a weighted combination of all marginals according to (3.53) increases the accuracy \u2013 only considering stable solutions, i.e., P\u0303MB , gives the most accurate approximations."
        },
        {
            "heading": "5.6.5 Fixed Point Evolution",
            "text": "To better understand the influence of the parameters (J, \u03b8) we specifically investigate how they affect the accuracy of the fixed points. Therefore, we fix the values of \u03b8 \u2208 {0, 0.1, 0.5}, vary J \u2208 [\u22122, 2], and compare the fixed point solutions obtained by NPHC to the exact solution. For illustrative purposes we consider the mean, averaged over all variables, of the exact solution \u3008m\u3009 and of the approximate solution \u3008m\u0303\u3009 for all fixed points obtained by NPHC; the results are illustrated for both the grid graph and the complete graph in Figure 5.6 and Figure 5.7.\nThe exact solution (red) is obtained by the junction tree algorithm [LS88]; solutions to (5.6) are obtained by NPHC and are depicted by blue dots (stable) and by green or black dots (unstable). We further emphasize the fixed point that maximizes ZB, i.e., P\u0303MAXB , in orange.\nWe will already discuss some implications of the local stability analysis here, but focus mainly on the accuracy of the fixed points and defer a thorough discussion of the stability analysis to Section 5.7.\nAll fixed points for a grid graph with N = 3 \u00d7 3 binary random variables are shown in\n\u2013 92 \u2013 November 8, 2019\n5.6 Number of Fixed Points and Marginal Accuracy\nFigure 5.6. The worst performance in terms of accuracy is observed for \u03b8 = 0. Despite the existence of one fixed point that corresponds to the exact solution with \u3008m\u3009 = \u3008m\u0303\u3009 = 0, BP fails to provide accurate marginals as this particular fixed point is only stable inside (P ). As the coupling strength |J| increases to the onset of phase transitions, two additional fixed points emerge. These additional solutions are symmetric, stable, and guarantee the convergence of BP on this graph (see Figure 5.6 (a)).\nFor \u03b8 6= 0 a unique stable fixed point exists inside (P ). If we gradually increase J until (J, \u03b8) \u2208 (F ) two additional fixed points emerge, one of which is unstable (see Figure 5.6 (b)). Note that the fixed point maximizing ZB (orange) remains stable for all values of J \u2208 [\u22122, 2]. An increase in \u03b8 (see Figure 5.6 (c)) does enlarge the region where a unique fixed point exists and further increases the accuracy of the fixed point maximizing ZB. For (J, \u03b8) \u2208 (AF ) a similar behavior is observed; i.e., for small values of \u03b8 the unstable fixed point has the highest accuracy, but as \u03b8 increases, the accuracy of the fixed point maximizing ZB increases as well.\nAll fixed points for the complete graph with N = 4 binary random variables are shown in Figure 5.7. For \u03b8 = 0 and large values of J the fixed point with \u3008m\u3009 = \u3008m\u0303\u3009 = 0 is unstable and is accompanied by two symmetric, stable fixed points (Figure 5.7 (a)). In contrast to the grid graph a unique and accurate fixed point exists for (J, \u03b8) \u2208 (AF ); yet, this of no avail as the fixed point is unstable (see Figure 5.5 (a)). This highlights that the existence of a unique, accurate, fixed point does not necessarily imply convergence of BP.\nNovember 8, 2019 \u2013 93 \u2013\n5 Solution Space of Belief Propagation: Number of Fixed Points and Their Stability\nFor \u03b8 6= 0 the non-convergent region (AF ) is slightly reduced, but the problem of a unique unstable fixed point persists. In contrast to the grid graph, the complete graph contains oddlength cycles, thus allowing for frustrations if all edges are repulsive. This points at a close connection between the existence of frustrations and the existence of a unique unstable solution. The complete graph behaves similar to the grid graph for (J, \u03b8) \u2208 (F ), i.e, the accuracy of the fixed point maximizing ZB increases as \u03b8 increases (see Figure 5.7 (b) and Figure 5.7 (c)).\nOur main findings are: First, increasing the field \u03b8 increases the accuracy of the fixed point maximizing ZB. Second, for \u03b8 6= 0 the fixed point maximizing ZB is unique and varies continuously under a change of J . Finally, the two stable fixed points are close to being symmetric, i.e., P\u0303 (1) Xi\n(xi) \u223c= 1 \u2212 P\u0303 (2)Xi (xi). Consequently combining both stable fixed points will not lead to good approximations unless a proper weighting by ZB is applied. Applying a proper weighting, however, leads to accurate approximations (cf. Table 5.1- 5.2)."
        },
        {
            "heading": "5.6.6 Runtime Analysis",
            "text": "The time required for solving (5.6) is presented in Table 5.3 for grid graphs with random factors (Section 5.6.2), grid graphs with unitary factors (Section 5.6.3), and fully connected graph with unitary factors (Section 5.6.4). Comparing the overall computation time of the NPHC method to BP it becomes obvious that NPHC is no alternative in terms of computational efficiency. It is, however, the only method that is guaranteed to obtain all fixed points \u2013 we were not able to apply the Gro\u0308bner basis method beyond a single-cycle graph with N = 4. For our computations we utilized a cluster-system with 160 CPUs.\nIf we compare the overall computation time to the actual computation time utilizing the parallel implementation it becomes obvious that NPHC benefits tremendously from the high degree of parallelization. The runtime of BP depends mainly on the number of iterations and less on the size of the graph. Consequently, the stability of fixed points directly affects the performance of BP (cf. non-convergent region in Figure 5.5 (a)). The NPHC method is much less sensitive to the stability of fixed point solutions; the mixed volume computation, which has the largest influence on the overall runtime, rather depends on the number of variables in (5.6). Note the mixed volume computation does not depend on the parameters. If one is interested in the fixed points for different parameter-sets on the same graph it would suffice to compute the mixed volume and the start system only once; we did not do this to allow for a fair comparison.\n\u2013 94 \u2013 November 8, 2019\n5.7 Empirical Stability Analysis of Belief Propagation on Ising Models"
        },
        {
            "heading": "5.7 Empirical Stability Analysis of Belief Propagation on Ising",
            "text": "Models\nSo far we have been rather sloppy with the notion of stability and assumed knowledge about the stability of the fixed points without discussing how to gain this knowledge. The fact that we have obtained all fixed points, irrespective of their properties under a specific map, admits a thorough stability-analysis for all of them.\nIn this section, we analyze the local stability of all fixed points for the models discussed so far. Then, we compare the results to known results of both infinite and finite-size graphs with vanishing local fields \u03b8 = 0. We restrict our analysis to attractive and repulsive models (cf. Section 2.4.3), because this allows us to change the behavior of BP with just a single parameter.\nWe briefly present the results of [MK05] for vanishing fields in Section 5.7.1. Then we extend the analysis to graphs with non-vanishing fields, discuss some empirical observations, and interpret the implications. Note that the results of our stability analysis are also illustrated in terms of the average mean \u3008m\u0303\u3009 over the couplings-strength in Figure 5.6 (for the 3 \u00d7 3 grid graph) and 5.7 (for the complete graph of size N = 4). All fixed points of BP are colored according to the discussion of the eigenvalue spectrum (cf. Figure 5.2): fixed points are depicted in blue if stable under BP (\u00b7), in black if stable under BPD (\u00b7), and in green if unstable. For reference we also illustrate the exact solution in red. A more formal analysis is presented in Section 5.8."
        },
        {
            "heading": "5.7.1 Vanishing Local Field",
            "text": "It is generally assumed that the case of vanishing local fields is the worst-case scenario [MK05]; we will confirm this assumption empirically and analytically. For \u03b8 = 0 a trivial exact solution exists \u2013 namely marginals that are uniform over all states, i.e., for all Xi \u2208 X the singleton marginals are P\u0303Xi(xi) = 0.5 and the mean is mi = 0.\nFor attractive models with J > 0 sufficiently small, BP converges to this trivial (paramagnetic) fixed point, which is unique and stable. As the coupling-strength increases, the eigenvalue with the largest magnitude \u03bbmax increases as well (see Theorem 4) and as \u03bbmax crosses the unit circle the paramagnetic fixed point remains unchanged but becomes unstable. At the same time, two additional fixed points appear \u2013 these fixed points are symmetric and stable (cf. Figure 5.6 (a) and Figure 5.7 (a)).\nFor repulsive models with J < 0 all entries of the Jacobian swap in sign, i.e., F \u2032\u2212(\u03bd\u25e6) = \u2212F \u2032+(\u03bd\u25e6), where \u2212 and + indicate the Jacobian for repulsive and attractive models, respectively. It follows that \u039b ( F \u2032\u2212(\u03bd\u25e6) ) = \u2212\u039b ( F \u2032+(\u03bd\u25e6) ) . Consequently, the local stability of the fixed point is invariant under a sign-change of J and instability occurs precisely for the same coupling strength as before (cf. Figure 5.6 (a) and Figure 5.7 (a)). There is one important difference though \u2013 the dominant eigenvalue is negative now, such that damping helps to achieve convergence."
        },
        {
            "heading": "5.7.2 Non-Vanishing Local Field",
            "text": "For reasons of simplicity we restrict our analysis to models with a single value \u03b8i = \u03b8 6= 0 for all variables. Because the Ising model is symmetric with respect to the local fields \u03b8i, it is sufficient to consider only non-negative local fields. Moreover, the same qualitative results hold if we allow for \u03b8i \u2265 0 in general.\nAttractive Models\nFor small values of J > 0 a unique fixed point exists to which BP converges and for which all eigenvalues lie inside the unit circle. If we gradually increase the coupling-strength to the point where instability occurred for \u03b8 = 0, we observe that all eigenvalues are still inside the\nNovember 8, 2019 \u2013 95 \u2013\n5 Solution Space of Belief Propagation: Number of Fixed Points and Their Stability\n.\n\u2013 96 \u2013 November 8, 2019\n5.7 Empirical Stability Analysis of Belief Propagation on Ising Models\nunit circle, i.e., the spectral radius \u03c1 ( F \u2032(\u03bd\u25e6) ) < 1; the fixed point is consequently stable (cf. Theorem 3 for more details). If we further increase \u03b8, the spectral radius decreases and a unique stable fixed point persists for even larger values of J (compare Figure 5.6 (b) and Figure 5.7 (b) with Figure 5.6 (c) and Figure 5.7 (c)).\nNow, if we increase J \u2013 although the fixed point remains stable \u2013 two additional fixed points emerge beyond some critical point JC(G, \u03b8) (see Figure 5.6 (b) and 5.7 (b)). As opposed to vanishing local fields, the unique fixed point is continuously deformed and remains stable though. The second stable fixed point corresponds to some self-preserving state of magnetization31 and is accompanied by another unstable fixed point (Figure 5.8 (b)).\nThe graph structure is supposed to influence the spectral radius as well. This becomes obvious if we enlarge the size of a graph while keeping its local structure unchanged, e.g., by increasing the grid-graph from N1 = 9 to N2 = 16. A comparison of Figure 5.9 with Figure 5.10 reveals an increased spectral radius, i.e., \u03c1 ( F \u2032N1(\u03bd\u25e6) ) < \u03c1 ( F \u2032N2(\u03bd\u25e6) ) (cf. Theorem 4). It is interesting that the largest eigenvalue \u2013 and its symmetric counterpart for bipartite graphs \u2013 are the only eigenvalues that experience a relevant increase in their real part. The real part of all other eigenvalues \u03bbi \u2208 \u039b ( F \u2032(\u03bd\u25e6) ) \\ { \u03bbmax : |Re{\u03bbmax} | = \u03c1 ( F \u2032(\u03bd\u25e6) )} is bounded by |Re{\u03bbi} | < 1.\nIn terms of accuracy, BP performs better for models with \u03b8 6= 0; compared to models with \u03b8 = 0, a stable fixed point exists that lies closer to the exact solution. Loosely speaking, increasing the local fields effectively reduces the influence of the couplings. This does not only lead to better convergence properties of BP (cf. Corollary 4.1), but reduces the approximationerror as well.\nRepulsive Models\nFor small values of J < 0 a unique fixed point exists for which all eigenvalues lie inside the unit circle. If we further decrease J , a change of behavior can be observed beyond some critical value JC(G,\u2212\u03b8). Interestingly, |JC(G,\u2212\u03b8)| \u2264 |JC(G, \u03b8)| with equality if and only if \u03b8 = 0. In Figure 5.8 (b) and 5.8 (c) we can also see that \u03c1 ( F \u2032\u2212(\u03bd\u25e6) ) > \u03c1 ( F \u2032+(\u03bd\u25e6) ) , i.e., the invariance of local stability under sign-change of J does not hold in general. Let J < JC(G,\u2212\u03b8), then it depends on the graph structure whether a unique fixed point exists and if damping is useful. It turns out that these two properties are closely connected: First, consider the complete graph with N = 4 or the grid-graph with periodic boundary conditions; these models have a unique fixed point but frustrations exist because of cycles with odd-length. Consequently \u039b ( F \u2032(\u03bd\u25e6) ) is non-symmetric (cf. Theorem 5). Besides the sign-\nchange, the spectral radius increases as well so that \u03c1 ( F \u2032\u2212(\u03bd\u25e6) ) > \u03c1 ( F \u2032+(\u03bd\u25e6) ) ; the dominant\neigenvalue, however, has a negative sign and R ( \u039b ( F \u2032(\u03bd\u25e6) )) < 1 so that an appropriate damping term exists that enforces BP to converge. Second, consider both grid-graphs with N1 = 9 and N2 = 16; these graphs are bipartite and\nhave a symmetric spectrum \u039b ( F \u2032(\u03bd\u25e6) ) (see Figure 5.9, Figure 5.10 (b), and Theorem 5). Because of the symmetric spectrum BP behaves similarly as in the attractive case: i.e., no damping term exists that would stabilize the unstable fixed point; two additional fixed points exist, however, that are stable.\nMultiple fixed points for repulsive models exist if and only if the underlying graph is bipartite. Such graphs can be decomposed into two disjoint subsets X = Y\u222aZ such that the means follow a \u201dcheckerboard-distribution\u201d where for all Xi \u2208 Y the mean is mi > 0 and for all Xj \u2208 Z the mean is mj < 0, or exactly the other way round. This explains the existence of two stable fixed points. Note that the according average mean \u3008m\u0303\u3009 behaves as follows: (i) if N is even both stable fixed points are symmetric and have the same average mean; (ii) if N is odd both stable fixed points have different average means, because the subsets differ in size, i.e., |Y| 6= |Z|. The\n31 Self-preserving states are stable fixed points where \u02dc\u3008m\u3009 points into the opposite direction as \u03b8.\nNovember 8, 2019 \u2013 97 \u2013\n5 Solution Space of Belief Propagation: Number of Fixed Points and Their Stability\ndifference in \u3008m\u0303\u3009 reduces as the number of variables increases. In the limit of N = \u221e, the average means of all fixed points collapse onto the same value.\nTo conclude our observations: either a unique fixed point exists, which may be unstable but can be stabilized by damping (black); or a fixed point exists which is unstable under any form of damping (green) but is accompanied by two stable fixed points (blue) (cf. Figure 5.6 and Figure 5.7). Therefore, non-vanishing fields increase the accuracy of BP (Figure 5.6 and Figure 5.7) and lead to better convergence properties (Theorem 3) in all experiments."
        },
        {
            "heading": "5.8 Theoretical Stability Analysis",
            "text": "Here we present some more formal arguments and explain the observations made in Section 5.7. We start with the Perron-Frobenius Theorem and specify some implications for the particular form of F \u2032(\u03bd\u25e6). Then we provide properties of the eigenvalue spectrum \u039b ( F \u2032(\u03bd\u25e6) ) and explain the influence of finite-size graphs in non-vanishing local fields. We consider only connected graphs (cf. Section 2.2), where all Xi have minimum degree min di \u2265 2. Note that it is straightforward to absorb any Xi with di = 1 into its neighbor.\nLemma 2. The Jacobian matrix F \u2032(\u03bd\u25e6) of a connected graph G is irreducible if min di \u2265 2.\nProof. The adjacency matrix of a connected undirected graph is irreducible. Let us construct the Jacobian F \u2032(\u03bd\u25e6) as in (5.20); note that we can partition F \u2032(\u03bd\u25e6) into N \u00d7N block matrices [F \u2032(\u03bd\u25e6)]ik of size di\u00d7dk each. These blocks contain non-zero values if and only if (i, j), (k, l) \u2208 E for Xj , Xk \u2208 X; i.e., if aik = 1. It follows that F \u2032(\u03bd\u25e6) is irreducible as well.\nTheorem 2 (Perron Frobenius Theorem). A real non-negative square matrix B has its spectral radius bounded as follows:\nmin i \u2211 j bij \u2264 \u03c1 ( B ) \u2264 max i \u2211 j bij . (5.27)\nIf B is irreducible the largest eigenvalue is a positive real number \u03bbmax = \u03c1 ( B ) .\nCorollary 2.1 (Implications for regular graphs). A regular graph has di = d for all Xi \u2208 X. If the couplings and fields are constant, \u2211 n F \u2032(\u03bd\u25e6)mn = c is constant for all rows. By the Perron Frobenius Theorem and by Lemma 2 it follows that \u03bbmax = c.\nConsider an infinite-size grid graph with unitary couplings J > 0. Under these assumptions it is fairly straightforward to provide theoretical insights that explain why the existence of nonvanishing fields enhances the convergence properties. We generalize the results subsequently, as the same argument can be applied to finite-size graphs with different Jij .\nTheorem 3. Let G\u221e be an infinite-size graph with di = d and purely attractive interactions J > 0. Then, the existence of a non-vanishing external field \u03b8 6= 0 stabilizes BP.\nProof. First assume a vanishing external field, i.e., \u03b8 = \u03b80 = 0, in which case the trivial fixed point has identical messages \u00b5\u25e6ij(xj) = 0.5 so that \u03bd \u25e6 ij = 0. Consequently, (5.21) reduces to\nF \u2032\u03b80(\u03bd\u25e6)mn= { tanh(J) if i = l and k \u2208 {\u2202(i)\\Xj} 0 else.\n(5.28)\nBecause all variables Xi \u2208 X(G\u221e) have equal degree \u2211 n F \u2032(\u03bd\u25e6)mn = c for all m. It follows\nfrom (5.21) and the Perron-Frobenius Theorem that \u03c1 ( F \u2032\u03b80(\u03bd\u25e6) ) = tanh(J) \u00b7 (di \u2212 1). Without\n\u2013 98 \u2013 November 8, 2019\n5.8 Theoretical Stability Analysis\nloss of generality we exploit symmetry properties and assume an external field \u03b8\u03b4 > 0. By (3.1) and (5.16) it becomes obvious that every fixed point message \u03bd\u25e6ij 6= 0 and hij 6= 0. Note that qualitatively, it does not matter whether hij is positive or negative as we only consider tanh 2(hij).\nBecause 0 \u2264 tanh(J) < 1 it follows that tanh(J) > tanh2(J) and consequently, as\nF \u2032\u03b8\u03b4(\u03bd \u25e6)mn = tanh(J)(1\u2212tanh2(hij)) 1\u2212tanh2(J) tanh2(hij)\nif i = l and k\u2208{\u2202(i)\\Xj} 0 else,\n(5.29)\nall non-zero entries of F \u2032\u03b80(\u03bd\u25e6)mn are element-wise larger than F \u2032\u03b8\u03b4(\u03bd \u25e6)mn. Let us define a fielddependent scaling term \u03ba\u03b8 \u2208 (0, 1) of the Jacobian matrix , then F \u2032\u03b8\u03b4(\u03bd \u25e6) = \u03ba\u03b8F \u2032\u03b80(\u03bd\u25e6).\nLoosely speaking the existence of some non-vanishing field reduces all entries of the Jacobian matrix and consequently reduces the spectral radius.\nNow choose a critical value JC at the onset of instability 32 so that \u03c1 ( F \u2032\u03b80(\u03bd\u25e6) ) = lim\n\u21920 (1 + ). If the external field dampens the spectral radius, so that \u03c1 ( F \u2032(\u03bd\u25e6) ) < 1 the unstable fixed point vanishes, and only a unique stable fixed point remains. Interestingly, this observation holds in all experiments, i.e., for attractive models an unstable fixed point exists if and only if multiple fixed points are present.\nSuppose we either have purely attractive or purely repulsive interactions, then all entries of the Jacobian matrix have the same sign as the couplings and are bounded.\nLemma 3. All entries of the Jacobian are bounded by F \u2032(\u03bd\u25e6)mn \u2208 [0, 1) if Jij > 0 and by F \u2032(\u03bd\u25e6)mn \u2208 (\u22121, 0] if Jij < 0.\nProof. If all couplings have the same sign, the qualitative result of (5.29) holds as well. For purely repulsive interactions we just have to swap signs.\nTheorem 4. The finite-size manifestations of phase transitions occur beyond the theoretical phase transitions. Let us define a parameter-set (JC , \u03b8C) for which \u03bbmax crosses the unit-circle. Then, for a graph GN with N nodes, the values of the critical parameters (JC , \u03b8C) decrease as N increases.\nConsider two finite-size graphs with identical structure33 with different size N1 < N2, then \u03c1 ( F \u2032N1(\u03bd\u25e6) ) \u2264 \u03c1 ( F \u2032N2(\u03bd\u25e6) ) \u2264 \u03c1 ( F \u2032\u221e(\u03bd\u25e6) ) .\nProof. Assume J > 0 and w.l.g. \u03b8 = 0 (for the influence of non-vanishing external field see Theorem 3). Then on G\u221e \u2013 or for any other grid-graph with periodic boundary conditions \u2013 the degree di = d is constant for all Xi \u2208 X. It follows from Corollary 2.1 that the largest eigenvalue is given by \u03bbmax = (di \u2212 1) \u00b7 tanh(J).34 Suppose we increase the couplings to Jnew > J and denote the change of its parameters as \u03ba = tanh(J\nnew) tanh(J) . It is obvious that \u03bb new max = \u03ba \u00b7 \u03bbmax.\nSuppose we have some finite-size graph where di is not constant but depends on Xi. If the couplings increase as before each row of F \u2032(\u03bd\u25e6) experiences a different amount of scaling. In all generality this is described byc1 0. . .\n0 cK  \u00b7 F \u2032(\u03bd\u25e6) (5.30) 32 If \u03bbmax = 1 we cannot infer from the linearized to the nonlinear map [Tes12]. We introduce an -small term\nto avoid this subtlety. 33 For example two-dimensional grid-graphs with N = 3\u00d7 3 or N = 4\u00d7 4 variables. 34 For vanishing fields, i.e., if \u03b8i = 0, the Jacobian matrix is fully defined by Jij .\nNovember 8, 2019 \u2013 99 \u2013\n5 Solution Space of Belief Propagation: Number of Fixed Points and Their Stability\nwhere ck depends on di and K = N\u2211 i=1 di. Let us reformulate (5.30) in all detail to\nW = max(di)\u2211 m=2 CmF \u2032(\u03bd\u25e6), (5.31)\nwhere Cm is a diagonal matrix with values Cm;kk = \u03bam if the k th line of F \u2032(\u03bd\u25e6) corresponds to a variable with dk \u2265 m and 0 otherwise. Since the largest eigenvalue is a positive real number it follows that the largest eigenvalue is the sum of the individual eigenvalues, so that\n\u03c1 ( W ) = max(di)\u2211 m=2 \u03c1 ( CmF \u2032(\u03bd\u25e6) ) . (5.32)\nWe still have to show that\n\u03c1 ( F \u2032N1(\u03bd\u25e6) ) (a) \u2264 \u03c1 ( F \u2032N2(\u03bd\u25e6) ) (b) \u2264 \u03c1 ( F \u2032\u221e(\u03bd\u25e6) ) , (5.33)\nwhere (b) follows from the existence of an associated eigenvector x such that \u03c1 ( W ) x \u2264 (di \u2212 1) \u00b7 tanh(J) with equality if and only if all Cm have only non-zero values on the main diagonal, i.e., all variables have equal degree. As GN2 has a larger portion of nodes with di = max di than GN1 , the average degree is also higher; i.e., d\u0302GN1 < d\u0302GN2 . Then (a) follows from (5.32).\nBy combination of Theorem 3 and Theorem 4 we get:\nCorollary 4.1. The existence of a non-vanishing external field stabilizes BP on finite-size graphs.\nNext, we extend the above observations to varying couplings and fields. We assume that all couplings Jij have the same sign, and all fields \u03b8i have the same sign, not necessarily the same as the couplings. Then the scaling coefficients ck in (5.30) depend not only on di, but on Jij and \u03b8i as well. Still, F \u2032(\u03bd\u25e6) can only contain either positive entries or negative entries; it follows that:\nCorollary 4.2. For infinite-size grid-graphs with either purely attractive interactions Jij > 0 or purely repulsive interactions Jij < 0, the existence of some non-vanishing fields \u03b8i 6= 0 stabilizes BP. Theorem 5. The eigenvalue-spectrum of the Jacobian F \u2032(\u03bd\u25e6) is symmetric if and only if the underlying graph is bipartite.\nProof. The adjacency matrix of any bipartite graph can be rearranged and written in block form\nA =\n[ 0 M\nMT 0\n] , (5.34)\nso that the eigenvalue-spectrum is symmetric [BH11]. By the same arguments as in Lemma 2 it follows that F \u2032(\u03bd\u25e6) has the same structure as A and thus a symmetric spectrum as well.\nCorollary 5.1. Assume we only have repulsive interactions. Then a graph is bipartite if and only if no frustrations occur.\nProof. A cycle is frustrated if and only if the product of all Jij along the corresponding edges is negative [MM09, p.45]. Frustrations can therefore only occur in graphs with cycles of odd length, which implies that the graph cannot be bipartite [KV05, Prop. 2.27].\n\u2013 100 \u2013 November 8, 2019\n5.9 Application: Error-Correcting Codes"
        },
        {
            "heading": "5.9 Application: Error-Correcting Codes",
            "text": "One of the most prominent applications where BP is successfully applied to loopy graphs is iterative decoding. We keep this section as self-contained as possible. For a thorough introduction we refer the interested reader to the textbooks [Mac03, Wym07]; the connection between BP and decoding is further explained in great detail in [KF98,AM00,KFL01].\nWe consider a binary symmetric channel (BSC) with a binary input Xi \u2208 X = {0, 1} = {xi, x\u0304i} and a binary output Yi \u2208 Y = {0, 1} = {yi, y\u0304i}. The channel is specified by the error-probability , where transmitted bits are flipped with probability . That is PXi|Yi(xi|yi) = PXi|Yi(x\u0304i|y\u0304i) = 1 \u2212 and PXi|Yi(xi|y\u0304i) = PXi|Yi(xi|yi) = (cf. Figure 5.11 (a)). Additional, redundant bits help to detect and correct transmission errors. The aim of error-correcting codes is to reach the desired error-correcting performance while introducing as little redundancy as necessary, i.e., to operate as close as possible to the theoretical limit. Suppose we transmit a codeword with block length N = 7 consisting of 4 source bits X1, . . . , X4 and three parity-check bits X5, X6, X7 that satisfy\nX1 \u2295X2 \u2295X3 \u2295X5 = 0, X2 \u2295X3 \u2295X4 \u2295X6 = 0, X1 \u2295X3 \u2295X4 \u2295X7 = 0,\nwhere \u2295 is an XOR, i.e., the sum in modulo-2 arithmetic. This linear irregular code is the (7,4) Hamming code [Mac03, Chapter 1].\nIn this example we assume that the sent message is x = (0, 0, 0, 0, 0, 0, 0)35 and that exactly one bit suffers from a bit flip. For irregular codes the degree of the variables di varies; therefore, we consider two scenarios: either y = (1, 0, 0, 0, 0, 0, 0) or y = (0, 0, 0, 0, 0, 1, 0)36.\nIt is often convenient to express a code in factorized form and represent it explicitly with a\n35 Note that the properties of the BSC are independent of the transmitted codeword x. 36 Normally the performance of a code is studied over an ensemble of sent codewords where each bit flips with\nprobability .\nNovember 8, 2019 \u2013 101 \u2013\n5 Solution Space of Belief Propagation: Number of Fixed Points and Their Stability\nfactor graph. A factor graph consists of variable nodes Yi and factor nodes fA where each factor fA acts as a function on all variables connected Yi = {Yi \u2208 \u2202(A)}.37 On a factor graph, BP operates similar as introduced in Section 3.2. Now, two different types of messages are sent along every edge: factor-to-variable messages rAi and variable-to-factor messages qiA. All messages are iteratively updated according to\nrn+1Ai (yi) = \u2211\nyk:Yk\u2208{\u2202(A)\\Yi}\nfA(Yk = yk, Yi = yi) \u220f\nYk\u2208{\u2202(A)\\Yi}\nqnkA(yk), (5.35)\nqn+1iA (yi) = \u03b1 n iA \u220f fB\u2208{\u2202(i)\\fA} rnBi(yi), (5.36)\nwhere \u03b1iA is chosen such that q n+1 iA (yi) + q n+1 iA (y\u0304i) = 1. After all messages converged to a fixed point, the marginals of the variable nodes are approximated by the product of all incoming messages\nP\u0303Yi(yi) = 1\nZ \u220f fB\u2208\u2202(i) rnBi(yi). (5.37)\nDetails of the factor graph representation can be found in [MM09, Chapter 9] and [Loe04]. Let us consider two types of factors: fi(Yi) = PXi|Yi(xi|yi) to model the BSC, and fA(Yi) to verify if all parity-checks are satisfied. Then fA(Yi) = 1 if the sum of all arguments \u2211\nYi yi\nis even and fA(Yi) = 0 if the sum is odd. The conditional probability for X = x to be the codeword, given the received codeword Y = y then is\nPX|Y(x|y) = 1\nZ 7\u220f i=1 fi(Yi) \u00b7 fa(Y1, Y2, Y3, Y5) \u00b7 fb(Y2, Y3, Y4, Y6) \u00b7 fc(Y1, Y3, Y4, Y7). (5.38)\nThe corresponding factor graph representation is shown in Fig 5.11 (b). Now we create a system of equations similar as in Section 5.3 and obtain all fixed points with NPHC. A unique fixed point exists for all settings \u2013 and this fixed point is stable, which justifies the application of BP on error-correcting codes. We further estimate the accuracy of\n37 Similar to Section 2.2, we use \u2202(\u00b7) to specify the neighbors of nodes and variables.\n\u2013 102 \u2013 November 8, 2019\n5.10 Conclusion\nthe approximation; this relates to the following question. If we communicate over a BSC, how vulnerable is BP decoding to an increased error probability? To answer this question we obtain the fixed points for \u2208 [0, 0.5] and compare the exact solution obtained by the junction tree algorithm PXi|Y(xi|y) (red), to the approximate solution P\u0303Xi|Y(xi|y) obtained by NPHC (blue) in Figure 5.12.38\nAn error can be corrected by BP decoding if P\u0303Yi|X(yi|x) > 0.5. With exact decoding a single bit-flip can be corrected for < 0.21. The fixed points obtained by NPHC, however, reveal that BP does not utilize the full potential of the code (Figure 5.12). If Y1 was corrupted the error can be corrected for < 0.13 (Figure 5.12 (a)). BP fails to correct the error if Y6 was flipped for all values of (Figure 5.12 (b)). The reason therefore is a systematic error: the check-bit only has a single connection to the parity-check function fc. According to (5.36) q n+1 6,c (yi) = \u03b17,7 \u00b7r7,7(yi); therefore Y6 does not incorporate any information from the remaining graph. To conclude, the higher the connectivity of a node, the more information of other bits is taken into account and the better the error-correction capability of BP and NPHC."
        },
        {
            "heading": "5.10 Conclusion",
            "text": "This chapter was an attempt to get a deeper understanding of BP\u2019s behavior, with potential implications for deriving convergence guarantees and finding stronger conditions for uniqueness of BP fixed points.\nThe specific focus was to apply the tools from Chapter 4, namely the NPHC method and the concept of linearization to answer some of the open questions associated with BP (cf. Section 3.6). In particular we utilized the NPHC method to characterize the solution space and to obtain all BP fixed point solutions before performing a local stability analysis to understand the convergence properties.\nAlthough this approach provided many interesting insights and answered some long-standing questions, some additional questions emerged in the process.\nThe focus on relatively small, well-structured models provides an obvious starting point and was essential in developing the results presented. Although many insights are expected to carry over to more general models, the generalization to larger models with fewer constraints on their potentials is primarily left open. Moreover, the general models considered here are seemingly too small as to exhibit an actual disordered behavior (characterized by the existence of multiple fixed points). The systems of polynomial equations considered so far, however, are already larger than most solvable problems; the generalization to even larger systems therefore remains problematic.\nOne key feature of the presented framework was the estimation of a favorable upper bound on the number of solutions. In particular, the BKK bound utilized the sparsity of the polynomial system induced by the graph structure and was tight in all our experiments. Computing the BKK bound and creating the start system, however, took up the major part of the overall runtime in our experiments. One will agree that the structure of the graph \u2013 together with the update equations \u2013 literally specifies the structure of the polynomial system. Yet, we only relied on generic, albeit well-established, methods to compute the BKK bound and to create the start-system. This suggests that one should take the graph structure into account, potentially yielding a much more efficient algorithm for computing the BKK bound.\nRegarding the approximation quality of BP, we exemplified an accuracy-gap between fixed points obtained by BP and the best possible fixed points (obtained by NPHC). In practice this justifies the exploration of multiple fixed points: one can then either consider a combination\n38 Note that for = 0.5 the transmission is random.\nNovember 8, 2019 \u2013 103 \u2013\n5 Solution Space of Belief Propagation: Number of Fixed Points and Their Stability\nof weighted marginals, providing strikingly accurate results in the models considered; or one can select the fixed point that maximizes ZB, often leading to the best approximation of the marginals. We will further investigate this topic in Chapter 7 where we discuss the advantages and pitfalls of both approaches. Moreover, we analyzed the fixed point maximizing the partition function in detail: we revealed how it continuously deforms under varying parameters and how the stability depends on the graph structure. The observation of a continuous deformation will serve as the main inspiration for Chapter 6 where we build upon this observation to propose an enhanced version of BP.\nRegarding the convergence properties of BP, we answered several questions on the basis of our empirical observations and our theoretical analysis. Graphs with vanishing local potentials are indeed a worst-case scenario; strong local potentials reduce the magnitude of all eigenvalues, thus helping to achieve convergence, and additionally increase the accuracy of the most accurate fixed point.\nThe graph structure affects the convergence properties as the spectral radius increases with the model size, which consequently degrades the performance of BP. Moreover, bipartite graphs exhibit a symmetric spectrum so that damping cannot help to achieve convergence; note that the ineffectiveness of damping for bipartite models was recently also observed for models with more general potentials [PI17]. Based on our observations it seems reasonable to conjecture that damping can only be used to stabilize a fixed point if it is unique. Whether this generalizes to spin glasses is by no means obvious. It would, for the purpose of understanding the convergence properties of spin glass models, be of interest to specify a distribution of the eigenvalues based on the distribution of the potentials.39\n39 One could hope to obtain similar results as in the study of spectral densities on random matrices, where it is for example well-established that the eigenvalues of sparse symmetric matrices are distributed according to the Wigner semicircle law [Wig58,Erd11].\n\u2013 104 \u2013 November 8, 2019\nUnderstanding the Behavior of Belief Propagation\n6 Enhancing Belief Propagation: Self-Guided Belief Propagation\n\u201dThe most significant dimension of freedom is the freedom from one\u2019s own ego - in other words, from the feeling that I am the center of everything.\u201d\n\u2013 Voytek Kurtyka\nThis chapter builds upon the insights from the preceding chapter and presents one possible way to make the performance of BP more robust. All models analyzed so far show that the minimum of the Bethe free energy is continuously deformed as the coupling strength increases. As a consequence of this observation, we progressively incorporate the pairwise potentials and keep track of the evolving fixed point. Although the idea of this algorithm was written down by the present author, such an approach may seem rather obvious; in fact Alexander Ihler revealed that he also had a similar idea in his mind already for quite some time [Ihl].\nWe begin this chapter with the introduction of our proposed algorithm self-guided belief propagation (SBP) in Section 6.2. In Section 6.3 we apply SBP to a wide range of models and discuss our empirical observations before we provide a formal analysis for the special case of attractive models with unidirectional local potentials in Section 6.4.\nAll theoretical results have been conducted by the present author. The implementation of the algorithm and the empirical evaluation in Section 6.3 were performed by the author\u2019s student Florian Kulmer. Both the theoretical and empirical results have been prepared for publication in [KKP18]."
        },
        {
            "heading": "6.1 Motivation",
            "text": "The analysis of BP\u2019s solution space in Chapter 5 revealed that accurate fixed points may exist even though BP fails to obtain them. This is in accordance with the observation in [WJ14a] that the Bethe approximation is often accurate despite the failure of BP to converge. The main aim of this chapter lies in enhancing BP so that it converges to accurate fixed points while preserving its simple nature (of just considering local interactions).\nBesides empirical analyses and some restricted theoretical studies, it remains an open problem to obtain a rigorous understanding of the limitations of BP for general graphs. Implementation details, as for example the initialization, play an important role in the case of multiple fixed points. In this case, it may depend on the initialization whether BP provides accurate marginals or not. The dependence of BP on such implementation details obviously poses a serious issue for providing performance guarantees.\nOne way to get convergence guarantees is to consider the equivalent optimization problem and to minimize the Bethe free energy FB. This, however, comes at the cost of an increased runtime complexity; polynomial-time algorithms only exist for restricted classes of problems and even approximating the global minimum might be problematic for graphical models with arbitrary potentials [CCG+11,Shi12,WJ14a]. Hence, the pursuit for methods that approximate the marginals with both runtime- and convergence-guarantees is still ongoing. These limitations\nNovember 8, 2019 \u2013 105 \u2013\n6 Enhancing Belief Propagation: Self-Guided Belief Propagation\nmotivate the search for modifications of BP that overcome these issues in order to increase the accuracy and enhance the convergence properties. In this chapter, we introduce self-guided belief propagation (SBP) that aims to fill this gap.\nThe evolution of the fixed points revealed a close relationship between the coupling strength and the performance of BP. Especially strong couplings reduce the accuracy and deteriorate the convergence properties. This and the observation that tuning the coupling strength continuously deforms the fixed point minimizing the Bethe free energy inspired us to construct a homotopy.\nMore precisely, we first consider only local potentials (where BP is exact and has a unique fixed point) and subsequently modify the model by increasing the pairwise potentials to the desired values. SBP thus solves a deterministic sequence of models that iteratively refines the Bethe approximation towards an accurate solution that is uniquely defined by the initial model.\nWe evaluate SBP for grid-graphs, complete graphs, and random graphs with Ising potentials and, compared to BP, we observe superior performance in terms of accuracy; in fact SBP achieves more accurate results than Gibbs sampling in a fraction of runtime. We theoretically demonstrate optimality of the selected fixed point for attractive models with unidirectional local potentials. Additionally SBP enhances the convergence properties and excels for general models where SBP provides accurate results despite the non-convergence of BP. We further expect that the ease of use lowers the hurdle for practical applications."
        },
        {
            "heading": "6.2 Self-Guided Belief Propagation (SBP)",
            "text": "In this section we present an intuitive justification of the proposed method and subsequently introduce SBP in detail. We further present practical considerations and pseudocode of SBP. A formal treatment of SBP is deferred to Section 6.4.\nThe current understanding of BP is that strong (pairwise) potentials negatively influence BP and that in incorporating the potentials slowly [BKMZ07] may reduce the overall number of iterations. Inspired by our observations in Chapter 5 that strong local potentials increase accuracy and lead to better convergence properties, we aim to reduce the influence of the pairwise potentials that negatively influence BP.\nSBP starts from a simple model with independent random variables and slowly incorporates the potential\u2019s strength, i.e., it solves the simple problem first and \u2013 by repetitive application of BP, keeps track of the fixed point as the interaction strength is increased by a scaling term. Again we resort to the homotopy continuation method for this purpose.\nMore formally, SBP considers an increasing length-K sequence {\u03b6k} where k = 1, . . . ,K such that \u03b6k < \u03b6k+1 and \u03b6k \u2208 [0, 1] with \u03b61 = 0 and \u03b6K = 1. This further indexes a sequence of probabilistic graphical models {Uk} that converges to the model of interest UK = U. Every probabilistic graphical model has a set of potentials \u03a8k = {\u03a6k(xi, xj),\u03a6k(xi)} associated, where \u03a6k(xi) = \u03a6Xi(xi) and the pairwise potentials at index k are exponentially scaled by\n\u03a6k(xi, xj) = exp(Jij\u03b6kxixj)\n= \u03a6Xi,Xj (xi, xj) \u03b6k . (6.1)\nWe further denote the fixed points of BP for Uk by \u00b5\u25e6k. The initialization determines the performance of BP if multiple fixed points exist; SBP always provides a favorable initialization for the model Uk by the preceding fixed point \u00b5\u25e6k\u22121 and performs the composite function\n\u00b5\u25e6K = BP\u25e6K ( BP\u25e6K\u22121 ( \u00b7 \u00b7 \u00b7 BP\u25e61 ( \u00b511 ))) . (6.2)\nThis may lead to problems if the fixed point becomes unstable for some value k < K in which case we cannot rely on BP to keep track of the fixed point anymore. Instead, SBP provides the last stable fixed point in that case, i.e., \u00b5\u25e6k\u22121 as the final estimate.\n\u2013 106 \u2013 November 8, 2019\n6.2 Self-Guided Belief Propagation (SBP)\nIn other words, SBP relaxes the problem of minimizing FB by making all variables independent (and the Bethe approximation exact). Then, the problem is deformed into the original one by increasing \u03b6 from zero to one. Thereby, a stationary point FB\u25e6 emerges as a well-behaved path (cf. Proposition 1 in Section 6.4) and SBP keeps track of it with BP constantly correcting the stationary point.\nWe illustrate how SBP approximates the marginals for a problem where BP fails to converge in Figure 6.1. Initially, SBP obtains the pseudomarginals for \u03b6 = 0 and then estimates the marginals of the desired problem by successively increasing \u03b6 and running BP to keep track of the emerging solution path. Note that the approximated marginals are already close to the exact ones in this example; experiments show that this is often the case (cf. Section 6.3)."
        },
        {
            "heading": "6.2.1 Practical Considerations",
            "text": "The procedure of SBP is essentially a path-tracking problem. As already discussed for the NPHC method in Section 4.3.3, such a problem consists of three key steps: prediction, correction, and step-size adaption. While the implementation details of these steps may influence the performance, we have found in our experiments that the efficiency of SBP remains largely unaffected. We will now describe the specific details used for all three steps throughout our experiments.\nIn practice the runtime of SBP is influenced by the difference between two successive fixed points \u00b5\u25e6k and \u00b5 \u25e6 k\u22121 \u2013 the difference is primarily determined by the number of steps K. Ideally, to reduce the difference, K should be as large as possible. This, however, increases the runtime as well (cf. Theorem 8); in practice we would choose K as small as possible but as large as necessary. Moreover, one can adaptively increase the step size if two successive fixed points are close, i.e., if \u00b5\u25e6k ' \u00b5\u25e6k\u22121 (cf. [SW05, pp.23], [AG03]). Our experiments show that it is sufficient to use rather coarse steps (we used K \u2264 10 for all reported experiments).\nAdditionally, instead of initializing BPk with its preceding fixed point messages, i.e., \u00b51k = \u00b5\u25e6k\u22121 one can (e.g., by spline extrapolation) estimate \u00b5 1 k = f(\u00b5 \u25e6 k\u22121,\u00b5 \u25e6 k\u22122, . . . ,\u00b5 \u25e6 k\u2212l) so that \u00b51k \u223c= \u00b5\u25e6k to reduce the overall number of iterations. We empirically observed that the benefit diminishes for l > 3."
        },
        {
            "heading": "6.2.2 Pseudocode",
            "text": "Pseudocode of SBP is presented in Algorithm 1. The maximum number of iterations for BP is given by NBP = 10 3. We randomly initialize \u00b511 and either use fixed step size or adaptive step\nNovember 8, 2019 \u2013 107 \u2013\n6 Enhancing Belief Propagation: Self-Guided Belief Propagation\nsize (adaptive stepsize = 1). The sequences of messages is contained in {\u00b5\u25e6k} = {\u00b5\u25e61, . . . ,\u00b5\u25e6k}. Cubic spline extrapolation is applied in ExtrapolateMsg to estimate the initial messages of the subsequent model. We further present the pseudocode for the adaptive step size controller in Algorithm 2.\nAlgorithm 1: Self-Guided Belief Propagation (SBP)\ninput : Graph G = (X,E), Potentials \u03a8 output: Fixed point messages \u00b5\u25e6\n1 initialization \u00b511 \u2190 \u00b51 2 k \u2190 1 3 stepinit \u2190 110 4 \u03b61 \u2190 0 5 while \u03b6 \u2264 1 do 6 \u03a8(\u03b6k) \u2190 ScalePotentials(\u03a8, \u03b6k) 7 (\u00b5, n) \u2190 BP(\u00b51k,\u03a8(\u03b6k), NBP) 8 if n < NBP then 9 \u00b5\u25e6k \u2190 \u00b5\n10 else 11 break\n12 if adaptive stepsize then 13 \u03b6k+1 \u2190 \u03b6k+ AdaptiveStepSize({\u00b5\u25e6k}, stepinit, k) 14 else 15 \u03b6k+1 \u2190 \u03b6k + stepinit 16 \u00b51k+1 \u2190 ExtrapolateMsg({\u00b5\u25e6k},{\u03b6k}) 17 k \u2190 k + 1 18 \u00b5\u25e6 \u2190 \u00b5\u25e6k\u22121\nAlgorithm 2: Adaptive Step Size Controller\ninput : Sequence of messages {\u00b5\u25e6k}, stepinit, k output: step\n1 step\u2190 stepinit 2 threshold \u2190 1 \u00b7 10\u22123 3 l\u2190 1 4 while ( MSE(\u00b5\u25e6k) \u2212 MSE(\u00b5\u25e6k\u2212l) ) < threshold do 5 l\u2190 l + 1 6 step\u2190 step+ stepinit \u00b7 l"
        },
        {
            "heading": "6.3 Experiments",
            "text": "We apply SBP to attractive (Section 6.3.2) and general (Section 6.3.3) models on n \u00d7 n grid graphs of different size, and to complete and random graphs (average degree of d\u0302G = 3) with N = 10 random variables. Experiments were performed for these graphs in order to render the computation of the exact marginals feasible and to make the results comparable to previous work [WTJS14,SJ08,MJGF09,SRF16].\n\u2013 108 \u2013 November 8, 2019\n6.3 Experiments"
        },
        {
            "heading": "6.3.1 Experimental Settings",
            "text": "SBP is evaluated and compared to BP, BPD (BP with damping), and Gibbs sampling. The exact marginals are obtained by the junction tree algorithm and the accuracy of the marginals is evaluated by the mean squared error EP (m) between the approximate marginals at the m th fixed point P\u0303mXi(xi) and the exact marginals PXi(xi). Additionally, we approximate the global minimum FB\u2217 by [WJ14a] and evaluate the mean squared error (MSEB) between the approximate marginals and the marginals obtained at the global minimum of the Bethe free energy P\u0303 \u2217Xi(xi). We further compare the runtime of all methods by counting the overall number of BP iterations and the number of iterations for Gibbs sampling.40\nWe consider L = 100 models with random potentials for every experiment. The initial messages are randomly initialized 100 times for each of these L models, before applying BP with and without damping. We consider BP (and BPD) as converged if at least a single message initialization (out of 100) exists for which BP converges. We report the convergence ratio, i.e., the number of experiments (or probabilistic graphical models) for which BP converged at least once divided by the overall number of models L.\nThe reported mean-squared error EP (m) and the number of iterations are averaged over all convergent runs of BP and BPD (i.e., BP\n\u25e6 and BP\u25e6D) while all runs that did not converge are discarded. SBP, on the other hand, allows obtaining an approximation of the terminal fixed point in case that this fixed point is unstable, which prevents BP and BPD from converging. Therefore, we average the error and the number of iterations over all L models for SBP (SBPall), Gibbs sampling (Gibbsall), and for minimization of the Bethe approximation (FB\u2217all).\nFor BP and SBP we set the maximum number of iterations to NBP = 10 3 and use random message scheduling. For BPD we choose a large damping factor = 0.9 to account for strong couplings. Such a large damping factor helps to prioritize convergence over runtime \u2013 this admits comparison of marginal accuracy for a wide range of models. The maximum number of iterations, however, has to be increased to NBP = 10\n4 in order to account for the slower convergence. Carefully selecting a damping factor that depends on a given model may reduce the number of iterations until convergence but can not increase the accuracy; moreover, if chosen too small BPD may fail to converge. The accuracy of SBP is only marginally affected by its parameters and we use the following parameters for all experiments: K \u2264 10 , adaptive step size, and cubic spline extrapolation. Gibbs sampling is run for 105 iterations.\n40 Computing the acceptance-probability requires similar runtime as one BP message update.\nNovember 8, 2019 \u2013 109 \u2013\n6 Enhancing Belief Propagation: Self-Guided Belief Propagation"
        },
        {
            "heading": "6.3.2 Attractive Models",
            "text": "We consider grid graphs with N = 100 random variables (10\u00d7 10), random graphs with N = 10 random variables, and complete graphs with N = 10 random variables. For each graph, we generate L = 100 models for every value of \u03b2 \u2208 {0, 0.5, . . . , 5} and sample the potentials according to \u03b8i \u223c U(\u22120.5, 0.5) and Jij \u223c U(0, \u03b2); i.e., we consider 1100 different parametrizations. Note that BP is randomly initialized 100 times for every considered parametrization. We compute EP (m) for every value of \u03b2 and visualize the mean and the standard deviation of the MSE\n41 as well as the number of iterations in Figure 6.2 (a).\nNote that BP (magenta) converges rapidly for all graphs considered; hence, there is no additional benefit for BPD (green) that only increases the number of iterations. SBP (blue) only slightly increases the number of iterations as compared to BP and converges in fewer iterations than BPD. Note that SBP is guaranteed to capture the global optimum if all local potentials are unidirectional (cf. Theorem 9-10). But even if we do allow for random local potentials, we em-\n41 Note that the mean-squared error is not Gaussian distributed but we report the standard deviation for simplicity.\n\u2013 110 \u2013 November 8, 2019\n6.3 Experiments\npirically observe that SBP consistently outperforms BP with respect to accuracy. This becomes especially evident for models with strong couplings: these models exhibit multiple stable fixed points [KMCP18] such that, depending on the initialization, BP often converges to inaccurate fixed points."
        },
        {
            "heading": "6.3.3 General Models",
            "text": "General models admit frustrated cycles and traditionally pose problems for BP and other methods that aim to minimize the Bethe approximation.\nFirst, in order to evaluate the performance of SBP we consider \u03b8i = \u03b8 \u2208 {0, 0.1, 0.4} and draw the couplings with equal probability from Jij \u2208 {\u22121, 1}; the results are summarized in Table 6.1. Although BP and BPD fail to converge for most models we observe that SBP stops after only a few iterations and significantly outperforms BP in terms of accuracy. In fact, SBP achieves accuracy competitive with Gibbs sampling but requires three orders of magnitude fewer iterations.\nSecond, we further apply SBP to general graphs and evaluate whether SBP provides a good approximation of the pseudomarginals that correspond to the global minimum of the Bethe free energy according to P\u0303 \u2217B = arg minLFB(P\u0303B) (cf. (3.29)). Therefore we consider grid graphs (of size 5 \u00d7 5), which still allows us to approximate FB\u2217 \u2013 and the related pseudomarginals \u2013 reasonable well by [WJ14a]. The results are summarized in Table 6.1 and show that SBP approximates P\u0303 \u2217B within the accuracy of our reference method (MSEB). We further report the number of times where SBP obtains the terminal fixed point, i.e., for UK , in Table 6.1 (Global Min). It becomes obvious that SBP approximates the terminal fixed point reasonably well, despite frequently stopping for \u03b6k < 1. Moreover, closer inspection of the accuracy reveals that SBP does not only approximate the \u201ccorrect\u201d pseudomarginals well (MSEB), but concurrently provides an accurate approximation of the exact marginals (MSE).\nThird, we investigate how the approximation quality depends on the scaling parameter \u03b6k. Therefore, we depict the evolution of the MSE (to the exact solution) and MSEB (to the approximate solution) in Figure 6.3. We observe that MSEB (blue) decreases monotonically with every iteration, which empirically verifies that SBP proceeds along a well-behaved solution path\nNovember 8, 2019 \u2013 111 \u2013\n6 Enhancing Belief Propagation: Self-Guided Belief Propagation\n(cf. Proposition 1). Note that MSEB decreases rapidly in the first iterations and SBP spends a major part of the overall runtime for slight improvements. The MSE to the exact solution, on the other hand, decreases first until it increases again as SBP incorporates stronger couplings. Stronger couplings tend to degrade the quality of the Bethe approximation in loopy graphs and lead to marginals that are increasingly biased towards one state [Wei00,KMCP18]. This explains why the MSE to the exact solution increases as SBP converges towards the terminal fixed point. One could exploit this behavior and restrict the runtime by stopping SBP after consumption of a fixed iteration budget; this may even increase the accuracy with respect to the exact solution.\nFinally, we aim to investigate the influence of the coupling strength: therefore we consider \u03b8i \u223c U(\u22120.5, 0.5) and Jij \u223c U(\u2212\u03b2, \u03b2) . For every \u03b2 \u2208 [0, 5] we execute L = 100 experiments and present the averaged results in Figure 6.2 (b). Note that we restrict the results to \u03b2 \u2264 2 on the grid graph because BP did only converge sporadically for models with stronger couplings. SBP requires only slightly more iterations than BP and fewer than BPD, even though we compare only to models where BP (or BPD) converged. The benefits of SBP become increasingly evident as the coupling strength increases. Again SBP (blue) significantly outperforms BP\u25e6 (magenta) and BP\u25e6D (green) on all graphs with respect to accuracy."
        },
        {
            "heading": "6.4 Theoretical Analysis of Self-Guided Belief Propagation",
            "text": "Here we present some more formal arguments and discuss the properties of SBP to understand under which conditions the algorithm (presented in Section 6.2) can be expected to perform well. We only present the most important Theorems and their implications below and defer the longer proofs to Appendix A.1."
        },
        {
            "heading": "6.4.1 Definitions",
            "text": "First, we fix our notation: we denote the pseudomarginals, corresponding to local minima of the Bethe free energy, of Uk by P\u0303mB (\u03b6k), and, with slight abuse of notation, we refer to the corresponding stationary point of the Bethe free energy by FBm(\u03b6k) = FB(P\u0303mB (\u03b6k)). Note that the superscript m accounts for the fact that we are only looking at the local minima of FB (cf. Section 3.3.2).\nIt is beneficial to study the behavior of SBP as K tends towards infinity. Therefore we consider the unit interval \u03b6 \u2208 [0, 1] to be the compact support of the functions FB(\u03b6) and P\u0303B(\u03b6). SBP is\n\u2013 112 \u2013 November 8, 2019\n6.4 Theoretical Analysis of Self-Guided Belief Propagation\ninspired by the idea to proceed along a so-called solution path as \u03b6 increases from zero to one in order to obtain the marginal distributions for the model of interest. Therefore, we shall consider a continuous homotopy function H(\u00b5, \u03b6) : R|\u00b5|+1 \u2192 R|\u00b5| that is defined by\nH(\u00b5, \u03b6) = \u00b5 \u2212 BP(\u00b5) where \u03a8 = \u03a8(\u03b6). (6.3)\nThen, a solution path\nc(\u03b6) : H(\u00b5, \u03b6) = 0 (6.4)\nexists that (i) has a start point c(\u03b6 = 0) = \u00b5 : H(\u00b5, \u03b6 = 0) = 0, (ii) an endpoint c(\u03b6 = 1) = \u00b5 : H(\u00b5, \u03b6 = 1) = 0, and (iii) is continuous over \u03b6 \u2208 [0, 1], i.e., it connects the start- with the endpoint. SBP then proceeds along some solution path from a given start- to its endpoint. Note that the solution path c(\u03b6) is defined along the solutions to the fixed point equations and thus it implicitly defines the pseudomarginals P\u0303mB (\u03b6) by (3.2) and (3.3). In particular, we refer to the start- and endpoint by P\u0303mB (\u03b6 = 0) and P\u0303 m B (\u03b6 = 1) respectively.\nThe following example in Figure 6.4 illustrates the fixed point evolution for a grid graph with attractive couplings. This example exhibits a unique solution path according to our definition; note, however, that a second curve exists, which lacks a start point and is therefore of no relevance for any method that proceeds along a solution path defined by the homotopy in (6.3)."
        },
        {
            "heading": "6.4.2 Properties of Self-Guided Belief Propagation",
            "text": "The following proposition summarizes the main properties of the solution path that is specified and followed by SBP.\nProposition 1 (Properties for attractive and general models).\n(1) BP has a unique fixed point \u00b5\u25e61 for \u03b61 = 0, so that SBP has a unique start point P\u0303 m B (\u03b6 = 0).\n(cf. Theorem 6)\n(2) A smooth (i.e., continuous) solution path originates from the start point P\u0303mB (\u03b6 = 0). (cf. Theorem 7)\n(3) SBP efficiently proceeds along this (unique) solution path. (cf. Theorem 8)\nNovember 8, 2019 \u2013 113 \u2013\n6 Enhancing Belief Propagation: Self-Guided Belief Propagation\nTheorem 6 (Proposition 1.1). A unique solution exists for \u03b6 = 0, i.e., a single start point P\u0303mB (\u03b6 = 0) exists, and BP is guaranteed to converge. Moreover, this start point is exact, i.e., P\u0303Xi(\u03b6 = 0) = PXi(\u03b6 = 0) for all Xi \u2208 X.\nProof. First, let us obtain the singleton marginals by summing over the pairwise marginals such that P\u0303Xi(xi) = \u2211 xj\u2208X P\u0303Xi,Xj (xi, xj). For \u03b61 = 0, that is for \u03a61(xi, xj) = 1 it follows that marginalizing over the pairwise marginals (cf. (3.3)) equates to\nP\u0303Xi(xi) = \u03a6(xi) \u220f\nXk\u2208{\u2202(i)\\Xj} \u00b5\u25e6ki(xi) \u00b7 \u2211 xj\u2208X \u03a6(xj) \u220f Xl\u2208{\u2202(j)\\Xi} \u00b5\u25e6lj(xj). (6.5)\nNote that according to (3.1) \u00b5\u25e6ji(xi) is equivalent to the second part of (6.5) so that\nP\u0303Xi(xi) = \u03a6(xi) \u220f\nXk\u2208\u2202(i)\n\u00b5\u25e6ki(xi), (6.6)\nwhich equals (3.2). It follows that P\u0303Xi(+1) = e \u03b8i/(e\u03b8i + e\u2212\u03b8i) = PXi(+1).\nNote that Theorem 6 concurs with the sandwich-bound [WJ13, Th.4] that reduces to PXi(+1) = \u03b8i/(e\n\u03b8i + e\u2212\u03b8i) = P\u0303Xi(+1) for Jij = 0. Theorem 6 thus reduces the problem of initializing SBP to computing \u00b5\u25e61, which can be done in linear time.\nTheorem 7 (Proposition 1.2). Let P\u0303mB (\u03b6) be the pseudomarginals that are defined along the unique solution path that originates from P\u0303mB (\u03b6 = 0). Then, P\u0303 m B (\u03b6) and the associated stationary points FBm(\u03b6) are continuous on their compact support \u03b6 \u2208 [0, 1].\nProof. First, we show that the Bethe free energy FB(\u03b6) itself is an analytic function. Consider (3.24) with the pairwise potentials defined by (6.1). Then, the derivative with respect to \u03b6 is given by\n\u2202FB(\u03b6) \u2202\u03b6 =\u2212 \u2202 \u2202\u03b6 \u2211 (i,j)\u2208E \u2211 xi,xj P\u0303Xi,Xj (xi, xj) ln \u03a6(xi, xj)\n=\u2212 \u2202 \u2202\u03b6 \u2211 (i,j)\u2208E \u2211 xi,xj P\u0303Xi,Xj (xi, xj) \u00b7 \u03b6 \u00b7 Jij \u00b7 xixj\n= \u2212 \u2211\n(i,j)\u2208E\nJij \u00b7 \u03c7ij . (6.7)\nAs an immediate consequence, we observe that FB(\u03b6) is continuously differentiable42 as (6.7) is a finite sum over finite terms.43\nWe specifically consider the minimum FBm(\u03b6) that emerges from the global minimum FBm(\u03b6 = 0) = FB\u2217(\u03b6 = 0); this start point is unique by Theorem 6. It follows by (6.7) that FBm(\u03b6) varies in a continuous fashion along the unique solution path for \u03b6 \u2208 [0, 1]. Further note that stationary points are in a one-to-one correspondence with fixed points of BP which completes the proof.\nFurther note that the set of stationary points is finite (cf. Section 3.3.1)44 and that pitchfork bifurcations may only occur if \u03b8i = 0 [PAM11], in which case SBP obtains the exact solution (cf. Theorem 9).\nTheorem 7 substantiates the claim that a smooth solution path emerges from the simple problem.\n42 Strictly speaking FB(\u03b6) is an analytic function. 43 This is in accordance with the fact that true phase transitions (singularities in the derivative of the free energy)\ncan occur only in the thermodynamic limit, where (6.7) is an infinite sum that equates to infinity. 44 This is also required for the one-step replica symmetry breaking assumption [MM09, Section19].\n\u2013 114 \u2013 November 8, 2019\n6.4 Theoretical Analysis of Self-Guided Belief Propagation\nTheorem 8 (Proposition 1.3). There exists some scaling factor \u03b6k \u2264 1 for which SBP converges to P\u0303mB (\u03b6k) in O(KNBP ).\nProof of Theorem 8. SBP increases \u03b6k as long as BP converges in fewer than NBP iterations, and stops otherwise. Consequently, BP corrects the accuracy of the fixed point for each value \u03b6k within a bounded number of iterations. The runtime of SBP is further determined by the choice of K, i.e., the step-size (cf. Section 6.2.1). Assume that SBP converges for \u03b6k, then it does so in O(K \u00b7NBP ).\nSBP is consequently capable of efficiently tracking the fixed point that emerges as \u03b6 increases and requires KNBP iterations at most. SBP may, however, only converge to a surrogate model for \u03b6k < 1 and is not guaranteed to obtain the pseudomarginals of the desired problem. One can characterize this error by computing a bound on |FBm(\u03b6k)\u2212FBm(\u03b6K)| given the difference between \u03a8k and \u03a8K (cf. [IFW05, Theorem 16]).\nCorollary 8.1. SBP obtains the pseudomarginals of the desired problem if and only if the endpoint P\u0303mB (\u03b6 = 1) is stable, i.e., if P\u0303 m B (\u03b6 = 1) \u2208 S. This is an immediate consequence of the fact that the convergence properties can only degrade along a given solution path (cf. Section 5.8).\nTheorem 6-8 are of fundamental importance but do not relate to the accuracy of the obtained stationary point. Assessing the quality of the Bethe approximation and the accuracy of BP for general models is still an open research question that is beyond the scope of this thesis. However, we further present Proposition 2 to discuss the accuracy of the obtained solution for attractive models.\nProposition 2 (Properties for attractive models with unidirectional fields). The solution path c(\u03b6) leads towards an accurate solution with P\u0303mB (\u03b6k) = P\u0303 \u2217 B(\u03b6k) and FB(P\u0303mB (\u03b6k)) = FB(P\u0303 \u2217B(\u03b6K)). (cf. Theorem 9 and Theorem 10)\nWe start by generalizing Griffiths\u2019 inequality [Gri67] to the fixed points of BP (Lemma 4) and subsequently provide Theorem 9-10 that discuss the accuracy of the fixed point obtained by SBP.\nLemma 4. Consider two attractive probabilistic graphical models U0 and U1 with equal G and with the potentials specified by \u03b8i > 0 and by J 0 ij and J 1 ij, where J 0 ij < J 1 ij for all (i, j) \u2208 E. Let us consider a fixed point of BP with positive means m0i \u2208 (0, 1]. Then, m0i < m1i and \u03c70ij < \u03c71ij.\nThe proof of Lemma 4 contains some tedious, although not too complicated, algebraic manipulations and is thus deferred to Appendix A.1.1\nTheorem 9 (Prop. 2). Consider an attractive model with \u03b8i > 0. 45 Then, mi(\u03b6) increases monotonically along the solution path c(\u03b6); in particular SBP minimizes the Bethe approximation error and is optimal with respect to marginal accuracy, i.e.,\nP\u0303mB (\u03b6) = arg min P\u0303kB\u2208M EP (k)\n= arg min P\u0303kB\u2208M EZ(k). (6.8)\nWe will only sketch the proof of Theorem 9 here and refer to Appendix A.1.2 for a more elaborate treatment. Essentially, we have to bring the preceding Theorems together to show that SBP obtains the most accurate marginals for attractive models with unidirectional fields.\nTheorem 6 explains how the initial model can be solved exactly and that it has positive mean and correlation for \u03b8i > 0. Consequently, Lemma 4 and Theorem 7 apply and guarantee a\n45 Note that equal results can be obtained for \u03b8i < 0 because of symmetry properties.\nNovember 8, 2019 \u2013 115 \u2013\n6 Enhancing Belief Propagation: Self-Guided Belief Propagation\ncontinuous solution path with monotonically increasing means mi(\u03b6). Optimality of the obtained fixed point then is an immediate consequence of the definition of FB (cf. (3.24)), the RSB assumption (which holds for these models, cf. Section 5.6.3), and the existence of at most two local minima, i.e., M \u2264 2, for the considered models (cf. Lemma 5 and [WTJS14]).\nNote that an attractive model with unidirectional fields is a special case of a (larger) attractive model with vanishing fields (cf. [Fis67] and [SKZ17, Section 1.2]).\nTheorem 10 (Proposition 2). Consider an attractive model with \u03b8i = 0. Then, SBP obtains the exact solution, i.e., P\u0303mB (\u03b6 = 1) = PB(\u03b6 = 1).\nProof. Only considering attractive models makes it straightforward to calculate the exact solution if all \u03b8i = 0. The exact solution always has zero mean for all random variables, i.e., mi(\u03b6) = 0 for all values of \u03b6. Moreover, the exact solution coincides with a stationary point FBm(\u03b6k) [MK07]. Depending on the coupling strength, this stationary point is a stable minimum (for sufficiently small values of Jij < JC) and a local maximum (for Jij > JC) (cf. Section 5.7.1 or to [MM09, pp.385]).\nSBP consistently obtains this fixed point irrespective of its stability. Therefore, note that by Theorem 6 all mi(\u03b6 = 0) = 0 and all messages are equal, i.e., \u00b5 \u25e6 1(\u03b6 = 0) = 1/2 for all (i, j) \u2208 E. SBP remains exactly on this fixed point and obtains the exact marginals as these fixed point messages can be represented exactly, without any quantization errors in binary arithmetic.\nTo conclude, for attractive models with \u03b8i \u2264 0 or \u03b8i \u2265 0, SBP either obtains the fixed point that corresponds to the global minimum FB\u2217 (Theorem 9), or it obtains the fixed point that corresponds to the exact solution, i.e., to minFG (Theorem 10).\nFor general models mi, need not increase monotonically along the solution path and it is not obvious whether SBP obtains the most accurate fixed point. However, the experimental results in Section 6.3.3 at least corroborate that SBP converges to accurate fixed points in many cases."
        },
        {
            "heading": "6.5 Conclusion",
            "text": "This chapter aimed at deriving a simple and robust method to efficiently perform approximate inference for models where BP fails to converge. The observations from our solution space analysis in Chapter 5 suggested that an accurate fixed point lies at the end of a solution path that emerges from the origin (where Jij = 0). We managed to enforce convergence towards this fixed point by changing the updated function accordingly. To do so, we resorted to the homotopy method once again and utilized its proven capability of keeping track of solution paths, where we relied on BP in the correction step.\nOur theoretical analysis of SBP for attractive models with unidirectional local fields revealed the existence of a unique, well-behaved solution path that leads to the best possible fixed point of BP and that can be tracked successfully by SBP. We further applied SBP to general models, where, despite the lack of theoretical guarantees, it exhibited a promising performance. Overall SBP consistently obtained marginals of better quality than BP with and without damping and approximated the marginals well on models for which BP did fail to converge at all.\nOne surprising effect of SBP was that it sometimes provided marginals that were even more accurate than the ones at the global minimum of FB. This occurred frequently, specifically whenever SBP terminated early because of BP failing to converge in the correction step before the pairwise potentials were set to their desired value.\nSo far we can only provide some hand-waving arguments for this behavior: Therefore, note that common wisdom of BP tells us that BP usually fails to converge because of strong pairwise potentials; additionally, we have seen how the marginals become overconfident and tend to be\n\u2013 116 \u2013 November 8, 2019\n6.5 Conclusion\nstrongly biased towards one state as the coupling-strength increases. The latter property explains why the approximation quality of FB degrades as its minimum moves further away from the minimum of FG . This implies that following the solution path to its very end only makes the accuracy of the estimated marginals worse. It seems that the former property implicitly prevents this from happening and that the stability breaks down somewhat close to the exact solution.\nFirst empirical evaluations suggest that this is indeed quite often the case. This raises an intriguing theoretical question: Is there really some fundamental connection between the exact marginals and the approximated marginals that are obtained at the onset of instability?\u2013 and if so, what is the nature of this connection?\nIn a more concrete manner, it also remains to generalize our theoretical analysis. So far we have only discussed the case of attractive models with unidirectional local fields, which \u2013 admittedly \u2013 is a rather restrictive setting. Although this restriction simplified our analysis significantly and provided some interesting insights into the nature of SBP, it still leaves some room for improvement.\nGoing back to our theoretical analysis once more, our statement of optimality relied on the fact that the global minimum of FB is optimal with respect to the accuracy of both the marginals and the free energy. Yet, it is not obvious if such a relationship generalizes to more general models as well. We will thus devote the next chapter to studying the underlying principles that relate the approximation quality of the marginals and of the free energy to each other.\nNovember 8, 2019 \u2013 117 \u2013\nUnderstanding the Behavior of Belief Propagation\n7 Understanding Belief Propagation:\nAccurate Marginals or Partition Function\n\u201dTo the wise, life is a problem; to the fool, a solution.\u201d\n\u2013 Marcus Aurelius\nThis chapter introduces and investigates a novel class of models; these are attractive models with varying local potentials that we term patch-potential models. The inspiration for this chapter is twofold: First, our solution space analysis was restricted to relatively small models so far, which limits the insights; this fueled the search for models that exhibit more than three fixed points. Second, the proof for optimality of SBP relied on the fact that the global minimum of the Bethe free energy provides the most accurate marginals. The reliance on this crucial detail led to questioning the common assumption that such a correspondence extends to more general models as well.\nWe begin with the introduction of patch potential models and the specification of the considered models in Section 7.2. The main contribution of Section 7.3 is the discussion of the solution space properties by means of an example. In particular, this includes a detailed evaluation of the error in the marginals and the partition function for a wide range of parameters. We then provide theoretical arguments in Section 7.4 that explain the differences between accurate marginals and an accurate partition function and show under which conditions a fixed point exists that approximates both quantities well.\nThe content of this chapter constitutes the major part of the work in [KP19]."
        },
        {
            "heading": "7.1 Motivation",
            "text": "We have restrained ourselves to relatively well-understood models so far: both the exhaustive analysis of the solution space in Chapter 5 and the validation of the proposed algorithm in Chapter 6 are limited to models with identical or random potentials. While models with identical potentials either have one or three fixed points \u2013 and are thus too restrictive as to fully understand the behavior of BP \u2013 models with random potentials lend themselves to an analysis in terms of expected behavior. The randomness of the potentials, however, introduces a solution space so complex that it cannot be analyzed exhaustively anymore.\nWe would like to have a model class, simple enough so that it can be well understood; yet complex enough so that it exhibits a rich solution space with more than just three fixed points. For this particular purpose, we introduce patch potential models, a rich class of attractive models with inherent structure. These models exhibit many interesting phenomena and provide deep insights into the relationship between the approximation quality of the marginals and the partition function.\nSo far we have observed that the approximation quality may be severely affected by the existence of multiple fixed points with varying accuracy. In the optimization literature, this issue is often eluded by obtaining and combining all fixed points (cf. RSB theory in Section 3.5.4).\nNovember 8, 2019 \u2013 119 \u2013\n7 Understanding Belief Propagation: Accurate Marginals or Partition Function\nAlthough a similar approach seems promising for the models studied so far (cf. Chapter 5), computing all fixed points is problematic for more general models. It is often simply not possible to obtain the set of all fixed points in an efficient manner. Remember that one particular aim of Chapter 5 was to specify model classes for which a unique fixed point exists; with the focus on patch potential models, we adhere to this tradition but aim to specify model classes for which a structured and well-behaved solution space is present. Instead of conditions for uniqueness, we are interested in conditions that allow the set of all fixed points to be obtained efficiently.\nOn the other hand, if one is only interested in selecting a single fixed point, one should obviously strive for selecting the best possible fixed point available. Unfortunately, however, fixed points cannot be compared with respect to the marginal accuracy unless the exact solution is available.\nConsidering the equivalent variational interpretation admits various provable convergent algorithms (cf. Section 3.3.3). Note that the correspondence to the Bethe free energy allows one to make some more quantitative statements regarding the approximation quality. At least for attractive models, it is well established that the Bethe free energy upper bounds the Gibbs free energy [Ruo12] so that the global minimum provides the most accurate approximation of the partition function. Similar properties are not known for the marginal accuracy, however, and, except for rather simple models (as analyzed in Chapter 5), it remains an open question whether accurate marginals are to be obtained at the global minimum of the Bethe free energy. It is a common conjecture that this is the case; if true, this would provide a simple way of comparing the marginal accuracy of multiple solutions in terms of their corresponding partition function. This chapter studies this relationship in detail and further provides sufficient conditions for the global minimum of FB to yield the most accurate marginals."
        },
        {
            "heading": "7.2 Model Specifications",
            "text": "We focus on one specific model class that represents a special case of binary pairwise models. We will consider only finite-size attractive models i.e., where all couplings Jij > 0 are positive; specifically, we consider models with equal couplings Jij = J for all edges (i, j) \u2208 E. We shall further distinguish three different types of attractive models that show increasingly complex behavior: (i) attractive models with vanishing local fields \u03b8i = 0; (ii) attractive models with unidirectional fields, i.e., either \u03b8i < 0 or \u03b8i > 0; and (iii) finally, attractive models with arbitrary local fields. Such models are particularly interesting in terms of their phase transitions and are studied under the name of ferromagnetic random-field Ising models (RFIM) in physics where all \u03b8i are drawn according to some distribution."
        },
        {
            "heading": "7.2.1 Definitions",
            "text": "Attractive models with vanishing fields either have a unique or two symmetric fixed points both for infinite-size models [MM09] as well as for finite-size models (cf. Chapter 5). The marginals of two fixed points m and k are considered as symmetric if\nP\u0303mXi(+1) = 1\u2212 P\u0303 kXi(+1) (7.1)\nfor all Xi. An eminent consequence of how the Bethe free energy was defined as a function of the pseudomarginals (cf. (3.24)) is that symmetric fixed points must also have the same value of FB. Attractive models with unidirectional fields show a similar behavior and \u2013 although not exactly symmetric \u2013 have two fixed points that are almost symmetric.\nAnother important concept are flipped random variables: a random variable is flipped if the\n\u2013 120 \u2013 November 8, 2019\n7.2 Model Specifications\nmarginals are not aligned with the local potential, i.e., if( P\u0303Xi(+1) P\u0303Xi(\u22121) \u2212 1 ) \u03b8i < 0. (7.2)\nWe further say that a fixed point is state-preserving if no random variable is flipped. If all marginals are in favor of the same state xi, i.e., if P\u0303Xi(xi) > 0.5 for all Xi \u2208 X we call the corresponding fixed point biased towards xi.\nAttractive models with arbitrary local fields exhibit many non-trivial properties, may have a complex solution space, and are studied as one of the simplest forms of disordered systems [You98]. Disordered systems are systems that potentially have many fixed points, whereas many random variables are flipped."
        },
        {
            "heading": "7.2.2 Patch Potential Models",
            "text": "The definition of patch potential models follows the definitions of the RFIM, with the main difference that the local potentials are not i.i.d but obey a correlation between neighboring random variables. Moreover, we will only consider models with identical values for all local fields, albeit possibly with different sign, i.e., \u03b8i \u2208 {\u2212\u03b8,+\u03b8}.\nDefinition 7.2.1. Patch potential models are binary pairwise models in accordance with (2.14) that have attractive couplings Jij = J > 0 and that consist of multiple non-overlapping patches Gi with G = \u22c3 i Gi. A patch Gi = (Xi,Ei) is a connected subgraph that is induced by a subset of nodes Xi \u2282 X with identical local potentials \u03b8i = \u03b8 or \u03b8i = \u2212\u03b8, where Ei = {(i, j) \u2208 E : Xi, Xj \u2208 Xi}.\nNote that we will only consider models with sufficiently large patches, so that the exact marginals are state-preserving. Let us first consider a minimal example that is rich enough to exhibit some non-trivial (i.e., non-symmetric) fixed points while being structured enough to admit only few fixed points. This example serves as a model that allows us to get some intuition (cf. Section 7.3) before we discuss the properties of patch potential models in a more general manner (cf. Section 7.4).\nExample 10 (Patch Potential Model). Let G = (X,E) be a regular two-dimensional grid graph of size n \u00d7 n with two equal-sized patches. All variables in G1 experience a positive local field \u03b81 = \u03b8 whereas all variables in G2 experience the same negative local field \u03b82 = \u2212\u03b8 (cf. Figure 7.1).\nThe patch potential model is especially appealing as the composition of relatively few patches admits a simplified treatment and comes with a couple of beneficial properties. In particular, we\nNovember 8, 2019 \u2013 121 \u2013\n7 Understanding Belief Propagation: Accurate Marginals or Partition Function\ncan identify a region in the parameter space (\u03b8, J) that features a structured and well-behaved solution space (cf. Section 7.3.2)."
        },
        {
            "heading": "7.3 Fixed Point Behavior",
            "text": "If BP converges, it often provides accurate results; if multiple fixed points exist, however, the performance may vary considerably between different fixed points. We briefly discuss the RSB (replica symmetry breaking) assumption that expresses the exact marginals as a combination of all fixed points and illustrate why its success is limited to optimization problems so far (Section 7.3.1).\nThen, we discuss the solution space of Example 10 over a range of parameters and specify different regions according to the structure of the solution space (Section 7.3.2).\nAssessing the approximation quality of a specific fixed point is required to state performance guarantees of BP. We recap existing results (Section 7.3.3) and discuss how the error of the pseudomarginals and the Bethe partition function are related for patch potential models (Section 7.3.4)."
        },
        {
            "heading": "7.3.1 Combination Of Fixed Points",
            "text": "(Non-) convexity of the Bethe free energy depends on the structure of the graph and the potentials. If the model has loops and sufficiently strong couplings multiple local fixed points will exist (cf. Chapter 5). The common narrative considers the existence of multiple fixed points as particularly problematic. While this arguably creates a scenario where the performance of BP is more inconsistent, it also opens the door for methods that rely on the RSB assumption (cf. Section 3.5.4). Remember that this allows us to form the exact solution according to\nPXi(xi) = 1\u2211 mZmB M\u2211 m=1 ZmB P\u0303mXi(xi),\nif all local minima of FB are known and M = {( Z1B, P\u0303 1B ) , . . . , ( ZMB , P\u0303MB )} is available. Note that the RSB-assumption is well-established for infinite-size models [MPV87]; the validity of it for finite-size models, however, remains controversial. Yet, the doubt about finite-size models primarily stems from the lack of rigorous analysis and apart from that, the RSB assumption is empirically well-confirmed for finite-size models as well (an exhaustive overview on recent developments is presented in [LCMRTR13]). Additionally, we computed the exact solution for all considered models using the junction tree algorithm and confirmed the RSB assumption for the patch-potential models as well.\nOne efficient way to evaluate (3.53) for constrained satisfaction problems is known as survey propagation [BMZ05]. The extension to more general models,however, still remains somewhat elusive.\nApproximate Survey Propagation\nWe have seen in Chapter 5 that a convex combination of the fixed points yields the exact solution in the presence of multiple fixed points. Despite such promising results, the estimation of the marginals according to this combination is hindered by the need for all M solutions. Obtaining all fixed points that correspond to local minima of the Bethe free energy is a complex task only possible for small-scale models (as in Chapter 5) and models with certain structure (e.g., random graphs [COP19]), or potential-type (e.g., for optimization problems [ZK16]).\n\u2013 122 \u2013 November 8, 2019\n7.3 Fixed Point Behavior\nAn approximate version of survey propagation was recently applied to similar models as in this work [SRF16]. This was achieved by assuming that the fraction of randomly initialized BP runs Pm\u00b5 that converges to the m\nth fixed point provides an approximation of the partition function ZmB . This assumption is valid for attractive models with vanishing local fields; yet it is unclear how this generalizes to models with non-vanishing local fields.\nWe aim to validate the assumption for regular grid graphs with n \u00d7 n variables, \u03b8 6= 0, and with couplings large enough to admit two fixed points. Therefore, we compare both measures for both fixed points by relating the ratio between the partition functions Z1B/Z2B to the ratio P 1\u00b5/P 2 \u00b5 . The log-ratio\n46 between both measures is depicted in Figure 7.2. One would expect a constant value close to zero if Pm\u00b5 provides a good estimate of ZmB ; this is obviously not the case as Z1B/Z2B grows more rapidly. We conclude that the fraction of convergent BP runs serves as a poor estimate of the partition function with the consequence that an approximate evaluation of (3.53) leads to inaccurate marginals. This is particularly true as the local field and the model size increase.\nThis raises two immediate questions: (i) Can we specify certain model-structures or parameter configurations that grant efficient methods to obtain all fixed points in order to evaluate (3.53)? (ii) If we obtain a subset of all fixed points M\u0303 \u2282M, can we compare the available fixed points and select the best one?"
        },
        {
            "heading": "7.3.2 Solution Space of Patch Potential Models",
            "text": "The solution space for a wide range of patch potential models is analyzed to answer whether parameter configurations exist for which all fixed points can be obtained efficiently. A more formal analysis that explains the subsequent observations is presented in Section 7.4.\nLet G be a 10\u00d710 grid graph with two equal-sized patches (Example 10). This model exhibits three different regions, separated by critical values JA(\u03b8) and JC(\u03b8); see Figure 7.3 and Figure 7.4 for an illustration of the decomposition into multiple fixed points according to (3.53).\nA unique fixed point exists for J < JA(\u03b8), i.e., inside region (I), and BP converges; this fixed point is state-preserving but slightly overestimates the marginals (cf. Section 7.3.4). Additional fixed points emerge inside region (II) as the coupling strength increases to JA(\u03b8) < J < JC(\u03b8). There are three fixed points (cf. Theorem 12) and all three fixed points are stable (cf. Theorem 13). These fixed points consist of two symmetric fixed points where all marginals favor one\n46 The log-ratio is independent of the coupling strength as long as J is large enough to admit two fixed points.\nNovember 8, 2019 \u2013 123 \u2013\n7 Understanding Belief Propagation: Accurate Marginals or Partition Function\nJ\nJA JC\nFigure 7.3: Illustration of the fixed points for all regions. The circle-width corresponds to the value of FB.\nparticular state and one state-preserving fixed point (cf. Section 7.3.4). As the coupling strength increases even further to J > JC(\u03b8), i.e., inside region (III), all three fixed points remain but are suddenly accompanied by many more fixed points. It will therefore be increasingly hard to obtain all fixed points numerically, so that one can only hope to obtain a subset of all fixed points in practice.\nThe actual boundaries between the regions are numerically estimated and are depicted in Figure 7.4. The fixed points are obtained by repeated application of BP (2000 times for each (\u03b8, J)) with different random initial conditions. Furthermore, we apply random scheduling to enhance the convergence properties as any predetermined schedule would favor a specific fixed point.\nTo answer question (i) from Section 7.3.1: one region exists in the parameter space (illustrated in blue) for which all fixed points can be obtained efficiently. For region (III) (illustrated in red), however, the number of fixed points suddenly increases and we cannot rely on BP to obtain all fixed points."
        },
        {
            "heading": "7.3.3 Approximation Accuracy",
            "text": "Let J > Jc(\u03b8) and assume that a subset M\u0303 \u2282 M of all fixed points that constitute minima of FB is provided; then, how can we select the best one? Unfortunately, there is no way to tell us how accurate a particular fixed point is (if we do not have access to the exact solution). It is therefore an important problem in its own to measure the accuracy, or at least provide a bound on the approximation error. We will first discuss established results regarding the accuracy of both the Bethe partition function and the pseudomarginals. Subsequently, we will delve into the particularities for patch potential models and show how the accuracy may differ between both objectives.\nWe measure the error of the partition function ZmB = ZB(P\u0303mB ) of the mth fixed point by EZ(m) in terms of the relative error between the log-partition functions according to (3.36). Note that we only consider attractive models, for which the Bethe partition function also bounds the partition function, i.e., ZB < Z [Ruo12]; obtaining the global minimum of FB is therefore optimal with respect to the error of the partition function as arg minZmB (EZ(m)) = exp(\u2212minLFB(P\u0303 m B )).\nThe error of the singleton marginals EP (m) is measured by the mean squared error (MSE) according to (3.38). We are not aware of an explicit relationship that connects both worlds and relates the error of the marginals to the error of the partition function, except for homogeneous47 attractive models (cf. Lemma 5). It is therefore often assumed that minimizing FB will be optimal in terms of marginal accuracy for more general models as well (cf. Chapter 6\n47 These are models that have a single value J for all edges and a single value \u03b8 for all variables\n\u2013 124 \u2013 November 8, 2019\n7.3 Fixed Point Behavior\nand [WTJS14]). More formally this assumption states that\narg min m\u2208M EZ(m) = arg min m\u2208M EP (m). (7.3)\nThis assumption is, however, not valid in general as we will now show in Section 7.3.4."
        },
        {
            "heading": "7.3.4 Comparison Of Marginal Accuracy and Partition Function Accuracy",
            "text": "We aim to evaluate the relationship between the accuracy of the pseudomarginals and the accuracy of the partition function and whether (7.3) holds in general. First, we state that (7.3) does hold for homogeneous attractive models that have two fixed points at most [WTJS14]. This is a direct consequence of (3.53) or by casting the model as a larger one with vanishing local fields (cf. discussion after Theorem 9).\nLemma 5. Attractive models with identical values \u03b8i = \u03b8 have two fixed points for J > JA(\u03b8). The fixed point m that minimizes EZ(m) further provides the global minimum minL(FB) and minimizes EP (m) as well.\nSecond, we empirically validate whether minimizing FB will provide the most accurate marginals for Example 10. Figure 7.5 illustrates the error in the marginals and the error in the partition function for all fixed points. The fixed point that provides the global minimum to FB, and thus minimizes EZ(m) is emphasized in blue, the fixed point minimizing EP (m) is emphasized in red, whereas the fixed point minimizing both quantities jointly is emphasized in green.\nLet us take a closer look at region (II) in particular: three fixed points exist that can be combined to yield the exact solution (see Figure 7.1 for the exact solution). Two of these fixed points, r and q, are each biased towards one state and, because of the symmetric model, have identical values FBr = FBq. The state preserving fixed point p on the other hand provides the most accurate marginals inside (II). However, while p also provides the global minimum of FB for small values of J, Figure 7.5 shows that FBp turns into a local minimum for J \u2265 0.65 . No principle relationship between the accuracy of the marginals and the partition function can therefore be observed inside (II) and (7.3) does not necessarily hold (cf. Theorem 15).\nFor region (III) many more fixed points (u, v, . . .) emerge that all have similar values EZ(u) and EP (u); we visualize some of them in Figure 7.5. These fixed points provide slightly more\nNovember 8, 2019 \u2013 125 \u2013\n7 Understanding Belief Propagation: Accurate Marginals or Partition Function\n0.4 0.5 0.6 0.7 0.8 0.9 1 1.1 1.2 1.3 1.4 1.5\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nP\u0303 rB\nP\u0303 qB\nP\u0303 pB\n(I) (II) (III)\nP\u0303 uB\nP\u0303 vB\nJ\nE P\n(m )\n(a)\n0.4 0.5 0.6 0.7 0.8 0.9 1 1.1 1.2 1.3 1.4 1.50\n0.02\n0.04\n0.06\n0.08\n0.1\n0.12\n(I) (II) (III)\nJ\nE Z\n(m )\n(b)\nFigure 7.5: Accuracy of the marginals (a) and of the partition function (b) for Example 10 with |\u03b8i| = 0.1: we emphasize the fixed points minimizing EZ(m) (blue), minimizing EP (m) (red), and minimizing both quantities (green).\naccurate marginals than the state-preserving one, although it should be noted that all fixed points do not approximate the marginals well inside (III). On the contrary, considering EZ(u), these additional fixed points provide the worst approximation to the partition function and have even higher values FBu > FBp > FBq. The biased fixed points p, q that approximate the marginals worst, on the other hand, approximate the partition function relatively well.\nWhy fixed points exist that minimize the marginal error but are only local minima of FB can, however, not be answered by the above observations. Closer inspection of FB for different types of fixed points reveals a threshold (black dots in Figure 7.4) below which (7.3) holds. Some mild assumptions on the solution space lead to a lower bound on this threshold (cf. Theorem 16) according to\n2J \u221a N \u2212 \u03b8N = 0. (7.4)\nThis bound, illustrated by the solid black line in Figure 7.4, becomes asymptotically exact. Note that the slope, defined by (7.4) increases with the model size N so that the global minimum of FB provides the most accurate marginals for a wider range of parameters."
        },
        {
            "heading": "7.4 Theoretical Analysis",
            "text": "Here we properly define the boundaries JA(\u03b8) and JC(\u03b8) between different regions and provide formal arguments that explain the observations from Section 7.3.2. While some properties are directly attributable to (3.53), several results are based on the fact that the patch potential model consists of multiple patches with a unidirectional local field. First, we need to prepare an alternative update equation that makes the interactions between two patches more explicit. For that purpose, we will introduce an effective field that acts on the boundary of each patch and incorporates the influence form all other patches.\nWe only present the most insightful proofs below and defer some longer proofs to the Appendix A.2. Additionally, we prepare some corollaries that simplify the results for models with two equal-sized patches as in Example 10.\n\u2013 126 \u2013 November 8, 2019\n7.4 Theoretical Analysis"
        },
        {
            "heading": "7.4.1 Effective Field",
            "text": "We introduce an effective field \u03b8\u0303i for all variables that lie on the patch-boundary to incorporate the interactions with the neighboring patches.\nTheorem 11 (Effective Field). Let Xi be a variable on the boundary of patch Xi that receives messages from inside, i.e., Xk \u2208 Xi, and outside, i.e., Xj \u2208 {X\\Xi}, the patch. The effective field \u03b8\u0303i acts on the boundary according to\n\u03b8\u0303i = \u03b8i + \u2211\nXj\u2208{\u2202(i)\\Xi}\natanh(2\u00b5ji(Xi = 1)\u2212 1). (7.5)\nThe proof of Theorem 11 is presented in Appendix A.2.1. Messages from outside the patch are now subsumed by \u03b8\u0303 and the additive terms in (7.5) will be positive if \u00b5ji(Xi = 1) > \u00b5ji(Xi = 0) and negative otherwise. This is particularly important in the definition of the region boundaries and admits \u201cindependent\u201d treatment of every patch.\n7.4.2 Definition of Region (II)\nThe notion of an effective field (Theorem 11) allows us to define the boundaries between the three distinct performance regions of patch potential models. We discuss the solution space in detail and what can be said about the performance of BP. Let us denote the second region, i.e., the region where the global behavior can be inferred by treating the patches individually by (II) = {\u03b8, J}.\nDefinition 7.4.1 (Region). A parameter set (\u03b8, J) \u2208 (II) if and only if the following conditions are satisfied: (1.) Let JA(Gi, \u03b8) denote the critical value for the couplings beyond which multiple fixed points exist.48 Then every patch Gi \u2208 G must have its respective threshold below the actual coupling strength, i.e., JA(Gi, \u03b8) < J (2.) Consider all pairs of patches Gi and Gj ; if one patch, e.g., Gi has its variables flipped, the imposed effective field on the boundary must stabilize the second patch Gj so that J < JA(Gj , \u03b8\u0303) = JC(Gj , \u03b8).\nThese conditions implicitly define the \u201cwell-behaved\u201d region (II). Definition 7.4.1.1 provides the lower boundary of region (II) as only a unique fixed point would exist otherwise. It may be less obvious how Definition 7.4.1.2 provides the upper boundary of region (II). Note that J < JA(Gj , \u03b8\u0303) is a necessary condition if Gi is flipped, as parts of Gj would flip otherwise and lead to disordered behavior (cf. Figure 7.5). The restriction to (II) and the exclusion of disordered solutions further validates the RSB assumption [MM09, Chapter 19].\n7.4.3 Properties Of Region (II)\nIn this chapter, we are particularly interested in understanding the properties of BP inside region (II) that complies with the subsequent properties. Note that the properties inside region (II) are a direct consequence of Definition 7.4.1. Many arguments will rely on the fact that every attractive model with vanishing or unidirectional local fields either has a unique stable or two stable fixed points (cf. Section 7.2).\nWe will first bound the number of possible BP fixed points.\n48 Note that an analytical solution only exists for graphs with vanishing fields of infinite size or periodic boundary conditions, but the threshold can be estimated numerically.\nNovember 8, 2019 \u2013 127 \u2013\n7 Understanding Belief Propagation: Accurate Marginals or Partition Function\nTheorem 12 (Existence). Let U be a patch potential model with (\u03b8, J) \u2208 (II). The amount of fixed points M grows with the number of patches (rather than the number of variables). Specifically, we have M = O(2(|Gi|)), where |Gi| denotes the number of patches.\nProof. First, assume that a given patch Gi is flipped; then by the definition of the patch potential model (Definition 7.2.1) and by Theorem 11 it follows that effective field \u03b8\u0303 is aligned with the local field of the variables at any neighbor patch Gj so that\nsgn(\u03b8\u0303j) = sgn(\u03b8j), (7.6)\n|\u03b8\u0303j | > |\u03b8j |. (7.7)\nFurther, let us recall the definition of (II) (Definition 7.4.1.2 in particular). It follows that the effective field stabilizes its neighbor patch Gj , i.e., JA(Gj , \u03b8\u0303) > JA(Gj , \u03b8) so that, according to the definition J < JA(Gj , \u03b8\u0303), and Gj admits only a unique solution.\nSecond, assume that Gi is not flipped; then it follows by Theorem 11 that \u03b8\u0303i < \u03b8i for Xi on the boundary of the neighbor patch Gj . This decrease in the local field reduces the threshold for the existence of two solutions to smaller values of J (cf. [KP17]) so that\nJA(Gj , \u03b8\u0303) < JA(G, \u03b8) < J. (7.8)\nBy Definition 7.4.1.1 it follows that the neighbor patch Gj has two fixed points now, and it depends on the initialization to which one BP will converge.\nFinally, we aim to show that the number of possible fixed points is bounded. Therefore we want to stress that the above arguments show how patches can either be aligned with the local potential or be flipped; it is crucial that every patch acts as one instance and that all variables belonging to one patch are aligned. Else the fixed point would be disordered which we rule out precisely by Definition 7.4.1. This and the fact that we are considering binary random variables limits the number of possible solutions to\nM \u2264 2|Gi|, (7.9)\nwhere |Gi| denotes the overall number of patches.\nCorollary 12.1 (Example 10). Let U be a patch potential models with two equal-sized patches (cf. Example 10). Then, for (\u03b8, J) \u2208 (II) three fixed points exist; these are one state preserving fixed point and two fixed points that have all variables biased towards one of both states. Note that both patches can not be flipped simultaneously inside (II) (cf. proof of Theorem 12) as one patch would stabilize, i.e., prohibit from flipping, the second patch.\nTheorem 12 is of great practical relevance for the RSB assumption (3.53), i.e., whether a combination of BP fixed points can form the exact solution. The fact that there is a relatively small number of fixed points makes the task of obtaining them practically feasible. Existence alone, however, is not sufficient as we have to rely on some numerical method that obtains all fixed points; if we aim to apply BP for that matter there is the additional requirement for all fixed points to be stable. Fortunately, it turns out that all fixed points inside (II) are stable indeed.\nTheorem 13 (Stability). Let U be a patch potential model with (\u03b8, J) \u2208 (II). Then, every fixed point P\u0303mB is a stable fixed point for BP.\nFinally, as an immediate consequence of the limited amount of fixed points (Theorem 12), all of which are stable (Theorem 13), it follows that the exact solution can be computed according to (3.53) in practice. One can for example apply BP repeatedly, possibly in parallel, with random initialization to obtain and combine all fixed points.\n\u2013 128 \u2013 November 8, 2019\n7.4 Theoretical Analysis\nMarginal Accuracy\nTheorem 14 (Marginal Accuracy). The MSE of the singleton marginals EP (k) of the k th solution P\u0303 kB relates to the ratio of the Bethe partition functions according to\nEP (k) = 2 N( \u2211 m ZmB )2 \u2211 Xi\u2208X \u2223\u2223\u2223\u2211 m\\k ZmB ( P\u0303mXi \u2212 P\u0303 kXi ) \u2223\u2223\u22232. Proof. According to (3.53) we can express the exact solution by the convex combination of all fixed points. Consequently, using symmetry properties of the binary random variables, the error is given by\nEP (k) = 2\nN \u2211 Xi\u2208X \u2223\u2223\u2223\u2223 \u2211 m ZmB P\u0303mXi\u2211 m ZmB \u2212 P\u0303 kXi \u2223\u2223\u2223\u22232\n= 2\nN \u2211 Xi\u2208X\n\u2223\u2223\u2223\u2223 \u2211 m ZmB P\u0303mXi \u2212 \u2211 m ZmB P\u0303 kXi\u2211\nm ZmB \u2223\u2223\u2223\u22232 = 2\nN (\u2211 m ZmB )2 \u2211 Xi\u2208X \u2223\u2223\u2223\u2211 m\\k ZmB P\u0303mXi \u2212 P\u0303 kXi \u2223\u2223\u22232, (7.10)\nwhere we first, bring everything on the same denominator so that the kth contribution cancels out subsequently.\nRepresenting the MSE according to Theorem 14 is particularly appealing as it omits the need for expressing the exact marginals. This further provides a way to express the ratio of the marginal error between two fixed points.\nCorollary 14.1. The MSE-ratio of two fixed points k and l is a ratio of weighted partition functions according to\nEP (k) EP (l) =\n\u2211 Xi\u2208X | \u2211 m\\k ZmB (P\u0303mXi \u2212 P\u0303 kXi)|2\u2211\nXi\u2208X |\u2211 m\\l ZmB (P\u0303mXi \u2212 P\u0303 lXi)|2 . (7.11)\nExpressing the ratio of the marginal error according to (7.11) is advantageous in elaborating on the difference between accuracy of the approximated marginals and the approximated partition function. We define the mismatch between P\u0303mXi at two fixed points k and l by\nQi(k, l) = P\u0303 k Xi(Xi = 1)\u2212 P\u0303 lXi(Xi = 1). (7.12)\nNow, let us denote the error of the state preserving fixed point by EP (p) and of the fixed point that has all marginals biases towards xi = 1 by EP (q). Then \u2013 maybe non-surprising as the exact solution is state preserving as well \u2013 we show that the state-preserving fixed point has the most accurate marginals. We first discuss the error-ratio in a general manner in Theorem 15 before considering the special case of a model with two patches, i.e., for Example 10.\nTheorem 15 (Error Ratio). Let U be a patch potential model with (\u03b8, J) \u2208 (II). The state preserving fixed point p provides more accurate marginals than the fixed point q that has all marginals biased to one state, i.e.,\nEP (p) EP (q) < 1. (7.13)\nNovember 8, 2019 \u2013 129 \u2013\n7 Understanding Belief Propagation: Accurate Marginals or Partition Function\nProof. To show that, irrespective of the value of ZB, EP (p)EP (q) < 1 we assume that the state preserving fixed point does not minimize the Bethe free energy, i.e., ZqB > Z p B.\nWithout loss of generality, we make some prior assumptions on the model: First, we assume that the overall number of variables with a positive local field equals the number of variables with a negative local field, i.e.,\n|{Xi \u2208 X : \u03b8i = +\u03b8}| = |{Xj \u2208 X : \u03b8i = \u2212\u03b8}|. (7.14)\nSecond, we assume that all patches are of equal size. And finally, we group all possible fixed points according to their marginals and denote them as follows: the state-preserving fixed point is referred to as p; all fixed points that have more patches biased towards Xi = +1 are referred to as s = 1, . . . , S, with q being the fixed point that has all variables biased towards Xi = +1; all fixed points that have more patches biased towards Xi = \u22121 are referred to as t = 1, . . . , T , with r being the fixed point that has all variables biased towards Xi = \u22121.\nThis has some implications that will ease the subsequent analysis significantly. Specifically, the number of fixed points favoring one state equals the number of fixed points favoring the other state, i.e., S = T by (7.14) and by Theorem 12.\nAnother important consequence of incorporating the interactions between patches into an effective field is that the number of variables that favor one state has an immediate influence on the value of the singleton marginals. It can be shown that the effective field, if stronger than the local field \u2013 note that the effective field is stronger than the local field whenever two neighboring patches are biased towards the same state \u2013 increases the bias of the variables. This intuitive statement is a consequence of the Griffiths-Hurst-Sherman inequality [Gri67] that can be extended to specific fixed points by straightforward manipulations (cf. [KKP18]). In essence this means that for all variables Xi \u2208 X we have\nP\u0303 qXi(Xi = 1) \u2265 P\u0303 s Xi(Xi = 1) \u2265 P\u0303 p Xi (Xi = 1) \u2265 P\u0303 tXi(Xi = 1) \u2265 P\u0303 rXi(Xi = 1). (7.15)\nMoreover, as all patches have equal size and because Jij = J as well as \u03b8i \u2208 {\u2212\u03b8, \u03b8} every fixed point s has a symmetric fixed point t that has the same value for the approximate partition function (cf. Proof of Theorem 16). That is, except for the state-preserving fixed point p all fixed points come in couples that satisfy\nZsB = ZtB. (7.16)\nWe will further utilize the properties of the mismatch Qi(k, l); in particular the symmetry property\nQi(k, l) = \u2212Qi(l, k), (7.17)\nand the expansion property\nQi(k, l) = Qi(k,m) +Qi(m, l). (7.18)\nFinally, we express the error ration between the state-preserving fixed point p and the biased\n\u2013 130 \u2013 November 8, 2019\n7.4 Theoretical Analysis\nfixed point q according to Corollary 14.1 so that\nEP (p) EP (q) =\n\u2211 Xi\u2208X | \u2211 m\\p ZmBQi(m, p)|2\u2211\nXi\u2208X | \u2211 m\\q ZmBQi(m, q)|2\n(a) =\n\u2211 Xi\u2208X |\u2211 s ZsBQi(s, p) + \u2211 t ZtBQi(t, p)|2\u2211\nXi\u2208X |\u2211 s\\q ZsBQi(s, q) + \u2211 t ZtBQi(t, q) + Z p BQi(p, q)|2\n(b) =\n\u2211 Xi\u2208X |\u2211 s\\q ZsB ( Qi(s, p) +Qi(t, p) ) + ZqB ( Qi(q, p) +Qi(r, p) ) |2\u2211\nXi\u2208X |\u2211 s\\q ZsB ( Qi(s, q) +Qi(t, q) ) + ZqBQi(r, q) + Z p BQi(p, q)|2 , (7.19)\nwhere (a) follows from splitting the sum into the fixed points s that are more biased towards Xi = 1 and into the fixed points t that are more biased towards Xi = \u22121. Note that the statepreserving fixed point p does not belong to either set and is consequently expressed explicitly in the denominator. For (b) we make use of (7.16) and arrange the terms by making the dependence on q and r explicit so that the sum goes over the same terms in the numerator and in the denominator.\nWe can further express the error ratio and bound it using Jensen\u2019s inequality according to\nEP (p) EP (q) (a) =\n\u2211 Xi\u2208X |\u2211 s\\q ZsB ( Qi(s, p) +Qi(t, p) ) + ZqB ( Qi(q, p) +Qi(r, p) ) |2\n\u2211 Xi\u2208X (\u2211 s\\q ZsB \u2223\u2223(Qi(s, q) +Qi(t, q))\u2223\u2223+ ZqB\u2223\u2223Qi(r, q)\u2223\u2223+ ZpB\u2223\u2223Qi(p, q)\u2223\u2223)2\n\u2264\n\u2211 Xi\u2208X (\u2211 s\\q ZsB \u2223\u2223(Qi(s, p) +Qi(t, p))\u2223\u2223+ ZqB\u2223\u2223(Qi(q, p) +Qi(r, p))\u2223\u2223)2\u2211\nXi\u2208X (\u2211 s\\q ZsB \u2223\u2223(Qi(s, q) +Qi(t, q))\u2223\u2223+ ZqB\u2223\u2223Qi(r, q)\u2223\u2223+ ZpB\u2223\u2223Qi(p, q)\u2223\u2223)2 , (7.20)\nwhere separating the norm does not change the result in (a) because of (7.15), which implies Qi(m, q) < 0 for all fixed points m 6= q.\nFor completing the proof we make use of the symmetry property (7.17) and the expansion property (7.18) in order to rearrange the terms; in particular note that Qi(q, s) + Qi(q, t) = Qi(q, s) +Qi(q, s) +Qi(s, p) +Qi(p, t), and that Qi(q, r) = Qi(q, p) +Qi(p, r) so that\nEP (p) EP (q) \u2264 \u2211\nXi\u2208X (\u2211 s\\q ZsB \u2223\u2223(Qi(s, p)\u2212Qi(p, t))\u2223\u2223+ ZqB\u2223\u2223(Qi(q, p)\u2212Qi(p, r))\u2223\u2223)2\u2211\nXi\u2208X (\u2211 s\\q ZsB \u2223\u2223(Qi(s, p)+Qi(p, t)+Qi(q, s)+Qi(q, s))\u2223\u2223+ZqB\u2223\u2223Qi(q, p) +Qi(p, r)\u2223\u2223+ZpB\u2223\u2223Qi(q, p)\u2223\u2223)2 .\nNote that we have applied (7.17) so that every mismatch-term is strictly positive. It is thus straightforward to see, by comparing all terms, that the numerator is strictly smaller than the denominator for every variable Xi \u2208 X so that\nEP (p) EP (q) < 1. (7.21)\nNovember 8, 2019 \u2013 131 \u2013\n7 Understanding Belief Propagation: Accurate Marginals or Partition Function\nIn particular for models with two equal-sized patches, we can simplify the error ratio (7.11) considerably.\nCorollary 15.1 (Example 10). Let d = Qi(q, r) > 0, then\nEP (p) EP (q) <\n\u2211 Xi\u2208X |Z q Bd|2\u2211\nXi\u2208X |Z q Bd+ Z p BQi(p, q)|2\n< 1. (7.22)\nNote that Corollary 15.1 is an immediate consequence of Theorem 15. Additionally we present an alternative proof that admits some intuitive arguments in Appendix A.2.2.\nIt follows that the state preserving fixed point p minimizes the marginal error inside (II) irrespective of FBp. This has drastic implications and forbids any relationship between the fixed point minimizing the marginal error and the one minimizing the partition function error.\nFree Energy Minimizing Fixed Point\nHowever, despite Theorem 15, the question remains where the difference between EZ(m) and EP (m) stems from?\nWe will now answer this question and provide conditions for arg minEZ(m) = arg minEP (m) to be valid. We further present an approximate condition for the state-preserving fixed point p to simultaneously provide the most accurate marginals and minimize FB. Therefore, let us define the following variables (cf. Section A.2.3 in the appendix for a formal introduction): EP is the set of all boundary edges; EC is the set of edges between variables that favor different states; Nf and Nc are the numbers of flipped and non-flipped variables; and \u2206SB is the difference in the entropy between two fixed points.\nTheorem 16. Let us consider the state-preserving fixed point p with FBp and some other fixed point with FBm. Then, FBp < FBm is the global minimum if\n2J(|EP | \u2212 |EC |) < \u03b8(N \u2212Nc +Nf ) + \u2206SB. (7.23)\nThe proof of Theorem 16 is deferred to Appendix A.2.3. For models with two equal-sized patches and couplings strong enough for the entropy-term to\nvanish we can further simplify (7.23) significantly and state that:\nCorollary 16.1 (Example 10). The state-preserving fixed point provides the most accurate marginals and the global minimum FBp if (\u03b8, J) \u2208 (II) and if\n2 \u221a NJ < N\u03b8. (7.24)\nProof. For the specific case of a grid graph with two equal-sized patches (Example 10) we can further simplify the condition from Theorem 16. Therefore, note that for FBp < FBq to be satisfied, we have Nc = Nf and |EP | = \u221a N so that (A.28) reduces to\n2J \u221a N \u2264 \u03b8N + \u2206SB. (7.25)\nThe definition of (II) requires strong interactions J so that the entropy terms in the free energy is small. We can consequently approximate (7.25) by neglecting the entropy terms.\n2J \u221a N \u2264 \u03b8N. (7.26)\nThese sufficient conditions for (7.3) provide a guideline when it would be safe to select the fixed point according to the partition function value. This correspondence tends to hold for models with strong local potentials \u03b8 and with increased model-size N as shown in Corollary 16.1.\n\u2013 132 \u2013 November 8, 2019\n7.5 Conclusion"
        },
        {
            "heading": "7.5 Conclusion",
            "text": "In Section 6.4 we had relied on the assumption that finding the global minimum of FB gives us the most accurate marginals and partition function. This is a strong assumption that \u2013 although never formally verified \u2013 was generally believed to be true.\nIn this chapter we aimed to verify whether such a relationship holds in general. Therefore, we studied the relationship between the accuracy of the marginals and the partition function at stationary points of the Bethe free energy, with far-reaching implications for approximate inference methods that operate on the Bethe free energy. To do so, we introduced a new class of models first. These so-called patch potential models exhibit lots of structure that enabled us to analyze the solution space thoroughly. Yet, patch potential models proved to be flexible enough to produce some surprising insights.\nFirst, we elaborated on the existence of a well-behaved region in parameter space for which the number of fixed points depends only on the number of patches (instead of the variables).\nSecond, this well-behaved region allowed us to assess the correspondence between the accuracy of the marginals and the partition function; we showed that this common assumption is not true in general and explained why \u2013 instead of the global minimum \u2013 the most accurate marginals may be found at a local minimum of FB.\nFinally, we inspected under which conditions this mismatch arises and further introduced guarantees for the global minimum of FB to remain optimal with respect to marginal accuracy. Note that the mismatch is effectively attributable to finite-size effects which might explain why this behavior is not known in the physics literature, where the focus lies on infinite-size \u2013 or at least very large \u2013 models. Yet, models of relatively small size do play an important role in many applications of BP.\nThe analysis of patch potential models may serve as a foundation that enables the extension of survey propagation to problems beyond constrained satisfaction problems. So far this has been limited by the potentially huge amount of fixed point. Our definition of the well-behaved region and the study of patch potential models as a whole suggests that it may be very much possible to obtain the set of all fixed points efficiently, as long as the model is sufficiently structured.\nMoreover, our newly developed understanding of the relationship between the accuracy of the marginals and the partition function has a notable impact on our theoretical analysis of SBP in Section 6.4. There we have limited our attention to attractive homogeneous models where the accuracy of both quantities can be used in an interchangeable way. Whether it is the minimum (i.e., the Bethe free energy) or the minimizer (i.e., the pseudomarginals) that is responsible for the optimality of SBP needs to be reassessed. It is not clear if it is really the global minimum of FB that originates from the start point. On the contrary, our minimal patch potential model of Example 10 shows that the state-preserving fixed point defines the solution path, so that SBP may very well be in favor of obtaining the fixed point with the most accurate marginals. This could be an explanation for the surprisingly accurate results of SBP for general models and would immensely strengthen the practical relevance of SBP as no method is known so far that deliberately aims to obtain fixed point with the most accurate marginals.\nNovember 8, 2019 \u2013 133 \u2013\nUnderstanding the Behavior of Belief Propagation\n8 Discussion and Open Questions\n\u201dThere must be some way out of here, Said the joker to the thief.\u201d\n\u2013 Bob Dylan\nIn this thesis, we performed a systematic analysis of BP\u2019s solution space. Finding all fixed points of BP is a relevant but hard problem, except for a few restricted models. We took inspiration from a set of diverse scientific fields to tackle this problem: in particular, we framed BP as dynamical systems and formulated the fixed point equations. Drawing from computational mathematics, we established a way to solve the set of fixed point equations and computed all fixed points for a range of problems.\nThe knowledge of the full solution space then served as a cornerstone for evaluating the accuracy and the convergence properties of BP. Moreover, we related those performance properties to the parameterization of a given model (i.e., the model size and the specifications of the potentials) and explained how the performance of BP changes if we tune the model.\nA detailed summary of our main contributions is presented in each chapter\u2019s conclusion. Here we summarize our most interesting findings and discuss how we advanced the understanding of BP. Our findings satisfy a twofold interest: they advance the theoretical understanding and suggest various possibilities of enhancing BP\u2019s performance.\n\u2022 From a theoretical perspective, we made the influence of the model parameters on the performance of BP explicit. We sum up our key-insights as follows:\nThe influence of the model size is hard to pinpoint down. On the one hand, larger models tend to have a higher average degree, which affects the convergence properties slightly negatively. On the other hand, we established a positive effect of the model size on the marginal accuracy.\nThe influence of the potentials is much more apparent. Strong pairwise potentials influence the performance of BP detrimentally, whereas strong local potentials increase the accuracy and the convergence properties.\nAdditionally, we demonstrated a considerable accuracy gap between the best possible and the worst possible fixed point of BP. This poses a fundamental problem when deriving performance guarantees as is not obvious to which fixed point BP will converge.\n\u2022 From a practical perspective, we justified the exploration of multiple fixed points, intending to select one with good approximation quality. We discussed the underlying mechanisms and limitations of established modifications, for example, damping or scheduling.\nWe further proposed one particular modification that aims to obtain the global minimum of the Bethe free energy. This method, SBP, consistently obtains more accurate marginals than BP and provides accurate marginals for models where BP fails to converge.\nEven the minimization of the Bethe free energy, however, has its limitations if one is only interested in the marginals; we exemplified and explained why the most accurate marginals may not be found at the global minimum of the Bethe free energy.\nNovember 8, 2019 \u2013 135 \u2013\n8 Discussion and Open Questions\nThe analysis of BP\u2019s solution space addressed some long-standing questions but we are still far away from understanding every aspect of BP. Below we summarize some intriguing questions that emerged while working on this thesis and that were left unanswered. We distinguish three different directions that appear to be worth pursuing in particular:\n1. Structure of the Solution Space:\nAlthough this thesis obtained the full solution space for some models and, in doing so, provided novel insights into the properties of BP, computing the solution space for more complex models remains elusive. As we have seen in Chapter 7, however, it is precisely the consideration of more complex models that can lead to fascinating insights.\nThe extension of the NPHC method to larger models was primarily hindered by the involved computation of the root count. So far, we only relied on some meta-heuristics to compute the root count. Most relevant models, be it grid graphs or models arising in the context of error-correcting codes, however, exhibit a special graph structure that directly determines the structure of the fixed point equations. A first step towards computing the root count for complex models could be to exploit the graph structure and thus reduce the overall complexity of this crucial step. Alternatively, it would be relevant (from a mathematical perspective) to reformulate the problem to an equivalent system of equations with fewer complex solutions and a lower BKK bound. Such problem-tailored approaches would extend the applicability of the NPHC method and, when applied to large models, would have the potential to provide even more insights into the behavior of BP.\nTraditionally, one is often interested in specifying regions with a unique fixed point. We have shown that this seems to be a rather limiting point of view as combinations of multiple fixed points often provide strikingly accurate results. Although obtaining the set of all fixed points is, in general, a hard problem, we have seen in Chapter 7 that certain regions in the parameter space exist for which only a few fixed points are present. This suddenly opens the door for numerical methods that efficiently obtain all fixed points and enhance the accuracy by combining them.\nWhen defining well-behaved regions of BP we would consequently like to initiate a shift from conditions for uniqueness to conditions for structured, albeit possibly multiple, fixed points. Fairly substantial work has been conducted in this matter for the special case of optimization problems. Developing similar results for more general models is a necessary step to assess the relevance of methods that rely on considering multiple fixed points and to elaborate on their capabilities.\n2. Global Convergence Properties: We have put a lot of emphasize on the local stability analysis in Chapter 5. Let us stress the notion of local here again: we considered a fixed point as stable if the messages will converge to the fixed point inside its neighborhood. In general, one can, however, not count on bringing the messages close enough to the fixed point and it is not only the local stability that is important for the convergence properties of BP; even more so it is the region of attraction. The region of attraction describes the range of message values for which BP will converge to a given fixed point. This is particularly relevant in practice as BP sometimes fail to converge despite the existence of stable fixed points (unless initialized very close to the fixed point).\nSuch a global perspective also highlights the influence of the initial message values on the convergence properties of BP. Besides being convenient, uniform initialization tends to favor good fixed points; it remains an open problem to answer what is so special about uniform initialization and under which conditions uniform initialization is optimal. We expect that extending the local stability analysis and accounting for the regions of attraction as well will provide many answers to those questions. Besides, this could potentially lead to variants of BP that, even if initialized far away from any fixed point, explore the\n\u2013 136 \u2013 November 8, 2019\nparameter space in a way such that the messages enter a region of attraction at some point, ultimately driving the messages into convergence.\n3. Enhancing BP with Respect to Marginal Accuracy: The main contribution of Chapter 7 was to elaborate on the difference between the accuracy of the marginals and the partition function. Our analysis revealed that the fixed point providing the most accurate marginals may only be a local minimum. Note that most variants that aim to enhance BP are either heuristics that only aim to enforce convergence or are theoretically well-motivated and focus on the quality of the partition function approximation.\nFor many applications, we are, however, ultimately interested in enhancing the quality of the marginals (cf. error-correcting codes in Section 5.9). It would thus be of great relevance to manipulate the update equation in a way that enforces convergence towards a fixed point with accurate marginals. Enforcing convergence towards one particular fixed point would further enable one to come up with realistic performance guarantees, even in the presence of multiple fixed points.\nWe have proposed SBP that is guaranteed to obtain the most accurate marginals for attractive models with unidirectional potentials. Looking at the evolution of the fixed points on patch-potential models, it seems as if SBP is capable of tracking the state-preserving fixed point as well. Thus, SBP is already one possible candidate for obtaining accurate marginals. It would be interesting to elaborate on this observation and substantiate this claim in a more formal way. Essentially, the advantages of SBP boil down to providing a favorable initialization for BP. This highlights the importance of the initial messages once more and raises the question if there is a general way of initializing the messages that will lead to accurate marginals.\nBesides the role of the initialization, scheduling plays an important role in determining the fixed point of BP. Again, one would hope that \u201cgood\u201d scheduling methods enforce convergence towards an accurate fixed point. Existing scheduling methods are either adaptive or only take the structure of the model into account. The performance of BP, however, not only depends on the structure of the model but on the potentials as well. Therefore, if the aim is to obtain accurate marginals, the value of the potentials should not be neglected and it seems promising to come up with a fixed schedule that takes the values of the potentials into account as well, for example by scheduling the messages in a way that prioritizes regions with strong local potentials.\nNovember 8, 2019 \u2013 137 \u2013\nUnderstanding the Behavior of Belief Propagation\nA Appendix\nA.1 Proofs from Chapter 6\nThis Section contains all the detailed proofs deferred from Section 6.4.\nA.1.1 Proof of Lemma 4\nEssentially, we first show that \u00b5ij(Xj = 1)/\u00b5ij(Xj = \u22121) increases monotonically with Jij and then express the pseudomarginals in terms of (3.2) and (3.3).\nLet us denote the messages on U0 and on U1 by \u00b5ij(xj)0 and \u00b5ij(xj)1 respectively. Further, let all local potentials be positive, i.e., \u03b8i > 0, and let all pairwise potentials of U1 be -larger than those of U0, i.e., 0 < J0ij = J1ij \u2212 . Note that by assumption mi \u2208 (0, 1] so that\n\u00b5ij(Xj = +1) \u2265 \u00b5ij(Xj = \u22121). (A.1)\nFirst, we show that for all (i, j) \u2208 E\n\u00b5\u25e6ij(Xj = +1)0 \u00b5\u25e6ij(Xj = \u22121)0 < \u00b5\u25e6ij(Xj = +1)1 \u00b5\u25e6ij(Xj = \u22121)1 . (A.2)\nTherefore, consider the update rule of (3.1) for both states\n\u00b5n+1ij (Xj = +1)1 \u221d eJij+\u03b8i+ \u220f\nXk\u2208{\u2202(i)\\Xj}\n\u00b5nki(Xi = +1)1 + e \u2212Jij\u2212\u03b8i\u2212 \u220f Xk\u2208{\u2202(i)\\Xj} \u00b5nki(Xi = \u22121)1, (A.3)\nand \u00b5n+1ij (Xj = \u22121)1 \u221d e\u2212Jij+\u03b8i\u2212 \u220f\nXk\u2208{\u2202(i)\\Xj}\n\u00b5nki(Xi = +1)1 + e Jij\u2212\u03b8i+ \u220f Xk\u2208{\u2202(i)\\Xj} \u00b5nki(Xi = \u22121)1. (A.4)\nIn (A.3) the larger product is multiplied by e and the smaller product is divided by e . For (A.4) it is exactly the other way round so that the ratio between the messages increases which proofs (A.2). We shall denote the imposed difference \u03b4 \u2208 R\u2217+ on the messages by\n\u00b5\u25e6ij(Xj = +1)1 = \u00b5 \u25e6 ij(Xj = +1)0 + \u03b4, (A.5) \u00b5\u25e6ij(Xj = \u22121)1 = \u00b5\u25e6ij(Xj = \u22121)0 \u2212 \u03b4. (A.6)\nSecond, we show that m0i < m 1 i which is an immediate consequence of plugging (A.5) and (A.6)\ninto (3.2).\nFinally, it remains to show that 0 (i) < \u03c70ij (ii) < \u03c71ij . Without loss of generality we assume that all variables have equal degree d+ 1 and constant coupling strength Jij = J. First we show that (i) holds, i.e., \u03c7 = \u03c70ij is positive. Let us express the marginals by (3.3) and denote the messages\nNovember 8, 2019 \u2013 139 \u2013\nA Appendix\nby \u00b5 = \u00b5ij(Xj = 1)0. It follows that \u00b5ij(Xj = \u22121)0 = (1\u2212 \u00b5) and that\n\u03c7=eJ+2\u03b8\u00b52d+eJ\u22122\u03b8(1\u2212\u00b5)2d\u2212 2e\u2212J\u00b5d(1\u2212\u00b5)d. (A.7)\nLet us further represent the messages by \u00b5 = 1/2 + x with x \u2208 [0, 1/2]. It follows that\n\u03c7 (a) \u2265 ( (1/2 + x)2d + (1/2\u2212 x)2d ) \u2212 2 (1/2 + x)d (1/2\u2212 x)d = ( (1/2\u2212 x)d \u2212 (1/2 + x)d )2 (b)\n\u22650, (A.8)\nwhere (a) follows from neglecting all exponential terms and thus upper bounding the positive term and lower bounding the negative term (with equality if and only if J = 0 and \u03b8 = 0) and (b) is a direct consequence of the square in (A.8). Now let us show that (ii) holds, i.e., \u03c7 increases monotonically, by taking the derivative of (A.7), so that\n\u2202\n\u2202\u00b5 \u03c7 =2d\n( eJ+2\u03b8\u00b52d\u22121 \u2212 eJ\u22122\u03b8 (1\u2212 \u00b5)2d\u22121 ) +2de\u2212J ( \u00b5d(1\u2212 \u00b5)d\u22121\u2212 \u00b5d\u22121(1\u2212 \u00b5)d ) (A.9)\n(a) \u22652de\u2212J ( \u00b5d(1\u2212 \u00b5)d\u22121 \u2212 \u00b5d\u22121(1\u2212 \u00b5)d ) (b) \u22650, (A.10)\nwhere (a) follows from neglecting the, strictly positive, first term in (A.9), and (b) is a direct consequence from (A.1).\nA.1.2 Proof of Theorem 9\nA unique start point P\u0303mB (\u03b6 = 0) exists by Theorem 6 that equals the exact pseudomarginals PB(\u03b6 = 0) and, for \u03b8i > 0, has positive mean and correlation), i.e., mi(\u03b6 = 0) > 0.\nConsequently, Lemma 4 applies, which further implies that mi(\u03b6) and \u03c7ij(\u03b6) are monotonically increasing; moreover, mi(\u03b6) and \u03c7ij(\u03b6) are continuous by Theorem 7. This further implies that the Bethe free energy FBm(\u03b6) decreases. Let FBm(\u03b6 = 1) correspond to the endpoint of the solution path c(\u03b6) that emerges from the origin; then, it immediately follows that the error with respect to the endpoint FBm(\u03b6 = 1) decreases along the solution path: i.e., consider two arbitrary values m, k \u2208 [0, 1] such that k > m, then |FBm(\u03b6m)\u2212FBm(\u03b6 = 1)| \u2265 |FBm(\u03b6k)\u2212FBm(\u03b6 = 1)|.\nIt remains to show that SBP obtains the fixed point FBm(\u03b6 = 1) that minimizes the error with respect to the exact free energy, i.e., m = arg minm\u2208MEZ(m) . Therefore consider the fact, that attractive models with \u03b8i > 0 have a unique fixed point that satisfies mi(\u03b6) \u2208 (0, 1] [YFW05]. A second minimum with negative means, however, may emerge for sufficiently large values of Jij . We denote this alternative stationary point by FBn. This minimum FBn, if it exists, is close to being symmetric, i.e., mmi (\u03b6)\u2212 = \u2212mni (\u03b6) and \u03c7mij (\u03b6) = \u03c7nij(\u03b6) + .\nNow let us express the Bethe free energy in (3.24) of both fixed points P\u0303mB and P\u0303 n B in terms of their energy and entropy according to FB = EB \u2212 SB (cf. Section 3.3). Then, as a consequence of symmetry of the entropy SB(P\u0303 n B) \u223c= SB(P\u0303mB ) and as a consequence of singleton marginals that are not aligned to the local potentials in (3.21) EB(P\u0303 n B) > EB(P\u0303 m B ). It follows that FB(P\u0303nB) \u2265 FB(P\u0303mB ) (cf. [PAM11]). Consequently, with FB(P\u0303mB ) being more negative it follows that the fixed point m constitutes the global minimum of the Bethe free energy.\nThat is, SBP proceeds along a solution path that leads towards the global minimum of the Bethe approximation. In particular, by considering the fact that the exact free energy is upper bounded by the Bethe approximation for attractive models [Ruo13], this implies that the fixed\n\u2013 140 \u2013 November 8, 2019\nA.1 Proofs from Chapter 6\npoint obtained by SBP indeed minimizes the approximation error, i.e, m = arg minm\u2208MEZ(m) holds.\nThis concurrently implies that P\u0303mB (\u03b6) is optimal with respect to marginal accuracy, i.e., no stable fixed point \u2013 which corresponds to a local minimum of the Bethe free energy \u2013 exists that provides more accurate marginals. Note that attractive models exhibit so-called replica symmetric solutions where the Bethe free energy has two minima at most. In particular, this allows one to express the exact marginals as a convex combination of all fixed points, i.e, PB =\n1\u2211 ZB\u2208M\nZB \u2211\n(P\u0303B ,ZB)\u2208M P\u0303B \u00b7 ZB (cf. Section 3.5.4).\nIt follows by the existence of at most two solutions that the fixed point that minimizes the Bethe free energy is also more accurate.\nNovember 8, 2019 \u2013 141 \u2013\nA.2 Proofs from Chapter 7\nA.2 Proofs from Chapter 7\nThis Section contains all the detailed proofs for Chapter 7\nA.2.1 Proof of Theorem 11\nHere we show how all incoming messages from outside the patch can be subsumed into an effective local field.\nProof. First let us revisit the update equation from Xi to Xj : Xj \u2208 Xi\n\u00b5n+1ij (xj) \u221d \u2211 xi\u2208X \u03a6(xi, xj)\u03a6(xi) \u220f Xk\u2208{\u2202(i)\\Xj} \u00b5nki(xi).\nNow we group the incoming messages into two groups, i.e., messages coming from outside the patch and messages coming from inside the patch so that\n\u00b5n+1ij (xj) \u221d \u2211 xi\u2208X \u03a6(xi, xj)\u03a6(xi) \u220f Xk\u2208\u2202(i)\\{Xj\u2229Xi} \u00b5nki(xi) \u220f Xk\u2208{Xi\u2229\u2202(i)\\Xj} \u00b5nki(xi).\nNow we make use of the fact that we are only dealing with binary random variables for which \u00b5ij(Xj = \u22121) = (1\u2212 \u00b5ij(Xj = 1)) and express the message explicitly by\n\u00b5n+1ij (xj) \u221d exp(Jxj) exp(\u03b8i) \u220f\nXk\u2208\u2202(i)\\{Xj\u2229Xi}\n\u00b5nki(Xi = 1) \u220f\nXk\u2208{Xi\u2229\u2202(i)\\Xj}\n\u00b5nki(Xi = 1)\n+ exp(\u2212Jxj) exp(\u2212\u03b8i) \u220f\nXk\u2208\u2202(i)\\{Xj\u2229Xi}\n(1\u2212 \u00b5nki(Xi = 1)) \u220f\nXk\u2208{Xi\u2229\u2202(i)\\Xj}\n(1\u2212 \u00b5nki(Xi = 1)).\n(A.11)\nWe now want to get rid of the first product and absorb the influence of these messages into the local field. In particular we aim to express it according to\nexp ( \u03b8\u0303ixi) = \u03a6(xi) \u220f Xk\u2208\u2202(i)\\{Xj\u2229Xi} \u00b5nki(xi)\n= exp ( (\u03b8i + c)xi ) \u00b7 exp(g). (A.12)\nIn order to do so we take the logarithm of the product over all messages in (A.12) and put them into the exponent with the local field so that for xi = +1\nexp(\u03b8\u0303i) = exp ( \u03b8i + \u2211 Xk\u2208\u2202(i)\\{Xj\u2229Xi} log ( \u00b5ki(Xi = 1) )) = exp ( \u03b8i +\n\u2211 Xk\u2208\u2202(i)\\{Xj\u2229Xi} ci + \u2211 Xk\u2208\u2202(i)\\{Xj\u2229Xi} gi ) , (A.13)\nand for xi = \u22121\nexp(\u2212\u03b8\u0303i) = exp ( \u2212 \u03b8i + \u2211 Xk\u2208\u2202(i)\\{Xj\u2229Xi} log ( 1\u2212 (\u00b5ki(Xi = 1) )) = exp ( \u2212 \u03b8i \u2212\n\u2211 Xk\u2208\u2202(i)\\{Xj\u2229Xi} ci + \u2211 Xk\u2208\u2202(i)\\{Xj\u2229Xi} gi ) . (A.14)\nNovember 8, 2019 \u2013 143 \u2013\nA Appendix\nEquating the coefficients for ci and gi in (A.13) and (A.14) gives us the final results:\nci = 1\n2\n( log\u00b5ki(Xi = 1)\u2212 log ( 1\u2212 \u00b5ki(Xi = 1) )) ,\nc = \u2211\nXk\u2208\u2202(i)\\{Xj\u2229Xi}\natanh ( 2\u00b5ki(Xi = 1)\u2212 1 ) , (A.15)\ng = 1\n2 log \u220f Xk\u2208\u2202(i)\\{Xj\u2229Xi} ( \u00b5ki(Xi = 1)\u2212 \u00b5ki(Xi = 1)2 ) . (A.16)\nNote that we can further express the message for the second state \u00b5ij(Xj = \u22121) in a similar way, with the only difference that the values of the pairwise potentials change. Consequently we get exactly the same result for g again; this allows us to neglect the influence of g altogether, as it will be canceled out when normalizing the messages so that the sum up to one.\nA.2.2 Proof of Corollary 15.1\nWe want to compare error of the state-preserving fixed point EP (p) to the error EP (q) of a fixed point that has all marginals biased towards one state. We already discussed the error-ratio in a general manner in the proof of Theorem 15 in Section 7.4.3; here we provide a more accessible proof for the special case of a model with two patches, i.e., for Example 10.\nProof. For a symmetric model with two equal-sized patches we evaluate the error ratio between the state-preserving fixed point p and one of the fixed points that have all marginals biased towards one state, these are q and r and have symmetric marginals. Further assume that the fixed points q and r minimize the Bethe free energy, i.e.,\nFBq = FBr < FBp, (A.17) ZqB = ZrB > Z p B. (A.18)\nThen we want to show that (A.18) does not imply that EP (q) < EP (p), i.e., we want to show that EP (p)EP (q) < 1 despite (A.18); therefore, we express the ratio of the marginal errors according to\nEP (p) EP (q) =\n\u2211 Xi |\u2211m\\pZmBQi(m, p)|2\u2211\nXi |\u2211m\\q ZmBQi(m, q)|2\n= \u2211 Xi |ZqBQi(q, p) + ZrBQi(r, p)|2\u2211\nXi |ZqBQi(r, q) + Z p BQi(p, q)|2\n. (A.19)\nNote that because of (A.18) we have\nEP (p) EP (q) =\n\u2211 Xi |ZqB ( Qi(q, p) +Qi(r, p) ) |2\u2211\nXi |ZqBQi(r, q) + Z p BQi(p, q)|2\n(a) =\n\u2211 Xi |ZqB ( Qi(q, p) +Qi(r, p) ) |2\u2211\nXi ( ZqBQi(q, r) + Z p BQi(q, p) )2 . (A.20) where (a) follows from the fact that Qi(r, q) < 0, Qi(p, q) < 0, and the symmetry property (7.17). We further denote the constant difference between the biased fixed points by\n0 < Qi(q, r) = d < 1. (A.21)\n\u2013 144 \u2013 November 8, 2019\nA.2 Proofs from Chapter 7\nWe can use the expansion property (7.18) to bound the numerator as |Qi(q, p) + Qi(r, p)|2 < Qi(q, r) 2 = d2 so that\nEP (p) EP (q) <\n\u2211 Xi ZqBd2\u2211\nXi ( ZqBd+ Z p BQi(q, p) )2 . (A.22) Which completes the proof as Qi(q, p) > 0\nA.2.3 Proof of Theorem 16\nLet us consider three different stationary points FBp (state-preserving), FBq (biased to one state), and FBm that has some patches flipped. Note that we consider FBq as a limiting case for FBm. We will denote the number of variables that are aligned with the local field Nc and the number of flipped variables Nf .\nThe set of boundary edges that connects two patches is denoted by\nEP = {(i, j) \u2208 E : Xi \u2208 Xi, Xj \u2208 Xj 6= Xi}, (A.23)\nand the set of edges that connects two patches that have their variables not aligned is denoted by\nEC = {(i, j) \u2208 E : Xi \u2208 Xi, Xj \u2208 Xj 6= Xi, sgn ( P\u0303Xi(Xi = 1)\u2212 0.5 ) 6= sgn ( P\u0303Xj (Xj = 1)\u2212 0.5 ) }.\n(A.24)\nNote that EP is constant for a specified model, whereas EC depends on the specific fixed point. We consequently have EC \u2264 EP with equality for the state preserving fixed point p.\nOur analysis is restricted to (\u03b8, J) \u2208 (II) per definition: one crucial consequence is that J > JA(\u03b8) and that most marginals either have P\u0303Xi(xi) \u2248 1 or P\u0303Xi(xi) \u2248 0. We will exploit this fact and express all marginals according to P\u0303Xi(xi) \u2208 {0, 1} which allows us to simplify FB, as defined in Section 3.3, according to\nFBp = \u2212N\u03b8 \u2212 ( |E| \u2212 2|EP | ) J \u2212 SpB (A.25)\nFBm = \u2212 ( Nc \u2212Nf ) \u03b8 \u2212 ( |E| \u2212 2|EC | ) J \u2212 SmB . (A.26)\nLet \u2206SB = S p B \u2212SmB be the difference in the entropy, then we can express the conditions for the state-preserving fixed point to have a lower value FBp \u2264 FBm according to\n\u2212N\u03b8 \u2212 ( |E| \u2212 2|EP | ) J \u2264 \u2212 ( Nc \u2212Nf ) \u03b8 \u2212 ( |E| \u2212 2|EC | ) J + \u2206SB\n2J(\u2212|EC |+ |EP |) \u2264 \u03b8(N \u2212Nc +Nf ) + \u2206SB (A.27)\nNow let us express (A.27) for the fixed points that has all variables biased to one state, i.e., |EC | = 0. Then, the state-preserving fixed point has a lower value FBp < FBq if\n2J |EP | \u2264 \u03b8(N \u2212Nc +Nf ) + \u2206SB (A.28)\nNovember 8, 2019 \u2013 145 \u2013\nA Appendix\nA.3 Pseudocode\nWe present the pseudocode for NIBP and WDBP. Removing the if then else clause in line 8 to 11 of NIBP and substituting it with \u00b5nm \u2190 \u00b5n+1m reduces Algorithm 3 to RBP. The maximum number of iterations is denoted by NBP = 2.5 \u00b7 105 and = 10\u22123. NrOfMessages = 2|E| denotes the overall number of messages in the graph.\nAlgorithm 3: Noise Injection Belief Propagation (NIBP)\ninput : Graph G = (X,E) output: Converged messages \u00b5n\n1 initialization\n2 for m\u2190 1 to NrOfMessages do 3 \u00b5n+1m \u2190 ComputeUpdate(\u00b5n) 4 rm \u2190 |\u00b5nm \u2212 \u00b5n+1m | 5 k \u2190 1 6 while n < NBP and max |\u00b5n \u2212 \u00b5n+1| > do 7 m\u2190 arg maxm rn 8 if OscillationDetection(\u00b5nm,L) then 9 \u00b5nm \u2190 \u00b5n+1m +N (0, \u03c3)\n10 else 11 \u00b5nm \u2190 \u00b5n+1m 12 for j \u2190 1 to NrOfMessages do 13 \u00b5n+1j \u2190 ComputeUpdate(\u00b5n) 14 rj \u2190 |\u00b5n+1j \u2212 \u00b5nj | 15 n = n+ 1\nAlgorithm 4: Weight Decay Belief Propagation (WDBP)\ninput : Graph G = (X,E) output: Converged messages \u00b5n\n1 initialization\n2 for m\u2190 1 to NrOfMessages do 3 \u00b5n+1m \u2190 ComputeUpdate(\u00b5n) 4 rm \u2190 |\u00b5nm \u2212 \u00b5n+1m | 5 NrUpdates (m) \u2190 1 6 k \u2190 1 7 while n < NBP and max |\u00b5n \u2212 \u00b5n+1| > do 8 m\u2190 arg maxm rn 9 \u00b5nm \u2190 \u00b5n+1m\n10 NrUpdates (m) \u2190 NrUpdates (m) + 1 11 for j \u2190 1 to NrOfMessages do 12 \u00b5n+1j \u2190 ComputeUpdate(\u00b5n) 13 rj \u2190 |\u00b5n+1j \u2212\u00b5 n j |\nNrUpdates(j)\n14 n = n+ 1\n\u2013 146 \u2013 November 8, 2019\nUnderstanding the Behavior of Belief Propagation\nB Guide to Quotes\nThe attentive reader has probably recognized that this thesis features an opening quote for every chapter. It was important to me to connect with all the quotes on a personal level (be it by one\u2019s art or by a general appreciation of a person\u2019s life) Yet, this was not enough: every quote must also have a close connection to the respective chapter. This connection, however, may not always be as obvious to the reader as it was to me. It is precisely for this reason that I will now reveal the perceived meaning of each quote, and how I see it connect to the content of the chapter.\nChapter 1: We quote the commonly used English translation \u201cUncertainty is an uncomfortable position. But certainty is an absurd one.\u201d of the original line \u201cLe doute n\u2019est pas une e\u0301tat bien agre\u0301able, mais l\u2019assurance est un e\u0301tat ridicule\u201d [Vol17, p.703]. This statement from Voltaire elegantly connects to the central concept of this thesis, probabilistic graphical model, as it can be interpreted as a praise for probability.\nChapter 2: \u201cA mind is like a parachute. It doesn\u2019t work if it is not open.\u201d This quote by Frank Zappa seems to align nicely with a background-chapter that brings different scientific fields together into one powerful concept. In particular, since it was Frank Zappa\u2019s ability to draw inspiration from an unimaginable wide range of musical genres that finally coalesced together and created some stunning albums.\nChapter 3: Ludwig Boltzmann played a formative role in the foundational years of statistical physics and laid out the groundwork for many concepts discussed in this chapter. Beyond that, there is undeniable truth in his statement \u201cThe stars bend like slaves to laws not decreed for them by human intelligence, but gleaned from them.\u201d quoted from [GRE91]. One could argue that this whole thesis adheres to this statement as we put considerable effort into carefully studying the behavior of belief propagation to develop a clearer picture of its underlying working-principles.\nChapter 4: \u201cInside a broken clock; Splashing the wine; With all the rain dogs; Taxi, we\u2019d rather walk...\u201d [Wai85]. The song figuratively refers to the rain dogs as the straying dogs that are seemingly lost after the rain has washed away all their scent. The underlying meaning is elegantly encompassed by just another quote of Tom Waits that states: \u201cWe are buried beneath the weight of information, which is being confused with knowledge; quantity is being confused with abundance and wealth with happiness.\u201d. Although arguable more general and touching a couple of interesting points of today\u2019s society, Tom Waits\u2019 statement reflects the experiences one makes when acquainting oneself with the subtleties of a new field. When embarking into uncharted territory and trying to grasp all impressions one can only sympathize with the rain dogs that must feel similar.\nChapter 5: \u201cTravel makes one modest. You see what a tiny place you occupy in the world.\u201d [Fla96]. Flaubert urges the need for always taking a look at the surrounding to put things into perspective. BP often behaves like a creature of habit, once it decides for one specific fixed point it neglects all others. In this chapter we break up with this habit and take a global perspective on the solution space, essentially traveling through the solution space.\nNovember 8, 2019 \u2013 147 \u2013\nB Guide to Quotes\nChapter 6: \u201cThe most significant dimension of freedom is the freedom from one\u2019s own ego - in other words, from the feeling that I am the center of everything.\u201d Voytek Kurtyka is an outstanding mountaineer that played an important role in bringing the modern alpine-style climbing into the greater ranges. More importantly, he was always honest about his inner struggle between the desire to nourish one\u2019s ego and the awareness that there is no inbred reason to take oneself to serious. One could argue that our proposed algorithm, self-guided BP, undergoes a similar struggle. At the beginning, all variables take up a purely ego-centered standpoint and no interactions between them take place. Only incrementally, as the couplings are incorporate, the variables contribute to the joint assignment of the model and put their own constraints behind.\nChapter 7: \u201cTo the wise, life is a problem; to the fool, a solution.\u201d Wise and fool are maybe strong words; but one could have been satisfied with the preceding chapters and could have neglected the unanswered questions lingering around instead of being curious and trying to gain some further insights. On a side-node, Marcus Aurelius was a great proponent of the philosophy of stoicism, a philosophy that is arguable beneficial when working on a thesis for many years.\nChapter 8: \u201cThere must be some way out of here; Said the joker to the thief\u201c [Dyl67]. This song puts you in the midst of a discussion, with the joker struggling with his purpose. Even though it was inevitably to experience some struggles in the process of writing everything up, there is no easy way out and, ultimately, it is just a question of getting things done before one can appreciate the completed task.\n\u2013 148 \u2013 November 8, 2019\nUnderstanding the Behavior of Belief Propagation\nBibliography\n[ACSW18] Sungsoo Ahn, Michael Chertkov, Jinwoo Shin, and Adrian Weller. Gauged mini-bucket elimination for approximate inference. Proceedings of AISTATS, 2018.\n[AG03] E. Allgower and K. Georg. Introduction to Numerical Continuation Methods. Society for Industrial and Applied Mathematics, 2003.\n[AM00] Srinivas M Aji and Robert J McEliece. The generalized distributive law. IEEE Transactions on Information Theory, 46(2):325\u2013343, 2000.\n[AS88] Winfried Auzinger and Hans J Stetter. An elimination algorithm for the computation of all zeros of a system of multivariate polynomial equations. In Proceedings of International Conference on Numerical Mathematics, pages 11\u201330. Springer, 1988.\n[AS89] Winfried Auzinger and Hans J Stetter. A study of numerical elimination for the solution of multivariate polynomial systems. Technical report, Institut fu\u0308r Angwandte und Numerische Mathematik, Vienna University of Technology, 1989.\n[Ber75] David N. Bernstein. The number of roots of a system of equations. Functional Analysis and Its Applications, 9:1\u20134, 1975.\n[Bet35] Hans A Bethe. Statistical theory of superlattices. Proceedings of the Royal Society of London. Series A-Mathematical and Physical Sciences, 150(871):552\u2013 575, 1935.\n[BF11] Richard L Burden and Douglas J Faires. Numerical Analysis. Brooks Cole Cengage, 9 edition, 2011.\n[BGT93] Claude Berrou, Alain Glavieux, and Punya Thitimajshima. Near Shannon limit error-correcting coding and decoding: Turbo-codes. 1. In Proceedings of IEEE International Conference on Communications, volume 2, pages 1064\u2013 1070, 1993.\n[BH11] Andries E Brouwer and Willem H Haemers. Spectra of Graphs. Springer, 2011.\n[BHSW] Daniel J. Bates, Jonathan D. Hauenstein, Andrew J. Sommese, and Charles W. Wampler. Bertini: Software for numerical algebraic geometry. Available at bertini.nd.edu with permanent doi: dx.doi.org/10.7274/R0H41PB5.\n[Bin87] K Binder. Finite size effects on phase transitions. Ferroelectrics, 73(1):43\u201367, 1987.\n[Bis06] Christopher Bishop. Pattern Recognition and Machine Learning. Springer, 2006.\n[BKMZ07] Alfredo Braunstein, Farbod Kayhan, Guido Montorsi, and Riccardo Zecchina. Encoding for the blackwell channel with reinforced belief propagation. In IEEE International Symposium on Information Theory, 2007.\n[BKvdEvdG01] Hans L Bodlaender, Arie MCA Koster, Frank van den Eijkhof, and Linda C van der Gaag. Pre-processing for triangulation of probabilistic networks. In Proceedings of UAI, pages 32\u201339, 2001.\nNovember 8, 2019 \u2013 149 \u2013\nBibliography\n[BMZ05] A Braunstein, M Me\u0301zard, and R Zecchina. Survey propagation: An algorithm for satisfiability. Random Structures & Algorithms, 27(2):201\u2013226, 2005.\n[Bru67] Stephen G Brush. History of the Lenz-Ising model. Reviews of Modern Physics, 39(4):883, 1967.\n[Buc65] Bruno Buchberger. Ein Algorithmus zum Auffinden der Basiselemente des Restklassenringes nach einem nulldimensionalen Polynomideal. PhD thesis, University of Innsbruck, 1965.\n[Buc06] Bruno Buchberger. Bruno buchberger\u2019s phd thesis 1965: An algorithm for finding the basis elements of the residue class ring of a zero dimensional polynomial ideal. Journal of Symbolic Computation, 41(3-4):475\u2013511, 2006.\n[BYGRS12] D Batra, P Yadollahpour, A Guzman-Rivera, and G Shakhnarovich. Diverse m-best solutions in markov random fields. In Computer Vision\u2013ECCV 2012, pages 1\u201316. Springer, 2012.\n[CC06] Michael Chertkov and Vladimir Y Chernyak. Loop series for discrete statistical models on graphs. Journal of Statistical Mechanics: Theory and Experiment, (06), 2006.\n[CCG+11] Venkat Chandrasekaran, Misha Chertkov, David Gamarnik, Devavrat Shah, and Jinwoo Shin. Counting independent sets using the Bethe approximation. SIAM Journal on Discrete Mathematics, 25(2):1012\u20131034, 2011.\n[CCT08] Michael Chertkov, Vladimir Y Chernyak, and Razvan Teodorescu. Belief propagation and loop series on planar graphs. Journal of Statistical Mechanics: Theory and Experiment, (05), 2008.\n[CL15] Tianran Chen and Tien-Yien Li. Homotopy continuation method for solving systems of nonlinear and polynomial equations. Communications in Information and System, 15(2):119\u2013307, 2015.\n[CLL14] Tianran Chen, Tsung-Lin Lee, and Tien-Yien Li. Hom4ps-3: A parallel numerical solver for systems of polynomial equations based on polyhedral homotopy continuation methods. In Proceedings of International Congress on Mathematical Software, pages 183\u2013190. Springer, 2014.\n[CLO92] D Cox, J Little, and D O\u2019Shea. Ideals, Varieties, and Algorithms, volume 3. Springer, 1992.\n[CLO05] D Cox, J Little, and D O\u2019Shea. Using Algebraic Geometry, volume 185. Springer, 2005.\n[CM17] T Chen and D Mehta. On the network topology dependent solution count of the algebraic load flow equations. IEEE Transactions on Power Systems, 2017.\n[CMN16] T Chen, D Mehta, and M Niemerg. A network topology dependent upper bound on the number of equilibria of the Kuramoto model. arXiv preprint arXiv:1603.05905, 2016.\n[Coo90] Gregory F Cooper. The computational complexity of probabilistic inference using Bayesian belief networks. Artificial Intelligence, 42(2-3):393\u2013405, 1990.\n[COP19] Amin Coja-Oghlan and Will Perkins. Bethe states of random factor graphs. Communications in Mathematical Physics, 366(1):173\u2013201, 2019.\n\u2013 150 \u2013 November 8, 2019\nBibliography\n[DE05] Alicia Dickenstein and Ioannis Z Emiris. Solving Polynomial Equations. Springer, 2005.\n[Die16] Reinhard Diestel. Graph Theory. Springer, 2016.\n[DL93] Paul Dagum and Michael Luby. Approximating probabilistic inference in Bayesian belief networks is NP-hard. Artificial Intelligence, 60(1):141\u2013153, 1993.\n[DM10] Amir Dembo and Andrea Montanari. Ising models on locally tree-like graphs. The Annals of Applied Probability, 20(2):565\u2013592, 2010.\n[DR03] Rina Dechter and Irina Rish. Mini-buckets: A general scheme for bounded inference. Journal of the ACM, 50(2):107\u2013153, 2003.\n[Dyl67] Bob Dylan. All Along The Watchtower. Columbia Records, 1967.\n[EG13] Frederik Eaton and Zoubin Ghahramani. Model reductions for inference: Generality of pairwise, binary, and planar factor graphs. Neural Computation, 25(5):1213\u20131260, 2013.\n[EH16] Bradley Efron and Trevor Hastie. Computer Age Statistical Inference. Cambridge University Press, 2016.\n[EMK06] Gal Elidan, Ian McGraw, and Daphne Koller. Residual belief propagation: Informed scheduling for asynchronous message passing. In Proceedings of UAI, 2006.\n[Erd11] La\u0301szlo\u0301 Erdo\u0308s. Universality of wigner random matrices: a survey of recent results. Russian Mathematical Surveys, 66, 2011.\n[Fey72] Richard P. Feynman. Statistical Mechanics: A Set Of Lectures. Benjamin/Cummings, 1972.\n[Fis67] Michael E Fisher. Critical temperatures of anisotropic ising lattices. ii. general upper bounds. Physical Review, 162(2):480, 1967.\n[Fla96] Gustave Flaubert. Flaubert in Egypt: A Sensibility on Tour: A Narrative Drawn from Gustave Flaubert\u2019s Travel Notes & Letters. Penguin, 1996.\n[Ge\u030177] Toulouse Ge\u0301rard. Theory of the frustration effect in spin glasses. Communications on Physics, 2:115\u2013119, 1977.\n[Gal68] Robert G Gallager. Information Theory and Reliable Communication, volume 2. Springer, 1968.\n[Geo11] Hans O Georgii. Gibbs Measures and Phase Transitions, volume 9. Walter de Gruyter, 2011.\n[GJ07] Amir Globerson and Tommi S Jaakkola. Convergent propagation algorithms via oriented trees. In Proceedings of UAI, 2007.\n[GMK07] Vicenc\u0327 Go\u0301mez, Joris M Mooij, and Hilbert J Kappen. Truncating the loop series expansion for belief propagation. Journal of Machine Learning Research, 8, 2007.\n[GRE91] GEORGE GREENSTEIN. Science: The bulldog: A profile of ludwig boltzmann. The American Scholar, 60(1):97\u2013105, 1991.\nNovember 8, 2019 \u2013 151 \u2013\nBibliography\n[Gri67] Robert B Griffiths. Correlations in Ising ferromagnets. ii. external magnetic fields. Journal of Mathematical Physics, 8(3):484\u2013489, 1967.\n[Gro59] David M Grobman. Homeomorphism of systems of differential equations. Doklady Akademii Nauk SSSR, 128(5):880\u2013881, 1959.\n[Har60] Philip Hartman. A lemma in the theory of structural stability of differential equations. Proceedings of the American Mathematical Society, 11(4):610\u2013620, 1960.\n[HC71] John M Hammersley and Peter Clifford. Markov fields on finite graphs and lattices. Unpublished manuscript, 1971.\n[Hes03] Tom Heskes. Stable fixed points of loopy belief propagation are minima of the Bethe free energy. In Proceedings of NIPS, volume 15, pages 359\u2013366, 2003.\n[Hes04] Tom Heskes. On the uniqueness of loopy belief propagation fixed points. Neural Computation, 16(11), 2004.\n[Hop82] John J Hopfield. Neural networks and physical systems with emergent collective computational abilities. Proceedings of the National Academy of Sciences, 79(8):2554\u20132558, 1982.\n[HS86] Geoffrey E Hinton and Terrence J Sejnowski. Learning and relearning in boltzmann machines. Parallel Distributed Processing: Explorations in the Microstructure of Cognition, 1(282-317):2, 1986.\n[HS95] Birkett Huber and Bernd Sturmfels. A polyhedral method for solving sparse polynomial systems. Mathematics of Computation, 64(212):1541\u20131555, 1995.\n[HS08] Tamir Hazan and Amnon Shashua. Convergent message-passing algorithms for inference over general graphs with convex free energies. In Proceedings of UAI, 2008.\n[Hua63] Kerson Huang. Statistical Mechanics. John Wiley & Sons Inc, 1963.\n[IFW05] Alexander T Ihler, John W Fisher, and Alan S Willsky. Loopy belief propagation: Convergence and effects of message errors. In Journal of Machine Learning Research, pages 905\u2013936, 2005.\n[Ihl] Alexander T Ihler. personal communication, UAI, 2017-08-12.\n[Ihl05] Alexander T Ihler. Inference in Sensor Networks: Graphical Models and Particle Methods. PhD thesis, MIT, 2005.\n[Ihl07] Alexander T Ihler. Accuracy bounds for belief propagation. In Proceedings of UAI, pages 183\u2013190, July 2007.\n[JGJS99] Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction to variational methods for graphical models. Machine Learning, 37(2):183\u2013233, 1999.\n[JJ97] Tommi Jaakkola and Michael I Jordan. Recursive algorithms for approximating probabilities in graphical models. In Proceedings of NIPS, pages 487\u2013493, 1997.\n[Kal60] Rudolph E Kalman. A new approach to linear filtering and prediction problems. Journal of Basic Engineering, 82(1):35\u201345, 1960.\n\u2013 152 \u2013 November 8, 2019\nBibliography\n[KF98] Frank R. Kschischang and Brendan J. Frey. Iterative decoding of compound codes by probability propagation in graphical models. IEEE Journal on Selected Areas in Communications, 16(2):219\u2013230, 1998.\n[KF09] Daphne Koller and Nir Friedman. Probabilistic Graphical Models: Principles and Techniques. MIT press, 2009.\n[KFD+07] Daphne Koller, Nir Friedman, Sas\u030co Dz\u030ceroski, Charles Sutton, Andrew McCallum, Avi Pfeffer, Pieter Abbeel, Ming-Fai Wong, David Heckerman, Chris Meek, et al. Introduction to Statistical Relational Learning. 2007.\n[KFL01] Frank R Kschischang, Branden J Frey, and Hans A Loeliger. Factor graphs and the sum-product algorithm. IEEE Transactions on Information Theory, 47(2):498\u2013519, 2001.\n[Kho78] Askold G Khovanski. Newton polyhedra and the genus of complete intersections. Funkts. Anal. Pril., 12(1):51\u201361, 1978.\n[KKP18] Christian Knoll, Florian Kulmer, and Franz Pernkopf. Self-guided belief propagation\u2013a homotopy continuation method. arXiv preprint arXiv:1812.01339, 2018.\n[KKR05] Achim Kehrein, Martin Kreuzer, and Lorenzo Robbiano. An algebraist\u2019s view on border bases. In Solving Polynomial Equations, pages 169\u2013202. Springer, 2005.\n[KMCP18] Christian Knoll, Dhagash Mehta, Tianran Chen, and Franz Pernkopf. Fixed points of belief propagation \u2013 an analysis via polynomial homotopy continuation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018.\n[Kol06] Vladimir Kolmogorov. Convergent tree-reweighted message passing for energy minimization. IEEE transactions on Pattern Analysis and Machine Intelligence, 28(10):1568\u20131583, 2006.\n[KP17] Christian Knoll and Franz Pernkopf. On loopy belief propagation \u2013 local stability analysis for non-vanishing fields. In Proceedings of UAI, 2017.\n[KP19] Christian Knoll and Franz Pernkopf. Belief propagation: Accurate marginals or accurate partition function \u2013 where is the difference? In Proceedings of UAI, 2019.\n[KPMC16] Christian Knoll, Franz Pernkopf, Dhagash Mehta, and Tianran Chen. Fixed point solutions of belief propagation. In NIPS-Workshop: Advances in Approximate Bayesian Inference, 2016.\n[KR00] Martin Kreuzer and Lorenzo Robbiano. Computational Commutative Algebra 1, volume 1. Springer, 2000.\n[KR05] Martin Kreuzer and Lorenzo Robbiano. Computational Commutative Algebra 2, volume 2. Springer, 2005.\n[KRTP15] Christian Knoll, Michael Rath, Sebastian Tschiatschek, and Franz Pernkopf. Message scheduling methods for belief propagation. In Proceedings of ECML PKDD, pages 295\u2013310. Springer, 2015.\n[Kus76] AG Kushnirenko. Newton polytopes and the Bezout theorem. Funkts. Anal. Pril., 10(3), 1976.\nNovember 8, 2019 \u2013 153 \u2013\nBibliography\n[KV05] Bernhard Korte and Jens Vygen. Combinatorial Optimization, volume 3. Springer, 2005.\n[LCMRTR13] Alejandro Lage-Castellanos, Roberto Mulet, Federico Ricci-Tersenghi, and Tommaso Rizzo. Replica cluster variational method: the replica symmetric solution for the 2d random bond ising model. Journal of Physics A: Mathematical and Theoretical, 46, 2013.\n[Li97] Tien-Yien Li. Numerical solution of multivariate polynomial systems by homotopy continuation methods. Acta numerica, 6:399\u2013436, 1997.\n[Li03] Tien-Yien Li. Solving polynomial systems by the homotopy continuation method. Handbook of Numerical Analysis, 11:209\u2013304, 2003.\n[LI11] Qiang Liu and Alexander T Ihler. Bounding the partition function using Ho\u0308lder\u2019s inequality. In Proceedings of ICML, pages 849\u2013856, 2011.\n[LK03] Martijn Leisink and Hilbert J Kappen. Bound propagation. Journal of Artificial Intelligence Research, 19:139\u2013154, 2003.\n[Loe04] Hans A Loeliger. An introduction to factor graphs. IEEE Signal Processing Magazine, 21(1):28\u201341, 2004.\n[LS88] Steffen L Lauritzen and David J Spiegelhalter. Local computations with probabilities on graphical structures and their application to expert systems. Journal of the Royal Statistical Society. Series B, pages 157\u2013224, 1988.\n[Mac01] David JC MacKay. A conversation about the Bethe free energy and sumproduct. Technical Report of MERL., 2001.\n[Mac03] David JC MacKay. Information Theory, Inference and Learning Algorithms. Cambridge University Press, 2003.\n[MGW09] Talya Meltzer, Amir Globerson, and Yair Weiss. Convergent message passing algorithms: a unifying view. In Proceedings of UAI, pages 393\u2013401, 2009.\n[MJGF09] Ofer Meshi, Ariel Jaimovich, Amir Globerson, and Nir Friedman. Convexifying the Bethe free energy. In Proceedings of UAI. AUAI Press, 2009.\n[MK05] Joris M Mooij and Hilbert J Kappen. On the properties of the Bethe approximation and loopy belief propagation on binary networks. Journal of Statistical Mechanics: Theory and Experiment, 2005(11):P11012, 2005.\n[MK07] Joris M Mooij and Hilbert J Kappen. Sufficient conditions for convergence of the sum\u2013product algorithm. IEEE Transactions on Information Theory, 53(12):4422\u20134437, 2007.\n[MK09] Joris M Mooij and Hilbert J Kappen. Bounds on marginal probability distributions. In Proceedings of NIPS, pages 1105\u20131112, 2009.\n[MLF11] Victorin Martin, Jean-Marc Lasgouttes, and Cyril Furtlehner. The role of normalization in the belief propagation algorithm. arXiv preprint arXiv:1101.4170, 2011.\n[MM82] Ernst W Mayr and Albert R Meyer. The complexity of the word problems for commutative semigroups and polynomial ideals. Advances in Mathematics, 46(3):305\u2013329, 1982.\n\u2013 154 \u2013 November 8, 2019\nBibliography\n[MM84] H Michael Mo\u0308ller and Ferdinando Mora. Upper and lower bounds for the degree of Gro\u0308bner bases. EUROSAM 84, pages 172\u2013183, 1984.\n[MM09] Marc Mezard and Andrea Montanari. Information, Physics, and Computation. Oxford University Press, 2009.\n[Mo\u0308l93] H Michael Mo\u0308ller. Systems of algebraic equations solved by means of endomorphisms. In International Symposium on Applied Algebra, Algebraic Algorithms, and Error-Correcting Codes, pages 43\u201356. Springer, 1993.\n[Moo08] Joris Marten Mooij. Understanding and Improving Belief Propagation. PhD thesis, Radboud University Nijmegen, 2008.\n[Mou07] Bernard Mourrain. Pythagore\u2019s dilemma, symbolic-numeric computation, and the border basis method. In Symbolic-Numeric Computation, pages 223\u2013243. Springer, 2007.\n[MPV87] Marc Mezard, Giorgio Parisi, and Miguel Virasoro. Spin Glass Theory and Beyond: An Introduction to the Replica Method and Its Applications, volume 9. World Scientific Publishing Co Inc, 1987.\n[MS95] H Michael Mo\u0308ller and Hans J Stetter. Multivariate polynomial equations with multiple zeros solved by matrix eigenproblems. Numerische Mathematik, 70(3):311\u2013329, 1995.\n[Mur12] Kevin P Murphy. Machine Learning: A Probabilistic Perspective. MIT press, 2012.\n[MWJ99] Kevin P Murphy, Yair Weiss, and Michael I Jordan. Loopy belief propagation for approximate inference: An empirical study. In Proceedings of UAI, pages 467\u2013475, 1999.\n[OW01] Manfred Opper and Ole Winther. Tractable approximations for probabilistic models: the adaptive Thouless-Anderson-Palmer mean field approach. Physical Review Letters, 86(17):3695, 2001.\n[PA02] Payam Pakzad and Venkat Anantharam. Belief propagation and statistical physics. In Proceedings of Conference on Information Sciences and Systems, 2002.\n[PAM11] Xaq Pitkow, Yashar Ahmadian, and Ken D Miller. Learning unbelievable probabilities. In Proceedings of NIPS, pages 738\u2013746, 2011.\n[Pea88] Judea Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann Publishers, 1988.\n[Pei36] R Peierls. Statistical theory of superlattices with unequal concentrations of the components. Proceedings of the Royal Society of London. Series A-Mathematical and Physical Sciences, 154(881):207\u2013222, 1936.\n[PI17] Wei Ping and Alexander Ihler. Belief propagation in conditional rbms for structured prediction. Proceedings of AISTATS, 2017.\n[PPT14] F Pernkopf, R Peharz, and S Tschiatschek. Introduction to Probabilistic Graphical Models. Academic Press\u2019 Library in Signal Processing, 2014.\n[RG14] Siamak Ravanbakhsh and Russell Greiner. Revisiting algebra and complexity of inference in graphical models. arXiv preprint arXiv:1409.7410, 2014.\nNovember 8, 2019 \u2013 155 \u2013\nBibliography\n[RG15] Siamak Ravanbakhsh and Russell Greiner. Perturbed message passing for constraint satisfaction problems. Journal of Machine Learning Research, 16:1249\u2013 1274, 2015.\n[RKDW10] Bjo\u0308rn S Ru\u0308ffer, Christopher M Kellett, Peter M Dower, and Steven R Weller. Belief propagation as a dynamical system: The linear case and open problems. IET Control Theory & Applications, 4(7):1188\u20131200, 2010.\n[Rot96] Dan Roth. On the hardness of approximate reasoning. Artificial Intelligence, 82(1-2):273\u2013302, 1996.\n[Ruo12] Nicholas Ruozzi. The Bethe partition function of log-supermodular graphical models. In Proceedings of NIPS, pages 117\u2013125, 2012.\n[Ruo13] Nicholas Ruozzi. Beyond log-supermodularity: lower bounds and the Bethe partition function. In Proceedings of UAI, pages 546\u2013555, 2013.\n[Sch00] Edward R. Scheinerman. Invitation to Dynamical Systems. Prentice Hall, 2000.\n[Shi12] Jinwoo Shin. Complexity of Bethe approximation. In Proceedings of AISTATS, pages 1037\u20131045, 2012.\n[SJ08] David Sontag and Tommi S Jaakkola. New outer bounds on the marginal polytope. In Proceedings of NIPS, pages 1393\u20131400, 2008.\n[SKZ14] Alaa Saade, Florent Krzakala, and Lenka Zdeborova\u0301. Spectral clustering of graphs with the Bethe Hessian. In Proceedings of NIPS, pages 406\u2013414, 2014.\n[SKZ17] Alaa Saade, Florent Krzakala, and Lenka Zdeborova\u0301. Spectral bounds for the ising ferromagnet on an arbitrary given graph. Journal of Statistical Mechanics, 2017(5), 2017.\n[SM07] Charles Sutton and Andrew McCallum. Improved dynamic schedules for belief propagation. In Proceedings of UAI, 2007.\n[SRF16] Christopher Srinivasa, Siamak Ravanbakhsh, and Brendan Frey. Survey propagation beyond constraint satisfaction problems. In Proceedings of AISTATS, pages 286\u2013295, 2016.\n[Ste04] Hans J Stetter. Numerical Polynomial Algebra, volume 85. SIAM, 2004.\n[Str16] Gilbert Strang. Introduction to Linear Algebra. Wellesley-Cambridge Press, 5 edition, 2016.\n[SW05] Andrew J Sommese and Charles W Wampler. The Numerical Solution of Systems of Polynomials Arising in Engineering and Science, volume 99. World Scientific, 2005.\n[Tes12] Gerald Teschl. Ordinary Differential Equations and Dynamical Systems. American Mathematical Society, 2012.\n[TJ02] Sekhar C Tatikonda and Michael I Jordan. Loopy belief propagation and Gibbs measures. In Proceedings of UAI, pages 493\u2013500, 2002.\n[TM06] Nobuyuki Taga and Shigeru Mase. Applications of Gibbs measure theory to loopy belief propagation algorithm. In Mexican International Conference on Artificial Intelligence, pages 197\u2013207, 2006.\n\u2013 156 \u2013 November 8, 2019\nBibliography\n[TR06] Peng Hui Tan and Lars K Rasmussen. Belief propagation for coded multiuser detection. In IEEE International Symposium on Information Theory, pages 1919\u20131923, 2006.\n[Ver99] Jan Verschelde. Algorithm 795: Phcpack: A general-purpose solver for polynomial systems by homotopy continuation. ACM Transactions on Mathematical Software (TOMS), 25(2):251\u2013276, 1999.\n[Vit67] Andrew Viterbi. Error bounds for convolutional codes and an asymptotically optimum decoding algorithm. IEEE Transactions on Information Theory, 13(2):260\u2013269, 1967.\n[Vol17] Voltaire. Oeuvres Comple\u0300tes. 1817.\n[Von13] Pascal O Vontobel. Counting in graph covers: A combinatorial characterization of the bethe entropy function. IEEE Transactions on Information Theory, 59(9):6018\u20136048, 2013.\n[Wai85] Tom Waits. Rain Dogs. Island Records, 1985.\n[Wai02] Martin Wainwright. Stochastic processes on graphs with cycles: geometric and variational approaches. PhD thesis, Massachusetts Institute of Technology, 2002.\n[Wat10] Yusuke Watanabe. Discrete geometric analysis of message passing algorithm on graphs. PhD thesis, The Graduate University for Advanced Studies, 2010.\n[WD16] Adrian Weller and Justin Domke. Clamping improves TRW and mean field approximations. In Artificial Intelligence and Statistics, pages 38\u201346, 2016.\n[Wei00] Yair Weiss. Correctness of local probability propagation in graphical models with loops. Neural Computation, 12(1), 2000.\n[Wel14] Adrian Weller. Methods for Inference in Graphical Models. PhD thesis, Columbia University, 2014.\n[Wel16] Adrian Weller. Uprooting and rerooting graphical models. In Proceedings of ICML, pages 21\u201329, 2016.\n[WF09] Yusuke Watanabe and Kenji Fukumizu. Graph zeta function in the Bethe free energy and loopy belief propagation. In Proceedings of NIPS, pages 2017\u20132025, 2009.\n[Wig58] Eugene P Wigner. On the distribution of the roots of certain symmetric matrices. The Annals of Mathematics, 67(2):325\u2013327, 1958.\n[WJ08] Martin J Wainwright and Michael I Jordan. Graphical models, exponential families, and variational inference. Foundations and Trends in Machine Learning, 1(1\u20132):1\u2013305, 2008.\n[WJ13] Adrian Weller and Tony Jebara. Bethe bounds and approximating the global optimum. In Proceedings of AISTATS, pages 618\u2013631, 2013.\n[WJ14a] Adrian Weller and Tony Jebara. Approximating the Bethe partition function. In Proceedings of UAI, 2014.\n[WJ14b] Adrian Weller and Tony Jebara. Clamping variables and approximate inference. In Proceedings of NIPS, pages 909\u2013917, 2014.\nNovember 8, 2019 \u2013 157 \u2013\nBibliography\n[WJW02] Martin J Wainwright, Tommi Jaakkola, and Alan S Willsky. Tree-based reparameterization for approximate inference on loopy graphs. In Proceedings of NIPS, pages 1001\u20131008, 2002.\n[WJW03a] Martin J Wainwright, Tommi S Jaakkola, and Alan S Willsky. Tree-based reparameterization framework for analysis of sum-product and related algorithms. IEEE Transactions on information theory, 49(5):1120\u20131146, 2003.\n[WJW03b] Martin J Wainwright, Tommi S Jaakkola, and Alan S Willsky. Tree-reweighted belief propagation algorithms and approximate ML estimation by pseudomoment matching. In Proceedings of AISTATS, 2003.\n[WJW05] Martin J Wainwright, Tommi S Jaakkola, and Alan S Willsky. A new class of upper bounds on the log partition function. IEEE Transactions on Information Theory, 51(7):2313\u20132335, 2005.\n[WSW08] Alan S Willsky, Erik B Sudderth, and Martin J Wainwright. Loop series and Bethe variational bounds in attractive graphical models. In Proceedings of NIPS, pages 1425\u20131432, 2008.\n[WT01] Max Welling and Yee Whye Teh. Belief optimization for binary networks: A stable alternative to loopy belief propagation. In Proceedings of UAI, pages 554\u2013561, 2001.\n[WT03] Max Welling and Yee Whye Teh. Approximate inference in Boltzmann machines. Artificial Intelligence, 143(1):19\u201350, 2003.\n[WTJS14] Adrian Weller, Kui Tang, Tony Jebara, and David Sontag. Understanding the Bethe approximation: When and how can it go wrong? In Proceedings of UAI, pages 868\u2013877, 2014.\n[Wym07] Henk Wymeersch. Iterative Receiver Design, volume 234. Cambridge University Press Cambridge, 2007.\n[YFW01] Jonathan S Yedidia, William T Freeman, and Yair Weiss. Generalized belief propagation. In Proceedings of NIPS, pages 689\u2013695, 2001.\n[YFW05] Jonathan S Yedidia, William T Freeman, and Yair Weiss. Constructing freeenergy approximations and generalized belief propagation algorithms. IEEE Transactions on Information Theory, 51(7):2282\u20132312, 2005.\n[You98] Allan Peter Young. Spin Glasses and Random Fields, volume 12. World Scientific, 1998.\n[YR03] Alan L Yuille and Anand Rangarajan. The concave-convex procedure. Neural Computation, 15(4):915\u2013936, 2003.\n[ZK16] Lenka Zdeborova\u0301 and Florent Krzakala. Statistical physics of inference: Thresholds and algorithms. Advances in Physics, 65(5):453\u2013552, 2016.\n\u2013 158 \u2013 November 8, 2019"
        }
    ],
    "year": 2022
}