{
    "abstractText": "This work investigates the importance of feature selection for improving the forecasting performance of machine learning algorithms for financial data. Artificial neural networks (ANN), convolutional neural networks (CNN), long-short term memory (LSTM) networks, as well as linear models were applied for forecasting purposes. The Feature Selection with Annealing (FSA) algorithm was used to select the features from about 1000 possible predictors obtained from 26 technical indicators with specific periods and their lags. In addition to this, the Boruta feature selection algorithm was applied as a baseline feature selection method. The dependent variables consisted of daily logarithmic returns and daily trends of ten financial data sets, including cryptocurrency and different stocks. Experiments indicate that the FSA algorithm increased the performance of ML models regardless of the problem type. The FSA hybrid machine learning models showed better performance in 10 out of 10 data sets for regression and 8 out of 10 data sets for classification. None of the hybrid Boruta models outperformed the hybrid FSA models. However, the BORCNN model performance was comparable to the best model for 4 out of 10 data sets for regression estimates. BOR-LR and BOR-CNN models showed comparable performance with the best hybrid FSA models in 2 out of 10 datasets for classification. FSA was observed to improve the model performance in both better performance metrics as well as a decreased computation time by providing a lower dimensional input feature",
    "authors": [
        {
            "affiliations": [],
            "name": "Hakan Pabuccu"
        },
        {
            "affiliations": [],
            "name": "Adrian Barbu"
        }
    ],
    "id": "SP:0f5045ccdfe5715ccdb320ec225768c336da7ce2",
    "references": [
        {
            "authors": [
                "E. Akyildirim",
                "A. Goncu",
                "A. Sensoy"
            ],
            "title": "Prediction of cryptocurrency returns using machine learning",
            "venue": "Annals of Operations Research, 297(1):3\u201336.",
            "year": 2021
        },
        {
            "authors": [
                "G.S. Atsalakis",
                "I.G. Atsalaki",
                "F. Pasiouras",
                "C. Zopounidis"
            ],
            "title": "Bitcoin price forecasting with neuro-fuzzy techniques",
            "venue": "European Journal of Operational Research, 276(2):770\u2013780.",
            "year": 2019
        },
        {
            "authors": [
                "M. Ballings",
                "D. Van den Poel",
                "N. Hespeels",
                "R. Gryp"
            ],
            "title": "Evaluating multiple classifiers for stock price direction prediction",
            "venue": "Expert systems with Applications, 42(20):7046\u20137056.",
            "year": 2015
        },
        {
            "authors": [
                "W. Bao",
                "J. Yue",
                "Y. Rao"
            ],
            "title": "A deep learning framework for financial time series using stacked autoencoders and long-short term memory",
            "venue": "PloS one, 12(7):e0180944.",
            "year": 2017
        },
        {
            "authors": [
                "A. Barbu",
                "Y. She",
                "L. Ding",
                "G. Gramajo"
            ],
            "title": "Feature selection with annealing for computer vision and big data learning",
            "venue": "IEEE transactions on pattern analysis and machine intelligence, 39(2):272\u2013286.",
            "year": 2016
        },
        {
            "authors": [
                "S.A. Basher",
                "P. Sadorsky"
            ],
            "title": "Forecasting bitcoin price direction with random forests: How important are interest rates, inflation, and market volatility? Machine Learning with Applications, page 100355",
            "year": 2022
        },
        {
            "authors": [
                "A. Bernal",
                "S. Fok",
                "R. Pidaparthi"
            ],
            "title": "Financial market time series prediction with recurrent neural networks",
            "venue": "State College: Citeseer.",
            "year": 2012
        },
        {
            "authors": [
                "T.A. Borges",
                "R.F. Neves"
            ],
            "title": "Ensemble of machine learning algorithms for cryptocurrency investment with different data resampling methods",
            "venue": "Applied Soft Computing, 90:106187.",
            "year": 2020
        },
        {
            "authors": [
                "Z. Chen",
                "C. Li",
                "W. Sun"
            ],
            "title": "Bitcoin price prediction using machine learning: An approach to sample dimension engineering",
            "venue": "Journal of Computational and Applied Mathematics, 365:112395.",
            "year": 2020
        },
        {
            "authors": [
                "K. Cho",
                "B. Van Merri\u00ebnboer",
                "C. Gulcehre",
                "D. Bahdanau",
                "F. Bougares",
                "H. Schwenk",
                "Y. Bengio"
            ],
            "title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
            "venue": "arXiv preprint arXiv:1406.1078.",
            "year": 2014
        },
        {
            "authors": [
                "L. Di Persio",
                "O. Honchar"
            ],
            "title": "Recurrent neural networks approach to the financial forecast of google assets",
            "venue": "International journal of Mathematics and Computers in simulation, 11:7\u201313.",
            "year": 2017
        },
        {
            "authors": [
                "E.F. Fama"
            ],
            "title": "Efficient capital markets: A review of theory and empirical work",
            "venue": "The journal of Finance, 25(2):383\u2013417.",
            "year": 1970
        },
        {
            "authors": [
                "F. Fang",
                "W. Chung",
                "C. Ventre",
                "M. Basios",
                "L. Kanthan",
                "L. Li",
                "F. Wu"
            ],
            "title": "Ascertaining price formation in cryptocurrency markets with machine learning",
            "venue": "The European Journal of Finance, pages 1\u201323.",
            "year": 2021
        },
        {
            "authors": [
                "I. Ghosh",
                "T.D. Chaudhuri",
                "E. Alfaro-Cort\u00e9s",
                "M. G\u00e1mez",
                "N. Gar\u0107\u0131a"
            ],
            "title": "A hybrid approach to forecasting futures prices with simultaneous consideration of optimality in ensemble feature selection and advanced artificial intelligence",
            "venue": "Technological Forecasting and Social Change, 181:121757.",
            "year": 2022
        },
        {
            "authors": [
                "A. Graves"
            ],
            "title": "Generating sequences with recurrent neural networks",
            "venue": "arXiv preprint arXiv:1308.0850.",
            "year": 2013
        },
        {
            "authors": [
                "Han",
                "J.-B.",
                "Kim",
                "S.-H.",
                "Jang",
                "M.-H.",
                "Ri",
                "K.-S."
            ],
            "title": "Using genetic algorithm and narx neural network to forecast daily bitcoin price",
            "venue": "Computational Economics, 56(2):337\u2013353.",
            "year": 2020
        },
        {
            "authors": [
                "M.R. Hassan",
                "B. Nath",
                "M. Kirley"
            ],
            "title": "A fusion model of hmm, ann and ga for stock market forecasting",
            "venue": "Expert systems with Applications, 33(1):171\u2013180.",
            "year": 2007
        },
        {
            "authors": [
                "D.O. Hebb"
            ],
            "title": "The first stage of perception: growth of the assembly",
            "venue": "The Organization of Behavior, 4:60\u201378.",
            "year": 1949
        },
        {
            "authors": [
                "B.M. Henrique",
                "V.A. Sobreiro",
                "H. Kimura"
            ],
            "title": "Literature review: Machine learning techniques applied to financial market prediction",
            "venue": "Expert Systems with Applications,",
            "year": 2019
        },
        {
            "authors": [
                "S. Hochreiter",
                "J. Schmidhuber"
            ],
            "title": "Long short-term memory",
            "venue": "Neural computation, 9(8):1735\u20131780.",
            "year": 1997
        },
        {
            "authors": [
                "Hsu",
                "S.-H.",
                "Hsieh",
                "J.P.-A.",
                "Chih",
                "T.-C.",
                "Hsu",
                "K.-C."
            ],
            "title": "A two-stage architecture for stock price forecasting by integrating self-organizing map and support vector regression",
            "venue": "Expert Systems with Applications, 36(4):7947\u20137951.",
            "year": 2009
        },
        {
            "authors": [
                "W. Huang",
                "Y. Nakamori",
                "Wang",
                "S.-Y."
            ],
            "title": "Forecasting stock market movement direction with support vector machine",
            "venue": "Computers & operations research, 32(10):2513\u20132522.",
            "year": 2005
        },
        {
            "authors": [
                "Kamijo",
                "K.-i.",
                "T. Tanigawa"
            ],
            "title": "Stock price pattern recognition-a recurrent neural network approach",
            "venue": "1990 IJCNN international joint conference on neural networks, pages 215\u2013221. IEEE.",
            "year": 1990
        },
        {
            "authors": [
                "Y. Kara",
                "M.A. Boyacioglu",
                "\u00d6.K. Baykan"
            ],
            "title": "Predicting direction of stock price index movement using artificial neural networks and support vector machines: The sample of the istanbul stock exchange",
            "venue": "Expert systems with Applications, 38(5):5311\u20135319.",
            "year": 2011
        },
        {
            "authors": [
                "T. Kimoto",
                "K. Asakawa",
                "M. Yoda",
                "M. Takeoka"
            ],
            "title": "Stock market prediction system with modular neural networks",
            "venue": "1990 IJCNN international joint conference on neural networks, pages 1\u20136. IEEE.",
            "year": 1990
        },
        {
            "authors": [
                "M. Kumar",
                "M. Thenmozhi"
            ],
            "title": "Forecasting stock index movement: A comparison of support vector machines and random forest",
            "venue": "Indian institute of capital markets 9th capital markets conference paper.",
            "year": 2006
        },
        {
            "authors": [
                "M.B. Kursa",
                "W.R. Rudnicki"
            ],
            "title": "Feature selection with the boruta package",
            "venue": "Journal of statistical software, 36:1\u201313.",
            "year": 2010
        },
        {
            "authors": [
                "S. Lahmiri",
                "S. Bekiros"
            ],
            "title": "Cryptocurrency forecasting with deep learning chaotic neural networks",
            "venue": "Chaos, Solitons & Fractals, 118:35\u201340.",
            "year": 2019
        },
        {
            "authors": [
                "M.T. Leung",
                "H. Daouk",
                "Chen",
                "A.-S."
            ],
            "title": "Forecasting stock indices: a comparison of classification and level estimation models",
            "venue": "International Journal of forecasting, 16(2):173\u2013190.",
            "year": 2000
        },
        {
            "authors": [
                "W.S. McCulloch",
                "W. Pitts"
            ],
            "title": "A logical calculus of the ideas immanent in nervous activity",
            "venue": "The bulletin of mathematical biophysics, 5(4):115\u2013133.",
            "year": 1943
        },
        {
            "authors": [
                "S. McNally",
                "J. Roche",
                "S. Caton"
            ],
            "title": "Predicting the price of bitcoin using machine learning",
            "venue": "2018 26th euromicro international conference on parallel, distributed and network-based processing (PDP), pages 339\u2013343. IEEE.",
            "year": 2018
        },
        {
            "authors": [
                "M. Minsky",
                "S. Papert"
            ],
            "title": "An introduction to computational geometry",
            "venue": "Cambridge tiass.,",
            "year": 1969
        },
        {
            "authors": [
                "T. Niu",
                "J. Wang",
                "H. Lu",
                "W. Yang",
                "P. Du"
            ],
            "title": "Developing a deep learning framework with two-stage feature selection for multivariate financial time series forecasting",
            "venue": "Expert Systems with Applications, 148:113237.",
            "year": 2020
        },
        {
            "authors": [
                "P. Nousi",
                "A. Tsantekidis",
                "N. Passalis",
                "A. Ntakaris",
                "J. Kanniainen",
                "A. Tefas",
                "M. Gabbouj",
                "A. Iosifidis"
            ],
            "title": "Machine learning for forecasting mid-price movements using limit order book data",
            "venue": "Ieee Access, 7:64722\u201364736.",
            "year": 2019
        },
        {
            "authors": [
                "D. Olson",
                "C. Mossman"
            ],
            "title": "Neural network forecasts of canadian stock returns using accounting ratios",
            "venue": "International Journal of Forecasting, 19(3):453\u2013465.",
            "year": 2003
        },
        {
            "authors": [
                "J. Patel",
                "S. Shah",
                "P. Thakkar",
                "K. Kotecha"
            ],
            "title": "Predicting stock and stock price index movement using trend deterministic data preparation and machine learning techniques",
            "venue": "Expert systems with applications, 42(1):259\u2013268.",
            "year": 2015
        },
        {
            "authors": [
                "J. Patel",
                "S. Shah",
                "P. Thakkar",
                "K. Kotecha"
            ],
            "title": "Predicting stock market index using fusion of machine learning techniques",
            "venue": "Expert Systems with Applications, 42(4):2162\u20132172.",
            "year": 2015
        },
        {
            "authors": [
                "K. Pawar",
                "R.S. Jalem",
                "V. Tiwari"
            ],
            "title": "Stock market price prediction using lstm rnn",
            "venue": "Emerging trends in expert applications and security, pages 493\u2013503. Springer.",
            "year": 2019
        },
        {
            "authors": [
                "B. Qian",
                "Y. Xiao",
                "Z. Zheng",
                "M. Zhou",
                "W. Zhuang",
                "S. Li",
                "Q. Ma"
            ],
            "title": "Dynamic multi-scale convolutional neural network for time series classification",
            "venue": "IEEE access, 8:109732\u2013109746.",
            "year": 2020
        },
        {
            "authors": [
                "M. Roondiwala",
                "H. Patel",
                "S. Varma"
            ],
            "title": "Predicting stock prices using lstm",
            "venue": "International Journal of Science and Research (IJSR), 6(4):1754\u20131756.",
            "year": 2017
        },
        {
            "authors": [
                "F. Rosenblatt"
            ],
            "title": "The perceptron: a probabilistic model for information storage and organization in the brain",
            "venue": "Psychological review, 65(6):386.",
            "year": 1958
        },
        {
            "authors": [
                "D.E. Rumelhart",
                "G.E. Hinton",
                "R.J. Williams"
            ],
            "title": "Learning representations by back-propagating errors",
            "venue": "nature, 323(6088):533\u2013536.",
            "year": 1986
        },
        {
            "authors": [
                "D. Shah",
                "H. Isah",
                "F. Zulkernine"
            ],
            "title": "Stock market analysis: A review and taxonomy of prediction techniques",
            "venue": "International Journal of Financial Studies, 7(2):26.",
            "year": 2019
        },
        {
            "authors": [
                "N. Smuts"
            ],
            "title": "What drives cryptocurrency prices? an investigation of google trends and telegram sentiment",
            "venue": "ACM SIGMETRICS Performance Evaluation Review, 46(3):131\u2013134.",
            "year": 2019
        },
        {
            "authors": [
                "X. Sun",
                "M. Liu",
                "Z. Sima"
            ],
            "title": "A novel cryptocurrency price trend forecasting model based on lightgbm",
            "venue": "Finance Research Letters, 32:101084.",
            "year": 2020
        },
        {
            "authors": [
                "A. Tsantekidis",
                "N. Passalis",
                "A. Tefas",
                "J. Kanniainen",
                "M. Gabbouj",
                "A. Iosifidis"
            ],
            "title": "Using deep learning to detect price change indications in financial markets",
            "venue": "25th European Signal Processing Conference (EUSIPCO),",
            "year": 2017
        },
        {
            "authors": [
                "H. Tyralis",
                "G. Papacharalampous"
            ],
            "title": "Variable selection in time series forecasting using random forests",
            "venue": "Algorithms, 10(4):114.",
            "year": 2017
        },
        {
            "authors": [
                "J.M. Valente",
                "S. Maldonado"
            ],
            "title": "Svr-ffs: A novel forward feature selection approach for high-frequency time series forecasting using support vector regression",
            "venue": "Expert Systems with Applications, 160:113729.",
            "year": 2020
        },
        {
            "authors": [
                "Wei",
                "L.-Y."
            ],
            "title": "A hybrid anfis model based on empirical mode decomposition for stock time series forecasting",
            "venue": "Applied Soft Computing, 42:368\u2013376.",
            "year": 2016
        },
        {
            "authors": [
                "D.C. Y\u0131ld\u0131r\u0131m",
                "I.H. Toroslu",
                "U. Fiore"
            ],
            "title": "Forecasting directional movement of forex data using lstm with technical and macroeconomic indicators",
            "venue": "Financial Innovation, 7(1):1\u201336.",
            "year": 2021
        },
        {
            "authors": [
                "Z. Zhang",
                "Dai",
                "H.-N.",
                "J. Zhou",
                "S.K. Mondal",
                "M.M. Gar\u0107\u0131a",
                "H. Wang"
            ],
            "title": "Forecasting cryptocurrency price using convolutional neural networks with weighted and attentive memory channels",
            "venue": "Expert Systems with Applications, 183:115378.",
            "year": 2021
        },
        {
            "authors": [
                "Z. Zhao",
                "R. Rao",
                "S. Tu",
                "J. Shi"
            ],
            "title": "Time-weighted lstm model with redefined labeling for stock trend prediction",
            "venue": "2017 IEEE 29th international conference on tools with artificial intelligence (ICTAI), pages 1210\u2013 1217. IEEE.",
            "year": 2017
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "After the financial crisis of 2008 and its effect on investors, institutions, academia, and governments, cryptocurrencies have gradually received attention as a resource of alternative revenue. Cryptocurrency is one of the most important investment tools that shows highly speculative behavior in the financial market.\n\u2217This research was supported by THE SCIENTIFIC AND TECHNOLOGICAL RESEARCH COUNCIL OF TU\u0308RKI\u0307YE\nar X\niv :2\n30 3.\n02 22\n3v 2\n[ cs\n.L G\n] 8\nM ar\nOver the last few years, especially due to the COVID-19 pandemic, cryptocurrency and stock investments have received great interest from investors worldwide. The large increase in the trading volume of the investment tools made it more important to forecast the future value of the assets.\nFinancial time series are ambiguous and have a nonlinear dynamic structure and chaotic movements, making forecasting quite difficult. Stock market and cryptocurrency indices depend on numerous macroeconomic factors such as political changes, the general outlook of the economy, investors\u2019 expectations as well as investment preferences and movements in other indices, making index forecasting quite difficult but also attractive. Many studies use conventional methods and machine learning algorithms such as artificial neural networks (ANN), support vector machines (SVM), naive Bayes (NB), and random forest (RF) to address the issue.\nAccording to the efficient-market hypothesis proposed by Fama (1970), stock prices are essential information and can be forecast based on trading data. Efficient market strategies should be developed to make safe and valid forecasts for stock index movements or other financial assets (Leung et al., 2000). Such market strategies protect investors from potential market risks and speculators. According to the efficient-market hypothesis, stock market indices can be effectively defined, and future forecasts can be made using trading data sets. This makes as much sense as the reflection of the political situation of a country or the public image of a firm on stock prices. Therefore, if we efficiently process information based on stock prices or stock index data using appropriate methods or algorithms, we can then forecast stock prices and trends. It can be said that all of the above-mentioned also apply to cryptocurrencies due to similar behavior.\nOver the years, numerous methods have been developed to forecast stock prices or returns and cryptocurrency prices or returns. One of the earliest of these methods is the classical regression analysis. Stock index data sets are pre-processed because they have a chaotic behavior and contain uncertainties and nonlinear relationships, resulting in information loss. Therefore, machine learning algorithms have recently been more popular than conventional methods (regression analysis, discriminant analysis, statistical methods, etc.) for financial time series forecasting.\nThe new research direction began with McCulloch and Pitts (1943) in the field of artificial neural networks. ANN theory was formulated quickly by Hebb (1949) and the Perceptron (Rosenblatt, 1958) in the next years. The research on artificial neural networks did not show much progress for a while, especially after Minsky and Papert (1969). The reason is that computers did not have enough power to produce high-accuracy results. The next period of rapid growth for neural networks was due to the reinvention of the back-propagation algorithm by Rumelhart et al. (1986), which made the ANN a prominent actor in this field of research. Algorithms such as artificial neural networks and support vector machines are commonly used because they do not make many restrictive assumptions on the data. Artificial neural networks (Hassan et al., 2007; Kara et al., 2011; Kimoto et al., 1990; Olson and Mossman, 2003; Wei, 2016) and\nsupport vector machines (Hsu et al., 2009; Huang et al., 2005; Kumar and Thenmozhi, 2006) have yielded successful results.\nThe non-sequential data type used in Multi-Layer Perceptron (MLP) models and temporal dependent structure of the input data made the Recurrent Neural Network (RNN) models more convenient compared to the MLPs in financial time series forecasting. Kamijo and Tanigawa (1990) applied the RNN model by using the Tokyo Stock Exchange data and obtained a 93.8% accuracy. The vanishing gradient problem specific to deep neural networks, as well as RNNs was addressed by the introduction of Long-Short Term Memory (LSTM) networks. Cho et al. (2014) and Di Persio and Honchar (2017) applied LSTM with dropout to the Google stock prices. Some similar results can be found in Bao et al. (2017); Pawar et al. (2019); Zhao et al. (2017).\nThe main objective of this research is to investigate whether feature selection methods can be used to improve the forecasting accuracy of different machine learning algorithms for stock and cryptocurrency returns."
        },
        {
            "heading": "2 Related Work",
            "text": "Statistical models make some simplifying assumptions to be able to obtain some theoretical guarantees and thus have a simple structure and potentially lover performance compared to machine learning techniques. Machine learning models are more general, make fewer simplifying assumptions, and offer better model-fitting capabilities. Due to these reasons, techniques such as ML and deep learning created a new research direction in the financial literature (Fang et al., 2021). To forecast the future value of financial assets and find the reason for these assets\u2019 behavior, numerous machine learning (ML) techniques were employed, such as SVM (Akyildirim et al., 2021), Support Vector Regression (SVR) (Kara et al., 2011), Random Forest (RF) (Patel et al., 2015a), and convolutional neural networks (CNN) (Tsantekidis et al., 2017). These works show that it is possible to use ML techniques to make more accurate forecasts and as alternative approaches to conventional techniques for investigating the behavior of financial assets. Nousi et al. (2019) applied several ML techniques such as single hidden layer feed-forward neural network, MLP, autoencoders, and bagof-features algorithms to forecast the mid-price movement of the stock price. The results indicated that the mentioned ML techniques could forecast price movement. Shah et al. (2019) organized the stock price prediction techniques into four categories, namely statistical methods, pattern recognition, machine learning, and sentiment analysis. The financial time series forecasting problems can also be categorized into classification and regression problems. According to Patel et al. (2015b), the most widely used two techniques are ANN and SVR, which are supervised methods, while the unsupervised methods are less popular. A review of machine learning techniques for financial time series forecasting is presented in Henrique et al. (2019). The authors note that ANN seems to be the most popular ML technique in the financial time series literature.\nMachine learning techniques like ANN, SVM, or decision trees can learn\nthe trends of stock or cryptocurrency returns from historical data. They can provide significant information and analysis of historical returns. Bernal et al. (2012) applied an RNN called Echo State Network to forecast the S&P 500 stock prices using a couple of technical indicators such as moving averages as an input. The proposed technique in this study outperformed the Kalman filter as a benchmark based on test error statistics. The authors used 50 different financial data sets and reported the forecasting results. Ballings et al. (2015) compared the performance of classifiers such as ANN, Logistic Regression (LR), SVM, and k-nearest neighbor with AdaBoost and RF. They reported the Random Forest algorithm as the best option to forecast the stock price movement based on area under curve (AUC) and cross-validation metrics. Borges and Neves (2020) looked at the most traded volume of 100 cryptocurrency prices from Binance with a 1-minute frequency, using the returns and some technical indicators as input. The authors regarded it as a classification problem, applying logistic regression, random forest, support vector machine, gradient tree boosting, and an ensemble of mentioned algorithms. Chen et al. (2020) used 5 min and daily bitcoin price index and trading price in USD to measure the LR, Linear discriminant analysis (LDA), RF, ANN, LSTM, and XGBoost algorithm performances. Four Blockchain information variables, trading variables, Google trend search volume indexes, and gold spot prices were used as input features. The authors reported that the LSTM model has the best accuracy level of 67% for 5 min data and LR and LDA have the best accuracy level of 65% for daily data. Sun et al. (2020) used 42 cryptocurrencies\u2019 daily data and applied light gradient boosting machine (GBM), SVM, and RF to forecast the movement of the prices. Sun et al. (2020) reported that light GBM outperformed other ML techniques based on accuracy performance metrics.\nLSTM networks have been applied in numerous studies in the financial forecasting literature. Lahmiri and Bekiros (2019) discussed the research problem as a regression by using the LSTM and generalized regression neural network (GRNN). Daily Bitcoin, Digital Cash, and Ripple prices were used as input features in this study.Lahmiri and Bekiros (2019) reported that LSTM had a significantly better performance than GRNN. Smuts (2019) applied LSTM using an input set that contains telegram chat groups talking for Bitcoin, Google trends, price volumes, and trading of bitcoin and Ethereum. Results showed that telegram data is a better predictor for bitcoin, however, Google trend data is better for ethereum, especially in the weekly periods. Di Persio and Honchar (2017) reached the level of accuracy of 72% on Google stock price by comparing different RNN model performances such as LSTM, gated recurrent unit (GRU), and a basic RNN. Roondiwala et al. (2017) also applied an LSTM to forecast Nifty prices by using the open, high, low, and close values as input variables. RMSE was used as a performance metric. McNally et al. (2018) used the Bitcoin prices\u2019 open, close, high, and low values and the hash and difficulty rate of the blockchain for different periods to construct an input set. The problem was considered a regression and classification, and LSTM and Bayesian recurrent neural networks were used as learning algorithms. The authors reported that 20 days are the best period for RNN and 100 days for LSTM. Fang et al. (2021)\nreported that the mid-price movement of Bitcoin could be predictable at an accuracy level of 78% using LSTM.\nHan et al. (2020) used a nonlinear autoregressive with exogenous variable (NARX) technique for predicting Bitcoin return in USD on a daily frequency. Bitcoin return was also used as an input feature, and the author reported that the NARX model could predict the trend but not breaks. Atsalakis et al. (2019) proposed a hybrid neuro-fuzzy controller (PATSOS) model to forecast the Bitcoin, Ethereum, Ripple, and Litecoin prices, considering the problem both as classification and regression. PATSOS was observed to have a significantly better forecast than other selected benchmark models in that study. Especially PATSOS predicted returns significantly better than the buy/hold strategies that are based on the predicted sign of the selected technical indicators. Basher and Sadorsky (2022) looked at forecasting Bitcoin price movement for predicting a possible trend in a specific period, and for planning asset allocation using this information. The research investigated the importance of cryptocurrency forecasting of different input variables such as inflation rate, interest rate, and market volatility. The authors applied tree-based machine learning techniques to forecast the direction of Bitcoin prices and reported accuracy levels between 75% and 80%.\nFeature selection is an important problem for financial time series forecasting, especially in social sciences. An effective feature selection process can improve the generalization ability of the forecasting models by removing many irrelevant features from the input space. However, this problem has not received enough attention in the financial literature. According to Niu et al. (2020) there are three types of feature selection methods: (1) embedded methods; (2) filterbased methods; (3) wrapper methods. Each of these methods has advantages and limitations and can be applied to financial time series forecasting problems.\nTyralis and Papacharalampous (2017) proposed a feature selection model based on a Random Forest algorithm to suggest an optimal set of input variables. The authors used two different time series datasets and compared the results with standard benchmark models. As a result, the RF model for feature selection showed better performance when using a few short-lag input features. Valente and Maldonado (2020) proposed an approach for time series forecasting by adapting the SVR for feature selection. The authors used an ARIMA model with feature selection to forecast the energy load. 1700 lags were used for high-frequency data and less than 400 lags were used for daily data. The proposed methodology showed slightly better performance compared to selected conventional techniques such as ARIMA models. Niu et al. (2020) proposed a two-stage multi-objective wrapper-based feature selection to determine the optimal input space for deep learning models. MAE, MSE, and MAPE statistics were used for evaluating the model performance. The authors reported that the proposed model for feature selection significantly improved the forecasting performance. Ghosh et al. (2022) proposed an ensemble feature selection algorithm to forecast the stock price using the national stock exchange data in India by assessing the COVID-19 effect. Spot prices, market sentiment, sectoral outlook, crude price volatility, and exchange rate volatility features were used\nin this study. The authors proposed a structural model to determine the effect of selected variables, comprising Boruta and Regularized Random Forest algorithms. Regularized greedy forest and deep neural network algorithms were used with PCA and autoencoders. It was indicated that the importance of the input features depends on the particular stock and the period under consideration.\nThe main contribution of this paper is to study the importance of feature selection for the forecasting problem using ML algorithms, in both classification and regression settings. It uses ten different financial time series data sets, including cryptocurrencies and stock markets. This study investigates the predictability of the selected stock and cryptocurrency return, which means profitability, based on ML techniques with and without feature selection as a regression problem, and the binary buy and sell strategies based on the same techniques as a classification problem. The study evaluates several ML techniques, namely Logistic Regression (LR), artificial neural networks (ANN), convolutional neural networks (CNN), and long short-term memory (LSTM), after applying the feature selection with annealing (FSA) Barbu et al. (2016) and the Boruta algorithm Ghosh et al. (2022) as a benchmark method."
        },
        {
            "heading": "3 Data Preparation",
            "text": "In this research, three different types of information were used from the original data.\n\u2022 Historical prices from Yahoo finance, using the Python finance API. The collected data contains the date, opening price, high price, low price, closing price, adjusted closing price, and volume information from a specific date interval depending on the data availability. The dataset information is presented in Table 1.\n\u2022 Technical indicators: four classes of technical indicators were used, namely momentum, volume, volatility, and trend indicators. A total of 26 technical indicators were gathered using the Python technical analysis library (TA-Lib). While calculating the technical indicators, six different periods were used: 2, 4, 8, 16, 32, and 64 for each of the indicators. A total of 185 features were obtained from these calculations.\n\u2022 Lag operator: The first five lag (1, 2, 3, 4, 5) periods were used for all calculated indicators and added to the feature pool. A total of 925 features were obtained using this operator.\nUsing these three sources, a feature pool was constructed from the historical stock and cryptocurrency data to apply feature selection algorithms and remove the irrelevant features from the model.\nAfter constructing the input feature pool, the datasets were prepared for analysis by conducting data preprocessing. This involved removing some rows (samples) from data that contained missing or infinite values obtained during the feature calculation process. Thus, different numbers of observations were\nobtained for each dataset. There also exist differences between stock and cryptocurrency data samples in terms of hours of operation and data availability. A total of 1105 features were obtained for the input feature space, as presented in Table 1.\nThe input dataset was standardized for both feature selection and forecasting techniques. We considered two prediction targets, a regression, and a classification target. Logarithmic returns were used as the target for the regression problem. For classification, a binary output (Eq. 1.) was used to represent the up and down movement of the logarithmic returns. The up and down movement was coded as {\u22121, 1} for FSA and {0, 1} for the forecasting models.\nyt =\n{ 1, if returnt > returnt\u22121\n\u22121, else (1)\nWe considered the forecasting problem of predicting the target at time t from the input features at time t \u2212 3. We observed that when trying to predict the target at time t from the input features at time t or t\u2212 1 a very high accuracy and high R2 values were obtained for regression, which means that the research problem was too simple. This is why in the end we settled on the forecasting problem of predicting the target at time t from the features at time t\u22123, where a lower accuracy and higher MSE values were obtained, but more reliable results.\nThe data was partitioned in 70% for training and 30% for testing for all models."
        },
        {
            "heading": "4 Problem and methodology",
            "text": "Financial time series forecasting is an important problem and numerous studies address it using statistical methods and machine learning techniques. However, while many published papers focus on improving the classification accuracy or decreasing the error for the regression problems they ignore the feature selection problem, which would help select the relevant features from a larger pool and\nhelp construct models with better generalization. We investigate the effect of feature selection on the performance of the selected techniques to forecast the financial time series. The methodology used in this paper is presented in Fig. 1."
        },
        {
            "heading": "4.1 Feature selection methodology",
            "text": "Feature selection is the problem of finding the relevant features in building a model, which can help improve generalization while reducing the computation time and the cost associated with obtaining the data. For that reason, we decided to apply the Feature Selection with Annealing (FSA) method Barbu et al. (2016) due to its good feature selection capabilities. FSA is a highly efficient feature selection algorithm that besides efficiency has statistical true feature recovery and convergence guarantees, is easy to implement, and can handle nonlinearity to a certain extent. In FSA, the feature selection problem is formulated as a constrained optimization problem,\n\u03b2 = argmin |{j,\u03b2j 6=0}| L(\u03b2) (2)\nwhere k is the number of relevant features (a given parameter), and the loss function L(\u03b2) is differentiable with respect to \u03b2. The key ideas in the algorithm design are a) using an annealing plan to lessen the greediness in reducing the dimensionality from p (the initial number of features) to k (desired number of features) and b) gradually removing the most irrelevant features to facilitate computation. The parameter ranges used in experiments are presented in Table 2.\nWe also implemented a Boruta feature selection process Ghosh et al. (2022) as a benchmark model to evaluate and compare the FSA algorithm performance. Boruta is an ensemble feature selection algorithm that uses a Random Forest together with a feature importance measure to select the relevant feature from the input space Kursa and Rudnicki (2010). We used the existing scikit-learn implementation of the Boruta algorithm available at https: //github.com/scikit-learn-contrib/boruta_py."
        },
        {
            "heading": "4.2 Forecasting methodology",
            "text": "This section gives an overview of the machine learning methods that will be used to forecast the financial time series in this paper, namely logistic regression, artificial neural network (ANN), convolutional neural network (CNN), and long short term memory (LSTM)."
        },
        {
            "heading": "4.2.1 Logistic regression",
            "text": "Logistic regression is a popular technique to model the probability of discrete (i.e. binary or multinomial) outcomes, indicated by the class label. Logistic regression is a multiple regression model with a dependent variable y \u2208 {0.1} for binary classification or y \u2208 {0.1, . . . , n} for multinomial classification. The dependent variable is predicted from the input feature vector x = (x1, ..., xp) \u2208 Rp. The logistic regression model can be expressed as follows,\nlog P (y = 1|x)\n1\u2212 P (y = 1|x) = \u03b20 + \u03b21x1 + \u00b7 \u00b7 \u00b7+ \u03b2pxp (3)\nLearning the parameters \u03b2 = (\u03b20, ..., \u03b2p) is done by optimization of the negative log-likelihood loss function, which can also have an L1 or L2 penalty term to improve generalization. We used the c parameter, which represents the inverse of regularization strength, and smaller c values specify stronger regularization.\nThe Python scikit-learn library is used for the LR implementation and the parameter levels that are used in the study are given in Table 3."
        },
        {
            "heading": "4.2.2 Artificial neural network",
            "text": "ANN has been commonly used for forecasting purposes of stock or cryptocurrency prices or the movement of these values. MLP\u2019s flexible structure and\ntheir success in many studies, such as Kara et al. (2011); Patel et al. (2015a), indicate that MLP might have an advantage compared to traditional statistical models. A feed-forward neural network model is used in this study with different numbers of hidden layers and number of neurons.\nThe log sigmoid is used for the output layer, and tangent sigmoid and rectified linear unit (ReLU) functions are used for the hidden layer neurons as activation functions. Stochastic gradient descent and ADAM optimization algorithms are used for training the ANN. The loss functions are the MSE loss for regression and the binary cross-entropy loss for classification problems. A comprehensive set of parameter combinations in the ranges presented in Table 4 have been explored to determine the best combination. The Python Keras library was used for the implementation of the ANN."
        },
        {
            "heading": "4.2.3 Convolutional neural network",
            "text": "CNN is one of the most popular machine learning (neural network) techniques, performing well on different regression or classification problems such as Qian et al. (2020). Each neuron receives an input signal from the input feature space and operates as in ANN. CNN structure lets the connection with locally sharing weights and determining the local features effectively(Zhang et al., 2021). One of the main differences between ANN and CNN is the field of usage. CNN has generally used pattern recognition within the images. CNN has a convolutional layer, pooling layer, and output layer, and the main components are convolutional and pooling layers. The convolutional layer extract local features, and the pooling layer reduce the dimension of the parameter in addition to minimizing the over-fitting possibility (Zhang et al., 2021). The inputs for the networks are the time series data features like a signal. Python Keras library is used for the implementation of CNN. CNN layer information was presented in Table 5."
        },
        {
            "heading": "4.2.4 Long short-term memory",
            "text": "The LSTM network developed by Hochreiter and Schmidhuber (1997) is an extension of Recurrent Neural Networks, redesigned to handle the vanishing gradient problem in recurrent neural networks. LSTM consists of a memory cell to store the information coming from inputs through the three gates, namely\nthe input gate i(t), the forget gate f(t), and the output gate c(t), which updates the information. LSTM can be formulated as follows (Chen et al., 2020):\nX = [ xt ht\u22121 ] ft = \u03b4(WfX + bf )\nit = \u03b4(WiX + bi)\nct = \u03b4(W0X + b0)\n(4)\nwhere xt and ht are the input and respectively the hidden state at time t, Wj ,Wi and W0 are the weight matrices of the corresponding components, bf , bi, b0 are the corresponding bias parameters, and \u03b4 is the activation function.\nWe adopt LSTM as a promising machine learning method for predicting the future based on past information without assuming any noise form. Most\nimportantly, LSTM could capture possibly nonlinear features behind the time series. The hyper-parameters of the LSTM model include the number of layers L and the number of training epochs E. Note that since a linear structure is a special case of a feed-forward neural network when armed with a linear transformer, one should expect that LSTM performs at least as well as MLP. The Python Keras library is used for the implementation."
        },
        {
            "heading": "4.3 Evaluation criteria",
            "text": "The above ML regression and classification techniques were used to forecast on the financial time series data introduced in Section 3. During feature selection, the MSE loss function was used for the regression problem, and the binary cross entropy loss function was used for the classification problems. To evaluate and compare the model performance, the MSE metric was used for the regression problems, and accuracy and area under the ROC curve (AUC) metrics were used for classification, both based on the set aside test set."
        },
        {
            "heading": "5 Results",
            "text": ""
        },
        {
            "heading": "5.1 Feature selection results",
            "text": "As indicated before, the data is high dimensional, with different numbers of samples for each dataset due to the market condition and data availability. To reduce the high dimensionality, the FSA algorithm Barbu et al. (2016) with a linear model was applied to each data set, and the obtained results are presented in Tables 7 and 8 together with the parameter combinations that obtained those results. Parameters \u00b5, s, and \u03b7 are the most important parameters for the FSA algorithm. The number of relevant features is generally different for the regression and classification problems as well as the annealing parameter (\u00b5). The annealing parameters are usually \u00b5 = 300 except for the \u201cTether\u201d, \u201cBitcoin\u201d and \u201cRipple\u201d data sets in both regression and classification and NKK for classification.\nFSA reduced the high dimensional feature space by more than 95% , achieving the same or higher accuracy results on the test set. The accuracy levels of FSA for each dataset are quite low, around 0.5, except for the Nasdaq data.\nThe accuracy level on Nasdaq is 0.748, which is the best value for classification problems, as seen in Table 8.\nThe Boruta feature selection method Ghosh et al. (2022), which is quite popular in the forecasting literature, was also evaluated and compared process with the FSA method. As seen in Table 9 the regression MSEs of the Boruta method are higher than FSA and accuracy values are lower than FSA."
        },
        {
            "heading": "5.2 Return forecasting",
            "text": "Three regression algorithms are tested on the ten financial time series data sets introduced in Section 3. The selected time series\u2019 daily values were used to forecast the selected stocks\u2019 returns, as well as cryptocurrency and gold prices. The MSE was used for model comparison, and the results are presented in Table 10. Paired t-test were used to compare the best result for each column (shown in bold with a star) with the other results. Results that are not significantly worse (p > 0.05) than the best result are also shown in bold, representing the top performing group.\nTable 9: Results obtained by the Boruta feature selection method Ghosh et al. (2022).\nRegression Classification Data Number of features MSE Number of features Accuracy TTH 82 0.361 13 0.498 ETH 15 34.46 67 0.401 BTC 12 24.82 88 0.467 RPL 19 43.04 66 0.451 TSL 14 20.80 28 0.434 NSDQ 28 2.901 15 0.705 NKK 6 1.905 17 0.423 NYSE 13 2.416 18 0.451 S&P500 18 1.857 20 0.412 GLD 14 1.406 117 0.428\nTable 10: Return forecasting. MSE values for the 10 datasets evaluated.\nModel/Data TTH ETH BTC RPL TSL NSDQ NKK NYSE S&P500 GLD Null 0.178 26.25 17.53 34.63 18.26 2.42 1.519 1.813 1.532 1.164 ANN 0.177 26.20 17.50 34.62 18.26 2.428 1.515 1.780 1.550 1.165 CNN 0.177 26.18 17.49 34.63 18.29 2.418 1.516 1.800 1.532 1.138 LSTM 0.209 27.01 17.11* 35.99 18.44 2.435 1.713 1.944 1.673 1.206 FSA-ANN 0.177 26.17* 17.50 34.33*18.18*2.399*1.509*1.773* 1.517* 1.167 FSA-CNN 0.172 26.22 17.52 34.63 18.26 2.418 1.519 1.804 1.532 1.053* FSA-LSTM 0.168* 26.98 17.12 35.99 18.39 2.431 1.719 1.938 1.679 1.207 BOR-ANN 0.177 26.68 17.71 34.98 18.29 2.417 1.520 1.821 1.691 1.165 BOR-CNN 0.178 26.81 17.54 34.63 18.22 2.418 1.527 1.805 1.531 1.162 BOR-LSTM 0.213 27.71 17.89 35.99 18.39 2.432 1.692 1.970 1.680 1.199\nThe null model that always predicts the average return on the training set was also evaluated as a baseline comparison. One can easily say that the FSA algorithm improved the forecasting performance in most cases. As seen in Table 10, at least one of the FSA based models is in the top performing group (bold) for all 10 datasets.\nIf we compare the FSA-based model performances to the other models, we see that at least one FSA-based model significantly outperformed all non-FSA models on 3 out of the 10 datasets (TTH, S&P500 and GLD). In contrast, the non-FSA models including the Boruta-based models never outperformed the FSA based models significantly.\nThe null model is in the top performing group on four datasets (RPL, TLS, NSDQ and NKK), which means that no model could improve the performance of the null model significantly on these datasets.\nOn the three remaining datasets (ETH, BTC and NYSE), FSA is in the top performing group together with some non-FS based models, but Boruta based models are not in the top performing group on these datasets.\nIn fact, examining the 6 datasets where the null model is not in the top performing group, we see that Boruta based models are not in the top performing\nTable 11: Return movement forecasting. Accuracy values for the 10 datasets evaluated.\nModel/Data TTH ETH BTC RPL TSL NSDQ NKK NYSE S&P500 GLD LR 0.524 0.515 0.507 0.517 0.515* 0.743 0.515 0.539* 0.518 0.503 ANN 0.487 0.498 0.485 0.527 0.494 0.745 0.535 0.516 0.538 0.496 CNN 0.502 0.517 0.490 0.535* 0.513 0.740 0.527 0.526 0.526 0.495 LSTM 0.482 0.529 0.523* 0.528 0.490 0.523 0.522 0.498 0.525 0.530 FSA-LR 0.500 0.525 0.515 0.532 0.513 0.737 0.518 0.532 0.510 0.540* FSA-ANN 0.486 0.535 0.523*0.535* 0.513 0.734 0.528 0.526 0.525 0.528 FSA-CNN 0.490 0.521 0.515 0.527 0.508 0.747*0.553* 0.529 0.525 0.528 FSA-LSTM 0.527*0.558* 0.509 0.515 0.463 0.521 0.528 0.534 0.554* 0.502 BOR-LR 0.493 0.464 0.513 0.533 0.461 0.530 0.442 0.512 0.477 0.492 BOR-ANN 0.484 0.467 0.513 0.522 0.488 0.509 0.544 0.512 0.468 0.500 BOR-CNN 0.468 0.473 0.516 0.506 0.470 0.530 0.540 0.521 0.483 0.503 BOR-LSTM 0.470 0.513 0.495 0.503 0.505 0.532 0.453 0.454 0.521 0.479\ngroup on any of these six datasets. Therefore FSA-based models significantly outperform Boruta-based models on these six datasets."
        },
        {
            "heading": "5.3 Forecasting the return movement",
            "text": "As stated before, four classification algorithms were applied to the ten financial time series datasets introduced in Section 3. Accuracy and AUC metrics are used for the model comparisons in classification. The experimental results are presented in Table 11-12. Again, a star represents the best model for each column and the models that are not significantly worse than the best model (p > 0.05) are shown in bold.\nIn terms of accuracy, from Table 11 one can see that an FSA-based model significantly outperforms all non-FSA based models including Boruta-based models on three datasets (ETH, NKK, GLD), which means that on these dataset FSA brings a significant contribution. Moreover, an FSA-based model is in the top performing group on all 10 datasets. In contrast, a Boruta-based method is in the top performing group on only four datasets and it does not bring a significant contribution because on each of the four datasets there is a plain model (one of LR, ANN CNN and LSTM) that is also in the top performing group.\nThe highest accuracy was obtained by many models on the Nasdaq data set. The FSA-CNN model showed the best accuracy of 0.747 and LR, ANN, and CNN were also in the top performing group in terms of accuracy.\nIn terms of AUC values, from Table 12 one could see that an FSA based model was in the top performing group for nine of the ten datasets (all but NYSE). It brought a significant contribution (where it was significantly better than the plain models) on one dataset (NKK). In contrast, Boruta based models were in the top performing group on two out of the 10 datasets and brought a significant contribution on one dataset (NKK). Especially for the NSDQ dataset, the Boruta based models are very far behind the plain and FSA models in terms of AUC.\nTable 12: Return movement forecasting. AUC values for the 10 datasets evaluated.\nModel/Data TTH ETH BTC RPL TSL NSDQ NKK NYSE S&P500 GLD LR 0.533 0.516 0.510 0.515 0.514 0.823 0.521 0.547* 0.531 0.505 ANN 0.500 0.547 0.494 0.566* 0.499 0.826* 0.556 0.539 0.563 0.540 CNN 0.527 0.512 0.477 0.524 0.525 0.821 0.547 0.492 0.530 0.501 LSTM 0.495 0.560 0.536* 0.536 0.511 0.502 0.549 0.526 0.474 0.538 FSA-LR 0.487 0.533 0.524 0.533 0.507 0.818 0.529 0.534 0.528 0.542* FSA-ANN 0.496 0.532 0.521 0.547 0.512 0.810 0.538 0.536 0.552 0.538 FSA-CNN 0.517 0.516 0.531 0.543 0.509 0.811 0.573* 0.527 0.532 0.531 FSA-LSTM 0.542*0.569* 0.503 0.509 0.544* 0.528 0.538 0.523 0.576* 0.522 BOR-LR 0.481 0.451 0.518 0.518 0.464 0.516 0.526 0.570 0.499 0.483 BOR-ANN 0.475 0.437 0.521 0.528 0.424 0.465 0.565 0.547 0.504 0.512 BOR-CNN 0.465 0.440 0.513 0.514 0.453 0.508 0.553 0.536 0.493 0.511 BOR-LSTM 0.493 0.520 0.510 0.498 0.457 0.497 0.509 0.507 0.475 0.481"
        },
        {
            "heading": "6 Conclusion and discussion",
            "text": "The cryptocurrency market has received a lot of attention recently due to offering an alternative investment asset to the stock market. The technology used for the cryptocurrency system is one of the major reasons for crypto attracting investors from around the world. The significant chaotic behavior in cryptocurrencies makes it very risky for investors, and investors need professional recommendations to better manage the risk. For that reason, researchers are interested in producing accurate forecasts for cryptocurrencies to support investors with their recommendations.\nThis work investigates the use of feature selection techniques for improving the forecasting performance of many common machine learning techniques. It introduces the first application of FSA (Feature Selection with Annealing) for this purpose and combines it with LR (logistic regression), ANN (artificial neural networks), CNN (convolutional neural networks), and LSTM (large short term memory) models to obtain linear and nonlinear models with feature selection. These feature selection based models are studied in classification where the aim is to forecast the \u201dup\u201d and \u201ddown\u201d movement of stocks and in regression where the aim is to forecast the actual stock returns.\nTen different financial time series data sets have been constructed for the experimental validation, where different technical indicators and lags have been used to obtain a 1105 dimensional feature vector. Technical indicators with specific periods and their first five lags were used to construct the 1105 dimensional input features pool for forecasting trends and returns. As response variables were investigated the 1, 2 and 3 days ahead returns and movements, similar to Y\u0131ld\u0131r\u0131m et al. (2021). Because the 1 and 2 days ahead response variables were predicted with high accuracy and high R2 values, the 3 days ahead trend and the logarithmic return values were chosen as response variables.\nThe paper investigates whether the FSA algorithm improves the forecasting performance of the selected machine learning techniques on these ten financial\ntime series datasets. In addition, the Boruta feature selection algorithm, a popular feature selection algorithm for financial data, was also evaluated and compared to the FSA algorithm.\nThe results indicate that the FSA algorithm improves or usually does not reduce the forecasting performance for most of the 10 data sets evaluated. The FSA procedure provided better estimations compared to the ML models, both in regression and classification. This means that FSA reduces the number of input features and provides accurate results. The lower dimensional input means the algorithm has a smaller computation time and memory requirements to produce accurate forecasts.\nOne of the most important conclusions drawn from this study is that the prediction performance of the Boruta algorithm remains at a minimum level. In many cases, the Boruta algorithm reduced the forecasting performance of the ML models in terms of accuracy or MSE metrics. Contrary to these results, the hybrid FSA models obtained the best results, so the FSA feature selection process improved the model performance in almost all cases.\nThere are still some possible directions for future work, such as employing and evaluating traditional statistical models such as ARMA/ARIMA and attention-based ML models. Moreover, a multi-class classifier could provide recommendations for different risk levels and other useful comments as compared to a binary classifier. Furthermore, to construct a real-time investor recommendation system, online learning algorithms and expert systems could be also involved in the research. These future works would require data sets with different temporal frequencies, which would have to be created and used in training and evaluation of these models."
        }
    ],
    "year": 2023
}