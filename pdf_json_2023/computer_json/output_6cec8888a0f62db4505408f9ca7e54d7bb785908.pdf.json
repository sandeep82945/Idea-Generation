{
    "abstractText": "As a type of biometric identification, a speaker identification (SID) system is confronted with various kinds of attacks. The spoofing attacks typically imitate the timbre of the target speakers, while the adversarial attacks confuse the SID system by adding a well-designed adversarial perturbation to an arbitrary speech. Although the spoofing attack copies a similar timbre as the victim, it does not exploit the vulnerability of the SID model and may not make the SID system give the attacker\u2019s desired decision. As for the adversarial attack, despite the SID system can be led to a designated decision, it cannot meet the specified text or speaker timbre requirements for the specific attack scenarios. In this study, to make the attack in SID not only leverage the vulnerability of the SID model but also reserve the timbre of the target speaker, we propose a timbre-reserved adversarial attack in the speaker identification. We generate the timbre-reserved adversarial audios by adding an adversarial constraint during the different training stages of the voice conversion (VC) model. Specifically, the adversarial constraint is using the target speaker label to optimize the adversarial perturbation added to the VC model representations and is implemented by a speaker classifier joining in the VC model training. The adversarial constraint can help to control the VC model to generate the speaker-wised audio. Eventually, the inference of the VC model is the ideal adversarial fake audio, which is timbre-reserved and can fool the SID system. Experimental results on the Audio deepfake detection (ADD) challenge dataset indicate that our proposed method improves the attack success rate significantly compare with the vanilla VC model without additionally introducing an adversarial noise to the attack speech. Objective and subjective evaluations illustrate that the quality of fake audio generated by our proposed method is better than directly adding adversarial perturbation to the VC-generated audio. Furthermore, the analysis shows that our generated adversarial fake audios also meet the specified text and target speaker timbre-reserved requirements of the attacker.",
    "authors": [
        {
            "affiliations": [],
            "name": "Qing Wang"
        },
        {
            "affiliations": [],
            "name": "Jixun Yao"
        },
        {
            "affiliations": [],
            "name": "Li Zhang"
        },
        {
            "affiliations": [],
            "name": "Pengcheng Guo"
        },
        {
            "affiliations": [],
            "name": "Lei Xie"
        }
    ],
    "id": "SP:6fcf45ca4bdd4dea3efa14cf853cbb062127a7de",
    "references": [
        {
            "authors": [
                "J.H. Hansen",
                "T. Hasan"
            ],
            "title": "Speaker recognition by machines and humans: A tutorial review",
            "venue": "IEEE Signal Processing Magazine, vol. 32, no. 6, pp. 74\u201399, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "D.A. Reynolds",
                "R.C. Rose"
            ],
            "title": "Robust text-independent speaker identification using gaussian mixture speaker models",
            "venue": "IEEE Transactions on Speech and Audio Processing, vol. 3, no. 1, pp. 72\u201383, 1995.",
            "year": 1995
        },
        {
            "authors": [
                "Z. Wu",
                "E.S. Chng",
                "H. Li"
            ],
            "title": "Detecting converted speech and natural speech for anti-spoofing attack in speaker recognition",
            "venue": "Proc. INTERSPEECH, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "Z. Wu",
                "N. Evans",
                "T. Kinnunen",
                "J. Yamagishi",
                "F. Alegre",
                "H. Li"
            ],
            "title": "Spoofing and countermeasures for speaker verification: a survey",
            "venue": "Speech Communication, vol. 66, pp. 130\u2013153, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "Z. Wu",
                "J. Yamagishi",
                "T. Kinnunen",
                "C. Hanil\u00e7i",
                "M. Sahidullah",
                "A. Sizov",
                "N. Evans",
                "M. Todisco",
                "H. Delgado"
            ],
            "title": "ASVspoof: the automatic speaker verification spoofing and countermeasures challenge",
            "venue": "IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 4, pp. 588\u2013604, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "Z. Wu",
                "T. Kinnunen",
                "N. Evans",
                "J. Yamagishi",
                "C. Hanil\u00e7i",
                "M. Sahidullah",
                "A. Sizov"
            ],
            "title": "ASVspoof 2015: the first automatic speaker verification spoofing and countermeasures challenge",
            "venue": "Proc. INTERSPEECH, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "T. Kinnunen",
                "M. Sahidullah",
                "H. Delgado",
                "M. Todisco",
                "N. Evans",
                "J. Yamagishi",
                "K.A. Lee"
            ],
            "title": "The ASVspoof 2017 challenge: Assessing the limits of replay spoofing attack detection",
            "venue": "2017.",
            "year": 2017
        },
        {
            "authors": [
                "M. Todisco",
                "X. Wang",
                "V. Vestman",
                "M. Sahidullah",
                "H. Delgado",
                "A. Nautsch",
                "J. Yamagishi",
                "N. Evans",
                "T. Kinnunen",
                "K.A. Lee"
            ],
            "title": "ASVspoof 2019: Future horizons in spoofed and fake audio detection",
            "venue": "arXiv preprint arXiv:1904.05441, 2019.",
            "year": 1904
        },
        {
            "authors": [
                "I.J. Goodfellow",
                "J. Shlens",
                "C. Szegedy"
            ],
            "title": "Explaining and harnessing adversarial examples",
            "venue": "arXiv preprint arXiv:1412.6572, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "N. Carlini",
                "D. Wagner"
            ],
            "title": "Towards evaluating the robustness of neural networks",
            "venue": "Proc. SP). IEEE, 2017, pp. 39\u201357.",
            "year": 2017
        },
        {
            "authors": [
                "C. Szegedy",
                "W. Zaremba",
                "I. Sutskever",
                "J. Bruna",
                "D. Erhan",
                "I. Goodfellow",
                "R. Fergus"
            ],
            "title": "Intriguing properties of neural networks",
            "venue": "arXiv preprint arXiv:1312.6199, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "A. Kurakin",
                "I. Goodfellow",
                "S. Bengio"
            ],
            "title": "Adversarial examples in the physical world",
            "venue": "arXiv preprint arXiv:1607.02533, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "N. Carlini",
                "D. Wagner"
            ],
            "title": "Audio adversarial examples: Targeted attacks on speech-to-text",
            "venue": "2018 IEEE Security and Privacy Workshops (SPW), 2018, pp. 1\u20137.",
            "year": 2018
        },
        {
            "authors": [
                "L. Sch\u00f6nherr",
                "K. Kohls",
                "S. Zeiler",
                "T. Holz",
                "D. Kolossa"
            ],
            "title": "Adversarial attacks against automatic speech recognition systems via psychoacoustic hiding",
            "venue": "Proc. NDSS, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "M. Alzantot",
                "B. Balaji",
                "M. Srivastava"
            ],
            "title": "Did you hear that? adversarial examples against automatic speech recognition",
            "venue": "arXiv preprint arXiv:1801.00554, 2018.",
            "year": 1801
        },
        {
            "authors": [
                "S. Sun",
                "P. Guo",
                "L. Xie",
                "M.-Y. Hwang"
            ],
            "title": "Adversarial regularization for attention based end-to-end robust speech recognition",
            "venue": "IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP), vol. 27, no. 11, pp. 1826\u20131838, 2019.",
            "year": 1826
        },
        {
            "authors": [
                "Y. Qin",
                "N. Carlini",
                "I. Goodfellow",
                "G. Cottrell",
                "C. Raffel"
            ],
            "title": "Imperceptible, robust, and targeted adversarial examples for automatic speech recognition",
            "venue": "Proc. ICML). PMLR, 2019, pp. 5231\u20135240.",
            "year": 2019
        },
        {
            "authors": [
                "H. Abdullah",
                "K. Warren",
                "V. Bindschaedler",
                "N. Papernot",
                "P. Traynor"
            ],
            "title": "SoK: The faults in our ASRs: An overview of attacks against automatic speech recognition and speaker identification systems",
            "venue": "Proc. SP. IEEE, 2021, pp. 730\u2013747.",
            "year": 2021
        },
        {
            "authors": [
                "D. Snyder",
                "D. Garcia-Romero",
                "G. Sell",
                "D. Povey",
                "S. Khudanpur"
            ],
            "title": "X-vectors: Robust DNN embeddings for speaker recognition",
            "venue": "Proc. ICASSP. IEEE, 2018, pp. 5329\u20135333.",
            "year": 2018
        },
        {
            "authors": [
                "L. Wan",
                "Q. Wang",
                "A. Papir",
                "I.L. Moreno"
            ],
            "title": "Generalized end-toend loss for speaker verification",
            "venue": "Proc. ICASSP. IEEE, 2018, pp. 4879\u20134883.",
            "year": 2018
        },
        {
            "authors": [
                "B. Desplanques",
                "J. Thienpondt",
                "K. Demuynck"
            ],
            "title": "ECAPA-TDNN: Emphasized channel attention, propagation and aggregation in TDNN based speaker verification",
            "venue": "arXiv preprint arXiv:2005.07143, 2020.",
            "year": 2005
        },
        {
            "authors": [
                "F. Kreuk",
                "Y. Adi",
                "M. Cisse",
                "J. Keshet"
            ],
            "title": "Fooling end-to-end speaker verification with adversarial examples",
            "venue": "Proc. ICASSP. IEEE, 2018, pp. 1962\u20131966.",
            "year": 2018
        },
        {
            "authors": [
                "Q. Wang",
                "P. Guo",
                "S. Sun",
                "L. Xie",
                "J.H. Hansen"
            ],
            "title": "Adversarial regularization for end-to-end robust speaker verification",
            "venue": "Proc. IN- TERSPEECH, 2019, pp. 4010\u20134014.",
            "year": 2019
        },
        {
            "authors": [
                "H. Abdullah",
                "W. Garcia",
                "C. Peeters",
                "P. Traynor",
                "K.R. Butler",
                "J. Wilson"
            ],
            "title": "Practical hidden voice attacks against speech and speaker recognition systems",
            "venue": "arXiv preprint arXiv:1904.05734, 2019.",
            "year": 1904
        },
        {
            "authors": [
                "J. Li",
                "X. Zhang",
                "C. Jia",
                "J. Xu",
                "L. Zhang",
                "Y. Wang",
                "S. Ma",
                "W. Gao"
            ],
            "title": "Universal adversarial perturbations generative network for speaker recognition",
            "venue": "Proc. ICME. IEEE, 2020, pp. 1\u20136.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Xie",
                "C. Shi",
                "Z. Li",
                "J. Liu",
                "Y. Chen",
                "B. Yuan"
            ],
            "title": "Real-time, universal, and robust adversarial attacks against speaker recognition systems",
            "venue": "Proc. ICASSP. IEEE, 2020, pp. 1738\u20131742.",
            "year": 2020
        },
        {
            "authors": [
                "Z. Li",
                "C. Shi",
                "Y. Xie",
                "J. Liu",
                "B. Yuan",
                "Y. Chen"
            ],
            "title": "Practical adversarial attacks against speaker recognition systems",
            "venue": "Proc. HOTMOBILE, 2020, pp. 9\u201314.",
            "year": 2020
        },
        {
            "authors": [
                "X. Li",
                "J. Zhong",
                "X. Wu",
                "J. Yu",
                "X. Liu",
                "H. Meng"
            ],
            "title": "Adversarial attacks on GMM i-vector based speaker verification systems",
            "venue": "Proc. ICASSP. IEEE, 2020, pp. 6579\u20136583.",
            "year": 2020
        },
        {
            "authors": [
                "G. Chen",
                "S. Chenb",
                "L. Fan",
                "X. Du",
                "Z. Zhao",
                "F. Song",
                "Y. Liu"
            ],
            "title": "Who is real bob? adversarial attacks on speaker recognition systems",
            "venue": "Proc. SP. IEEE, 2021, pp. 694\u2013711.",
            "year": 2021
        },
        {
            "authors": [
                "A. Jati",
                "C.-C. Hsu",
                "M. Pal",
                "R. Peri",
                "W. AbdAlmageed",
                "S. Narayanan"
            ],
            "title": "Adversarial attack and defense strategies for deep speaker recognition systems",
            "venue": "Computer Speech & Language, vol. 68, p. 101199, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Q. Wang",
                "P. Guo",
                "L. Xie"
            ],
            "title": "Inaudible adversarial perturbations for targeted attack in speaker recognition",
            "venue": "Proc. INTERSPEECH, pp. 4228\u2013 4232, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "X. Zhang",
                "X. Zhang",
                "M. Sun",
                "X. Zou",
                "K. Chen",
                "N. Yu"
            ],
            "title": "Imperceptible black-box waveform-level adversarial attack towards automatic speaker recognition",
            "venue": "Complex & Intelligent Systems, pp. 1\u201315, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J. Yi",
                "R. Fu",
                "J. Tao",
                "S. Nie",
                "H. Ma",
                "C. Wang",
                "T. Wang",
                "Z. Tian",
                "Y. Bai",
                "C. Fan"
            ],
            "title": "ADD 2022: the first audio deep synthesis detection challenge",
            "venue": "Proc. ICASSP. IEEE, 2022, pp. 9216\u20139220.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Wang",
                "R. Skerry-Ryan",
                "D. Stanton",
                "Y. Wu",
                "R.J. Weiss",
                "N. Jaitly",
                "Z. Yang",
                "Y. Xiao",
                "Z. Chen",
                "S. Bengio"
            ],
            "title": "Tacotron: Towards endto-end speech synthesis",
            "venue": "arXiv preprint arXiv:1703.10135, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "Y. Ren",
                "Y. Ruan",
                "X. Tan",
                "T. Qin",
                "S. Zhao",
                "Z. Zhao",
                "T.-Y. Liu"
            ],
            "title": "Fastspeech: Fast, robust and controllable text to speech",
            "venue": "Proc. NeurIPS, vol. 32, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "L.-W. Chen",
                "W. Guo",
                "L.-R. Dai"
            ],
            "title": "Speaker verification against synthetic speech",
            "venue": "Proc. ISCSLP. IEEE, 2010, pp. 309\u2013312.",
            "year": 2010
        },
        {
            "authors": [
                "V. Shchemelinin",
                "M. Topchina",
                "K. Simonchik"
            ],
            "title": "Vulnerability of voice verification systems to spoofing attacks by TTS voices based on automatically labeled telephone speech",
            "venue": "Proc. SPECOM. Springer, 2014, pp. 475\u2013481.",
            "year": 2014
        },
        {
            "authors": [
                "W. Cai",
                "A. Doshi",
                "R. Valle"
            ],
            "title": "Attacking speaker recognition with deep generative models",
            "venue": "arXiv preprint arXiv:1801.02384, 2018.",
            "year": 1801
        },
        {
            "authors": [
                "S. Mehri",
                "K. Kumar",
                "I. Gulrajani",
                "R. Kumar",
                "S. Jain",
                "J. Sotelo",
                "A. Courville",
                "Y. Bengio"
            ],
            "title": "SampleRNN: An unconditional end-toend neural audio generation model",
            "venue": "arXiv preprint arXiv:1612.07837, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "A. v. d. Oord",
                "S. Dieleman",
                "H. Zen",
                "K. Simonyan",
                "O. Vinyals",
                "A. Graves",
                "N. Kalchbrenner",
                "A. Senior",
                "K. Kavukcuoglu"
            ],
            "title": "Wavenet: A generative model for raw audio",
            "venue": "arXiv preprint arXiv:1609.03499, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "E. Wenger",
                "M. Bronckers",
                "C. Cianfarani",
                "J. Cryan",
                "A. Sha",
                "H. Zheng",
                "B.Y. Zhao"
            ],
            "title": "Hello, It\u2019s Me\u201d: Deep learning-based speech synthesis attacks in the real world",
            "venue": "Proc. CCS. ACM, 2021, pp. 235\u2013251. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 11",
            "year": 2021
        },
        {
            "authors": [
                "B. Sisman",
                "J. Yamagishi",
                "S. King",
                "H. Li"
            ],
            "title": "An overview of voice conversion and its challenges: From statistical modeling to deep learning",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing (TASLP), vol. 29, pp. 132\u2013157, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "L. Sun",
                "S. Kang",
                "K. Li",
                "H. Meng"
            ],
            "title": "Voice conversion using deep bidirectional long short-term memory based recurrent neural networks",
            "venue": "Proc. ICASSP. IEEE, 2015, pp. 4869\u20134873.",
            "year": 2015
        },
        {
            "authors": [
                "L. Sun",
                "K. Li",
                "H. Wang",
                "S. Kang",
                "H. Meng"
            ],
            "title": "Phonetic posteriorgrams for many-to-one voice conversion without parallel data training",
            "venue": "Proc.ICME. IEEE, 2016, pp. 1\u20136.",
            "year": 2016
        },
        {
            "authors": [
                "T. Hayashi",
                "W.-C. Huang",
                "K. Kobayashi",
                "T. Toda"
            ],
            "title": "Nonautoregressive sequence-to-sequence voice conversion",
            "venue": "Proc. ICASSP. IEEE, 2021, pp. 7068\u20137072.",
            "year": 2021
        },
        {
            "authors": [
                "L.-J. Liu",
                "Y.-N. Chen",
                "J.-X. Zhang",
                "Y. Jiang",
                "Y.-J. Hu",
                "Z.-H. Ling",
                "L.- R. Dai"
            ],
            "title": "Non-parallel voice conversion with autoregressive conversion model and duration adjustment",
            "venue": "Proc. Joint Workshop for the Blizzard Challenge and Voice Conversion Challenge 2020, 2020, pp. 126\u2013130.",
            "year": 2020
        },
        {
            "authors": [
                "T. Hayashi",
                "K. Kobayashi",
                "T. Toda"
            ],
            "title": "An investigation of streaming non-autoregressive sequence-to-sequence voice conversion",
            "venue": "Proc. ICASSP. IEEE, 2022, pp. 6802\u20136806.",
            "year": 2022
        },
        {
            "authors": [
                "H. Kameoka",
                "T. Kaneko",
                "K. Tanaka",
                "N. Hojo"
            ],
            "title": "Stargan-vc: Nonparallel many-to-many voice conversion using star generative adversarial networks",
            "venue": "Proc. SLT. IEEE, 2018, pp. 266\u2013273.",
            "year": 2018
        },
        {
            "authors": [
                "T. Kaneko",
                "H. Kameoka"
            ],
            "title": "Cyclegan-vc: Non-parallel voice conversion using cycle-consistent adversarial networks",
            "venue": "Proc. EUSIPCO. IEEE, 2018, pp. 2100\u20132104.",
            "year": 2018
        },
        {
            "authors": [
                "Y.Y. Lin",
                "C.-M. Chien",
                "J.-H. Lin",
                "H.-y. Lee",
                "L.-s. Lee"
            ],
            "title": "Fragmentvc: Any-to-any voice conversion by end-to-end extracting and fusing finegrained voice fragments with attention",
            "venue": "Proc. ICASSP. IEEE, 2021, pp. 5939\u20135943.",
            "year": 2021
        },
        {
            "authors": [
                "B. Nguyen",
                "F. Cardinaux"
            ],
            "title": "NVC-NET: End-to-end adversarial voice conversion",
            "venue": "Proc. ICASSP. IEEE, 2022, pp. 7012\u20137016.",
            "year": 2022
        },
        {
            "authors": [
                "J. Kim",
                "J. Kong",
                "J. Son"
            ],
            "title": "Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech",
            "venue": "Proc. ICML. PMLR, 2021, pp. 5530\u20135540.",
            "year": 2021
        },
        {
            "authors": [
                "J. Shen",
                "R. Pang",
                "R.J. Weiss",
                "M. Schuster",
                "N. Jaitly",
                "Z. Yang",
                "Z. Chen",
                "Y. Zhang",
                "Y. Wang",
                "R. Skerrv-Ryan"
            ],
            "title": "Natural TTS synthesis by conditioning wavenet on Mel spectrogram predictions",
            "venue": "Proc. ICASSP. IEEE, 2018, pp. 4779\u20134783.",
            "year": 2018
        },
        {
            "authors": [
                "Y. Ren",
                "C. Hu",
                "X. Tan",
                "T. Qin",
                "S. Zhao",
                "Z. Zhao",
                "T.-Y. Liu"
            ],
            "title": "Fastspeech 2: Fast and high-quality end-to-end text to speech",
            "venue": "Proc. ICLR, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A. Kashkin",
                "I. Karpukhin",
                "S. Shishkin"
            ],
            "title": "HiFi-VC: High quality asrbased voice conversion",
            "venue": "arXiv preprint arXiv:2203.16937, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "R.K. Das",
                "X. Tian",
                "T. Kinnunen",
                "H. Li"
            ],
            "title": "The attacker\u2019s perspective on automatic speaker verification: an overview",
            "venue": "Proc. INTERSPEECH, pp. 4213\u20134217, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "J. Kong",
                "J. Kim",
                "J. Bae"
            ],
            "title": "HiFi-GAN: Generative adversarial networks for efficient and high fidelity speech synthesis",
            "venue": "Proc. NeurIPS, vol. 33, pp. 17 022\u201317 033, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Liu",
                "R. Xue",
                "L. He",
                "X. Tan",
                "S. Zhao"
            ],
            "title": "Delightfultts 2: End-toend speech synthesis with adversarial vector-quantized auto-encoders",
            "venue": "Proc. Interspeech, 2022, pp. 1581\u20131585.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Shi",
                "H. Bu",
                "X. Xu",
                "S. Zhang",
                "M. Li"
            ],
            "title": "Aishell-3: A multi-speaker mandarin tts corpus and the baselines",
            "venue": "arXiv preprint arXiv:2010.11567, 2020.",
            "year": 2010
        },
        {
            "authors": [
                "H. Bu",
                "J. Du",
                "X. Na",
                "B. Wu",
                "H. Zheng"
            ],
            "title": "Aishell-1: an open-source mandarin speech corpus and a speech recognition baseline",
            "venue": "Proc. O-COCOSDA. IEEE, 2017, pp. 1\u20135.",
            "year": 2017
        },
        {
            "authors": [
                "Z. Yao",
                "D. Wu",
                "X. Wang",
                "B. Zhang",
                "F. Yu",
                "C. Yang",
                "Z. Peng",
                "X. Chen",
                "L. Xie",
                "X. Lei"
            ],
            "title": "Wenet: Production oriented streaming and nonstreaming end-to-end speech recognition toolkit",
            "venue": "Proc. Interspeech, 2021, pp. 4054\u20134058.",
            "year": 2021
        },
        {
            "authors": [
                "J. Deng",
                "J. Guo",
                "N. Xue",
                "S. Zafeiriou"
            ],
            "title": "Arcface: Additive angular margin loss for deep face recognition",
            "venue": "Proc. CVPR. IEEE/CVF, 2019, pp. 4690\u20134699.",
            "year": 2019
        },
        {
            "authors": [
                "H. Delgado",
                "N. Evans",
                "T. Kinnunen",
                "K.A. Lee",
                "X. Liu",
                "A. Nautsch",
                "J. Patino",
                "M. Sahidullah",
                "M. Todisco",
                "X. Wang"
            ],
            "title": "Asvspoof 2021: Automatic speaker verification spoofing and countermeasures challenge evaluation plan",
            "venue": "arXiv preprint arXiv:2109.00535, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "H. Tak",
                "J.-w. Jung",
                "J. Patino",
                "M. Kamble",
                "M. Todisco",
                "N. Evans"
            ],
            "title": "End-to-end spectro-temporal graph attention networks for speaker verification anti-spoofing and speech deepfake detection",
            "venue": "arXiv preprint arXiv:2107.12710, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J.-w. Jung",
                "H.-S. Heo",
                "H. Tak",
                "H.-j. Shim",
                "J.S. Chung",
                "B.-J. Lee",
                "H.-J. Yu",
                "N. Evans"
            ],
            "title": "Aasist: Audio anti-spoofing using integrated spectrotemporal graph attention networks",
            "venue": "Proc. ICASSP. IEEE, 2022, pp. 6367\u20136371.",
            "year": 2022
        },
        {
            "authors": [
                "C.-C. Lo",
                "S.-W. Fu",
                "W.-C. Huang",
                "X. Wang",
                "J. Yamagishi",
                "Y. Tsao",
                "H.-M. Wang"
            ],
            "title": "Mosnet: Deep learning based objective assessment for voice conversion",
            "venue": "arXiv preprint arXiv:1904.08352, 2019.",
            "year": 1904
        },
        {
            "authors": [
                "L. Van der Maaten",
                "G. Hinton"
            ],
            "title": "Visualizing data using t-sne.",
            "venue": "Journal of machine learning research,",
            "year": 2008
        },
        {
            "authors": [
                "Z. Cai",
                "C. Zhang",
                "M. Li"
            ],
            "title": "From speaker verification to multispeaker speech synthesis, deep transfer with feedback constraint",
            "venue": "arXiv preprint arXiv:2005.04587, 2020.",
            "year": 2005
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014Speaker identification, adversarial attack, timbre-reserved, voice conversion.\nI. INTRODUCTION Speaker identification (SID) [1], [2] is the process of automatically inferring the identity of a speaker from a spoken utterance. As one of the most prominent biometric authentication methods, it is critical to ensure the robustness of the SID system. A variety of attacks attempt to challenge the robustness of the SID system. For instance, spoofing attack [3]\u2013[5] commonly includes impersonation, replay, voice conversion, and speech synthesis. The ASVspoof challenge [6]\u2013[8] is\nManuscript received December 27, 2022; revised May 31, 2023; accepted August 10, 2023. (Corresponding author: Lei Xie.)\nQing Wang, Jixun Yao, Li Zhang, Pengcheng Guo, and Lei Xie are with the Audio, Speech and Langauge Processing Group (ASLP@NPU), School of Computer Science, Northwestern Polytechnical University, Xi\u2019an 710129, China (e-mail: qingwang@nwpu-aslp.org; yaojx@mail.nwpu.edu.cn; alivacheung@gmail.com; guopengcheng1220@gmail.com; lxie@nwpu.edu.cn).\ndedicated to addressing this problem, and it is organized to contribute to the development of countermeasures to protect speaker recognition from the threat of spoofing attacks. Depending upon the scenarios of the spoof samples attacking the SID system, the spoofing attack can be broadly classified into two categories, which are physical access (PA) attacks and logical access (LA) attacks [8]. In the PA attacks, the samples are applied as input to the SID system through the sensor, which directly attacks the SID system. The LA attacks are the direct injection into the SID system, and the most common approaches are speech synthesis and voice conversion. Both of them aim to generate a speech based on the voice of the target speaker. However, a spoofing attack normally depends on the quality and size of the dataset and cannot take advantage of SID model\u2019s vulnerability, so that the spoofing attack is difficult to accurately be classified to the target speaker by the SID system since the spoofing attacks do not consider the downstream SID task.\nMoreover, the adversarial attack [9]\u2013[12] is considered as another type of attack, which is emerged in the last few years. An adversarial attack is a malicious attempt that exploits the vulnerability of the network itself and tries to perturb the original sample into a new sample. The new sample, which is also known as an adversarial example, can be misclassified by the network. Adversarial attacks also can impact speech processing tasks [13]\u2013[18]. Since recent work has explored different deep neural network (DNN) architectures [19]\u2013[21] to produce compact speaker embeddings, the SID system is also vulnerable to adversarial examples. In recent years, many researchers have successfully conducted adversarial attacks on SID systems [22]\u2013[30]. However, the perturbation is added to an arbitrary speech to achieve the targeted adversarial attack, which is hard to meet the requirement of speaker similarity and intelligibility and can easily be detected by humans. What\u2019s more, although the perturbation is usually designed too small to be heard by human beings through multiple approaches [31], [32], the adversarial perturbation is not strictly inaudible.\nInspired by these two kinds of attacks above, qualified fake audio for attacking the SID model should have the ability to deceive both machines and humans at the same time. To fool the machine, the downstream SID task needs to be considered so that the fake audio has a distinctive speaker attribute, which can make the SID model make the designated decision. From a human perception perspective, when the timbre or the text of the fake audio is far different from the target speaker\u2019s real audio, this kind of fake audio is easy to be detected. Therefore, the timbre and text information also are concerns when we\nar X\niv :2\n30 9.\n00 92\n9v 1\n[ cs\n.S D\n] 2\nS ep\n2 02\n3\nconduct attacks on the SID model. Moreover, the quality of the fake audio is also crucial in the attack.\nIn this study, when we conduct an adversarial attack on the speaker identification model, we aim to take the target speaker\u2019s timbre and text information into consideration. To this end, we propose a timbre-reserved adversarial attack in the SID system, which is to make the attack in SID not only exploit the vulnerability of the SID model but also reserve the timbre of the target speaker, as well as customize the text of the fake speech. In particular, a speaker classifier is jointly trained with the VC model to determine whether the representations belong to the target speaker. If it is not classified into the target speaker class, an adversarial perturbation is added to the VC model representation, which constrains the representations to be classified to the target speaker. Then the perturbed representation proceeds with optimizing the VC model. Consequently, the VC model can generate fake audio with distinctive target speaker information. We adopt various levels of VC model representation during the VC model training. In non-autoregressive based VC model training, the adversarial constraint is added to the Mel-spectrogram and latent representation. While in the end-to-end VC model training, we add the adversarial constraint to the reconstructed waveform. After that, by adding adversarial constraints to optimize various voice conversion frameworks, we can obtain the timbre-reserved and speaker-wised adversarial fake audios for attacking the speaker identification system.\nWe evaluate our proposed methods on the Audio deepfake detection (ADD) challenge dataset [33]. With the given text and speakerID, the fake audios generated by our proposed method improve the attack success rate significantly compared with the vanilla VC model. Since the proposed methods do not introduce extra adversarial noise to the attack speech, the objective and subjective evaluations also illustrate that the quality of fake audio generated by our proposed method is better than directly adding adversarial perturbation to the VC generated audio. Furthermore, we also analyze the speaker similarity and intelligibility of the fake audio, which both meet the requirements.\nWe summarize our main contributions as follows. \u2022 To our best knowledge, we are the first to propose adding\nadversarial perturbation generation into voice conversion framework training to achieve timbre-reserved adversarial attacks for the speaker identification system. \u2022 We explore adding adversarial constraints to different levels of representation of the VC training, which are the VC predicted Mel-spectrogram, latent representation, and reconstructed waveform, respectively. \u2022 The timbre-reserved fake audio not only preserves the timbre of the target speaker but also can effectively targeted attack the SID system, and the text of the fake audio can be customized as well.\nThe rest of the paper is organized as follows. In Section II, related works are introduced. In Section III, we detail the proposed timbre-reserved adversarial attack in the SID system. Datasets and experimental setup are described in Section IV. Section V presents the experimental results and analysis. We conclude in Section VI."
        },
        {
            "heading": "II. RELATED WORKS",
            "text": ""
        },
        {
            "heading": "A. Spoofing attacks in speaker identification",
            "text": "In recent years, there are various kinds of attacks on speaker identification. One of the most severe threats is a spoofing attack. The spoofed speech samples can be obtained through impersonation, replay, voice conversion, or speech synthesis. Especially, speech synthesis and VC are also the main concerns as typical types of deep fake in SID and are the logical access (LA) tasks in spoofing attacks as well.\nSpeech synthesis, also known as text-to-speech (TTS), takes arbitrary text as input and generates speech as output [34], [35]. With the development of the TTS techniques, synthesized speech is more indistinguishable from human speech. In [5], [6], [36], [37], TTS-generated fake audios were used as the spoofing attack in speaker recognition systems. However, the attack on the speaker recognition system using the TTSgenerated fake audio is not very effective, since the downstream SID task is not taken into consideration. In [38], Cai et al. proved that samples generated with SampleRNN [39] and WaveNet [40] were unable to fool deep learning based speaker recognition system. Wenger et al. [41] generated synthetic speech using publicly available systems that can already fool both humans and several popular software systems, but the performance of the attack success rate is not that high enough. In addition, the quality and size of the training set for the TTS model are typically insufficient for genuine attacking a SID model, as a result, it is difficult to attack the SID system with speech synthesis itself.\nVoice conversion (VC) is a process that converts or transforms the voice of the original speaker into the target speaker while keeping the linguistic content [42]. In this study, we mainly focus on the generation of fake audio by VC. With the development of VC, different kinds of VC models have been explored, and they are typically divided into three types: autoregressive based VC model [43], [44], non-autoregressive based VC model [45]\u2013[49], and end-to-end VC model [50]\u2013 [52].\nThe first type of VC model is autoregressive based. The typical autoregressive based VC model [43], [44] is an improved version of Tacotron 2 [53], which leverages an encoderdecoder framework and takes Mel-spectrograms or the bottleneck features of automatic speech recognition (ASR) model as inputs. However, the autoregressive VC model predicts Melspectrogram frame by frame and can not train in a parallel way, which is unsuitable as the baseline model of adversarial attack. As a result, we do not use it as our VC baseline model.\nSecondly, various non-autoregressive VC models capable of synthesizing speech in a non-autoregressive manner have been proposed to speed up the training and inference process enormously. The non-autoregressive models generate parallel sequences, solving word skipping and repetition problems caused by incorrect attention alignment. The typical non-autoregressive VC model is based on FastSpeech 2 [54], and consists of multiple feed-forward transformer (FFT) blocks. Mel-spectrograms generated using a nonautoregressive method do not rely on the previous frames and are suitable for adding adversarial constraints. Therefore, the\nFastSpeech-based VC model is used as our first VC baseline model for generating fake audio.\nThe third type is fully end-to-end (E2E) VC models, which can generate speech waveform from waveform or bottleneck features directly. Different from the previous two kinds of VC models, the E2E VC model requires less human annotation and feature development, while the joint optimization of the E2E VC model can avoid the distribution mismatch between the acoustic model and the vocoder. Moreover, the E2E VC model can also reduce training and deployment costs. In this study, to comprehensively investigate the effect of adversarial constraints for different levels of representations of the VC models, we adopt the HifiGAN-based E2E VC model [55] as our second baseline model.\nDifferent from the TTS-based fake audio generation, the VC model does not have high requirements on the quality and size of the training data. However, neither the TTS nor the VC model takes the downstream SID task into consideration, and the performance of attacking the SID model by spoofed audios is not as expected. To better attack the SID model, besides preserving the timbre of the target speaker, the attack needs to leverage the vulnerability of the SID network as well."
        },
        {
            "heading": "B. Adversarial attack in speaker identification",
            "text": "The adversarial attack is typically divided into two categories according to whether the attackers expect the model is misleading to a specific output, which are targeted and non-targeted attacks. We mainly focus on targeted attacks in speaker identification tasks in this study. Das et al. [56] gave an overview of various types of attack on speaker verification focusing on potential threats of adversarial attacks and spoofing countermeasures from the attacker\u2019s perspective. In [25], Li et al. explored the existence of the universal adversarial perturbations (UAPs) in speaker recognition systems and proposed to generate UAPs by learning the mapping from the low-dimensional normal distribution to the UAP subspace via a generative model. Xie et al. [26] proposed the realtime, universal, and robust adversarial attack against DNNbased speaker recognition system by adding audio-agnostic universal perturbations. Li et al. [27] launched a practical and systematic adversarial attack against speaker recognition systems and integrated the estimated room impulse response into the adversarial example training for over-the-air attack. In our previous work [31], to targeted attack the speaker recognition system, we generated inaudible adversarial perturbations based on psychoacoustic principle of frequency masking. Zhang et al. [32] performed black-box waveformlevel targeted adversarial attacks against speaker recognition systems by generating imperceptible adversarial perturbations based on auditory masking. In [29], Chen et al. proposed FAKEBOB to craft adversarial examples and conducted a comprehensive and systematic study of the adversarial attacks on speaker recognition systems to understand their security weakness in the practical black-box setting.\nHowever, to achieve a targeted adversarial attack, all the studies mentioned above add adversarial perturbation to an arbitrary speech. From a human perception perspective, these\nadversarial audio sounds completely different from the target speaker\u2019s timbre. When the attack needs to meet the requirement of text and speaker similarity, these adversarial attack is easy to be detected. Furthermore, even though these methods introduce small perturbations as inaudible as possible to the fake audio, the perturbations are usually directly added to the waveform and it can still be perceived by humans."
        },
        {
            "heading": "III. METHODOLOGY",
            "text": "In this section, we introduce how we generate timbrereserved fake audio. We add adversarial constraints in different levels of representations of voice conversion model training: VC predicted Mel-spectrogram, latent representation, and reconstructed waveform."
        },
        {
            "heading": "A. Adversarial constraint",
            "text": "To make the fake audios generated by the VC model have distinctive speaker attributes, we add an adversarial constraint process into VC model training, which can adversely restrict the VC model representations and lead to the speaker classifier giving the target prediction. In the adversarial constraint process, suppose that a well-trained speaker identification model is f(\u00b7), and the Mel-spectrogram M is the input of the SID model, which is the VC model predicted representation. And its corresponding speaker label predicted by the SID model f(\u00b7) is y, while its VC target speaker label is y\u2032. When y = y\u2032, the Mel-spectrogram M generated from the VC model has the right speaker attribute. In contrast, when y \u0338= y\u2032, the adversarial constraint process is used to modify M generated from VC model, and the adversarial constraint \u03b4 can be defined as follows:\nminLCE(f(M + \u03b4), y \u2032), s.t. \u2225\u03b4\u2225 < \u03f5, (1)\nwhere LCE(\u00b7) is the loss function to make adversarial examples lead the SID model predicting the specific target speaker label, and the hyperparameter \u03f5 is used to control the maximum perturbation generated. The \u03b4 is initialized to a zero vector and \u03f5 is gradually reduced from a large value. For each iteration, \u03b4 is updated as follows:\n\u03b4 \u2190 clip\u03f5(\u03b4 \u2212 lr \u00b7 sign(\u2207\u03b4LCE(f(M + \u03b4), y\u2032))), (2)\nwhere lr is the learning rate, \u2207\u03b4LCE is the gradient of SID model with respect to \u03b4. When we attempt to perturb the original VC model, this adversarial constraint is added in various VC representations during the VC model training to reserve the distinctive target speaker information."
        },
        {
            "heading": "B. Adversarial constraint on Mel-spectrogram",
            "text": "Figure 1 is an overview of our first strategy of adversarial constraint on Mel-spectrogram, which consists of a nonautoregressive based voice conversion model and an attack constraint process. The VC model is trained beforehand and then jointly trained with the attack constraint process. In order to make the VC model predict the Mel-spectrogram with a distinctive target speaker attribute, we conduct our first strategy in the following steps: first, the voice conversion\nmodel converts the source speaker\u2019s identity to the target speaker y\u2032; then, the adversarial constraint is added to optimize the predicted Mel-spectrogram when the prediction of speaker classifier is not as same as the target speaker.\nFor VC model training, a pre-trained ASR model is first used to extract speaker-independent linguistic information from the source waveform. We employ the bottleneck feature extracted from the ASR final encoder layer as linguistic information. The acoustic model predicts the Mel-spectrogram by a 6-layer conformer encoder-decoder structure similar to FastSpeech 2 [54]. The speaker embedding from the speaker encoder is fed to the decoder as conditions for target voice generation. We adopt the L1 loss as reconstruction loss to optimize the VC model and is defined as:\nLrec = ||Mgt \u2212 M\u0302 ||1, (3)\nwhere Mgt and M\u0302 represent the ground truth Mel-spectrogram of source speech and the VC predicted Mel-spectrogram, respectively.\nAs for joint training with attack constraint, we use the Melspectrogram predicted by the VC model to attack the speaker classifier, which is a well-trained speaker identification model sharing the same speaker set as the VC training data. If the attack fails, as shown in the right part of Figure 1, we add a tiny adversarial perturbation to the predicted Mel-spectrogram to generate the adversarial Mel-spectrogram Madv with an adversarial constraint which can be defined as:\nMadv = M\u0302 + \u03b4adv\ns.t. \u2225\u03b4adv\u2225 < \u03f5, (4)\nhere, \u03b4adv represents the tiny adversarial perturbation and \u03f5 is used to control the maximum adversarial perturbation generated. The tiny adversarial perturbation can be optimized by:\nminLCE (f(Madv), y\u2032) , (5)\nwhere LCE aims to make the Madv fool the well-trained speaker identification system into predicting a specified target label. Therefore, the joint training with adversarial constraint can be optimized by the following loss function:\nLadv = { ||Mgt \u2212 M\u0302 ||1, if succeeded, ||Madv \u2212 M\u0302 ||1, if failed.\n(6)\nA well-trained speaker classifier f(\u00b7) is added behind the VC model, the Mel-spectrogram predicted from the VC model M\u0302 is used as the input of f(\u00b7), and the system determines whether the prediction of the speaker classifier f(M\u0302) is the target speaker y\u2032 or not. When f(M\u0302) \u0338= y\u2032, in other words, the attack failed, we add an adversarial perturbation to the Melspectrogram as the adversarial constraint. In order to force the predicted Mel-spectrogram M\u0302 of the VC model can be classified to the target speaker, we expect to minimize the L1 loss between the predicted Mel-spectrogram M\u0302 and the Melspectrogram with the adversarial perturbation Madv so that the VC model can fool the well-trained speaker recognition system. The adversarial perturbation \u03b4adv is optimized by Equation 2 until the predicted label f(M\u0302) is the target label. In contrast, when the speaker classifier gives a prediction of the target speaker label, which means the attack succeeds, the VC model is optimized only using the original reconstruction loss in Equation 3.\nAfter VC model joint training with the attack constraint process, the generation of deep fake audio is based on the HifiGAN vocoder [57], whose input is the Mel-spectrogram predicted by the adversarial constrained VC model."
        },
        {
            "heading": "C. Adversarial constraint on latent representation",
            "text": "As shown in Figure 2, we add the adversarial constraint to the latent representation of the VC model in the second strategy. The encoder takes the bottleneck (BN) feature as the input and outputs the high-level linguistic representation, while the speaker encoder is adopted to generate speaker embedding. The latent representation z is the concatenation of the high-level linguistic representation and speaker embedding. The adversarial constraint on latent representation zadv is operated by adding the adversarial perturbation, which can be formulated as follow:\nzadv = z + \u03b4adv. (7)\nThe following optimization of tiny adversarial perturbation and joint training is as same as the first strategy:\nminLCE (f(Dec(zadv)), y \u2032) , (8)\nEncoder Speaker Encoder\nUpsampling Layer\nResidual Stack\nConv Layer\n3 x\nReconstructed Waveform\nBN SpkID\nHiFiGAN-based Decoder\nFig. 3: The architecture of end-to-end voice conversion model.\nwhere Dec(\u00b7) represents the FFT-based decoder and Dec(zadv) is the Mel-spectrogram predicted by the FFT-based decoder from the adversarial perturbed latent representation. Next, the second strategy of adversarial constraint on latent representation can be optimized by:\nLadv = { ||Mgt \u2212 M\u0302 ||1, if succeeded, ||Dec(zadv)\u2212 M\u0302 ||1, if failed.\n(9)\nThe adversarial perturbation is added to the latent representation when f(M\u0302) \u0338= y\u2032. And we minimize the difference of predicted Mel-spectrogram M\u0302 and the Mel-spectrogram Dec(zadv) decoded from the perturbed latent representation. The adversarial perturbation \u03b4adv is optimized by Equation 2 until the predicted label f(M\u0302) is the target label y\u2032. When the attack succeeds, the VC model is optimized as normal in Equation 3."
        },
        {
            "heading": "D. Adversarial constraint on reconstructed waveform",
            "text": "In addition to adding adversarial perturbation on Melspectrogram or latent representation, we also add adversarial perturbation on the reconstructed waveform. An end-to-end HiFiGAN-based voice conversion model replaces the VC model and the detailed architecture is illustrated in Figure 3,\nthe VC module goes through single-stage training for efficient end-to-end learning. The VC model consists of a convolutional long short-term memory (CLSTM) based encoder, a speaker encoder, and a HiFiGAN-based decoder [57], which aim at high-level linguistic representation encoding and waveform reconstruction, respectively. CLSTM consists of three stacks of convolution layers, the LeakyReLU activation function, and an LSTM layer. The speaker embedding from the lookup table is fed to the decoder as conditions for target voice generation. The architecture of the decoder generator follows the same configuration as HiFi-GAN [57].\nThe training objective of the end-to-end VC HiFiGAN [55], which consists of reconstruction loss Lrec, feature matching loss Lfm, and adversarial loss Lvc-adv. As for reconstruction loss, we compute L1 loss between spectrograms of the source waveform Xs and predicted waveform X\u0302s, and the reconstruction loss can be formulated as:\nLrec = ||Ms \u2212 M\u0302s||1, (10)\nwhere Ms and M\u0302s represents the Mel-spectrogram of source waveform and predicted waveform, respectively. To improve the performance of voice conversion, we also employ adversarial training for more natural speech. The adversarial generator loss is calculated as:\nLgenvc-adv = (D(X\u0302s)\u2212 1)2, (11)\nLdisvc-adv = (D(Xs)\u2212 1)2 +D(X\u0302s)2, (12)\nwhere D(\u00b7) is a discriminator network. For adversarial training stability, feature matching loss is also used as:\nLfm = T\u2211\ni=1\n1\nNi\n\u2225\u2225\u2225Di(Xs)\u2212Di(X\u0302s)\u2225\u2225\u2225 1 , (13)\nwhere T denotes the total number of layers in the discriminator and Di produces the feature map of the i-th layer of the discriminator with Ni number of features.\nFigure 4 is an overview of the third strategy, the adversarial perturbation is directly added to the VC predicted waveform and can be defined by:\nXadv = X\u0302 + \u03b4adv, (14)\nwhere X\u0302 represents the VC reconstructed waveform and Xadv represents the adversarial perturbed reconstruction waveform. To adapt the input of the attacked speaker classifier, we convert\nthe waveform to Mel-spectrogram after adding adversarial perturbation and optimize the VC model as follows:\nLadv = { ||Mgt \u2212Mel(X\u0302)||1, if succeeded, ||Mel(Xadv)\u2212Mel(X\u0302)||1, if failed,\n(15)\nwhere Mel(\u00b7) is computing the Mel-spectrogram from waveform. When f(Mel(X\u0302)) \u0338= y\u2032, we minimize the L1 loss between the Mel-spectrogram of the VC reconstructed waveform Mel(X\u0302) and the Mel-spectrogram of the perturbed reconstruction waveform Mel(Xadv). For the same purpose as the former strategies, we expect the adversarial constraint can force the VC model to generate the waveform with distinctive speaker attributes."
        },
        {
            "heading": "E. Generation of timbre-reserved fake audio",
            "text": "After the VC models with adversarial constraints are trained, the generation process of fake audio is shown in Figure 5. With the given text, we generate the audio of a random speaker by a TTS system. The TTS system is based on the FastSpeech [54] model and modified the encoder and decoder structure inspired by the DelightfulTTS 2 [58] conformer block. Then the given speakerID and the TTS generated audio are the inputs of the adversely constrained VC model to predict the attack Mel-spectrogram. A HifiGAN vocoder [57] is followed to reconstruct the waveform from Mel-spectrogram. After that, the timbre-reserved fake audios are obtained for the adversarial attack against the SID model."
        },
        {
            "heading": "IV. EXPERIMENTAL SETUP",
            "text": ""
        },
        {
            "heading": "A. Datasets",
            "text": "In this study, we use AISHELL-3 [59] to train the speaker identification model and voice conversion model. AISHELL-3\nis a multi-speaker Mandarin Chinese audio corpus containing 88035 recordings from 218 native speakers. And the test set of AISHELL-1 [60] is used to evaluate the performance of the SID model. For the timbre-reserved adversarial attack, the dataset of audio deepfake detection (ADD) challenge [33] is used to evaluate our proposed method. Since ADD challenge corpus is an open-source dataset, and is specifically designed for deep fake audio attack and detection, we employ this corpus to establish a solution to first address the adversarial attack while remaining the timbre and also customizing the text. The training and adaptation sets for ADD Track 1 and 2 are used to train the fake detection models and the test set of these two tracks are used to evaluate the detection models. In the test set of ADD challenge Track 3.1, 10 speaker IDs and 500 texts are given to generate the fake audio. We generate fake audio according to the given text and speaker identities and the fake audio can fool the fake detection model and SID model. Furthermore, the generated attack samples need also to meet the certain requirement of intelligibility and similarity."
        },
        {
            "heading": "B. Setup",
            "text": "The detailed experimental setup of all the models shown in Figure 5 is described as follows:\n\u2022 TTS model: The TTS model we use to generate waveforms from the given text is a 6-layer conformer encoderdecoder structure, which is similar to the DelightfulTTS 2 proposed in [58]. With the given text, we generate the waveforms by using a randomly selected speaker timbre, and the waveforms are used as the source waveform of the VC system. \u2022 VC models: For VC model training, a pre-trained ASR model by the WeNet toolkit [61] is first used to extract speaker-independent linguistic information from the source waveform and the ASR model is available on the official website 1. After that, the non-autoregressive and end-to-end VC models are adopted to generate the fake audios: Firstly, the non-autoregressive VC model used to convert source speaker timbre to attack speaker timbre is an 8-layer transformer encoder-decoder structure similar\n1https://github.com/wenet-e2e/wenet\nto FastSpeech2 [54] and we choose HifiGAN with multiband processing as the vocoder [57]. On the other hand, the end-to-end VC model consists of a convolutional long short-term memory (CLSTM) encoder and a HiFiGANbased decoder. CLSTM consists of three stacks of convolution layers followed by the LeakyReLU activation function and an LSTM layer. After these VC models are trained, we add the speaker classifier in VC models to adversely constrain the distinctive speaker information to the generation. In the experiments of adversarial constraint on Mel-spectrogram, the learning rate lr in Equation 2 is set to be 8e \u2212 4 and the \u03b4 is updated 1000 times for each mini-batch. We use the l\u221e norm to measure the perturbation bound. The \u03f5 starts from 0.8. While in the experiments on latent representation, the learning rate lr is set to be 1e\u22124 and \u03f5 starts from 0.1. Moreover, the end-to-end VC model is trained with a lr of 5e\u2212 4 and a \u03f5 starting from 0.5, and the \u03b4 is updated 2000 times for each mini-batch. \u2022 Speaker identification model: The SID model we used in this study is ECAPA-TDNN [21], which is also used as the speaker classifier in voice conversion. The EER of this model on the AISHELL-1 test set is 1.91%. The architecture of ECAPA-TDNN in this study incorporates 3 SERes2Block modules, and channel size and the dimension of the bottleneck in the SEBlocks are set to 1024 and 256, respectively. The loss function is additive angular margin softmax (AAM-softmax) [62] with a margin of 0.2 and a scale of 30."
        },
        {
            "heading": "C. Evaluation metrics",
            "text": "We adopt several criteria listed below to measure the performance of various kinds of generated fake audio.\n\u2022 Attack success rate: is used to evaluate the performance of targeted attacks in speaker identification. The attack success rate is also the accuracy predicted from the SID, denoted as Acc. Formally, the accuracy is calculated as:\nAcc = Ns N , (16)\nwhere N is the total number of fake audios we generated to test and Ns refers to the number of audios attacking successfully. The higher the Acc in a targeted attack means the better the targeted attack is conducted. \u2022 Deception success rate (DSR): reflects the degree of fooling the audio deepfake detection model by the generated utterances, which is defined as followed:\nDSR = W\nA \u2217M , (17)\nwhere W is the count of wrong detection samples by all the detection models, A is the count of all the evaluation samples, and M is the number of detection models. Since we follow the rule of ADD challenge track 3.1 to generate fake audio, we also evaluate the DSR in our experiments. In order to prove that our generated timbre-reserved fake audios also have the\nability to cheat the detection systems and have comparable results with the ADD challenge participating teams. We evaluate the fake audio on open-source deep fake detection models provided by the ASVspoof Challenge [63] and ADD Challenge [33], which are available on the websites: https://github.com/asvspoof-challenge/ 2021/tree/main/DF/Baseline-RawNet2 [64] and https:// github.com/clovaai/aasist [65]. We further implement two similar fake detection models as the baseline systems listed in [33], which are based on GMM and ECAPATDNN models. We test the EER of these two models on the test set of Track 1 and 2, which are 25.6%, 33.1%, 45.7%, and 40.7%, respectively. These are comparable to the EER of the baseline system in [33]. \u2022 Objective evaluation: The MOSNet [66] was proposed to automatically predict the mean opinion score (MOS) of an utterance. We use the objective mean opinion score (oMOS) predicted by the MOSNet as an objective metric of the generated fake audio quality. A pretrained MOSNet 2\nis used to make the o-MOS prediction, and the higher o-MOS represents the better quality of the audio. \u2022 Subjective evaluation: We also conduct subjective evaluations to value the fake audio from the human perceptibility of audio. The evaluation set contains 20 samples of each VC system and a total of 100 samples. We use comparative mean opinion score (CMOS) to compare the quality of the fake audios generated by the vanilla VC model and other adversarial based methods. For each pair of utterances in CMOS test, 25 native participants are asked to give a score ranging from -3 (the proposed system is much worse than baseline) to 3 (the proposed system is much better than baseline) with intervals of 1. Moreover, to further effectively evaluate the quality and similarity of the fake audio, we also conduct a MOS test on all the fake audio. Participants are asked to listen to the provided audio and evaluate their quality and similarity on a 5-point scale: 1-bad, 2-poor, 3-fair, 4-good, 5-excellent. \u2022 Character error rate (CER): In order to make sure the generated fake audio meets the specific text requirements, the CER is used to evaluate the intelligibility of the fake audio, the CER is used to evaluate the intelligibility of the fake audio, and can be calculated as follows:\nCER = S +D + I\nN , (18)\nwhere S represents the number of substitution errors, D represents the number of deletion errors, I is the number of insertion errors, and N represents the total number of characters in the reference answer. he model we used to calculate CER is a pre-trained ASR model by the WeNet toolkit [61], which is available on the official website: https://github.com/wenet-e2e/wenet/tree/ main/examples/wenetspeech.\n2https://github.com/lochenchou/MOSNet"
        },
        {
            "heading": "V. EXPERIMENTAL RESULTS AND ANALYSIS",
            "text": "In this section, we conduct experiments to test our proposed strategies on ADD challenge dataset and then give further analyses on our method."
        },
        {
            "heading": "A. Results on different reconstructed representations",
            "text": "Table I shows the attack success rates and deception success rates of the fake audios generated by various kinds of generation methods. The first method is only generated by the vanilla VC model, which is adopted as the lower limit of all the comparisons and is denoted as \u2018VC\u2019 in the Table. The second fake audio generation method is to add adversarial perturbation directly based on the fake audios generated by the VC model using the approach we previously proposed in [31]. Since the perturbation is directly optimized by the SID system and straightforwardly added to the waveform, this method is the upper limit of the adversarial attack and is denoted as \u2018VC+adv\u2019. The rest three proposed methods are the VC model trained with multiple kinds of adversarial constraints as we described in Section III, and we denote these three strategies based on the location of adversarial constraints added as \u2018Mel\u2019, \u2018Latent\u2019, and \u2018Waveform\u2019, respectively.\n1) Attack success rate: As shown in Table I, the attack success rate of VC model based fake audio is 29.60%, while the VC audio with adversarial perturbation achieves 76.50%. Meanwhile, the Acc results of our proposed strategies are 60.58%, 54.94%, and 66.30%, respectively. We can observe that the fake audios generated based on all these three proposed timbre-reserved adversarial strategies are significantly improved compared to the fake audios generated by the original vanilla VC model, which are 30.98%, 25.34%, and 36.7% absolute improvement. And our proposed methods also have comparable results with the direct perturbations to the vanilla VC generated fake audios. Since the adversarial constraint for latent representation is difficult to have an effect on the SID, in other words, it is hard to optimize, the result of the second strategy is the lowest of these three strategies.\nDue to the adversarial constraint being added only during the model training instead of directly added to the attack waveform, the attack success rates of the proposed strategies have a small gap with the VC+adv method. The results also illustrate that the performance varies when the adversarial constraint conducts on the different kinds of reconstructed representation. Moreover, as the reconstructed representation with adversarial constraint gets closer to the SID model, the better attack effect.\n2) Deception success rate: As shown in Table I, the DSR results of different kinds of generation methods. We can observe that the DSR results of all these three proposed strategies outperform the vanilla VC and VC+adv method. In addition, the DSR results of all these fake audio generation methods have a comparable performance with the ADD challenge participating teams [33], which indicates that the fake audio generated by our proposed strategies also has the ability to fool the detection systems."
        },
        {
            "heading": "B. Objective and subjective evaluation",
            "text": "1) Objective evaluation: The MOSNet [66] prediction oMOS is employed as the objective measurement and Figure 6 shows the objective performance of the o-MOS of various generation methods. We can observe that all the o-MOS results of fake audio generated by our proposed strategies are higher than the methods of directly adding adversarial perturbation to the audio. This indicates that compared to methods of adding adversarial perturbation directly to the audio, the fake audios generated based on our proposed strategies have better quality since we add adversarial constraints to the VC model avoiding directly introducing extra perturbations to the fake audio.\n2) Subjective evaluation: We conduct a CMOS evaluation shown in Table II. The result shows that our proposed strategies achieve 0.22, 0.20, and 0.17 CMOS higher than the VC+adv method, demonstrating the fake audio generated by our proposed strategies is more similar to audio generated by the vanilla VC model. Compared to the VC+adv method introducing extra noise to the waveform, our proposed strategies can get rid of the influence of the additional perturbation by adding adversarial constraints in the generation model.\nWe also conduct MOS tests on various generation methods, to evaluate the quality and speaker similarity of all kinds of fake audio. The results of the MOS test are shown in Table II. We can observe from the MOS results of quality that our proposed strategies outperform the VC+adv method, indicating that the audio quality generated from our proposed strategies is better than the VC+adv method, while is comparable to the quality of the audio generated by the vanilla VC model. This demonstrates that the adversarial constraint added during the VC model training does not influence the quality of the generated audio. For the speaker similarity, the MOS results of our proposed strategies are also slightly higher than the VC+adv method.\nThese subjective results demonstrate the proposed strategies based on adversarial constraints can avoid the quality damage\ncaused by extra adversarial perturbations and also do not influence the capability of the VC model."
        },
        {
            "heading": "C. Analysis",
            "text": "1) Speaker similarity of fake audios: As shown in Figure 7 and 8, we utilize a pre-trained speaker encoder [21] to extract speaker embeddings of real and fake speech, which are visualized through t-SNE [67]. The distribution of male and female speakers are respectively visualized. In these two figures, all the \u2018x\u2019 represents the real data from various speakers, while all the \u2018o\u2019 represents the generated fake audio. Different colors on behalf of different speakers. In particular, the fake audios in the left parts of both figures are generated by the vanilla non-autoregressive based VC model, while those in the right are generated by the VC model with adversarial constraint.\nWe can observe the fake audio samples generated by the VC with adversarial constraints are more clustered and nearer to the real audio samples than the samples generated by the original VC model. It means the similarity of the fake audio generated by the proposed method is closer to the real audio of target speakers, since the adversarial constraints exploit the weakness of SID model.\nFrom the analysis above, the fake audios generated by our proposed methods are not only timbre-reserved but also more speaker-wise than the original vanilla VC model. The speaker similarity is improved after the adversarial constraint, which is beneficial to successful attacks.\n2) Intelligibility of fake audios: The CER results are shown in Table III. Since our proposed adversarial attack is also text customized, we can see that the CERs of all kinds of fake audio generation methods are low, which indicates that the text of the fake audio can meet specific requirements. In the specified attack scenarios, we also can customize the text.\nFrom the discussion mentioned above, both speaker similarity and intelligibility are indicated to be guaranteed."
        },
        {
            "heading": "D. Comparison of different kinds of constraints",
            "text": "Cai et al. [68] added a speaker identity-related loss to constrain the centralized to improve the speaker similarity between the synthesized speech and its natural reference audio. This feedback constraint (FC) also can be added to VC model to constrain the distance of the VC predicted Mel-spectrogram and the real Mel-spectrogram. As shown in Table IV, we can observe that our proposed adversarial constraint based on Melspectrogram outperforms the feedback constraint (FC). Since the adversarial constraints exploit the weakness of SID model, it is more beneficial to adversarial attack, while the FC mainly focuses on generating Mel-spectrogram fitting the real Melspectrogram as much as possible."
        },
        {
            "heading": "VI. CONCLUSION",
            "text": "In this study, we propose to generate timbre-reserved fake audios for the adversarial attack in the speaker identification\nsystem. By adding adversarial constraints to different levels of representations of the VC model, we can generate timbrereserved and speaker-wised fake audio to attack the SID model. Experiments on ADD challenge corpus show that our proposed strategies significantly improve the attack success rate compared to the vanilla VC model. The objective and subjective evaluation demonstrate that adversarial constraints do not affect the quality of the VC model and also can get rid of the influence of the extra noise by adversely constraining the fake audio generation model instead of directly adding adversarial perturbation to the waveform. Moreover, we also analyze the speaker similarity and intelligibility of the fake audio, the speaker similarity is improved after the adversarial constraint, which is beneficial to attack successfully. And this can also prove that our generated timbre-reserved fake audio is speaker-wised and the text of our adversarial attack can be customized."
        }
    ],
    "title": "Timbre-reserved Adversarial Attack in Speaker Identification",
    "year": 2023
}