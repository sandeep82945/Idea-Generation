{
    "abstractText": "Spiking Neural Networks (SNNs) emerged as a promising solution in the field of Artificial Neural Networks (ANNs), attracting the attention of researchers due to their ability to mimic the human brain and process complex information with remarkable speed and accuracy. This research aimed to optimise the training process of Liquid State Machines (LSMs), a recurrent architecture of SNNs, by identifying the most effective weight range to be assigned in SNN to achieve the least difference between desired and actual output. The experimental results showed that by using spike metrics and a range of weights, the desired output and the actual output of spiking neurons could be effectively optimised, leading to improved performance of SNNs. The results were tested and confirmed using three different weight initialisation approaches, with the best results obtained using the Barabasi-Albert random graph method.",
    "authors": [
        {
            "affiliations": [],
            "name": "Pavithra Koralalage"
        },
        {
            "affiliations": [],
            "name": "Ireoluwa Fakeye"
        },
        {
            "affiliations": [],
            "name": "Pedro Machado"
        },
        {
            "affiliations": [],
            "name": "Jason Smith"
        },
        {
            "affiliations": [],
            "name": "Salisu Wada Yahaya"
        },
        {
            "affiliations": [],
            "name": "Andreas Oikonomou"
        }
    ],
    "id": "SP:0bd56b69ac09b47b2b8735d0bd39fe44587c46bd",
    "references": [
        {
            "authors": [
                "P. Machado",
                "G. Cosma",
                "T.M. McGinnity"
            ],
            "title": "Natcsnn: A convolutional spiking neural network for recognition of objects extracted from natural images",
            "venue": "International Conference on Artificial Neural Networks, pp. 351\u2013362, Springer, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "S. Schliebs",
                "N. Kasabov"
            ],
            "title": "Evolving spiking neural network\u2014a survey",
            "venue": "Evolving Systems, vol. 4, no. 2, pp. 87\u201398, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "D. Auge",
                "J. Hille",
                "E. Mueller",
                "A. Knoll"
            ],
            "title": "A survey of encoding techniques for signal processing in spiking neural networks",
            "venue": "Neural Processing Letters, vol. 53, no. 6, pp. 4693\u20134710, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A. Fountain",
                "C. Merkel"
            ],
            "title": "Effect of biologically-motivated energy constraints on liquid state machine dynamics and classification performance",
            "venue": "Neuromorphic Computing and Engineering, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "W. Zhang",
                "P. Li"
            ],
            "title": "Spiking neural networks with laterally-inhibited self-recurrent units",
            "venue": "2021 International Joint Conference on Neural Networks (IJCNN), pp. 1\u20138, IEEE, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A.V. Gavrilov",
                "K.O. Panchenko"
            ],
            "title": "Methods of learning for spiking neural networks. a survey",
            "venue": "2016 13th International Scientific- Technical Conference on Actual Problems of Electronics Instrument Engineering (APEIE), vol. 2, pp. 455\u2013460, IEEE, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "S. Hwang",
                "J. Lee",
                "J. Kung"
            ],
            "title": "Adaptive input-to-neuron interlink development in training of spike-based liquid state machines",
            "venue": "2021 IEEE International Symposium on Circuits and Systems (ISCAS), pp. 1\u2013 5, IEEE, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J. Gupta",
                "S. Sharma"
            ],
            "title": "Design and analysis of configurable multilayer spiking neural network",
            "venue": "2020 2nd PhD Colloquium on Ethically Driven Innovation and Technology for Society (PhD EDITS), pp. 1\u20132, IEEE, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "A. Mohemmed",
                "S. Schliebs",
                "S. Matsuda",
                "N. Kasabov"
            ],
            "title": "Training spiking neural networks to associate spatio-temporal input\u2013output spike patterns",
            "venue": "Neurocomputing, vol. 107, pp. 3\u201310, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "A.N. Burkitt"
            ],
            "title": "A review of the integrate-and-fire neuron model: I. homogeneous synaptic input",
            "venue": "Biological cybernetics, vol. 95, no. 1, pp. 1\u201319, 2006.",
            "year": 2006
        },
        {
            "authors": [
                "J. Vreeken"
            ],
            "title": "Spiking neural networks, an introduction",
            "venue": "2003.",
            "year": 2003
        },
        {
            "authors": [
                "J.L. Lobo",
                "J. Del Ser",
                "A. Bifet",
                "N. Kasabov"
            ],
            "title": "Spiking neural networks and online learning: An overview and perspectives",
            "venue": "Neural Networks, vol. 121, pp. 88\u2013100, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "A. Taherkhani",
                "A. Belatreche",
                "Y. Li",
                "G. Cosma",
                "L.P. Maguire",
                "T.M. McGinnity"
            ],
            "title": "A review of learning in biologically plausible spiking neural networks",
            "venue": "Neural Networks, vol. 122, pp. 253\u2013272, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "A. Tavanaei",
                "M. Ghodrati",
                "S.R. Kheradpisheh",
                "T. Masquelier",
                "A. Maida"
            ],
            "title": "Deep learning in spiking neural networks",
            "venue": "Neural networks, vol. 111, pp. 47\u201363, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "E.M. Izhikevich"
            ],
            "title": "Simple model of spiking neurons",
            "venue": "IEEE Transactions on neural networks, vol. 14, no. 6, pp. 1569\u20131572, 2003.",
            "year": 2003
        },
        {
            "authors": [
                "V.N. Zadorozhnyi",
                "E.B. Yudin"
            ],
            "title": "Structural properties of the scalefree barabasi-albert graph",
            "venue": "Automation and Remote Control, vol. 73, no. 4, pp. 702\u2013716, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "J. Gomez-Gardenes",
                "Y. Moreno"
            ],
            "title": "From scale-free to erdos-renyi networks",
            "venue": "Physical Review E, vol. 73, no. 5, p. 056124, 2006.",
            "year": 2006
        },
        {
            "authors": [
                "D. Bowes",
                "R. Adams",
                "L. Canamero",
                "V. Steuber",
                "N. Davey"
            ],
            "title": "The role of lateral inhibition in the sensory processing in a simulated spiking neural controller for a robot",
            "venue": "2009 IEEE Symposium on Artificial Life, pp. 179\u2013183, IEEE, 2009.",
            "year": 2009
        },
        {
            "authors": [
                "Z. Fang",
                "Z. Dawei",
                "Z. Ke"
            ],
            "title": "Image pre-processing algorithm based on lateral inhibition",
            "venue": "2007 8th international conference on electronic measurement and instruments, pp. 2\u2013701, IEEE, 2007.",
            "year": 2007
        },
        {
            "authors": [
                "Z. Zhao",
                "L. Qu",
                "L. Wang",
                "Q. Deng",
                "N. Li",
                "Z. Kang",
                "S. Guo",
                "W. Xu"
            ],
            "title": "A memristor-based spiking neural network with high scalability and learning efficiency",
            "venue": "IEEE Transactions on Circuits and Systems II: Express Briefs, vol. 67, no. 5, pp. 931\u2013935, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "W. Gerstner",
                "R. Kempter",
                "J.L. Van Hemmen",
                "H. Wagner"
            ],
            "title": "A neuronal learning rule for sub-millisecond temporal coding",
            "venue": "Nature, vol. 383, no. 6595, pp. 76\u201378, 1996.",
            "year": 1996
        },
        {
            "authors": [
                "N. Rathi",
                "P. Panda",
                "K. Roy"
            ],
            "title": "Stdp-based pruning of connections and weight quantization in spiking neural networks for energy-efficient recognition",
            "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, vol. 38, no. 4, pp. 668\u2013677, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "Q. Wang",
                "P. Li"
            ],
            "title": "D-lsm: Deep liquid state machine with unsupervised recurrent reservoir tuning",
            "venue": "2016 23rd International Conference on Pattern Recognition (ICPR), pp. 2652\u20132657, IEEE, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "J.D. Victor"
            ],
            "title": "Spike train metrics",
            "venue": "Current opinion in neurobiology, vol. 15, no. 5, pp. 585\u2013592, 2005.",
            "year": 2005
        },
        {
            "authors": [
                "J. Dauwels",
                "F. Vialatte",
                "T. Weber",
                "A. Cichocki"
            ],
            "title": "On similarity measures for spike trains",
            "venue": "International Conference on Neural Information Processing, pp. 177\u2013185, Springer, 2008.",
            "year": 2008
        },
        {
            "authors": [
                "T. Kreuz",
                "J.S. Haas",
                "A. Morelli",
                "H.D. Abarbanel",
                "A. Politi"
            ],
            "title": "Measuring spike train synchrony",
            "venue": "Journal of neuroscience methods, vol. 165, no. 1, pp. 151\u2013161, 2007.",
            "year": 2007
        },
        {
            "authors": [
                "J.D. Victor",
                "K.P. Purpura"
            ],
            "title": "Nature and precision of temporal coding in visual cortex: a metric-space analysis",
            "venue": "Journal of neurophysiology, vol. 76, no. 2, pp. 1310\u20131326, 1996.",
            "year": 1996
        },
        {
            "authors": [
                "S. Schreiber",
                "J.-M. Fellous",
                "D. Whitmer",
                "P. Tiesinga",
                "T.J. Sejnowski"
            ],
            "title": "A new correlation-based measure of spike timing reliability",
            "venue": "Neurocomputing, vol. 52, pp. 925\u2013931, 2003.",
            "year": 2003
        },
        {
            "authors": [
                "J.D. Hunter",
                "J.G. Milton"
            ],
            "title": "Amplitude and frequency dependence of spike timing: implications for dynamic regulation",
            "venue": "Journal of neurophysiology, vol. 90, no. 1, pp. 387\u2013394, 2003.",
            "year": 2003
        },
        {
            "authors": [
                "D. Das",
                "S. Bhattacharya",
                "U. Pal",
                "S. Chanda"
            ],
            "title": "Plsm: A parallelized liquid state machine for unintentional action detection",
            "venue": "arXiv preprint arXiv:2105.09909, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Z. Kang",
                "S. Wang",
                "L. Wang",
                "S. Li",
                "L. Qu",
                "W. Xu"
            ],
            "title": "Hardware-aware liquid state machine generation for 2d/3d network-on-chip platforms",
            "venue": "Journal of Systems Architecture, vol. 124, p. 102429, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "S. Tian",
                "L. Qu",
                "L. Wang",
                "K. Hu",
                "N. Li",
                "W. Xu"
            ],
            "title": "A neural architecture search based framework for liquid state machine design",
            "venue": "Neurocomputing, vol. 443, pp. 174\u2013182, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "F. Ponulak",
                "A. Kasi\u0144ski"
            ],
            "title": "Supervised learning in spiking neural networks with resume: sequence learning, classification, and spike shifting",
            "venue": "Neural computation, vol. 22, no. 2, pp. 467\u2013510, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "M.-O. Gewaltig",
                "M. Diesmann"
            ],
            "title": "Nest (neural simulation tool)",
            "venue": "Scholarpedia, vol. 2, no. 4, p. 1430, 2007.",
            "year": 2007
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014Spiking Neural Networks (SNN), Liquid State Machine (LSM), Victor Purpura Distance, Van Rossum Distance, Excitatory Synapses, Inhibitory Synapses, NEST, Weight initialisation, random weights, Barabasi-Albert graph, Erdos Renyi graph\nI. INTRODUCTION\nThe evolving journey of Artificial Neural Network (ANN) is based on attempting to mimic the behaviour of biological neurons. The capability of the mammalian brain to understand and process remarkable information within an unbelievably short amount of time is taken into consideration when enhancing the neural networks.Spiking Neural Network (SNN) has evolved based on the behaviour of the mammalian visual cortex. The visual cortex is mainly responsible for advanced image processing within a short period of time. Similar to the behaviour of the visual cortex of the mammalian brain, SNN use spikes over time as the input for the network, and neurons spike when they reach their threshold value. This mimicked behaviour capable of offering low-power and highperformance computational paradigm [1]\u2013[3].\nLiquid State Machine (LSMs) are a type of recurrent SNN that is considered to be more biologically accurate than other neural network methods [4]. LSMs are well-known for their computational power and low energy consumption, making them a commonly used tool for complex classification tasks such as speech recognition and image classification. Optimising the training process is a crucial aspect in the development of SNNs, but it still lacks proper research attention. The\neffective assignment of weight ranges in SNNs plays a crucial role in achieving accurate solutions that are also energy efficient.\nThis article presents a comprehensive investigation into optimising the training process of the LSM. The study sheds light on how to make use of spike metrics to compare spike trains and determine the accuracy of the output. The article also highlights the importance of balanced excitatory and inhibitory synapses in achieving optimal output. In addition, the study explains the significance of weight initialisation, which involves assigning initial weights to the synapses, and how it can impact the performance of the LSM. Through experiments with different weight ranges, the article identifies the most effective weight range for the LSM, making it a valuable resource for researchers and practitioners looking to optimise their training processes.\nThe article is organised as follows: Section II presents a thorough review of the relevant literature in the field. The methodology used in the study is described in detail in Section III. The results of the analysis are presented in Section IV, where the findings are discussed and interpreted. Finally, the article concludes in Section V which summarises the main contributions of the study and outlines potential future work in the field."
        },
        {
            "heading": "II. LITERATURE REVIEW",
            "text": "SNNs have emerged as the third generation of ANNs, surpassing the limitations of traditional neural networks [5]. SNNs mimic the biological processes of the human brain, making them capable of providing high-performance solutions with low energy consumption. These networks have proven to solve complex problems that were difficult to address with classical neural networks [3], [6].\nTraditional neural networks rely on stochastic gradient descent algorithms, which require all neurons\u2019 weights to be adjusted in each iteration through the backpropagation algorithm. This constant adjustment of neurons\u2019 weights requires a significant amount of power and time. On the other hand, SNNs only activate neurons that reach their threshold values. The communication between neurons in SNNs is also done via discrete spikes over time, rather than the continuous flow\nar X\niv :2\n30 2.\n03 50\n6v 2\n[ cs\n.N E\n] 1\n0 Se\np 20\nof information in traditional neural networks. This activation process requires much less energy and time, making SNNs an efficient alternative to traditional neural networks [7].\nAn illustration of biological neurons with incoming postsynaptic potentials including incoming Spike After Potential (SAP) and behaviour of Inhibitory Post Synaptic Potential (IPSP) and Excitatory Post Synaptic Potential (EPSP) are shown in Fig. 1.\nA key characteristic of SNN is their ability to mimic the behaviour of the mammalian brain and process complex information quickly and accurately. This is achieved through their utilisation of various network architectures, including feedforward, recurrent, and hybrid models [5]. Among these, the feed-forward architecture is the most commonly used due to its simplicity and ease of transitioning from traditional neural networks [8]. A basic SNN structure may consist of three layers, with the hidden layer comprised of spiking neurons. Input information can be converted into spike trains through encoding techniques such as rate coding and temporal coding [7], [9].\nThe Integrate-and-Fire (IaF) is a simplistic spiking neuron model that treats the neuron as an electronic circuit. The membrane potential is calculated based on the synaptic input and the input current, and spikes are generated when the membrane potential reaches the threshold. This model is similar to the Leaky-Integrate-and-Fire (LIF) model, but with a focus on the neuron\u2019s ability to mimic the spike-generating behaviour of real neurons, rather than the leaky behaviour of traditional electronic circuits [10].\nThe LIF model shown in Fig. 2, on the other hand, is a widely-used spiking neuron model that is popular due to its low computational cost, ease of coding, and high degree of\naccuracy. The LIF model captures the characteristic properties of external input based on the charge across a leaky membrane with a clear threshold, making it a one-dimensional SNN. Like the IaF model, the LIF neuron is considered as an electronic circuit and simulates the potential of the neuron as a voltage across a capacitor connected in parallel in a leaky conductance path. The dynamics of the membrane potential u(t) of a single neuron can be calculated using the following equation:\ndu(t)\ndt = urest \u2212 u(t) \u03c4m + I(t) cm (1)\nwhere \u03c4m is membrane time constant, urest is resting potential cm is membrane capacitance and I(t) is neuron input [13], [14]\nRealistic neuron models such as the Hodgkin-Huxley and FitzHugh-Nagumo [12] offer greater biological accuracy, but come at the cost of increased computational demands. On the other hand, the Izhikevich model [15] strikes a balance between biological realism and computational efficiency, offering more biological accuracy than the Hodgkin-Huxley model while still having a computational cost similar to the simpler LIF model. However, these complex models may not be suitable for future applications that require acceleration of the SNN using dedicated parallelisable hardware, due to their higher computational demands.\nThe behaviour of a spiking neuron is determined by the type of connections it has with other neurons, which can be classified as either excitatory or inhibitory. Excitatory connections positively impact the network\u2019s output, and the connected neurons are known as excitatory neurons. In contrast, inhibitory connections negatively impact the network\u2019s output and the connected neurons are referred to as inhibitory neurons. Fast spiking neurons emit consistent, high-frequency spikes, while Low-threshold spikes emit high-frequency spikes with varying frequency. The behaviour of spiking neurons can be depicted in Fig. 3 with compared to biological neuron.\nThe crucial role of excitatory and inhibitory neurons in the functioning of SNNs lies in their ability to identify relevant features and activate only the necessary neurons with the appropriate threshold, thereby increasing the efficiency of the SNNs [15].\nThe method of weight initialisation for synapses is a crucial factor in the performance of a SNN. The weight values of synapses are among the most critical parameters that shape the behaviour of neurons within the network. There are various techniques for initialising the weight values of synapses, including random weight assignment within a specified range, the use of random graph models, etc. One such model is the Barabasi-Albert Graph, proposed by A. Barabasi and R. Albert in 1999. This model depicts the random growth of a network by adding edges to existing vertices in a manner proportional to the number of connections of a given vertex. Essentially, the more connections a vertex has, the higher the likelihood of it receiving new connections [16]. On the other hand, the Erdos Renyi Graph is one of the earliest forms of random graph models. This graph is created by randomly connecting nodes with a specified probability of each node [17]. Unlike the Barabasi-Albert Graph, the Erdos Renyi Graph does not exhibit preferential connections between nodes. In this article, three main weight initialisation techniques were employed. The first approach involved randomly assigning weights within a range for the synapses within the SNN. The Barabasi-Albert graph and the Erdos Renyi random graph were then utilised as the next methods for assigning weights to the synapses.\nThe role of Lateral Inhibition in optimising the performance of SNN is a crucial topic of discussion. Lateral Inhibition is a biologically-inspired mechanism in the retina, which is responsible for edge detection and wide-range light intensity processing, as explained by Bowes et al. [18]. Electrophysiological experiments by Hartline and his research team identified the lateral inhibition process in the neural circuit [19]. These experiments revealed that receptors in the network are inhibited by their adjacent receptors, and inhibition among cells is mutual. The inhibition from nearby receptors is stronger compared to that from far away receptors, leading to inconsistent receptor activities.\nIn SNNs, lateral inhibition works by sending signals to other neurons to prevent them from firing for a targeted spike, thus allowing only the relevant neurons to be activated and reducing\ninformation overload [7]. This mechanism reduces the need for membrane reset functions and helps to prevent neurons from over-saturation. Zhao et al. [20] discussed a method to reduce the complexity of the lateral inhibition connection in the SNN, leading to increased scalability compared to the two commonly used methods, which are challenging to implement. These two methods are: (1) every excitatory neuron is connected to a relevant inhibitory neuron and (2) every inhibitory neuron is connected to all excitatory neurons, except the neuron receiving the spike. Zhang et al. [5] proposed a conceptual diagram for the use of lateral inhibition in the SNN architecture, using backpropagation to calculate errors and adjust weights. As described by Zhang et al. [5], lateral inhibition in the SNN reduces the activities of all other neurons once a desired output spike is generated.\nLearning algorithms are also crucial in projecting inputs towards a desired output and for accurately identifying important features. Unlike classical neural networks, the learning algorithms used in SNN are based on spike trains and the time between each spike, rather than stochastic gradient descent. Three commonly used learning algorithms in SNN are Hebbian learning, Oja learning, and Synaptic Time Dependent Plasticity (STDP). Hebbian learning is a well-known and established method for training neural networks, and it is unique in that it is not based on gradient descent. Instead, weights are based on the correlation between inputs and outputs. This makes Hebbian learning more biologically plausible and more efficient for situations where gradient descent is not possible, and it can be used for both supervised and unsupervised tasks. Oja learning builds upon the Hebbian approach but addresses the issue of unbound weight growth, which can contradict experimental results in neural networks. Oja learning suggests normalising the synaptic weights before using them in the network to provide a fair range of values for learning. STDP is a Hebbian-based unsupervised learning algorithm that offers several advantages over classical Neural Networks learning algorithms. It is based on local event-based data and does not require additional energy or a large set of annotated data. In STDP, synapse weights are determined by the time difference between pre-spikes and post-spikes of neurons. It is mainly used to train excitatory neurons and classify them as critical or non-critical neurons. The weight update can be calculated using the equation provided in the original text.\n\u2206wj = N\u2211 f=1 N\u2211 n=1 W (tin \u2212 t j f ) (2)\nThe alteration in the weight \u2206wj of a synapse connecting presynaptic neuron j is based on the correlation between the timing of presynaptic spike arrivals and postsynaptic spikes. The arrival times of presynaptic spikes at synapse j are represented as tjf , where f = 1, 2, 3, ... denotes the number of presynaptic spikes. Similarly, the firing times of the postsynaptic neuron are represented as tin, where n = 1, 2, 3, ... labels the firing events. The overall weight change \u2206wj that\nresults from a stimulation protocol that involves a sequence of pre and postsynaptic spikes is expressed as [21].\nW (x) = A+ + exp(\u2212x/\u03c4+), forx > 0 (3)\nThe STDP function W (x) determines the degree of synaptic strengthening or weakening based on the relative timing between presynaptic and postsynaptic spikes. It is a crucial component of the STDP learning algorithm, and its selection can greatly impact the performance of the network. A widely used form of the STDP function is the exponential function, which has been shown to effectively model the synaptic plasticity observed in biological neurons [21].\nW (x) = \u2212A\u2212 + exp(\u2212x/\u03c4\u2212), forx < 0 (4)\nThe parameters A+ and A\u2212 may vary depending on the current weight wj of the synapse. The time constants of the STDP functions are typically around 10ms for both \u03c4+ and \u03c4\u2212 [21].\nThe alteration in weight is determined by the timing relationship between the arrival of the presynaptic spike and the postsynaptic spike. If the postsynaptic spike occurs directly after the presynaptic spike, the weight will increase. On the other hand, if there is a considerable time gap between the two spikes, the weight will decrease. The weight updates are based on the importance of the neurons, determined by their weight increase. If a neuron has a noticeable weight increase, it is considered critical and its weight is updated. Conversely, if a neuron does not show a significant weight increase, it is considered non-critical and its weight is set to zero. During the training process, the training data set is divided into multiple batches. The weights of critical neurons are updated, while the weights of non-critical neurons are set to zero. However, these non-critical neurons can still contribute to later batches, as they may become critical for different input sets. This approach makes the network scalable and stable, as reported in various studies [6], [22], [23].\nThe properties and patterns of spikes and spike trains play a crucial role in determining the similarities and differences among spikes. This is essential for capturing important data within a spiking neural network to achieve optimised results. Victor et al. [24] categorise spike metrics into three types: spike-time metrics, spike-interval metrics, and multi-neuronal cost-based metrics. The choice of metric depends on the cost of inserting and deleting inter-spike intervals and the need to identify spike patterns as a series of events. Dauwels et al. [25] and Kreuz et al. [26] discuss two main spike train metrics. The Victor-Purpura Spike Train Metric calculates the distance between two point processes through minimum cost transformations using event insertion, deletion, and movement operations. The cost of event insertion or deletion is set to one and movement is proportional to the time change [27]. The Van Rossum Similarity Measure converts the two point processes into continuous time series. The Schreiber et al. [28] Similarity Measure is based on correlation and involves convolving each spike with a filter of fixed width. The Hunter-Milton Similarity\nMeasure begins from the nearest spike and is calculated as the average of the entire series [29].\nLSMs can be defined as a biologically inspired recurrent spiking neural network that has high computational power and is widely used for tasks such as speech recognition, image classification, and word recognition. When compared to classical neural networks, LSMs performs well with high accuracy and low power consumption. LSMs are composed of three main components: the input layer, the liquid layer, and the read-out layer [4], [23], [30]\u2013[32]. The input layer converts the input spike trains into spikes using encoding mechanisms, such as rate coding, time coding, and phase coding, and the weights are initialised using a fixed weight matrix. The liquid layer, consisting of LIF neurons, extracts random features from the inputs, and there are two main types of neurons within the liquid layer: primary and auxiliary neurons, which are divided based on their role in the feature extraction process. The read-out layer provides the final output of the network and is responsible for training the liquid layer to read-out layer connected neurons weights. To optimise the training process of LSMs, a supervised-learning algorithm named ReSuMe can be used [4], [23], [30]\u2013[32]. The ReSuMe algorithm trains the weights in the read-out layer based on the desired output and the actual output of the network [33]. During the training phase, the ReSuMe algorithm adjusts the weights in the readout layer to minimise the difference between the desired output and the actual output. The algorithm performs this by updating the weights based on the error gradient, calculated using the backpropagation algorithm, and the learning rate, which determines the speed of the weight update. The ReSuMe algorithm is an effective solution for optimising the training process of LSM and improving its accuracy and performance.\nFinally, the training process of LSM can be performed using the supervised-learning ReSuMe algorithm or unsupervised learning STDP. When the expected patterns are known, ReSuMe can be used to train the network by updating the weights from the liquid layer to the read-out layer. ReSuMe uses a suitable learning algorithm to adjust the weights and provide the desired output. On the other hand, when the expected patterns are not known, unsupervised learning STDP can be used to train the network by using STDP rules to adjust the weights. In conclusion, LSM has proven to be a powerful and efficient tool for processing complex information with high accuracy and low energy consumption."
        },
        {
            "heading": "III. METHODOLOGY",
            "text": "As part of the methodology for this study and experiment setting, the selection of a SNN simulator is a crucial aspect. NEST [34] was selected for its extensive documentation, plentiful examples, and extensive simulation capabilities, making it easier for the developer to get started. The NEST simulation environment was set up in Oracle Virtualbox1, and Python was used as the programming language, with Jupyter Notebook as the computing platform. Several NEST library packages [34]\n1Available online: https://www.virtualbox.org/, last accessed 06/06/2022\nwere utilised, including network topology creation, weight setting, and recording device connection to monitor and record spikes generated by the neurons.\nThe initial step was to determine a method of evaluating the spikes generated by the network. This involved comparing the desired output spike train and the actual spike train generated by the network to measure their deviation and assess the impact of the weight assigned to each synapse on the error.\nThe main evaluation tool was the Victor Purpura (VP) spike metric, and the Van Rossum (VR) spike metric was used as a secondary option if the VP distance was not sufficient for comparing the actual and desired spike trains. The Elephant - Electrophysiology Analysis Toolkit2 was used to calculate the VP and VR distances.\nLIF neurons were used to build the network, and various experiments were conducted to understand the behaviour of the neurons based on configurations such as threshold voltage and current. Spike recorders were attached to both the input and output neurons, with the input neuron\u2019s spike train serving as the desired output and the output neuron\u2019s spike train as the actual output for comparison.\nThe process of defining the SNN topology started with the simplest basic topology and gradually progressed to more complex ones. Neurons were connected by synapses, and their weights were randomly assigned within a predefined range. At first, only excitatory synapses were evaluated, with inhibitory synapses added later. The first network topology used was a 2-layer SNN consisting of 2 SNN neurons for input and another 2 SNN neurons for output, connected by random weights within the range of 10-400, and all synapses were STDP synapses.\nFig. 4 depicts the network topology of a 2-layer SNN. After conducting experiments and analysing results from the 2-layer network, it was decided to expand the network to a 3-layer topology. The new network consisted of input, middle/hidden, and output layers, each with 5 LIF neurons. However, the comparison between the desired and actual spike trains through the use of VP and VR distances did not show a significant difference from the 2-layer network, leading to\n2Available online: https://elephant.readthedocs.io/en/latest/, last accessed on 10/08/2022\nthe decision to increase the number of neurons in each layer to 100. This allowed for further observation of the network\u2019s behaviour when different weight ranges were assigned. Building upon the insights gained from the previous experiments, the implementation was further expanded to include the LSM architecture, which can be visualised in Fig. 5.\nThe defined LSM architecture consists of two neurons in the input layer, eight neurons in the liquid layer, and two neurons in the read-out layer. In the first epoch of the training, the neurons in the liquid layer are connected to each other with random weights within a predefined range. Subsequently, in each epoch, the weights between the input, liquid and the readout layers respectively are assigned randomly through STDP synapses. The weights and connections within the liquid layer remain unchanged during the training process.\nThe aim of this approach is to assign weights with a more sensible direction instead of random assignments and to determine a weight range that results in the minimum VP and VR distances. The LSM approach has revealed a range of weights that result in the lowest VP and VR distances. However, the problem with this approach is that the weights are assigned randomly, making it difficult to verify if the weight range is the optimal one for the training process.\nTo address this issue, weights are initialised using random graphs. The Barabasi-Albert graph is used to generate a random graph of weights within the defined range. The weights are then ordered according to the degree of vertices, with the first 80% of nodes with the highest degree of vertices being assigned as the weights for training in the input layer to the liquid layer and the liquid layer to the read-out layer.\nThe interconnection of neurons within the liquid layer takes place in the first epoch, with random weights between two randomly selected neurons. A fixed number of inhibitory synapses are added. The input and output neurons are connected directly with a high fixed weight to ensure that the output neuron spikes when the input neuron spikes.\nThe same weight array is generated using the Erdos Renyi graph and used in the synapses similarly to the Barabasi-\nAlbert graph. The NetworkX python package3 is used for implementation and to generate these random graphs."
        },
        {
            "heading": "IV. RESULTS ANALYSIS",
            "text": "In this section, the relevant results are discussed. The 2- layer SNN topology, consisting of 2 LIF neurons per layer, was tested with various weight ranges, and it was found that a significant difference in VP and VR distance was observed throughout the weight range of 10 and 400.\nFig. 6 shows the VP difference between input spikes and output spikes. It is clearly visible that the difference drastically increases when the weight hits 400. Weights below 400 result in a minimum VP difference.\nFig. 7 depicts the VR difference of input and output spikes also shows the same behaviour as Victor Purpura difference. The difference starts to increase when the weight reaches 400.\nHowever, there was not a noticeable decrease in the distance when the weight was assigned within a certain range. Therefore, the results of the 3-layer SNN with a small number of neurons did not show any clear evidence of a weight range that could result in the lowest VP and VR distances. To\n3Available online, https://networkx.org/, last accessed 03/02/2023\nfurther explore this issue, a 3-layer SNN was implemented with a larger population of neurons. This time, the experiment showed a noticeable decrease in the VP and VR distances when the weights were assigned within a specific range. This result highlights the importance of using a large number of neurons when implementing a 3-layer SNN topology.\nThe experiment also revealed that the weight range can play a crucial role in the performance of SNNs. It is essential to find the optimal weight range that can provide the lowest VP and VR distances to enhance the accuracy and efficiency of the SNNs.\nFig. 8 and Fig. 9 show the behaviour of input and output spike over the weights, the increase of weight shows a significant VP and VR differences increase. Weights below 6 are the best range, which results in the lowest difference.\nThe results of the above experiments on various network topologies indicated that the discrepancy between the actual and desired output increased with the increase in weight value. This trend was also observed in LSM. Through weight assignment within the range of 1-50, it was observed that the lowest difference was achieved within the weight range of 10- 20. In order to improve the results further, the experiment was\nextended to use a random graph weight initialisation method, the Barabasi-Albert method, instead of purely random weights. The results of this implementation reveals the impact of weight initialisation on the accuracy of the system.\nFrom the results depicted in Fig. 10 and Fig. 11, it was observed that the Victor Purpura difference of input spikes and output spikes over weight values displayed a more focused distribution compared to the Van Rossum difference graph. The latter demonstrated that the lowest difference was obtained within the weight range of 10-20. Both graphs clearly indicate that the difference in spikes increases as the weight values increase. The same experiment was then repeated using Erdos Renyi as the random graph instead of the Barabasi-Albert graph.\nIn the final experiment, LSM was tested with three main weight initialization approaches for various weight ranges, including 1-10, 1-20, 1-50, and 1-100, among others. The behaviours of the VP and VR differences were closely monitored for each weight range, which was initialised using purely random initialisation, the Barabasi-Albert graph, and the Erdos Renyi graph. Table I showcases the lowest difference achieved for each weight range using the three main weight initialisation methods.\nThe highlighted rows are the minimum VR difference\nobtained in each weight range. As per the above experiment, it is clearly visible that the majority of the least difference was obtained through Barabasi-Albert random graphs and the best weight range for all the experiment results is below 20."
        },
        {
            "heading": "V. CONCLUSION AND FUTURE WORK",
            "text": "Based on the results of the research, it can be concluded that the difference between the input spike train and output spike train, as measured by both VP and VR distances, increases as the weights of the synapses increase. The Victor Purpura distance has a clearer distinction when the neuron population is large, and for such populations, the relationship between weights and the VP orVR distances follows an exponential pattern.\nIn the LSM experiments, the VP distance remained constant, while the VRdifference was affected by the weight range. This highlights the importance of using multiple spike metrics when observing network behaviour. The relationship between weights and spike metric differences was consistent across all network topologies tested in the research. The identified weight range for LSM topology is the optimal range for assigning excitatory synapses in order to minimise the spike metric difference. As further research, the weight range for inhibitory synapses can be determined in order to result in a similar spike train to the input spike train. The current implementation does not take into account lateral inhibition, so this will be a future area of research to observe the impact of this property on training optimisation. Additionally, the propagation delays of pre-synaptic neurons have not been considered in the current research, so this will also be a future area of investigation to improve the accuracy of the solution.\nFrom a societal perspective, the findings of this research have implications for the broader field of Artificial Intelligence (AI). Understanding the relationship between synaptic weights and spike metric differences contributes to the development of more efficient and accurate neural network models. These advancements can positively impact various AI applications, such as image recognition, natural language processing, and autonomous systems, leading to improved performance and reliability. Moreover, the optimization of training processes based on these findings can reduce computational resources and energy consumption, making AI systems more sustainable. In terms of novel exploitation and business models, the research outcomes offer opportunities for developing specialised AI solutions. Companies can leverage the insights gained from this research to create advanced neural network architectures with optimized synaptic weights. Such innovations can be applied in areas like anomaly detection, pattern recognition, and predictive analytics, enabling businesses to make more informed decisions and enhance operational efficiency. Additionally, organizations specializing in AI hardware and software can incorporate these findings into their products and services, providing customers with more efficient and accurate AI tools.\nThe current experiments have only been performed with small populations of neurons. It is expected that noise will\nincrease in larger populations, so it will be important to incorporate a mechanism for reducing noise in future work. Furthermore, the current approach can be extended to consider other important factors that can impact the training optimisation process of LSM, such as lateral inhibition, propagation delays of pre-synaptic neurons, noise reduction mechanisms, and the impact of larger neuron populations. Additionally, the solution of this research can be developed into a package or library that can be used by researchers and developers to optimise the range of weights for different SNN topologies. This package should be able to provide the optimal weight range based on inputs such as network topology, number of neurons per layer, number of synapses, and type of synapses."
        }
    ],
    "title": "Weights Initialisation of Liquid State Machines",
    "year": 2023
}