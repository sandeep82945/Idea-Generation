{
    "abstractText": "Precisely detecting solar active regions (AR) from multi-spectral images is a challenging task yet important in understanding solar activity and its influence on space weather. A main challenge comes from each modality capturing a different location of these 3D objects, as opposed to more traditional multi-spectral imaging scenarios where all image bands observe the same scene. We present a multi-task deep learning framework that exploits the dependencies between image bands to produce 3D AR detection where different image bands (and physical locations) each have their own set of results. Different feature fusion strategies are investigated in this work, where information from different image modalities is aggregated at different semantic levels throughout the network. This allows the network to benefit from the joint analysis while preserving the band-specific information. We compare our detection method against baseline approaches for solar image analysis (multichannel coronal hole detection, SPOCA for ARs (Verbeeck et al. Astron Astrophys 561:16, 2013)) and a state-of-the-art deep learning method (Faster RCNN) and show enhanced performances in detecting ARs jointly from multiple bands. We also evaluate our proposed approach on synthetic data of similar spatial configurations obtained from annotated multi-modal magnetic resonance images.",
    "authors": [],
    "id": "SP:aaea1ca64f4b2f27b6d319e8d0c0969a012d6724",
    "references": [
        {
            "authors": [
                "M Almahasneh",
                "A Paiement",
                "X Xie",
                "J. Aboudarham"
            ],
            "title": "Active region detection in multi-spectral solar images. In: International conference on pattern recognition applications and methods; 2021",
            "year": 2021
        },
        {
            "authors": [
                "A Benkhalil",
                "V Zharkova",
                "S Zharkov",
                "S. Ipson"
            ],
            "title": "Active region detection and verification with the solar feature catalogue",
            "venue": "Sol Phys. 2006;235:87\u2013106",
            "year": 2006
        },
        {
            "authors": [
                "J. Bezdek"
            ],
            "title": "Objective function clustering. In: Pattern recognition with fuzzy objective function algorithms. Advanced applications in pattern",
            "year": 1981
        },
        {
            "authors": [
                "B Crabbe",
                "A Paiement",
                "S Hannuna",
                "M. Mirmehdi"
            ],
            "title": "Skeleton-free body pose estimation from depth images for movement analysis",
            "venue": "IEEE international conference on computer vision workshops;",
            "year": 2015
        },
        {
            "authors": [
                "J Dai",
                "Y Li",
                "K He",
                "J. Sun"
            ],
            "title": "R-fcn: Object detection via region-based fully convolutional networks. In: Advances in neural information processing systems",
            "venue": "Curran Associates,",
            "year": 2016
        },
        {
            "authors": [
                "N Dalal",
                "B. Triggs"
            ],
            "title": "Histograms of oriented gradients for human detection",
            "venue": "IEEE computer society conference on computer vision and pattern recognition;",
            "year": 2005
        },
        {
            "authors": [
                "A Eitel",
                "JT Springenberg",
                "L Spinello",
                "M Riedmiller",
                "W. Burgard"
            ],
            "title": "Multimodal deep learning for robust rgb-d object recognition",
            "venue": "IEEE/RSJ international conference on intelligent robots and systems;",
            "year": 2015
        },
        {
            "authors": [
                "MO Gani",
                "S Kuiry",
                "A Das",
                "M Nasipuri",
                "N. Das"
            ],
            "title": "Multispectral object detection with deep",
            "year": 2021
        },
        {
            "authors": [
                "Z Guo",
                "X Li",
                "H Huang",
                "N Guo",
                "Q. Li"
            ],
            "title": "Deep learning-based image segmentation on multimodal medical imaging",
            "venue": "IEEE Trans Radiat Plasma Med Sci",
            "year": 2019
        },
        {
            "authors": [
                "S Hwang",
                "J Park",
                "N Kim",
                "Y Choi",
                "I. Kweon"
            ],
            "title": "Multispectral pedestrian detection: Benchmark dataset and baselines",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition;",
            "year": 2015
        },
        {
            "authors": [
                "T Ishii",
                "E Simo-Serra",
                "S Iizuka",
                "Y Mochizuki",
                "A Sugimoto",
                "H Ishikawa",
                "R. Nakamura"
            ],
            "title": "Detection by classification of buildings in multispectral satellite imagery",
            "venue": "In: International conference on pattern recognition;",
            "year": 2016
        },
        {
            "authors": [
                "R Jarolim",
                "A Veronig",
                "S Hofmeister",
                "M Temmer",
                "S Heinemann",
                "T Podladchikova",
                "K. Dissauer"
            ],
            "title": "Multi-channel coronal hole detection with a CNN. In: Machine learning in heliophysics; 2019",
            "year": 2019
        },
        {
            "authors": [
                "R Krishnapuram",
                "J. Keller"
            ],
            "title": "The possibilistic C-means algorithm: insights and recommendations",
            "venue": "IEEE Trans Fuzzy Syst",
            "year": 1996
        },
        {
            "authors": [
                "H Law",
                "J. Deng"
            ],
            "title": "Cornernet: Detecting objects as paired keypoints",
            "venue": "European conference on computer vision;",
            "year": 2018
        },
        {
            "authors": [
                "W Liu",
                "D Anguelov",
                "D Erhan",
                "C Szegedy",
                "S Reed",
                "C-Y Fu",
                "A. Berg"
            ],
            "title": "SSD: single shot multibox detector",
            "venue": "European conference on computer vision;",
            "year": 2016
        },
        {
            "authors": [
                "BH Menze",
                "A Jakab",
                "S Bauer",
                "J Kalpathy-Cramer",
                "K Farahani",
                "J Kirby",
                "Y Burren",
                "N Porz",
                "J Slotboom",
                "R Wiest"
            ],
            "title": "The multimodal brain tumor image segmentation benchmark (BRATS)",
            "venue": "IEEE Trans Med Imaging",
            "year": 2015
        },
        {
            "authors": [
                "S Mohajerani",
                "T Krammer",
                "P. Saeedi"
            ],
            "title": "A cloud detection algorithm for remote sensing images using fully convolutional neural networks. In: IEEE 20th international workshop on multimedia signal processing; 2018",
            "year": 2018
        },
        {
            "authors": [
                "S Mohajerani",
                "P. Saeedi"
            ],
            "title": "Cloud-Net: an end-to-end cloud detection algorithm for Landsat 8 imagery. In: IEEE international geoscience and remote sensing symposium; 2019",
            "year": 2019
        },
        {
            "authors": [
                "K Osorio",
                "A Puerto",
                "C Pedraza",
                "D Jamaica",
                "L. Rodr\u00edguez"
            ],
            "title": "A deep learning approach for weed detection in lettuce crops using multispectral images. AgriEngineering",
            "year": 2020
        },
        {
            "authors": [
                "N. Otsu"
            ],
            "title": "A threshold selection method from gray-level histograms",
            "venue": "IEEE Trans Syst Man Cybernet",
            "year": 1979
        },
        {
            "authors": [
                "OAB Penatti",
                "K Nogueira",
                "JA. dos Santos"
            ],
            "title": "Do deep features generalize from everyday objects to remote sensing and aerial scenes domains? In: Proceedings of the IEEE conference on computer vision and pattern recognition",
            "year": 2015
        },
        {
            "authors": [
                "J Redmon",
                "S Divvala",
                "R Girshick",
                "A. Farhadi"
            ],
            "title": "You only look once: Unified, real-time object detection",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition;",
            "year": 2016
        },
        {
            "authors": [
                "S Ren",
                "K He",
                "R Girshick",
                "J. Sun"
            ],
            "title": "Faster R-CNN: towards real-time object detection with region proposal networks",
            "venue": "IEEE Trans Pattern Anal Mach Intell",
            "year": 2015
        },
        {
            "authors": [
                "K Revathy",
                "S Lekshmi",
                "S. Prabhakaran Nayar"
            ],
            "title": "Fractal-based fuzzy technique for detection of active regions from solar images",
            "year": 2005
        },
        {
            "authors": [
                "O Russakovsky",
                "J Deng",
                "H Su",
                "J Krause",
                "S Satheesh",
                "S Ma",
                "Z Huang",
                "A Karpathy",
                "A Khosla",
                "M Bernstein",
                "A Berg",
                "L. Fei-Fei"
            ],
            "title": "ImageNet large scale visual recognition challenge",
            "venue": "International journal of computer vision",
            "year": 2015
        },
        {
            "authors": [
                "H Schunker",
                "AC Birch",
                "RH Cameron",
                "DC Braun",
                "L Gizon",
                "RB. Burston"
            ],
            "title": "Average motion of emerging solar active region polarities-i two phases of emergence",
            "venue": "Astron Astrophys",
            "year": 2019
        },
        {
            "authors": [
                "H Schunker",
                "D Braun",
                "A Birch",
                "R Burston",
                "L. Gizon"
            ],
            "title": "Sdo/hmi survey of emerging active regions for helioseismology",
            "venue": "Astronomy & Astrophysics",
            "year": 2016
        },
        {
            "authors": [
                "K Simonyan",
                "A. Zisserman"
            ],
            "title": "Two-stream convolutional networks for action recognition in videos",
            "venue": "Neural Inf Process Syst",
            "year": 2014
        },
        {
            "authors": [
                "X Song",
                "S Gao",
                "C. Chen"
            ],
            "title": "A multispectral feature fusion network for robust pedestrian detection",
            "venue": "Alexandria Eng J",
            "year": 2021
        },
        {
            "authors": [
                "X Sun",
                "MG Bobra",
                "JT Hoeksema",
                "Y Liu",
                "Y Li",
                "C Shen",
                "S Couvidat",
                "AA Norton",
                "GH. Fisher"
            ],
            "title": "Why is the great solar active region 12192 flare-rich but cme-poor",
            "venue": "Astrophys J Lett. 2015;804(2):L28",
            "year": 2015
        },
        {
            "authors": [
                "K Takumi",
                "K Watanabe",
                "Q Ha",
                "A Tejero-De-Pablos",
                "Y Ushiku",
                "T. Harada"
            ],
            "title": "Multispectral object detection for autonomous vehicles",
            "venue": "Proceedings of the on thematic workshops of ACM multimedia;",
            "year": 2017
        },
        {
            "authors": [
                "C Verbeeck",
                "V Delouille",
                "B Mampaey",
                "RD. Visscher"
            ],
            "title": "The SPoCAsuite: software for extraction, characterization, and tracking of SN Computer Science",
            "venue": "SN Computer Science active regions and coronal holes on EUV images. Astron Astrophys",
            "year": 2013
        },
        {
            "authors": [
                "Viola P",
                "Jones MJ"
            ],
            "title": "Robust real-time face detection",
            "venue": "Int J Comput Vision",
            "year": 2004
        },
        {
            "authors": [
                "J Wagner",
                "V Fischer",
                "M Herman",
                "S. Behnke"
            ],
            "title": "Multispectral pedestrian detection using deep fusion convolutional neural networks",
            "venue": "European symposium on artificial neural networks;",
            "year": 2016
        }
    ],
    "sections": [
        {
            "text": "Vol.:(0123456789)\nSN Computer Science\nKeywords Object detection\u00a0\u00b7 Solar images\u00a0\u00b7 Active regions\u00a0\u00b7 Multi-spectral images\u00a0\u00b7 Deep neural networks"
        },
        {
            "heading": "Introduction",
            "text": "Active regions (ARs) detection is essential in studying solar behaviors and space weather. The solar atmosphere is monitored on multiple wavelengths, as seen in Fig.\u00a01. However, unlike traditional multi-spectral scenarios such as Earth imaging, e.g. [11, 17, 34], where multiple imaging bands reveal different aspects (e.g. composition) of a same scene, different bands image the solar atmosphere at different temperatures, which correspond to different altitudes\n[24]. Therefore, imaging the sun using different wavelengths shows different 2D cuts of the 3D objects that span the solar atmosphere. This makes handling the multi-spectral nature of the data not straightforward. Moreover, the variety of shapes and brightness, and fuzzy boundaries, of ARs also introduce a high complexity in precisely localising them.\nVery few solutions were presented to the AR detection problem. Most of these methods exploited single image bands only. Benkhalil et\u00a0al. [2] proposed a method for singleband images from Paris-Meudon Spectroheliograph (PM/ SH) and SOHO/EIT. In [24], ARs were segmented from a single band at a time, which the authors justify by the fact that they each provide information from a different solar altitude, and they showed how the area of ARs differs between the different bands. While we also aim at getting specialised results for each image band, we argue that interdependencies exist between bands, which can be exploited for increased robustness. The SPOCA method [32], used in the Heliophysics Feature Catalogue (HFC),1 segments ARs and coronal holes from SOHO/EIT 171\u00a0\u00c5 and 195\u00a0\u00c5 combined images. These two bands image overlapping (but This article is part of the topical collection \u201cPattern Recognition Applications and Methods\u201d guest edited by Ana Fred, Maria De Marsico and Gabriella Sanniti di Baja. * Majedaldein Almahasneh 809508@swansea.ac.uk 1 Department of\u00a0Computer Science, Swansea University, Swansea, UK\n2 Universit\u00e9 de Toulon, Aix Marseille Univ, CNRS, LIS, Marseille, France\n3 Observatoire de Paris/PSL, Paris, France 1 http:// vopar is- helio. obspm. fr/ hfc- gui/.\nSN Computer Science\nSN Computer Science\ndifferent) regions of the solar atmosphere. SPOCA considers that they should yield identical detections. This approximation may result in a bad analysis of at least one of these bands. We provide separate but related results for these bands. We also exploit more bands for richer information on the solar atmosphere, with separate results for each band.\nSPOCA\u2019s segmentation is based on clustering, with Fuzzy and Possibilistic C-means [13], followed by morphological operations. The method of [2] uses local thresholding and morphological operations followed by region growing. The method was evaluated against manual detections (synoptic maps) produced at PM and National Oceanic and Atmospheric Administration (NOAA), and detected similar numbers of ARs as PM, and \u223c 50% more than NOAA. In [24], ARs were segmented by computing the pixel-wise fractal dimension (a measure of non-linear growth that reflects the degree of irregularity over multiple scales) in a convolutional fashion, and feeding the resulting feature map to a Fuzzy C-means [3] algorithm. Overall, these methods, mainly based on clustering and morphological operations, are very pre- and post-processing dependant. This makes them difficult to adapt to new image domains. We address this limitation using deep learning (DL).\nObject detection has evolved dramatically in the past two decades, from hand-crafted features based detection (e.g. Haar [33], and HOG [6]) to deep neural networks (DNN) such as YOLO [22], SSD [15], R-FCN [5], Cornernet [14], and Faster RCNN [23].\nDeep learning methods generally aim at analysing 2D images or dense 3D volumes, while the sparse 3D nature of the solar imaging data requires designing a specialised DL framework. Furthermore, multi-spectral images are commonly treated in a similar manner to RGB images, by stacking different bands into composite multi-channel images, extracting a common feature map, and producing a single detection result for the composite image, e.g. [8, 9, 11, 18, 19, 21]. This multi-channel strategy is ill-suited to the solar imaging scenario, since different images show different scenes and should have their own detection results.\nAnother common approach is to aggregate information from different bands at different levels, e.g. feature level and image level, [7, 9, 10, 28, 29, 31, 34]. This feature fusion strategy demonstrates the potential for DNNs to improve localisation by exploiting the multi-spectral aspect of the data.\nIn [34], a feature-fusion approach was proposed where HOG features extracted separately from the RGB+thermal images were concatenated before performing the final analysis by a fully connected layer. This strategy obtained better results than the previously mentioned image-level fusion. Authors discussed that the network may optimise the learned features for each band. Moreover, they reckon that small misalignments may be overcome as spatial\ninformation gets less relevant in late network stages. This may be an advantage in our case of images showing different parts of a scene.\nHowever, when comparing image-level and feature-level fusion, [9] found on the contrary that image fusion worked best when segmenting soft tissue sarcomas in multi-modal medical images. These different results suggest that there is no universal best fusion strategy, and it needs to be adapted to each case. In our detection scenario, we investigate the best stages where to apply fusion.\nAnother feature-fusion strategy was used in [12] to segment coronal holes from 7 SDO bands and a magnetogram. The method relies on training a CNN to segment coronal holes from a single band, followed by fine-tuning the learned CNN over the other bands consecutively. The feature maps of each specialised CNN are used in combination as input to a final segmentation CNN, resulting in a unique final prediction. The production of a unique localisation result for all multi-spectral images is a common limitation to all cited works, which we address in this study with a multi-task network. In this work, continuing from our previous work [1] on AR detection, we introduce MultiSpectral-MultiTask-CNN (MSMT-CNN), a multi-tasking DNN framework, as a robust solution for solar AR detection that takes into consideration the multi-spectral aspect of the data and the three-dimensional spatial dependencies between image bands. This multi-spectral and multi-tasking concept may be applied to any CNN backbone.\nThe 3D nature of our multi-spectral imaging scenario, which differs from previous multi-spectral applications, requires a new benchmark. We introduce two annotated datasets comprised of solar images from both ground and space, and which cover evenly all phases of solar activity, which follows an 11-year cycle. To the best of our knowledge, no detection ground-truth was previously available for such data. A labelling tool was hence designed to cope with its temporal and multi-spectral nature and will be also released."
        },
        {
            "heading": "Methodology",
            "text": "While some existing works were developed for analysing multi-spectral images, to our best knowledge, the problem of detecting objects over sparse 3D multi-spectral imagery, in which different bands show different scenes, was not yet addressed. Our framework exploits jointly several time-matched image bands in parallel, to predict separate, although related, detection results for each image. This framework is general and may be used with any DNN backbone, we demonstrate it using Faster RCNN [23].\nThe intuition behind our framework manifests in three key principles:\nSN Computer Science\n1. Extracting features from different image bands individually using parallel feature extraction branches. This allows the network to learn independent features from each band, according to their specific modality. 2. Aggregating the learned features from the different branches using some appropriate fusion operator. This assists the network to jointly analyse the extracted features from different bands and thus learn their interdependencies. In this work, we test fusion by addition and concatenation, at different feature levels (i.e. early and late fusion). 3. Generating a set of results per image band, based on a multi-task loss, allowing the detection of different sections or layers of 3D objects.\nPoints 1 and 3 are motivated by the nature of the multi-spectral data, where different bands image different locations in a 3D scene, each providing a unique information. Our multitasking framework aims at getting specialised results for each image band, in contrast to most existing works where focus is on producing a unique prediction to all image bands. This is crucial since the localisation information may differ from one band to another in solar (sparse) multi-spectral images. Yet, all bands are correlated, which motivates point 2. Our framework exploits the inter-dependencies between the different bands by its joint analysis strategy, increasing the robustness of its performance in individual bands.\nFurthermore, our framework emulates how experts manually detect ARs (see also Sect.\u00a0\u201cData\u201d), where a suspected region\u2019s correlation with other bands is evaluated prior to its final classification. This demonstrates the usefulness and importance of accounting for (spatially and temporally) neighbouring slices in robustly detecting ARs.\nThe MSMT-CNN framework is very modular and flexible. It may accommodate any number of available multispectral images. Additionally, since different scenarios may require different fusion strategies (as suggested by existing works), the modularity of our framework allows it to be easily adapted to different types and levels of feature fusion (e.g. addition and concatenation, early and late). This modular design also allows our framework to adopt different backbone architectures (e.g. Faster RCNN in our experiments). Indeed, its three key principles are applicable to any backbone, as they are not architecture dependent.\nPre\u2011processing\nThe input of our system are time-matched observations, possibly acquired by different instruments or at different orientations of the same instrument. As such they need to be spatially aligned. We harmonise the radius and center location of the solar disk, either using SOHO image preparation routines, or through Otsu [20] thresholding of the solar\ndisc of PM/SH images followed by minimum enclosing circle fitting and re-projection into a unified center and radius. Orientation is normalised by SOHO and PM routines to a vertical north-south solar axis. Although this process does not correct a possible small time difference and resulting east-west rotation of the Sun between two acquisitions, it ensures a sufficient alignment for our purpose of AR detections from spatially (and temporally) correlated solar disks.\nThe SOHO/EIT images are prepared by EIT routines. We eliminate any prominences or solar eruptions by masking out all areas outside the solar disk. The contrast of SOHO/ MDI Magnetograms is enhanced by intensity rescaling. Contrast enhancement was not used on SOHO/EIT and PM/SH images, as it was found to have minimal effect on our detection results.\nBoth datasets are augmented using north-south flipping, east-west flipping, and a combination of the two. Augmentation with arbitrary rotations of the images is a popular way of augmenting astronomy datasets. However, such rotations are ruled out from our study because ARs tend to appear predominantly alongside the solar equator.\nFinally, a single-channel solar image was repeated along the depth axis resulting in a 3-channel image to match the pre-trained CNN\u2019s input depth.\nMSMT\u2011CNN\nOur detection DNN is presented in Fig.\u00a02. A CNN (ResNet50 or VGG16 in our experiments) is first used as a feature extraction network. Parallel branches (subnetworks) produce a feature map per image band, following the late feature fusion strategy. This allows the subnetworks\u2019 filters to be optimised for their input bands individually. The feature maps are then concatenated across the bands.\nThe combined feature map is jointly analysed by one parallel module per image band that performs Faster RCNN\u2019s region proposal network (RPN). The RPN stage uses three aspect ratios ([1:1], [1:2], [2:1]) and four sizes of anchor (32, 64, 128, and 256 pixel width), found empirically to match well the typical size and shape of ARs. One specialised RPN per image band is trained.\nAt training, for each band, the correspondent region proposals along with the combined feature map are used by a Faster RCNN\u2019s detector module to perform the final prediction for the band. However, at testing time, the bandspecialised detector modules use the region proposals from all bands. This combination of region proposals helps finding potential AR locations in bands where they are more difficult to identify.\nIt is good to note that during training, the RPN\u2019s proposals for a band are filtered (i.e. labeled as positive or negative) with respect to their overlap with the band\u2019s ground-truth. Hence, combining them in the training time would mean\nSN Computer Science\nimplicitly inheriting the ground-truth of a band to another, in contradiction with the band-specific ground-truth used for training the detector module. This may hinder the learning of both the RPN and detector modules. Therefore, region proposals are only combined at testing time to ensure a better learning of the final detection modules.\nUsing the combined feature map for both RPN prediction and classification helps the network learn the relationship between the image bands and hence provide more consistent region proposals and final predictions. We demonstrate in Sect.\u00a0\u201cExperiments\u201d that this is particularly helpful in cases where an AR is difficult to detect in a single band.\nThe network is trained in the same way as the original Faster RCNN, using all input bands and branches according to a combined loss function:\nwhere b and i refer to the image band and the index of the bounding box being processed, respectively. The terms Lcls and Lreg are the bounding-box classification loss and the bounding-box regression loss defined in [23]. Ncls and Nreg represent the size of the mini batch being processed and the number of anchors, respectively. balances the classification and the regression losses (we set to 10 as suggested in [23]). p and p\u22c6 are the predicted anchor\u2019s class probability and its actual label, respectively. Lastly, t and t\u22c6 represent the predicted bounding box coordinates and the ground-truth\n(1)\nL = \u2211\nb\n(\n1\nNcls\n\u2211\ni\nLcls(pbi , p \u2217 bi )\n+ 1\nNreg\n\u2211\ni\np\u2217 bi Lreg(tbi , t \u2217 bi )\n)\ncoordinates, respectively. It is worth noting that our proposed framework is not limited to using Faster RCNN\u2019s loss and may be trained with using other task-suitable loss functions.\nDuring training, the weights of each stage (i.e. feature extraction, region proposal, and detection) are stored independently whenever the related Faster RCNN loss decreases. At testing time, the best performing set of weights is retrieved per stage. We refer to this practice as \u2018MultiObjective Optimisation\u2019 (MOO). The improved performance that we observe in Sect.\u00a0\u201cExperiments\u201d may be explained by each stage having a different objective to optimise, which may be reached at different times.\nIn this paper, we experiment with a 2, 3, and 4-band pipeline. However, the approach may generalise straightforwardly to n bands and new imaging modalities. Similarly, our framework may exploit any DNN architecture, and may be updated with new state-of-the-art DL architectures easily."
        },
        {
            "heading": "Experiments",
            "text": "Our framework was implemented using Tensorflow and run on an NVIDIA GeForce GTX 1080 Ti. We evaluate our detection stage using precision, recall, and F1-score. Solar ARs are dynamic structures that are constantly changing (e.g. merging and splitting, from and to multiple regions) during their lifetime [26, 27, 30, 32]. Accordingly, we design a dynamic criterion to match the characteristics of AR. Particularly, we address two prevalent scenarios, (1) an AR area is detected by multiple neighbouring bounding boxes, or (2)\nSN Computer Science\na cluster of neighboring ARs is detected in a single bounding box. See Fig.\u00a03. Therefore, a detection is considered a true positive if its intersection with a ground-truth box is greater or equal to 50% of either the predicted or groundtruth area, and is of an area that lies within the AR area distribution of the annotated dataset. We empirically found that this provides a good trade-off of precision over recall. Additionally, we evaluate our detection using IoU (intersection over union) thresholds of 25% and 50%, and compare it to baseline methods. Note that both our annotation and evaluation processes were validated by a solar physics expert. Non maximum suppression (NMS) is used to discard any redundant detections.\nAll tested CNN were initialised with pre-trained ImageNet [25] weights. Indeed, we demonstrated in [4] that CNNs pre-trained on RGB images may fine-tune and adapt well to other modalities such as depth images, provided that the image\u2019s gain and contrast are suitably enhanced to match those of the pre-training RGB images."
        },
        {
            "heading": "Data",
            "text": "We consider data from SOHO (space-based) and PM observatory (ground-based). We use the bands 171 and 195\u00a0\u00c5 (transition region), 284\u00a0\u00c5 (corona), and 304\u00a0\u00c5 (chromosphere and base of transition region) from SOHO/EIT, 3934\u00a0\u00c5 (chromosphere) from PM/SH, and line-of-sight magnetograms (photosphere) from SOHO/MDI, as illustrated in Fig.\u00a01. To account for the regular solar cycle, we select images evenly from each of three periods of varying solar activity level, with years 2002\u201303, 2004\u201305, and 2008\u201310 for high, medium, and low activity respectively.\nWe publish two new datasets with detection annotations (i.e bounding boxes): the Lower Atmosphere Dataset (LAD) and Upper Atmosphere Dataset (UAD). All annotations were validated by a solar physics expert.\nLocalising ARs in multi-spectral images can be challenging due to inconsistent numbers of polarity centers, shapes, sizes, and activity levels. To mitigate this issue, when manually annotating, we exploit neighboring image bands, magnetograms, as well as temporal information where we examine the evolution of a suspected AR to validate its detection and localisation. We designed a new multi-spectral labelling tool2 which displays, side by side, images from an auxiliary modality and from a sequence of three previous and three subsequent time steps.\nAuxiliary imaging modalities may have different observation frequencies and times, therefore we work with timematched images, i.e. the time-closest image, if any, in a 12-h\nwindow during which ARs may not undergo any significant change.\nARs have a high spatial coherence in 3934\u00a0\u00c5 and magnetogram images due to the physical proximity of the two imaged regions. Hence, when annotating 3934\u00a0\u00c5 images, time-matched magnetograms were used as an extra support. Furthermore, the 3934\u00a0\u00c5 bounding boxes could be considered to be good approximations of magnetograms\u2019 annotations. Table\u00a01 presents an overview of annotated images for both datasets. We split the datasets into training and testing sets in the following proportions. For LAD, we use 213 images (1380 bounding box) for training, and 53 images (406 bounding box) for testing. For UAD, we use 283 images for training, and 40 images for testing. This amounts to 2205, 1919, 2341, and 2016 training bounding boxes in the 304, 171, 195, and 284\u00a0\u00c5 bands respectively, and 287, 262, 330, and 263 testing bounding boxes.\nTo compare against SPOCA, we consider a subset of the UAD testing set for which SPOCA detection results are available in HFC: the SPOCA subset. It consists of 26 testing images (181, 168, 213, and 166 bounding boxes in the 304, 171, 195 and 284\u00a0\u00c5 images respectively)."
        },
        {
            "heading": "Independent Detection on\u00a0Single Image Bands",
            "text": "We first compare detection results of Faster RCNN over individual image bands analysed independently (Table\u00a02). This aims to evaluate different feature extraction DNNs, and will further serve as baseline to assess our proposed framework.\nThe ResNet50 architecture consistently produces better results than the VGG one with higher F1 scores in all experiments. Based on these results, we adopt the ResNet50 architecture as backbone for our framework in the next experiments.\nWhen comparing the detection results per image band, we notice that 304\u00a0\u00c5 images are repeatedly amongst the most difficult to analyse in UAD, having the lowest F1-scores in all tests. On the other hand, 171\u00a0\u00c5 has the best results of UAD bands, followed by 284 and 195\u00a0\u00c5. This may be explained by ARs having a denser or less ambiguous appearance in 171, 195, and 284\u00a0\u00c5 image bands than in 304\u00a0\u00c5 since they are higher in the corona. A similar observation can be made in the LAD dataset when comparing the magnetogram results to 3934\u00a0\u00c5, where magnetograms observe a lower altitude than 3934\u00a0\u00c5.\nWe also notice a strong contrast between a same detector\u2019s precision and recall on the different UAD bands. This further demonstrates that these bands are not equal in how easily they may be analysed, even though they were acquired at the same time with same size and resolution.\nDetections are visually verified to be poorer for small ARs and for spread and faint ones with more ambiguous boundaries. Visual inspection also confirms the different 2 Our labelling tool will be released on the project\u2019s website.\nSN Computer Science\nperformances in various bands being caused by differing visual complexities of ARs. These observations suggest that detecting ARs using information provided by a single band may be an under-constrained problem."
        },
        {
            "heading": "Joint Detection on\u00a0Multiple Image Bands",
            "text": "We now present the results of our framework when detecting ARs over the LAD/UAD bands jointly. Joint detection results are summarised in Table\u00a03.\nIn our first experiment, we compare early fusion (pixel level concatenation) against late fusion (feature level concatenation or addition), on the LAD dataset. Overall, the three approaches show an enhanced performance in contrast to single band based detection. However, we find that late fusion with concatenation shows higher performance than early fusion, having 0.90 F1-score versus 0.88 for magnetograms, while both scored 0.89 over 3934\u00a0\u00c5. We further test late fusion using element wise addition and observe a decrease of 1% and 3% in the F1-score over 3934\u00a0\u00c5 and Magnetogram, respectively. Hence, we choose the late fusion with concatenation approach for all the following experiments.\nWe also evaluate the benefit of our MOO strategy using our 2-band architecture over the UAD. This approach\nSN Computer Science\ngenerally improves the F1-scores in most bands comparing to the non-MOO architectures. This behaviour may indicate that the two feature extraction stages were indeed more effectively optimised for their different tasks at different epochs. We retain this MOO approach for all other experiments.\nOn the UAD dataset, with various combinations of two bands, we notice a general improvement over single band detections. In addition, the performance varies in correspondence to the bands being used. Combining bands that are difficult to analyse (304 or 195\u00a0\u00c5 that have lowest F1-scores in the single band analyses) with easier bands (171 and 284\u00a0\u00c5) unsurprisingly enhances their respective performance. More interestingly, combining the difficult 304 and 195\u00a0\u00c5 bands together also improve on their individual performance. Similarly, when combining bands that are easier to analyse (171 and 284\u00a0\u00c5), in contrast to using combinations of difficult and easy bands in the analysis, performances are also improved over their individual analyses. Following these settings, our two-band based approach was able to record higher or similar F1-scores in contrast to the best performing single-band detector. This supports our hypothesis that joint detection may provide an increased robustness through learning the inter-dependencies between the image bands.\nMoreover, the most dramatic improvement in F1-scores across both LAD and UAD datasets is for the 3934\u00a0\u00c5 images when magnetograms are added to the analysis. This is in line with the current understanding of AR having strong magnetic signatures.\nGenerally, in the UAD dataset, we find that using a combination of 2 bands produces the best results in comparison to using 3 or 4 bands. This may be caused by the fact that optimising the network for multiple tasks (2, 3, or 4 detection tasks) simultaneously increases the complexity of the problem. While the network successfully learned to produce better detections in the case of 2 bands, it was difficult to find a generalised yet optimal model for 3 or 4 bands at the same time.\nFurthermore, since bands imaging consecutive layers of solar atmosphere are expected to be highly correlated, we test our framework by combining directly neighbouring bands together, such that a prediction for a band is performed using the band\u2019s own feature map combined with the feature map(s) of its available (1 or 2) direct neighbour(s). This approach gets the highest recall score on the UAD band 284\u00a0\u00c5 of all tests, where it is combined with the 195\u00a0\u00c5 band. However, it does not improve the performance on the other bands comparing to the single-band and 2-band based experiments.\nWe compare against state-of-the-art SPOCA [32] on the SPOCA subset, and against the first stage of [12] (sequentially fine-tuned networks) by adapting their approach to\nFaster RCNN and testing it on UAD. SPOCA detections were obtained from 171\u00a0\u00c5 and 195\u00a0\u00c5 images only, combined as two channels of an RGB image, and SPOCA produces a single detection for both bands. We compare this detection against the ground truth of each of the bands individually. To prove the robustness and versatility of our detector, we also experiment with a combination of chromosphere, transition region, and corona bands on the SPOCA subset in addition to the whole UAD.\nOn the SPOCA subset, over the bands 171\u00a0\u00c5 and 195\u00a0\u00c5 for which it is designed, SPOCA gets the poorest performance of all multi-band and single-band experiments. It is worth noting that this method relies on manually tuned parameters according to the developers\u2019 own definition and interpretation of AR boundaries, which may differ from the ones we used when annotating the dataset. While supervised DL-based methods could integrate this definition during training, SPOCA could not perform such adaptation. This may have had a negative impact on its scores. Furthermore, visual inspection shows a poor performance for SPOCA on low solar activity images, see Fig.\u00a04. This may be due to the use of clustering in SPOCA, since in low activity periods the number of AR pixels (if any) is significantly smaller than solar background pixels, which makes it hard to identify clusters.\nMoreover, the fine-tuned networks of [12] suffer from a high rate of false positives, and show a close performance to single band detection using Faster RCNN with an identical precision, recall and F1-score over the band 304\u00a0\u00c5 and a slight decrease over the other 3 bands. This may be due to the fact that its transfer learning does not incorporate the inter-dependencies directly when analysing the different bands.\nWe further evaluate our method using IoU and compare our results to both, single band based detection by FasterRCNN and joint detection from SPOCA and the fine tuned network of [12] (see Table\u00a04). When using an IoU threshold of 50%, we find that our method produces the highest F1 score over the 195 \u00c5 band amongst all methods on both UAD and SPOCA datasets, with a comparable performance over 171 \u00c5 and 284 \u00c5. On the other hand, our method shows a drop in performance on 304 \u00c5 comparing to Faster RCNN and the sequentially fine-tuned network. Generally, all methods show a significant decrease in the F1 scores when using IoU based criterion, with SPOCA being the lowest amongst all methods. A similar pattern is observed when using a less strict IoU threshold of 25%. During our visual inspection, we notice that in some cases, detected ARs are regarded false positives when evaluated against the IoU criterion. Particularly, when a cluster of neighbouring active region areas (i.e. neighbouring ARs) is detected as a single AR structure, or when an AR is detected by multiple neighbouring boxes. See Fig.\nSN Computer Science\n\u201cExperiments\u201d. These observations suggest that an IoU based criterion does not perfectly capture the dynamic characteristics of solar ARs, even when incorporating an IoU threshold as low as 25%. Unlike generic object detection tasks, where object morphology and boundaries are well defined, ARs are dynamic structures that are continuously evolving (e.g. merging and splitting, emerging and dying out) [26, 27, 30, 32]. Therefore, when designing our evaluation criterion (Sect.\u00a0\u201cExperiments\u201d), we take into account the aforementioned scenarios associated with solar ARs.\nTo further demonstrate the benefits of our joint analysis based approach, we create a synthetic dataset from the BraTS multi-modal dataset [16] of similar spatial configurations to the solar imaging bands. BraTS consists of full 3D MR image volumes of brain in four modalities (T1GD, T1, T2, and Flair) and three classes: enhancing tumour (ET), necrotic and non-enhancing tumour core (NCR/NET), and peritumoural edema (ED). We create the synthetic dataset by selecting one 2D slice of each\nimage modality separated by a spatial gap of size 1 voxel. This emulates the solar images scenario where each band shows ARs in a different solar altitude.\nAlthough this gap size may seem much lower than for solar images, they are justified by the speed of change of the imaged brain from one slice to another neighbouring one being much larger than for the generally smoother ARs. For each modality, we use a total of 11,533 and 190 training and testing images, respectively. We evaluate our detection approach with different fusions, over the 4 bands of BraTS-prime dataset, and compare it against single band based detection using Faster RCNN. See Tables\u00a05 and 6. All fusion strategies significantly outperform single band detectors, with an average F1-score increase of 18%, 28%, and 32% for early addition, early concatenation, and late concatenation fusion, respectively. This confirms our hypothesis that exploiting inter-dependencies between the image bands by the joint analysis may provide a superior performance in contrast to single band based detection.\nSN Computer Science"
        },
        {
            "heading": "Conclusion",
            "text": "We presented MSMT-CNN, a multi-branch and multi-tasking framework to tackle the 3D solar AR detection problem from multi-spectral images that observe different cuts of the 3D solar atmosphere. MSMT-CNN analyses multiple image bands jointly to produce consistent detection across them.\nIt is a flexible framework that may use any CNN backbone, and may be straightforwardly generalised to any number and modalities of images. Our findings suggest that jointly analysing information from different image bands, at different feature levels, can elevate the detection performance across the image bands. MSMT-CNN showed competitive\nSN Computer Science\nresults against baseline and state-of-the-art detection methods. Future research could investigate incorporating multilevel feature fusion in multi-view imaging systems such as cross sectional imaging in the medical domain which demonstrates an interesting direction for future research."
        },
        {
            "heading": "Declarations",
            "text": "Conflict of Interest The authors declare that they have no conflict of interest.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article\u2019s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article\u2019s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/."
        }
    ],
    "title": "MSMT\u2010CNN for Solar Active Region Detection with Multi\u2010Spectral Analysis",
    "year": 2022
}