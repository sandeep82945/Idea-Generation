{
    "abstractText": "In recent years, it has become increasingly popular to accelerate numerical simulations using Graphics Processing Unit (GPU)s. In multiphysics simulations, the various combined methodologies may have distinctly different computational characteristics. Therefore, the best-suited hardware architecture can differ between the simulation components. Furthermore, not all coupled software frameworks may support all hardware. These issues predestinate or even force hybrid implementations, i.e., different simulation components running on different hardware. We introduce a hybrid coupled fluid-particle implementation with geometrically resolved particles. The simulation utilizes GPUs for the fluid dynamics, whereas the particle simulation runs on Central Processing Unit (CPU)s. We examine the performance of two contrasting cases of a fluidized bed simulation on a heterogeneous supercomputer. The hybrid overhead (i.e., the CPU-GPU communication) is negligible. The fluid simulation shows good performance utilizing nearly the entire memory bandwidth. Still, the GPU run time accounts for most of the total time. The parallel efficiency in a weak scaling benchmark for 1024 A100 GPUs is up to 71%. Frequent CPU-CPU communications occurring in the particle simulation are the leading cause of the decrease in parallel efficiency. The results show that hybrid implementations are promising for large-scale multiphysics simulations on heterogeneous supercomputers. \u2217Corresponding author. E-mail addresses: samuel.kemmler@fau.de (S. Kemmler), christoph.rettinger@fau.de (C. Rettinger), harald.koestler@fau.de (H. K\u00f6stler) Preprint submitted to Computer Physics Communications March 22, 2023 ar X iv :2 30 3. 11 81 1v 1 [ cs .C E ] 2 1 M ar 2 02 3",
    "authors": [
        {
            "affiliations": [],
            "name": "Samuel Kemmlera"
        },
        {
            "affiliations": [],
            "name": "Christoph Rettinger"
        },
        {
            "affiliations": [],
            "name": "Harald K\u00f6stler"
        }
    ],
    "id": "SP:34743d429e24d3587884e78810e1153e1a94503d",
    "references": [
        {
            "authors": [
                "C. Rettinger",
                "C. Godenschwager",
                "S. Eibl",
                "T. Preclik",
                "T. Schruff",
                "R. Frings",
                "U. R\u00fcde"
            ],
            "title": "Fully Resolved Simulations of Dune Formation in Riverbeds",
            "venue": "in: J. M. Kunkel, R. Yokota, P. Balaji, D. Keyes (Eds.), High Performance Computing, Vol. 10266, Springer International Publishing, Cham",
            "year": 2017
        },
        {
            "authors": [
                "C. Schwarzmeier",
                "C. Rettinger",
                "S. Kemmler",
                "J. Plewinski",
                "F. N\u00fa\u00f1ez- Gonz\u00e1lez",
                "H. K\u00f6stler",
                "U. R\u00fcde"
            ],
            "title": "B",
            "venue": "Vowinckel, Particle-resolved simulation of antidunes in free-surface flows ",
            "year": 2023
        },
        {
            "authors": [
                "A.G. Kidanemariam",
                "M. Uhlmann"
            ],
            "title": "Direct numerical simulation of pattern formation in subaqueous sediment",
            "venue": "Journal of Fluid Mechanics 750 ",
            "year": 2014
        },
        {
            "authors": [
                "Z. Benseghier",
                "P. Cu\u00e9llar",
                "L.-H. Luu",
                "S. Bonelli",
                "P. Philippe"
            ],
            "title": "A parallel GPU-based computational framework for the micromechanical analysis of geotechnical and erosion problems",
            "venue": "Computers and Geotechnics 120 ",
            "year": 2020
        },
        {
            "authors": [
                "B. Vowinckel",
                "T. Kempe",
                "J. Fr\u00f6hlich"
            ],
            "title": "Fluid\u2013particle interaction in turbulent open channel flow with fully-resolved mobile beds",
            "venue": "Advances in Water Resources 72 ",
            "year": 2014
        },
        {
            "authors": [
                "C. Rettinger",
                "S. Eibl",
                "U. R\u00fcde",
                "B. Vowinckel"
            ],
            "title": "Rheology of mobile sediment beds in laminar shear flow: Effects of creep and polydispersity",
            "venue": "Journal of Fluid Mechanics 932 ",
            "year": 2022
        },
        {
            "authors": [
                "T. Shimokawabe",
                "T. Endo",
                "N. Onodera",
                "T. Aoki"
            ],
            "title": "A Stencil Framework to Realize Large-Scale Computations Beyond Device Memory Capacity on GPU Supercomputers",
            "venue": "in: 2017 IEEE International Conference on Cluster Computing (CLUSTER), IEEE, Honolulu, HI, USA",
            "year": 2017
        },
        {
            "authors": [
                "D. Rohr",
                "S. Kalcher",
                "M. Bach",
                "A.A. Alaqeeliy",
                "H.M. Alzaidy",
                "D. Eschweiler",
                "V. Lindenstruth",
                "S.B. Alkhereyfy",
                "A. Alharthiy",
                "A. Almubaraky",
                "I. Alqwaizy",
                "R. Bin Suliman"
            ],
            "title": "An Energy-Efficient Multi- GPU Supercomputer",
            "venue": "in: 2014 IEEE Intl Conf on High Performance Computing and Communications, 2014 IEEE 6th Intl Symp on Cyberspace Safety and Security, 2014 IEEE 11th Intl Conf on Embedded Software and Syst (HPCC,CSS,ICESS), IEEE, Paris",
            "year": 2014
        },
        {
            "authors": [
                "G. Oyarzun",
                "R. Borrell",
                "A. Gorobets",
                "A. Oliva"
            ],
            "title": "Portable implementation model for CFD simulations",
            "venue": "Application to hybrid CPU/GPU supercomputers, International Journal of Computational Fluid Dynamics 31 (9) ",
            "year": 2017
        },
        {
            "authors": [
                "M. Holzer",
                "M. Bauer",
                "H. K\u00f6stler",
                "U. R\u00fcde"
            ],
            "title": "Highly efficient lattice Boltzmann multiphase simulations of immiscible fluids at high-density ratios on CPUs and GPUs through code generation",
            "venue": "The International Journal of High Performance Computing Applications 35 (4) ",
            "year": 2021
        },
        {
            "authors": [
                "C. Kotsalos",
                "J. Latt",
                "J. Beny",
                "B. Chopard"
            ],
            "title": "Digital blood in massively parallel CPU/GPU systems for the study of platelet transport",
            "venue": "Interface Focus 11 (1) ",
            "year": 2021
        },
        {
            "authors": [
                "Y. He",
                "F. Muller",
                "A. Hassanpour",
                "A.E. Bayly"
            ],
            "title": "A CPU-GPU crossplatform coupled CFD-DEM approach for complex particle-fluid flows",
            "venue": "Chemical Engineering Science 223 ",
            "year": 2020
        },
        {
            "authors": [
                "M. Sousani",
                "A.M. Hobbs",
                "A. Anderson",
                "R. Wood"
            ],
            "title": "Accelerated heat transfer simulations using coupled DEM and CFD",
            "venue": "Powder Technology 357 ",
            "year": 2019
        },
        {
            "authors": [
                "D. Jajcevic",
                "E. Siegmann",
                "C. Radeke",
                "J.G. Khinast"
            ],
            "title": "Large-scale CFD\u2013DEM simulations of fluidized granular systems",
            "venue": "Chemical Engineering Science 98 ",
            "year": 2013
        },
        {
            "authors": [
                "M. Xu",
                "F. Chen",
                "X. Liu",
                "W. Ge",
                "J. Li"
            ],
            "title": "Discrete particle simulation of gas\u2013solid two-phase flows with multi-scale CPU\u2013GPU hybrid computation",
            "venue": "Chemical Engineering Journal 207\u2013208 ",
            "year": 2012
        },
        {
            "authors": [
                "H. Norouzi",
                "R. Zarghami",
                "N. Mostoufi"
            ],
            "title": "New hybrid CPU-GPU solver for CFD-DEM simulation of fluidized beds",
            "venue": "Powder Technology 316 ",
            "year": 2017
        },
        {
            "authors": [
                "P. Valero-Lara",
                "F.D. Igual"
            ],
            "title": "M",
            "venue": "Prieto-Mat\u0301\u0131as, A. Pinelli, J. Favier, Accelerating fluid\u2013solid simulations (Lattice-Boltzmann & Immersed- Boundary) on heterogeneous architectures, Journal of Computational Science 10 ",
            "year": 2015
        },
        {
            "authors": [
                "J.R. d. S. Junior",
                "E.W. Clua",
                "A. Montenegro",
                "P.A. Pagliosa"
            ],
            "title": "Fluid Simulation with Two-Way Interaction Rigid Body Using a Heterogeneous GPU and CPU Environment",
            "venue": "in: 2010 Brazilian Symposium on Games and Digital Entertainment,",
            "year": 2010
        },
        {
            "authors": [
                "D. Roehm",
                "A. Arnold"
            ],
            "title": "Lattice Boltzmann simulations on GPUs with ESPResSo",
            "venue": "The European Physical Journal Special Topics 210 (1) ",
            "year": 2012
        },
        {
            "authors": [
                "B. Sheikh",
                "A. Pak"
            ],
            "title": "Numerical investigation of the effects of porosity and tortuosity on soil permeability using coupled three-dimensional discreteelement method and lattice Boltzmann method",
            "venue": "Physical Review E 91 (5) ",
            "year": 2015
        },
        {
            "authors": [
                "A. Khan",
                "H. Sim",
                "S.S. Vazhkudai",
                "A.R. Butt",
                "Y. Kim"
            ],
            "title": "An Analysis of System Balance and Architectural Trends Based on Top500 Supercomputers",
            "venue": "in: The International Conference on High Performance Computing in Asia-Pacific Region, ACM, Virtual Event Republic of Korea",
            "year": 2021
        },
        {
            "authors": [
                "J. Kim",
                "S. Seo",
                "J. Lee",
                "J. Nah",
                "G. Jo",
                "J. Lee"
            ],
            "title": "SnuCL: An OpenCL framework for heterogeneous CPU/GPU clusters",
            "venue": "in: Proceedings of 29 the 26th ACM International Conference on Supercomputing, ACM, San Servolo Island, Venice Italy",
            "year": 2012
        },
        {
            "authors": [
                "C. Rettinger",
                "U. R\u00fcde"
            ],
            "title": "A comparative study of fluid-particle coupling methods for fully resolved lattice Boltzmann simulations",
            "venue": "Computers & Fluids 154 ",
            "year": 2017
        },
        {
            "authors": [
                "C. Rettinger",
                "U. R\u00fcde"
            ],
            "title": "An efficient four-way coupled lattice Boltzmann \u2013 discrete element method for fully resolved simulations of particle-laden flows",
            "venue": "Journal of Computational Physics 453 ",
            "year": 2022
        },
        {
            "authors": [
                "T. Kr\u00fcger",
                "H. Kusumaatmaja",
                "A. Kuzmin",
                "O. Shardt",
                "G. Silva",
                "E.M. Viggen"
            ],
            "title": "The Lattice Boltzmann Method: Principles and Practice",
            "venue": "Graduate Texts in Physics, Springer International Publishing, Cham",
            "year": 2017
        },
        {
            "authors": [
                "Y.H. Qian"
            ],
            "title": "D",
            "venue": "D\u2019Humi\u00e8res, P. Lallemand, Lattice BGK Models for Navier-Stokes Equation, Europhysics Letters (EPL) 17 (6) ",
            "year": 1992
        },
        {
            "authors": [
                "X. He",
                "L.-S. Luo"
            ],
            "title": "Lattice Boltzmann Model for the Incompressible Navier\u2013Stokes Equation",
            "venue": "Journal of Statistical Physics 88 (3/4) ",
            "year": 1997
        },
        {
            "authors": [
                "A.J.C. Ladd",
                "R. Verberg"
            ],
            "title": "Lattice-Boltzmann Simulations of Particle- Fluid Suspensions",
            "venue": "Journal of Statistical Physics 104 (5) ",
            "year": 2001
        },
        {
            "authors": [
                "P.A. Cundall",
                "O.D.L. Strack"
            ],
            "title": "A discrete numerical model for granular assemblies",
            "venue": "G\u00e9otechnique ",
            "year": 1979
        },
        {
            "authors": [
                "Y. Fukumoto",
                "H. Yang",
                "T. Hosoyamada",
                "S. Ohtsuka"
            ],
            "title": "2-D coupled fluidparticle numerical analysis of seepage failure of saturated granular soils around an embedded sheet pile with no macroscopic assumptions",
            "venue": "Computers and Geotechnics 136 ",
            "year": 2021
        },
        {
            "authors": [
                "B.D. Jones",
                "J.R. Williams"
            ],
            "title": "Fast computation of accurate sphere-cube intersection volume",
            "venue": "Engineering Computations 34 (4) ",
            "year": 2017
        },
        {
            "authors": [
                "M. Bauer",
                "S. Eibl",
                "C. Godenschwager",
                "N. Kohl",
                "M. Kuron",
                "C. Rettinger",
                "F. Schornbaum",
                "C. Schwarzmeier",
                "D. Th\u00f6nnes",
                "H. K\u00f6stler",
                "U. R\u00fcde"
            ],
            "title": "waLBerla: A block-structured high-performance framework for multiphysics simulations",
            "venue": "Computers & Mathematics with Applications 81 ",
            "year": 2021
        },
        {
            "authors": [
                "S. Eibl"
            ],
            "title": "U",
            "venue": "R\u00fcde, A Modular and Extensible Software Architecture for Particle Dynamics ",
            "year": 2019
        },
        {
            "authors": [
                "M. Bauer",
                "H. K\u00f6stler",
                "U. R\u00fcde"
            ],
            "title": "Lbmpy: Automatic code generation for efficient parallel lattice Boltzmann methods",
            "venue": "Journal of Computational Science 49 ",
            "year": 2021
        },
        {
            "authors": [
                "D. Ernst",
                "M. Holzer",
                "G. Hager",
                "M. Knorr",
                "G. Wellein"
            ],
            "title": "Analytical performance estimation during code generation on modern GPUs",
            "venue": "Journal of Parallel and Distributed Computing 173 ",
            "year": 2023
        },
        {
            "authors": [
                "E. Biegert",
                "B. Vowinckel",
                "E. Meiburg"
            ],
            "title": "A collision model for grainresolving simulations of flows over dense",
            "venue": "mobile, polydisperse granular sediment beds, Journal of Computational Physics 340 ",
            "year": 2017
        },
        {
            "authors": [
                "P. Costa",
                "B.J. Boersma",
                "J. Westerweel",
                "W.-P. Breugem"
            ],
            "title": "Collision model for fully resolved simulations of flows laden with finite-size particles",
            "venue": "Physical Review E 92 (5) ",
            "year": 2015
        },
        {
            "authors": [
                "G. Hager",
                "G. Wellein"
            ],
            "title": "Introduction to High Performance Computing for Scientists and Engineers",
            "venue": "zeroth Edition, CRC Press",
            "year": 2010
        },
        {
            "authors": [
                "C. Feichtinger",
                "J. Habich",
                "H. K\u00f6stler",
                "U. R\u00fcde",
                "T. Aoki"
            ],
            "title": "Performance modeling and analysis of heterogeneous lattice Boltzmann simulations on CPU\u2013GPU clusters",
            "venue": "Parallel Computing 46 ",
            "year": 2015
        }
    ],
    "sections": [
        {
            "text": "In recent years, it has become increasingly popular to accelerate numerical simulations using Graphics Processing Unit (GPU)s. In multiphysics simulations, the various combined methodologies may have distinctly different computational characteristics. Therefore, the best-suited hardware architecture can differ between the simulation components. Furthermore, not all coupled software frameworks may support all hardware. These issues predestinate or even force hybrid implementations, i.e., different simulation components running on different hardware. We introduce a hybrid coupled fluid-particle implementation with geometrically resolved particles. The simulation utilizes GPUs for the fluid dynamics, whereas the particle simulation runs on Central Processing Unit (CPU)s. We examine the performance of two contrasting cases of a fluidized bed simulation on a heterogeneous supercomputer. The hybrid overhead (i.e., the CPU-GPU communication) is negligible. The fluid simulation shows good performance utilizing nearly the entire memory bandwidth. Still, the GPU run time accounts for most of the total time. The parallel efficiency in a weak scaling benchmark for 1024 A100 GPUs is up to 71%. Frequent CPU-CPU communications occurring in the particle simulation are the leading cause of the decrease in parallel efficiency. The results show that hybrid implementations are promising for large-scale multiphysics simulations on heterogeneous supercomputers.\n\u2217Corresponding author. E-mail addresses: samuel.kemmler@fau.de (S. Kemmler), christoph.rettinger@fau.de (C. Rettinger), harald.koestler@fau.de (H. Ko\u0308stler)\nPreprint submitted to Computer Physics Communications March 22, 2023\nar X\niv :2\n30 3.\n11 81\n1v 1\n[ cs\n.C E\n] 2\n1 M\nKeywords: Hybrid implementation, High-performance computing, Particulate flow, Lattice Boltzmann method, Discrete element method"
        },
        {
            "heading": "1. Introduction",
            "text": "Numerical multiphysics simulations are a powerful technique for conducting in-depth investigations of complex physical phenomena by providing detailed data, which is challenging, if not impossible, to collect in experiments. A prominent example is coupled fluid-particle simulations with geometrically resolved particles. Such simulations have been used in the literature to understand the formation and dynamics of dunes in river beds [1, 2, 3], investigate the erosion kinetics of soil under an impinging jet [4] and to analyze mobile sediment beds [5, 6]. A significant challenge with these simulations is that they are very computationally expensive, which is why they often run on supercomputers. Especially supercomputers containing Graphics Processing Unit (GPU)s have become increasingly popular in recent years [7, 8, 9] as they offer unprecedented computing power. In such multiphysics simulations, the various combined methodologies may exhibit distinctly contrasting computational properties, e.g., problem sizes, parallel and sequential portions, frequency of conditions, and branching. Therefore, the best-suited hardware architecture can differ between the simulation components. Furthermore, there are practical reasons that can challenge multiphysics simulations: not all coupled software frameworks and modules may support the same hardware. We face these challenges in coupled fluid-particle simulations with geometrically resolved particles. The fluid simulation using a structured, grid-based methodology is ideally suited for GPU parallelization [10]. The situation is different for the particle simulation because it operates on unstructured data structures with irregular accesses due to frequent branching. On top of that, when combining the particle simulation with a fluid simulation using a fully resolved coupling, the number of particles is typically at least three orders of magnitude smaller than the number of fluid cells, leading to an imbalance of the workload. These characteristics of the particle simulation do not predestine it for GPUs. Porting the entire fluid-particle simulations from the Central Processing Unit (CPU) to the GPU would be an enormous expenditure of time. Latter is hard to justify for the particle simulation as it is debatable if the computational properties of the particle simulation are suited for GPUs at all. This is a major\nreason why coupled fluid-particle implementations still are often limited to CPUs [1, 3, 5], even though GPUs would be best suited for the dominating part: the fluid simulation. These issues predestinate or even force a hybrid implementation, i.e., utilizing GPUs for the fluid dynamics and the coupling. In contrast, the Particle Dynamics (PD) simulation runs on CPUs. Hybrid coupled fluid-particle simulations have been used several times in the literature. On the one hand, a common approach is to use the CPU for the fluid simulation and the GPU for the PD, for example, when simulating deformable bodies (using the finite element method) in a fluid, i.e., blood cells in cellular blood flow [11]. Here, the focus is more on structural mechanics, as the finite element method dominates the overall run time. The latter approach has also been taken in the literature to couple commercial Computational Fluid Dynamics (CFD) solvers on the CPU such as Ansys [12, 13], or AVL fire [14] with the Discrete Element Method (DEM) on the GPU. This approach also appears in the context of fluidized beds [15, 16]. On the other hand, several publications have used the approach promising for our problem: using the CPU for the particles and the GPU for the fluid. One variant is to run the Lattice Boltzmann Method (LBM) on a single GPU and use a single CPU for the particles [17], or use the GPU for both the fluid and the particles, but the CPU supports the particle dynamics through collision detection [18]. Another idea is to use either multiple CPUs for the particle simulation [19], or multiple GPUs for the LBM [20]. The Top500 list reports the most powerful supercomputer worldwide. The number of heterogeneous supercomputers, i.e., systems with additional accelerators such as GPUs, in the Top500 list has steadily increased in the last years, accounting for roughly 28% at the end of 2019 [21]. Each node of such a heterogeneous supercomputer typically consists of multiple CPUs and multiple GPUs [22]. On the one hand, hybrid implementation can fully utilize the capacities of a heterogeneous supercomputer by using both the CPUs and the GPUs. On the other hand, for a responsible use of a heterogeneous supercomputer, the performance has to meet specific criteria. First, the penalty introduced by the hybrid implementation (i.e., the CPU-GPU communication) has to be negligible compared to the overall run time. Second, the performance-critical fluid simulation should show good performance, i.e., fully utilize the GPU memory bandwidth. Third, the share of the GPU run time in the total run time must be sufficiently high to justify using a heterogeneous cluster. Fourth, it has to show a satisfactory weak scaling performance for the responsible use of multiple supercomputer nodes. How-\never, none of the above publications provide a framework for geometrically resolved fluid-particle simulations fulfilling all the criteria. In this paper, we want to investigate if hybrid implementations are suitable for large-scale coupled fluid-particle simulations on heterogeneous supercomputers. Therefore, we supplement the existing literature by introducing a multi-CPU-multi-GPU hybrid implementation for coupled fluid-particle simulations with geometrically resolved particles. To evaluate our implementation based on the above criteria, we provide in-depth performance analysis on a state-of-the-art heterogeneous supercomputer comparing two cases of a fluidized bed simulation with a distinctly different number of particles per volume. We assess the hybrid penalty, investigate the run time distribution among the various simulation components and provide an in-depth analysis of the weak scaling performance for up to 1024 A100 GPUs. We use code generation technologies to obtain highly efficient compute kernels and combine them with handwritten codes. The paper starts in Section 2 with an introduction of the numerical methods, i.e., the LBM, the DEM, and the corresponding coupling. The numerical methods are followed by Section 3 with a description of the implementation. In Section 4, we provide a detailed performance examination. This examination includes the CPU-GPU communication overhead, the DEM performance, the run times of the different simulation components, a weak scaling, a roofline model, and an estimation of the hybrid speedup. The paper concludes with Section 5."
        },
        {
            "heading": "2. Numerical methods",
            "text": "Generally, fully resolved coupled fluid-particle simulations consist of three components: fluid dynamics, particle physics, and fluid-particle coupling. In this section, we introduce the background of the here applied methods based on the work of Rettinger et al. [23, 24]. Fig. 1 illustrates coupled fluid-particle simulations using the Partially Saturated Cells Method (PSM), which will be explained in the upcoming sections."
        },
        {
            "heading": "2.1. Lattice Boltzmann method",
            "text": "We use the LBM with the D3Q19 lattice model for the hydrodynamics simulation, an alternative to conventional Navier-Stokes solvers [25]. We evolve 19 Particle Distribution Functions (PDFs) fq with q \u2208 {0, . . . , 18} for every cell of a three-dimensional cartesian lattice. Each fq is associated with\na lattice velocity cq. The underlying update rule is based on the Boltzmann equation, typically split into the streaming and collision steps. The celllocal collision relaxes the PDFs towards a thermodynamic equilibrium. The streaming propagates the post-collision PDFs f\u0303q to neighboring cells. The collision step for the lattice cell x at time step t is defined as\nf\u0303q(x, t) = fq(x, t) + Cq(x, t) + Fq(x, t), (1)\nwith Cq being the collision operator and Fq the forcing operator. The streaming step\nfq(x+ cq\u2206t, t+\u2206t) = f\u0303q(x, t) (2)\ndistributes the PDFs to neighboring cells. \u2206t is the time step size, which is typically 1 in the context of the LBM. The most commonly applied collision operator is based on the Single Relaxation Time (SRT) model (also known as the BGK model [26]). It relaxes the PDFs towards their equilibrium using a single relaxation time \u03c4 :\nCSRTq (x, t) = \u2206t\n\u03c4 (f eqq (\u03c1f,U f)\u2212 fq(x, t)). (3)\nThe relaxation time \u03c4 is linked to the kinematic fluid viscosity \u03bd by:\n\u03bd = (\u03c4 \u2212\u2206t/2)c2s . (4)\nThe equilibrium is defined as\nf eqq (\u03c1f,U f) = wq ( \u03c1f + \u03c10 ( cq \u00b7U f c2s + (cq \u00b7U f)2 2c4s \u2212 U f \u00b7U f 2c2s )) (5)\nfor incompressible flows [27] with \u03c10 = 1 and the lattice speed of sound cs = 1/ \u221a 3. The cell-local quantities\n\u03c1f(x, t) = \u2211 q fq(x, t) and U f(x, t) = 1 \u03c10 \u2211 q fq(x, t)cq+ \u2206t 2\u03c10 f ext (6)\nare calculated based on the moments of the PDFs. The forcing operator\nFq(x, t) = \u2206twq [ cq \u2212U f c2s + cq \u00b7U f c4s \u00b7 cq ] \u00b7 f ext (7)\nincorporates external forces using the constant force density f ext [28]."
        },
        {
            "heading": "2.2. Particle dynamics",
            "text": "The total force F p,i acting on a particle i consists of the following components:\nF p,i = F col p,i + F hyd p,i + F ext p,i . (8)\nIn addition to the hydrodynamic force F hydp,i and external forces F ext p,i (e.g., gravity), the particle interactions exert forces F colp,i on each other due to collisions. The equations of motion have to be integrated to simulate the particle movements."
        },
        {
            "heading": "2.2.1. Particle interactions using the discrete element method",
            "text": "The collision between particle i and j is modeled using the DEM [29]. The collision force F colp,i and torque T col p,i on particle i are computed as\nF colp,i = \u2211 j,j 6=i (F colij,n + F col ij,t), (9)\nT colp,i = \u2211 j,j 6=i (xcpij \u2212 xp,i)\u00d7 F colij,t, (10)\nwhere the normal part of the collision force F colij,n acting on particle i with position xp,i is computed as a linear spring-dashpot model:\nF colij,n = \u2212kn\u03b4ij,nnij \u2212 dnU cp ij,n. (11)\nHere, kn and dn are the normal stiffness and damping coefficients, nij the normal vector, \u03b4ij,n is the penetration depth and U cp ij,n is the normal component of the relative velocity of the surface of the particle at the contact point xcpij . The tangential part of the collision force\nF colij,t = \u2212kt\u03b4ij,t \u2212 dtU cp ij,t (12)\nuses the tangential stiffness and damping coefficients kt and dt and U cp ij,t is the tangential component of the relative velocity of the surface of the particle at the contact point.\n\u03b4ij,t = \u222b t ti U cpij,t(t \u2032)dt\u2032 (13)\nis the accumulated relative tangential motion between two particles where ti is the time step of the impact. For more details, see Rettinger et al. [24]."
        },
        {
            "heading": "2.2.2. Integration of the particle properties",
            "text": "We update the particle\u2019s position and velocity by solving the NewtonEuler equations of motion using the Velocity Verlet integrator:\nxp,i(t+\u2206tp) = xp,i(t) +\u2206tpUp,i(t) + \u2206t2p\n2mp,i F p,i(t), (14)\nUp,i(t+\u2206tp) = Up,i(t) + \u2206tp\n2mp,i (F p,i(t) + F p,i(t+\u2206tp)), (15)\nwhere mp,i is the mass of the particle i. xp,i(t + \u2206tp) is computed at the beginning of each particle time step using the old force. Then, the new force F p,i(t+\u2206tp) is computed using the updated position. At the end of the time step, the particle velocity Up,i(t+\u2206tp) is computed using the updated force. Updating the angular velocity is done analogously."
        },
        {
            "heading": "2.3. Partially saturated cells method",
            "text": "The task of the coupling is to perform momentum exchange between the fluid and the solid phase. The PSM modifies the LBM collision step from Eq. (1) by introducing the solid volume fraction B(x, t) resulting in\nf\u0303q(x, t) = fq(x, t) + C PSM q (x, t) + (1\u2212B(x, t))Fq(x, t), (16)\nwhereB(x, t) is the fraction of the fluid cell x being (partly) covered by one or more particles. Section 2.3.2 explains this solid volume fraction computation in detail. The modified collision operator CPSMq used in Eq. (16) is defined as\nCPSMq (x, t) = (1\u2212B(x, t))CSRTq (x, t) + \u2211 I Bi(x, t)C solid q,i (x, t). (17)\nCSRTq is the LBM collision operator described in Eq. (3). B(x, t) is the sum over the individual overlap fractions Bi(x, t) of all particles I. If B(x, t) > 1, it is normalized to 1. B(x, t) > 1 can occur if colliding particles are allowed to overlap during the contact. Then a single fluid cell can be, e.g., entirely covered by two particles, i.e., B(x, t) = 2. The solid collision operator\nCsolidq,i (x, t) = [fq\u0304(x, t)\u2212 f eq q\u0304 (\u03c1f,U f)]\u2212 [fq(x, t)\u2212 f eqq (\u03c1f,Up,i(x, t))] (18)\nacts when particles intersect with a cell. There exist different variants of the solid collision operator. fq\u0304 corresponds to the inverse lattice velocity of fq. Up,i(x, t) is the velocity of particle i evaluated at the cell center x and is computed as\nUp,i(xi, t) = Up,i(t) + \u2126p,i(t)\u00d7 (xi \u2212 xp,i(t)), (19)\nwith the translational particle velocityUp,i(t), the rotational particle velocity \u2126p,i(t) and the particle center of gravity xp,i(t). xi are the cell centers of all cells intersecting with the particle. So far, we have only considered the influence of the particles on the fluid. However, the fluid also influences the particles through hydrodynamic forces. We compute the force F fpp,i(t) and torque T fp p,i(t) exerted by the fluid on particle i as\nF fpp,i(t) = (\u2206x)3\n\u2206t \u2211 xi [Bi(xi, t) \u2211 q (Csolidq,i (xi, t)cq\u0304)], (20)\nT fpp,i(t) = (\u2206x)3\n\u2206t \u2211 xi [Bi(xi, t)(xi \u2212 xp,i)\u00d7 \u2211 q (Csolidq,i (xi, t)cq\u0304)], (21)\nWe chose the PSM as it has been used successfully in the context of GPUs [4, 30]. Its similarities to pure LBM and the accompanying Single Instruction, Multiple Data (SIMD) nature make this coupling method predestined for GPUs."
        },
        {
            "heading": "2.3.1. Lubrication correction",
            "text": "The lubrication force and torque act on two particles approaching each other. The two particles squeeze out the fluid inside the gap, which exerts a force in the opposite direction of the relative motion. However, this effect would only be covered correctly by the fluid-particle coupling for a very fine grid resolution which is computationally too expensive. As the lubrication force has a significant influence, we compute lubrication correction force terms to compensate for the inability of the coupling method to represent these forces correctly. We compute lubrication correction terms due to normal- and tangential translations and rotations. Therefore, the total hydrodynamic force F hydp,i and torque T hyd p,i is a sum of the force from the fully resolved fluid-particle coupling method (see Section 2.3), and the lubrication correction:\nF hydp,i = F fp p,i + F lub,cor p,i , (22)\nT hydp,i = T fp p,i + T lub,cor p,i . (23)"
        },
        {
            "heading": "2.3.2. Particle mapping",
            "text": "A coupled fluid-particle simulation using the PSM requires the computation of the solid volume fraction Bi(x, t) (see Section 2.3), i.e., the fraction of a fluid cell x being (partly) covered by a particle i. We restrict ourselves to spherical particles. Jones et al. [31] tackle the problem that no unique analytical solution exists to compute this overlapping fraction for spheres. They propose a linear approximation derived from the analytical solution for\na specific cell orientation relative to the particle surface as depicted in Fig. 2. Grid cells with the dimensionless edge size 1 (as it is the case for the LBM) are assumed in the following.\nThe overlap fraction between a lattice cell and a particle is computed as\n= Vi = Va \u2212 Vb = Va \u2212 (D + r \u2212 1/2), (24) where Va is the union of Vi and Vb. D is the distance from the cell center to the sphere surface (negative if the cell center lies inside the sphere). There is a cell-particle overlap if \u2208 ]0, 1]. We can reformulate this as\n= Va \u2212 (D + r \u2212 1/2) = \u2212D + f(r), (25) where f(r) = Va\u2212r+1/2 only depends on the particle radius r and therefore is constant for each particle respectively. Va is computed as\n(26)\nVa = \u222b 1/2 \u22121/2 \u222b 1/2 \u22121/2 \u221a r2 \u2212 x2 \u2212 y2dxdy\n= (1/12\u2212 r2) tan\u22121( 1 2\n\u221a r2 \u2212 1/2\n1/2\u2212 r2 ) +\n1\n3\n\u221a r2 \u2212 1/2\n+ (r2 \u2212 1/12) tan\u22121( 1/2\u221a r2 \u2212 1/2 )\u2212 4 3 r3 tan\u22121(\n1/4 r \u221a r2 \u2212 1/2 ).\nJones et al. showed that this approximation yields accurate results also for arbitrary cell orientations and is more computationally efficient than contemporary techniques."
        },
        {
            "heading": "3. Implementation",
            "text": "We implemented our hybrid coupled fluid-particle simulation within the massively parallel multiphysics framework waLBerla [32] (https://www. walberla.net/). waLBerla supports highly efficient and scalable LBM simulations on both CPUs and GPUs [10]. The MesaPD module [33] empowers waLBerla to perform particle simulations on CPUs using the DEM. Large-scale simulations require the usage of numerous nodes of a supercomputer. Each node of a heterogeneous supercomputer typically consists of multiple CPUs and multiple GPUs. Each CPU core belongs to one GPU. We divide the simulation domain into multiple blocks and exclusively assign each block to a GPU. The CPU cores belonging to the respective GPU are responsible for updating the particles whose center of mass lies inside that block (local particles). Additionally, particles can overlap with a given block whose center of mass lies in another block (ghost particles). This overlapping causes the need for communication between the CPUs. The particle computations within a block are parallelized among the CPU cores using OpenMP. The communication between the blocks is implemented using the CUDA-Aware Message Passing Interface, also allowing for direct GPU-GPU communications. Fig. 3 illustrates the different components of the simulation, on which hardware they are running, the workflow, and the necessary communications. We will explain the figure in detail in the following sections. Generally speaking, the GPU is responsible for all operations on fluid cells (i.e., the LBM and the coupling), whereas the CPU performs all computations on particles. The associated data structures are consequently located in the respective memories (fluid cells in GPU memory, particles in CPU memory)."
        },
        {
            "heading": "3.1. Fluid dynamics and coupling on the GPU",
            "text": "Performing an LBM update step requires the communication of boundary cells between neighboring GPUs. However, the first three kernels do not need neighboring information. Therefore, the communication is hidden behind those kernels by starting a non-blocking send before the particle mapping. A time step begins with the coupling from the particles to the fluid."
        },
        {
            "heading": "3.1.1. Coupling from the particles to the fluid",
            "text": "For the particle mapping, the GPU has to check overlaps for all cellparticle combinations. This check quickly becomes very computationally expensive, even though there is no overlap for most cell-particle combinations. Therefore, we reduce the computational effort by dividing each block into k sub-blocks in each dimension. The CPU inserts every particle into all sub-blocks that overlap with this particle. Using sub-blocks allows the GPU to consider only a tiny subset of particles when computing the overlaps for a particular grid cell, namely the particles overlapping with the subblock the cell is located in. Our coupled fluid-particle simulation requires the communication of various data. Fig. 4 gives an overview of the different communications from the perspective of a CPU and GPU responsible for the same block. We will explain the communications in the following. For every particle i, the position xp,i, radius ri, and f(ri) (see Section 2.3.2) are transferred from the CPU to the GPU. In addition, the number of overlapping particles per sub-block and the corresponding particle IDs are transferred from the CPU to the GPU. Then, the GPU performs the particle mapping. In detail, we described the solid volume fraction computation (i.e., the particle mapping) in Section 2.3.2. In our simulation, a maximum of two particles can overlap with a given grid cell due to the geometrically resolved spherical particles and appropriate DEM parameters allowing only a small particleparticle penetration. Therefore, the grid we use to store Bi(x, t) can store two fraction values per grid cell. Next, the linear and angular velocities Up,i and \u2126p,i of the particles have to be synchronized between the CPUs such that every CPU not only has the correct velocities for its local particles but also for the ghost particles. Next, those velocities are transferred from the CPU to the GPU so that the GPU can compute the velocities of the overlapping particles at the cell center for every cell (see Eq. (19))."
        },
        {
            "heading": "3.1.2. Fluid simulation",
            "text": "Next, the PSM inner kernel is performed. The term \u2018inner\u2019 indicates that this kernel updates all cells except the outermost layer of cells. Skipping the outermost layer ensures this routine can be called without waiting for the previously started GPU-GPU communication to finish. The PSM kernel has the highest workload of the entire simulation. It is, therefore, performance-critical. We use the code generation framework lbmpy [34] to obtain highly efficient and scalable LBM CUDA kernels. lbmpy allows to formulate LBM methods (such as the PSM) as a symbolic representation and\ngenerates optimized and parallel compute kernels. We integrate those generated compute kernels within our simulation in waLBerla. Next, we wait for the non-blocking GPU-GPU communication started at the beginning of the time step to finish. Depending on the available hardware, this may return instantly if the previous computations completely hide the communication. The next step is the LBM boundary handling, which enforces boundary conditions to the fluid simulation by correctly updating the fluid cells at the domain\u2019s boundary. Since the neighboring values are now available, we then update the outermost layer of cells in the PSM outer kernel. The last step on the GPU is the coupling from the fluid to the particles."
        },
        {
            "heading": "3.1.3. Coupling from the fluid to the particles",
            "text": "Finally, the GPU reduces the forces and torques exerted by the fluid on the particles F fpp,i and T fp p,i (see Eqs. (20) and (21)). Then, F fp p,i and T fp p,i are transfered from the GPU to the CPU to be available for the upcoming DEM simulation on the CPU. A single particle may overlap with cells located on multiple blocks. Thus, multiple GPUs may have computed F fpp,i and T fp p,i for the same particle i. Therefore, the corresponding CPUs have to reduce these force and torque contributions exerted by the coupling on the particles into a single variable (CPU-CPU communication). The time loop continues with the PD on the CPU."
        },
        {
            "heading": "3.2. Particle dynamics on the CPU",
            "text": "The first step of the PD simulation on the CPU is the pre-force integration of the velocities to update the particle positions (see Eq. (14)). The latter particle movement requires synchronization between the CPUs to account for the position update, which potentially moves particles from one block to another, making other CPUs responsible for the particles. Computing particle-particle interactions by iterating over all particle pairs can quickly become very expensive due to its O(n2) complexity. Therefore, we insert the particles into linked cells such that iterating over the particle pairs is limited to neighboring linked cells. The linked cells have a size of 1.01 times the particle diameter. Next, the lubrication correction routine is applied to all particle pairs with particles close to each other but not yet in contact (see Section 2.3.1). The particle-particle interactions are modeled using the DEM kernel, which exerts forces and torques on overlapping particles. The DEM kernel needs history information from the previous time step, i.e., the accumulated tangential motion between the two colliding particles (see Eq. (13)). Since a different process may have handled the previous collision, reducing the collision histories between the CPUs is necessary. Then, the hydrodynamic forces and torques from the coupling, and the gravitational force are added to the total force. Since different processes may have added forces and torques to the same particle, those contributions have to be reduced into one process (another CPU-CPU communication). Then, the post-integration is applied to update the particle velocities (see Eq. (15)). Here, communication can be omitted because the velocities are unused until the subsequent communication in the upcoming sub-cycle. Typically, N particle sub-cycles are performed per time step since the DEM requires a finer resolution in time than the LBM for an accurate contact representation [24]. After completing N sub-cycles, the next time step starts with the fluid dynamics on the GPU."
        },
        {
            "heading": "4. Performance analysis",
            "text": "We use the Juwels Booster cluster for the performance measurements. Each GPGPU node consists of four Nvidia A100 40 GB, two AMD EPYC 7402 processors (24 cores per chip), and eight NUMA domains. Thus, six cores belong to one NUMA domain. The following will refer to a GPU and an associated NUMA domain with its six cores as a CPU-GPU pair. All GPUs within a node are connected via NVLinks, allowing direct GPU-GPU communications. A scaling benchmark (i.e., a 1:1 read/write ratio) yields a\nmemory bandwidth of about 1400 GB/s for the A100 40 GB [35]. We use 20 cells per diameter to geometrically resolve the particles [6, 24, 36, 37]. The upcoming sections first introduce the computational properties of the simulated cases, followed by their performance results."
        },
        {
            "heading": "4.1. Simulation setups",
            "text": "We study the performance of our hybrid coupled fluid-particle implementation using a fluidized bed simulation. We compare two cases: the dilute case and the dense case. They exhibit different characteristics regarding the number of particles per volume and the number of particle-particle interactions. We choose these two cases to investigate how different particle workloads on the CPUs influence the overall performance of the hybrid CPUGPU implementation. We use ten particle sub-cycles (see Section 3) per time step. We discretize the simulated domain using 500\u00d7 200\u00d7 800 = 80\u00d7 106 fluid cells. The inflow Boundary Condition (BC) on the bottom and pressure (outflow) BC on top of the domain govern the fluid dynamics. The remaining four boundaries are no-slip conditions ensuring a zero fluid velocity. The particle Reynolds number is 1.0, the Galileo number is around 8.9, the gravitational acceleration is 9.81 m/s2, and the particle fluid density ratio is 1.1. Planes surrounding the domain prevent the particles from leaving the domain. The dilute case contains 627 particles. Fig. 5 (left) illustrates the dilute setup. Due to the low particle concentration, the effort for computing the particle-particle interactions (collisions and lubrication corrections) is low in the dilute case. The dense case is generally the same setup as the dilute case, except that the particle concentration is significantly higher (see Fig. 5 on the right), resulting in 8073 particles, almost a 13-fold increase compared to the dilute case."
        },
        {
            "heading": "4.2. Performance results",
            "text": "In this section, we present and analyze the performance results. We first look into the scaling of the PD on multiple CPU cores to assess our initial assumption that the particle simulation part of such a coupled simulation would not benefit from a GPU parallelization. Next, we look at the individual run times of the different simulation components to understand the bottlenecks. Furthermore, we present a weak scaling benchmark for both cases up to 1024 CPU-GPU pairs. Finally, we demonstrate the acceleration potential of hybrid implementations by comparing it to a large-scale CPUonly simulation from the literature. For all results, we average over 500 time\nsteps. In the following, we will refer to the performance criteria formulated in the introduction."
        },
        {
            "heading": "4.2.1. Particle scaling on the CPU",
            "text": "Using OpenMP with static scheduling, we parallelize the PD simulation on the CPU. Per-particle computations (e.g., adding gravitational forces) are parallelized among the particles, and particle-particle interactions are parallelized among the linked cells in one dimension. We use multiple cores of a single CPU and measure the run time of the PD simulation. The parallel efficiency En is defined based on the serial run time ts and the run time using n cores in parallel tp(n):\nEn = ts\ntp(n) \u00b7 n . (27)\nFor a perfectly linear scaling code, i.e., tp(n) = ts/n, the maximum achievable parallel efficiency Emax is 1 \u2200n. For a non-scaling code, i.e., tp(n) = ts \u2200n, the minimum achievable parallel efficiency Emin is 1/n. Fig. 6 reports the parallel efficiency of the PD simulation for both cases for up to six cores, i.e., the size of a NUMA domain. We do not use more cores than available in a NUMA domain since the PD implementation is not NUMA-aware. We observe a similar pattern regarding the parallel efficiency in both the dilute and the dense case. Both show a decrease down to around 45% for six cores. A parallel efficiency of about 45% when using a comparatively small number of six cores is a poor scaling result. We expect this performance for several reasons. First, the PD simulation consists of ten subsequent sub-cycles per time step, each containing several consecutive tasks (see Fig. 3). Each task consists of an OpenMP parallel loop with an implicit OpenMP barrier at the end, causing idle threads to wait for all threads to end the loop. Second, modifying the force and torque of a particle must be enclosed with an atomic region to avoid race conditions, which harms the parallel efficiency, but is a central task and thus required in several PD routines. Third, the workload (for the DEM and lubrication routine) per particle can differ significantly depending on the particle\u2019s location inside the domain. The latter aspects hold for the dense and the dilute case, explaining the poor parallel efficiency in both cases. The poor parallel efficiency of the particle methodology supports our initial assumption that the particle simulation part would not benefit\nfrom a GPU parallelization in the context of geometrically resolved coupled simulations."
        },
        {
            "heading": "4.2.2. Run times of different simulation components",
            "text": "We investigate the run times of the different simulation components to analyze the penalty introduced by the hybrid implementation, i.e., the CPUGPU communication, assess the GPU performance, and detect the overall bottlenecks. To evaluate the performance of the PSM kernel on the GPU, we build a roofline model [38] for an LBM kernel. As this kernel is memory bound, we determine the maximal possible performance for the given kernel and hardware when fully utilizing the memory bandwidth (i.e., the performance lightspeed estimation). The PSM kernel comprises the LBM kernel plus additional memory transfers depending on the number of overlapping particles. As this number differs from cell to cell, the PSM roofline model is not straightforward. Therefore, we use the LBM model, keeping in mind that this is a too-optimistic performance lightspeed estimation. Since we use the D3Q19 lattice model, we read and write 19 PDFs per cell and time step. This results in 19 reads and 19 writes (double-precision), i.e., 304 bytes to update one lattice cell [39]. The domain consists of 8e7 fluid cells. This results in the following minimal run time according to the roofline model:\nTmin = 304 B/cell \u00b7 8e7 cells/time step\n1400 GB/s = 17.4 ms/time step. (28)\nWe divide the total run time into the following components: the PSM kernel (PSM), the CPU-GPU communication (comm), the particle mapping (mapping), setting the particle velocities (setU), reducing the hydrodynamic forces F fpp,i and torques T fp p,i on the particles (redF), computing the PD and finally the remaining tasks (other), e.g., the LBM boundary handling. Fig. 7 reports the run times per time step for these components using a single CPU-GPU pair. In the dilute case, the PSM kernel needs about 42% more time per time step than the LBM lightspeed estimation, and the dense case 83% more time. The CPU-GPU communication is negligible for both cases. All components take longer in the dense case than in the dilute case. While in the dilute case, the PSM kernel accounts for the majority of the run time, the PD simulation needs more time than the PSM kernel in the dense case. Still, most of the run time is spent on GPU routines in the dense case. The penalty introduced by the hybrid implementation is negligible because we only transfer a small amount of double-precision values per particle but\nno fluid cells. The penalty, therefore, shows that a hybrid parallelization with the presented technique is a viable approach, and the first criterion is met. The performance of the PSM kernel is close to utilizing the total memory bandwidth of the A100, especially considering that the roofline model does not consider the memory traffic due to the solid part of the PSM kernel. Therefore, the second criterion is met. There is a significant performance gain for the PSM kernel on the GPU compared to a CPU-only implementation since the PSM kernel is utilizing almost the entire memory bandwidth, which is typically way lower on CPUs. Even in the dense case, the GPU run time accounts for most of the total time because the fluid workload is much higher than the particle workload. Therefore, the third criterion is met."
        },
        {
            "heading": "4.2.3. Weak scaling",
            "text": "When increasing the simulation domain further to simulate physically relevant scenarios, using a single CPU-GPU pair is often insufficient. Instead, multiple pairs or even multiple nodes of a supercomputer have to be used. Therefore, a satisfactory weak scaling is inevitable. For weak scaling, the problem size is increased with an increasing number of CPU-GPU pairs, keeping the workload per CPU-GPU pair constant. When having a perfect weak scaling, the performance per CPU-GPU pair stays constant, independent of the number of CPU-GPU pairs used. Performance is work over time. In the context of LBM, MLUPs is a standard performance metric for weak scaling, meaning how many mega lattice cell updates the hardware performs per second. We use the total run time for computing the MLUPs, containing both the CPU (particles) and the GPU (fluid, coupling) time. We have conducted a few benchmarking runs and will use the best sample in the following weak-scaling plots. We start with a single CPU-GPU pair and a single domain block as described in Section 4.1. We then double the number of CPU-GPU pairs several times until we reach 1024. At the same time, we double the domain blocks/size alternately in each direction: 2\u00d7 1\u00d7 1 blocks (two GPUs), 2\u00d72\u00d71 blocks (four GPUs), 2\u00d72\u00d72 (eight GPUs), etc. Fig. 8 reports the weak scaling for both cases. To the best of our knowledge, this is the most extensive weak scaling of a hybrid fluid-particle implementation presented in the literature. We observe a roughly three times higher performance for the dilute case than for the dense case. Both cases show a parallel efficiency decrease particularly strong in the beginning. The parallel efficiency is 71% in the dilute case and 53% in the dense case when using 1024 CPU-GPU pairs, which corresponds\nto a domain size of 8000\u00d7 1600\u00d7 6400 = 8.192\u00d7 1010 fluid cells. A similar scaling behavior has been observed in the literature both for other hybrid [11] and CPU-only fluid-particle implementations [1]. Interpreting the overall weak scaling behavior requires an in-depth analysis of the scaling of the different simulation components. When using a single CPU-GPU pair, the dominating routines are the PD, the PSM kernel, and the coupling (i.e., particle mapping, setting the particle velocities and reducing the hydrodynamic forces F fpp,i and torques T fp p,i on the particles). Additionally, we must now consider the communication (comm) overhead that arises from using multiple CPU-GPU pairs. On the one hand, this is the PD communication (CPU-CPU communication), on the other hand, the PSM communication (GPU-GPU communication). Fig. 9 and Fig. 10 show the weak scaling behavior of the dominating simulation components for both cases. The communication numbers cover the communications themselves, but also load imbalances between two communication. The different components show similar qualitative scaling behavior when comparing the two cases. The PSM kernel scales quite well in both cases. The corresponding GPU-GPU communication (PSM comm) is negligible. The PD run time increases initially and then shows saturation. The CPUCPU communication (PD comm) increases drastically, overtaking the run\ntime of the PSM kernel and the coupling in the dense case. The PD and the corresponding communication are more relevant for the overall scaling in the dense case than in the dilute case. The coupling scales similarly to the PD. The PSM workload per GPU stays constant in the weak scaling explaining the nearly perfect scaling. Since we are hiding the PSM communication (see Section 3), we expect it to be negligible. We expect the PD and the coupling run time to increase initially because the number of neighboring blocks increases. More neighboring blocks lead to more ghost particles per block, resulting in a higher workload. This effect fades out when blocks have neighbors in all directions resulting in an almost linear scaling from this point on. This phenomenon has been reported in the literature [1]. The methodology requires ten particle sub-cycles per time step and three communications per sub-cycle for a physically accurate simulation. Additionally, the simulation requires two CPU-CPU communications per time step apart from the subcycles (see Fig. 3). We have 32 CPU-CPU communications per time step, which cannot be hidden behind other routines. It is the dominating factor for the decrease of the overall weak scaling performance in both cases. We assume this is due to these frequent synchronizations between the processes and the high pressure on the network. Reducing the collision history is part of PD comm, which includes swapping old and new contact information. Since this swap is also necessary without using multiple CPU-GPU pairs, PD comm is bigger zero even when using a single CPU-GPU pair. Overall, we observe a weak scaling performance that justifies using multiple supercomputer nodes. Therefore, the fourth criterion is met."
        },
        {
            "heading": "4.2.4. Potential speedup of hybrid implementations",
            "text": "We expect that the speedup of the hybrid implementation compared to a CPU-only code Shyb can be estimated as\nShyb \u2248 1\n1 + fracacc \u00b7 ( BWCPU BWGPU\n\u2212 1) , (29)\nwhere BWCPU and BWGPU are the CPU and GPU memory bandwidths. fracacc is the CPU-only run time fraction of the component accelerated by the hybrid implementation. In our case, this is the PSM and the coupling. We assume fracacc is memory bound. We compare our hybrid performance results with one of the largest CPU-only simulations of polydisperse sediment beds [6]. The authors conducted the latter simulation on 320 Intel Xeon\nPlatinum 8174 CPUs. In total, they computed 2.25 \u00b7 1015 lattice cell updates in 48 hours, which leads to a performance of around 41 MLUPs per CPU vs. 377 MLUPs per CPU-GPU pair in the dense case when using 1024 GPUs. The latter numbers result in a measured speedup of around 9.2. For the Intel Xeon Platinum 8174, we measured a memory bandwidth BWCPU of 70 GB/s. The estimated speedup based on Eq. (29) is around 10.3 (assuming fracacc = 0.95 and BWGPU = 1400 GB/s) which is similar to the measured speedup. The latter computation is only a rough estimate since it ignores effects due to different CPUs, networks, physical setups, etc."
        },
        {
            "heading": "5. Conclusion",
            "text": "In this paper, we have introduced a hybrid coupled fluid-particle implementation with geometrically resolved particles. We use GPUs for the fluid dynamics, whereas the particle simulation runs on CPUs. We have addressed the issue that in multiphysics simulations, different methodologies can have distinctly different computational properties, implying that the best-suited hardware architecture may differ between the simulation components. The paper has examined the performance of this approach for two cases of a fluidized bed simulation that differ significantly in terms of the number of particles per volume. The particle methodology scales poorly, reaching a parallel efficiency of about 45% when using only six CPU cores. This scaling result supports our initial assumption that the particle simulation part of such a coupled simulation would not benefit from a GPU parallelization. The penalty introduced by the hybrid implementation (i.e., CPU-GPU communication) is negligible because we are transferring only a small amount of data per particle but no fluid cells. The performance of the fluid simulation is close to utilizing the whole memory bandwidth of the A100, implying that the GPU is the best choice for the fluid simulation. In both cases, the GPU routines take most of the run time. In a weak scaling benchmark, the hybrid fluid-particle implementation reaches a parallel efficiency of 71% in the dilute case and 53% in the dense case when using 1024 CPU-GPU pairs. The PD methodology requires 32 CPU-CPU communications per time step which is the driving force for the decrease of the overall parallel efficiency. Our results are limited insofar as different numbers of particle sub-cycles, fluid cells per diameter, etc., will result in different performance results. We have formulated four criteria that a hybrid implementation must meet to be suitable for the responsible use of heterogeneous supercomputers. The performance\nresults have shown that our hybrid implementation fulfills all criteria making it suitable for large-scale simulations on heterogeneous supercomputers. In the future, we plan to investigate the particle communications in more detail regarding the bottleneck and optimization possibilities. We have shown the acceleration potential of hybrid implementations. Therefore, we plan to run coupled fluid-particle simulations of unprecedented sizes to better understand complex physical phenomena occurring in sea and river beds.\nDeclaration of competing interest\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\nData availability\nData is available on Zenodo: DOI:10.5281/zenodo.7752954."
        },
        {
            "heading": "Acknowledgements",
            "text": "The authors gratefully acknowledge the Gauss Centre for Supercomputing e.V. (www.gauss-centre.eu) for funding this project by providing computing time on the GCS Supercomputer JUWELS at Ju\u0308lich Supercomputing Centre (JSC). The authors gratefully acknowledge the scientific support and HPC resources provided by the Erlangen National High Performance Computing Center (NHR@FAU) of the Friedrich-Alexander-Universita\u0308t Erlangen-Nu\u0308rnberg (FAU). The hardware is funded by the German Research Foundation (DFG).\nFunding\nThe authors thank the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) for funding the project 433735254. The DFG had no direct involvement in this paper.\nAuthor ORCIDs\nS. Kemmler, https://orcid.org/0000-0002-9631-7349; C. Rettinger, https://orcid.org/0000-0002-0605-3731; H. Ko\u0308stler, https://orcid. org/0000-0002-6992-2690;\nAuthor contributions\nS. Kemmler: Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data Curation, Writing - Original Draft, Visualization, Project administration; C. Rettinger: Conceptualization, Writing - Review and Editing; H. Ko\u0308stler: Resources, Writing - Review and Editing, Supervision, Funding acquisition;"
        }
    ],
    "title": "Efficient and scalable hybrid fluid-particle simulations with geometrically resolved particles on heterogeneous CPU-GPU architectures",
    "year": 2023
}