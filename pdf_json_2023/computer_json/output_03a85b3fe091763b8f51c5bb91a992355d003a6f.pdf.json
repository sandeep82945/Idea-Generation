{
    "abstractText": "Discovery of causally related concepts is one of the key challenges in extracting knowledge from observational data. Lower-dimensional \u201ccausal macrovariables\u201d represent concepts which preserve all relevant causal information in high-dimensional systems. Existing causal macrovariable discovery algorithms are limited by assumptions about known and controllable interventions. We propose a variational autoencoder-inspired architecture with regularization terms for semi-supervised causal macrovariable discovery. These terms impose domain knowledge regarding visual causal concepts to differentiate between correlation and causation. Experiments on both synthetic and real-world datasets with known causal dynamics show that our method can discover more concise and precise causal macrovariables than unsupervised",
    "authors": [
        {
            "affiliations": [],
            "name": "Aruna Jammalamadaka"
        },
        {
            "affiliations": [],
            "name": "Lingyi Zhang"
        },
        {
            "affiliations": [],
            "name": "Joe Comer"
        },
        {
            "affiliations": [],
            "name": "Sasha Strelnikoff"
        },
        {
            "affiliations": [],
            "name": "Ryan Mustari"
        },
        {
            "affiliations": [],
            "name": "Tsai-Ching Lu"
        },
        {
            "affiliations": [],
            "name": "Rajan Bhattacharyya"
        }
    ],
    "id": "SP:4deb5f9c27d35c75e9d41c67f8b7ce3a275a79e7",
    "references": [
        {
            "authors": [
                "S. Beckers",
                "F. Eberhardt",
                "J.Y. Halpern"
            ],
            "title": "Approximate causal abstractions",
            "venue": "Uncertainty in Artificial Intelligence, 606\u2013615. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "K. Chalupka",
                "T. Bischoff",
                "P. Perona",
                "F. Eberhardt"
            ],
            "title": "Unsupervised discovery of el nino using causal feature learning on microlevel climate data",
            "venue": "Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence, 72\u201381.",
            "year": 2016
        },
        {
            "authors": [
                "K. Chalupka",
                "P. Perona",
                "F. Eberhardt"
            ],
            "title": "Visual causal feature learning",
            "venue": "Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence, 181\u2013 190. AUAI Press.",
            "year": 2015
        },
        {
            "authors": [
                "R.T. Chen",
                "X. Li",
                "R.B. Grosse",
                "D.K. Duvenaud"
            ],
            "title": "Isolating sources of disentanglement in variational autoencoders",
            "venue": "Advances in neural information processing systems 31.",
            "year": 2018
        },
        {
            "authors": [
                "T. Di Liberto"
            ],
            "title": "The walker circulation: Ensos atmospheric buddy",
            "venue": "NOAA Climate. gov.",
            "year": 2014
        },
        {
            "authors": [
                "F. Eberhardt"
            ],
            "title": "Causal emergence: When distortions in a map obscure the territory",
            "venue": "https: //simons.berkeley.edu/talks/causalemergence-when-distortions-mapobscure-territory. Simons Institute for the",
            "year": 2022
        },
        {
            "authors": [
                "I. Guyon",
                "A. Statnikov",
                "B.B. Batu"
            ],
            "title": "Cause effect Pairs in machine learning",
            "venue": "Springer.",
            "year": 2019
        },
        {
            "authors": [
                "I. Higgins",
                "L. Matthey",
                "A. Pal",
                "C. Burgess",
                "X. Glorot",
                "M. Botvinick",
                "S. Mohamed",
                "A. Lerchner"
            ],
            "title": "betavae: Learning basic visual concepts with a constrained variational framework",
            "venue": "International conference on learning representations.",
            "year": 2017
        },
        {
            "authors": [
                "B. H\u00f6ltgen"
            ],
            "title": "Encoding causal macrovariables",
            "venue": "Workshop on Causal Inference and Machine Learning: Why now? (WHY), NeurIPS.",
            "year": 2021
        },
        {
            "authors": [
                "F. Locatello",
                "M. Tschannen",
                "S. Bauer",
                "G. R\u00e4tsch",
                "B. Sch\u00f6lkopf",
                "O. Bachem"
            ],
            "title": "Disentangling factors of variation using few labels",
            "venue": "arXiv preprint arXiv:1905.01258.",
            "year": 2019
        },
        {
            "authors": [
                "S.M. Lundberg",
                "Lee",
                "S.-I."
            ],
            "title": "A unified approach to interpreting model predictions",
            "venue": "Advances in neural information processing systems 30.",
            "year": 2017
        },
        {
            "authors": [
                "J. Mitrovic",
                "B. McWilliams",
                "J. Walker",
                "L. Buesing",
                "C. Blundell"
            ],
            "title": "Representation learning via invariant causal mechanisms",
            "venue": "arXiv preprint arXiv:2010.07922.",
            "year": 2020
        },
        {
            "authors": [
                "V. Pillai",
                "H. Pirsiavash"
            ],
            "title": "Explainable models with consistent interpretations",
            "venue": "UMBC Student Collection.",
            "year": 2021
        },
        {
            "authors": [
                "B. Sch\u00f6lkopf",
                "F. Locatello",
                "S. Bauer",
                "N.R. Ke",
                "N. Kalchbrenner",
                "A. Goyal",
                "Y. Bengio"
            ],
            "title": "Towards causal representation learning 2021",
            "venue": "arXiv preprint arXiv:2102.11107.",
            "year": 2021
        },
        {
            "authors": [
                "R.R. Selvaraju",
                "M. Cogswell",
                "A. Das",
                "R. Vedantam",
                "D. Parikh",
                "D. Batra"
            ],
            "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
            "venue": "Proceedings of the IEEE international conference on computer vision, 618\u2013626.",
            "year": 2017
        },
        {
            "authors": [
                "X. Shen",
                "F. Liu",
                "H. Dong",
                "Q. Lian",
                "Z. Chen",
                "T. Zhang"
            ],
            "title": "Weakly supervised disentangled generative causal representation learning",
            "venue": "Journal of Machine Learning Research 23:1\u201355.",
            "year": 2022
        },
        {
            "authors": [
                "N. Tishby",
                "F.C. Pereira",
                "W. Bialek"
            ],
            "title": "The information bottleneck method",
            "venue": "arXiv preprint physics/0004057.",
            "year": 2000
        },
        {
            "authors": [
                "F. Tr\u00e4uble",
                "E. Creager",
                "N. Kilbertus",
                "F. Locatello",
                "A. Dittadi",
                "A. Goyal",
                "B. Sch\u00f6lkopf",
                "S. Bauer"
            ],
            "title": "On disentangled representations learned from correlated data",
            "venue": "International Conference on Machine Learning, 10401\u2013 10412. PMLR.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "Introduction",
            "text": "Discovering causal abstractions from observations of complex systems has increasing importance for various highstakes operational tasks. Decision makers such as intelligence analysts, maintenance, logistics, and planning personnel are increasingly overwhelmed with the high volume of data they need to forage through and evaluate. Deriving recommendations by fusing multiple sources of visual data (e.g., satellite imagery, radar, grids or rasters representing terrain, elevation, or weather) is even more challenging. While some analysts are highly skilled in image processing and machine learning methods, many decision-makers are not, resulting in a need for interpretable and robust visual explanations for analyst recommendations. Motivated by this need, we propose a novel framework to abstract and display causal concepts from visual data, while allowing non-technical domain experts to provide visual annotations on aspects of the data they believe to be causal factors for their decisions.\nCausal macrovariables, first introduced in Chalupka, Perona, and Eberhardt (2015), are lower-dimensional representations of high-dimensional \u201cmicrovariables\u201d X and Y which preserve all relevant causal information. They attempt to answer which changes in Y are caused by which changes in X , \u2217This author is now at University of Connecticut. \u2020This author is now at iRhythm Technologies, Inc. Copyright (c) 2023, HRL Laboratories, LLC. All rights reserved\ngiven the observed variation in both. Causal macrovariables provide an approximate causal abstraction (Beckers, Eberhardt, and Halpern, 2020) of a cause-effect system X \u2192 Y , as opposed to other concept learning paradigms which focus on learning concepts within X with respect to a categorical classification. Take for example the El Nin\u0303o phenomenon, which describes a tongue of hot water that extends from the west coast of the Americas into the Pacific Ocean. In this way, the \u201cEl Nin\u0303o\u201d macrovariable supervenes upon the specific sea surface temperatures (SST) which constitute it. As demonstrated in Chalupka et al. (2016), high-dimensional wind speed (WS) (X) and SST (Y ) measurements can be used to discover macrovariables corresponding to westerly equatorial winds causing the El Nin\u0303o phenomenon (Figure 1). These macrovariables are contextual in that they are unique to the cause-effect system of interest; causal macrovariables capturing aspects of SST which cause California rainfall may have entirely different representations.\nAs pointed out in Scho\u0308lkopf et al. (2021), learning causal macrovariables requires some sort of supervision, and which variables can be extracted and their level of abstraction depends on which distribution shifts, explicit interventions, and other supervision signals are available. Unsupervised causal macrovariable discovery algorithms (Chalupka, Perona, and Eberhardt, 2015; Ho\u0308ltgen, 2021) are limited by their need for controllable interventions on the data generating process to distinguish correlation from causation. These interventions are not always possible, e.g., in the climate science example\nabove. A related field of research is representation learning, where the goal is to learn factors of variation within a single highdimensional variable X (e.g., (Chen et al., 2018; Tra\u0308uble et al., 2021; Locatello et al., 2019)), or with respect to a scalar value y(e.g., image classification in (Mitrovic et al., 2020)). These methods often utilize self-supervision in the form of strong assumptions about the data generating process (e.g., contrastive learning or concept independence). Our method refrains from making such strong assumptions and addresses the case where Y may also be high-dimensional.\nIn this paper, we choose to impose human supervision in the form of domain-expert-provided regions of interest (ROI) to enhance the practical use of causal macrovariable discovery methods. To our knowledge, we are the first to enable an end-user to provide this supervision in the visual domain, which (although not explored in this paper) paves the way for passive collection of supervision using eye-tracking devices. Our specific contributions are\n\u2022 Extensions of (Ho\u0308ltgen, 2021) which provide more stable and semantically meaningful results by leveraging nonlinear spatial relationships in image data.\n\u2022 A modified loss function which incorporates regularization terms to impose domain expert supervision in the form of cause or effect ROIs.\n\u2022 Quantitative evaluation of discovered causal macrovariables which more concisely and precisely capture known causal dynamics on both synthetic and real-world datasets.\nThe remainder of this paper is structured as follows. Problem Setup and Related Work provides a mathematical definition of causal macrovariables and summarizes related work in deriving them from two high-dimensional datasets. Supervised Causal Autoencoder outlines our novel \u201cSCAE\u201d architecture and its advantages. Experiments describes the synthetic and real-world datasets, metrics, and results. Finally, Discussion concludes with a summary of the paper and potential avenues for future work."
        },
        {
            "heading": "Problem Setup and Related Work",
            "text": "We assume a dataset of paired observations X,Y = {(Xi, Yi)}Ni=1 of two high-dimensional, continuous random microvariables X and Y where we believe that there are aspects of X which cause aspects of Y . The dimension of X and Y need not be the same, however, let us assume for ease of notation that they are both dimension M . Our goal is to find functions f and g yielding K-dimensional macrovariables X\u0304 = f(X) and Y\u0304 = g(Y ) which ideally capture all and only the causal information between X and Y , where K << M . As opposed to the prior work described below, our method also assumes the availability of annotated ROIs \u03b1(Xi) and/or \u03b1(Yi), which correspond to visual causes or effects respectively, for a subset of observations i \u2208 S.\nChalupka, Perona, and Eberhardt (2015) attempt to solve this problem using a method called Causal Feature Learning (CFL). The method finds equivalence classes of the conditional distribution P (Y |X) by grouping together Xi\u2019s that\nare observed to cause similar Yi\u2019s and Yi\u2019s which are observed to be caused by similar Xi\u2019s. This grouping produces an observation-based (correlational) partition of the space of X and Y such that f and g classify observed microvariables into scalar macrovariable categories. They also prove the Causal Coarsening Theorem, which dictates that if one can perform interventions the true causal macrovariables resulting from equivalence classes of P (Y |do(X = x)) can be found by further merging (or coarsening) the correlational macrovariables. This idea is renamed \u201crefinements\u201d in Mitrovic et al. (2020). The drawbacks of this method are that it does not allow for continuous-valued macrovariables (which provide a richer representation), does not provide a data-driven method for determining the number of causal macrovariables, and does not lend itself easily to ROI-based supervision.\nHo\u0308ltgen (2021) proposes a new characterization of causal macrovariables as information bottlenecks (Tishby, Pereira, and Bialek, 2000) between X and Y , using a novel neural network architecture called the Causal Autoencoder (CAE). Inspired by the variational autoencoder (VAE), the approach consists of training two connected stochastic neural networks (netX and netY ) simultaneously. Each network encoder provides a dimension M to dimension K compression of the input (X or Y ) with respect to the output (Y or X) such that X\u0304 = (x1, ..., xK) and Y\u0304 = (y1, ..., yK). Because this method overcomes the drawbacks of CFL, we explain it in more detail below and provide our extensions in Supervised Causal Autoencoder.\nWe compare our results to CFL and CAE due to the lack of supervised methods for causal macrovariable discovery. Since CFL and CAE are entirely unsupervised, SCAE does not claim to \u201coutperform\u201d them, rather, our comparisons provide empirical evidence that the proposed method can effectively incorporate image-based annotations from domain experts that are familiar with visual causes and effects present in the data.\nCausal Autoencoder The CAE architecture consists of two encoder-decoder networks, netX and netY , tied together by a linear function between their information bottlenecks. In the following, we describe netX , but netY has the same structure with input and output inverted. In a standard \u03b2-VAE framework (Higgins et al., 2017), the information bottleneck is meant to trade-off compression and reconstruction of the input X . In the CAE, the information bottleneck is instead computed between X (containing causes) and Y (containing effects). Assuming that causal macrovariables X\u0304 and Y\u0304 contain all causal connections between X and Y , they must also contain all mutual information shared by X and Y , i.e., I(X\u0304;Y ) = I(X;Y ) = I(Y\u0304 ;X) = I(Y ;X). Therefore, the optimal assignment of X\u0304 minimizes the information bottleneck functional L = I(X\u0304;X) \u2212 \u03b2 \u00b7 I(X\u0304;Y ) where \u03b2 governs the trade-off between compression and prediction.\nThe goal of netX is to learn three functions: the encoder f(X) = X\u0304 , the bottleneck-to-bottleneck relationship between macrovariables \u02c6\u0304Y = h(X\u0304), and the decoder Y\u0302 = j(X\u0304). The corresponding loss function for netX is,\nlossnetX = d1(Y, Y\u0302 ) + \u03b2DKL ( N (0, 1)|q(X\u0304|X) ) + (1)\n\u03b3d2(Y\u0304 , \u02c6\u0304Y )\nwhere d1 and d2 are appropriate distance metrics (in our case mean squared error), q(X\u0304|X) denotes the activation distribution of bottleneck neurons over input samples, and DKL denotes the Kullback-Leibler (KL) divergence. \u03b3 and \u03b2 are hyperparameters used to weight different terms of the loss function. Similarly, netY performs the same compression, reconstruction, and estimate of relation to X\u0304 , but with respect to Y . The total loss is then lossnetX + lossnetY ."
        },
        {
            "heading": "Supervised Causal Autoencoder",
            "text": "Due to the symmetry of the CAE (and mutual information in general), the causal direction between X and Y need not be known a priori. However, instead of post-processing macrovariables using transfer entropy, additive noise models, or other causal scoring methods (Guyon, Statnikov, and Batu, 2019) to determine causal direction, we restrict our application to the case where aspects of X cause aspects of Y , since we assume the causal direction would be known in most real-world decision-making scenarios. Given this restriction, we no longer require our macro-level functional causal model h to be constrained by a linear relationship. We relax this constraint by adding one extra neuron between each xk and yk, but keep the injective mapping between macrovariable dimensions in order to maintain strong and sparse correlations. We also convert the encoders and decoders to convolutional neural networks (CNNs) to better capture spatial image features. These changes enhance the stability of discovered causal macrovariables, as detailed in Results.\nSCAE builds upon the CAE by incorporating a regularization term which forces the encoders to pay attention to provided ROIs to derive visual cause and effect concepts within the X and Y images, respectively. The regularization provides a third trade-off in the loss function, beyond the standard compression and reconstruction for \u03b2-VAEs. As stated in Shen et al. (2022), this form of supervision has the\nadvantage of allowing for semi-supervision, as opposed to latent conditional models which require supervision to apply to every sample. The supervision term is implemented using a Grad-CAM heatmap (Selvaraju et al., 2017) to identify the regions that each xk or yk is focusing on within each (Xi or Yi) image. Other explainable artificial intelligence methods that produce heatmaps (e.g., SHAP features of Lundberg and Lee (2017)) could also be used. We compute the L1-norm between this heatmap and a binary image mask of the annotated ROI as follows,\n\u03c6(Yi) = |Ak(Yi)\u2212 \u03b1(Yi)|1, (2) where Ak(Yi) is the Grad-CAM heatmap for Yi with respect to the supervised concept yk, and \u03b1(Yi) is an image mask annotating the corresponding visual concept in that same image. The loss function for netY becomes,\nlossnetY = d1(X, X\u0302) + \u03b2DKL ( N (0, 1)|q(Y\u0304 |Y ) ) + (3)\n\u03b3d2(X\u0304, \u02c6\u0304X) +\n\u03bb |S| \u2211 i\u2208S \u03c6(Yi)\nwhere \u03bb is the weight of the supervised loss term. Due to the symmetry of the architecture, we can use this term to impose supervision on either netX or netY , or both. The idea is similar to the Grad-CAM-based regularization applied in Pillai and Pirsiavash (2021), where it is used for selfsupervision via data augmentation.\nTo train the SCAE-CNN architecture, we use multiple phases. In Phase 1, we train the network for n epochs with only the d1 and \u03c6 loss terms (setting \u03b2 and \u03b3 to zero). In Phase 2, we set \u03b2 to its optimal value for the next n epochs, while keeping \u03b3 at zero. Finally, in Phase 3 we set \u03b3 to its optimal value (as in the CAE) and train the network for those remaining terms. This allows the network to first pick up the supervision information, while minimizing training inconsistencies. As mentioned in Ho\u0308ltgen (2021), causal macrovariables cannot be unique because there are infinitely many transformations of derived macrovariables for which causal relationships hold; however, training with supervision in Phase 1 allows us to guide the network to find the macrovariable representation that best captures the ROIs specified by the domain expert. The network is less likely to get stuck in a local minimum when we start with strong guidance on the proper gradient direction, and this also enhances the stability of the training process.\nExperiments We evaluate our method with two experimental datasets: the synthetic dataset (with causal ground-truth) introduced in Ho\u0308ltgen (2021), and the climate dataset introduced in Chalupka et al. (2016). In both datasets, we empirically show that given enough supervision, our supervised framework is able to perform the aforementioned causal coarsening.\nDatasets and Metrics As is often necessary for evaluation of novel causal discovery methods, we simulate a synthetic dataset for which the causal ground truth is known. In\nthis synthetic dataset, X and Y are random variables in R64 which can be visualized as 8x8 pixel gray-scale images. The generative model for the data has two-dimensional macrovariables: X\u0304 = (x1, x2) and Y\u0304 = (y1, y2) which correspond to pixel value averages in the top/bottom halves of X and the right/left halves of Y , respectively. Scalar values per sample are generated as\nx1 := c1 + n X 1 , y1 := c 3 1 + n Y 1 , y2 := tanh(x2) + n Y 2\nwith unobserved confounder c1 and x2 \u223c U([\u22121, 1]) and nX1 , n Y 1 , n Y 2 \u223c U([\u22120.2, 0.2]) all uniformly distributed and mutually independent. Pixel values within each half of each image are generated by distributing the appropriate scalar value shift (plus noise) across all but one pixel, then shifting the last pixel by the remaining amount. As shown in Figure 3, x1 and y1 have a common cause c1 (i.e., they are correlated but x1 does not directly cause y1), whereas x2 is a direct cause of y2. Supervision is imposed by highlighting the left half of Y images corresponding to y2 (yellow, Figure 3) indicating that this is an effect of interest.\nThe \u201cEl Nin\u0303o Dataset\u201d is the real-world climate science dataset from Introduction. This dataset assumes that macrovariables within zonal WS maps (X) cause macrovariables within SST maps (Y ). The goal of causal feature learning in this domain is to discover the El Nin\u0303o phenomenon from 35 years of climate data from a region in the Equatorial Pacific ocean. The dataset enables us to compare our results with the experimental results of both CFL and CAE.\nAlthough we cannot obtain ground-truth causal macrovariables for this dataset, Chalupka et al. (2016) introduce a precision metric to measure the quality of the resulting macrovariables based on known qualities of El Nin\u0303o. The National Oceanic and Atmospheric Administration (NOAA) defines El Nin\u0303o as a positive three-month running mean SST anomaly of more than 0.5\u25e6C in the Nin\u0303o 3.4 region. Therefore, in CFL precision is computed per macrovariable category as the percentage of Y observations with a spatial average within the Nin\u0303o 3.4 region that is > 0.5 warmer than the dataset mean. In SCAE, a macrovariable Y\u0304 is a continuous-valued K-dimensional vector which can represent a mixture of multiple \u201ceffect\u201d concepts. We consider an image Yi to \u201ccontain\u201d\nan effect concept yk if it produces an activation value that lies within the top 25th percentile of yk\u2019s activation values (i.e., {Yi | gk(Yi) > P75(gk(Y))}). Precision is then computed per concept over the set of images containing that concept.\nPrecision is computed similarly for the synthetic dataset; for each yk we calculate the percentage of Yis containing that yk which have a spatial average within the left half of the image that is greater than the average of the total dataset.\nQualitatively, climate scientists have noted that a largerthan-average westerly wind component in the west-equatorial region is a feature associated with the causes of El Nin\u0303o (Di Liberto, 2014). This suggests that there should be a causal concept representing WS maps with this characteristic mapped to the effect concept representing El Nin\u0303o. In this dataset, we provide the Nin\u0303o 3.4 region as an image mask to guide our construction of causal macrovariables (highlighted in yellow, Figure 1).\nImplementation details For the synthetic dataset, we use a 2-layer CNN ([input, output] channels: [1,8], [8,16]) and a 3x3 kernel. For the El Nin\u0303o dataset we use a 3-layer CNN ([input, output] channels: [1,32], [32,64], [64,64]) and a 3x3 kernel. As with most neural architectures, the number of layers has a strong impact on performance; too many layers will overfit to the training data and too few will underfit.\nFollowing the CAE paper, we conducted a grid-search for \u03b3, \u03b2 (\u03b3 = 1, 0.1, 0.01, \u03b2 = 1, 0.1, 0.01) in order to to find the best result for both our architecture and the CAE. The total number of bottleneck neurons was set to the same as in CAE: 4 for the synthetic data and 16 for the El Nin\u0303o data (this parameter was not searched). In the synthetic dataset, we trained 1000 epochs (100, 100, 800 for Phases 1,2,3) with hyperparameters \u03b3 = 0.1, \u03b2 = 0.01, and \u03bb = 10\u22126. For the El Nin\u0303o dataset, we train a total of 2000 epochs (200, 200, 1600 for Phases 1,2,3) using hyperparameters \u03b3 = 1, \u03b2 = 0.01, and \u03bb = 10\u22126. As in the CAE, we consider bottleneck neurons as \u201cinformative\u201d (representing a causal concept xk or yk) if the standard deviation of their activation distribution across samples is > 0.95. We additionally filter out bottleneck neurons which are only informative in either netX or netY , since these represent information which is needed to reconstruct X\u0302 or Y\u0302 , but is not relevant to the causal connection between the two datasets. This is akin to the idea of preserving the \u201ccontent\u201d and discarding the \u201cstyle\u201d in the representation learning literature. Because we are asking the model to reconstruct Y from information in X (and vice versa), it is unsurprising that the optimal \u03b2 values for both datasets are significantly less than 1, implying that we are favoring reconstruction over constraining the capacity of the bottleneck in order to achieve a low overall loss.\nResults: Synthetic Dataset The relaxation of linear constraints on the X\u0304 to Y\u0304 mapping, and addition of CNN encoders and decoders contribute to the stability of the results across random initializations. To quantify this, we ran 10 random network initializations without supervision on the synthetic dataset and computed the mean and standard deviation of the number of informative conceptsK (Table 1). Since\nwe are not imposing supervision, we expect the system to discover two correlation-based concepts (K = 2). Although the results in the table are not statistically significant, we find that the mean is closest to 2 and has the least variability when using the CNN and nonlinear adjustments.\nSubject matter expertise, such as highlighted image regions, may be expensive or otherwise difficult to obtain. Figure 4 shows the Grad-CAM result from varying the percentage of annotated image masks per batch. The visual concepts represented by X\u0304 and Y\u0304 seem to vary smoothly, but K drops from two to one (the ground truth value) when 50% of images (5000 samples) are supervised. Using annotations on 100% of Y images, SCAE correctly detects one causal macrovariable relationship x2 \u2192 y2 in the synthetic dataset. Although we have only supervised netY to focus on y2, we observe that netx has also been driven to focus on x2 due to the inter-connectivity of the overall loss function. Figure 5 further demonstrates further that the concepts corresponding to neuron 3 of both bottlenecks, correctly capture x2 and y2 from the ground truth model. Concepts x1 and y1 are still captured by neuron 0 of netX and neuron 2 of netY respectively, however, their functional relationship (correlation) has been broken.\nVisualizations of discovered concepts for the synthetic dataset are omitted to save space, since they look very similar to the Grad-CAM heatmaps shown in Figure 4. Precision metrics show that causal concepts are clearly differentiated by whether the spatial average of values within the y2 region fall above or below the dataset average. CAE discovers macrovariables which capture both the correlative relationship between x1 and y1 (precision= 0.5, since these are random with respect to y2) and the causal relationship between x2 and y2\n(precision= 1.0). SCAE coarsens these such that only the causal relationship between x2 and y2 (precision= 1.0) is preserved.\nResults: El Nin\u0303o Dataset Using Nin\u0303o 3.4 annotations on 100% of Y images, SCAE detects two causal concepts for the El Nin\u0303o dataset. We do not know the correct K in this case, but attempt to judge the quality of our results based on the characteristics described in Datasets and Metrics as\ncompared to the four macrovariables discovered by CAE and CFL. Fig. 6 shows the final Grad-CAM heatmap for neuron 0 (x1, y1) and neuron 4 (x2, y2). Macrovariables from netX seemingly capture deviations in wind speeds across the image frame. By taking the difference in Grad-CAM heatmaps (bottom row), we can see that Neuron 4 focuses more on westerly equatorial winds. This agrees with the results in Figure 7, i.e., the more extreme \u201cWeqt\u201d and \u201cEl Nin\u0303o\u201d concepts correspond to x2 and y2. Discovered concepts from netY both strongly focus on temperature deviations within the Nin\u0303o 3.4 region, with negligible differences.\nWe display discovered visual causal concepts by taking the top and bottom 25th percentile of their activation values, followed by the mean of the images that produced them, and then subtracting off the mean image for the whole dataset. Figure 7 shows these mean-subtracted average images with respect to each concept for CAE (Top) and SCAE (Bottom). Inset text in quotes provides closest mapping to the semantic labels of macrovariables discovered by CFL in Chalupka et al. (2016), where the number of macrovariables |X\u0304| = |Y\u0304 | = 4 must be specified a priori. According to the precision metric, SCAE\u2019s y2 is able to detect El Nin\u0303o with 100% precision, as opposed to the maximum 85% precision from CAE\u2019s y4. SCAE does this while preserving the known causal relationship between Westerly Equatorial Winds (\u201cWEqt\u201d) and the occurrence of the El Nin\u0303o phenomenon. However, the more remarkable result is that SCAE is able to achieve this improved performance using high and low activation values for only 2 cause-effect concepts, as opposed to the four visually redundant macrovariables from CFL or CAE. We attribute this to the use of continuous-valued macrovariables and supervision for causal coarsening, respectively."
        },
        {
            "heading": "Discussion",
            "text": "The existence of causal macrovariables in any microvariable system is not guaranteed. However, we have shown that in cases where macro-level descriptions of micro-level dynamics are assumed to exist, it is possible to utilize some amount of human supervision in order to enhance the representation of discovered macrovariables such that they more concisely represent the known causal dynamics of the system. This work attempts to push the state-of-the-art in disentanglement\nand causal representation learning by considering a highdimensional Y where conventional class labels are undefined. This limits the ability to produce data augmentations which can be used for self-supervised learning, because we do not know (without human supervision) what types of augmentations preserve relevant effects in the Y images.\nOur method of supervision is limited in its ability to specify causal factors which cannot be represented visually, as well as the need to completely annotate any image in S, i.e., the user cannot annotate only one of multiple causal factors present in the image. In addition, if there are multiple causal factors within one image, they must be annotated as separate image masks if we seek to disentangle them. Despite these limitations, we show that in a synthetically generated dataset (where the causal ground-truth is known), our methods are able to distinguish between correlation and direct causation, a differentiation which is otherwise unobtainable from purely observational data. In addition, we are able to capture the macrovariables corresponding to El Nin\u0303o and La Nin\u0303a with higher precision and more concisely than existing methods.\nFurther investigation into the amount of supervision required to obtain accurate causal macrovariables in various settings is surely needed. Although annotating thousands of input or output images in a dataset may seem tedious, this number should stay relatively constant as the size of the dataset increases, assuming stationarity of causal concepts. Our trained architecture can be used for automated prediction of effect macrovariables (e.g., El Nin\u0303o) from any new input Xi (e.g., WS map), which saves time and effort in applications where subject matter experts currently analyze high-dimensional datasets manually. Towards this goal of enhancing practicality and operationalizing the system, we are also pursuing the use of eye-tracking data from human subject experiments as means of passive supervision.\nAcknowledgement This material is based upon work supported by the Office of Naval Research (ONR) under Contract No. N0001419C2024. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Office of Naval Research (ONR)."
        }
    ],
    "title": "Semi-supervised Learning of Visual Causal Macrovariables",
    "year": 2023
}