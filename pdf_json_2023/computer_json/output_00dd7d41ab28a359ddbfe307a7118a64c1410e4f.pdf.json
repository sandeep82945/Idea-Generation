{
    "abstractText": "Department of Electronic Engineering and Computer Science, School of Science and Technology, Hong Kong Metropolitan University, Ho Man Tin, Hong Kong SAR, China International Center for AI and Cyber Security Research and Innovations, Department of Computer Science and Information Engineering, Asia University, Taichung 413, Taiwan Symbiosis Centre for Information Technology (SCIT), Symbiosis International University, Pune, India Lebanese American University, Beirut, 1102, Lebanon Center for Interdisciplinary Research at University of Petroleum and Energy Studies (UPES), Dehradun, Uttarakhand, India Department of Computer Science, Dar Alhekma University, Jeddah, Saudi Arabia Department of Computer Science and Engineering, School of Technology, Pandit Deendayal Energy University, Gandhinagar, India Instituto de Telecomunica\u00e7\u00f5es, Aveiro, Portugal Asia University, Taichung 41354, Taiwan School of Information Technology, Skyline University College, P.O. Box 1797, UAE Al-Balqa Applied University, Salt, Jordan Department of Information and Communication Engineering, Yeungnam University, Gyeongsan, Republic of Korea",
    "authors": [
        {
            "affiliations": [],
            "name": "Kwok Tai Chui"
        },
        {
            "affiliations": [],
            "name": "Brij B. Gupta"
        },
        {
            "affiliations": [],
            "name": "Rutvij H. Jhaveri"
        },
        {
            "affiliations": [],
            "name": "Hao Ran Chi"
        },
        {
            "affiliations": [],
            "name": "Varsha Arya"
        },
        {
            "affiliations": [],
            "name": "Ammar Almomani"
        },
        {
            "affiliations": [],
            "name": "Ali Nauman"
        }
    ],
    "id": "SP:852d359ed856bea7940ed0216795c05b5a44c211",
    "references": [
        {
            "authors": [
                "K. Ferlay"
            ],
            "title": "Global Cancer Observatory: Cancer Today, International Agency for Research on",
            "year": 2020
        },
        {
            "authors": [
                "K. Shankar",
                "E. Perumal",
                "M. Elhoseny",
                "F. Taher",
                "B.B. Gupta",
                "A.A.A. El-Latif"
            ],
            "title": "Synergic deep learning for smart Health diagnosis of COVID-19 for connected living and smart cities",
            "venue": "ACM Transactions on Internet Technology, vol. 22, no. 3, pp. 1\u201314, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "B.N. Zamora-Mendoza",
                "H. Sandoval-Flores",
                "M. Rod\u0155\u0131guez- Aguilar"
            ],
            "title": "Determination of global chemical patterns in exhaled breath for the discrimination of lung damage in postCOVID patients using olfactory technology",
            "venue": "Talanta, vol. 256, Article ID 124299, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "N. Wijbenga",
                "R.A. Hoek",
                "B.J. Mathot"
            ],
            "title": "Diagnostic performance of electronic nose technology in chronic lung allograft dysfunction",
            "venue": "Te Journal of Heart and Lung Transplantation, vol. 42, no. 2, pp. 236\u2013245, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "S.J. Pan",
                "Q. Yang"
            ],
            "title": "A survey on transfer learning",
            "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 22, no. 10, pp. 1345\u20131359, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "K. Weiss",
                "T.M. Khoshgoftaar",
                "D. Wang"
            ],
            "title": "A survey of transfer learning",
            "venue": "J. Big Data, vol. 3, no. 1, pp. 9\u201340, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "F. Zhuang",
                "Z. Qi",
                "K. Duan"
            ],
            "title": "A comprehensive survey on transfer learning",
            "venue": "Proceedings of the IEEE, vol. 109, no. 1, pp. 43\u201376, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A. Abdellatif",
                "H. Abdellatef",
                "J. Kanesan",
                "C.O. Chow",
                "J.H. Chuah",
                "H.M. Gheni"
            ],
            "title": "An efective heart disease detection and severity level classifcation model using machine learning and hyperparameter optimization methods",
            "venue": "IEEE Access, vol. 10, pp. 79974\u201379985, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "G. Suganya",
                "M. Premalatha",
                "S. Geetha",
                "G.J. Chowdary",
                "S. Kadry"
            ],
            "title": "Detection of COVID-19 cases from chest X-rays 12 International Journal of Intelligent Systems using deep learning feature extractor and multilevel voting classifer",
            "venue": "International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, vol. 30, no. 05, pp. 773\u2013793, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J.W. Son",
                "J.Y. Hong",
                "Y. Kim"
            ],
            "title": "How many private data are needed for deep learning in lung nodule detection on CT scans? A retrospective multicenter study",
            "venue": "Cancers, vol. 14, no. 13, pp. 3174\u20133219, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "F. Shi",
                "B. Chen",
                "Q. Cao"
            ],
            "title": "Semi-supervised deep transfer learning for benign-malignant diagnosis of pulmonary nodules in chest CT images",
            "venue": "IEEE Transactions on Medical Imaging, vol. 41, no. 4, pp. 771\u2013781, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "R.V.M. da N\u00f3brega",
                "P.P. Rebou\u00e7as Filho",
                "M.B. Rodrigues",
                "S.P.P. da Silva",
                "C.M.J.M. Dourado J\u00fanior",
                "V.H.C. de Albuquerque"
            ],
            "title": "Lung nodule malignancy classifcation in chest computed tomography images using transfer learning and convolutional neural networks",
            "venue": "Neural Computing & Applications, vol. 32, no. 15, pp. 11065\u201311082, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "P. Yadlapalli",
                "D. Bhavana",
                "S. Gunnam"
            ],
            "title": "Intelligent classifcation of lung malignancies using deep learning techniques",
            "venue": "International Journal of Intelligent Computing and Cybernetics, vol. 15, no. 3, pp. 345\u2013362, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "M. Humayun",
                "R. Sujatha",
                "S.N. Almuayqil",
                "N.Z. Jhanjhi"
            ],
            "title": "A transfer learning approach with a convolutional neural network for the classifcation of lung carcinoma",
            "venue": "Healthcare, vol. 10, no. 6, pp. 1058\u20131115, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Sasaki",
                "Y. Kondo",
                "T. Aoki",
                "N. Koizumi",
                "T. Ozaki",
                "H. Seki"
            ],
            "title": "Use of deep learning to predict postoperative recurrence of lung adenocarcinoma from preoperative CT",
            "venue": "International Journal of Computer Assisted Radiology and Surgery, vol. 17, no. 9, pp. 1651\u20131661, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "R. McConnell",
                "R. Kwok",
                "J.C. Curlander",
                "W. Kober",
                "S.S. Pang"
            ],
            "title": "Psi-s correlation and dynamic time warping: two methods for tracking ice foes in SAR images",
            "venue": "IEEE Transactions on Geoscience and Remote Sensing, vol. 29, no. 6, pp. 1004\u20131012, 1991.",
            "year": 1991
        },
        {
            "authors": [
                "A. Meiseles",
                "L. Rokach"
            ],
            "title": "Source model selection for deep learning in the time series domain",
            "venue": "IEEE Access, vol. 8, pp. 6190\u20136200, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "W. Zhang",
                "L. Deng",
                "L. Zhang",
                "D. Wu"
            ],
            "title": "A survey on negative transfer",
            "venue": "IEEE/CAA Journal of Automatica Sinica, vol. 10, no. 2, pp. 305\u2013329, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "J. Gui",
                "Z. Sun",
                "Y. Wen",
                "D. Tao",
                "J. Ye"
            ],
            "title": "A review on generative adversarial networks: algorithms, theory, and applications",
            "venue": "IEEE Transactions on Knowledge and Data Engineering, p. 1, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "A. Odena",
                "C. Olah",
                "J. Shlens"
            ],
            "title": "Conditional image synthesis with auxiliary classifer GANs",
            "venue": "Proceedings of the 34th International Conference on Machine Learning, pp. 2642\u20132651, JMLR, Sydney, Australia, August 2017.",
            "year": 2017
        },
        {
            "authors": [
                "M. Mirza",
                "S. Osindero"
            ],
            "title": "Conditional generative adversarial nets",
            "venue": "2014, https://arxiv.org/abs/1411.1784.",
            "year": 2014
        },
        {
            "authors": [
                "H.J.W.L. Aerts",
                "E.R. Velazquez",
                "R.T.H. Leijenaar"
            ],
            "title": "Decoding tumour phenotype by noninvasive imaging using a quantitative radiomics approach",
            "venue": "Nature Communications, vol. 5, no. 1, pp. 4006\u20134009, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "H.J. Aerts",
                "E.R. Velazquez",
                "R.T. Leijenaar"
            ],
            "title": "Corrigendum: decoding tumour phenotype by noninvasive imaging using a quantitative radiomics approach",
            "venue": "Nature Communications, vol. 5, pp. 1\u20139, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "S.G. Armato",
                "L. Hadjiiski",
                "G.D. Tourassi"
            ],
            "title": "LUNGx Challenge for computerized lung nodule classifcation: refections and lessons learned",
            "venue": "Journal of Medical Imaging, vol. 2, no. 2, pp. 020103-020104, 2015.",
            "year": 2010
        },
        {
            "authors": [
                "O. Grove",
                "A.E. Berglund",
                "M.B. Schabath"
            ],
            "title": "Quantitative computed tomographic descriptors associate tumor shape complexity and intratumor heterogeneity with prognosis in lung adenocarcinoma",
            "venue": "PLoS One, vol. 10, no. 3, pp. 01182611\u2013e118314, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "J. Yang",
                "H. Veeraraghavan",
                "S.G. Armato"
            ],
            "title": "Autosegmentation for thoracic radiation treatment planning: a grand challenge at AAPM 2017",
            "venue": "Medical Physics, vol. 45, no. 10, pp. 4568\u20134581, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "A. Krizhevsky",
                "G. Hinton"
            ],
            "title": "Learning Multiple Layers of Features from Tiny Images",
            "year": 2019
        },
        {
            "authors": [
                "J. Deng",
                "W. Dong",
                "R. Socher",
                "L.J. Li",
                "K. Li",
                "L. Fei-Fei"
            ],
            "title": "Imagenet: a large-scale hierarchical image database",
            "venue": "Proceedings of the 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248\u2013255, IEEE, Miami, FL, USA, June 2009.",
            "year": 2009
        },
        {
            "authors": [
                "T.Y. Lin",
                "M. Maire",
                "S. Belongie"
            ],
            "title": "Microsoft coco: common objects in context",
            "venue": "Proceedings of the European Conference on Computer Vision, pp. 740\u2013755, Springer, Zurich, Switzerland, September 2014.",
            "year": 2014
        },
        {
            "authors": [
                "K.A. Kurdziel",
                "J.H. Shih",
                "A.B. Apolo"
            ],
            "title": "Te kinetics and reproducibility of 18F-sodium fuoride for oncology using current PETcamera technology",
            "venue": "Journal of Nuclear Medicine, vol. 53, no. 8, pp. 1175\u20131184, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "X. Li",
                "R.G. Abramson",
                "L.R. Arlinghaus"
            ],
            "title": "Multiparametric magnetic resonance imaging for predicting pathological response after the frst cycle of neoadjuvant chemotherapy in breast cancer",
            "venue": "Investigative Radiology, vol. 50, no. 4, pp. 195\u2013204, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "K.T. Chui",
                "B.B. Gupta",
                "H.R. Chi"
            ],
            "title": "Transfer learningbasedmulti-scale denoising convolutional neural network for prostate cancer detection",
            "venue": "Cancers, vol. 14, no. 15, p. 3687, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "D. Azar",
                "R. Moussa",
                "G. Jreij"
            ],
            "title": "A comparative study of nine machine learning techniques used for the prediction of diseases",
            "venue": "International Journal of Artifcial Intelligence, vol. 16, no. 2, pp. 25\u201340, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "A. Gaurav",
                "B.B. Gupta",
                "P.K. Panigrahi"
            ],
            "title": "A comprehensive survey on machine learning approaches for malware detection in IoT-based enterprise information system",
            "venue": "Enterprise Information Systems, pp. 1\u201325, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "M. Kumar",
                "S. Singhal",
                "S. Shekhar",
                "B. Sharma",
                "G. Srivastava"
            ],
            "title": "Optimized stacking ensemble learning model for breast cancer detection and classifcation using machine learning",
            "venue": "Sustainability, vol. 14, no. 21, Article ID 13998, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J. Wu",
                "S. Guo",
                "H. Huang",
                "W. Liu",
                "Y. Xiang"
            ],
            "title": "Information and communications technologies for sustainable development goals: state-of-the-art, needs and perspectives",
            "venue": "IEEE Communications Surveys & Tutorials, vol. 20, no. 3, pp. 2389\u20132406, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "J. Wu",
                "S. Guo",
                "J. Li",
                "D. Zeng"
            ],
            "title": "Big data meet green challenges: big data toward green applications",
            "venue": "IEEE Systems Journal, vol. 10, no. 3, pp. 888\u2013900, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "R. Atat",
                "L. Liu",
                "J. Wu",
                "G. Li",
                "C. Ye",
                "Y. Yang"
            ],
            "title": "Big data meet cyber-physical systems: a panoramic survey",
            "venue": "IEEE Access, vol. 6, pp. 73603\u201373636, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "A. Almomani",
                "M. Alauthman",
                "M.T. Shatnawi"
            ],
            "title": "BB Phishing website detection with semantic features based on machine learning classifers: a comparative study",
            "venue": "International Journal on Semantic Web and Information Systems, vol. 18, no. 1, pp. 1\u201324, 2022. International Journal of Intelligent Systems 13",
            "year": 2022
        },
        {
            "authors": [
                "I. Cviti\u0107",
                "D. Perakovi\u0107",
                "M. Peri\u0161a",
                "B. Gupta"
            ],
            "title": "Ensemble machine learning approach for classifcation of IoTdevices in smart home",
            "venue": "International Journal of Machine Learning and Cybernetics, vol. 12, no. 11, pp. 3179\u20133202, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "F. Gerges",
                "F. Shih",
                "D. Azar"
            ],
            "title": "Automated diagnosis of acne and rosacea using convolution neural networks",
            "venue": "Proceedings of the 2021 4th International Conference on Artifcial Intelligence and Pattern Recognition, pp. 607\u2013613, Xiamen, China, September 2021.",
            "year": 2021
        },
        {
            "authors": [
                "H. Guan",
                "M. Liu"
            ],
            "title": "Domain adaptation for medical image analysis: a survey",
            "venue": "IEEE Transactions on Biomedical Engineering, vol. 69, no. 3, pp. 1173\u20131185, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "S. Wang",
                "L. Dong",
                "X. Wang",
                "X. Wang"
            ],
            "title": "Classifcation of pathological types of lung cancer from CT images by deep residual neural networks with transfer learning strategy",
            "venue": "Open Medicine, vol. 15, no. 1, pp. 190\u2013197, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "H. Tan",
                "J.H.T. Bates",
                "C. Matthew Kinsey"
            ],
            "title": "Discriminating TB lung nodules from early lung cancers using deep learning",
            "venue": "BMC Medical Informatics and Decision Making, vol. 22, no. 1, pp. 161\u2013167, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Z.H. Zhan",
                "L. Shi",
                "K.C. Tan",
                "J. Zhang"
            ],
            "title": "A survey on evolutionary computation for complex continuous optimization",
            "venue": "Artifcial Intelligence Review, vol. 55, no. 1, pp. 59\u2013 110, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "R. Abu Khurma",
                "I. Aljarah",
                "A. Sharieh",
                "M. Abd Elaziz",
                "R. Dama\u0161evi\u010dius"
            ],
            "title": "Krilavi\u010dius, \u201cA review of the modifcation strategies of the nature inspired algorithms for feature selection problem,\u201dMathematics",
            "venue": "vol. 10,",
            "year": 2022
        },
        {
            "authors": [
                "J. Manhas",
                "R.K. Gupta",
                "P.P. Roy"
            ],
            "title": "A review on automated cancer detection in medical images using machine learning and deep learning based computational techniques: challenges and opportunities",
            "venue": "Archives of Computational Methods in Engineering, vol. 29, no. 5, pp. 2893\u20132933, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "V.R. Gottumukkala",
                "N. Kumaran",
                "V.C. Sekhar"
            ],
            "title": "BLSNet: skin lesion detection and classifcation using broad learning system with incremental learning algorithm",
            "venue": "Expert Systems, vol. 39, pp. 1\u201316, 2022. 14 International Journal of Intelligent Systems",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "Research Article",
            "text": ""
        },
        {
            "heading": "Multiround Transfer Learning and Modified Generative",
            "text": ""
        },
        {
            "heading": "Adversarial Network for Lung Cancer Detection",
            "text": "Kwok Tai Chui ,1 Brij B. Gupta ,2,3,4,5,6 Rutvij H. Jhaveri ,7 Hao Ran Chi,8 Varsha Arya,4,9 Ammar Almomani,10,11 and Ali Nauman12\n1Department of Electronic Engineering and Computer Science, School of Science and Technology, Hong Kong Metropolitan University, Ho Man Tin, Hong Kong SAR, China 2International Center for AI and Cyber Security Research and Innovations, Department of Computer Science and Information Engineering, Asia University, Taichung 413, Taiwan 3Symbiosis Centre for Information Technology (SCIT), Symbiosis International University, Pune, India 4Lebanese American University, Beirut, 1102, Lebanon 5Center for Interdisciplinary Research at University of Petroleum and Energy Studies (UPES), Dehradun, Uttarakhand, India 6Department of Computer Science, Dar Alhekma University, Jeddah, Saudi Arabia 7Department of Computer Science and Engineering, School of Technology, Pandit Deendayal Energy University, Gandhinagar, India 8Instituto de Telecomunica\u00e7\u00f5es, Aveiro, Portugal 9Asia University, Taichung 41354, Taiwan 10School of Information Technology, Skyline University College, P.O. Box 1797, UAE 11Al-Balqa Applied University, Salt, Jordan 12Department of Information and Communication Engineering, Yeungnam University, Gyeongsan, Republic of Korea\nCorrespondence should be addressed to Kwok Tai Chui; jktchui@hkmu.edu.hk and Brij B. Gupta; gupta.brij@gmail.com\nReceived 20 October 2022; Revised 31 January 2023; Accepted 14 February 2023; Published 3 March 2023\nAcademic Editor: Lianyong Qi\nCopyright \u00a9 2023Kwok Tai Chui et al.Tis is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.\nLung cancer has been the leading cause of cancer death for many decades. With the advent of artifcial intelligence, various machine learning models have been proposed for lung cancer detection (LCD). Typically, challenges in building an accurate LCD model are the small-scale datasets, the poor generalizability to detect unseen data, and the selection of useful source domains and prioritization of multiple source domains for transfer learning. In this paper, a multiround transfer learning and modifed generative adversarial network (MTL-MGAN) algorithm is proposed for LCD. Te MTL transfers the knowledge between the prioritized source domains and target domain to get rid of exhaust search of datasets prioritization among multiple datasets, maximizing the transferability with a multiround transfer learning process, and avoiding negative transfer via customization of loss functions in the aspects of domain, instance, and feature. In regard to the MGAN, it not only generates additional training data but also creates intermediate domains to bridge the gap between the source domains and target domains. 10 benchmark datasets are chosen for the performance evaluation and analysis of the MTL-MGAN. Te proposed algorithm has signifcantly improved the accuracy compared with related works. To examine the contributions of the individual components of the MTLMGAN, ablation studies are conducted to confrm the efectiveness of the prioritization algorithm, the MTL, the negative transfer avoidance via loss functions, and the MGAN. Te research implications are to confrm the feasibility of multiround transfer learning to enhance the optimal solution of the target model and to provide a generic approach to bridge the gap between the source domain and target domain using MGAN.\nHindawi International Journal of Intelligent Systems Volume 2023, Article ID 6376275, 14 pages https://doi.org/10.1155/2023/6376275"
        },
        {
            "heading": "1. Introduction",
            "text": "Cancer is the second leading cause of global death, according to the World Health Organization [1]. Among all types of cancers, lung cancer is ranked frst that has caused 1.8 million deaths in each year. Lung cancer detection (LCD) in the early stage is important for medical staf to tailor-make the treatment plan and perform the prognostic estimation. LCD using artifcial intelligence receives increasing attention in both academia and practice in view of the inadequacies of medical staf [2] and the heavy workload [3]. Reducing the time spent on medical diagnosis provides more time to medical doctors to concentrate on professional surgery and consultation and thus leveraging the healthcare quality. In this paper, we consider the traditional lung cancer screening via biomedical imaging, instead of an emerging approach using breath by the electronic nose [4, 5].\nTe traditional machine learning model is trained with a dataset that often reaches a bottleneck in achieving excellent model performance (e.g., in terms of sensitivity, specifcity, and accuracy) to fulfl the mission-critical medical diagnosis. In addition, large-scale datasets may not be available for training an accurate deep learning-based model for all applications. Tese drive the emerging research trend in applying transfer learning, that performs knowledge transfer from the source domain to the target domain. In literature, it is well demonstrated the superiority and applicability of transfer learning in many research applications [6, 8]. Attention is drawn to a more general scenario, where the source domain and target domain are diferent but related (less difcult) or diferent and unrelated (more challenging). Te issue of the negative transfer becomes more severe with the increase of dissimilarities between the source domain and target domain because there are more unrelated samples from the source domain [8]. Te loss functions can be formulated to reduce the impact of negative transfer.\nTe rest of the paper is organized as follows. Section 1 is divided into three subsections to present a summary of the related works, a discussion of the research limitations of the related works, and the major research contributions of our work. Section 2 presents the design and formulations of the proposed algorithm for LCD. Section 3 summarizes the details of the 10 benchmark datasets and presents the performance evaluation and comparison. To investigate the contributions of the individual components of the proposed algorithm, ablation studies are conducted in Section 4. At last, in Section 5, a conclusion is drawn with future research directions.\n1.1. Related Works. Although existing works [9\u201316] formulated the transfer learning problems with a single source domain and single target domain, the discussion has merit as these works fell into the same research area, i.e., transfer learning for LCD. In the following, two common types of formulations will be discussed: (i) transfer learning between the similar source domain and target domain [9\u201312] and (ii) transfer learning between the distant source domain and target domain [13\u201316].\nTe discussion begins with the transfer learning problem using a similar source domain and target domain. In [9], a hybrid residual and deep neural networks was proposed for the transfer learning from Luna16 to a small-scale dataset (125 chest computed tomography (CT) scans) collected by researchers in Shandong Provincial Hospital.Te ablation study showed that the transfer learning strategy enhanced the accuracy of the LCDmodel from 79.5% to 85.7%. ImageNet was served as the source model in the transfer learning strategy to fne-tune the target model [10]. VGG16 and deep neural network were used to build the LCD model, which was evaluated using two benchmark datasets. Transfer learning enhanced the accuracy of the model from 87.5% to 90.8%. To transfer the knowledge from LUNA16 to the target domain of the Gangneung Asan Hospital for LCD, a YOLOX algorithm was used [11]. Results showed a slight enhancement of the model\u2019s accuracy from 89.7% to 90.9%. Some scenarios also suggested that improper settings in the fne-tuning of the target model may lead to deterioration on the model performance, which is a well-known issue of negative transfer. In [12], a nodule identifcation convolutional neural network was pretrained that would transfer knowledge to the target model (using data collected from some hospitals). Semisupervised deep transfer learning was designed and implemented. Results showed that the sensitivity, specifcity, and accuracy were improved from 90.2% to 92.2%, 66.3% to 78.6%, and 83.4% to 88.3%, respectively.\nOn the other hand, the transfer learning problems are formulated with distant sources and target domains. Te work [13] conducted an exploratory analysis on 11 common feature extractors for the source domain (ImageNet), including NASNetLarge, NASNetMobile, DenseNet201, DenseNet169, InceptionResNetV2, ResNet50, InceptionV3, Xception, MobileNet, VGG19, and VGG16. Te knowledge was transferred to build various classifers, such as random forest, K-nearest neighbors, support vector machine, multilayer perceptron, and Na\u0308\u0131ve Bayes. Results revealed that ResNet50 with support vector machine achieved the best performance with sensitivity and accuracy of 85.4% and 88.4%, respectively. Te work also demonstrated the efectiveness of the pretrained model using ImageNet to perform transfer learning on the target domain of chest CT [14]. Four common architectures, namely, DenseNet169, MobileNet, VGG19, and VGG16 were used to build the LCDmodel.Te performance of the model was the best with VGG 16, yielding an accuracy of 91.3%. A recent work [15] has reported a difculty in the transfer learning strategy without model overftting. Te model was with 98.8% and 83.4% of training accuracy and testing accuracy, respectively. ImageNet was served as the source domain for the knowledge transfer of a VGG19 pretrained model to the target domain of 150 patients with CTscans [16]. Te model achieved sensitivity, specifcity, and accuracy of 75%, 87%, and 82%, respectively.\n1.2. Research Limitations of the Related Works. Te major research limitations of the related works are summarized as follows:\n(i) Lack of studies in multiround transfer learning for LCD: existing works considered one-round transfer learning for LCD where only one source domain was involved. Although the target model receives a beneft in the enhancement of model\u2019s performance, the model is usually having room for further enhancement (not yet achieved global optimal solution). With more source datasets, it is expected that more unseen data and potential knowledge can be transferred (positive transfer) to further enhance the performance of the target model.\n(ii) Lack of studies in negative transfer between the source domain and the target domain: theoretically, one can formulate the transfer learning problem with the source dataset and target dataset with high similarities [9\u201312] or low similarities [13\u201316]. Te negative transfer becomes more severe with the decrease in similarities because more unrelated samples can be found in the source dataset. If knowledge from unrelated samples is transferred to the target model, the model\u2019s performance becomes worsened. It is needed to avoid negative transfer to ensure the enhancement of performance of the target model, i.e., to guarantee the model moves towards the global optimal solution.\n(iii) Lack of studies in the creation of intermediate domains as a bridge between the source and target domains: controlling the knowledge transfer from the source domain to the target domain is important to enhance the chance of positive transfer. Intermediate domains should be used to break down the transfer learning problem into multiple subproblems. In this consideration, the similarities between the source domain and intermediate domain, as well as between intermediate domain and target domain, are higher than that in the original formulation, between the source domain and the target domain.\n1.3. Research Contributions of Our Work. A multiround transfer learning and modifed generative adversarial network (MTL-MGAN) algorithm is proposed to address the research limitations. Te research contributions of our work are summarized as follows:\n(i) Enhancing the optimal solutions of the LCD model with multiround transfer learning: it has been demonstrated inmany existing works for the benefts of transfer learning from the source model to the target model. Applying transfer learning multiple times (multiround transfer learning) with multiple source models is expected to enhance the optimal solutions of the LCDmodel (target model) where the performance of the target model in the next round is better than that in the current round. Tis strategy outperforms traditional single-round transfer learning. Ablation study reveals that multiround transfer learning (MTL) enhances the average sensitivity, specifcity, and accuracy of the LCDmodel by 8.28%, 8.21%, and 8.26%, respectively.\n(ii) Te loss functions are designed to minimize the impact of negative transfer: data heterogeneity is always existing between the source domain and target domain. Terefore, transfer learning is experienced discrepancies in the joint distributions between the source domain and the target domain. Reformulating the loss functions in domains, instances, and features for the reliable selection of relevant data and knowledge aims to enhancing the performance of the target model. Existing works did not fully consider the issue of negative transfer in the architecture of the transfer learning-based deep learning models. Te ablation study shows that the proposed algorithm enhances the sensitivity, specifcity, and accuracy of the LCDmodel by 1.57\u20132.23%, 1.42\u20132.26%, and 1.53\u20132.24%, respectively.\n(iii) A modifed generative adversarial network (MGAN) is designed to create two intermediate domains as bridges between the source domain and target domain: bridging the gap between the source and target domains is important to maximize the enhancement of the performance of the LCDmodel, particularly when the distant source domain is selected. It is worth noting that the merit comes to the applicability of distant source domains where a wide variety of source domains can be selected to contribute to the target model. It could also serve as a generic formulation for distant transfer learning between various types of the source domain and target domain. Te MGAN is designed to incorporate the advantages of various baseline generative adversarial network (GAN) algorithms. Te rationale is to generate more relevant samples in source domains to enhance the model transferability. In other words, the unrelated samples become less dominant as more relevant samples are available with MGAN. Ablation study shows that the MGAN enhances the sensitivity, specifcity, and accuracy of the LCD model by 3.07\u20134.61%, 2.92\u20134.33%, and 3.15\u20134.47%, respectively."
        },
        {
            "heading": "2. Methodology",
            "text": "Te design and formulations of the MTL-MGAN are presented. Tis section is comprised of the overview of the MTL-MGAN, the prioritization algorithm, the loss functions, and the MGAN.\n2.1. Overview of the MTL-MGAN. Before the illustration of the design and formulations of the proposed MTL-MGAN, an overview of the architecture is shown in Figure 1. For better visualization, it shows a scenario with multiple source datasets and one target dataset. Consider M source datasets (Ds1, . . . ,DsM) and one target dataset (TD). All source datasets are ranked in terms of the similarities between source datasets and target datasets using a prioritization algorithm (details in Subsection 2.2). Te output of the algorithm provides prioritized source datasets in descending\norder, where the highest similarity frst, denoted by (PDs1, . . . ,PDsN), with N\u2264M because some of the source datasets could be removed if they contain a signifcant portion of unrelated samples that may lead to negative transfer to the target domain. A threshold can be defned to flter sourcetarget dataset pairs with low similarity. Te removal of these pairs reduces the severity of negative transfer because more irrelevant knowledge could potentially be transferred to the target model. Both prioritized multiple source datasets and target datasets will perform MGAN to create intermediate domains as bridges. Te trained target model Dt is updated usingMTLwith the repetitions of the abovementioned steps.\n2.2. Prioritization Algorithm for Multiple Source Datasets. Selecting appropriate source models to be transferred is important to avoid the waste of efort to transfer limited knowledge to the target domain. More importantly, the transfer of irrelevant knowledge to the target domain, as a well-known issue of negative transfer, should be avoided. Among relevant source models, for those carrying similarities (relevant samples) to the target domain, it is desired to prioritize the models to be transferred (one-to-one transfer learning) in descending order of similarities between the source and target domains. Te rationale is due to the enhancement of the robustness of the target model during initial iterations to lower the impact of negative transfer from less similar source domains during later iterations. In addition, prioritization of multiple source datasets helps eliminate source-target domain pairs with low similarity (a threshold can be defned).\nTo design the prioritization algorithm for multiple source models, a hybrid approach is proposed to merge (i)\nmodifed 2D dynamic warping (M2DW): traditional 2D dynamic warping (2DW) using bidirectional mapping optimally aligns between two images on a similarity basis. However, M2DW performs well only with even resolutions across multiple sensors [17]. Te proposed M2DW flls the gap to enable uneven resolutions that are commonly used in practice; (ii) Silhouette coefcient: inspired by [18], where Silhouette coefcient was used to select the source domains using only with pretrained model and target domain. Our work extends the consideration with the aid of the characteristics of the source domains. To begin with, the design and formulations of the M2DW algorithm are presented.\nTe algorithm frst runs through the classes of each dataset and then takes the mean of the image set for each class. Initializing the 2DW barycenter averaging with the medoid of the time series set. Te iteration carries out for every pair of datasets using one-to-one mapping. Te distance between any pair of datasets equals to the minimal 2DW distance between classes.\nTe total similarity score SSij for dataset Di with Ni sequences and dataset Dj with Nj sequences is given by the following equation:\nSSij \ufffd \nm\u2208 1,Ni[ ],n\u2208 1,Nj \nsmn, (1)\nwhere smn is the similarity score between them th sequence in Di and the nth sequence in Dj. Regarding the Silhouette coefcient, the target training datasets is frst encoded with every source models. Te average Silhouette coefcient SC for each set of encodings is measured with the following formulations:\nSCi \ufffd (h \u2212 g)\nmax (g, h) ,\ng \ufffd x\u2208G,x\u2260id(i, x) \nNG \u2212 1(  ,\nh \ufffd\nmin H\u2260G x\u2208Hd(i, x) \nNH ,\nSC \ufffd i\u2208Ls(i) \nNL ,\n(2)\nwhere SCi is the Silhouette coefcient for a single encoding vector i, d is the distance between two encodings, G and H are some labels of i, L is the label for the fnal model, and NG, NH, and NL are the number of encodings labeled for labels G, H, and L, respectively.\nTe total similarity scores for all pairs are normalized and weighted with the results using the Silhouette coefcient. As a result, the priorities of the source domains (to be transferred) are obtained."
        },
        {
            "heading": "2.3. Minimizing the Negative Transfer with Loss Functions.",
            "text": "Transfer learning does not guarantee to improve the performance of the target model, that is a commonly known issue of negative transfer. A recent survey on negative transfer [19] summarizes the solutions into three types: (i) secure transfer: the objective function is defned to ensure positive transfer to the target model; (ii) distant transfer: low similarities between source dataset and target dataset may\nhappen when the datasets are in diferent domains (research topics). Some researchers demonstrated the efectiveness of setting up an intermediate domain to bridge between the source and target domains; (iii) transferability enhancement: enhancing the data quality in the source datasets leads to the improvement of the transfer learning to the target model.\nTe frst approach is not chosen because of the requirement of the full understanding of all source domains and restrictions on the design and formulations of the transfer learning problem. It is not feasible based on the research initiative to allow distant transfer learning with a wide variety of dissimilar source datasets and target datasets. Te second approach is also not appropriate that requires knowledge of source domains and experiences challenging to obtain or create an intermediate domain. Terefore, the last approach is considered to enhance the data transferability between the source domain and the target domain. To comprehensively enhance the data quality, we have formulated the optimization problems in the aspects of domains, instances, and features. Te rationale is to fully consider the entire transfer learning process to ensure negative transfer avoidance in all phases. After the selection of useful samples (knowledge), unequal weighting factors are introduced to the frst and second-order features. Penalization may also be performed for unrelated samples.\nRegarding domains, we frst consider the moment distance dmoment(DS, DT) for the measurement of the similarity between every pair of domains. Denote the source domains as DS \ufffd D1, \u00b7 \u00b7 \u00b7 , DNs  with a total number of source domains Ns and single target domain DT. Te moment distance is defned as\ndmoment DS, DT(  \ufffd\n Ns i\ufffd1 F Xsi  1 \u2212F Xt(  1 \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd2 \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd +  Ns i\ufffd1 F Xsi  2 \u2212F Xt(  2 \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd2 \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd  \nNs ,\n(3)\nwhere F(Xsi) 1 and F(Xsi) 2 are the average operation of the 1st order and 2nd order features with the si source domain, respectively. Likewise, F(Xt) 1 and F(Xt) 2 are the average operation of the 1st order and 2nd order features with the target domain, respectively.\nEquation (3) assumes equal weighting factors for all source domains; however, this cannot precisely describe the fact that diferent extent of similarities exists between multiple source domains and target domain. Terefore, modifed moment distance dmodi fied(DS, DT) is proposed:\ndmoment DS, DT(  \ufffd\n Ns i\ufffd1\u03b1i F Xsi  1 \u2212F Xt(  1 \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd2 +  Ns i\ufffd1\u03b1i \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd F Xsi  2 \u2212F Xt(  2 \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd2  \nNs ,\n(4)\nwhere \u03b1i is the normalized weight (i\u03b1i \ufffd 1) of the source domain si.\nIn the aspect of instances, the consideration is on the transfer of useful components in the source domain to the target domain. A minimization problem of the transfer learning based on component Ci can be formulated as\nmin Mi \u03b2i M T i Mi  + ci \u03b1i \u2212 min \u03b1i \n\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd + \u03b4Ti, (5)\nwhere Mi is the Mahalanobis distance of Ci, \u03b2i is the hyperparameter to control the generalization error of Mi, ci is the hyperparameter to control the regularization of the samples in Ci, and \u03b4Ti is the loss function (or error) to predict\na sample in DT. Te loss function is calculated by the following equation:\n\u03b4Ti \ufffd SWDwithin \u2212 SWDacross, (6)\nwhere SWDwithin and SWDacross are the sum of the weighted diferences within classes and across classes, respectively.\nIn the aspect of features, for those with small singular values can be penalized via singular value decomposition (SVD) with penalization. Te feature matrix F \ufffd [f1, \u00b7 \u00b7 \u00b7 , fN] is denoted with size N. Te representation of F using SVD is given by the following equation:\nF \ufffd U\u03a3VT, (7)\nwhere U is the left singular vector, \u03a3 is the singular value matrix of F, and V is the right singular vector. Rearrange the singular values of \u03a3 as [\u03bc1, \u00b7 \u00b7 \u00b7 , \u03bcN] in descending order. Te idea for transferability enhancement in the feature layer is to penalize the smallest p singular values:\nLpenalize(F) \ufffd \u03c1 p\ni\ufffd1 \u03bci( \n2 , (8)\nwhere \u03c1 is the hyperparameter to control the strength of penalization and p equals to the number of penalized singular values.\n2.4. MGAN for the Creation of Intermediate Domains. Recall the rationale of the creation of intermediate domains between the source domain and the target domain, is to increase the similarities between the source domain and the target domain. In each round of MTL, two intermediate domains are created. One intermediate domain ID-MGANs is based on the source domain and another ID-MGANt is\nbased on the target domain using MGAN. Te intermediate domains link closely with the source domain and the target domain to ensure they are based on the distribution of the original datasets (source dataset and target dataset). Figure 2 introduces the architecture of the transfer learning process with two intermediate domains.Tis has divided the original transfer learning process between the source domain and target domain into three subproblems: (i) subproblem 1: transfer learning between the source domain and IDMGANs; (ii) subproblem 2: transfer learning between IDMGANs and ID-MGANt; (iii) subproblem 3: transfer learning between ID-MGANt and target domain.\nTe baseline GAN is often not performing well in recent complex machine learning problems because of the fatal theory corruption with random noise vector [20]. Two popular (with highcitations in the research publications) variants of GANs namely auxiliary classifer GAN [21] and conditional GAN [22] were thus proposed to solve the limitation. In this paper, we combine these variants of GANs, as the architecture of MGAN.\nFigure 3 shows the architecture of theMGAN. Defne the notations: noise vector n, conditional variable c, generatorG, latent variable z, data distribution X, and discriminator D. MGAN is featured with (i) all generated samples are assigned with label and (ii) adding additional input, conditional variable to the discriminator. Te idea of the algorithm is to use G to fool D, with c. G knows the mapping between latent space and data distribution whereas D classifers the generated samples from the ground truth distribution.\nDefne the loss functions Lsource and Lclass for the source and class, respectively.Te objective functions of the MGAN are formulated as follows:\nLsource \ufffd E logP Source \ufffd fake Xfake    + E logP Source \ufffd real Xreal   ,\nLclass \ufffd E logP Class \ufffd c Xfake    + E logP Class \ufffd c Xreal   ,\nGenerator: maxLclass \u2212 Lsource,\nDiscriminator: maxLclass + Lsource.\n(9)"
        },
        {
            "heading": "3. Performance Evaluation and Comparison",
            "text": "To evaluate the performance of the MTL-MGAN, 10 benchmark datasets are selected. Te performance of the MTL-MGAN is analyzed. Tis is followed by the performance comparison between the MTL-MGAN and existing works.\n3.1. BenchmarkDatasets. 10 benchmark datasets are selected for which fve of them are related to lung cancer datasets (with higher similarities given the application is LCD) and the remaining fve of them are related to nonlung cancer datasets (with lower similarities). Te fve lung cancer datasets are NSCLC-Radiomics [23], NSCLC-RadiomicsGenomics [24], SPIE-AAPM Lung CT Challenge [25], LungCT-Diagnosis [26], and Lung CT Segmentation Challenge 2017 [27]. Te nonlung cancer datasets are CIFAR-10 dataset [28], ImageNet dataset [29], Microsoft Common Objects in Context [30] of images for multidisciplinary research, prostate cancer dataset NaF Prostate [31], and breast cancer dataset QIN-Breast [32].\nTrivially, it is expected that the similarities between lung cancer datasets [23\u201327] are high and thus the model experiences less severity of negative transfer. For image datasets of multidiscipline, the datasets [28\u201330] contain highly dissimilar samples which are more prone to negative transfer. For prostate cancer [31] and breast cancer datasets [32], there exist some similarities between datasets because of the nature of cancer images. Tese hypotheses will be examined in the following sections.\n3.2. Performance Evaluation of the MTL-MGAN. Tis research study is intended to conduct research on the prioritization of source datasets, the negative transfer avoidance, generation of intermediate domains, and the multiple transfer learning so that the feature extraction and classifcation algorithms are not major research directions. Terefore, the convolutional neural network is employed as the basic architecture of the target model.\nTo examine the issue of model overftting and better fne-tuning the models, 5-fold cross-validation is adopted that has been justifed as a common setting of k-fold crossvalidation (with k \ufffd 5) [33, 34]. Since 10 benchmark datasets are chosen, at most, the target model performs 9-\nround of MTL-MGAN from nine source datasets. Te training will stop when negative transfer becomes severe, i.e., the performance (accuracy) of the target model is less than that of the target model using the preceding source dataset.\nFigure 4 shows the accuracy of the 5 target models (lung cancer-related) in each round of MTL-MGAN. Several following observations are drawn:\n(i) Te maximum number of rounds of MTL-MGAN varies across the target models. Te ascending order is given by seven rounds in NSCLC-RadiomicsGenomics [24] and LungCT-Diagnosis [26], eight rounds in NSCLC-Radiomics [23] and Lung CT Segmentation Challenge 2017 [27], and nine rounds in SPIE-AAPM Lung CT Challenge [25].\n(ii) Te rank in ascending order for the overall percentage improvement between the frst and last round of iteration using MTL-MGAN is 6.85% in SPIE-AAPM Lung CT Challenge [25], 7.00% in NSCLC-Radiomics [23], 8.16% in NSCLC-Radiomics-Genomics [24], 8.70% in Lung CT Segmentation Challenge 2017 [27], and 9.92% in LungCTDiagnosis [26].\n(iii) Te percentage improvement per round using MTL-MGAN in ascending order is 0.761% in SPIE-AAPM Lung CT Challenge [25], 0.875% in NSCLC-Radiomics [23], 1.09% in Lung CT Segmentation Challenge 2017 [27], 1.17% in NSCLCRadiomics-Genomics [24], and 1.42% in LungCTDiagnosis [26].\n3.3. Performance Comparison with Related Works. Te performance comparison between our work and related works covered in Section 1.1 is shown in Table 1. We summarize the observations in each column as follows:\n(i) Source domain and target domain: the related works [9\u201312] formulated the transfer learning problem using a similar source domain and target domain whereas other works [13\u201316] considered the distant source and target domains. Our work considered 10 benchmark datasets to evaluate the MTL using similar and distant sources and target domains.\n(ii) Intermediate domains: related works [9\u201316] did not introduce any intermediate domains to bridge the gap between the source domain and target domain. Our work creates two intermediate domains using MGAN to reduce the level of dissimilarities between the source domain and target domain and thus enhancing the transferability. Particularly, it is important when the source domain and target domain are highly difered from each other.\n(iii) Methodology: the related works formulated the classifcation problems using traditional deep learning algorithms. In view of the research limitations, our work proposed the prioritization algorithm, the multiple transfer learning, the negative transfer avoidance algorithm by designing loss functions, and the MGAN.\n(iv) Cross-validation: related works [9\u201312, 14, 15] did not employ cross-validation. Te performance evaluation possessed limitations in partial utilization of the dataset and lack of information on the evaluation of potential model overftting when it comes to a deep learning environment. Related works [13, 16] adopted 10-fold cross-validation whereas our work used 5-fold cross-validation. Both 5-fold and 10-fold settings are commonly used in literature with comparable performance [35, 36].\n(v) Ablation study: related works [13\u201316] did not conduct an ablation study. It is an important element to evaluate the contributions of individual components of the transfer learning model on the performance enhancement of the target model. It is worth noting that negative transfer may exist that is equivalent to a worsened performance on the target model after transfer learning. Other\nrelated works [9\u201312] and our work carry out ablation studies and report the contributions of the transfer learning model in the enhancement of model performance.\n(vi) Sensitivity: related works [9\u201311, 14, 15] did not report the sensitivity. It is important to report both the sensitivity and specifcity to ensure that biased classifcation is not observed. Te works [13, 16] reported the sensitivity of the LCD model when transfer learning is applied. Te work [12] revealed the improvement of sensitivity by 2.22% using the transfer learning model. Our work shows an improvement of sensitivity by 6.86\u201310.8% in the fve target models.\n(vii) Specifcity: similar to the sensitivity of the model, observation is made for the absence of reporting of the specifcity and only the result after using the transfer learning model. Te work [12] improved the specifcity by 18.6%, nevertheless, model overftting is observed. Our work shows an improvement of specifcity by 6.70\u201310.4% in the fve target models.\n(viii) Accuracy: all related works and our work report the accuracy. Related works [13\u201316] only reported the results after applying the transfer learning model. Te percentage improvement of the accuracy is 7.80% [9], 3.77% [10], 1.34% [11], 5.88% [12], and 6.85\u20139.92% (our work)."
        },
        {
            "heading": "4. Ablation Studies",
            "text": "To evaluate the benefts of the components of the MTLMGAN, ablation studies are carried out on four key components namely prioritization algorithm, MTL, negative transfer avoidance with loss functions, and MGAN.\nTa bl\ne 1:\nPe rf or m an ce\nco m pa ri so n be tw ee n M TL\n-M G A N\nan d re la te d w or ks .\nW or k\nSo ur ce do m ai n\nIn te rm\ned ia te\ndo m ai ns\nTa rg et do m ai n\nM et ho\ndo lo gy\nC ro ss -v al id at io n\nA bl at io n st ud\ny Se ns iti vi ty\nSp ec if ci ty\nA cc ur ac y\n[9 ]\nLu na 16\nN o\nSh an do\nng pr ov in ci al\nho sp ita\nl Re\nsid ua ln\neu ra ln\net w or k\nan d de ep\nne ur al ne tw or k\nN o\nYe s\nN /A\nN /A\nFr om\n79 .5 % to 85 .7 %\n[1 0]\nIm ag eN\net N o\nN at io na ll un\ng sc re en in g tr ia l\nan d th e na tio\nna li ns tit ut e of\nal le rg y an d in fe ct io us\ndi se as eT B po rt al\nV G G 16\nan d de ep\nne ur al\nne tw or k\nN o\nYe s\nN /A\nN /A\nFr om\n87 .5 % to 90 .8 %\n[1 1]\nLu na 16\nN o\nG an gn\neu ng\nA sa n ho\nsp ita\nl YO\nLO X\nN o\nYe s\nN /A\nN /A\nFr om\n89 .7 % to 90 .9 %\n[1 2]\nN od\nul e\nid en tif\nca tio n C N N\nN o\nW es tC\nhi na\nho sp ita\nlo fS\nic hu an un iv er sit y, Ru iji n ho sp ita lo f Sh an gh ai Jia o To ng un iv er sit y Sc ho ol of m ed ic in e, an d C ha ng zh en g ho sp ita lo fs ec on d m ili ta ry m ed ic al un iv er sit y\nSe m isu\npe rv ise\nd de ep\ntr an sf er\nle ar ni ng\nN o\nYe s\nFr om\n90 .2 %\nto 92 .2 %\nFr om\n66 .3 %\nto 78 .6 %\nFr om\n83 .4 % to 88 .3 %\n[1 3]\nIm ag eN\net N o\nLI D C /I D RI\nRe sN\net 50\nan d SV\nM 10 -f ol d\nN o\n85 .4 %\nN /A\n88 .4 %\n[1 4]\nIm ag eN\net N o\nC he st\nC T\nV G G 16 ,V\nG G 19 ,\nM ob\nile N et\nan d\nD en se N et 16 9\nN o\nN o\nN /A\nN /A\n91 .3 %\nw ith\nV G G 16\n(b es t)\n[1 5]\nV G G 16\nN o\nIr aq -o nc ol og y te ac hi ng\nho sp ita l an d na tio na lc en te r fo r ca nc er di se as es\nSe m isu\npe rv ise\nd de ep\ntr an sf er\nle ar ni ng\nN o\nN o\nN /A\nN /A\n98 .8 %\n(t ra in in g)\nan d 83 .4 % (t es tin g)\n[1 6]\nIm ag eN\net N o\nC T\nD ee p co nv\nol ut io na l\nne ur al\nne tw or k\n10 -f ol d\nN o\n75 %\n87 %\n82 %\nO ur w or k\nU p to\n9 of\n[2 4\u2013\n32 ]\nPD -M\nG A N s an d\nD -M\nG A N t\n[2 3]\nM TL\n-M G A N\n5- fo ld\nYe s\nFr om\n91 .8 %\nto 98 .1 %\nFr om\n90 .9 %\nto 97 .3 %\nFr om\n91 .4 % to 97 .8 %\nO ur w or k\nU p to\n9 of\n[2 3,\n25 \u20133\n2] PD\n-M G A N s an d D -M G A N t\n[2 4]\nM TL\n-M G A N\n5- fo ld\nYe s\nFr om\n90 .2 %\nto 97 .6 %\nFr om\n91 .4 %\nto 98 .7 %\nFr om\n90 .7 % to 98 .1 %\nO ur w or k\nU p to\n9 of\n[2 3,\n24 ,2\n6\u2013 32 ]\nPD -M\nG A N s an d\nD -M\nG A N t\n[2 5]\nM TL\n-M G A N\n5- fo ld\nYe s\nFr om\n91 % to 97 .4 %\nFr om\n92 .5 %\nto 98 .7 %\nFr om\n92 % to 98 .3 %\nO ur w or k\nU p to\n9 of\n[2 3\u2013\n25 ,2\n7\u2013 32 ]\nPD -M\nG A N s an d\nD -M\nG A N t\n[2 6]\nM TL\n-M G A N\n5- fo ld\nYe s\nFr om\n89 .3 %\nto 98 .9 %\nFr om\n90 .0 %\nto 99 .4 %\nFr om\n89 .7 % to 99 .2 %\nO ur w or k\nU p to\n9 of\n[2 3\u2013\n26 ,2\n8\u2013 32 ]\nPD -M\nG A N s an d\nD -M\nG A N t\n[2 7]\nM TL\n-M G A N\n5- fo ld\nYe s\nFr om\n91 .1 %\nto 98 .9 %\nFr om\n90 .3 %\nto 98 .3 %\nFr om\n90 .8 % to 98 .7 %\n4.1. Contribution of the Prioritization Algorithm. Te prioritization algorithm helps ranking the similarities of the multiple source domains to the target domain. Table 2 compares the number of MTL-MGAN execution with and without the prioritization algorithm.Te scenario without the prioritization algorithm is equivalent to the exhaustive search (the total number of executions can be found by permutation).Te results are identical across diferent target domains.\n4.2. Contribution of theMTL. Te sensitivity, specifcity, and accuracies of the target model with and without MTL are summarized in Table 3. Observations are drawn as follows:\n(i) Sensitivity: the improvement by MTL is 6.86% [23], 8.20% [24], 7.03% [25], 10.8% [26], and 8.56% [27]. Te average sensitivity of the fve target models is 8.28%.\n(ii) Specifcity: the improvement by MTL is 7.04% [23], 7.99% [24], 6.70% [25], 10.4% [26], and 8.86% [27]. Te average specifcity of the fve target models is 8.21%.\n(iii) Precision: the improvement by MTL is 7.02% [23], 8.41% [24], 6.89% [25], 10.4%, and 8.72% [27]. Te average precision of the fve target models is 8.29%.\n(iv) F-measure: the improvement by MTL is 6.91% [23], 8.02% [24], 6.99% [25], 10.0% [26], and 8.68% [27].Te average F-measure of the fve target models is 8.12%.\n(v) Accuracy: the improvement by MTL is 7.00% [23], 8.16% [24], 6.85% [25], 10.6% [26], and 8.70% [27].Te average accuracy of the fve target models is 8.26%.\n4.3. Contribution of theNegative TransferAvoidancewith Loss Functions. Recall the loss functions are designed based on three aspects: domains, instances, and features. Table 4 summarizes the performance of the target model with and without the design of loss function in domains, instances, and features.\nTe comparisons are as follows:\n(i) Domains: the improvements of the sensitivity, specifcity, precision, F-measure, and accuracy are ranged 2.09\u20132.40%, 1.97\u20132.47%, 2.07\u20132.40%, 2.07\u20132.52%, and 2.18\u20132.30%. Te average improvements of the fve target models are 2.23%, 2.26%, 2.27%, 2.31%, and 2.24% in sensitivity, specifcity, precision, F-measure, and accuracy, respectively.\n(ii) Instances: the improvements of the sensitivity, specifcity, precision, F-measure, and accuracy are ranged 1.67\u20132.06%, 1.84\u20132.40%, 1.55\u20132.41%, 1.75\u20132.29%, and 1.85\u20132.17%. Te average improvements of the fve target models are 1.97%, 2.05%, 2.06%, 2.01%, and 1.99% in sensitivity, specifcity, precision, F-measure, and accuracy, respectively.\n(iii) Features: the improvements of the sensitivity, specifcity, precision, F-measure, and accuracy are ranged 1.33\u20131.76%, 1.23\u20131.57%, 1.43\u20131.55%, 1.14\u20132.28%, and 1.34\u20131.66%. Te average improvements of the fve target models are 1.57%, 1.42%, 1.47%, 1.53%, and 1.53% in sensitivity, specifcity, precision, F-measure, and accuracy, respectively.\n4.4. Contribution of the MGAN. MGAN is applied to create two intermediate domains based on the source domain and target domain. Table 5 verifes the contributions of MGAN. Te improvements of the sensitivity, specifcity, precision, F-measure, and accuracy are ranged 3.07\u20134.61%, 2.92\u20134.33%, 3.06\u20134.81%, 2.18\u20134.24%, and 3.15\u20134.47%, respectively. Te average improvements in sensitivity, specifcity, precision, F-measure, and accuracy using with the inclusion of MGAN are 3.61%, 3.56%, 3.70%, 3.32%, and 3.58%, respectively.\n4.5. Complexity of the Algorithms. It can be seen from the results that the prioritization algorithm is important to signifcantly reduced the trials of the MTL-MGAN with diferent orders of multiple source datasets. Tis also refects a signifcant reduction in the complexity of the model that avoids unnecessary computing power on exhaustive search. Regarding MTL, which is the strategy to perform multiple times of the transfer learning process. To avoid negative transfer, the loss functions are designed based on the aspects of domains, instances, and features. Although this increases the complexity of the optimization algorithm, the ablation study (Section 4.3) confrms the efectiveness of loss functions. Creating two intermediate domains using MGAN increases the time and computing power of the transfer learning process, however, they contribute to the avoidance of negative transfer."
        },
        {
            "heading": "5. Conclusion",
            "text": "Te technological advancement of the machine learning algorithms has received attention in recent years to enhance the medical diagnosis of lung cancers. Responding to the research limitations of existing lung cancer detection models in multiround transfer learning, negative transfer, and lack of bridge between source and target domains, we have proposed a multiround transfer learning and modifed generative adversarial network algorithm with a prioritization algorithm and modifed loss functions in domains, instances, and features perspectives. 10 benchmark datasets are selected to evaluate the performance of the proposed\nalgorithm. It signifcantly enhances the performance of the lung cancer detection model, compared with related works. Ablation studies also provide convincing results to reveal the contributions of the components of the proposed algorithm in the aspects of prioritization algorithm, multiple transfer learning, customized loss functions in domains, instances, features, and modifed generative adversarial network.\nTe implication of the proposed algorithm releases the constraints in the selection of source domains and target domains. Terefore, it can contribute to various research areas, such as sustainable development goals [37], green applications [38], cyber-physical systems [39, 40], smart homes [41], and medical diagnosis [6, 7, 42]. To enhance the efciency of the optimization algorithm, future investigations could be conducted with various types of optimization approaches, which details can be referred to in review articles [46, 47].\nSeveral future research directions are suggested such as (i) reducing the number of rounds of transfer learning by enhancing the negative transfer avoidance algorithm and generating more relevant samples; (ii) evaluating more baseline deep learning algorithms [43] such as recurrent neural networks, long short-term memory, gated recurrent network, self organization maps, and deep neural network; (iii) including more distant source datasets that are highly dissimilar to the target domain; (iv) modifying the transfer learning process with incremental learning [44] to gradually transfer knowledge between the source and target domains as well as reduce the impact of negative transfer.\nList 1 Summary of the acronyms and symbols."
        },
        {
            "heading": "Acronyms",
            "text": "2DW: 2D dynamic warping MTL: Multiround transfer learning c: Conditional variable MTL-MGAN: Multiround transfer learning and modifed generative adversarial network CT: Computed tomography n: Noise vector d: Distance between two encodings NG: Number of encodings labeled for label G dmoment(DS, DT): Moment distance NH: Number of encodings labeled for label H dmodi fied(DS, DT)\n: Modifed moment distance NL: Number of encodings labeled for label L D: Discriminator PDs1 , . . . ,PDsN N: Prioritized source datasets with N\u2264M DS: Source domains p: Number of penalized singular values Ds1 , . . . ,DsM: M source datasets smn: Similarity score between the m th sequence in Di and the nth sequence in Dj Dt: Trained target model SCi: Silhouette coefcient for a single encoding vector DT: Single target domain SC: Average Silhouette coefcient F \ufffd [f1, \u00b7 \u00b7 \u00b7 , fN]\n: Feature matrix with size N SSij: Total similarity score for dataset Di with Ni sequences and dataset Dj with Nj sequences F(Xsi) 1: Average operation of the 1st order features with the si source domain SVD: Singular value decomposition F(Xsi)\n2: Average operation of the 2nd order features with the si source domain\nSWDacross: Sum of the weighted diferences across classes F(Xt) 1: Average operation of the 1st order features with the target domain Xt SWDwithin: Sum of the weighted diferences within classes F(Xt)\n2: Average operation of the 2nd order features with the target domain Xt\nTD: Target dataset G: Generator U: Left singular vector GAN: Generative adversarial network V: Right singular vector H: Some labels of i X: Data distribution ID-MGANs: Intermediate domain based on the source domain using MGAN z: Latent variable ID-MGANt: Intermediate domain based on the target\ndomain using MGAN\n\u03b1i: Normalized weight (i\u03b1i \ufffd 1) of the source domain si L: Label for the fnal model \u03b2i: Hyperparameter to control the generalization error of Mi LCD: Lung cancer detection ci: Hyperparameter to control the\nregularization of the samples in component Ci\nM2DW: Modifed 2D dynamic warping \u03b4Ti: Loss function to predict a sample in DT Mi: Mahalanobis distance of component Ci \u03c1: Hyperparameter to control the strength of penalization MGAN: Modifed generative adversarial network \u03a3: Singular value matrix of F."
        },
        {
            "heading": "Data Availability",
            "text": "Te data used to support the study are included in the paper."
        },
        {
            "heading": "Conflicts of Interest",
            "text": "Te authors declare that there are no conficts of interest."
        }
    ],
    "title": "Multiround Transfer Learning and Modified Generative Adversarial Network for Lung Cancer Detection",
    "year": 2023
}