{
    "abstractText": "In this work, we introduce a new approach for face stylization. Despite existing methods achieving impressive results in this task, there is still room for improvement in generating high-quality artistic faces with diverse styles and accurate facial reconstruction. Our proposed framework, MMFS, supports multi-modal face stylization by leveraging the strengths of StyleGAN and integrates it into an encoder-decoder architecture. Specifically, we use the mid-resolution and high-resolution layers of StyleGAN as the decoder to generate high-quality faces, while aligning its low-resolution layer with the encoder to extract and preserve input facial details. We also introduce a two-stage training strategy, where we train the encoder in the first stage to align the feature maps with StyleGAN and enable a faithful reconstruction of input faces. In the second stage, the entire network is fine-tuned with artistic data for stylized face generation. To enable the fine-tuned model to be applied in zero-shot and one-shot stylization tasks, we train an additional mapping network from the large-scale Contrastive-Language-Image-Pre-training (CLIP) space to a latent w+ space of fine-tuned StyleGAN. Qualitative and quantitative experiments show that our framework achieves superior performance in both one-shot and zero-shot face stylization tasks, outperforming state-of-the-art methods by a large margin. CCS Concepts \u2022 Computing methodologies \u2192 Image processing;",
    "authors": [
        {
            "affiliations": [],
            "name": "Mengtian Li"
        },
        {
            "affiliations": [],
            "name": "Yi Dong"
        },
        {
            "affiliations": [],
            "name": "Minxuan Lin"
        },
        {
            "affiliations": [],
            "name": "Haibin Huang"
        },
        {
            "affiliations": [],
            "name": "Pengfei Wan"
        },
        {
            "affiliations": [],
            "name": "Chongyang Ma"
        }
    ],
    "id": "SP:1fb2438e7ed4854d13db64dac211ced9f4d59826",
    "references": [
        {
            "authors": [
                "Y. ALALUF",
                "O. PATASHNIK",
                "D. COHEN-OR"
            ],
            "title": "ReStyle: A Residual-Based StyleGAN Encoder via Iterative Refinement",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision",
            "year": 2021
        },
        {
            "authors": [
                "R. ABDAL",
                "Y. QIN",
                "P. WONKA"
            ],
            "title": "Image2stylegan: How to embed images into the stylegan latent space",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision",
            "year": 2019
        },
        {
            "authors": [
                "R. ABDAL",
                "Y. QIN",
                "P. WONKA"
            ],
            "title": "Image2stylegan++: How to edit the embedded images",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2020
        },
        {
            "authors": [
                "H. CHEFER",
                "S. BENAIM",
                "R. PAISS",
                "L. WOLF"
            ],
            "title": "Image-based clip-guided essence transfer",
            "venue": "In European Conference on Computer Vision",
            "year": 2022
        },
        {
            "authors": [
                "J. CHONG M",
                "D. FORSYTH"
            ],
            "title": "Jojogan: One shot face stylization",
            "venue": "In European Conference on Computer Vision",
            "year": 2022
        },
        {
            "authors": [
                "M. CARON",
                "H. TOUVRON",
                "I. MISRA",
                "H. J\u00c9GOU",
                "J. MAIRAL",
                "P. BOJANOWSKI",
                "A. JOULIN"
            ],
            "title": "Emerging properties in self-supervised vision transformers",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision",
            "year": 2021
        },
        {
            "authors": [
                "Y. CHOI",
                "Y. UH",
                "J. YOO",
                "J.-W. HA"
            ],
            "title": "Stargan v2: Diverse image synthesis for multiple domains",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2020
        },
        {
            "authors": [
                "J. DENG",
                "J. GUO",
                "N. XUE",
                "S. ZAFEIRIOU"
            ],
            "title": "Arcface: Additive angular margin loss for deep face recognition",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2019
        },
        {
            "authors": [
                "P. DHARIWAL",
                "A. NICHOL"
            ],
            "title": "Diffusion models beat GANs on image synthesis",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "R. GAL",
                "O. PATASHNIK",
                "H. MARON",
                "H. BERMANO A",
                "G. CHECHIK",
                "D. COHEN-OR"
            ],
            "title": "Stylegan-nada: Clip-guided domain adaptation of image generators",
            "venue": "ACM Transactions on Graphics (TOG) 41,",
            "year": 2022
        },
        {
            "authors": [
                "X. HUANG",
                "S. BELONGIE"
            ],
            "title": "Arbitrary style transfer in real-time with adaptive instance normalization",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision",
            "year": 2017
        },
        {
            "authors": [
                "X. HUANG",
                "M.-Y. LIU",
                "S. BELONGIE",
                "J. KAUTZ"
            ],
            "title": "Multimodal unsupervised image-to-image translation",
            "venue": "In European Conference on Computer Vision",
            "year": 2018
        },
        {
            "authors": [
                "M. HEUSEL",
                "H. RAMSAUER",
                "T. UNTERTHINER",
                "B. NESSLER",
                "S. HOCHREITER"
            ],
            "title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium",
            "venue": "In Advances in Neural Information Processing Systems (2017)",
            "year": 2017
        },
        {
            "authors": [
                "F. HAN",
                "S. YE",
                "M. HE",
                "M. CHAI",
                "J. LIAO"
            ],
            "title": "Exemplar-based 3d portrait stylization",
            "venue": "IEEE Transactions on Visualization and Computer Graphics 29,",
            "year": 2021
        },
        {
            "authors": [
                "W. JANG",
                "G. JU",
                "Y. JUNG",
                "J. YANG",
                "X. TONG",
                "S. LEE"
            ],
            "title": "Stylecarigan: caricature generation via stylegan feature map modulation",
            "venue": "ACM Transactions on Graphics (TOG) 40,",
            "year": 2021
        },
        {
            "authors": [
                "K. KANG",
                "H. KIM S",
                "S. CHO"
            ],
            "title": "Gan inversion for out-of-range images with geometric transformations",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision",
            "year": 2021
        },
        {
            "authors": [
                "Li"
            ],
            "title": "Multi-Modal Face Stylization with a Generative Prior IEEE/CVF",
            "venue": "Conference on Computer Vision and Pattern Recognition",
            "year": 2019
        },
        {
            "authors": [
                "T. KARRAS",
                "S. LAINE",
                "M. AITTALA",
                "J. HELLSTEN",
                "J. LEHTINEN",
                "T. AILA"
            ],
            "title": "Analyzing and improving the image quality of stylegan",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2020
        },
        {
            "authors": [
                "G. KWON",
                "C. YE J"
            ],
            "title": "Clipstyler: Image style transfer with a single text condition",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2022
        },
        {
            "authors": [
                "G. KWON",
                "C. YE J"
            ],
            "title": "One-shot adaptation of gan in just one clip",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "year": 2023
        },
        {
            "authors": [
                "M. LIU",
                "Q. LI",
                "Z. QIN",
                "G. ZHANG",
                "P. WAN",
                "W. ZHENG"
            ],
            "title": "Blendgan: implicitly gan blending for arbitrary stylized face generation",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "LIU A. H",
                "LIU Y.-C",
                "YEH Y.-Y",
                "WANG Y.-C. F"
            ],
            "title": "A unified feature disentangler for multi-domain image translation and manipulation",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2018
        },
        {
            "authors": [
                "LEE H.-Y",
                "TSENG H.-Y",
                "MAO Q",
                "HUANG J.-B",
                "LU Y.-D",
                "SINGH M",
                "YANG M.-H"
            ],
            "title": "Drit++: Diverse image-to-image translation via disentangled representations",
            "venue": "International Journal of Computer Vision 128,",
            "year": 2020
        },
        {
            "authors": [
                "Y. LI",
                "R. ZHANG",
                "J. LU",
                "E. SHECHTMAN"
            ],
            "title": "Few-shot image generation with elastic weight consolidation",
            "venue": "arXiv preprint arXiv:2012.02780",
            "year": 2020
        },
        {
            "authors": [
                "B. LI",
                "Y. ZHU",
                "Y. WANG",
                "C.-W. LIN",
                "B. GHANEM",
                "L. SHEN"
            ],
            "title": "Anigan: Style-guided generative adversarial networks for unsupervised anime face generation",
            "venue": "IEEE Transactions on Multimedia",
            "year": 2021
        },
        {
            "authors": [
                "S. MO",
                "M. CHO",
                "J. SHIN"
            ],
            "title": "Freeze the discriminator: a simple baseline for fine-tuning gans",
            "venue": "arXiv preprint arXiv:2002.10964",
            "year": 2020
        },
        {
            "authors": [
                "Q. NICHOL A",
                "P. DHARIWAL"
            ],
            "title": "Improved denoising diffusion probabilistic models",
            "venue": "In International Conference on Machine Learning (ICML)",
            "year": 2021
        },
        {
            "authors": [
                "U. OJHA",
                "Y. LI",
                "J. LU",
                "A. EFROS A",
                "J. LEE Y",
                "E. SHECHTMAN",
                "R. ZHANG"
            ],
            "title": "Few-shot image generation via cross-domain correspondence",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2021
        },
        {
            "authors": [
                "N. PINKNEY J",
                "D. ADLER"
            ],
            "title": "Resolution dependent gan interpolation for controllable image synthesis between domains",
            "venue": "arXiv preprint arXiv:2010.05334",
            "year": 2020
        },
        {
            "authors": [
                "O. PATASHNIK",
                "Z. WU",
                "E. SHECHTMAN",
                "D. COHEN-OR",
                "D. LISCHINSKI"
            ],
            "title": "Styleclip: Text-driven manipulation of stylegan imagery",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision",
            "year": 2021
        },
        {
            "authors": [
                "E. RICHARDSON",
                "Y. ALALUF",
                "O. PATASHNIK",
                "Y. NITZAN",
                "Y. AZAR",
                "S. SHAPIRO",
                "D. COHEN-OR"
            ],
            "title": "Encoding in style: a stylegan encoder for image-to-image translation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2021
        },
        {
            "authors": [
                "R. ROMBACH",
                "A. BLATTMANN",
                "D. LORENZ",
                "P. ESSER",
                "B. OMMER"
            ],
            "title": "High-resolution image synthesis with latent diffusion models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2022
        },
        {
            "authors": [
                "ROBB E",
                "CHU W.-S",
                "KUMAR A",
                "HUANG J.-B"
            ],
            "title": "Fewshot adaptation of generative adversarial networks",
            "venue": "arXiv preprint arXiv:2010.11943",
            "year": 2020
        },
        {
            "authors": [
                "D. RUTA",
                "A. GILBERT",
                "S. MOTIIAN",
                "B. FAIETA",
                "Z. LIN",
                "J. COLLOMOSSE"
            ],
            "title": "HyperNST: Hyper-Networks for Neural Style Transfer",
            "venue": "In European Conference on Computer Vision",
            "year": 2023
        },
        {
            "authors": [
                "G. SONG",
                "L. LUO",
                "J. LIU",
                "W.-C. MA",
                "C. LAI",
                "C. ZHENG"
            ],
            "title": "CHAM T.-J.: Agilegan: stylizing portraits by inversion-consistent transfer learning",
            "venue": "ACM Transactions on Graphics (TOG) 40,",
            "year": 2021
        },
        {
            "authors": [
                "O. TOV",
                "Y. ALALUF",
                "Y. NITZAN",
                "O. PATASHNIK",
                "D. COHENOR"
            ],
            "title": "Designing an encoder for stylegan image manipulation",
            "venue": "ACM Transactions on Graphics (TOG) 40,",
            "year": 2021
        },
        {
            "authors": [
                "N. TUMANYAN",
                "O. BAR-TAL",
                "S. BAGON",
                "T. DEKEL"
            ],
            "title": "Splicing vit features for semantic appearance transfer",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2022
        },
        {
            "authors": [
                "Y. WANG",
                "A. GONZALEZ-GARCIA",
                "D. BERGA",
                "L. HERRANZ",
                "S. KHAN F",
                "D. WEIJER J. V"
            ],
            "title": "Minegan: effective knowledge transfer from gans to target domains with few images",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2020
        },
        {
            "authors": [
                "Y. WANG",
                "C. WU",
                "L. HERRANZ",
                "J. VAN DE WEIJER",
                "A. GONZALEZ-GARCIA",
                "B. RADUCANU"
            ],
            "title": "Transferring gans: generating images from limited data",
            "venue": "In European Conference on Computer Vision",
            "year": 2018
        },
        {
            "authors": [
                "Y. WANG",
                "R. YI",
                "Y. TAI",
                "C. WANG",
                "L. MA"
            ],
            "title": "CtlGAN: Fewshot Artistic Portraits Generation with Contrastive Transfer Learning",
            "year": 2022
        },
        {
            "authors": [
                "S. YANG",
                "L. JIANG",
                "Z. LIU",
                "C. LOY C"
            ],
            "title": "Pastiche Master: Exemplar-Based High-Resolution Portrait Style Transfer",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2022
        },
        {
            "authors": [
                "S. YANG",
                "L. JIANG",
                "Z. LIU",
                "C. LOY C"
            ],
            "title": "Vtoonify: Controllable high-resolution portrait video style transfer",
            "venue": "ACM Transactions on Graphics (TOG) 41,",
            "year": 2022
        },
        {
            "authors": [
                "YI R",
                "LIU Y.-J",
                "LAI Y.-K",
                "ROSIN P. L"
            ],
            "title": "Apdrawinggan: Generating artistic portrait drawings from face photos with hierarchical gans",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2019
        },
        {
            "authors": [
                "J. YANIV",
                "Y. NEWMAN",
                "A. SHAMIR"
            ],
            "title": "The face of art: landmark detection and geometric style in portraits",
            "venue": "ACM Transactions on Graphics (TOG) 38,",
            "year": 2019
        },
        {
            "authors": [
                "C. YANG",
                "Y. SHEN",
                "Z. ZHANG",
                "Y. XU",
                "J. ZHU",
                "Z. WU",
                "B. ZHOU"
            ],
            "title": "One-shot generative domain adaptation",
            "year": 2021
        },
        {
            "authors": [
                "P. ZHU",
                "R. ABDAL",
                "J. FEMIANI",
                "P. WONKA"
            ],
            "title": "Mind the gap: Domain gap control for single shot domain adaptation for generative adversarial networks",
            "venue": "In International Conference on Learning Representations (2022)",
            "year": 2022
        },
        {
            "authors": [
                "P. ZHU",
                "R. ABDAL",
                "Y. QIN",
                "J. FEMIANI",
                "P. WONKA"
            ],
            "title": "Improved stylegan embedding: Where are the good latents? arXiv preprint arXiv:2012.09036 (2020)",
            "year": 2020
        },
        {
            "authors": [
                "R. ZHANG",
                "P. ISOLA",
                "A. EFROS A",
                "E. SHECHTMAN",
                "O. WANG"
            ],
            "title": "The unreasonable effectiveness of deep features as a perceptual metric",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2018
        },
        {
            "authors": [
                "W. ZHENG",
                "Q. LI",
                "X. GUO",
                "P. WAN",
                "Z. WANG"
            ],
            "title": "Bridging clip and stylegan through latent alignment for image editing",
            "year": 2022
        },
        {
            "authors": [
                "Z. ZHANG",
                "Y. LIU",
                "C. HAN",
                "T. GUO",
                "T. YAO",
                "T. MEI"
            ],
            "title": "Generalized one-shot domain adaptation of generative adversarial networks",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2022
        },
        {
            "authors": [
                "Y. ZHANG",
                "Y. WEI",
                "Z. JI",
                "J. BAI",
                "W ZUO"
            ],
            "title": "Towards diverse and faithful one-shot adaption of generative adversarial networks",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "CCS Concepts \u2022 Computing methodologies \u2192 Image processing;"
        },
        {
            "heading": "1. Introduction",
            "text": "Artistic face stylization has been a popular research topic during the past few years, especially in the fields of computer graphics,\n\u2217Joint first authors. \u2020Corresponding author: jackiehuanghaibin@gmail.com\ncomputer vision, and machine learning [HYH\u221721,YNS19,YLLR19]. Given a facial image, artistic stylization methods aim to automatically transform it into a stylized version that is consistent with the input content but in a particular style. It can be used to create captivating visuals for various purposes, such as in the\n\u00a9 2023 Eurographics - The European Association for Computer Graphics and John Wiley & Sons Ltd.\nar X\niv :2\n30 5.\n18 00\nentertainment industry, social media, and virtual reality [ALZ\u221723, Jam18, RBL\u221722].\nHowever, achieving high-quality face stylization is a challenging task due to the complexity of facial structure and humans\u2019 perceptual sensitivity to subtle artifacts in the output. Compared to general image-to-image style transfer tasks, the prior knowledge humans have about facial features poses a significant challenge in preserving the original facial structure while applying the stylization process. Moreover, various downstream application scenarios can further complicate the stylization process, including situations where one model needs to support multiple styles, only a reference style image is given, or even just a simple textual description of the desired style is provided [CUYH20, KY22, LLYW18].\nIn this study, we aim to address the above challenges in generating diverse stylized faces and offer a unified framework called MultiModal Face Stylization (MMFS). MMFS allows for the generation of diverse stylized faces and provides control over the stylization process via either a single reference image or textual description. Our approach is based on three key observations. First, with the advancements in generative models, it is now possible to generate realistic and diverse human faces with fine-grained details using methods such as StyleGAN [KLA19, KLA\u221720]. We can leverage this generative prior as a strong basis for faithful face reconstruction during stylization. Second, we can improve this generative prior by fine-tuning it with large artistic face datasets such as AAHQ [LLQ\u221721], which enables us to support various styles using a single model. The architecture of StyleGAN2 also enables us to disentangle the latent space with respect to semantic attributes, enabling us to control the face content and style separately. Finally, we can leverage the semantic power of CLIP models, which allows us to align textual descriptions with the style latent space and enables us to support cross-domain guided stylization.\nIn light of these observations, we propose an encoder-decoder based architecture with a two-stage training strategy for our MMFS framework. Specifically, the core of MMFS is a StyleGAN-like generator, which is trained to accurately reconstruct realistic input faces in the first stage. Unlike previous methods that use w or w+ space for face reconstruction [AQW19, KLA\u221720], we propose to align the low-resolution layer of StyleGAN2 with a convolution based encoder. Compared with direct projection of the input image into a low-dimensional latent space, a convolution-based encoder preserves fine details in input facial images and results in better reconstructions. In practice, we use a pre-trained StyleGAN2 as our generative prior and fix its weights, while training the encoder to predict the layer at the 32\u00d7 32 resolution in a self-reconstruction manner. Such architecture allows us to leverage the fine-grained details of StyleGAN2 while also preserving important features of the input image. In the second stage, we further fine-tune the entire network with artistic data for stylized face generation. The encoder is trained for style-free semantic and structural feature extraction and the decoder is trained to generate a stylized face from these structural features together with a latent style vector sampled from the latent w space of StyleGAN2. Our design also naturally enables the disentanglement of content and style which can be controlled by the encoder and the decoder separately.\nAfter the two-stage training, we obtain a face stylization frame-\nwork that can transform a real face image into a randomly stylized version. To make the framework controllable for downstream tasks, we train an additional network to bridge the input image or textual guidance to the latent style vector. Specifically, we leverage the pre-trained CLIP model and adopt a four-layer transformer to map its space and the latent style space. Similar to the first stage, this mapping network can be trained using a self-reconstruction strategy, where random samples are generated for CLIP-w paired learning. Importantly, since the image and text are aligned in the CLIP feature space, the learned mapping network can be trained with images only but support textual guidance as well.\nWe conduct experiments to evaluate the effectiveness of MMFS in both one-shot and zero-shot stylization tasks. Our results demonstrate that MMFS is capable of generating high-quality stylized faces while preserving fine-grained details, as shown in Figure 1.\nTo summarize, our contributions are as follows:\n\u2022 We propose MMFS, a novel framework which utilizes StyleGAN2 as a generative prior and integrate it into an encoder-decoder architecture. A two-stage training strategy is further designed to train MMFS for high-quality stylized face generation with diverse style support. \u2022 MMFS is a flexible framework that can be adapted to various downstream tasks, including one-shot and zero-shot stylization. We also introduce a novel loss for better style preservation in one-shot stylizaton. \u2022 Our experimental results demonstrate that MMFS generates better output than existing methods qualitatively and achieves state-ofthe-art performance quantitatively."
        },
        {
            "heading": "2. Related Work",
            "text": "Face generative model. Recent StyleGAN methods [KLA19, KLA\u221720] have substantially improved face generation benchmarks. To accurately reconstruct and manipulate images of real faces, optimized-based GAN inversion methods [AQW19, AQW20] propose several constraint functions to obtain the latent directly. For example, II2S [ZAQ\u221720] introduces a PN space to find the trade-off point of diversity and quality. However, inversion based methods are typically time-consuming. As a result, a series of encoder-based approaches are widely used to achieve fidelity and editability of synthetic portraits efficiently. pSp [RAP\u221721] and e4e [TAN\u221721] adopt a well-designed pretrained encoder network to map portrait images into the latent space. ReStyle [APCO21] utilizes iterative refinement to gradually modify the latent space vector via the residual-based encoder.\nThese inversion techniques have also been introduced for artistic face generation. Toonify [PA20] uses a layer swapping scheme to interpolate between real photos and cartoon images, which preserves the texture characteristic of the style and the structural fidelity of the photo. StyleCariGAN [JJJ\u221721] achieves caricature generation using shape exaggeration blocks to modulate StyleGAN2 feature maps. AniGAN [LZW\u221721] designs special fusion blocks and a doublebranch discriminator to learn domain-specific and domain-shared information. AgileGAN [SLL\u221721] proposes a hierarchical VAE to encode the content into the z+ space to ensure consistency with the prior distribution. However, these methods only transmit spatially\n\u00a9 2023 Eurographics - The European Association for Computer Graphics and John Wiley & Sons Ltd.\nstructural signals through some stacks of one-dimensional vectors, which lead to generation results of limited fidelity. To address this issue, we propose a spatial encoder to modulate the feature map of StyleGAN2 directly.\nMulti-modal face stylization. The diversity of generation results is a key feature of face stylization models. MUNIT [HLBK18] designs two encoder-decoder modules to exchange content and style features via adaptive instance normlization [HB17] to allow the style latent to be sampled. DRIT++ [LTM\u221720] enhances the disentanglement of representations by introducing a domain discriminator. StarGAN v2 [CUYH20] allows image and latent guided generation in a style branch to enrich the diversity of control condition. However, the decoder of these methods cannot achieve high-quality face stylization due to the lack of generative priors. More recently, BlendGAN [LLQ\u221721] transfers the scheme in StyleGAN2 decoder and achieves desirable face stylization results in terms of both quality and variety. Unfortunately, the decoupling ability is naturally limited by the w/w+ space of StyleGAN2, lacking of sufficient spatial information. To address these weaknesses, our approach explicitly decouples the content and style information in StyleGAN2, so that the content structure is not significantly affected while changing the style.\nGuided face stylization. Given out-of-domain style reference images, the model should be able to adapt to the new domain. Some few-shot methods [LZLS20, MCS20, RCKH20, WGGB\u221720, WWH\u221718] focus on transferring to the target domain based on several reference images. For instance, Transferring GANs [WWH\u221718] defines the adaption of generation model by limited data as a transfer task. FreezeD [MCS20] freezes the lower layers of the discriminator to avoid overfitting. FSGA [OLL\u221721] proposes the CDC loss to keep the diversity of generated output. CtlGAN [WYT\u221722] constructs a novel encoder and the CDT loss to perform the stylization task.\nUnder a more strict setting of only one reference image, JoJoGAN [CF22] perturbs the part of inversion latent to augment\nstyle. Mind-The-Gap [ZAFW22] constructs a geometric constraint relation in the CLIP space to shift domains while maintaining the overall structure. Similarly, OneshotCLIP [KY23] makes full use of the semantic consistency of the same portrait in the CLIP space via contrastive learning. DiFa [ZWJ\u221722] designs local and global adaptation functions to enhance the diversity and fidelity. Instead of fine-tuning the entire generator, GenDA [YSZ\u221721] imports two light modules called attribute adaptor and attribute classifier to refine the model. Generalized one-shot adaptation [ZLH\u221722] focuses on preserving decorations in a new domain. HyperNST [RGM\u221723] defines one-shot face stylization as a style transfer problem.\nFor zero-shot face stylization, the desired style information is described by input text, rather than a reference image. StyleGANNADA [GPM\u221722] uses the text direction in the CLIP space as guidance to optimize styles. StyleCLIP [PWS\u221721] adopts a latent mapper to hierarchically adjust the latent. TargetCLIP [CBPW22] adds an essence vector to the source latent to make multiple views in the CLIP space. With another usage of CLIP, CSLA [ZLG\u221722] projects the CLIP embedding into the latent space directly to edit image attributes. To provide feasibility under different conditions, our framework is designed to support both one-shot and zero-shot face stylization simultaneously."
        },
        {
            "heading": "3. Method",
            "text": "In this section, we formally introduce MMFS. As shown in Figure 2, the core of the network architecture is an encoder-decoder framework integrated with StyleGAN2, which has been pre-trained with FFHQ and serves as a generative prior for high-quality face generation. To ensure stable and efficient training of this network, we propose a two-stage training strategy. First, we train the encoder to approximate the low-resolution feature maps of the pre-trained StyleGAN2. Next, we fine-tune the entire encoder-decoder network with artistic portrait images, allowing the network to learn a domain translation from real faces to stylized ones. To enable guided face\n\u00a9 2023 Eurographics - The European Association for Computer Graphics and John Wiley & Sons Ltd.\nstylization, we further train an additional mapping network from the CLIP feature space to the trained latent w+ space. This additional network provides a unified control for both one-shot and zero-shot face stylization."
        },
        {
            "heading": "3.1. Stage I: Encoder Pre-training",
            "text": "As demonstrated in [KLA19,KKC21], the mid- and high-resolution layers of StyleGANs are more related to semantic attributes and details of generated faces, while the low-resolution layers are responsible for controlling facial structure. Based on this observation, the first stage of MMFS is to train the encoder component of the network and to align its output with the 32\u00d7 32 feature maps of StyleGAN2 to ensure faithful reconstruction of the input face.\nDuring the training process, we sample a batch of images Ire f using the pre-trained StyleGAN2 generator Gsty with random noises z. These images and the corresponding noises are then fed into the encoder-decoder framework (E and D), resulting in reconstructed images Irec = D(E(Gsty(z)),z).\nWe use an L1 loss and a perceptual loss to penalize the difference between the reconstructed images and sampled images. To further preserve fine-grained details, we use the discriminator D of StyleGAN2 to evaluate the perceptual loss, as suggested by JoJoGAN [CF22]. Thus, the objective to optimize in this stage is:\nLstage\u22121 =L1(Irec,Ire f ) +\u03bbperc \u2211 l\u2208{ls} L1(Dl(Irec),Dl(Ire f )) (1)\nwhere \u03bbperc is the weight of the perceptual loss term and is set to 4.0 empirically, Dl(\u00b7) denotes the l-th layer features extracted by the discriminator, and {ls} denotes the set of layers of the discriminator to compute the perceptual loss.\nWe compare the reconstruction results of our encoder-decoder network in Stage I with other GAN-inversion and StyleGAN2 encoding methods, including pSp [RAP\u221721], e4e [TAN\u221721], and II2S [ZAQ\u221720]. As shown in Figure 3, our method can faithfully recover fine-grained details including facial features and hair regions."
        },
        {
            "heading": "3.2. Stage II: Encoder and Decoder Fine-tuning",
            "text": "In the second training stage of MMFS, we utilize the selfreconstruction model obtained in the first stage and fine-tune the entire encoder-decoder network for face stylization. In this stage, our goal is to optimize the encoder to extract structural features that are independent of the desired artistic style, while making the decoder produce diverse stylized faces based on these structural features.\nTowards this end, we leverage the following two key components. First, our decoder is StyleGAN-like, which can map a noise z \u2208 N (0,1) into the latent space w and modulate convolution layers to produce diverse and high-fidelity results. Here z only controls the style, since the structural representation is obtained by the encoder from the input image. To generate a stylized face image from a real face image Ir with a noise z sampled from N (0,1), we use an adversarial loss to encourage our generated image to align with the distribution of stylized images Is:\nLadv = EIs(logD(Is)) +EIr ,z(log(1\u2212D(D(E(Ir),z)))) (2)\nwhere D(\u00b7) denotes the prediction of discriminator.\nTo further constrain the structure of the generated image to be similar to the input image, we use DINO-ViT [CTM\u221721] and its self-similarity metric. Specifically, we measure the selfsimilarity of DINO-ViT features to obtain the structure representation [TBTBD22]:\nSl(I)i j = sim(Kli (I),K l j(I)) (3)\nwhere sim(\u00b7, \u00b7) is cosine similarity, and Kli (\u00b7) denotes the keys of the i-th token in the l-th layer of DINO-ViT. We measure the selfsimilarity for both generated image and input image, and encourage them to match each other, using the following structure loss:\nLst = \u2225Sl(D(E(Ir),z))\u2212Sl(Ir)\u2225F , (4)\nwhere \u2225 \u00b7 \u2225F denotes the Frobenius norm. Here we adopt the last transformer layer of DINO-ViT.\nThe full objective in this stage for encoder and decoder fine-tuning is:\nLstage\u22122 = Ladv +\u03bbstLst (5)\nwhere \u03bbst balances the two terms and is set to 0.5. We also leverage R1 regularization as in StyleGAN2 [KLA\u221720]."
        },
        {
            "heading": "3.3. Guided Face Stylization",
            "text": "After the two-stage training, we obtain a fully trained encoderdecoder network that can randomly transform a realistic face image to a stylized version. However, this stylization process is random and beyond control. To enable more precise and controllable stylization, we propose to train an additional mapping network that can yield a latent code in the w+ space from a given style image.\nLearning CLIP mapping. To enable guided stylization, we leverage a pre-trained CLIP model [RKH\u221721] to extract image features and train a mapping network to convert CLIP features to w+ latent codes. Due to the alignment of images and text in the CLIP feature\n\u00a9 2023 Eurographics - The European Association for Computer Graphics and John Wiley & Sons Ltd.\nspace, we can achieve text-guided stylization even if we only train on image data.\nIn our implementation, we use a four-layer transformer as the mapping network, with the input being the CLIP feature and a learnable positional embedding epos \u2208 Rnl\u00d7dc , where nl is the number of latent codes in the w+ space, and dc is the dimension of the CLIP feature. This mapping network is trained in a selfreconstruction manner, with the same loss function as the first stage (Section 3.1). To obtain a reference image for training, we randomly stylize an input image Ir with noise z: Ire f = D(E(Ir),z). We then extract the CLIP features FCLIP(\u00b7) of Ire f , feed them into the mapping network M(\u00b7) to obtain the corresponding latent codes, and reconstruct the image as\nIrec = D(E(Ir),M(FCLIP(Ire f )+ epos)). (6)\nThe weight of the perceptual loss is set to \u03bbperc = 4.0 in this stage. The discriminator trained in the second stage (Section 3.2) is used to measure the perceptual difference.\nZero-shot stylization. Our encoder-decoder based network, which is equipped with a CLIP mapping, supports both text and image guided stylization in a single forward inference. However, due to the limited amount of training data, the network may not always generate high-quality images that match the style prompt. To further improve the stylization results, we introduce a fine-tuning progress that allows us to generate stylized images that are specific to a particular style prompt. In this fine-tuning stage, we use the encoderdecoder network as a prior, while the CLIP mapping plays a role in providing an initial solution.\nGiven a text prompt ptext, we obtain the w+ latent code using CLIP mapping: cw+ = M(FCLIP(ptext)+ epos), and stylize an input image Ir with this latent code: Is = D(E(Ir),cw+). To measure the consistency between Is and ptext, we use the directional CLIP loss [PWS\u221721]. Meanwhile, the structure of Is should be consistent with Ir. Thus, we optimize the following objective:\nL0\u2212shot(ptext) = \u2225Sl(Is)\u2212Sl(Ir)\u2225F + \u03bbc(1\u2212 sim(dCLIP(Is),dCLIP(ptext)))\n(7)\nwhere \u03bbc is the weight of cosine similarity, and dCLIP(\u00b7) denotes the direction from the text/image anchor to CLIP features FCLIP(\u00b7). We freeze all the parameters except those of the decoder during this fine-tuning process.\nOne-shot stylization. For an image prompt pimage, we perform a fine-tuning step similar to the aforementioned zero-shot stylization. To further improve the quality of stylized image, we propose a new loss term that makes the best use of pimage. This new loss is built on the tokens of CLIP ViT image encoder. Specifically, we construct a group of orthogonal basis U \u2208 Rdt\u00d7min(dt ,nt ) (dt and nt denote the dimension and number of tokens) of ViT\u2019s tokens using SVD, which constitutes a feature space that is relevant to pimage. If the stylized image Is has a style similar to the image prompt, the corresponding ViT tokens should remain unchanged as much as possible after being projected onto these orthogonal bases and then projected back. To achieve this goal, we add the following loss term to the fine-tuning\nobjective:\nL1\u2212shot = L0\u2212shot(pimage)+\n\u03bbpro j\u2225UUT T lCLIP(Is)\u2212T l CLIP(Is)\u22251 (8)\nwhere \u03bbpro j controls the impact of the projection loss and T lCLIP(\u00b7) denotes the l-th layer\u2019s tokens of the CLIP ViT image encoder. We use the 4th layer in all of our experiments."
        },
        {
            "heading": "4. Experiments",
            "text": "Our proposed method, MMFS, is capable of generating stylized face images with high-quality and large diversity. In this section, we first describe our evaluation setups and then compare MMFS with several state-of-the-art baseline methods both qualitatively and quantitatively to evaluate the effectiveness of our approach in various stylization tasks."
        },
        {
            "heading": "4.1. Experimental Setup and Implementation Details",
            "text": "Baseline methods. We compare our MMFS with several previous methods on random stylization, including MUNIT [HLBK18], DRIT++ [LTM\u221720], and BlendGAN [LLQ\u221721]. All of them are trained on FFHQ [KLA19] and AAHQ [LLQ\u221721] datasets. For guided stylization, we conduct qualitative comparisons with MTG [ZAFW22], DiFa [ZWJ\u221722], JoJoGAN [CF22] and BlendGAN [LLQ\u221721] on one-shot stylization, and compare our method with StyleGAN-NADA [PWS\u221721] and CLIPstyler [KY22] on zero-shot stylization.\nEvaluation metrics. We evaluate our method quantitatively from three perspectives: quality, identity-preservation, and diversity. To measure the quality, we use Frechet Inception Distance (FID) [HRU\u221717] to calculate the difference between the stylized images and samples from AAHQ. A lower FID score indicates a\n\u00a9 2023 Eurographics - The European Association for Computer Graphics and John Wiley & Sons Ltd.\nbetter stylization quality. For identity-preservation, we use the cosine similarity of Arcface [DGXZ19] features to evaluate the variation of face identity before and after stylization. To make this metric more intuitive, we use one minus the cosine similarity (Arcface-Dist). A lower Arcface-Dist value indicates better preservation of face identity. To measure diversity, we use LPIPS [ZIE\u221718] to calculate the perceptual difference between two randomly stylized images. A higher LPIPS value indicates larger diversity in stylization results.\nNetwork training. We implement our method in PyTorch and train the network with Adam optimizer using a batch size of 8. We train the network with 10000, 90000, 60000 iterations at Stage I, Stage II, and CLIP mapping, respectively. For one-shot and zero-shot finetuning, we find 200 iterations are sufficient to achieve reasonable results and avoid over-fitting. In both Stages I and II, we set the learning rate to be 0.001, \u03b21 = 0.1 and \u03b22 = 0.999 for the optimizer. During the learning of CLIP mapping and zero/one-shot fine-tuning, the \u03b21 is set to 0.9 and the learning rate is decreased to 0.0002. Moreover, we apply an EMA decay of 0.999 (Stage I, Stage II, and CLIP mapping) or 0.99 (zero/one-shot fine-tuning) for our network except for the discriminator to stabilize the training process.\nTiming statistics. The training process takes about 30 minutes per 1000 iterations. At test time, the image encoding, text encoding, random sampling, and one/zero-shot stylization step each takes 6.54, 6.59, 15.59, and 15.85 milliseconds on average, measured with single NVIDIA Tesla V100 GPU."
        },
        {
            "heading": "4.2. Random Stylization",
            "text": "We randomly select 10,000 face images from the FFHQ dataset, and feed them, in addition, the style code z sampled from N (0,1), to each method. Table 1 shows the quantitative comparison between our approach and previous methods. Note that the blending indicator of BlendGAN is set to 6 for a trade-off between stylization strength and structural consistency. The stylized images of MUNIT and DRIT++ suffer from artifacts and blurriness while BlendGAN produces high-fidelity faces but of less noticeable style and lower diversity due to the above trade-off. In comparison, our method reaches a better balance of consistency and stylization strength, achieving both the best visual quality and diversity, as shown in Figure 4, thanks to the generative prior and our encoder-decoder based framework.\n\u00a9 2023 Eurographics - The European Association for Computer Graphics and John Wiley & Sons Ltd."
        },
        {
            "heading": "4.3. Guided Stylization",
            "text": "One-shot stylization. In our evaluation of one-shot stylization, we compare our proposed MMFS with state-of-the-art methods, including MTG, JoJoGAN, DiFa, and BlendGAN. The results are shown in Figure 5. From the visual comparison, we can observe that MTG and JoJoGAN tend to generate twisty facial features when given slightly irregular style images, while the eye\u2019s orientation of their stylized faces is always under the influence of reference. The overall style strength of DiFa is relatively weaker than others except for BlendGAN, and the eyes rolling back frequently occurs in its stylized faces.\nAdditionally, all these methods suffer from a common dilemma: they are all built on StyleGAN2, and thus a GAN inversion is indispensable to project the input image onto the StyleGAN2 w+ space. Unfortunately, this inversion is not perfect and leads to the loss of structure. Furthermore, some methods (e.g., MTG and BlendGAN) rely on style-mixing, which can aggravate structural changes or weaken the style. In contrast, our proposed method\nintegrates a StyleGAN2 into the encoder-decoder framework, allowing for both faithful reconstruction and high-quality stylization.\nZero-shot stylization. In Figure 6, we present qualitative comparisons between our proposed MMFS and state-of-the-art methods on zero-shot stylization. Similar to StyleGAN-based methods in one-shot stylization, StyleGAN-NADA suffers from significant modifications of structure and identity in its stylized faces. Furthermore, StyleGAN-NADA often generates unwanted colorful stripes in the background region of its generated images. Meanwhile, CLIPstyler adopts an encoder-decoder framework without the need for GAN inversion, ensuring more faithful reconstruction of input faces. However, its decoder and training strategy restrict its ability to generate high-quality face images, resulting in local artifacts. In contrast, our approach leverages a two-stage training strategy and integrates StyleGAN2 into the encoder-decoder framework, allowing for both faithful reconstruction of input faces and highquality stylization, resulting in superior performance in zero-shot stylization. It is also worth mentioning that recent diffusion model based methods [DN21,ND21] have demonstrated superior performance for zero-shot stylization. However, it remains challenging\n\u00a9 2023 Eurographics - The European Association for Computer Graphics and John Wiley & Sons Ltd.\nfor these methods to maintain facial structure of input during the image-to-image translation process.\nStyle interpolation. Thanks to the StyleGAN-like decoder, our style code in the w+ space can be interpolated naturally. Figure 7 shows the visual results of style interpolation, in which the style of generated images gradually changes from one to another."
        },
        {
            "heading": "4.4. Ablation Study",
            "text": "Training strategy. To evaluate the advantage of our proposed training strategy, we conduct an experiment in which we remove the generative prior and train the entire encoder-decoder from scratch in Stage II. In Figure 8, we show the FID scores with respect to the training iterations. We observe that training from scratch leads to slower convergence and higher FID scores compared to our twostage training strategy. By leveraging the pre-trained StyleGAN2 and the encoder in Stage I, our fine-tuning process in Stage II converges faster and achieves consistently lower FID scores.\nProjection loss. Figure 9 demonstrates the positive impact of the projection loss proposed for one-shot stylization. While the directional CLIP loss encourages global semantic alignment between the stylized image and the style prompt, it may still not be sufficient enough to provide guidance for low-level features and fine-grained details. To further refine results in one-shot scenarios, our projection loss uses ViT tokens from the style reference to provide additional patch-level clues.\nFine-tuning. As shown in Figure 10, the additional fine-tuning for zero-shot and one-shot stylization results in stylized images that better match the style prompt. When using only the CLIP mapping, the stylized results are reasonable, but the color tone and texture details may not always be consistent with the guidance. Fine-tuning allows for the direction of the text or image prompt in the CLIP space to provide a global cue to improve these issues.\nCLIP features. We further conduct an experiment to validate the impact of different CLIP [RKH\u221721] layers in one-shot fine-tuning. As shown in Figure 11, shallower layers contain more low-level features, leading to better color match, while deeper layers are more semantic. We applied the proposed projection loss on deep layers which provides similar guidance as directional CLIP loss. We currently choose the 4th layer of CLIP features for a trade-off between low-level and semantic features, which works well in our experiments."
        },
        {
            "heading": "4.5. Limitations",
            "text": "Although MMFS achieves high-quality face stylization with large diversity in all the cases of random/one-shot/zero-shot stylization, there is still potential for further improvement. Specifically, our current implementation does not support significant geometric\n\u00a9 2023 Eurographics - The European Association for Computer Graphics and John Wiley & Sons Ltd.\ndeformation such as caricature, as shown in Figure 12. This issue may be addressed or alleviated by incorporating a learnable deformation module into the network. Moreover, our generated images are limited within the cropped region of FFHQ. Our method also has limited performance for input with large pose variations or significant occlusions that are out-of-distribution of FFHQ, as shown in Figure 13. Integrating our MMFS with VToonify [YJLL22b] is a promising future direction to unlock the power for full-size image stylization."
        },
        {
            "heading": "5. Conclusion",
            "text": "In this study, we present MMFS as a novel framework that leverages the strengths of StyleGAN2 and an encoder-decoder architecture to generate high-quality and diverse stylized faces while preserving fine-grained details. With our two-stage training strategy and CLIP-guided mapping network, MMFS offers a flexible and effective approach for one-shot and zero-shot stylization tasks. Our experimental results demonstrate that MMFS outperforms state-ofthe-art methods by a large margin in both quantitative and qualitative evaluations. Overall, MMFS presents a promising solution for artistic face stylization with a wide range of potential applications. It is also worth exploring to expand this generative prior based framework to other applications beyond face stylization.\nAcknowledgements. We thank the anonymous reviewers for their valuable feedback and constructive suggestions."
        }
    ],
    "title": "Multi-Modal Face Stylization with a Generative Prior",
    "year": 2023
}