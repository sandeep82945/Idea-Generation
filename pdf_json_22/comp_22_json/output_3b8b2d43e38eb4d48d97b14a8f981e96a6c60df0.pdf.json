{
    "abstractText": "Huge pretrained language models (LMs) have demonstrated surprisingly good zero-shot capabilities on a wide variety of tasks. This gives rise to the appealing vision of a single, versatile model with a wide range of functionalities across disparate applications. However, current leading techniques for leveraging a \u201cfrozen\u201d LM\u2014i.e., leaving its weights untouched\u2014still often underperform fine-tuning approaches which modify these weights in a task-dependent way. Those, in turn, suffer forgetfulness and compromise versatility, suggesting a tradeoff between performance and versatility. The main message of this paper is that current frozenmodel techniques such as prompt tuning are only the tip of the iceberg, and more powerful methods for leveraging frozen LMs can do just as well as fine tuning in challenging domains without sacrificing the underlying model\u2019s versatility. To demonstrate this, we introduce three novel methods for leveraging frozen models: input-dependent prompt tuning, frozen readers, and recursive LMs, each of which vastly improves on current frozen-model approaches. Indeed, some of our methods even outperform fine-tuning approaches in domains currently dominated by the latter. The computational cost of each method is higher than that of existing frozen model methods, but still negligible relative to a single pass through a huge frozen LM. Each of these methods constitutes a meaningful contribution in its own right, but by presenting these contributions together we aim to convince the reader of a broader message that goes beyond the details of any given method: that frozen models have untapped potential and that fine-tuning is often unnecessary.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yoav Levine"
        },
        {
            "affiliations": [],
            "name": "Itay Dalmedigos"
        },
        {
            "affiliations": [],
            "name": "Ori Ram"
        },
        {
            "affiliations": [],
            "name": "Yoel Zeldes"
        },
        {
            "affiliations": [],
            "name": "Daniel Jannai"
        },
        {
            "affiliations": [],
            "name": "Dor Muhlgay"
        },
        {
            "affiliations": [],
            "name": "Yoni Osin"
        },
        {
            "affiliations": [],
            "name": "Opher Lieber"
        },
        {
            "affiliations": [],
            "name": "Barak Lenz"
        },
        {
            "affiliations": [],
            "name": "Shai Shalev-Shwartz"
        },
        {
            "affiliations": [],
            "name": "Amnon Shashua"
        },
        {
            "affiliations": [],
            "name": "Kevin Leyton-Brown"
        },
        {
            "affiliations": [],
            "name": "Yoav Shoham"
        }
    ],
    "id": "SP:5313c03dd1fe1a02a639afb37071302f7a10098a",
    "references": [
        {
            "authors": [
                "Vamsi Aribandi",
                "Yi Tay",
                "Tal Schuster",
                "Jinfeng Rao",
                "Huaixiu Steven Zheng",
                "Sanket Vaibhav Mehta",
                "Honglei Zhuang",
                "Vinh Q Tran",
                "Dara Bahri",
                "Jianmo Ni"
            ],
            "title": "Ext5: Towards extreme multi-task scaling for transfer learning",
            "venue": "arXiv preprint arXiv:2111.10952,",
            "year": 2021
        },
        {
            "authors": [
                "Tom B Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "year": 2005
        },
        {
            "authors": [
                "Danqi Chen",
                "Adam Fisch",
                "Jason Weston",
                "Antoine Bordes"
            ],
            "title": "Reading Wikipedia to answer open-domain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
            "year": 2017
        },
        {
            "authors": [
                "Jean-Baptiste Cordonnier",
                "Andreas Loukas",
                "Martin Jaggi"
            ],
            "title": "On the relationship between selfattention and convolutional layers",
            "venue": "arXiv preprint arXiv:1911.03584,",
            "year": 2019
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2019
        },
        {
            "authors": [
                "Martin Fajcik",
                "Martin Docekal",
                "Karel Ondrej",
                "Pavel Smrz"
            ],
            "title": "R2-D2: A modular baseline for open-domain question answering",
            "venue": "Punta Cana, Dominican Republic,",
            "year": 2021
        },
        {
            "authors": [
                "Neil Houlsby",
                "Andrei Giurgiu",
                "Stanislaw Jastrzebski",
                "Bruna Morrone",
                "Quentin De Laroussilhe",
                "Andrea Gesmundo",
                "Mona Attariyan",
                "Sylvain Gelly"
            ],
            "title": "Parameter-efficient transfer learning for nlp",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Edward J Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "venue": "arXiv preprint arXiv:2106.09685,",
            "year": 2021
        },
        {
            "authors": [
                "Gautier Izacard",
                "Edouard Grave"
            ],
            "title": "Leveraging passage retrieval with generative models for open domain question answering",
            "venue": "arXiv preprint arXiv:2007.01282,",
            "year": 2020
        },
        {
            "authors": [
                "Gautier Izacard",
                "Edouard Grave"
            ],
            "title": "Distilling knowledge from reader to retriever for question answering",
            "venue": "CoRR, abs/2012.04584,",
            "year": 2020
        },
        {
            "authors": [
                "Andrew Jaegle",
                "Sebastian Borgeaud",
                "Jean-Baptiste Alayrac",
                "Carl Doersch",
                "Catalin Ionescu",
                "David Ding",
                "Skanda Koppula",
                "Daniel Zoran",
                "Andrew Brock",
                "Evan Shelhamer"
            ],
            "title": "Perceiver io: A general architecture for structured inputs & outputs",
            "venue": "arXiv preprint arXiv:2107.14795,",
            "year": 2021
        },
        {
            "authors": [
                "Feihu Jin",
                "Jinliang Lu",
                "Jiajun Zhang",
                "Chengqing Zong"
            ],
            "title": "Instance-aware prompt learning for language understanding and generation",
            "venue": "arXiv preprint arXiv:2201.07126,",
            "year": 2022
        },
        {
            "authors": [
                "Vladimir Karpukhin",
                "Barlas Oguz",
                "Sewon Min",
                "Patrick Lewis",
                "Ledell Wu",
                "Sergey Edunov",
                "Danqi Chen",
                "Wen-tau Yih"
            ],
            "title": "Dense passage retrieval for open-domain question answering",
            "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 6769\u20136781,",
            "year": 2020
        },
        {
            "authors": [
                "Diederik Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Tom Kwiatkowski",
                "Jennimaria Palomaki",
                "Olivia Redfield",
                "Michael Collins",
                "Ankur Parikh",
                "Chris Alberti",
                "Danielle Epstein",
                "Illia Polosukhin",
                "Jacob Devlin",
                "Kenton Lee",
                "Kristina Toutanova",
                "Llion Jones",
                "Matthew Kelcey",
                "Ming-Wei Chang",
                "Andrew M. Dai",
                "Jakob Uszkoreit",
                "Quoc Le",
                "Slav Petrov"
            ],
            "title": "Natural questions: A benchmark for question answering research",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Kenton Lee",
                "Ming-Wei Chang",
                "Kristina Toutanova"
            ],
            "title": "Latent retrieval for weakly supervised open domain question answering",
            "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Brian Lester",
                "Rami Al-Rfou",
                "Noah Constant"
            ],
            "title": "The power of scale for parameter-efficient prompt tuning",
            "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Yoav Levine",
                "Noam Wies",
                "Daniel Jannai",
                "Dan Navon",
                "Yedid Hoshen",
                "Amnon Shashua"
            ],
            "title": "The inductive bias of in-context learning: Rethinking pretraining example design",
            "venue": "In 10th International Conference on Learning Representations (ICLR),",
            "year": 2022
        },
        {
            "authors": [
                "Xiang Lisa Li",
                "Percy Liang"
            ],
            "title": "Prefix-tuning: Optimizing continuous prompts for generation",
            "venue": "arXiv preprint arXiv:2101.00190,",
            "year": 2021
        },
        {
            "authors": [
                "Opher Lieber",
                "Or Sharir",
                "Barak Lenz",
                "Yoav Shoham"
            ],
            "title": "Jurassic-1: Technical details and evaluation",
            "venue": "White Paper",
            "year": 2021
        },
        {
            "authors": [
                "Xiao Liu",
                "Kaixuan Ji",
                "Yicheng Fu",
                "Zhengxiao Du",
                "Zhilin Yang",
                "Jie Tang"
            ],
            "title": "P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks",
            "venue": "arXiv preprint arXiv:2110.07602,",
            "year": 2021
        },
        {
            "authors": [
                "Sewon Min",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Hannaneh Hajishirzi"
            ],
            "title": "Metaicl: Learning to learn in context",
            "venue": "arXiv preprint arXiv:2110.15943,",
            "year": 2021
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeff Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll L Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "arXiv preprint arXiv:2203.02155,",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jeff Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "arXiv preprint arXiv:1910.10683,",
            "year": 2019
        },
        {
            "authors": [
                "Ori Ram",
                "Gal Shachaf",
                "Omer Levy",
                "Jonathan Berant",
                "Amir Globerson"
            ],
            "title": "Learning to retrieve passages without supervision",
            "venue": "In North American Association for Computational Linguistics (NAACL),",
            "year": 2022
        },
        {
            "authors": [
                "Sylvestre-Alvise Rebuffi",
                "Hakan Bilen",
                "Andrea Vedaldi"
            ],
            "title": "Learning multiple visual domains with residual adapters",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Adam Roberts",
                "Colin Raffel",
                "Noam Shazeer"
            ],
            "title": "How much knowledge can you pack into the parameters of a language model",
            "venue": "arXiv preprint arXiv:2002.08910,",
            "year": 2020
        },
        {
            "authors": [
                "Stephen Robertson",
                "Hugo Zaragoza"
            ],
            "title": "The probabilistic relevance framework: BM25 and beyond",
            "venue": "Found. Trends Inf. Retr.,",
            "year": 2009
        },
        {
            "authors": [
                "Victor Sanh",
                "Albert Webson",
                "Colin Raffel",
                "Stephen H Bach",
                "Lintang Sutawika",
                "Zaid Alyafeai",
                "Antoine Chaffin",
                "Arnaud Stiegler",
                "Teven Le Scao",
                "Arun Raja"
            ],
            "title": "Multitask prompted training enables zero-shot task generalization",
            "venue": "arXiv preprint arXiv:2110.08207,",
            "year": 2021
        },
        {
            "authors": [
                "Or Sharir",
                "Barak Peleg",
                "Yoav Shoham"
            ],
            "title": "The cost of training nlp models: A concise overview",
            "venue": "arXiv preprint arXiv:2004.08900,",
            "year": 2020
        },
        {
            "authors": [
                "Devendra Singh",
                "Siva Reddy",
                "Will Hamilton",
                "Chris Dyer",
                "Dani Yogatama"
            ],
            "title": "End-to-end training of multi-document reader and retriever for open-domain question answering",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Tianyi Tang",
                "Junyi Li",
                "Wayne Xin Zhao"
            ],
            "title": "Context-tuning: Learning contextualized prompts for natural language generation",
            "venue": "arXiv preprint arXiv:2201.08670,",
            "year": 2022
        },
        {
            "authors": [
                "Tu Vu",
                "Brian Lester",
                "Noah Constant",
                "Rami Al-Rfou",
                "Daniel Cer"
            ],
            "title": "Spot: Better frozen model adaptation through soft prompt transfer",
            "venue": "arXiv preprint arXiv:2110.07904,",
            "year": 2021
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Y Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M Dai",
                "Quoc V Le"
            ],
            "title": "Finetuned language models are zero-shot learners",
            "venue": "arXiv preprint arXiv:2109.01652,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "The best way these days to optimize performance for a given NLP task is usually to fine tune a pretrained LM. A side effect of doing so is that performance degrades significantly on other tasks. Partly in response, considerable recent work has been devoted to fine tuning huge LMs simultaneously on many (in some cases, over 100) curated NLP tasks (Sanh et al., 2021; Wei et al., 2021; Min et al., 2021; Aribandi et al., 2021; Ouyang et al., 2022). These formidable efforts have been effective in the sense that they have produced models that exhibit high performance on inputs taken from any of the curated tasks, and, indeed, from other similar tasks.\nHowever, fine tuning the LM, even in the above \u201cmassively multi-tasked\" settings, limits the versatility and extensibility of the resulting model. We argue that versatile natural language interfaces can be built on top of frozen LMs. This approach offers two key advantages over multi-task fine-tuned models.\n1. Non-forgetfulness: Once the original LM is fine tuned on any multi-task suite, it can suffer from catastrophic forgetfulness on capabilities far enough from these tasks (manifesting, for example, in perplexity degradation). A frozen LM will never suffer forgetfulness, since it remains unchanged.\nar X\niv :2\n20 4.\n10 01\n9v 1\n[ cs\n.C L\n] 2\n1 A\npr 2\n2. Extensibility: When attempting to add a new task to a fine-tuned LM, there is no guarantee that performance on the original task suite will be retained, so the model must be retrained on all tasks together. Given the cost of training such models\u2014in some cases, millions of dollars (Sharir et al., 2020)\u2014it is clearly infeasible to do so repeatedly. In contrast, when adding new capabilities as new external components over a frozen backbone, there is no cross interference between capabilities.\nOf course, we are not the first to notice these compelling advantages. Some of the leading approaches for leveraging frozen models include prompt tuning (Lester et al., 2021), prefix tuning (Li & Liang, 2021), adapter tuning (Rebuffi et al., 2017; Houlsby et al., 2019), and low rank adaptation (Hu et al., 2021). All of these methods share the idea of training a very small number of parameters around a frozen model to achieve optimized performance on given tasks. However, while these techniques are able to reach fine-tuning performance for certain tasks, state-of-the-art performance in many practical settings is still based on fine-tuned models (e.g., Wei et al. (2021); Fajcik et al. (2021)).\nTo demonstrate that frozen LMs still have considerable untapped potential, our general approach is to design more ambitious external scaffolding that can squeeze more out of a frozen LM. The key observation is that existing frozen LM methods are so compact that there is room to expand them significantly while still paying a negligible price relative to the single pass through the huge LM.\nWe focus on two settings in which the go-to standard is still fine-tuned models. The first, already discussed above, is massive multi-tasking: asking a single model to simultaneously address many NLP tasks. The variety of existing multi-tasked models are all fine tuned; no frozen model method has been considered in this setting. Our second setting is a challenging individual task, in which leading methods are all fine tuned (Chen et al., 2017; Lee et al., 2019; Karpukhin et al., 2020; Roberts et al., 2020): open-domain question answering, asking a model to answer general-knowledge questions. Open-domain question answering has two popular variants: \u201copen book\u201d, in which the model is given access to documents retrieved from a predefined corpus (web, books, proprietary corpora) that are likely to contain the information relevant to a given input, and \u201cclosed book\u201d, in which the model is trained to reply to the input query with no auxiliary information.\nWe show that even in challenging settings such as massive multi-tasking or open-domain question answering (in either its open- or closed-book variants), a single frozen LM can compete with leading fine-tuning approaches. In order to show this, we introduce three different frozen-model methods. While each of these contributions might serve as the core of a standalone paper, we bundle them together to convey what we see as a bigger message: that frozen LMs have considerable potential that is underutilized by current frozen-model approaches. This matters because, beyond our finding that our methods can match or exceed the performance of specialized fine-tuning techniques in challenging domains, there are other advantages to leveraging frozen LMs: notably, avoiding the considerable cost of training and serving many different specialized models for different use cases; and retaining the LM\u2019s versatility, non-forgetfulness, and extensibility.\nThe following sections describe three novel methods for building powerful neural systems around frozen models.\nInput-dependent prompt-tuning for massively multi-tasking a frozen LM (Section 2). We show that the gains achieved by fine tuning in the multi-task setup can be achieved with a frozen LM and a much smaller trained encoder that generates an input-specific neural prompt. Specifically, we slightly exceed the multi-tasking performance of T0++ (Sanh et al., 2021), a fine-tuned 11B-parameter model, with an input-dependent prompt-tuned J1-Large (Lieber et al., 2021), a frozen 7B-parameter model. This eliminates the need not only for expensive multi-task fine tuning, but can also reduce the cost of hosting multiple models. For example, rather than hosting both a massively multi-tasked model and an LM separately to support different use cases, a single huge LM can be maintained along with a small encoder that can prompt it per input when needed.\nRetrieval-augmented generation with a huge frozen LM (Section 3). In the open-book variant of the open-domain question-answering setting, the answer generator typically attends to 100+ retrieved documents, and is therefore called a reader. Current readers are fine tuned for this longcontext functionality. Because it is prohibitively expensive to fine tune huge models to attend to 100+ retrieved documents, readers tend to be relatively small, typically having fewer than 1B parameters. We introduce huge LMs into this pipeline as frozen readers. To do so, we use a re-ranking stage to\ncondense relevant information from 100+ retrieved documents into the input sequence length of the frozen LM reader. We show that frozen LMs can reach and surpass leading fine tuning approaches on Natural Questions, a prominent open-domain question answering benchmark.\nRecursively applying a frozen LM (Section 4). A huge LM is a powerful and highly expensive resource, but existing approaches use this resource only once per input query. We show that a single pass through the LM extracts useful information in the closed-book variant of open-domain question answering (no retrieved documents), but leaves considerable additional information unexploited. As a result, we are able to achieve large performance gains of 4.4 points in the closed-book setting of Natural Questions by performing two consecutive passes through a single frozen LM, as compared to performing a single pass through the same frozen LM. We describe two ways of instantiating this broad idea, which we dub LM recursion: textual LM recursion (Section 4.1) and neural LM recursion (Section 4.2). The effectiveness of these methods raises the economically disruptive possibility of paying for better performance at evaluation time rather than at training time. That is, rather than pretraining an enormous model that must be used on all inputs, one might vary the number of passes through a single frozen model based on an assessment of the input\u2019s difficulty."
        },
        {
            "heading": "2 INPUT-DEPENDENT PROMPT TUNING FOR MULTI-TASKING A FROZEN LM",
            "text": "While huge pretrained LMs often exhibit impressive diverse zero-shot performance, the practice of massively multi-tasking an LM via fine tuning it simultaneously on many diverse NLP tasks has been shown to dramatically improve performance across tasks and domains. For example, Sanh et al. (2021) and Aribandi et al. (2021) fine tuned the 11B parameter T5 model (Raffel et al., 2019) on their curated suites of 62 and 107 datasets, respectively, and present two new multi-tasked models called T0 and EX-T5, respectively. Wei et al. (2021) fine tuned Google\u2019s internal 137B parameter pretrained LM on their curated suite of 60 datasets, producing a multi-tasked model called FLAN. Min et al. (2021) fine tuned the 770M parameter GPT2 (Radford et al., 2019) on a curated suite of 142 datasets, and Ouyang et al. (2022) fine tuned the 175B parameter GPT3 (Brown et al., 2020) on disparate datasets of human instructions, using reinforcement learning from human feedback, producing a new multi-tasked InstructGPT model.\nBelow, we present an approach that we call input-dependant prompt tuning (ID-PT) for massively multi-tasking an LM while keeping it frozen. ID-PT trains a very small external network that receives an input from one of the many curated datasets, and creates a neural prompt on-the-fly that best prepares the frozen LM for addressing this input (see Figure 1). We conducted experiments using the training set of Sanh et al. (2021) and comparing to their model, since both are publicly available. We performed ID-PT on a frozen 7B parameter J1-Large model and reach the performance of Sanh et al\u2019s fine-tuned 11B parameter T0++ model after training on only half of their training examples. We believe that this demonstrates that there is no real need for the above mentioned multitude of huge fine-tuned LMs targeting the multi-task domain. One can maintain and serve a single frozen LM as a backbone, and perform ID-PT to externally tune it on different task suites. Moreover, as we show in later sections, this enables a new workflow in which a single huge LM is deployed to support a wide range of different NLP applications.\n2.1 THE ID-PT ARCHITECTURE\nThe prompt-tuning method of Lester et al. (2021) is a simple and effective method for externally tuning a frozen model. For a given task, a fixed number of continuous token embeddings is optimized when concatenated to the input embeddings of each training example (illustrated in Figure 1a). When trained on a single dataset (and when given access to a largeenough model), prompt tuning has been shown to yield performance competitive with fine tuning (Lester et al., 2021; Liu et al., 2021). This is an inspiring finding, since the fraction of parameters trained during prompt tuning is tiny\nrelative to full model size (\u223c 0.001%). However, as we show below, prompt tuning falls short of fine tuning in the multi-task domain.\nWe conjecture that this occurs because the trained prompt embeddings are shared across all tasks in the diverse multi-task suite. Indeed, Vu et al. (2021) show that the prompt embeddings learned for disparate NLP tasks are far apart in embedding space, suggesting that no single prompt embedding would perform well for a wide range of different tasks.\nOur input-dependant prompt tuning (ID-PT) method aims to address this potential shortcoming. It creates prompt embeddings as a function of the input, thus allowing the prompt to vary substantially across tasks. Specifically, whereas regular prompt tuning involves directly training a prompt embedding p, in ID-PT a prompt generator is trained to receive the input x and produce an input dependent prompt p(x) (illustrated in Figure 1b). The prompt generation network itself must therefore have access to a good representation of natural language in order to discern between inputs representing different functionalities. We constructed our prompt generator around a small T5-base encoder (Raffel et al., 2019), thereby leveraging its language encoding abilities acquired via extensive pretraining.\nFigure 2 expands on the prompt generator architecture, which consists of 3 components: (1) a frozen T5-base encoder; (2) a learned prompt for prompt-tuning the frozen T5 encoder for its functionality within the prompt generator (overall 330K learned parameters); and (3) a learned cross-attention network that translates the variable-length output sequence of the T5 encoder (of length equal to the length of input x) into a fixed length prompt p(x). The crossattention network, operating in the hidden dimension of T5-base, 768, was implemented following Jaegle et al. (2021), first producing a fixed-length output sequence by using a cross attention layer with a fixed number of query vectors (overall 7M learned parameters), and then applying 2 self-attention layers over the fixed length sequence (overall 2 \u00d7 7M learned parameters). Finally, a shared learned matrix (overall 3M learned parameters) expanded the resulting representations into the hidden dimension of J1Large, 4096, finally producing p(x).\nThis prompt generator architecture illustrates and makes concrete some of our earlier advocacy for enhancing the processing that takes place outside huge LMs. During prompt tuning, we trained \u223c 1.6M parameters, while in contrast implementing ID-PT required training \u223c 25M parameters in addition to leveraging a \u223c 110M parameter frozen T5 encoder. While ID-PT is thus considerably more \u201cheavy weight\u201d than prompt tuning, ID-PT is still a very small increment to the frozen model, adding \u223c 0.3% to the number of parameters and \u223c 1% to inference time."
        },
        {
            "heading": "2.2 EXPERIMENTAL SETUP",
            "text": "We considered P3 (Sanh et al., 2021), a publicly available multi-task suite that includes 62 NLP datasets grouped into 12 task types (a full list of these datasets appears in the appendix). For each dataset, the P3 suite includes various natural language prompt formats, referred to as templates, which represent diverse natural manners of presenting and addressing the NLP query (e.g., in the case of a natural language inference dataset, a template could be: \u201cIf {Premise} is true, is it also true that {Hypothesis}?\"). We also leveraged T0, a model based on T5 that was fine tuned on the P3 training set by the same authors. More specifically, they released three models, dubbed T0, T0+, and T0++, which they fine tuned on 39, 49, and 55 of the P3 tasks, respectively.1\n1Sanh et al. (2021) focused on measuring generalization to unseen tasks and therefore they did not train on all 62 datasets.\nWe perform ID-PT for a frozen 7B parameter J1-Large model on the released training data of T0++ and compare its performance to the released 11B parameter T0++ model.\nWe followed the same training protocol of Sanh et al. (2021). Specifically, we combined and shuffled the examples from all 55 training sets into a single training set, treating any dataset with over 500,000 examples as having 500,000 / NUM-TEMPLATES examples for the purposes of sampling, where NUM-TEMPLATES is the number of different natural language templates created for the dataset. We performed checkpoint selection by choosing the checkpoint that yielded the highest average score on the development sets. We truncated input and output sequences to fit into J1-Large\u2019s 2048- token context window. We used a batch size of 32, and trained the prompt generator via the Adam optimizer (Kingma & Ba, 2014) with parameters \u03b21 = 0.9, \u03b22 = 0.95, = 10\u22126, and weight decay of 0.1.\nWe experimented with different fixed prompt lengths at the output of the prompt generator. This quantity reflects the capacity of the interface between the externally trained prompt generator and the frozen J1-Large LM. Lester et al. (2021) experimented with prompt lengths of up to 150 for the single-task prompt-tuning setup. In this multi-tasked setting, in which the prompt space should facilitate tuning the frozen LM into a wider range of functionalities, we studied the effect of using longer prompts, considering lengths in {100, 200, 300, 400}. For each prompt length, we explored learning rates in {5 \u00b710\u22125, 7.5 \u00b710\u22125, 1.5 \u00b710\u22124} for runs limited to 10% of the overall T0++ training, which amounted to 125K training steps given our batch size.2 After performing 10% of the overall T0++ training, we continued training with the prompt length and learning rate that got the best average score on the development sets. We trained prompt tuning, the frozen model method baseline to ID-PT, with a batch size of 32, and trained via the Adam optimizer (Kingma & Ba, 2014) with parameters \u03b21 = 0.9, \u03b22 = 0.999, = 10\u22126, and weight decay of 0.\nFor evaluation, we closely followed the technical protocol of Sanh et al. (2021), except that we report the scores on all tasks with publicly available development or test sets, while they evaluated only held-out tasks. We do so because even though Sanh et al. (2021) focus on zero-shot generalization, the benefits of massive multi-tasking reported by the above-mentioned studies pertain also to the many tasks within the multi-task training suites. Overall, 7 datasets do not include publicly released development sets and a different set of 9 datasets do not include publicly released test sets (full details on these sets appear in the appendix). Therefore, while we train on all datasets, we report scores on development or test sets only in the cases of datasets for which the respective splits exist. Following Sanh et al. (2021), for a given dataset, we report the median performance across all of the dataset\u2019s natural language prompt templates.\nOur goal in this section is to demonstrate that frozen models can be massively multi-tasked without compromising performance relative to fine tuning. However, beyond the facts that T0++ was fine tuned on the P3 multi-task suite and that we kept J1-Large frozen and fit parameters external to it, the two models differ in several other aspects. Most importantly: (1) T0++ is a 11B parameter encoder-decoder model, while J1-Large is a 7B parameter decoder-only model; (2) T0++ is initialized from the LM-adapted T5 model of Lester et al. (2021), which was pretrained on 1.1 trillion tokens, while J1-Large was pretrained on 300 billion tokens; and (3) T0++ has a maximum of 1024 encoder tokens for the input and 256 decoder tokens for the output. In contrast, J1-Large has a context window of 2048 tokens, of which 400 were reserved as prompt tokens, leaving 1648 tokens for input and output that could potentially be employed in our ID-PT + J1-Large experiments. While points (1) and (2) above constitute inherent advantages for T0++, (3) somewhat disadvantages T0++ and so can be seen as a confounder in our exploration of the power of frozen LMs versus fine tuning. To make the comparison as fair as possible, we discarded training examples that did not fit into the context of T0++ (there were few of these). During decoding, we set the maximum decode length at 256 tokens, as in Sanh et al. (2021)."
        },
        {
            "heading": "2.3 RESULTS",
            "text": "We now turn to experimental results. Table 1 shows the average test set scores of ID-PT + J1-Large and T0++ per task cluster, and across datasets (see full list with per dataset scores in Table 7). The two models generally seem on par, with some task clusters showing small performance differences, and others higher variance: ID-PT + J1-Large performed much better in the sentiment and paraphrasing\n2This also takes into account the fact that we did not use sequence packing, whereas Sanh et al. (2021) did.\ntask clusters, and T0++ performed much better in the structure-to-text and summarization task clusters. Overall, ID-PT + J1-Large slightly surpassed the performance of T0++ in the test score average across datasets.\nFigure 3 shows the average development set scores we observed for ID-PT + J1-Large at different points during training. The prompt length experiment at 10% of the T0++ training data shows that longer than usual prompt lengths were indeed useful in this multi-tasked setting (see numerical results for ID-PT and prompt tuning at different prompt lengths in table 9 in the appendix). We continued training with the 400 prompt tokens variant, which reached the average development set score of T0++ after training on roughly 50% of its training data. The test set evaluation given in Table 1 was performed at this checkpoint. Figure 3 also shows results for the prompt tuning baseline, which underperformed in this multi-tasked setting, demonstrating the need for input dependence in our ID-PT method for multi-tasking frozen LMs. The breakdown of development set scores by cluster and task is given in the appendix.\nFor further insight into the input dependence of ID-PT, we measured the average distance between generated prompt tokens of different input examples. Table 8 in the appendix shows that while the average cosine distance4 between generated prompt embeddings of two examples from the same natural language templates of the same dataset was around 0.03, this distance increases to around 0.07 for different natural language templates of the same dataset, and increases further to 0.17 for inputs corresponding to different datasets. This shows that our prompt generator has learned to produce dataset-dependant prompts. Moreover, our observation of large distances between the prompts generated for different datasets helps to shed light on the weaker performance of prompt tuning, which has no alternative to sharing the same prompt tokens across all datasets. Lastly, the fact that ID-PT still generated somewhat variable prompts for examples within each dataset hints that the method may have the ability to offer gains even in the single-task regime. Contemporary works by Tang et al. (2022); Jin et al. (2022) further investigate this possibility."
        },
        {
            "heading": "3 A FROZEN LM READER FOR OPEN-DOMAIN QUESTION ANSWERING",
            "text": "The dominant approach for performing open-domain question answering (ODQA) is the retrieve\u2013read framework (Chen et al., 2017; Lee et al., 2019; Karpukhin et al., 2020), also referred to as open-book question answering. Given a question, this approach first employs a retriever over a large evidence corpus (e.g. Wikipedia) to fetch a set of relevant documents that may contain the answer (typically, on the order of 100 documents are retrieved). A retrieval-augmented reader is then used to answer the question given these documents. Standard pretrained LMs are trained on context windows much shorter than 100 documents, and so they require long-context fine tuning in order to be used as readers. This operation is very expensive\u2014prohibitively so for large LMs. Therefore, leading readers do not typically exceed 1B parameters (Karpukhin et al., 2020; Izacard & Grave, 2020a).\nAn inherent drawback of relying on small retrieval-augmented readers is that they do not enjoy the world knowledge or deduction capabilities of huge LMs. There is thus an opportunity in combining the power of strong supervised retrievers with that of huge LMs. To address this, we used an external re-ranking module for increasing the chance of getting the answer in a small amount of passages that fits into the frozen LM\u2019s context window. While the retriever relevance scores are computed based on separate dense representations of the question and passage (Karpukhin et al., 2020; Ram et al., 2022), the re-ranker predicts each document\u2019s relevance score after jointly attending to both the question and the passage (Karpukhin et al., 2020). We prompt tune the frozen LM to extract answers from re-ranked documents that appear in its context.\nOur simple re-ranking approach facilitates non trivial performance by the frozen LM reader. Our results show that a frozen J1-Grande-17B model can surpass the score of the fine-tuned Fusion-in-Decoder (FiD) model of Izacard & Grave (2020a) on the (open) Natural Questions\nbenchmark (Kwiatkowski et al., 2019), when both are given access to the same set of retrieved documents. We further boost the results by utilizing a stronger retrieval system, namely a hybrid\n3T0++ vastly underperforms on dbpedia_14, one of the topic classification datasets (see Table 7), and we leave that dataset out of the cluster average and out of the overall average. Though it appears in the topic classification task cluster of the P3 dataset, we conjecture that T0++ may not have trained on this dataset.\n4We measure cosine distance between two vectors a and b by 1\u2212 a\u00b7b|a||b| .\napproach combining Spider (Ram et al., 2022) and BM25 (Robertson & Zaragoza, 2009). Our frozen LM reader is able to perform significantly better than the strong end-to-end-trained EMDR2 (Singh et al., 2021) and on par with the distilled-retriever FiD model of Izacard & Grave (2020b), both prominent fine-tuned models."
        },
        {
            "heading": "3.1 EXPERIMENTAL SETUP",
            "text": "At a high level, we trained a re-ranker to produce improved passage relevance scores by jointly attending to the question and passage. We then greedily added passages to our context in descending order, until the context length of our frozen LM reader was full. We thus prepared training data for prompt tuning our frozen LMs to serve as readers. The full details of our experimental setup follow.\nDataset & Evidence Corpus. We used the open-domain version of the popular Natural Questions (\u201cNQ\u201d) benchmark (Kwiatkowski et al. 2019), which was popularized by Lee et al. (2019) and has since been widely used for ODQA. The training data consists of \u223c80K questions along with gold annotations of answers. As evidence corpus, we adopted the Wikipedia corpus as Karpukhin et al. (2020), which consists of roughly 21 million passages of 100 words each.\nRetrievers. To generate inputs for our re-ranker, we experimented with two different retrievers from the literature.\n\u2022 DPR-NQ (Karpukhin et al., 2020): A supervised dense retriever trained in a contrastive fashion on NQ.\n\u2022 Spider-NQ + BM25 (Ram et al., 2022; Robertson & Zaragoza, 2009): A self-supervised dense retriever trained on the recurring span retrieval task. Here we use the hybrid model described in Ram et al. (2022), where the dense retriever is Spider, fine-tuned on NQ (similar to DPR) and the sparse model is BM25 (Robertson & Zaragoza, 2009).\nRe-ranker Training. We trained our re-ranker following the same protocol used by Karpukhin et al. (2020) to train their extractive reader. We based the re-ranker architecture on the 110M parameter BERT-base (Devlin et al., 2019) model, such that a forward pass through the re-ranker incurs negligible run-time cost relative to a single pass through our 7B/17B parameter frozen LMs. During training, we sampled one positive and 23 negative passages from the top 100 passages returned by the retrieval system for each question. The training objective was to maximize the marginal log-likelihood of the start and end of all the correct answer spans in the positive passage (the answer string may appear multiple times in one passage), combined with the log-likelihood of the positive passage being selected. We used a batch size of 16, and trained the re-ranker for up to 30K steps with a learning rate of 1 \u00b7 10\u22125 using Adam (Kingma & Ba, 2014), linear scheduling with warm-up, and dropout rate of 0.1. Contemporary work (Anonymous, 2022) investigates a similar form of re-ranking, for the benefit of a fine-tuned reader.\nPreparing data for prompt tuning. At inference time, we discarded the start and end scores of the extractive reader, and only used its passage-level scores as re-ranking scores. Given those, we greedily added passages to our context in descending order, until the context length of our frozen LM reader was full. We note an important subtlety in the way we prepared the data used to prompt tune our LMs. In initial experiments training the re-ranker, we observed clear overfitting on the training set: our re-ranker performed especially well on the inputs used to train it. We did not want this bias to impact our prompt tuning, which of course we wanted to generalize to test data. Therefore, we randomly split the training set into two halves, denoted training-A and training-B, over which we trained two re-rankers, denoted re-ranker-A and re-ranker-B. We then used re-ranker-B to process the training-A data and likewise used re-ranker-A to process the training-B data, merging the two to yield our LM prompt tuning training set. We trained a third re-ranker on the entire training set, denoted re-ranker-All, and used it in order to create the data for the development and test sets.\nPrompt tuning. We prompt tuned our frozen J1-Large-7B and J1-Grande-17B LMs to serve as readers over the data prepared by the re-ranker. We used batch size 32, and considered learning rates in {1 \u00b7 10\u22121, 5 \u00b7 10\u22121} for J1-Large and {3 \u00b7 10\u22122, 1 \u00b7 10\u22121} for J1-Grande, reporting the best results on the development set and measuring test scores for the best development set configuration.\nBaselines. We compare our model to numerous popular baselines, all of which are generative. Specifically, we consider RAG (Lewis et al., 2020), Retro (Borgeaud et al., 2021), EMDR2 (Singh et al., 2021) and FiD/FiD-Distill Izacard & Grave (2020a;b). For fair comparison, we differentiate models that use DPR for retrieval from those that leverage stronger ones.\nAblations. To help us to understand the contribution of the re-ranking module, we ran the same experiment when greedily packing passages into the context window of the frozen LM based on the original retriever relevance scores, which are computed based on separate dense representations of the question and passage."
        },
        {
            "heading": "3.2 RESULTS",
            "text": "We now turn to describe our experimental results. Table 2 shows the utility of using a re-ranker when packing documents into the context window of our LM, which can contain 17 of the 100 retrieved passages. When using DPR (Karpukhin et al., 2020) as our retrieval system, we increased the recall at the input to our LM (i.e., the percentage of questions for which the answer appears in the context window of the frozen LM) from 77.2% to 80.4%, thereby improving downstream performance (measured by exact match) by 2.1 points (from 46.6% to 48.7%). Similarly, we observe significant gains from re-ranking when leveraging stronger retrievers like Spider+BM25.\nTable 3 shows the results of our systems on the test set of NQ, compared to various generative baselines. In the setting where all models use the same retriever \u2013 DPR \u2013 our frozen J1-Grande-17B reader obtains the best result, surpassing the score of the FiD model (Izacard & Grave, 2020a) which was fine-tuned to attend to all 100 retrieved documents at decoding time.\nOur frozen J1-Large-7B outperforms the similarly-sized Retro-7.5B model (Borgeaud et al., 2021), which has a similar decoder-only architecture, but was highly customized to the open-book setting: it was pretrained with a retrieval component and then fine tuned to attend to 20 passages. The frozen J1-Large-7B surpasses Retro by 3.3 points with no re-ranker, i.e., when the \u223c 17 passages shown at its input are a subset of the 20 passages shown to Retro, showing that frozen, decoder-only LMs can outperform specialized ODQA reader architectures when given the same set of retrieved documents. J1-Large-7B surpasses Retro by 4.4 points when the \u223c17 passages at its input are re-ranked. When not limited to the DPR retriever, our frozen J1-Grande-17B matches the performance of the strong fine-tuned FiD-Distill model (Izacard & Grave, 2020b), and outperforms EMDR2 (Singh et al., 2021), which jointly fine tuned both retriever and reader end-to-end. Overall, our results demonstrate that huge frozen language models serve as excellent readers for ODQA, and do not fall behind more elaborate prominent fine-tuned readers."
        },
        {
            "heading": "4 RECURSIVELY APPLYING A FROZEN LM",
            "text": "Existing applications of transformer-based LMs run a given input through the LM only once. While this is a natural choice, made in most other deep neural network applications, we identify an opportunity to diverge from this design pattern in the case of LMs. Since both the input and output spaces of an LM are in natural language, and since the same LM can serve a multitude of functionalities, it makes sense in principle to re-apply an LM to its own output, an operation which we dub LM recursion.\nIn this section we present two distinct methods for putting this idea into practice (see Figure 5) and give experimental evidence that each of them can produce significant gains. In Section 4.1, we present a textual approach, in which output text is sampled after the first pass through the frozen LM and reinserted into the same frozen LM. In Section 4.2 we present a neural approach, in which a small trainable network maps the vector representation at the output of the frozen LM to a vector representation input for the next iteration through the same frozen LM.\nWe evaluated LM recursion in the closed-book setting of open domain question answering, focusing on the Natural Questions benchmark (Kwiatkowski et al., 2019). We experimented with our 7B parameter LM J1-Large, and show that by iterating twice through the model, both methods yielded substantial gains relative to leading frozen model methods that leverage the the same frozen model only once, and that neural recursive LMs outperformed textual recursive LMs. Notably, by iterating\ntwice through our 7B parameter model, neural recursive LMs approached the performance of a single pass through our 17B parameter LM, J1-Grande.\nThe prospect of improving performance by recursively applying an LM to its own output has the potential to be a game changer for the economics of serving LMs. Given an LM with unsatisfactory performance on a certain task, an existing performance improvement vertical is to pretrain an even larger LM. However, pretraining larger and larger LMs quickly becomes prohibitively expensive, and huge models are expensive to deploy even at evaluation time. Moreover, the need for improved performance may only arise in certain tasks or for certain inputs within a task. Improvement by re-applying the existing LM over its own output allows paying double the cost of a single forward pass and getting double the compute only when needed, a more focused and much cheaper option than pretraining and deploying a model of double the size."
        },
        {
            "heading": "4.1 TEXTUAL LM RECURSION",
            "text": "In the first form of LM recursion that we explore, the two frozen LMs interact via text. More specifically, as illustrated in Figure 5b, we sample many outputs after the first pass through the LM, which we then reinsert into the same LM for a second refining pass.\nIn contrast to existing re-ranking approaches, which train external re-ranking modules to pick the correct answer out of several generated candidates, here the frozen LM takes on the re-ranking role itself. This approach can also be contrasted with the retrieval-based approach presented in Section 3, where the first pass \u201cretrieves\u201d several candidate answers from within the model itself, rather than from an external corpus. Both views raise the question of whether such a form of LM recursion can improve performance \u2013 can a model improve itself? We show below that it can and comment on potential reasons why.\nAs schematically shown in Figure 5b, we first trained a prompt, p1, for prompt tuning the LM on the question answering task, and then used it to sample candidate answers from the model. Then, we trained another prompt, p2, for prompt tuning the LM again, this time on the task of producing an\nanswer when given the question along with the candidate answers sampled from the first pass through the LM.\nFor the first stage, we trained p1 until convergence. We used batch size of 32 and considered learning rates in {3 \u00b7 10\u22122, 1 \u00b7 10\u22121, 3 \u00b7 10\u22121, 5 \u00b7 10\u22121} with 0.5% warmup. Notably, Lester et al. (2021) introduced prompt tuning without learning rate decay; preliminary experiments led us to agree that decay was unnecessary for prompt tuning performance. We therefore trained p1 until we observed it converged, after about 100K training steps (see Figure 6); we treat the performance in that point as reflecting the full potential of this prominent single-pass frozen model approach.\nNext, we sampled n candidate answers from the p1 prompt-tuned model via sampling at temperature 1 and ordered them according to their probability estimated by the model. Clearly, it becomes more likely that we will sample the correct answer when we sample multiple candidates, as compared to the typical case of providing a single answer via greedy decoding. Indeed, Table 5 shows the recall at n samples on the development set of Natural Questions increasing with n, for n in {1, 8, 16, 64}; this reached 44% for n = 64. However, leveraging this enhanced recall during the second prompttuning stage is not trivial, for two reasons: (1) while the recall grows with n, so does the number of confounders; and (2) the same model is used to re-rank itself, so if the model does not provide the correct answer after the first pass, then by definition some confounders were ranked higher than the correct answer by the same frozen LM.\nWe conducted the second prompt tuning stage of p2 with same training configuration as that of p1, where the model was now trained to provide the answer when given the question followed by the list of sampled answers. Table 5 shows the development set scores of our textual LM recursion method for different numbers of sampled candidates after training for 10K steps on the Natural Questions training set. Indeed, while recall increases with n, there are diminishing returns in terms of performance. Notably, for n = 1 the recall is lower than the p1 prompt-tuning performance since we sampled in temperature 1, while EM performance was measured with temperature 0. Despite this low recall, the performance of p2 prompt-tuning is not degraded relative to p1 at n = 1, and we conjecture that the p2 prompt-tuned model learns to solve the\nclosed-book QA task in parallel to benefiting from correct answers that appeared in its input.\nWe chose n = 16 as the best number of sampled candidates, and continued training a textual recursive LM with n = 16 until it reached 100K steps. Table 4 compares its test set score with that of the saturated prompt tuning method and of the neural recursive LM method (to be presented next), all trained for the same number of steps. Textual LM recursion improved on the saturated prompt tuning model that provided its input candidates by 1.8 points, despite both the candidate proposal and candidate choosing processes being based on the same frozen LM. We conjecture that this is because while the first LM pass provides the candidates at its final representation, in the second LM pass all candidates are considered and processed jointly with the question, already from the input stage, which increases the expressivity of the question answering process (Levine et al., 2022)."
        },
        {
            "heading": "4.2 NEURAL LM RECURSION",
            "text": "Motivated by the evident usefulness of textual recursive LMs, we now propose a more direct\u2014and, as we will show, even stronger\u2014method for leveraging an LM twice per instance. Textual recursive LMs suffer from several drawbacks: (1) inference is slow, due to the intermediate operation of sampling; (2) our reliance on sampling means that we discard some of the information contained in the final embedding layer of the first pass; (3) both passes through the LM are prompt tuned to answer the input question, where in principle the first pass could be less constrained, since question answering ability matters only after the second LM pass; and (4) textual recursive LMs use very few trained"
        },
        {
            "heading": "LM Passes LM size Model Connector Layers Pretrain Init? EM",
            "text": "parameters. Given that our runtime is dominated by the substantial cost of making two passes through the LM, we can afford to use somewhat heavier machinery to unlock its full potential.\nOur neural recursive LMs approach addresses all of these drawbacks. Specifically, we trained a Connector network that connects the output of the first pass through the frozen LM with the input of its second pass (see Figure 5c). The inputs to the Connector network are the output embeddings after the first LM pass (before translation into the token vocabulary space), and the outputs of the Connector network are injected into the first Transformer layer of the second LM pass (bypassing its token embedding matrix). We implemented the Connector as an autoregressive network composed of a small number of unidirectional Transformer layers, the same building blocks comprising the frozen LM. Therefore, by \u201cstitching together\u201d two replicas of the frozen LM via such a Connector network, we get an LM\u2013Connector\u2013LM network, which is essentially a deeper (mostly frozen) autoregressive Transformer architecture.\nAblations. We performed ablations to investigate two key design dimensions: (1) the number of Transformer layers in the Connector network and (2) their initialization. (1) We wanted the Connector size to be a small fraction of the frozen J1-Large LM\u2019s size of 7B parameters, so we trained 1- and 2-layered Connectors of sizes 1/32 and 1/16 of the 32-layered J1-Large LM, training 200M and 400M parameters, respectively. (2) We compared random initialization of the Connector network to initializing it by pretraining the entire LM\u2013Connector\u2013LM stack on the self-supervised language modeling task. We did this by keeping both LMs frozen but passed gradients through the frozen LM to optimize the Connector\u2019s parameters; we gave this optimization procedure access to up to 3% of the pretraining corpus used to train J1-Large.\nBaselines. Running the large LM twice means doubling inference time. In order to determine whether this second pass through the frozen LM leads to performance gains, we compare primarily to prompt-tuning Lester et al. (2021), as an established single-pass frozen model method. Additionally, since the Connector introduces a nontrivial number of learned parameters, which could affect performance even if the second pass through the frozen LM is unhelpful, we ran two additional single-LM-pass baselines, Connector\u2013LM and LM\u2013Connector: consisting of a Connector that runs either before or after a single pass through the LM, respectively.\nTraining details. We first trained for 10K steps with batch size 32. We used the Adam optimizer (Kingma & Ba, 2014) with parameters \u03b21 = 0.9, \u03b22 = 0.95, = 10\u22126, and weight decay of 0.1. We considered learning rates in {1 \u00b7 10\u22125, 3 \u00b7 10\u22125, 1 \u00b7 10\u22124, 3 \u00b7 10\u22124} with 0.5% warmup and linear decay. After initial experimentation on the development set, we set the decay scheduling of the best neural recursive LM architecture for 100K training steps. We did this for comparability with the saturated prompt tuning method (see Figure 6) and with textual recursive LMs that trained for this many steps, though neural-LM recursion does not necessarily converge at the same point as it has more training parameters.\nResults Table 6 presents development set EM scores for all of our ablations and baselines. Overall, after 10K training steps, LM recursion led to clear gains, with 2.2\u20133.7 point gains over corresponding\nsingle pass baselines. The strongest single-pass baseline was the Connector\u2013LM variant, which can be viewed as prompt-tuning on steroids, as it trains more parameters than prompt-tuning at the input to the LM. Indeed, it improved over prompt tuning by 1.5 points. LM\u2013Connector\u2013LM trained the same number of parameters but improved by additional 2.2 points due to its use of a second pass through the frozen LM. Importantly, performing a double pass through the 7B-parameter J1-Large model closed roughly three quarters of the 5-point gap between the prompt-tuned 7B-parameter J1-Large model and the prompt-tuned 17B-parameter J1-Grande model, offering some support for our intuition that multiple passes through a small LM could serve as an alternative to a larger model.\nOur ablations suggest that initializing the Connector via pretraining the recursive LM on the language modeling task was vital to its fine-tuning performance. Doing the same for the prompt tuning baseline did not seem to similarly help. Using a 2-layered Connector network is better than using a 1-layered Connector network (perhaps due to the expressive weakness of a single Transformer layer (Cordonnier et al., 2019)). We did not try deeper Connector networks because of our desire to keep the number of trained parameters relatively small, but note that this design space is an obvious avenue for future work.\nWe continued training the best recursive LM variant (pretrained, 2-layer Connector) until it reached 100K steps, to the point where the prompt tuning baseline had converged and a comparison could be made when training over the same data as a leading single-pass frozen model method that exhausted its potential. Table 4 shows this comparison on the Natural Questions test set. Our Neural recursive LM improved over prompt tuning by 4.4 points (or, over 20%), and over our textual recursive LM by 2.6 points.\nTo better understand the differences between our two LM recursion methods, we examined the set of questions for which the saturated prompt tuning baseline was not able to provide the correct answer even when 64 answers were sampled from it. Given that this setting gave rise to recall of 44% (see Table 5), this set was roughly half of the Natural Questions development set. We treat this subset as questions that were beyond the scope of a single pass through our frozen J1-Large model, and ask how well both LM recursion methods performed on such questions. Our textual recursive LM received an EM score of 3.8%, implying that it was rarely able to go beyond extracting a correct answer from those sampled during the first pass through the LM. In contrast, the neural recursive LM received a higher EM score of 12% on this question subset, implying an enhanced ability to answer questions that are beyond the scope of a single LM pass.\nOverall, the methods presented in this section demonstrate that while LMs are treated as single-use resources per query, significant performance gains can be achieved by treating them as resources that can be progressively revisited for improved performance per query."
        },
        {
            "heading": "5 CONCLUSIONS",
            "text": "While fine-tuning huge LMs can often yield excellent performance, this approach is expensive at training time, requires serving a plethora of models at runtime, and provides poor adaptability in the face of variations in the targeted task. This paper has shown that a better alternative exists: freezing a single, huge pretrained LM and learning much smaller neural modules that specialize the LM to different tasks. While prompt tuning, prefix tuning, and other existing frozen model methods cited above can be seen as a simple instantiations of this idea, this paper shows that much more complex architectures can achieve much stronger performance.\nTo make this case, we introduced three novel design patterns for such neural adaptors: input-dependent prompt tuning; frozen readers; and LM recursion (presenting both neural and textual variants). We showed that our methods match and often exceed prominent fine tuning approaches in massive multi-tasking and in open-domain question answering. There is clearly much more to learn about ways in which each of these architectures can be optimized to perform as well as possible in each of the domains in which we applied it; we have already given pointers to what we consider particularly low-hanging fruit throughout the paper.\nMore importantly and more broadly, however, we believe that our results point towards a new stage in the application of huge LMs to practical problems, in which the design of task-specific neural middleware will often take the place of fine tuning. The result will be a landscape in which fine tuning is typically an unnecessary extravagance and the key engineering challenge is finding the best way to stand on the shoulders of giant frozen language models."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "We acknowledge helpful advice from Yonatan Belinkov, Omri Abend, and Moshe Tennenholtz, and further comments and assistance from our colleagues at AI21 Labs."
        }
    ],
    "title": "FROZEN LANGUAGE MODELS",
    "year": 2022
}