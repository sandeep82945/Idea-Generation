{
    "abstractText": "Generalized complete intersection Calabi-Yau manifold (gCICY) is a new construction of Calabi-Yau manifolds established recently. However, the standard algebraic method to generate new gCICYs is very laborious. Because of this complexity, the number of gCICYs and their classification still remains unknown. In this paper, we try to make some progress in this direction using neural networks. Our results showed that the trained models cannot only get high accuracy on the type (1, 1) and type (2, 1) gCICYs existing in the literature but also achieve a 97% accuracy in predicting new gCICYs not used in the training. This shows that machine learning is an effective method to classify and generate new gCICY.",
    "authors": [
        {
            "affiliations": [],
            "name": "Wei Cui"
        },
        {
            "affiliations": [],
            "name": "Xin Gao"
        },
        {
            "affiliations": [],
            "name": "Juntao Wang"
        }
    ],
    "id": "SP:2571e4e8dcf900f01aec323b426623fc924b83bc",
    "references": [
        {
            "authors": [
                "T. Hubsch"
            ],
            "title": "Commun",
            "venue": "Math. Phys. 108, 291 ",
            "year": 1987
        },
        {
            "authors": [
                "P. Green",
                "T. Hubsch"
            ],
            "title": "Commun",
            "venue": "Math. Phys. 109, 99 ",
            "year": 1987
        },
        {
            "authors": [
                "P. Candelas",
                "A.M. Dale",
                "C.A. Lutken",
                "R. Schimmrigk"
            ],
            "title": "Nucl",
            "venue": "Phys. B298, 493 ",
            "year": 1988
        },
        {
            "authors": [
                "P. Candelas",
                "C.A. Lutken",
                "R. Schimmrigk"
            ],
            "title": "Nucl",
            "venue": "Phys. B306, 113 ",
            "year": 1988
        },
        {
            "authors": [
                "L.B. Anderson",
                "X. Gao",
                "J. Gray",
                "S.-J. Lee"
            ],
            "title": "J",
            "venue": "High Energy Phys. 10 ",
            "year": 2017
        },
        {
            "authors": [
                "L.B. Anderson",
                "F. Apruzzi",
                "X. Gao",
                "J. Gray",
                "S.-J. Lee"
            ],
            "title": "Nucl",
            "venue": "Phys. B906, 441 ",
            "year": 2016
        },
        {
            "authors": [
                "L.B. Anderson",
                "F. Apruzzi",
                "X. Gao",
                "J. Gray",
                "S.-J. Lee"
            ],
            "title": "Phys",
            "venue": "Rev. D 93, 086001 ",
            "year": 2016
        },
        {
            "authors": [
                "P. Berglund",
                "T. H\u00fcbsch"
            ],
            "title": "Adv",
            "venue": "Theor. Math. Phys. 22, 261 ",
            "year": 2018
        },
        {
            "authors": [
                "Q. Jia",
                "H. Lin",
                "J. Math"
            ],
            "title": "Phys",
            "venue": "(N.Y.) 61, 052301 ",
            "year": 2020
        },
        {
            "authors": [
                "P. Berglund",
                "T. Hubsch"
            ],
            "title": "SciPost Phys",
            "venue": "4, 009 ",
            "year": 2018
        },
        {
            "authors": [
                "M.A. Nielsen"
            ],
            "title": "Neural Networks and Deep Learning (Determination Press",
            "venue": "San Francisco,",
            "year": 2015
        },
        {
            "authors": [
                "F. Ruehle"
            ],
            "title": "Phys",
            "venue": "Rep. 839, 1 ",
            "year": 2020
        },
        {
            "authors": [
                "Y.-H. He"
            ],
            "title": "From the string landscape to the mathematical landscape: A machine-learning outlook",
            "venue": "Lie Theory and Its Applications in Physics. LT 2021. Springer Proceedings in Mathematics & Statistics ",
            "year": 2022
        },
        {
            "authors": [
                "A. Cole",
                "G. Shiu"
            ],
            "title": "J",
            "venue": "High Energy Phys. 03 (2019) 054. MACHINE LEARNING ON GENERALIZED COMPLETE ... PHYS. REV. D 107, 086004 ",
            "year": 2023
        },
        {
            "authors": [
                "A. Cole",
                "A. Schachner",
                "G. Shiu"
            ],
            "title": "J",
            "venue": "High Energy Phys. 11 ",
            "year": 2019
        },
        {
            "authors": [
                "A. Cole",
                "S. Krippendorf",
                "A. Schachner"
            ],
            "title": "and G",
            "venue": "Shiu, in 35th Conference on Neural Information Processing Systems ",
            "year": 2021
        },
        {
            "authors": [
                "F. Ruehle"
            ],
            "title": "J",
            "venue": "High Energy Phys. 08 ",
            "year": 2017
        },
        {
            "authors": [
                "J. Halverson",
                "B. Nelson",
                "F. Ruehle"
            ],
            "title": "J",
            "venue": "High Energy Phys. 06 ",
            "year": 2019
        },
        {
            "authors": [
                "Y.-H. He"
            ],
            "title": "Int",
            "venue": "J. Mod. Phys. A 36, 2130017 ",
            "year": 2021
        },
        {
            "authors": [
                "I. Bena",
                "J. Blab\u00e4ck",
                "M. Gra\u00f1a",
                "S. L\u00fcst"
            ],
            "title": "Adv",
            "venue": "Applied Clifford Algebras 32, 7 ",
            "year": 2022
        },
        {
            "authors": [
                "J. Carifio",
                "J. Halverson",
                "D. Krioukov",
                "B.D. Nelson"
            ],
            "title": "J",
            "venue": "High Energy Phys. 09 ",
            "year": 2017
        },
        {
            "authors": [
                "Y.-N. Wang",
                "Z. Zhang"
            ],
            "title": "J",
            "venue": "High Energy Phys. 08 ",
            "year": 2018
        },
        {
            "authors": [
                "M. Bies",
                "M. Cveti\u010d",
                "R. Donagi",
                "L. Lin",
                "M. Liu",
                "F. Ruehle"
            ],
            "title": "J",
            "venue": "High Energy Phys. 01 ",
            "year": 2021
        },
        {
            "authors": [
                "X. Gao",
                "H. Zou"
            ],
            "title": "Phys",
            "venue": "Rev. D 105, 046017 ",
            "year": 2022
        },
        {
            "authors": [
                "R. Altman",
                "J. Carifio",
                "J. Halverson",
                "B.D. Nelson"
            ],
            "title": "J",
            "venue": "High Energy Phys. 03 ",
            "year": 2019
        },
        {
            "authors": [
                "M. Demirtas",
                "L. McAllister",
                "A. Rios-Tascon"
            ],
            "title": "Fortschr",
            "venue": "Phys. 68, 2000086 ",
            "year": 2020
        },
        {
            "authors": [
                "K. Bull",
                "Y.-H. He",
                "V. Jejjala",
                "C. Mishra"
            ],
            "title": "Phys",
            "venue": "Lett. B 785, 65 ",
            "year": 2018
        },
        {
            "authors": [
                "Y.-H. He"
            ],
            "title": "The Calabi\u2013Yau Landscape: From Geometry",
            "venue": "to Physics, to Machine Learning, Lecture Notes in Mathematics ",
            "year": 2021
        },
        {
            "authors": [
                "Y.-H. He",
                "A. Lukas"
            ],
            "title": "Phys",
            "venue": "Lett. B 815, 136139 ",
            "year": 2021
        },
        {
            "authors": [
                "H. Erbin",
                "R. Finotello"
            ],
            "title": "Phys",
            "venue": "Rev. D 103, 126014 ",
            "year": 2021
        },
        {
            "authors": [
                "D.S. Berman",
                "Y.-H. He",
                "E. Hirst"
            ],
            "title": "Phys",
            "venue": "Rev. D 105, 066002 ",
            "year": 2022
        },
        {
            "authors": [
                "D. Klaewer",
                "L. Schlechter"
            ],
            "title": "Phys",
            "venue": "Lett. B 789, 438 ",
            "year": 2019
        },
        {
            "authors": [
                "C.R. Brodie",
                "A. Constantin",
                "R. Deen",
                "A. Lukas"
            ],
            "title": "Fortschr",
            "venue": "Phys. 68, 1900087 ",
            "year": 2020
        },
        {
            "authors": [
                "L.B. Anderson",
                "M. Gerdes",
                "J. Gray",
                "S. Krippendorf",
                "N. Raghuram",
                "F. Ruehle"
            ],
            "title": "J",
            "venue": "High Energy Phys. 05 ",
            "year": 2021
        },
        {
            "authors": [
                "V. Jejjala",
                "D.K. Mayorga Pena",
                "C. Mishra"
            ],
            "title": "J",
            "venue": "High Energy Phys. 08 ",
            "year": 2022
        },
        {
            "authors": [
                "M. Larfors",
                "A. Lukas",
                "F. Ruehle",
                "R. Schneider"
            ],
            "title": "Mach",
            "venue": "Learn. Sci. Tech. 3, 035014 ",
            "year": 2022
        },
        {
            "authors": [
                "C.T.C. Wall"
            ],
            "title": "Inventiones Mathematicae 1",
            "venue": "355 ",
            "year": 1966
        },
        {
            "authors": [
                "T. Hubsch"
            ],
            "title": "Calabi-Yau Manifolds: A Bestiary for Physicists",
            "venue": "(World Scientific, Singapore,",
            "year": 1994
        },
        {
            "authors": [
                "M. Larfors",
                "R. Schneider"
            ],
            "title": "Fortschr",
            "venue": "Phys. 68, 2000034 ",
            "year": 2020
        },
        {
            "authors": [
                "S. Gukov",
                "J. Halverson",
                "F. Ruehle",
                "P. Su\u0142kowski"
            ],
            "title": "Mach",
            "venue": "Learn. Sci. Tech. 2, 025035 ",
            "year": 2021
        },
        {
            "authors": [
                "T.R. Harvey",
                "A. Lukas"
            ],
            "title": "J",
            "venue": "High Energy Phys. 08 ",
            "year": 2021
        },
        {
            "authors": [
                "A. Constantin",
                "T.R. Harvey",
                "A. Lukas"
            ],
            "title": "Fortschr",
            "venue": "Phys. 70, 2100186 ",
            "year": 2022
        },
        {
            "authors": [
                "S. Abel",
                "A. Constantin",
                "T.R. Harvey",
                "A. Lukas"
            ],
            "title": "Fortschr",
            "venue": "Phys. 70, 2200034 ",
            "year": 2022
        },
        {
            "authors": [
                "G. K\u00e1ntor",
                "V. Niarchos",
                "C. Papageorgakis"
            ],
            "title": "Phys",
            "venue": "Rev. D 105, 025018 ",
            "year": 2022
        },
        {
            "authors": [
                "V. Braun"
            ],
            "title": "J",
            "venue": "High Energy Phys. 04 ",
            "year": 2011
        },
        {
            "authors": [
                "J. Gray",
                "J. Wang"
            ],
            "title": "J",
            "venue": "High Energy Phys. 07 ",
            "year": 2022
        },
        {
            "authors": [
                "A. Braun",
                "A. Lukas",
                "C. Sun"
            ],
            "title": "Commun",
            "venue": "Math. Phys. 360, 935 ",
            "year": 2018
        },
        {
            "authors": [
                "F. Carta",
                "J. Moritz",
                "A. Westphal"
            ],
            "title": "J",
            "venue": "High Energy Phys. 05 ",
            "year": 2020
        },
        {
            "authors": [
                "R. Altman",
                "J. Carifio",
                "X. Gao",
                "B.D. Nelson"
            ],
            "title": "J",
            "venue": "High Energy Phys. 03 ",
            "year": 2022
        },
        {
            "authors": [
                "C. Crin\u00f2",
                "F. Quevedo",
                "A. Schachner",
                "R. Valandro"
            ],
            "title": "J",
            "venue": "High Energy Phys. 08 ",
            "year": 2022
        },
        {
            "authors": [
                "S.K. Donaldson"
            ],
            "title": "Some Numerical Results in Complex Differential Geometry (2005), arXiv:math/0512625",
            "year": 2005
        },
        {
            "authors": [
                "M. Headrick",
                "T. Wiseman"
            ],
            "title": "Classical Quantum Gravity 22",
            "venue": "4931 ",
            "year": 2005
        },
        {
            "authors": [
                "V. Braun",
                "T. Brelidze",
                "M.R. Douglas",
                "B.A. Ovrut"
            ],
            "title": "J",
            "venue": "High Energy Phys. 05 ",
            "year": 2008
        },
        {
            "authors": [
                "M.R. Douglas",
                "R.L. Karp",
                "S. Lukic",
                "R. Reinbacher",
                "J. Math"
            ],
            "title": "Phys",
            "venue": "(N.Y.) 49, 032302 ",
            "year": 2008
        },
        {
            "authors": [
                "M.R. Douglas",
                "R.L. Karp",
                "S. Lukic",
                "R. Reinbacher"
            ],
            "title": "J",
            "venue": "High Energy Phys. 12 ",
            "year": 2007
        },
        {
            "authors": [
                "M. Headrick",
                "A. Nassar"
            ],
            "title": "Adv",
            "venue": "Theor. Math. Phys. 17, 867 ",
            "year": 2013
        },
        {
            "authors": [
                "W. Cui",
                "J. Gray"
            ],
            "title": "J",
            "venue": "High Energy Phys. 05 ",
            "year": 2020
        },
        {
            "authors": [
                "M. Larfors",
                "D. Passaro",
                "R. Schneider"
            ],
            "title": "J",
            "venue": "High Energy Phys. 05 (2021) 105. WEI CUI, XIN GAO, and JUNTAO WANG PHYS. REV. D 107, 086004 ",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": "Machine learning on generalized complete intersection Calabi-Yau manifolds\nWei Cui ,2,3,* Xin Gao,1,\u2020 and Juntao Wang 2,3,\u2021 1College of Physics, Sichuan University, Chengdu 610065, China\n2Beijing Institute of Mathematical Sciences and Applications, Beijing 101408, China 3Yau Mathematical Sciences Center, Tsinghua University, Beijing 100084, China\n(Received 2 February 2023; accepted 22 March 2023; published 10 April 2023)\nGeneralized complete intersection Calabi-Yau manifold (gCICY) is a new construction of Calabi-Yau manifolds established recently. However, the standard algebraic method to generate new gCICYs is very laborious. Because of this complexity, the number of gCICYs and their classification still remains unknown. In this paper, we try to make some progress in this direction using neural networks. Our results showed that the trained models cannot only get high accuracy on the type (1, 1) and type (2, 1) gCICYs existing in the literature but also achieve a 97% accuracy in predicting new gCICYs not used in the training. This shows that machine learning is an effective method to classify and generate new gCICY.\nDOI: 10.1103/PhysRevD.107.086004\nI. INTRODUCTION\nThe perturbative superstring theories are required to have an underlying ten-dimensional spacetime, which must be compactified down to four dimensions in order to describe the real world. The methods for compactifying the tendimensional spacetime are best understood in the context of supersymmetric compactifications, which require the compactification manifold to be a Calabi-Yau (CY) threefold (X) if no more general fluxes are turned on. One of the main methods to construct the Calabi-Yau manifolds is realized as complete intersections of polynomial hypersurfaces embedded in a well-known \u201cambient space\u201d A, i.e., products of projective spaces, abbreviated here as CICYs. These complete intersection Calabi-Yau (CICY) threefolds were first discussed and classified in a series of articles in the 1980s [1\u20134]. The most tractable descriptions of the CICY database were obtained in [5], where all the divisor classes of the CY threefold are simply inherited from the ones of the ambient space and hence a favorable description. Recently a novel extension and generalization, called generalized complete intersections (gCI), has been introduced in constructing the Calabi-Yau manifold [6], known\nas gCICY. It has attracted lots of study from the perspective of string phenomenology and also pure mathematics since it was invented. In gCICYs, some of those hypersurfaces have negative degrees over some factors in the ambient space A. It is convenient to write the matrix in the form where columns contain only semipositive integers on the left while columns contain negative integers on the right. For example, a typical gCICY configuration matrix is\nX \u00bc 2 6666664 Pn1 Pn2 .. .\nPnm\na11 a1KA a21 a2KA .. . . . . .. .\nam1 amKA\nb11 b1KB b21 b2KB .. . . . . .. .\nbm1 bmKB\n3 7777775 ; \u00f01\u00de\nwhere the entries in the columns fa\u03b1g, \u03b1 \u00bc 1; 2;\u2026; KA contain only semipositive integers and the entries in the columns fb\u03b2g \u03b2 \u00bc 1; 2;\u2026; KB contain negative integers. As shown in [6], when certain conditions are satisfied, such configurations can also lead to well-defined CY manifolds and it was called the \u00f0KA;KB\u00de type of gCICY. In [6], (1, 1) and (2, 1) types of gCICY were constructed with some limitations. We will summarize the construction of the \u00f0KA;KB\u00de type of gCICY in Sec. II. The diffeomorphism class and cohomology of such generalized complete intersections Calabi-Yau (gCICY) have been studied in [6\u20139] and were shown to be extendable to non-Fano varieties [10,11]. A rigorous scheme-theoretic definition of such a generalized complete intersection is provided in [12]. *cwei@bimsa.cn \u2020xingao@scu.edu.cn \u2021juntao.wang@bimsa.cn Published by the American Physical Society under the terms of the Creative Commons Attribution 4.0 International license. Further distribution of this work must maintain attribution to the author(s) and the published article\u2019s title, journal citation, and DOI. Funded by SCOAP3.\nPHYSICAL REVIEW D 107, 086004 (2023)\n2470-0010=2023=107(8)=086004(12) 086004-1 Published by the American Physical Society\nHowever, as we will briefly show in Sec. II that the construction of gCICY, in general, can be very hard, this depends nontrivially on the underlying manifold data and it presents an interesting challenge for machine learning [13\u201315]. Machine learning has been a good implement in theoretical physics research and has led to fruitful results during the past couple of years. With the help of machine learning people are able to deal with problems with more computational efficiency, especially problems involving big data, for example, studying the landscape of string flux vacua [16\u201325] as well as F-theory compactifications [26\u201328]. This technique allows people to learn lots of quantities of Calabi-Yau manifolds, from its toric building blocks like the orientifold structure [29] and triangulations [30,31], to the calculation of Hodge numbers [32\u201336], line bundle cohomology [37,38] and numerical metrics [39\u201343]. The newly established gCICY database [6] is the ideal data for machine learning in several aspects. First, the explicit formulas to determine a gCICY are not known and calculations rely on complicated and computationally intense algorithms. Second, in the original construction [6], the primary time constraints arise from the calculation of line bundle cohomology. Taking negative entries in the configuration matrices as varying parameters, the computation time of line bundle cohomologies and the number of possible matrices to be considered increase drastically with the increasing of those parameters. So to complete the initial scan in finite time, the authors left for future study of configurations whose calculation time was longer than five minutes or the negative entries in this configuration is greater than 4. Third, other than the negative entries in the configuration matrices, there are the other two parameters in the construction of gCICYs, the type \u00f0m; n\u00de of configuration matrices. There is even no mechanism to tell us if m or n will stop increasing somewhere. So theoretically, what we need to test, without further guidance, is a dataset of an infinite number of matrices. So in principle, it is almost formidable to get intuitions on classifying gCICYs by large-scale scanning using traditional calculation tools. This is exactly where we can exploit the power of machine learning in solving problems with large datasets and its power on prediction. In our work here, we will use neural networks as our first trial in this direction. For the possibility of getting a better machine learning model, we enlarge the existing dataset of type (2, 1) by finishing the calculation for all the matrices with negative entries larger than \u22124, by relaxing the time constraints put in [6]. Based on the full type (1, 1) and the partial type (2, 1) data (with negative entry larger than \u22124), we trained a neural network model which can achieve at least 90% precision on both validation and testing datasets. As we pointed out before, we need to use machine learning to tell us, at least partially, what will happen if we step out of what we already know, to predict\nsomething new. On the other hand, we should also have a rough idea of how precise the prediction could be. To do this, we first generate a dataset of 10,000 gCICYS and 10,000 non-gCICYS with negative entries \u22125 or \u22126 and then use our trained model on it to test the model\u2019s predicting power. What is impressive is that, even though this dataset is generated randomly and quite different from the data for model training, the prediction precision of this model is 97%. This means that the trained neural network is not overfitting, grasping features that not only belong to those matrices with negative entries larger than \u22124 but also beyond. Guided by this result, we believe that the trained model should have already some features that are not sensitive to the negative entries and can predict if a matrix represents a gCICY or not in a rather precise way. So together with this paper, we will attach two Mathematica files with all the predictions for configuration matrices with negative entries \u22125 or \u22126. This paper is organized as follows. In Sec. II, we briefly summarize the algorithm of how to construct a generalized Calabi-Yau manifold and also the basic idea of how to do its classification. In Sec. III, we will show in some detail how we train a neural network model by using data in type (1,1) and type (2,1) and then do a prediction. One thing in this section we want to emphasize is that we did a prediction in a rather precise way, which is not always guaranteed in using machine learning. Finally, in Sec. IV, we conclude our work and point out a few future directions."
        },
        {
            "heading": "II. THE CONSTRUCTION OF gCICYs",
            "text": "In this section, we briefly summarize the construction of gCICYs. An ordinary CICY X is defined as a complete intersection of the zero loci of K multihomogeneous polynomials in the ambient space A \u00bc Pn1 \u00d7 Pn2 \u00d7 \u00d7 Pnm . The degrees of these polynomials are collected in a m \u00d7 K matrix called configuration matrix, which is always used to represent the CICY X and it can be schematically written as\nX \u00bc 2 666664 Pn1 Pn2 .. .\nPnm\nq11 q1K q21 q2K .. . . . . .. .\nqr1 qrK\n3 777775 ; \u00f02\u00de\nwhere each entry qia \u2208 Z\u22650 specifies the degree of homogeneity of the ath defining polynomial in the ith projective ambient space and it should be a non-negative integer. In addition, these qia should satisfy the Calabi-Yau condition ni \u00fe 1 \u00bc P K a\u00bc1 q i a for i \u00bc 1;\u2026; m. If X is a threefold, then\nthere should be one more condition that P\nm i ni \u00bc K \u00fe 3.\nThe configuration matrix provides an efficient way to describe CICYs [1\u20134]. The favorable description of the CICY database was obtained in [5], where all the divisor classes of the CY threefold are simply inherited from the\n086004-2\nones of the ambient space.1 Only about half of the descriptions in the original database [3] have this property. Given a generalized configuration matrix introduced in (1), the corresponding CY manifold, if it exists, can be constructed in the following two steps. First, consider the first KA columns containing only non-negative entries. Each one of these columns gives a degree fa\u03b1g homogeneous polynomial. The complete intersection of the vanishing loci of these polynomials defines a codimensional KA hypersurface M in A denoted by\nM \u00bc 2 666664 Pn1 Pn2 .. .\nPnm\na11 a1KA a21 a2KA .. . . . . .. .\nar1 arKA\n3 777775 : \u00f03\u00de\nIn the rest of this paper, we will always denoteM to be the submatrix of X containing only columns with positive components. Second, consider the last KB columns in (1). Unlike in the situation of regular CICYs, generally one cannot get a well-defined manifold using these negative valued columns. However, as observed in [6], manifolds can also be constructed using rational functions instead of homogeneous polynomials. To be specific, we can define L\u03b2 \u00bc OM\u03b2\u22121\u00f0b1\u03b2\u2026; bm\u03b2 \u00de to be the line bundle associated to the column fb\u03b2g with \u03b2 \u00bc 1; 2;\u2026; KB. Here fM\u03b2g is a sequence of nested hypersurface inside M given by\nM \u00bc M0 \u27f6 r1\u00bc0 M1 \u27f6 r2\u00bc0 \u27f6rKB\u00bc0MKB \u00bc X;\nwhere r\u03b2 \u2208 H0\u00f0M\u03b2\u22121; L\u03b2\u00de \u2260 0 are nontrivial global sections of each L\u03b2 and are, in general, not polynomials but rational functions. Thus, in order to have a well-defined manifold, we need to have nontrivial global sections, i.e.,\nh0\u00f0M\u03b2\u22121;L\u03b2\u00de \u2260 0; \u03b2 \u00bc 1; 2;\u2026; KB: \u00f04\u00de\nIn this way, one can construct a gCICY X from a general configuration matrix containing negative numbers. Notice that in the construction of gCICY described above, the order of KA semipositive columns can be arbitrary while the order of KB columns is not random. A different order of these columns may lead to failure of the above condition. The resulting gCICY X in this construction is a codimensionKA \u00fe KB hypersurface inA. Thus, if we wantX to be an N-fold, one needs to have P ni \u2212 KA \u2212 KB \u00bc 3.\nSince the columns of KA and columns KB play different roles in the construction, we will call the resulting manifold as type \u00f0KA;KB\u00de gCICY. In addition, to define a CY manifold, the entries in the configuration matrix need to obey the Calabi-Yau condition, i.e.,\nni \u00fe 1 \u00bc XKA \u03b1\u00bc1 ai\u03b1 \u00fe XKB \u03b2\u00bc1 bi\u03b2 for i \u00bc 1; 2;\u2026; m: \u00f05\u00de\nWe are mainly interested in the gCICYs threefold. Without stating explicitly, all configuration matrices that will be considered in Sec. III have already obeyed the threefold condition and the Calabi-Yau condition. By definition, every gCICY can be described by a configuration matrix described above, however, not every such matrix describes a gCICY. To be able to represent a gCICY, this matrix has to obey the cohomology condition expressed in (4), i.e., to check if a given matrix describes a gCICY, one needs to at least compute KB line bundle cohomology. Furthermore, to make sure the gCICY is well defined, one requires that the trivial line bundle cohomology on X is\nh \u00f0X;O\u00de \u00bc f1; 0; 0; 1g: \u00f06\u00de\nThis condition is sufficient to avoid the nonreduced, nonconnected and non-CY geometries [6]. Thus, to check if a given configuration matrix describes a well-defined gCICY, one needs to compute at least KB \u00fe 1 line bundle cohomologies. However, the computation of line bundle cohomologies is in general laborious. Thus, our knowledge about gCICYs is still limited. In this paper, we will combine this limited knowledge and the powerful machine learning algorithm to train a binary classifier that can efficiently distinguish the configuration matrices that can describe gCICYs and those that cannot. We hope that our model can learn the essential knowledge about gCICY encoded in the configuration matrix and extend their power beyond the training data and find more gCICYs."
        },
        {
            "heading": "III. LEARNING gCICYs AND ITS PREDICTION",
            "text": "In this section, we will use the artificial neural network (ANN) to determine if a given configuration matrix defines a well-defined gCICYor not. We will do this for both type (1, 1) and type (2, 1) configuration matrices."
        },
        {
            "heading": "A. Type (1, 1) gCICYs",
            "text": "The simplest gCICYs are given by configuration matrices with one column of positive entries and one column of negative entries as below: 1There exits a stronger notion of \u201cK\u00e4hler favorability\u201d where K\u00e4hler cones on X descend from an ambient space [5]. In some cases, the \u201cfavorable\u201d geometry is not K\u00e4hler favorable since the K\u00e4hler cone of X is actually larger than the positive orthant one.\n086004-3\nX\u00f01;1\u00de \u00bc 2 666664 Pn1 Pn2 .. .\nPnm\na1\na2\n..\n.\nam\nb1\nb2\n..\n.\nbm\n3 777775 ; \u00f07\u00de\nwhere the ais are all semipositive, whereas some of the bis can take negative values. A configuration matrix X\u00f01;1\u00de defines a gCICY if it satisfies the conditions (4) and (6) discussed in Sec. II. The type (1, 1) gCICYs have been classified in [6]. In particular, imposing the condition that h0\u00f0M;L\u00de \u2260 0 puts a bound on the entries in X\u00f01;1\u00de. With this bound, one could find that there are a total of 28 type (1, 1) gCICYs as in the ambient spaces\nP4 \u00d7 P1;P3 \u00d7 P1 \u00d7 P1;P2 \u00d7 P2 \u00d7 P1;\nP2 \u00d7 P1 \u00d7 P1 \u00d7 P1;P1 \u00d7 P1 \u00d7 P1 \u00d7 P1 \u00d7 P1:\nAmong them, there are 16 gCICYs that are given by definite configuration matrices while the configuration matrices for the rest of the 12 depend on an arbitrary positive integer number i \u2208 Z>0. For example, one of them has the form\n2 64 P1 P1\nP3\n2\u00fe i 1\n0\n\u2212i 1\n4\n3 75; i \u2208 Z>0: \u00f08\u00de\nHere each integer number i will give us a configuration matrix, which may define a gCICY. It seems that such configuration matrices lead to an infinite number of gCICYs. However, as shown in [6], this infinite number of gCICYs can be either related to each other by the ineffective split transition and CICY identities [3] (type II/ III) or they are just the multiple copies of the same manifold (type I).2 Thus, there are only 28 simply connected type (1, 1) gCICYs. In this paper, we will apply machine learning algorithms to study the classification of the gCICYs. Although type (1, 1) gCICYs have been fully classified, we will use them as our first example to show that machine learning is a viable and powerful way to solve such kinds of mathematical problems."
        },
        {
            "heading": "1. Data generation",
            "text": "As a first step for this task, we need to generate a proper dataset for the ANN. Since the maximum number of rows\nin the product of projective spaces is 5, the configuration matrices can be expressed as 5 \u00d7 2 matrices. For matrices that are not in the size of 5 \u00d7 2, we will pad them with zeros. Thus, our data will be in the form of 5 \u00d7 2 with labels 1 for gCICYs and 0 for those that are not gCICYs. Machine learning algorithms normally require big data. But we have only 28 type (1, 1) gCICYs here. To construct a large enough dataset, we can take more representatives in the 12 classes of indefinite configuration matrices including\n(i) seven type II/III infinite classes. For each class, we take three matrices, with i \u00bc 1, 2, 3, respectively. Although they correspond to the same gCICY, from the perspective of configuration matrices, they are different. In this sense, they are useful for machine learning.\n(ii) five type I infinite classes. The different matrices in the type I class will lead to multiple copies of the same gCICY. The number of copies is controlled by the entry i in the matrices. Since we are interested in the simply connected gCICYs, we will only take the matrix with i \u00bc 1 in each class. Other than the above two kinds of matrices, we also put into our dataset the 16 gCICYs represented by definite configuration matrices. So overall, we have 16\u00fe 3 \u00d7 7\u00fe 5 \u00bc 42 different configuration matrices, which seems too less for machine learning algorithms. However, we can perform data enhancement by the permutation of rows and columns for these matrices and this will lead to a dataset with 2200 matrices. All of them describe codimensional (1, 1) gCICYs and we will denote them by M5\u00d72. Besides that, we also need another dataset of matrices\nM\u03035\u00d72 that do not correspond to gCICYs. These matrices should look similar to the configuration matrices of gCICYs so that one cannot distinguish them easily by simple principles without calculations. Besides the property of not being gCICY, we also require the M\u03035\u00d72 should satisfy the following conditions: (1) M\u03035\u00d72 should obey the Calabi-Yau condition. (2) The first column contains only semipositive integers\nand the second column contains at least one negative integer. (3) The entries of the first/second column in M\u03035\u00d72 should be in the range \u00bd0; 5 =\u00bd\u22123; 5 respectively, which is the same range for entries in M5\u00d72.\n(4) The matrices in M\u03035\u00d72 do not correspond to gCICYs. A large set of configuration matrices satisfying all of the above constraints can be constructed. We randomly choose 2200 of them for machine learning and denote them by M\u03035\u00d72 in the following. In summary, we have constructed M5\u00d72 consisting of 2200 matrices describing gCICYs and also M\u03035\u00d72 consisting of 2200 matrices that do not define gCICYs but otherwise are similarwith those inM5\u00d72.Wewill label them together as 2For a detailed discussion on different types of these equivalent classes, we refer to the original paper [6]. Notice that, by Wall\u2019s theorem [44,45], the configuration matrices from type II or III infinite classes may define different gCICYs, especially when quantum effects are taken into account [8,11].\n086004-4\nD \u00bc fM5\u00d72 \u2192 1g \u222a fM\u03035\u00d72 \u2192 0g; \u00f09\u00de\nand this is the dataset that we will use in machine learning. As is customary, we need to disjointly split the above dataset into a training set, a validation set, and a test set. We typically use 10% of the dataset for validation and the remainder of it for training and testing. All of them are randomly generated from the full dataset. The validation set is used to monitor learning progress during the training process and the test set is used to evaluate the trained neural network. Now let us turn to more details on the machine learning part."
        },
        {
            "heading": "2. Learning type (1, 1) gCICY",
            "text": "We will use a standard forward-feed, fully connected neural network for this task. This neural network is a map from the input data, a 5 \u00d7 2matrix, to output data, a length2 vector. The two components of the output vector summing to one can be understood as the probabilities for the matrix to define a gCICYor not. The architecture of the neural network is as follows:\nN 10\u00d72 \u00bc \u00f0F\u2218G10\u00d740; F\u2218G40\u00d720; S\u2218G20\u00d72\u00de; \u00f010\u00de where Gn\u00d7m is a fully connected layer of neurons with n inputs andm outputs, F is the standard Relu function and S is a softmax layer transforming two real numbers into a probability distribution of two possible outcomes. We implement this neural network using the PyTorch package [46]. We use 90% of the dataset (3960) for training and testing and 10% of them (440) for validation. We will denote them by T and V, respectively. The set T is further divided into different portions for training and testing. More specifically, we will train our neural network with 10%; 20%;\u2026; 90% of T and test the trained model on the rest of T and also the validation set V. We will train the neural network defined in (10) with Adam optimizer and \u201cCrossEntropyLoss\u201d loss function in 200 epochs. The initial learning rate is 0.025 and we will reduce it by half after every ten epochs. In Fig. 1, we showed the learning result for using only 10% of T , as we can see that the loss function converges nicely. Then, we evaluate our trained model and it can achieve 97.2% accuracy on the training set, 92.5% on the testing set, and 93.4% on the validation set. We also study the performance of our neural network trained with different portions of T . The complete result is presented in Fig. 2 where the error bar comes from the fact that each training set at a given percentage is randomly chosen 5 times and each of them gives a different result. One can see that accuracy of the training, testing, and validation increases with the size of training data and one can get as high as 96% on the testing set and 98% on the validation set. This is a rather convincing performance by a relatively simple feed-forward network."
        },
        {
            "heading": "3. Prediction",
            "text": "Besides the evaluation of the testing and validation set which are randomly chosen from the same dataset, it is\n086004-5\nmore interesting to ask if our trained model can extend their classification power to a completely new dataset. For this purpose, we will construct more type (1, 1) gCICYs beyond the current dataset D and test the predicting power of our trained model on this new dataset. According to the classification of type (1, 1) gCICYs, one can generate more gCICYs by including more configuration matrices in the seven infinite classes of type II/ III. We have considered the matrices with i \u00bc 1, 2, 3 and also their permutations in D. To get more gCICYs, we can generate seven more matrices by simply taking i \u00bc 4. Even though these new matrices define the same gCICYs which are already covered by the matrices with i \u00bc 1, 2, 3, we can still use them for prediction because it is not a priori that the trained machine learning model recognizes them. After padding and data enhancement, we can obtain 400 new 5 \u00d7 2 matrices and we denote this dataset by P, which is different from D by construction. Now, we can study the prediction power of our trained models using the dataset P. The result is shown in the bottom of Fig. 2. Again the error bar comes from the fact that the models are trained with five different sets of data that are randomly chosen from T in a fixed size. On average, we can see that our model can successfully predict at least 85% of the gCICYs in the new datasetP. Therefore, we can say that our machine learning model has learned the essential information to classify the gCICY and can be a useful tool beyond the training set."
        },
        {
            "heading": "B. Type (2, 1) gCICYs",
            "text": "The type (2, 1) gCICYs are defined by the complete intersection of two hypersurfaces determined by two polynomials and one hypersurface defined by a rational function. Their configuration matrices can generally be written as follows:\nX\u00f02;1\u00de \u00bc 2 666664 Pn1 Pn2 .. .\nPnN\na11 a 1 2 a21 a 2 2\n.. . .. .\nam1 a m 2\nb1\nb2\n..\n.\nbm\n3 777775 ; \u00f011\u00de\nwhere ai1; a i 2 are semipositive integers, b i are allowed to take negative integers and all the ai together with bi should satisfy the Calabi-Yau condition. For type (2, 1) gCICYs, there are only ten possible ambient spaces:\nP5 \u00d7 P1; P4 \u00d7 P2; P3 \u00d7 P3; P4 \u00d7 P1 \u00d7 P1;\nP3 \u00d7 P2 \u00d7 P1; P2 \u00d7 P2 \u00d7 P2; P3 \u00d7 P1 \u00d7 P1 \u00d7 P1;\nP2 \u00d7 P2 \u00d7 P1 \u00d7 P1; P2 \u00d7 P1 \u00d7 P1 \u00d7 P1 \u00d7 P1;\nP1 \u00d7 P1 \u00d7 P1 \u00d7 P1 \u00d7 P1 \u00d7 P1:\nThere exist many more (might be infinite) type (2, 1) gCICYs compared with the type (1, 1) case. As described in Sec. II, a gCICY configuration matrix should at least obey conditions (4) and (6), i.e., there should exist a nontrivial global section of L\u2261OM\u00f0b1; b2;\u2026; bm\u00de and the trivial line bundle cohomology on X should be h \u00f0X;O\u00de \u00bc f1; 0; 0; 1g. The configuration matrices satisfying both of the two conditions can be constructed as follows:\n(i) As shown in [6], to ensure the existence of global sections of L onM, the negative components bi can only appear in the configuration matrix in the following patterns, in one P1 or two P1 factors, or in one P2. Each matrix of this kind actually defines an infinite class of configuration matrices. The configuration matrix depending on arbitrary integers will be called generalized configuration matrix. In fact, one can show that for all the possible ambient spaces, there are totally 34, 192 generalized configuration matrices.3\n(ii) One also needs to impose the condition that h \u00f0X;O\u00de \u00bc f1; 0; 0; 1g. This can remove the type I infinite class, but not the type II/III.4 Unfortunately, up to now, there does not exist a classification for type (2, 1) gCICYs. However, we can still construct a subset of them with a significant number of data which is sufficient for machine learning. Following the previous work [6], we start with the 34, 192 generalized configuration matrices and allow the parameter to vary from 1 to 4. In this way, one can obtain 65, 148 independent configuration matrices with the most negative entry greater or equal to \u22124. Then, we need to compute the line bundle cohomology of h0\u00f0M;L\u00de and h \u00f0X;O\u00de for these configuration matrices. The details on the calculation of the line bundle cohomology are reviewed in the Appendix. Different from the previous scan in [6], we remove the computing time constraint and consider all these 65, 148 matrices. The computation of line bundle cohomology is in general time consuming and formidable to do on a laptop, so we finished our scan on the \u201cDragonTooth\u201d cluster at Virginia Tech. Our result yielded 3, 962 type (2, 1) gCICYs that include the 2, 862 gCICYs found in the previous scan of [6]. The distribution of these gCICYs among different ambient spaces is given in Table I. In summary, we construct the complete dataset for all codimension (2, 1) gCICYs with negative entries larger than \u22125. The explicit calculation becomes more and more time consuming as we decrease the negative entries in the configuration matrices. In the later part of this section, we\n3Notice that this is only a necessary condition. One still needs to compute h0\u00f0M;L\u00de explicitly to make sure a given configuration matrix indeed defines a gCICY.\n4It is not clear how to remove the type II/III redundancies yet, as there may still exist some redundancies in the counting of these spaces like ineffective splits [6].\n086004-6\nwill try to use the trained model for matrices with negative entry greater than \u22124 to do some prediction on cases with negative entry smaller than \u22125, to see if it can give us some clue to resolve this classification problem in the future."
        },
        {
            "heading": "1. Data generation",
            "text": "We take the scan result described above to be the dataset for machine learning. Specifically, we have 3,962 configuration matrices as gCICYs and 61, 116 matrices as nongCICYs. Notice that the latter 61, 116 matrices, except for the property of not being gCICYs, are in many ways similar to the former 3, 962 matrices. After all, both of them are generated from the same generalized configuration matrices with entries in the same range \u00bd\u22124; 7 . For this reason, those matrices form the ideal dataset for our classification problem. To generate the dataset for machine learning, we will first pad all these matrices into the form of 6 \u00d7 3. This is the maximum dimension for matrices considered in our case. Furthermore, we can perform rows and columns (only the first two) of permutations to these matrices. This leads to a total of 2, 046, 858 matrices for gCICYs and 42, 660, 174 matrices for those which are not gCICYs. We randomly choose 100, 000 matrices from each of these two classes and label the gCICYs as 1 and 0 for those which are not gCICYs, and we denote this dataset as D."
        },
        {
            "heading": "2. Learning type (2, 1) gCICYs",
            "text": "Similar to the type (1, 1) case, we will use a forwardfeed, fully connected neural network here. But, the structure of the neural network is more complicated. It is given by\nN 18\u00d72 \u00bc \u00f0F\u2218G18\u00d7100; F\u2218G100\u00d760; F\u2218G60\u00d730; F\u2218G30\u00d715; F\u2218G15\u00d78; S\u2218G8\u00d72\u00de; \u00f012\u00de\nwhere the input is a 6 \u00d7 3 matrix while the output is a length-2 vector representing the probabilities for the matrix\nto be gCICY or not gCICY. We train this network in 200 epochs. The initial learning rate is 0.01 and will be reduced by 70% after every ten epochs. The optimizer is Adam and the loss function is \u201cCrossEntropyLoss.\u201d From the dataset D, we randomly choose 90% of them for training and testing, denoted by T . The rest of 10% of them (20, 000) will be used for validation denoted by V. Again, we will train our neural network with 10%; 20%;\u2026; 90% of T and evaluate it on V every epoch. We implement this neural network with the PyTorch package and train it with the dataset generated above. In Fig. 3, we plot the training curve for the model trained with 10% of T . As can be seen from this figure, the loss function converges nicely. The trained model can achieve 94.4%, 89.3%, and 89.6% on training, testing, and validation set. We train our neural network with different portions of T and evaluate the trained models on the testing and validation set. The training and evaluation process is repeated 5 times, and the result is plotted in the top of Fig. 4. The error bar reflects the stability of our result when we chose\nTABLE I. The distribution of codimension (2, 1) gCICYs founded in products of projective spaces.\nEmbedding projective spaces Number of classes of generalized configuration matrices Number of spaces found in previous scan [6] Number of spaces found in our scan"
        },
        {
            "heading": "P5 \u00d7 P1 168 28 67",
            "text": ""
        },
        {
            "heading": "P4 \u00d7 P2 210 6 9",
            "text": ""
        },
        {
            "heading": "P4 \u00d7 P1 \u00d7 P1 1,197 229 369",
            "text": ""
        },
        {
            "heading": "P3 \u00d7 P2 \u00d7 P1 1,800 263 341",
            "text": ""
        },
        {
            "heading": "P2 \u00d7 P2 \u00d7 P2 550 12 12",
            "text": ""
        },
        {
            "heading": "P3 \u00d7 P1 \u00d7 P1 \u00d7 P1 4,410 545 860",
            "text": ""
        },
        {
            "heading": "P2 \u00d7 P2 \u00d7 P1 \u00d7 P1 5,235 520 683",
            "text": ""
        },
        {
            "heading": "P2 \u00d7 P1 \u00d7 P1 \u00d7 P1 \u00d7 P1 12,180 770 1098",
            "text": ""
        },
        {
            "heading": "P1 \u00d7 P1 \u00d7 P1 \u00d7 P1 \u00d7 P1 \u00d7 P1 8,442 360 523",
            "text": "Total 34,192 2,733 3,962\n086004-7\ndifferent training sets (at a fixed percentage of T ) randomly. As before, one can see that the accuracy of the training, testing and validation increases with the size of the training data. The accuracy on both testing and validation set are above 92%. This shows that our trained neural network can effectively distinguish the configuration matrices of type (2, 1) gCICYs from general 6 \u00d7 3 matrices. However, the matrices considered here are only for type (2, 1) gCICYs with smallest entries \u22124. It is more interesting to check if our trained model can predict the gCICYs that are unseen in the dataset D."
        },
        {
            "heading": "3. Prediction",
            "text": "To do that, we need to construct a new dataset that should contain matrices unseen in the training set D. This new set can be generated in the following steps. From the 34, 192 generalized configuration matrices, by taking the parameter to be 5 or 6, we can have a much larger set of possible matrices. To find gCICYs, we did a scan by computing the line bundle cohomologies of h0\u00f0M;L\u00de and h \u00f0X;O\u00de for these matrices. Unfortunately, the time complexity of the computation increases dramatically with the increase of the\nentries in the matrix.5 From our preliminary scan, we find 45, 259 gCICYs and 229, 540 non-gCICYs. After padding and permutation of rows and the first two columns, we obtain 460, 122 gCICYs and 4, 408, 940 non-gCICYs. We randomly choose 10, 000 in each case and denoted by P. Notice that every matrix in the prediction dataset P contains \u22125 or \u22126 while the smallest entry of the matrix in D is \u22124. Thus, the set P is unseen in the training set D and can be used for prediction. We compute the accuracy of our trained model on the prediction set P and plot the result in the bottom of Fig. 2. Again, the evaluation is performed on models trained with five different datasets in the size. We can see that our model can achieve up to 97% accuracy for prediction. Therefore, we conclude that machine learning is an effective and useful tool to study the classification of gCICY. With the trained model, we find 16, 014 gCICYs from 92, 867 configuration matrices with most negative entries \u22125 and 20, 167 gCICYs from 125, 104 configuration matrices with most negative entries \u22126. Because of the high accuracy of the prediction, we believe that our machine learning model can find almost all gCICYs with the most negative entries \u22125 and \u22126. The configuration matrices of these predicted gCICYs can be found in [47]."
        },
        {
            "heading": "IV. CONCLUSION AND OUTLOOK",
            "text": "In this paper, we used machine learning techniques to study the classification of gCICYs. For data preparation and for its usefulness, we fully classified a subclass of type (2, 1) gCICYs, namely, type (2, 1) gCICYs with negative entries greater than or equal to \u22124. This work extends the partial work in this direction initiated in [6]. To show the effectiveness of machine learning, we first used an ANN model with three layers to study the fully classified type (1, 1) gCICYs. By tuning hyperparameters, we can get a model which can achieve as high as 96% precision on testing and 98% on validation. In machine learning, we usually choose a small portion of the data to train the model. Thus, if we get a model which performs very well in the training, this model may not do well for prediction just because it may learn the features belonging to the training data only. This phenomenon is known as overfitting in the machine learning literature. However, in our case here, the model we got could achieve 85% precision in prediction. Given the relatively small number of type (1, 1) gCICYs and the simple structure of the neural network used for this preliminary work, this result indicates the strong potential of machine learning for this kind of classification problem. The high precision on prediction in this part gives us the confidence that machine learning could be a reliable tool in classifying the large number of unknown gCICYs.\n5We impose a constraint in the scan by only considering the cases without involving complicated polynomial computation.\n086004-8\nTo confirm this idea, we moved on to study the classification of type (2, 1) gCICYs. Unlike type (1, 1) gCICYs, there are significantly more type (2, 1) gCICYs. As previously stated, a classification only exists for those with negative entries greater than or equal to \u22124. So type (2, 1) gCICYs provide an ideal arena for machine learning, given the large datasets and potential for discovering new insights. In the first part of this study, we trained a neural network model with six layers. The data used are those classified with entries less than 4. Again, in this case, we can get at least 92% precision on both testing and validation. This result is not that unexpected since, given enough data and epochs for training, we could get even higher precision. The problem is, can we generalize this model to something we do not know? Can we really use machine learning techniques to predict new knowledge? To answer these two questions, we use the trained model (from the data with negative entries no smaller than 4) to data with negative entries bigger than 4. In order to have some sense of how good the model\u2019s performance is, we first tried this procedure on some data which we already knew are gCICYs or not, and also of course they should have negative entries smaller than 4. The result of this prediction part is great, 97%. This means that the model trained from the data with negative entries no smaller than 4 should have already learned gCICYs\u2019main common features which can generalize to all gCICYs regardless of the range of the negative entries. Given this fact, we did a prediction by using this model for more gCICYs that are not classified yet. The predicted gCICYs are stored in the files g21N5.mx and g21N6.mx. This prediction should have a high precision as indicated by the experiment. From the experiments we did for type (1, 1) and type (2, 1) gCICYs, we can see that the neural network works very well for the classification of gCICYs. Given that the precision for prediction is so high, especially for type (2, 1) gCICYs, we are confident that neural networks could actually do more. One possible direction is that neural networks may be able to help us to find a quick analytic way to judge if one configuration matrix represents a gCICY or not. Another possible direction is that we can try to apply other machine learning techniques, like reinforcement learning [22,48\u201353], to this problem. Unlike the work with a neural network, for reinforcement learning, the model is not trained by a subset of data first and then generalized to more data. Instead, reinforcement learning will do the work in an unsupervised way and could give us positive examples constantly. So it is natural to see if this technique is more time competitive compared with the neural networks. On the other hand, we can use the machine learning technique to explore some substructure of gCICY, especially the discrete symmetry structure considered in both CICY cases [54,55] and toric cases [56]. It is interesting to find the Z2 orientifold structure [57\u201359] in\ngCICYs that will be important for type II string compactifications. Besides, the numerical Ricci-flat metric of Calabi-Yau manifolds has been studied for the regular CICY [39\u201343,60\u201366]. We could also study how to extend these methods to compute the numerical metric of gCICYs. In addition, it is interesting to study the heterotic model building on the gCICYs. The sum of line bundle models has been considered in [67]. We will study the heterotic models with the more general Monad bundles."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "We want to thank Tristan Hubsch, Mohsen Karkheiran, James Gray, and Aleksei Zaitsev for the useful discussion. X. Gao was supported in part by NSFC under Grant No. 12005150.W. Cui is supported in part by the fellowship of China Postdoctoral Science Foundation No. 2022M7 20507. The authors acknowledge Advanced Research Computing at Virginia Tech for providing computational resources and technical support that have contributed to the results reportedwithin this paper.W.Cuiwould like to thank the Tsinghua Sanya International Mathematics Forum (TSIMF) for hospitality where part of the work was completed."
        },
        {
            "heading": "APPENDIX: CALCULATION OF LINE BUNDLE COHOMOLOGY",
            "text": "In order to understand the first gCICY condition (4) and also how we use it to generate more data for prediction, we will review the calculation of the line bundle cohomology. We need to calculate the cohomology of L, which is given by the last column of X\u00f02;1\u00de:\nL \u00bc O\u00f0b1; b2;\u2026; bm\u00de: \u00f0A1\u00de More specifically, what we need to calculate is\nh0\u00f0M;L\u00de \u00f0A2\u00de and to do this we need to consider the following Koszul sequence:\n0\u2192N 1\u2297N 2\u2297L\u2192 \u00f0N 1\u2295N 2\u00de\u2297L\u2192L\u2192LjM\u21920; \u00f0A3\u00de\nwhich could be further broken up into two short exact sequences:\n0\u2192N 1\u2297N 2\u2297L\u2192 \u00f0N 1\u2295N 2\u00de\u2297L\u2192K\u21920; \u00f0A4\u00de\n0 \u2192 K \u2192 L \u2192 LjM \u2192 0; \u00f0A5\u00de\nwhere K is a carefully designed vector bundle to keep the exactness of the sequence. The long exact sequences of cohomology associated with the above two short exact sequences are given by\n086004-9\n0 \u2192 h0\u00f0A;N 1 \u2297 N 2 \u2297 L\u00de \u2192 h0\u00f0A; \u00f0N 1 \u2295 N 2\u00de \u2297 L\u00de \u2192 h0\u00f0A;K\u00de \u2192 h1\u00f0A;N 1 \u2297 N 2 \u2297 L\u00de \u2192 h1\u00f0A; \u00f0N 1 \u2295 N 2\u00de \u2297 L\u00de \u2192 h1\u00f0A;K\u00de \u2192\nand\n0 \u2192 h0\u00f0A;K\u00de \u2192 h0\u00f0A;L\u00de \u2192 h0\u00f0M;L\u00de \u2192 h1\u00f0A;K\u00de \u2192 h1\u00f0A;L\u00de \u2192 h1\u00f0M;L\u00de \u2192 :\nTo write the above complicated-looking sequences in a concise way, we can put them into the following cohomology tables:\nN 1 \u2297 N 2 \u2297 L \u00f0N 1 \u2295 N 2\u00de \u2297 L K h0 a01 a 0 2 a 0 3\nh1 a11 a 1 2 a 1 3\nh2 a21 a 2 2 a 2 3\nh3 a31 a 3 2 a 3 3\nh4 a41 a 4 2 a 4 3\n; \u00f0A6\u00de\nK L LjM h0 a03 a 0 4 a 0 5\nh1 a13 a 1 4 a 1 5\nh2 a23 a 2 4 a 2 5\nh3 a33 a 3 4 a 3 5\nh4 a43 a 4 4 a 4 5\n: \u00f0A7\u00de\nHere each entry aij represents the cohomology of a certain line bundle. For example, a01 is just the zeroth cohomology ofN 1 \u2297 N 2 \u2297 L onA, h0\u00f0A;N 1 \u2297 N 2 \u2297 L\u00de. The basic idea of getting h0\u00f0M;L\u00de is to first calculate cohomologies of certain line bundles on the product of projective spaces and then use maps from the above long exact sequences. From the K\u00fcnneth formula we can know that, on the product of projective spaces, a line bundle\u2019s cohomology can be written as the product form:\nhn\u00f0Pn1 \u00d7 \u00d7 Pnm;O\u00f0q1;\u2026; qm\u00de\u00de \u00bc \u2a01\nk1\u00fe \u00fekm\u00bcn hk1\u00f0Pn1 ;O\u00f0q1\u00de\u00de \u00d7 \u00d7 hkm\u00f0Pnm;O\u00f0qm\u00de\u00de:\n\u00f0A8\u00de\nOn a single productive space, a line bundle\u2019s cohomology hq\u00f0Pn;O\u00f0k\u00de\u00de is given by\nhq\u00f0Pn;O\u00f0k\u00de\u00de \u00bc 8>>>< >>>: k\u00fen n ; q \u00bc 0; k > \u22121 1; q \u00bc n; k \u00bc \u2212n \u2212 1 \u2212k\u22121 \u2212k\u2212n\u22121 ; q \u00bc n; k < \u2212n \u2212 1\n0; otherwise:\n\u00f0A9\u00de\nOne important thing here we will use heavily later is that on a projective space Pn, if the line bundle O has a negative entry, then the only possible nonzero cohomology should be hn\u00f0Pn;O\u00de. Let us look at one example to see how the rewriting of the cohomology table can help us to chase the calculation of cohomology and even let us select certain examples in an algorithmic way. The configuration matrix we want to look at is\nP1\nP5 0 14 3 1\u22121 : \u00f0A10\u00de\nFor this case, we need to calculate the cohomology of O\u00f01;\u22121\u00de on\nM \u00bc P1\nP5 0 14 3 : \u00f0A11\u00de\nThe long exact sequence for this calculation is\n0 \u2192 O\u00f00;\u22128\u00de \u2192 O\u00f01;\u22125\u00de \u2295 O\u00f00;\u22123\u00de \u2192 O\u00f0\u22121; 1\u00de \u2192 O\u00f0\u22121; 1\u00dejM \u2192 0; \u00f0A12\u00de\nfrom which we can get the following cohomology table according to (A6):\nO\u00f00;\u22128\u00de O\u00f01;\u22125\u00de \u2295 O\u00f00;\u22123\u00de K h0 0 0 a05 h1 0 0 a15 h2 0 0 a25 h3 0 0 a35 h4 0 0 a45 h5 21 0 a55 : \u00f0A13\u00de\nHere all the negative entries are on theP5 factor, whichmeans that the only nonzero cohomologies should be in the h5 row. By chasing the long exact sequence along with this table, it is not hard to see that the only nonzero cohomology of K is h4\u00f0P1 \u00d7 P5; K\u00de \u00bc 21. Through a similar procedure, it is not hard to write down the other part of the cohomology table which corresponds to (A7):\n086004-10\nK O\u00f01;\u22121\u00de O\u00f01;\u22121\u00dejM h0 0 0 0\nh1 0 0 0\nh2 0 0 0\nh3 0 0 21\nh4 21 0 0\nh5 0 0 0\n: \u00f0A14\u00de\nFrom this example we can see that h5 of O\u00f00;\u22128\u00de finally maps to h3 of O\u00f01;\u22121\u00dejM. More precisely, we have exactly two maps, h5\u00f0P1 \u00d7 P5;O\u00f00;\u22128\u00de\u00de first maps to h4\u00f0P1 \u00d7 P5;O\u00f01;\u22125\u00de \u2295 O\u00f00;\u22123\u00de\u00de, and then to h3\u00f0M;O\u00f01;\u22121\u00de\u00de. So if we look at the position of the number 21 in the cohomology table from left to right, it seems like it is climbing a ladder, from the row of h5 to the row of h3. This phenomenon follows from the exactness of the exact sequence and the number of maps, and it is actually determined by the cohomology table\u2019s column number (which is actually determined by the codimension of M). So from the analysis of this example, we can say that a configuration matrix in the following form,\nP1\nP5 \u2026 \u2026 \u2026\u2212n\n\u00f0A15\u00de\nwith n > 0, will not give us a gCICY. The basic idea is, by using the Bott-Borel-Weil theorem and also the K\u00fcnneth formula, the nonzero cohomologies should all be in row h5 or row h6. But given that there are as many as two maps in the cohomology table, we can at most have a nonzero h3 or h4 for L on M. More explicitly, the first part of the cohomology table for (A15) could be\nN 1 \u2297 N 2 \u2297 L \u00f0N 1 \u2295 N 2\u00de \u2297 L K h0 0 0 0\nh1 0 0 0\nh2 0 0 0\nh3 0 0 0\nh4 0 0 z\nh5 x y y \u2212 x\u00fe z\n;\nz \u00bc ker\u00f0x \u2192 y\u00de \u00f0A16\u00de\nor\nN 1 \u2297 N 2 \u2297 L \u00f0N 1 \u2295 N 2\u00de \u2297 L K h0 0 0 0\nh1 0 0 0\nh2 0 0 0\nh3 0 0 0\nh4 0 0 0\nh5 0 x x\u00fe u h6 y z z \u2212 y\u00fe u\n;\nu \u00bc ker\u00f0y \u2192 z\u00de: \u00f0A17\u00de\nNo matter in any case, the only possible nonzero cohomology for K could be at row h3, h4, or h5. So without writing down the other part of the cohomology table and doing the chasing, we can see clearly that there is no chance to get a nonzero h0\u00f0M;L\u00de, there is just not enough steps for any of the nonzero hi to \u201cclimb\u201d up to the row of h0 of L on M. This example just illustrates one way that we used to rule out configuration matrices that will not give us gCICYs. We refer to [6] for a more general discussion.\n[1] T. Hubsch, Commun. Math. Phys. 108, 291 (1987). [2] P. Green and T. Hubsch, Commun. Math. Phys. 109, 99\n(1987). [3] P. Candelas, A. M. Dale, C. A. Lutken, and R. Schimmrigk,\nNucl. Phys. B298, 493 (1988). [4] P. Candelas, C. A. Lutken, and R. Schimmrigk, Nucl. Phys.\nB306, 113 (1988). [5] L. B. Anderson, X. Gao, J. Gray, and S.-J. Lee, J. High\nEnergy Phys. 10 (2017) 077. [6] L. B. Anderson, F. Apruzzi, X. Gao, J. Gray, and S.-J. Lee,\nNucl. Phys. B906, 441 (2016). [7] L. B. Anderson, F. Apruzzi, X. Gao, J. Gray, and S.-J. Lee,\nPhys. Rev. D 93, 086001 (2016). [8] P. Berglund and T. H\u00fcbsch, Adv. Theor. Math. Phys. 22, 261\n(2018).\n[9] Q. Jia and H. Lin, J. Math. Phys. (N.Y.) 61, 052301 (2020). [10] P. Berglund and T. Hubsch, SciPost Phys. 4, 009 (2018). [11] P. Berglund and T. H\u00fcbsch, arXiv:2205.12827. [12] A. Garbagnati and B. van Geemen, Nucl. Phys. B925, 135\n(2017). [13] M. A. Nielsen, Neural Networks and Deep Learning\n(Determination Press, San Francisco, 2015). [14] F. Ruehle, Phys. Rep. 839, 1 (2020). [15] Y.-H. He, From the string landscape to the mathematical\nlandscape: A machine-learning outlook, in Lie Theory and Its Applications in Physics. LT 2021. Springer Proceedings in Mathematics & Statistics (Springer, Singapore, 2022), Vol. 396, 10.1007/978-981-19-4751-3_2. [16] A. Cole and G. Shiu, J. High Energy Phys. 03 (2019) 054.\n086004-11\n[17] A. Cole, A. Schachner, and G. Shiu, J. High Energy Phys. 11 (2019) 045. [18] S. Krippendorf, R. Kroepsch, and M. Syvaeri, arXiv:2107.04039. [19] A. Cole, S. Krippendorf, A. Schachner, and G. Shiu, in 35th Conference on Neural Information Processing Systems (Curran Associates, 2021), arXiv:2111.11466. [20] Y.-H. He,Machine-Learning the Landscape (Springer, New York, 2021). [21] F. Ruehle, J. High Energy Phys. 08 (2017) 038. [22] J. Halverson, B. Nelson, and F. Ruehle, J. High Energy\nPhys. 06 (2019) 003. [23] Y.-H. He, Int. J. Mod. Phys. A 36, 2130017 (2021). [24] I. Bena, J. Blab\u00e4ck, M. Gra\u00f1a, and S. L\u00fcst, Adv. Applied\nClifford Algebras 32, 7 (2022). [25] Y.-H. He, S. Lal, and M. Z. Zaz, arXiv:2111.04761. [26] J. Carifio, J. Halverson, D. Krioukov, and B. D. Nelson,\nJ. High Energy Phys. 09 (2017) 157. [27] Y.-N. Wang and Z. Zhang, J. High Energy Phys. 08 (2018)\n009. [28] M. Bies, M. Cveti\u010d, R. Donagi, L. Lin, M. Liu, and F.\nRuehle, J. High Energy Phys. 01 (2021) 196. [29] X. Gao and H. Zou, Phys. Rev. D 105, 046017 (2022). [30] R. Altman, J. Carifio, J. Halverson, and B. D. Nelson,\nJ. High Energy Phys. 03 (2019) 186. [31] M. Demirtas, L. McAllister, and A. Rios-Tascon, Fortschr.\nPhys. 68, 2000086 (2020). [32] K. Bull, Y.-H. He, V. Jejjala, and C. Mishra, Phys. Lett. B\n785, 65 (2018). [33] Y.-H. He, The Calabi\u2013Yau Landscape: From Geometry, to\nPhysics, to Machine Learning, Lecture Notes in Mathematics (Springer, 2021). [34] Y.-H. He and A. Lukas, Phys. Lett. B 815, 136139 (2021). [35] H. Erbin and R. Finotello, Phys. Rev. D 103, 126014 (2021). [36] D. S. Berman, Y.-H. He, and E. Hirst, Phys. Rev. D 105,\n066002 (2022). [37] D. Klaewer and L. Schlechter, Phys. Lett. B 789, 438\n(2019). [38] C. R. Brodie, A. Constantin, R. Deen, and A. Lukas,\nFortschr. Phys. 68, 1900087 (2020). [39] L. B. Anderson, M. Gerdes, J. Gray, S. Krippendorf, N.\nRaghuram, and F. Ruehle, J. High Energy Phys. 05 (2021) 013. [40] V. Jejjala, D. K. Mayorga Pena, and C. Mishra, J. High Energy Phys. 08 (2022) 105. [41] M. R. Douglas, S. Lakshminarasimhan, and Y. Qi, arXiv:2012.04797. [42] M. Larfors, A. Lukas, F. Ruehle, and R. Schneider, arXiv:2111.01436.\n[43] M. Larfors, A. Lukas, F. Ruehle, and R. Schneider, Mach. Learn. Sci. Tech. 3, 035014 (2022). [44] C. T. C. Wall, Inventiones Mathematicae 1, 355 (1966). [45] T. Hubsch, Calabi-Yau Manifolds: A Bestiary for Physicists\n(World Scientific, Singapore, 1994). [46] A. e. a. Paszke, In Advances in Neural Information Process-\ning Systems 32 (Curran Associates, Inc., San Jose, 2019). [47] See Supplemental Material at http://link.aps.org/\nsupplemental/10.1103/PhysRevD.107.086004, for predicted configuration matrices for type \u00f02; 1\u00de gCICY with smallest entries \u22125 and \u22126 are in the Mathematica files g21N5.mx and g21N6.mx attached with the paper. [48] M. Larfors and R. Schneider, Fortschr. Phys. 68, 2000034 (2020). [49] S. Gukov, J. Halverson, F. Ruehle, and P. Su\u0142kowski, Mach. Learn. Sci. Tech. 2, 025035 (2021). [50] T. R. Harvey and A. Lukas, J. High Energy Phys. 08 (2021) 161. [51] A. Constantin, T. R. Harvey, and A. Lukas, Fortschr. Phys. 70, 2100186 (2022). [52] S. Abel, A. Constantin, T. R. Harvey, and A. Lukas, Fortschr. Phys. 70, 2200034 (2022). [53] G. K\u00e1ntor, V. Niarchos, and C. Papageorgakis, Phys. Rev. D 105, 025018 (2022). [54] V. Braun, J. High Energy Phys. 04 (2011) 005. [55] J. Gray and J. Wang, J. High Energy Phys. 07 (2022) 116. [56] A. Braun, A. Lukas, and C. Sun, Commun. Math. Phys.\n360, 935 (2018). [57] F. Carta, J. Moritz, and A. Westphal, J. High Energy Phys.\n05 (2020) 107. [58] R. Altman, J. Carifio, X. Gao, and B. D. Nelson, J. High\nEnergy Phys. 03 (2022) 087. [59] C. Crin\u00f2, F. Quevedo, A. Schachner, and R. Valandro,\nJ. High Energy Phys. 08 (2022) 050. [60] S. K. Donaldson, Some Numerical Results in Complex\nDifferential Geometry (2005), arXiv:math/0512625. [61] M. Headrick and T. Wiseman, Classical Quantum Gravity\n22, 4931 (2005). [62] V. Braun, T. Brelidze, M. R. Douglas, and B. A. Ovrut,\nJ. High Energy Phys. 05 (2008) 080. [63] M. R. Douglas, R. L. Karp, S. Lukic, and R. Reinbacher,\nJ. Math. Phys. (N.Y.) 49, 032302 (2008). [64] M. R. Douglas, R. L. Karp, S. Lukic, and R. Reinbacher,\nJ. High Energy Phys. 12 (2007) 083. [65] M. Headrick and A. Nassar, Adv. Theor. Math. Phys. 17,\n867 (2013). [66] W. Cui and J. Gray, J. High Energy Phys. 05 (2020) 044. [67] M. Larfors, D. Passaro, and R. Schneider, J. High Energy\nPhys. 05 (2021) 105.\n086004-12"
        }
    ],
    "title": "Machine learning on generalized complete intersection Calabi-Yau manifolds",
    "year": 2023
}