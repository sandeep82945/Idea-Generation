{
    "abstractText": "Multi-task Imitation Learning (MIL) aims to train a policy capable of performing a distribution of tasks based on multi-task expert demonstrations, which is essential for general-purpose robots. Existing MIL algorithms suffer from low data efficiency and poor performance on complex longhorizontal tasks. We develop Multi-task Hierarchical Adversarial Inverse Reinforcement Learning (MH-AIRL) to learn hierarchically-structured multi-task policies, which is more beneficial for compositional tasks with long horizons and has higher expert data efficiency through identifying and transferring reusable basic skills across tasks. To realize this, MH-AIRL effectively synthesizes context-based multi-task learning, AIRL (an IL approach), and hierarchical policy learning. Further, MH-AIRL can be adopted to demonstrations without the task or skill annotations (i.e., stateaction pairs only) which are more accessible in practice. Theoretical justifications are provided for each module of MH-AIRL, and evaluations on challenging multi-task settings demonstrate superior performance and transferability of the multitask policies learned with MH-AIRL as compared to SOTA MIL baselines.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jiayu Chen"
        },
        {
            "affiliations": [],
            "name": "Dipesh Tamboli"
        },
        {
            "affiliations": [],
            "name": "Tian Lan"
        },
        {
            "affiliations": [],
            "name": "Vaneet Aggarwal"
        }
    ],
    "id": "SP:050191373a4f87eec0250d8a5aa2fa46b2e5e300",
    "references": [
        {
            "authors": [
                "A.O. Al-Abbasi",
                "A. Ghosh",
                "V. Aggarwal"
            ],
            "title": "Deeppool: Distributed model-free algorithm for ride-sharing using deep reinforcement learning",
            "venue": "IEEE Transactions on Intelligent Transportation Systems,",
            "year": 2019
        },
        {
            "authors": [
                "J. Andreas",
                "D. Klein",
                "S. Levine"
            ],
            "title": "Modular multitask reinforcement learning with policy sketches",
            "venue": "In Proceedings of the 34th International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "A. Balachandran",
                "V. Aggarwal",
                "E. Halepovic",
                "J. Pang",
                "S. Seshan",
                "S. Venkataraman",
                "H. Yan"
            ],
            "title": "Modeling web quality-of-experience on cellular networks",
            "venue": "In Proceedings of the 20th annual international conference on Mobile computing and networking,",
            "year": 2014
        },
        {
            "authors": [
                "X. Bian",
                "O.M. Maldonado",
                "S. Hadfield"
            ],
            "title": "SKILL-IL: disentangling skill and knowledge in multitask imitation learning",
            "venue": "In IEEE/RSJ International Conference on Intelligent Robots and Systems,",
            "year": 2022
        },
        {
            "authors": [
                "L. Bottou"
            ],
            "title": "Large-scale machine learning with stochastic gradient descent",
            "venue": "In Proceedings of the 19th International Conference on Computational Statistics,",
            "year": 2010
        },
        {
            "authors": [
                "J. Chen",
                "A.K. Umrawal",
                "T. Lan",
                "V. Aggarwal"
            ],
            "title": "Deepfreight: A model-free deep-reinforcement-learning-based algorithm for multi-transfer freight delivery",
            "venue": "In Proceedings of the 31st International Conference on Automated Planning and Scheduling,",
            "year": 2021
        },
        {
            "authors": [
                "J. Chen",
                "V. Aggarwal",
                "T. Lan"
            ],
            "title": "ODPP: A unified algorithm framework for unsupervised option discovery based on determinantal point process",
            "venue": "CoRR, abs/2212.00211,",
            "year": 2022
        },
        {
            "authors": [
                "J. Chen",
                "T. Lan",
                "V. Aggarwal"
            ],
            "title": "Multi-agent covering option discovery based on kronecker product of factor graphs",
            "venue": "IEEE Transactions on Artificial Intelligence, pp",
            "year": 2022
        },
        {
            "authors": [
                "J. Chen",
                "T. Lan",
                "V. Aggarwal"
            ],
            "title": "Scalable multi-agent covering option discovery based on kronecker graphs",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "J. Chen",
                "M. Haliem",
                "T. Lan",
                "V. Aggarwal"
            ],
            "title": "Multi-agent deep covering option discovery",
            "venue": "CoRR, abs/2210.03269,",
            "year": 2022
        },
        {
            "authors": [
                "J. Chen",
                "T. Lan",
                "V. Aggarwal"
            ],
            "title": "Option-aware adversarial inverse reinforcement learning for robotic control",
            "venue": "arXiv preprint arXiv:2210.01969,",
            "year": 2022
        },
        {
            "authors": [
                "J. Chung",
                "K. Kastner",
                "L. Dinh",
                "K. Goel",
                "A.C. Courville",
                "Y. Bengio"
            ],
            "title": "A recurrent latent variable model for sequential data",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2015
        },
        {
            "authors": [
                "T.M. Cover"
            ],
            "title": "Elements of information theory",
            "year": 1999
        },
        {
            "authors": [
                "M.P. Deisenroth",
                "P. Englert",
                "J. Peters",
                "D. Fox"
            ],
            "title": "Multitask policy search for robotics",
            "venue": "In IEEE International Conference on Robotics and Automation,",
            "year": 2014
        },
        {
            "authors": [
                "C. Devin",
                "D. Geng",
                "P. Abbeel",
                "T. Darrell",
                "S. Levine"
            ],
            "title": "Compositional plan vectors",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2019
        },
        {
            "authors": [
                "N. Duminy",
                "S.M. Nguyen",
                "J. Zhu",
                "D. Duhaut",
                "J. Kerdreux"
            ],
            "title": "Intrinsically motivated open-ended multi-task learning using transfer learning to discover task",
            "venue": "hierarchy. CoRR,",
            "year": 2021
        },
        {
            "authors": [
                "S.R. Eddy"
            ],
            "title": "Hidden markov models",
            "venue": "Current Opinion in Structural Biology,",
            "year": 1996
        },
        {
            "authors": [
                "C. Finn",
                "P. Abbeel",
                "S. Levine"
            ],
            "title": "Model-agnostic metalearning for fast adaptation of deep networks",
            "venue": "In Proceedings of the 34th International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "C. Finn",
                "T. Yu",
                "T. Zhang",
                "P. Abbeel",
                "S. Levine"
            ],
            "title": "Oneshot visual imitation learning via meta-learning",
            "venue": "In Proceedings of the 1st Annual Conference on Robot Learning,",
            "year": 2017
        },
        {
            "authors": [
                "C. Florensa",
                "Y. Duan",
                "P. Abbeel"
            ],
            "title": "Stochastic neural networks for hierarchical reinforcement learning",
            "venue": "In Proceedings of the 5th International Conference on Learning Representations. OpenReview.net,",
            "year": 2017
        },
        {
            "authors": [
                "R. Fox",
                "R. Berenstein",
                "I. Stoica",
                "K. Goldberg"
            ],
            "title": "Multitask hierarchical imitation learning for home automation",
            "venue": "In Proceedings of the 15th IEEE International Conference on Automation Science and Engineering,",
            "year": 2019
        },
        {
            "authors": [
                "J. Fu",
                "K. Luo",
                "S. Levine"
            ],
            "title": "Learning robust rewards with adversarial inverse reinforcement learning",
            "year": 2017
        },
        {
            "authors": [
                "J. Fu",
                "A. Kumar",
                "O. Nachum",
                "G. Tucker",
                "S. Levine"
            ],
            "title": "D4RL: datasets for deep data-driven reinforcement learning",
            "year": 2004
        },
        {
            "authors": [
                "X. Fu",
                "D. Peddireddy",
                "V. Aggarwal",
                "Jun",
                "M.B.-G"
            ],
            "title": "Improved dexel representation: A 3-d cnn geometry descriptor for manufacturing cad",
            "venue": "IEEE Transactions on Industrial Informatics,",
            "year": 2021
        },
        {
            "authors": [
                "D. Galvin"
            ],
            "title": "Three tutorial lectures on entropy and counting",
            "venue": "arXiv preprint arXiv:1406.7872,",
            "year": 2014
        },
        {
            "authors": [
                "C. Gao",
                "Y. Jiang",
                "F. Chen"
            ],
            "title": "Transferring hierarchical structures with dual meta imitation learning",
            "venue": "In Conference on Robot Learning,",
            "year": 2022
        },
        {
            "authors": [
                "N. Geng",
                "Q. Bai",
                "C. Liu",
                "T. Lan",
                "V. Aggarwal",
                "Y. Yang",
                "M. Xu"
            ],
            "title": "A reinforcement learning framework for vehicular network routing under peak and average constraints",
            "venue": "IEEE Transactions on Vehicular Technology,",
            "year": 2023
        },
        {
            "authors": [
                "S.K.S. Ghasemipour",
                "S. Gu",
                "R.S. Zemel"
            ],
            "title": "Smile: Scalable meta inverse reinforcement learning through context-conditional policies",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2019
        },
        {
            "authors": [
                "A. Gupta",
                "V. Kumar",
                "C. Lynch",
                "S. Levine",
                "K. Hausman"
            ],
            "title": "Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning",
            "venue": "In Proceedings of the 3rd Annual Conference on Robot Learning,",
            "year": 2019
        },
        {
            "authors": [
                "I. Higgins",
                "L. Matthey",
                "A. Pal",
                "C.P. Burgess",
                "X. Glorot",
                "M.M. Botvinick",
                "S. Mohamed",
                "A. Lerchner"
            ],
            "title": "betavae: Learning basic visual concepts with a constrained variational framework",
            "venue": "In Proceedings of the 5th International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "J. Ho",
                "S. Ermon"
            ],
            "title": "Generative adversarial imitation learning",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2016
        },
        {
            "authors": [
                "Jensen",
                "J.L.W. V"
            ],
            "title": "Sur les fonctions convexes et les in\u00e9galit\u00e9s entre les valeurs moyennes",
            "venue": "Acta mathematica,",
            "year": 1906
        },
        {
            "authors": [
                "M. Jing",
                "W. Huang",
                "F. Sun",
                "X. Ma",
                "T. Kong",
                "C. Gan",
                "L. Li"
            ],
            "title": "Adversarial option-aware hierarchical imitation learning",
            "venue": "In Proceedings of the 38th International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "D.P. Kingma",
                "M. Welling"
            ],
            "title": "Auto-encoding variational bayes",
            "venue": "In Proceedings of the 2nd International Conference on Learning Representations,",
            "year": 2014
        },
        {
            "authors": [
                "A.R. Kosiorek",
                "S. Sabour",
                "Y.W. Teh",
                "G.E. Hinton"
            ],
            "title": "Stacked capsule autoencoders",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2019
        },
        {
            "authors": [
                "C. Li",
                "D. Song",
                "D. Tao"
            ],
            "title": "The skill-action architecture: Learning abstract action embeddings for reinforcement learning",
            "venue": "In Submissions of the 9th International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "X. Luo",
                "X. Ma",
                "M. Munden",
                "Wu",
                "Y.-J",
                "Y. Jiang"
            ],
            "title": "A multisource data approach for estimating vehicle queue length at metered on-ramps",
            "venue": "Journal of Transportation Engineering, Part A: Systems,",
            "year": 2022
        },
        {
            "authors": [
                "X. Ma",
                "A. Karimpour",
                "Wu",
                "Y.-J"
            ],
            "title": "Statistical evaluation of data requirement for ramp metering performance assessment",
            "venue": "Transportation Research Part A: Policy and Practice,",
            "year": 2020
        },
        {
            "authors": [
                "J Massey"
            ],
            "title": "Causality, feedback and directed information",
            "venue": "In Proc. Int. Symp. Inf. Theory Applic.(ISITA-90),",
            "year": 1990
        },
        {
            "authors": [
                "A.Y. Ng",
                "S. Russell"
            ],
            "title": "Algorithms for inverse reinforcement learning",
            "venue": "In Proceedings of the 7th International Conference on Machine Learning,",
            "year": 2000
        },
        {
            "authors": [
                "A.Y. Ng",
                "D. Harada",
                "S. Russell"
            ],
            "title": "Policy invariance under reward transformations: Theory and application to reward shaping",
            "venue": "In Proceedings of the 16th International Conference on Machine Learning,",
            "year": 1999
        },
        {
            "authors": [
                "D. Peddireddy",
                "X. Fu",
                "A. Shankar",
                "H. Wang",
                "B.G. Joung",
                "V. Aggarwal",
                "J.W. Sutherland",
                "Jun",
                "M.B.-G"
            ],
            "title": "Identifying manufacturability and machining processes using deep 3d convolutional networks",
            "venue": "Journal of Manufacturing Processes,",
            "year": 2021
        },
        {
            "authors": [
                "D. Pomerleau"
            ],
            "title": "Efficient training of artificial neural networks for autonomous navigation",
            "venue": "Neural Computation,",
            "year": 1991
        },
        {
            "authors": [
                "S. Ross",
                "G.J. Gordon",
                "D. Bagnell"
            ],
            "title": "A reduction of imitation learning and structured prediction to no-regret online learning",
            "venue": "In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics,",
            "year": 2011
        },
        {
            "authors": [
                "M. Sharma",
                "A. Sharma",
                "N. Rhinehart",
                "K.M. Kitani"
            ],
            "title": "Directed-info GAIL: learning hierarchical policies from unsegmented demonstrations using directed information",
            "venue": "In Proceedings of the 7th International Conference on Learning Representations. OpenReview.net,",
            "year": 2019
        },
        {
            "authors": [
                "A. Singh",
                "E. Jang",
                "A. Irpan",
                "D. Kappler",
                "M. Dalal",
                "S. Levine",
                "M. Khansari",
                "C. Finn"
            ],
            "title": "Scalable multitask imitation learning with autonomous improvement",
            "venue": "IEEE International Conference on Robotics and Automation,",
            "year": 2020
        },
        {
            "authors": [
                "R.S. Sutton",
                "A.G. Barto"
            ],
            "title": "Reinforcement learning: An introduction",
            "venue": "MIT press,",
            "year": 2018
        },
        {
            "authors": [
                "R.S. Sutton",
                "D. Precup",
                "S.P. Singh"
            ],
            "title": "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning",
            "venue": "Artificial Intelligence,",
            "year": 1999
        },
        {
            "authors": [
                "E. Todorov",
                "T. Erez",
                "Y. Tassa"
            ],
            "title": "Mujoco: A physics engine for model-based control",
            "venue": "In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems,",
            "year": 2012
        },
        {
            "authors": [
                "A. Vaswani",
                "N. Shazeer",
                "N. Parmar",
                "J. Uszkoreit",
                "L. Jones",
                "A.N. Gomez",
                "L. Kaiser",
                "I. Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2017
        },
        {
            "authors": [
                "P. Wang",
                "D. Liu",
                "J. Chen",
                "H. Li",
                "C. Chan"
            ],
            "title": "Decision making for autonomous driving via augmented adversarial inverse reinforcement learning",
            "venue": "In IEEE International Conference on Robotics and Automation,",
            "year": 2021
        },
        {
            "authors": [
                "L. Yu",
                "T. Yu",
                "C. Finn",
                "S. Ermon"
            ],
            "title": "Meta-inverse reinforcement learning with probabilistic context variables",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2019
        },
        {
            "authors": [
                "T. Yu",
                "P. Abbeel",
                "S. Levine",
                "C. Finn"
            ],
            "title": "One-shot hierarchical imitation learning of compound visuomotor",
            "venue": "tasks. CoRR,",
            "year": 2018
        },
        {
            "authors": [
                "T. Yu",
                "C. Finn",
                "A. Xie",
                "S. Dasari",
                "T. Zhang",
                "P. Abbeel",
                "S. Levine"
            ],
            "title": "One-shot imitation from observing humans via domain-adaptive meta-learning",
            "venue": "arXiv preprint arXiv:1802.01557,",
            "year": 2018
        },
        {
            "authors": [
                "S. Zhang",
                "S. Whiteson"
            ],
            "title": "DAC: the double actor-critic architecture for learning options",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2019
        },
        {
            "authors": [
                "B.D. Ziebart",
                "A.L. Maas",
                "J.A. Bagnell",
                "A.K. Dey"
            ],
            "title": "Maximum entropy inverse reinforcement learning",
            "venue": "In Proceedings of the 33rd AAAI Conference on Artificial Intelligence,",
            "year": 2008
        },
        {
            "authors": [
                "non-negative. B"
            ],
            "title": "The Analogy with the VAE Framework Variational Autoencoder (VAE) (Kingma & Welling, 2014) learns a probabilistic encoder P\u03b7(V |U) and decoder P\u03be(U |V ) which map between data U and latent variables V by optimizing the evidence lower bound (ELBO) on the marginal distribution P\u03be(U), assuming the prior distributions P",
            "year": 2014
        },
        {
            "authors": [
                "Mujoco tasks. E"
            ],
            "title": "Implementation Details of MH-GAIL MH-GAIL is a variant of our algorithm by replacing the AIRL component with GAIL. Similar with Section 4.2, we need to provide an extension of GAIL with the one-step option model, in order to learn a hierarchical policy. The extension method follows Option-GAIL",
            "venue": "(Jing et al.,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "The generalist robot, which can autonomously perform a wide range of tasks, is one of the essential targets of robotic learning. As an important approach, Imitation Learning (IL) enables the agent to learn policies based on expert demon-\n1School of Industrial Engineering, Purdue University, West Lafayette, IN 47907, USA 2Elmore Family School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN 47907, USA 3Department of Electrical and Computer Engineering, George Washington University, Washington DC 20052, USA 4Department of Computer Science and AI Initiative, King Abdullah University of Science and Technology, Thuwal 23955, KSA. Correspondence to: Jiayu Chen <chen3686@purdue.edu>.\nProceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s).\nstrations and is especially effective for problems where it\u2019s difficult to discover task solutions autonomously through Reinforcement Learning (RL). To train a general-purpose agent, Multi-task/Meta Imitation Learning (MIL) algorithms (Finn et al., 2017b; Deisenroth et al., 2014; Singh et al., 2020) have been proposed to learn a parameterized policy that is a function of both the current observation and the task and is capable of performing a range of tasks following a particular distribution. The key insight of these algorithms is that the successful control for one task can be informative for other related tasks. However, a critical challenge for them is to acquire enough data for the agent to generalize broadly across tasks. Typically, a large number of demonstrations are required for each task in that distribution, and the required amount increases with task difficulty. Moreover, the learned multi-task policy cannot be transferred to tasks out of that distribution (Yu et al., 2019; Ghasemipour et al., 2019), which limits its general use.\nHierarchical Imitation Learning (HIL) has the potential to reduce the required demonstrations. In HIL, the agent learns a two-level policy, which can be modeled with the option framework (Sutton et al., 1999), from the expert data. Specifically, the low-level policies (i.e., skills) are designated to accomplish certain subtasks in a complex task, while the high-level policy is for scheduling the switch among the skills to solve the entire task. For multi-task settings, learning a hierarchical policy enables the agent to identify basic skills that can be useful in solving a distribution of tasks and to transfer them across tasks during training. In this case, each skill can be trained with demonstrations from different tasks rather than limited to a single one, and, with the shared skills, an agent mainly needs to update its high-level policy rather than learning an entire policy for each task. The expert data efficiency is significantly improved since demonstrations among different tasks are reused for learning skills and the burden of multi-task policy learning becomes lower. Further, in RL and IL, hierarchies exhibit a number of benefits, including better performance on long-horizontal complex tasks (Florensa et al., 2017; Jing et al., 2021) and the possibility of skill transfer between distinct tasks (Andreas et al., 2017).\nIn this paper, we propose MH-AIRL to introduce hierarchies to MIL. As discussed above, such hierarchies can improve expert data efficiency so that the agent can achieve supe-\nar X\niv :2\n30 5.\n12 63\n3v 2\n[ cs\n.L G\n] 2\n8 Ju\nn 20\n23\nrior performance based on a limited number of demonstrations. Further, basic skills can be extracted from the learned policies and reused in out-of-distribution tasks for better transferability (i.e., addressing the core concern of multitask learning). For example, it enables locomotion skills to be reused for multiple goal-achieving tasks of the same robot agent, yet in distinct scenarios. Different from previous Multi-task Hierarchical IL (MHIL) algorithms (Fox et al., 2019; Yu et al., 2018a; Gao et al., 2022; Bian et al., 2022), MH-AIRL is context-based and thus can be applied to demonstrations without any (skill or task) annotations, which are more accessible in practice. To this end, we extend both the multi-task learning and imitation learning modules (i.e., the core components of MIL), with the option framework (i.e., the hierarchical learning module). For multi-task learning, we condition the learned policy on a Hierarchical Latent Context Structure, where the task code and skill segmentation serve as the global and local context variables respectively. To compel the casual relationship of learned policy and latent variables, we start from the definition of mutual information and directed information and derive an easier-to-handle lower bound for each of them, serving as the optimization objectives. For imitation learning, we propose H-AIRL, which redefines a SOTA IL algorithm \u2013 AIRL (Fu et al., 2017) in an extended state-action space to enable our algorithm to recover a hierarchical policy (rather than a monolithic one) from expert trajectories. Finally, an actor-critic framework \u2013 HPPO is proposed to synthesize the optimization of the three modules above.\nThe contributions are as follows: (1) Our work presents the first MHIL algorithm based on demonstrations without any (skill or task) annotations, i.e., state-action pairs only. This greatly generalizes the applicability of our algorithm and reduces the cost of building expert datasets. (2) The newlyproposed H-AIRL and HPPO can be independently used for Hierarchical IL and RL, respectively. They are shown to achieve improved performance than SOTA HIL and HRL baselines. (3) We provide theoretical proof and ablation study for each algorithm module, and show the superiority of our algorithm through comparisons with SOTA baselines on a series of challenging multi-task settings from Mujoco (Todorov et al., 2012) and D4RL (Fu et al., 2020)."
        },
        {
            "heading": "2. Related Work",
            "text": "Machine Learning has found successful applications across a wide array of sectors such as transportation (Al-Abbasi et al., 2019; Chen et al., 2021; Luo et al., 2022; Ma et al., 2020), manufacturing (Peddireddy et al., 2021; Fu et al., 2021), networking (Balachandran et al., 2014; Geng et al., 2023), robotics (Gao et al., 2022; Gonzalez et al., 2023), etc. In the field of robotics, one of the key objectives is developing a \u2018generalist\u2019 robot, capable of executing a mul-\ntitude of tasks with human-like precision. To achieve this, multi-task robotic learning proves to be a highly effective methodology. In this section, we succinctly delineate Multitask IL and Multi-task HIL, illustrating the contributions and significance of our research in this evolving field.\nMulti-task/Meta IL algorithms have been proposed to learn a parameterized policy, which is capable of performing a range of tasks following a particular distribution, from a mixture of expert demonstrations. Based on the meta/multi-task learning techniques used, current MIL algorithms can be categorized as gradient-based or context-based. Gradient-based MIL, such as (Finn et al., 2017b; Yu et al., 2018b), integrates a gradient-based meta learning algorithm \u2014 MAML (Finn et al., 2017a) with supervised IL to train a policy that can be fast adapted to a new task with one-step gradient update. Context-based MIL, such as (Ghasemipour et al., 2019; Yu et al., 2019), learns a latent variable to represent the task contexts and trains a policy conditioned on the task context variable. Thus, with the corresponding task variable, the policy can be directly adopted to a new task setting. However, these algorithms do not make use of the option framework to learn a hierarchical policy like ours. In Section 5.1, we compare our algorithm with MIL baselines from both categories and show that it achieves better performance on a wide range of challenging long-horizon tasks.\nMulti-task HIL aims at recovering a multi-task hierarchical policy based on expert demonstrations from a distribution of tasks, which synthesizes the advantages of Multi-task IL and HIL. We present here the previous study in this area. The algorithms proposed in (Fox et al., 2019) and (Duminy et al., 2021) are limited to a certain type of robot. They provide predefined subtask decomposition, like picking and placing dishes, to simplify hierarchical learning, and have access to segmented expert demonstrations. However, our algorithm is proposed to automatically discover a hierarchical policy from unsegmented demonstrations and the discovered policy should capture the subtask structure of the demonstrations without supervision. In (Yu et al., 2018a), they propose to let the robot learn a series of primitive skills from corresponding demonstrations first, and then learn to compose learned primitives into multi-stage skills to complete a task. Thus, they predefine the types of skills and provide demonstrations corresponding to each skill. Also, in their setting, each new task has to be a sequence of predefined skills. A very recent work (Gao et al., 2022) integrates MAML and the option framework for MHIL. Like (Bian et al., 2022) and (Devin et al., 2019), this algorithm can be applied to demonstrations without the skill annotations, but these demonstrations have to be categorized by the task, in accordance with the requirements of MAML. Consequently, our research introduces the first MHIL algorithm that relies on demonstrations devoid of task or skill annotations. This makes it significantly more practical for real-world applications."
        },
        {
            "heading": "3. Background",
            "text": "In this section, we introduce Adversarial Inverse Reinforcement Learning (AIRL), Context-based Meta Learning, and the One-step Option Framework, corresponding to the three components of our algorithm: IL, multi-task learning, and hierarchical policy learning, respectively. They are based on the Markov Decision Process (MDP), denoted by M = (S,A,P, \u00b5,R, \u03b3), where S is the state space, A is the action space, P : S \u00d7 A \u00d7 S \u2192 [0, 1] is the transition function (PSt+1St,At \u225c P(St+1|St, At)), \u00b5 : S \u2192 [0, 1] is the distribution of the initial state, R : S\u00d7A \u2192 R is the reward function, and \u03b3 \u2208 (0, 1] is the discount factor."
        },
        {
            "heading": "3.1. Adversarial Inverse Reinforcement Learning",
            "text": "While there are several other ways to perform IL, such as supervised imitation (e.g., Behavioral Cloning (BC) (Pomerleau, 1991)) and occupancy matching (e.g., GAIL (Ho & Ermon, 2016)), we adopt Inverse Reinforcement Learning (IRL) because it uses not only the expert data but also selfexploration of the agent with the recovered reward function for further improvement (Ng & Russell, 2000; Wang et al., 2021). Comparisons with BC- and GAIL-based algorithms will be provided in Section 5. IRL aims to infer an expert\u2019s reward function from demonstrations, based on which the expert\u2019s policy can be recovered. Maximum Entropy IRL (Ziebart et al., 2008) solves IRL as a maximum likelihood estimation (MLE) problem shown as Equation 1. \u03c4E \u225c (S0, A0, \u00b7 \u00b7 \u00b7 , ST ) denotes the expert trajectory. Z\u03d1 is the partition function which can be calculated with Z\u03d1 = \u2211 \u03c4E P\u0302\u03d1(\u03c4E).\nmax \u03d1 E\u03c4E [logP\u03d1(\u03c4E)] = max \u03d1 E\u03c4E\n[ log P\u0302\u03d1(\u03c4E)\nZ\u03d1\n] ,\nP\u0302\u03d1(\u03c4E) = \u00b5(S0) T\u22121\u220f t=0 PSt+1St,At exp(R\u03d1(St, At))\n(1)\nSince Z\u03d1 is intractable for problems with large state-action space, the authors of (Fu et al., 2017) propose AIRL to solve this MLE problem in a sample-based manner, through alternatively training a discriminator f\u03d1 and policy network \u03c0 in an adversarial setting. The discriminator is trained by minimizing the cross-entropy loss between the expert demonstrations \u03c4E and generated samples \u03c4 by \u03c0:\nmin \u03d1 T\u22121\u2211 t=0 \u2212E\u03c4E [ logDt\u03d1 ] \u2212 E\u03c4 [ log(1\u2212Dt\u03d1) ] (2)\nHere, Dt\u03d1 = D\u03d1(St, At) = exp(f\u03d1(St,At))\nexp(f\u03d1(St,At))+\u03c0(At|St) . Meanwhile, the policy \u03c0 is trained with RL using the reward function defined as logDt\u03d1 \u2212 log(1\u2212Dt\u03d1). It is shown that, at optimality, f\u03d1 can serve as the recovered reward function R\u03d1 and \u03c0 is the recovered expert policy."
        },
        {
            "heading": "3.2. Context-based Meta Learning",
            "text": "We consider the Meta IRL setting: given a distribution of tasks P (T ), each task sampled from P (T ) has a corresponding MDP, and all of them share the same S and A but may differ in \u00b5, P , and R. The goal is to train a flexible policy \u03c0 on a set of training tasks sampled from P (T ), which can be quickly adapted to unseen test tasks sampled from the same distribution. As a representative, context-based Meta IRL algorithms (Ghasemipour et al., 2019; Yu et al., 2019) introduce the latent task variable C, which provides an abstraction of the corresponding task T , so each task can be represented with its distinctive components conditioning on C, i.e., (\u00b5(S0|C),P(S\u2032|S,A,C),R(S,A|C)). These algorithms learn a context-conditioned policy \u03c0(A|S,C) from the multi-task expert data, through IRL and by maximizing the mutual information (Cover, 1999) between the task variable C and the trajectories from \u03c0(A|S,C). Thus, given C for a new task, the corresponding \u03c0(A|S,C) can be directly adopted. Context-based methods can adopt off-policy data, making them more align with the goal of our work \u2013 learning from demonstrations. Thus, we choose context-based Meta IRL as our base algorithm.\nGiven expert trajectories sampled from a distribution of tasks (i.e., C \u223c prior(\u00b7)) and assuming that the demonstrative trajectories of each task are from a corresponding expert policy \u03c0E(\u03c4E |C), context-based Meta IRL recovers both the task-conditioned reward function R\u03d1(S,A|C) and policy \u03c0(S,A|C) by solving an MLE problem:\nmax \u03d1 EC\u223cprior(\u00b7),\u03c4E\u223c\u03c0E(\u00b7|C) [logP\u03d1(\u03c4E |C)] ,\nP\u03d1(\u03c4E |C) \u221d \u00b5(S0|C) T\u22121\u220f t=0 PSt+1St,At,C e R\u03d1(St,At|C) (3)\nwhere PSt+1St,At,C \u225c P(St+1|St, At, C). Like Equation 1, this can be efficiently solved through AIRL. We provide the AIRL framework to solve Equation 3 in Appendix A.1."
        },
        {
            "heading": "3.3. One-step Option Framework",
            "text": "As proposed in (Sutton et al., 1999), an optionZ \u2208 Z can be described with three components: an initiation set IZ \u2286 S, an intra-option policy \u03c0Z(A|S) : S \u00d7 A \u2192 [0, 1], and a termination function \u03b2Z(S) : S \u2192 [0, 1]. An option Z is available in state S if and only if S \u2208 IZ . Once the option is taken, actions are selected according to \u03c0Z until it terminates stochastically according to \u03b2Z , i.e., the termination probability at the current state. A new option will be activated by a high-level policy \u03c0Z(Z|S) : S\u00d7Z \u2192 [0, 1] once the previous option terminates. In this way, \u03c0Z(Z|S) and \u03c0Z(A|S) constitute a hierarchical policy for a certain task. Hierarchical policies tend to have superior performance on complex long-horizontal tasks which can be broken down into a series of subtasks (Chen et al., 2022a;b;c;d).\nThe one-step option framework (Li et al., 2021) is proposed to learn the hierarchical policy without the extra need to justify the exact beginning and breaking condition of each option, i.e., IZ and \u03b2Z . First, it assumes that each option is available at each state, i.e., IZ = S,\u2200Z \u2208 Z . Second, it drops \u03b2Z through redefining the high-level and low-level (i.e., intra-option) policies as \u03c0\u03b8(Z|S,Z \u2032) (Z \u2032: the option in the last timestep) and \u03c0\u03d5(A|S,Z) respectively and implementing them as end-to-end neural networks with the Multi-Head Attention (MHA) mechanism (Vaswani et al., 2017), which enables it to temporally extend options in the absence of the termination function. Intuitively, if Z \u2032 still fits S, \u03c0\u03b8(Z|S,Z \u2032) will assign a larger attention weight to Z \u2032 and thus has a tendency to continue with it; otherwise, a new option with better compatibility will be sampled. Then, the option is sampled at each timestep rather than after the last one terminates. With this simplified framework, we only need to train the hierarchical policy, i.e., \u03c0\u03b8 and \u03c0\u03d5, of which the structure design with MHA is in Appendix A.2."
        },
        {
            "heading": "4. Proposed Approach",
            "text": "In this section, we propose Multi-task Hierarchical AIRL (MH-AIRL) to learn a multi-task hierarchical policy from a mixture of expert demonstrations. First, the learned policy is multi-task by conditioning on the task context variable C. Given C \u223c prior(\u00b7), the policy can be directly adopted to complete the corresponding task. In practice, we can usually model a class of tasks by specifying the key parameters of the system and their distributions (i.e., prior(C)), including the property of the agent (e.g., mass and size), circumstance (e.g., friction and layout), and task setting (e.g., location of the goals). In this case, directly recovering a policy, which is applicable to a class of tasks, is quite meaningful. Second, for complex long-horizontal tasks which usually contain subtasks, learning a monolithic policy to represent a structured activity can be challenging and inevitably requires more demonstrations. In contrast, a hierarchical policy can make full use of the subtask structure and has the potential for better performance. Moreover, the learned low-level policies can be transferred as basic skills to out-of-distribution tasks for better transferability, while the monolithic policy learned with previous Meta IL algorithms cannot.\nIn Section 4.1 and 4.2, we extend context-based Meta Learning and AIRL with the option framework, respectively. In Section 4.3, we synthesize the three algorithm modules and propose an actor-critic framework for optimization."
        },
        {
            "heading": "4.1. Hierarchical Latent Context Structure",
            "text": "As mentioned in Section 3.2, the current task for the agent is encoded with the task variableC, which serves as the global context since it is consistent through the episode. As mentioned in Section 3.3, at each step, the hierarchical policy\nagent will first decide on its option choice Z using \u03c0\u03b8 and then select the primitive action based on the low-level policy \u03c0\u03d5 corresponding to Z. In this case, the policy learned should be additionally conditioned on Z besides the task code C, and the option choice is specific to each timestep t \u2208 {0, \u00b7 \u00b7 \u00b7 , T}, so we view the option choices Z0:T as the local latent contexts. C and Z0:T constitute a hierarchical latent context structure shown as Figure 1. Moreover, realworld tasks are often compositional, so the agent requires to reason about the subtask at hand while dealing with the global task. Z0:T and C provide a hierarchical embedding, which enhances the expressiveness of the policy trained with MH-AIRL, compared with context-based Meta IL which only employs the task context. In this section, we define the mutual and directed information objectives to enhance the causal relationship between the hierarchical policy and the global & local context variables which the policy should condition on, as an extension of context-based Meta-IL with the one-step option model.\nContext-based Meta IL algorithms establish a connection between the policy and task variable C, so that the policy can be adapted among different task modes according to the task context. This can be realized through maximizing the mutual information between the trajectory generated by the policy and the corresponding C, i.e., I(X0:T ;C), where X0:T = (X0, \u00b7 \u00b7 \u00b7 , XT ) = ((A\u22121, S0), \u00b7 \u00b7 \u00b7 , (AT\u22121, ST )) = \u03c4 . A\u22121 is a dummy variable. On the other hand, the local latent variables Z0:T have a directed causal relationship with the trajectory X0:T shown as the probabilistic graphical model in Figure 1. As discussed in (Massey et al., 1990; Sharma et al., 2019), this kind of connection can be established by maximizing the directed information (a.k.a., causal information) flow from the trajectory to the latent factors of variation, i.e., I(X0:T \u2192 Z0:T ). In our multi-task framework, we maximize the conditional directed information I(X0:T \u2192 Z0:T |C), since for each task c, the corresponding I(X0:T \u2192 Z0:T |C = c) should be maximized.\nDirectly optimizing the mutual or directed information objective is computationally infeasible, so we instead maximize their variational lower bounds as follows: (Please refer to Appendix B.1 and B.2 for the definition of mutual and directed information and derivations of their lower bounds. For simplicity, we use XT to represent X0:T , and so on.)\nLMI \u225c H(C) + E XT ,ZT ,C logP\u03c8(C|X0:T ) LDI \u225c T\u2211 t=1 [ E Xt,Zt,C logP\u03c9(Zt|X0:t, Z0:t\u22121, C)\n+H(Zt|X0:t\u22121, Z0:t\u22121, C)]\n(4)\nwhere H(\u00b7) denotes the entropy, P\u03c8 and P\u03c9 are the variational estimation of the posteriors P (C|X0:T ) and\nP (Zt|X0:t, Z0:t\u22121, C) which cannot be calculated directly. P\u03c8 and P\u03c9 are implemented as neural networks, H(C) is constant, and H(Zt|X0:t\u22121, Z0:t\u22121, C) is the entropy of the output of the high-level policy network (Appendix B.1), so LMI and LDI can be computed in real-time. Moreover, the expectation on Xt, Zt, C in LMI and LDI can be estimated in a Monte-Carlo manner (Sutton & Barto, 2018): C \u223c prior(\u00b7), (X0:t, Z0:t) \u223c P\u03b8,\u03d5(\u00b7|C), where P\u03b8,\u03d5(X0:t, Z0:t|C) is calculated by: (See Appendix B.1.)\n\u00b5(S0|C) t\u220f i=1 [\u03c0\u03b8(Zi|Si\u22121, Zi\u22121, C)\u00b7\n\u03c0\u03d5(Ai\u22121|Si\u22121, Zi, C)PSiSi\u22121,Ai\u22121,C ] (5)\nCombining Equation 4 and 5, we can get the objectives with respect to \u03c0\u03b8 and \u03c0\u03d5, i.e., the hierarchical policy defined in the one-step option model. By maximizing LMI and LDI , the connection between the policy and the hierarchical context structure can be established and enhanced. In LMI and LDI , we also introduce two variational posteriors P\u03c8 and P\u03c9 and update them together with \u03c0\u03b8 and \u03c0\u03d5. An analogy of our learning framework with Variational Autoencoder (VAE) (Kingma & Welling, 2014) is provided in Appendix B.3, which provides another perspective to understand the proposed objectives."
        },
        {
            "heading": "4.2. Hierarchical AIRL",
            "text": "In this section, we consider how to recover the taskconditioned hierarchical policy from a mixture of expert demonstrations {(XE0:T , ZE0:T , CE)}. Current algorithms, like AIRL (Fu et al., 2017) or Meta AIRL (Ghasemipour et al., 2019; Yu et al., 2019), can not be directly adopted since they don\u2019t take the local latent codes ZE0:T into consideration. Thus, we propose a novel hierarchical extension of AIRL, denoted as H-AIRL, as a solution, which is also part of our contributions. Further, it\u2019s usually difficult to annotate the local and global latent codes, i.e., ZE0:T and CE , of an expert trajectory XE0:T , so we propose an ExpectationMaximization (EM) adaption of H-AIRL as well to learn the multi-task hierarchical policy based on only the unstructured expert trajectories {XE0:T }.\nFirst, we define the task-conditioned hierarchical policy. When observing a state St at timestep t \u2208 {0, \u00b7 \u00b7 \u00b7 , T \u2212 1} during a certain task C, the agent needs first to decide on its option choice based on St and its previous option choice Zt using the high-level policy \u03c0\u03b8(Zt+1|St, Zt, C), and then decide on the action with the corresponding low-level policy \u03c0\u03d5(At|St, Zt+1, C). Thus, the task-conditioned hierarchical policy can be acquired with the chain rule as:\n\u03c0\u03b8(Zt+1|St, Zt, C) \u00b7 \u03c0\u03d5(At|St, Zt+1, C)\n= \u03c0\u03b8,\u03d5(Zt+1, At|St, Zt, C) = \u03c0\u03b8,\u03d5(A\u0303t|S\u0303t, C) (6)\nwhere the first equality holds because of the onestep Markov assumption (i.e., \u03c0\u03d5(At|St, Zt, Zt+1, C) = \u03c0\u03d5(At|St, Zt+1, C)), S\u0303t \u225c (St, Zt) and A\u0303t \u225c (Zt+1, At) denote the extended state and action space respectively.\nNext, by substituting (St, At) with (S\u0303t, A\u0303t) and \u03c4E with the hierarchical trajectory (X0:T , Z0:T ) in Equation 3, we can get an MLE problem shown as Equation 7, from which we can recover the task-conditioned hierarchical reward function and policy. The derivation is in Appendix C.1.\nmax \u03d1\nEC,(XT ,ZT )\u223c\u03c0E(\u00b7|C) [ logP\u03d1(X T , ZT |C) ] ,\nP\u03d1(X0:T , Z0:T |C) \u221d P\u0302\u03d1(X0:T , Z0:T |C)\n= \u00b5(S0|C) T\u22121\u220f t=0 PSt+1St,At,C e R\u03d1(St,Zt,Zt+1,At|C)\n(7)\nEquation 7 can be efficiently solved with the adversarial learning framework shown as Equation 8 (C,CE \u223c prior(\u00b7), (XE0:T , ZE0:T ) \u223c \u03c0E(\u00b7|CE), and (X0:T , Z0:T ) \u223c \u03c0\u03b8,\u03d5(\u00b7|C)). At optimality, we can recover the hierarchical policy of the expert as \u03c0\u03b8,\u03d5 with these objectives, of which the justification is provided in Appendix C.2.\nmin \u03d1 \u2212ECE ,(XE0:T ,ZE0:T ) T\u22121\u2211 t=0 logD\u03d1(S\u0303 E t , A\u0303 E t |CE)\n\u2212 EC,(X0:T ,Z0:T ) T\u22121\u2211 t=0 log(1\u2212D\u03d1(S\u0303t, A\u0303t|C)),\nmax \u03b8,\u03d5 LIL = EC,(X0:T ,Z0:T ) T\u22121\u2211 t=0 RtIL\n(8)\nwhere the reward functionRtIL = logD t \u03d1\u2212log(1\u2212Dt\u03d1) and Dt\u03d1 = D\u03d1(S\u0303t, A\u0303t|C) = exp(f\u03d1(S\u0303t,A\u0303t|C))\nexp(f\u03d1(S\u0303t,A\u0303t|C))+\u03c0\u03b8,\u03d5(A\u0303t|S\u0303t,C) .\nIn practice, the unstructured expert data {XE0:T }, i.e., trajectories only, is more accessible. In this case, we can view the latent contexts as hidden variables in a hidden Markov model (HMM) (Eddy, 1996) shown as Figure 1 and adopt an EM-style adaption to our algorithm, where we use the variational posteriors introduced in Section 4.1 to\nsample the corresponding CE , ZE0:T for each X E 0:T . In the E step, we sample the global and local latent codes with CE \u223c P\u03c8(\u00b7|XE0:T ), ZE0:T \u223c P\u03c9(\u00b7|XE0:T , CE). P\u03c8 and P\u03c9 represent the posterior networks forC andZ0:T respectively, with the parameters \u03c8 and \u03c9, i.e., the old parameters before being updated in the M step. Then, in the M step, we optimize the hierarchical policy and posteriors with Equation 4 and 8. Note that the expert data used in the first term of Equation 8 should be replaced with (XE0:T , Z E 0:T , CE) collected in the E step. By this adaption, we can get the solution of the original MLE problem (Equation 7), i.e., the recovered expert policy \u03c0\u03b8,\u03d5, with only unstructured expert data, which is proved in Appendix C.3."
        },
        {
            "heading": "4.3. Overall Framework",
            "text": "In Section 4.1, we propose LMI(\u03b8, \u03d5, \u03c8) and LDI(\u03b8, \u03d5, \u03c9) to establish the causal connection between the policy and hierarchical latent contexts. Then, in Section 4.2, we propose H-AIRL to recover the hierarchical policy from multitask expert demonstrations, where the policy is trained with the objective LIL(\u03b8, \u03d5). In this section, we introduce our method to update the hierarchical policy and posteriors with these objectives, and describe the overall algorithm framework. Detailed derivations of \u2207\u03b8,\u03d5,\u03c8LMI , \u2207\u03b8,\u03d5,\u03c9LDI and \u2207\u03b8,\u03d5LIL are in Appendix D.1, D.2, and D.3, respectively.\nFirst, the variational posteriors P\u03c8 and P\u03c9 can be updated with the gradients shown in Equation 9 through Stochastic Gradient Descent (SGD) (Bottou, 2010).\n\u2207\u03c8LMI = E C,XT ,ZT \u2207\u03c8 logP\u03c8(C|X0:T )\n\u2207\u03c9LDI = T\u2211 t=1 E C,Xt,Zt \u2207\u03c9 logP\u03c9(Zt|Xt, Zt\u22121, C) (9)\nNext, the gradients with respect to \u03b8 and \u03d5, i.e., the hierarchical policy, are computed based on the overall objective:\nL = \u03b11L MI + \u03b12L DI + \u03b13L IL (10)\nwhere \u03b11:3 are the weights (only the ratios \u03b11\u03b13 , \u03b12 \u03b13 matter) and fine-tuned as hyperparameters. Based on L, we can get the unbiased gradient estimators with respect to \u03b8 and \u03d5: (Derivations are in Appendix D.4.)\n\u2207\u03b8L = E C,XT ,ZT [ T\u2211 t=1 \u2207\u03b8 log \u03c0\u03b8(Zt|St\u22121, Zt\u22121, C)\u00b7\n(Rett \u2212 bhigh(St\u22121, Zt\u22121|C))]\n\u2207\u03d5L = E C,XT ,ZT [ T\u2211 t=1 \u2207\u03d5 log \u03c0\u03d5(At\u22121|St\u22121, Zt, C)\u00b7\n(Rett \u2212 blow(St\u22121, Zt|C))] (11)\nRett = \u03b11 logP\u03c8(C|X0:T )\n+ T\u2211 i=t [\u03b12 log P\u03c9(Zi|Xi, Zi\u22121, C) \u03c0\u03b8(Zi|Si\u22121, Zi\u22121, C) + \u03b13R i\u22121 IL ] (12)\nRett represents the return at timestep t, while bhigh and blow are the baseline terms for training \u03c0\u03b8 and \u03c0\u03d5, respectively. Further, we claim that the advantage functions for training \u03c0\u03b8 and \u03c0\u03d5 are given byRett\u2212bhigh(St\u22121, Zt\u22121|C) and Rett \u2212 blow(St\u22121, Zt|C), respectively, based on which we can optimize the hierarchical policy via off-the-shelf RL algorithms. In our implementation, we adopt PPO (Schulman et al., 2017) to train \u03c0\u03b8 and \u03c0\u03d5 with their corresponding advantage functions, respectively. This forms a novel Hierarchical RL (HRL) algorithm \u2013 HPPO, which has shown superiority over RL and HRL baselines in our experiment.\nIn Appendix D.5, we provide the overall algorithm as Algorithm 1 and illustrate the interactions among the networks in MH-AIRL in Figure 5."
        },
        {
            "heading": "5. Evaluation and Main Results",
            "text": "MH-AIRL is proposed to learn a multi-task hierarchical policy from a mixture of (unstructured) expert demonstrations. The learned policy can be adopted to any task sampled from a distribution of tasks. In this section: (1) We provide an ablation study with respect to the three main components of our algorithm: context-based multi-task/meta learning, option/hierarchical learning, and imitation learning. (2) We show that the hierarchical policy learning can significantly improve the agent\u2019s performance on challenging long-horizontal tasks. (3) Through qualitative and quantitative results, we show that our algorithm can capture the subtask structure within the expert demonstrations and that the learned basic skills for the subtasks (i.e., options) can be transferred to tasks not within the task distribution to aid learning, for better transferability.\nThe evaluation is based on three Mujoco (Todorov et al., 2012) locomotion tasks and the Kitchen task from the D4RL benchmark (Fu et al., 2020). All of them are with continuous state & action spaces, and contain compositional subtask structures to make them long-horizontal and a lot more challenging. To be specific: (1) In HalfCheetah-MultiVel, the goal velocity v is controlled by a 1-dim Gaussian context variable. The HalfCheetah agent is required to speed up to v/2 first, then slow down to 0, and finally achieve v. (2) In Walker-RandParam, the Walker agent must achieve the goal velocity 4 in three stages, i.e., [2, 0, 4]. Meanwhile, the mass of the agent changes among different tasks, which is controlled by an 8-dim Gaussian context variable. (3) In Ant-MultiGoal, a 3D Ant agent needs to reach a certain goal, which is different in each task and controlled by a 2-dim Gaussian context variable (polar coordinates). Moreover, the agent must go through certain subgoals. For\nexample, if the goal is (x, y) and |x| > |y|, the agent must go along [(0, 0), (x, 0), (x, y)]. (4) In Kitchen-MultiSeq, there are seven different subtasks, like manipulating the microwave, kettle, cabinet, switch, burner, etc. Each task requires the sequential completion of four specific subtasks. Twenty-four permutations are chosen and so 24 tasks, each of which is sampled with the same probability and controlled by a discrete context variable (input as one-hot vectors). Note that the states of the robot agents only contain their original states (defined by Mujoco or D4RL) and the task context variable, and do not include the actual task information, like the goal (velocity) and subgoal list. The task information is randomly generated by a parametric model of which the parameter is used as the context variable (i.e., the Gaussian vectors as mentioned above). The mapping between context variables and true task information is unknown to the learning agent. This makes the learning problem more challenging and our algorithm more general, since a vector of standard normal variables can be used to encode multiple types of task information.\nThese scenarios are designed to evaluate our algorithm on a wide range of multi-task setups. First, the agent needs to adapt across different reward functions in (1) and (3) since the rewarding state changes, and adjust across different transition functions in (2) since the mass change will influence the robotic dynamics. Next, different from (1)- (3), discrete context variables are adopted in (4), and (4) provides more realistic and challenging robotic tasks for evaluation. The expert data for Mujoco tasks are from expert agents trained with an HRL algorithm (Zhang & Whiteson, 2019) and specifically-designed rewards. While for the Kitchen task, we use the human demonstrations provided by (Gupta et al., 2019). Note that the demonstra-\ntions (state-action pairs only) do not include the rewards, task or option variables. Codes for reproducing all the results are on https://github.com/LucasCJYSDL/Multi-taskHierarchical-AIRL."
        },
        {
            "heading": "5.1. Effect of Hierarchical Learning",
            "text": "In this part, we evaluate whether the use of options can significantly improve the learning for challenging compound multi-task settings. We compare MH-AIRL with SOTA Meta Imitation Learning (MIL) baselines which also aim to train a policy that can be fast adapted to a class of related tasks but does not adopt options in learning. Contextbased MIL, such as PEMIRL (Yu et al., 2019) and SMILE (Ghasemipour et al., 2019), learns a context-conditioned policy that can be adopted to any task from a class by applying the task variable. While the policy learned with Gradientbased MIL, such as MAML-IL (Finn et al., 2017b) which integrates MAML (Finn et al., 2017a) (a commonly-adopted Meta Learning algorithm) and Behavioral Cloning (BC), has to be updated with gradients calculated from trajectories of the new task, before being applied. We select PEMIRL, SMILE, and MAML-IL from the two major categories of MIL as our baselines. All the algorithms are trained with the same expert data, and evaluated on the same set of test tasks (not contained in the demonstrations). Note that, unlike the others, MAML-IL requires expert data of each test task besides the task variable when testing and requires the expert demonstrations to be categorized by the task when training, which may limit its use in practical scenarios. Our algorithm is trained based on unstructured demonstrations and is only provided with the task context variable for testing.\nIn Figure 2, we record the change of the episodic reward\n(i.e., the sum of rewards for each step in an episode) on the test tasks as the number of training samples increases. The training is repeated 5 times with different random seeds for each algorithm, of which the mean and standard deviation are shown as the solid line and shadow area, respectively. Our algorithm outperforms the baselines in all tasks, and the improvement is more significant as the task difficulty goes up (i.e., in Ant & Kitchen), which shows the effectiveness of hierarchical policy learning especially in complex tasks. MAML-IL makes use of more expert information in both training and testing, but its performance gets worse on more challenging tasks. This may be because it is based on BC, which is a supervised learning algorithm prone to compounding errors (Ross et al., 2011)."
        },
        {
            "heading": "5.2. Ablation Study",
            "text": "We proceed to show the effectiveness of the IL and contextbased multi-task learning components through an ablation study. We propose two ablated versions of our algorithm: (1) MH-GAIL \u2013 a variant by replacing the AIRL component of MH-AIRL with GAIL (Ho & Ermon, 2016) (another commonly-used IL algorithm), of which the details are in Appendix E.2. (2) H-AIRL \u2013 a version that does not consider the task context C, which means P\u03c8 (i.e., the posterior for C) is not adopted, LMI is eliminated from Equation 10, and other networks do not use C as input. H-AIRL can be viewed as a newly-proposed HIL algorithm since it integrates the option framework and IL. To be more convincing, we also use two SOTA HIL algorithms \u2013 Option-GAIL (Jing et al., 2021) and DI-GAIL (Sharma et al., 2019), as the baselines. The training with the HIL algorithms is based on the same multi-task expert data as ours.\nIn Appendix E.1, we provide the plots of the change of episodic rewards on the test tasks. The training with each algorithm is repeated for 5 times with different random seeds. For each algorithm, we compute the average episodic reward after the learning converges in each of the 5 runs, and record the mean and standard deviation in Table 1 as the convergence performance. First, we can see that our algorithm performs the best on all tasks over the ablations, showing the effectiveness of all the main modules of our algorithm. Second, MH-GAIL performs better than HIL\nbaselines, showing the necessity of including the contextbased multi-task learning component. Without this component, HIL algorithms can only learn an average policy for a class of tasks from the mixture of multi-task demonstrations. Last, H-AIRL, the newly-proposed HIL algorithm, performs better than the SOTA HIL baselines on Mujoco tasks. A comprehensive empirical study on H-AIRL is provided in (Chen et al., 2022e)."
        },
        {
            "heading": "5.3. Analysis on the Learned Hierarchical Policy",
            "text": "In this section, we do the case study to analyze if the learned hierarchical policy can capture the sub-task structure in the demonstrations, and if the learned options can be transferred to tasks out of the task distribution. Capturing the subtask structures in real-life tasks can be essential for the (multitask) policy learning, because: (1) It is more human-like to split a complex task into more manageable subtasks to learn separately and then synthesize these skills to complete the whole task. (2) In some circumstances, the basic skills learned from one task setting can be reused in other task settings so the agent only needs to update its high-level policy over the same skill set, significantly lowering the learning difficulty. We test our algorithm on Mujoco-MultiGoal (Figure 3(a)) where the agent is required to achieve a goal corresponding to the task variable (2-dim Gaussian). The expert demonstrations include 100 goal locations in the Cell and the expert agent only moves horizontally or vertically. We test the learned hierarchical policy on 8 sparsely distributed goal locations, of which the trajectories are shown as Figure 3(d). We can see: (1) Four options (labeled with different colors) are discovered based on the demonstrations, each of which corresponds to a particular forward direction (green: up, yellow: down, etc.). These options are shared among the tasks. (2) The agent knows how to switch among the options to complete the tasks in stages (i.e., horizontal and vertical) with the learned high-level policy. Thus, our algorithm can effectively capture the compositional structure within the tasks and leverage it in the multi-task policy learning, which explains its superior performance. More analysis results of the learned hierarchical policy on HalfCheetah-MultiVel and Walker-RandParam are in Appendix E.3.\nNext, previous Meta/Multi-task Learning algorithms can\nlearn a policy for a class of tasks whose contexts follow a certain distribution, but the learned policy cannot be transferred as a whole to tasks out of this class. In contrast, our algorithm recovers a hierarchical policy, of which the low-level part can be reused as basic skills for new tasks not necessarily in the same class, resulting in substantially improved transferability of the learned policy. To show this, we reuse the options discovered in PointCell as the initialization of the low-level part of the hierarchical policy for the goal-achieving tasks in new scenarios \u2013 PointRoom and PointMaze (Figure 3(b) and 3(c)). In each scenario, we select 4 challenging goals (starting from the center point) for evaluation, which are labeled as red points in the figure. Unlike the other evaluation tasks, we provide the agent sparse reward signals (a positive reward for reaching the goal only) instead of expert data, so they are RL rather than IL tasks. We use HPPO proposed in Section 4.3 and initialize it with the transferred options (i.e., HPPO-init). To be more convincing, we use two other SOTA HRL and RL algorithms \u2013 DAC (Zhang & Whiteson, 2019) and PPO (Schulman et al., 2017), as baselines. In Figure 3(e) and 3(f), we plot the episodic reward change in the training process of each algorithm, where the solid line and shadow represent the mean and standard deviation of the performance across the 4 different goals in each scenario. We can see that the reuse of options significantly accelerate the learning process and the newly proposed HRL algorithm performs much better than the baselines. Note that the other algorithms are trained for more episodes since they do not adopt the transferred options. We show that, in scenarios for which we do not have expert data or dense rewards, we can make use of the basic skills learned from expert demonstrations for similar task scenarios to effectively aid the learning, which provides\na manner to bridge IL and RL."
        },
        {
            "heading": "6. Conclusion and Discussion",
            "text": "In this paper, we propose MH-AIRL to learn a hierarchical policy that can be adopted to perform a class of tasks, based on a mixture of multi-task unannotated expert data. We evaluate our algorithm on a series of challenging robotic multi-task settings. The results show that the multi-task hierarchical policies trained with MH-AIRL perform significantly better than the monotonic policies learned with SOTA Multi-task/Meta IL baselines. Further, with MH-AIRL, the agent can capture the subtask structures in each task and form a skill for each subtask. The basic skills can be reused for different tasks in that distribution to improve the expert data efficiency, and can even be transferred to more distinct tasks out of the distribution to solve long-timescale sparse-reward RL problems.\nThe primary limitation of our study is the inherent complexity of the overall framework, which comprises five networks as depicted in Figure 5. This complexity arises from our algorithm\u2019s integration of AIRL, context-based Meta IL, and the option framework. This amalgamation introduces certain challenges in the training process, particularly in determining the optimal number of training iterations for each network within each learning episode. After careful finetuning, we established a training iteration ratio of 1:3:10 for the discriminator, hierarchical policy, and variational posteriors, respectively. Despite this complexity, our evaluations across a wide variety of tasks utilized a consistent set of hyperparameters, showing the robustness of our approach."
        },
        {
            "heading": "A. Appendix on the Background and Related Works",
            "text": ""
        },
        {
            "heading": "A.1. AIRL Framework to Solve Equation 3",
            "text": "For each task C, we need to recover the task-specific reward function R\u03d1(S,A|C) and policy \u03c0(A|S,C) based on the corresponding expert trajectories \u03c4E \u223c \u03c0E(\u00b7|C) which can be solved by AIRL as mentioned in Section 3.1. Thus, we have the following objective functions for training, which is a simple extension of AIRL (Ghasemipour et al., 2019; Yu et al., 2019):\nmin \u03d1\nEC [ \u2212E\u03c4E\u223c\u03c0E(\u00b7|C) [ T\u22121\u2211 t=0 logD\u03d1(St, At|C) ] \u2212 E\u03c4\u223c\u03c0(\u00b7|C) [ T\u22121\u2211 t=0 log(1\u2212D\u03d1(St, At|C)) ]] (13)\nmax \u03c0\nEC [ E\u03c4\u223c\u03c0(\u00b7|C) [ T\u22121\u2211 t=0 logD\u03d1(St, At|C)\u2212 log(1\u2212D\u03d1(St, At|C)) ]] (14)\nwhere D\u03d1(S,A|C) = exp(f\u03d1(S,A|C))/[exp(f\u03d1(S,A|C)) + \u03c0(A|S,C)].\nA.2. Implementation of the Hierarchical Policy in the One-step Option Model\nIn this section, we give out the detailed structure design of the hierarchical policy introduced in Section 3.3, i.e., \u03c0\u03b8(Z|S,Z \u2032) and \u03c0\u03d5(A|S,Z), which is proposed in (Li et al., 2021). This part is not our contribution, so we only provide the details for the purpose of implementation.\nAs mentioned in Section 3.3, the structure design is based on the Multi-Head Attention (MHA) mechanism (Vaswani et al., 2017). An attention function can be described as mapping a query, i.e., q \u2208 Rdk , and a set of key-value pairs, i.e., K = [k1 \u00b7 \u00b7 \u00b7 kn]T \u2208 Rn\u00d7dk and V = [v1 \u00b7 \u00b7 \u00b7 vn]T \u2208 Rn\u00d7dv , to an output. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. To be specific:\nAttention(q,K, V ) = n\u2211 i=1 [ exp(q \u00b7 ki)\u2211n j=1 exp(q \u00b7 kj) \u00d7 vi ] (15)\nwhere q,K, V are learnable parameters, exp(q\u00b7ki)\u2211n j=1 exp(q\u00b7kj)\nrepresents the attention weight that the model should pay to item i. In MHA, the query and key-value pairs are first linearly projected h times to get h different queries, keys and values. Then, an attention function is performed on each of these projected versions of queries, keys and values in parallel to get h outputs which are then be concatenated and linearly projected to acquire the final output. The whole process can be represented as Equation 16, where W qi \u2208 Rdk\u00d7dk ,WKi \u2208 Rdk\u00d7dk ,WVi \u2208 Rdv\u00d7dv ,WO \u2208 Rndv\u00d7dv are the learnable parameters.\nMHA(q,K, V ) = Concat(head1, \u00b7 \u00b7 \u00b7 , headh)WO, headi = Attention(qW qi ,KW K i , V W V i ) (16)\nIn this work, the option is represented as an N -dimensional one-hot vector, where N denotes the total number of options to learn. The high-level policy \u03c0\u03b8(Z|S,Z \u2032) has the structure shown as:\nq = linear(Concat[S,WTCZ \u2032]), denseZ =MHA(q,WC ,WC), Z \u223c Categorical(\u00b7|denseZ) (17)\nWC \u2208 RN\u00d7E is the option context matrix of which the i-th row represents the context embedding of the option i. WC is also used as the key and value matrix for the MHA, so dk = dv = E in this case. Note that WC is only updated in the MHA module. Intuitively, \u03c0\u03b8(Z|S,Z \u2032) attends to all the option context embeddings in WC according to S and Z \u2032. If Z \u2032 still fits S, \u03c0\u03b8(Z|S,Z \u2032) will assign a larger attention weight to Z \u2032 and thus has a tendency to continue with it; otherwise, a new skill with better compatibility will be sampled.\nAs for the low-level policy \u03c0\u03d5(A|S,Z), it has the following structure:\ndenseA =MLP (S,W T CZ), A \u223c Categorical/Gaussian(\u00b7|denseA) (18)\nwhere MLP represents a multilayer perceptron, A follows a categorical distribution for the discrete case or a gaussian distribution for the continuous case. The context embedding corresponding to Z, i.e., WTCZ, instead of Z only, is used as input of \u03c0\u03d5 since it can encode multiple properties of the option Z (Kosiorek et al., 2019)."
        },
        {
            "heading": "B. Appendix on the Hierarchical Latent context Structure",
            "text": ""
        },
        {
            "heading": "B.1. A Lower Bound of the Directed Information Objective",
            "text": "In this section, we give out the derivation of a lower bound of the directed information from the trajectory sequence X0:T to the local latent context sequence Z0:T conditioned on the global latent context C, i.e., I(X0:T \u2192 Z0:T |C) as follows:\nI(X0:T \u2192 Z0:T |C) = T\u2211 t=1 [I(X0:t;Zt|Z0:t\u22121, C)]\n= T\u2211 t=1 [H(Zt|Z0:t\u22121, C)\u2212H(Zt|X0:t, Z0:t\u22121, C)]\n\u2265 T\u2211 t=1 [H(Zt|X0:t\u22121, Z0:t\u22121, C)\u2212H(Zt|X0:t, Z0:t\u22121, C)]\n= T\u2211 t=1\n[H(Zt|X0:t\u22121, Z0:t\u22121, C)+\u2211 X0:t,C, Z0:t\u22121 P (X0:t, Z0:t\u22121, C) \u2211 Zt P (Zt|X0:t, Z0:t\u22121, C) logP (Zt|X0:t, Z0:t\u22121, C)]\n(19)\nIn Equation 19, I(V ar1;V ar2|V ar3) denotes the conditional mutual information, H(V ar1|V ar2) denotes the conditional entropy, and the inequality holds because of the basic property related to conditional entropy: increasing conditioning cannot increase entropy (Galvin, 2014). H(Zt|X0:t\u22121, Z0:t\u22121, C) is the entropy of the high-level policy \u03c0\u03b8(Zt|St\u22121, Zt\u22121), where the other variables in X0:t\u22121, Z0:t\u22121 are neglected due to the one-step Markov assumption, and more convenient to obtain. Further, the second term in the last step can be processed as follows:\u2211\nZt\nP (Zt|X0:t, Z0:t\u22121, C) logP (Zt|X0:t, Z0:t\u22121, C)\n= \u2211 Zt P (Zt|X0:t, Z0:t\u22121, C) [ log P (Zt|X0:t, Z0:t\u22121, C) P\u03c9(Zt|X0:t, Z0:t\u22121, C) + logP\u03c9(Zt|X0:t, Z0:t\u22121, C) ]\n= DKL(P (\u00b7|X0:t, Z0:t\u22121, C)||P\u03c9(\u00b7|X0:t, Z0:t\u22121, C)) + \u2211 Zt P (Zt|X0:t, Z0:t\u22121, C) logP\u03c9(Zt|X0:t, Z0:t\u22121, C)\n\u2265 \u2211 Zt P (Zt|X0:t, Z0:t\u22121, C) logP\u03c9(Zt|X0:t, Z0:t\u22121, C)\n(20)\nwhere DKL(\u00b7) denotes the Kullback-Leibler (KL) Divergence which is non-negative (Cover, 1999), P\u03c9(Zt|X0:t, Z0:t\u22121, C) is a variational estimation of the posterior distribution of Zt given X0:t and Z0:t\u22121, i.e., P (Zt|X0:t, Z0:t\u22121, C), which is modeled as a recurrent neural network with the parameter set \u03c9 in our work. Based on Equation 19 and 20, we can obtain a lower bound of I(X0:T \u2192 Z0:T |C) denoted as LDI :\nLDI = T\u2211 t=1 [ \u2211\nX0:t,C, Z0:t\nP (X0:t, Z0:t, C) logP\u03c9(Zt|X0:t, Z0:t\u22121, C) +H(Zt|X0:t\u22121, Z0:t\u22121, C)] (21)\nNote that the joint distribution P (X0:t, Z0:t, C) has a recursive definition as follows:\nP (X0:t, Z0:t, C) = prior(C)P (X0:t, Z0:t|C) = prior(C)P (Xt|X0:t\u22121, Z0:t, C)P (Zt|X0:t\u22121, Z0:t\u22121, C)P (X0:t\u22121, Z0:t\u22121|C)\n(22)\nP (X0, Z0|C) = P ((S0, A\u22121), Z0|C) = \u00b5(S0|C) (23)\nwhere \u00b5(S0|C) denotes the distribution of the initial states for task C. Equation 23 holds because A\u22121 and Z0 are dummy variables which are only for simplifying notations and never executed and set to be constant across different tasks. Based on\nEquation 22 and 23, we can get:\nP (X0:t, Z0:t, C) = prior(C)\u00b5(S0|C) t\u220f i=1 P (Zi|X0:i\u22121, Z0:i\u22121, C)P (Xi|X0:i\u22121, Z0:i, C)\n= prior(C)\u00b5(S0|C) t\u220f i=1 P (Zi|X0:i\u22121, Z0:i\u22121, C)P ((Si, Ai\u22121)|X0:i\u22121, Z0:i, C)\n= prior(C)\u00b5(S0|C) t\u220f i=1 P (Zi|X0:i\u22121, Z0:i\u22121, C)P (Ai\u22121|X0:i\u22121, Z0:i, C)P(Si|Si\u22121, Ai\u22121, C)\n= prior(C)\u00b5(S0|C) t\u220f i=1 \u03c0\u03b8(Zi|Si\u22121, Zi\u22121, C)\u03c0\u03d5(Ai\u22121|Si\u22121, Zi, C)P(Si|Si\u22121, Ai\u22121, C)\n(24)\nIn Equation 24, prior(C) is the known prior distribution of the task context C, P(Si|Si\u22121, Ai\u22121, C) is the transition dynamic of task C, P (Zi|X0:i\u22121, Z0:i\u22121, C) and P (Ai\u22121|X0:i\u22121, Z0:i, C) can be replaced with \u03c0\u03b8 and \u03c0\u03d5, respectively, due to the one-step Markov assumption.\nTo sum up, we can adopt the high-level policy, low-level policy and variational posterior to get an estimation of the lower bound of the directed information objective through Monte Carlo sampling (Sutton & Barto, 2018) according to Equation 21 and 24, which can then be used to optimize the three networks."
        },
        {
            "heading": "B.2. A Lower Bound of the Mutual Information Objective",
            "text": "In this section, we give out the derivation of a lower bound of the mutual information between the trajectory sequence X0:T and its corresponding task context C, i.e., I(X0:T ;C).\nI(X0:T ;C) = H(C)\u2212H(C|X0:T ) = H(C) + \u2211 X0:T P (X0:T ) \u2211 C P (C|X0:T ) logP (C|X0:T )\n= H(C) + \u2211 X0:T P (X0:T ) \u2211 C P (C|X0:T ) log P (C|X0:T ) P\u03c8(C|X0:T ) + \u2211 X0:T ,C P (X0:T , C) logP\u03c8(C|X0:T )\n= H(C) + \u2211 X0:T P (X0:T )DKL(P (\u00b7|X0:T ||P\u03c8(\u00b7|X0:T )) + \u2211 X0:T ,C P (X0:T , C) logP\u03c8(C|X0:T )\n\u2265 H(C) + \u2211\nX0:T ,C\nP (X0:T , C) logP\u03c8(C|X0:T )\n= H(C) + \u2211 C prior(C) \u2211 X0:T P (X0:T |C) logP\u03c8(C|X0:T )\n= H(C) + \u2211 C prior(C) \u2211\nX0:T ,Z0:T\nP (X0:T , Z0:T |C) logP\u03c8(C|X0:T )\n(25)\nIn Equation 25, H(\u00b7) denotes the entropy, prior(C) denotes the known prior distribution of the task context C, P (X0:T , Z0:T |C) can be calculated with Equation 24 by setting t = T , and P\u03c8(C|X0:T ) is a variational estimation of the posterior distribution P (C|X0:T ) which is implemented as a recurrent neural network with the parameter set \u03c8. Note that the inequality holds because the KL-Divergence, i.e., DKL(\u00b7), is non-negative."
        },
        {
            "heading": "B.3. The Analogy with the VAE Framework",
            "text": "Variational Autoencoder (VAE) (Kingma & Welling, 2014) learns a probabilistic encoder P\u03b7(V |U) and decoder P\u03be(U |V ) which map between data U and latent variables V by optimizing the evidence lower bound (ELBO) on the marginal distribution P\u03be(U), assuming the prior distributions PU (\u00b7) and PV (\u00b7) over the data and latent variables respectively. The authors of (Higgins et al., 2017) extend the VAE approach by including a parameter \u03b2 to control the capacity of the latent V ,\nof which the ELBO is: max \u03b7,\u03be\nE U\u223cPU (\u00b7) V\u223cP\u03b7(\u00b7|U)\n[ logP\u03be(U |V )\u2212 \u03b2DKL(P\u03b7(V |U)||PV (V )) ] (26)\nThe first term can be viewed as the reconstruction accuracy of the data U from V , and the second term works as a regularizer for the distribution of the latent variables V , where DKL denotes the KL Divergence (Cover, 1999). VAE can efficiently solve the posterior inference problem for datasets with continuous latent variables where the true posterior is intractable, through fitting an approximate inference model P\u03be (i.e., the variational posterior). The variational lower bound, i.e., ELBO, can be straightforwardly optimized using standard stochastic gradient methods, e.g., SGD (Bottou, 2010).\nAs shown in Figure 4, the optimization of LMI (Equation 4) can be viewed as using \u03c0\u03b8 and \u03c0\u03d5 as the encoder and P\u03c8 as the decoder and then minimizing the reconstruction error of C from X0:T , and the regularizer term in Equation 26 is neglected (i.e., \u03b2 = 0). As for the optimization of LDI (Equation 4), at each timestep t, \u03c0\u03d5 and P\u03c9 form a conditional VAE between Zt and Xt, which is conditioned on the history information and task code, i.e., (X0:t\u22121, Z0:t\u22121, C), with the prior distribution of Zt provided by \u03c0\u03b8. Compared with the VAE objective (i.e., Equation 26), \u03c0\u03d5 and P\u03c9 in LDI work as the encoder and decoder respectively; \u03c0\u03b8 provides the prior, which corresponds to PU (\u00b7).\nBoth P\u03c8 and P\u03c9 use sequential data as input and thus are implemented with RNN. The variational posterior for the task code, i.e., P\u03c8(C|X0:T ) takes the trajectory X0:T as input and is implemented as a bidirectional GRU (Mangal et al., 2019) to make sure that both the beginning and end of the trajectory are equally important. On the other hand, the variational posterior for the local latent code, i.e., P\u03c9(Zt|X0:t, Z0:t\u22121, C), is modeled as P\u03c9(Zt|Xt, Zt\u22121, C, ht\u22121), where ht\u22121 is the internal hidden state of an RNN. ht\u22121 is recursively maintained with the time series using the GRU rule, i.e., ht\u22121 = GRU(Xt\u22121, Zt\u22122, ht\u22122), to embed the history information in the trajectory, i.e., X0:t\u22121 and Z0:t\u22122. Note that the RNN-based posterior has been used and justified in the process for sequential data (Chung et al., 2015)."
        },
        {
            "heading": "C. Appendix on Hierarchical AIRL",
            "text": ""
        },
        {
            "heading": "C.1. Derivation of the MLE Objective",
            "text": "In Equation 27, Z0 is a dummy variable which is assigned before the episode begins and never executed. It\u2019s implemented as a constant across different episodes, so we have P (S0, Z0|C) = P (S0|C) = \u00b5(S0|C), where \u00b5(\u00b7|C) denotes the initial state distribution for task C. On the other hand, we have P (St+1, Zt+1|St, Zt, Zt+1, At, C) = P (Zt+1|St, Zt, Zt+1, At, C)P (St+1|St, Zt, Zt+1, At, C) = P(St+1|St, At, C), since the transition dynamic P is irrelevant to the local latent codes Z and only related the task context C.\nP\u03d1(X0:T , Z0:T |C) \u221d \u00b5(S\u03030|C) T\u22121\u220f t=0 P(S\u0303t+1|S\u0303t, A\u0303t, C) exp(R\u03d1(S\u0303t, A\u0303t|C))\n= P (S0, Z0|C) T\u22121\u220f t=0 P (St+1, Zt+1|St, Zt, Zt+1, At, C) exp(R\u03d1(St, Zt, Zt+1, At|C))\n= \u00b5(S0|C) T\u22121\u220f t=0 P(St+1|St, At, C) exp(R\u03d1(St, Zt, Zt+1, At|C))\n(27)"
        },
        {
            "heading": "C.2. Justification of the Objective Function Design in Equation 8",
            "text": "In this section, we prove that by optimizing the objective functions shown in Equation 8, we can get the solution of the MLE problem shown as Equation 7, i.e., the task-conditioned hierarchical reward function and policy of the expert.\nIn Appendix A of (Fu et al., 2017), they show that the discriminator objective (the first equation in 8) is equivalent to the MLE objective (Equation 7) where f\u03d1 serves as R\u03d1, when DKL(\u03c0(\u03c4)||\u03c0E(\u03c4)) is minimized. The same conclusion can be acquired by simply replacing {St, At, \u03c4} with {(St, Zt), (Zt+1, At), (X0:T , Z0:T )}, i.e., the extended definition of the state, action and trajectory, in the original proof, which we don\u2019t repeat here. Then, we only need to prove that EC [DKL(\u03c0\u03b8,\u03d5(X0:T , Z0:T |C)||\u03c0E(X0:T , Z0:T |C))] can be minimized through the second equation in 8:\nmax \u03b8,\u03d5 EC\u223cprior(\u00b7),(X0:T ,Z0:T )\u223c\u03c0\u03b8,\u03d5(\u00b7|C) T\u22121\u2211 t=0 RtIL\n= max \u03b8,\u03d5 E C,X0:T ,Z0:T [ T\u22121\u2211 t=0 logD\u03d1(St, Zt, Zt+1, At|C)\u2212 log(1\u2212D\u03d1(St, Zt, Zt+1, At|C)) ]\n= max \u03b8,\u03d5 E C,X0:T ,Z0:T [ T\u22121\u2211 t=0 f\u03d1(St, Zt, Zt+1, At|C)\u2212 log \u03c0\u03b8,\u03d5(Zt+1, At|St, Zt, C) ]\n= max \u03b8,\u03d5 E C,X0:T ,Z0:T [ T\u22121\u2211 t=0 f\u03d1(St, Zt, Zt+1, At|C)\u2212 log(\u03c0\u03b8(Zt+1|St, Zt, C)\u03c0\u03d5(At|St, Zt+1, C)) ]\n= max \u03b8,\u03d5 E C,X0:T ,Z0:T\n[ log \u220fT\u22121 t=0 exp(f\u03d1(St, Zt, Zt+1, At|C))\u220fT\u22121\nt=0 \u03c0\u03b8(Zt+1|St, Zt, C)\u03c0\u03d5(At|St, Zt+1, C)\n]\n\u21d0\u21d2 max \u03b8,\u03d5 E C,X0:T ,Z0:T\n[ log \u220fT\u22121 t=0 exp(f\u03d1(St, Zt, Zt+1, At|C))/ZC\u03d1\u220fT\u22121\nt=0 \u03c0\u03b8(Zt+1|St, Zt, C)\u03c0\u03d5(At|St, Zt+1, C)\n]\n(28)\nNote that ZC\u03d1 = \u2211 X0:T ,Z0:T\nP\u0302\u03d1(X0:T , Z0:T |C) (defined in Equation 7) is the normalized function parameterized with \u03d1, so the introduction of ZC\u03d1 will not influence the optimization with respect to \u03b8 and \u03d5 and the equivalence at the last step holds. Also, the second equality shows that the task-conditioned hierarchical policy is recovered by optimizing an\nentropy-regularized policy objective where f\u03d1 serves as R\u03d1. Further, we have:\nmax \u03b8,\u03d5 E C,X0:T ,Z0:T\n[ log \u220fT\u22121 t=0 exp(f\u03d1(St, Zt, Zt+1, At|C))/ZC\u03d1\u220fT\u22121\nt=0 \u03c0\u03b8(Zt+1|St, Zt, C)\u03c0\u03d5(At|St, Zt+1, C)\n]\n= max \u03b8,\u03d5 E C,X0:T ,Z0:T\n[ log \u00b5(S0|C) \u220fT\u22121 t=0 P(St+1|St, At, C) \u220fT\u22121 t=0 exp(f\u03d1(St, Zt, Zt+1, At|C))/ZC\u03d1\n\u00b5(S0|C) \u220fT\u22121 t=0 P(St+1|St, At, C) \u220fT\u22121 t=0 \u03c0\u03b8(Zt+1|St, Zt, C)\u03c0\u03d5(At|St, Zt+1, C)\n]\n= max \u03b8,\u03d5\nEC\u223cprior(\u00b7),(X0:T ,Z0:T )\u223c\u03c0\u03b8,\u03d5(\u00b7|C) [ log\n\u03c0E(X0:T , Z0:T |C) \u03c0\u03b8,\u03d5(X0:T , Z0:T |C) ] = max\n\u03b8,\u03d5 EC\u223cprior(\u00b7) [\u2212DKL(\u03c0\u03b8,\u03d5(X0:T , Z0:T |C)||\u03c0E(X0:T , Z0:T |C))]\n\u21d0\u21d2 min \u03b8,\u03d5 EC\u223cprior(\u00b7) [DKL(\u03c0\u03b8,\u03d5(X0:T , Z0:T |C)||\u03c0E(X0:T , Z0:T |C))]\n(29)\nwhere the second equality holds because of the definition of \u03c0E (Equation 7 with f\u03d1 serving as R\u03d1) and \u03c0\u03b8,\u03d5 (Equation 34)."
        },
        {
            "heading": "C.3. Justification of the EM-style Adaption",
            "text": "Given only a dataset of expert trajectories, i.e., DE \u225c {X0:T }, we can still maximize the likelihood estimation EX0:T\u223cDE [logP\u03d1(X0:T )] through an EM-style adaption: (We use X0:T , C, Z0:T instead of XE0:T , CE , ZE0:T for simplicity.)\nEX0:T\u223cDE [logP\u03d1(X0:T )] = EX0:T\u223cDE log  \u2211 C,Z0:T P\u03d1(X0:T , C, Z0:T )  = EX0:T\u223cDE\nlog  \u2211 C,Z0:T P\u03d1(X0:T , C, Z0:T ) P\u03d1(C,Z0:T |X0:T ) P\u03d1(C,Z0:T |X0:T )  = EX0:T\u223cDE [ log [ E(C,Z0:T )\u223cP\u03d1(\u00b7|X0:T ) P\u03d1(X0:T , C, Z0:T )\nP\u03d1(C,Z0:T |X0:T ) ]] \u2265 EX0:T\u223cDE [ E(C,Z0:T )\u223cP\u03d1(\u00b7|X0:T ) log P\u03d1(X0:T , C, Z0:T )\nP\u03d1(C,Z0:T |X0:T ) ] = EX0:T\u223cDE ,C\u223cP\u03c8(\u00b7|X0:T ),Z0:T\u223cP\u03c9(\u00b7|X0:T ,C) [ log P\u03d1(X0:T , C, Z0:T )\nP\u03d1(C,Z0:T |X0:T ) ] = EX0:T ,C,Z0:T [logP\u03d1(X0:T , C, Z0:T )]\u2212 EX0:T ,C,Z0:T [ logP\u03d1(C,Z0:T |X0:T )\n] = EX0:T ,C,Z0:T [logP\u03d1(X0:T , Z0:T |C)]\u2212 EX0:T ,C,Z0:T [ \u2212 log prior(C) + logP\u03d1(C,Z0:T |X0:T ) ]\n(30)\nwhere we adopt the Jensen\u2019s inequality (Jensen, 1906) in the 4-th step. Also, we note that P\u03c8,\u03c9(C,Z0:T |X0:T ) provides a posterior distribution of (C,Z0:T ), which corresponds to the generating process led by the hierarchical policy. As justified in C.2, the hierarchical policy is trained with the reward function parameterized with \u03d1. Thus, the hierarchical policy is a function of \u03d1, and the network P\u03c8,\u03c9 corresponding to the hierarchical policy provides a posterior distribution related to the parameter set \u03d1, i.e., (C,Z0:T ) \u223c P\u03d1(\u00b7|X0:T ) \u21d0\u21d2 C \u223c P\u03c8(\u00b7|X0:T ), Z0:T \u223c P\u03c9(\u00b7|X0:T , C), due to which the 5-th step holds. Note that \u03d1, \u03c8, \u03c9 denote the parameters \u03d1, \u03c8, \u03c9 before being updated in the M step.\nIn the second equality of Equation 30, we introduce the sampled global and local latent codes in the E step as discussed in Section 4.2. Then, in the M step, we optimize the objectives shown in Equation 4 and 8 for iterations, by replacing the samples in the first term of Equation 8 with (X0:T , C, Z0:T ) collected in the E step. This is equivalent to solve the MLE problem: max\u03d1 EX0:T\u223cDE ,C\u223cP\u03c8(\u00b7|X0:T ),Z0:T\u223cP\u03c9(\u00b7|X0:T ,C) [logP\u03d1(X0:T , Z0:T |C)], which is to maximize a lower bound of the original objective, i.e., EX0:T\u223cDE [logP\u03d1(X0:T )], as shown in the last step of Equation 30. Thus, the original objective can be optimized through this EM procedure. Note that the second term in the last step is a function of the old parameter \u03d1 so that it can be overlooked when optimizing with respect to \u03d1."
        },
        {
            "heading": "C.4. State-only Adaption of H-AIRL",
            "text": "In AIRL (Fu et al., 2017), they propose a two-component design for the discriminator as follows:\nf\u03d1,\u03b6(St, St+1) = g\u03d1(St) + \u03b3h\u03b6(St+1)\u2212 h\u03b6(St) (31)\nwhere \u03b3 is the discount factor in MDP. Based on f\u03d1,\u03b6(St, St+1), they can further get D\u03d1,\u03b6(St, St+1) which is used in Equation 2 for AIRL training. As proved in (Fu et al., 2017), g\u03d1, h\u03b6 and f\u03d1,\u03b6 can recover the true reward, value and advantage function, respectively, under deterministic environments with a state-only ground truth reward. With this state-only design, the recovered reward function is disentangled from the dynamics of the environment in which it was trained, so that it can be directly transferred to environments with different transition dynamics, i.e., P , for the policy training. Moreover, the additional shaping term h\u03b6 helps mitigate the effects of unwanted shaping on the reward approximator g\u03d1 (Ng et al., 1999). This design can also be adopted to H-AIRL (Equation 8) by redefining Equation 31 on the extended state space (first defined in Section 4.2):\nf\u03d1,\u03b6(S\u0303t, S\u0303t+1|C) = g\u03d1(S\u0303t|C) + \u03b3h\u03b6(S\u0303t+1|C)\u2212 h\u03b6(S\u0303t|C) = g\u03d1(St, Zt|C) + \u03b3h\u03b6(St+1, Zt+1|C)\u2212 h\u03b6(St, Zt|C)\n(32)\nIn this way, we can recover a hierarchical reward function conditioned on the task context C, i.e., g\u03d1(St, Zt|C), which avoids unwanted shaping and is robust enough to be directly applied in a new task with different dynamic transition distribution from prior(C). The proof can be done by simply replacing the state S in the original proof (Appendix C of (Fu et al., 2017)) with its extended definition S\u0303, so we don\u2019t repeat it here."
        },
        {
            "heading": "D. The Proposed Actor-Critic Algorithm for Training",
            "text": ""
        },
        {
            "heading": "D.1. Gradients of the Mutual Information Objective Term",
            "text": "The objective function related to the mutual information: LMI = \u2211 C prior(C) \u2211\nX0:T ,Z0:T\nP (X0:T , Z0:T |C) logP\u03c8(C|X0:T ) (33)\nAfter introducing the one-step Markov assumption to Equation 24, we can calculate P (X0:T , Z0:T |C) as Equation 34, where \u03c0\u03b8 and \u03c0\u03d5 represent the hierarchical policy in the one-step option framework.\nP (X0:T , Z0:T |C) = \u00b5(S0|C) T\u220f t=1 \u03c0\u03b8(Zt|St\u22121, Zt\u22121, C)\u03c0\u03d5(At\u22121|St\u22121, Zt, C)P(St|St\u22121, At\u22121, C) (34)\nFirst, the gradient with respect to \u03c8 is straightforward as Equation 35, which can be optimized as a standard likelihood maximization problem.\n\u2207\u03c8LMI = \u2211 C prior(C) \u2211\nX0:T ,Z0:T\nP (X0:T , Z0:T |C)\u2207\u03c8 logP\u03c8(C|X0:T ) (35)\nNow we give out the derivation of \u2207\u03b8LMI : \u2207\u03b8LMI = \u2211 C prior(C) \u2211\nX0:T ,Z0:T\n\u2207\u03b8P\u03b8,\u03d5(X0:T , Z0:T |C) logP\u03c8(C|X0:T )\n= \u2211 C prior(C) \u2211\nX0:T ,Z0:T\nP\u03b8,\u03d5(X0:T , Z0:T |C)\u2207\u03b8 logP\u03b8,\u03d5(X0:T , Z0:T |C) logP\u03c8(C|X0:T )\n= E C,X0:T , Z0:T [\u2207\u03b8 logP\u03b8,\u03d5(X0:T , Z0:T |C) logP\u03c8(C|X0:T )]\n= E C,X0:T , Z0:T [ T\u2211 t=1 \u2207\u03b8 log \u03c0\u03b8(Zt|St\u22121, Zt\u22121, C) logP\u03c8(C|X0:T )\n] (36)\nwhere the last equality holds because of Equation 34. With similar derivation as above, we have:\n\u2207\u03d5LMI = E C,X0:T , Z0:T [ T\u2211 t=1 \u2207\u03d5 log \u03c0\u03d5(At\u22121|St\u22121, Zt, C) logP\u03c8(C|X0:T ) ] (37)"
        },
        {
            "heading": "D.2. Gradients of the Directed Information Objective Term",
            "text": "Next, we give out the derivation of the gradients related to the directed information objective term, i.e., LDI . We denote the two terms in Equation 21 as LDI1 and L DI 2 respectively. Then, we have \u2207\u03b8,\u03d5LDI = \u2207\u03b8,\u03d5LDI1 +\u2207\u03b8,\u03d5LDI2 . The derivations are as follows:\n\u2207\u03b8LDI1 = T\u2211 t=1 \u2211 C prior(C) \u2211\nX0:t,Z0:t\n\u2207\u03b8P\u03b8,\u03d5(X0:t, Z0:t|C) logP\u03c9(Zt|X0:t, Z0:t\u22121, C)\n= T\u2211 t=1 \u2211 C prior(C) \u2211\nX0:t,Z0:t\nP\u03b8,\u03d5(X0:t, Z0:t|C) t\u2211 i=1 \u2207\u03b8 log \u03c0\u03b8(Zi|Si\u22121, Zi\u22121, C) logP t\u03c9\n= T\u2211 t=1 \u2211 C prior(C) \u2211\nX0:t,Z0:t \u2211 Xt+1:T , Zt+1:T P\u03b8,\u03d5(X0:T , Z0:T |C) t\u2211 i=1 \u2207\u03b8 log \u03c0\u03b8(Zi|Si\u22121, Zi\u22121, C) logP t\u03c9\n= T\u2211 t=1 \u2211 C prior(C) \u2211\nX0:T ,Z0:T\nP\u03b8,\u03d5(X0:T , Z0:T |C) t\u2211 i=1 \u2207\u03b8 log \u03c0\u03b8(Zi|Si\u22121, Zi\u22121, C) logP t\u03c9\n= \u2211 C prior(C) \u2211\nX0:T ,Z0:T\nP\u03b8,\u03d5(X0:T , Z0:T |C) T\u2211 t=1 logP t\u03c9 t\u2211 i=1 \u2207\u03b8 log \u03c0\u03b8(Zi|Si\u22121, Zi\u22121, C)\n= \u2211 C prior(C) \u2211\nX0:T ,Z0:T\nP\u03b8,\u03d5(X0:T , Z0:T |C) T\u2211 i=1 \u2207\u03b8 log \u03c0\u03b8(Zi|Si\u22121, Zi\u22121, C) T\u2211 t=i logP t\u03c9\n= E C,X0:T , Z0:T [ T\u2211 i=1 \u2207\u03b8 log \u03c0\u03b8(Zi|Si\u22121, Zi\u22121, C) T\u2211 t=i logP\u03c9(Zt|X0:t, Z0:t\u22121, C) ]\n= E C,X0:T , Z0:T [ T\u2211 t=1 \u2207\u03b8 log \u03c0\u03b8(Zt|St\u22121, Zt\u22121, C) T\u2211 i=t logP\u03c9(Zi|X0:i, Z0:i\u22121, C) ]\n(38)\nwhere P t\u03c9 = P\u03c9(Zt|X0:t, Z0:t\u22121, C) for simplicity. The second equality in Equation 38 holds following the same derivation in Equation 36. Then, the gradient related to LDI2 is:\n\u2207\u03b8LDI2 = \u2207\u03b8 T\u2211 t=1 H(Zt|X0:t\u22121, Z0:t\u22121, C)\n= \u2212\u2207\u03b8[ T\u2211 t=1 \u2211 C prior(C) \u2211\nX0:t\u22121,Z0:t\nP\u03b8,\u03d5(X0:t\u22121, Z0:t|C) logP (Zt|X0:t\u22121, Z0:t\u22121, C)]\n= \u2212\u2207\u03b8[ T\u2211 t=1 \u2211 C prior(C) \u2211\nX0:t\u22121,Z0:t\nP\u03b8,\u03d5(X0:t\u22121, Z0:t|C) log \u03c0\u03b8(Zt|St\u22121, Zt\u22121, C)]\n= \u2212\u2207\u03b8[ \u2211 C prior(C) \u2211\nX0:T ,Z0:T\nP\u03b8,\u03d5(X0:T , Z0:T |C) T\u2211 t=1 log \u03c0\u03b8(Zt|St\u22121, Zt\u22121, C)]\n= \u2212[ \u2211 C prior(C) \u2211\nX0:T ,Z0:T\n\u2207\u03b8P\u03b8,\u03d5(X0:T , Z0:T |C) T\u2211 t=1 log \u03c0\u03b8(Zt|St\u22121, Zt\u22121, C)+\n\u2211 C prior(C) \u2211\nX0:T ,Z0:T\nP\u03b8,\u03d5(X0:T , Z0:T |C) T\u2211 t=1 \u2207\u03b8 log \u03c0\u03b8(Zt|St\u22121, Zt\u22121, C)]\n(39)\n= \u2212 E C,X0:T , Z0:T [ T\u2211 t=1 \u2207\u03b8 log \u03c0\u03b8(Zt|St\u22121, Zt\u22121, C) [ T\u2211 i=1 log \u03c0\u03b8(Zi|Si\u22121, Zi\u22121, C) + 1 ]]\n= \u2212 E C,X0:T , Z0:T [ T\u2211 t=1 \u2207\u03b8 log \u03c0\u03b8(Zt|St\u22121, Zt\u22121, C) T\u2211 i=t log \u03c0\u03b8(Zi|Si\u22121, Zi\u22121, C) ] (40)\nThe third equality holds because we adopt the one-step Markov assumption, i.e., the conditional probability distribution of a random variable depends only on its parent nodes in the probabilistic graphical model (shown as Figure 1). The fourth equality holds out of similar derivation as steps 2-4 in Equation 38. The last equality can be obtained with Equation 46 in the next section, where we prove that any term which is from \u2211T i=1 log \u03c0\u03b8(Zi|Si\u22121, Zi\u22121, C) + 1 and not a function of Zt will not influence the gradient calculation in Equation 39 and 40.\nWith similar derivations, we have:\n\u2207\u03d5LDI1 = E C,X0:T , Z0:T [ T\u2211 t=1 \u2207\u03d5 log \u03c0\u03d5(At\u22121|St\u22121, Zt, C) T\u2211 i=t logP\u03c9(Zi|X0:i, Z0:i\u22121, C) ] (41)\n\u2207\u03d5LDI2 = \u2212 E C,X0:T , Z0:T [ T\u2211 t=1 \u2207\u03d5 log \u03c0\u03d5(At\u22121|St\u22121, Zt, C) T\u2211 i=t log \u03c0\u03b8(Zi|Si\u22121, Zi\u22121, C) ] (42)\nAs for the gradient with respect to \u03c9, it can be computed with:\n\u2207\u03c9LDI = \u2207\u03c9LDI1 = T\u2211 t=1 \u2211 C prior(C) \u2211\nX0:t,Z0:t\nP\u03b8,\u03d5(X0:t, Z0:t|C)\u2207\u03c9 logP\u03c9(Zt|X0:t, Z0:t\u22121, C) (43)\nStill, for each timestep t, it\u2019s a standard likelihood maximization problem and can be optimized through SGD."
        },
        {
            "heading": "D.3. Gradients of the Imitation Learning Objective Term",
            "text": "We consider the imitation learning objective term LIL, i.e., the trajectory return shown as:\nLIL = \u2211 C prior(C) \u2211\nX0:T ,Z0:T\nP\u03b8,\u03d5(X0:T , Z0:T |C) T\u22121\u2211 i=0 RIL(Si, Zi, Zi+1, Ai|C) (44)\nFollowing the similar derivation with Equation 36, we can get:\n\u2207\u03b8LIL = E C,X0:T , Z0:T [ T\u2211 t=1 \u2207\u03b8 log \u03c0\u03b8(Zt|St\u22121, Zt\u22121, C) T\u22121\u2211 i=0 RIL(Si, Zi, Zi+1, Ai|C) ] (45)\nFurther, we note that for each t \u2208 {1, \u00b7 \u00b7 \u00b7 , T}, \u2200i < t\u2212 1, we have:\nE C,X0:T , Z0:T [\u2207\u03b8 log \u03c0\u03b8(Zt|St\u22121, Zt\u22121, C)RIL(Si, Zi, Zi+1, Ai|C)]\n= \u2211 C prior(C) \u2211\nX0:T ,Z0:T\nP\u03b8,\u03d5(X0:T , Z0:T |C)\u2207\u03b8 log \u03c0\u03b8(Zt|St\u22121, Zt\u22121, C)RIL(Si, Zi, Zi+1, Ai|C)\n= \u2211 C prior(C) \u2211\nX0:t\u22121, Z0:t \u2211 Xt:T , Zt+1:T P\u03b8,\u03d5(X0:T , Z0:T |C)\u2207\u03b8 log \u03c0\u03b8(Zt|St\u22121, Zt\u22121, C)RiIL\n= \u2211 C prior(C) \u2211\nX0:t\u22121, Z0:t\nP\u03b8,\u03d5(X0:t\u22121, Z0:t|C)\u2207\u03b8 log \u03c0\u03b8(Zt|St\u22121, Zt\u22121, C)RiIL\n(46)\n= \u2211 C prior(C) \u2211\nX0:t\u22121, Z0:t\u22121\nP\u03b8,\u03d5(X0:t\u22121, Z0:t\u22121|C) \u2211 Zt \u03c0\u03b8(Zt|St\u22121, Zt\u22121, C)\u2207\u03b8 log \u03c0\u03b8(Zt|St\u22121, Zt\u22121, C)RiIL\n= \u2211 C prior(C) \u2211\nX0:t\u22121, Z0:t\u22121\nP\u03b8,\u03d5(X0:t\u22121, Z0:t\u22121|C)RiIL \u2211 Zt \u03c0\u03b8(Zt|St\u22121, Zt\u22121, C)\u2207\u03b8 log \u03c0\u03b8(Zt|St\u22121, Zt\u22121, C)\n= \u2211 C prior(C) \u2211\nX0:t\u22121, Z0:t\u22121\nP\u03b8,\u03d5(X0:t\u22121, Z0:t\u22121|C)RiIL \u2211 Zt \u2207\u03b8\u03c0\u03b8(Zt|St\u22121, Zt\u22121, C)\n= \u2211 C prior(C) \u2211\nX0:t\u22121, Z0:t\u22121\nP\u03b8,\u03d5(X0:t\u22121, Z0:t\u22121|C)RiIL\u2207\u03b8 \u2211 Zt \u03c0\u03b8(Zt|St\u22121, Zt\u22121, C)\n= \u2211 C prior(C) \u2211\nX0:t\u22121, Z0:t\u22121\nP\u03b8,\u03d5(X0:t\u22121, Z0:t\u22121|C)RIL(Si, Zi, Zi+1, Ai|C)\u2207\u03b81 = 0\n(47)\nwhere RiIL = RIL(Si, Zi, Zi+1, Ai|C) for simplicity. We use the law of total probability in the third equality, which we also use in the later derivations. The fifth equality holds because i < t\u2212 1 and RIL(Si, Zi, Zi+1, Ai|C) is irrelevant to Zt. Based on Equation 45 and 46, we have:\n\u2207\u03b8LIL = E C,X0:T , Z0:T [ T\u2211 t=1 \u2207\u03b8 log \u03c0\u03b8(Zt|St\u22121, Zt\u22121, C) T\u22121\u2211 i=t\u22121 RIL(Si, Zi, Zi+1, Ai|C) ] (48)\nWith similar derivations, we can obtain:\n\u2207\u03d5LIL = E C,X0:T , Z0:T [ T\u2211 t=1 \u2207\u03d5 log \u03c0\u03d5(At\u22121|St\u22121, Zt, C) T\u22121\u2211 i=t\u22121 RIL(Si, Zi, Zi+1, Ai|C) ] (49)"
        },
        {
            "heading": "D.4. The Overall Unbiased Gradient Estimator",
            "text": "To sum up, the gradients with respect to \u03b8 and \u03d5 can be computed with \u2207\u03b8,\u03d5L = \u2207\u03b8,\u03d5(\u03b11LMI + \u03b12LDI + \u03b13LIL), where \u03b11:3 > 0 are the weights for each objective term and fine-tuned as hyperparameters. Combining Equation (36, 38, 39, 48) and Equation (37, 41, 42, 49), we have the actor-critic learning framework shown as Equation 11, except for the baseline terms, bhigh and blow.\nFurther, we claim that Equation 11 provides unbiased estimation of the gradients with respect to \u03b8 and \u03d5. We proof this by showing that E [\u2211T t=1 \u2207\u03b8 log \u03c0t\u03b8bhigh(St\u22121, Zt\u22121|C) ] = E [\u2211T t=1 \u2207\u03d5 log \u03c0t\u03d5blow(St\u22121, Zt|C) ] = 0, as follows:\nE C,X0:T , Z0:T [ T\u2211 t=1 \u2207\u03b8 log \u03c0\u03b8(Zt|St\u22121, Zt\u22121, C)bhigh(St\u22121, Zt\u22121|C) ]\n= \u2211 C prior(C) \u2211\nX0:T ,Z0:T\nP\u03b8,\u03d5(X0:T , Z0:T |C) T\u2211 t=1 \u2207\u03b8 log \u03c0\u03b8(Zt|St\u22121, Zt\u22121, C)bhigh(St\u22121, Zt\u22121|C)\n= \u2211 C prior(C) T\u2211 t=1 \u2211 X0:T ,Z0:T P\u03b8,\u03d5(X0:T , Z0:T |C)\u2207\u03b8 log \u03c0\u03b8(Zt|St\u22121, Zt\u22121, C)bhigh(St\u22121, Zt\u22121|C)\n= \u2211 C prior(C) T\u2211 t=1 \u2211 X0:t\u22121,Z0:t P\u03b8,\u03d5(X0:t\u22121, Z0:t|C)\u2207\u03b8 log \u03c0\u03b8(Zt|St\u22121, Zt\u22121, C)bhigh(St\u22121, Zt\u22121|C)\n\u2211 X0:t\u22121,Z0:t P\u03b8,\u03d5(X0:t\u22121, Z0:t|C)\u2207\u03b8 log \u03c0\u03b8(Zt|St\u22121, Zt\u22121, C)bhigh(St\u22121, Zt\u22121|C)\n= \u2211\nX0:t\u22121, Z0:t\u22121\nP\u03b8,\u03d5(X0:t\u22121, Z0:t\u22121|C) \u2211 Zt \u03c0\u03b8(Zt|St\u22121, Zt\u22121, C)\u2207\u03b8 log \u03c0\u03b8(Zt|St\u22121, Zt\u22121, C)bhigh(St\u22121, Zt\u22121|C)\n= \u2211\nX0:t\u22121,Z0:t\u22121 P\u03b8,\u03d5(X0:t\u22121, Z0:t\u22121|C)bhigh(St\u22121, Zt\u22121|C) \u2211 Zt \u2207\u03b8\u03c0\u03b8(Zt|St\u22121, Zt\u22121, C)\n= \u2211\nX0:t\u22121,Z0:t\u22121\nP\u03b8,\u03d5(X0:t\u22121, Z0:t\u22121|C)bhigh(St\u22121, Zt\u22121|C)\u2207\u03b81 = 0\nE C,X0:T , Z0:T [ T\u2211 t=1 \u2207\u03d5 log \u03c0\u03d5(At\u22121|St\u22121, Zt, C)blow(St\u22121, Zt|C) ]\n= \u2211 C prior(C) \u2211\nX0:T ,Z0:T\nP\u03b8,\u03d5(X0:T , Z0:T |C) T\u2211 t=1 \u2207\u03d5 log \u03c0\u03d5(At\u22121|St\u22121, Zt, C)blow(St\u22121, Zt|C)\n= \u2211 C prior(C) T\u2211 t=1 \u2211 X0:T ,Z0:T P\u03b8,\u03d5(X0:T , Z0:T |C)\u2207\u03d5 log \u03c0\u03d5(At\u22121|St\u22121, Zt, C)blow(St\u22121, Zt|C)\n= \u2211 C prior(C) T\u2211 t=1 \u2211 X0:t,Z0:t P\u03b8,\u03d5(X0:t, Z0:t|C)\u2207\u03d5 log \u03c0\u03d5(At\u22121|St\u22121, Zt, C)blow(St\u22121, Zt|C)\n= \u2211 C prior(C) T\u2211 t=1 \u2211 X0:t\u22121,Z0:t P\u03b8,\u03d5(X0:t\u22121, Z0:t|C) \u2211 Xt P\u03d5(Xt|X0:t\u22121, Z0:t, C)\u00b7\n\u2207\u03d5 log \u03c0\u03d5(At\u22121|St\u22121, Zt, C)blow(St\u22121, Zt|C)\n= \u2211 C prior(C) T\u2211 t=1 \u2211 X0:t\u22121,Z0:t P\u03b8,\u03d5(X0:t\u22121, Z0:t|C) \u2211 At\u22121 \u03c0\u03d5(At\u22121|St\u22121, Zt, C)\u00b7\n\u2207\u03d5 log \u03c0\u03d5(At\u22121|St\u22121, Zt, C)blow(St\u22121, Zt|C) \u2211 St P(St|St\u22121, At\u22121, C)\n= \u2211 C prior(C) T\u2211 t=1 \u2211 X0:t\u22121,Z0:t P\u03b8,\u03d5(X0:t\u22121, Z0:t|C)blow(St\u22121, Zt|C) \u2211 At\u22121 \u03c0\u03d5(At\u22121|St\u22121, Zt, C)\u00b7\n\u2207\u03d5 log \u03c0\u03d5(At\u22121|St\u22121, Zt, C)\n= \u2211 C prior(C) T\u2211 t=1 \u2211 X0:t\u22121,Z0:t P\u03b8,\u03d5(X0:t\u22121, Z0:t|C)blow(St\u22121, Zt|C) \u2211 At\u22121 \u2207\u03d5\u03c0\u03d5(At\u22121|St\u22121, Zt, C)\n= \u2211 C prior(C) T\u2211 t=1 \u2211 X0:t\u22121,Z0:t P\u03b8,\u03d5(X0:t\u22121, Z0:t|C)blow(St\u22121, Zt|C)\u2207\u03d5 \u2211 At\u22121 \u03c0\u03d5(At\u22121|St\u22121, Zt, C)\n= \u2211 C prior(C) T\u2211 t=1 \u2211 X0:t\u22121, Z0:t P\u03b8,\u03d5(X0:t\u22121, Z0:t|C)blow(St\u22121, Zt|C)\u2207\u03d51 = 0\nAlgorithm 1 Multi-task Hierarchical Adversarial Inverse Reinforcement Learning (MH-AIRL) 1: Input: Prior distribution of the task variable prior(C), expert demonstrations {XE0:T } (If the task or option annotations,\ni.e., {CE} or {ZE0:T }, are provided, the corresponding estimation in Step 6 is not required.) 2: Initialize the hierarchical policy \u03c0\u03b8 and \u03c0\u03d5, discriminator f\u03d1, posteriors for the task context P\u03c8 and option choice P\u03c9 3: for each training episode do 4: Generate M trajectories {(C,X0:T , Z0:T )} by sampling the task C \u223c prior(\u00b7) and then exploring it with \u03c0\u03b8 and \u03c0\u03d5 5: Update P\u03c8 and P\u03c9 by minimizing LMI and LDI (Eq. 9) using SGD with {(C,X0:T , Z0:T )} 6: Estimate the expert global and local latent codes with P\u03c8 and P\u03c9 , i.e., CE \u223c P\u03c8(\u00b7|XE0:T ), ZE0:T \u223c P\u03c9(\u00b7|XE0:T , CE) 7: Update f\u03d1 by minimizing the cross entropy loss in Eq. 8 based on {(C,X0:T , Z0:T )} and {(CE , XE0:T , ZE0:T )} 8: Train \u03c0\u03b8 and \u03c0\u03d5 by HPPO, i.e., Eq. 11, based on {(C,X0:T , Z0:T )} and f\u03d1 which defines D\u03d1 and RIL 9: end for\nD.5. Illustrations of Interactions among Networks in MH-AIRL"
        },
        {
            "heading": "E. Appendix on Evaluation Results",
            "text": ""
        },
        {
            "heading": "E.1. Plots of the Ablation Study",
            "text": "E.2. Implementation Details of MH-GAIL\nMH-GAIL is a variant of our algorithm by replacing the AIRL component with GAIL. Similar with Section 4.2, we need to provide an extension of GAIL with the one-step option model, in order to learn a hierarchical policy. The extension method follows Option-GAIL (Jing et al., 2021) which is one of our baselines. MH-GAIL also uses an adversarial learning framework that contains a discriminator D\u03d1 and a hierarchical policy \u03c0\u03b8,\u03d5, for which the objectives are as follows:\nmax \u03d1\nEC\u223cprior(\u00b7),(S,A,Z,Z\u2032)\u223c\u03c0E(\u00b7|C) [log(1\u2212D\u03d1(S,A,Z, Z \u2032|C))] +\nEC\u223cprior(\u00b7),(S,A,Z,Z\u2032)\u223c\u03c0\u03b8,\u03d5(\u00b7|C) [logD\u03d1(S,A,Z, Z \u2032|C)]\nmax \u03b8,\u03d5 LIL = max \u03b8,\u03d5 EC\u223cprior(\u00b7),(X0:T ,Z0:T )\u223c\u03c0\u03b8,\u03d5(\u00b7|C) T\u22121\u2211 t=0 RtIL, R t IL = \u2212 logD\u03d1(St, At, Zt+1, Zt|C)\n(50)\nwhere (S,A,Z, Z \u2032) denotes (St, At, Zt+1, Zt), t = {0, \u00b7 \u00b7 \u00b7 , T \u2212 1}. It can be observed that the definition of RtIL have changed. Moreover, the discriminator D\u03d1 in MH-GAIL is trained as a binary classifier to distinguish the expert demonstrations (labeled as 0) and generated samples (labeled as 1), and does not have a specially-designed structure like the discriminator D\u03d1 in MH-AIRL, which is defined with f\u03d1 and \u03c0\u03b8,\u03d5, so that it cannot recover the expert reward function."
        },
        {
            "heading": "E.3. Analysis of the Learned Hierarchical Policy on HalfCheetah-MultiVel and Walker-RandParam",
            "text": "First, we randomly select 6 task contexts for HalfCheetah-MultiVel and visualize the recovered hierarchical policy as the velocity change of each episode in Figure 7(a). It can be observed that the agent automatically discovers two options (Option 1: blue, Option 2: orange) and adopts Option 1 for the acceleration phase (0 \u2192 v/2 or 0 \u2192 v) and Option 2 for the deceleration phase (v/2 \u2192 0). This shows that MH-AIRL can capture the compositional structure within the tasks very well and transfer the learned basic skills to boost multi-task policy learning.\nSecond, we note that, for some circumstances, the basic skills need to be conditioned on the task context. For the MujocoMultiGoal/MultiVel tasks, the basic skills (e.g., Option 2: decreasing the velocity) can be directly transferred among the tasks in the class and the agent only needs to adjust its high-level policy according to the task variable (e.g., adopting Option 2 when achieving v/2). However, for tasks like Walker-RandParam, the skills need to adapt to the tasks, since the mass of the agent changes and so do the control dynamics. As shown in Figure 7(b), the learning performance would drop without conditioning the low-level policy (i.e., option) on the task context, i.e., MH-AIRL-no-cnt."
        }
    ],
    "title": "Multi-task Hierarchical Adversarial Inverse Reinforcement Learning",
    "year": 2023
}