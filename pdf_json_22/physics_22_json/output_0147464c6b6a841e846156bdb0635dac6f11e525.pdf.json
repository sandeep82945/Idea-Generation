{
    "abstractText": "Consider an online convex optimization problem where the loss functions are self-concordant barriers, smooth relative to a convex function h, and possibly non-Lipschitz. We analyze the regret of online mirror descent with h. Then, based on the result, we prove the following in a unified manner. Denote by T the time horizon and d the parameter dimension. 1. For online portfolio selection, the regret of \u1ebcG, a variant of exponentiated gradient due to Helmbold et al. [1], is \u00d5(T 2/3d1/3) when T > 4d/log d . This improves on the original \u00d5(T 3/4d1/2) regret bound for \u1ebcG. 2. For online portfolio selection, the regret of online mirror descent with the logarithmic barrier is \u00d5( p T d ). The regret bound is the same as that of Soft-Bayes due to Orseau et al. [2] up to logarithmic terms. 3. For online learning quantum states with the logarithmic loss, the regret of online mirror descent with the log-determinant function is also \u00d5( p T d ). Its per-iteration time is shorter than all existing algorithms we know.",
    "authors": [
        {
            "affiliations": [],
            "name": "Chung-En Tsai"
        },
        {
            "affiliations": [],
            "name": "Hao-Chung Cheng"
        },
        {
            "affiliations": [],
            "name": "Yen-Huan Li"
        }
    ],
    "id": "SP:96229ab48cc4ca709b3dc28881ec4a4c7d34dac9",
    "references": [
        {
            "authors": [
                "David P. Helmbold",
                "Robert E. Shapire",
                "Yoram Singer",
                "Manfred K. Warmuth"
            ],
            "title": "On-line portfolio selection using multiplicative updates",
            "venue": "Math. Financ.,",
            "year": 1998
        },
        {
            "authors": [
                "Laurent Orseau",
                "Tor Lattimore",
                "Shane Legg"
            ],
            "title": "Soft-Bayes: Prod for mixtures of experts with log-loss",
            "venue": "In Proc. 28th Int. Conf. Algorithmic Learning Theory,",
            "year": 2017
        },
        {
            "authors": [
                "Thomas M. Cover",
                "Erick Ordentlich"
            ],
            "title": "Universal portfolios with side information",
            "venue": "IEEE Trans. Inf. Theory,",
            "year": 1996
        },
        {
            "authors": [
                "Adam Kalai",
                "Santosh Vempala"
            ],
            "title": "Efficient algorithms for universal portfolios",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2002
        },
        {
            "authors": [
                "Haipeng Luo",
                "Chen-Yu Wei",
                "Kai Zheng"
            ],
            "title": "Efficient online portfolio with logarithmic regret",
            "venue": "In Adv. Neural Information Processing Systems",
            "year": 2018
        },
        {
            "authors": [
                "Zakaria Mhammedi",
                "Alexander Rakhlin"
            ],
            "title": "Damped Online Newton Step for Portfolio Selection",
            "venue": "In Proc. 35th Annu. Conf. Learning Theory,",
            "year": 2022
        },
        {
            "authors": [
                "Julian Zimmert",
                "Naman Agarwal",
                "Satyen Kale"
            ],
            "title": "Pushing the efficiency-regret Pareto frontier for online learning of portfolios and quantum states",
            "venue": "In Proc. 35th Annu. Conf. Learning Theory,",
            "year": 2022
        },
        {
            "authors": [
                "Chien-Ming Lin",
                "Yu-Ming Hsu",
                "Yen-Huan Li"
            ],
            "title": "An Online Algorithm for Maximum-Likelihood Quantum State Tomography",
            "venue": "In 24th Annu. Conf. Quantum Information Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Yurii Nesterov",
                "Arkadii Nemirovskii"
            ],
            "title": "Interior-Point Polynomial Algorithms in Convex Programming",
            "year": 1994
        },
        {
            "authors": [
                "Heinz H. Bauschke",
                "J\u00e9r\u00f4me Bolte",
                "Marc Teboulle"
            ],
            "title": "A descent lemma beyond Lipschitz gradient continuity: first-order methods revisited and applications",
            "venue": "Math. Oper. Res.,",
            "year": 2017
        },
        {
            "authors": [
                "Haihao Lu",
                "Robert M. Freund",
                "Yurii Nesterov"
            ],
            "title": "Relatively smooth convex optimization by first-order methods, and applications",
            "venue": "SIAM J. Optim.,",
            "year": 2018
        },
        {
            "authors": [
                "Nima Eshraghi",
                "Ben Liang"
            ],
            "title": "Dynamic regret of online mirror descent for relatively smooth convex cost functions",
            "venue": "IEEE Control Syst. Lett.,",
            "year": 2022
        },
        {
            "authors": [
                "Manfred K. Warmuth",
                "Dima Kuzmin"
            ],
            "title": "Online variance minimization",
            "venue": "In 19th Annu. Conf. Learning Theory, COLT",
            "year": 2006
        },
        {
            "authors": [
                "Sanjeev Arora",
                "Satyen Kale"
            ],
            "title": "A combinatorial, primal-dual approach to semidefinite programs",
            "venue": "In STOC \u201907: Proc. 39th Annu. ACM Symp. Theory of computing,",
            "year": 2007
        },
        {
            "authors": [
                "Wouter M. Koolen",
                "Wojtek Kot\u0142owski",
                "Manfred K. Warmuth"
            ],
            "title": "Learning eigenvectors for free",
            "venue": "In Adv. Neural Information Processing Systems",
            "year": 2011
        },
        {
            "authors": [
                "Scott Aaronson",
                "Xinyi Chen",
                "Elad Hazan",
                "Satyen Kale",
                "Ashwin Nayak"
            ],
            "title": "Online learning of quantum states",
            "venue": "In Adv. Neural Information Processing Systems",
            "year": 2018
        },
        {
            "authors": [
                "Xinyi Chen",
                "Elad Hazan",
                "Tongyang Li",
                "Zhou Lu",
                "Xinzhao Wang",
                "Rui Yang"
            ],
            "title": "Adaptive online learning of quantum states. 2022",
            "year": 2022
        },
        {
            "authors": [
                "Feidiao Yang",
                "Jiaqing Jiang",
                "Jialin Zhang",
                "Xiaoming Sun"
            ],
            "title": "Revisiting online quantum state learning",
            "venue": "In Proc. AAAI Conf. Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Wojciech Kot\u0142owski",
                "Gergely Neu"
            ],
            "title": "Bandit principal component analysis",
            "venue": "In Proc. 32nd Conf. Learning Theory,",
            "year": 2019
        },
        {
            "authors": [
                "Jacob Abernethy",
                "Elad Hazan",
                "Alexander Rakhlin"
            ],
            "title": "Competing in the dark: An efficient algorithm for bandit linear optimization",
            "venue": "In Proc. 21st Annu. Conf. Learning Theory,",
            "year": 2008
        },
        {
            "authors": [
                "Alexander Rakhlin",
                "Karthik Sridharan"
            ],
            "title": "Online learning with predictable sequences",
            "venue": "In Proc. 26th Annu. Conf. Learning Theory, pages 993\u20131019,",
            "year": 2013
        },
        {
            "authors": [
                "Elad Hazan"
            ],
            "title": "Introduction to online convex optimization",
            "venue": "Found. Trends Opt.,",
            "year": 2016
        },
        {
            "authors": [
                "Nicol\u00f2 Cesa-Bianchi",
                "G\u00e1bor Lugosi"
            ],
            "title": "Prediction, Learning, and Games",
            "year": 2006
        },
        {
            "authors": [
                "Tim van Erven",
                "Dirk van der Hoeven",
                "Wojciech Kot\u0142owski",
                "Wouter M. Koolen"
            ],
            "title": "Open Problem: Fast and Optimal Online Portfolio Selection",
            "venue": "In Proc. 33rd Annu. Conf. Learning Theory,",
            "year": 2020
        },
        {
            "authors": [
                "Lijun Zhang",
                "Tianbao Yang",
                "Jinfeng Yi",
                "Rong Jin",
                "Zhi-Hua Zhou"
            ],
            "title": "Improved dynamic regret for non-degenerate functions",
            "venue": "In Adv. in Neural Information Processing Systems",
            "year": 2017
        },
        {
            "authors": [
                "Nathan Srebro",
                "Karthik Sridharan",
                "Ambuj Tewari"
            ],
            "title": "Smoothness, low-noise and fast rates",
            "venue": "In Adv. Neural Information Processing Systems",
            "year": 2010
        },
        {
            "authors": [
                "Francesco Orabona"
            ],
            "title": "A modern introduction to online learning. 2022. arXiv:1912.13213v5 [cs.LG",
            "year": 1912
        },
        {
            "authors": [
                "Ali Jadbabaie",
                "Alexander Rakhlin",
                "Shahin Shahrampour",
                "Karthik Sridharan"
            ],
            "title": "Online optimization: Competing with dynamic comparators",
            "venue": "In Proc. 18th Int. Conf. Artificial Intelligence and Statistics,",
            "year": 2015
        },
        {
            "authors": [
                "Aryan Mokhtari",
                "Shahin Shahrampour",
                "Ali Jadbabaie",
                "Alejandro Ribeiro"
            ],
            "title": "Online optimization in dynamic environments: Improved regret rates for strongly convex problems",
            "venue": "In IEEE 55th Conf. Decision and Control (CDC),",
            "year": 2016
        },
        {
            "authors": [
                "Jacob D. Abernethy",
                "Elad Hazan",
                "Alexander Rakhlin"
            ],
            "title": "Interior-Point Methods for Full-Information and Bandit Online Learning",
            "venue": "IEEE Trans. Inf. Theory,",
            "year": 2012
        },
        {
            "authors": [
                "Marc Teboulle"
            ],
            "title": "A simplified view of first order methods for optimization",
            "venue": "Math. Program., Ser. B,",
            "year": 2018
        },
        {
            "authors": [
                "Elad Hazan",
                "Amit Agarwal",
                "Satyen Kale"
            ],
            "title": "Logarithmic regret algorithms for online convex optimization",
            "venue": "Mach. Learn.,",
            "year": 2007
        },
        {
            "authors": [
                "Rajendra Bhatia"
            ],
            "title": "Matrix Analysis",
            "year": 1997
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n21 0.\nConsider an online convex optimization problem where the loss functions are self-concordant barriers, smooth relative to a convex function h, and possibly non-Lipschitz. We analyze the regret of online mirror descent with h. Then, based on the result, we prove the following in a unified manner. Denote by T the time horizon and d the parameter dimension.\n1. For online portfolio selection, the regret of E\u0303G, a variant of exponen-\ntiated gradient due to Helmbold et al. [1], is O\u0303(T 2/3d1/3) when T > 4d/log d . This improves on the original O\u0303(T 3/4d1/2) regret bound for E\u0303G.\n2. For online portfolio selection, the regret of online mirror descent\nwith the logarithmic barrier is O\u0303( p T d ). The regret bound is the same as that of Soft-Bayes due to Orseau et al. [2] up to logarithmic terms.\n3. For online learning quantum states with the logarithmic loss, the\nregret of online mirror descent with the log-determinant function is also O\u0303( p T d ). Its per-iteration time is shorter than all existing algorithms we know."
        },
        {
            "heading": "1 Introduction",
            "text": "Online portfolio selection (OPS) is an online convex optimization problem more than three decades old. Unlike standard online convex optimization problems, the loss functions in OPS are neither Lipschitz nor smooth. Existing analyses of standard algorithms, such as online mirror descent (OMD) and follow the regularized leader (FTRL), are hence not directly applicable. The optimal regret in OPS is known to be O(d logT ), where d denotes the parameter dimension and T the time horizon, and achieved by Universal Portfolio Selection (UPS) [3, 4]. The per-iteration time of UPS, however, is too high for the algorithm to be practical [5]. O(d) is perhaps the lowest per-iteration time one can expect; this is actually achieved by E\u0303G [1], the barrier subgradient method (BSM) [6], and Soft-Bayes [2]. Nevertheless, the regret bound for E\u0303G is O\u0303(d1/2T 3/4); the regret bound for BSM and Soft-Bayes is O\u0303( p T d ). Both are significantly higher than that of UPS in terms of the dependence on T . Recent researches on OPS focus on achieving logarithmic regret (in T ) with acceptable periteration time. ADA-BARRONS achieves O(d2poly(logT )) regret with O(poly(d)T ) per-iteration time [7]. DONS with AdaMix [8] and BISONS [9] both possess the same O(d2poly(logT )) regret guarantee and reduce the per-iteration time to O(poly(d)). We summarize existing and our results in the form of an \u201cefficiency-regret Pareto frontier\u201d in Figure 1.\nThe generalization of OPS for the quantum setup (Section 3.3), interestingly, corresponds to online learning quantum states. This generalization can be understood either as online maximum-likelihood quantum state estimation [10] or a quantum version of probability forecasting with the logarithmic loss [9]. The quantum gen-\neralization is even more challenging: The dimension grows exponentially with the number of qubits (quantum bits). Scalability with respect to the dimension is hence critical.\nTo the best of our knowledge, there are only two algorithms for the quantum\ngeneralization. Both are obtained via generalizing existing OPS algorithms.\n1. Q-Soft-Bayes [10] is a quantum generalization of Soft-Bayes. It inherits the\nO( \u221a T d log d) regret guarantee of Soft-Bayes with O\u0303(d3) per-iteration time.\n2. Schr\u00f6dinger\u2019s-BISONS [9] is a quantum generalization of BISONS. It has O(d3 log2 T )\nregret and O\u0303(d6) per-iteration time1.\nIn OPS, unless the dimension is extremely high, one would prefer BISONS to SoftBayes. In the quantum generalization, regarding the exponentially growing dimension, the situation differs. It turns out that Q-Soft-Bayes\u2019 scalability with respect to the dimension makes it competitive.\nIn this paper, instead of pursuing a logarithmic regret with acceptable dimension scalability, we focus on sublinear-regret algorithms that scale with dimension. In particular, we prove the following.\n1. For OPS, the regret of E\u0303G, a variant of exponentiated gradient proposed by\nHelmbold et al. [1], is indeed O\u0303(d1/3T 2/3) when T > 4d/log d . This improves upon the O\u0303(d1/2T 3/4) regret bound by Helmbold et al. [1].\n2. For OPS, LB-OMD (OMD with the logarithmic barrier) yields O\u0303( p dT ) regret\nwith O\u0303(d) per-iteration time when T > d . Both are the same as those of BSM and Soft-Bayes.\n3. For the quantum generalization of OPS, Q-LB-OMD (OMD with the log-det\nfunction) yields O\u0303( p\ndT ) regret with O\u0303(d3) per-iteration time when T > d . Both the regret bound and time cost are the same as those of Q-Soft-Bayes. Nevertheless, each iteration of Q-Soft-Bayes requires computing one matrix exponential and two matrix logarithms, whereas each iteration of Q-LB-OMD only requires computing one eigendecomposition; the time cost of all other operations in the two algorithms are negligible.\nThe three results above are derived in a unified manner. We approach OPS and its quantum generalization via analyzing the regret of OMD for loss functions that are relatively smooth, possibly non-Lipschitz, self-concordant barriers. This is motivated by the fact that the loss functions in OPS are standard instances of selfconcordant barriers [11] and of relatively smooth functions [12], simultaneously. For online convex optimization with such loss functions, we derive a regret bound for OMD. Then, we prove the three results mentioned above as simple consequences of this result, thereby providing a principled approach to achieving the performances of Soft-Bayes and BSM and slightly better performances than Q-Soft-Bayes.\n1Zimmert et al. [9] only claims a O(poly(d)) per-iteration time, achieved by second-order convex opti-\nmization methods. We hence evaluate the per-iteration time as O\u0303((d2)3).\nOnline portfolio selection and online learning quantum states are two special cases covered by our framework. In most relatively smooth convex optimization problems, the loss functions are also self-concordant barriers. Therefore, our framework also applies to most problems studied by Bauschke et al. [12], Lu et al. [13], and Eshraghi and Liang [14] for relatively smooth convex optimization, in their online convex optimization formulations.\nNotation Let N \u2208N. We write [N ] for the set {1, . . . , N }. We write R+ and R++ for the sets of non-negative numbers and strictly positive numbers, respectively. We denote by Sd , Sd+, and S d ++ the set of Hermitian matrices, Hermitian positive semi-definite matrices, and Hermitian positive definite matrices in Cd\u00d7d , respectively. For any vecror v \u2208 Rd , we write v(i ) for its i -th entry. Let f : Rd \u2192 [\u2212\u221e,\u221e] be an extendedvalued function. Its domain is denoted by dom f . The interior and relative interior of a set S are denoted by intS and riS , respectively. Let H \u2208Sd with eigendecomposition H = \u2211 i \u03bbi Pi , where \u03bbi are eigenvalues and Pi are projections. Let f be a function whose domain contains {\u03bbi }. Then, f (H) := \u2211 i f (\u03bbi )Pi ."
        },
        {
            "heading": "2 Related Work",
            "text": "Relevant literature on OPS have been reviewed in Section 1."
        },
        {
            "heading": "2.1 Online Learning on the Set of Quantum States",
            "text": "Warmuth and Kuzmin [15] and Arora and Kale [16] independently discovered OMD with the von Neumann entropy for online principal component analysis (PCA) and semi-definite programming, respectively. The loss functions they considered are linear. Koolen et al. [17] studied a quantum generalization of probability forecasting with the matrix entropic loss. Whereas the problem formulation looks similar to the quantum generalization of OPS, the specific matrix entropic loss admits simple solutions based on existing probability forecasting algorithms. Another problem closely related to the quantum generalization is online shadow tomography [18, 19]. Online shadow tomography corresponds to online convex optimization with the absolute loss on the set of quantum states. The absolute loss is Lipschitz and hence standard from the online convex optimization perspective. Yang et al. [20] studied online convex optimization on the set of quantum states with general Lipschitz losses. The most relevant to this paper is perhaps the work on bandit PCA by Kot\u0142owski and Neu [21]. As in this paper, Kot\u0142owski and Neu [21] also considered OMD with the log-det function and showed that its iteration rule, though lacking a closed-form expression, can be computed efficiently."
        },
        {
            "heading": "2.2 Self-Concordance",
            "text": "The notions of self-concordance and self-concordant barriers were originally proposed for analyzing and developing convex optimization algorithms [11]. Self-concordant\nbarriers have been shown useful as regularizers in several online learning scenarios, such as bandit linear optimization [22] and optimistic online learning [23]; see also the monograph by Hazan [24]. In online learning, examples of self-concordant barriers as loss functions include probability forecasting [25, Chapter 9], OPS, and the quantum generalization of OPS. Except for the analysis by van Erven et al. [26], existing results do not explicitly use general properties of self-concordant barrier. Zhang et al. [27] studied the dynamic regret with losses that are self-concordant and strictly convex; it is easily checked that the strict convexity assumption is violated in OPS and its quantum generalization."
        },
        {
            "heading": "2.3 Relative Smoothness",
            "text": "The notion of relative smoothness was also originally proposed for analyzing and developing convex optimization algorithms, as a generalization of smoothness [12, 13]. Although smoothness is a standard assumption in convex optimization literature, this notion is relatively rarely exploited in online convex optimization. A few examples include the L\u22c6 bound [28, 29] and studies on the dynamic regret by, e.g., Jadbabaie et al. [30], Mokhtari et al. [31], Zhang et al. [27]. There seems to be only one paper on online convex optimization with relatively smooth losses [14]. The paper considers dynamic regret that in the worst case does not guarantee a sublinear regret; its requirement of a Lipschitz Bregman divergence is violated in LB-OMD."
        },
        {
            "heading": "2.4 Comparison with BSM and LB-FTRL",
            "text": "Our analysis exploits the properties of self-concordant barriers, which may look familiar to online learning experts. The use of self-concordant barriers in online learning was perhaps popularized by the \u201cinterior-point methods\u201d proposed by Abernethy et al. [32]. For OPS, the analyses of BSM [6] and LB-FTRL [26] also exploit the properties of self-concordant barriers. The difference lies in where the barrier properties are used. In the analyses of the interior-point methods, BSM, and LB-FTRL, it is the regularizer that is assumed to be a self-concordant barrier; in our analysis, it is the loss functions that are assumed to be self-concordant barriers. The difference is illustrated by our improved analysis of E\u0303G. The entropy regularizer is not a self-concordant barrier, so the analyses of the interior-point methods, BSM, and LB-FTRL do not directly apply. However, E\u0303G naturally fits in our framework.\nIndeed, BSM coincides with LB-FTRL with linearized losses. The original analysis of BSM by Nesterov [6] appears to be complicated to us. We provide an arguably simpler regret analysis of BSM, based on the approaches of Abernethy et al. [32] and van Erven et al. [26], in Appendix F."
        },
        {
            "heading": "3 Problem Formulations and Preliminaries",
            "text": ""
        },
        {
            "heading": "3.1 Online Convex Optimization",
            "text": "An online convex optimization problem is a multi-round game between two players, say Learner and Reality. Let X be a convex set. In the t-th round, Learner\nannounces some xt \u2208 X , given the history { x1, . . . , xt\u22121, f1, . . . , ft\u22121 }; then, Reality announces a proper closed convex function ft : X \u2192 (\u2212\u221e,+\u221e], given the history { x1, . . . , xt , f1, . . . , ft\u22121 }. The loss suffered by Learner in the t-th round is given by ft (xt ). Let T \u2208N be the time horizon. The regret is defined as\nRT (x) := T\u2211\nt=1 ft (xt )\u2212\nT\u2211 t=1 ft (x), \u2200x \u2208X .\nWe will write RT , without an input x, for the quantity supx\u2208X RT (x)."
        },
        {
            "heading": "3.2 Online Portfolio Selection",
            "text": "Let d \u2208N. OPS corresponds to an online convex optimization problem where\n\u2022 the set X is the probability simplex \u2206 := { (x(1), . . . , x(d)) \u2208Rd+ | \u2211d\ni=1 x(d) = 1} and\n\u2022 the loss function ft are given by ft (x) :=\u2212 log\u2329at , x\u232a for some at \u2208Rd+.\nWe assume that \u2016at\u2016\u221e \u2264 1. If this is not the case, we can consider the sequence (a\u0303t )t\u2208N for a\u0303t := at /\u2016at\u2016\u221e instead of (at )t\u2208N; the regret value is not affected.\nInterpretation OPS characterizes a game of multi-round investment. Suppose there are d assets and Learner has w1 dollars. In the first round, Learner distributes the w1 dollars to the assets following the fractions given by x1. The vector a1 denotes the price relatives of the d assets in that round. Then, after the first round, Learner has w2 = w1 \u2329a1, x1\u232a dollars. It is easily seen that after T rounds, we have\n\u2212 log wT+1\nw1 =\nT+1\u2211 t=1 ft (xt ).\nA small cumulative loss implies a large wealth growth rate."
        },
        {
            "heading": "3.3 Quantum Generalization of OPS",
            "text": "The following three concepts are needed to understand the quantum generalization\n1. A quantum state is represented by a density matrix, a Hermitian positive semi-\ndefinite matrix of unit trace. If there are q qubits, then the dimension of the density matrix is 2q \u00d7 2q . The vector of eigenvalues of a density matrix is a vector in the probability simplex.\n2. A measurement is represented by a positive operator-valued measure (POVM),\na set of Hermitian positive semi-definite matrices summing up to the identity matrix. If we consider 1-by-1 matrices, i.e., real numbers, then a POVM also corresponds to a vector in the probability simplex.\n3. Let \u03c1 \u2208 Cd\u00d7d be a density matrix and { M1, . . . , MK } \u2282 Cd\u00d7d be a POVM. The measurement outcome is a random variable \u03b7 taking values in [K ] such that\nP ( \u03b7= k ) = tr ( Mk\u03c1 ) , \u2200k \u2208 [K ].\nFor convenience, we denote the set of density matrices in Cd\u00d7d by Dd . We will denote density matrices by \u03c1 and \u03c3 instead of x and y , following the convention in quantum information.\nLet d \u2208N. The quantum generalization of OPS corresponds to an online convex optimization problem where\n\u2022 the set X equals Dd and \u2022 the loss function ft are given by ft (\u03c1) :=\u2212 logtr ( At\u03c1 ) for some At \u2208Sd+.\nThe quantum setup is challenging due to the presence of non-commutativity. When all matrices involved share the same eigenbasis, one may ignore the eigenbasis and focus on the vectors of eigenvalues. Then, it is easily seen that the generalization becomes equivalent to OPS. The reader is referred to, e.g., [15] and [16], for how the non-commutativity issue complicates the multiplicative weight update in the quantum case.\nInterpretation Fix a POVM { M1, . . . , MK } \u2282Cd\u00d7d . The class of all probability distributions on [K ] associated with the POVM, parameterized by Dd , is given by\nP := { (tr ( Mk\u03c1 ) )k\u2208[K ] | \u03c1 \u2208Dd } .\nConsider the following problem of probability forecasting with the logarithmic loss [25, Chapter 9]. In the t-th round, Learner announces a probability distribution pt = (pt (k))k\u2208[K ] \u2208 P ; then, Reality announces some \u03b7t \u2208 [K ]; the loss suffered by Learner is given by \u2212 log pt (\u03b7t ). It is easily checked that the probability forecasting problem is equivalent to the quantum generalization of OPS. Another closely related interpretation, based on maximum-likelihood quantum state tomography, was considered by [10]."
        },
        {
            "heading": "3.4 Online Mirror Descent",
            "text": "We focus on OMD in this paper. Similar results to those in this paper can be derived also for FTRL with linearized losses. Consider the general online convex optimization problem in Section 3.1. Let h be a Legendre function such that the closure of domh contains X . Then, h is differentiable on int domh. Define the associated Bregman divergence as\nDh(x, y) := h(x)\u2212h(y)\u2212\u2329\u2207h(y), x \u2212 y\u232a , \u2200(x, y) \u2208 domh\u00d7 int domh. (1)\nOMD with h iterates as the following.\n\u2022 Let x1 \u2208X \u2229 int domh.\n\u2022 For each t \u2208N, compute\nxt+1 = argmin x\u2208X \u03b7\u2329\u2207 ft (xt ), x \u2212 xt \u232a+Dh (x, xt ) (2)\nfor some learning rate \u03b7> 0.\nFor OMD to be well defined, we always assume that \u2207 ft (xt ) exists for all t \u2208N. Below are two famous instances of OMD.\n\u2022 OMD becomes online gradient descent when h = (1/2)\u2016\u00b7\u201622.\n\u2022 OMD becomes exponentiated gradient when h is the negative Shannon en-\ntropy and X is the probability simplex."
        },
        {
            "heading": "3.5 Function Class",
            "text": "We will consider the class of relatively smooth self-concordant barrier loss functions.\nDefinition 1 (Bauschke et al. [12], Lu et al. [13]). Let X be a convex set in Rd . We say that a convex function f is L-smooth relative to a differentiable convex function h on X for some L > 0 if the function Lh\u2212 f is convex on X .\nRelative smoothness becomes smoothness when h = (1/2)\u2016\u00b7\u201622. Since the gradient of a convex function is a monotone mapping, the following lemma immediately follows.\nLemma 2. Let f be a convex function L-smooth relative to a differentiable convex function h on X for some L > 0. Then,\nL \u2329\u2207h(y)\u2212\u2207h(x), y \u2212 x\u232a \u2265 \u2329\u2207 f (y)\u2212\u2207 f (x), y \u2212 x\u232a , \u2200x, y \u2208X .\nDefinition 3 (Nesterov [33]). We say a function f is M f -self-concordant for some M f \u2265 0 if\n\u2223\u2223D3 f (x)[u,u,u] \u2223\u2223\u2264 2M f \u2329u,\u22072 f (x)u\u232a 3/2 , \u2200x \u2208dom f ,u \u2208Rd ,\nwhere\nD3 f (x)[u,u,u] := d3 f\ndt 3 (x + tu) \u2223\u2223\u2223\u2223 t=0 .\nFor a self-concordant function, the monomonicity of its gradient mapping can\nbe strengthened [33, Theorem 5.1.8].\nLemma 4. Let f be an M f -self-concordant function for some M f \u2265 0. Then,\n\u2329\u2207 f (y)\u2212\u2207 f (x), y \u2212 x\u232a \u2265 \u2225\u2225y \u2212 x \u2225\u22252 x\n1+M f \u2225\u2225y \u2212 x \u2225\u2225 x , \u2200x, y \u2208dom f ,\nwhere \u2225\u2225y \u2212 x \u2225\u2225 x := \u2329y \u2212 x,\u22072 f (x)(y \u2212 x)\u232a1/2.\nDefinition 5 (Nesterov [33]). We say a convex function f is a \u03bd-self-concordant barrier if it is 1-self-concordant and\n\u2329\u2207 f (x),u\u232a2 \u2264 \u03bd\u2329u,\u22072 f (x)u\u232a , \u2200x \u2208dom f ,u \u2208Rd .\nWe now verify that the loss functions in OPS and its quantum generalization are relatively smooth self-concordant barriers. The following proposition is already proved by, e.g., Bauschke et al. [12, Lemma 7] and Nesterov [33, Example 5.3.1].\nProposition 6. Consider the function f (x) =\u2212 log\u2329a, x\u232a for some a \u2208Rd+, a 6= 0.\n1. The function is 1-smooth relative to the logarithmic barrier on Rd++.\n2. The function is a 1-self-concordant barrier.\nThe proof of the following proposition is deferred to Appendix A.\nProposition 7. Consider the function f (\u03c1) :=\u2212 logtr ( A\u03c1 )\nfor some A \u2208Sd+, A 6= 0.\n1. The function is 1-smooth relative to the log-det function on riDd .\n2. The function is a 1-self-concordant barrier."
        },
        {
            "heading": "4 Main Results",
            "text": "The results mentioned in Section 1 are presented in this section. Notice that there is a slight abuse of notations. The loss functions are always denoted as ft , though they differ in different subsections; for OPS, the iterates are always denoted as as xt , though the iterates are generated by different algorithms in different subsections. For the quantum generalization of OPS, the iterates are denoted as \u03c1t instead of xt , following the convention of the quantum information community."
        },
        {
            "heading": "4.1 Online Self-Concordant and Relatively Smooth Minimization",
            "text": "Our study of OPS and its quantum generalization is based on the following general result.\nTheorem 8. Consider an online convex optimization problem where the loss functions are 1-self-concordant barriers and L-smooth relative to a Legendre function h. Then, OMD with h and learning rate \u03b7 \u2208 (0,1/L) achieves\nRT (x) \u2264 Dh (x, x1)\n\u03b7 +\nT L\u03b7\n1\u2212L\u03b7 .\nThe proof of Theorem 8 is short, so we present it here. We start with the following lemma. The lemma should be familiar to convex optimization experts. Indeed, the lemma is obtained by simply adding time indices to the objective function in, e.g., the analysis by Lu et al. [13], Teboulle [34] for mirror descent minimizing convex relatively smooth functions. We provide a proof in Appendix B for completeness.\nLemma 9. It holds that\nT\u2211 t=1 ft (xt+1)\u2212 T\u2211 t=1 ft (x) \u2264 Dh (x, x1) \u03b7 .\nNotice that ft (xt+1) is not realizable because computing xt+1 needs \u2207 ft . We then bound the difference between the actual regret and the \u201cone-step-look-ahead\u201d regret in Lemma 9.\nLemma 10. It holds that\nT\u2211 t=1 ft (xt )\u2212 T\u2211 t=1 ft (xt+1) \u2264 T L\u03b7 1\u2212L\u03b7 .\nProof. Define the local norm \u2016u\u2016t := \u2329u,\u22072 ft (xt )u\u232a 1/2\nand rt := \u2016xt \u2212 xt+1\u2016t . By the optimality condition of (2), we have\n\u2329\u03b7\u2207 ft (xt )+\u2207h(xt+1)\u2212\u2207h(xt ), xt \u2212 xt+1\u232a \u2265 0. (3)\nThen, we write\nrt \u2265 \u2329\u2207 ft (xt ), xt \u2212 xt+1\u232a\n\u2265 1\n\u03b7 \u2329\u2207h(xt+1)\u2212\u2207h(xt ), xt+1 \u2212 xt \u232a\n\u2265 1\nL\u03b7 \u2329\u2207 ft (xt+1)\u2212\u2207 ft (xt ), xt+1 \u2212 xt \u232a\n\u2265 r 2t\nL\u03b7(1+ rt ) .\nIn the above, the first inequality follows from the definition of a self-concordant barrier (Definition 5); the second follows by rearranging (3); the third follows from the relative smoothness of f ; the fourth follows from Lemma 4. Solving the inequality for rt , we get\nrt \u2264 L\u03b7\n1\u2212L\u03b7 .\nThen, we write\nft (xt )\u2212 ft (xt+1) \u2264 \u2329\u2207 ft (xt ), xt \u2212 xt+1\u232a \u2264 rt \u2264 L\u03b7\n1\u2212L\u03b7 .\nThe lemma follows by summing the inequality from t = 1 to t = T .\nCombining Lemma 9 and Lemma 10, Theorem 8 follows."
        },
        {
            "heading": "4.2 E\u0303G",
            "text": "The original exponentiated gradient update requires the loss functions to be Lipschitz to achieve a sublinear regret. The Lipschitz assumption, as discussed in Section 1, does not hold in OPS and hence also its quantum generalization. Helmbold et al. [1] proposed a variant of the exponentiated gradient update, named E\u0303G, that gets rid of the Lipschitz assumption in OPS. The E\u0303G algorithm is presented in Algorithm 1, where e := (1, . . . ,1) \u2208 Rd and Dh denotes the Bregman divergence defined by h (1). Notice that x\u0302t+1 admits the closed-form expression\nx\u0302t+1(i ) \u221d x\u0302t (i )e\u2212\u03b7\u2207i f\u0302t (x\u0302t ), \u2200i \u2208 [d],\nwhere \u2207i f\u0302t (x\u0302t ) denotes the i -th entry of \u2207 f\u0302t (x\u0302t ).\nAlgorithm 1 E\u0303G for OPS.\nInput: \u03b7> 0, \u03b3 \u2208 (0,1). 1: x1 \u2190 e/d . 2: x\u03021 \u2190 e/d . 3: h(x) :=\n\u2211d i=1 x(i ) log x(i )\u2212 \u2211d i=1 x(i ).\n4: for all t \u2208N do 5: a\u0302t \u2190 (1\u2212\u03b3/d)at + (\u03b3/d)e. 6: f\u0302t (x) :=\u2212 log\u2329a\u0302t , x\u232a. 7: x\u0302t+1 \u2190 argminx\u2208\u2206 \u03b7\u2329\u2207 f\u0302t (x\u0302t ), x \u2212 x\u0302t \u232a+Dh (x, x\u0302t ). 8: xt+1 \u2190 (1\u2212\u03b3)x\u0302t+1 + (\u03b3/d)e. 9: end for\nTheorem 11. Suppose that T > 4d/log d. Then, setting\n\u03b3= 22/3d1/3\n(T logd)1/3 and \u03b7=\n\u03b3 p\nd \u221a\nT d\u03b3+d \u221a logd ,\nE\u0303G for OPS satisfies\nRT \u2264 25/3T 2/3d1/3(logd)2/3 +2\u22122/3T 1/3d2/3(logd)4/3\n= O\u0303(T 2/3d1/3).\nAs stated in Section 1, the original regret bound for E\u0303G is O\u0303(d1/2T 3/4). The key to\nthe improved regret bound is the following proposition.\nProposition 12. For any t \u2208N, the loss function ft =\u2212 log \u2329at , x\u232a in OPS is Gt -smooth relative to the negative Shannon entropy h (see Algorithm 1) on ri\u2206 for\nGt := sup x\u2208\u2206\n\u2225\u2225\u2207 ft (x) \u2225\u2225 \u221e = max\ni , j\u2208[d ]\nat (i ) at ( j ) ,\nwhere at (i ) and at ( j ) denote the i -th and j -th entry of at , respectively.\nA proof of the proposition above is provided in Appendix C. Though the proposition above seems to require the loss functions to be Lipschitz, the issue of nonLipschitz losses in OPS has been handled by the specific form of E\u0303G. Notice that G\u0302t := supx\u2208\u2206 \u2225\u2225\u2207 f\u0302t (x) \u2225\u2225 \u221e is upper bounded by d/\u03b3. In view of Proposition 12, Theo-\nrem 8 can be applied to the sequences of points {x\u0302t } and loss functions { f\u0302t } , with relative smooth parameter d/\u03b3. This explains why Gt does not appear in Theorem 11. The rest of the analysis is to estimate the difference between the regrets of {xt } and {x\u0302t }. A proof of Theorem 11 is provided in Appendix D, which essentially follows Helmbold et al. [1]."
        },
        {
            "heading": "4.3 LB-OMD",
            "text": "LB-OMD is presented in Algorithm 2, where Dh denotes the Bregman divergence defined by h. The iterate xt+1 does not have a closed-form expression. Nevertheless, as pointed by Kot\u0142owski and Neu [21, Appendix B], xt+1 can be efficiently computed by Newton\u2019s method minimizing a self-concordant function on R. The per-iteration time is hence O\u0303(d).\nAlgorithm 2 LB-OMD, online mirror descent with the logarithmic barrier, for OPS.\nInput: \u03b7> 0. 1: h(x) :=\u2212 \u2211d i=1 log x(i ). 2: x1 = e/d . 3: for all t \u2208N do 4: xt+1 \u2190 argminx\u2208\u2206 \u03b7\u2329\u2207 ft (xt ), x \u2212 xt \u232a+Dh (x, xt ). 5: end for\nTheorem 13. Suppose that T > d. Setting\n\u03b7= \u221a d log T p\nT + \u221a d log T ,\nLB-OMD satisfies\nRT \u2264 2 \u221a T d log T +d log T +2 = O\u0303( p T d ).\nWe will directly analyze the regret of a quantum generalization of LB-OMD (see the next subsection) in Appendix E. The proof Theorem 13 is similar and simpler and hence skipped."
        },
        {
            "heading": "4.4 Q-LB-OMD",
            "text": "Q-LB-OMD is presented in Algorithm 3, where I denotes the identity matrix and Dh denotes the Bregman divergence defined by h. Following the convention of quantum information, we denote the iterates by \u03c1t instead of xt . All matrices in the algorithm are of dimension d \u00d7d . Q-LB-OMD is a direct quantum generalization\nof LB-OMD, in the sense that when all matrices involved share the same eigenbasis, then Q-LB-OMD is equivalent to LB-OMD. Similar to LB-OMD, \u03c1t+1 does not have a closed-form expression. Nevertheless, as pointed by Kot\u0142owski and Neu [21, Appendix B], the iterate \u03c1t+1 can be computed by one eigendecomposition of \u03c1t followed by Newton\u2019s method minimizing a self-concordant function on R. The periteration time is hence O(d3 ). In Section 1, we have shown that the per-iteration time of Q-LB-OMD is the shortest, in comparison to Q-Soft-Bayes and Schr\u00f6dinger\u2019sBISONS.\nAlgorithm 3 Q-LB-OMD, online mirror descent with the log-det function, for the quantum generalization of OPS.\nInput: \u03b7> 0. 1: h(\u03c1) :=\u2212 log det\u03c1. 2: \u03c11 = I /d . 3: for all t \u2208N do 4: \u03c1t+1 \u2190 argmin\u03c1\u2208Dd \u03b7\u2329\u2207 ft (\u03c1t ),\u03c1\u2212\u03c1t \u232a+Dh (\u03c1,\u03c1t ). 5: end for\nTheorem 14. The statement of Theorem 13 also holds for Q-LB-OMD.\nA proof of the theorem above is provided in Appendix E."
        },
        {
            "heading": "5 Concluding Remarks",
            "text": "We have achieved the following.\n1. Provide an improved regret bound of E\u0303G for OPS.\n2. Show that LB-OMD is on the current efficiency-regret Pareto frontier for OPS.\n3. Show that Q-LB-OMD inherits the regret bound of LB-OMD and hence, re-\ngarding its scalability with respect to the dimension in both regret and periteration time, is competitive among existing algorithms.\nThe key idea in our analyses is to exploit the self-concordant barrier property and relative smoothness of the loss functions simultaneously. We are not aware of any literature that adopts this approach. In this paper, we demonstrate the benefit of this approach with a standard algorithm: OMD with a constant learning rate. This approach may help us develop and analyze other, perhaps more complicated, algorithms for OPS and its quantum generalization.\nThe regret bounds in this paper are likely to be sub-optimal for the problem class we consider: online convex optimization with losses that are relatively smooth and self-concordant barriers. Indeed, it is easily checked that a self-concordant barrier is necessarily exp-concave [33]. Therefore, for the problem class we consider, logarithmic regrets may be achieved by the exponentially weighted online optimization (EWOO) algorithm [35]. For example, EWOO coincides with UPS, known to be\nregret-optimal, for online portfolio selection. Zimmert et al. [9] claimed that EWOO achieves a O(d2 log T ) regret for online learning quantum states with the logarithmic loss, but we have not found a proof.\nRecall that, as discussed in the introduction, our aim is not to seek for regretoptimal algorithms, but to strike a balance between efficiency and regret. EWOO requires evaluating the expectation of a data-determined probability distribution in each iteration, which is computationally very expensive. The per-iteration time of UPS is already formidable. We are not aware of any polynomial-time implementation of EWOO for online learning quantum states with the logarithmic loss."
        },
        {
            "heading": "Acknowledgments",
            "text": "We thank the anonymous reviewers for their inspiring comments.\nC.-E. Tsai and Y.-H. Li are supported by the Young Scholar Fellowship (Einstein Program) of the National Science and Technology Council of Taiwan under grant numbers MOST 108-2636-E-002-014, MOST 109-2636-E-002-025, MOST 110-2636- E-002-012, MOST 111-2636-E-002-019, and NSTC 112-2636-E-002-003 and by the research project \u201cPioneering Research in Forefront Quantum Computing, Learning and Engineering\u201d of National Taiwan University under grant numbers NTU-CC111L894606 and NTU-CC-112L893406.\nH.-C. Cheng is supported by the Young Scholar Fellowship (Einstein Program) of the National Science and Technology Council (NSTC) in Taiwan (R.O.C.) under Grant MOST 111-2636-E-002-026, Grant MOST 111-2119-M-007-006, Grant MOST 111-2119-M-001-004, and is supported by the Yushan Young Scholar Program of the Ministry of Education in Taiwan (R.O.C.) under Grant NTU-111V1904-3, and Grant NTU-111L3401, and by the research project \u201cPioneering Research in Forefront Quantum Computing, Learning and Engineering\u201d of National Taiwan University under Grant No. NTU-CC-111L894605.\""
        },
        {
            "heading": "A Proof of Proposition 7",
            "text": "A.1 Relative Smoothness\nWe will use the following lemma [36, Exercise IV.2.7].\nLemma 15 (A matrix Cauchy-Schwarz inequality). Let K ,L \u2208Cd\u00d7d . Then,\n(tr |K L|)2 \u2264 tr ( |K |2 ) tr ( |L|2 )\nwhere |A| := (A\u2217A)1/2.\nDefine \u03d5(\u03c1) := logtr ( A\u03c1 ) \u2212 logdet\u03c1. It suffices to show that \u03d5 is convex on riDd ; equivalently, it suffices to show that D2\u03d5(\u03c1)[\u03c3,\u03c3] \u2265 0 for all \u03c1 \u2208 riDd and \u03c3 \u2208Sd [12, Proposition 17.7]. Since \u03d5(\u03c1) = log tr ( A\u03c1 ) \u2212tr log ( \u03c1 ) [37, Theorem 3.13], we write [37, Theorem 3.23 and Example 3.20]\nD2\u03d5(\u03c1)[\u03c3,\u03c3] = d2\u03d5\ndt 2 (\u03c1+ t\u03c3) \u2223\u2223\u2223\u2223 t=0\n= d\ndt\n{ tr(A\u03c3)\ntr ( A\u03c1+ t A\u03c3 ) \u2212 tr\n[ \u03c3(\u03c1+ t\u03c3)\u22121 ]}\u2223\u2223\u2223\u2223 t=0\n= { \u2212 ( tr(A\u03c3)\ntr ( A\u03c1+ t A\u03c3 ) )2 + tr [ \u03c3(\u03c1+ t\u03c3)\u22121\u03c3(\u03c1+ t\u03c3)\u22121 ]}\u2223\u2223\u2223\u2223 t=0\n=\u2212 ( tr(A\u03c3)\ntr ( A\u03c1 ) )2 + tr [ (\u03c3\u03c1\u22121)2 ] .\nNow, it suffices to prove that (tr(A\u03c3))2 \u2264 (tr ( A\u03c1 ) )2 tr ( (\u03c3\u03c1\u22121)2 ) . Let K = \u03c11/2 A\u03c11/2 and L = \u03c1\u22121/2\u03c3\u03c1\u22121/2. Applying Lemma 15 with K = \u03c11/2 A\u03c11/2 and L = \u03c1\u22121/2\u03c3\u03c1\u22121/2, we have\n(tr(A\u03c3))2 = (tr(K L))2\n\u2264 (tr |K L|)2 \u2264 tr ( |K |2 ) tr ( |L|2 ) = tr ( (A\u03c1)2 ) tr ( (\u03c3\u03c1\u22121)2 ) \u2264 (tr ( A\u03c1 ) )2 tr ( (\u03c3\u03c1\u22121)2 ) .\nIn the above, the first inequality follows from Weyl\u2019s majorant theorem [36, Theorem II.3.6]; the second inequality follows from Lemma 15. As for the third inequality, because A,\u03c1 \u2208SD+ , A\u03c1 has non-negative eigenvalues \u03bbi \u2265 0 (i \u2208 [D]). Then,\ntr ( (A\u03c1)2 ) = d\u2211\ni=1 \u03bb2i \u2264\n( d\u2211\ni=1 \u03bbi\n)2 = (tr ( A\u03c1 ) )2.\nThis completes the proof.\nA.2 Self-Concordant Barrier Property First, dom f = {\u03c1 \u2208Sd | tr ( A\u03c1 ) > 0} is open in Sd . A direct calculation gives\nD f (\u03c1)[\u03c3] =\u2212 tr(A\u03c3)\ntr ( A\u03c1 ) , D2 f (\u03c1)[\u03c3,\u03c3] =\n( tr(A\u03c3)\ntr ( A\u03c1 ) )2\nD3 f (\u03c1)[\u03c3,\u03c3,\u03c3] =\u22122 ( tr(A\u03c3)\ntr ( A\u03c1 ) )2 .\nThe self-concordant barrier property of f follows."
        },
        {
            "heading": "B Proof of Lemma 9",
            "text": "We will use the following lemma [34, Lemma 3.1].\nLemma 16 (Bregman proximal inequality). Let \u03d5 : Rd \u2192 (\u2212\u221e,\u221e] be a proper closed convex function. Let x \u2208 int domh and \u03b7> 0. Define\nx+ \u2208 argmin u\u2208Rd\n\u03d5(u)+ Dh(u, x)\n\u03b7 .\nThen,\n\u03d5(x+)\u2212\u03d5(u) \u2264 1\n\u03b7 (Dh(u, x)\u2212Dh (u, x+)\u2212Dh (x+, x)) , \u2200u \u2208domh.\nBy the L-smoothness of ft relative to h and convexity of ft , we write\nft (xt+1) \u2264 ft (xt )+\u2329\u2207 ft (xt ), xt+1 \u2212 xt \u232a+LDh (xt+1, xt ) = ft (xt )+\u2329\u2207 ft (xt ), x \u2212 xt \u232a+\u2329\u2207 ft (xt ), xt+1 \u2212 x\u232a+LDh (xt+1, xt ) \u2264 ft (x)+\u2329\u2207 ft (xt ), xt+1 \u2212 x\u232a+LDh (xt+1, xt ).\nApplying Lemma 16 to (2), we write\n\u2329\u2207 ft (xt ), xt+1 \u2212 x\u232a \u2264 1\n\u03b7 (Dh (x, xt )\u2212Dh (x, xt+1)\u2212Dh (xt+1, xt )) .\nCombining the two inequalities, we get\nft (xt+1) \u2264 ft (x)+ Dh(x, xt )\n\u03b7 \u2212\nDh (x, xt+1)\n\u03b7 +\n( L\u2212 1\n\u03b7\n) Dh (xt+1, xt )\n\u2264 ft (x)+ Dh(x, xt )\n\u03b7 \u2212\nDh (x, xt+1)\n\u03b7 ,\nwhere the last line is by the assumption \u03b7< 1/L. The lemma then follows by a telescopic sum."
        },
        {
            "heading": "C Proof of Proposition 12",
            "text": "We drop the subscript t in ft , at , and Gt for convenience. It suffices to show that \u22072 f (x) \u2264G\u22072h(x) for all x \u2208 ri\u2206. We write\n\u2329v,\u22072 f (x)v\u232a = ( \u2329a, v\u232a \u2329a, x\u232a )2\n= ( d\u2211\ni=1\na(i )x(i ) \u2329a, x\u232a v(i ) x(i )\n)2\n\u2264 d\u2211\ni=1\na(i )x(i )\n\u2329a, x\u232a\n( v(i )\nx(i )\n)2\n= d\u2211\ni=1\na(i ) \u2329a, x\u232a (v(i ))2 x(i )\n\u2264G d\u2211\ni=1\n(v(i ))2\nx(i ) ,\nwhere the first inequality follows from Jensen\u2019s inequality and the second follows from the definition of G. It remains to notice that\nd\u2211\ni=1\n(v(i ))2\nx(i ) = \u2329v,\u22072h(x)v\u232a ."
        },
        {
            "heading": "D Proof of Theorem 11",
            "text": "The proof strategy is similar to that of Helmbold et al. [1, proof of Theorem 4.2]. Since \u2016at\u2016\u221e \u2264 1, \u2016a\u0302t\u2016\u221e \u2264 1. Then, by the definition of a\u0302t , we have\nG\u0302\u221e := sup t\u2208N max i , j\u2208[d ]\na\u0302t (i ) a\u0302t ( j ) \u2264 d \u03b3 .\nNotice that Dh (x, x1) \u2264 logd for all x \u2208\u2206. Theorem 8 then implies\nT\u2211 t=1 f\u0302t (x\u0302t )\u2212 T\u2211 t=1 f\u0302t (x) \u2264 logd \u03b7 + T d\u03b7 \u03b3\u2212d\u03b7 . (4)\nBy the definition of a\u0302t , we have\n\u2329a\u0302t , x\u232a = ( 1\u2212 \u03b3\nd\n) \u2329at , x\u232a+ \u03b3\nd \u2265 \u2329at , x\u232a .\nTherefore, T\u2211\nt=1 f\u0302t (x) \u2264\nT\u2211 t=1 ft (x). (5)\nFollowing the argument before (4.4) of Helmbold et al. [1, proof of Theorem 4.2], we have\nlog \u2329at , xt \u232a \u2265 log\u2329a\u0302t , x\u0302t \u232a+ log ( 1\u2212\u03b3+ \u03b3\nd\n) .\nBy Jensen\u2019s inequality2 ,\nlog ( 1\u2212\u03b3+ \u03b3\nd\n) \u2265 (1\u2212\u03b3) log1+\u03b3 log 1\nd =\u2212\u03b3 logd .\nTherefore, T\u2211\nt=1 ft (xt )\u2212\nT\u2211 t=1 f\u0302t (x\u0302t ) \u2264 \u03b3T log d . (6)\nCombining (4), (5), (6), we get\nRT \u2264 logd\n\u03b7 +\nT d\u03b7\n\u03b3\u2212d\u03b7 +\u03b3T logd .\nWe first choose\n\u03b7= \u03b3 \u221a logd \u221a\nT d\u03b3+d \u221a logd ,\nwhich gives\nRT \u2264 2 \u221a\nT d log d p \u03b3 + d log d \u03b3 +\u03b3T logd .\nWe then choose \u03b3 that minimizes the sum of the first and third terms. The constraint on T is to ensure that \u03b3\u2208 (0,1)."
        },
        {
            "heading": "E Proof of Theorem 14",
            "text": "Since Dh (x, I /d) can be arbitrarily large when x approaches the boundary of Dd , we cannot directly apply Theorem 8. We make use of the following lemma [7, Lemma 10].\nLemma 17. Define\n\u03c1 := ( 1\u2212 1\nT\n) \u03c1+ I\nT d , \u2200\u03c1 \u2208Dd .\nThen,\nRT (\u03c1) \u2264 RT (\u03c1)+2. 2Helmbold et al. [1, proof of Theorem 4.2] assumes \u03b3 \u2208 (0,1/2] and bounds the quantity above by \u22122\u03b3.\nProof. The proof of Luo et al. [7, Lemma 10] only requires the convexity of ft and the\nfact that tr ( At\u03c1 ) \u2264 tr ( At\u03c1/(1\u22121/T ) ) (in our context) and hence directly extends for the quantum case.\nBy Lemma 17, we have\nRT (\u03c1) \u2264 RT (\u03c1)+2\n\u2264 Dh (\u03c1, I /d)\n\u03b7 +\nT\u03b7\n1\u2212\u03b7 +2.\nIt remains to notice that\nDh (\u03c1, I /d) =\u2212 logdet\u03c1\u2212d log d =\u2212 tr ( log\u03c1 ) \u2212d log d \u2264 d log T."
        },
        {
            "heading": "F Alternative Analysis of LB-FTRL with Linearized Losses",
            "text": "Algorithm 4 LB-FTRL with linearized losses\nInput: \u03b7> 0. 1: h(x) :=\u2212\n\u2211d i=1 log x(i ).\n2: x1 \u2208 argminx\u2208\u2206 h(x). 3: for all t \u2208N do 4: xt+1 \u2190 argminx\u2208\u2206 \u03b7 \u2211t \u03c4=1 \u2329\u2207 f\u03c4(x\u03c4), x \u2212 x\u03c4\u232a+h(x). 5: end for\nAlgorithm 5 FTRL with self-concordant barrier regularizer\nInput: \u03b7> 0, a constraint set X , and a self concordant barrier R for X . 1: x1 \u2208 argminx\u2208X R(x). 2: for all t \u2208N do 3: xt+1 \u2190 argminx\u2208X \u03b7 \u2211t \u03c4=1 \u2329\u2207 f\u03c4(x\u03c4), x \u2212 x\u03c4\u232a+R(x). 4: end for\nThis section provides an arguably simpler analysis of LB-FTRL with linearized\nlosses (Algorithm 4) for OPS, showing that the regret of Algorithm 4 is O\u0303( p dT ) .\nWe start with the following regret bound for FTRL with a self-concordant barrier\nregularizer due to Abernethy et al. [32].\nTheorem 18 (Abernethy et al. [32]). Let \u2016\u00b7\u2016x be the local norm associated with R at the point x. Assume that \u03b7 \u2225\u2225\u2207 ft (xt ) \u2225\u2225\u22172\nxt \u2264 1/4 for all t \u2208 N. Then, for any x \u2208 X ,\nAlgorithm 5 satisfies\nRT (x) \u2264 R(x)\u2212R(x1)\n\u03b7 +2\u03b7\nT\u2211\nt=1\n\u2225\u2225\u2207 ft (xt ) \u2225\u2225\u22172\nxt , \u2200x \u2208X .\nThe logarithmic barrier h is not a self-concordant barrier for \u2206 , so Theorem 18 does not directly apply to Algorithm 4. Nevertheless, there is a trick to handle the issue. As van Erven et al. [26] and Mhammedi and Rakhlin [8] do, consider an affine transformation \u03a0 : W \u2192\u2206 defined as follows.\nW := { w \u2208Rd\u22121+ : wi \u2265 0 \u2200i \u2208 [d \u22121], d\u22121\u2211\ni=1 wi \u2264 1\n} ,\n\u03a0(w) = Aw +ud , A = (\nId\u22121 \u2212e\u22a4\n) , ud = (0, . . . ,0,1)\u22a4,\nwhere with a slight abuse of notation, e denotes the all-one vector in Rd\u22121. The transformation \u03a0 is a bijection between W and \u2206. Then, Algorithm 4 can be equivalently written as\nw1 \u2208 argmin w\u2208W h\u0303(w),\nwt+1 \u2208 argmin w\u2208W\n\u03b7 t\u2211\n\u03c4=1 \u2329\u2207 f\u0303\u03c4(w\u03c4), w \u2212w\u03c4\u232a+ h\u0303(w), \u2200t \u2265 1,\nxt =\u03a0(wt ), \u2200t \u2265 1,\nwhere f\u0303\u03c4 = f\u03c4 \u25e6\u03a0 and h\u0303 = h\u25e6\u03a0. It is easily checked that h\u0303 is a self-concordant barrier for W . We then obtain the following result, as a corollary of Theorem 18.\nTheorem 19. Let \u2016\u00b7\u2016x be the local norm associated with h at the point x. Assume that \u03b7 \u2225\u2225\u2207 ft (xt ) \u2225\u2225\u22172 xt \u2264 1/4 for all t \u2208N. Then, Algorithm 4 satisfies\nRT (x) \u2264 h(x)\u2212h(x1)\n\u03b7 +2\u03b7\nT\u2211\nt=1\n\u2225\u2225\u2207 ft (xt ) \u2225\u2225\u22172\nxt , \u2200x \u2208\u2206.\nProof. By the equivalence mentioned above, Theorem 18 implies\nT\u2211 t=1 f\u0303t (wt )\u2212 f\u0303t (w) \u2264 h\u0303(w)\u2212 h\u0303(w1) \u03b7 +2\u03b7 T\u2211 t=1 \u2225\u2225\u2207 f\u0303t (wt ) \u2225\u2225\u22172 wt , \u2200w \u2208W ,\nwhere \u2016\u00b7\u2016w is the local norm associated with h\u0303 at w . It remains to notice that \u2225\u2225\u2207 f\u0303t (wt ) \u2225\u2225\u22172 wt = \u2329\u2207 ft (xt ), A(A\u22a4\u22072h(xt )A)\u22121 A\u22a4\u2207 ft (xt )\u232a \u2264 \u2225\u2225\u2207 ft (xt ) \u2225\u2225\u22172 xt .\nThen, the theorem follows.\nThe following fact was used in [26]. We provide a proof for completeness.\nLemma 20. It holds that \u2225\u2225\u2207 ft (xt ) \u2225\u2225\u22172 xt \u2264 1 for all t \u2208N. Proof. By Proposition 6, we have \u2207 ft (xt )\u2207 ft (xt )\u22a4 =\u22072 ft (xt ) \u2264\u22072h(xt ). For any \u03b5> 0,\n\u2329\u2207 ft (xt ), (\u03b5I +\u22072h(xt ))\u22121\u2207 ft (xt )\u232a\n\u2264 \u2329\u2207 ft (xt ), (\u03b5I +\u2207 ft (xt )\u2207 ft (xt )\u22a4)\u22121\u2207 ft (xt )\u232a = \u2225\u2225\u2207 ft (xt ) \u2225\u22252 2\n\u03b5+ \u2225\u2225\u2207 ft (xt ) \u2225\u22252 2 .\nLetting \u03b5\u2192 0 completes the proof.\nBy Theorem 19, Lemma 20, and Lemma 17, we obtain the following.\nCorollary 21. For \u03b7\u2264 1/4, Algorithm 4 satisfies\nRT (x) \u2264 d log T\n\u03b7 +2\u03b7T +2, \u2200x \u2208\u2206.\nOptimizing over \u03b7, we get the desired O\u0303( p dT ) regret bound."
        }
    ],
    "title": "Online Self-Concordant and Relatively Smooth Minimization, With Applications to Online Portfolio Selection and Learning Quantum States",
    "year": 2023
}