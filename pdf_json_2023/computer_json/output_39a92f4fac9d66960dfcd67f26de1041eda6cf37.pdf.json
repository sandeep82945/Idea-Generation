{
    "abstractText": "Amateurs working on mini-films and short-form videos usually spend lots of time and effort on the multi-round complicated process of setting and adjusting scenes, plots, and cameras to deliver satisfying video shots. We present Virtual Dynamic Storyboard (VDS) to allow users storyboarding shots in virtual environments, where the filming staff can easily test the settings of shots before the actual filming. VDS runs on a \u201cpropose-simulate-discriminate\u201d mode: Given a formatted story script and a camera script as input, it generates several character animation and camera movement proposals following predefined story and cinematic rules to allow an off-the-shelf simulation engine to render videos. To pick up the top-quality dynamic storyboard from the candidates, we equip it with a shot ranking discriminator based on shot quality criteria learned from professional manual-created data. VDS is comprehensively validated via extensive experiments and user studies, demonstrating its efficiency, effectiveness, and great potential in assisting amateur video production. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Los Angeles \u201923, August 06\u201310, 2023, Los Angeles, USA \u00a9 2023 Association for Computing Machinery. ACM ISBN 9798400701528. . . $15.00 https://doi.org/10.1145/3588028.3603647 CCS CONCEPTS \u2022 Information systems\u2192Multimedia content creation.",
    "authors": [
        {
            "affiliations": [],
            "name": "Anyi Rao"
        },
        {
            "affiliations": [],
            "name": "Xuekun Jiang"
        },
        {
            "affiliations": [],
            "name": "Yuwei Guo"
        },
        {
            "affiliations": [],
            "name": "Linning Xu"
        },
        {
            "affiliations": [],
            "name": "Lei Yang"
        },
        {
            "affiliations": [],
            "name": "Libiao Jin"
        },
        {
            "affiliations": [],
            "name": "Dahua Lin"
        },
        {
            "affiliations": [],
            "name": "Bo Dai"
        }
    ],
    "id": "SP:339bc058a5b38cf0fd32a8ff09e9d4e3c442bd56",
    "references": [
        {
            "authors": [
                "Ido Arev",
                "Hyun Soo Park",
                "Yaser Sheikh",
                "Jessica Hodgins",
                "Ariel Shamir."
            ],
            "title": "Automatic editing of footage from multiple social cameras",
            "venue": "ACM Transactions on Graphics (TOG) 33, 4 (2014), 1\u201311.",
            "year": 2014
        },
        {
            "authors": [
                "Hrishikesh Bhaumik",
                "Siddhartha Bhattacharyya",
                "Mausumi Das Nath",
                "Susanta Chakraborty."
            ],
            "title": "Real-time storyboard generation in videos using a probability distribution based threshold",
            "venue": "2015 Fifth International Conference on Communication Systems and Network Technologies. IEEE, 425\u2013431.",
            "year": 2015
        },
        {
            "authors": [
                "Simon Brodeur",
                "Ethan Perez",
                "Ankesh Anand",
                "Florian Golemo",
                "Luca Celotti",
                "Florian Strub",
                "Jean Rouat",
                "Hugo Larochelle",
                "Aaron Courville."
            ],
            "title": "HoME: a Household Multimodal Environment",
            "venue": "International Conference on Learning Representations Workshop.",
            "year": 2018
        },
        {
            "authors": [
                "Khyathi Chandu",
                "Eric Nyberg",
                "Alan W Black."
            ],
            "title": "Storyboarding of recipes: grounded contextual generation",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 6040\u20136046.",
            "year": 2019
        },
        {
            "authors": [
                "Peggy Chi",
                "Nathan Frey",
                "Katrina Panovich",
                "Irfan Essa."
            ],
            "title": "Automatic Instructional Video Creation from a Markdown-Formatted Tutorial",
            "venue": "The 34th Annual ACM Symposium on User Interface Software and Technology. 677\u2013690.",
            "year": 2021
        },
        {
            "authors": [
                "Matteo Fabbri",
                "Guillem Bras\u00f3",
                "Gianluca Maugeri",
                "Orcun Cetintas",
                "Riccardo Gasparini",
                "Aljo\u0161a O\u0161ep",
                "Simone Calderara",
                "Laura Leal-Taix\u00e9",
                "Rita Cucchiara."
            ],
            "title": "Motsynth: How can synthetic data help pedestrian detection and tracking",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision. 10849\u201310859.",
            "year": 2021
        },
        {
            "authors": [
                "Quentin Galvane",
                "Marc Christie",
                "Chrsitophe Lino",
                "R\u00e9mi Ronfard."
            ],
            "title": "Cameraon-rails: automated computation of constrained camera paths",
            "venue": "Proceedings of the 8th ACM SIGGRAPH Conference on Motion in Games. 151\u2013157.",
            "year": 2015
        },
        {
            "authors": [
                "Quentin Galvane",
                "Christophe Lino",
                "Marc Christie",
                "Julien Fleureau",
                "Fabien Servant",
                "Fra\u0146 ois-louis Tariolle",
                "Philippe Guillotel."
            ],
            "title": "Directing cinematographic drones",
            "venue": "ACM Transactions on Graphics (TOG) 37, 3 (2018), 1\u201318.",
            "year": 2018
        },
        {
            "authors": [
                "Xiaofeng Gao",
                "Ran Gong",
                "Tianmin Shu",
                "Xu Xie",
                "Shu Wang",
                "Song-Chun Zhu."
            ],
            "title": "VRKitchen: An interactive 3D environment for learning real life cooking tasks",
            "venue": "International Conference on Machine Learning Workshop.",
            "year": 2019
        },
        {
            "authors": [
                "Christoph Gebhardt",
                "Otmar Hilliges."
            ],
            "title": "Optimization-based User Support for Cinematographic Quadrotor Camera Target Framing",
            "venue": "Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 1\u201313.",
            "year": 2021
        },
        {
            "authors": [
                "Louis D Giannetti",
                "Jim Leach."
            ],
            "title": "Understanding movies",
            "venue": "Vol. 1. Prentice Hall Upper Saddle River, New Jersey.",
            "year": 1999
        },
        {
            "authors": [
                "Dan B Goldman",
                "Brian Curless",
                "David Salesin",
                "Steven M Seitz."
            ],
            "title": "Schematic storyboarding for video visualization and editing",
            "venue": "Acm transactions on graphics (tog) 25, 3 (2006), 862\u2013871.",
            "year": 2006
        },
        {
            "authors": [
                "Mirko Gschwindt",
                "Efe Camci",
                "Rogerio Bonatti",
                "Wenshan Wang",
                "Erdal Kayacan",
                "Sebastian Scherer."
            ],
            "title": "Can a robot become a movie director? learning artistic principles for aerial cinematography",
            "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 1107\u20131114.",
            "year": 2019
        },
        {
            "authors": [
                "Kaiming He",
                "Haoqi Fan",
                "Yuxin Wu",
                "Saining Xie",
                "Ross Girshick."
            ],
            "title": "Momentum contrast for unsupervised visual representation learning",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 9729\u20139738.",
            "year": 2020
        },
        {
            "authors": [
                "Li-wei He",
                "Michael F Cohen",
                "David H Salesin."
            ],
            "title": "The virtual cinematographer: A paradigm for automatic real-time camera control and directing",
            "venue": "Proceedings of the annual conference on Computer graphics and interactive techniques. 217\u2013224.",
            "year": 1996
        },
        {
            "authors": [
                "Yingqing He",
                "Tianyu Yang",
                "Yong Zhang",
                "Ying Shan",
                "Qifeng Chen."
            ],
            "title": "Latent Video Diffusion Models for High-Fidelity Video Generation with Arbitrary Lengths",
            "venue": "arXiv preprint arXiv:2211.13221 (2022).",
            "year": 2022
        },
        {
            "authors": [
                "Fangzhou Hong",
                "Mingyuan Zhang",
                "Liang Pan",
                "Zhongang Cai",
                "Lei Yang",
                "Ziwei Liu."
            ],
            "title": "AvatarCLIP: Zero-Shot Text-Driven Generation and Animation of 3D Avatars",
            "venue": "ACM Transactions on Graphics (TOG) 41, 4 (2022), 1\u201319.",
            "year": 2022
        },
        {
            "authors": [
                "Chong Huang",
                "Yuanjie Dang",
                "Peng Chen",
                "Xin Yang",
                "Kwang-Ting Tim Cheng."
            ],
            "title": "One-Shot Imitation Drone Filming of Human Motion Videos",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (2021).",
            "year": 2021
        },
        {
            "authors": [
                "Chong Huang",
                "Chuan-En Lin",
                "Zhenyu Yang",
                "Yan Kong",
                "Peng Chen",
                "Xin Yang",
                "Kwang-Ting Cheng."
            ],
            "title": "Learning to film from professional human motion videos",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 4244\u20134253.",
            "year": 2019
        },
        {
            "authors": [
                "Hui Huang",
                "Dani Lischinski",
                "Zhuming Hao",
                "Minglun Gong",
                "Marc Christie",
                "Daniel Cohen-Or."
            ],
            "title": "Trip Synopsis: 60km in 60sec",
            "venue": "Computer Graphics Forum, Vol. 35. Wiley Online Library, 107\u2013116.",
            "year": 2016
        },
        {
            "authors": [
                "Hongda Jiang",
                "Marc Christie",
                "Xi Wang",
                "Bin Wang",
                "Baoquan Chen."
            ],
            "title": "Camera Keyframing with Style and Control",
            "venue": "ACM Transactions on Graphics (TOG) (2021).",
            "year": 2021
        },
        {
            "authors": [
                "Hongda Jiang",
                "Bin Wang",
                "Xi Wang",
                "Marc Christie",
                "Baoquan Chen."
            ],
            "title": "Exampledriven virtual cinematography by learning camera behaviors",
            "venue": "ACM Transactions on Graphics (TOG) 39, 4 (2020), 45\u20131.",
            "year": 2020
        },
        {
            "authors": [
                "Xuekun Jiang",
                "Libiao Jin",
                "Anyi Rao",
                "Linning Xu",
                "Dahua Lin."
            ],
            "title": "Jointly Learning the Attributes and Composition of Shots for Boundary Detection in Videos",
            "venue": "IEEE Transactions on Multimedia (2021).",
            "year": 2021
        },
        {
            "authors": [
                "Mackenzie Leake",
                "Abe Davis",
                "Anh Truong",
                "Maneesh Agrawala."
            ],
            "title": "Computational video editing for dialogue-driven scenes",
            "venue": "ACM Trans. Graph. 36, 4 (2017), 130\u20131.",
            "year": 2017
        },
        {
            "authors": [
                "Junhua Liao",
                "Haihan Duan",
                "Xin Li",
                "Haoran Xu",
                "Yanbing Yang",
                "Wei Cai",
                "Yanru Chen",
                "Liangyin Chen."
            ],
            "title": "Occlusion Detection for Automatic Video Editing",
            "venue": "Proceedings of the ACM International Conference on Multimedia. 2255\u20132263.",
            "year": 2020
        },
        {
            "authors": [
                "Christophe Lino",
                "Marc Christie."
            ],
            "title": "Intuitive and efficient camera control with the toric space",
            "venue": "ACM Transactions on Graphics (TOG) 34, 4 (2015), 1\u201312.",
            "year": 2015
        },
        {
            "authors": [
                "Amaury Louarn",
                "Marc Christie",
                "Fabrice Lamarche."
            ],
            "title": "Automated staging for virtual cinematography",
            "venue": "Proceedings of the 11th Annual International Conference on Motion, Interaction, and Games. 1\u201310.",
            "year": 2018
        },
        {
            "authors": [
                "Amaury Louarn",
                "Quentin Galvane",
                "Fabrice Lamarche",
                "Marc Christie."
            ],
            "title": "An interactive staging-and-shooting solver for virtual cinematography",
            "venue": "Motion, Interaction and Games. 1\u20136.",
            "year": 2020
        },
        {
            "authors": [
                "Piotr Mirowski",
                "Kory W Mathewson",
                "Jaylen Pittman",
                "Richard Evans."
            ],
            "title": "Cowriting screenplays and theatre scripts with language models: An evaluation by industry professionals",
            "venue": "arXiv preprint arXiv:2209.14958 (2022).",
            "year": 2022
        },
        {
            "authors": [
                "Partha Pratim Mohanta",
                "Sanjoy Kumar Saha",
                "Bhabatosh Chanda."
            ],
            "title": "A novel technique for size constrained video storyboard generation using statistical run test and spanning tree",
            "venue": "International Journal of Image and Graphics 13, 01 (2013), 1350001.",
            "year": 2013
        },
        {
            "authors": [
                "KL Bhanu Moorthy",
                "Moneish Kumar",
                "Ramanathan Subramanian",
                "Vineet Gandhi."
            ],
            "title": "Gazed\u2013gaze-guided cinematic editing of wide-angle monocular video recordings",
            "venue": "Proceedings of the CHI Conference on Human Factors in Computing Systems. 1\u201311.",
            "year": 2020
        },
        {
            "authors": [
                "Nvidia."
            ],
            "title": "Omniverse Platform",
            "venue": "https://www.nvidia.com/omniverse/.",
            "year": 2023
        },
        {
            "authors": [
                "Thomas Oskam",
                "Robert W Sumner",
                "Nils Thuerey",
                "Markus Gross."
            ],
            "title": "Visibility transition planning for dynamic camera control",
            "venue": "Proceedings of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation. 55\u201365.",
            "year": 2009
        },
        {
            "authors": [
                "Tian Pan",
                "Yibing Song",
                "Tianyu Yang",
                "Wenhao Jiang",
                "Wei Liu."
            ],
            "title": "Videomoco: Contrastive video representation learning with temporally adversarial examples",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 11205\u201311214.",
            "year": 2021
        },
        {
            "authors": [
                "Alejandro Pardo",
                "Fabian Caba",
                "Juan Le\u00f3n Alc\u00e1zar",
                "Ali K Thabet",
                "Bernard Ghanem."
            ],
            "title": "Learning to Cut by Watching Movies",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision. 6858\u20136868.",
            "year": 2021
        },
        {
            "authors": [
                "Alejandro Pardo",
                "Fabian Caba Heilbron",
                "Juan Le\u00f3n Alc\u00e1zar",
                "Ali Thabet",
                "Bernard Ghanem."
            ],
            "title": "MovieCuts: A New Dataset and Benchmark for Cut Type Recognition",
            "venue": "arXiv preprint arXiv:2109.05569 (2021).",
            "year": 2021
        },
        {
            "authors": [
                "David Pizzi",
                "Jean-Luc Lugrin",
                "Alex Whittaker",
                "Marc Cavazza."
            ],
            "title": "Automatic generation of game level solutions as storyboards",
            "venue": "IEEE Transactions on Computational",
            "year": 2010
        },
        {
            "authors": [
                "XiaoyuWu",
                "Dahua Lin",
                "Libiao Jin"
            ],
            "title": "Temporal and Contextual Transformer",
            "year": 2022
        },
        {
            "authors": [
                "Dahua Lin"
            ],
            "title": "A Unified Framework for Shot Type Classification Based on",
            "year": 2020
        },
        {
            "authors": [
                "vision. Springer",
                "102\u2013118. Rmi Ronfard",
                "Vineet Gandhi",
                "Laurent Boiron",
                "A Murukutla"
            ],
            "title": "The prose story",
            "year": 2022
        },
        {
            "authors": [
                "Mottaghi",
                "Luke Zettlemoyer",
                "Dieter Fox"
            ],
            "title": "Alfred: A benchmark for inter",
            "year": 2020
        },
        {
            "authors": [
                "Sebastian Starke",
                "He Zhang",
                "Taku Komura",
                "Jun Saito"
            ],
            "title": "Neural state machine",
            "year": 2019
        },
        {
            "authors": [
                "Anh Truong",
                "Maneesh Agrawala"
            ],
            "title": "A Tool for Navigating and Editing",
            "venue": "character-scene interactions. ACM Trans. Graph. 38,",
            "year": 2019
        },
        {
            "authors": [
                "Van Gool."
            ],
            "title": "Temporal segment networks for action recognition in videos",
            "venue": "IEEE",
            "year": 2018
        },
        {
            "authors": [
                "Patrick Ye",
                "Timothy Baldwin"
            ],
            "title": "Towards Automatic Animated Storyboarding",
            "year": 2008
        },
        {
            "authors": [
                "yong Noh"
            ],
            "title": "Virtual Camera Layout Generation using a Reference Video",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Los Angeles \u201923, August 06\u201310, 2023, Los Angeles, USA \u00a9 2023 Association for Computing Machinery. ACM ISBN 9798400701528. . . $15.00 https://doi.org/10.1145/3588028.3603647\nCCS CONCEPTS \u2022 Information systems\u2192Multimedia content creation.\nKEYWORDS virtual storyboard, cinematography, neural networks\nACM Reference Format: Anyi Rao, Xuekun Jiang, Yuwei Guo, Linning Xu, Lei Yang, Libiao Jin, Dahua Lin, and Bo Dai. 2023. Dynamic Storyboard Generation in an Engine-based Virtual Environment for Video Production. In Special Interest Group on Computer Graphics and Interactive Techniques Proceedings (SIGGRAPH \u201923 Proceedings), August 06\u201310, 2023, Los Angeles, CA, USA. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3588028.3603647"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Born in 1930, storyboarding technique helps video directors and cinematographers design each shot1, figure out potential problems, and communicate ideas to save time and resources raised in practical video production [Wikipedia 2023]. A conventional storyboard (Fig. 2(a,b)) represents each shot with one or two still frames, depicting the scene layout, character actions, as well as camera parameters such as scale and angle. However, such a static storyboard is often dry and rigid, since it lacks the intrinsic capability to faithfully demonstrate dynamic semantics including character and camera movements. Although arrows and textual instructions can be used to indicate movement directions as amendments, they often lead to\n1Video is usually made up of several shots, where each one is a series of visual continuous frames.\nar X\niv :2\n30 1.\n12 68\n8v 3\nsignificant semantic ambiguity in the final static storyboard. Moreover, most conventional storyboards are hand-painted sketches that require both time and drawing skills. A rudimentary storyboard of a target video lasting for minutes can take artists hours to finish, let alone amateur creators who lack skills.\nGiven the aforementioned drawbacks, it is thus of great demand to improve conventional storyboard generation from two aspects: 1) producing dynamic storyboards instead of static ones, where the demonstration of dynamic semantics is straightforward. 2) building a handy tool that can create customized storyboards in a semi-auto way. There are two candidate solutions, namely neural video generative models as well as virtual cinematography. While neural video generation models have made significant progress in recent years [He et al. 2022; Singer et al. 2022], their outputs still suffer from frame-wise spatial-temporal inconsistency, let alone meet the cinematic requirements of dynamic storyboards. A better alternative is virtual cinematography [He et al. 1996], which includes three modules, real-time executor to drive characters move, virtual cinematographer to handle the visual layout, and render to output the results, which can be adapted for storyboard generation. Still, two significant problems need to be further addressed in order to fulfill this task with enough high quality. First, most existing works [Fabbri et al. 2021; Jiang et al. 2021a, 2020; Shah et al. 2018] only study the camera behavior under fixed plots and environments that require heavy manual force to modify. Secondly, the full action space of camera movements is too large to allow effective per-frame decisions [Truong et al. 2018]. The camera action space in filming consists of seven degrees of freedom (7DoF), where a decision on camera position, rotation, and focal length should be determined for each frame. Moreover, abrupt changes can easily occur when creating video frame by frame, yet one of our goals is to produce smooth videos with harmonious content and cinematic styles.\nIn this paper, we propose a novel virtual dynamic storyboard approach in an efficient and customized way for amateurs, as shown in Fig.4. It follows a \u201cpropose-simulate-discriminate\u201d paradigm. For each shot, it first translates the inputted high-level story and camera script in the form of (\u27e8char\u27e9 \u27e8do\u27e9 \u27e8sth/swh\u27e9) tuples for story scripts and (\u27e8movement\u27e9 \u27e8scale\u27e9 \u27e8angle\u27e9) tuples for camera scripts, into proposals of shot hyperparameters. Each proposal is then executed by the simulation module built on top of a modern graphic engine (e.g., Unity, Unreal, Omniverse [Nvidia 2023; Unity 2023; Unreal 2023]) to acquire its corresponding rendered output. Finally, a datadriven discriminator is adopted to rank all these rendered outputs as a recommendation for users to choose the final dynamic demonstration of this shot. Our key insights are as follows: 1) The enormous action space for all possible character and camera trajectories on a frame-by-frame basis can easily lead to low-quality shot candidates in terms of inter-frame abruptness and less meaningful camera\nmotions, let alone the desired artistic feelings. Our shot-based story and camera proposal generation significantly reduces the search space for appropriate camera configurations, further ensuring the results are more plausible. 2) By wrapping the graphic engine into a simulation module, the proposed approach significantly improves its accessibility and computational efficiency, thanks to engines\u2019 rich and highly-structured functionalities and highly-optimized computation pipeline. Moreover, the proposed approach also readily enjoys a series of additional benefits brought by the graphic engine, such as function-wise scalability and high rendering quality. 3) There might be several plausible shots satisfying the requirements of story and camera scripts. No universal criteria can serve as direct supervision and evaluation metrics for generated shots. We resort to a data-driven learning approach that aims to automatically discover rules from professional manual-created clips. with a shot ranking discriminator. It learns from a large amount of data to evaluate shots by distinguishing between different quality samples. Through the class-aware contrastive objectives, our approach can successfully select shots with top-ranked qualities among generated proposals.\nIn summary, we contribute Virtual Dynamic Storyboard (VDS): (1) It takes story and camera scripts as input and translates them into dynamic shot sequences for pre-production that follows cinematic filming rules. (2) The design of our cinematic filming module with the associated camera action subspace not only brings interpretable control to users but also effectively reduces the action space which leads to more plausible results. (3) The shot ranking discriminator mitigates the gap that there are no ground truth and universal criteria by learning from professional manual-created clips with carefully designed class-aware contrastive objectives. (4) Extensive user studies and qualitative evaluations demonstrate the effectiveness of VDS. Codes will be released upon publication. Assumption. Though our tool can generate a sequence of shots, its technique novelty mainly focuses on how to compose a single shot. The users have the freedom to decide how different shots are connected to bring out a sequence and their transition effects. Hence, we assume that each shot duration is equal to its corresponding atomic character action and they are connected directly in our paper to form a sequence, noting that different shots can be \u201ccut\u201d and connected in various ways using our shots as raw data [Pardo et al. 2021a,b]. This corresponds to the industry process that applies multiple cameras to record one action and take them as raw inputs to the post-editing process with \u201ccut\u201d."
        },
        {
            "heading": "2 RELATEDWORK",
            "text": "Storyboard and dynamic content creation. Storyboards are investigated in keyframe or text summarization from videos [Bhaumik et al. 2015; Mohanta et al. 2013; Ronfard et al. 2022], textual\nscript writing [Chandu et al. 2019; Mirowski et al. 2022] and video creation assistance [Goldman et al. 2006; Pizzi et al. 2010; Ye and Baldwin 2008]. Among the most relevant ones, Ye et al [Ye and Baldwin 2008] focus on the language mapping between the action descriptions and avatar animation instead of the visual content and PIzzi et al [Pizzi et al. 2010] only generate sketch-style static images. Besides, intelligent creation tools are in great demand as they can help users efficiently create customized dynamic content, e.g., video, animation [Louarn et al. 2018, 2020]. Some researchers focus on several key steps, such as frame composition [Zhong et al. 2021], shot selection [Jiang et al. 2021b; Liao et al. 2020], shot cut suggestion [Pardo et al. 2021a]. Others tackle high-level automatic procedures with simple user interactions and take multiple videos captured by different cameras to produce a coherent video in different application scenarios [Arev et al. 2014; Leake et al. 2017; Truong and Agrawala 2019] using different data sources [Chi et al. 2021; Moorthy et al. 2020; Rao et al. 2022a,b; Wang et al. 2019]. Our system also belongs to high-level automatic creation that takes story/camera scripts as input. Camera control for cinematography. Camera motion plays an important role in delivering content from a given environment [He et al. 1996; Wu et al. 2018]. Early works start from handcrafting quality criteria such as visibility and smoothness [Galvane et al. 2015; Huang et al. 2016] for route planning [Oskam et al. 2009]. However, they are limited in the scope of possible actions and lack generalizability in broader scenarios. Later works tend to directly imitate exemplar videos to generate similar camera trajectories with SfM [Sanokho et al. 2014]. Jiang et al [Jiang et al. 2020] use deep neural networks to extract camera behaviors from real movie clips based on toric space [Lino and Christie 2015], which is helpful in imitating camera motion [Yoo et al. 2021] and keyframing [Jiang et al. 2021a]. However, the inaccuracy of SfM and toric space estimation may severely affect their performance. Another research direction targets drone photography [Galvane et al. 2018; Gebhardt and Hilliges 2021], which explores imitation learning to learn from experts. Reinforcement learning is used to maximize an aestheticbased reward [Gschwindt et al. 2019; Huang et al. 2019] with extension to style control [Huang et al. 2021]. However, the shot styles they studied are quite limited compared to the broad categories of plausible shots in real film production, and it is generally not easy to acquire ground truth trajectory data for training. Simulation and virtual environments. Simulation engines can facilitate the training of machine learning models in autonomous driving, drones, robots and so on [Brodeur et al. 2018; Gao et al. 2019; Richter et al. 2016], with the potential to augment an infinite number of data samples. While many virtual platforms for realworld scenes can only complete some single agent tasks [Starke et al. 2019; Zhang et al. 2018], a line of research focus on social AI to construct multi-people tasks recently [Savva et al. 2019; Shridhar et al. 2020] and develop virtual scenes are based on Unity [Unity 2023] or Unreal [Unreal 2023]. With developed APIs, our VDS supports script-level auto control over Unity [Unity 2023] and Omniverse [Nvidia 2023] engines. It is able to simulate over 100 kinds of camera shot styles and character actions to fulfill a storyboard, and render it out."
        },
        {
            "heading": "3 VIRTUAL DYNAMIC STORYBOARD",
            "text": "The framework of Virtual Dynamic Storyboard (VDS) is shown in Fig. 4. It first proposes several candidates of executable parameters for each shot\u2019s story script and camera script as inputs (Sec. 3.1 and Sec. 3.2), following the story and cinematic rules. The proposal storyboard videos are then rendered by an engine-based simulator that satisfy the corresponding parameters. Through class-aware contrastive learning, a shot ranking discriminator helps to score the generated proposals and assists users in selecting their favorites (Sec. 3.3). The practical UI is presented in Sec. 3.4."
        },
        {
            "heading": "3.1 Story Script Proposal",
            "text": "One of the raw inputs to VDS is a sentence of story scripts in the format of (\u27e8char\u27e9 \u27e8do\u27e9 \u27e8sth/swh\u27e9). Although graphic engines have strong ability in scene establishment and character animation with manual force (mouse clicking/dragging and keyboard typing), these functions cannot be directly called with the aforementioned story scripts. To fulfill this goal, we develop an engine-based simulation module with a series of automatic APIs including: scene selection, character placement and animation, camera control.\nAfter the selection of characters and scenes from available assets, each story script allows a chosen virtual character to interact with a predefined scene and executes its animation for each timestamp in the simulator. Specifically, 1) \u27e8char\u27e9 and \u27e8sth/swh\u27e9: \u27e8char\u27e9 links the selected character asset. And scene is represented as a hierarchical tree, e.g., a house is composed of several rooms, which in turn are composed of several objects. From this scene graph, each object and place have their position, which can be associated with the \u27e8sth/swh\u27e9 in the story scripts. 2) \u27e8do\u27e9: For each atomic action \u27e8do\u27e9, we retrieve corresponding pre-recorded \ud835\udc41 proposal animation clips [a1, a2, . . . , a\ud835\udc41 ] from a predefined animation clip pool and associate them with the character and object/place. Each clip a\ud835\udc5b can be executed by the simulator that output a video reflecting the semantics of atomic motion. If the atomic action involves a distance-motion, e.g., walk/run,\ud835\udc40 proposal paths [p\ud835\udc5b,1, p\ud835\udc5b,2, . . . , p\ud835\udc5b,\ud835\udc40 ] are generated between the character and the object for a\ud835\udc5b . Each paths are selected with the scene graph to avoid objects getting in the way. The motion trajectory of the character can be accurately represented by p\ud835\udc5b,\ud835\udc5a = (\ud835\udc65\ud835\udc61\ud835\udc5d\ud835\udc5b,\ud835\udc5a , \ud835\udc66 \ud835\udc61 \ud835\udc5d\ud835\udc5b,\ud835\udc5a\n, \ud835\udc67\ud835\udc61\ud835\udc5d\ud835\udc5b,\ud835\udc5a ), \ud835\udc61 \u2208 [0,\ud835\udc47 ) in the world coordinate, and \ud835\udc47 is equal to the length of a\ud835\udc5b .\nThe story parameters s \u2208 S of each executable storyboard animations for one story script can be represented as:\ns = (a\ud835\udc5b, p\ud835\udc5b,\ud835\udc5a), s \u2208 S. (1)"
        },
        {
            "heading": "3.2 Camera Script Proposal",
            "text": "Preliminaries for camera control. Following our assumption, each story script (\u27e8char\u27e9 \u27e8do\u27e9 \u27e8sth/swh\u27e9) will produceS executable animations, and the filmed shot duration is the same as the character\u2019s animation length, which is specified by its corresponding action clip a. Thus, in order to take a shot for each atomic action, it is necessary to obtain a list of 7DoF camera parameters that represent the camera trajectory:\nc = {(\ud835\udc65\ud835\udc61 , \ud835\udc66\ud835\udc61 , \ud835\udc67\ud835\udc61 ), (\ud835\udefc\ud835\udc61 , \ud835\udefd\ud835\udc61 , \ud835\udefe\ud835\udc61 ), \ud835\udc53 \ud835\udc61 }, \ud835\udc61 \u2208 [0,\ud835\udc47 ), c \u2208 C, (2) where (\ud835\udc65\ud835\udc61 , \ud835\udc66\ud835\udc61 , \ud835\udc67\ud835\udc61 ) stands for the position in world coordinates. The roll \ud835\udefc\ud835\udc61 , pitch \ud835\udefd\ud835\udc61 , and yaw \ud835\udefe\ud835\udc61 describe the camera rotations along the\n\ud835\udc4d\ud835\udc36 , \ud835\udc4b\ud835\udc36 , \ud835\udc4c\ud835\udc36 axes in the camera local coordinate, where the upward direction is specified by a \u201cLook-At\u201d constraint.2 The focal length \ud835\udc53 is set in the commonly used range [30, 80]mm of films. Cinematic camera control. To associate the produced shots with the classical cinematic style filming language, and provide easy control to users, we introduce three dimensions for shot type control and allow the input in the format of (\u27e8movement\u27e9 \u27e8scale\u27e9 \u27e8angle\u27e9), which are widely used in the filming industry to increase production efficiency [Rao et al. 2020]. The definition of these three factors and their complete list of subcategories can be found in [Giannetti and Leach 1999]. Instead of freely searching from the general camera action space, we identify a set of camera subspaces that defines meaningful camera trajectories in terms of shot scale, angle, and movements.\nIn the following, we adopt a human-centric explanation and introduce a direction vector represented in the spherical coordinate system (\ud835\udc5f\ud835\udc61 , \ud835\udf03\ud835\udc61 , \ud835\udf11\ud835\udc61 ) in terms of the radial distance \ud835\udc5f\ud835\udc61 , polar angle \ud835\udf03\ud835\udc61 , and azimuthal angle \ud835\udf11\ud835\udc61 to facilitate control. This representation explicitly derives the relative positions between the main character and the cameras, and bears the merit of having an one-to-one mapping from (\ud835\udc65\ud835\udc61 , \ud835\udc66\ud835\udc61 , \ud835\udc67\ud835\udc61 ) to (\ud835\udc5f\ud835\udc61 , \ud835\udf03\ud835\udc61 , \ud835\udf11\ud835\udc61 ) via: (\ud835\udc65\ud835\udc61 , \ud835\udc66\ud835\udc61 , \ud835\udc67\ud835\udc61 ) = (\ud835\udc65\ud835\udc61\ud835\udc5d , \ud835\udc66\ud835\udc61\ud835\udc5d , \ud835\udc67\ud835\udc61\ud835\udc5d ) + \ud835\udc5f\ud835\udc61 (cos\ud835\udf11\ud835\udc61 sin\ud835\udf03\ud835\udc61 , sin\ud835\udf11\ud835\udc61 sin\ud835\udf03\ud835\udc61 , cos\ud835\udf03\ud835\udc61 ) . (3)\nBased on the specification of the shot scale and angle, we can determine the camera parameters for the keyframes in a shot, while the type of camera movement finally determines the camera parameter for each interpolated timestamp \ud835\udc61 in-between [0,\ud835\udc47 ). Fig. 3 shows an illustration of the camera parameters and a selection of shot types. Some basic categories are shown below, which can be naturally extended to more with details explained in the supplementary.\n3.2.1 Shot Angle. This is largely determined by the relative position between camera and target, with a special focus put on their altitude difference. As shown in Fig. 3, different angles are reflected by the vector \ud835\udf03 , e.g., \ud835\udf03 = \ud835\udf0b/2 represents eye-level shot, \ud835\udf03 = 2\ud835\udf0b/5 produces high-angle shot, and \ud835\udf03 = 4\ud835\udf0b/5 serves low-angle shot. 2The commonly adopted \u201cLook-At\u201d constraint for camera is realized by a TR matrix that positions/rotates the camera to look at a target point in space from its positioned point, according to the orientation set up by the up-vector and the looking direction. It positions the camera to be horizontal in most cases except for some advanced effects, such as \u201croll\u201d shots. Details are specified in the supplementary.\n3.2.2 Shot Scale. It is determined by the size of the target object within the frame, which is implemented with a virtual sphere centered on the target character with the distance radius \ud835\udc5f\ud835\udc61 and the focal length \ud835\udc53 \ud835\udc61 . For example, for \ud835\udc53 \ud835\udc61 = 50 mm and a character with height \u210e, \ud835\udc5f\ud835\udc61 \u2208 {0.2\u210e, 0.5\u210e,\u210e} is pre-set for close-ups, medium, full shots respectively. \ud835\udc53 \ud835\udc61 and \ud835\udc5f\ud835\udc61 can be adjusted accordingly to meet individual needs.\n3.2.3 Shot Movement. In the human-centric scenario, the camera movements are correlated with the motion trajectory of the character. Based on the above controls for shot scale and angle, we elaborate the control of camera movements that depicts the camera parameters for \u2200\ud835\udc61 \u2208 [0,\ud835\udc47 ). Nine basic movement types are briefly explained below. Static shot keeps the 7DoF as constants throughout the time duration with the reference target coordinate as the character\u2019s start (\ud835\udc650\ud835\udc5d , \ud835\udc660\ud835\udc5d , \ud835\udc670\ud835\udc5d ) or end (\ud835\udc65\ud835\udc47\u22121\ud835\udc5d , \ud835\udc66\ud835\udc47\u22121\ud835\udc5d , \ud835\udc67\ud835\udc47\u22121\ud835\udc5d ) position. Follow shot aims to follow the character\u2019s motion trajectory within the time duration \ud835\udc47 , and keeps looking-at the character. With an easing function\ud835\udc64\ud835\udf06 (\ud835\udc61) parameterized by \ud835\udf06 to control the movement rhythm, the camera position at time \ud835\udc61 \u2208 [0,\ud835\udc47 ) is determined by,\n(\ud835\udc65\ud835\udc61 , \ud835\udc66\ud835\udc61 , \ud835\udc67\ud835\udc61 ) = (\ud835\udc65\ud835\udc64\ud835\udf06 ( \ud835\udc61 \ud835\udc47 \u22121 ) \ud835\udc5d , \ud835\udc66 \ud835\udc64\ud835\udf06 ( \ud835\udc61\ud835\udc47 \u22121 ) \ud835\udc5d , \ud835\udc67 \ud835\udc64\ud835\udf06 ( \ud835\udc61\ud835\udc47 \u22121 ) \ud835\udc5d )\n+ (\ud835\udc5f cos\ud835\udf11 sin\ud835\udf03, \ud835\udc5f sin\ud835\udf11 sin\ud835\udf03, \ud835\udc5f cos\ud835\udf03 ), (4)\n\ud835\udc64\ud835\udf06 (\ud835\udc61 ) =  \ud835\udf06\ud835\udc61 \u2212 1 \ud835\udf06 \u2212 1 , \ud835\udf06 \u2208 (0, 1) \u222a (1,\u221e), \ud835\udc61, \ud835\udf06 = 1. (5)\nIn general, a large \ud835\udf06 makes the shot \u201cslow first, fast later\u201d and vice versa. The camera rotation parameters are determined by the Look-At constraint. Push/pull shot compresses/enlarges the shooting space to focus on a single object or show the surroundings. It adjusts the distance \ud835\udc5f between the camera and the subject with a zoom ratio parameter \ud835\udf07 and easing function\ud835\udc64\ud835\udf06 (\ud835\udc61) in Eq. (5),\n\ud835\udc5f\ud835\udc61 = ( (\ud835\udf07 \u2212 1)\ud835\udc64\ud835\udf06 (\ud835\udc61 ) + 1)\ud835\udc5f 0 . (6)\nZoom shot are similarly achieved by adjusting the camera focal length and its scale and angle are defined based on the first frame. Tilt shot and pan shot rotate the pitch angle \ud835\udefd or the yaw angle \ud835\udefe respectively to shift audience\u2019s focus from one to the other vertically or horizontally. Dolly (horizontal) and pedestal (vertical) shots are implemented by specifying a camera trajectory based on starting and ending points with the easing function, which is similar to push/pull. Arc shot is usually applied to a character staying at a specific location, where the camera moves around the subject rotating the azimuthal angle \ud835\udf11 in the direction vector.\nThe above definition constrain some variables in c depending on the shot angle, scale and movement types. Then we enumerate the unconstrained variables within their ranges (a subspace of 7DoF) to acquire camera trajectory proposals."
        },
        {
            "heading": "3.3 Shot Ranking Discriminator",
            "text": "For a given story and camera script, the subspaces defined above can produce multiple S \u00b7 C plausible shot proposals rendered by our simulator differing in their specific parameters, e.g., character paths and camera views. The next question is how to effectively evaluate these shots and select the optimal shots that look best in the combination of content, environment, and proposed camera\ntrajectory. Since there are no standard metrics for evaluating a good shot, and simple criteria such as smoothness are quite limited in telling the overall shot quality, we propose a data-driven shot ranking discriminator to score the quality of generated shots, such that users can easily select high-quality shots based on the scores. To acquire more capacity in discriminating the spatial-temporal structure among shot proposals, it is implemented with a TSN structure [Wang et al. 2018] that samples 8 images from a shot of size 224 \u00d7 224, and subsequently feeds each image to a ResNet50 and fuses extracted features in the end.\nWe train the binary classifier using the professional manualcreated clips as positive samples and the randomly generated virtual samples as negative ones, under the assumption that the professional manual-created clips represent higher quality. The network is trained with the loss,\nL\ud835\udc4f = \u2212\ud835\udc66 log(\ud835\udc5d\ud835\udc4f ) + (1 \u2212 \ud835\udc66) log(1 \u2212 \ud835\udc5d\ud835\udc4f ) . (7)\nDuring inference, the generated samples can be sorted by their classification scores for being categorized as \u201cprofessional\u201d. And our goal is to find the one with the highest score that fools the network into treating them as professional samples.\nNevertheless, we found that the network could only learn superficial appearance-level criteria to distinguish two classes, e.g., color/texture due to the shot type variance. The features of the generated samples to be ranked also stick to each other in the feature space, and it is less informative to distinguish their quality in terms of likeness towards high-quality shots. To overcome this problem, we design the following two objectives to facilitate the pick-up of high-quality shots. Class-aware contrastive objectives. Considering the variance among generated samples caused by their intrinsic shot styles, we add a class-aware loss to encourage it to be more class type specific and be expert in determining the corresponding shot type\u2019s quality:\nL\ud835\udc50 = \u2212 \ud835\udc40\u2211\ufe01 \ud835\udc50=1 \ud835\udc66 log(\ud835\udc5d\ud835\udc50 ), (8)\nwhere\ud835\udc40 is the total number of shot types in training data. To better select the high-quality generated shots, we need to magnify the feature difference among samples in different quality. Inspired by recent successes in video contrastive learning [Pan et al. 2021] that could learn high-level features for each shot, such as layout, temporal pace, which is helpful to determine the generated quality, we propose to include a contrastive objective,\nL\ud835\udc5e = \u2212 log exp (\ud835\udc67\ud835\udc5e\ud835\udc67+/\ud835\udf0f )\u2211\ud835\udc3e 1 exp (\ud835\udc67\ud835\udc5e\ud835\udc67\ud835\udc58/\ud835\udf0f ) . (9)\nThe loss is computed over one sample \ud835\udc67\ud835\udc5e and \ud835\udc3e other samples {\ud835\udc67\ud835\udc58 } in the training set. A large \ud835\udc3e means that more shots are taken into account when maximizing the difference among shot clips which leads to better performance. \ud835\udc67+ comes from different frames at other timestamps within the same shot as the target sample \ud835\udc67\ud835\udc5e .\nIn summary, at the training time, the shot ranking discriminator has been optimized with the following composite loss, with the aim of figuring out the high-quality ones from multiple shots: L = L\ud835\udc4f + L\ud835\udc50 + L\ud835\udc5e .We take 5, 000 professional manual-created clips as positive samples, and 5, 000 randomly generated shots as negative samples. For the positive samples, 9 professional designers select the VDS generated samples in high quality with double checked by each other. The negative samples come from a random perturbation in the camera action space. At the inference time, the shot ranking discriminator sorts S \u00b7 C proposals of each shot according to their classification scores, and the sample with the highest positive score \ud835\udc5d\ud835\udc4f will be chosen as the final output."
        },
        {
            "heading": "3.4 Practical User Interface",
            "text": "As a pre-production tool designed for the practical video production pipeline, Virtual Dynamic Storyboard is used to rehearse the plot and provide a guide in the downstream on-stage videography. To fulfill this goal, we resort to the advice from professional storyboard staff and present a two-stage user interface prototype including Environment Setting Stage and Filming Stage. In Fig. 5, we show a brief introduction of it with alphabets highlights of each panel. The key design idea is to divide the storyboard creation into static and dynamic stages, that allow users to apply clearer controls on the static scenes, dynamic characters and cameras.\nFollowing this idea, the first step is to prepare scene and character assets for the story and camera scripts in a static Environment Setting Stage. Choose Scene window (a) and Choose Characters window (b) allow user to select the characters and their initial locations in story. Users can freely use Scene Setting window (d) to control the viewing angle of the monitor camera to facilitate the operation of the scene and view the results through Monitor View window (c).\nWith the chosen assets, the system is ready to produce dynamic storyboard in the Filming Setting Stage. Input windows (e, f) provide users with text input to the system. Users can type story scripts and camera script to check the results in the Output window (g) of the corresponding story. To monitor the generated results timely, the keyframe of each shot can be found in the Preview window (i). Users are allowed to click the drag box and select more cases according to the ranking score. Additionally, in the Statistics window\n(h), we visualize some basic real-time statistics of the generated storyboards, e.g., the shot style counting, total shot number, etc."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 Implementation Details",
            "text": "The assets used in VDS contain over 20 different scenes, 100 actions and 20 characters. The character height \u210e in the simulation engine is set as 1.6\ud835\udc5a. Each story and camera script pair obtains 40\u223c200 proposals. For a fixed shot scale, we generate proposals with 8 different azimuthal angles to represent different shooting directions. For the easing function, we enumerate \ud835\udf06 \u2208 {0.1, 1, 10}. In push/zoom-in shots, \ud835\udf07 is set to 0.5 \u223c 0.8 and for pull/zoom-out shots, \ud835\udf07 is set to 1.0 \u223c 1.2 . In tilt and pan shots, the angle change is constrained within 30\u25e6\u223c60\u25e6, and we enumerate all the combinations of the camera moving directions (e.g. up/down), ending points (on/off the person). In arc shots, the azimuthal angle changes within 90\u25e6\u223c120\u25e6. We perform the inference of VDS on a laptop with an Intel i7 CPU and an NVIDIA 2080Ti GPU to output 720P videos and the processing time is shown in Tab. 1. The training process of shot ranking discriminator takes batch size 128, 1e\u22123 as the learning rate with 60 training epochs. To compute L\ud835\udc5e , we used the momentum updated dictionary [He et al. 2020] and set the dictionary size as 6553 corresponding to \ud835\udc3e . And L\ud835\udc50 is the sum of shot movement, scale, angle classification losses. More details can refer to the supplementary."
        },
        {
            "heading": "4.2 User Evaluation on Storyboard Designer",
            "text": "Setting. To evaluate the performance of VDS in practical usage, we invite 20 amateur designers to compare the usage of hand-paint and our VDS. 8 of them are skilled users who learn painting over two years while the rest 12 of them are beginners without painting skills. Before the experiments, they take a 30-minute training session to familiarize with the tool and read reference material showing some examples of the types of activities and characters. After the training, each of them first uses traditional hand-paint to create storyboards for 2 different practical stories. Each story depicts a scene containing \u223c10 shots. Then they utilize VDS to generate top 5 results for each shot and pick up one to create the dynamic storyboards on the same stories. In the process, we count\ntheir creation time to study the time efficiency, and ask them to pair-wised self-evaluate which approach can generate better results that reach their expectation. In the end, we ask them to vote which method holds higher flexibility in creation in Tab. 2. Results. Fig. 6 compares the storyboard generated by hand-paint and VDS with the same story and camera script in an indoor and an outdoor scene respectively. Beginners without painting skills have difficulties in expressing the story with the camera script, e.g., the character just stands in front of door in the 5-th row of Fig. 6 (a). The skilled users tend to use simple shapes and omit details in the backgrounds to design, e.g., the road is depicted by some simple lines in the 9-th row of Fig. 6 (b). For actions, hand-paint also tends to use some symbols, e.g., uses musical notes to indicate that the character is singing in the 12-th row of Fig. 6 (b). It is observed that hand-paint requires painting skills for designers and more time cost to create a better storyboard. Thanks to VDS\u2019s support for rich action and scenes, amateurs do not need painting skills to create a dynamic storyboard. And it also provides easier modification and more options for users as shown in Fig. 6 (d).\nTab. 2 shows that VDS provides a lower creation time cost of 3 minutes per shot to reach their expectation. And they vote for its high flexibility on camera settings. Compared to the hand-paint that can freely draw anything, VDS sacrifices some of its story flexibility. To have a deeper understanding of their choices, we conduct a semi-structured interview with them. The designers enjoyed the\nefficiency, performance and rich camera choices brought by VDS. Though the new tool sacrifices a bit of flexibility on story design, 11 out 20 of themmentioned that this is relevant to their painting skills under different scenarios. \u201cWhen I deal with common cases, e.g., a couple talking in a bedroom, hand-paint allows me to design freely. But if I want to design some things in a different style, e.g., the magic realism like The Lord of the Rings or Postmodernism like Westworld, the provided assets ease our design process.\u201d Based on the above observation, we believe that as the development of more assets, VDS will provide more flexibility when we put more available assets in it. And it also opens up opportunities for those people without painting skills to design their dynamic storyboards."
        },
        {
            "heading": "4.3 User Evaluation on Storyboard Reader",
            "text": "Setting. The most important functionality of storyboard is to convey the ideas of creators and guide the videography process. To validate this, we invite 22 amateurs as storyboard readers, which be splitted into 3 groups (8,10,4) according to the year they use storyboard in video production.\nFacing the storyboards created by the above designers in Sec. 4.2, we ask the readers whether they can understand the content in the storyboard and get the information for filming. To further understand which aspects play important roles, we conduct pair-wise rating on the elements of story and camera, where we set the handpaint as 4 and asks them to rate corresponding VDS results in seven-point Likert scale. Results. Tab. 3 shows that the results produced by VDS reach significantly better performance in content delivery and instruction ability, with above 70% for readers without much experience, and above 95% for people with \u223c2-year experience. Readers with more experiences appreciate its better incorporation and usefulness in the practical video production pipeline. As we dive deeper into the affecting components behind it, it is found that people prefer the story setting of VDS, i.e., character, action and scene. As mentioned above, hand-painted storyboards tend to be very simple and simplify the characters, actions, and scenes to a great extent. Only those designers who are skilled in painting can accurately convey their ideas with the hand-paint. In contrast, the dynamic storyboards generated with VDS make it easy for users to understand the information in the story. For camera movement, users prefer VDS too. Still images are unfriendly for users to restore the camera motion. Instead, VDS produces more intuitive results, and users can directly see how the frames proceed.\nTo have a deeper understanding of VDS and its comparison with conventional hand-paint, we conduct a semi-structured interview with participants and collect the reasons why they prefer or dislike VDS or hand-paint. They feel enthusiastic about the ability offered by VDS to speed up their creation procedure, and consider this tool a good substitute for the storyboard, which is more descriptive and vivid. Among the 22 storyboard users, 16 of them prefer VDS due to its dynamics which let the shot to be much easier to understand. 19 of the participants appreciate its standardized process that is similar to the real-world videography. Especially for the skilled users, they give the highest scores and praise the potential towards a more standardized pipeline for the whole video production.\nWe also receive feedback on the drawbacks and suggestions from them such as \u201c..., looking forward to adding more characters into the tool, ...\u201d, \u201c.., it would be better if it could support more characters facial expressions\u201d. These comments reveal insightful and exciting directions for future improvements to our system."
        },
        {
            "heading": "4.4 Ablation Study on Shot Ranking",
            "text": "To show the rationality of our shot ranking discriminator, we additionally collect 200 shots picked by professional designers from VDS proposals and score them with 20,000 generated proposals by the ranking network together. All designers\u2019 shots are ranked in the top 10% which shows that our shot ranking discriminator is able to pick up high-quality shots. We also demonstrate the effectiveness of the loss function for training the shot ranking discriminator by visualizing the top 3 shot candidates of fixed scripts (Fig. 7). The top 3 results from Ours w/o L\ud835\udc50 + L\ud835\udc5e look similar and their shot types are not accurate. e.g., the shot scale of Anna does not perfectly match the medium type. With the help of class-aware contrastive objectives, the full model is able to display more accurate shot types matched with the input scripts, and the top 3 results provide enough diversity for users to pick up. This is matched with our expectation, since the ablated model without the presence of L\ud835\udc50 + L\ud835\udc5e has difficulty in learning a class-aware distinguishable feature space for shot proposals."
        },
        {
            "heading": "5 DISCUSSION AND CONCLUSION",
            "text": "Virtual Dynamic Storyboard (VDS) is a semi-auto storyboard creation tool performing on virtual environments. It takes user-specified\nstory script and camera script as inputs and proposes multiple plausible character and camera trajectories that can be rendered to be dynamic shots. A shot ranking discriminator then sorts these generated shots with class-aware contrastive objectives to output the top results. Experiments show that our tool can effectively compose dynamic storyboards and assist amateurs in their creation. It also bears the following limitations. Quality of assets. The quality of VDS\u2019s results heavily depends on the quality of assets, i.e., character model, action animation and virtual environment. For example, VDS can not generate rich expressions for a character if this character asset itself lacks detailed modeling on the face. As our design is easy to extend with different assets, we will continue to improve the quality of VDS with more advanced 3D assets including characters, associated actions, prefabs and scenes. There are also opportunities to incorporate character/motion synthesis [Hong et al. 2022; Wang et al. 2021]. Trade-off of flexibility and efficiency on control. VDS provides a relatively flexible and fast way to generate dynamic storyboards for pre-production, but it does not yet allow users to control everything as they do with a pen. Our current version of VDS addresses the most important steps in setting a shot, i.e., the story plot and the shot cinematic styles. For different user groups, the interaction portal for control needs to be adjusted to meet their requirements and preferences. It is promising to provide more detailed control on character and camera for professionals, e.g., details of characters, lens aperture, though it would trigger more creation time cost. And for novice users, it would be more friendly to provide more fuzzy input control."
        }
    ],
    "title": "Dynamic Storyboard Generation in an Engine-based Virtual Environment for Video Production",
    "year": 2023
}