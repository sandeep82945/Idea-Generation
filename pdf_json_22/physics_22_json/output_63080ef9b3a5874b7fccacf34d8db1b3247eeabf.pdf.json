{
    "abstractText": "Machine-learned regression models represent a promising tool to implement accurate and computationally affordable energy-density functionals to solve quantum many-body problems via density functional theory. However, while they can easily be trained to accurately map ground-state density profiles to the corresponding energies, their functional derivatives often turn out to be too noisy, leading to instabilities in self-consistent iterations and in gradient-based searches of the ground-state density profile. We investigate how these instabilities occur when standard deep neural networks are adopted as regression models, and we show how to avoid them by using an ad-hoc convolutional architecture featuring an inter-channel averaging layer. The main testbed we consider is a realistic model for noninteracting atoms in optical speckle disorder. With the inter-channel average, accurate and systematically improvable ground-state energies and density profiles are obtained via gradient-descent optimization, without instabilities nor violations of the variational principle.",
    "authors": [
        {
            "affiliations": [],
            "name": "E. Costa"
        },
        {
            "affiliations": [],
            "name": "G. Scriva"
        },
        {
            "affiliations": [],
            "name": "R. Fazio"
        },
        {
            "affiliations": [],
            "name": "S. Pilati"
        }
    ],
    "id": "SP:d83c6384f0628a974879c1158f947ac7735191b5",
    "references": [
        {
            "authors": [
                "K. Burke"
            ],
            "title": "Perspective on density functional theory",
            "venue": "J. Chem. Phys. 136, 150901 ",
            "year": 2012
        },
        {
            "authors": [
                "P. Hohenberg",
                "W. Kohn"
            ],
            "title": "Inhomogeneous electron gas",
            "venue": "Phys. Rev. 136, B864 ",
            "year": 1964
        },
        {
            "authors": [
                "M. Levy"
            ],
            "title": "Universal variational functionals of electron densities",
            "venue": "first-order density matrices, and natural spinorbitals and solution of the v-representability problem, Proc. Natl. Acad. Sci. U.S.A. 76, 6062 ",
            "year": 1979
        },
        {
            "authors": [
                "E. Lieb"
            ],
            "title": "Density functionals for Coulomb systems",
            "venue": "International Journal of Quantum Chemistry 24, 243 ",
            "year": 1983
        },
        {
            "authors": [
                "A.J. Cohen",
                "P. Mori-S\u00e1nchez",
                "W. Yang"
            ],
            "title": "Challenges for density functional theory",
            "venue": "Chem. Rev. 112, 289 ",
            "year": 2012
        },
        {
            "authors": [
                "V. Dunjko",
                "H.J. Briegel"
            ],
            "title": "Machine learning & artificial intelligence in the quantum domain: A review of recent progress",
            "venue": "Rep. Prog. Phys. 81, 074001 ",
            "year": 2018
        },
        {
            "authors": [
                "G. Carleo",
                "I. Cirac",
                "K. Cranmer",
                "L. Daudet",
                "M. Schuld",
                "N. Tishby",
                "L. Vogt-Maranto",
                "L. Zdeborov\u00e1"
            ],
            "title": "Machine learning and the physical sciences",
            "venue": "Rev. Mod. Phys. 91, 045002 ",
            "year": 2019
        },
        {
            "authors": [
                "J. Carrasquilla"
            ],
            "title": "Machine learning for quantum matter",
            "venue": "Adv. Phys.-X 5, 1797528 ",
            "year": 2020
        },
        {
            "authors": [
                "H.J. Kulik",
                "T. Hammerschmidt",
                "J. Schmidt",
                "S. Botti",
                "M.A.L. Marques",
                "M. Boley",
                "M. Scheffler",
                "M. Todorovi\u0107",
                "P. Rinke",
                "C. Oses",
                "A. Smolyanyuk",
                "S. Curtarolo",
                "A. Tkatchenko",
                "A.P. Bart\u00f3k",
                "S. Manzhos",
                "M. Ihara",
                "T. Carrington",
                "J. Behler",
                "O. Isayev",
                "M. Veit",
                "A. Grisafi",
                "J. Nigam",
                "M. Ceriotti",
                "K.T. Sch\u00fctt",
                "J. Westermayr",
                "M. Gastegger",
                "R.J. Maurer",
                "B. Kalita",
                "K. Burke",
                "R. Nagai",
                "R. Akashi",
                "O. Sugino",
                "J. Hermann",
                "F. No\u00e9",
                "S. Pilati",
                "C. Draxl",
                "M. Kuban",
                "S. Rigamonti",
                "M. Scheidgen",
                "M. Esters",
                "D. Hicks",
                "C. Toher",
                "P.V. Balachandran",
                "I. Tamblyn",
                "S. Whitelam",
                "C. Bellinger",
                "L.M. Ghiringhelli"
            ],
            "title": "Roadmap on machine learning in electronic structure",
            "venue": "Electronic Structure 4, 023004 ",
            "year": 2022
        },
        {
            "authors": [
                "J.C. Snyder",
                "M. Rupp",
                "K. Hansen",
                "K.-R. M\u00fcller",
                "K. Burke"
            ],
            "title": "Finding density functionals with machine learning",
            "venue": "Phys. Rev. Lett. 108, 253002 ",
            "year": 2012
        },
        {
            "authors": [
                "L. Li",
                "J.C. Snyder",
                "I.M. Pelaschier"
            ],
            "title": "J",
            "venue": "Huang, U.- N. Niranjan, P. Duncan, M. Rupp, K.-R. M\u00fcller, and K. Burke, Understanding machine-learned density functionals, Int. J. Quantum Chem. 116, 819 ",
            "year": 2016
        },
        {
            "authors": [
                "F. Brockherde",
                "L. Vogt",
                "L. Li",
                "M.E. Tuckerman",
                "K. Burke",
                "K.-R. M\u00fcller"
            ],
            "title": "Bypassing the Kohn-Sham equations with machine learning",
            "venue": "Nat. Commun. 8, 872 ",
            "year": 2017
        },
        {
            "authors": [
                "K. Ryczko",
                "D.A. Strubbe",
                "I. Tamblyn"
            ],
            "title": "Deep learning and density-functional theory",
            "venue": "Phys. Rev. A 100, 022512 ",
            "year": 2019
        },
        {
            "authors": [
                "J. Kirkpatrick",
                "B. McMorrow",
                "D.H.P. Turban",
                "A.L. Gaunt",
                "J.S. Spencer",
                "A.G.D.G. Matthews",
                "A. Obika",
                "L. Thiry",
                "M. Fortunato",
                "D. Pfau",
                "L.R. Castellanos",
                "S. Petersen",
                "A.W.R. Nelson",
                "P. Kohli",
                "P. Mori-S\u00e1nchez",
                "D. Hassabis",
                "A.J. Cohen"
            ],
            "title": "Pushing the frontiers of density functionals by solving the fractional electron problem",
            "venue": "Science 374, 1385 ",
            "year": 2021
        },
        {
            "authors": [
                "J. Nelson",
                "R. Tiwari",
                "S. Sanvito"
            ],
            "title": "Machine learning density functional theory for the Hubbard model",
            "venue": "Phys. Rev. B 99, 075132 ",
            "year": 2019
        },
        {
            "authors": [
                "J.R. Moreno",
                "G. Carleo",
                "A. Georges"
            ],
            "title": "Deep learning the hohenberg-kohn maps of density functional theory",
            "venue": "Phys. Rev. Lett. 125, 076402 ",
            "year": 2020
        },
        {
            "authors": [
                "M.M. Denner",
                "M.H. Fischer",
                "T. Neupert"
            ],
            "title": "Efficient learning of a one-dimensional density functional theory",
            "venue": "Phys. Rev. Res. 2, 033388 ",
            "year": 2020
        },
        {
            "authors": [
                "R. Meyer",
                "M. Weichselbaum",
                "A.W. Hauser"
            ],
            "title": "Machine learning approaches toward orbital-free density functional theory: Simultaneous training on the kinetic energy density functional and its functional derivative",
            "venue": "J. Chem. Theory Comput. 16, 5685 ",
            "year": 2020
        },
        {
            "authors": [
                "K. Ryczko",
                "S.J. Wetzel",
                "R.G. Melko",
                "I. Tamblyn"
            ],
            "title": "Toward orbital-free density functional theory with small data sets and deep learning",
            "venue": "J. Chem. Theory Comput. 18, 1122\u20131128 ",
            "year": 2022
        },
        {
            "authors": [
                "J.C. Snyder",
                "M. Rupp",
                "K.-R. M\u00fcller",
                "K. Burke"
            ],
            "title": "Nonlinear gradient denoising: Finding accurate extrema from inaccurate functional derivatives",
            "venue": "Int. J. Quantum Chem. 115, 1102 ",
            "year": 2015
        },
        {
            "authors": [
                "L. Li",
                "T.E. Baker",
                "S.R. White",
                "K. Burke"
            ],
            "title": "Pure density functional for strong correlation and the thermodynamic limit from machine learning",
            "venue": "Phys. Rev. B 94, 245129 ",
            "year": 2016
        },
        {
            "authors": [
                "A. Paszke",
                "S. Gross",
                "S. Chintala",
                "G. Chanan",
                "E. Yang",
                "Z. DeVito",
                "Z. Lin",
                "A. Desmaison",
                "L. Antiga",
                "A. Lerer"
            ],
            "title": "11 Automatic differentiation in pytorch",
            "venue": "NIPS-W ",
            "year": 2017
        },
        {
            "authors": [
                "W.C. Witt"
            ],
            "title": "B",
            "venue": "G. del Rio, J. M. Dieterich, and E. A. Carter, Orbital-free density functional theory for materials research, J. Mater. Res. 33, 777\u2013795 ",
            "year": 2018
        },
        {
            "authors": [
                "J.W. Goodman"
            ],
            "title": "Speckle phenomena in optics: theory and applications",
            "venue": "(Roberts and Company Publishers,",
            "year": 2007
        },
        {
            "authors": [
                "J. Billy",
                "V. Josse",
                "Z. Zuo",
                "A. Bernard",
                "B. Hambrecht",
                "P. Lugan",
                "D. Cl\u00e9ment",
                "L. Sanchez-Palencia",
                "P. Bouyer",
                "A. Aspect"
            ],
            "title": "Direct observation of Anderson localization of matter waves in a controlled disorder",
            "venue": "Nature 453, 891 ",
            "year": 2008
        },
        {
            "authors": [
                "G. Roati"
            ],
            "title": "C",
            "venue": "D\u2019Errico, L. Fallani, M. Fattori, C. Fort, M. Zaccanti, G. Modugno, M. Modugno, and M. Inguscio, Anderson localization of a non-interacting Bose\u2013 Einstein condensate, Nature 453, 895 ",
            "year": 2008
        },
        {
            "authors": [
                "T. Prat",
                "N. Cherroret",
                "D. Delande"
            ],
            "title": "Semiclassical spectral function and density of states in speckle potentials",
            "venue": "Phys. Rev. A 94, 022114 ",
            "year": 2016
        },
        {
            "authors": [
                "S. Pilati",
                "P. Pieri"
            ],
            "title": "Supervised machine learning of ultracold atoms with speckle disorder",
            "venue": "Sci. Rep. 9, 1 ",
            "year": 2019
        },
        {
            "authors": [
                "P. Mujal",
                "A. Polls",
                "S. Pilati",
                "B. Juli\u00e1-D\u00edaz"
            ],
            "title": "Few-boson localization in a continuum with speckle disorder",
            "venue": "Phys. Rev. A 100, 013603 ",
            "year": 2019
        },
        {
            "authors": [
                "E. Abrahams",
                "P.W. Anderson",
                "D.C. Licciardello",
                "T.V. Ramakrishnan"
            ],
            "title": "Scaling theory of localization: Absence of quantum diffusion in two dimensions",
            "venue": "Phys. Rev. Lett. 42, 673 ",
            "year": 1979
        },
        {
            "authors": [
                "P.W. Anderson"
            ],
            "title": "Absence of diffusion in certain random lattices",
            "venue": "Phys. Rev. 109, 1492 ",
            "year": 1958
        },
        {
            "authors": [
                "J. Huntley"
            ],
            "title": "Speckle photography fringe analysis: assessment of current algorithms",
            "venue": "Appl. Opt. 28, 4316 ",
            "year": 1989
        },
        {
            "authors": [
                "M. Modugno"
            ],
            "title": "Collective dynamics and expansion of a bose-einstein condensate in a random potential",
            "venue": "Phys. Rev. A 73, 013606 ",
            "year": 2006
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv:1412.6980 ",
            "year": 2014
        },
        {
            "authors": [
                "E. Costa",
                "G. Scriva",
                "S. Pilati"
            ],
            "title": "and R",
            "venue": "Fazio, Data for Deep learning density functionals for gradient descent optimization ",
            "year": 2022
        },
        {
            "authors": [
                "R. Pederson",
                "B. Kalita",
                "K. Burke"
            ],
            "title": "Machine learning and density functional theory",
            "venue": "arXiv:2205.01591 ",
            "year": 2022
        },
        {
            "authors": [
                "K. Ryczko",
                "J.T. Krogel",
                "I. Tamblyn"
            ],
            "title": "Machine learning diffusion monte carlo energy densities",
            "venue": "arXiv:2205.04547 ",
            "year": 2022
        },
        {
            "authors": [
                "R. Nagai",
                "R. Akashi",
                "O. Sugino"
            ],
            "title": "Completing density functional theory by machine learning hidden messages from molecules",
            "venue": "Npj Comput. Mater. 6, 1 ",
            "year": 2020
        },
        {
            "authors": [
                "L. Li",
                "S. Hoyer",
                "R. Pederson",
                "R. Sun",
                "E.D. Cubuk",
                "P. Riley",
                "K. Burke"
            ],
            "title": "Kohn-sham equations as regularizer: Building prior knowledge into machine-learned physics",
            "venue": "Phys. Rev. Lett. 126, 036401 ",
            "year": 2021
        },
        {
            "authors": [
                "K. Mills",
                "K. Ryczko",
                "I. Luchak",
                "A. Domurad",
                "C. Beeler",
                "I. Tamblyn"
            ],
            "title": "Extensive deep neural networks for transferring small scale learning to large scale systems",
            "venue": "Chem. Sci. 10, 4129 ",
            "year": 2019
        },
        {
            "authors": [
                "N. Saraceni",
                "S. Cantori",
                "S. Pilati"
            ],
            "title": "Scalable neural networks for the efficient learning of disordered quantum systems",
            "venue": "Phys. Rev. E 102, 033301 ",
            "year": 2020
        },
        {
            "authors": [
                "P. Mujal"
            ],
            "title": "\u00c0lex Mart\u00ednez Miguel",
            "venue": "A. Polls, B. Juli\u00e1- D\u00edaz, and S. Pilati, Supervised learning of few dirty bosons with variable particle number, SciPost Phys. 10, 73 ",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Deep learning density functionals for gradient descent optimization\nE. Costa,1, 2 G. Scriva,1, 2 R. Fazio,3, 4 and S. Pilati1, 2 1School of Science and Technology, Physics Division, Universit\u00e0 di Camerino, 62032 Camerino, Italy\n2INFN-Sezione di Perugia, 06123 Perugia, Italy 3Abdus Salam ICTP, Strada Costiera 11, I-34151 Trieste, Italy\n4Dipartimento di Fisica, Universit\u00e0 di Napoli \u201cFederico II\u201d, Monte S. Angelo, I-80126 Napoli, Italy\nMachine-learned regression models represent a promising tool to implement accurate and computationally affordable energy-density functionals to solve quantum many-body problems via density functional theory. However, while they can easily be trained to accurately map ground-state density profiles to the corresponding energies, their functional derivatives often turn out to be too noisy, leading to instabilities in self-consistent iterations and in gradient-based searches of the ground-state density profile. We investigate how these instabilities occur when standard deep neural networks are adopted as regression models, and we show how to avoid them by using an ad-hoc convolutional architecture featuring an inter-channel averaging layer. The main testbed we consider is a realistic model for noninteracting atoms in optical speckle disorder. With the inter-channel average, accurate and systematically improvable ground-state energies and density profiles are obtained via gradient-descent optimization, without instabilities nor violations of the variational principle.\nI. INTRODUCTION\nDensity functional theory (DFT) is the workhorse of computational material science and quantum chemistry [1]. It is based on rigorous theorems [2\u20134] certifying that the ground-state energy can be computed minimizing a (generally unknown) functional of the density profile, allowing bypassing computationally prohibitive wave-function based methods. However, the available approximations for the density functional are reliable only for weakly correlated materials, while in the regime of strong electron correlations dramatic failures may occur [5]. In recent years, machine-learning (ML) algorithms have reached remarkable breakthroughs in various branches of physics research [6\u20139], and they have also been adopted in the framework of DFT, both for continuous-space [10\u201314] and for one-dimensional tightbinding models [15\u201317]. These algorithms pave the way to data based approaches to the development of density functionals. Furthermore, they facilitate the implementation of computationally convenient strategies based on orbital-free DFT [18, 19]. Most previous studies adopted relatively simple regression models, such as, e.g., kernel ridge regression, showing that moderately large training sets of ground-state density profiles and corresponding energies allow reconstructing remarkably accurate density functionals. Also artificial neural networks have been adopted (see, e.g., [13, 19]), considering standard architectures such as the convolutional neural networks (CNNs) popular in the field of computer vision. Unfortunately, in the case of continuous-space models, severe drawbacks have emerged when such ML functionals have been employed in self-consistent calculations and in gradient-based optimizations. Specifically, the functional derivatives turned out to be too noisy, leading to unphysical density profiles and to strong violation of the variational principle [10, 19, 20]. Some remedial strategies have already been explored. Essentially, they resort to gradient denoising via dimensionality reduction [20]\nor basis truncation [21], to constrained optimization, or they aim at exploiting additional information (e.g., energy derivatives) in the training process [14, 19]. These strategies have provided significant benefits, but they have some limitations, as they might lead to variational biases or they require additional data which is far less accessible. Due to the pivotal role played by DFT, further complementary strategies are highly desirable.\nIn this Article, we investigate the use of deep neural networks as regression models to reconstruct continuousspace density functionals from training data. Our main finding is that a tailored convolutional network featuring an inter-channel averaging operation allows avoiding the drawbacks mentioned above. Following analogous previous studies [10, 11, 18\u201320], the testbed we consider is a single particle model, but we mostly focus on a more realistic Hamiltonian which describes ultracold atoms moving in one-dimensional optical speckle patterns. Our analysis is complemented by addressing deepwell models from the literature (see Appendix). Our aim is to develop a sufficiently effective deep-learned functional to allow searching the ground-state energy and the density profile of previously unseen instances of speckle disorder via gradient-descent optimization. We show that the most popular network architectures, namely, the standard CNNs, are inadequate for this task. Indeed, while they provide remarkably accurate energy predictions when fed with exact ground-state density profiles, their functional derivatives are too noisy. This leads to instabilities in the gradient-descent search of the density profile which minimizes the energy functional, unless the accuracy is jeopardized via an early halting of the optimization procedure. We demonstrate that these instabilities can be avoided with the tailored neural network. This is inspired by an ensemble-averaging mechanism, and it features, beyond the standard multi-channel convolutional layers, additional layers that perform an interchannel averaging operation. We show that this feature allows us iterating gradient-descent steps at will, providar X iv :2 20 5. 08 36 7v 2 [ ph ys ic s. co\nm p-\nph ]\n7 N\nov 2\n02 2\n2 ing accurate results that can be systematically improved by increasing the number of channels.\nThe rest of the article is organized as follows: in Section II we describe the formalism of DFT based on deep learning, the structure of the standard and of the averagechannel neural networks, as well as the gradient-descent technique used to find ground-state energies and densities. The main testbed model we address is described in Section III. Therein we also report details on the dataset, on the protocol used for network training, and on the accuracy reached in the regression task. The results obtained in gradient-descent optimization with the standard and with the average-channel neural networks are compared in Section IV. The instabilities occurring with standard networks are highlighted, and their suppression with the inclusion of the average layer is discussed in detail. Section V provides a summary of the main findings and some comments on future perspectives. To favor comparison with previous studies [10, 18], in the Appendix A we report the test of the average-channel neural network in deep potential wells defined by the sum of three Gaussian functions."
        },
        {
            "heading": "II. DENSITY FUNCTION THEORY WITH ARTIFICIAL NEURAL NETWORKS",
            "text": "ML provides novel promising approaches to learn energy-density functionals for DFT from data. These functionals have the potential to accurately describe strongly correlated systems. However, their variational minimization to search for the ground-state density profile turned out to be problematic due to noisy functional derivatives [20], [11], [10]. This problem is already evident in noninteracting systems. Henceforth, in the following we focus on single-particle problems, but the technique we develop can be applied to interacting systems via the creation of suitable training sets.\nIn this article we consider one-dimensional singleparticle Hamiltonians written in the form:\nH = \u2212 ~ 2\n2m\nd2\ndx2 + V (x), (1)\nwhere ~ is the reduced Planck constant and m is the particle mass. The external potential V (x) is compatible with the adopted periodic boundary conditions. In the framework of DFT, one aims at computing the groundstate energy egs of the Hamiltonian (1) from a functional of the density profile: egs = E[ngs]. Here, ngs indicates the density profile\nngs(x) = |\u03c8gs(x)|2 , (2)\nwhere \u03c8gs(x) is the ground-state wave function. The first Hohenberg-Kohn theorem guarantees that, in principle, this functional exists [2]. In practice, it is convenient to separate the known potential energy contribution, seek-\ning for a functional for the kinetic energy only[22]:\ntgs = egs \u2212 \u222b L 0 dxngs(x)V (x) \u2261 T [ngs]. (3)\nIt is worth pointing out that we do not adopt the KohnSham formalism. As in previous studies on ML-based DFT [18, 19], [20], [11], [10], the orbital-free formalism is used, attempting to approximate the kinetic energy (eventually together with energy terms in interacting systems) as a functional of the density. If successful, this attempt would therefore also lead to a significant reduction of computational cost compared to the more demanding Kohn-Sham approach.\nDeep-learning techniques can be adopted in the DFT framework. The first task is to train a deep neural network to map ground-state density profiles to the corresponding kinetic energies, therefore learning the unknown functional T [ngs]. This can be achieved via supervised learning from a dataset including many instances of density profiles associated to the corresponding kinetic energies, {ngs,k, tgs,k}. The integer k labels the instances in the dataset. The parameters of the neural network, collectively denoted with \u03c9, are optimized by minimizing the loss function, namely, the mean squared error\nL(\u03c9) = 1 Ntrain Ntrain\u2211 k=1 \u2223\u2223\u2223tgs,k \u2212 T\u0303gs,k(\u03c9)[ngs,k]\u2223\u2223\u22232 , (4) where T\u0303gs,k(\u03c9)[ngs,k] denotes the kinetic energy predicted by the neural network and Ntrain is the number of instances in the training set. This optimization can be performed using the stochastic gradient descent algorithms or one of its many successful variants."
        },
        {
            "heading": "A. Neural networks",
            "text": "The first regression model we consider is a standard CNN. Its structure is familiar from many fields where deep learning has proven successful such as, e.g, image recognition. It is composed of Nb = 3 convolutional blocks. Each block includes a convolutional layer with Nc channels (this hyperparameter is specified in Table I), whose filter size is kf = 13 and padding type is periodic, and an average pooling layer with a kernel size ks = 2. Two variants of this network are considered, using two popular activation functions, namely the ReLU function, defined as\nReLU(x) = { x if x > 0 0 otherwise , (5)\nand the Softplus function\nSoftplus(x) = ln (1 + exp(x)). (6)\nThe last convolutional block is processed through a flattening layer and then connected to a dense layer with\n3 only one neuron (identity activation function) to generate a scalar output. As discussed in detail in Section IV, these standard CNNs turn out to be inadequate for the DFT framework. In particular, their functional derivatives are too noisy to perform a gradient-based search of the ground state. Therefore, we introduce a novel tailored architecture which features an inter-channel average operation. In the following, this network will be referred to as average-channel CNN (CNN). Specifically, this model is composed of two convolutional blocks, each including Nc convolutional channels (kf = 13 and periodic padding), an average pooling layer (ks = 4 in the first block and ks = 2 in the second block), and, notably, an additional layer, where each neuron computes the average of the activations of the corresponding neurons in all channels of the previous layer. The activation function is ReLU. The last convolutional block passes a flatten layer and is then connected to one dense layer with only one neuron, as in the standard CNN case. It is worth pointing out that this average operation reduces the scaling of the number of parameters from quadratic to linear in Nc. This allows considering architectures with many channels without facing prohibitive computational costs nor overfitting problems. In each architecture adopted in this article, all convolutional blocks feature the same number of channels. Explorations performed with different numbers lead to similar findings, so we do not discuss them to avoid burdening the presentation. Hereafter, we describe the operations performed by the convolutional blocks more formally.\nIn a standard CNN, the action of the n-th convolutional block corresponds to the following convolution operation:\nh\u03b1n(x) = 1\nks \u222b x+ks/2 x\u2212ks/2 dy\nact \u2211 \u03b2 \u222b y+kf/2 y\u2212kf/2 dx\u2032 W\u03b1\u03b2n (y, x \u2032)h\u03b2n\u22121(x \u2032) + v\u03b1n , (7) where act is the chosen activation function, the matrices W\u03b1\u03b2n represent, for each filter \u03b1 and input channel \u03b2, a kernel of size kf , and v\u03b1 are the set of biases for each filter. Instead, in the CNN, the n-th block has an additional inter-channel average operation, which is expressed as:\nh\u0304n(x) = 1\nNc \u2211 \u03b1 h\u03b1n(x), (8)\nwhere\nh\u03b1n(x) = 1\nks \u222b x+ks/2 x\u2212ks/2 dy\nact [\u222b y+kf/2 y\u2212kf/2 dx\u2032 W\u03b1n (y, x \u2032)h\u0304n\u22121(x \u2032) + v\u03b1n ] (9) represents the previous Nc parallel convolution operations. Notice that in Eqs. (7) and (8) integrals actually\nindicate discrete operations. All the neural networks considered in this work are implemented and trained using the Pytorch library, exploiting automatic differentiation to compute discrete functional derivatives [23]."
        },
        {
            "heading": "B. Formalism of gradient-descent optimization",
            "text": "Once the kinetic energy functional has been learned by the neural network, and consequently we assume T [n] \u2261 T\u0303\u03c9[n], both the ground-state energy and the density corresponding to a new instance of the Hamiltonian can be obtained from the variational principle. Indeed, the second Hohenberg-Kohn theorem ensures that the (exact) functional is minimized by the ground-state density profile [2]. This can be expressed using the Euler equation:\n\u03b4T [n] \u03b4n(x) + V (x)\u2212 \u00b5 = 0 (10)\nwhere \u00b5 is a normalization constraint. Its solution can be efficiently obtained using the gradient-descent algorithm, as usually done in orbital-free DFT [24]. Specifically, one iterates the following update rule:\nnt+1(x) = nt(x)\u2212 \u03b7 ( \u03b4T [nt]\n\u03b4n(x) + V (x)\u2212 \u00b5t\n) , (11)\nstarting from a reasonably chosen initial profile n0(x). In this equation, \u03b7 > 0 is the chosen learning rate, the integer t = 0, 1, 2, . . . , tmax labels the steps, and the adaptive coefficient \u00b5t is introduced to ensure the normalization condition: \u222b\ndxnt(x) = 1. (12)\nTo ensure the density never becomes negative, it is convenient to perform a variable change in Eq. (11): \u03c7 =\u221a n(x). This leads to the update rule:\n\u03c7t+1(x) = \u03c7t(x)\u2212\u03b7 ( \u03b4T [\u03c72t ]\n\u03b4\u03c7(x) + 2\u03c7t(x)V (x)\u2212 2\u03c7t(x)\u00b5t\n) ,\n(13) where the normalization coefficient is computed as:\n\u00b5t =\n\u222b dx ( 1\n2\n\u03b4T [\u03c72t ] \u03b4\u03c7t(x) \u03c7t(x) + \u03c7 2 t (x)V (x) ) \u222b\ndx \u03c72t (x)\n. (14)\nThe computation of the square of the function \u03c7 is performed by an additional layer in Pytorch. Henceforth, the functional derivative with respect to \u03c7 can be directly computed exploiting automatic differentiation.\nWhether the gradient descent algorithm reaches the ground-state or not depends on two major issues. First, the optimization might get stuck in a local minimum. Indeed, the optimization landscape is not proven to be convex, even for the (unknown) exact functional. Convexity\n4 can be instead proven for an extended functional, defined in a domain including density profiles not corresponding to ground states [4]. This problem can be mitigated by repeated the minimization process starting from different initial profiles, or by introducing random steps based on, e.g., Metropolis-type algorithms. The second issue is the accuracy of the functional derivative. Noisy and inaccurate derivatives might create unphysical density profiles, clearly not corresponding to ground states. Since the regression model was not trained on such profiles, it might provide very inaccurate energy predictions, even lower than the exact ground-state energy egs. This leads to dramatic failures of the gradient descent optimization, even to large violations of the variational principle. This problem has already been emphasized in the literature, and it was indicated as a major challenge to be overcome for the further development of ML based DFTs. In Refs. [20], [10] [11], the adopted regression model was kernel ridge regression. This model typically requires smaller datasets for training, but it is less efficient than the deep neural networks in systematically extracting further information from larger and larger datasets. Even kernel ridge regression led to noisy derivatives. To circumvent this problem, the authors introduced two main techniques, referred to as local principal component analysis and non linear gradient denoising. They aim at projecting the functional derivative to the manifold tangent to the one spanned by ground-state density profiles. Other works included derivative data in the training process \u2013 also adopting standard CNNs \u2014 using the Sobolev Loss [18, 19]. We emphasize that our goal is to train the regression model using only ground-state density profiles and the corresponding energies, avoiding resorting to less accessible data such as exited-state properties or energy gradients."
        },
        {
            "heading": "III. TESTBED MODEL AND TRAINING DATASET",
            "text": "The main testbed model we consider is the singleparticle model (1), where the (random) external potential V (x) is designed to represent the effect of optical speckle patterns on ultracold atoms. Notice that another\ntestbed, borrowed from the literature, is considered in the Appendix A, allowing us to further characterize the domain of applicability of the CNN. The speckle potentials can be created by applying a specific filter in Fourier space to a random complex gaussian field [25]. The filter corresponds to the aperture of the optical apparatus used to experimentally create the field, and it fixes the characteristic size of the speckle grains. In fact, with this choice the Hamiltonian (1) describes early coldatom experiments on Anderson localization in one dimension [26, 27]. The statistical and the spectral properties of optical speckle patterns are known [25, 28, 29]. The intensity of the potential V in a point x follows the probability distribution\nP (V ) = exp ( \u2212 V V0 ) , (15)\nfor V \u2265 0, and P (V ) = 0 otherwise; V0 \u2265 0 is the average intensity, and it also coincides with the standard deviation. It is the unique parameter determining the disorder strength. The two-point autocorrelation function satisfies the following equation:\n\u3008V (x\u2032 + x)V (x\u2032)\u3009 V 20 \u2212 1 = sin(\u03c0x/\u03b3) 2 (\u03c0x/\u03b3)2 , (16)\nwhere \u03b3 determines the correlation length, namely, the size of the typical speckle grains. In the above equation, the brackets \u3008\u00b7\u3009 indicate the average over many random realizations of the speckle pattern. This ensemble average coincides with the spatial average for sufficiently large systems. The correlation energy Ec = ~ 2\n2m\u03b3 separates the strong disorder regime V0 Ec, where the low-energy orbitals are localized on a length scale of order \u03b3 [30] due to the Anderson localization phenomenon [31], from the weak disorder regime V0 Ec, where their localization length is much larger. Notice that in one dimension any disorder strength induces Anderson localization in a sufficiently large system [32]. In the following, we consider the relatively large system size L = 14\u03b3 and the intermediate disorder strength V0 = 0.5Ec. This choice allows generating rather variegate ground-state density profiles with different shapes and varying degrees of localization, depending on the details of the specific realization of the speckle pattern. Therefore, the Hamiltonian (1) represents a stringent testbed for the DFT framework. Two representative instances of the speckle pattern are shown in Fig. 1, together with the corresponding ground-state density profiles.\nDifferent random realizations of the speckle pattern can be efficiently generated on a discrete grid with the algorithm described in Refs. [33, 34]. We choose a fine grid with Ng = 256 points, such that the grid step \u03b4x = L/Ng \u03b3. The ground-state energy egs and the corresponding orbital \u03c8gs(x) are determined via exact diagonalization using a high-order finite difference formula. Computations performed with finer grids show that the discretization error is negligible. The choice of such a fine\n5 grid allows us computing all spatial integrals (see, e.g, those in Eqs. (3) and (14)) with the discrete approximation \u222b L 0 dx \u2212\u2192 \u03b4x \u2211Ng i=1. Higher order approximations lead to essentially indistinguishable results for our purposes. Furthermore, the functional derivative in Eq. (13) is computed as\n\u03b4T [\u03c8] \u03b4\u03c8(x) = \u2202T [\u03c8]\n\u2202\u03c8(xi)\n1\n\u03b4x . (17)\nFor this, we exploit Pytorch automatic differentiation. The training of the neural networks, namely, the minimization of the loss function Eq. (4), is performed using the Adam algorithm [35]. The chosen learning rate is lr = 10\n\u22124, the minibatch size is Nb = 100 and the training epochs are Ne = 1200. The other parameters of the Adam algorithm are set at their suggested default values. Our global dataset is composed of 150000 instances. As customary in deep-learning studies, we split it into a training set (81%), a validation set (9%), and a test set (10%). To measure the accuracy in the regression task, we consider the coefficient of determination, defined as:\nR2 = 1\u2212\n\u2211Ntest k=1 \u2223\u2223\u2223tgs,k \u2212 T\u0303\u03c9[ngs,k]\u2223\u2223\u22232 Ntest\u03c32 , (18)\nwhere Ntest is the number of instances in the test set and \u03c32 = 1Ntest \u2211Ntest k=1 (tgs,k \u2212 t\u0304gs)\n2 is the variance of their kinetic energies; with t\u0304gs = 1Ntest \u2211Ntest k=1 tgs,k we denote the average kinetic energy. After training, the two variants of standard CNNs reach remarkable accuracies on the test set, meaning that, when they are provided with an exact ground-state density profile corresponding to a previously unseen speckle pattern, they accurately predict the associated ground-state kinetic energy and, via Eq. (3), also the total energy. The R2 scores obtained with these two CNNs are reported in Table I. The R2 scores reached by the CNN are comparable, but slightly inferior, to the ones obtained by the standard CNNs (see Table I). Remarkably, despite of this (slightly) lower performance in kinetic energy predictions, the averaging operation drastically suppresses the noise in the functional derivative, allowing the use of CNN in a gradient-based search of the ground-state energy and density profile. This is discussed in Section IV."
        },
        {
            "heading": "IV. RESULTS FOR GRADIENT-DESCENT OPTIMIZATION",
            "text": "To be suitable for the DFT framework, the deeplearned functional T\u0303\u03c9[n] should allow iterating the gradient descent process as long as required to reach the minimum of E[n] \u2261 T\u0303\u03c9[n] + \u222b dxV (x)n(x). Hereafter, we denote with nmin(x) the density profile reached after gradient-descent optimization, and with emin = E[nmin(x)] the corresponding energy. The latter represents our estimate for the ground-state energy egs. Importantly, energies significantly lower than egs should\n0 5 10 14 x\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nn( x)\nCNN CNN Softplus ground-state density\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\nV( x)\n(a)\npotential\n0 5 10 14 x\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nn( x)\nCNN CNN ReLU ground-state density\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\nV( x)\n(b)\npotential\nFIG. 1. Panels (a) and (b) show the external potentials V (x) representing two instances of the optical speckle pattern (right vertical axis, the unit is the correlation energy Ec) and the corresponding ground-state density profiles (left vertical axis, units of 1/\u03b3). The spatial variable x is in units of the correlation length \u03b3. In panel (a) the exact ground state is compared to the DFT result obtained using the standard CNN with Softplus activation function and using the CNN. In panel (b) the standard CNN with ReLU activation is considered instead.\nnever occur during the optimization process, as they would constitute a violation of the variational principle. As explained in Section II, the possible freezing in a local minimum significantly larger than egs can be circumvented by repeating the optimization from a different initial profile. Figure 1 displays the density profiles reached after tmax = 10000 steps of gradient descent for two representative instances of the speckle pattern. For these and all other results reported below, the learning rate used in gradient descent is \u03b7 = 10\u22123. Clearly, the standard CNNs lead to unphysical profiles, while the CNN provides an accurate approximation of the exact profile ngs(x). To shed light on this phenomenon, we analyze\n6 the behavior of the energy discrepancy\n\u2206e = egs \u2212 emin (19)\nand of the density discrepancy\n|\u2206n| = \u221a\u222b dx(ngs(x)\u2212 nmin(x))2, (20)\nbased on the L2 metric\n|n| = \u221a\u222b dxn2(x) (21)\nalong the gradient-descent process. Specifically, we consider the average of the relative energy error \u3008\u2206e/egs\u3009, of the relative absolute energy error \u3008|\u2206e|/egs\u3009, and of the relative density error \u3008|\u2206n|/|ngs|\u3009 computed over a test set of 500 instances. Their dependence on the number of gradient-descent steps is shown in Fig. 2. The vertical bars indicate the standard deviation over this test set, meaning that they represent the fluctuations among different realizations of the speckle pattern. For both standard CNNs, after an initial decrease, the average absolute error increases. This means that the optimization process in not reliable, as it should be halted at an unknown intermediate number of steps. The average relative error becomes negative, indicating a violation of the variational principle. The density error also increases after many steps, corresponding to the formation of unphysical density profiles with large spurious spatial fluctuations, as exemplified in Fig. 1. Instead, the absolute relative energy error corresponding to the CNN (with Nc = 260 channels) systematically decreases until it saturates around a small value corresponding to \u223c 0.5%. The average error saturates close to \u3008\u2206e/egs\u3009 \u223c 0, meaning that significant violation of the variational principle do not occur. The histograms shown in Fig. 3 compare the energy and the density errors obtained with the CNN and with the standard CNN with ReLU activation function (this outperforms the corresponding model with Softplus activation) after tmax = 10000 steps of the gradient descent optimization. The CNN energies are concentrated around zero error, while the standard CNN results are broadly distributed in the region of negative energy errors, corresponding to strong violations of the variational principle.\nNotably, the energy predictions obtained by performing gradient-descent optimization with the CNN systematically improve when the number of channels Nc increases. This effect is shown in Fig. 4. Notice that, for small Nc, small violations of the variational principle still occur in rare cases. However, they vanish for larger Nc. In fact, the average absolute energy discrepancy obtained after gradient descent with the Nc = 260 CNN is \u3008|\u2206e|/e\u3009 ' 0.2%, which is approximately twice the corresponding discrepancy obtained on a test set of exact grounds-state profiles ngs(x), namely, \u3008|\u2206eML|/e\u3009 '\n0.0\n0.2\n0.4\n| e|\n/e\n(a)\nCNN CNN ReLU\n0.1%. Notice that this approximate doubling effect is expected, since the former error is also affected by the approximation in the density profile, while the latter corresponds to the CNN prediction on the exact profile. This means that gradient-descent optimization successfully identifies the ground state, within the residual uncertainty of the ML model. Increasing Nc leads also to more accurate density profiles (see panel (b) of Fig. 4) and to the reduction of the spatial noise observed in the results provided by standard CNN (see Fig. 1 and also Refs.[18] [10]).\nTo quantify this spatial noise, we consider the following\n7 0.08 0.06 0.04 0.02 0.00 e/e 0 50 100 150 200 250 No . i ns ta nc es (a) CNN CNN ReLU\n0.0 0.2 0.4 0.6 | n|/|n|\n0\n50\n100\n150\n200\n250\nNo . i\nns ta\nnc es\n(b)\nCNN CNN ReLU\nFIG. 3. Histograms of the relative energy discrepancies \u2206e/e [panel (a)] and the density discrepancies |\u2206n| / |n| [panel (b)] for a test set of 500 speckle-pattern instances, after tmax = 10000 steps of the gradient-descent optimization. The results of the standard CNN with ReLU activation function are compared to the ones of the CNN.\nmetric:\n\u2206A = 1\nNg \u2329 Ng\u2211 i=1 (\u2223\u2223\u2223\u2223\u2207nmin(xi)\u2207ngs(xi) \u2223\u2223\u2223\u2223\u2212 1) \u232a . (22)\nIt measures the error in the derivative of the density profile. Inaccurate profiles are characterized by large positive values \u2206A 1, due to spurious spatial fluctuations, while exact predictions lead to \u2206A = 0. We find that large Nc values lead to accurate and smooth density profiles (see panel (c) of Fig. 4), indicating the effectiveness of the average-channel layer. Residual local spurious fluctuations in the density profiles might be further suppressed via filtering procedures in post processing. Still, accurate DFT predictions are essential to avoid introducing biases by strong filtering procedures.\nIt is worth further emphasizing that using the standard metrics of deep learning on training and test sets of exact density profiles is not necessarily helpful to predict the performance of the ML functional in the variational minimization of DFT. This is exemplified by the\n0.04 0.02 0.00 e/e\n0 20 40 60 80\n100 120 140 160\nNo . i\nns ta\nnc es (a)\nNc = 60 Nc = 140 Nc = 260\n0.00 0.05 0.10 0.15 0.20 | n|/|n|\n0\n20\n40\n60\n80\n100\n120\nNo . i\nns ta\nnc es (b)\nNc = 60 Nc = 140 Nc = 260\n0 20 40 A\n0\n50\n100\n150\n200\n250\nNo . i\nns ta\nnc es (c)\nNc = 60 Nc = 140 Nc = 260\nFIG. 4. Histograms of the relative energy discrepancies \u2206e/e [panel (a)], of the density discrepancies |\u2206n|/|n| [panel (b)], and of the noise metric \u2206A defined in Eq. 22 [panel (c)], for a test set of 500 speckle-pattern instances, after tmax = 10000 steps of the gradient descent optimization. The results of the CNNs with different number of convolutional channels Nc are shown.\nscatter plots of Fig. 5. They display the errors obtained after gradient descent optimization versus the coefficient of determination Eq.(18) computed on a test set of exact ground-state density profiles. The two standard CNNs and the CNN with three values of Nc are considered.\n8 No (anti) correlation is clearly noticeable, meaning that high prediction accuracies on exact ground-states do not necessarily correspond to highly effective functionals for DFT. This should be taken into account in the future development of deep learning techniques for DFT."
        },
        {
            "heading": "V. CONCLUSIONS",
            "text": "The progress in data-based DFT is currently being hindered by the instabilities encountered when using ML functionals in gradient-based optimization. We presented a promising approach to circumvent this problem. This relies on the implementation of a deep neural network tailored to DFT. Specifically, we have shown that when an inter-channel averaging layer is included, beyond the standard convolutional, pooling, and dense layers, gradient-descent optimization can be iterated at will, obtaining accurate ground-state energies and density profiles and avoiding violations of the variational principle beyond residual uncertainties from the imperfect training of the regression model. Our analysis has focused on a realistic one-dimensional model for non-interacting atoms in optical speckle disorder, which leads to rather\nvariegate density profiles compared to models addressed in previous studies on ML based DFT. For completeness, in the Appendix A the performance of the CNN in Gaussian-well models borrowed from the literature is demonstrated. On the one hand, this further analysis indicates the rather general range of applicability of our tailored neural network. On the other hand, it points out the need of training sets including significantly variegate density profiles. To favor future comparative studies, our training datasets are made freely available at Ref. [36].\nAdditional challenges are going to be faced in the further development of ML techniques for DFT [37]. A further assessment of the network effectiveness should focus on higher dimensional and interacting models. In the DFT formalism, moving from single-particle to manybody problems is less challenging than in wave-function based methods, since observables are still obtained from the single-particle density. Therefore, we expect the CNN to be useful also in the many-body context. Clearly, generating training datasets is, in that case, more demanding [19, 38]. The learning process has to be accelerated, and the following strategies could be adopted. Incorporating physics knowledge into the deep-learning framework is a possible strategy [39, 40]. Another promising approach is transfer learning. This technique has already proven suitable to accelerate the supervised learning of the ground-state properties of both non-interacting and interacting quantum systems [41\u201343]. Interestingly, even extrapolations were proven feasible, meaning that (scalable) networks trained on relatively small systems provided accurate predictions for larger sizes or larger particle numbers. These techniques might be adopted also in the framework of DFT. We leave these endeavors to future investigations."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This work was supported by the Italian Ministry of University and Research under the PRIN2017 project CEnTraL 20172H2SC4. S.P. acknowledges PRACE for awarding access to the Fenix Infrastructure resources at Cineca, which are partially funded by the European Union\u2019s Horizon 2020 research and innovation program through the ICEI project under the Grant Agreement No. 800858. S. P. also acknowledges the Cineca award under the ISCRA initiative, for the availability of high performance computing resources and support."
        },
        {
            "heading": "Appendix A: Gaussian-well potentials",
            "text": "To favor direct comparison, and to further characterize the effectiveness of the CNN, here we consider a different testbed model, borrowed from previous studies. It describes narrow potential wells which confine the particle in the central region of a finite box. Setting the units such that ~2/(m`2) = 1, where ` is the length unit, the\n9 0.010 0.005 0.000 0.005 0.010 e/e 0 5 10 15 20 25 30 No . i ns ta nc es (a) gaussian\n0.00 0.02 0.04 0.06 0.08 0.10 | n|/|n|\n0\n2\n4\n6\n8\n10\nNo . i\nns ta\nnc es (b)\ngaussian\nFIG. 6. Histograms of the relative energy discrepancies \u2206e/e [panel (a)] and the density discrepancies |\u2206n| / |n| [panel (b)] for a test set of 100 Gaussian-well potentials, after tmax = 10000 steps of the gradient-descent optimization.\n(adimensional) single particle Hamiltonian reads:\nH = \u22121 2\nd2\ndx2 + V (x), (A1)\nwith the external potential\nV (x) = \u2212 3\u2211 i=1 ai exp [ \u2212 (x\u2212 bi) 2 2c2i ] . (A2)\nThe model parameters ai, bi, and ci are sampled from uniform probability distributions in the following ranges: ai \u2208 [1, 40], bi \u2208 [1.8, 4.2], and ci \u2208 [0.12, 0.4]. The box size is L = 6, and hard-wall boundary conditions are adopted. The chosen box is L = 6, rather than L = 1 as in Ref.[18], so that the ground-state density profiles essentially vanish before reaching the boundaries. This strongly suppresses the role of the choice of the boundary conditions. Indeed, we find negligible variations in the ground-state energies with hard-wall compared to periodic boundary conditions. With the size L = 1, the boundary effects are sizable, and the density profiles corresponding to different potential instances display small\n0 1 2 3 4 5 6 x\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nn( x) min density ground-state density\n60\n50\n40\n30\n20\n10\n0\nV( x)\npotential\nFIG. 7. External potential V (x) corresponding to an instance of the Gaussian-well potential (right vertical axis, unit of h 2 m`2 ) and the corresponding ground-state density profiles (left vertical axis, units of 1/`). ` is the length unit, used on the horizontal axis. The density configuration obtained by gradient-descent minimization is compared to the exact density profile.\nvariations when hard-wall boundaries are adopted. This does not allow an effective training of the deep neural network, reintroducing instabilities in the gradient-descent optimization. We find that this problem is solved either enlarging the system size, e.g., to L = 6, as shown hereafter, or by adopting periodic boundary conditions in a small box, as mentioned later on in this paragraph.\nA CNN is trained on a global dataset of 150000 instances (90% train, 10% validation), using the same structure (2 blocks, pooling size [4, 2], kernel size 13, and 260 hidden channels) and the same hyperparameters as in the case of the speckle potential with a number of epochs Ne = 3000. However, here the convolutional layers use zero padding, as opposed to the periodic padding adopted for the speckle potentials, which are defined within periodic boundary conditions. The gradient descent performance on a test set of 100 instances is visualized in Fig. 6. Again, we find that gradient-descent optimization (tmax = 10000) leads to accurate ground-state energies (\u3008|\u2206e|/e\u3009 ' 0.14%) and density profiles (\u3008|\u2206n|/|n|\u3009 ' 3% ), without sizable violation of the variational condition. For illustrative purposes, an example of Gaussian-well potential, with the corresponding density profile obtained with L = 6, is shown in Fig. 7. To facilitate comparison, we report the model details and the performance metrics using the units and the conventions adopted in some previous studies. These considered one-electron models in atomic units, so that ` corresponds to the Bohr radius and the energy unit ~2/(m`2) corresponds to one Hartree (Ha). The mean kinetic energy of the L = 6 model is t\u0304gs ' 3.994Ha and the standard deviation is quite large, namely \u03c3 ' 1.457Ha, with maximum value 12.957Ha\n10\nand minimum value 0.2605Ha, corresponding to considerably variegate ground states. The average kinetic-energy prediction-error on a test set of Ntest = 15000 instances is \u3008|\u2206tML|\u3009 \u2261 N\u22121test \u2211Ntest k=1\n\u2223\u2223\u2223tgs,k \u2212 T\u0303\u03c9[ngs,k]\u2223\u2223\u2223 ' 8 \u00b7 10\u22124Ha; the standard deviation of |\u2206tML| is \u03c3 ' 11 \u00b7 10\u22124Ha. After gradient descent, the average absolute kinetic-energy error is\n\u2329\u2223\u2223\u2223T\u0303\u03c9[nmin]\u2212 tgs\u2223\u2223\u2223\u232a ' 0.0171Ha and the corresponding standard deviation \u03c3 ' 0.0191Ha. The average absolute density discrepancy is \u3008|\u2206n|\u3009 = 0.031 and the corresponding standard deviation is \u03c3 ' 0.008.\nIt is also worth mentioning that, considering L = 1 and periodic boundary conditions, the accuracy metrics\nare comparable as above, namely: \u3008|\u2206e|/e\u3009 ' 0.3% and \u3008|\u2206n|/|n|\u3009 ' 3%. In this case, periodic padding is used, and the first left and right periodic images of the Gaussian wells are included to make the potential V (r) essentially periodic. The potential parameters are sampled in the following ranges bi \u2208 [0.1, 0.9], ai \u2208 [1, 40] and ci \u2208 [0.03, 0.1]. When combined with periodic boundary conditions, these allow creating sufficiently variegate density profiles despite of the small system size. These findings indicate that the CNN represents a flexible regression model to apply ML-based DFT to rather arbitrary external potentials, whereby the density profiles display significant variations among different random realization of the sample.\n[1] K. Burke, Perspective on density functional theory, J. Chem. Phys. 136, 150901 (2012). [2] P. Hohenberg and W. Kohn, Inhomogeneous electron gas, Phys. Rev. 136, B864 (1964). [3] M. Levy, Universal variational functionals of electron densities, first-order density matrices, and natural spinorbitals and solution of the v-representability problem, Proc. Natl. Acad. Sci. U.S.A. 76, 6062 (1979). [4] E. Lieb, Density functionals for Coulomb systems, International Journal of Quantum Chemistry 24, 243 (1983). [5] A. J. Cohen, P. Mori-S\u00e1nchez, and W. Yang, Challenges for density functional theory, Chem. Rev. 112, 289 (2012). [6] V. Dunjko and H. J. Briegel, Machine learning & artificial intelligence in the quantum domain: A review of recent progress, Rep. Prog. Phys. 81, 074001 (2018). [7] G. Carleo, I. Cirac, K. Cranmer, L. Daudet, M. Schuld, N. Tishby, L. Vogt-Maranto, and L. Zdeborov\u00e1, Machine learning and the physical sciences, Rev. Mod. Phys. 91, 045002 (2019). [8] J. Carrasquilla, Machine learning for quantum matter, Adv. Phys.-X 5, 1797528 (2020). [9] H. J. Kulik, T. Hammerschmidt, J. Schmidt, S. Botti, M. A. L. Marques, M. Boley, M. Scheffler, M. Todorovi\u0107, P. Rinke, C. Oses, A. Smolyanyuk, S. Curtarolo, A. Tkatchenko, A. P. Bart\u00f3k, S. Manzhos, M. Ihara, T. Carrington, J. Behler, O. Isayev, M. Veit, A. Grisafi, J. Nigam, M. Ceriotti, K. T. Sch\u00fctt, J. Westermayr, M. Gastegger, R. J. Maurer, B. Kalita, K. Burke, R. Nagai, R. Akashi, O. Sugino, J. Hermann, F. No\u00e9, S. Pilati, C. Draxl, M. Kuban, S. Rigamonti, M. Scheidgen, M. Esters, D. Hicks, C. Toher, P. V. Balachandran, I. Tamblyn, S. Whitelam, C. Bellinger, and L. M. Ghiringhelli, Roadmap on machine learning in electronic structure, Electronic Structure 4, 023004 (2022). [10] J. C. Snyder, M. Rupp, K. Hansen, K.-R. M\u00fcller, and K. Burke, Finding density functionals with machine learning, Phys. Rev. Lett. 108, 253002 (2012). [11] L. Li, J. C. Snyder, I. M. Pelaschier, J. Huang, U.N. Niranjan, P. Duncan, M. Rupp, K.-R. M\u00fcller, and K. Burke, Understanding machine-learned density functionals, Int. J. Quantum Chem. 116, 819 (2016). [12] F. Brockherde, L. Vogt, L. Li, M. E. Tuckerman, K. Burke, and K.-R. M\u00fcller, Bypassing the Kohn-Sham\nequations with machine learning, Nat. Commun. 8, 872 (2017). [13] K. Ryczko, D. A. Strubbe, and I. Tamblyn, Deep learning and density-functional theory, Phys. Rev. A 100, 022512 (2019). [14] J. Kirkpatrick, B. McMorrow, D. H. P. Turban, A. L. Gaunt, J. S. Spencer, A. G. D. G. Matthews, A. Obika, L. Thiry, M. Fortunato, D. Pfau, L. R. Castellanos, S. Petersen, A. W. R. Nelson, P. Kohli, P. Mori-S\u00e1nchez, D. Hassabis, and A. J. Cohen, Pushing the frontiers of density functionals by solving the fractional electron problem, Science 374, 1385 (2021). [15] J. Nelson, R. Tiwari, and S. Sanvito, Machine learning density functional theory for the Hubbard model, Phys. Rev. B 99, 075132 (2019). [16] J. R. Moreno, G. Carleo, and A. Georges, Deep learning the hohenberg-kohn maps of density functional theory, Phys. Rev. Lett. 125, 076402 (2020). [17] M. M. Denner, M. H. Fischer, and T. Neupert, Efficient learning of a one-dimensional density functional theory, Phys. Rev. Res. 2, 033388 (2020). [18] R. Meyer, M. Weichselbaum, and A. W. Hauser, Machine learning approaches toward orbital-free density functional theory: Simultaneous training on the kinetic energy density functional and its functional derivative, J. Chem. Theory Comput. 16, 5685 (2020). [19] K. Ryczko, S. J. Wetzel, R. G. Melko, and I. Tamblyn, Toward orbital-free density functional theory with small data sets and deep learning, J. Chem. Theory Comput. 18, 1122\u20131128 (2022). [20] J. C. Snyder, M. Rupp, K.-R. M\u00fcller, and K. Burke, Nonlinear gradient denoising: Finding accurate extrema from inaccurate functional derivatives, Int. J. Quantum Chem. 115, 1102 (2015). [21] L. Li, T. E. Baker, S. R. White, and K. Burke, Pure density functional for strong correlation and the thermodynamic limit from machine learning, Phys. Rev. B 94, 245129 (2016). [22] In interacting systems, the unknown functional would include also correlation effects, while the mean-field contribution would be conveniently encoded in the so-called Hartree term. [23] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer,\n11\nAutomatic differentiation in pytorch, in NIPS-W (2017). [24] W. C. Witt, B. G. del Rio, J. M. Dieterich, and E. A.\nCarter, Orbital-free density functional theory for materials research, J. Mater. Res. 33, 777\u2013795 (2018). [25] J. W. Goodman, Speckle phenomena in optics: theory and applications (Roberts and Company Publishers, 2007). [26] J. Billy, V. Josse, Z. Zuo, A. Bernard, B. Hambrecht, P. Lugan, D. Cl\u00e9ment, L. Sanchez-Palencia, P. Bouyer, and A. Aspect, Direct observation of Anderson localization of matter waves in a controlled disorder, Nature 453, 891 (2008). [27] G. Roati, C. D\u2019Errico, L. Fallani, M. Fattori, C. Fort, M. Zaccanti, G. Modugno, M. Modugno, and M. Inguscio, Anderson localization of a non-interacting Bose\u2013 Einstein condensate, Nature 453, 895 (2008). [28] T. Prat, N. Cherroret, and D. Delande, Semiclassical spectral function and density of states in speckle potentials, Phys. Rev. A 94, 022114 (2016). [29] S. Pilati and P. Pieri, Supervised machine learning of ultracold atoms with speckle disorder, Sci. Rep. 9, 1 (2019). [30] P. Mujal, A. Polls, S. Pilati, and B. Juli\u00e1-D\u00edaz, Few-boson localization in a continuum with speckle disorder, Phys. Rev. A 100, 013603 (2019). [31] E. Abrahams, P. W. Anderson, D. C. Licciardello, and T. V. Ramakrishnan, Scaling theory of localization: Absence of quantum diffusion in two dimensions, Phys. Rev. Lett. 42, 673 (1979). [32] P. W. Anderson, Absence of diffusion in certain random lattices, Phys. Rev. 109, 1492 (1958). [33] J. Huntley, Speckle photography fringe analysis: assess-\nment of current algorithms, Appl. Opt. 28, 4316 (1989). [34] M. Modugno, Collective dynamics and expansion of a\nbose-einstein condensate in a random potential, Phys. Rev. A 73, 013606 (2006). [35] D. P. Kingma and J. Ba, Adam: A method for stochastic optimization, arXiv:1412.6980 (2014). [36] E. Costa, G. Scriva, S. Pilati, and R. Fazio, Data for Deep learning density functionals for gradient descent optimization (2022). [37] R. Pederson, B. Kalita, and K. Burke, Machine learning and density functional theory, arXiv:2205.01591 (2022). [38] K. Ryczko, J. T. Krogel, and I. Tamblyn, Machine learning diffusion monte carlo energy densities, arXiv:2205.04547 (2022). [39] R. Nagai, R. Akashi, and O. Sugino, Completing density functional theory by machine learning hidden messages from molecules, Npj Comput. Mater. 6, 1 (2020). [40] L. Li, S. Hoyer, R. Pederson, R. Sun, E. D. Cubuk, P. Riley, and K. Burke, Kohn-sham equations as regularizer: Building prior knowledge into machine-learned physics, Phys. Rev. Lett. 126, 036401 (2021). [41] K. Mills, K. Ryczko, I. Luchak, A. Domurad, C. Beeler, and I. Tamblyn, Extensive deep neural networks for transferring small scale learning to large scale systems, Chem. Sci. 10, 4129 (2019). [42] N. Saraceni, S. Cantori, and S. Pilati, Scalable neural networks for the efficient learning of disordered quantum systems, Phys. Rev. E 102, 033301 (2020). [43] P. Mujal, \u00c0lex Mart\u00ednez Miguel, A. Polls, B. Juli\u00e1D\u00edaz, and S. Pilati, Supervised learning of few dirty bosons with variable particle number, SciPost Phys. 10, 73 (2021)."
        }
    ],
    "title": "Deep learning density functionals for gradient descent optimization",
    "year": 2022
}