{
    "abstractText": "Schlieren imaging is an optical technique to observe the flow of transparent media, such as air or water, without any particle seeding. However, conventional frame-based techniques require both high spatial and temporal resolution cameras, which impose bright illumination and expensive computation limitations. Event cameras offer potential advantages (high dynamic range, high temporal resolution, and data efficiency) to overcome such limitations due to their bio-inspired sensing principle. This paper presents a novel technique for perceiving air convection using events and frames by providing the first theoretical analysis that connects event data and schlieren. We formulate the problem as a variational optimization one combining the linearized event generation model with a physically-motivated parametrization that estimates the temporal derivative of the air density. The experiments with accurately aligned frameand event camera data reveal that the proposed method enables event cameras to obtain on par results with existing frame-based optical flow techniques. Moreover, the proposed method works under dark conditions where frame-based schlieren fails, and also enables slow-motion analysis by leveraging the event camera\u2019s advantages. Our work pioneers and opens a new stack of event camera applications, as we publish the source code as well as the first schlieren dataset with high-quality frame and event data. https://github.com/tub-rip/event based bos.",
    "authors": [
        {
            "affiliations": [],
            "name": "Shintaro Shiba"
        },
        {
            "affiliations": [],
            "name": "Friedhelm Hamann"
        },
        {
            "affiliations": [],
            "name": "Yoshimitsu Aoki"
        },
        {
            "affiliations": [],
            "name": "Guillermo Gallego"
        }
    ],
    "id": "SP:233a7eae19e772094037aa458c88b0977aa9e777",
    "references": [
        {
            "authors": [
                "G.S. Settles"
            ],
            "title": "Schlieren and shadowgraph techniques: visualizing phenomena in transparent media",
            "venue": "Springer Science & Business Media,",
            "year": 2001
        },
        {
            "authors": [
                "G.S. Settles",
                "M.J. Hargather"
            ],
            "title": "A review of recent developments in schlieren and shadowgraph techniques",
            "venue": "Meas. Sci. Technol., vol. 28, no. 4, p. 042001, 2017.",
            "year": 2001
        },
        {
            "authors": [
                "G.S. Settles",
                "A. Liberzon"
            ],
            "title": "Schlieren and BOS velocimetry of a round turbulent helium jet in air",
            "venue": "Optics and Lasers in Eng., vol. 156, p. 107104, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "P. Lichtsteiner",
                "C. Posch",
                "T. Delbruck"
            ],
            "title": "A 128\u00d7128 120 dB 15 \u03bcs latency asynchronous temporal contrast vision sensor",
            "venue": "IEEE J. Solid-State Circuits, vol. 43, no. 2, pp. 566\u2013576, 2008.",
            "year": 2008
        },
        {
            "authors": [
                "T. Finateu",
                "A. Niwa",
                "D. Matolin",
                "K. Tsuchimoto",
                "A. Mascheroni",
                "E. Reynaud",
                "P. Mostafalu",
                "F. Brady",
                "L. Chotard",
                "F. LeGoff",
                "H. Takahashi",
                "H. Wakabayashi",
                "Y. Oike",
                "C. Posch"
            ],
            "title": "A 1280x720 backilluminated stacked temporal contrast event-based vision sensor with 4.86\u03bcm pixels, 1.066Geps readout, programmable event-rate controller and compressive data-formatting pipeline",
            "venue": "IEEE Intl. Solid-State Circuits Conf. (ISSCC), 2020, pp. 112\u2013114.",
            "year": 2020
        },
        {
            "authors": [
                "G. Gallego",
                "T. Delbruck",
                "G. Orchard",
                "C. Bartolozzi",
                "B. Taba",
                "A. Censi",
                "S. Leutenegger",
                "A. Davison",
                "J. Conradt",
                "K. Daniilidis",
                "D. Scaramuzza"
            ],
            "title": "Event-based vision: A survey",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 44, no. 1, pp. 154\u2013180, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "M. Raffel",
                "C.E. Willert",
                "J. Kompenhans"
            ],
            "title": "Particle image velocimetry: a practical guide",
            "year": 1998
        },
        {
            "authors": [
                "Y. Ding",
                "Z. Li",
                "Z. Chen",
                "Y. Ji",
                "J. Yu",
                "J. Ye"
            ],
            "title": "Full-volume 3d fluid flow reconstruction with light field piv",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 2023.",
            "year": 2023
        },
        {
            "authors": [
                "H. Richard",
                "M. Raffel"
            ],
            "title": "Principle and applications of the background oriented schlieren (BOS) method",
            "venue": "Meas. Sci. Technol., vol. 12, no. 9, p. 1576, 2001.",
            "year": 2001
        },
        {
            "authors": [
                "E. Goldhahn",
                "J. Seume"
            ],
            "title": "The background oriented schlieren technique: sensitivity, accuracy, resolution and application to a three-dimensional density field",
            "venue": "Experiments in fluids, vol. 43, pp. 241\u2013249, 2007.",
            "year": 2007
        },
        {
            "authors": [
                "B. Atcheson",
                "W. Heidrich",
                "I. Ihrke"
            ],
            "title": "An evaluation of optical flow algorithms for background oriented schlieren imaging",
            "venue": "Exp. Fluids, vol. 46, pp. 467\u2013476, 2009.",
            "year": 2009
        },
        {
            "authors": [
                "B.E. Schmidt",
                "M.R. Woike"
            ],
            "title": "Wavelet-based optical flow analysis for background-oriented schlieren image processing",
            "venue": "AIAA Journal, vol. 59, no. 8, pp. 3209\u20133216, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "R. Hooke"
            ],
            "title": "Of a new property in the air",
            "venue": "Micrographia, Observation LVIII, pp. 217\u2013219, 1665.",
            "year": 1665
        },
        {
            "authors": [
                "M. Raffel"
            ],
            "title": "Background-oriented schlieren (BOS) techniques",
            "venue": "Exp. Fluids, vol. 56, no. 3, pp. 1\u201317, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "P. Krehl",
                "S. Engemann"
            ],
            "title": "August toepler\u2014the first who visualized shock waves",
            "venue": "Shock Waves, vol. 5, pp. 1\u201318, 1995.",
            "year": 1995
        },
        {
            "authors": [
                "O. Sommersel",
                "D. Bjerketvedt",
                "S. Christensen",
                "O. Krest",
                "K. Vaagsaether"
            ],
            "title": "Application of background oriented schlieren for quantitative measurements of shock waves from explosions",
            "venue": "Shock Waves, vol. 18, pp. 291\u2013297, 2008.",
            "year": 2008
        },
        {
            "authors": [
                "K. Hayasaka",
                "Y. Tagawa",
                "T. Liu",
                "M. Kameda"
            ],
            "title": "Optical-flowbased background-oriented schlieren technique for measuring a laser-induced underwater shock wave",
            "venue": "Experiments in Fluids, vol. 57, pp. 1\u201311, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "M.J. Hargather",
                "G.S. Settles"
            ],
            "title": "Natural-background-oriented schlieren imaging",
            "venue": "Experiments in fluids, vol. 48, no. 1, pp. 59\u201368, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "J.T. Heineck",
                "D.W. Banks",
                "N.T. Smith",
                "E.T. Schairer",
                "P.S. Bean",
                "T. Robillos"
            ],
            "title": "Background-oriented schlieren imaging of supersonic aircraft in flight",
            "venue": "AIAA Journal, vol. 59, no. 1, pp. 11\u2013 21, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J.H. Tu",
                "C.W. Rowley",
                "D.M. Luchtenburg",
                "S.L. Brunton",
                "J.N. Kutz"
            ],
            "title": "On dynamic mode decomposition: Theory and applications",
            "venue": "J. Computational Dynamics, vol. 1, no. 2, pp. 391\u2013421, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "L. Venkatakrishnan",
                "G. Meier"
            ],
            "title": "Density measurements using the background oriented schlieren technique",
            "venue": "Experiments in Fluids, vol. 37, pp. 237\u2013247, 2004.",
            "year": 2004
        },
        {
            "authors": [
                "R. Benosman",
                "C. Clercq",
                "X. Lagorce",
                "S.-H. Ieng",
                "C. Bartolozzi"
            ],
            "title": "Event-based visual flow",
            "venue": "IEEE Trans. Neural Netw. Learn. Syst., vol. 25, no. 2, pp. 407\u2013417, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "A.Z. Zhu",
                "L. Yuan",
                "K. Chaney",
                "K. Daniilidis"
            ],
            "title": "EV-FlowNet: Self-supervised optical flow estimation for event-based cameras",
            "venue": "Robotics: Science and Systems (RSS), 2018, pp. 1\u20139.",
            "year": 2018
        },
        {
            "authors": [
                "J.J. Hagenaars",
                "F. Paredes-Valles",
                "G.C.H.E. de Croon"
            ],
            "title": "Self-supervised learning of event-based optical flow with spiking neural networks",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS), vol. 34, 2021, pp. 7167\u20137179.",
            "year": 2021
        },
        {
            "authors": [
                "S. Shiba",
                "Y. Aoki",
                "G. Gallego"
            ],
            "title": "Secrets of event-based optical flow",
            "venue": "Eur. Conf. Comput. Vis. (ECCV), 2022, pp. 628\u2013645.",
            "year": 2022
        },
        {
            "authors": [
                "\u2014\u2014"
            ],
            "title": "Fast event-based optical flow estimation by triplet matching",
            "venue": "IEEE Signal Process. Lett., pp. 1\u20135, 2023. This article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2023.3328188 This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/ 15",
            "year": 2023
        },
        {
            "authors": [
                "G. Gallego",
                "H. Rebecq",
                "D. Scaramuzza"
            ],
            "title": "A unifying contrast maximization framework for event cameras, with applications to motion, depth, and optical flow estimation",
            "venue": "IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), 2018, pp. 3867\u20133876.",
            "year": 2018
        },
        {
            "authors": [
                "U.M. Nunes",
                "Y. Demiris"
            ],
            "title": "Robust event-based vision model estimation by dispersion minimisation",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 2021.",
            "year": 2021
        },
        {
            "authors": [
                "X. Peng",
                "L. Gao",
                "Y. Wang",
                "L. Kneip"
            ],
            "title": "Globally-optimal contrast maximisation for event cameras",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 44, no. 7, pp. 3479\u20133495, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "S. Shiba",
                "Y. Aoki",
                "G. Gallego"
            ],
            "title": "A fast geometric regularizer to mitigate event collapse in the contrast maximization framework",
            "venue": "Adv. Intell. Syst., p. 2200251, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "H. Kim",
                "S. Leutenegger",
                "A.J. Davison"
            ],
            "title": "Real-time 3D reconstruction and 6-DoF tracking with an event camera",
            "venue": "Eur. Conf. Comput. Vis. (ECCV), 2016, pp. 349\u2013364.",
            "year": 2016
        },
        {
            "authors": [
                "H. Rebecq",
                "T. Horstsch\u00e4fer",
                "G. Gallego",
                "D. Scaramuzza"
            ],
            "title": "EVO: A geometric approach to event-based 6-DOF parallel tracking and mapping in real-time",
            "venue": "IEEE Robot. Autom. Lett., vol. 2, no. 2, pp. 593\u2013600, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "A.Z. Zhu",
                "N. Atanasov",
                "K. Daniilidis"
            ],
            "title": "Event-based visual inertial odometry",
            "venue": "IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), 2017, pp. 5816\u20135824.",
            "year": 2017
        },
        {
            "authors": [
                "J. Hidalgo-Carri\u00f3",
                "G. Gallego",
                "D. Scaramuzza"
            ],
            "title": "Event-aided direct sparse odometry",
            "venue": "IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), Jun. 2022, pp. 5781\u20135790.",
            "year": 2022
        },
        {
            "authors": [
                "X. Zhang",
                "L. Yu"
            ],
            "title": "Unifying motion deblurring and frame interpolation with events",
            "venue": "IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), Jun. 2022, pp. 17 765\u201317 774.",
            "year": 2022
        },
        {
            "authors": [
                "S. Tulyakov",
                "A. Bochicchio",
                "D. Gehrig",
                "S. Georgoulis",
                "Y. Li",
                "D. Scaramuzza"
            ],
            "title": "Time lens++: Event-based frame interpolation with parametric non-linear flow and multi-scale fusion",
            "venue": "IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), Jun. 2022, pp. 17 755\u2013 17 764.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Gao",
                "S. Li",
                "Y. Li",
                "Y. Guo",
                "Q. Dai"
            ],
            "title": "Superfast: 200\u00d7 video frame interpolation via event camera",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell., no. 01, pp. 1\u201317, nov 5555."
        },
        {
            "authors": [
                "Y. Suh",
                "S. Choi",
                "M. Ito",
                "J. Kim",
                "Y. Lee",
                "J. Seo",
                "H. Jung",
                "D.-H. Yeo",
                "S. Namgung",
                "J. Bong",
                "J. seok Kim",
                "P.K.J. Park",
                "J. Kim",
                "H. Ryu",
                "Y. Park"
            ],
            "title": "A 1280x960 Dynamic Vision Sensor with a 4.95-\u03bcm pixel pitch and motion artifact minimization",
            "venue": "IEEE Int. Symp. Circuits Syst. (ISCAS), 2020, pp. 1\u20135.",
            "year": 2020
        },
        {
            "authors": [
                "B. Pfrommer"
            ],
            "title": "Frequency Cam: Imaging periodic signals in realtime",
            "venue": "arXiv, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "C.E. Willert",
                "J. Klinner"
            ],
            "title": "Event-based imaging velocimetry: an assessment of event-based cameras for the measurement of fluid flows",
            "venue": "Exp. Fluids, vol. 63, no. 6, pp. 1\u201320, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Wang",
                "R. Idoughi",
                "W. Heidrich"
            ],
            "title": "Stereo event-based particle tracking velocimetry for 3D fluid flow reconstruction",
            "venue": "Eur. Conf. Comput. Vis. (ECCV), 2020, pp. 36\u201353.",
            "year": 2020
        },
        {
            "authors": [
                "D. Gehrig",
                "H. Rebecq",
                "G. Gallego",
                "D. Scaramuzza"
            ],
            "title": "EKLT: Asynchronous photometric feature tracking using events and frames",
            "venue": "Int. J. Comput. Vis., vol. 128, pp. 601\u2013618, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "G. Gallego",
                "C. Forster",
                "E. Mueggler",
                "D. Scaramuzza"
            ],
            "title": "Eventbased camera pose tracking using a generative event model",
            "venue": "2015, arXiv:1510.01972.",
            "year": 2015
        },
        {
            "authors": [
                "S. Bryner",
                "G. Gallego",
                "H. Rebecq",
                "D. Scaramuzza"
            ],
            "title": "Eventbased, direct camera tracking from a photometric 3D map using nonlinear optimization",
            "venue": "IEEE Int. Conf. Robot. Autom. (ICRA), 2019.",
            "year": 2019
        },
        {
            "authors": [
                "L. Pan",
                "M. Liu",
                "R. Hartley"
            ],
            "title": "Single image optical flow estimation with an event camera",
            "venue": "IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), 2020, pp. 1669\u20131678.",
            "year": 2020
        },
        {
            "authors": [
                "F. Paredes-Valles",
                "G.C.H.E. de Croon"
            ],
            "title": "Back to event basics: Self-supervised learning of image reconstruction for event cameras via photometric constancy",
            "venue": "IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), 2021, pp. 3445\u20133454.",
            "year": 2021
        },
        {
            "authors": [
                "Z. Zhang",
                "A. Yezzi",
                "G. Gallego"
            ],
            "title": "Formulating event-based image reconstruction as a linear inverse problem with deep regularization using optical flow",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 2022.",
            "year": 2022
        },
        {
            "authors": [
                "R. Graca",
                "T. Delbruck"
            ],
            "title": "Unraveling the paradox of intensitydependent DVS pixel noise",
            "venue": "Int. Image Sensor Workshop (IISW), 2021.",
            "year": 2021
        },
        {
            "authors": [
                "C. Scheerlinck",
                "N. Barnes",
                "R. Mahony"
            ],
            "title": "Continuous-time intensity estimation using event cameras",
            "venue": "Asian Conf. Comput. Vis. (ACCV), 2018, pp. 308\u2013324.",
            "year": 2018
        },
        {
            "authors": [
                "C. Brandli",
                "L. Muller",
                "T. Delbruck"
            ],
            "title": "Real-time, high-speed video decompression using a frame- and event-based DAVIS sensor",
            "venue": "IEEE Int. Symp. Circuits Syst. (ISCAS), 2014, pp. 686\u2013 689.",
            "year": 2014
        },
        {
            "authors": [
                "A.Z. Zhu",
                "D. Thakur",
                "T. Ozaslan",
                "B. Pfrommer",
                "V. Kumar",
                "K. Daniilidis"
            ],
            "title": "The multivehicle stereo event camera dataset: An event camera dataset for 3D perception",
            "venue": "IEEE Robot. Autom. Lett., vol. 3, no. 3, pp. 2032\u20132039, Jul. 2018.",
            "year": 2032
        },
        {
            "authors": [
                "M. Gehrig",
                "W. Aarents",
                "D. Gehrig",
                "D. Scaramuzza"
            ],
            "title": "DSEC: A stereo event camera dataset for driving scenarios",
            "venue": "IEEE Robot. Autom. Lett., vol. 6, no. 3, pp. 4947\u20134954, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S.N. Barkas"
            ],
            "title": "An introduction to fast Poisson solvers",
            "venue": "Philips J Res, vol. 37, no. 5-6, pp. 231\u2013264, 2005.",
            "year": 2005
        },
        {
            "authors": [
                "H. Kim",
                "A. Handa",
                "R. Benosman",
                "S.-H. Ieng",
                "A.J. Davison"
            ],
            "title": "Simultaneous mosaicing and tracking with an event camera",
            "venue": "British Mach. Vis. Conf. (BMVC), 2014.",
            "year": 2014
        },
        {
            "authors": [
                "D.P. Kingma",
                "J.L. Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "Int. Conf. Learn. Representations (ICLR), 2015.",
            "year": 2015
        },
        {
            "authors": [
                "C. Brandli",
                "R. Berner",
                "M. Yang",
                "S.-C. Liu",
                "T. Delbruck"
            ],
            "title": "A 240x180 130dB 3\u03bcs latency global shutter spatiotemporal vision sensor",
            "venue": "IEEE J. Solid-State Circuits, vol. 49, no. 10, pp. 2333\u20132341, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "G. Taverni",
                "D.P. Moeys",
                "C. Li",
                "C. Cavaco",
                "V. Motsnyi",
                "D.S.S. Bello",
                "T. Delbruck"
            ],
            "title": "Front and back illuminated Dynamic and Active Pixel Vision Sensors comparison",
            "venue": "IEEE Trans. Circuits Syst. II, vol. 65, no. 5, pp. 677\u2013681, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "M. Muglikar",
                "M. Gehrig",
                "D. Gehrig",
                "D. Scaramuzza"
            ],
            "title": "How to calibrate your event camera",
            "venue": "IEEE Conf. Comput. Vis. Pattern Recog. Workshops (CVPRW), 2021, pp. 1403\u20131409.",
            "year": 2021
        },
        {
            "authors": [
                "F. Hamann",
                "G. Gallego"
            ],
            "title": "Stereo co-capture system for recording and tracking fish with frame- and event cameras",
            "venue": "26th International Conference on Pattern Recognition (ICPR), Visual observation and analysis of Vertebrate And Insect Behavior (VAIB) Workshop, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "G. Farneb\u00e4ck"
            ],
            "title": "Two-frame motion estimation based on polynomial expansion",
            "venue": "Scandinavian Conf. on Im. Analysis (SCIA), 2003, pp. 363\u2013370.",
            "year": 2003
        },
        {
            "authors": [
                "Z. Huang",
                "X. Shi",
                "C. Zhang",
                "Q. Wang",
                "K.C. Cheung",
                "H. Qin",
                "J. Dai",
                "H. Li"
            ],
            "title": "FlowFormer: A transformer architecture for optical flow",
            "venue": "Eur. Conf. Comput. Vis. (ECCV), 2022, pp. 668\u2013685.",
            "year": 2022
        },
        {
            "authors": [
                "Z. Huang",
                "T. Zhang",
                "W. Heng",
                "B. Shi",
                "S. Zhou"
            ],
            "title": "Real-time intermediate flow estimation for video frame interpolation",
            "venue": "Eur. Conf. Comput. Vis. (ECCV), 2022, pp. 624\u2013642.",
            "year": 2022
        },
        {
            "authors": [
                "H. Rebecq",
                "R. Ranftl",
                "V. Koltun",
                "D. Scaramuzza"
            ],
            "title": "High speed and high dynamic range video with an event camera",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 43, no. 6, pp. 1964\u20131980, 2021.",
            "year": 1964
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014Event camera, Schlieren imaging, Background-oriented schlieren, Optical flow, Low-level vision.\n\u2726"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "S ENSING the flow of transparent media, such as air orwater, is important for various applications from aerodynamics to gas leakage detection. Optical imaging is a useful tool to examine such transparent media because it can capture the media with high detail in space-time remotely. Among existing methods, schlieren imaging is a simple but efficient optical tool for seeing the \u201cinvisible\u201d [1], [2]: inhomogeneities in transparent media that are not necessarily perceived by the naked eyes. It requires simple recording settings: lenses, cameras, and mirrors or background patterns to image how light rays deviate due to refractive index variations in the media. While it was initially conceived as a visualization technique, recent developments in schlieren and shadowgraphy have extended the usage to velocimetry [2], [3]. However, it requires a high-speed camera with a large spatial resolution to analyze the velocity of the flow, such as convection. This is not only a constraint for realworld applications but also a limitation of the methodology because: (i) achieving high shutter speeds requires unnaturally bright illumination, which is not always practical, (ii) transmitting and processing the large amount of redundant data acquired involves high bandwidth, storage, and powerhungry components, and (iii) regardless of the large power consumption, the trade-off between speed and spatial resolution limits accuracy in estimating the flow velocity.\n\u2022 S. Shiba and Y. Aoki are with Department of Electronics and Electrical Engineering, Faculty of Science and Technology, Keio University, Kanagawa, Japan. E-mail: sshiba@keio.jp \u2022 S. Shiba, F. Hamann and G. Gallego are with Department of Electrical Engineering and Computer Science, Technische Universita\u0308t Berlin, Berlin, Germany. \u2022 F. Hamann and G. Gallego are with Science of Intelligence Excellence Cluster, Berlin, Germany. \u2022 G. Gallego is with Einstein Center Digital Future, Berlin, Germany.\nEvent cameras [4], [5] are novel bio-inspired sensors that respond to pixel-wise intensity changes, which are not always visible to conventional frame-based cameras. They offer advantages such as high speed, high dynamic range (HDR), low power consumption, and data efficiency (temporal redundancy suppression) [6], which makes them potential candidates to overcome the limitations of traditional (i.e., frame-based) schlieren techniques. However, despite these potential capabilities, the application of event cameras to imaging applications is yet to be explored and developed.\nThis paper presents a novel technique, event-based background-oriented schlieren (BOS), for sensing air convection with event cameras and proposes a novel method to estimate the temporal derivative of air density from events and frames (Fig. 1). Throughout the paper, we tackle the following challenges of event-based BOS: (i) Theory.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n2\nThere is no established mathematical theory for eventbased schlieren techniques. (ii) Data. Event cameras sense only increments of schlieren as opposed to the larger differences with respect to a reference in frame-based BOS. (iii) Methodology. The origin of events in BOS (flickering because they happen only at the edges of the background pattern) and large amounts of noise are novel and difficult for previous work in event-based vision. (iv) Evaluation. The real-world ground truth of the air density is not easy to obtain, hence we need some proxy ground truth and baselines.\nFirst, we develop a theoretical connection between the schlieren and events, showing that event cameras can sense the inhomogeneities of transparent media in a more direct way (as flickering events) compared to frame-based cameras. Such direct sensing of schlieren through event data enables us to observe air convection at high speed more precisely and under challenging lighting conditions. Second, we propose a novel method that extends the linearized event generation model with physically-inspired parameterization to estimate the temporal density fluctuation due to the schlieren. Third, in order to evaluate the estimated density change, whose real-world ground truth is not easy to obtain, we establish the evaluation method using optical flow, by revealing the theoretical connection between the temporal density change and optical flow (i.e., pixel displacement). Using a co-located frame-based camera enables us to benchmark different methods of estimating temporal density change as a computer vision problem. The experimental results show that: (i) our proposed method recovers the flow that corresponds to the temporal change of density gradient by comparing with the standard framebased methods and other baseline methods, (ii) flickeringlike events are a more direct measurement of such schlieren, (iii) event cameras record the density inhomogeneities even in poor lighting conditions, which state-of-the-art framebased algorithms cannot provide, and (iv) the high temporal resolution of event cameras enables slow-motion schlieren analysis.\nThe main technical contributions of this work are:\n\u2022 A novel method for computation of schlieren combining events and frames (Secs. 3 and 4). The proposed method is rigorously obtained and well connected with the physical model of the sensors involved via the linearized event generation model. \u2022 The first schlieren event-frames dataset (Sec. 5). We publicly provide recordings of several schlieren scenes by means of events and frames, at high resolution (1 Mpixel), accurately synchronized and calibrated using an in-house acquisition system. \u2022 A thorough comparison with baseline methods despite the lack of truly ground truth data in this type of turbulent fluid dynamics phenomena (Sec. 6). \u2022 A simulation experiment based on the high-speed frame data from [3] to confirm the results of event-based BOS using those from frame-based BOS (Sec. 7).\nTo the best of our knowledge, this is the first work showing the potential advantages of event cameras for schlieren imaging applications."
        },
        {
            "heading": "2 RELATED WORK",
            "text": ""
        },
        {
            "heading": "2.1 Background-Oriented Schlieren",
            "text": "Schlieren photography was invented in 1864 to study the flow of air around objects moving at supersonic speed [1]. In contrast to other imaging and velocimetry techniques such as particle image velocimetry [7], [8], it does not require any particle seeding in the media of interest. Among different schlieren-imaging techniques (see Tab. 1), BOS is a relatively recent technique since it utilizes digital image processing [9]. In BOS (Fig. 2), an object of interest with density variations (e.g., the hot air stream from a burning candle) is placed between the camera and a constant (non-moving) background pattern. The schlieren generates complex deformation to the background pattern, which is observed by cameras as the apparent motion of the background pattern with respect to a reference image (without density variations) [9]. Different methods have been proposed to compute the displacement vector field of the apparent motion, such as using crosscorrelation [10], optical flow [11], or wavelet-based analysis [12]. As equally important as the data processing method is the data acquisition setup. Best practices for parameter settings, such as the distance from the camera to the background and the media, are provided in [2].\nBOS has been used to image various transparent media, such as shock waves from explosions [16], turbulent flows [3], and shock waves underwater [17]. Also, the background pattern of BOS can be extended to natural images [18], which allows us to image the flow with a large field-ofview (FOV). In [19], BOS is utilized to visualize supersonic jets in flight, by leveraging the natural vegetation of the terrain seen from above as the constant background pattern. The large FOV is one of the unique characteristics of BOS, unlike other schlieren techniques, which enables measuring\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n3 natural outdoor scenes [2]. Notwithstanding, BOS can be used as input to other analysis tools, such as Dynamic Mode Decomposition (DMD) to reveal the main frequency modes of variation of the signal in space and time [20], which ultimately inform about the physical parameters of the turbulent flow. Recently, some works have extended BOS from an imaging technique to a quantitative method, e.g., [21] measures density of axisymmetric supersonic flow. In [3], a method is proposed to extract velocity data from flows. For this application, Kymography works better than classical image correlation, and the self-similarity of round turbulent jet velocity appears in the schlieren results."
        },
        {
            "heading": "2.2 Event Cameras",
            "text": "Event cameras are a relatively new technology compared to BOS imaging with standard frame-based cameras. Since the 2008 seminal work [4], they have been slowly commercialized and explored in computer vision and robotics for various applications. Event cameras naturally respond to motion in the scene at high speed and HDR in a dataefficient manner, hence large progress has been made in motion-related tasks, such as optical flow estimation [22]\u2013 [26], ego-motion estimation [27]\u2013[30], SLAM [31]\u2013[34], or video deblurring and frame interpolation [35]\u2013[37].\nOnly recently, the larger spatial resolution of event cameras and higher fill factor of their pixels [5], [38] has enabled fine-detail applications that were not possible with older models. Some works have explored event cameras for detecting small changes in the scene. These include vibration monitoring [39], particle-image velocimetry [40], and time-resolved 3D fluid flow reconstruction via collimated illumination [41]. These works open another stack of event camera applications in the field of fluid dynamics. Eventbased BOS aims at pushing the limits, by imaging and quantifying flow fields without any particle seeding. Event cameras are available from several manufacturers, costing in the range of 2000 to 6000 USD (as of 2023). We refer to [6], [42] for more details on the different camera types and manufacturers.\nPhysics-based Methods. The method developed in this work is related to a body of literature that leverages the physics of the camera (event generation model) in an optimization framework to either estimate some motion parameters (e.g., feature tracks [43], camera ego-motion [34], [44], [45], optical flow with additionally-provided frame information [46]), and/or a grayscale image given the motion [46]\u2013[48]. Our work builds on top of the event generation model of the camera [6], extending it to the considered BOS problem. Specifically, we extend [43], which was designed for sparse patches around keypoints undergoing Euclidean in-plane motion, to the case of dense flow fields caused by complex (i.e., non-rigid) schlieren."
        },
        {
            "heading": "3 EVENT-BASED SCHLIEREN",
            "text": ""
        },
        {
            "heading": "3.1 Principles of Frame-based BOS",
            "text": "In frame-based BOS the schlieren object S (e.g., a gas with varying density) produces an apparent displacement of the background pattern, which is measured with respect to the initial state (i.e., image acquired in the absence of density\n!!\"# !$ !%\nFrame-based BOS\nEvent-based BOS\n\u0394#(!!\"#, !$)\n\u0394#(!!\"#, !%)\n'(#)\n\u0394'\n!\"\n#\nFig. 3: Frame-based BOS and event-based BOS.\ngradient). The displacement \u2206x .= (\u2206x,\u2206y)\u22a4 is directly related to the small deflection angle \u03f5 .= (\u03f5x, \u03f5y)\u22a4 (Fig. 2) via the distance from lens to S (ZA), the distance from S to the background (ZD), and the focal length of the lens f [14]:\n\u2206x \u2248 f (\nZD ZD + ZA \u2212 f\n) \u03f5. (1)\nOn the other hand, for the refractive index n, the angle \u03f5 is the result of aggregating the spatial gradient \u2202n/\u2202x along the length Z of the schlieren object S on the optical axis:\n\u03f5 = 1\nn\n\u222b \u2202n\n\u2202x dz =\nZ\nn\u221e\n\u2202n \u2202x , (2)\nwhere the ambient-air refractive index is given as n\u221e. Finally, n is related to the density \u03c1 of the gas (schlieren object) via the Gladstone-Dale relation, n = G\u03c1 + 1, with constant G = 2.23\u00d7 10\u22124m3/kg [14].\nIn short, the spatial gradient of the density \u2202\u03c1/\u2202x within a gas causing schlieren can be directly quantified by measuring the pixel displacement \u2206x:\n\u2206x \u221d \u2202\u03c1 \u2202x , (3)\nas summarized in Tab. 1. Here, the displacement is measured against the initial state (the background pattern), hence the corresponding density-gradient field is the change with respect to the initial (also called \u201creference\u201d) state."
        },
        {
            "heading": "3.2 Principles of Event-based BOS",
            "text": "One of the main differences between frame-based BOS and event-based BOS is that event cameras only sense temporal changes of the scene, while the former measures the displacement between a reference frame and the current frame (Fig. 3). Hence, the key challenge is how we can relate events (the asynchronous intensity changes between two timestamps t1 and t2) to the density \u03c1. Since events are very noisy [6], [49], accumulating the differences between far away timestamps to estimate the same displacement as frame-based BOS (3) leads to high noise levels [50], [51], which makes it difficult to estimate this displacement with events.\nIn order to establish the theoretical connection between schlieren and events, let us first extend the previous framebased BOS theory to compute the displacement between\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n4 Event Camera\nConventional Industry Camera\nBeamsplitter\nLight\n(a) Recording system (b) Events and frame\nFig. 4: (a) Actual synchronized data recording system, combining an event camera and a frame-based camera via a beamsplitter (Sec. 5.1). (b) Data: events (red and blue, colored according to polarity) during a short time window overlaid on a grayscale frame (a 100 \u00d7 100 pixel region for better visualization).\ntwo nearby timestamps. Given frames at timestamps t1, t2, their displacements from a reference frame at tref (1) are \u2206x(tref, t1) and \u2206x(tref, t2). The optical flow v(x) = \u2202x/\u2202t between consecutive frames for small \u2206t = t2 \u2212 t1 is\nv(x) = \u2206x(tref, t2)\u2212\u2206x(tref, t1)\n\u2206t . (4)\nFrom the frame-based BOS theory, the displacement at each timestamp can be related to the density gradient as follows (3):\n\u2206x(tref, t1) \u221d \u2202\u03c1t1 \u2202x , \u2206x(tref, t2) \u221d \u2202\u03c1t2 \u2202x .\n(5)\nPlugging (5) into (4), using finite-difference approximations and Schwarz\u2019s theorem, gives:\nv(x) \u221d 1 \u2206t (\u2202\u03c1t2 \u2202x \u2212 \u2202\u03c1t1 \u2202x ) = 1\n\u2206t\n\u2202\n\u2202x\n( \u03c1t2 \u2212 \u03c1t1 ) \u2248 \u2202\n\u2202x\n\u2202 \u2202t \u03c1,\n= \u2202\n\u2202t\n\u2202\n\u2202x \u03c1. (Schwarz\u2019s thm)\n(6)\nThat is, the optical flow between two nearby timestamps is related to the temporal derivative of the density gradient (see the last row of Tab. 1). Since events are the measurements between such nearby timestamps, the key question is how can optical flow (i.e., spatio-temporal derivative of the density) be estimated from event data."
        },
        {
            "heading": "4 ESTIMATION METHOD",
            "text": "One of the main challenges of event-based BOS is its data modality: events generated by schlieren objects are sparse, happening only at the edges of the background pattern (Fig. 4b) and in a flickering form. Previous event-based optical flow estimation methods [23]\u2013[25] often assume a continuous, non-flickering apparent motion of the visual patterns on the image plane. Also, events triggered during the short time interval needed to capture fine details of the complex motion patterns are few compared to those in\nscenes from typical optical flow benchmarks [52], [53]. Consequently prior methods fail to produce accurate flow since they are not tailored to the schlieren scenario, as we show in Sec. 6.2. Due to these challenges, we propose a method that combines events and knowledge of the background pattern (e.g., frames) to estimate the flow. The proposed method extends the linearized event generation model (LEGM) [34], [43], [45], [46] to the characteristics of our problem. The overall pipeline is described in Fig. 5."
        },
        {
            "heading": "4.1 Event Generation Model",
            "text": "An event ek . = (xk, tk, pk) conveys that the logarithmic brightness L at pixel xk changes by a specified contrast sensitivity C [4], [6]:\n\u2206L(xk, tk) . = L(xk, tk)\u2212 L(xk, tk \u2212\u2206tk) = pk C, (7)\nwhere polarity pk \u2208 {+1,\u22121} is the sign of the brightness change, and \u2206tk is the time since the last event at pixel xk. Given a set of events E .= {ek}Nek=1, summing their polarities pixelwise produces a brightness increment image:\n\u2206L(x) = \u2211 k pkC \u03b4(x\u2212 xk), (8)\nwhere the Kronecker \u03b4 selects the pixel xk. The LEGM states that, assuming brightness constancy during a small \u2206t = tNe \u2212 t1, the increment (7) is caused by brightness gradients \u2207L moving with image velocity v [44]:\n\u2206L(x) \u2248 \u2212\u2207L(x) \u00b7\u2206x = \u2212\u2207L(x) \u00b7 v(x)\u2206t. (9)"
        },
        {
            "heading": "4.2 Optimization Objective",
            "text": "We cast the problem of estimating the displacement (6) as an optimization one, where we minimize the mismatch between the event data (in the form of (8)) and its prediction \u2206L\u0302 via (9) exploiting the knowledge of the background pattern from a frame L\u0302. This idea is summarized in Fig. 5.\nTo allow for the fact that L\u0302 may not be perfectly aligned with the corresponding events, we augment the model (9) with a translation warp L\u0302(W(x;p)), where W(x;p) = x+ p, and p denotes a small per-pixel translation.\nOur composite objective (i.e., loss) function implies a joint optimization over the flow and alignment parameters:\nE(v,p) . = Edata(v,p; E) + Ereg(v,p; E). (10)\nThe data-fidelity term measures the goodness of fit between the event data E and its prediction with our model:\nEdata . = \u2225\u2225\u2225\u2225\u2225 \u2206L\u0302\u2225\u2206L\u0302\u22252 (x)\u2212 \u2206L\u2225\u2206L\u22252 (x) \u2225\u2225\u2225\u2225\u2225 \u03b3 , (11)\nwhere \u03b3 is the L1 norm (robust norm). Since C in (8) is unknown, we compute the difference between normalized brightness increments (norms are over the pixel domain \u2126).\nThe regularizer penalizes the non-smoothness of the flow v and the magnitude of the per-pixel translation p:\nEreg . = \u03bb1\u2225w(x)\u2207v(q(x))\u22251 + \u03bb2\u2225p(x)\u22251. (12)\nThe flow regularizer (first term in (12)) is explained in Sec. 4.4, after the flow parameterization is introduced. For the second term, the magnitude of p is given by its L1 norm over the pixel domain. In the experiments, we set \u03bb1 = 0.5 and \u03bb2 = 0.1.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n5 Frame\nPredicted \u0394L\nWarp Dot Product\nMeasured \u0394L\nSpatial gradient\nSpatial gradient\nEvents\np q\n\u0394L error\nGaussian blur\nNormalize\nNormalize\u222b dt\nFlow v\nFig. 5: Block diagram of the objective Edata in (11). On the top branch, events are integrated in time using (8) and smoothed with a Gaussian kernel (\u03c3 = 2 px) to produce the measured brightness increment image \u2206L. The bottom branch shows how to compute the predicted brightness increment \u2206L\u0302 from the frame and the problem unknowns: the translation field p and the Poisson parameters of the flow, q. The flow v and p are pseudo-colored (see color wheel). Same data as Fig. 4."
        },
        {
            "heading": "4.3 Physically-motivated Parameterization",
            "text": "Swapping the mixed derivatives (Schwarz\u2019s theorem) in (6), the flow v \u223c \u2202\u2202x \u2202\u03c1 \u2202t is interpreted as the spatial gradient of \u2202\u03c1 \u2202t . Thus (6) admits two interpretations. (i) from left to right: once estimated, the flow may be Poisson-integrated [54] to obtain \u2202\u03c1\u2202t , (as the best L\n2 fit to the estimated flow [48], [55]). (ii) from right to left: the flow may be obtained as the spatial (e.g., Sobel) gradient of a scalar field \u2202\u03c1\u2202t . In contrast to most optical flow estimation methods, which parametrize v(x) directly in terms of its x and y components, we go one step further and exploit the above second interpretation of (6) to parametrize the flow by means of q \u2261 \u2202\u03c1\u2202t , which we call the Poisson parameters of the flow. This not only reduces the complexity of the problem (number of variables being optimized), thus conferring robustness, but also provides a strong link with the physical meaning of the variables: according to (6), the resulting flow actually represents the schlieren objects. Figure 6 shows examples of the Poisson parameters q.\nFigure 5 summarizes the visual quantities involved in the calculation of (11). The candidate scalar parameter field q is converted (via Sobel operator) into the vector flow field v. The flow v and translation field p are used in the augmented model of (9) to generate a predicted (i.e., modeled) brightness increment image. On the other hand, events E are summed in (8) and Gaussian-smoothed to produce a measured brightness increment image. The difference between the measured and predicted brightness increments provides an error signal that is used to drive the iterative refinement of the unknown variables p and q."
        },
        {
            "heading": "4.4 Flow Regularizer",
            "text": "We penalize the non-smoothness of the flow using a weighted Total Variation (TV) (see (12)). As illustrated in Fig. 4b, it is difficult to estimate accurate flow in regions with very few events, which correspond to constant (e.g., zero) flow, hence we impose this prior knowledge as a regularizer to encourage zero flow therein. Specifically, from the events E we compute a Gaussian-smoothed histogram h(x; E) = \u2211 k N (x;xk, \u03c32) (with \u03c3 = 5 px) and normalize it to the range [0, 1]. Then, we define weight function\nw(x) . = 1 \u2212 \u03b1/h(x; E)(x) (large in ill-posed regions with very few events), with \u03b1 = 0.95 in the experiments."
        },
        {
            "heading": "4.5 Optimization",
            "text": "Multi-scale. For improved convergence, a coarse-to-fine patch-based approach is used for v,p and the loss function (10). The coarsest patch size is 64 \u00d7 64 px and we use four resolution levels in a pyramidal fashion, resulting in finest patches of 8 \u00d7 8 px. To reach pixel density from the finest patches, we use bilinear interpolation.\nImplementation. We use events in the fixed time interval (i.e., 120 fps) for optimization across all sequences. As an optimizer, we use Adam [56] with 600 iterations. The learning rate is set to 0.05, with the decay of 0.1. The initialization of the first frame at the coarsest scale is: zero for p and v (when applicable) and random in [\u22121, 1] for the Poisson parameters q. We found the latter to be better than also setting q to zero. Then, the initialization of the next levels uses the optimization results from the previous scales (i.e., coarse-to-fine approach)."
        },
        {
            "heading": "5 PHYSICAL SETUP AND DATA",
            "text": ""
        },
        {
            "heading": "5.1 Recording Setup",
            "text": "Co-capture System. To achieve high-quality recordings of frames and events, we build our own acquisition system. Although some devices exist that record colocated events and frames (such as DAVIS [57], [58]), their data quality (resolution, dynamic range, etc.) is limited and not suitable for BOS applications. Our custom-built co-capture system consists of a frame camera (Basler acA1300-200um, 1280\u00d71024 px) and the latest generation event camera (Prophesee EVK3 Gen4, 1280\u00d7720 px [5]), sharing the same optical axis by using a beamsplitter (Plate Bs C-Mount VIS50R/50T). Both cameras are hardware-triggered for accurate synchronization and are calibrated to achieve accurate pixel alignment, following [59]. Figure 4 shows the camera system and an example of acquired data. Further details about the used recording system can be found in [60].\nOptical Setup. The field of view (FOV) of our cameras is limited by the beamsplitter (\u2248 15\u25e6), hence we set the distance between the cameras and the background to 3.3 m.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n6\nWe use randomly-generated background patterns that cover the whole FOV, where black dots (covering approximately 2 to 3 px in the image plane) are printed on a white paper.\nThe data quality also depends on the distance between the camera and the schlieren object. The schlieren are more visible (larger pixel displacement \u2206x) by keeping ZA small (object closer to the camera). At the same time, the camera system has to be focused both on the background pattern and the schlieren object, thus ZA cannot be too small. We experimentally found distance ZD = 1.6 m to be a good compromise between both opposing effects. To control the scene brightness and achieve uniform illumination in the background, we use LED panels (four Eurolite LED PLL360). This illumination allows us to lower the aperture to an f-number of 10, leading to a higher depth of field. Note that our beamsplitter setup leads to a 50% split of the light reaching each camera of the acquisition system."
        },
        {
            "heading": "5.2 Data Acquired",
            "text": "We record multiple sequences with natural and non-natural (forced) air convection, which are summarized in Tab. 2. For natural convection, we use heat sources, such as a hot plate, a hair dryer (switched off), and ice. To demonstrate the HDR capabilities of event cameras, we record the data in (i) bright conditions (\u2248 4000 lx) and (ii) low-light conditions (\u2248 225 lx). The low-light condition is set to be darker than normal office lighting, which is a more natural condition for real-world applications.\nEach sequence is approximately 10 to 20 seconds long and consists of events, frames and a calibration parameter file. The recording starts with the scene in the absence of the schlieren object, which is useful for frame-based BOS methods (reference frame). All sequences are recorded at normal room temperature (\u2248 24\u25e6C). For the forced convection sequence of the running hair dryer, we set the event camera\u2019s refractory period to its minimum possible value to capture the fast dynamics of the airflow. In total, we\nTABLE 2: Parameters of the recorded sequences.\nSequence Convection Luminance Duration Event rate [lx] [s] [Mev/s]\nHot plate 1 Natural 4000 19.4 11.3 Hot plate 2 Natural 225 19.8 5.1 Hair dryer (OFF) 1 Natural 4000 13.5 5.1 Hair dryer (OFF) 2 Natural 4000 19.7 5.3 Hair dryer (OFF) 3 Natural 225 14.7 2.8 Crushed ice Natural 4000 17.4 5.0 Hair dryer (ON) Forced 4000 13.4 15.0 Breathing 1 Forced 4000 12.8 4.0 Breathing 2 Forced 4000 13.0 3.7\nrecord nine sequences, each of which has up to 200M events. Regarding storage, events take about 1/10 of the data size required to store frames (e.g., 800 MB vs. 7.8 GB for a hot plate sequence).\nFrames of sample sequences are shown in Fig. 7. Each frame is mapped from its original resolution (1280\u00d71024 px) to the event-camera resolution (1280\u00d7720 px) (see Sec. 5.1).\nSince we cannot obtain real ground truth (GT), we use frame-based estimated flow as GT flow (Fig. 7). The calculation of the flow is based on the classical Farneba\u0308ck algorithm [61] with four pyramidal scales at the frame rate (120 fps). We test different parameters and find no significant dif-\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n7\nference on the quality of the results. Before settling for Farneba\u0308ck\u2019s algorithm, we tested recent DNN-based stateof-the-art methods, such as [62], [63], and found that they do not produce reasonable flow. Figure 8 shows the comparison of several frame-based optical flow estimation methods: two state-of-the-art optical flow and video-frame interpolation works [62], [63] and Farneba\u0308ck\u2019s method. Due to the large gap between the training datasets of [62], [63] and our dataset, these recent DNN-based methods fail to estimate reasonable flow. Farneba\u0308ck\u2019s algorithm works robustly and better, because (i) the background pattern is parallel to the image plane, (ii) the scene has no occlusions, (iii) the background pattern has clear and random edges that are useful to calculate the deformation between two frames. Since we cannot determine the real GT, we do not explore a further analysis of frame-based estimation methods, which we leave for future research, such as simulation. That is, to establish the first event-based BOS problem settings we leverage the knowledge of established frame-based BOS techniques. Note that the quantitative evaluation is only based on the well-illuminated sequences, since the framebased flow degrades in dark scenes (see Sec. 6.6). We publish the dataset and the code to compute the GT."
        },
        {
            "heading": "6 EXPERIMENTAL EVALUATION",
            "text": "This section reports the performance of the proposed estimation method and its properties. First, we explain the baseline methods and evaluation metrics (Sec. 6.1). Second, we benchmark the accuracy of all methods considered (Sec. 6.2). Third, we show the capabilities of our method in low-light conditions (Sec. 6.3) and how it achieves high temporal resolution (1200 Hz \u201cslow motion\u201d) in Sec. 6.4, including a velocimetry application (Sec. 6.5). Finally, we analyze the proposed method further, especially regarding the dependency on frames (Secs. 6.6 and 6.7), its sensitivity\nTABLE 3: Details of the benchmark. \u201cROI position\u201d contains the coordinates of the top-left corner.\nSequence ROI size [px] ROI position [px] Duration [s] Total events\nHot plate 1 640\u00d7720 [320, 0] 10 to 14 51 900 802 Hot plate 2 (dark) 640\u00d7720 [420, 0] 12 to 14 12 912 262 Hair dryer (OFF) 1 640\u00d7640 [320, 0] 4 to 7 13 498 252 Hair dryer (OFF) 2 512\u00d7640 [384, 0] 6 to 7 4 089 883 Hair dryer (dark) 512\u00d7640 [384, 0] 5 to 7 3 460 579 Crushed ice 512\u00d7512 [384, 208] 8 to 11 5 856 190 Hair dryer (ON) 1280\u00d7200 [0, 260] 3.3 to 4.3 17 860 129 Breathing 1 590\u00d7600 [400, 0] 4.36 to 5.5 2 783 122 Breathing 2 640\u00d7640 [447, 0] 2.5 to 3.5 1 811 889\nTotal \u2013 \u2013 18.14 114 173 108\nto hyper-parameters (Sec. 6.8), and the effect of event warping (Sec. 6.9)."
        },
        {
            "heading": "6.1 Evaluation Metrics and Baseline Methods",
            "text": "Evaluation Metrics. We evaluate the proposed method in terms of optical flow v accuracy. Two variants of the method are assessed: (i) using q as parameterization, from which we obtain v afterwards via (6), and (ii) using v directly.\nThe optical flow evaluation metrics are the average endpoint error (AEE), the percentage of pixels with AEE > 1 px (denoted by \u201c% Out\u201d), and the angular error (AE). We select the time interval (from 1 to 4 s) and region of interest (ROI) to remove objects, such as a hair dryer and a face from the scene. All metrics are computed over pixels with at least one event inside the ROI.\nTable 3 reports the detailed duration, ROI, and total number of events used for the benchmark. The duration is selected such that the quality of schlieren is the best and stable. For the \u201cHair dryer (ON)\u201d sequence, we limit the height of the ROI due to extremely large number of events observed: otherwise, we set the ROI to have approximately 720\u00d7512 px.\nBaselines. As baseline flow estimators we use the two self-implemented methods from events because, to the best of our knowledge, there are no methods that estimate schlieren flow from event camera data.\n\u2022 The Multi-reference Contrast Maximization (MCM) [25] is a state-of-the-art optical flow estimation algorithm from events alone. It is a model-based method, hence there is no mismatch in the training dataset (due to our specific background pattern). We use the events between two consecutive frames (i.e., in a time span of 8.3 ms). \u2022 Flow estimation from reconstructed intensity images: we use E2VID [64] (a learning-based approach) to compute grayscale images from events and then apply the same (frame-based) optical estimator as the one for the GT. Images are reconstructed at 120 fps, i.e., the same frequency as the frames.\nTo the best of our knowledge, we found no methods with publicly-available implementation combining events and frames to estimate the optical flow, we therefore believe this is a best-effort comparison. Also, notice that we do not train a Deep Neural Network (DNN) model with the supervisory GT flow, as the purpose of the paper is not a purely datadriven approach, but to develop an interpretable modelbased method, by deriving a connection between the physical parameters and the data.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n8"
        },
        {
            "heading": "6.2 Optical Flow Evaluation",
            "text": "Flow accuracy is reported in Tab. 4. We evaluate on illuminated sequences for valid GT flows from frames (please see Sec. 6.3 for the dark sequences). Consistently for almost all sequences, the proposed method (\u201cOurs (Poisson)\u201d) provides the best accuracy compared with the baseline methods. Due to the nature of schlieren, the GT flow magnitude has normally subpixel values. Hence, we find that the angular error (AE) is a more reliable metric for the purpose of this benchmark. The largest magnitude of the displacement (\u2248 3 px) is observed in the hotplate sequences. Still, it is remarkable that the proposed method achieves AEE < 1 pixel. We acknowledge that the proposed method utilizes both event and frame data, while the baselines use only event data as input. This is further discussed in Sec. 6.6.\nAlso, it is noticeable that the Poisson-parameterized estimation (\u201cOurs (Poisson)\u201d) results in better accuracy than the flow-parameterized estimation (\u201cOurs (Flow)\u201d). This clearly states the effectiveness of our physically-motivated parameterization. It provides not only a smaller number of parameters, as discussed in Sec. 4, but also contributes with better accuracy.\nAdditionally, we observe that the forced convection usually has a smaller displacement magnitude than natural convection. This is because the optical flow v, which we evaluate on, is the temporal derivative of the density gradient. In the forced convection case (e.g., hair dryer (ON)), the spatio-temporal changes of the air density at a pixel might be smaller than in the natural, heat-induced schlieren, since the advection of the flow is dominant, which can be seen as nearly constant.\nFigure 9 shows qualitative results. Although the GT flow is based on a classical, general-purpose estimation method, it provides remarkably reasonable flow. The baseline methods (MCM and E2VID) fail to estimate reasonable flow from events. Especially, we find the alignment-based method [25] fails to estimate schlieren flow. This is because most events are generated at the edges of the background pattern, resulting in an uneven spatial distribution despite air being actually moving, and consequently, triggering more flickering events. The E2VID-based method surprisingly reconstructs edge structures of the background pattern (see also Sec. 6.7) in spite of this specific (flickering) event input, and estimates comparable flow. However, it fails to recover the fine structure of the flow. Finally, the flow estimated by our method resembles the GT flow the most, and it even seems to capture more fine-scale (high-frequency) structures."
        },
        {
            "heading": "6.3 HDR Experiment",
            "text": "So far we have established that the proposed method is able to recover the fine flow structure of the schlieren object. However, schlieren based on events has another interesting aspect: as shown on the left column of Fig. 9, the existence of schlieren is already visible in the event data histogram. By contrast, the schlieren structure is not visible to the naked eye on the raw frame data but only as the result of optical flow processing. The fact, that schlieren is observable in a more direct way using events, allows us to leverage the advantages of the event camera itself, such as HDR and high temporal resolution.\nFigure 10 shows qualitative results of the frame-based and event-based schlieren imaging under poor illumination. The frame-based schlieren method fails to estimate realistic flow under such conditions, as it needs intense lighting sources, especially if high-speed cameras are used. Due to the insufficient brightness, the quality of the frames collapses even after normalization (i.e., using the entire grayscale range). On the other hand, the event data captures the schlieren structure (Fig. 10, top right). Furthermore, the proposed algorithm combining events and frames is surprisingly robust against such low-quality image inputs. Using natural light (225 lx) the result (Fig. 10, bottom right) shows the potential of event cameras to push the limits for future BOS applications. We further discuss the effect of the amount of illumination in Sec. 6.6."
        },
        {
            "heading": "6.4 Super-Slow Motion",
            "text": "Event-based BOS also enables us to see the schlieren at markedly higher temporal resolution (i.e., slow motion) than conventional frames. To this end, we conduct a streakschlieren analysis [3]. The streak analysis focuses on a single column of the schlieren image to see how it evolves in time, by showing an x \u2212 t diagram (kymogram) of the air convection. The frame-based schlieren method uses for example Poisson images as schlieren images. For event-based methods, schlieren images can be either Poisson images or simply event histograms. Figure 11 shows a comparison of kymograms obtained from frames at 120 Hz (the frame rate) and obtained from events (10\u00d7 higher rate, i.e., at 1200 Hz). Event-based BOS can provide high temporal resolution kymograms due to the asynchronous nature of event data. Compared with the frame-based analysis (Fig. 11a), the event-based one (Fig. 11b) shows thinner lines of schlieren in space-time. The slow motion schlieren visualization is best viewed in the supplementary video."
        },
        {
            "heading": "6.5 Velocimetry",
            "text": "One can perform velocimetry by fitting curves to the kymograms [3]. Let us analyse the speed of propagation of schlieren (\u2202\u03c1/\u2202t in the case of Poisson image) along one direction (e.g., vertical). Figure 11b shows an example on the hot plate sequence. By fitting a curve (line), the flow propagates 166 px during approximately 68.8 milliseconds. The geometry of the BOS setup (focal length f = 25mm, distance to object ZA = 1.7m, pixel size 4.86\u00b5m) leads to an approximate velocity of 0.805 m/s."
        },
        {
            "heading": "6.6 Dependency on Frames",
            "text": "The proposed method uses events and frames. Naturally, the question arises to which extent the algorithm relies on which signal. To this end, we present the ablation study with different brightness levels (see also Sec. 6.3). Figure 12 shows the qualitative results for both: frame-based method and our method (frame plus events), for different illumination levels (measured with a Voltcraft MS-1300 light meter). As clearly shown, the frame-based flow (column (b)) starts to deteriorate when the illumination is 1000 lx or smaller. For a better performance, we even normalize the range of the frames used (the exposure time is fixed to maintain the frame rate\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n9\nTABLE 4: Results of optical flow estimation.\nHair dryer (OFF) 1 Hair dryer (OFF) 2 Hot plate 1 Hair dryer (ON)\nAEE \u2193 %Out \u2193 AE \u2193 AEE \u2193 %Out \u2193 AE \u2193 AEE \u2193 %Out \u2193 AE \u2193 AEE \u2193 %Out \u2193 AE \u2193\nMCM [25] 1.425 35.639 0.621 0.421 10.886 0.476 0.400 21.789 0.426 0.287 5.933 0.712 E2VID [64] 1.055 39.068 0.677 1.091 37.734 0.670 1.092 32.121 0.611 0.811 25.997 0.587 Ours (Flow) 0.675 22.104 0.404 0.688 24.930 0.448 0.810 30.289 0.544 0.310 6.756 0.258 Ours (Poisson) 0.383 9.319 0.299 0.395 10.174 0.337 0.487 12.215 0.421 0.215 0.924 0.202\nCrushed ice Breathing 1 Breathing 2\nMCM [25] 1.090 96.964 0.823 1.769 49.552 0.853 2.056 78.690 0.973 E2VID [64] 1.249 55.030 0.791 1.014 42.072 0.692 1.056 43.348 0.699 Ours (Flow) 0.587 21.815 0.452 0.665 11.872 0.341 0.557 17.716 0.438 Ours (Poisson) 0.326 5.177 0.301 0.345 6.322 0.203 0.476 8.028 0.410\nH ot\npl at e H ot pl at e\nH ai\nr dr\nye r\nH ai\nr dr\nye r\nBr ea\nth in\ng\n(a) Input events (b) GT (Flow from frames) (c) MCM [25] (events only) (d) Flow from E2VID frames (e) Ours\nFig. 9: Qualitative comparison between different flow estimation methods.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n10\nof 120 fps). However, this does not provide significantly better results that can compete with those of our method. By contrast, the following two points are remarkable about our method: (i) schlieren is still visible at 110 lx in the event histograms, indicating the HDR capabilities of the noisy input data (column (c)), and (ii) the estimated flow (column (d)) still looks reasonable when the illumination is as low as 225 lx, despite our method using the naturally darker frame as an input (column (a)). Note that our method does not work when the frame is completely black (less than 50 lx). All the above indicates that the proposed method requires frames, but it can overcome the limited dynamic range of\nthe frames due to the HDR advantages of event cameras."
        },
        {
            "heading": "6.7 Towards a Frame-Free Method",
            "text": "The proposed method utilizes the information from events and a frame, however the quality of the frame data does not need to be the best, as shown in the previous section. Hence, an interesting challenge is to replace frame data with intensity reconstruction from events, such that the proposed method could be extended to be frame-free. To this end, Figure 13 shows the comparison of the different input frames. Instead of using an acquired frame as an input to the proposed method, we reconstruct intensity images using E2VID [64] and feed them as input. Despite the large visual difference between the two different inputs, the output flow and Poisson images seem to have similar structures. Although we do not further investigate the quality of the intensity reconstruction, the results show future possible extension towards frame-free event-based BOS methods."
        },
        {
            "heading": "6.8 Effect of the Regularizers and the Translation Field",
            "text": "Ablation. To assess the importance of the regularization and the translation field parameters p, we conduct an ablation study. The top half of Tab. 5 reports optical flow accuracy of the proposed method, the one without regularization, and the one without the translation. There is a significant improvement due to the regularizers: without regularizers, the estimated q and p become not smooth anymore, which leads to irregular flow estimation. The effect of p is relatively minor, but still noticeable.\nSensitivity Analysis. We test different weights for each regularizer \u03bb1, \u03bb2 in (12). The weights are set as follows: we fix one parameter (\u03bb1 = 0.5), and vary \u03bb2 between 0.01 and 1.0; then we fix the other parameter (\u03bb2 = 0.1) and vary \u03bb1 between 0.05 and 1.0. The flow accuracy is reported in the bottom half of Tab. 5. We observe on-par accuracy when \u03bb1 = 1.0 with respect to the base condition (the top row)."
        },
        {
            "heading": "6.9 Effect of Event Warping",
            "text": "In some of the previous event-based flow estimation methods [47], [48], warping the events using the estimated optical flow produces sharp intermediate images that improve convergence (e.g., of the image reconstruction task). We test the possible efficacy of such an event warping in the BOS setting. As shown in Fig. 14, warping events with the estimated flow does not have a sharpening effect in schlieren. This is because: (i) event warping leads to sharpening if events are generated by moving edges (e.g., [25]), which does not hold true in BOS (see also the large errors of the \u201cMCM\u201d method in Fig. 9 and Tab. 4), and (ii) the time window between consecutive frames (e.g., at 120 fps) is small enough to already produce sharp brightness increment images. Hence, we do not examine further sharpness/deblur-based approaches."
        },
        {
            "heading": "7 STUDIES ON A HELIUM JET EXPERIMENT",
            "text": "The theoretical connection between events and schlieren that we establish in (6) highlights an important difference in the experimental settings between frame-based and\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n11\n40 00 lx 20 00 lx\n10 00 lx 50 0 lx\n22 5 lx 11 0 lx\n< 50\nlx\n(a) Frames (b) Flow from frames (c) Events (d) Ours\nFig. 12: Ablation study for different illumination levels. Flow (b) uses normalized frames of the original ones (a) as input, while our method (d) uses events (c) and the original frames. The frame-based flow deteriorates at around 500 lx, while events and the estimated flow capture the schlieren even at illumination levels as low as 225 lx and 110 lx.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n12\nTABLE 5: Effect of the regularizers and the translation field.\nHot plate Crushed Ice Dryer\nAEE \u2193 %Out \u2193 AE \u2193 AEE \u2193 %Out \u2193 AE \u2193 AEE \u2193 %Out \u2193 AE \u2193\nOurs (\u03bb1 = 0.5, \u03bb2 = 0.1) 0.487 12.215 0.421 0.326 5.177 0.301 0.395 10.174 0.337\nw/o regularizers (i.e., \u03bb1 = \u03bb2 = 0) 3.371 82.039 1.111 2.499 76.325 1.017 1.233 48.626 0.756 w/o translation model (i.e., p = 0) 0.591 18.609 0.488 0.368 7.791 0.313 0.394 10.896 0.324\n\u03bb1 = 0.05, \u03bb2 = 0.1 0.586 14.468 0.494 0.518 11.295 0.440 0.449 11.104 0.387 \u03bb1 = 1.0, \u03bb2 = 0.1 0.482 10.462 0.416 0.390 3.849 0.349 0.378 7.274 0.330 \u03bb1 = 0.5, \u03bb2 = 0.01 0.509 11.001 0.440 0.437 5.482 0.386 0.398 7.129 0.344 \u03bb1 = 0.5, \u03bb2 = 1.0 0.517 11.598 0.443 0.429 5.609 0.379 0.409 8.112 0.350\n(a) Input (b) Output Poisson (c) Output Flow\nFig. 13: Towards a frame-free method. The top row shows the originally proposed method with the frame-based camera input. The bottom row shows an E2VID-reconstructed image as the alternative input. In spite of the large quality difference between the two inputs (a), the output Poisson and flow images have some visual similarities (b,c).\nevent-based BOS: event-based BOS focuses on the temporal derivative of the air density, which requires a timedependent flow. Steady flows that are often used for framebased BOS (e.g., supersonic flow) do not satisfy the time dependency. More precisely, \u201csteady\u201d flow in frame-based BOS means that it can be averaged over multiple frames (i.e., \u201c\u2202\u03c1/\u2202x\u201d in Tab. 1) to improve the signal-to-noise ratio. This difference makes it difficult to apply the experimental knowledge of common frame-based BOS data to the event-based one. Nonetheless, we conduct a simulation experiment to make a stronger connection between these\ncomplementary schlieren methods. To this end, we use a frame-based BOS dataset that is well documented through in experimental evaluation [3]. The dataset consists of highspeed frames (6000 fps) recording a round turbulent helium jet in air, whose flow has self-similarity (axisymmetric) properties. Each sequence has 3000 images, of 1024 \u00d7 512 px and with 167\u00b5s exposure time, i.e., 0.5s of data.\nWe analyze the velocimetry in accordance with the selfsimilarity property using simulated events. The analyzed sequence is the jet with Reynolds number Red = 5, 980. First, we run ESIM [65] to simulate events from the highspeed frames. We use a contrast sensitivity of 0.05 for both positive and negative events to have a reasonable signal-tonoise ratio. Events are shown in Fig. 15a. Then, velocimetry (as in Sec. 6.5) provides the velocity at different pixel locations on the image plane. Instead of manually fitting lines to the kymogram using a drawing software as in [3], we detect lines automatically, as follows: first, we extract patches along the x/d axis of the image plane (e.g., 100\u00d7100 px); then, we smooth the patches with a Gaussian filter and estimate the slope of the dominant direction within each patch iteratively, by rotating the patch and finding the angle that maximizes the magnitude of the rotated patch gradient in a predefined direction.\nFigure 15b shows the self-similarity by analyzing the simulated events. Similarly to [3, Fig. 7], the estimated velocity is symmetric and consistent with the theoretical values (dashed line) along the relative distance using the jet spreading rate. We observe larger error accumulation than the purely frame-based method in [3], which can be attributed, among other factors, to simulation inaccuracies. Figure 15c shows the comparison of the velocity at the center of the jet nozzle along with the distance. The velocity values are similar and reasonably close to the results in [3, Fig. 8]. Moreover, they seem to follow the 1/x-type decay, which agrees with the theoretical model of turbulent flow: The frame-based results in [3, Fig. 8] degrade approximately for x/d \u2264 80, while the simulated event-based results provide a better fit here.\nKymograms are shown in Fig. 16. The curve patterns in the event-based kymogram look consistent with the framebased kymogram in [3, Fig. 9], although they show some artifacts due to the simulation. As mentioned in [3], even 6000 fps is not enough for the pixels close to the nozzle (e.g., x/d < 20, where the flow is very fast. Hence, the event data that is simulated using the frames inherit such a limitation: the kymogram becomes noisier for smaller x/d (closer to the nozzle). The high temporal resolution of event cameras\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n13\nx/ d\nr/r1/2\nnozzle0\n200\n(a) Events\nU/ U 0\nr/r1/2\nexp (\u2212 ' '( )\u2044\u2044 )\n1.41 )\n79 89\n59 69\n99\n(b) Similarity analysis\nU 0 [m\n/s ]\nx/d\n410 0 1\u2044\n(c) Velocity analysis\nFig. 15: Helium jet experiment (image data comes from [3]). (a) Event data (simulated from frames) and geometry of the scene. (b) Similarity analysis of jet radial profiles for simulated events. Different colors indicate different distances from the nozzle, x/d = {59, 69, 79, 89, 99}. (c) Velocimetry at the center of the jet nozzle: the velocity estimated from the simulated events follows the 1/x-type decay. Following the same notation as [3, Fig. 7,8], x/d is the distance normalized by the size of the nozzle (d=1.4mm), r/r1/2 is the radius normalized by the jet half-width, U0 is the estimated velocity profile along the axis of the jet, and U is the estimated velocity (parallel to the jet axis) at each pixel.\ncould help overcome such a rate limit of frame-based BOS."
        },
        {
            "heading": "8 DISCUSSION",
            "text": "Let us summarize some findings of the first event-based BOS technique.\n\u2022 The megapixel race has enabled new applications for event cameras in the field of fluid dynamics. \u2022 Event-based BOS can capture the spatio-temporal derivative of the media density without any particle seeding. \u2022 As event cameras only sense the temporal changes of the scene, they capture incremental changes of schlieren, as opposed to changes with respect to a reference frame.\n\u2022 Schlieren can be observed directly in the form of flickering events, while it is more indirect to perceive in frames. \u2022 Event-based BOS can overcome the limitations of dynamic range and temporal resolution of traditional frame-based BOS. \u2022 Due to the data sparsity, event-based BOS is approximately ten times more data efficient (smaller storage size). However, estimating the density fluctuations is more challenging than using frames. \u2022 The knowledge of the background pattern (frames) can be used to handle the problems of sparsity and noise. \u2022 The proposed method casts the problem into an optical flow estimation one (i.e., in the realm of computer vision). \u2022 State-of-the-art optical flow methods (e.g., DNN) do not work well for schlieren because of the very different data properties (e.g., edge motions vs. complex density fluctuations). \u2022 Directly parameterizing the temporal derivative of the density reduces the complexity of the problem, provides a link with the physical meaning of the variables and yields the most accurate results. \u2022 Event-based BOS enables high-temporal resolution velocimetry analysis using kymograms.\nLimitations. The proposed BOS technique using events shows advantages over frame-based BOS in terms of HDR capabilities and temporal resolution, lowering the demand for bright illumination and high-speed cameras. However, in other aspects, it inherits the limitations of frame-based BOS. Optically, the estimated brightness gradient is a mean value integrated along the optical axis, and the technique inherently has a trade-off between the observed displacement and the obtained sharpness of the gradient under investigation.\nAdditionally BOS is sensitive to vibrations, due to the underlying assumption that the small perceived changes are\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n14\nonly caused by refractive index variations. Specific to event cameras is that the signal is noisy, and careful tuning of the camera\u2019s biases is necessary. The proposed method furthermore relies on a combination of events and frames, thus an accurate spatio-temporal alignment of both data sources is required. The flow estimation method does not run in real time. However, raw events visualized as histograms can be computed online and resemble schlieren images. While the proposed multi-scale approach improves convergence of the optimization, it limits the spatial resolution of the flow, which is a similar limitation as in frame-based BOS.\nSince event-based BOS relies on temporal changes, flows are observed well if they are time-dependent. This property makes it difficult to use steady flows to validate event-based BOS, and hence to evaluate frame- and event-based BOS in strictly the same settings. Also, event-based BOS cannot improve the signal-to-noise ratio like the frame-based one does by averaging multiple frames. Frame-based BOS and event-based BOS have their own strengths, which may be complementary (e.g., frame-based BOS is good for steady flows, whereas event-based BOS is good for time-dependent flows). This could be further investigated in the future."
        },
        {
            "heading": "9 CONCLUSION",
            "text": "We have presented the first event-based BOS imaging and an algorithm to estimate the temporal derivative of the air density gradient. The approach has been mathematically rigorously obtained and has a physically-motivated parametrization. Using the frame-based method as GT the experiments evidenced that our approach outperforms all other tested methods. We furthermore illustrated how the advantages of event cameras could be leveraged for BOS applications, lowering the requirements for high illumination and visualizing the turbulent eddies at a significantly higher temporal resolution. Additional studies on a highspeed schlieren dataset of a Helium jet provided further validation of the method. We release the code and dataset to the public and hope that this research opens up new possibilities for the computer vision community."
        },
        {
            "heading": "ACKNOWLEDGMENT",
            "text": "We would like to thank Prof. Dr. A. Liberzon at Tel Aviv University for useful discussions and advice. We also thank the anonymous reviewers for valuable suggestions. This research was funded by the German Academic Exchange Service (DAAD), Research Grant-Bi-nationally Supervised Doctoral Degrees/Cotutelle, 2021/22 (57552338) and the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany\u2019s Excellence Strategy \u2013 EXC 2002/1 \u201cScience of Intelligence\u201d \u2013 project number 390523135."
        }
    ],
    "title": "Event-based Background-Oriented Schlieren",
    "year": 2023
}