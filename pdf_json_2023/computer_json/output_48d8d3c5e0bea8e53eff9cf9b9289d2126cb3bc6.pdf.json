{
    "abstractText": "Results: In this work, we describe Q\u03b5, a graph convolutional network (GCN) that utilizes a minimal set of atom and residue features as inputs to predict the global distance test total score (GDTTS) and local distance difference test (lDDT) score of a decoy. To improve the model\u2019s performance, we introduce a novel loss function based on the \u03b5-insensitive loss function used for SVM regression. This loss function is specifically designed for evaluating the characteristics of the quality assessment problem and provides predictions with improved accuracy over standard loss functions used for this task. Despite using only aminimal set of features, it matches the performance of recent state-of-the-art methods like DeepUMQA.",
    "authors": [
        {
            "affiliations": [],
            "name": "Soumyadip Roy"
        },
        {
            "affiliations": [],
            "name": "Asa Ben-Hur"
        }
    ],
    "id": "SP:7a3fef51d63275bb6c3fa6a0ac3d255d266e026c",
    "references": [
        {
            "authors": [
                "N. Akhter",
                "G. Chennupati",
                "H. Djidjev",
                "A. Shehu"
            ],
            "title": "Decoy selection for protein structure prediction via extreme gradient boosting and ranking",
            "venue": "BMC Bioinformatics 21, 189. doi:10.1186/s12859-020-3523-9",
            "year": 2020
        },
        {
            "authors": [
                "B. Al-Lazikani",
                "J. Jung",
                "Z. Xiang",
                "B. Honig"
            ],
            "title": "Protein structure prediction",
            "venue": "Curr. Opin. Chem. Biol. 5, 51\u201356. doi:10.1016/s1367-5931(00)00164-2",
            "year": 2001
        },
        {
            "authors": [
                "F. Baldassarre",
                "D. Men\u00e9ndez Hurtado",
                "A. Elofsson",
                "H. Azizpour"
            ],
            "title": "GraphQA: protein model quality assessment using graph convolutional networks",
            "venue": "Bioinformatics 37, 360\u2013366. doi:10.1093/bioinformatics/btaa714",
            "year": 2020
        },
        {
            "authors": [
                "CASP"
            ],
            "title": "CASP",
            "venue": "[Dataset]. Available at: https://predictioncenter.org/download_ area/.",
            "year": 2021
        },
        {
            "authors": [
                "C. Chen",
                "X. Chen",
                "A. Morehead",
                "T. Wu",
                "J. Cheng"
            ],
            "title": "3D-equivariant graph neural networks for protein model quality assessment",
            "venue": "Bioinformatics 39, btad030. doi:10.1093/bioinformatics/btad030",
            "year": 2023
        },
        {
            "authors": [
                "J. Cheng",
                "Choe",
                "M.-H",
                "A. Elofsson",
                "Han",
                "K.-S",
                "J. Hou",
                "Maghrabi",
                "A. H"
            ],
            "title": "Estimation of model accuracy in CASP13",
            "venue": "Proteins Struct. Funct. Bioinforma",
            "year": 2019
        },
        {
            "authors": [
                "G. Derevyanko",
                "S. Grudinin",
                "Y. Bengio",
                "G. Lamoureux"
            ],
            "title": "Deep convolutional networks for quality assessment of protein folds",
            "venue": "Bioinformatics 34, 4046\u20134053. doi:10.1093/bioinformatics/bty494",
            "year": 2018
        },
        {
            "authors": [
                "H. Drucker",
                "C.J.C. Burges",
                "L. Kaufman",
                "A. Smola",
                "V. Vapnik"
            ],
            "title": "Support vector regression machines,\u201d in Advances in neural information processing systems",
            "venue": "Editors M. Mozer, M. Jordan, and T. Petsche (MIT Press), 9, 155\u2013161.",
            "year": 1996
        },
        {
            "authors": [
                "A. Elnaggar",
                "M. Heinzinger",
                "C. Dallago",
                "G. Rehawi",
                "Y. Wang",
                "L Jones"
            ],
            "title": "ProtTrans: toward understanding the language of life through self-supervised learning",
            "venue": "IEEE Trans. pattern analysis Mach. Intell",
            "year": 2021
        },
        {
            "authors": [
                "M. Fey",
                "J.E. Lenssen"
            ],
            "title": "Fast graph representation learning with PyTorch Geometric",
            "venue": "arXiv preprint arXiv:1903.02428.",
            "year": 2019
        },
        {
            "authors": [
                "A. Fout",
                "J. Byrd",
                "B. Shariat",
                "A. Ben-Hur"
            ],
            "title": "Protein interface prediction using graph convolutional networks, 30",
            "year": 2017
        },
        {
            "authors": [
                "Guo",
                "S.-S.",
                "J. Liu",
                "Zhou",
                "X.-G.",
                "Zhang",
                "G.-J."
            ],
            "title": "DeepUMQA: ultrafast shape recognition-based protein model quality assessment using deep learning",
            "venue": "Bioinformatics 38, 1895\u20131903. doi:10.1093/bioinformatics/btac056",
            "year": 2022
        },
        {
            "authors": [
                "J. Haas",
                "A. Barbato",
                "D. Behringer",
                "G. Studer",
                "S. Roth",
                "M Bertoni"
            ],
            "title": "Continuous Automated Model EvaluatiOn (CAMEO) complementing the critical assessment of structure prediction in CASP12",
            "venue": "Proteins Struct. Funct. Bioinforma",
            "year": 2018
        },
        {
            "authors": [
                "N. Hiranuma",
                "H. Park",
                "M. Baek",
                "I. Anishchenko",
                "J. Dauparas",
                "D. Baker"
            ],
            "title": "Improved protein structure refinement guided by deep learning based accuracy estimation",
            "venue": "Nat. Commun. 12, 1340. doi:10.1038/s41467-021-21511-x",
            "year": 2021
        },
        {
            "authors": [
                "D.M. Hurtado",
                "K. Uziela",
                "A. Elofsson"
            ],
            "title": "Deep transfer learning in the assessment of the quality of protein models",
            "venue": "arXiv preprint arXiv:1804.06281.",
            "year": 2018
        },
        {
            "authors": [
                "S. Ioffe",
                "C. Szegedy"
            ],
            "title": "Batch normalization: accelerating deep network training by reducing internal covariate shift,",
            "venue": "in International conference on machine learning (pmlr),",
            "year": 2015
        },
        {
            "authors": [
                "J. Jumper",
                "R. Evans",
                "A. Pritzel",
                "T. Green",
                "M. Figurnov",
                "O Ronneberger"
            ],
            "title": "Highly accurate protein structure prediction with alphafold",
            "venue": "Nature 596,",
            "year": 2021
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980",
            "year": 2014
        },
        {
            "authors": [
                "A. Kryshtafovych",
                "M. Antczak",
                "M. Szachniuk",
                "T. Zok",
                "R.C. Kretsch",
                "R Rangan"
            ],
            "title": "New prediction categories in casp15",
            "venue": "Proteins Struct. Funct. Bioinforma",
            "year": 2023
        },
        {
            "authors": [
                "J. Liu",
                "K. Zhao",
                "G. Zhang"
            ],
            "title": "Improved model quality assessment using sequence and structural information by enhanced deep neural networks",
            "venue": "Briefings Bioinforma. 24, bbac507. doi:10.1093/bib/bbac507",
            "year": 2022
        },
        {
            "authors": [
                "J. Liu",
                "K. Zhao",
                "G. Zhang"
            ],
            "title": "Improved model quality assessment using sequence and structural information by enhanced deep neural networks",
            "venue": "Briefings Bioinforma. 24, bbac507. doi:10.1093/bib/bbac507",
            "year": 2023
        },
        {
            "authors": [
                "J. Lundstr\u00f6m",
                "L. Rychlewski",
                "J. Bujnicki",
                "A. Elofsson"
            ],
            "title": "Pcons: A neuralnetwork\u2013based consensus predictor that improves fold recognition",
            "venue": "Protein Sci. 10, 2354\u20132362. doi:10.1110/ps.08501",
            "year": 2001
        },
        {
            "authors": [
                "A.H. Maghrabi",
                "L.J. McGuffin"
            ],
            "title": "Estimating the quality of 3D protein models using the ModFOLD7 server",
            "venue": "Protein Struct. Predict. 2165, 69\u201381. doi:10.1007/ 978-1-0716-0708-4_4",
            "year": 2020
        },
        {
            "authors": [
                "V. Mariani",
                "M. Biasini",
                "A. Barbato",
                "T. Schwede"
            ],
            "title": "lDDT: a local superposition-free score for comparing protein structures and models using distance difference tests",
            "venue": "Bioinformatics 29, 2722\u20132728. doi:10.1093/bioinformatics/btt473",
            "year": 2013
        },
        {
            "authors": [
                "L.J. McGuffin",
                "R. Adiyaman",
                "A.H. Maghrabi",
                "A.N. Shuid",
                "D.A. Brackenridge",
                "Nealon",
                "J. O"
            ],
            "title": "IntFOLD: an integrated web resource for high performance protein structure and function prediction",
            "venue": "Nucleic acids Res",
            "year": 2019
        },
        {
            "authors": [
                "L.J. McGuffin",
                "F.M. Aldowsari",
                "S.M. Alharbi",
                "R. Adiyaman"
            ],
            "title": "ModFOLD8: accurate global and local quality estimates for 3D protein models",
            "venue": "Nucleic acids Res. 49, W425\u2013W430. doi:10.1093/nar/gkab321",
            "year": 2021
        },
        {
            "authors": [
                "L.J. McGuffin",
                "N.S. Edmunds",
                "A.G. Genc",
                "S.M. Alharbi",
                "B.R. Salehe",
                "R. Adiyaman"
            ],
            "title": "Prediction of protein structures, functions and interactions using the IntFOLD7, MultiFOLD and ModFOLDdock servers",
            "venue": "Nucleic Acids Res., gkad297. doi:10.1093/nar/gkad297",
            "year": 2023
        },
        {
            "authors": [
                "J. Moult",
                "J.T. Pedersen",
                "R. Judson",
                "K. Fidelis"
            ],
            "title": "A large-scale experiment to assess protein structure prediction methods",
            "venue": "Proteins. [Dataset]. doi:10.1002/prot. 340230303",
            "year": 1995
        },
        {
            "authors": [
                "K. Olechnovi\u010d",
                "\u010c. Venclovas"
            ],
            "title": "VoroMQA: assessment of protein structure quality using interatomic contact areas",
            "venue": "Proteins Struct. Funct. Bioinforma. 85, 1131\u20131145. doi:10.1002/prot.25278",
            "year": 2017
        },
        {
            "authors": [
                "G. Pag\u00e8s",
                "B. Charmettant",
                "S. Grudinin"
            ],
            "title": "Protein model quality assessment using 3D oriented convolutional neural networks",
            "venue": "Bioinformatics 35, 3313\u20133319. doi:10. 1093/bioinformatics/btz122",
            "year": 2019
        },
        {
            "authors": [
                "A. Paszke",
                "S. Gross",
                "F. Massa",
                "A. Lerer",
                "J. Bradbury",
                "G Chanan"
            ],
            "title": "PyTorch: an imperative style, high-performance deep learning library",
            "venue": "Adv. neural Inf. Process. Syst",
            "year": 2019
        },
        {
            "authors": [
                "A. Ray",
                "E. Lindahl",
                "B. andWallner"
            ],
            "title": "Improvedmodel quality assessment using ProQ2. BMC Bioinforma",
            "year": 2012
        },
        {
            "authors": [
                "A. Shehu"
            ],
            "title": "A review of evolutionary algorithms for computing functional conformations of protein molecules",
            "venue": "Computer-Aided Drug Discov., 31\u201364. doi:10.1007/ 7653_2015_47",
            "year": 2015
        },
        {
            "authors": [
                "J. Skolnick",
                "M. Gao",
                "H. Zhou",
                "S. Singh"
            ],
            "title": "AlphaFold 2: why it works and its implications for understanding the relationships of protein sequence, structure, and function",
            "venue": "J. Chem. Inf. Model. 61, 4827\u20134831. doi:10.1021/acs.jcim. 1c01114",
            "year": 2021
        },
        {
            "authors": [
                "K. Uziela",
                "N. Shu",
                "B. Wallner",
                "A. Elofsson"
            ],
            "title": "ProQ3: improved model quality assessments using Rosetta energy terms",
            "venue": "Sci. Rep. 6, 33509\u201333510. doi:10.1038/ srep33509",
            "year": 2016
        },
        {
            "authors": [
                "K. Uziela",
                "D. Menendez Hurtado",
                "N. Shu",
                "B. Wallner",
                "A. Elofsson"
            ],
            "title": "ProQ3D: improved model quality assessments using deep learning",
            "venue": "Bioinformatics 33, 1578\u20131580. doi:10.1093/bioinformatics/btw819",
            "year": 2017
        },
        {
            "authors": [
                "M. Varadi",
                "S. Anyango",
                "M. Deshpande",
                "S. Nair",
                "C. Natassia",
                "G Yordanova"
            ],
            "title": "AlphaFold protein structure database: massively expanding the structural coverage of protein-sequence space with high-accuracy models",
            "venue": "Nucleic acids Res. 50,",
            "year": 2022
        },
        {
            "authors": [
                "B. Wallner",
                "A. Elofsson"
            ],
            "title": "Can correct protein models be identified? Protein Sci",
            "venue": "12, 1073\u20131086. doi:10.1110/ps.0236803",
            "year": 2003
        },
        {
            "authors": [
                "A. Zemla"
            ],
            "title": "LGA: a method for finding 3D similarities in protein structures",
            "venue": "Nucleic Acids Res. 31, 3370\u20133374. doi:10.1093/nar/gkg571",
            "year": 2003
        }
    ],
    "sections": [
        {
            "text": "Protein quality assessment with a loss function designed for high-quality decoys\nSoumyadip Roy and Asa Ben-Hur*\nDepartment of Computer Science, Colorado State University, Fort Collins, CO, United States\nMotivation: The prediction of a protein 3D structure is essential for understanding protein function, drug discovery, and disease mechanisms; with the advent of methods like AlphaFold that are capable of producing very high-quality decoys, ensuring the quality of those decoys can provide further confidence in the accuracy of their predictions.\nResults: In this work, we describe Q\u03f5, a graph convolutional network (GCN) that utilizes a minimal set of atom and residue features as inputs to predict the global distance test total score (GDTTS) and local distance difference test (lDDT) score of a decoy. To improve the model\u2019s performance, we introduce a novel loss function based on the \u03f5-insensitive loss function used for SVM regression. This loss function is specifically designed for evaluating the characteristics of the quality assessment problem and provides predictions with improved accuracy over standard loss functions used for this task. Despite using only aminimal set of features, it matches the performance of recent state-of-the-art methods like DeepUMQA.\nAvailability: The code for Q\u03f5 is available at https://github.com/soumyadip1997/ qepsilon.\nKEYWORDS\nprotein structure quality assessment, deep learning, graph convolutional networks, epsilon-insensitive loss function, critical assessment of structure prediction\n1 Introduction\nPredicting a protein\u2019s 3D structure from its amino acid sequence has been an area of avid interest for many years (Al-Lazikani et al., 2001). Recently, significant progress has been made in this field with the introduction of AlphaFold, a deep learning system that achieved remarkable accuracy in predicting protein structures (Jumper et al., 2021). While experimental identification of native protein structures remains a time-consuming and costly process, computational methods have made it possible to generate thousands of tertiary structures, known as decoys, in a matter of hours (Shehu, 2015). However, identifying the best structure remains a challenge. Therefore, it is necessary to employ a quality assessment stage to identify high-quality, near-native decoys among the generated decoys (Akhter et al., 2020). This remains true even with AlphaFold\u2019s recent breakthrough performance (Chen et al., 2023). Furthermore, with the subsequent availability of genomewide predicted structures across many species (Varadi et al., 2022), the quality assessment problem is as relevant as ever.\nIn this work, we address the decoy quality assessment problem with the help of graph convolutional networks (GCNs); we introduce a novel loss function inspired by the support vector regression, \u03f5-insensitive loss function, that is designed to take into account our"
        },
        {
            "heading": "OPEN ACCESS",
            "text": ""
        },
        {
            "heading": "EDITED BY",
            "text": "Mensur Dlakic, Montana State University, United States"
        },
        {
            "heading": "REVIEWED BY",
            "text": "Hiroto Saigo, Kyushu University, Japan Lim Heo, Michigan State University, United States\n*CORRESPONDENCE Asa Ben-Hur, asa@colostate.edu\nRECEIVED 31 March 2023 ACCEPTED 29 September 2023 PUBLISHED 17 October 2023"
        },
        {
            "heading": "CITATION",
            "text": "Roy S and Ben-Hur A (2023), Protein quality assessment with a loss function designed for high-quality decoys. Front. Bioinform. 3:1198218. doi: 10.3389/fbinf.2023.1198218"
        },
        {
            "heading": "COPYRIGHT",
            "text": "\u00a9 2023 Roy and Ben-Hur. This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.\nFrontiers in Bioinformatics frontiersin.org01\nTYPE Methods PUBLISHED 17 October 2023 DOI 10.3389/fbinf.2023.1198218\nintuition about what makes a good quality assessment predictor, namely, that it focuses on making correct predictions for those decoys that matter: decoys with high quality. We compare our method, called Q\u03f5, to other state-of-the-art methods and demonstrate that our method outperforms most of those methods while using only a very basic set of features computed from a decoy\u2019s sequence, without the need for engineered features."
        },
        {
            "heading": "2 Related work",
            "text": "Current techniques for quality assessment can be divided into two categories. One is single-model methods that operate on single structural models to estimate their quality (Wallner and Elofsson, 2003). The second category consists of methods that use consistency among several candidates to estimate quality (Lundstr\u00f6m et al., 2001). Protein quality assessment methods have been evaluated in the Critical Assessment of Structure Prediction (CASP) competition (Moult et al., 1995) since CASP7. The CASP13 single-model methods, the focus of this work, performed comparably or better than consensus methods for the first time (Cheng et al., 2019). A variety of single-model approaches have been proposed, and currently, machine learning-based methods dominate this area.\nUntil a few years ago, methods that use standard machine learning techniques with a large collection of engineered features computed from sequence and structure were the prevalent approaches for quality assessment. The ProQ series of methods (ProQ, ProQ2, ProQ3, and ProQ3D) (Uziela et al., 2017) used features such as the distribution of atom\u2013atom contacts, residue\u2013residue contacts, solvent accessibility, secondary structure, surface area, and evolutionary information. ProQ3 (Uziela et al., 2016) also incorporated features based on Rosetta energies. ProQ3D (Uziela et al., 2017) used the descriptors of ProQ3 as inputs in conjunction with a multi-layer perceptron and was one of the top performers of CASP13.\nThe current state-of-the-art method for quality assessment uses deep learning, including various types of 3D convolutional networks and graph neural networks, which have been demonstrated to be effective tools for modeling protein 3D structures (Derevyanko et al., 2018; Fout et al., 2017). Deep convolutional networks as a tool for the representation of decoy structures were introduced by Derevyanko et al. (2018). Their method, 3DCNN, used 3D convolutional networks applied to a volumetric representation of a decoy structure. The Ornate method by Pag\u00e8s et al. (2019) improved upon 3DCNN by defining a canonical orientation for each residue. The GraphQA method by Baldassarre et al. (2020) employed a graph convolutional network with an extensive number of engineered features and achieved state-of-the-art performance on CASP13 decoys. Chen et al. (2023) used a graph neural network to estimate the accuracy of AlphaFold models, which is one of the current state-of-the-art methods, and improved on the results obtained with DeepAccNet by Hiranuma et al. (2021) while borrowing many ideas from its architecture. They used a combination of categorical loss and L2-loss on the lDDT scores to distinguish between decoys of varying quality levels. The DeepUMQA method uses 3D convolution over a collection of residue-level engineered features (Guo et al., 2022), and its\nsuccessor, DeepUMQA2 (Liu et al., 2023), is also a state-of-theart performer.\nMost existing methods for quality assessment rely on engineered features. In contrast, our approach uses sequence embeddings computed using protein language models; convolutional layers applied to both atomic- and residue-level graphs are then used to put them in the context of the decoy structure. In combination with a novel loss function specifically designed for the quality assessment problem, our method can outperform the recent DeepUMQA method (Guo et al., 2022)."
        },
        {
            "heading": "3 Methods",
            "text": ""
        },
        {
            "heading": "3.1 The quality assessment problem",
            "text": "Computational methods for predicting a protein\u2019s 3D structure produce large numbers of decoy conformations for a given target protein. In quality assessment, we seek to rank these decoys based on their similarity to the experimentally determined native structure. We address this as a regression problem: our method is designed to predict the global distance test total score (GDTTS) (Zemla, 2003) and the local distance difference test (lDDT) score (Mariani et al., 2013), which are the official CASP scores for global-level decoy quality. While several recent methods were designed to predict the lDDT score (Hiranuma et al., 2021; Chen et al., 2023), we used both scores to allow for direct comparison with GraphQA, which is the most similar approach to the method presented here and would allow us to compare withmore recent QAmethods like DeepUMQA and DeepUMQA2. GDTTS measures the percentage of residues in the superimposed predicted structure that are within a certain distance threshold of their corresponding residues in the true structure. lDDT score is a superposition-free score that represents the local distance difference among all atoms in a predicted structure, thereby providing an idea of the local quality of the predicted structure. Decoy structures with high GDTTS and lDDT score (> 0.85) indicate that they closely resemble the native structure. In what follows, we describe Q\u03f5, a graph convolutional network that is trained on labeled decoy 3D structures, utilizing a basic set of features generated from atoms and residues using a combination of the L1-loss and a modification of the SVM regression \u03f5-insensitive loss function (Drucker et al., 1996)."
        },
        {
            "heading": "3.2 Atom- and residue-level graph convolution",
            "text": "Graph convolution is a powerful approach for representing protein 3D structures (Fout et al., 2017) and has proven its value for the quality assessment problem (Baldassarre et al., 2020). In order to enable us to forgo engineered features, we have chosen to represent the 3D structure of a decoy using dual graphs at the atom and residue levels (see Figure 1). Each of the graphs is a nearest neighbor graph where a pair of nodes is connected by an edge if their distance in the structure is less than a given threshold, where 6\u00c5 was the selected value in our experiments, and the distance between residues is the minimum distance between their atoms. We used up\nFrontiers in Bioinformatics frontiersin.org02\nFIGURE 1 Graph representation of a decoy structure. The structure of a decoy is represented using two graphs: one at the atomic level (left) and one at the residue level (right). Our graph convolution operation at the atom level differentiates between edges within a residue and edges across neighboring residues.\nFIGURE 2 Q\u03f5 model architecture illustrating how an input decoy structure is propagated through multiple graph convolutional layers (GCNatom for the atomlevel representation andGCNresidue for the residue-level representation of a protein); the outputs of the two sets of convolutional layers are concatenated and fed through a multi-layer perceptron (MLP) to generate local scores that are then averaged to compute the predicted GDTTS or lDDT score.\nFrontiers in Bioinformatics frontiersin.org03\nto 20 nearest neighbors to define the edges in both the atom-level and the residue-level graphs.\nWe perform graph convolution separately at the atom and residue levels. First, we describe the atom-level graph convolution (GCNatom). Each atom i is assigned a feature vector v(l)i that contains the features for layer l of graph convolution. The representation of a source atom v(l)i is updated based on its neighbors within the same residue (N (s)(i)) and the neighbors across residues (N (o)(i)) according to\nv l+1( )i ReLU(W c( )l v l( )i + 1|N s( ) i( )|W s( )l \u2211 j\u2208N s( ) i( ) v l( )j\n+ 1|N o( ) i( )|W o( ) l \u2211 j\u2208N o( ) i( ) v l( )j + b l( )v ),\n(1)\nwhere W(c)l is the weight matrix with respect to the source atom in layer l, W(s)l is the weight matrix with respect to the neighboring atoms in layer l within the same residue as that of the source atom, W(o)l is the weight matrix with respect to the neighboring atoms in layer l that belong to a different residue than the source atom, and finally, b(l)v is the bias in layer l for the atom-level GCN. The inputs to the atom-level convolution are derived from one-hot encoding of the atom type as described in the following sections.\nFIGURE 3 Themodified \u03f5-insensitive loss uses a variable-sized band around the diagonal in which a predicted score is not penalized. The band becomes smaller as the GDTTS or lDDT score increases, reflecting our expectation for precise predictions for decoys that are closer to the native structure.\nIn parallel to the atom-level convolution, we perform convolution over the residues that make up a decoy structure. This operation, denoted as GCNresidue, is used to update the residue-level representation r(l)i , which is the feature vector for residue i in layer l of the network. This operation is defined as follows:\nr l+1( )i ReLU W cr( )l r l( )i + 1 |R i( )|W r( ) l \u2211 j\u2208R i( ) r l( )j + b l( )r\u239b\u239d \u239e\u23a0, (2)\nwhere R(i) is the set of the neighboring residues of residue i, W(cr)l is the weight matrix with respect to the source residue in layer l, W(r)l is the weight matrix with respect to the neighboring residues in layer l, and b(l)r is the bias in layer l. The inputs to the\nresidue-level convolution are embeddings computed using ProtTrans (Elnaggar et al., 2021) as described in the following sections."
        },
        {
            "heading": "3.3 Network architecture",
            "text": "The architecture for Q\u03f5 includes four graph convolutional layers that aggregate information at the atomic level (GCNatom) and four graph convolutional layers that pass information at the residue level (GCNresidue). To ensure model stability and generalization, we apply batch normalization (Ioffe and Szegedy, 2015) after each application of an activation function to normalize the activations across the nodes in the\nFrontiers in Bioinformatics frontiersin.org05\ngraph. To create a coherent representation at the residue level, we apply a maximum pooling operation to the output of the final layer of GCNatom. The final residue-level representation is obtained by concatenating the output of the pooled atomiclevel convolution and the output from the residue-level GCN. This concatenated output is passed through a multi-layer perceptron (MLP), which outputs a single output per residue of the decoy structure. The final output of the network, which is our predicted value of GDTTS or lDDT score, is then produced by averaging over the node-level scores. This process is shown in Figure 2."
        },
        {
            "heading": "3.4 Atom and residue features",
            "text": "Our method performs convolution at both the atom and residue levels. Here, we describe the features used at both levels."
        },
        {
            "heading": "3.4.1 Atom features",
            "text": "We represent the atoms using one-hot encoding by grouping atoms into 11 different types (Derevyanko et al., 2018). This grouping reflects both the type of atom (carbon, oxygen, or nitrogen) and its context within the residue (e.g., alpha carbon or the different group an atom belongs to). In doing so, we are able to incorporate important\nFrontiers in Bioinformatics frontiersin.org06\ninformation of the atoms while also capturing the relationships between the atoms and their corresponding residues."
        },
        {
            "heading": "3.4.2 Residue features",
            "text": "We compute residue features by feeding the amino acid sequence of a decoy to the ProtTrans protein language model (Elnaggar et al., 2021). ProtTrans embeddings provide a very useful representation of the amino acid sequence, capturing relationships between residues and their structural context (Elnaggar et al., 2021). We take the embeddings from the last hidden state of the transformer attention stack of the ProtTrans model, with an output embedding of 1,024 dimensions, which serves as the input to the residue-level GCN.\n3.5 A modified \u03f5-insensitive loss\nIn this work, we address quality assessment as a regression problem with the objective of predicting GDTTS or lDDT score of a decoy. We propose a novel loss function that captures our desiderata for a quality assessment model: when it comes to poor decoys, we do not care about the accuracy of the prediction as long as we can differentiate it from a good decoy. On the other hand, the more accurate the decoy, the more accurate we want our prediction to be. This is especially important given the recent improvement in the quality of protein structure prediction methods. To achieve this goal, we modify the \u03f5-insensitive loss, which is the loss function employed in SVM regression (Drucker et al., 1996), as follows:\nTABLE 5 Q ablation study.\nMethod R Rtarget \u03c1 RMSE\nQ\u03f5 (with atom and residue features, pre-trained with L1-loss and modified \u03f5-insensitive loss) 0.90 0.80 0.89 0.11\nQ\u03f5 without modified \u03f5-insensitive loss 0.75 0.66 0.69 0.17\nQ\u03f5 without L1-loss 0.70 0.59 0.62 0.20\nQ\u03f5 with a constant \u03f5 (0.2) 0.63 0.55 0.66 0.24\nQ\u03f5 with only L2-loss 0.65 0.52 0.56 0.23\nQ\u03f5 without residue features 0.70 0.65 0.69 0.19\nQ\u03f5 without atom features 0.79 0.77 0.76 0.18\nEach of the major elements of Q\u03f5 is removed, demonstrating that each of them provides a major contribution to the performance of the method.\nTABLE 6 Model selection over the  hyperparameter values.\nScore ranges and results Low value Mid value High value\n\u03f5 for 0\u20130.1 0.40 0.45 0.50\n\u03f5 for 0.1\u20130.2 0.35 0.40 0.45\n\u03f5 for 0.2\u20130.3 0.30 0.35 0.40\n\u03f5 for 0.3\u20130.4 0.25 0.30 0.35\n\u03f5 for 0.4\u20130.5 0.20 0.25 0.30\n\u03f5 for 0.5\u20130.6 0.15 0.2 0.25\n\u03f5 for 0.6\u20130.7 0.10 0.15 0.20\n\u03f5 for 0.7\u20130.8 0.05 0.1 0.15\n\u03f5 for > 0.8 0.005 0.01 0.015\nR on CASP12 (validation set) (GDTTS) 0.84 0.89 0.82\nR on CASP12 (validation set) (lDDT score) 0.81 0.84 0.77\nR on CASP13 (test set) (GDTTS) 0.86 0.90 0.85\nR on CASP14 (test set) (GDTTS) 0.80 0.81 0.79\nR on CASP13 (test set) (lDDT score) 0.84 0.86 0.83\nR on CASP14 (test set) (lDDT score) 0.82 0.83 0.80\nThe top half shows the values of \u03f5 for each score range. The lower half shows the performance for each combination of values (low/mid/high); R stands for the Pearson correlation coefficient. Results are shown for the validation set (first two rows) and the test set for both GDTTS and lDDT score.\nFrontiers in Bioinformatics frontiersin.org07\nL y, y\u2032( ) max 0, |y \u2212 y\u2032| \u2212 \u03f5 y( )( ), (3) where y and y\u2032 are the true and predicted scores, respectively. As in the standard \u03f5-insensitive loss, this defines a tube of size \u03f5 within which there is no penalty; outside the tube, the loss grows linearly as in the L1-loss, which is defined as L(y, y\u2032) = |y \u2212 y\u2032|. In our application, the size of the tube is a function \u03f5(y). In this work, we used a tube defined as shown in Figure 3. The motivation for the modified \u03f5-insensitive loss function is that the model should not try too hard to accurately fit poor-quality decoys where we do not need good accuracy anyhow. As decoy quality increases, models are trained to learn a fit that is much more accurate."
        },
        {
            "heading": "3.6 Network training",
            "text": "We have trained our network to predict GDTTS and lDDT score. For GDTTS prediction, we first pre-train Q\u03f5 with the L1-loss\nfor 50 epochs, followed by training with the modified \u03f5-insensitive loss for the next 10 epochs. To train the network with lDDT scores, we select the best model from GDTTS (\u201cbest\u201d with respect to the validation set) and train it with the modified \u03f5-insensitive loss for another 50 epochs, keeping the same network architecture and hyperparameters.\nThe network was implemented in PyTorch (Paszke et al., 2019) and optimized using the Adammethod (Kingma and Ba, 2014) with default parameters except for a learning rate of 0.001; training used a batch size of 70. Since our training set is highly imbalanced, i.e., contains very few high-quality decoys, we used the imbalanced sampler from the torchsampler package. During training, we monitored the loss over the validation set and used the model that gave the minimum loss. Our implementation used the PyTorch Lightning framework for training and testing and PyTorch Geometric (Fey and Lenssen, 2019) for performing graph convolution. Model selection was performed using the hyperparameters and values described in Table 1. We iterated over all parameters and, for each one, chose the value that gave the highest\nFrontiers in Bioinformatics frontiersin.org08\nPearson correlation coefficient on the validation set. Following model selection, training took approximately 42 h on an NVIDIA RTX 3090 GPU."
        },
        {
            "heading": "3.7 Data",
            "text": "We collected decoys from CASP9 to CASP14 along with their labels from the CASP website (CASP, 2021). We used CASP9\u2013CASP12 as our training and validation sets andCASP13 andCASP14 as our test sets (see Table 2). In order tomatch the decoys used in experiments performed by others, we created two separate datasets for GDDTS evaluation (CASP13 and CASP14) and two datasets for the evaluation of lDDT score prediction (CASP13 and CASP14).\nIn CASP15, the focus shifted from predicting the accuracy of single-chain decoys to that of multi-chain complexes (Kryshtafovych et al., 2023). However, some of the targets were composed of single chains, and we chose to focus on those targets in our evaluation, leading to a dataset with 17 targets."
        },
        {
            "heading": "4 Results",
            "text": "We compare Q\u03f5 with other methods that have either state-ofthe-art or very good performance in CASP13 and CASP14. In our first set of experiments, we sought to compare our method with GraphQA, which uses a similar graph convolution architecture and was trained to predict GDTTS (Baldassarre et al., 2020). The results in Table 3 indicate that Q\u03f5 outperforms GraphQA and several other recent methods trained to predict GDTTS despite not using engineered features; a detailed analysis of the contribution of the various components of the Q\u03f5 architecture is described in an ablation study in the following section.\nThe quality assessment community is transitioning to the use of the lDDT score, so we also compare Q\u03f5withmore recentmethods evaluated with lDDT. In this evaluation, the performance of Q\u03f5was similar to that of DeepUMQA but outperformed by its successor, DeepUMQA2 (see Table 4). Results fromEnQA (Chen et al., 2023), whose performancewas similar to that of DeepUMQA2, are also better than those of Q\u03f5. Both methods use more complex architectures and extensive engineered features; DeepUMQA2 also used evolutionary information, including structural features from homologous templates.\nTo understand the contribution of the proposed modified \u03f5-insensitive loss to the performance of Q\u03f5, a scatter plot of true versus predicted GDTTS for the decoys in CASP13 and CASP14 is shown in Figure 4. We observe that the modified \u03f5-insensitive loss leads to better learning of decoys of all quality levels compared to the L1-loss and leads to a pattern where the predictions are limited to a band around the true scores, which is a highly desirable property for a quality assessment method. It was interesting that the width of the band is similar across all quality levels, despite the loss having a variable width band compared to the original \u03f5-insensitive loss function."
        },
        {
            "heading": "4.1 Ablation study",
            "text": "To demonstrate the contribution of each of the major components of our method, we performed an ablation study with\nrespect to GDTTS prediction, and its results are shown in Table 5. The first component we varied was the loss function. We observe that the pre-training with the L1-loss is key for the method\u2019s performance, serving to bootstrap the learning process. We also observe that performance dropped when using the original \u03f5-insensitive loss function, L1-loss, or L2-loss. This clearly shows the contribution of the proposed modification to the \u03f5-insensitive loss. Our next observation is that both the residue-level and atomlevel convolutional blocks are crucial for the performance of the method. This is due to each of them providing different and complementary information. The residue-level blocks use ProtTrans embeddings, which have been documented to provide a variety of information regarding a residue\u2019s evolutionary history and structural context within the protein (Elnaggar et al., 2021). The atom-level convolutional blocks provide a more fine-grained view of a decoy structure, complementing the information at the residue level."
        },
        {
            "heading": "4.2 \u03f5-threshold selection",
            "text": "The modified \u03f5-insensitive loss has nine threshold parameters associated with the epsilon insensitive loss function, one for each bin of the prediction score. In our experiments, we have used the values shown in Figure 3. In order to determine that our initial choice was good, we ran an experiment where we varied all the values in a coordinated manner: we chose nine values lower or higher than the initial values (the columns low and high in Table 6). As shown in Table 6, reducing or increasing the values of all the thresholds in a coordinated fashion led to reduced accuracy on the validation set. As a sanity check, we verified that a similar decrease is observed on the test set as well."
        },
        {
            "heading": "4.3 Local quality assessment with Q\u03f5",
            "text": "In this section, we demonstrate the ability of Q\u03f5 to make accurate predictions at the residue level, despite being trained only on global quality scores. This ability is a byproduct of the architecture of the network, where the global predicted score is an average of residuelevel node summary scores (see Figure 2). This forces the network to learn accurate local scores, as demonstrated in the results shown in Table 7. Similar to the global prediction problem, the performance of Q\u03f5 is between that of DeepUMQA and DeepUMQA2."
        },
        {
            "heading": "4.4 Results on CAMEO decoys",
            "text": "For further validation of the performance of Q\u03f5, we evaluated its performance on decoys from the CAMEO evaluation project (Haas et al., 2018). We downloaded decoys used from 13 May 2022 to 06 May 2023 and followed the same evaluation protocol used by CAMEO: the area under the ROC (AUROC) curve and area under the precision recall (AUPR) curve were calculated using a local lDDT score threshold of 0.6, and the obtained results are shown in Table 8. Again, we note that Q\u03f5 was not trained on local scores (unlike the other methods) and yet is able to perform almost on par with DeepUMQA2. As mentioned previously, this can be traced to\nFrontiers in Bioinformatics frontiersin.org09\nthe fact that the global prediction score computed by Q\u03f5 is evaluated by directly averaging local node summary scores, forcing those scores to reflect a local measure of quality."
        },
        {
            "heading": "4.5 Performance on AlphaFold2 decoys",
            "text": "In CASP14, AlphaFold2 provided, for the first time, decoys with near experimental resolution (Skolnick et al., 2021), with a median GDTTS of 92.4, making it the first team to achieve the highest level of accuracy in CASP. We gathered the decoys submitted by the AlphaFold team (team no 427) from the CASP14 website and evaluated Q\u03f5 on their decoys. We also ran AlphaFold2 version 2.3.1 on CASP15 single-chain targets. The results of this experiment are shown in Table 9. While AlphaFold2 provided better accuracy than our method, its value provided independent validation for the quality of AlphaFold2 predictions. EnQA (Chen et al., 2023) slightly improves on the quality of AlphaFold2 lDDT score estimates; however, it does so by using the AlphaFold2 scores as one of its features. Therefore, the results of the EnQA method are expected to be highly correlated with those of AlphaFold2 and less useful for independent verification of its predictions."
        },
        {
            "heading": "5 Conclusion and future work",
            "text": "In this study, we proposed a novel loss function to enhance the performance of deep learning for quality assessment of decoy structures. Our approach performed close to other state-of-the-artmethods while at the same time removing the need for engineered features computed from the protein structure, relying solely on features computed from the decoy sequence, demonstrating what is possible with a pure deep learning approach. These features were integrated using graph convolutional layers that operate at both the atom and residue levels, thereby improving the network\u2019s performance. The comparison of our approach with AlphaFold2 indicates there is a need for further research to provide accuracy estimates that improve on the local scores computed by AlphaFold2 in order to provide independent validation of the quality of its predicted structures.\nOur approach can be extended in multiple ways. First, although it performs well in predicting local scores, the method is trained using only global quality scores. Joint learning of both global and local scores can potentially improve performance for both tasks. Second, we treated the prediction of GDDTS and lDDT score as independent tasks; there is a potential gain in addressing multiple quality scores at the same time (Baldassarre et al., 2020). Finally, in this work, we chose to focus on the contribution of the loss function to method performance, so we used a relatively simple graph\nconvolutional network similar to that used in GraphQA (Baldassarre et al., 2020). Finally, we expect that the proposed loss function can be applied to regression problems, whose objective is to detect high-quality objects, and has the potential to be a useful addition to any deep learning toolbox."
        },
        {
            "heading": "Data availability statement",
            "text": "The datasets presented in this study can be found in online repositories. The names of the repository/repositories and accession number(s) can be found in the article/Supplementary Material."
        },
        {
            "heading": "Author contributions",
            "text": "This study was conceived by AB-H, and all the experiments were performed by SR. SR and AB-H analyzed the results and wrote the manuscript. All authors contributed to the article and approved the submitted version."
        },
        {
            "heading": "Funding",
            "text": "This project was supported by NSF-ABI award #1564840."
        },
        {
            "heading": "Acknowledgments",
            "text": "The authors would like to thank Jianlin Cheng for fruitful discussions on the quality assessment problem."
        },
        {
            "heading": "Conflict of interest",
            "text": "The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest."
        },
        {
            "heading": "Publisher\u2019s note",
            "text": "All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors, and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher."
        }
    ],
    "title": "Protein quality assessment with a loss function designed for high-quality decoys",
    "year": 2023
}