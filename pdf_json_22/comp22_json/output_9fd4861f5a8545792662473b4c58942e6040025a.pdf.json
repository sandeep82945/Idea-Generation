{
    "abstractText": "In critical infrastructure applications, timely and consistent fault detection and diagnosis is an increasingly important operational process, especially in the energy sector where safety is of the utmost importance. To realise this, engineers have to manually analyse data acquired from several assets using predefined diagnostic processes, but this is a time-consuming process requiring significant amounts of specialist expert knowledge. Data-driven approaches to support fault detection and diagnosis, and other similar problems, can produce accurate results comparable to what the engineers can achieve in a fraction of the time. However, the majority of these data-driven techniques are black box techniques and lack explainability which is often necessary for explaining decisions about critical assets in the power generation industry. Knowledge-based systems, such as rule-based expert systems have been shown to provide not only accurate decisions but also the explanation and reasoning behind these decisions in some related applications. However, there is a significant time cost associated with the development of knowledge-based systems, and in particular with the knowledge elicitation process, where the domain expert\u2019s knowledge is formalised and is encoded into the system. This challenge is commonly referred to as the knowledge elicitation bottleneck. In this paper, we present a novel approach to performing the knowledge elicitation using a set of symbolic primitives (rise, fall, fluctuate, and stable) to parameterise typical time-series condition monitoring data. The knowledge is represented by using a common language that can easily be communicated with (and from) the domain experts. This allows for the quick and accurate elicitation of the domain experts knowledge, but also the formalisation and implementation of the knowledge into a rapidly produced diagnostic system. Further to this, due to the parametrisation of the knowledge, it is possible to iteratively improve the knowledge base by updating these parameters based on new unseen data. This approach was applied to the Tennessee Eastman dataset, which is simulated data of a real-world industrial process. It was found that by using this approach it was possible to accurately and quickly capture the knowledge required to detect several faults within the case study dataset, but also provided fully explained reasons why each fault was detected by relating the explanations to the symbolic primitives previously defined.",
    "authors": [
        {
            "affiliations": [],
            "name": "Andrew Young"
        },
        {
            "affiliations": [],
            "name": "Andrew Duncan"
        },
        {
            "affiliations": [],
            "name": "Craig Michie"
        },
        {
            "affiliations": [],
            "name": "Stephen D.J. McArthur"
        }
    ],
    "id": "SP:288c7435de5b09c60eafcd43548dc66b6488ead1",
    "references": [
        {
            "authors": [
                "C. Angeli"
            ],
            "title": "Diagnostic expert systems: From expert\u2019s knowledge to real-time",
            "year": 2010
        },
        {
            "authors": [
                "S. Pearson Education. verkiev"
            ],
            "title": "Tennessee eastman process simulation dataset",
            "venue": "URL: https://www.",
            "year": 2020
        },
        {
            "authors": [
                "J. Zhi-nong",
                "W. Zhong-qing"
            ],
            "title": "Development of the task-based expert",
            "year": 2012
        },
        {
            "authors": [
                "H. Ba\u015fa\u011fao\u011flu",
                "J. Winterle"
            ],
            "title": "Interpretable vs. noninter",
            "year": 2021
        },
        {
            "authors": [
                "1016/j.eswa.2020.114498. hen",
                "C.-H",
                "Z. Rao"
            ],
            "title": "MRM: A Matrix representation and mapping approach",
            "year": 2008
        },
        {
            "authors": [],
            "title": "Knowledge acquisition for real-time knowledge-based systems",
            "year": 1987
        },
        {
            "authors": [
                "J. 2209257. Cullen",
                "A. Bryman"
            ],
            "title": "The knowledge acquisition bottleneck: time",
            "year": 1988
        },
        {
            "authors": [
                "X. Ji",
                "C. Rainey",
                "J. Zhang",
                "W. Lu"
            ],
            "title": "Integrating machine learning",
            "venue": "reassessment? Expert Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Y. S2589004220308488. Ding",
                "L. Ma",
                "J. Ma",
                "C. Wang",
                "C. Lu"
            ],
            "title": "A generative adversarial network",
            "year": 2019
        },
        {
            "authors": [
                "M. Press. Erlandsson",
                "A. Jansson"
            ],
            "title": "Collegial verbalisation\u2013a case study on a new",
            "year": 2007
        },
        {
            "authors": [
                "J.G. 535\u2013543. Gammack"
            ],
            "title": "Different techniques and different aspects on declarative",
            "year": 1987
        },
        {
            "authors": [
                "CA San Francisco",
                "Y. Lei",
                "S. Xing",
                "T. Yan",
                "N. Li"
            ],
            "title": "Deep convolutional transfer learning",
            "year": 2019
        },
        {
            "authors": [
                "R.R. Kogan Page. Hoffman"
            ],
            "title": "The problem of extracting the knowledge of experts",
            "year": 1987
        },
        {
            "authors": [
                "London: Palgrave Macmillan UK"
            ],
            "title": "The practical vibration primer",
            "year": 1978
        },
        {
            "authors": [
                "L. Company. Johnson",
                "N.E. Johnson"
            ],
            "title": "Knowledge elicitation involving teachback",
            "year": 1987
        },
        {
            "authors": [
                "P. 1_5. Johnson",
                "I. Zualkernan",
                "S. Garber"
            ],
            "title": "Specification of expertise",
            "year": 1987
        },
        {
            "authors": [
                "C. Aldrich"
            ],
            "title": "Fault detection in the Tennessee",
            "year": 2017
        },
        {
            "authors": [
                "M. World Congress. LaFrance"
            ],
            "title": "The Knowledge Acquisition Grid: a method for training knowledge",
            "year": 1987
        },
        {
            "authors": [
                "J. Liu",
                "X. Kong",
                "F. Xia",
                "X. Bai",
                "L. Wang",
                "Q Qing"
            ],
            "title": "Artificial intelligence in the 21st century",
            "venue": "IEEE Access, PP,",
            "year": 2018
        },
        {
            "authors": [
                "I. Lomov",
                "M. Lyubimov",
                "I. Makarov",
                "L.E. Zhukov"
            ],
            "title": "Fault detection in Tennessee Eastman process with temporal deep learning models",
            "venue": "Journal of Industrial Information Integration,",
            "year": 2021
        },
        {
            "authors": [
                "Z. Meng",
                "X. Guo",
                "Z. Pan",
                "D. Sun",
                "S. Liu"
            ],
            "title": "Data segmentation and augmentation methods based on raw data using deep neural networks approach for rotating machinery fault diagnosis",
            "venue": "IEEE Access,",
            "year": 2019
        },
        {
            "authors": [
                "N. Milton"
            ],
            "title": "11 Acquiring knowledge from subject matter experts",
            "venue": "Knowledge Service Engineering Handbook, 253. oradi, M., & Samwald, M. (2021). Post-hoc explanation of black-box classifiers using confident itemsets. Expert Systems with Applications, 165, Article 113941. http://dx.doi.org/10.1016/j.eswa.2020.113941.",
            "year": 2012
        },
        {
            "authors": [
                "J.D. ovak",
                "A.J. Ca\u00f1as"
            ],
            "title": "The theory underlying concept maps and how to construct them. Florida Institute for Human and Machine Cognition",
            "venue": "Expert Systems with Applications,",
            "year": 2006
        },
        {
            "authors": [
                "A. afea",
                "H. Hassen",
                "M. Hazman"
            ],
            "title": "Automatic knowledge acquisition tool for irrigation and fertilization expert systems",
            "venue": "Expert Systems with Applications,",
            "year": 2003
        },
        {
            "authors": [
                "A. Ragab",
                "M. El-koujok",
                "M. Amazouz",
                "S. Yacout"
            ],
            "title": "Fault detection and diagnosis in the Tennessee Eastman Process using interpretable knowledge discovery",
            "venue": "In 2017 annual reliability and maintainability symposium (RAMS) (pp. 1\u20137)",
            "year": 2017
        },
        {
            "authors": [
                "R.W. chvaneveldt",
                "F.T. Durso",
                "T.E. Goldsmith",
                "T.J. Breen",
                "N.M. Cooke",
                "Tucker",
                "R. G"
            ],
            "title": "Measuring the structure of expertise",
            "venue": "International Journal of Man-Machine Studies,",
            "year": 1985
        },
        {
            "authors": [
                "N.R. Shadbolt",
                "A.M. Burton"
            ],
            "title": "Experiments in knowledge elicitation",
            "year": 1990
        },
        {
            "authors": [
                "W. 109\u2013136). Springer. Wagner",
                "J. Otto",
                "Q. Chung"
            ],
            "title": "Knowledge acquisition for expert systems",
            "year": 2002
        },
        {
            "authors": [
                "S. eiss",
                "C. Kulikowski"
            ],
            "title": "A practical guide to designing expert systems (pp",
            "year": 1984
        },
        {
            "authors": [
                "329\u2013330). Rowman",
                "L. Allanheld. en",
                "X. Li",
                "L. Gao",
                "Y. Zhang"
            ],
            "title": "A new convolutional neural network-based",
            "year": 2018
        },
        {
            "authors": [
                "B.-S",
                "Lim",
                "D.-S",
                "A.C.C. Tan"
            ],
            "title": "VIBEX: An expert system for vibration",
            "year": 2005
        }
    ],
    "sections": [
        {
            "text": "Expert Systems With Applications 208 (2022) 118169\nA 0\nb\nContents lists available at ScienceDirect\nExpert SystemsWith Applications\njournal homepage: www.elsevier.com/locate/eswa\nParameterisation of domain knowledge for rapid and iterative prototyping of knowledge-based systems Andrew Young a,\u2217, Graeme West a, Blair Brown a, Bruce Stephen a, Andrew Duncan b, Craig Michie a, Stephen D.J. McArthur a a Institute for Energy and Environment, Department of Electronic and Electrical Engineering, University of Strathclyde, 204 George Street, Glasgow, G1 1XW, UK b The Alan Turing Institute, British Library, 96 Euston Rd, London NW1 2DB, UK\nA R T I C L E I N F O\nKeywords: Condition monitoring Expert systems Knowledge based systems Knowledge elicitation Automation Signal to symbol transformation\nA B S T R A C T\nIn critical infrastructure applications, timely and consistent fault detection and diagnosis is an increasingly important operational process, especially in the energy sector where safety is of the utmost importance. To realise this, engineers have to manually analyse data acquired from several assets using predefined diagnostic processes, but this is a time-consuming process requiring significant amounts of specialist expert knowledge. Data-driven approaches to support fault detection and diagnosis, and other similar problems, can produce accurate results comparable to what the engineers can achieve in a fraction of the time. However, the majority of these data-driven techniques are black box techniques and lack explainability which is often necessary for explaining decisions about critical assets in the power generation industry. Knowledge-based systems, such as rule-based expert systems have been shown to provide not only accurate decisions but also the explanation and reasoning behind these decisions in some related applications. However, there is a significant time cost associated with the development of knowledge-based systems, and in particular with the knowledge elicitation process, where the domain expert\u2019s knowledge is formalised and is encoded into the system. This challenge is commonly referred to as the knowledge elicitation bottleneck.\nIn this paper, we present a novel approach to performing the knowledge elicitation using a set of symbolic primitives (rise, fall, fluctuate, and stable) to parameterise typical time-series condition monitoring data. The knowledge is represented by using a common language that can easily be communicated with (and from) the domain experts. This allows for the quick and accurate elicitation of the domain experts knowledge, but also the formalisation and implementation of the knowledge into a rapidly produced diagnostic system. Further to this, due to the parametrisation of the knowledge, it is possible to iteratively improve the knowledge base by updating these parameters based on new unseen data. This approach was applied to the Tennessee Eastman dataset, which is simulated data of a real-world industrial process. It was found that by using this approach it was possible to accurately and quickly capture the knowledge required to detect several faults within the case study dataset, but also provided fully explained reasons why each fault was detected by relating the explanations to the symbolic primitives previously defined."
        },
        {
            "heading": "1. Introduction",
            "text": "For many critical assets across the power generation sector, accurate and efficient fault detection and diagnosis are extremely important. This is due to both the overriding need to ensure safe continued operation of the asset and the costs associated with downtime of the plant, through reduced or zero power generation but also with potentially expensive and time-consuming repairs or replacements, and also the cost related to the time associated with diagnosing said fault.\n\u2217 Corresponding author. E-mail addresses: andrew.young.101@strath.ac.uk (A. Young), graeme.west@strath.ac.uk (G. West), blair.brown@strath.ac.uk (B. Brown),\nruce.stephen@strath.ac.uk (B. Stephen), a.duncan@imperial.ac.uk (A. Duncan), c.michie@strath.ac.uk (C. Michie), s.mcarthur@strath.ac.uk (S.D.J. McArthur).\nEvaluation of faults is traditionally a time-consuming, manually intensive process and automation of this process provides the opportunity to introduce efficiency savings. To ensure the adoption of any automated approach similar or improved accuracy must be achieved over what can already be achieved manually by the engineers. Two main approaches to automating fault diagnosis are; data-driven approaches and knowledge-based approaches. With the increases in computing power available over the last two decades, it has been observed in Liu\nvailable online 19 July 2022 957-4174/\u00a9 2022 The Authors. Published by Elsevier Ltd. This is an open access ar\nhttps://doi.org/10.1016/j.eswa.2022.118169 Received 6 September 2021; Received in revised form 18 January 2022; Accepted\nticle under the CC BY license (http://creativecommons.org/licenses/by/4.0/).\n13 July 2022\nh t t t t\nt t a t o e a t c b t t\nb s t t t h t e r q t h c a\nr r h d\ns a\n2\nn L\ni t c d t\net al. (2018) that there has been a huge swing to more data-driven approaches over the traditional knowledge-based approaches. However, for many critical assets in power generation there is often a requirement to provide supporting evidence when making decisions and before implementing related solutions, due to the costs associated with reduced power generation, or expensive repairs or replacements. Hence, the lack of explainability of many data-driven approaches is currently a problem that is being extensively researched (Chakraborty, Ba\u015fa\u011fao\u011flu, and Winterle (2021) and Moradi and Samwald (2021)), owever, this is a problem that is yet to be solved. In addition to his, due to many of these types of assets having conservative design olerances, to compensate for and avoid lost revenue during downtime, here is frequently a lack of failure case studies to train data-driven echniques and ensure their accuracy or effectiveness. Knowledge-based techniques present an opportunity to address both\nhe lack of: explainability; and, training data by capturing and codifying he domain expert\u2019s knowledge and expertise. Expert knowledge is direct replacement for explainability as diagnostic results can be\nraced back to the knowledge from which it was derived. Training f models is not required as the model, in this case, is one of the xpert\u2019s knowledge, which is assumed to contain years of experience nd training. This knowledge in many cases has already been recorded hrough documented diagnostic procedures, however, this is not always omplete enough to be formalised into the rules for a knowledgeased system. The main drawback of this type of approach is the time aken to elicit this required knowledge from the engineers to enable his approach. In Cullen and Bryman (1988) this has been coined the \u2018\u2018knowledge elicitation bottleneck\u2019\u2019.\nThis paper presents a novel approach for tackling elements of the knowledge elicitation bottleneck for fault diagnosis problems and provides a case study of its use with a publicly available engineering data set. The next section of this paper provides more details of the various approaches to automated fault diagnosis, with an overview of the trade-offs between data-driven and knowledge-based approaches. This is important as most data driven approaches, by their nature lack the explainability inherent in knowledge based approaches have. Although, for knowledge-based approaches to become more viable the time taken to develop these systems needs to be addressed. Section three presents the various current approaches to perform knowledge elicitation for the development of knowledge-based systems. A new streamlined approach to performing knowledge elicitation using symbolic primitives is presented in section four, along with a step by step process for how to perform a knowledge elicitation session using this approach. This looks to address the issue of the time taken in the development of these systems, specifically in the knowledge elicitation phase. Section five demonstrates the proposed approach for knowledge elicitation and the development of a prototype expert system using a condition monitoring case study dataset. Conclusions and future work are presented in section six."
        },
        {
            "heading": "2. Background",
            "text": ""
        },
        {
            "heading": "2.1. Fault diagnosis",
            "text": "Fault diagnosis in many industrial applications involves engineers following a predefined diagnostic procedure. These procedures have typically developed over many years based on physical laws, industry standards and the engineers understanding of the specific asset under analysis and how the individual process variables, e.g. temperature, vibration, pressure, flow, current, voltage, etc., change under different fault conditions. Based on these factors, the engineers will look for features, or trends in the specific process variable to determine if a fault condition has been met, then by cross-referencing this with the current operating conditions of the plant will determine if any actions\nare required to be undertaken.\nFig. 1 shows an example of an industrial fault detection (or trouleshooting) procedure (Asturias & Gagen, 2007). This process involves plitting the troubleshooting process into two stages, first identifying he issue and secondly performing the actual repair. The seven steps hat must be taken to identify the fault are; gather information, verify he issue, try quick fixes, use appropriate diagnostics, perform a splitalf search, use additional resources to research the issue, and escalate he issue (if necessary). In this example diagnosis process, after gathring the relevant data the system has to be assessed by an expert, as eferenced by \u2018\u2018schedule service call\u2019\u2019, to verify the problem, failing any uick fixes resolving the issue, an extensive diagnostic procedure has o be undertaken involving; running diagnostics, performing a splitalf search, and general research into the problem. Each of these steps an be a very time-consuming process for the experts, therefore, it is dvantageous to automate as much of this process as possible. The main parts in these types of processes that can be automated\nelate specifically to the assessment of gathered information or data, unning diagnostics, assessing how the data changes over time, and ow the relationship between various datastreams change all under ifferent fault conditions. The two approaches discussed in this paper to automate these\nteps are categorised into data-driven approaches and knowledge-based pproaches.\n.2. Data-driven approaches\nThere are several data-driven approaches to automated fault diagosis, however, the current state of the art is predominately Machine earning oriented. A convolutional neural network-based approach to fault diagnosis\ns proposed in Wen, Li, Gao, and Zhang (2018), the authors highlight he limitation of the approach being able to classify common fault onditions, i.e. those that have a large amount of labelled training ata available. Therefore, for faults that have not been learned the echnique would be unable to classify them. The authors of Guo, Lei, Xing, Yan, and Li (2019) propose a deep convolutional transfer learning network (DCTLN) method for fault diagnosis of unlabelled data. The results show that the DCTLN trained on labelled data from one machine could accurately classify unlabelled data acquired for a new machine. As before, this approach requires labelled data to exist for a machine similar to the machine currently being analysed. In Ding, Ma, Ma, Wang, and Lu (2019) the authors propose a generative adversarial network-based approach in an attempt to solve the \u2018\u2018small sample size problem\u2019\u2019, by producing an augmented training dataset that was used to accurately classify the given faults. Also in Meng, Guo, Pan, Sun, and Liu (2019), the authors propose a new data augmentation by segmenting the sample data and randomly recombining the segments to artificially produce a larger amount of training data.\nFrom the literature, for industrial fault diagnostics for a critical asset where there is extremely limited labelled faulty training data (if any at all) and supporting evidence is required when making diagnostic conclusions; in general, machine learning or data-driven approaches are unable to solve this type of problem. This is mainly due to the large amount of training data required to learn the faults or the lack of explainability due to the black-box nature of these techniques. In addition to this, for these types of industrial problems, there is a significant amount of domain expert knowledge available due to the current engineers having to perform this fault diagnosis process manually. The process of incorporating the domain expert knowledge into data-driven models is currently an active area of research but is far from a trivial\nproblem (Deng, Ji, Rainey, Zhang, & Lu, 2020).\n2\nk k d e u e c d\ns k L t m r l t a s r a\n.3. Knowledge-based approaches\nAn alternative class of methods to solve this problem involves nowledge-based approaches. Unlike data-driven approaches, nowledge-based approaches do not require a large amount of training ata to be available, but instead, encode a significant amount of xpert knowledge or human expertise into the system that can be sed to perform fault diagnosis. As the domain experts knowledge is ncoded into the system, they are not only able to produce diagnostic onclusions but are also able to explain the reasoning behind why a ecision has been made. In Qian, Li, Jiang, and Wen (2003) the authors present an expert ystem for fault diagnosis of chemical processes, using a pre-existing nowledge base. A Vibration-based expert system is presented in Yang, im, and Tan (2005) and uses a decision tree style approach to perform he analysis. This decision tree is generated from the cause-symptom atrix presented in Jackson et al. (1978). Due to the nature of how the ules are formalised and stored in a rule-based expert system, having ow generality and low expandability (Gao, Cecati, & Ding, 2015) a ask-based diagnostic expert system was proposed in Bo, Zhi-nong, nd Zhong-qing (2012). This allows for the rules to be customised for pecific assets and stored as more general rules, however, this also equires a well-structured logic to exist and applies only to similar ssets. As stated in Angeli (2010), Rafea, Hassen, and Hazman (2003),\nand Shadbolt and Smart (2015) the main hurdle to overcome in the\ndevelopment of knowledge-based systems is the knowledge elicitation, or knowledge acquisition, bottleneck. Various research has been undertaken in an attempt to solve this problem (Chen & Rao, 2008; Wagner, Otto, & Chung, 2002; Xing, Huang, & Shi, 2003; Xiong, Litz, & Ressom, 2002), however, the majority of this research has focused on ways of extending the current knowledge elicitation techniques by the inclusion of more participants or additional data sources."
        },
        {
            "heading": "3. Knowledge elicitation",
            "text": "Knowledge elicitation is the process of attempting to elicit knowledge from domain experts using various types of methods and techniques. This information can be gathered from a variety of sources, e.g. technical manual, case studies, textbooks, etc., however, this is typically done by direct interaction with the domain expert. Generally, this is because the knowledge is often derived through practical experience. This implies that there is often the requirement for the expert to be involved in the knowledge elicitation process, representing the main challenge for the knowledge engineer. The person conducting the knowledge elicitation session is therefore required to determine an appropriate way for the experts to communicate their knowledge. The approach proposed in this paper aims to build on this premise by developing an approach that is both familiar to the domain expert, easy for them to communicate their understanding, and also easy to codify this knowledge into a knowledge base. Additionally, issues arise during knowledge elicitation, for example, due to the nature of the\nd s\nE f m b r a k w\n4\n4\nt d n\ne r\ni T t o p t A F\nm o t\nknowledge being acquired by the expert over several years and hence becoming part of their routine, it can often, therefore, be difficult for the experts to formalise their knowledge. Also, for larger organisations, the knowledge can be distributed across a number of experts in various geographical locations or with different management oversight.\nVarious approaches can be used to perform this knowledge elicitation. The main techniques have been categorised into five main categories (Shadbolt & Smart, 2015); interviews, protocol analysis, iagramming, sorting and rating, and constrained processing which are hown below:\n\u2022 Interviews \u2013 Structured\n\u2217 Fixed Probe (Shadbolt & Burton, 1990) \u2217 Focused Interviews (Hart, 1989) \u2217 Forward Scenario Simulation (Grover, 1983) \u2217 Critical Decision Method (Hoffman, 1998)\n\u2013 Semi-Structured\n\u2217 Knowledge Acquisition Grid (LaFrance, 1987) \u2217 Teach Back (Johnson & Johnson, 1987)\n\u2013 Unstructured (Weiss & Kulikowski, 1984)\n\u2022 Protocol Analysis \u2013 Verbal\n\u2217 Online (Johnson, Zualkernan, & Garber, 1987) \u2217 Offline (Elstein, Shulman, & Sprafka, 1978) \u2217 Shadowing (Clarke, 1987) \u2217 Collegial Verbalization (Erlandsson & Jansson, 2007)\n\u2013 Behavioural (Ericsson & Simon, 1984)\n\u2022 Diagramming \u2013 Laddered Grid (Corbridge, Rugg, Major, Shadbolt, & Burton, 1994)\n\u2013 Concept Mapping (Novak & Ca\u00f1as, 2006) \u2013 Process Mapping (Milton, 2012)\n\u2022 Sorting and Rating \u2013 Concept Sorting (Gammack, 1987) \u2013 Repertory Grid (Shaw & Gaines, 1987) \u2013 Pathfinder (Schvaneveldt et al., 1985)\n\u2022 Constrained Processing \u2013 Limited-Information Task (Hoffman, 1987) \u2013 20 Questions (Grover, 1983)\nach of these approaches has different positives and negatives, and a ull discussion of this is beyond the scope of this paper. However, the ain negative for all of these approaches is they are time-consuming for oth the knowledge engineer and the domain expert. They also either equire face to face meetings between both parties which takes time way from the day job of the domain experts, or requires access for the nowledge engineer to observe the domain expert performing the task, hich for certain industries may not be possible.\n. Methodology\n.1. Background\nIn the literature, it is agreed that the knowledge elicitation botleneck is a major problem to overcome before being able to rapidly evelop and deploy industrial fault diagnostic expert systems. The\novel approach proposed in this paper alters the typical expert system\narchitecture by the inclusion of a signal to symbol transformation (SST) Costello, West, McArthur, and Campbell (2012) and Young, West, Brown, Stephen, and McArthur (2019) step. Fig. 2 shows the updated xpert system architecture with the inclusion of an SST between the eal world view of data and the inference engine. SST is used to transform time series data gathered from the asset\nnto a series of symbolic primitives that accurately represent the data. his is achieved by segmenting the data into discrete time intervals, hen for each time segment a symbolic primitive is assigned, examples f, these primitives are intentionally rationalised (to minimise comlexity) as: rising, falling, fluctuating or stable, see Fig. 3. However, he symbolic primitives can be customised for the specific application. n example of this applied to two-time series data sources is shown in ig. 4. The asset-specific knowledge that needs to be elicited from the doain experts is the subtle difference for each symbolic primitives. Each f the differences for each primitive has been assigned a parameter, his is shown in Fig. 5. Fig. 3(a) shows an example of how the \ud835\udc65 and \ud835\udc66\nparameters are defined for a stable trend. Here the parameters define\ni\nt o\nTable 1 Example format of pressure datastream specific rules. (\u2013 is Stable, F is Fluctuating, \u2191 is Rising and \u2193 is Falling).\nName D1 x y z D2 x y z D3 x y z D4 x y z\nFault 1 \u2013 1.5 2.0 N/A \u2191 600 2.0 0.25 \u2193 600 1.5 0.5 F 600 10 N/A\no a n 4 d s i A p 5 s l i T\n5\n5\nn p\nFig. 5. Definition of parameters for subtle difference in symbolic primitives. Where \ud835\udf07 s the mean of the signal.\nhe maximum deviation from the mean of the signal, both in terms f an increase and a decrease. Conversely, Fig. 3(d) gives an example of a fluctuating trend, for the data samples that fall outside of the stable regions, the number of fluctuations (mean crossings) is calculated if this is above the threshold defined by \ud835\udc66 the signal is considered fluctuating. For Fig. 3(b) and Fig. 3(c) the \ud835\udc65, \ud835\udc66 and \ud835\udc67 parameters are defined the same except where \ud835\udc66 defined an increase for a rising trend and a decrease for a falling trend. The \ud835\udc65 parameter defines the time the trend occurs over and the \ud835\udc67 parameter is the expected deviation in the rise or fall of the trend.\nBy eliciting a combination of symbolic primitives for specific datastreams and eliciting for each of the \ud835\udc65, \ud835\udc66, and \ud835\udc67 parameters for each primitive it will be possible to construct a rule that can be directly interpreted by an expert system."
        },
        {
            "heading": "4.2. Knowledge elicitation session",
            "text": "In the previous subsection, an approach to alter the standard expert system architecture was described to include a SST step that allows for the parametrisation of the expert knowledge required to perform fault diagnosis. In addition to this, four example symbolic primitives were described and how each of these symbols were parametrised.\nThis section describes how framing the problem in this manner allows for more efficient domain knowledge capture from engineers in a knowledge elicitation session. In all of the knowledge elicitation approaches mentioned in Section 3, the knowledge engineer has to understand the process that the expert adopts when analysing their problem. The proposed approach simplifies the explanation of this process into several key questions; What datastreams are relevant to the given fault?, What trends are associated with the relevant datastreams?, What is the expected magnitude of these trends?, and How long will the fault take to manifest?\nBelow, we provide a step by step description of how to perform a knowledge elicitation session using the proposed approach. 0. Fault selection: The fault selection step, numbered step zero in the process as it occurs before the knowledge elicitation session. In this step, the domain experts must collate example case studies for each\nT\nAlgorithm 1: Signal to symbol transformation. Where \ud835\udc65, \ud835\udc66 and \ud835\udc67 are defined in Fig. 5 if 50% of data (< x*mean(data) or > y*mean(data)) then\nResult: Stable else if Number of mean crossings > y then\nResult: Fluctuating else\nCalculate average of first and last 10% of data for x period of time; if First < y*Last \u00b1 z*Last then\nResult: Rising else if First > y*Last \u00b1 z*Last then\nResult: Falling\nfault they want the system to be able to detect. Ideally, this should be in a digital format (e.g. plots of process variables from a condition monitoring system) to allow for ease of sharing, and accurately defining of parameters in Step 4. 1. Datastream selection: The initial stage in the knowledge elicitation session, is datastream selection. Based on the case study data under investigation and supplied from the domain experts, all datastreams except the specific datastreams they would use to determine the fault are eliminated. In the case of the eliminated datastreams, these \u2018\u2018Don\u2019t care\u2019\u2019 or \u2018\u2018N/A\u2019\u2019 states are ignored and not referenced in the diagnostic process. 2. Time Interval selection: The second stage involves exploration of the case study data, to determine the approximate time frame/scale during which the fault will present or develop. That is the time from steady-state normal operation to a fault occurring, or being fully represented in the datastreams under analysis. Using the SST as described above, this time is used to segment the data. While this time interval does not need to be exact, and it is assumed that no two fault cases will be the same, a relatively accurate time interval is necessary to produce an accurate rule for the expert system. 3. Symbolic Primitive selection: For the next stage, each datastream is required to be assigned a symbolic primitive. Fig. 5 shows an example f four trends that can be used, however, additional primitives can be dded to this list assuming appropriate parameters are defined for each ew primitive. . Parameter Calculation: The final stage before a rule can be prouced involves the setting of the individual parameters. Algorithm 1 hows pseudocode for how the symbolic primitive can be defined for ndividual time segments using the four previously defined primitives. s before this can be updated with the addition of new symbolic rimitives. . Diagnostic Rule Induction: Implementation of the pseudocode hown in Algorithm 1, allows for the symbolic primitives to be calcuated for any input datastream. A diagnostic rule can then be stored n the knowledge base for any number of datastreams, as shown in able 1.\n. Case study: Tennessee eastman process simulation data\n.1. Background\nThe Tennessee Eastman process is a hypothetical set of interconected industrial processes based on the physical behaviours of actual lant that was modelled computationally by Downs and Vogel (1993). he process contains five major units: the condenser, compressor,\nl t t t d\nreactor, vapour/liquid separator and the product stripper, as shown in Fig. 6. This dataset has been used extensively in many areas including fault detection, plant-wide control, and statistical process monitoring. In Lomov, Lyubimov, Makarov, and Zhukov (2021) a temporal deep earning model was proposed and in Krishnannair and Aldrich (2017) he use of nonlinear singular spectrum analysis is proposed both for he purposes of fault detection. The dataset used in this section is he \u2018\u2018TEP_Faulty_Training\u2019\u2019 portion of the Tennessee Eastman Process ataset (Averkiev, 2020). This contains 52 individual monitored process variables, detailed in Table 7, and 8. This portion of the dataset contains 500 simulations of 20 specific fault types each with 500 samples per fault. The sampling rate for the dataset was 3 min, therefore giving approximately 40 years worth of condition monitoring data with complete ground truth."
        },
        {
            "heading": "5.2. Knowledge capture",
            "text": "Using the process proposed in this paper the knowledge necessary to capture each of the five faults in the dataset was captured. For the application case study, as there were no domain experts involved in the process, all 52 datastreams were assessed visually to determine which datastreams were necessary to detect the given fault. To provide substantially different faults, fault numbers 1, 2, 5, 6, and 7 (as defined in the dataset) were selected for analysis. For Fault 1, this was assessed as xmeas_1, xmeas_4, xmeas_7, xmeas_13, xmeas_16, xmeas_19, xmeas_22, xmeas_34, xmv_3, xmv_4, and xmv_9, the individual time segments relating to fault 1 are shown in Fig. 7.\nFor each of the ten datastreams selected, three randomly selected examples of fault 1 were selected. The signal to symbol transformation was then performed. For the rising and falling trends the average rise or fall was taken as the \ud835\udc66 parameters, and the max variation was taken as the \ud835\udc67 parameter. Fluctuating trends are defined as the number of mean crossing from the segment of data, therefore, the \ud835\udc66 was defined as the max mean crossings out of the three examples. Finally, the stable trends were defined as the range where less than 50% of the data was\noutside of the boundary. The extracted knowledge for Fault 1 is shown in Table 2. As there are 500 samples per fault in the dataset the time interval for each piece of knowledge was set to 500. This process was then repeated for the remaining 4 faults, and the extracted knowledge\nis shown in Tables 2 to 6.\nmentioned in the tables were set as \u2018\u2018don\u2019t care\u2019\u2019 states. This produced a knowledge-based expert system that contained 5 rules that were able to accurately detect Faults 1, 2, 5, 6 and 7 from the Tennessee Eastman Process dataset."
        },
        {
            "heading": "5.3. Results",
            "text": "Using this approach it was possible to produce a system that can accurately detect specific types of faults in the condition monitoring dataset. Out of the 10,000-time segments analysed only 8 (0.08%) false positives were produced, these were 6 from fault class 1 and 2 from fault class 2. Slightly more false positives were produced, 55 (0.55%) fault class 5 and 7 faults class 7 were detected as no-fault. However, as this system was being designed for fault diagnostics, more false positives and fewer false negatives was preferred. There was also 2 (0.02%) instances of fault class 1 that was detected as fault class 5, however, upon further analysis, it was visually impossible to differentiate these two instances for the general fault 5 class. Fig. 8 presents a confusion matrix for the full system."
        },
        {
            "heading": "6. Comparison with state-of-the-art",
            "text": "The methods highlighted in Section 3 are general approaches that can be applied to the capture of a wide range of knowledge from a broad range of domains. In this paper, we focus on the capture of a specific class of knowledge, that is associated with the interpretation of time series data. By constraining the type of knowledge being elicited allows additional structure to be placed around the representation of the captured knowledge, which in turn offers the opportunity to streamline the process.\nComparing approaches for the capture of human expertise is challenging, not least in that no two subject matter experts will respond to the interview process in the same manner, and attempting to elicit the same knowledge from the same expert, using two different methods in consecutive sessions is going to favour the latter method as the expert will be re-running through the same material. To attempt to address this issue a study was performed which compared the capture of a set of condition assessment rules derived for the health of an industrial asset through the examination of multiple streams of time series data. A subset of the rules pertaining to a specific fault mode were captured using a structured interview process, while a second subset of rules for a different fault mode was captured using the proposed methodology. In the former, 2 three hour knowledge elicitation sessions were held with the domain experts and it was possible to understand the problem, capture the relevant datastreams, and capture the relevant rules, however, within these two sessions it was not possible to parametrise the rules. For the same problem, in 1 two hour knowledge elicitation session it was possible to capture the relevant datastreams, capture the relevant rules, and parametrise the faults detected in four different case study datasets. While the specifics of the rules and parameters captured in each of these approaches are different, it was felt that they were suitably similar to permit valid comparisons in terms of the time required to be drawn. In conclusion, comparing the proposed technique with the standard structured interview, showed in this example a twofold saving in time."
        },
        {
            "heading": "7. Conclusion and future work",
            "text": "This paper has introduced a new approach to tackle the knowledge elicitation process for a sub-set of problems, namely the codification and capture of knowledge involved in the interpretation of time series data. This type of problem is seen across many engineering domains and is particularly prevalent in the development of expert systems for supporting the health assessment of industrial assets. While knowledgedriven approaches, such as expert systems were prominent in the 1980\u2019s and 1990\u2019s advances in neural networks, and in particular the advent of deep neural networks have resulted in significant advances in many AI applications. However, for many engineering applications such as health monitoring and assessment of industrial assets, there\nis still the need for the explainability that is currently unavailable\nfrom purely data drive black-box approaches. As a result, the capture and codification of specialist knowledge relating to the interpretation of condition monitoring data is of importance, and the issue of the knowledge elicitation bottleneck still exists.\nThis proposed approach has been shown to offer benefits in terms of time-saving. Historically, the main drawback to the development of these types of systems was the time involved in the capturing of domain expert knowledge from already time-poor engineers. The reduction in time, specifically in the knowledge elicitation phase, from a business perspective and the development of these types of systems places much less of a time burden on the already busy engineers. Future work will look at further reducing the time associated with capturing the domain expert knowledge. This could be achieved through the development of a learning algorithm that can process historical data and automatically propose new rules based upon repeating patterns that have been autonomously identified in historic data. This system could be implemented in a human-in-the-loop fashion to allow for the engineers to have the final say on what proposed rules are added to the knowledge base.\nThis new approach to performing knowledge elicitation for knowledge-based systems allows for the parameterisation of domain expert knowledge and the rapid prototyping of an expert system. This is important as there is an inherent brittleness in traditional expert systems due to the formalisation of the knowledge, and the ease in which the knowledge can be updated. It was demonstrated using a case study that it was possible to produce a knowledge-based system that can detect specific faults quickly and accurately. Also using this\nA\nA\nB\nC\nC\napproach to formulate the knowledge in a symbolic manner results in the explanation of the results in an accessible way that does not require any specialist training. Due to the nature of how an expert system is designed using this approach, updating specific parameters allows for the iterative improvement of the knowledge base and hence the system without the need to redesign the entire system. Future work will focus on the development of an incremental learning algorithm to propose improvements to existing domain knowledge based on user-defined false positives and negatives.\nThis paper represents a step towards reducing the knowledge elicitation bottleneck, but there are limitations to the proposed approach. It currently focuses only on the interpretation of time series data and anticipates the captured knowledge will reason about trends in data. Furthermore, temporal relationships between multiple data streams are relatively common in condition monitoring data but have not been fully addressed here. For example, seeing an increase in one parameter, flow rate, for example, may result in a corresponding, but delayed increase in value in another parameter, say temperature. Secondly, the elicitation process is still predominantly a human to human process, which is then transferred to the expert system. Significant research is still required to enable the domain expert to directly interface with the diagnostic system in a two-way manner, so that new knowledge can be imparted into the system directly by the expert as new, previously unseen faults arise. This data imbalance, where there are often large volumes of normal operating data, and very few instances of failure data is another common challenge, and capture and codification of this diagnostic knowledge in a manner that they can be shared across multiple instances of expert systems remains an open problem. Finally, the other significant area of research in this field will be the integration of the data-driven with knowledge-driven approaches, particularly if large volumes of operating data can be leveraged to provide increased confidence or evidence in the human-derived parametrised values.\nCRediT authorship contribution statement\nAndrew Young: Conceptualization, Methodology, Software, Validation, Data curation, Writing \u2013 original draft, Writing \u2013 review & editing, Visualization. Graeme West: Conceptualization, Methodology, Supervision, Writing \u2013 review & editing, Funding acquisition. Blair Brown: Conceptualization, Methodology, Writing \u2013 review & editing, Project administration. Bruce Stephen: Conceptualization, Methodology, Writing \u2013 review & editing. Andrew Duncan: Conceptualization, Writing \u2013 review & editing, Funding acquisition. Craig Michie: Conceptualization, Writing \u2013 review & editing, Funding acquisition. Stephen D.J. McArthur: Conceptualization, Writing \u2013 review & editing, Funding acquisition.\nDeclaration of competing interest\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was funded by the Engineering and Physical Sciences Research Council, UK under grant EP/R004889/1.\nAppendix\nSee Tables 7 and 8."
        }
    ],
    "title": "Parameterisation of domain knowledge for rapid and iterative prototyping of knowledge-based systems",
    "year": 2022
}