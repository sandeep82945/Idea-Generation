{
    "abstractText": "Connected and autonomous vehicles (CAVs) promise next-gen transportation systems with enhanced safety, energy efficiency, and sustainability. One typical control strategy for CAVs is the so-called cooperative adaptive cruise control (CACC) where vehicles drive in platoons and cooperate to achieve safe and efficient transportation. In this study, we formulate CACC as a multi-agent reinforcement learning (MARL) problem. Diverging from existing MARL methods that use centralized training and decentralized execution which require not only a centralized communication mechanism but also dense inter-agent communication during training and online adaptation, we propose a fully decentralized MARL framework for enhanced efficiency and scalability. In addition, a quantization-based communication scheme is proposed to reduce the communication overhead without significantly degrading the control performance. This is achieved by employing randomized rounding numbers to quantize each piece of communicated information and only communicating non-zero components after quantization. Extensive experimentation in two distinct CACC settings reveals that the proposed MARL framework consistently achieves superior performance over several contemporary benchmarks in terms of both communication efficiency and control efficacy.",
    "authors": [
        {
            "affiliations": [],
            "name": "Dong Chen"
        },
        {
            "affiliations": [],
            "name": "Kaixiang Zhang"
        },
        {
            "affiliations": [],
            "name": "Yongqiang Wang"
        },
        {
            "affiliations": [],
            "name": "Xunyuan Yin"
        },
        {
            "affiliations": [],
            "name": "Zhaojian Li"
        }
    ],
    "id": "SP:009aa8114cb3059265de1fc84e602b1c9692cfc6",
    "references": [
        {
            "authors": [
                "Z. Wang",
                "S. Jin",
                "L. Liu",
                "C. Fang",
                "M. Li",
                "S. Guo"
            ],
            "title": "Design of intelligent connected cruise control with vehicle-to-vehicle communication delays",
            "venue": "IEEE Transactions on Vehicular Technology, vol. 71, no. 8, pp. 9011\u20139025, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "W. Liu",
                "M. Hua",
                "Z. Deng",
                "Y. Huang",
                "C. Hu",
                "S. Song",
                "L. Gao",
                "C. Liu",
                "L. Xiong",
                "X. Xia"
            ],
            "title": "A systematic survey of control techniques and applications: From autonomous vehicles to connected and automated vehicles",
            "venue": "arXiv preprint arXiv:2303.05665, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "T. Chu",
                "U. Kalabi\u0107"
            ],
            "title": "Model-based deep reinforcement learning for cacc in mixed-autonomy vehicle platoon",
            "venue": "Proceedings of 2019 IEEE 58th Conference on Decision and Control, 2019, pp. 4079\u20134084.",
            "year": 2019
        },
        {
            "authors": [
                "M. Parvini",
                "A. Gonz\u00e1lez",
                "A. Villamil",
                "P. Schulz",
                "G. Fettweis"
            ],
            "title": "Joint resource allocation and string-stable cacc design with multi-agent reinforcement learning",
            "venue": "2023.",
            "year": 2023
        },
        {
            "authors": [
                "T. Chu",
                "S. Chinchali",
                "S. Katti"
            ],
            "title": "Multi-agent reinforcement learning for networked system control",
            "venue": "Proceedings of International Conference on Learning Representations, 2020. [Online]. Available: https://openreview.net/forum?id=Syx7A3NFvH",
            "year": 2020
        },
        {
            "authors": [
                "H. Zhu",
                "Y. Zhou",
                "X. Luo",
                "H. Zhou"
            ],
            "title": "Joint control of power, beamwidth, and spacing for platoon-based vehicular cyber-physical systems",
            "venue": "IEEE Transactions on Vehicular Technology, vol. 71, no. 8, pp. 8615\u20138629, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "C. Massera Filho",
                "M.H. Terra",
                "D.F. Wolf"
            ],
            "title": "Safe optimization of highway traffic with robust model predictive control-based cooperative adaptive cruise control",
            "venue": "IEEE Transactions on Intelligent Transportation Systems, vol. 18, no. 11, pp. 3193\u20133203, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "W. Gao",
                "Z.-P. Jiang",
                "K. Ozbay"
            ],
            "title": "Data-driven adaptive optimal control of connected vehicles",
            "venue": "IEEE Transactions on Intelligent Transportation Systems, vol. 18, no. 5, pp. 1122\u20131133, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "A.M. Al-Jhayyish",
                "K.W. Schmidt"
            ],
            "title": "Feedforward strategies for cooperative adaptive cruise control in heterogeneous vehicle strings",
            "venue": "IEEE Transactions on Intelligent Transportation Systems, vol. 19, no. 1, pp. 113\u2013122, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "C. Wu",
                "A.M. Bayen",
                "A. Mehta"
            ],
            "title": "Stabilizing traffic with autonomous vehicles",
            "venue": "Proceedings of 2018 IEEE International Conference on Robotics and Automation, 2018, pp. 6012\u20136018.",
            "year": 2018
        },
        {
            "authors": [
                "S. Gong",
                "A. Zhou",
                "S. Peeta"
            ],
            "title": "Cooperative adaptive cruise control for a platoon of connected and autonomous vehicles considering dynamic information flow topology",
            "venue": "Transportation research record, vol. 2673, no. 10, pp. 185\u2013198, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "M. Wang"
            ],
            "title": "Infrastructure assisted adaptive driving to stabilise heterogeneous vehicle strings",
            "venue": "Transportation Research Part C: Emerging Technologies, vol. 91, pp. 276\u2013295, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "S. Feng",
                "Y. Zhang",
                "S.E. Li",
                "Z. Cao",
                "H.X. Liu",
                "L. Li"
            ],
            "title": "String stability for vehicular platoon control: Definitions and analysis methods",
            "venue": "Annual Reviews in Control, vol. 47, pp. 81\u201397, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "I.G. Jin",
                "G. Orosz"
            ],
            "title": "Dynamics of connected vehicle systems with delayed acceleration feedback",
            "venue": "Transportation Research Part C: Emerging Technologies, vol. 46, pp. 46\u201364, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "C. Desjardins",
                "B. Chaib-Draa"
            ],
            "title": "Cooperative adaptive cruise control: A reinforcement learning approach",
            "venue": "IEEE Transactions on intelligent transportation systems, vol. 12, no. 4, pp. 1248\u20131260, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "L. Lei",
                "T. Liu",
                "K. Zheng",
                "L. Hanzo"
            ],
            "title": "Deep reinforcement learning aided platoon control relying on v2x information",
            "venue": "IEEE Transactions on Vehicular Technology, vol. 71, no. 6, pp. 5811\u20135826, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "L. Jiang",
                "Y. Xie",
                "N.G. Evans",
                "X. Wen",
                "T. Li",
                "D. Chen"
            ],
            "title": "Reinforcement learning based cooperative longitudinal control for reducing traffic oscillations and improving platoon stability",
            "venue": "Transportation Research Part C: Emerging Technologies, vol. 141, p. 103744, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "T. Liu",
                "L. Lei",
                "K. Zheng",
                "K. Zhang"
            ],
            "title": "Autonomous platoon control with integrated deep reinforcement learning and dynamic programming",
            "venue": "IEEE Internet of Things Journal, vol. 10, no. 6, pp. 5476\u20135489, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "M. Li",
                "Z. Li",
                "S. Wang",
                "B. Wang"
            ],
            "title": "Anti-disturbance self-supervised reinforcement learning for perturbed car-following system",
            "venue": "IEEE Transactions on Vehicular Technology, 2023. 11",
            "year": 2023
        },
        {
            "authors": [
                "T. Haarnoja",
                "A. Zhou",
                "P. Abbeel",
                "S. Levine"
            ],
            "title": "Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor",
            "venue": "Proceedings of International conference on machine learning. PMLR, 2018, pp. 1861\u20131870.",
            "year": 2018
        },
        {
            "authors": [
                "T.P. Lillicrap",
                "J.J. Hunt",
                "A. Pritzel",
                "N. Heess",
                "T. Erez",
                "Y. Tassa",
                "D. Silver",
                "D. Wierstra"
            ],
            "title": "Continuous control with deep reinforcement learning",
            "venue": "arXiv preprint arXiv:1509.02971, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "D. Jia",
                "D. Ngoduy"
            ],
            "title": "Enhanced cooperative car-following traffic model with the combination of v2v and v2i communication",
            "venue": "Transportation Research Part B: Methodological, vol. 90, pp. 172\u2013191, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "A. Peake",
                "J. McCalmon",
                "B. Raiford",
                "T. Liu",
                "S. Alqahtani"
            ],
            "title": "Multiagent reinforcement learning for cooperative adaptive cruise control",
            "venue": "Proceedings of 2020 IEEE 32nd International Conference on Tools with Artificial Intelligence, 2020, pp. 15\u201322.",
            "year": 2020
        },
        {
            "authors": [
                "G. Raja",
                "K. Kottursamy",
                "K. Dev",
                "R. Narayanan",
                "A. Raja",
                "K.B.V. Karthik"
            ],
            "title": "Blockchain-integrated multiagent deep reinforcement learning for securing cooperative adaptive cruise control",
            "venue": "IEEE transactions on intelligent transportation systems, vol. 23, no. 7, pp. 9630\u20139639, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "M. Hua",
                "C. Zhang",
                "F. Zhang",
                "Z. Li",
                "X. Yu",
                "H. Xu",
                "Q. Zhou"
            ],
            "title": "Energy management of multi-mode plug-in hybrid electric vehicle using multiagent deep reinforcement learning",
            "venue": "arXiv preprint arXiv:2303.09658, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "R. Lowe",
                "Y.I. Wu",
                "A. Tamar",
                "J. Harb",
                "O. Pieter Abbeel",
                "I. Mordatch"
            ],
            "title": "Multi-agent actor-critic for mixed cooperative-competitive environments",
            "venue": "Advances in neural information processing systems, vol. 30, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "K. Zhang",
                "Z. Yang",
                "H. Liu",
                "T. Zhang",
                "T. Basar"
            ],
            "title": "Fully decentralized multi-agent reinforcement learning with networked agents",
            "venue": "Proceedings of International Conference on Machine Learning. PMLR, 2018, pp. 5872\u20135881.",
            "year": 2018
        },
        {
            "authors": [
                "K. Zhang",
                "Z. Yang",
                "T. Ba\u015far"
            ],
            "title": "Multi-agent reinforcement learning: A selective overview of theories and algorithms",
            "venue": "Handbook of reinforcement learning and control, pp. 321\u2013384, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "V. Mnih",
                "K. Kavukcuoglu",
                "D. Silver",
                "A.A. Rusu",
                "J. Veness",
                "M.G. Bellemare",
                "A. Graves",
                "M. Riedmiller",
                "A.K. Fidjeland",
                "G. Ostrovski"
            ],
            "title": "Human-level control through deep reinforcement learning",
            "venue": "nature, vol. 518, no. 7540, pp. 529\u2013533, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "V. Mnih",
                "A.P. Badia",
                "M. Mirza",
                "A. Graves",
                "T. Lillicrap",
                "T. Harley",
                "D. Silver",
                "K. Kavukcuoglu"
            ],
            "title": "Asynchronous methods for deep reinforcement learning",
            "venue": "International conference on machine learning. PMLR, 2016, pp. 1928\u20131937.",
            "year": 2016
        },
        {
            "authors": [
                "C.J. Watkins",
                "P. Dayan"
            ],
            "title": "Q-learning",
            "venue": "Machine learning, vol. 8, pp. 279\u2013292, 1992.",
            "year": 1992
        },
        {
            "authors": [
                "C. Szepesv\u00e1ri"
            ],
            "title": "Algorithms for reinforcement learning",
            "year": 2022
        },
        {
            "authors": [
                "T. Chu",
                "J. Wang",
                "L. Codec\u00e0",
                "Z. Li"
            ],
            "title": "Multi-agent deep reinforcement learning for large-scale traffic signal control",
            "venue": "IEEE Transactions on Intelligent Transportation Systems, vol. 21, no. 3, pp. 1086\u20131095, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "D. Chen",
                "M.R. Hajidavalloo",
                "Z. Li",
                "K. Chen",
                "Y. Wang",
                "L. Jiang",
                "Y. Wang"
            ],
            "title": "Deep multi-agent reinforcement learning for highway on-ramp merging in mixed traffic",
            "venue": "IEEE Transactions on Intelligent Transportation Systems, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "M. Tan"
            ],
            "title": "Multi-agent reinforcement learning: Independent vs. cooperative agents",
            "venue": "Proceedings of the Tenth International Conference on Machine Learning, 1993, pp. 330\u2013337.",
            "year": 1993
        },
        {
            "authors": [
                "J. Foerster",
                "N. Nardelli",
                "G. Farquhar",
                "T. Afouras",
                "P.H. Torr",
                "P. Kohli",
                "S. Whiteson"
            ],
            "title": "Stabilising experience replay for deep multi-agent reinforcement learning",
            "venue": "Proceedings of International conference on machine learning. PMLR, 2017, pp. 1146\u20131155.",
            "year": 2017
        },
        {
            "authors": [
                "J. Foerster",
                "I.A. Assael",
                "N. De Freitas",
                "S. Whiteson"
            ],
            "title": "Learning to communicate with deep multi-agent reinforcement learning",
            "venue": "Advances in neural information processing systems, vol. 29, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "S. Sukhbaatar",
                "R. Fergus"
            ],
            "title": "Learning multiagent communication with backpropagation",
            "venue": "Advances in neural information processing systems, vol. 29, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "D. Chen",
                "K. Chen",
                "Z. Li",
                "T. Chu",
                "R. Yao",
                "F. Qiu",
                "K. Lin"
            ],
            "title": "Powernet: Multi-agent deep reinforcement learning for scalable powergrid control",
            "venue": "IEEE Transactions on Power Systems, vol. 37, no. 2, pp. 1007\u20131017, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M. Bando",
                "K. Hasebe",
                "A. Nakayama",
                "A. Shibata",
                "Y. Sugiyama"
            ],
            "title": "Dynamical model of traffic congestion and numerical simulation",
            "venue": "Physical review E, vol. 51, no. 2, p. 1035, 1995.",
            "year": 1995
        },
        {
            "authors": [
                "D. Alistarh",
                "D. Grubic",
                "J. Li",
                "R. Tomioka",
                "M. Vojnovic"
            ],
            "title": "QSGD: Communication-efficient SGD via gradient quantization and encoding",
            "venue": "Proceedings of Advances in Neural Information Processing Systems, 2017, pp. 1709\u20131720.",
            "year": 2017
        },
        {
            "authors": [
                "Y. Wang",
                "T. Ba\u015far"
            ],
            "title": "Quantization enabled privacy protection in decentralized stochastic optimization",
            "venue": "IEEE Transactions on Automatic Control, vol. 68, no. 7, pp. 4038\u20134052, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "A.M. Saxe",
                "J.L. McClelland",
                "S. Ganguli"
            ],
            "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
            "venue": "arXiv preprint arXiv:1312.6120, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "M. Plappert",
                "R. Houthooft",
                "P. Dhariwal",
                "S. Sidor",
                "R.Y. Chen",
                "X. Chen",
                "T. Asfour",
                "P. Abbeel",
                "M. Andrychowicz"
            ],
            "title": "Parameter space noise for exploration",
            "venue": "arXiv preprint arXiv:1706.01905, 2017.",
            "year": 1905
        },
        {
            "authors": [
                "Q. Dai",
                "X. Xu",
                "W. Guo",
                "S. Huang",
                "D. Filev"
            ],
            "title": "Towards a systematic computational framework for modeling multi-agent decision-making at micro level for smart vehicles in a smart world",
            "venue": "Robotics and Autonomous Systems, vol. 144, p. 103859, 2021.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014Cooperative adaptive cruise control, multiagent reinforcement learning, connected autonomous vehicles, quantization-based efficient communication.\nI. INTRODUCTION\nCOnnected and autonomous vehicles (CAVs) have recentlygained significant attention due to their promise to create safe and sustainable future transportation systems [1], [2]. One pivotal technology of CAVs, known as cooperative adaptive cruise control (CACC), has been recognized for its capability to increase road efficiency, alleviate traffic congestion, and reduce both energy consumption and exhaust emissions [3], [4]. Specifically, by utilizing real-time vehicle-to-vehicle (V2V) communication, the primary objective of CACC is\n1Dong Chen is with the Department of Electrical and Computer Engineering, Michigan State University, Lansing, MI, 48824, USA. Email: chendon9@msu.edu.\n2Kaixiang Zhang and Zhaojian Li are with the Department of Mechanical Engineering, Michigan State University, Lansing, MI, 48824, USA. Email: {zhangk64, lizhaoj1}@msu.edu.\n3Yongqiang Wang is with the Department of Electrical and Computer Engineering, Clemson University, Clemson, SC, 29630, USA. Email: yongqiw@clemson.edu.\n4Xunyuan Yin is with the School of Chemistry, Chemical Engineering and Biotechnology, Nanyang Technological University, 62 Nanyang Drive, 637459, Singapore. Email: xunyuan.yin@ntu.edu.sg.\n5Dimitar Filev is with Hagler Institue for Advanced Study, Texas A&M University, College Station, Texas, 77843-3572, USA. Email: dimitar.filev@gmail.com.\n\u2217Zhaojian Li and Xunyuan Yin are corresponding authors. This work was partially supported by the National Science Foundation\nprojects CNS-2219488 and CNS-2219487.\nto adaptively coordinate a fleet of vehicles as a means to minimize the car-following headway and speed variations while preserving safety [5].\nYet, developing a robust CACC paradigm that tightly integrates computing, communication, and control technologies presents a considerable challenge, especially in the presence of limited onboard communication bandwidth and constrained computing resources [6]. Classical control theory and optimization-based methodologies have been employed to tackle the CACC problem [7]\u2013[11]. Specifically, some research targets the car-following model [7] and string stability [12], [13], modeling CACC within the context of a two-vehicle system. In contrast, other studies pose the CACC as optimal control problems [8], [10], [14]. These approaches hinge on precise system modeling [7], [12], [13] that are not generally available. They also typically involve online optimization, which requires significant computation resources to support real-time engineering systems [3].\nOn the other hand, CAV platoon control has also been conceptualized as a sequential decision-making problem and addressed using data-driven strategies such as reinforcement learning (RL) [1], [3], [6], [15]\u2013[19]. In particular, in [17], Soft Actor-Critic (SAC) [20] is adopted to mitigate traffic oscillations and enhance platoon stability. Furthermore, the deep deterministic policy gradient (DDPG) algorithm [21] is employed in [1] for CACC, taking into account both timevarying leading vehicle velocity and communication delays via wireless V2V communication technology. A policy-gradient RL approach is developed in [15] to ensure a safe longitudinal distance to a front vehicle. However, these approaches primarily focus on a vehicle fleet of only 2 vehicles (i.e., leaderfollower architecture). To control multiple CAVs, centralized RL approaches are commonly developed, which rely heavily on the high-bandwidth capabilities of vehicle-to-cloud (V2C) or vehicle-to-infrastructure (V2I) communication [22]. For instance, in [3], a centralized RL controller is introduced for the CACC problem in mixed-traffic scenarios via V2C communication. While these centralized control strategies have demonstrated promising results, they bear the burden of heavy communication overheads and are often plagued by a single point of failure and the curse of dimensionality [5]. These factors make them impractical for deployment in large-scale CACC systems prevalently envisioned in the future landscape.\nMore recently, multi-agent reinforcement learning (MARL) has emerged as a promising solution to address the CACC control problem involving multiple CAVs, owing to its capabilities of online adaptation and solving complex problems [5], [23], [24]. For instance, a MARL framework with both\nar X\niv :2\n30 8.\n02 34\n5v 4\n[ ee\nss .S\nY ]\n2 1\nD ec\n2 02\n3\n2 local and global reward designs is developed and evaluated in [23] on two platoons of 3 and 5 CAVs, concluding that the local reward design (i.e., independent MARL) outperforms the global reward design. However, our experiments demonstrate that while independent MARL achieves promising performance in straightforward CACC scenarios, it falls short in more complex situations (see Section V). In [25], the authors extend CACC strategies with a novel MADDPG [26] algorithm that addresses energy management challenges through a relevance ratio to ensure cooperative agent behavior. In [5], a learnable communication MARL protocol is developed to reduce information loss across two CACC scenarios, and each agent (i.e., AV) learns a decentralized control policy based on local observations and messages from connected neighbors. Moreover, blockchain is incorporated into the MARL (i.e., MADDPG) framework to enhance the privacy of CACC. Despite these advances, the aforementioned approaches uniformly adopt a Centralized Training and Decentralized Execution (CTDE) strategy, wherein agents use additional global information to guide training in a centralized manner and make decisions based on decentralized local policies [27], [28]. However, in many real-world scenarios, such as CACC, deploying a central controller (e.g., cloud facilities or roadside units) for training or online adaptation can be prohibitively expensive and complex. Moreover, the central controller needs to communicate with all local agents to exchange information, which perpetually amplifies the communication overhead on the single controller [27].\nIn this paper, we formulate CACC as a fully decentralized MARL problem, in which the agents are connected via a sparse communication network without the need for a centralized controller. To achieve this, we introduce a decentralized MARL algorithm based on a novel policy gradient update mechanism. Throughout the training process, each agent takes an individual action based solely on locally available information at each step. To stabilize training and counteract the inherent non-stationarity in MARL [28], each agent shares its estimate of the value function with its neighbors on the network, collectively aiming to maximize the average rewards of all agents across the network. Furthermore, a novel quantization-based communication scheme is further proposed, which greatly improves the communication efficiency in decentralized stochastic optimization without a substantial compromise on optimization accuracy. The main contributions and the technical advancements of this paper are summarized as follows.\n1) We formulate the CACC problem as a fully decentralized MARL framework, which facilitates fast convergence without relying on a centralized controller for both training and execution. The developed codes are available in our open-source repository1. 2) We introduce a novel effective and scalable MARL algorithm, featuring a quantization-based communication protocol to significantly enhance communication efficiency without major performance compromise. The quantization process condenses complex parameters of\n1https://github.com/DongChen06/MACACC\nthe critic network into discrete representations, facilitating efficient information exchange among agents. 3) We conduct comprehensive experiments on two CACC scenarios, and the results show that the proposed approach consistently outperforms several state-of-the-art MARL algorithms.\nThe remainder of this paper is organized as follows. Section II provides a brief overview of RL and MARL concepts. In Section III, the considered CACC problem is formulated. The problem formulation and the proposed MARL framework are introduced in Section IV whereas experiments, results, and discussions are presented in Section V. Lastly, in Section VI, we conclude the paper, summarize our contributions, and suggest potential insights for future research."
        },
        {
            "heading": "II. BACKGROUND",
            "text": "In this section, we provide an overview of the preliminaries of RL and several leading-edge MARL algorithms. These MARL algorithms will later serve as benchmarks for comparison in Section V."
        },
        {
            "heading": "A. Preliminaries of Reinforcement Learning (RL)",
            "text": "RL, often mathematically formulated as a Markov decision process (MDP), has shown great promise as a data-driven method for learning adaptive control policies [5]. Recent advancements in deep neural networks (DNNs) have further amplified their learning capabilities for intricate tasks. Successful examples of these algorithms include deep Q-network (DQN) [29], deep deterministic policy gradient (DDPG) [21], and advantage actor-critic (A2C [30]).\nIn an RL setting, at each time step t, the agent observes the state st \u2208 S \u2286 Rn from the environment, and performs an action at \u2208 A \u2286 Rm according a learned policy \u03c0(at|st). Then the environment evolves to a new state st+1 according to the transition dynamics p(\u00b7|st, at), and emits an immediate reward rt = r(st, at, st+1) to the agent. The objective of an RL agent is to learn an optimal policy \u03c0\u2217 : S \u2192 A that maps from state to action, maximizing the accumulated reward Rt = \u2211T k=0 \u03b3\nkrt+k, where rt+k is the reward at time step t + k, and \u03b3 \u2208 (0, 1] and T represent the discount factor and episode length, respectively. The state-action function is denoted as Q\u03c0(st, at) = E(Rt|st, at), representing the expected return starting from state st and taking an immediate action at, then following policy \u03c0 afterward. The optimal Q-function Q\u2217(st, at) = max\u03c0 Q\u03c0(st, at) determines the optimal greedy policy \u03c0\u2217(at|st). The state value function V \u03c0(st) = E(Rt|st) represents the expected return if starting from st and immediately following the policy \u03c0.\nIn Q-learning, the Q-function, denoted as Q\u03b8, is usually parameterized by a set of parameters \u03b8, utilizing function approximators such as Q-tables [31], linear regression (LR) [32], or DNNs [29]. The temporal difference (T Q\u03b8\u2212 \u2212Q\u03b8)(st, at) is employed to update \u03b8, where T and Q\u03b8\u2212 represent the dynamic programming (DP) operator and a frozen recent model \u03b8\u2212 [3], respectively. \u03f5 \u2212 greedy and experience replay are commonly applied in deep Q-learning to reduce the estimation variance [32]. In contrast, the policy \u03c0\u03b8 is typically directly approximated by a set of parameters \u03b8\n3 within the policy gradient method. The update of \u03b8 aims to enhance the likelihood and the loss function, represented as L(\u03c0\u03b8) = E\u03c4\u223c\u03c0\u03b8 [ \u2211T t=0 \u2207\u03b8 log \u03c0\u03b8(at|st)Rt]. Compared to Q-learning, the policy gradient is robust to nonstationary transitions within each trajectory, despite suffering from high variance [33]. [33]. Actor-critic algorithms, such as A2C\n[30], enhance the policy gradient method by introducing the advantage function A\u03c0(st, at) = Q\u03c0\u03b8 (st, at)\u2212Vw(st), thereby reducing the variance of sample return. The parameters \u03b8 are updated with the policy loss function, defined as L = E\u03c0\u03b8 [ \u2211T t=0 \u2207\u03b8 log \u03c0\u03b8(at|st)At], while the value function is updated as L = minw ED[(Rt+\u03b3Vw\u2212(st)\u2212Vw(st))2], where D and w\u2212 represent the experience replay buffer accumulating previous experiences and the parameters from prior iterations used in a target network, respectively [34]. Nevertheless, RL often encounters scalability issues in numerous real-world control problems involving multiple controllable agents, attributed to non-stationarity and partial observability [5]."
        },
        {
            "heading": "B. Multi-Agent Reinforcement Learning (MARL)",
            "text": "To tackle the challenges of scalability inherent in RL, MARL has been proposed, in which each individual agent can adapt and learn its specific policy based solely on its local observations [33]. Independent Q-learning (IQL) [35] represents the most straightforward and widely utilized methodology in this context. In IQL, each local Q-function is solely dependent on the local action, i.e., Qi(s, a) \u2248 Qi(s, ai). Similar to IQL, an alternative actor-critic version of MARL known as Independent Advantage Actor-Critic (IA2C) has been proposed in [3]. While IQL and IA2C present fully scalable solutions, they encounter difficulties in dealing with partial observability and non-stationary MDP, primarily due to its inherent assumption that all other agents\u2019 behaviors form part of the environmental dynamics, even though their policies are continually updated during the training process [33].\nTo tackle the non-stationary and partial-observability issues prevalent in MARL, in [27], the critic network is fully decentralized but each agent takes global observations and actions and then performs consensus updates. Although their approach eliminates the need for a centralized controller during the training phase, it still necessitates access to global information. Several studies have focused on leveraging communication to address the issue of partial observability. For instance, FPrint [36] investigates the impact of direct communication among agents, demonstrating that sharing low-dimensional policy fingerprints can enhance performance. In DIAL [37], each DQN agent generates the communication message together with action-value estimation, then the message is encoded and integrated with other input signals at the receiver\u2019s end. In contrast, CommNet [38] offers a more generalized communication protocol, but it merely calculates the mean of all messages rather than encoding them. NeurComm [5], [39] introduces a learnable communication protocol, where communication messages are encoded and concatenated to minimize information loss. However, these strategies generally implement a centralized controller (i.e., information aggregator or centralized critic networks) during the training and online adaptation phases, and the communication messages are typi-\nCAV Platoon leader\n1\nCAVCAV\n\ud835\udf08 2\n\u210e2 \u2026\n\ud835\udf08-1\n\u210e\ud835\udf08\nV2V communication network\n\u2026\nFigure 1: Framework of the CACC system.\ncally raw or encoded network parameters, which often impose a burden on the communication channels due to the volume of information being transmitted.\nTo tackle the aforementioned issues, in this paper, we present a fully decentralized MARL algorithm for the CACC problems, which not only offers satisfactory control performance but also facilitates efficient communication through a quantization-based communication protocol. Section V provides performance comparisons between the proposed algorithm and the previously mentioned benchmarks, demonstrating the effectiveness and potential benefits of our approach."
        },
        {
            "heading": "III. COOPERATIVE ADAPTIVE CRUISE CONTROL (CACC)",
            "text": "In this section, we introduce the system model for vehicle platooning along with the behavior model employed within the platoon. Furthermore, we present two representative CACC scenarios considered in this paper.\nA. Vehicle Dynamics\nAs shown in Figure 1, we consider a platoon, comprising V CAVs, driving along a straight road. For simplicity, we assume that all vehicles in the system share identical characteristics such as maximum allowed acceleration and deceleration. The platooning system is guided by a platoon leader vehicle (PL, 1st vehicle), while the platoon member vehicles (PMs, i \u2208 2, ...,V) travel behind the PL. Each PM i maintains a desired inter-vehicle distance (IVD) hi and velocity vi relative to its preceding vehicle i\u22121, based on its unique spacing policy [6]. The one-dimensional dynamics of vehicle i can be expressed as follows:\nh\u0307i = vi\u22121 \u2212 vi, (1a) v\u0307i = ui, (1b)\nwhere vi\u22121 and ui symbolize the velocity of its preceding vehicle and the acceleration of vehicle i, respectively. As per the design outlined in [3], the discretized vehicle dynamics, given a sampling time \u2206t, can be described by\nhi,t+1 = hi,t + \u222b t+\u2206t t (vi\u22121,\u03c4 \u2212 vi,\u03c4 )d\u03c4, (2a)\nvi,t+1 = vi,t + ui,t\u2206t. (2b)\nIn order to guarantee both comfort and safety, each vehicle must follow the following constraints [3]:\nhi,t \u2265 hmin, (3a) 0 \u2264 vi,t \u2264 vmax, (3b)\numin \u2264 ui,t \u2264 umax, (3c)\n4 where hmin = 1 m, vmax = 30 m/s, umin = \u22122.5 m/s2 < 0 and umax = 2.5 m/s2 > 0 represent the minimum safe headway, maximum speed, deceleration, and acceleration limits, respectively.\nB. Vehicle Behavior\nThe behaviors of vehicles in the platoon are simulated using the optimal velocity model (OVM) [40]. The OVM has been widely used in traffic flow modeling due to its ability to capture real human driving behaviors [5]. The principal equation of OVM for the ith vehicle is defined as follows:\nui,t = \u03b1i(v \u25e6(hi,t;h s, hg)\u2212 vi,t) + \u03b2i(vi\u22121,t \u2212 vi,t), (4)\nwhere \u03b1i and \u03b2i are the headway gain and relative velocity gain, respectively. These parameters serve as representations of human driver behavior, encapsulating the influence of both spacing and relative speed in determining vehicle acceleration. Here, hs = 5 m and hg = 35 m denote the stop headway and full-speed headway, both of which are key to understanding traffic dynamics at different vehicle densities. Furthermore, v\u25e6 represents the headway-based velocity policy, which is defined as:\nv\u25e6(h) \u225c  0, if h < hs, 1 2vmax(1\u2212 cos (\u03c0 h\u2212hs hg\u2212hs )), if h\ns \u2264 h \u2264 hg, vmax, if h > hg.\n(5) This policy function serves as an optimal velocity strategy for each vehicle based on the current headway to the preceding vehicle. At small headways less than or equal to hs, the optimal velocity is zero, highlighting the need for the ego vehicle to stop for preventing potential collisions. For headways within the range hs to hg , the optimal velocity gradually increases by following a cosine curve until reaching the maximum velocity. For large headways greater than or equal to hg , the optimal velocity is capped at the vehicle\u2019s maximum speed, ensuring both safety and efficiency in the traffic flow. This strategy significantly contributes to maintaining fluidity in vehicular traffic under various density conditions. The OVM will be used to simulate the behavior of vehicles where the RL will train the driving hyper-parameters (\u03b1i, \u03b2i). See Section IV for more details on the RL formulation. In this paper, our primary focus is on validating the fully decentralized MARL approach in the context of CACC. As for the exploration of more complex vehicle dynamics [13], this will be a key area of investigation in our future work."
        },
        {
            "heading": "C. Two CACC Scenarios",
            "text": "In this paper, following [5] two different CACC scenarios are investigated: \u201cCatchup\u201d and \u201cSlowdown\u201d. Recall that the objective of CACC is to adaptively control a fleet of CAVs in order to reduce the car-following headway to a pre-specified value (e.g., h\u2217 = 20 m) and achieve a target velocity (e.g., v\u2217 = 15 m/s), by leveraging real-time V2V communications. For the \u201cCatchup\u201d scenario, the platoon members (PMs) (i = 2, ...,V) are initialized with states vi,0 = v\u2217t and hi,0 = h \u2217 t , while the platoon leader (PL) is initialized with states v1,0 = v\u2217t and h1,0 = a \u00b7 h\u2217t , where a is a random variable uniformly distributed between 1.5 and 2.5. In contrast,\nduring the \u201cSlowdown\u201d scenario, all vehicles (i = 1, ...,V) have initial velocities vi,0 = b \u00b7 v\u2217t and hi,0 = h\u2217t , where b is uniformly distributed between 1.5 and 2.5. Here, v\u2217t linearly decreases to 15 m/s within the first 30 seconds and then remains constant. The \u201cSlowdown\u201d scenario poses a more complex and challenging task than the \u201cCatchup\u201d scenario due to the necessity for all vehicles to coordinate their deceleration rates and maintain safe inter-vehicle distances with higher accuracy, thereby requiring more precise control strategies. Examples of the headway and speed profiles of the CAVs in these scenarios are illustrated in Figures 3 and 4."
        },
        {
            "heading": "IV. CACC AS MARL",
            "text": "In this section, we first formulate the considered CACC problem as a partially observable Markov decision process (POMDP). Subsequently, we present our fully decentralized MARL algorithm, which represents our primary strategy for addressing the challenges presented in the CACC problem. Then, we introduce the quantization-based communication protocol to enhance the efficiency of agent communication in the MARL framework without major performance degradation."
        },
        {
            "heading": "A. MARL Formulation",
            "text": "In this paper, we model the CACC problem as a modelfree multi-agent network [5], where each agent (i.e., AV) is capable of communicating with the vehicles ahead and behind via V2V communication channels. We denote the global state space and action space as S := \u00d7i\u2208\u03bdSi and A := \u00d7i\u2208\u03bdAi, respectively. The intrinsic dynamics of the system can be characterized by the state transition distribution P: S \u00d7 A \u00d7 S \u2192 [0, 1]. We propose a fully decentralized MARL framework where each agent i (equivalently, AV i) has a partial view of the environment, specifically the surrounding vehicles, which accurately reflects the practical scenario where AVs are limited to sensing or communicating with neighboring vehicles, thereby rendering the overall dynamical system as a POMDP. This POMDP, MG , can be delineated by the following tuple MG = ({Ai,Si,Ri}i\u2286V+1, T ):\n\u2022 Action space: In the considered CACC problem, the action at \u2208 Ai is straightforwardly related to the longitudinal control. However, due to the data-driven nature of RL, formulating a safe and robust longitudinal control strategy poses a significant challenge [3]. To address this, we adopt OVM (see Section III-B, [40]) to carry out the longitudinal vehicle control. The OVM control behavior is affected by various hyperparameters: headway gains \u03b1, relative velocity gain \u03b2, stop headway hs, and fullspeed headway hg . Usually, (\u03b1; \u03b2) represents the driving behavior of a human driver. However, following [5], we leverage MARL to propose suitable values of (\u03b1; \u03b2) for each OVM controller. These recommended values are selected from a set of four different levels: {(0, 0), (0.5, 0), (0, 0.5), (0.5, 0.5)}. Subsequently, the longitudinal action can be computed using Eq. 4 and Eq. 5. \u2022 State space: The state space represents the description of the environment. The state of agent i, Si, is defined as [v, vdiff , vh, h, u], where v = (vi,t \u2212 vi,0)/vi,0\n5 denotes the current normalized vehicle speed. vdiff = clip((vi\u22121,t \u2212 vi,t)/5,\u22122, 2) represents clipped vehicle speed difference with its leading vehicle. vh = clip((v\u25e6(h) \u2212 vi,t)/5,\u22122, 2), h = (hi,t + (vi\u22121,t \u2212 vi,t)\u2206t \u2212 h\u2217)/h\u2217, and u = ui,t/umax are the headwaybased velocity defined in Eq. 5, normalized headway distance, and acceleration, respectively.\n\u2022 Reward function: The reward function ri,t is pivotal for training the RL agents to exhibit the desired behaviors. With our objective being the training of our agents to achieve a predefined car-following headway h\u2217 = 20 m and velocity v\u2217 = 15 m/s, the reward assigned to the ith agent at each time step t is designed as follows:\nri,t = w1(hi,t \u2212 h\u2217)2 + w2(vi,t \u2212 v\u2217)2\n+ w3u 2 i,t + w4(2hs \u2212 hi,t)2+,\n(6)\nwhere w1, w2, w3, and w4 are the weighting coefficients. In this equation, the first two terms, (hi,t \u2212 h\u2217)2 and (vi,t\u2212v\u2217)2, penalize deviations from the desired headway and velocity, encouraging the agent to achieve these targets closely. The third term, u2i,t, is included to minimize abrupt accelerations, thereby promoting smoother and more comfortable rides for passengers. Lastly, the term (2hs \u2212 hi,t)2+ functions as a safety constraint, penalizing the agent heavily if the inter-vehicle distance is less than twice the stop headway hs, which is critical for preventing collisions and ensuring the safety of the vehicle platoon. The \u201c+\u201d operator represents that this term contributes to the reward only when hi,t is less than 2hs, similar to the Rectified Linear Unit (ReLU) function. This comprehensive reward design serves to balance performance, comfort, and safety considerations in the CACC system. Upon a collision, if the intervehicle distance hi,t \u2264 1 m, each agent is subjected to a substantial penalty of 1000, resulting in immediately terminating the training episode. \u2022 Transition probabilities: The transition probability, T (s\u2032|s, a), characterizes the underlying dynamics of the system. Given that our approach is a model-free MARL framework, we do not assume any prior knowledge of this transition probability while developing our MARL algorithm."
        },
        {
            "heading": "B. Fully Decentralized MARL",
            "text": "In this paper, we formulate the CACC as a fully decentralized MARL problem, where each agent (i.e., an AV) independently decides its action based on its local observation during both training and execution. Importantly, this structure does not require a centralized controller, meaning that each agent possesses its own individual policy networks. During the learning phase, agents rely on locally received rewards to train and update these networks. In this paper, we employ a MARL framework in which each agent is equipped with its actor-critic network [30], and the policy network for agent i is updated with gradient ascend and the gradient is defined as:\n\u2207\u03b8L(\u03c0\u03b8i) = E\u03c0\u03b8i\n[ T\u2211\nt=0\n\u2207\u03b8 log \u03c0\u03b8i(ai,t|si,t)A \u03c0\u03b8i i,t\n] , (7)\nwhere A \u03c0\u03b8i i,t = ri,t + \u03b3V \u03c0\u03d5i (si,t+1) \u2212 V \u03c0\u03d5i (si,t) is the advantage function and V \u03c0\u03d5i (si,t) is the state value function, which is updated following the loss function:\nLV \u03c0\u03d5i = min\n\u03d5i EDi\n[ ri,t + \u03b3V \u03c0\u03d5i (si,t+1)\u2212 V \u03c0\u03d5i (si,t) ]2 . (8)\nDespite each agent learning independently, the overall goal of the cooperative MARL framework is to optimize the average global reward rg,t = 1V \u2211V i=1 ri,t. To address the non-stationary, in [27], the update of the policy network is executed independently by each agent, eliminating the need for inferring other agents\u2019 policies. However, when it comes to updating the critic network, a collaborative approach is adopted, in which each agent shares its estimate of the value function xi with its neighboring agents within the network through a \u201cmean\u201d operation, i.e., xk+1i = 1 |Ni| \u2211 j\u2208Ni x k j , where Ni is the neighboring set of agent i. This allows for the joint evolution and continuous improvement of the system\u2019s overall performance. However, their approach is based on the assumption that all agents are homogeneous, sharing the same characteristics. While this simplifies the problem structure, it does not adequately represent the intrinsic diversity of individual agents, which is particularly relevant for the CACC scenario where diverse strategies are needed based on vehicles\u2019 positions, speeds, and proximities. To address this concern, we propose a novel update strategy that fosters a balance between individual learning and collaborative influence from neighboring agents. Specifically, we assume that the agents interact on an undirected graph, and the interaction can be described by a weight matrix W . If agent i and agent j can communicate and interact with each other, then the (i, j)-th entry of W , i.e., wij , is positive (e.g., 1.0). Otherwise, wij is zero. During the training, the update strategy is designed as\nxk+1i = x k i + \u03f5 \u2211 j\u2208Ni \u03c9ij(x k j \u2212 xki )\u2212 \u03bbgki , (9)\nwhere gki is the gradient that agent i obtains at iteration k for optimization, \u03f5 is the scaling factor used to modulate the impact or collaborative influence from neighboring agents, and \u03bb = 5.0 \u00d7 10\u22124 is the learning rate that adjusts the influence of the gradient on the update process. This novel update strategy fosters collaboration among the agents while preserving the individual learning capabilities of each, thereby striking a balance between global performance optimization and localized adaptivity.\nRemark 1. The scaling factor \u03f5 plays an important role in the update scheme. If \u03f5 is too large, the agent allocates more influence from the neighboring agents. On the other hand, if \u03f5 is too small, the agent will concentrate more on its own updates. In this work, we use cross-validations and find that \u03f5 = 1.0 \u00d7 10\u22123 and \u03f5 = 1.0 \u00d7 10\u22124 are the optimal choices for \u201cCatchup\u201d and \u201cSlowdown\u201d scenarios, respectively.\nThe update strategy of the proposed fully decentralized MARL for CACC (abbreviated as MACACC) is given in Algorithm 1. C. Quantization-based Communication Protocol\nTo enhance the communication efficiency among agents in our MARL framework, we propose a strategy of trans-\n6 Algorithm 1: MACACC for CACC 1 Public parameters: W , \u03f5, \u03bb, x0i for all i, the total\nnumber of iterations k 2 for ith agent do 3 Determine the local gradient gki for the critic network; 4 Send critic estimate to all neighboring agents j \u2208 Ni; 5 After receiving xkj from all j \u2208 Ni, update network\nparameters as\nxk+1i = x k i + \u03f5 \u2211 j\u2208Ni \u03c9ij(x k j \u2212 xki )\u2212 \u03bbgki\n6 end\nmitting quantized parameters, rather than the raw parameters of the critic network. This approach is especially important for autonomous driving applications that are often subject to limited communication bandwidth. By transmitting compact, quantized parameters instead of raw data, we ensure optimal use of available bandwidth, thereby fostering efficient and effective communication among the vehicles in the network. Quantization-based techniques have been studied in distributed optimization and learning [41], [42], however, to the best of our knowledge, this is the first work that incorporates a quantization-based communication protocol into the MARL framework for achieving communication-efficient CACC.\nLet the parameters of the critic/value network be denoted as x = [x1, x2, ..., xd]\nT , with d representing the dimension of the parameter vector. We then apply a quantization function Q(x) to these parameters, yielding a quantized parameter vector [q1, q2, ..., qd] T . The quantization rule is defined as:\nqi = r \u00b7 sign(xi)bi. (10)\nIn (10), r is a non-negative real number no less than the \u2113\u221e norm of x, and sign(\u00b7) represents the sign function, which returns the sign of any given real number. The factor bi is a random variable following a designed distribution determined by the magnitude of the corresponding parameter xi. Let n be the resolution of the quantization, and then bi is selected from the set {0, 1n , 2 n , \u00b7 \u00b7 \u00b7 , 1}, indicating that qi \u2208 {\u2212r,\u2212n\u22121n r, \u00b7 \u00b7 \u00b7 , 0, \u00b7 \u00b7 \u00b7 , n\u22121 n r, r}. Let 0 \u2264 m \u2264 n \u2212 1 be an integer such that |xi| belongs to the interval [mn r, m+1 n r]. Then the probability distribution of bi is determined by\nP (bi = m+ 1 n |x) = n|xi| \u2212mr r , (11a) P (bi = m\nn |x) = 1\u2212 n|xi| \u2212mr r , (11b)\nP (bi = l\nn |x) = 0, l = 0, \u00b7 \u00b7 \u00b7 ,m\u2212 1,m+ 2, \u00b7 \u00b7 \u00b7 , n.(11c)\nIt can be concluded that if the magnitude of |xi| is closer to m+1 n r, then the higher the probability that bi will be m+1 n , and vice versa. We denote the quantization-based MACACC algorithm as QMACACC (n). An extremely condensed version of QMACACC is QMACACC (1), in which only three discrete numbers {\u2212r, 0, r} are used to represent each parameter, and\nbi is defined as:\nP (bi = 1|x) = |xi| r , (12a)\nP (bi = 0|x) = 1\u2212 |xi| r . (12b)\nRemark 2. The quantization resolution n is a crucial hyperparameter in the quantization scheme. When n is small, the sparse quantization intervals could lead to excessive loss of information, negatively impacting the performance of the MARL framework. Conversely, when n is large, the computational and communication overheads could increase due to the larger number of potential quantized values. Therefore, choosing an appropriate value of n is crucial for balancing communication efficiency and the performance of the MARL framework. An empirical evaluation of different n values will be conducted in Section V.\nThe update strategy of the proposed quantization-based MARL (i.e., QMACACC (n)) for CACC is given in Algorithm 2.\nAlgorithm 2: QMACACC (n) for CACC 1 Public parameters: W , \u03f5, \u03bb, x0i for all i, the total\nnumber of iterations k 2 for ith agent do 3 Determine the local gradient gki for the critic network; 4 Quantize critic estimate according to Eqs. 10 and 11 and send to all neighboring agents j \u2208 Ni; 5 After receiving Q(xkj ) from all j \u2208 Ni, update\nstate as\nxk+1i = x k i + \u03f5 \u2211 j\u2208Ni \u03c9ij(Q(xkj )\u2212Q(xki ))\u2212 \u03bbgki\n6 end"
        },
        {
            "heading": "V. EXPERIMENTAL RESULTS & DISCUSSIONS",
            "text": "In this section, we evaluate our MARL framework in two CACC scenarios detailed in Section III-C. Firstly, we benchmark our approach against several state-of-the-art MARL strategies. Then, we demonstrate the effectiveness of our quantization-based communication protocol."
        },
        {
            "heading": "A. General Setups",
            "text": "To demonstrate the efficiency and robustness of the proposed approach, we compare it with several state-of-theart MARL benchmark controllers discussed in Section II-B. Specifically, IA2C performs independent learning, while ConseNet [27] takes the \u201cmean\" operation during updating critic networks, and FPrint [36] incorporates the neighbors\u2019 policy into the inputs. DIAL [37], CommNet [38] and NeurComm [5], on the other hand, are implementations with learnable communication protocols, incorporating more messages from the neighbors, e.g., neighboring states or policy information, relying on higher communication bandwidth. All algorithms use the same DNN structures: one fully-connected layer for input state encoding and one LSTM layer for message extracting. All hidden layers have 64 units. During the training, the network is initialized with the state-of-the-art orthogonal initializer [43]. We train each model over 1M steps, with\n7 0.0 0.2 0.4 0.6 0.8 1.0 Training steps 1e6 \u22122000 \u22121500 \u22121000 \u2212500 0 Tr ai ni ng re w ar d Catchup IA2C NuerComm ConseNet CommNet DIAL FPrint MACACC 0.0 0.2 0.4 0.6 0.8 1.0 Training steps 1e6 \u22124000 \u22123500 \u22123000 \u22122500 \u22122000 \u22121500 \u22121000 \u2212500 Tr ai ni ng re w ar d Slowdown\nFigure 2: Training curves comparison between the proposed MARL policy (MACACC) and 6 state-of-the-art MARL benchmarks.\nTemporal Average Metrics IA2C FPrint ConseNet NeurComm CommNet DIAL MACACC\navg vehicle headway [m] 19.43 20.02 20.28 21.77 22.38 21.86 19.91 avg vehicle velocity [m/s] 15.00 15.34 15.30 15.04 15.01 15.01 15.32 collision number 0 0 0 0 0 0 0 avg vehicle headway [m] - 15.16 9.23 11.45 4.90 9.71 20.44 avg vehicle velocity [m/s] - 13.10 8.08 10.32 4.18 8.91 16.61 collision number 50 14 29 22 38 26 0\n\u03b3 = 0.99, actor learning rate 5.0 \u00d7 10\u22124, and critic learning rate 2.5 \u00d7 10\u22124. Also, each algorithm is trained three times with different random seeds for generalization purposes. Each training takes about 12 hours on a Ubuntu 18.04 server with an AMD 9820X processor and 64 GB memory.\nThe hyperparameter w1, w2, w3, and w4 in the reward\nfunction (6) are set to -1.0, -1.0, -0.1, and -5.0, respectively, with a significant emphasis on penalizing situations where the safety headway distance is insufficient. Considering a simulated traffic environment over a period of T = 60 seconds, we define \u2206t = 0.1 seconds as the interaction period between RL agents and the traffic environment, so that the environment\n8 0.0 0.2 0.4 0.6 0.8 1.0 Training steps 1e6 \u22121500 \u22121250 \u22121000 \u2212750 \u2212500 \u2212250 0 Tr ai ni ng re w ar d Catchup ConseNet MACACC QMACACC (1) QMACACC (2) QMACACC (4) 0.0 0.2 0.4 0.6 0.8 1.0 Training steps 1e6 \u22124000 \u22123500 \u22123000 \u22122500 \u22122000 \u22121500 \u22121000 \u2212500 Tr ai ni ng re w ar d Slowdown FPrint MACACC QMACACC (1) QMACACC (2) QMACACC (4) \u2212100 \u221250\nFigure 5: Training curves comparison between the proposed MARL policy (MACACC) and Quantization-based MACACC (QMACACC (n)).\nn=0 n=4 n=2 n=1 0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nN or\nm al\niz ed\nc om\nm un\nic at\ned b\nits Catchup\nn=0 n=4 n=2 n=1 0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nN or\nm al\niz ed\nc om\nm un\nic at\ned b\nits Slowdown\n1.00\n1.02 1.04 N or m al iz ed e va lu at io n re\nw ar\nd\n0.7\n0.8\n0.9\n1.0\nN or\nm al\niz ed\ne va\nlu at\nio n\nre w\nar d\nFigure 6: Transmitted message and execution performance comparison between the proposed MARL policy (MACACC (n = 0)) and Quantization-based MACACC (QMACACC (n = 1, 2, 4)).\nis simulated for \u2206t seconds after each MDP step. In the following experiments, we assume the platoon size to be V = 8, implying that there are a total of 8 CAVs in the platoon. The impact of different platoon sizes on our model\u2019s performance will be studied and presented in Section V-D."
        },
        {
            "heading": "B. Comparison with State-of-the-Art Benchmarks",
            "text": "Figure 2 shows the performance comparison in terms of the learning curves between the proposed approach MACACC and several state-of-the-art MARL benchmarks. As expected, the proposed approach achieves the best performance, evidenced by higher training rewards in both CACC scenarios. In the more challenging \u201cSlowdown\u201d environment, the proposed approach shows its greater advantages of sample efficiency as seen from the fastest convergence speed and best training reward compared to other algorithms.\nAfter training, we evaluate each algorithm 50 times with different initial conditions. Table I shows the evaluation performance comparison over the trained MARL policies. The proposed method consistently outperforms the benchmarks in all CACC scenarios in terms of the evaluation reward, which reveals the overall evaluation metrics including vehicle headway, velocity, acceleration, and safety as described in Eq. 6. Table II shows the key evaluation metrics in CACC. The best headway and velocity averages are the closest ones to h\u2217 = 20 m, and v\u2217 = 15 m/s. Note the averages are only computed from safe execution episodes, and we use another metric \u201ccollision number\u201d to count the number of episodes where a collision happens within the horizon. Ideally, \u201ccollisionfree\u201d is the top priority. It is clear that our approach achieves promising performance in the \u201cCatchup\u201d environment, and the best performance in the harder \u201cSlowdown\u201d environment. All algorithms achieve relatively good performance in the \u201cCatchup\u201d environment with zero collision number. It is surprising that IA2C achieves excellent average vehicle velocity at v\u2217. However, it demonstrates high collision numbers (i.e., 50) in the \u201cSlowdown\u201d scenario due to non-stationary issues since there is no communication between agents. FPrint yields the best average vehicle headway in the \u201cCatchup\u201d environment, while it has 14 out of 50 collisions during the testing. On the other hand, NeurComm and CommNet show great average vehicle velocity in the \u201cCatchup\u201d environment, however, they failed to track the optimal headway, resulting in average high headway of 21.77 m and 22.38 m, respectively. It is noted that ConseNet achieves promising performance in\n9 0.00 0.25 0.50 0.75 1.00 Training steps 1e6 \u2212150 \u2212100 \u221250 Tr ai ni ng re w ar d Platoon size = 2 ConseNet MACACC QMACACC (1) 0.00 0.25 0.50 0.75 1.00 Training steps 1e6 \u2212200 \u2212150 \u2212100 \u221250 0 Tr ai ni ng re w ar d Platoon size = 8 ConseNet MACACC QMACACC (1) 0.00 0.25 0.50 0.75 1.00 Training steps 1e6 \u2212250 \u2212200 \u2212150 \u2212100 \u221250 Tr ai ni ng re w ar d Platoon size = 12 ConseNet MACACC QMACACC (1)\n0.00 0.25 0.50 0.75 1.00 Training steps 1e6\n\u2212400\n\u2212300\n\u2212200\n\u2212100\nTr ai\nni ng\nre w\nar d\nPlatoon size = 2\nFPrint MACACC QMACACC (1)\n0.00 0.25 0.50 0.75 1.00 Training steps 1e6\n\u2212600\n\u2212400 \u2212200 Tr ai ni ng re w ar\nd\nPlatoon size = 8\nFPrint MACACC QMACACC (1)\n0.00 0.25 0.50 0.75 1.00 Training steps 1e6\n\u2212400\n\u2212350\n\u2212300\n\u2212250\n\u2212200\n\u2212150\nTr ai\nni ng\nre w\nar d\nPlatoon size = 12\nFPrint MACACC QMACACC (1)\n\u221215\n\u221210\nFigure 7: Normalized training curves comparison between MACACC, QMACACC (1), and the top baseline methods with different platoon sizes in the CACC scenarios: \u201cCatchup\u201d (above) and \u201cSlowdown\u201d (below).\nthe \u201cCatchup\u201d environment, with a zero collision rate, average vehicle headway (20.28 m), and velocity (15.30 m/s) close to the optimal values. However, it yields high collision numbers (29 out of 50) in the \u201cSlowdown\u201d scenario as it simply encourages all agents to behave similarly via the \u201caverage\u201d operations during training, which is especially impractical for complex scenarios, such as \u201cSlowdown\u201d, where agents need to react differently to the speed and headway changes.\nFigures 3 and 4 show the corresponding headway and velocity profiles for the selected controllers for the two CACC scenarios. In the \u201cCatchup\u201d scenario, as expected, the MACACC controller is able to achieve steady state v\u2217 and h\u2217 for the first and last vehicles of the platoon, whereas the ConseNet controller still faces difficulties in eliminating the perturbation through the platoon. In a harder \u201cSlowdown\u201d environment, MACACC is still able to achieve optimal headway at about 60 seconds and reach the optimal velocity quickly. However, FPrint fails the control task with a collision that happened at about 35 seconds. This may be because simply incorporating neighboring agents\u2019 policies might not be sophisticated enough to accurately model and adapt to the intricacies among agents."
        },
        {
            "heading": "C. Performance of the Quantization-based MACACC",
            "text": "In this subsection, we evaluate the effectiveness of the proposed quantization-based communication protocol with different quantization resolutions. As shown in Figure 5, in the less complex \u201cCatchup\u201d scenario, minor quantization appears to improve control performance. This could be attributed to the fact that the quantization process introduces a level of randomness during the training phase, thereby fostering improved\nexploration by the agents, as discussed in [44]. Conversely, in the more challenging \u201cSlowdown\u201d scenario, the impact of quantization results in a more significant performance degradation relative to the \u201cCatchup\u201d scenario. Nonetheless, even with extremely quantized communication, such as QMACACC (1), our proposed approach continues to surpass the performance of the robust baseline method, FPrint.\nFigure 6 presents the number of bits required for each communicated parameter as well as the corresponding test performance at varying quantization resolutions. For better visualization, these values are normalized with corresponding maximum values. Within the \u201cCatchup\u201d scenario, QMACACC (1) manages to achieve 98.63% of the control performance achieved by the non-quantized version, i.e., QMACACC (0), while only requiring 12.5% of the communicated bits. However, in the \u201cSlowdown\u201d scenario, QMACACC (1) can only realize 64.64% of the control performance of the non-quantized version, i.e., QMACACC (0). This underscores a trade-off between the benefits of enhanced communication efficiency brought about by quantization and the associated diminution in control performance.\nD. Impact of Platoon Size\nIn this subsection, we explore how variations in platoon sizes affect the performance of our model. The normalized training curves comparison among MACACC, QMACACC (1), and the top-performing baseline methods under different platoon sizes (i.e., V \u2208 2, 8, 12) in the two CACC scenarios is illustrated in Figure 7. As the platoon size increases, the performance of all algorithms decreases in both CACC\n10\nscenarios. That is attributed to the increased complexities and intricacies associated with managing and coordinating a larger number of agents. As the platoon size increases, the algorithms must deal with more sophisticated inter-agent dynamics, thereby extending the computation time and potentially slowing down the convergence toward an optimal policy. Despite the varying platoon sizes, all the algorithms achieve comparable performance in the \u201cCatchup\u201d scenario. However, in the more challenging \u201cSlowdown\u201d environment, MACACC (n = 0, 1) consistently outperforms the baseline method (i.e., FPrint) under different platoon sizes, showing the impressive scalability of our proposed approach."
        },
        {
            "heading": "E. Robustness to Unseen Scenarios",
            "text": "In this subsection, we assess MACACC\u2019s performance in new scenarios. Although the algorithms were trained as outlined in Section III-C, they were evaluated in distinct scenarios. In the \u201cCatchup\u201d environment, the platoon leader (PL) is initially set with states v1,0 = v\u2217t and h1,0 = a \u00b7 h\u2217t , for a \u2208 [1.5, 2.5], and subsequently tested with a \u2208 [2.5, 3.5]. In the \u201cSlowdown\u201d scenario, every vehicle (i = 1, \u00b7 \u00b7 \u00b7 ,V) starts with velocities vi,0 = b \u00b7 v\u2217t and hi,0 = h\u2217t , where b \u2208 [1.5, 2.5], and is tested on b \u2208 [0.5, 1.5]. Table III shows the execution results in these unexplored scenarios. It is clear that MACACC consistently surpasses baseline methods in both settings. However, MACACC (1) shows slightly inferior performance compared to MACACC, attributable to quantization."
        },
        {
            "heading": "VI. CONCLUSION",
            "text": "In this paper, we have addressed the CACC problem by formulating it as a fully decentralized MARL problem. This novel approach eliminated the need for a centralized controller during both training and execution, thereby enhancing the system\u2019s scalability and robustness. Additionally, we introduced an innovative quantization-based communication protocol, which significantly enhanced the communication efficiency among the agents. To validate our proposed approach, we undertook comprehensive experiments and compared it with several state-of-the-art MARL algorithms. The results demonstrated that our approach can provide superior control performance and communication efficiency. These results underscore the potential of our fully decentralized MARL and quantization-based communication protocol as a robust and effective solution for real-world CACC problems.\nIn this paper, we employed the optimal velocity model (OVM) to emulate certain aspects of traffic flow behaviors due to its simplicity and efficacy. However, it is worth noting that OVM oversimplifies some intricate nature of human driving behaviors, and its performance may degrade at high traffic densities where interactions between vehicles become more complex. As a result, future research endeavors will focus on\nthe integration of more comprehensive human driver models and vehicle dynamics to improve simulation accuracy and stability. Additionally, to eliminate the need for a training process, we plan to explore the best response dynamics approach, as proposed in [45], where agents communicate their actions using the QMACACC protocol."
        }
    ],
    "title": "Communication-Efficient Decentralized Multi-Agent Reinforcement Learning for Cooperative Adaptive Cruise Control",
    "year": 2023
}