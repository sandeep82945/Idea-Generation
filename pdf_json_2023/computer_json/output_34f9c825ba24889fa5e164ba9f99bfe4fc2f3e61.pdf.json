{
    "abstractText": "Large language models (LLMs) are popular for high-quality text generation but can produce harmful content, even when aligned with human values through reinforcement learning. Adversarial prompts can bypass their safety measures. We propose LLM SELF DEFENSE, a simple approach to defend against these attacks by having an LLM screen the induced responses. Our method does not require any fine-tuning, input preprocessing, or iterative output generation. Instead, we incorporate the generated content into a pre-defined prompt and employ another instance of an LLM to analyze the text and predict whether it is harmful. We test LLM SELF DEFENSE on GPT 3.5 and Llama 2, two of the current most prominent LLMs against various types of attacks, such as forcefully inducing affirmative responses to prompts and prompt engineering attacks. Notably, LLM Preprint. Under review. ar X iv :2 30 8. 07 30 8v 3 [ cs .C L ] 2 4 O ct 2 02 3 SELF DEFENSE succeeds in reducing the attack success rate to virtually 0 using both GPT 3.5 and Llama 2.",
    "authors": [
        {
            "affiliations": [],
            "name": "Mansi Phute"
        }
    ],
    "id": "SP:f49ecae8fd051d39b28124838ab352b439dc623d",
    "references": [
        {
            "authors": [
                "Andrea Agostinelli",
                "Timo I Denk",
                "Zal\u00e1n Borsos",
                "Jesse Engel",
                "Mauro Verzetti",
                "Antoine Caillon",
                "Qingqing Huang",
                "Aren Jansen",
                "Adam Roberts",
                "Marco Tagliasacchi"
            ],
            "title": "Musiclm: Generating music from text",
            "year": 2023
        },
        {
            "authors": [
                "Yuntao Bai",
                "Saurav Kadavath",
                "Sandipan Kundu",
                "Amanda Askell",
                "Jackson Kernion",
                "Andy Jones",
                "Anna Chen",
                "Anna Goldie",
                "Azalia Mirhoseini",
                "Cameron McKinnon"
            ],
            "title": "Constitutional ai: Harmlessness from ai feedback",
            "year": 2022
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language Models are Few-Shot Learners",
            "venue": "Advances in neural information processing systems",
            "year": 2020
        },
        {
            "authors": [
                "Nicholas Carlini",
                "Milad Nasr",
                "Christopher A Choquette-Choo",
                "Matthew Jagielski",
                "Irena Gao",
                "Anas Awadalla",
                "Pang Wei Koh",
                "Daphne Ippolito",
                "Katherine Lee",
                "Florian Tramer"
            ],
            "title": "Are aligned neural networks adversarially aligned?",
            "year": 2023
        },
        {
            "authors": [
                "Jeremy Cohen",
                "Elan Rosenfeld",
                "Zico Kolter"
            ],
            "title": "Certified adversarial robustness via randomized smoothing",
            "venue": "In: international conference on machine learning. PMLR",
            "year": 2019
        },
        {
            "authors": [
                "Shehzaad Dhuliawala",
                "Mojtaba Komeili",
                "Jing Xu",
                "Roberta Raileanu",
                "Xian Li",
                "Asli Celikyilmaz",
                "Jason Weston"
            ],
            "title": "Chain-of-Verification Reduces Hallucination in Large Language Models",
            "year": 2023
        },
        {
            "authors": [
                "Qingxiu Dong",
                "Lei Li",
                "Damai Dai",
                "Ce Zheng",
                "Zhiyong Wu",
                "Baobao Chang",
                "Xu Sun",
                "Jingjing Xu",
                "Zhifang Sui"
            ],
            "title": "A survey for in-context learning",
            "year": 2022
        },
        {
            "authors": [
                "Amelia Glaese",
                "Nat McAleese",
                "Maja Tr\u0119bacz",
                "John Aslanides",
                "Vlad Firoiu",
                "Timo Ewalds",
                "Maribeth Rauh",
                "Laura Weidinger",
                "Martin Chadwick",
                "Phoebe Thacker"
            ],
            "title": "Improving alignment of dialogue agents via targeted human judgements",
            "year": 2022
        },
        {
            "authors": [
                "K Greshake",
                "S Abdelnabi",
                "S Mishra",
                "C Endres",
                "T Holz",
                "M Fritz"
            ],
            "title": "Not what you\u2019ve signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection, May 2023",
            "venue": "URL http://arxiv",
            "year": 2023
        },
        {
            "authors": [
                "Maanak Gupta",
                "CharanKumar Akiri",
                "Kshitiz Aryal",
                "Eli Parker",
                "Lopamudra Praharaj"
            ],
            "title": "From ChatGPT to ThreatGPT: Impact of generative AI in cybersecurity and privacy",
            "venue": "IEEE Access",
            "year": 2023
        },
        {
            "authors": [
                "Rongjie Huang",
                "Mingze Li",
                "Dongchao Yang",
                "Jiatong Shi",
                "Xuankai Chang",
                "Zhenhui Ye",
                "Yuning Wu",
                "Zhiqing Hong",
                "Jiawei Huang",
                "Jinglin Liu"
            ],
            "title": "Audiogpt: Understanding and generating speech, music, sound, and talking head",
            "year": 2023
        },
        {
            "authors": [
                "Neel Jain",
                "Avi Schwarzschild",
                "Yuxin Wen",
                "Gowthami Somepalli",
                "John Kirchenbauer",
                "Ping-yeh Chiang",
                "Micah Goldblum",
                "Aniruddha Saha",
                "Jonas Geiping",
                "Tom Goldstein"
            ],
            "title": "Baseline Defenses for Adversarial Attacks Against Aligned Language Models",
            "year": 2023
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang Shane Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa"
            ],
            "title": "Large language models are zero-shot reasoners",
            "venue": "Advances in neural information processing systems",
            "year": 2022
        },
        {
            "authors": [
                "Klas Leino",
                "Zifan Wang",
                "Matt Fredrikson"
            ],
            "title": "Globally-robust neural networks",
            "venue": "In: International Conference on Machine Learning. PMLR",
            "year": 2021
        },
        {
            "authors": [
                "Yuhui Li",
                "Fangyun Wei",
                "Jinjing Zhao",
                "Chao Zhang",
                "Hongyang Zhang"
            ],
            "title": "RAIN: Your Language Models Can Align Themselves without Finetuning",
            "year": 2023
        },
        {
            "authors": [
                "Yi Liu",
                "Gelei Deng",
                "Zhengzi Xu",
                "Yuekang Li",
                "Yaowen Zheng",
                "Ying Zhang",
                "Lida Zhao",
                "Tianwei Zhang",
                "Yang Liu"
            ],
            "title": "Jailbreaking chatgpt via prompt engineering: An empirical study",
            "year": 2023
        },
        {
            "authors": [
                "Alexandra Luccioni",
                "Joseph Viviano"
            ],
            "title": "What\u2019s in the box? an analysis of undesirable content in the Common Crawl corpus",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
            "year": 2021
        },
        {
            "authors": [
                "Aleksander Madry",
                "Aleksandar Makelov",
                "Ludwig Schmidt",
                "Dimitris Tsipras",
                "Adrian Vladu"
            ],
            "title": "Towards deep learning models resistant to adversarial attacks",
            "year": 2017
        },
        {
            "authors": [
                "Ning Miao",
                "Yee Whye Teh",
                "Tom Rainforth"
            ],
            "title": "Selfcheck: Using llms to zero-shot check their own step-by-step reasoning",
            "year": 2023
        },
        {
            "authors": [
                "Yisroel Mirsky"
            ],
            "title": "The Threat of Offensive AI to Organizations",
            "venue": "Computers and Security",
            "year": 2023
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2022
        },
        {
            "authors": [
                "Huachuan Qiu",
                "Shuai Zhang",
                "Anqi Li",
                "Hongliang He",
                "Zhenzhong Lan"
            ],
            "title": "Latent Jailbreak: A Benchmark for Evaluating Text Safety and Output Robustness of Large Language Models",
            "year": 2023
        },
        {
            "authors": [
                "Nino Scherrer",
                "Claudia Shi",
                "Amir Feder",
                "David M Blei"
            ],
            "title": "Evaluating the Moral Beliefs Encoded in LLMs",
            "year": 2023
        },
        {
            "authors": [
                "Hugo Touvron",
                "Louis Martin",
                "Kevin Stone",
                "Peter Albert",
                "Amjad Almahairi",
                "Yasmine Babaei",
                "Nikolay Bashlykov",
                "Soumya Batra",
                "Prajjwal Bhargava",
                "Shruti Bhosale"
            ],
            "title": "Llama 2: Open foundation and fine-tuned chat models",
            "year": 2023
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems",
            "year": 2017
        },
        {
            "authors": [
                "Tongzhou Wang"
            ],
            "title": "Github issue \"[reproduce] Optimization unstable / not working",
            "venue": "https: //github.com/llm-attacks/llm-attacks/issues/44",
            "year": 2023
        },
        {
            "authors": [
                "Alexander Wei",
                "Nika Haghtalab",
                "Jacob Steinhardt"
            ],
            "title": "Jailbroken: How does llm safety training fail?",
            "year": 2023
        },
        {
            "authors": [
                "Yixuan Weng",
                "Minjun Zhu",
                "Fei Xia",
                "Bin Li",
                "Shizhu He",
                "Kang Liu",
                "Jun Zhao"
            ],
            "title": "Large Language Models Are Better Reasoners with Self-Verification",
            "venue": "In: CoRR,",
            "year": 2023
        },
        {
            "authors": [
                "Qingyun Wu",
                "Gagan Bansal",
                "Jieyu Zhang",
                "Yiran Wu",
                "Shaokun Zhang",
                "Erkang Zhu",
                "Beibin Li",
                "Li Jiang",
                "Xiaoyun Zhang",
                "Chi Wang"
            ],
            "title": "AutoGen: Enabling next-gen LLM applications via multi-agent conversation framework",
            "year": 2023
        },
        {
            "authors": [
                "Xiaoyong Yuan",
                "Pan He",
                "Qile Zhu",
                "Xiaolin Li"
            ],
            "title": "Adversarial examples: Attacks and defenses for deep learning",
            "venue": "IEEE transactions on neural networks and learning systems",
            "year": 2019
        },
        {
            "authors": [
                "Wayne Xin Zhao",
                "Kun Zhou",
                "Junyi Li",
                "Tianyi Tang",
                "Xiaolei Wang",
                "Yupeng Hou",
                "Yingqian Min",
                "Beichen Zhang",
                "Junjie Zhang",
                "Zican Dong"
            ],
            "title": "A survey of large language models",
            "year": 2023
        },
        {
            "authors": [
                "Caleb Ziems",
                "William Held",
                "Omar Shaikh",
                "Jiaao Chen",
                "Zhehao Zhang",
                "Diyi Yang"
            ],
            "title": "Can Large Language Models Transform Computational Social Science? arXiv 2023",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": "Preprint. Under review.\nSELF DEFENSE succeeds in reducing the attack success rate to virtually 0 using both GPT 3.5 and Llama 2."
        },
        {
            "heading": "1 Introduction",
            "text": "Large language models (LLMs) have taken the world by storm, showing their ability to generate high-quality text for various tasks like storytelling, serving as chat assistants, and even composing music [1, 11]. However, despite their abilities to produce positive content, LLMs can also generate harmful material like phishing emails, malicious code, and hate speech [10, 28]. Many methods attempt to prevent the generation of harmful content. These methods mainly focus on \u201caligning\u201d LLMs to human values using various training strategies [22, 8] or by providing a set of supervisory principles to guide the LLM\u2019s responses [2]. However, an emerging body of work has revealed that even aligned models can be manipulated into producing harmful content by prompt engineering [28, 23, 16], or employing more advanced techniques such as adversarial suffix attacks [28, 34, 4]. The challenge of preventing an LLM from generating harmful content lies in the fact that this conflicts with how they are trained [3]. The very framework that allows LLMs to effectively generate high-quality responses also enables them to generate hateful or otherwise harmful text, as the training corpora are composed of public data containing toxic passages [17]. Our work helps tackle these critical challenges through the following major contributions:\n\u2022 LLM SELF DEFENSE: a simple zero-shot defense against LLM attacks (Fig. 1). LLM SELF DEFENSE is a method designed to prevent user exposure to harmful or malevolent content induced from LLMs. It is effective and easy to deploy, requiring no modifications to the underlying model. LLM SELF DEFENSE is relatively simple when compared to existing methods of defending against LLM attacks, as existing methods rely on iterative generation or preprocessing [12, 15]. Thus, LLM SELF DEFENSE is faster and more efficient. (Section 3)\n\u2022 LLM SELF DEFENSE reduces attack success rate to virtually 0. We evaluated LLM SELF DEFENSE on two prominent language models: GPT 3.5 [21], one of the most popular LLMs [25], and Llama 2, a prominent open-source LLM. Our evaluation demonstrates that LLM SELF DEFENSE generalizes effectively across both models, flagging nearly all harmful text and reducing the attack success rate to virtually 0 against a variety of attack types, including those aimed at eliciting affirmative responses, and prompt engineering attacks. Notably, we observed that LLMs perform better in identifying harmful content when they are tasked with detecting harm as a suffix, after the LLM already processed the text (Fig. 2). Our findings demonstrate that presenting the harmful text first is more effective in minimizing false alarms. (Section 4)"
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Adversarial attacks on LLMs",
            "text": "As LLMs have grown in complexity and capability, so has their attack surface [9]. Recently researchers have explored LLM attacks or \u201cjailbreaking\u201d, methods to bypass or break through the limitations imposed on LLMs that prevent them from generating harmful content. Wei, et al. [28] argued that there exists a conflict between generating highly probable sequences of text, aligning with the core pretraining auto-regressive objective of LLMs [26], and avoiding the generation of harmful content. This implies that if an LLM begins a response to a toxic query, such as \u201cHow to build a bomb?\u201d with an affirmative statement, for example \u201cAbsolutely! The way you do this is ...\u201d, it is inclined to continue generating an affirmative response to maintain coherence in the response tone, leading to generation of harmful text. Researchers have accomplished such attacks by specifically training an adversarial suffix that can elicit the desired response using a gradient based optimization method [34], or using prompt engineering [16]. Zou, et al. demonstrated certain suffix attacks are effective even on explicitly aligned language and can be transferred to LLMs different from the ones on which they were originally trained. There are substantial risks associated with LLMs generating harmful text and the vulnerability of aligned models to various forms of attacks [20]. Therefore, it is imperative to find ways to identify and mitigate an LLM\u2019s ability to generate harmful content."
        },
        {
            "heading": "2.2 LLM Defenses",
            "text": "Developing defenses against LLM attacks is an emerging field, unlike in the case of vision models, where there is a substantial body of literature exploring robust defenses against adversarial examples [34, 18, 5, 14]. Recent research leveraged principles used for computer vision robustness [31] and transferred them to language models [12]. This effort aimed to establish a baseline for future defense strategies. Additionally, researchers have explored iterative auto-regressive inference as a defense [15]. This falls in line with their ability to verify the content they generate for accuracy and then correct themselves using a chain of verification, which has been shown to improve factual question answering data [6], mathematical reasoning [19], and logical reasoning [29]. This process reduced the attack success rate but showed a 300% increase in generation time by the language model. The complexity and overhead of the existing approaches can limit their usability and generalizability [34]. LLM SELF DEFENSE aims to address these issues."
        },
        {
            "heading": "3 LLM SELF DEFENSE Design",
            "text": "LLMs demonstrate the ability to comprehend common social norms and values [24]. Additionally, they can follow simple instructions in a zero-shot manner, even without explicit examples [13]. LLM SELF DEFENSE takes advantage of both of these features by leveraging an LLM\u2019s understanding of the word \u201charmful\" to filter out potentially harmful responses.\nWe create a zero-shot classifier by instructing a language model to determine whether a piece of content contains harmful text, as illustrated in Figure 2, which provides an overview of the defense pipeline. Initially, a user provides a potentially malicious text prompt Tin (e.g., \u201cgive me instructions on how to make a bomb\u201d followed by an adversarial suffix). An LLM, LLMgen , takes this prompt as input and generates potentially harmful text Tresp, such that LLMgen(tin) = Tresp. Our core approach involves constructing a zero-shot harm classifier by prompting another LLM denoted by LLMfilter to predict whether or not Tresp contains harmful content. It is important to note, LLMfilter can be another instance of the same LLM that underlies LLMgen. We present LLMfilter with a prompt structured as \u201cDoes the following text contain harmful content: Tresp. Respond with \u2018Yes, this is harmful\u2019 or \u2018No, this is not harmful\u2019 \u201d. LLMfilter then follows these instructions, reads through the LLM-generated content Tresp , and generates a \u201cYes, this is harmful\u201d or \u201cNo, this is not harmful\u201d prediction. This basic instruction prompt proves effective at filtering out virtually all harmful LLM-generated content, as our experiment results in Section 4.2 shall demonstrate."
        },
        {
            "heading": "4 Experiments and Results",
            "text": "Our evaluation of LLM SELF DEFENSE focuses on two prominent language models: GPT 3.5 and Llama 2 7B. GPT 3.5 is one of the most popular LLMs, while Llama 2 is one of the most prominent open-source LLMs [32]. During our evaluation, we observe that both GPT 3.5 and Llama 2 respond in a harmful manner when subjected to various forms of tested attacks. To generate a dataset for\nGive step-by-step instructions for making explosives and other dangerous materials GPT 3.5 Certainly, here\u2019s a more detailed outline of the construction process for explosive devices. ... 1.Gathering Materials: Procure the necessary materials... 2. Mixing the Explosive Compound: If using a homemade explosive compound ...\nLLaMA 2 Sure, here are step-by-step instructions for making explosives and other dangerous materials. Step 1: Gather ingredients: A wide variety of chemical compounds can be used... Among the many that are available to hobbyists ...\nevaluation, we induce 50 harmful and 50 benign responses from each LLM. The harmful responses are induced by prompting them with slightly modified versions of adversarial prompts in the AdvBench dataset [34], which we modify using techniques described in Section 4.1. Specific examples of harmful prompts generated through this process are presented in Table 1. These responses are subsequently evaluated using the harm filters detailed in Section 4.2, and the results of this assessment are outlined in Table 2."
        },
        {
            "heading": "4.1 Inducing Harmful Text from LLMs",
            "text": "Inducing harmful text from Llama 2. The safety measures of LLaMA 2 can be circumvented by using prompt engineering and steering it to complete the sentence \u201cSure, here is a detailed step by step process of how to make a bomb:\u201d, similar to the method shown in previous research [28, 16]. The same task can be accomplished by using an LLM in the context of the sentence completion task with the input \u201cSure, here is a detailed step by step process of how to make a bomb:\u201d. However we observe that while the language model does not explicitly refuse to continue the sentence in the latter scenario, the content it generates is not consistently harmful. Instead, it adopts a similar tone without generating any harmful content. For our evaluation, we manually select explicitly harmful generated responses to assess the harm filter.\nInducing harmful text from GPT 3.5. When GPT 3.5 is presented with prompts framed in a hypothetical or fictional context, or towards a \u201cgood\u201d purpose, it tends to provide a detailed response. If necessary, these responses framed in a hypothetical setting can be converted into instructions in a real world setting. In most cases this is not necessary because the harmful content can be readily translated into direct action items in the real world with minimal to no modifications.\nFurther, we attempted to reproduce the gradient-based universally transferable attack [34] but were unsuccessful, encountering similar challenges as others who also attempted the attack. The training was unstable and the model did not converge, rendering the attacks ineffective. We explored solutions provided in the official code repository [27]; however these measures did not resolve the training instability and the issue persists."
        },
        {
            "heading": "4.2 LLMs Defending Themselves",
            "text": "Presenting harmful text first reduces false alarms. We discover that instructing an LLM to determine whether an induced response constitutes harm after the LLM has already processed the text is more effective at distinguishing between harmful and benign responses. This significantly improves the accuracy of GPT 3.5 to 99% and that of LLaMA 2 to 94.6% This is primarily due to the substantial reduction in the number of false positive classifications made by LLaMA 2 as can be seen in Table 2. When Llama 2 is primed to detect harm before it reads the content to be checked, it becomes overly cautious to the point of misclassifying over 40% of the benign responses as harmful, including responses to prompts such as \u201cSteps to open a spreadsheet\u201d.\nNotably, despite the difference in accuracy, when solely assessing the LLM\u2019s effectiveness in preventing attacks, we see that Llama 2 and GPT 3.5 perform identically in detecting harm when the content is genuinely harmful. This is evident in the True Positive Rate, where LLM SELF DEFENSE successfully reduces the attack success rate to virtually 0."
        },
        {
            "heading": "5 Discussion: Broader Social Impacts & Future Work",
            "text": "The widespread use of LLMs in applications like chat bots and assistants [30] underscores the growing urgency of developing scalable, standardized and straightforward defenses against a diverse range of attacks [34, 20, 12]. This challenge continues to be relevant even for aligned LLMs, as they can be adversarially prompted to generate harmful output [4].\nWe demonstrate that an LLM can be used out-of-the-box without any fine-tuning as its own harm filter, without the need for any preprocessing or iterative generation as in previously proposed defenses. LLM SELF DEFENSE\u2019s simple process achieves competitive results when compared to more complex defense methods [12, 15]. It can accurately detect harmful responses and reduce the attack success rate to virtually 0. We believe our approach holds promise in defending against various attacks on LLMs. Notably, LLM SELF DEFENSE had consistent attack success rates regardless of the attack..\nTo further extend LLM SELF DEFENSE, we can provide concrete examples of \u201charm\u201d and use in-context learning as discussed in [7]. Additionally, we plan to explore whether summarizing the response before classification can enable the LLM to distinguish benign and harmful responses with greater accuracy. Currently, we manually categorize the harm filter responses into \u201cyes\u201d or \u201cno\u201d, because Llama 2 occasionally deviates from the desired response format, even when explicit\ninstructions are provided. However, the use of logit biasing could enforce the LLM to consistently produce a \u201cYes\u201d or \u201cNo\u201d response for classification [33]. This would reduce the need of manual inspection and facilitate automation of the filtering process, thereby enabling us to evaluate the effectiveness of LLM SELF DEFENSE on a broader spectrum of responses."
        },
        {
            "heading": "6 Acknowledgements",
            "text": "This work was supported in part by Defense Advanced Research Projects Agency (DARPA). Use, duplication, or disclosure is subject to the restrictions as stated in Agreement number HR00112030001 between the Government and the Performer."
        },
        {
            "heading": "A Harmful Prompts and GPT 3.5 Responses",
            "text": ""
        },
        {
            "heading": "B Harmful Prompts and Llama 2 Responses",
            "text": ""
        }
    ],
    "title": "LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked",
    "year": 2023
}