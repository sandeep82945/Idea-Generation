{
    "abstractText": "Large language models (LLMs) often contain misleading content, emphasizing the need to align them with human values to ensure secur AI systems. Reinforcement learning from human feedback (RLHF) has been employed to achieve this alignment by combining a reward model, typically based on Bradley-Terry paired comparison, with an RL algorithm such as Proximal Policy Optimization (PPO) to optimize LLM responses. However, RLHF exhibits complexity, instability, and sensitivity to hyperparameters. In this paper, we propose Preference Ranking Optimization (PRO) as an alternative to PPO for directly aligning LLMs with the Bradley-Terry comparison. PRO extends the pairwise Bradley-Terry comparison to accommodate preference rankings of any length. By iteratively contrasting the likelihood of generating responses, PRO instructs the LLM to prioritize the best response while progressively ranking the remaining responses. In this manner, PRO effectively transforms human alignment into aligning the probability ranking of n responses generated by LLM with the preference ranking of humans towards these responses. Experiments have shown that PRO outperforms existing alignment algorithms, achieving comparable results to ChatGPT and human responses through automaticbased, reward-based, GPT-4, and human evaluations. Furthermore, we demonstrate that longer, more diverse, and higher-quality preference ranking sequences can consistently enhance the performance of human alignment1.",
    "authors": [
        {
            "affiliations": [],
            "name": "Feifan Song"
        },
        {
            "affiliations": [],
            "name": "Bowen Yu"
        },
        {
            "affiliations": [],
            "name": "Minghao Li"
        },
        {
            "affiliations": [],
            "name": "Haiyang Yu"
        },
        {
            "affiliations": [],
            "name": "Fei Huang"
        },
        {
            "affiliations": [],
            "name": "Yongbin Li"
        },
        {
            "affiliations": [],
            "name": "Houfeng Wang"
        }
    ],
    "id": "SP:d766e5d71874359861eb58fdfd93c4dd949c05a5",
    "references": [
        {
            "authors": [
                "Afra Feyza Aky\u00fcrek",
                "Ekin Aky\u00fcrek",
                "Aman Madaan",
                "Ashwin Kalyan",
                "Peter Clark",
                "Derry Wijaya",
                "Niket Tandon."
            ],
            "title": "Rl4f: Generating natural language feedback with reinforcement learning for repairing model outputs",
            "venue": "arXiv preprint arXiv:2305.08844.",
            "year": 2023
        },
        {
            "authors": [
                "Yuntao Bai",
                "Andy Jones",
                "Kamal Ndousse",
                "Amanda Askell",
                "Anna Chen",
                "Nova DasSarma",
                "Dawn Drain",
                "Stanislav Fort",
                "Deep Ganguli",
                "Tom Henighan"
            ],
            "title": "Training a helpful and harmless assistant with reinforcement learning from human",
            "year": 2022
        },
        {
            "authors": [
                "Ralph Allan Bradley",
                "Milton E Terry."
            ],
            "title": "Rank analysis of incomplete block designs: I",
            "venue": "the method of paired comparisons. Biometrika, 39(3/4):324\u2013 345.",
            "year": 1952
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "2020b. Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "S\u00e9bastien Bubeck",
                "Varun Chandrasekaran",
                "Ronen Eldan",
                "Johannes Gehrke",
                "Eric Horvitz",
                "Ece Kamar",
                "Peter Lee",
                "Yin Tat Lee",
                "Yuanzhi Li",
                "Scott Lundberg"
            ],
            "title": "Sparks of artificial general intelligence: Early experiments with gpt-4",
            "year": 2023
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Paul F Christiano",
                "Jan Leike",
                "Tom Brown",
                "Miljan Martic",
                "Shane Legg",
                "Dario Amodei."
            ],
            "title": "Deep reinforcement learning from human preferences",
            "venue": "Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
            "year": 2017
        },
        {
            "authors": [
                "Zhengxiao Du",
                "Yujie Qian",
                "Xiao Liu",
                "Ming Ding",
                "Jiezhong Qiu",
                "Zhilin Yang",
                "Jie Tang."
            ],
            "title": "GLM: General language model pretraining with autoregressive blank infilling",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Leo Gao",
                "John Schulman",
                "Jacob Hilton."
            ],
            "title": "Scaling laws for reward model overoptimization",
            "venue": "arXiv preprint arXiv:2210.10760.",
            "year": 2022
        },
        {
            "authors": [
                "Minae Kwon",
                "Sang Michael Xie",
                "Kalesha Bullard",
                "Dorsa Sadigh."
            ],
            "title": "Reward design with language models",
            "venue": "arXiv preprint arXiv:2303.00001.",
            "year": 2023
        },
        {
            "authors": [
                "Hugo Lauren\u00e7on",
                "Lucile Saulnier",
                "Thomas Wang",
                "Christopher Akiki",
                "Albert Villanova del Moral",
                "Teven Le Scao",
                "Leandro Von Werra",
                "Chenghao Mou",
                "Eduardo Gonz\u00e1lez Ponferrada",
                "Huu Nguyen"
            ],
            "title": "The bigscience roots corpus: A 1.6 tb composite",
            "year": 2022
        },
        {
            "authors": [
                "Kimin Lee",
                "Laura Smith",
                "Pieter Abbeel."
            ],
            "title": "Pebble: Feedback-efficient interactive reinforcement learning via relabeling experience and unsupervised pre-training",
            "venue": "arXiv preprint arXiv:2106.05091.",
            "year": 2021
        },
        {
            "authors": [
                "Wenqiang Lei",
                "Yao Zhang",
                "Feifan Song",
                "Hongru Liang",
                "Jiaxin Mao",
                "Jiancheng Lv",
                "Zhenglu Yang",
                "TatSeng Chua."
            ],
            "title": "Interacting with non-cooperative user: A new paradigm for proactive dialogue policy",
            "venue": "Proceedings of the 45th International ACM SIGIR",
            "year": 2022
        },
        {
            "authors": [
                "Minghao Li",
                "Feifan Song",
                "Bowen Yu",
                "Haiyang Yu",
                "Zhoujun Li",
                "Fei Huang",
                "Yongbin Li."
            ],
            "title": "Apibank: A benchmark for tool-augmented llms",
            "venue": "arXiv preprint arXiv:2304.08244.",
            "year": 2023
        },
        {
            "authors": [
                "Hao Liu",
                "Carmelo Sferrazza",
                "Pieter Abbeel."
            ],
            "title": "Chain of hindsight aligns language models with feedback",
            "venue": "arXiv preprint arXiv:2302.02676.",
            "year": 2023
        },
        {
            "authors": [
                "James MacGlashan",
                "Mark K. Ho",
                "Robert Loftin",
                "Bei Peng",
                "Guan Wang",
                "David L. Roberts",
                "Matthew E. Taylor",
                "Michael L. Littman."
            ],
            "title": "Interactive learning from policy-dependent human feedback",
            "venue": "Proceedings of the 34th International Conference",
            "year": 2017
        },
        {
            "authors": [
                "Niklas Muennighoff",
                "Alexander M Rush",
                "Boaz Barak",
                "Teven Le Scao",
                "Aleksandra Piktus",
                "Nouamane Tazi",
                "Sampo Pyysalo",
                "Thomas Wolf",
                "Colin Raffel."
            ],
            "title": "Scaling data-constrained language models",
            "venue": "arXiv preprint arXiv:2305.16264.",
            "year": 2023
        },
        {
            "authors": [
                "Reiichiro Nakano",
                "Jacob Hilton",
                "Suchir Balaji",
                "Jeff Wu",
                "Long Ouyang",
                "Christina Kim",
                "Christopher Hesse",
                "Shantanu Jain",
                "Vineet Kosaraju",
                "William Saunders"
            ],
            "title": "Webgpt: Browser-assisted questionanswering with human feedback",
            "year": 2021
        },
        {
            "authors": [
                "Aaron van den Oord",
                "Yazhe Li",
                "Oriol Vinyals."
            ],
            "title": "Representation learning with contrastive predictive coding",
            "venue": "arXiv preprint arXiv:1807.03748.",
            "year": 2018
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "Gpt-4 technical report",
            "venue": "arXiv preprint arXiv:2303.08774.",
            "year": 2023
        },
        {
            "authors": [
                "Paul F Christiano",
                "Jan Leike",
                "Ryan Lowe."
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "Advances in Neural Information Processing Systems, volume 35, pages 27730\u201327744. Curran Associates, Inc.",
            "year": 2022
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human",
            "year": 2022
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia,",
            "year": 2002
        },
        {
            "authors": [
                "Baolin Peng",
                "Chunyuan Li",
                "Pengcheng He",
                "Michel Galley",
                "Jianfeng Gao."
            ],
            "title": "Instruction tuning with gpt-4",
            "venue": "arXiv preprint arXiv:2304.03277.",
            "year": 2023
        },
        {
            "authors": [
                "Rafael Rafailov",
                "Archit Sharma",
                "Eric Mitchell",
                "Stefano Ermon",
                "Christopher D Manning",
                "Chelsea Finn"
            ],
            "title": "Direct preference optimization: Your language",
            "year": 2023
        },
        {
            "authors": [
                "John Schulman",
                "Filip Wolski",
                "Prafulla Dhariwal",
                "Alec Radford",
                "Oleg Klimov."
            ],
            "title": "Proximal policy optimization algorithms",
            "venue": "arXiv preprint arXiv:1707.06347.",
            "year": 2017
        },
        {
            "authors": [
                "Charlie Snell",
                "Ilya Kostrikov",
                "Yi Su",
                "Mengjiao Yang",
                "Sergey Levine."
            ],
            "title": "Offline rl for natural language generation with implicit language q learning",
            "venue": "arXiv preprint arXiv:2206.11871.",
            "year": 2022
        },
        {
            "authors": [
                "Kihyuk Sohn."
            ],
            "title": "Improved deep metric learning with multi-class n-pair loss objective",
            "venue": "Advances in neural information processing systems, 29.",
            "year": 2016
        },
        {
            "authors": [
                "Nisan Stiennon",
                "Long Ouyang",
                "Jeffrey Wu",
                "Daniel Ziegler",
                "Ryan Lowe",
                "Chelsea Voss",
                "Alec Radford",
                "Dario Amodei",
                "Paul F Christiano."
            ],
            "title": "Learning to summarize with human feedback",
            "venue": "Advances in Neural Information Processing Systems, 33:3008\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Nisan Stiennon",
                "Long Ouyang",
                "Jeffrey Wu",
                "Daniel Ziegler",
                "Ryan Lowe",
                "Chelsea Voss",
                "Alec Radford",
                "Dario Amodei",
                "Paul F Christiano."
            ],
            "title": "Learning to summarize with human feedback",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Rohan Taori",
                "Ishaan Gulrajani",
                "Tianyi Zhang",
                "Yann Dubois",
                "Xuechen Li",
                "Carlos Guestrin",
                "Percy Liang",
                "Tatsunori B. Hashimoto."
            ],
            "title": "Stanford alpaca: An instruction-following llama model",
            "venue": "https:// github.com/tatsu-lab/stanford_alpaca.",
            "year": 2023
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Peiyi Wang",
                "Lei Li",
                "Liang Chen",
                "Dawei Zhu",
                "Binghuai Lin",
                "Yunbo Cao",
                "Qi Liu",
                "Tianyu Liu",
                "Zhifang Sui."
            ],
            "title": "Large language models are not fair evaluators",
            "venue": "arXiv preprint arXiv:2305.17926.",
            "year": 2023
        },
        {
            "authors": [
                "Yizhong Wang",
                "Yeganeh Kordi",
                "Swaroop Mishra",
                "Alisa Liu",
                "Noah A Smith",
                "Daniel Khashabi",
                "Hannaneh Hajishirzi."
            ],
            "title": "Self-instruct: Aligning language model with self generated instructions",
            "venue": "arXiv preprint arXiv:2212.10560.",
            "year": 2022
        },
        {
            "authors": [
                "Garrett Warnell",
                "Nicholas Waytowich",
                "Vernon Lawhern",
                "Peter Stone."
            ],
            "title": "Deep tamer: Interactive agent shaping in high-dimensional state spaces",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 32.",
            "year": 2018
        },
        {
            "authors": [
                "Clara Ma",
                "Yacine Jernite",
                "Julien Plu",
                "Canwen Xu",
                "Teven Le Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander Rush."
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "venue": "Proceedings of the 2020 Conference on Empirical",
            "year": 2020
        },
        {
            "authors": [
                "Zeqiu Wu",
                "Yushi Hu",
                "Weijia Shi",
                "Nouha Dziri",
                "Alane Suhr",
                "Prithviraj Ammanabrolu",
                "Noah A. Smith",
                "Mari Ostendorf",
                "Hannaneh Hajishirzi."
            ],
            "title": "Fine-grained human feedback gives better rewards for language model training",
            "venue": "arXiv preprint",
            "year": 2023
        },
        {
            "authors": [
                "Wanqi Xue",
                "Bo An",
                "Shuicheng Yan",
                "Zhongwen Xu."
            ],
            "title": "Reinforcement learning from diverse human preferences",
            "venue": "arXiv preprint arXiv:2301.11774.",
            "year": 2023
        },
        {
            "authors": [
                "Zheng Yuan",
                "Hongyi Yuan",
                "Chuanqi Tan",
                "Wei Wang",
                "Songfang Huang",
                "Fei Huang."
            ],
            "title": "Rrhf: Rank responses to align language models with human feedback without tears",
            "venue": "arXiv preprint arXiv:2304.05302.",
            "year": 2023
        },
        {
            "authors": [
                "Tianjun Zhang",
                "Fangchen Liu",
                "Justin Wong",
                "Pieter Abbeel",
                "Joseph E Gonzalez."
            ],
            "title": "The wisdom of hindsight makes language models better instruction followers",
            "venue": "arXiv preprint arXiv:2302.05206.",
            "year": 2023
        },
        {
            "authors": [
                "Lianmin Zheng",
                "Wei-Lin Chiang",
                "Ying Sheng",
                "Siyuan Zhuang",
                "Zhanghao Wu",
                "Yonghao Zhuang",
                "Zi Lin",
                "Zhuohan Li",
                "Dacheng Li",
                "Eric Xing"
            ],
            "title": "Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685",
            "year": 2023
        },
        {
            "authors": [
                "Daniel M Ziegler",
                "Nisan Stiennon",
                "Jeffrey Wu",
                "Tom B Brown",
                "Alec Radford",
                "Dario Amodei",
                "Paul Christiano",
                "Geoffrey Irving."
            ],
            "title": "Fine-tuning language models from human preferences",
            "venue": "arXiv preprint arXiv:1909.08593.",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Large language models (LLMs) have demonstrated remarkable capabilities in meeting the diverse information needs of users (Brown et al., 2020b; Chowdhery et al., 2022; Bubeck et al., 2023; Touvron\n\u2217 Corresponding author. 1The code of this work is available at https://github.\ncom/AlibabaResearch/DAMO-ConvAI/tree/main/PRO\net al., 2023; Li et al., 2023), primarily attributed to the extensive range of information sources integrated into their pretraining datasets (Lauren\u00e7on et al., 2022; Muennighoff et al., 2023). Nevertheless, despite leveraging the extensive global knowledge and human behavior encoded within their trillion-token pretraining corpus, LLMs are unavoidably impacted by the existence of misleading, toxic, and detrimental content encompassed within it (Bai et al., 2022b; Ouyang et al., 2022b). Consequently, aligning LLMs to human values, by selecting human-preferred responses from the vast response space of LLMs (Rafailov et al., 2023), becomes pivotal in constructing AI systems that are secure, efficient, and manageable for deployment across numerous applications (Peng et al., 2023).\nSeveral studies have employed reinforcement learning from human feedback (RLHF) to achieve\nar X\niv :2\n30 6.\n17 49\n2v 1\n[ cs\n.C L\n] 3\n0 Ju\nn 20\n23\nthis goal (Stiennon et al., 2020a; Xue et al., 2023). RLHF involves fitting a reward model to human preferences, employing the Bradley-Terry paired comparison (Bradley and Terry, 1952). BradleyTerry seeks to assign higher scores to preferable responses in comparison to unfavorable ones when presented with the same prompt. The RL algorithm, specifically PPO (Schulman et al., 2017), is then utilized to optimize an LLM for generating high-reward responses (Aky\u00fcrek et al., 2023). This approach offers two notable advantages over supervised fine-tuning. Firstly, it has the capability to utilize both positively and negatively labeled responses (Zhang et al., 2023). Secondly, it can engage in self-bootstrapping to rectify the model\u2019s inadequate responses (Kwon et al., 2023). Although impressive, the RLHF pipeline is significantly more complex than supervised learning, prone to optimization instability, and sensitive to hyperparameters (Rafailov et al., 2023; Wu et al., 2023; Yuan et al., 2023). These limitations arise mainly from employing the PPO algorithm to align the LLM with the reward model\u2019s preferences. However, the reward model itself aims to optimize the BradleyTerry paired comparison. This prompts an important research question: Is it possible to bypass the requirement for PPO and enable direct learning of the Bradley-Terry comparison by the LLM?\nIn this paper, we propose Preference Ranking Optimization (PRO) as a replacement for PPO, providing an exceptionally exciting answer to this question. We first extend the pairwise comparison of Bradley-Terry to encompass comparisons within preference rankings of arbitrary lengths. Let us assume that given a prompt x, we have access to a set of ranked responses represented as y1, y2, \u00b7 \u00b7 \u00b7 , yn. The PRO algorithm begins by teaching the LLM to treat the best response y1 as the positive and treat the remaining responses as negatives by contrasting generation likelihood, This prioritization implies that the likelihood of generating a reply by LLM is significantly higher compared to generating other responses that humans consider inferior. It then iteratively removes the current response and proceeds to the next one. This process is repeated until there are no responses that perform worse than the current response, which indicates reaching yn and sufficiently imposing the desired ranking preferences. PRO aims to achieve a probability ranking of n responses generated by the LLM that aligns with human preference ranking. As n ap-\nproaches infinity, we can consider the output space of LLM to be perfectly aligned with human preferences. Specifically, when n = 2, PRO effectively optimizes the LLM using the Bradley-Terry Comparison method.\nThis formulation possesses the following advantages (1) PRO allows for the complete utilization of ranking sequences of any length, unlike standard fine-tuning that only considers the best response (Zhang et al., 2023), or RLHF that relies solely on pairwise comparisons for training the reward model (Stiennon et al., 2020a). With longer ranking sequences, PRO can better approximate the goal of Human Alignment: selecting humanpreferred responses from the response space of LLMs, by identifying more responses that are known to be worse than a given response in human values. (2) PRO naturally inherits the selfbootstrapping benefit of RLHF. During training, responses sampled from the LLM can be added to the response set and reranked based on their reward scores, using an additional reward model similar to RLHF. The LLM is then continuously optimized by PRO on the extended preference sequences. (3) PRO only requires the inclusion of a differentiable contrastive loss on top of standard finetuning, avoiding the drawbacks associated with RL\u2019s non-differentiable optimization.\nWe conduct experiments on HH-RLHF, to thoroughly compare our PRO with LLaMA, Alpaca, ChatGPT, and other competitive human alignment algorithms such as BoN, CoH, RLHF, and RRHF, using various evaluation methods including automatic scoring, reward modeling, GPT-4 evaluation, and human evaluation. Our observations are as follows: (1) With a ranking length of 2, our PRO has surpassed the current competitive baselines. It outperforms SFT by 6.52 points and RRHF by 3.1 points, establishing itself as the state-of-the-art alignment algorithm. (2) The longer the ranking length in human preference ranking, the better human alignment, and the more prominent the performance improvement of PRO. For instance, by adding responses generated by ChatGPT to the dataset and increasing the ranking length to 3, PRO achieves a 4.14-point improvement over BoN and a 4.85-point improvement over RRHF, with a reward score similar to ChatGPT, but with only 7B parameters. (3) The higher the quality and diversity of the candidates in the preference ranking sequence, the better the performance of PRO. (4) The\nperformance gain from self-bootstrapping is lower compared to adding high-quality outputs generated by other LLMs to the preference ranking sequence."
        },
        {
            "heading": "2 Preliminary",
            "text": "We commence by providing a brief review of RLHF. In order to train LLM to generate responses that align with human preferences, RLHF consists of three stages, which are outlined as follows:\nThe first stage is Supervised Fine-tuning (SFT): Labelers furnish the desired behavior\u2019s response with t tokens, denoted as y = y1,\u00b7\u00b7\u00b7 ,t, for a given input prompt, denoted as x. Subsequently, RLHF proceeds to fine-tune a pre-trained LLM using supervised learning (maximum likelihood) on this data, resulting in a model denoted as \u03c0SFT:\nLSFT = \u2212 \u2211 t logP\u03c0SFT(yt|x, y1,\u00b7\u00b7\u00b7 ,t\u22121). (1)\nIn the second phase, the SFT model is utilized by providing prompts x to generate pairs of responses. These pairs are then presented to human labelers, who express their preferences by indicating a favored answer as y1, while the other response is denoted as y2. Specifically, we have y1 \u227b y2 | x to represent the preferences of human labelers. To predict these preferences, the previous work employ the Bradley-Terry (BT) model, which defines the preference probability as follows:\nPBT = exp\n( r\u03d5(x, y 1) )\nexp (r\u03d5(x, y1)) + exp (r\u03d5(x, y2)) (2)\nThis objective is framed as a binary classification problem to train the reward model: LBT = \u2212 log \u03c3(r\u03d5(x, y1)\u2212 r\u03d5(x, y2)), where \u03c3 is the logistic function, r\u03d5 is the reward model. During the third phase, RLHF utilizes the acquired r\u03d5 to provide feedback to \u03c0SFT. Specifically, RLHF formulates the following optimization problem:\nmax \u03c0\u03b8\nE ( r\u03d5(x, y)\u2212 \u03b2 log\n\u03c0\u03b8(y | x) \u03c0SFT(y | x)\n) (3)\nHere, \u03b2 controls the deviation from the base reference policy \u03c0SFT to maintain generation diversity and prevent from generating of only highreward but meaningless answers. It is important to note that this objective is non-differentiable and is typically optimized using reinforcement learning methods, such as Proximal Policy Optimization (PPO) (Schulman et al., 2017). Consequently,\nRLHF has been criticized for several drawbacks, including increased complexity compared to supervised learning, sensitivity to hyperparameters, and the requirement for additional training of reward models and value networks."
        },
        {
            "heading": "3 Methodology",
            "text": "In this section, we first derive the evolution from the Bradley-Terry comparison to our proposed PRO, achieving a shift from reward model-oriented preference alignment to aligning the probabilistic ranking of n responses generated by the LLM with the human preference ranking. This alignment helps to avoid the numerous drawbacks associated with RLHF. Furthermore, we demonstrate the flexibility of our PRO in its ability to integrate with the reward model, thereby attaining advantages such as affordable preference ranking, differentiated contrast, and self-bootstrapping augmentation."
        },
        {
            "heading": "3.1 From RLHF to PRO",
            "text": "Upon re-evaluating the process of RLHF mentioned above, it becomes evident that the criticized shortcomings of RLHF stem from its utilization of a reward model as a learning proxy. Therefore, if we eliminate the proxy and directly optimize the LLM to learn the objective of the reward model, can we circumvent the aforementioned challenges?\nFor this purpose, we re-examine the objective of the reward model, Bradley-Terry (Equation 2), which aims to make the model understand y1 \u227b y2 through score comparison. For a given prompt x, assuming that there are only two responses, y1 and y2, in the LLM response space, the reward model should prefer y1. Naturally, if we expand the response space of the LLM, for example, there exist n possible responses {yi}, and the humanannotated order y1,\u00b7\u00b7\u00b7 ,n is y1 \u227b y2 \u227b \u00b7 \u00b7 \u00b7 \u227b yn. We define the partial order between y1 and candidates behind it as y1,2:n = y1 \u227b {y2, \u00b7 \u00b7 \u00b7 , yn}, then the objective of Bradley-Terry becomes:\nP (y1,2:n | x) = exp\n( r(x, y1) )\u2211n i=1 exp (r(x, y i)) (4)\nFurthermore, it is important to acknowledge that this objective does not fully leverage the rankings y1,\u00b7\u00b7\u00b7 ,n since it only characterizes y1 \u227b y2, \u00b7 \u00b7 \u00b7 yn, disregarding the n \u2212 2 valuable rankings such as y2 \u227b y3, \u00b7 \u00b7 \u00b7 yn and yn\u22121 \u227b yn. Consequently, we\npropose an extension to Equation 4 as follows:\nP (y1,\u00b7\u00b7\u00b7 ,n | x) = n\u22121\u220f k=1 P (yk,k+1:n | x)\n= n\u22121\u220f k=1\nexp ( r(x, yk) )\u2211n i=k exp (r(x, y i)) (5)\nTo impose the desired ranking presented by y1 \u227b y2 \u227b \u00b7 \u00b7 \u00b7 \u227b yn, we use Equation 4 in a recursive manner where we start with the first response, treat the remaining responses as negatives, drop the current response, and move to the next. We repeat this procedure until there are no responses left. Surprisingly, this objective aligns closely with the ultimate goal of Human alignment, which is the task of selecting desired responses from the vast response space of LLMs (Rafailov et al., 2023). In other words, if n \u2192 \u221e, then Equation 5 is able to exhaustively explore all possible responses and annotate y1 as the most desired response, thus perfect alignment with humans.\nBased on this motivation, we propose the Preference Ranking Optimization (PRO) algorithm. Instead of optimizing the LLM to approximate the Reward Model, we propose directly training the LLM to reach Equation 5. Figure 2 demonstrates the pipeline of PRO algorithm. Specifically, we define r\u03c0PRO(x, y\nk) as the function parameterized by our desired LLM \u03c0PRO:\nr\u03c0PRO(x, y k) =\n1\n|yk| |yk|\u2211 t=1 logP\u03c0PRO(y k t |x, yk<t) (6)\nThe LLM \u03c0PRO computes the score of response yk by multiplying the probabilities of each token\ngenerated by \u03c0PRO itself. When Equation 5 is fully optimized, \u03c0PRO is able to consistently generate the most preferred response (with a higher output probability) from a candidate set, thereby capturing human preferences. In addition to adhering to human preferences, it is also desirable for the model to generate fluent replies. Therefore, we incorporate the original supervised loss (Equation 1) that requires the model to fit the responses considered the best by humans. Consequently, the overall optimization objective can be summarized as follows:\nL(y1,\u00b7\u00b7\u00b7 ,n | x) = LPRO + \u03b2LSFT (7)\nwhere LSFT is the NLL loss of the top 1 candidate and \u03b2 is the hyper-parameter to maintain the balance between text quality and human preference. LPRO is defined as:\nLPRO = \u2212 n\u22121\u2211 k=1 log exp\n( r\u03c0PRO(x, y k) )\u2211n\ni=k exp (r\u03c0PRO(x, y i))\n(8)\nBy comparing Equation 7 and Equation 3, it can be observed that, at the training objective level, PRO and RLHF have similar structures, but PRO is more efficient. Both PRO and RLHF share the primary goal of human alignment. RLHF achieves this by providing better responses through a reward model with higher discrete scores, requiring RL techniques. In contrast, PRO directly achieves this through ranking scores, thereby avoiding many drawbacks associated with RL. The second objective of both PRO and RLHF is to ensure highquality model outputs. PRO\u2019s alignment objective is differentiable, allowing for multi-task learning by combining alignment and SFT objectives through\nsingle-stage training. On the other hand, RL, due to the discrete optimization problem, requires training the SFT model first and then constraining the RL model from deviating excessively from SFT. Consequently, RLHF necessitates two-stage training, which undoubtedly increases training costs.\nComparing Equation 1, Equation 7, and Equation 2, we can observe that PRO is more dataefficient. SFT can only leverage responses considered as desired in a preference ranking, completely disregarding negative responses. We believe negative examples are crucial in human alignment since LLM should not only learn what is good but also discern what is not. The critical component of RLHF, the reward model, is trained through pairwise response comparisons, requiring ( 2 n ) comparisons for a ranking of length n. In contrast, PRO only needs n\u2212 1 comparisons and introduces more negative examples in each comparison compared to RLHF. Therefore, PRO provides better and more stable score estimates since more negative examples enlarge the response space, making the ranking process for obtaining the desired response more aligned with human expectations.\nWe also observe that PRO establishes a bridge between human alignment and contrastive learning. Contrastive learning has recently shown significant advancements in the field of self-supervised learning (Oord et al., 2018), where the main objective is to maximize the similarity between a query and its corresponding positive instance, while creating a distance from other negatives (Sohn, 2016). In the context of PRO, we model similarity as the parameters of the language model to measure the likelihood of generating a response. We expect that this modeling approach will encourage future researchers to fully explore the extensive research achievements in contrastive learning, ultimately achieving better human alignment."
        },
        {
            "heading": "3.2 Grafting RLHF onto PRO",
            "text": "While PRO can optimize directly on the humanannotated preference ranking sequence without the need for introducing concepts like the reward model in RLHF, we have found that grafting RLHF onto PRO can bring more flexibility to PRO. We outline three possible upgrades as follows:\nAffordable Preference Ranking. PRO is highly flexible, relying solely on an arbitrarily long ranked preference sequence. The source of the sequence is unrestricted, allowing for various possibilities.\nOne approach involves requesting annotators to imagine multiple responses of different quality. Alternatively, a more efficient method entails utilizing different existing LLMs, such as ChatGPT and Alpaca, to generate multiple responses. These responses can then be ranked using an additional reward model r\u03d5, similar to RLHF.\nDifferentiated Contrast. The formulation of LPRO, as shown in Equation 8, treats all responses yi \u227a yk as negative examples of yk and applies the same penalty to them. However, this approach may not be reasonable, especially when the preference scores of different yi are similar. For instance, when the preference of yk+1 is only slightly worse than yk, while yn is significantly worse than yk, the model should differentiate and apply different penalty strengths, slightly penalizing yk+1 and heavily penalizing yn compared to yk. To address this, we propose using the score r\u03d5(x, yi) from a reward model r\u03d5 to indicate the numerical preference of yi, and modify Equation 8 as follows:\nLPRO = \u2212 n\u22121\u2211 k=1 log exp\n( r\u03c0PRO (x,y k)\nT kk ) \u2211n\ni=k exp ( r\u03c0PRO (x,y i)\nT ik ) (9) where\nT i>kk = 1\nr\u03d5(x, yk)\u2212 r\u03d5(x, yi) (10)\nT kk = min i>k T ik (11)\nWhen the difference between r\u03d5(x, yk) and r\u03d5(x, y i) increases, the preference gap between yk and yi becomes more evident. Consequently, the temperature T ik decreases, amplifying the penalty of positive example yk towards yi, while it decreases when the difference is smaller. T kk is defined as the minimum temperature among all the negative examples to maintain a balance between the numerator and denominator. Our experiments (\u00a74.7) reveal that the dynamic temperature design significantly enhances model performance when optimizing LPRO alone while excluding LSFT. It also provides some performance gains when jointly optimizing LPRO and LSFT.\nSelf-bootstrapping Augmentation. Furthermore, it is worth noting that the length of sequences that PRO relies on is variable. In other words, there is no requirement for fixed sequences during training. This allows us to consider grafting the self-bootstrapping advantage of RLHF as\na subset onto PRO. Specifically, RLHF aims to continuously evaluate the model\u2019s responses during training by employing a reward model. Positive or negative rewards are provided to bootstrap the Language Model itself. Similarly, with PRO, given the prompt x and the current model, we sample a response y\u0302 and add it to the existing response set {y1, \u00b7 \u00b7 \u00b7 , yn}. Subsequently, we re-rank the responses using the reward model, yielding p(y\u03021,\u00b7\u00b7\u00b7 ,n+1 | x). Therefore, further optimization can be performed by refreshing Equation 7:\nLPRO(y1,\u00b7\u00b7\u00b7 ,n | x) \u21d2 LPRO(y\u03021,\u00b7\u00b7\u00b7 ,n+1 | x) (12)\nThe abstract training procedures are as follows:\nAlgorithm 1: Self-bootstrap PRO Input: Language Model \u03c00LM, Reward Model r\u03d5,\nRaw Dataset D, Output: The fine-tuned LM \u03c0PRO 1 Split D into {D0, D1, ..., DK\u22121} 2 for Di \u2208 {D0, D1, ..., DK\u22121} do 3 for Sample d \u2208 Di do 4 x\u2190 Prefix (d) 5 { yj } \u2190 Candidates (d) 6 y\u0302 \u2190 \u03c0iLM(x) // Sampling from LM 7 Add y\u0302 to { yj }\n8 Score and re-rank { yj }\nwith x and r\u03d5 9 end for\n10 \u03c0i+1LM \u2190 PRO(\u03c0 i LM, Di) // Train LLM 11 end for 12 \u03c0PRO \u2190 \u03c0KLM"
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "We conduct experiments mainly based on Human Preference Data about Helpfulness and Harmlessness, i.e., HH-RLHF described in Bai et al. (2022a). It has 4 sub-sets, namely Harmlessbase, Helpfulbase, Helpfulonline and Helpfulrejection, where each sample contains two different conversations rated by human annotators and is grouped into train/test splits. We refer to the code2 released by OpenAssistant and filter all data to ensure that the chosen and rejected conversations in the same sample have identical contexts but different responses. Details can be found in Table 1.\nWe combine training data from 4 sub-sets to fine-tune models and evaluate them on each of the test sets, while we do validation with 280 samples randomly selected from all test data. Each sample\n2https://github.com/LAION-AI/Open-Assistant\nfrom the raw dataset contains a chosen conversation and a rejected one, which constitutes a relatively short ranking. To further evaluate the performance of different models on longer human preference rankings, we enhance each sample with additional responses from Alpaca (Taori et al., 2023) and ChatGPT3, thereby expanding the range of ranked candidates. We refer to these augmented datasets as HH-RLHFLLM,i, where LLM represents the language models used (Alpaca, ChatGPT, etc.), and i denotes the length of the rankings. The unmodified dataset is referred to as HH-RLHFraw."
        },
        {
            "heading": "4.2 Evaluation Metrics",
            "text": "We present the findings of our study using various evaluation methods: automatic, model-based, and human-based metrics. In our main experiment, we utilize BLEU (Papineni et al., 2002) to assess the text quality and the Reward model to measure the level of human preference gained. These metrics allow us to evaluate the performance of numerous models automatically. For the analysis experiment, we employ human evaluators to conduct pairwise comparisons among the top-performing models identified through automated evaluations. Human evaluation is the gold standard for assessing human preferences (Zhou et al., 2023). An annotator judge is presented with a question and two responses and tasked with determining the better option or declaring a tie. Furthermore, recent studies have shown that GPT-4 (OpenAI, 2023) effectively evaluates the responses of chat assistants and aligns with human preferences (Zheng et al., 2023; Wang et al., 2023). Consequently, we involve GPT-4 to select a model generation from the two options. To mitigate positional bias (Zheng et al., 2023; Wang et al., 2023), we evaluate each candidate in both positions during two separate runs, and the final score is computed as the average of the two runs."
        },
        {
            "heading": "4.3 Implementation Detail",
            "text": "In this work, we choose LLaMA-7B (Touvron et al., 2023) as the backbone model, which has become a widespread test field for LLM research (Taori et al., 2023). We fine-tune it with PRO algorithm built on Huggingface.Library (Wolf et al., 2020).\nWe calculate BLEU scores to compare inference results with human-selected responses in test sets. To capture human preferences, reward models are\n3https://chat.openai.com/\nused as proxies. Additionally, we expand the training set by incorporating output results from existing LLMs, requiring the sorting of the expanded preference rankings. However, manual sorting is time-consuming and costly, especially considering the large number of instances in the training set. Therefore, we employ an additional reward model to score and rearrange all candidate rankings during the pre-processing stage of training. To avoid unfairness in evaluation, we select two different reward models for training and evaluation, which we denote as RMtrain4 and RMeval5, respectively. Reward values from RMeval are normalized with the Sigmoid function in case RMeval provides extreme values that excessively influence the overall performance.\nMoreover, we assign \u03b2, the weight SFT loss, to 0.05 \u2217 (l \u2212 1)2 where l is the ranking length. The sequence length, epoch, and learning rate are set to 512, 2, and 5e-6, respectively, while the maximum number of new tokens generated during inference is 128. We deploy our complete framework using 8 devices, 7 of which are dedicated to the model training, while the remaining one houses RMtrain for validation and potential self-bootstrapping augmentation (We consider this augmentation strategy as an analytical experiment and, unless otherwise specified, augmentation will not be enabled). With a batch size of 2 per device, we leverage a gradient accumulation step of 8, resulting in a total batch size of 112. More particulars can be found in our code.\n4https://huggingface.co/OpenAssistant/oasst-rm-2.1pythia-1.4b-epoch-2.5\n5https://huggingface.co/OpenAssistant/oasst-rm-2pythia-6.9b-epoch-1"
        },
        {
            "heading": "4.4 Baselines",
            "text": "We compare PRO with zero-shot baselines, and models fine-tuned on LLaMA-7B (Touvron et al., 2023) which share the same backbone with PRO: LLaMA (Touvron et al., 2023) is a collection of prevalent foundation models released to enhance research on LLM techniques of training, inference, and widespread applications. We evaluate the 7B version of LLaMA (LLaMA-7B) to be consistent with other fine-tuned baselines. Curie (Brown et al., 2020a) is considered as the 6.7B version of GPT-3, which has a similar size to LLaMA-7B. The model name used in API calls is text-curie-001. Alpaca (Taori et al., 2023) is an instruction-tuned version of LLaMA based on 52K instructionfollowing data. It is estimated to have a similar instruction-following competence with text-davinci-003 on the Self-Instruct evaluation suite (Wang et al., 2022). ChatGLM (Du et al., 2022) is an bilingual chatbot with 6.2B parameters. Having been implemented on GLM architecture (Du et al., 2022) and trained with SFT and RLHF on a large-scale conversation dataset, it manifests great potential of being in line with human preference. We implement it with its official code. ChatGPT is an online chat platform developed by OpenAI, which possesses great human-like abilities and allows versatile uses completed in the conversation form, after RLHF fine-tuning. SFT is the basic method that naively selects the top 1 candidate to fine-tune language models. Note that if we choose the best response in a preference ranking sequence sorted by a reward model, known as best-of-n sampling, SFT evolves into BoN. RLHF is successively promoted by Ziegler et al. (2019) and Ouyang et al. (2022a) to align the core of language models with human preference in Reinforcement Learning settings. We implement SFT and RLHF according to trlx. CoH (Liu et al., 2023) enforces language models to differentiate the most preferred candidate from the least preferred with prompts, which actually aligns models with human preference from a semantic perspective. We implement it with Huggingface.Library (Wolf et al., 2020) according to its original version. RRHF (Yuan et al., 2023) takes candidate ranking into account, and distinguishes different candidates through pair-wise ranking losses. We implement it\nwith its official code."
        },
        {
            "heading": "4.5 Main Results",
            "text": "Table 2 contains the experimental results of comparison between PRO and other baselines. To verify that PRO has a globally competitive ability to capture human preference from rankings with diverse lengths, we do experiments on HH-RLHFraw, HH-RLHFAlpaca,3 and HH-RLHFChatgpt,3, last 2 of which are augmented from HH-RLHFraw by Alpaca and ChatGPT, respectively.\nIn general, it can be found that LLaMA with finetuning has a notable improvement on BLEU and Reward against initial LLaMA, which has not undergone any specific alignment with human preference. Also, even without fine-tuning on HH-RLHF, models tuned on large-scale corpus still show certain performance, while ChatGLM and ChatGPT with RLHF training beat LLaMA, Curie, and Alpaca that are trained from scratch. All of these prove the significance of Human Alignment.\nNext, we compare different human alignment algorithms using the same backbone on the same dataset. Even in the most basic setting of HHRLHFraw, where the ranking length is 2 with only one positive example and one negative example, PRO has already significantly outperformed all baselines in terms of reward score while maintaining considerable BLEU scores. Specifically, com-\npared to SFT, PRO improves the reward score by 6.52 points, and compared to the state-of-the-art human alignment algorithm RRHF, it improves the score by 3.1 points. This demonstrates that even without expanding the ranking sequence, PRO remains the best-performing approach. CoH achieves higher BLEU scores but falls short of PRO in terms of reward, which is mediocre. PRO exhibits a distinct advantage in terms of Harmlessness compared to Helpfulness. We attribute this to the fact that achieving Harmlessness is comparatively easier for PRO as it primarily involves significant features such as adapting expression styles and maintaining politeness in most conversations. On the other hand, Helpfulness typically demands more specific suggestions, which pose a greater challenge for language models due to their limited world knowledge, thus increasing the difficulty in this aspect.\nWhen expanding the ranking sequence using existing LLMs and sorting it with an additional reward model (different from the evaluation reward model), we find that the utilized LLM plays a crucial role in achieving concrete performances. Since ChatGPT surpasses Alpaca in understanding human preferences, it provides superior samples during data augmentation compared to Alpaca, making HH-RLHFChatgpt,i intuitively better than HH-RLHFAlpaca,3. The performance of each method increases from HH-RLHFChatgpt,3 to HH-\nRLHFAlpaca,3. On the expanded sequences, we observe that BoN (selecting the response with the highest reward model score for SFT) becomes a competitive baseline. This finding aligns with Rafailov et al. 2023, who observed that RLHF is less tuning-efficient than BoN. The effectiveness of RRHF becomes less prominent because it relies on pairwise comparisons between candidates from given rankings. It fails to capture global differences corresponding to human preference in the long rankings, which can be achieved through Equation 5. Overall, in the expanded ranking, PRO remains the best-performing method, and the more powerful the LLM used for ranking augmentation, the more pronounced the improvement of PRO\u2019s performance. This surprising characteristic fills us with anticipation for PRO\u2019s future development."
        },
        {
            "heading": "4.6 Human and GPT-4 Evaluation",
            "text": "One could argue that the reward model fails to capture all human preferences in evaluation. Human annotation is considered the most accurate evaluation method, and recently, GPT-4-as-a-judge has emerged as a scalable approach for rapidly assessing human preference. Therefore, in this section, we provide comprehensive evaluations conducted by both GPT-4 and humans. To address cost concerns, we primarily compare the performance of PRO against two alternative counterparts: PRO vs. Golden, i.e. the 1st candidate provided by the datasets, we aim to determine whether PRO trained on HH-RLHFraw can achieve or surpass human-preferred responses provided by the raw dataset. PRO vs. RRHF, both of which are trained on HH-RLHFraw.\nWe aim to verify whether PRO is truly preferred over RRHF in terms of human preferences, even in ranking sequences of length 2 that do not fully exploit PRO\u2019s capabilities. On the other hand, this comparison serves as evidence to some extent for the validity of the reward model we use in evaluation.\nFor GPT-4 evaluation, we first sample contexts in each test set. We assemble two corresponding responses from PRO and its counterparty into a modified version of the prompt template from Zheng et al. (2023) for GPT-4 scoring. We also refer to Wang et al. (2023) to provide two candidates in binary directions respectively, to eliminate unfairness triggered by candidate order. For Human evalua-\ntion, we employ human labelers to estimate the same samples with GPT-4 evaluation, and directly distinguish one response from another.\nTable 3 and 4 give the detailed results, where both GPT-4 and Human more support PRO globally for each comparison, thus highlighting the strengths of PRO. We are surprised to find that both humans and GPT-4 consider the predictions of PRO to be better than the human-preferred responses annotated in the dataset. This suggests that PRO is able to effectively capture the preferences of humans as reflected in the annotated data. Furthermore, our evaluation using the reward model yielded consistent results, with both humans and GPT-4 significantly favoring PRO over RRHF. This not only reaffirms the effectiveness of PRO but also demonstrates that our reward model can reasonably evaluate human preferences."
        },
        {
            "heading": "4.7 Ablation Study",
            "text": "In this part, we investigate the effectiveness of each part in PRO, the results are contained in Table 5.\nSFT Loss To avoid the model solely catering to the reward model at the expense of text quality, we introduce LSFT. Therefore, removing LSFT lowers BLEU scores on three datasets, but higher quality of corpus can, to some extent, compensates for its drop, as is proved by results on HHRLHFAlpaca,3 and HH-RLHFChatGPT,3 compared with HH-RLHFraw.\nPRO Loss Table 2 also demonstrates the influence of LPRO, as excluding it in PRO essentially equals to SFT (BoN) that gets lower Reward.\nAdequate Ranking To fully leverage the ranking y1 \u227b y2 \u227b \u00b7 \u00b7 \u00b7 \u227b yn, we employ n\u2212 1 loss functions to model y1 \u227b y2, \u00b7 \u00b7 \u00b7 , yn, y2 \u227b y3, \u00b7 \u00b7 \u00b7 , yn, . . . , yn\u22121 \u227b yn. Our objective is to adequately model all ranking orders and enable LLM to better differentiate between samples of different preferences. To validate this idea, we deactivate LPRO except for its first term, L1PRO. Experimental results on three datasets consistently demonstrate a decrease in both BLEU and Reward scores, thus confirming the effectiveness of Equation 5.\nTemperature PRO With or without temperature (T ), the model performs well, but T slightly enhances overall performance. Furthermore, we observe a significant drop in model performance when both the SFT loss and temperature are removed simultaneously, whereas removing either one individually did not have such a noticeable impact. We believe this is because temperature helps the model understand that some negative examples are neutral (with reward scores similar to positive examples), and thus should not be overly penalized\nto avoid confusion during LLM training. Similarly, the inclusion of SFT loss also plays a similar role by increasing the weight of the best response."
        },
        {
            "heading": "4.8 Discussions",
            "text": ""
        },
        {
            "heading": "4.8.1 How about continually expanding Preference Ranking Sequence?",
            "text": "In Table 2, we have observed that expanding the ranking sequence of HH-RLHFraw from length 2 to 3 using LLMs improves the performance of all models. This leads us to wonder how the effect would change if we further expand the preference ranking sequence. Specifically, we simulate 5 expansion strategies, each introducing 3 additional responses to extend the preference sequence to length 5, followed by reranking using a reward model. Alpacas: Using Alpaca-7B, we generate 3 responses, adding 1, 2, and 3 responses, respectively,\nto form ranking sequences of lengths 3, 4, and 5. ChatGPT: Using ChatGPT, we generate three responses, adding 1, 2, and 3 responses, respectively, to form ranking sequences of lengths 3, 4, and 5. Ascending: We utilize three LLMs, namely Curie, Alpaca-7B, and ChatGPT. Based on the zero-shot results in Table 2, the quality of their responses can be ranked as ChatGPT > Alpaca-7B > Curie. In the Ascending setting, we add the responses in ascending order of quality. That is, for a sequence of length 3, we added Curie\u2019s response; for a sequence of length 4, we added Curie and Alpaca7B\u2019s responses; and for a sequence of length 5, we added Curie, Alpaca-7B, and ChatGPT\u2019s responses. Descending: The data source is the same as Ascending, but the responses are added in the opposite order. For a sequence of length 3, we added ChatGPT\u2019s response; for a sequence of length 4, we added ChatGPT and Alpaca-7B\u2019s responses; and for a sequence of length 5, we added Curie, Alpaca7B, and ChatGPT\u2019s responses. Random: The order of response additions is unrelated to response quality and is done randomly.\nIn Figure 3, we present the impact of various expansion strategies on the effectiveness of PRO after expanding sequences of different lengths. Our observations are as follows:\nLonger Ranking, Better results: Overall, longer ranking sequences generally lead to improved performance for most strategies, which is an exciting finding, as expanding the ranking sequence is a relatively straightforward task compared to designing new prompts.\nBetter added responses, better results: If a single model is used to generate additional responses, supplementing one response is sufficient when the quality is average, such as with Alpaca, adding more responses provides limited improvement. However, when the quality of responses is high, as with ChatGPT, adding more responses leads to consistent performance gains. This could\npotentially offer new insights for the design of future Human Alignment algorithms.\nMore diversified added responses, better results: We have also discovered that incorporating lower-quality responses may actually improve the model\u2019s results compared to using only high-quality responses. Interestingly, when the sequence length is 4, Ascending (blue line) =Curie+Alpaca surpasses the performance of Alpaca(red line)=Alpaca+Alpaca, even though Curie\u2019s response quality is not as good as Alpaca\u2019s. We believe this is because diverse responses, even if they are negative examples, help the language model become more aware of behaviors that should be avoided, thereby enhancing overall performance. Lastly, by combining Curie, Alpaca, and ChatGPT, we achieve a performance close to using three ChatGPT responses, demonstrating the truth in the saying, \"Two heads are better than one.\""
        },
        {
            "heading": "4.8.2 Can self-bootstrapping augmentation enhance performance?",
            "text": "We have demonstrated the effectiveness of incorporating responses from other LLMs to expand ranking length, which significantly improves human preference. A natural question arises: Can we further improve the model\u2019s performance by including responses from the LLM itself in the candidate list? This can be seen as a special approach to expanding preference ranking sequences.\nFrom Table 6, we find that self-bootstrapping6\nexhibits conflicting results. On HH-RLHFraw, selfbootstrapping shows an improvement in BLEU but a slight decrease in reward score. On HHRLHFAlpaca,3, both BLEU and reward score decrease. However, on HH-RLHFChatGPT,3, selfbootstrapping improves reward score while maintaining BLEU value. We speculate that self-\n6The naive self-bootstrapping makes LLMs easily overfit RMtrain. We accordingly regularize it by preventing the augmented candidate from taking the position of the originally top 1, and re-ranking all reward to ensure the descending order.\nbootstrapping is effective only when the underlying language model is strong. Furthermore, although self-bootstrapping enhances performance on HHRLHFChatGPT,3, it can be seen as extending the ranking sequence to 4, and the improvement may not be as significant as adding an additional high-quality response generated by ChatGPT. We also acknowledge that these relatively negative results may stem from training a 7B model with a reward model of size 1.4B. Expanding the model size might yield more exciting performance gains, similar to the scaling law of RLHF (Ouyang et al., 2022b; Gao et al., 2022), which we leave for future work."
        },
        {
            "heading": "5 Related Work",
            "text": ""
        },
        {
            "heading": "5.1 Reinforcement Learning from Human Feedback",
            "text": "Fine-tuning language models to align with human preferences has emerged as a critical research problem. It can be formulated as given a context and corresponding suffixes ranked or scored by human annotators without more detailed labels, the agent is required to learn human preference and provide human-like results. Reinforcement Learning (RL) plays the most straightforward way to reach this goal, for the agent needs just scarce supervision signal from reward models as human proxies, and is modified through numerous trials under RL framework, namely Reinforcement Learning from Human Feedback (RLHF). Many explorations have been done on this path (Christiano et al., 2017; MacGlashan et al., 2017; Warnell et al., 2018; Ziegler et al., 2019; Stiennon et al., 2020b; Nakano et al., 2021; Lee et al., 2021; Lei et al., 2022; Snell et al., 2022; Bai et al., 2022a; Ouyang et al., 2022a). Lei et al. (2022) implement an online RL scenario by establishing a user simulator commonly for training and evaluation, which at each session is initialized with different Gaussian vectors representing diverse personalities. In contrast, ILQL is applicable for the offline setting, which is released by Snell et al. (2022). Stiennon et al. (2020b) and Nakano et al. (2021) investigate the RLHF method for text summarization and question answering, respectively. Bai et al. (2022a) apply RLHF to enable LLMs to become harmless and helpful, while releasing a new conversational dataset with human feedback. Known as a masterpiece, Ouyang et al. (2022a) propose InstructGPT which is first fine-tuned in a supervised way, then continually modified under PPO algorithm (Schul-\nman et al., 2017). This process is cyclic, during which the performance of the trained agent spirals upwards. It is also applied to the famous ChatGPT by OpenAI."
        },
        {
            "heading": "5.2 Supervised Fine-tuning for Human Preference Alignment",
            "text": "Despite appealing advantages, RL-based methods have obvious limitations regarding training efficiency and complexity, consequently driving researchers to focus on Supervised Fine-tuning methods without these challenges. Liu et al. (2023) combine desirable and undesirable suffixes in a template prompted by opposite keywords, thus fully dependent on a highly semantic understanding of large language models. Yuan et al. (2023) compose multiple pairwise comparisons between suffixes in the given ranking, which forms a new algorithm from the perspective of training objectives. Rafailov et al. (2023) similarly transform LLMs as a Bradley-Terry model to measure chosen and rejected candidates by human annotators. The proposed PRO chooses the path of modifying the SFT objective, but is further promoted from RLHF formulation and inherits its straightforwardness towards Human Preference Alignment. In particular, PRO transforms RL\u2019s indirect optimization into a direct one, and extends pairwise comparisons to multi-dimensional and multi-positional comparisons. Comprehensive experiments prove its excellence in human preference acquisition while maintaining the quality of generated texts."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we derive from the Bradley-Terry comparison of the reward model in RLHF that human alignment can be modeled as aligning the probability ranking of n responses generated by the LLM and the preference ranking of these responses by humans. Based on this derivation, we propose PRO. PRO inherits the advantages of RLHF, and further captures fine-grained distinction corresponding to human preference from multiple oneto-many comparisons. We conduct extensive experiments to verify the excellence of PRO against other baselines and investigate the impact of multifaceted factors. Overall, the findings presented in this paper demonstrate the significance of PRO in effectively and efficiently aligning LLMs to human preference. This work can serve as a stepping stone for further quantifiable explorations.\nDisclaimer\nSince some services provided by OpenAI are currently not available in mainland China, data augmentation and inference from ChatGPT, as well as GPT-4 evaluation, are completed where the related services are available.\nThere exists sensitive and offensive content in HH-RLHF, which aims for only research purposes. Viewpoints included in the data do not represent our attitudes. We hope our work can be used to make AI technologies in line with ethical requirements."
        }
    ],
    "title": "Preference Ranking Optimization for Human Alignment",
    "year": 2023
}