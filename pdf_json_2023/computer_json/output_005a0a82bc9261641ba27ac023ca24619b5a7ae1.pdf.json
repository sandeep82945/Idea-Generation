{
    "abstractText": "Despite the significant research efforts on trajectory prediction for automated driving, limited work exists on assessing the prediction reliability. To address this limitation we propose an approach that covers two sources of error, namely novel situations with out-of-distribution (OOD) detection and the complexity in in-distribution (ID) situations with uncertainty estimation. We introduce two modules next to an encoderdecoder network for trajectory prediction. Firstly, a Gaussian mixture model learns the probability density function of the ID encoder features during training, and then it is used to detect the OOD samples in regions of the feature space with low likelihood. Secondly, an error regression network is applied to the encoder, which learns to estimate the trajectory prediction error in supervised training. During inference, the estimated prediction error is used as the uncertainty. In our experiments, the combination of both modules outperforms the prior work in OOD detection and uncertainty estimation, on the Shifts robust trajectory prediction dataset by 2.8% and 10.1%, respectively. The code is publicly available. \u00a9 2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.",
    "authors": [
        {
            "affiliations": [],
            "name": "Julian Wiederer"
        },
        {
            "affiliations": [],
            "name": "Julian Schmidt"
        },
        {
            "affiliations": [],
            "name": "Ulrich Kressel"
        },
        {
            "affiliations": [],
            "name": "Klaus Dietmayer"
        },
        {
            "affiliations": [],
            "name": "Vasileios Belagiannis"
        }
    ],
    "id": "SP:8d3982769f8d043498160fc978e8278d959ea839",
    "references": [
        {
            "authors": [
                "A. Malinin",
                "N. Band",
                "Y. Gal",
                "M. Gales",
                "A. Ganshin",
                "G. Chesnokov",
                "A. Noskov",
                "A. Ploskonosov",
                "L. Prokhorenkova",
                "I. Provilkov",
                "V. Raina",
                "V. Raina",
                "D. Roginskiy",
                "M. Shmatova",
                "P. Tigas",
                "B. Yangel"
            ],
            "title": "Shifts: A dataset of real distributional shift across multiple large-scale tasks",
            "venue": "Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M. Bahari",
                "S. Saadatnejad",
                "A. Rahimi",
                "M. Shaverdikondori",
                "A.H. Shahidzadeh",
                "S.-M. Moosavi-Dezfooli",
                "A. Alahi"
            ],
            "title": "Vehicle trajectory prediction works, but not everywhere",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2022, pp. 17 123\u201317 133.",
            "year": 2022
        },
        {
            "authors": [
                "B. Ivanovic",
                "Y. Lin",
                "S. Shrivastava",
                "P. Chakravarty",
                "M. Pavone"
            ],
            "title": "Propagating state uncertainty through trajectory forecasting",
            "venue": "2022 International Conference on Robotics and Automation (ICRA). IEEE, 2022, pp. 2351\u20132358.",
            "year": 2022
        },
        {
            "authors": [
                "Z. Zhou",
                "L. Ye",
                "J. Wang",
                "K. Wu",
                "K. Lu"
            ],
            "title": "Hivt: Hierarchical vector transformer for multi-agent motion prediction",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 8823\u20138833.",
            "year": 2022
        },
        {
            "authors": [
                "M. Liang",
                "B. Yang",
                "R. Hu",
                "Y. Chen",
                "R. Liao",
                "S. Feng",
                "R. Urtasun"
            ],
            "title": "Learning lane graph representations for motion forecasting",
            "venue": "Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part II 16. Springer, 2020, pp. 541\u2013556.",
            "year": 2020
        },
        {
            "authors": [
                "T. Salzmann",
                "B. Ivanovic",
                "P. Chakravarty",
                "M. Pavone"
            ],
            "title": "Trajectron++: Dynamically-feasible trajectory forecasting with heterogeneous data",
            "venue": "European Conference on Computer Vision. Springer, 2020, pp. 683\u2013700.",
            "year": 2020
        },
        {
            "authors": [
                "B. Varadarajan",
                "A. Hefny",
                "A. Srivastava",
                "K.S. Refaat",
                "N. Nayakanti",
                "A. Cornman",
                "K. Chen",
                "B. Douillard",
                "C.P. Lam",
                "D. Anguelov"
            ],
            "title": "Multipath++: Efficient information fusion and trajectory aggregation for behavior prediction",
            "venue": "2022 International Conference on Robotics and Automation (ICRA). IEEE, 2022, pp. 7814\u20137821.",
            "year": 2022
        },
        {
            "authors": [
                "J. Wiederer",
                "J. Schmidt",
                "U. Kressel",
                "K. Dietmayer",
                "V. Belagiannis"
            ],
            "title": "A benchmark for unsupervised anomaly detection in multi-agent trajectories",
            "venue": "2022 IEEE 25th International Conference on Intelligent Transportation Systems (ITSC), 2022, pp. 130\u2013137.",
            "year": 2022
        },
        {
            "authors": [
                "N. Chakraborty",
                "A. Hasan",
                "S. Liu",
                "T. Ji",
                "W. Liang",
                "D.L. McPherson",
                "K. Driggs-Campbell"
            ],
            "title": "Structural attention-based recurrent variational autoencoder for highway vehicle anomaly detection",
            "venue": "arXiv preprint arXiv:2301.03634, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "A. Pustynnikov",
                "D. Eremeev"
            ],
            "title": "Estimating uncertainty for vehicle motion prediction on yandex shifts dataset",
            "venue": "arXiv preprint arXiv:2112.08355, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A. Postnikov",
                "A. Gamayunov",
                "G. Ferrer"
            ],
            "title": "Transformer based trajectory prediction",
            "venue": "arXiv preprint arXiv:2112.04350, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "T. Gilles",
                "S. Sabatini",
                "D.V. Tsishkou",
                "B. Stanciulescu",
                "F. Moutarde"
            ],
            "title": "Uncertainty estimation for cross-dataset performance in trajectory prediction",
            "venue": "2022 International Conference on Robotics and Automation Workshop (ICRA Workshop), 2022.",
            "year": 2022
        },
        {
            "authors": [
                "B. Lakshminarayanan",
                "A. Pritzel",
                "C. Blundell"
            ],
            "title": "Simple and scalable predictive uncertainty estimation using deep ensembles",
            "venue": "Advances in neural information processing systems, vol. 30, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "Y. Gal",
                "Z. Ghahramani"
            ],
            "title": "Dropout as a bayesian approximation: Representing model uncertainty in deep learning",
            "venue": "international conference on machine learning. PMLR, 2016, pp. 1050\u20131059.",
            "year": 2016
        },
        {
            "authors": [
                "K. Lee",
                "K. Lee",
                "H. Lee",
                "J. Shin"
            ],
            "title": "A simple unified framework for detecting out-of-distribution samples and adversarial attacks",
            "venue": "Advances in neural information processing systems, vol. 31, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "N.A. Ahuja",
                "I. Ndiour",
                "T. Kalyanpur",
                "O. Tickoo"
            ],
            "title": "Probabilistic modeling of deep features for out-of-distribution and adversarial detection",
            "venue": "NeurIPS Bayesian Deep Learning Workshop, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Chai",
                "B. Sapp",
                "M. Bansal",
                "D. Anguelov"
            ],
            "title": "Multipath: Multiple probabilistic anchor trajectory hypotheses for behavior prediction",
            "venue": "CoRL, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "J. Strohbeck",
                "V. Belagiannis",
                "J. M\u00fcller",
                "M. Schreiber",
                "M. Herrmann",
                "D. Wolf",
                "M. Buchholz"
            ],
            "title": "Multiple trajectory prediction with deep temporal and spatial convolutional neural networks",
            "venue": "2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020, pp. 1992\u20131998.",
            "year": 2020
        },
        {
            "authors": [
                "J. Gao",
                "C. Sun",
                "H. Zhao",
                "Y. Shen",
                "D. Anguelov",
                "C. Li",
                "C. Schmid"
            ],
            "title": "Vectornet: Encoding hd maps and agent dynamics from vectorized representation",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 11 525\u201311 533.",
            "year": 2020
        },
        {
            "authors": [
                "J. Schmidt",
                "J. Jordan",
                "F. Gritschneder",
                "K. Dietmayer"
            ],
            "title": "Cratpred: Vehicle trajectory prediction with crystal graph convolutional neural networks and multi-head self-attention",
            "venue": "2022 International Conference on Robotics and Automation (ICRA). IEEE Press, 2022, p. 7799\u20137805.",
            "year": 2022
        },
        {
            "authors": [
                "M.-F. Chang",
                "J. Lambert",
                "P. Sangkloy",
                "J. Singh",
                "S. Bak",
                "A. Hartnett",
                "D. Wang",
                "P. Carr",
                "S. Lucey",
                "D. Ramanan"
            ],
            "title": "Argoverse: 3d tracking and forecasting with rich maps",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 8748\u20138757.",
            "year": 2019
        },
        {
            "authors": [
                "Y.-C. Hsu",
                "Y. Shen",
                "H. Jin",
                "Z. Kira"
            ],
            "title": "Generalized odin: Detecting out-of-distribution image without learning from out-of-distribution data",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 10 951\u201310 960.",
            "year": 2020
        },
        {
            "authors": [
                "J. Hornauer",
                "V. Belagiannis"
            ],
            "title": "Heatmap-based out-of-distribution detection",
            "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2023, pp. 2603\u20132612.",
            "year": 2023
        },
        {
            "authors": [
                "J. Ren",
                "P.J. Liu",
                "E. Fertig",
                "J. Snoek",
                "R. Poplin",
                "M. Depristo",
                "J. Dillon",
                "B. Lakshminarayanan"
            ],
            "title": "Likelihood ratios for out-of-distribution detection",
            "venue": "Advances in neural information processing systems, vol. 32, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "C. Richter",
                "N. Roy"
            ],
            "title": "Safe visual navigation via deep learning and novelty detection",
            "venue": "Robotics: Science and Systems, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "V. Sindhwani",
                "H. Sidahmed",
                "K. Choromanski",
                "B. Jones"
            ],
            "title": "Unsupervised anomaly detection for self-flying delivery drones",
            "venue": "2020 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2020, pp. 186\u2013192.",
            "year": 2020
        },
        {
            "authors": [
                "J. Wiederer",
                "A. Bouazizi",
                "M. Troina",
                "U. Kressel",
                "V. Belagiannis"
            ],
            "title": "Anomaly detection in multi-agent trajectories for automated driving",
            "venue": "Proceedings of the 5th Conference on Robot Learning, ser. Proceedings of Machine Learning Research, A. Faust, D. Hsu, and G. Neumann, Eds., vol. 164. PMLR, 08\u201311 Nov 2022, pp. 1223\u2013 1233.",
            "year": 2022
        },
        {
            "authors": [
                "T. Monninger",
                "J. Schmidt",
                "J. Rupprecht",
                "D. Raba",
                "J. Jordan",
                "D. Frank",
                "S. Staab",
                "K. Dietmayer"
            ],
            "title": "Scene: Reasoning about traffic scenes using heterogeneous graph neural networks",
            "venue": "IEEE Robotics and Automation Letters, vol. 8, no. 3, pp. 1531\u20131538, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "C.M. Bishop"
            ],
            "title": "Mixture density networks",
            "venue": "Aston University, 1994.",
            "year": 1994
        },
        {
            "authors": [
                "C.M. Bishop",
                "N.M. Nasrabadi"
            ],
            "title": "Pattern recognition and machine learning",
            "year": 2006
        },
        {
            "authors": [
                "B. Ivanovic",
                "M. Pavone"
            ],
            "title": "The trajectron: Probabilistic multiagent trajectory modeling with dynamic spatiotemporal graphs",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 2375\u20132384.",
            "year": 2019
        },
        {
            "authors": [
                "D. Hendrycks",
                "K. Gimpel"
            ],
            "title": "A baseline for detecting misclassified and out-of-distribution examples in neural networks",
            "venue": "CoRR, vol. abs/1610.02136, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "I. Loshchilov",
                "F. Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "ICLR, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "I. Loshchilov",
                "F. Hutter"
            ],
            "title": "SGDR: Stochastic gradient descent with warm restarts",
            "venue": "International Conference on Learning Representations, 2017.",
            "year": 2017
        }
    ],
    "sections": [
        {
            "text": "\u00a9 2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.\nI. INTRODUCTION\nReliable trajectory prediction is critical for the safe motion planning of an automated vehicle. However, trajectory prediction failures inevitably occur, for example in out-ofdistribution scenarios (OOD) [1], [2] or even uncertain indistribution (ID) situations [3]. Yet, the current trajectory prediction approaches [4], [5], [6], [7] do not deliver measures of reliability for the model predictions.\nThe prior work assesses the reliability of trajectory prediction with OOD detection [8], [9] and uncertainty estimation [10], [11], [12]. OOD detection distinguishes between ID and OOD scenarios, while uncertainty estimation determines the certainty of the prediction regardless of whether it is ID or OOD. Both tasks are necessary for safe automated driving. For instance, a situation with low uncertainty and a high OOD score needs to be treated carefully because it is novel w.r.t. to the training data. In another ID case, high uncertainty also needs to be taken into consideration because the predictions may be unreliable. Although both tasks are addressed independently in the literature, a joint formulation\n1Julian Wiederer, Julian Schmidt and Ulrich Kressel are with Mercedes-Benz Group AG, 70546 Stuttgart, Germany. {julian.wiederer, julian.sj.schmidt, ulrich.kressel}@mercedes-benz.com\n2Julian Wiederer and Vasileios Belagiannis are with the Department of Multi-media Communication and Signal Processing, Friedrich-Alexander-Universita\u0308t, 91058 Erlangen, Germany. vasileios.belagiannis@fau.de\nJulian Schmidt and Klaus Dietmayer are with the Institute of Measurement, Control and Microtechnology, University Ulm, 89081 Ulm, Germany. klaus.dietmayer@uni-ulm.de\n4 project page: https://github.com/againerju/joodu\nis still missing. The only attempt to jointly formulate both tasks is based on model ensembles [1] using Bootstrapped Ensembles [13] or Dropout Ensembles [14]. Both approaches compute the uncertainty from the output of multiple forward passes during inference, which is not desirable for realtime systems due to the computational cost. In contrast to the prior work, we refrain from ensembles and address the aforementioned limitations.\nWe propose a model-agnostic approach to expand an existing encoder-decoder network for trajectory prediction by joint OOD detection and uncertainty estimation. Given a trajectory prediction encoder, we introduce two modules to output the OOD score and the uncertainty, respectively. Our novelty lies in the joint formulation, as well as in our OOD detection approach. Firstly, inspired by approaches for OOD detection in image classification [15], [16], we leverage generative modeling on the encoder features. A Gaussian mixture model on the latent representation learns the probability density function of the ID features during training and detects OOD samples in regions of the feature space with low likelihood of this density function. Secondly, we add an error regression network [11] on the scene encoder for uncertainty estimation and train it with a supervised regression loss to estimate the prediction error. During inference, the estimated error is used as uncertainty. Fig. 1 illustrates our approach, including the differences to existing ar X\niv :2\n30 8.\n01 70\n7v 2\n[ cs\n.R O\n] 4\nA ug\n2 02\n3\ntrajectory prediction models. In the experiments on the Shifts dataset [1], our method outperforms the prior work by a large margin. Therefore, we adopt an existing trajectory prediction approach [4] to the new dataset and apply our modules to measure the reliability of the predictions.\nIn summary, we make the following contributions: \u2022 We present a joint model for trajectory prediction with\nOOD detection and uncertainty estimation, where we add two modules to a shared scene encoder. \u2022 We propose a Gaussian mixture model on the latent representation to detect the OOD samples in regions with a low likelihood of the encoder feature space. \u2022 We outperform the prior work in both tasks, OOD detection and uncertainty estimation, on the Shifts dataset [1] by a large margin."
        },
        {
            "heading": "II. RELATED WORK",
            "text": "In the following, we discuss the existing methods for trajectory prediction with a focus on OOD detection and uncertainty estimation.\na) Multi-Modal Trajectory Prediction: Multi-modal trajectory prediction approaches try to cover all possible futures by predicting a set of trajectories [4], [5], [6], [7], [17], [18] instead of only a single outcome. In recent studies, the encoder-decoder is the default architecture for this task, where the observed scene is encoded into a latent feature space and decoded from the latent space into future motion [4], [5], [6], [7], [17], [19], [20]. While these methods achieve impressive results on the trajectory prediction benchmarks [21], they tend to fail in scenes that are very different from the training data [2]. In addition, in some cases, errors in situations that are similar to the training data can also not be prevented [1]. Examples are highly complex scenarios with many possible outcomes. In practice, the current methods would have to rely on their prediction in all situations. Instead of unconditionally trusting the predictions, we propose two modules to assess the reliability of the prediction, namely the OOD detection and the uncertainty estimation. Both modules are applied to the scene encoder of an existing prediction model.\nb) Out-of-Distribution Detection for Trajectory Prediction: The goal of OOD detection is to discriminate the ID from OOD samples. While the main research is in the field of computer vision [22], [23], [24], in recent years, OOD detection has shown applications in robotics. For example, OOD detection approaches help robots to navigate through novel environments [25], drones to avoid collisions in cases other agents show abnormal flying behavior [26] or detect abnormal driving behavior like ghost drivers [9], [27]. Likewise, OOD detection can help to detect novel scenarios in trajectory prediction. Recently, this task has been addressed by Malinin et al. [1]. They propose to detect the OOD scenarios by aggregating the confidences of multiple prediction models using deep ensembles. However, ensembles require multiple prediction models during training and inference, which has limited application in real-time systems like automated driving. In this work, we propose\nan efficient method attached to the shared scene encoder. Our approach is closer to the feature-based OOD detection approaches [15], [16]. In particular, we present a Gaussian mixture model on the latent representation for modeling the probability density function of the neural network features and detecting the OOD scenarios in regions of the feature space with low likelihood.\nc) Uncertainty for Trajectory Prediction: While OOD detection helps to assess the prediction quality in cases the test distribution is different from the training distribution, prediction errors in ID scenarios may inevitably occur. In literature, these errors have been tackled by estimating the uncertainty [1]. For example, in deep ensembles, the alignment between the outputs of a set of models is used as uncertainty [1]. Gilles et al. avoid computationally demanding deep ensembles and compute the uncertainty as the integral of the prediction heatmap from a single model [12]. The idea of spectral-normalized Gaussian processes (SNGP) has also been applied to the task of uncertainty estimation [10]. The SNGP is directly applied to the scene encoder and the uncertainty is measured by the predicted Gaussian process variance. Similarly, our uncertainty module is applied to the latent features of the scene encoder, but we use error regression to estimate the uncertainty similar to [11]. Our error regression network is trained to approximate the trajectory prediction error. We demonstrate the effectiveness of the approach in comparison with the prior methods on the Shifts dataset [1]. Furthermore, we show that the error regression can estimate the prediction error on ID samples while the Gaussian mixture model can detect the OOD samples."
        },
        {
            "heading": "III. METHOD",
            "text": "Consider the traffic scene represented by a set of N agents and the scene context I, commonly provided by an HD-map. The number of agents N can vary between different scenes. Each agent i \u2208 {1, ..., N} is described by its sequence of states xi = {sti}0t=\u2212Th+1 observed over Th historical time steps, where sti \u2208 Rd contains the d agent states in time step t. The dynamic scene is summarized in X = {xi}Ni=1. In addition, each agent is assigned the OOD label \u03b1i \u2208 {0, 1}, indicating if the scene is ID, \u03b1 = 0, or OOD, \u03b1 = 1. The ground-truth future trajectory of agent i for the next Tf time steps is denoted as yi = {(xti, yti)} Tf t=1, where (x t i, y t i) is the agent location in x- and y-coordinates at time step t. Our first goal is to predict the conditional distribution p(yi|X, I) over the future trajectory of an agent i in the scene given the dynamic X and the static scene context I. Second, we propose to additionally compute an OOD score \u03b1\u0302i, which is small for ID, i.e. \u03b1i = 0, and large for OOD samples, i.e. \u03b1i = 1. Thrid, we estimate, at the same time, the uncertainty e\u0302i that approximates the true prediction error ei. For that reason, we assume the prediction error ei = E(yi, p(yi|X, I)) between the ground-truth trajectory yi and the predicted distribution p(yi|X, I) is measured by the error measurement E(\u00b7, \u00b7). We denote the output set for agent i as Yi = {p(yi|X, I), \u03b1\u0302i, e\u0302i}."
        },
        {
            "heading": "A. OOD Detection and Uncertainty Estimation",
            "text": "Fig. 2 gives an overview of our approach. Next, we describe the trajectory prediction network composed of the scene encoder fe (Sec. III-B) and the trajectory prediction decoder gp (Sec. III-C). The scene encoder computes the latent representation hi = fe(X, I) for each agent i by incorporating social interactions between agents in X and the scene context I. Subsequently, the trajectory prediction decoder outputs the probability distribution p(yi|{hi}Ni=1) = gp({hi}Ni=1) over the future states yi of agent i given all agent encodings {hi}Ni=1. To jointly predict the trajectory, OOD score and the uncertainty, we introduce two modules to the scene encoder, namely the OOD detection and the uncertainty estimation. The OOD detection, denoted as good, predicts a scalar-valued score \u03b1\u0302i = good(hi) given the latent feature vector hi quantifying if the scene is rather an ID or OOD (Sec. III-D). At the same time, the uncertainty estimation, denoted as gu, estimates the uncertainty in the predicted trajectories e\u0302i = gu(hi) from the same features (Sec. III-E). The scene encoder is shared across the trajectory prediction decoder and both modules. Both, the trajectory prediction decoder and the uncertainty estimation are represented by deep neural networks. For the OOD detection module, we present a Gaussian mixture model on the latent representation, which can detect the outliers in low-density regions of its distribution.\nThe model is trained in two stages (Sec. III-F). First, we train the trajectory prediction network and then optimize the two additional modules given the fixed scene encoder. We end up with a trajectory predictor and two modules to assess the reliability of the predictions resulting from this predictor. Although we show results with the proposed trajectory prediction model, both modules can be easily adapted to any scene encoder for OOD detection and uncertainty estimation, which outputs a feature vector hi per agent."
        },
        {
            "heading": "B. Scene Encoder",
            "text": "Lately, vectorization-based scene encoders have been getting a lot of attention [4], [5], [28]. Therefore, we use the local encoder from HiVT as the scene encoder fe and follow the pre-processing as proposed by Zhou et al. [4]. Inputs to the scene encoder are the observed states of all agents X = {xi}Ni=1 and the contextual information I provided by a set of L lane vectors V = {vl}Ll=1 with vl \u2208 Ro containing o lane features. At first, we generate a translation invariant scene representation. Therefore, the scene elements including the past state vectors and the lane segments from the HD-map are transformed into a vector representation and augmented with relative position vectors between elements to preserve distance information. Then, for each agent i the encoder extracts the spatio-temporal features hi = fe(xn\u2208Ni ,vm\u2208Ni) from the observed state sequence of the agents xn\u2208Ni \u2286 X and the lanes vm\u2208Ni \u2286 V in the local neighborhood Ni, determined by a circle with radius r around the corresponding agent. The encoder outputs the set of all agent feature vectors {hi}Ni=1."
        },
        {
            "heading": "C. Multi-modal Trajectory Prediction Decoder",
            "text": "To cover multiple trajectory modes, we make the assumption that the target distribution p(yi|{hi}Ni=1) follows a mixture density distribution [29]. Each density component k \u2208 {1, ...,K} represents one of K possible future trajectories and is described by a sequence of independent bi-variate Gaussian distributions, with the joint distribution defined as\u220fTf\nt=1 N (\u00b5ti,k,\u03a3 t i,k) over the Tf future time steps with the center location \u00b5ti,k \u2208 R2 and the covariance \u03a3 t i,k \u2208 R2\u00d72 in time step t. The density components are combined as a weighted-sum using the mixing coefficients \u03c0i,k from the categorical distribution q(k) = \u03c0i,k, with \u2211K k=1 \u03c0i,k = 1. We summarize the set of covariances, center locations and mixing coefficients for agent i with \u03a3i, \u00b5i and \u03c0i. The resulting density over the future trajectory is defined as\np(yi|\u03c0i,\u00b5i,\u03a3i) = K\u2211\nk=1\n\u03c0i,k Tf\u220f t=1 N (\u00b5ti,k,\u03a3 t i,k). (1)\nWe assume the diagonal covariance \u03a3ti,k = (\u03c3 t i,k) 2I, with the identity matrix I \u2208 R2\u00d72 and standard deviation \u03c3ti,k; and denote y\u0302i,k = {\u00b5ti,k} Tf t=1 the mean trajectory of mode k and agent i. The distribution parameters {\u03a3i,\u00b5i,\u03c0i}Ni=1 = gp({hi}Ni=1) for all agents are predicted by the trajectory prediction decoder gp given the encoding features {hi}Ni=1. The decoder is composed of the global message passing network and the aggregation network as proposed by [4], and a separate multi-layer perceptron (MLP) for each distribution parameter. For the mixing coefficients, we predict the unnormalized coefficients \u03c0\u0303i,k first, and use the softmax function to convert them to probabilities \u03c0i,k [29]. We denote the resulting trajectory prediction model HiVT\u2217."
        },
        {
            "heading": "D. Out-of-Distribution Detection",
            "text": "Since OOD scenarios are rare and in most cases inaccessible during training, we describe OOD detection as one-class\nclassification problem, i.e. only ID samples are available during training. We propose an OOD detector on the latent representation space of the scene encoder to discriminate OOD from ID samples. To this end, we estimate the parameters of a parametric probability distribution representing the ID from the training feature vectors and identify the OOD samples in regions with low density during testing. Our OOD detection module good is based on the assumption that the probability distribution of the latent features p(hi) follows a mixture of multivariate Gaussian distributions [15], [16]. Therefore we define the Gaussian mixture model q(hi) =\u2211C\ni=1 \u03d5cN (hi|\u00b5c,\u03a3c), where \u03d5c is the mixing coefficient, \u00b5c is the mean and \u03a3c is the covariance matrix of mixture component c = {1, ..., C}. We denote the OOD detector as latent GMM (lGMM) throughout the experiments. During inference, the lGMM outputs the OOD score\n\u03b1\u0302i = \u2212 log q(hi) = \u2212 log ( C\u2211 i=1 \u03d5cN (hi|\u00b5c,\u03a3c) ) (2)\nas the negative log-likelihood under the Gaussian mixture distribution q(hi), where an ID scenario has a low and an OOD scenario has a high negative log-likelihood."
        },
        {
            "heading": "E. Uncertainty Estimation",
            "text": "To accurately detect prediction errors in ID scenarios, we introduce an uncertainty estimation network, denoted as gu, which is a small MLP applied to the scene encoder. We formulate the problem of uncertainty estimation as a regression task, similar to [11], and train the neural network to predict the true trajectory prediction error ei for the agent i, given the encoder feature vector hi. During inference, the error regression network, we denote as Ereg throughout the experiments, outputs the uncertainty as\ne\u0302i = gu(hi). (3)"
        },
        {
            "heading": "F. Two-phase Model Training",
            "text": "Our training process is divided into two phases. First, we learn the parameters of the scene encoder fe and the trajectory prediction decoder gp by optimizing the prediction loss Lp. The prediction loss Lp,i for the agent i is defined as the negative log-likelihood on the mixture of Gaussian distributions\nLp,i =\u2212 log  K\u2211 k=1 \u03c0i,k Tf\u220f t=1 N ( yti |y\u0302ti,k,\u03a3 t i,k ) (4) with the locations of the ground truth trajectory yti , the predicted locations of the k mixture modes y\u0302ti,k = \u00b5 t i,k and the corresponding covariance matrices \u03a3ti,k. Once the trajectory prediction model is trained, the OOD detection and uncertainty estimation modules are optimized. During this stage, in order to avoid impacting the trajectory prediction, both the weights of the scene encoder and the trajectory prediction decoder are fixed. The parameters {\u03d5c,\u00b5c,\u03a3c}Cc=1 of the lGMM are estimated using the EMAlgorithm [30]. The Ereg is trained in a supervised manner\nwith the error regression loss Lu,i defined as the meansquared-error\nLu,i = \u2225(ei \u2212 e\u0302i)\u22252 (5)\nbetween the estimated error e\u0302i and the true prediction error ei. The prediction error ei can be set to any error measurement E(\u00b7, \u00b7), for example the prediction loss ei = Lp,i or one of the evaluation metrics as explained in the experiments Sec. IV-A. Since the training is decoupled from the training of the trajectory prediction method, our reliability modules can be considered as post-hoc methods. They can be easily added to an existing trajectory prediction model without the need for expensive re-training."
        },
        {
            "heading": "IV. EXPERIMENTS",
            "text": "We first describe the experimental setup, consisting of the Shifts dataset [1], the evaluation protocol and our implementation. Secondly, our approach is compared with the baselines for trajectory prediction, OOD detection and uncertainty estimation. Finally, we visualize qualitative results and analyze the runtime."
        },
        {
            "heading": "A. Experimental Setup",
            "text": "Dataset. We evaluate our method on the Shifts dataset [1]. Shifts is unique because it is the only dataset available for OOD detection and uncertainty estimation in trajectory prediction. It consists of an ID training set (train) with 388 406 sequences recorded during drives in Moscow without precipitation. In contrast to training, 9569 of the 36 605 validation sequences (dev) and 9939 of the 36 804 test sequences (eval) are affected by distribution shifts, denoted as OOD samples. Therefore, we can form three dev and three eval sets denoted as ID, OOD and Full. The OOD samples are recorded in adverse weather conditions like rain or snow and cities like Ann-Arbor or Tel Aviv. Each scene contains the states of all dynamic agents, including pedestrians and vehicles, in a 2D bird\u2019s-eye-view coordinate system recorded at 5Hz. Pedestrians and vehicles are described by the position vector and the velocity vector in each time step, vehicles with additional acceleration and yaw angle. HD-maps are provided for all locations. The 10 second recordings are divided into 5 seconds observation and 5 seconds prediction horizon.\nEvaluation Protocol. Our evaluation is three-fold. First, we evaluate trajectory prediction and then OOD detection and uncertainty estimation. The agent index i is discarded in the definition of the metrics.\nTo measure the quality of the predicted trajectories, we use the minimum average displacement error (minADE) and the minimum final displacement error (minFDE) [4]. In addition, we consider the more practical weighted ADE, denoted as wADE(y, y\u0302) = \u2211K k=1 \u03c0k \u00b7 ADE(y, y\u0302k) [1], where we use the mixture coefficients \u03c0k to average the ADE over the K modes, which we denote as our main metric. Analogously, we compute the wFDE. While highly intuitive, the metrics have two limitations: They do not include the predicted distribution and suffer from modecollapse as derived by Malinin et al. [1]. We address both\nlimitations by providing the negative log-likelihood score as introduced in [31]. The NLL evaluates the likelihood of the ground truth trajectory under the predicted Gaussian mixture distribution. All metrics are calculated for the ID, the OOD and the Full eval set.\nWe use the standard metric to evaluate the OOD detection [32], namely the area under the receiver operating characteristic curve (AUROC). The ROC curve plots the true positive rate against the false positive rate at various thresholds of the predicted OOD score. The area under the ROC curve evaluates if the OOD scores can detect the OOD samples. An optimal classifier achieves 100% AUROC, a random classifier 50% AUROC. The uncertainty estimation is evaluated by computing the area under the retention curve (R-AUC) [1]. Retention curves measure the agreement between the uncertainty e\u0302 and the prediction error e, which can be computed by any error metric, e.g. e = wADE. First, the list of all prediction errors e on the eval set is sorted with descending uncertainty e\u0302. Then iteratively, the subset of samples with the highest uncertainty is discarded and the error e is averaged over the remaining samples. If the uncertainty properly represents the error, the average error is supposed to shrink with the decreasing retention fraction. The optimal retention curve is obtained by sorting the samples in descending order of the true prediction error e, subsequently denoted as Oracle. We use the wADE as the error metric, i.e. e = wADE, to compute the wADE R-AUC.\nImplementation Details. The agent states sti \u2208 R7 are filled with the vectorized position, the velocity and the acceleration in x- and y-coordinates, as well as a binary flag indicating whether the agent is a vehicle or a pedestrian. We set the acceleration to zero for pedestrians. Each lane vector vl \u2208 R10 contains the x- and y-coordinate of a vectorized centerline segment plus a set of context features derived from the HD-map: The speed limit, the lane availability vector, which is derived from the traffic light state, and the lane priority. The size of the scene encoder features hi \u2208 R128 is set to 128 and the radius of the receptive field to r = 50 meters [4]. The trajectory prediction decoder uses a couple of MLPs to output the distribution parameters: A three-layer MLP for the mixing coefficients \u03c0\u0303i,k and two two-layer MLPs for the mean \u00b5i,k and the variance \u03c3 2 i,k, respectively. Our motion decoder predicts a set of K = 5 trajectory modes as defined in Shifts [1]. The uncertainty decoder Ereg is composed of a three-layer MLP to estimate e\u0302i.\nIn the first phase of training, the prediction loss Lp is optimized for 64 epochs with initial learning rate 1 \u00d7 10\u22124 and batch size 48 on four NVIDIA Tesla V100 GPUs. In the second phase, the weights of the Ereg module are learned using the regression loss Lu for 100 epochs with learning rate 1\u00d710\u22123 and batch size 1024. We define wADE(yi, y\u0302i) as the error function E(\u00b7, \u00b7) to compute the regression target ei = log(wADE(yi, y\u0302i)) and scale it with the logarithm to reduce the output range. Both training phases use the AdamW optimizer [33] with a cosine annealing learning rate scheduler [34]. Simultaneously, the parameters of the lGMM are fit using the EM-Algorithm for a maximum of 100 iterations, after initialization with the k-means algorithm. We choose K = 6 mixture components from K \u2208 {1, 2, 3, 4, 6, 8, 12, 16} based on the highest AUROC on the dev set. The hyperparameter search is shown in Fig. 3."
        },
        {
            "heading": "B. Comparison with State-of-the-Art",
            "text": "The results are split into two parts: The evaluation of trajectory prediction, see Table I, and the evaluation of OOD detection and uncertainty estimation, see Table II. In the tables the highest scores are denoted in bold and the second highest scores are underlined. The symbol \u201d-\u201d means, that the results have not been reported. The arrows, \u2193 and \u2191, indicate the direction of better performance.\nMulti-modal Trajectory Prediction. We compare our trajectory prediction method HiVT\u2217 with six state-of-the-art models on the Shifts eval set in Table I. BC and DIM use rasterized scene inputs and output a set of trajectories by sampling from a learned uni-modal Gaussian distribution. In addition to the single BC and DIM models with K = 1, we show the results of the ensembles with K = 5 [1]. In particular, the single model variants with K = 1 perform poorly throughout all metrics. Averaging the results over the bootstrapped ensemble helps to boost performance, especially on the wADE and wFDE. However, ensembles involve multiple forward passes and therefore have limited applicability in real-time systems like automated vehicles. VNT [10] combines a graph-based encoder on a vectorized scene with a transformer-based decoder and ViT [11] a vision transformerbased encoder on a rasterized scene with an MLP decoder. Both models predict a Gaussian mixture distribution over future trajectories, but only predict a deterministic mean without a covariance, which is set to the identity matrix \u03a3 = I during training. In contrast, we follow a probabilistic approach and predict the standard deviation in addition to the mean locations with the covariance matrix defined as \u03a3ti,k = (\u03c3 t i,k)\n2I. This is advantageous on the NLL metric, which particularly evaluates the probabilistic multi-modal predictions. Overall our method out-performs the baselines in terms of minADE, minFDE and NLL and shows first and second best results on wADE and wFDE. Notice, all models show a performance drop on the OOD set in comparison to the ID set.\nOOD Detection and Uncertainty Estimation. Table II shows the results of our approach, the combination of lGMM and Ereg, in comparison with the prior work on OOD"
        },
        {
            "heading": "TO PREDICT THE OOD SCORE AND THE UNCERTAINTY, RESPECTIVELY.",
            "text": "detection and uncertainty estimation, respectively. The results of VNT and ViT are collected from the Shifts leaderboard [1] on 07/31/2023. In contrast to our approach, where we address both tasks with different methods, the prior works use the predicted uncertainty e\u0302 as OOD score \u03b1\u0302, simultaneously. The uncertainty of the BC and DIM backbones [1] is computed by model averaging (MA), i.e. averaging the confidences of all trajectories predicted by the single model for K = 1 or the ensemble for K = 5. The ensembles, K = 5, reach lower wADE R-AUC compared to the single models, K = 1, while the single models are slightly better on AUROC. For the VNT, the uncertainty is estimated by the predicted variance of a spectral-normalized Gaussian process (SNGP), which is applied to the encoder feature space [10]. ViT uses error regression Ereg [11] to estimate the uncertainty. For a fair comparison and consistent results, we adopt the prior approaches to our trajectory prediction model. We do not consider ensembles due to their limited application in realtime systems, i.e. K = 1 in all experiments.\nIn addition to MA [1], SNGP [10] and Ereg [11], we present another simple yet effective baseline. To this end, we compute the uncertainty as the negative log-likelihood under the output Gaussian mixture distribution p(yi|\u03c0i,\u00b5i,\u03a3i) and denote the baseline NLL. The idea is, that the model is certain about its prediction for small standard deviations and uncertain otherwise. NLL performs well on uncertainty estimation, but falls behind Ereg. We compare the retention curves in Fig. 4. From the results it becomes clear, that it is\nnot sufficient to use the same method for OOD detection and uncertainty estimation, since none of the prior approaches performs well on both tasks, simultaneously. Therefore, we propose to learn both tasks jointly with two expert models, namely lGMM for OOD detection and Ereg for uncertainty estimation. On the one hand, lGMM is best for OOD detection, because it learns a probability density function over the ID scenarios during training, while on the other hand, Ereg outperforms the baselines for uncertainty estimation, due to the supervised training on the wADE error. Our lGMM significantly outperforms all prior approaches by at least 2.8%, including the ensemble models [1], on OOD detection."
        },
        {
            "heading": "C. Results on the Shifts Motion Prediction Challenge",
            "text": "In the following, we provide the results on the two metrics defined for the Shifts motion prediction challenge [1]. This is for trajectory prediction the corrected NLL defined as cNLL = NLL \u2212 Tf log(2\u03c0) by subtracting Tf log(2\u03c0) from NLL to ensure the minimum value is zero [1] and for uncertainty estimation, the area under the retention curve of the cNLL denoted as cNLL R-AUC. Although used in the challenge, the cNLL is less expressive than the NLL, since the correction assumes the constant covariance \u03a3 = I, which is clearly not desirable for probabilistic motion prediction where the variance can take on any value. Nevertheless, we train a variant of our model with the variance fixed to one, denoted as HiVT\u2217 (\u03a3ti,k = I), and optimize both lGMM and Ereg on the new scene encoder. We compare the results with the prior approaches [1], [10], [11] in Table III. Our model outperforms the baselines in all metrics with an impressive improvement of 23.4% on cNLL over the second best ViT.\nUncertainty u\nOOD \u03b1\nUncertainty u OOD \u03b1\nUncertainty u OOD \u03b1 Uncertainty u\nOOD \u03b1"
        },
        {
            "heading": "D. Qualitative Results",
            "text": "Fig. 5 shows qualitative results of our method illustrating the trajectory prediction as well as the OOD score \u03b1\u0302 and the uncertainty e\u0302. From left to right, we show two OOD and two ID scenarios. In the OOD scenarios, the trajectory prediction is inaccurate resulting in large prediction errors, as indicated by the blue triangle. The scenarios fall in regions with a low likelihood of the probability density function of the lGMM, resulting in high OOD scores. In the ID scenarios, the model can predict a multi-modal future but misses the actual behavior of a lane change in the first scenario and a right turn prediction in the second scenario, which results in high prediction error. The large prediction error is successfully detected by the Ereg module, which predicts large uncertainty in both cases. In each situation, despite it being ID or OOD, both modules assess the reliability of the prediction, where lGMM detects the OOD scenarios and Ereg estimates the uncertainty."
        },
        {
            "heading": "E. Runtime and Learnable Model Parameters",
            "text": "Table IV lists the runtime and the number of learnable model parameters of the scene encoder fe, the trajectory prediction decoder gp, the OOD detection good and the uncertainty estimation gu. Assuming that the inferences are executed sequentially, the OOD detection and the uncertainty estimation cause a 0.14ms increase in runtime, which is a\nTABLE IV EVALUATION OF RUNTIME AND LEARNABLE MODEL PARAMETERS.\nfe gp good gu\nInference Time 2ms\u2217 0.8ms\u2217 0.05ms\u2020 0.09ms\u2217 Model Parameters 1 488 768 1 203 052 99 078 5712\n\u2217NVIDIA GeForce RTX 2080 Ti. \u2020Intel Core i9-10900X @ 3.7 GHz.\nrelative increase of only 5%. This low increase in runtime illustrates the benefit of using a joint feature space hi for all modules. Previous work mainly relied on ensembles of multiple models (e.g., K = 5) to achieve reasonable results for OOD detection and uncertainty estimation. Using 5 models instead of only 1 trajectory predictor is a relative runtime increase of 400%."
        },
        {
            "heading": "V. CONCLUSION",
            "text": "We presented a trajectory prediction model with joint OOD detection and uncertainty estimation. The model is composed of the latent Gaussian mixture model and the error regression network to assess the reliability of trajectory prediction in ID as well as in OOD scenarios using a shared scene encoder. We demonstrated the efficacy of the proposed approach with experimental results on the Shifts dataset. Our results show that generative modeling of the latent features improves the OOD detection. Additionally, the regression of the prediction error is a simple yet effective way to estimate the current prediction error. Unlike the prior work, like the ensemble-based methods, our approach can extend existing trajectory prediction models to assess the prediction reliability without retraining the prediction model and with low computational overhead. It remains to be investigated, how the OOD score and the uncertainty can be used by a downstream planner."
        },
        {
            "heading": "ACKNOWLEDGMENT",
            "text": "The research leading to these results is funded by the German Federal Ministry for Economic Affairs and Energy\nwithin the project \u201cKI Delta Learning\u201d (Fo\u0308rderkennzeichen 19A19013A). The authors would like to thank the consortium for the successful cooperation."
        }
    ],
    "title": "Joint Out-of-Distribution Detection and Uncertainty Estimation for Trajectory Prediction",
    "year": 2023
}