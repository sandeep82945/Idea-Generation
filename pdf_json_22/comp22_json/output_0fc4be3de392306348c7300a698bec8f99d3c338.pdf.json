{
    "abstractText": "3D audio-visual production aims to deliver immersive and interactive experiences to the consumer. Yet, faithfully reproducing real-world 3D scenes remains a challenging task. This is partly due to the lack of available datasets enabling audio-visual research in this direction. In most of the existing multi-view datasets, the accompanying audio is neglected. Similarly, datasets for spatial audio research primarily offer unimodal content, and when visual data is included, the quality is far from meeting the standard production needs. We present \u201cTragic Talkers\u201d, an audio-visual dataset consisting of excerpts from the \u201cRomeo and Juliet\u201d drama captured with microphone arrays and multiple co-located cameras for light-field video. Tragic Talkers provides ideal content for object-based media (OBM) production. It is designed to cover various conventional talking scenarios, such as monologues, two-people conversations, and interactions with considerable movement and occlusion, yielding 30 sequences captured from a total of 22 different points of view and two 16-element microphone arrays. Additionally, we provide voice activity labels, 2D face bounding boxes for each camera view, 2D pose detection keypoints, 3D tracking data of the mouth of the actors, and dialogue transcriptions. We believe the community will benefit from this dataset as it can assist multidisciplinary research. Possible uses of the dataset are discussed. \u2217This is the author\u2019s version of the work. It is posted here for your personal use. This paper is published under a Creative Commons Attribution (CC-BY) license. The definitive version was published in CVMP \u201922, https://doi.org/10.1145/3565516.3565522. CCS CONCEPTS \u2022 Computing methodologies \u2192 Virtual reality; Scene understanding; Vision for robotics; 3D imaging; Tracking; \u2022 Hardware \u2192 Emerging tools and methodologies.",
    "authors": [
        {
            "affiliations": [],
            "name": "Davide Berghi"
        },
        {
            "affiliations": [],
            "name": "Marco Volino"
        },
        {
            "affiliations": [],
            "name": "Philip J. B. Jackson"
        }
    ],
    "id": "SP:01763320501ab932eabbca8ea8126ee8fb676a32",
    "references": [
        {
            "authors": [
                "Triantafyllos Afouras",
                "Yuki M. Asano",
                "Francois Fagan",
                "Andrea Vedaldi",
                "Florian Metze."
            ],
            "title": "Self-Supervised Object Detection From Audio-Visual Correspondence",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 10575\u201310586.",
            "year": 2022
        },
        {
            "authors": [
                "Xavier Alameda-Pineda",
                "Jordi Sanchez-Riera",
                "Johannes Wienke",
                "Vojtech Franc",
                "Jan Cech",
                "Kaustubh Kulkarni",
                "Antoine Deleforge",
                "Radu P. Horaud."
            ],
            "title": "RAVEL: An Annotated Corpus for Training Robots with Audiovisual Abilities",
            "venue": "Journal on Multimodal User Interfaces 7, 1-2 (2013), 79\u201391. http://hal.inria.fr/hal-00720734/en",
            "year": 2013
        },
        {
            "authors": [
                "R. Arandjelovic",
                "A. Zisserman."
            ],
            "title": "Look, Listen and Learn",
            "venue": "IEEE/CVF Inter. Conf. on Computer Vision. 609\u2013617.",
            "year": 2017
        },
        {
            "authors": [
                "Elise Arnaud",
                "Heidi Christensen",
                "Yan-Chen Lu",
                "Jon Barker",
                "Vasil Khalidov",
                "Miles Hansard",
                "Bertrand Holveck",
                "Herv\u00e9 Mathieu",
                "Ramya Narasimha",
                "Elise Taillant",
                "Florence Forbes",
                "Radu Horaud."
            ],
            "title": "The CAVA Corpus: Synchronised Stereoscopic and Binaural Datasets with Head Movements",
            "venue": "Proceedings of the 10th International Conference on Multimodal Interfaces (Chania, Crete, Greece)",
            "year": 2008
        },
        {
            "authors": [
                "Davide Berghi",
                "Adrian Hilton",
                "Philip J.B. Jackson."
            ],
            "title": "Visually Supervised Speaker Detection and Localization via Microphone Array",
            "venue": "IEEE 23rd Inter. Workshop on Multimedia Signal Processing.",
            "year": 2021
        },
        {
            "authors": [
                "Davide Berghi",
                "Hanne Stenzel",
                "Marco Volino",
                "Adrian Hilton",
                "Philip J.B. Jackson."
            ],
            "title": "Audio-Visual Spatial Alignment Requirements of Central and Peripheral Object Events",
            "venue": "2020 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW). 666\u2013667. https://doi.org/10.1109/VRW50115.2020. 00184",
            "year": 2020
        },
        {
            "authors": [
                "Miguel Blanco Galindo",
                "Philip Coleman",
                "Philip J.B. Jackson."
            ],
            "title": "Microphone array geometries for horizontal spatial audio object capture with beamforming",
            "venue": "J. of the Audio Engineering Society 68, 5 (2020), 324\u2013337. https://doi.org/10.17743/ jaes.2020.0025",
            "year": 2020
        },
        {
            "authors": [
                "M. Brandstein",
                "D. Ward."
            ],
            "title": "Microphone Arrays: Signal Processing Techniques and Applications",
            "venue": "Springer. https://books.google.co.uk/books?id=nND6ObXSNoEC",
            "year": 2001
        },
        {
            "authors": [
                "Akin Caliskan",
                "Armin Mustafa",
                "Evren Imre",
                "Adrian Hilton."
            ],
            "title": "Multi-view Consistency Loss for Improved Single-Image 3D Reconstruction of Clothed People",
            "venue": "Asian Conference on Computer Vision. Springer International Publishing, Cham, 71\u201388.",
            "year": 2020
        },
        {
            "authors": [
                "Z. Cao",
                "G. Hidalgo Martinez",
                "T. Simon",
                "S. Wei",
                "Y.A. Sheikh."
            ],
            "title": "OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (2019).",
            "year": 2019
        },
        {
            "authors": [
                "Trevor J. Cox",
                "Adrian Hilton"
            ],
            "title": "An Audio-Visual System for Object-Based",
            "year": 2018
        },
        {
            "authors": [
                "Harvey F. Silverman",
                "Ying Yu"
            ],
            "title": "A Real-Time SRP-PHAT Source",
            "year": 2007
        },
        {
            "authors": [
                "Weipeng He",
                "Petr Motlicek",
                "Jean-Marc Odobez"
            ],
            "title": "Deep Neural Networks",
            "year": 2018
        },
        {
            "authors": [
                "Han Xiaoguang"
            ],
            "title": "Deep Fashion3D: A Dataset and Benchmark for 3D",
            "year": 2020
        },
        {
            "authors": [
                "Shohei Nobuhara",
                "Yaser Sheikh"
            ],
            "title": "Panoptic Studio: A Massively Multiview",
            "year": 2015
        },
        {
            "authors": [
                "Mar\u00edn-Jim\u00e9nez",
                "Rafael Mu\u00f1oz Salinas"
            ],
            "title": "The AVA Multi-View Dataset",
            "year": 2014
        },
        {
            "authors": [
                "Michael J. Black"
            ],
            "title": "Learning to Dress 3D People in Generative Clothing",
            "year": 2020
        },
        {
            "authors": [
                "vastava",
                "Tuomas Virtanen"
            ],
            "title": "A Dataset of Dynamic Reverberant Sound",
            "year": 2021
        },
        {
            "authors": [
                "Noguer."
            ],
            "title": "3DPeople: Modeling the Geometry of Dressed Humans",
            "venue": "Interna-",
            "year": 2019
        },
        {
            "authors": [
                "2576\u20132588. Iran R Roman",
                "Juan Pablo Bello"
            ],
            "title": "Micarraylib: Software for Reproducible",
            "year": 2021
        },
        {
            "authors": [
                "Florian Schweiger",
                "Chris Pike",
                "Tom Nixon",
                "Matt Firth",
                "Bruce Weir",
                "Paul Golds",
                "Marco Volino",
                "Craig Cieciura",
                "Mohd Izhar",
                "Nick Graham-Rack",
                "Philip J.B. Jackson",
                "Alex Ang."
            ],
            "title": "Tools for 6-DoF Immersive Audio-visual Content Capture and Production",
            "venue": "IBC. https://www.bbc.co.uk/rd/publications/tools-six-depth-offield-immersive-audiovisual-content-capture-production",
            "year": 2022
        },
        {
            "authors": [
                "Hanne Stenzel",
                "Davide Berghi",
                "Marco Volino",
                "Philip J.B. Jackson."
            ],
            "title": "Naturalistic audio-visual volumetric sequences dataset of sounding actions for six degree-offreedom interaction",
            "venue": "2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW). 637\u2013638. https://doi.org/10.1109/VRW52623.2021. 00201",
            "year": 2021
        },
        {
            "authors": [
                "Hanne Stenzel",
                "Philip J.B. Jackson."
            ],
            "title": "Perceptual thresholds of audio-visual spatial coherence for a variety of audio-visual objects",
            "venue": "Conf. on Audio for Virtual and Augmented Reality.",
            "year": 2018
        },
        {
            "authors": [
                "T. Strybel",
                "K. Fujimoto."
            ],
            "title": "Minimum audible angles in the horizontal and vertical planes: Effects of stimulus onset asynchrony and burst duration",
            "venue": "The J. of the Acoustical Society of America 108 6 (2000), 3092\u20135.",
            "year": 2000
        },
        {
            "authors": [
                "Garvita Tiwari",
                "Bharat Lal Bhatnagar",
                "Tony Tung",
                "Gerard Pons-Moll."
            ],
            "title": "SIZER: A Dataset and Model for Parsing 3D Clothing and Learning Size Sensitive 3D Clothing",
            "venue": "European Conference on Computer Vision (ECCV). Springer.",
            "year": 2020
        },
        {
            "authors": [
                "Matt Trumble",
                "Andrew Gilbert",
                "Charles Malleson",
                "Adrian Hilton",
                "John Collomosse."
            ],
            "title": "Total Capture: 3D Human Pose Estimation Fusing Video and Inertial Sensors",
            "venue": "2017 British Machine Vision Conference (BMVC).",
            "year": 2017
        },
        {
            "authors": [
                "Arun Balajee Vasudevan",
                "Dengxin Dai",
                "Luc Van Gool."
            ],
            "title": "Semantic Object Prediction and Spatial Sound Super-Resolution with Binaural Sounds",
            "venue": "The European Conf. on Computer Vision. 638\u2013655. https://doi.org/10.1007/978-3-030-58548-8_37",
            "year": 2020
        },
        {
            "authors": [
                "Shifeng Zhang",
                "Xiangyu Zhu",
                "Zhen Lei",
                "Hailin Shi",
                "Xiaobo Wang",
                "S. Li."
            ],
            "title": "S3FD: Single Shot Scale-Invariant Face Detector",
            "venue": "2017 IEEE International Conference on Computer Vision (ICCV) (2017), 192\u2013201.",
            "year": 2017
        },
        {
            "authors": [
                "Zhengyou Zhang."
            ],
            "title": "Flexible camera calibration by viewing a plane from unknown orientations",
            "venue": "Proceedings of the Seventh IEEE International Conference on Computer Vision, Vol. 1. 666\u2013673 vol.1. https://doi.org/10.1109/ICCV.1999.791289",
            "year": 1999
        }
    ],
    "sections": [
        {
            "text": "\u2217This is the author\u2019s version of the work. It is posted here for your personal use. This paper is published under a Creative Commons Attribution (CC-BY) license. The definitive version was published in CVMP \u201922, https://doi.org/10.1145/3565516.3565522.\nCCS CONCEPTS \u2022 Computing methodologies \u2192 Virtual reality; Scene understanding; Vision for robotics; 3D imaging; Tracking; \u2022 Hardware \u2192 Emerging tools and methodologies.\nKEYWORDS dataset, multi-view, microphone array, active speaker detection, object-based media"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "In 3D immersive production, there is little room for spatial misalignment between the audio and the visual displays. To provide realistic immersive experiences, accurate positioning of the audio sources with respect to the visual events that trigger them is therefore essential [Berghi et al. 2020; Stenzel and Jackson 2018]. With synthetic audio-visual assets, this is typically achieved through a manual process performed by the producer. The objects are located at the desired positions in the virtual environment enabling six degree-of-freedom (6-DoF) interaction of the end-user. However, to achieve the same result and faithfully reproduce real-world scenes, an object-based audio-visual representation is required [Coleman et al. 2018; Pike et al. 2016]. That is, object events must be extracted from the audio and visual streams and partnered with metadata describing specific attributes of the objects themselves, such as their content or positions in space. An approach to enable real-world object-based media (OBM) production for 6-DoF interaction is by extracting the visual foreground objects and three-dimensionally reconstructing them. This is possible through a multi-view data capture [Hartley and Zisserman 2004]. Meanwhile, the acoustic ar X iv :2\n21 2.\n01 89\n2v 1\n[ ee\n2D pose keypoints\ud835\udc4e , transc. \ud835\udc4e Only a subset. \ud835\udc4f SSLR is mostly an audio-only dataset. We refer to the subset named \u2018Human\u2019 that includes video recordings of human speakers. \ud835\udc50 In addition to two 16-element microphone arrays, audio is recorded with two lavalier microphones and a 4-channel FOA mic too.\nsound-field of the audio events must be captured and tracked over time. This is possible through multi-channel audio, for example, recorded with a microphone array [Brandstein and Ward 2001].\nProducing such audio-visual content can be extremely costly as it requires expensive equipment and studio time. To the best of our knowledge, there are no available datasets with these characteristics. Existing datasets with multi-view video content usually neglect the audio component [Ma et al. 2020; Pumarola et al. 2019; Trumble et al. 2017]. On the other hand, datasets with multi-channel audio lack video content [Politis et al. 2021] or, when provided, the quality is rather low [He et al. 2018; Lathoud et al. 2004]. Therefore, we propose Tragic Talkers: a sound- and light-fields dataset captured with the aid of two co-located audio-visual sensing devices. Colocating audio and visual sensors is particularly useful to maintain the same fixed coordinate frame across the two modalities. The dataset consists of drama scenes, therefore, it is suitable for OBM production applications or OBM research. The actors were also provided with dedicated close microphones to record high-quality voice signals useful for reference and association when beamforming algorithms are applied. Furthermore, we offer a range of curated labels to enable research in multiple domains, e.g., active speaker detection, speaker diarization, or body pose detection. A preliminary version of the Tragic Talkers dataset was used in our work [Berghi et al. 2021]. However, the previous version was smaller in size (20 sequences instead of 30) and was only labeled for 2D bounding boxes and voice activity.\nThe rest of this paper is organized as follows: In chapter 2 existing datasets for 3D multimedia content are presented and their limitations discussed; In chapter 3 the recording set-up employed to capture Tragic Talkers is described; In chapter 4 we motivate the design of the dataset and present the provided labels; In chapter\n5 few key uses for the dataset are presented; Finally, in chapter 6 we conclude the paper. Tragic Talkers is available for download at https://cvssp.org/data/TragicTalkers/. The website provides readyto-use PyTorch python scripts for data loading and audio-visual feature extraction."
        },
        {
            "heading": "2 BACKGROUND",
            "text": "Datasets for 3D audio-visual content production and research are important for a number of reasons. First of all, they represent a useful benchmark for evaluating new algorithms. Secondly, producing new multi-view and multi-channel audio-visual content can be extremely costly and time-consuming. Additionally, consistentlycaptured data can be employed in machine learning algorithms training. Existing datasets that offer 3D multimedia content are typically designed to serve a specific technical aspect. 3DPeople [Pumarola et al. 2019], CAPE [Ma et al. 2020], SIZER [Tiwari et al. 2020], and Deep Fashion3D [Heming et al. 2020] are examples of real and synthetic datasets for 3D clothing modeling. AVAMVG [L\u00f3pez-Fern\u00e1ndez et al. 2014] is a multi-view dataset for 3D gait recognition. TotalCapture [Trumble et al. 2017] provides multi-view video data for 3D pose estimation. However, none of the aforementioned datasets provide audio data. Volumetric datasets that also recorded the audio component are HUMAN4D [Chatzitofis et al. 2020], MHAD [Ofli et al. 2013], CMUPanoptic [Joo et al. 2015], and NAVVS [Stenzel et al. 2021], even though only MHAD and NAVVS released the audio recordings. Even in these rare exceptions, the audio sources are recorded with dedicated microphones and not with the intention of capturing the directional cues of the acoustic sound-field of the performance, as it would be possible with microphone arrays or binaural microphones. To the best of our"
        },
        {
            "heading": "2 (left).",
            "text": "knowledge, Tragic Talkers is the first dataset that simultaneously provides sound- and light-field video content.\nIn the literature, there exists a variety of datasets captured with microphone arrays for sound source localization and tracking. Some of them are summarized in [Roman and Bello 2021]. In these cases, however, they lack visual content. A good amount of multi-channel audio datasets that also include the visual modality are produced by the Inria group1 for audio-visual tracking research, including the AVASM dataset [Deleforge et al. 2015], AVDIAR [Gebru et al. 2018], RAVEL [Alameda-Pineda et al. 2013], and CAVA [Arnaud et al. 2008]. Qian et al. [Qian et al. 2019] propose a summary of these and other datasets. However, in most cases, the resolution of the video stream is very low. Additionally, they are primarily recordedwith the intent of testing speaker tracking algorithms. Therefore, the actors are asked to perform tasks such as counting, talking on top of each other, and in some cases improvising a conversation. In contrast, Tragic Talkers offers high-quality video content captured in a studio with the help of two student actors. The actors play famous lines from the tragedy of Romeo and Juliet and their costumes are chosen to resemble the clothing of the Italian Renaissance period. All these features representing the quality of performance and integrating stagecraft skill, method, and material, are typically referred to by the film industry with the term production value.\nSimilar to [Qian et al. 2019], in Table 1 we propose a summary of audio-visual datasets captured with microphone arrays for speechrelated research. We omit some of the obsolete ones and include a few that have been released recently. In the table, a column has been reserved for the employment of actors. Excluding Tragic Talkers, EasyCom [Donley et al. 2021] is the only dataset that proposes an acting performance as it presents groups of people talking at a shammed restaurant table ordering food from an actor restaurant server that collects the preferences and describes the dishes. AV16.3 [Lathoud et al. 2004] is the only dataset of the list that was not captured from a co-located platform but with three cameras located in different positions of the room and their resolution is fairly low. In the other datasets, the visual component is either captured with a single camera [Donley et al. 2021; He et al. 2018; Qian et al. 2019] or with a single stereo pair [Arnaud et al. 2008; Gebru et al. 2018]. Tragic Talkers is the only dataset that is captured with two separate sets of 11 high resolution cameras for light-field video production.\n1https://www.inria.fr/en\nAdditionally, Tragic Talkers is captured against a chroma-consistent background facilitating silhouette extraction. Excluding EasyCom [Donley et al. 2021], all the datasets are quite small and therefore unsuitable for learning-based research. Tragic Talkers can be either seen as a 30-sequences dataset captured from 22 different points of view or, considering the two rigs separately, as a 60-sequences dataset captured with a single rig. Viewed this way, it offers 3.8 hours of total annotated video data with audio that is semantically, temporally and spatially coherent. Therefore, we believe it is of sufficient scale as to enable a range of machine-learning research."
        },
        {
            "heading": "3 CAPTURING SET-UP",
            "text": "Tragic Talkers was captured with the aid of two twin Audio-Visual Array (AVA) Rigs. Each AVA Rig is a custom device consisting of a 16-element microphone array and 11 cameras fixed on a flat perspex sheet supported by a tripod. Therefore, the AVA Rig represents a co-located multi-sensing platform. That is, each sensor is in a fixed relative position with respect to the other sensors of the array and the same coordinate frame is shared across the modalities. The array has size 0.6m\u00d70.4m and is compact and lightweight enough to be mounted on a standard production tripod. The 11 cameras (Grasshopper3 USB32) capture synchronized 2448\u00d72048p video frames at 30 fps. They are positioned in three rows and are equally spaced from one another, with the exception of the central camera. The camera array has been designed to support 6-DoF content production for the limited volume of a seated user. Before the capture session, intrinsics and extrinsics of the camera arrays were calibrated using a chart [Hartley and Zisserman 2004; Zhang 1999]. Therefore, the two camera rigs share the same world reference coordinated with the origin being the central camera of AVA Rig 1. Audio is sampled at 48 kHz, 24 bits. The microphone array can be used for sound source localization. It has a horizontal aperture of 450mm and a vertical aperture of only 40mm. This results in higher resolution when localizing audio sources along the azimuth direction than elevation, which is consistent with human perception [Strybel and Fujimoto 2000]. The horizontal design of the microphone array is log-spaced to focus on broad frequency coverage from 500Hz to 8 kHz, to better support the horizontal speech band resolution. After the capture session, audio recordings have been channel-wisely multiplied by gain factors to set their\n2https://www.flir.com/products/grasshopper3-usb3/?model=GS3-U3-51S5C-C\nroot mean square (RMS) magnitude to a common value and achieve level calibration, as in [Vasudevan et al. 2020].\nDuring the data capture, the two AVA Rigs were placed side by side at a distance of about 2m from one another to provide alternative view points and emulate a plausible camera arrangement in a standard video production set, generating an angle of about 30\u00b0 to 50\u00b0 between the rigs and each subject. In addition to the acoustic sound-field captured by the microphone arrays, actors were accoutered with two lavalier microphones to record high-quality voice signals and a first-order ambisonic room microphone (Soundfield DSF13) useful for reverb estimation was placed above the capturing volume. A clapper-board was employed at the beginning and at the end of each take to enable manual alignment of the audio and visual streams. The actors were asked to start and conclude their performance with the T-pose. The entire dataset was captured against a blue background to facilitate silhouette extraction by chroma-keying and 3D reconstruction using camera calibration data. Background images of the empty studio captured with the AVA Rigs are provided."
        },
        {
            "heading": "4 DESIGN OF DATASET",
            "text": "The Tragic Talkers dataset is designed to suit a variety of research areas including, but not limited to, light-field and sound-field processing. The intention to generate high quality content for nextgeneration 3D audio-visual production led to the decision of employing semi-professional actors with predefined scripts and selected costumes. Therefore, we chose to capture excerpts from Shakespeare\u2019s Romeo and Juliet drama, hence the name \u2018Tragic Talkers\u2019, where student actors played the role of the two key characters. In the rest of this paper, we will refer to \u2018Juliet\u2019 as the actress who interpreted Juliet and, similarly, to \u2018Romeo\u2019 as the actor who interpreted Romeo. Tragic Talkers consists of 30 sequences ranging from about 13 to 43 seconds in length (21 seconds on average), yielding over 400K frames (3.8 hours) of video data considering the 22 separate views. In order to produce a diverse set of talking conditions useful to speech-related research, three different types of scenes were recorded: monologues with single-character oration performance to provide male-only and female-only speech content; conversations with the two actors standing side-by-side exchanging lines from the famous tragedy; and interactive scenes where the actors were asked to move within the capture volume while talking, occluding each other and improvising their conversation. The last provides more challenging content for applications such as 2D and 3D audio-visual tracking. Although the dataset does not include scenarios with multiple simultaneous voices, the nature of the recordings allows synthetic creation of audio-visual cocktail parties by combining speech sequences: the audio is mixed, while the respective actors\u2019 video extracted from the background using silhouettes is overlaid to create augmented multi-talker sequences."
        },
        {
            "heading": "4.1 Labels",
            "text": "Along with the audio-visual data, we provide an extensive set of labels useful to test a variety of algorithms. Here, we summarize the available labels and explain how they have been generated. First of all, we have manually produced the voice activity (VA) labels 3https://www.soundfield.com/#/products/dsf1\nTable 2: Speech activity over the dataset\u2019s 30 sequences.\nPartition Duration Active Silent Romeo Juliet Development 519s 271s 248s 116s 155s\nset 52% 48% 43% 57% Test 121s 64s 57s 31s 33s set 53% 47% 48% 52% Overall 640s 335s 305s 147s 188s 52% 48% 44% 56%\n47.6% 23.0% 29.4%52.4% Silence Romeo\nJuliet\n40.9%\n23.6%\n35.6% Monologues\nConversa\ufffdons\nInterac\ufffdons\nOverall Speech Activity Dialogue Distribution\nwith actor ID association. In order to achieve that, we employed the software Adobe Audition4 to manually find the onset and offset of each speech segment as, alongside the audio tracks, it also provides the visual reference of the spectrograms. Table 2 reports the distribution between speech and silence across the dataset, as well as the distribution of Romeo\u2019s speech and Juliet\u2019s speech. Secondly, we provide the sequence transcriptions with the respective speaker\n4https://www.adobe.com/products/audition.html\n0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 8.0 8.5 9.0\nTime (sec)\n9.5\nRomeo! My dear?\nAt what o\u2019clock to-morrow shall I send to thee?\nAt the hour of nine.\nI will not fail\n\u2018tis twenty years till then.\nVideo frames\nAudio spectrogram\nVA Romeo\nVA Juliet\nTranscripts\n0 1 2 3 4 5 6 7 8 9 -40\n-35\n-30\nM ic\nro ph\non e\nle ve\nl [ dB\n]\nRomeo spot Juliet spot Soundfield\n0 1 2 3 4 5 6 7 8 9 time [s]\n-180 -135 -90 -45\n0 45 90\n135 180\nAz im\nut h\n[d eg\n]\nFigure 5: Audio-visual content of an excerpt from the sequence \u2018interactive1_t2\u2019 depicted by video frames overlaid with detected-face bounding boxes (reported at 1 fps) and audio spectrogram. Alongside are voice activity (VA) labels for Romeo and Juliet, and the speech transcriptions over time. Themiddle axes below show normalized sound levels at spot microphones worn by Romeo and Juliet, which correspond with the voice activity labels above, and the soundfield microphone mounted in the room. The bottom axes illustrate the active speakers\u2019 azimuth angle in relation to the soundfield microphone, exploiting the directivity of the first-order-ambisonic audio signals.\nID. To generate them, we employed Otter.ai5 Then, we manually adjusted the incorrect transcriptions.\nTo enable speaker localization and tracking, 2D face bounding boxes for each camera view are provided. Face detection was performed on each video frame with S3FD [Zhang et al. 2017]. When the face detector failed, for instance when the actors are not facing the camera, a bounding box was drawn by a manual educated guess. In this way, the actors are continuously tracked in the 2D domain throughout the entire duration of the sequence. Face tracks were generated from the per-frame detections based on intersection over union (IoU) across adjacent frames. Then, the corners of the bounding boxes have been interpolated over time to provide smooth temporal coherence. In addition to that, we selected 5 sequences from the dataset to constitute a test set. In order to be heterogeneous and representative of the dataset, the test set includes two monologues, one from Romeo and one from Juliet,\n5https://otter.ai/\none conversation, and two interactive sequences. The test set represents a useful benchmark for learning-based algorithms and it was labeled for 3D mouth position. To generate the 3D mouth labels, we employed OpenPose [Cao et al. 2019] to detect 2D skeletal keypoints of the two actors on the 2D video frames. In order to provide spatial and temporal coherence to accurately track the 3D location of the mouth joint, the 2D detections have been sorted as described in [Malleson et al. 2020]. The sorted 2D detections are combined with confidences to remove the impact of potentially unreliable detections and estimate the 3D locations via camera triangulation. 2D skeleton keypoints detected with OpenPose are provided. Examples of 3D mouth detections over time from one of the interactive sequences are reported in Figure 4."
        },
        {
            "heading": "5 USES OF THE DATASET",
            "text": "Tragic Talkers can be employed for multiple applications. Here we propose some of the key use cases for this dataset. The availability\nof 2D and 3D tracking data clearly suggests the employment of this dataset in speaker localization and tracking applications. This can either be addressed with audio-only algorithms, for instance by applying beamforming [Blanco Galindo et al. 2020] or SRP-PHAT [Do et al. 2007], or with audio-visual solutions, as in [Qian et al. 2019]. Tragic Talkers enables object-based spatial audio production through the generation of tracking metadata and automatic association with the high-quality signals from the additional microphones provided. For instance, [Mohd Izhar et al. 2020] proposed to generate automatic 3D spatial metadata with an audio-visual tracker and utilize it to drive a spatial beamformer to steer the listening directivity of the microphone array towards the predicted positions of the audio sources. The beamformed audio signals are\nthen automatically associated to, and replaced by, the high-quality recording spatialized at the predicted positions.\nThe large amount of views with which the dataset was captured enables learning-based audio-visual research for 2D active speaker detection with multi-channel audio signals [Berghi et al. 2021; Donley et al. 2021]. In fact, over 3.8 hours of monocular video content can be generated from the 30 sequences provided, all accompanied by multi-channel audio recordings. For instance, in our earlier work [Berghi et al. 2021] where a preliminary version of the Tragic Talkers dataset was employed, we adopted a student-teacher learning pipeline. We proposed to generate pseudo-labels from the video sequences with a pre-trained active speaker detector (teacher) and used them to train a multi-channel audio network (student) to detect and regress the position of the speakers directly in the video frames. The availability of the dialogue transcriptions also allows the speaker diarization task to be performed [Gebru et al. 2018]. The light-field camera array on each of the two AVA Rigs allows 3D dynamic foreground character reconstruction for immersive production. For example, the central camera of the AVA Rigs can be employed as a reference view for an operator during the editing and compositing of the experience. The surrounding cameras can be used for automatic partial 3D reconstruction of Romeo or Juliet. The size of the aperture of the camera array enables 6-DoF movement in the limited volume of a seated user. The partially reconstructed foreground avatars can then be inserted into a virtual environment. An example of 3D reconstruction of Romeo and Juliet inserted in a background location of our choice is proposed in the lower image of Figure 6. Furthermore, the 3D avatars can be accompanied by their respective voices, spatialized as previously described, to produce an object-based audio-visual immersive experience, as suggested in [Schweiger et al. 2022].\nThe bi-modal multi-view content offered in Tragic Talkers is optimal to evaluate rendering algorithms too. In the context of audio-visual text-to-video synthesis (see, for instance, Synthesia6), Tragic Talkers can be employed as an evaluation set to test audiovisual speech rendering algorithms from different viewpoints. This could study different view point interpolation or replace one character with an avatar, and supports formal subjective experiments for perceptual quality evaluation. Another potential use of the data involves the construction of synthetic datasets, for instance, by replacing the background with synthetic locations [Caliskan et al. 2020], or combining the sequences to create different combinations of multi-talker scenarios, which can be useful as part of a data augmentation strategy. In addition to these tasks, we envisage a contribution to fundamental audio-visual machine learning may be explored through the use of these data."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "This paper introduces the Tragic Talkers dataset, the first soundfield and light-field dataset, produced with drama students performing excerpts from Shakespeare\u2019s famous tragedy, Romeo and Juliet. Tragic Talkers was captured with the aid of two twin AVA Rigs, each consisting of an 11-element light-field camera array and a 16-element microphone array. High-quality speech data recorded\n6https://www.synthesia.io/\nwith two lavalier spot microphones as well as a first-order ambisonics room microphone are also provided. The dataset is labeled for voice activity, 2D face bounding boxes, 3D mouth coordinates, and dialogue transcriptions. We discussed potential uses of the dataset, including 2D or 3D speaker tracking, object-based media production, and light-field 3D reconstruction for 6-DoF interaction.\nIn future work, we aim to use Tragic Talkers to investigate the effect of different temporal, spatial and semantic aspects of audiovisual coherence [Afouras et al. 2022; Arandjelovic and Zisserman 2017]."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This work is supported by InnovateUK (105168) \u2018Polymersive: Immersive video production tools for studio and live events\u2019 and a PhD studentship from the Doctoral College of the University of Surrey. For the purpose of open access, the authors have applied a Creative Commons Attribution (CC BY) license to any Author Accepted Manuscript version arising. Data supporting this study are available from our web server (DOI 10.15126/surreydata.900446, https://cvssp.org/data/TragicTalkers). Thanks to actors Phoebe Salem and Mason Stickland for their time and availability; to Mohd Azri Mohd Izhar, Hansung Kim, and Charles Malleson for their contribution to the audio-visual recordings; to Umar Marikkar for developing utility scripts supporting easy access to the data, e.g., data loaders and audio-visual feature extractors; to Alexander Todd for helping during the laser cut of the AVA Rigs."
        }
    ],
    "title": "Tragic Talkers: A Shakespearean Sound- and Light-Field Dataset for Audio-Visual Machine Learning Research",
    "year": 2022
}