{
    "abstractText": "Reciprocity, or the tendency of individuals to mirror behavior, is a key measure that describes information exchange in a social network. Users in social networks tend to engage in different levels of reciprocal behavior. Differences in such behavior may indicate the existence of communities that reciprocate links at varying rates. In this paper, we develop methodology to model the diverse reciprocal behavior in growing social networks. In particular, we present a preferential attachment model with heterogeneous reciprocity that imitates the attraction users have for popular users, plus the heterogeneous nature by which they reciprocate links. We compare Bayesian and frequentist model fitting techniques for large networks, as well as computationally efficient variational alternatives. Cases where the number of communities are known and unknown are both considered. We apply the presented methods to the analysis of a Facebook wallpost network where users have nonuniform reciprocal behavior patterns. The fitted model captures the heavy-tailed nature of the empirical degree distributions in the Facebook data and identifies multiple groups of users that differ in their tendency to reply to and receive responses to wallposts.",
    "authors": [
        {
            "affiliations": [],
            "name": "Daniel Cirkovic"
        },
        {
            "affiliations": [],
            "name": "Tiandong Wang"
        }
    ],
    "id": "SP:61bf53307aa28cfe69817c6f83e18b63e2d94a99",
    "references": [
        {
            "authors": [
                "Sayan Banerjee",
                "Shankar Bhamidi",
                "Iain Carmichael"
            ],
            "title": "Fluctuation bounds for continuous time branching processes and nonparametric change point detection in growing networks",
            "venue": "arXiv preprint arXiv:1808.02439,",
            "year": 2018
        },
        {
            "authors": [
                "Albert-L\u00e1szl\u00f3 Barab\u00e1si",
                "R\u00e9ka Albert"
            ],
            "title": "Emergence of scaling in random networks. science",
            "year": 1999
        },
        {
            "authors": [
                "Jan Beirlant",
                "Yuri Goegebeur",
                "Johan Segers",
                "Jozef L Teugels"
            ],
            "title": "Statistics of extremes: theory and applications, volume 558",
            "year": 2004
        },
        {
            "authors": [
                "Shankar Bhamidi",
                "J Michael Steele",
                "Tauhid Zaman"
            ],
            "title": "Twitter event networks and the superstar model",
            "venue": "The Annals of Applied Probability,",
            "year": 2015
        },
        {
            "authors": [
                "Shankar Bhamidi",
                "Jimmy Jin",
                "Andrew Nobel"
            ],
            "title": "Change point detection in network models: Preferential attachment and long range dependence",
            "venue": "The Annals of Applied Probability,",
            "year": 2018
        },
        {
            "authors": [
                "Peter J Bickel",
                "Aiyou Chen"
            ],
            "title": "A nonparametric view of network models and newman\u2013 girvan and other modularities",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2009
        },
        {
            "authors": [
                "David M Blei",
                "Alp Kucukelbir",
                "Jon D McAuliffe"
            ],
            "title": "Variational inference: A review for statisticians",
            "venue": "Journal of the American statistical Association,",
            "year": 2017
        },
        {
            "authors": [
                "Diana Cai",
                "Trevor Campbell",
                "Tamara Broderick"
            ],
            "title": "Finite mixture models do not reliably learn the number of components",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Justin Cheng",
                "Daniel M Romero",
                "Brendan Meeder",
                "Jon Kleinberg"
            ],
            "title": "Predicting reciprocity in social networks",
            "venue": "IEEE Third International Conference on Privacy, Security, Risk and Trust and 2011 IEEE Third International Conference on Social Computing,",
            "year": 2011
        },
        {
            "authors": [
                "Daniel Cirkovic",
                "Tiandong Wang",
                "Sidney Resnick"
            ],
            "title": "Preferential attachment with reciprocity: Properties and estimation",
            "venue": "arXiv preprint arXiv:2201.03769,",
            "year": 2022
        },
        {
            "authors": [
                "Daniel Cirkovic",
                "Tiandong Wang",
                "Xianyang Zhang"
            ],
            "title": "Likelihood-based changepoint detection in preferential attachment networks",
            "venue": "arXiv preprint arXiv:2206.01076,",
            "year": 2022
        },
        {
            "authors": [
                "Aaron Clauset",
                "Cosma Rohilla Shalizi",
                "Mark EJ Newman"
            ],
            "title": "Power-law distributions in empirical data",
            "venue": "SIAM review,",
            "year": 2009
        },
        {
            "authors": [
                "Gabor Csardi",
                "Tamas Nepusz"
            ],
            "title": "The igraph software package for complex network research",
            "venue": "InterJournal, complex systems,",
            "year": 2006
        },
        {
            "authors": [
                "J-J Daudin",
                "Franck Picard",
                "St\u00e9phane Robin"
            ],
            "title": "A mixture model for random graphs",
            "venue": "Statistics and computing,",
            "year": 2008
        },
        {
            "authors": [
                "Arthur P Dempster",
                "Nan M Laird",
                "Donald B Rubin"
            ],
            "title": "Maximum likelihood from incomplete data via the em algorithm",
            "venue": "Journal of the Royal Statistical Society: Series B (Methodological),",
            "year": 1977
        },
        {
            "authors": [
                "Sophie Donnet",
                "St\u00e9phane Robin"
            ],
            "title": "Accelerating bayesian estimation for network poisson models using frequentist variational estimates",
            "venue": "Journal of the Royal Statistical Society: Series C (Applied Statistics),",
            "year": 2021
        },
        {
            "authors": [
                "Sylvia Fr\u00fchwirth-Schnatter",
                "Gertraud"
            ],
            "title": "Malsiner-Walli. From here to infinity: sparse finite versus dirichlet process mixtures in model-based clustering",
            "venue": "Advances in data analysis and classification,",
            "year": 2019
        },
        {
            "authors": [
                "Sylvia Fr\u00fchwirth-Schnatter",
                "Gertraud Malsiner-Walli",
                "Bettina Gr\u00fcn"
            ],
            "title": "Generalized mixtures of finite mixtures and telescoping sampling",
            "venue": "Bayesian Analysis,",
            "year": 2021
        },
        {
            "authors": [
                "Junxian Geng",
                "Anirban Bhattacharya",
                "Debdeep Pati"
            ],
            "title": "Probabilistic community detection with unknown number of communities",
            "venue": "Journal of the American Statistical Association,",
            "year": 2019
        },
        {
            "authors": [
                "Bruce Hajek",
                "Suryanarayana Sankagiri"
            ],
            "title": "Community recovery in a preferential attachment graph",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 2019
        },
        {
            "authors": [
                "Paul W Holland",
                "Kathryn Blackmond Laskey",
                "Samuel Leinhardt"
            ],
            "title": "Stochastic blockmodels: First steps",
            "venue": "Social networks,",
            "year": 1983
        },
        {
            "authors": [
                "Hawoong Jeong",
                "Zoltan N\u00e9da",
                "Albert-L\u00e1szl\u00f3 Barab\u00e1si"
            ],
            "title": "Measuring preferential attachment in evolving networks",
            "venue": "EPL (Europhysics Letters),",
            "year": 2003
        },
        {
            "authors": [
                "Bo Jiang",
                "Zhi-Li Zhang",
                "Don Towsley"
            ],
            "title": "Reciprocity in social networks with capacity constraints",
            "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
            "year": 2015
        },
        {
            "authors": [
                "Brian Karrer",
                "Mark EJ Newman"
            ],
            "title": "Stochastic blockmodels and community structure in networks",
            "venue": "Physical review E,",
            "year": 2011
        },
        {
            "authors": [
                "Pierre Latouche",
                "Etienne Birmele",
                "Christophe Ambroise"
            ],
            "title": "Variational bayesian inference and complexity control for stochastic block models",
            "venue": "Statistical Modelling,",
            "year": 2012
        },
        {
            "authors": [
                "Gertraud Malsiner-Walli",
                "Sylvia Fr\u00fchwirth-Schnatter",
                "Bettina Gr\u00fcn"
            ],
            "title": "Model-based clustering based on sparse finite gaussian mixtures",
            "venue": "Statistics and computing,",
            "year": 2016
        },
        {
            "authors": [
                "Catherine Matias",
                "Vincent Miele"
            ],
            "title": "Statistical clustering of temporal networks through a dynamic stochastic block model",
            "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
            "year": 2017
        },
        {
            "authors": [
                "Jeffrey W Miller",
                "David B Dunson"
            ],
            "title": "Robust bayesian inference via coarsening",
            "venue": "Journal of the American Statistical Association,",
            "year": 2018
        },
        {
            "authors": [
                "Jeffrey W Miller",
                "Matthew T Harrison"
            ],
            "title": "Mixture models with a prior on the number of components",
            "venue": "Journal of the American Statistical Association,",
            "year": 2018
        },
        {
            "authors": [
                "Alan Mislove",
                "Massimiliano Marcon",
                "Krishna P Gummadi",
                "Peter Druschel",
                "Bobby Bhattacharjee"
            ],
            "title": "Measurement and analysis of online social networks",
            "venue": "In Proceedings of the 7th ACM SIGCOMM conference on Internet measurement,",
            "year": 2007
        },
        {
            "authors": [
                "Mark EJ Newman"
            ],
            "title": "Clustering and preferential attachment in growing networks",
            "venue": "Physical review E,",
            "year": 2001
        },
        {
            "authors": [
                "Mark EJ Newman",
                "Stephanie Forrest",
                "Justin Balthrop"
            ],
            "title": "Email networks and the spread of computer viruses",
            "venue": "Physical Review E,",
            "year": 2002
        },
        {
            "authors": [
                "Krzysztof Nowicki",
                "Tom A B Snijders"
            ],
            "title": "Estimation and prediction for stochastic blockstructures",
            "venue": "Journal of the American statistical association,",
            "year": 2001
        },
        {
            "authors": [
                "Sidney I Resnick"
            ],
            "title": "Heavy-tail phenomena: probabilistic and statistical modeling",
            "venue": "Springer Science & Business Media,",
            "year": 2007
        },
        {
            "authors": [
                "Bimal Viswanath",
                "Alan Mislove",
                "Meeyoung Cha",
                "Krishna P Gummadi"
            ],
            "title": "On the evolution of user interaction in facebook",
            "venue": "In Proceedings of the 2nd ACM workshop on Online social networks,",
            "year": 2009
        },
        {
            "authors": [
                "Phyllis Wan",
                "Tiandong Wang",
                "Richard A Davis",
                "Sidney I Resnick"
            ],
            "title": "Fitting the linear preferential attachment model",
            "venue": "Electronic Journal of Statistics,",
            "year": 2017
        },
        {
            "authors": [
                "Tiandong Wang",
                "Sidney Resnick"
            ],
            "title": "Measuring reciprocity in a directed preferential attachment network",
            "venue": "Advances in Applied Probability,",
            "year": 2021
        },
        {
            "authors": [
                "Tiandong Wang",
                "Sidney Resnick"
            ],
            "title": "Random networks with heterogeneous reciprocity",
            "venue": "arXiv preprint arXiv:2208.00348,",
            "year": 2022
        },
        {
            "authors": [
                "Tiandong Wang",
                "Sidney I Resnick"
            ],
            "title": "A directed preferential attachment model with poisson measurement",
            "venue": "arXiv preprint arXiv:2008.07005,",
            "year": 2020
        },
        {
            "authors": [
                "Tiandong Wang",
                "Sidney I Resnick"
            ],
            "title": "Asymptotic dependence of in-and out-degrees in a preferential attachment model with reciprocity",
            "year": 2022
        },
        {
            "authors": [
                "Tiandong Wang",
                "Panpan Zhang"
            ],
            "title": "Directed hybrid random networks mixing preferential attachment with uniform attachment mechanisms",
            "venue": "Annals of the Institute of Statistical Mathematics,",
            "year": 2022
        },
        {
            "authors": [
                "Mingzhang Yin",
                "YX Rachel Wang",
                "Purnamrita Sarkar"
            ],
            "title": "A theoretical case study of structured variational inference for community detection",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2020
        },
        {
            "authors": [
                "Yunpeng Zhao",
                "Elizaveta Levina",
                "Ji Zhu"
            ],
            "title": "Community extraction for social networks",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2011
        },
        {
            "authors": [
                "Vinko Zlati\u0107",
                "Hrvoje"
            ],
            "title": "\u0160tefan\u010di\u0107. Model of wikipedia growth based on information exchange via reciprocal arcs",
            "venue": "EPL (Europhysics Letters),",
            "year": 2011
        }
    ],
    "sections": [
        {
            "text": "Keywords: Variational inference, Community detection, Preferential attachment, Bayesian methods"
        },
        {
            "heading": "1. Introduction",
            "text": "A frequent goal in the statistical inference of social networks is to develop models that adequately capture and quantify common types of user interaction. One such feature is the propensity of users to generate links with other users that already have attracted a large number of links (Newman, 2001; Jeong et al., 2003). In order to model this \u201crich get richer\u201d self-organizing feature of nodes in a growing network, Baraba\u0301si and Albert (1999) developed the preferential attachment (PA) model. The classical the preferential attachment model posits that as users enter a growing network, they connect with other users with probability proportional to their degree. This simple mechanism produces powerlaw degree distributions, yet another feature of many real-world networks (Mislove et al., 2007). Since it\u2019s inception, many generalizations of the preferential attachment model have\n\u00a92022 Daniel Cirkovic and Tiandong Wang.\nLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at http://jmlr.org/papers/v23/21-0000.html.\nar X\niv :2\n30 8.\n10 11\n3v 1\n[ st\nat .M\nL ]\n1 9\nbeen developed to capture more features of growing networks (Bhamidi et al., 2015; Hajek and Sankagiri, 2019; Wang and Zhang, 2022; Wang and Resnick, 2020).\nAnother common feature of online social networks is a significant degree of reciprocity (see Newman et al., 2002; Zlatic\u0301 and S\u030ctefanc\u030cic\u0301, 2011, for example). Reciprocity describes the tendency of users to reply to links and is typically measured by the proportion of reciprocal links in a network (Jiang et al., 2015). A recent study by Wang and Resnick (2022a) found that the traditional directed preferential attachment model often produces a negligible proportion of reciprocal links. Motivated by this finding, Wang and Resnick (2022c) and Cirkovic et al. (2022a) developed a preferential attachment model with reciprocity that is a more realistic choice for fitting to social networks. The model assumes that upon the generation of a link between nodes through the typical preferential attachment scheme, the users reciprocate the link with a probability \u03c1 \u2208 (0, 1) that is common to all users in the network. The model was used to analyze a Facebook wallpost network.\nAlthough an improvement, the model of Cirkovic et al. (2022a) fails to account for the heterogeneity of reciprocal behavior in a social network. In reality, it is na\u0308\u0131ve to assume all users in a large network engage in similar levels of reciprocity. Such an assumption has caused Cirkovic et al. (2022a) to remove a subset of nodes which apparently engaged in dissimilar reciprocal behavior from their analysis of the Facebook wallpost network. Further, when a link is made between two nodes u and v, it is likely that the decision of whether or not to reciprocate the link depends on the direction of the original link, (u, v) or (v, u). For example, a celebrity in a social network may be less likely to reply to a message sent by a fan, whereas a fan is very likely to respond to message sent by the celebrity. Recently, Wang and Resnick (2022b) relax the assumption of having only one reciprocity parameter \u03c1 to the case where reciprocity probabilities are different for users belonging to different communication classes. Theoretical results in Wang and Resnick (2022c) are obtained by assuming no new edge is added between existing nodes.\nIn this paper, we consider a further generalization of the model presented in Wang and Resnick (2022b) to allow for more realistic assumptions, i.e. heterogeneous, asymmetric reciprocity as well as edges between existing nodes. We assume that each user in the network is equipped with a communication class that governs its tendency to reciprocate edges. In the network generation process, initial edges between nodes are generated via preferential attachment, while the decision to reciprocate the edge is decided by a stochastic blockmodellike scheme. We describe three methods to fit such a model to observed networks, both when the number of communication classes is known and unknown. Specifically, we propose a fully Bayesian approach, along with variationally Bayesian and frequentist approaches. The approaches and their peformance on synthetic networks are then compared through simulation studies. Finally, we reconsider the Facebook wallpost network as in Cirkovic et al. (2022a), and use the heterogeneous reciprocal preferential attachment model to glean new insights into communication patterns on Facebook."
        },
        {
            "heading": "2. The PA Model with Heterogeneous Reciprocity",
            "text": ""
        },
        {
            "heading": "2.1 The model",
            "text": "In this section, we present the preferential attachment model with heterogeneous reciprocity. Let G(n) be the graph after n steps and V (n) be the set of nodes in G(n). Attach to each\nnode v a communication type Wv, where {Wv, v \u2265 1} are iid random variables with\nP(Wv = r) = \u03c0r, for K\u2211 r=1 \u03c0r = 1. (1)\nDefine the vector \u03c0 \u2261 (\u03c0r)r. Let W (n) := {Wv : v \u2208 V (n)} denote the set of group types for all nodes in G(n). Throughout we assume that the communication group of node v is generated upon creation and remains unchanged throughout the graph evolution. Also, denote the set of directed edges in G(n) by\nE(n) := {(u, v) : u, v \u2208 V (n)}.\nThroughout this paper, we always assume G(n) = (V (n), E(n),W (n)), for n \u2265 0. We initialize the model with seed graph G(0). G(0) consists of |V (0)| nodes, each of which is also endowed with its own communication class randomly according to (1). The edges E(0) will have no impact on inference other than setting the initial degree distribution. For each new edge (u, v) with Wu = r,Wv = m, the reciprocity mechanism adds its reciprocal counterpart (v, u) instantaneously with probability \u03c1m,r \u2208 [0, 1], for m, r \u2208 {1, 2, . . . ,K}. Here \u03c1m,r measures the probability of adding a reciprocal edge from a node in groupm to a node in group r. Note that the matrix \u03c1 := (\u03c1m,r)m,r is not necessarily a stochastic matrix, but can be an arbitrary matrix in MK\u00d7K([0, 1]), the set of all K \u00d7K matrices with entries belonging to [0, 1].\nWe now describe the evolution of the network G(n+1) from G(n). Let ( Dinv (n), D out v (n) ) be the in- and out-degrees of node v \u2208 V (n), and we use the convention that Dinv (n) = Doutv (n) = 0 if v /\u2208 V (n).\n1. With probability \u03b1 \u2208 [0, 1], add a new node |V (n)|+1 with a directed edge (|V (n)|+ 1, v), where v \u2208 V (n) is chosen with probability\nDinv (n) + \u03b4in\u2211 v\u2208V (n)(D in w (n) + \u03b4in) = Dinv (n) + \u03b4in |E(n)|+ \u03b4in|V (n)| , (2)\nwhere \u03b4in > 0 is an offset parameter, and update the node set V (n + 1) = V (n) \u222a {|V (n)|+ 1} and W (n+ 1) =W (n) \u222a {W|V (n)|+1}. The new node |V (n)|+ 1 belongs to group r with probability \u03c0r. If node v belongs to group m, then a reciprocal edge (v, |V (n)| + 1) is added with probability \u03c1m,r. Update the edge set as E(n + 1) = E(n) \u222a {(|V (n)| + 1, v), (v, |V (n)| + 1)}. If the reciprocal edge is not created, set E(n+ 1) = E(n) \u222a {(|V (n)|+ 1, v)}.\n2. With probability \u03b2 \u2208 [0, 1 \u2212 \u03b1], generate a directed edge (u, v) between two existing nodes u, v \u2208 V (n) with probability\nDinv (n) + \u03b4in\u2211 v\u2208V (n)(D in w (n) + \u03b4in) Doutv (n) + \u03b4out\u2211 v\u2208V (n)(D out v (n) + \u03b4out)\n= Dinv (n) + \u03b4in |E(n)|+ \u03b4in|V (n)| Doutv (n) + \u03b4out |E(n)|+ \u03b4out|V (n)| ,\n(3)\nwhere \u03b4out > 0 is also an offset parameter. If node u belongs to group r and node v belongs to group m, then a reciprocal edge (v, u) is added with probability \u03c1m,r. Update the edge set as E(n + 1) = E(n) \u222a {(u, v), (v, u)}. If the reciprocal edge is not created, set E(n + 1) = E(n) \u222a {(u, v)}. Finally, update V (n + 1) = V (n) and W (n+ 1) =W (n).\n3. With probability \u03b3 \u2261 1 \u2212 \u03b1 \u2212 \u03b2, add a new node |V (n)| + 1 with a directed edge (v, |V (n)|+ 1), where v \u2208 V (n) is chosen with probability\nDoutv (n) + \u03b4out\u2211 v\u2208V (n)(D out v (n) + \u03b4out) = Doutv (n) + \u03b4out |E(n)|+ \u03b4out|V (n)| , (4)\nand update the node set V (n + 1) = V (n) \u222a {|V (n)| + 1}, W (n + 1) = W (n) \u222a {W|V (n)|+1}. The new node |V (n)|+1 belongs to group r with probability \u03c0r. If node v belongs to group m, then a reciprocal edge (|V (n)|+1, v) is added with probability \u03c1r,m. Update the edge set as E(n+1) = E(n)\u222a {(v, |V (n)|+1, v), (|V (n)|+1, v)}. If the reciprocal edge is not created, set E(n+ 1) = E(n) \u222a {(v, |V (n)|+ 1)}.\nLet {Jk} be iid Categorical random variables that indicate under which scenario the transition from G(k) to G(k+ 1) has occurred. That is, P(Jk = 1) = \u03b1, P(Jk = 2) = \u03b2 and P(Jk = 3) = 1\u2212 \u03b1 \u2212 \u03b2. At each step k, we denote the outcome of the reciprocal event via Rk where Rk = 1 if a reciprocal edge is added and Rk = 0 otherwise."
        },
        {
            "heading": "2.2 Likelihood inference",
            "text": "Suppose we observe the evolution of the graph sequence {G(k)}nk=0 so that we have the edges ek = E(k) \\ E(k \u2212 1) added at each step according to the description in Section 2.1. Here,\nek = { {(sk, tk), (tk, sk)} if Rk = 1 {(sk, tk)} if Rk = 0.\n(5)\nLet \u03b8 = (\u03b1, \u03b2, \u03b4in, \u03b4out). With these ingredients, the likelihood associated with the graph sequence {G(k)}nk=0 is given by\np ((ek) n k=1,W (n) | \u03b8,\u03c0,\u03c1) = \u03b1 \u2211n k=1 1{Jk=1}\u03b2 \u2211n k=1 1{Jk=2}(1\u2212 \u03b1\u2212 \u03b2) \u2211n k=1 1{Jk=3}\n\u00d7 n\u220f\nk=1\n( Dintk(k \u2212 1) + \u03b4in\n|E(k \u2212 1)|+ \u03b4in|V (k \u2212 1)| )1{Jk\u2208{1,2}} ( Doutsk (k \u2212 1) + \u03b4out |E(k \u2212 1)|+ \u03b4out|V (k \u2212 1)| )1{Jk\u2208{2,3}}\n\u00d7 K\u220f r=1 \u03c0 \u2211n k=1 1{Jk=1}1{Wsk=r} + \u2211n k=1 1{Jk=3}1{Wtk=r} r\n\u00d7 K\u220f r=1 K\u220f m=1 \u03c1 \u2211n k=1 1{Wsk=r} 1{Wtk=m} 1{Rk=1} m,r (1\u2212 \u03c1m,r) \u2211n k=1 1{Wsk=r} 1{Wtk=m} 1{Rk=0}\n\u2261 p((ek)nk=1 | \u03b8)\u00d7 p((ek)nk=1,W (n) | \u03c0,\u03c1).\nThe function p(\u00b7 | \u03b8) collects the likelihood terms dependent on \u03b8 and likewise p(\u00b7 | \u03c0,\u03c1) collects the terms dependent on \u03c0 and \u03c1. Such factorization implies that the estimation of the parameters \u03b8 and \u03c0,\u03c1 can be conducted independently. The frequentist estimation of \u03b8 in homogeneous reciprocal PA models has already been considered in Cirkovic et al. (2022a). These estimators are unchanged in the heterogeneous case. Naturally, the maximum likelihood estimators (MLE) for \u03b1 and \u03b2 are given by \u03b1\u0302 = n\u22121 \u2211n k=0 1{Jk=1} and\n\u03b2\u0302 = n\u22121 \u2211n\nk=0 1{Jk=2}. The MLE for \u03b4in satisfies\nn\u2211 k=1 1{Jk\u2208{1,2}} 1 Dintk(k \u2212 1) + \u03b4\u0302in \u2212 n\u2211 k=1 1{Jk\u2208{1,2}} |V (k \u2212 1)| |E(k \u2212 1)|+ \u03b4\u0302inN(k \u2212 1) = 0, (6)\nwhere (6) is obtained by setting \u2202\u2202\u03b4in log p((ek) n k=1 | \u03b8) = 0. The MLE for \u03b4out is obtained similarly. The estimators \u03b1\u0302 and \u03b2\u0302 are strongly consistent for \u03b1 and \u03b2, while consistency for \u03b4\u0302in and \u03b4\u0302out has not yet been verified since the reciprocal component of the model interferes with traditional techniques to analyze consistency in non-reciprocal preferential attachment models as in Wan et al. (2017). Estimation of \u03c1 and \u03c0 is considerably more involved, and will be the main focus of this paper.\nThe reciprocal component of the preferential attachment model with heterogeneous reciprocity is reminiscent of a stochastic block model. Nodes first attach via the preferential attachment rules in (2), (3) and (4), then a stochastic-block-model type mechanism dictates the reciprocal behavior. A large portion of the literature on stochastic block modeling is concerned with community detection Bickel and Chen (2009); Holland et al. (1983); Karrer and Newman (2011); Zhao et al. (2011). Here we are primarily concerned with the estimation of \u03c1 and \u03c0, and consider the recovery of W (n) as a secondary goal. The optimal recovery of \u03c1 and \u03c0 hinges on the correct specification of K, the number of reciprocal clusters. We will thus examine cases when K is known a priori, as well as cases where it must be inferred from the data.\nWe also note that a minor nuisance of modeling reciprocal PA models is the observation of the random variable Rk. Upon observations the edges {(sk, tk), (tk, sk)}, it is not possible to identify whether the second edge was generated under Rk = 1 or Jk = 2. Since, upon observation, the probability that the edge was generated under Jk = 2 is extremely small for large networks, we assume all such reciprocated edges are generated under Rk = 1. In real-world networks, however, time will often pass between message replies. For such networks, we will thus employ window estimators from Cirkovic et al. (2022a). We defer further discussion of window estimators to Section 6.\nWe will continue to consider the estimation of \u03c1 and \u03c0 based on p((ek) n k=1,W (n) | \u03c0,\u03c1). Since W (n) is unobservable, a natural probabilistic approach would marginalize over the unobservable communication types, and form a complete-data likelihood p((ek) n k=1 | \u03c0,\u03c1). This, however, involves a sum over all latent configurations of W (n) which is analytically intractable, as well as computationally infeasible for large networks. Such difficulties encourage attempts to learn W (n) from the conditional distribution of W (n) given (ek) n k=1 (a\u0301 la an EM Algorithm Dempster et al. (1977)) and jointly estimate W (n),\u03c0 and \u03c1. Often, these attempts are computationally infeasible due to the lack of factorization in the conditional distribution. In the following section, we will consider both Bayesian and frequentist estimation methods for \u03c0 and \u03c1 where K is known. We will first present an \u201cideal\u201d fully\nBayesian approach, and then move on to variationally Bayesian and frequentist approximations to that ideal. Afterwards, we will discuss how to perform model selection when K is unknown for each of these methods."
        },
        {
            "heading": "3. Inference for a known number of communication types",
            "text": ""
        },
        {
            "heading": "3.1 Bayesian inference",
            "text": "For Bayesian inference of the heterogeneous reciprocal PA model we follow Nowicki and Snijders (2001) and employ independent and conditionally conjugate priors\n\u03c1m,r i.i.d.\u223c Beta(a, b), m, r = 1, . . . ,K, \u03c0 \u223c Dirichlet(\u03b7, . . . , \u03b7). (7)\nThe prior specification (7) leads to a simple Gibbs sampler that draws approximate samples from the posterior p (\u03c1,\u03c0,W (n) | (ek)nk=1). We present the Gibbs sampler as Algorithm 1. Here, \u03c1 and \u03c0 are initialized from prior draws and W (n) is initialized by drawing from p (Wv | \u03c0) for v = 1, . . . |V (n)|. Although the sampler is standard, many samples are required to sufficiently explore the posterior distribution. For large networks, this can be computationally onerous, and hence we appeal to variational alternatives."
        },
        {
            "heading": "3.2 Variational inference",
            "text": "In this section, we present variational alternatives for approximating posteriors associated with the heterogeneous reciprocal PA model. The aim of variational inference is to approximate the conditional distribution of latent variables z given data x via a class of densities Q typically chosen to circumvent computational inconveniences. If Bayesian inference is being performed, the latent variables z can also encompass the model parameters (\u03c0 and \u03c1 in our setting). The variational inference procedure aims to find the density q\u22c6 \u2208 Q that minimizes the Kullback-Leibler (KL) divergence from p(\u00b7 | x), i.e.\nq\u22c6 = argmin q\u2208Q KL (q(\u00b7) || p(\u00b7 | x)) . (8)\nWe will restrict Q to the mean-field family, that is, the family of densities where components z are mutually independent. Naturally, such restriction will prevent q\u22c6 from capturing the dependence structure between the latent variables. Recently, however, some more structured, expressive families have been proposed that may improve the approximation; see for instance Yin et al. (2020). Conveniently, using the definition of the conditional density, the objective (8) can be expressed as\nKL (q(\u00b7) || p(\u00b7 | x)) = Eq[log q(z)]\u2212 Eq[log p(z,x)] + log p(x) \u2261 \u2212ELBO(q) + log p(x), (9)\nso that minimizing the KL divergence from p(\u00b7 | x) to q(\u00b7) is equivalent to maximizing the evidence lower bound (ELBO(q)) since log p(x) does not depend on q. For more on variational inference, see Blei et al. (2017).\nAlgorithm 1 Gibbs sampling for heterogeneous reciprocal PA with known K\nInput: Graph G(n), # communication types K, prior parameters a, b, \u03b7, # MCMC iterations M Output: Approximate samples from the posterior p (\u03c1,\u03c0,W (n) | (ek)nk=1) Initialize: Draw \u03c0 and \u03c1 from (7), draw Wv \u223c Multinomial(\u03c0) for v \u2208 V (n) for i = 1 to M do 1. Sample W (n) from its conditional posterior\nfor all v \u2208 V (n) do Sample Wv according to\nP (Wv = \u2113 | \u03c0,\u03c1, (Wu)u\u0338=v, (ek)nk=1)\n\u221d \u03c0r K\u220f\nm=1\n\u03c1\n\u2211 k:sk=v 1{Wtk=m} 1{Rk=1} m,\u2113 (1\u2212 \u03c1m,\u2113) \u2211 k:sk=v 1{Wtk=m} 1{Rk=0}\n\u00d7 K\u220f r=1 \u03c1 \u2211 k:tk=v 1{Wsk=r} 1{Rk=1} \u2113,r (1\u2212 \u03c1\u2113,r) \u2211 k:tk=v 1{Wsk=r} 1{Rk=0}\nfor \u2113 = 1, . . . ,K end for\n2. Sample \u03c1 from its conditional posterior for m = 1 to K do\nfor r = 1 to K do Sample \u03c1m,r from\n\u03c1m,r | \u03c0,W (n), (ek)nk=1 \u223c Beta\n( a+\nn\u2211 k=1 1{Wsk=r}1{Wtk=m}1{Rk=1},\nb+ n\u2211 k=1 1{Wsk=r}1{Wtk=m}1{Rk=0}\n)\nend for end for\n3. Sample \u03c0 from its conditional posterior\n\u03c0 | \u03c1,W (n), (ek)nk=1 \u223c Dirichlet \u03b7 + \u2211 v\u2208V (n) 1{Wv=1}, . . . , \u03b7 + \u2211 v\u2208V (n) 1{Wv=K}  end for"
        },
        {
            "heading": "3.2.1 Bayesian Variational Inference",
            "text": "Now we consider solving the variational problem (8) for the probabilistic model presented in Section 3.1. Although we have presented a sampler in Algorithm 1 that draws approximate samples from the posterior, we aim for an estimate that sacrifices modeling the dependence in the posterior distribution in favor of computation time. Variational inference for stochastic blockmodels in the Bayesian setting was studied in Latouche et al. (2012). Following their strategy, we posit a mean-field variational family:\nq(\u03c0,\u03c1,W (n)) = q(\u03c0)q(\u03c1)q(W (n)) = q(\u03c0) K\u220f\nm=1 K\u220f r=1 q(\u03c1m,r) \u220f v\u2208V (n) qv(Wv). (10)\nWe further assume that the variational densities have the following forms:\nq(\u03c0) \u221d K\u220f r=1 \u03c0d11 \u00b7 \u00b7 \u00b7\u03c0 dK K , d1, . . . , dK \u2265 0,\nq(\u03c1m,r) \u221d \u03c1 \u03c9m,r m,r (1\u2212 \u03c1m,r)\u03bem,r , \u03c9m,r, \u03bem,r \u2265 0, m, r = 1, . . . ,K,\nqv(Wv) = K\u220f r=1 \u03c4 1{Wv=r} v,r , \u03c4v,r \u2265 0, r = 1, . . . ,K, v = 1, . . . |V (n)|,\nand additionally \u2211K\nr=1 \u03c4v,r = 1 for all v \u2208 V (n). In other words, the posterior of \u03c0 is approximated by a Dirichlet(d1, . . . , dK) distribution, and the component-wise posteriors of \u03c1 and W (n) are approximated by Beta(\u03c9m,r, \u03bem,r) and Multinomial(1, (\u03c4v,r) K r=1) distributions, respectively. In Algorithm 2 we present a coordinate ascent variational inference (CAVI) algorithm for optimizing the ELBO. Here, \u03c8(\u00b7) is the digamma function. Note that in step 3 of algorithm, we write \u2211 k:sk=v \u2261 \u2211\nk:sk=v,sk \u0338=tk for brevity of notation. The inclusion of self-loops makes the optimization of the ELBO much more difficult, hence their exclusion. Here, the class probabilities, \u03c4u,r, are initialized uniformly at random. We omit the calculations for the derivation of this algorithm, as they are very similar to Latouche et al. (2012).\nTo monitor the convergence of Algorithm 2, we recommend computing the ELBO after each iteration of the CAVI algorithm and terminating the algorithm once the increase in the ELBO is less than some predetermined threshold \u03f5. Specifically, if the ELBO is computed after step 2, it has the simplified form:\nELBO(q) = log\n( \u0393(K\u03b7) \u220fK r=1 \u0393(dr)\n\u0393( \u2211K\nr=1 dr)\u0393(\u03b7) K\n) +\nK\u2211 r=1 K\u2211 m=1 log ( \u0393(a+ b)\u0393(\u03c9m,r)\u0393(\u03bem,r) \u0393(\u03c9m,r + \u03bem,r)\u0393(a)\u0393(b) )\n\u2212 \u2211\nv\u2208V (n) K\u2211 r=1 \u03c4v,r log \u03c4v,r.\n(11)"
        },
        {
            "heading": "3.2.2 Variational Expectation Maximization",
            "text": "In this section we consider frequentist estimation of the PA model with heterogeneous reciprocity through a variational expectation maximization algorithm (VEM). VEM for\nAlgorithm 2 CAVI for heterogeneous reciprocal PA with known K\nInput: Graph G(n), # communication types K, prior parameters a, b, \u03b7, tolerance \u03f5 > 0 Output: Variational approximation to the posterior q\u22c6\nInitialize: Draw \u03c4v,r, r = 1, . . . ,K uniformly at random from the K-simplex for every v \u2208 V (n) while the increase in ELBO(q) is greater than \u03f5 do 1. Update q(\u03c0)\nfor r = 1 to K do\ndr = \u03b7 + \u2211\nv\u2208V (n)\n\u03c4u,r\nend for 2. Update q(\u03c1)\nfor m = 1 to K do for r = 1 to K do\n\u03c9m,r = a+ n\u2211 k=1 \u03c4sk,r\u03c4tk,m1{Rk=1}\n\u03bem,r = b+ n\u2211 k=1 \u03c4sk,r\u03c4tk,m1{Rk=0}\nend for end for\n3. Update ELBO(q) according to (11) 4. Update q(W (n))\nfor all v \u2208 V (n) do for \u2113 = 1 to K do\n\u03c4v,\u2113 \u221d exp { \u03c8 (d\u2113) \u2212 \u03c8 ( K\u2211 r=1 dr )}\n\u00d7 K\u220f\nm=1\nexp { \u03c8 (\u03c9m,\u2113)\n\u2211 k:sk=v \u03c4tk,m1{Rk=1} + \u03c8 (\u03bem,\u2113) \u2211 k:sk=v \u03c4tk,m1{Rk=0}\n\u2212 \u03c8(\u03c9m,\u2113 + \u03bem,\u2113) \u2211\nk:sk=v\n\u03c4tk,m\n}\n\u00d7 K\u220f r=1 exp\n{ \u03c8 (\u03c9\u2113,r)\n\u2211 k:tk=v \u03c4sk,r1{Rk=1} + \u03c8 (\u03be\u2113,r) \u2211 k:tk=v \u03c4sk,r1{Rk=0}\n\u2212 \u03c8(\u03c9\u2113,r + \u03be\u2113,r) \u2211\nk:tk=v\n\u03c4sk,r\n}\nend for end for\nend while\nstochastic blockmodel data was first considered in Daudin et al. (2008) which further inspired many interesting generalizations that could enhance the reciprocal PA model (see Matias and Miele, 2017, for example). The VEM algorithm augments the traditional EM algorithm by approximating the E-step for models in which the conditional distribution of the latent variables given the observed data is computationally intractable. The VEM estimates thus serve as a computationally efficient approximation to the maximum likelihood estimates of \u03c0 and \u03c1. Although a frequentist procedure, the VEM algorithm may enhance Bayesian inference of stochastic blockstructure data. For example, since the dimension of the posterior p (\u03c0,\u03c1 | (ek)nk=1) does not grow with the size of the data, one might expect a Bernstein-von-Mises phenomena to occur. The VEM estimates may thus approximate the posterior mean or even be leveraged to enhance posterior sampling as in Donnet and Robin (2021).\nAs in Section 3.2.1, we approximate the distribution of the communication types given the observed network, p (W (n) | \u03c0,\u03c1, (ek)nk=1), via the mean-field approximation\nq(W (n)) = \u220f\nv\u2208V (n)\nqv(Wv).\nVia the mean-field family assumption, the ELBO is given by\nELBO(q,\u03c0,\u03c1)\n=Eq [log p (W (n), (ek) n k=1 | \u03c0,\u03c1)]\u2212 Eq [log q(W (n))]\n= n\u2211\nk=1 K\u2211 r=1 ( 1{Jk=1}\u03c4sk,r + 1{Jk=3}\u03c4tk,r ) log \u03c0r \u2212 \u2211 v\u2208V (n) K\u2211 r=1 \u03c4v,r log \u03c4v,r + n\u2211\nk=1 K\u2211 r=1 K\u2211 m=1 \u03c4sk,r\u03c4tk,m ( 1{Rk=1} log \u03c1m,r + 1{Rk=0} log(1\u2212 \u03c1m,r) ) .\n(12)\nNote that from (9), maximizing (12) with respect to q (the E-step) is equivalent to minimizing the KL divergence from p (\u00b7 | \u03c0,\u03c1, (ek)nk=1) to q(\u00b7) and maximizing (12) with respect to \u03c0 and \u03c1 is equivalent to the M-step in the usual EM algorithm. Thus, the E-step is equivalent to performing variational inference for p (\u00b7 | \u03c0,\u03c1, (ek)nk=1) where \u03c0 and \u03c1 are evaluated at their current estimates \u03c0\u0302VEM and \u03c1\u0302VEM.\nThe VEM algorithm for the heterogeneous reciprocal PA model is given in Algorithm 3. As in Algorithm 2, we write \u2211 k:sk=v \u2261 \u2211\nk:sk=v,sk \u0338=tk for ease of notation. We describe the intialization of the algorithm at the end of Appendix A in Algorithm 5. We further provide some derivations of the VEM algorithm in Appendix B. Similar types of computations can be employed to derive Algorithm 2. As in Algorithm 2, we recommend cycling through the updates of \u03c4\u0302v,\u2113 in the E-step until the ELBO no longer increases beyond a prespecified threshold \u03f5 > 0."
        },
        {
            "heading": "4. Model selection for an unknown number of communication types",
            "text": "In this section we extend the methods discussed in Section 3 to the case where the number of communication types is not known a priori. This can be viewed as a model selection problem, where the Bayesian solution places a prior on K while the variationally Bayesian and EM algorithms aim to imitate marginal likelihood-based procedures.\nAlgorithm 3 VEM for heterogeneous reciprocal PA with known K\nInput: Graph G(n), # communication types K, tolerances \u03f5, \u03ba > 0 Output: Variational EM estimates \u03c0\u0302VEM and \u03c1\u0302VEM\nInitialize: Draw \u03c4\u0302v,r, r = 1, . . . ,K uniformly at random from the K-simplex for every v \u2208 V (n), run Algorithm 5 to initialize \u03c0\u0302VEM and \u03c1\u0302VEM while at least one of the elements of \u03c0\u0302VEM and \u03c1\u0302VEM change by more than \u03ba in absolute value do 1. E-step: Update q\u0302 via\nwhile the increase in ELBO(q) is greater than \u03f5 do for all v \u2208 V (n) do\nfor \u2113 = 1 to K do\n\u03c4\u0302v,\u2113 \u221d \u03c0\u0302\u2113 K\u220f\nm=1\n\u03c1\u0302\n\u2211 k:sk=v\n\u03c4\u0302tk,m1{Rk=1} m,\u2113 (1\u2212 \u03c1\u0302m,\u2113)\n\u2211 k:sk=v \u03c4\u0302tk,m1{Rk=0}\n\u00d7 K\u220f r=1 \u03c1\u0302 \u2211 k:tk=v \u03c4\u0302sk,r1{Rk=1} \u2113,r (1\u2212 \u03c1\u0302\u2113,r) \u2211 k:tk=v \u03c4\u0302tk,r1{Rk=0}\nend for end for Update ELBO(q) according to (12)\nend while 2. M-step: Update \u03c0\u0302VEM and \u03c1\u0302VEM via\nfor m = 1 to K do\n\u03c0\u0302m = \u2211\nv\u2208V (n)\n\u03c4\u0302v,m\nfor r = 1 to K do\n\u03c1\u0302m,r = \u2211n k=1 \u03c4\u0302sk,r \u03c4\u0302tk,m1{Rk=1}\u2211n\nk=1 \u03c4\u0302sk,r \u03c4\u0302tk,m\nend for end for\nend while"
        },
        {
            "heading": "4.1 A prior on K",
            "text": "This section extends the Bayesian solution in Section 3.1 to making inference on the unknown number of communication classes K. In a fully Bayesian framework, K is assigned a prior and inference is made on the posterior of K given the observed data. This, however, often requires the use of complicated reversible jump MCMC (RJMCMC) algorithms to make valid posterior inference on K. Generically, mixture models with a prior on the number of mixture components are known as mixture of finite mixture (MFM) models. For Bayesian MFMs, Miller and Harrison (2018) derived the Dirichlet process-like properties of MFMs and proposed a collapsed Gibbs sampler that circumvented the need for RJMCMC. Geng et al. (2019) utilized a similar collapsed Gibbs sampler for learning the number of components in a stochastic block model. Unfortunately, such collapsed Gibbs samplers require analytically marginalizing over K, restricting our ability to make inference on \u03c0 without some ad-hoc post-processing of the posterior samples. Recently, a telescoping sampler has been developed by Fru\u0308hwirth-Schnatter et al. (2021) for MFMs that obviates the need to marginalize over K. Rather, K is explicitly sampled in the scheme by distinguishing between K, the number of mixture components, and K+, the number of filled mixture components.\nFor the heterogeneous reciprocal PA model, we adopt the prior specification in (7) and additionally let K \u2212 1 follow a beta-negative-binomial (BNB) distribution with parameters c1, c2 and c3 as recommended by Fru\u0308hwirth-Schnatter et al. (2021). The BNB distribution is a hierachical generalization of the Poisson, geometric, and negative-binomial distribution. If K \u2212 1 \u223c BNB(c1, c2, c3) then the probability mass function on K is given by\np(K) = \u0393(c1 +K \u2212 1)B(c1 + c2,K \u2212 1 + c3)\n\u0393(c1)\u0393(K)B(c2, c3) , K = 1, 2, . . . ,\nwhere B denotes the beta function. As discussed in Fru\u0308hwirth-Schnatter et al. (2021), the BNB distribution allows for the user to specify a heavier tail on the number of mixture components which is essential in order for the telescoping sampler to mix well. Previous analyses in Geng et al. (2019) and Miller and Harrison (2018) specify that K\u22121 \u223c Poisson(1), which is a highly informative choice with a light tail.\nWe present the telescoping sampler for heterogeneous reciprocal PA models in Algorithm 4. For ease of notation, we do not distinguish betweenW (n), the communication types, and the random partition of the |V (n)| nodes into K+ clusters induced by W (n). However, the alternating between sampling on the parameter space of the mixture distribution and the set partition space is a key aspect that allows K to be directly sampled from the conditional posterior of K given the partition induced by W (n) (Step 3 in Algorithm 4). We refer to Fru\u0308hwirth-Schnatter et al. (2021) for more details on the telescoping sampler. Note that within the sampler, K only decreases if one of the K+ filled components loses all of its membership in Step 1. Thus, in order for the sampler to mix well, K must occasionally exceed K+, emphasizing the need for a heavier-tailed prior on K.\nFru\u0308hwirth-Schnatter et al. (2021) also present a dynamic mixture of finite mixture model where the prior on \u03c0 is taken to be be Dirichlet(\u03c6/K,\u03c6/K, . . . , \u03c6/K) for some \u03c6 > 0. This specification would induce a sparse mixture model where a large number of mixture components K would be fit, but a majority of them would be unfilled (Fru\u0308hwirth-Schnatter and Malsiner-Walli, 2019; Malsiner-Walli et al., 2016). In this sense, the posterior distributions\non K and K+ would differ greatly. Though this is undesirable for learning the parameters of a mixture model, it may be useful for analyses more focused on partitioning nodes into a small number of classes with similar reciprocal behavior."
        },
        {
            "heading": "4.2 Imitations of the marginal likelihood",
            "text": "In this section we review criteria for choosing the number of communication types K for the variational methods proposed in Section 3.2. A typical strategy for Bayesian model selection is choosing the model that maximizes the marginal likelihood, or the probability distribution that is obtained by integrating the likelihood over the prior distribution of the parameters. For many of the same reasons presented in Section 2.2, the marginal likelihood is not avaible for stochastic blockmodel data. Instead, for the Bayesian Variational Inference method presented in Section 3.2.1, Latouche et al. (2012) recommend employing the ELBO as the model selection criterion. From (9), it can be seen that\nELBO(q) = \u2212KL (q(\u00b7) || p(\u00b7|(ek)nk=1))) + log p ((ek)nk=1) \u2264 log p ((ek)nk=1) .\nThat is, the ELBO lower bounds the marginal likelihood, and if the variational approximation to the posterior is good, the ELBO should approximate it. Though, there is no evidence that the variational approximation results in a sufficiently small KL divergence such that this is a worthwhile approximation. Regardless, this criteria is often used in practice (Blei et al., 2017).\nFor the VEM algorithm, Daudin et al. (2008) recommend employing the Integrated Classification Likelihood (ICL). Although the VEM algorithm is a frequentist procedure, the ICL criterion is derived by assuming a Jeffrey\u2019s prior on \u03c0 (\u03b7 = 1/2) and further employs a BIC approximation to the distribution of (ek) n k=1 given W (n). The ICL for reciprocal PA models is given by\nICL(K) = log p((ek) n k=1, W\u0302 (n) | \u03c0\u0302VEM, \u03c1\u0302VEM)\u2212\nK2 2 log n\u2212 K \u2212 1 2 log |V (n)|,\nwhere W\u0302 (n) is the modal approximation of W (n) given by W\u0302v = argmax\u2113=1,...,K \u03c4\u0302v,\u2113."
        },
        {
            "heading": "5. Simulation Studies",
            "text": "In this section, we evaluate the performance of the estimation procedures presented in Sections 3 and 4 on synthetic datasets. We evaluate the performance of estimation methods for \u03c0 and \u03c1 when K is known, as well as the accuracy of the model selection criteria presented in Section 4 when K is unknown. When K is known, we employ the Monte Carlo averages of the approximate posterior samples, the posterior means of the variational densities and the variational EM estimates as point estimators of \u03c0 and \u03c1 for the fully Bayesian (B), Variational Bayes (VB) and Variational EM (VEM) methods, respectively. Since the B and VB methods produce approximate posteriors, we also provide marginal coverage rates of credible intervals constructed using the element-wise 2.5% and 97.5% quantiles of the respective posteriors for \u03c0 and \u03c1. In the case of known K, we further provide the average Rand index for estimating (Wv)v\u2208V (n) for each method. When K is unknown, we record the frequencies of the estimated K under each model selection criteria. We employ the posterior mode as the estimated K for the fully Bayesian method.\nAlgorithm 4 Telescoping sampler for heterogeneous reciprocal PA with known K\nInput: Graph G(n), parameters a, b, \u03b7, c1, c2, c3, K initial/max values Kinit, Kmax, # MCMC iterations M Output: Approximate samples from the posterior p (\u03c1,\u03c0,W (n),K | (ek)nk=1) Initialize: Set K = Kinit, draw \u03c0 and \u03c1 from (7), draw Wv \u223c Multinomial(\u03c0) for v \u2208 V (n) for i = 1 to M do 1. Sample W (n) from its conditional posterior\nfor all v \u2208 V (n) do Sample Wv according to\nP (Wv = \u2113 | \u03c0,\u03c1, (Wu)u\u0338=v, (ek)nk=1)\n\u221d \u03c0\u2113 K\u220f\nm=1\n\u03c1\n\u2211 k:sk=v 1{Wtk=m} 1{Rk=1} m,\u2113 (1\u2212 \u03c1m,\u2113) \u2211 k:sk=v 1{Wtk=m} 1{Rk=0}\n\u00d7 K\u220f r=1 \u03c1 \u2211 k:tk=v 1{Wsk=r} 1{Rk=1} \u2113,r (1\u2212 \u03c1\u2113,r) \u2211 k:tk=v 1{Wsk=r} 1{Rk=0} ,\nfor \u2113 = 1, . . . ,K end for\nand determine the number of filled components K+. Relabel the communication classes such that the first K+ components are filled and the rest are empty. 2. Sample the filled components of \u03c1 from its conditional posterior\nfor m = 1 to K+ do for r = 1 to K+ do\n\u03c1m,r | \u03c0,W (n), (ek)nk=1 \u223c Beta\n( a+\nn\u2211 k=1 1{Wsk=r}1{Wtk=m}1{Rk=1},\nb+ n\u2211\nk=1\n1{Wsk=r}1{Wtk=m}1{Rk=0}\n) ,\nend for end for\n3. Sample K from\np(K|W (n)) \u221d p(K) K! (K \u2212K+)!\n\u0393(\u03b7K)\n\u0393(|V (n)|+ \u03b7K)\u0393(\u03b7)K+ K+\u220f r=1 \u0393  \u2211 v\u2208V (n) 1{Wv=r} + \u03b7  , where K = K+,K++1, . . . ,Kmax. If K > K+, generate K\u2212K+ empty components and fill the corresponding \u03c1 components with draws from the prior Beta(a, b). 4. Sample \u03c0 from its conditional posterior\n\u03c0 | \u03c1,W (n), (ek)nk=1 \u223c Dirichlet \u03b7 + \u2211 v\u2208V (n) 1{Wv=1}, . . . , \u03b7 + \u2211 v\u2208V (n) 1{Wv=K}  end for 14\nIn each simulation, we assume non-informative priors Dirichlet(1/2, . . . , 1/2) on \u03c0 and Beta(1/2, 1/2) on \u03c1 for the VB and B methods. Although a prior is not explicitly assumed for the VEM method, the ICL model selection criterion implicitly assumes the same prior on \u03c0, hence these choices are consistent. For the VEM algorithm, we terminate the E-step once either the ELBO has increased by less than \u03f5 = 0.01 or the total number of iterations exceed 500, and terminate the entire algorithm once the element-wise differences in the parameters fall below \u03ba = 0.01. We also terminate the VB algorithm via the same conditions as in the E-step of the VEM algorithm. We run the fully Bayesian method for M = 5,000 MCMC samples, and discard the first half as burn-in. Further, when K is unknown, we assume a BNB(1, 4, 3) prior on K as recommended by Fru\u0308hwirth-Schnatter et al. (2021) and set Kmax = 20. For the variational methods, we search over K = 1, 2, 3, 4."
        },
        {
            "heading": "5.1 Simulations on Pragmatic Networks",
            "text": "In this section we evaluate the presented model estimation and selection criteria on networks which are generated to reflect those found in real-world applications. In particular, we generate a PA network with heterogeneous reciprocity such that \u03b8 = (\u03b1, \u03b2, \u03b4in, \u03b4out) = (0.15, 0.8, 1, 1),\n\u03c0 = [ 0.8 0.2 ] and \u03c1 = [ 0.5 0.9 0.05 0.2 ] .\nThis network generating process contains two groups, the first of which can be thought of as typical users, and the other can be thought of celebrities. Here typical users will often reciprocate the messages from celebrities, but a celebrity is far less likely to respond to a typical user. As one might expect, there are far more typical users than celebrities in this network.\nTable 1 displays the means and standard errors of the element-wise point estimators across the simulations, as well as the coverage of credible intervals produced from the B and VB methods. Here, the estimation procedures have virtually identical performance in terms of point estimation. Further, the coverage rates for the fully Bayesian method hover around the expected 95%, while the coverage rates for the Variational Bayes method vary across the parameters. The VB method seems to have difficultly capturing the larger reciprocity \u03c112 = 0.9, as well as \u03c01 = 0.8. The methods also perform similarly in terms of classification, as the average Rand index for the communication types are given by 0.767, 0.767 and 0.766 for the B, VB and VEM methods respectively.\nTable 2 displays the performance of the model selection criteria on the same preferential attachment model but for unknown K. For the fully Bayesian method, we initialize at Kinit = 4 in order to exhibit insensitivity of the telescoping sampler to initialization. Note that the ELBO and ICL select the correct class for every simulated data set, while the fully Bayesian method has a slight tendency to over-select the number of classes. Clearly, however, analysis of such networks result in variational methods that perform comparably to the fully Bayesian method, at less computational cost.\nWe continue our simulations by evaluating the performance of the estimation procedures on 100 synthetic networks generating from a PA network with heterogeneous reciprocity\nsuch that \u03b8 = (0.15, 0.8, 1, 1) but now\n\u03c0 = [ 0.8 0.2 ] and \u03c1 = [ 0.5 0 0.05 0.2 ] .\nNote that the only difference from this simulation set-up and the previous one is that \u03c112 has decreased from 0.9 to 0. The inclusion of 0 into the \u03c1 matrix is motivated by the data example in Section 6, where we find a group of users that do not receive reciprocal edges. This set-up is analogous to a diagonally-dominant stochastic block model where users are likely to communicate within groups but not across groups.\nTable 3 displays the point estimates for all three methods, along with the coverage probabilities for the B and VB methods. With the decrease in \u03c112, the variational methods struggle to recover \u03c122. This is sensible since class 2 communicating with class 2 should be the least common communication type according to \u03c0 and, unlike the case when \u03c112 = 0.9, the difference between the communication classes is not obvious. Otherwise, the estimation accuracy of the other parameters is relatively consistent across all the methods. Although coverage rates are similar to Table 1, we also observe a reduction in the coverage of \u03c122. Evidently, equal-tailed credible intervals are a poor choice for capturing \u03c112 and if one had prior knowledge on the behavior of \u03c1, a highest posterior density interval would be a sensible choice. The average Rand index for the B, VB and VEM methods are given by 0.763, 0.763 and 0.760, respectively, again indicating that the methods classify similarly when the number of edges far exceeds the number of nodes.\nTable 4 displays the performance of the model selection criteria presented in Section 4 for the preferential attachment model as in 3. Again, K is initialized at Kinit = 4 for the fully Bayesian method. Note that, again, the variational methods select the correct number of clusters in each simulation, while the telescoping sampler has a slight tendency to overfit."
        },
        {
            "heading": "5.2 Comparisons to the SBM",
            "text": "In this section we evaluate the same estimation and model selection procedures on synthetic networks with a comparably low number of edges relative to the number of nodes. Such networks serve to highlight the additional difficulties faced by estimating reciprocal PA models compared to stochastic block models. We simulate 100 preferential attachment networks of size n = 30,000 from a PA model with \u03b8 = (\u03b1, \u03b2, \u03b4in, \u03b4out) = (0.75, 0, 0.8, 0.8) and the reciprocal component governed by\n\u03c0 = [ 0.6 0.4 ] and \u03c1 = [ 0.1 0.4 0.5 0.8 ] .\nWang and Resnick (2022b) have showed that, under suitable conditions, such heterogeneous reciprocal PA models with \u03b2 = 0 exhibit networks with out/in-degrees that exhibit a complex extremal dependence structure (see Appendix A for more details). Additionally, since \u03b2 = 0, such models allow for the complete observation of the reciprocal edge events as there are no Jk = 2 edges that could be mistaken as reciprocal edges.\nHere we assume K is known. Table 5 displays the average value of the point estimates of \u03c0 and \u03c1 for each method, as well as their associated standard errors. Clearly, the fully Bayesian method outperforms both the VB and VEM methods by producing accurate point estimates with lower standard errors. Additionally, the coverage rates for the fully Bayesian method are near the expected 95% level, while the VB method produces posteriors which do not reliably capture the true \u03c0 and \u03c1. The fully Bayesian method also dominates in terms of classification, as the average Rand index for the communication types are given by 0.590, 0.583 and 0.552 for the B, VB and VEM methods, respectively.\nThe superiority of the fully Bayesian method compared to the variational methods is unsurprising in this setting. Although variational methods exhibit strong point estimation for stochastic block models, estimation for PA models with heterogeneous reciprocity is an inherently harder problem. Namely, in a directed stochastic block model, each node has the opportunity to connect to every other node in the network. This results in m(m\u2212 1) many potential edges formmany nodes in the network. For the PA model, one expects the number of potential edges to scale linearly with the number of nodes. Thus, there is inherently less observed information that can be leveraged to learn the latent communication classes. Such lack of information induces a multimodal ELBO, and therefore the variational methods struggle to find a global optimum. The fully Bayesian method is better able to incorporate this uncertainty since it is sampling from, not optimizing, a multimodal posterior.\nTable 6 displays the performance of the model selection criteria for 100 networks generated under the same PA model. For the fully Bayesian method, we initialize at Kinit = 1. Despite the poor performance of the VB method at the parameter estimation, it captures the true K = 2 the most often, indicating that the ELBO is a good model selection criteria. The VEM algorithm always chooses K = 1, though we expect that this is again due to the lack of information in the data. The likelihood associated with \u03c0 has a much larger role in the ICL for PA models than in stochastic block models. This, combined with the poor estimation of the classes for known K, results in the poor performance of the ICL criteria."
        },
        {
            "heading": "6. Data Example",
            "text": "Now we apply the heterogeneous reciprocal PA model to the Facebook wall post data from KONECT analyzed in Viswanath et al. (2009) and Cirkovic et al. (2022a). The Facebook wall post data tracks a group of users in New Orleans and their wallposts from September 9th, 2004 to January 22nd, 2009. The network is temporal: when user u posts to user v\u2019s wall, a directed edge (u, v) is generated and the timestamp of the post is recorded. The full dataset consists of 876,933 wallposts and 46,952 users. In Figure 1, we display the out/indegree of each user in a trimmed version of the network; we postpone the discussion of the data cleaning procedure to the following paragraph. Note that upon first observation, the degree distribution indicates the existence of two populations that exhibit differing reciprocal behavior. The first group, concentrated on the out-degree axis, mostly post on other users\u2019 walls while not receiving any posts on their own. The second group both sends and receives wall posts at a commensurate rate. Further, the marginal out/in-degree distributions exhibit power law tails as indicated by Figure 2 where, on the log-log scale, the empirical tail functions seems to scale linearly with large degrees.\nIn Cirkovic et al. (2022a), the Facebook wall post data was analyzed assuming that each user exhibited homogeneous reciprocal behavior. In the wording of Section 2.1, it\nwas assumed that \u03c0 \u2261 1 and \u03c1 \u2261 \u03c1 \u2208 R. In doing so, the users concentrated on the out-degree axis in Figure 1 were excluded from the analysis as the homogeneous model could not model the observed hetereogenous reciprocal behavior. Additionally, by virtue of extreme value-based methods being sensitive to the choice of seed graph, Cirkovic et al. (2022a) also removed nodes that became inactive as the graph evolved, a phenomena not modeled by the proposed PA model. The likelihood-based methodology in Cirkovic et al. (2022a) returned a homogeneous reciprocity estimate of \u03c1\u0302 = 0.28. The flexibility provided by the heterogeneous reciprocal PA model aims to capture the additional, intricate dynamics underlying the Facebook wall post data not previously considered in Cirkovic et al. (2022a).\nAccording the analysis of the Facebook wallpost data in Viswanath et al. (2009), there observes a sudden uptick in the number of wall posts from July 2008 and onwards. They conjecture that this uptick is likely due to a Facebook redesign, introduced in July, that allowed users to interact with more wall posts through friend feeds. This likely results in a distributional shift in the network\u2019s evolution, and thus we discard the portion of the network observed beyond June, 2008, resulting in a network with 22,286 nodes 165,776 edges. This observation, however, may lead to additional analyses via changepoint detection (see Banerjee et al., 2018; Bhamidi et al., 2018; Cirkovic et al., 2022b, for example). Additionally, the evolution of the PA network specified in Section 2.1 posits that every new edge must attach to at least one node that was previously observed in the network evolution. In order to better adhere to this assumption we define a sequence of networks by first selecting the node with the largest total degree and pairing it with the first node it makes a connection with to create a seed graph G(0). Then, we only retain the edges (u, v) that are (i) observed after the introduction of the seed graph and (ii) u \u2208 V (k\u2212 1) or v \u2208 V (k\u2212 1). This trimming procedure results in a connected network of 16,099 nodes and 123,920 edges that could have realistically been generated by a heterogeneous reciprocal PA model.\nThe reciprocal PA model assumes that reciprocal edges (tk, sk) are generated instantaneously with their parent edge (sk, tk). However in the Facebook wall post network, it is likely that in the time between reciprocated wall posts, wall posts between other users have been generated. Thus, similar to Cirkovic et al. (2022a), we employ window estimators to identify reciprocal edges. That is, if ek = (sk, tk) has a reciprocal counterpart (tk, sk) appear in 24 hours, we attribute the event Rk = 1 to the edge ek, redefine ek := ek \u222a (tk, sk) and drop (tk, sk) from the edgelist. This results in an edgelist that is in alignment with Section 2.1.\nTo conclude our exploratory data analysis, we study the tail behavior of the out/indegrees for the trimmed Facebook network. We employ the minimum distance procedure (Clauset et al., 2009) on the total degrees to obtain a threshold beyond which a powerlaw tail for the in/out-degree can be safely assumed. The minimum distance procedure computes a tail threshold of 51. Note that computing the tail threshold on the total degree implicitly assumes that the out/in-degree tails have the same power-law index. We find this to be a reasonable assumption as indicated by the similarity of the empirical tail functions in Figure 2. In fact, using a threshold of 51, the tail index estimates for the out/in-degrees are 2.212 and 2.231, respectively. Further observation of Figure 1 indicates that, beyond this threshold, there is an extremal dependence structure in the out/in-degree distribution; nodes with total degree larger than 51 tend to cluster around multiple lines through the origin. This extremal dependence structure is further analyzed in Appendix A.\nWe fit the VEM, VB and fully Bayesian methods to the Facebook wall post network. For the VEM algorithm, we terminate the variational E-step when the increase in the ELBO is less than \u03f5 = 0.1 and terminate the overall algorithm once the largest absolute difference in the estimated components of \u03c0 and \u03c1 between M-steps falls below \u03ba = 0.001. For the Bayesian methods, we again assume non-informative priors on \u03c0 and \u03c1. Analogous to the VEM algorithm, we terminate the VB procedure once the change in the ELBO falls below \u03f5 = 0.1. Both the VEM and VB methods are fit for K = 1, . . . , 10. The telescoping sampler for the fully Bayesian method is ran forM = 100,000 MCMC iterates, where the first 90,000 iterates are discarded as burn-in. Within the telescoping sampler, we set Kmax = 20.\nThe global PA parameters \u03b8 are estimated by maximizng the likelihood p(\u00b7 | \u03b8). Maximum likelihood returns (\u03b1\u0302, \u03b2\u0302, \u03b4\u0302in, \u03b4\u0302out) = (0.071, 0.829, 1.756, 1.571). The small values of \u03b4\u0302in and \u03b4\u0302out indicate that preferential attachment is indeed a viable mechanism to describe how users send and receive wall posts. Analyzing the reciprocal component of the model, the VEM algorithm identifies 3 clusters, while the VB and fully Bayesian algorithm identify 6 and 11 clusters, respectively. Figure 3 displays the ICL, ELBO and posterior of K for the VEM, VB and fully Bayesian methods. The ICL criterion clearly identifies K = 3 as the choice that optimally balances model parsimony with fidelity to the data. Though the VB method chooses K = 6, we note that the ELBO for the VB method becomes very flat at K = 4, indicating that perhaps a simpler model may fit the data nearly as well as the model with K = 6 mixture components. We suspect that the fully Bayesian method overfits the number of mixture components due to model misspecification. It is unlikely that the Facebook wallpost data exactly follows the specification in Section 2.1. For example, there is empirical evidence that the degree of each node may influence reciprocal behavior (Cheng et al., 2011). There is strong evidence that mixtures of finite mixtures do not reliably learn\nthe number of mixture components under model misspecification (Cai et al., 2021; Miller and Dunson, 2018).\nThe estimates of \u03c0 and \u03c1 for VEM and VB are\n\u03c0\u0302VEM = 0.5380.251 0.211  , \u03c1\u0302VEM =  0.242 0.273 0.0010.597 0.650 0.001 0.0701 0.053 0.001 \n\u03c0\u0302VB = \n0.122 0.285 0.153 0.060 0.197 0.184\n , \u03c1\u0302VB = \n0.088 0.094 0.084 0.038 0.001 0.083 0.383 0.427 0.431 0.182 0.001 0.375 0.670 0.699 0.718 0.433 0.001 0.641 0.467 0.464 0.499 0.214 0.002 0.437 0.089 0.089 0.059 0.036 0.005 0.082 0.206 0.225 0.230 0.098 0.001 0.201\n (13)\nWe also plot the marginal posteriors for \u03c0 and \u03c1 obtained by the telescoping sampler in Figure 4. Note that all three methods identify a group of nodes that receive nearly no reciprocal edges as indicated by a column of near-zero estimates in \u03c1. Additionally, the telescoping sampler seems to overfit the number of clusters by producing a cluster whose mixture weight, \u03c011, has a posterior mean of 0.0008. Class 11 also has marginal posteriors for \u03c1 that clearly have not yet mixed well. Hence, we caution making inference on node classes that either have a small number of nodes in them, or continually drop in and out of the sampler.\nFigure 5 displays the degree distribution of the trimmed Facebook wall post network, grouped by the VEM cluster estimates. The VEM algorithm clearly identifies cluster 3 as nodes that do not receive reciprocal edges. Despite, cluster 2 having a heavier tail, and clusters 1 and 2 tend to concentrate in similar regions of R+. Further, the similarity of the estimates \u03c1\u0302VEM indicate that classes 1 and 2 engage in similar reciprocal behavior.\nThese visual measures warrant further inspection on the differences between class 1 and 2. Figure 6 displays the discrete time of the last post made by each node in the network that posts more than once. Note that nodes in class 1 are more likely to become inactive in the early period of the network evolution. These inactive nodes were noted by Cirkovic\net al. (2022a) and Viswanath et al. (2009) as well. The lighter tails of class 1 thus can be explained by the relatively short lifetimes of the nodes, as such nodes do not have as long enough time to send and receive wallposts. The VEM algorithm may have picked up on this inactivity by proxy. Though, such observations warrant extension to a preferential attachment model that incorporates nodes that become inactive over time."
        },
        {
            "heading": "7. Conclusion",
            "text": "In this paper, we outline a preferential attachment model with heterogeneous reciprocity, and offer three methods for fitting the model to both simulated and real-world networks. Through simulations, we find that when analyzing networks that have many edges compared to the number of nodes, the variational alternatives offer similar performance to the fully Bayesian method in terms of point estimation at less computational cost. However,\nthe credible intervals generated by the fully Bayesian method more reliably capture the true data-generating parameters. We also compare the ability of each method to select the number of communication classes in heterogeneous reciprocal PA networks. Generally speaking, when the number of edges are again large compared to the number of nodes, all three methods consistently choose the true number of classes, with the fully Bayesian method having a slight tendency to overfit. We then showcase the ability of the heterogeneous reciprocal PA model to capture non-uniform reciprocal behavior across users in the Facebook wallpost network. The proposed model clearly offers the additional flexibility needed to model such data.\nUpon analyzing the Facebook wallpost network, we find that the VEM algorithm uncovered two reciprocal classes that engage in somewhat similar reciprocal behavior, though one of the classes consisted of more inactive users. The propensity of some users to become inactive in a network as it evolves over time is a common feature of many networks, and warrants the extension of the preferential attachment model to account for such behavior. In future work, we will also consider models that allow for users to become inactive as the network grows over time.\nAcknowledgments and Disclosure of Funding\nThe research work was partly supported by NSF Grant DMS-2210735."
        },
        {
            "heading": "Appendix A: Statistical Tools for Multivariate Extremes",
            "text": "Here we detail, in a non-technical fashion, some tools used to analyze data subject to extremal observations. For more rigorous treatments, we refer to the works of Beirlant et al. (2004) and Resnick (2007). A central goal in the study of multivariate extremes is to identify how extremes cluster. In other words, if one or more components of a random vector is large, how likely is that the other components of the random vector will also be large? For PA models with homogeneous reciprocity, Cirkovic et al. (2022a) proved that the extremal out/in-degrees tend to cluster on a line through the origin. With heterogeneous reciprocity, Wang and Resnick (2022b) proved that the model with \u03b2 = 0 generates extreme out/in-degrees that concentrate on multiple lines through the origin.\nAn exploratory tool used to identify where such extremes cluster in R2+ is the angular density, a plot of the angles\n\u0398r \u2261 { Doutv (n)/(D out v (n) +D in v (n)) : v \u2208 V (n), Doutv (n) +Dinv (n) > r } for some large threshold r. Intuitively, if the angular density concentrates mass around some point in (0, 1), then one would expect extremes to cluster on a line through the origin. On the other hand, if the angular density only places mass on the set {0, 1}, then the out/in-degrees are asymptotically independent ; a large in-degree does not necessarily imply a large out-degree, and vice versa. Figure 7 displays the angular density for the Facebook wallpost data analyzed in Section 6.\nWhen the angular density concentrates mass on the set {0.5, 1}, it indicates the existence of two extremal populations: one that has approximately equal in/out-degree, and another that has high out-degree but small in-degree. The threshold for \u0398r was chosen as r = 51 by the minimum distance method applied to the total degrees (Clauset et al., 2009). The minimum distance method chooses a threshold that minimizes the KolmogorovSmirnov distance between a power-law tail and the emprical tail of the observations above\nthe threshold. Note that the angular density is naturally sensitive to the choice of r. If r is chosen too large, some extremal features of the data may be passed over, while if r is chosen too small, the extremal behavior will be corrupted by non-extremal observations.\nWe now have the tools to describe the intialization of the VEM algorithm for a fixed K presented in Section 3.2.2. First, the set \u0398r is constructed via threshold r chosen by the minimum distance method available in R package igraph (Csardi et al., 2006). We then employ K-means on the set \u0398r to determine an initial clustering of nodes. Note that this only clusters nodes with total degree larger than r. This clustering is then used to compute empirical class probabilities (\u03c0\u0302r) K r=1 and empirical reciprocities (\u03c1\u0302m,r) K m,r=1. Note that (\u03c1\u0302m,r) K m,r=1 is computed only on edges that connect nodes which both have total degree larger than r. (\u03c0\u0302r) K r=1 and (\u03c1\u0302m,r) K m,r=1 are thus used as initial parameter values and the the initial (\u03c4w,\u2113)w\u2208V (n),\u2113\u2208{1,...,K} are chosen according to a uniform distribution on the Ksimplex. The full initialization algorithm is given in Algorithm 5.\nAlgorithm 5 Initialization of VEM for heterogeneous reciprocal PA\nInput: Graph G(n), # communication types K Output: Initial variational EM estimates \u03c0\u0302VEM and \u03c1\u0302VEM 1. Compute the tail threshold r according to the minimum distance procedure. 2. Construct the sets\n\u2135r = { v \u2208 V (n) : Doutv (n) +Dinv (n) > r } \u0398r = { Doutv (n)/(D out v (n) +D in v (n)) : v \u2208 \u2135r\n} 3. Employ K-means on \u0398r to form initial communication class estimates W\u0302v for v \u2208 \u2135r 4. Form initial VEM estimates via for m = 1 to K do\n\u03c0\u0302m = 1 |\u2135r| \u2211 v\u2208\u2135r 1{W\u0302v=m}\nfor r = 1 to K do\n\u03c1\u0302m,r =\n\u2211 k:W\u0302sk=r,W\u0302tk=m\n1{Rk=1}\u2223\u2223\u2223{k : W\u0302sk = r, W\u0302tk = m}\u2223\u2223\u2223 end for\nend for"
        },
        {
            "heading": "Appendix B: Sample Derivations for the VEM Algorithm",
            "text": "In this appendix we present some sample derivations for the variational EM algorithm presented in Section 3.2.2. We note that the derivations are very similar to those of Daudin et al. (2008) and Latouche et al. (2012), though we reformulate them in our setting for\nconvenience. The same type of calculations can be employed to derive the variational Bayes algorithm.\nDerivation of the ELBO\nIn this section we derive (12). Recall that we posit the mean-field variational family on the communication types W (n) given by\nq(W (n)) = \u220f\nv\u2208V (n)\nqv(Wv).\nThen, the ELBO is given by\nELBO(q,\u03c0,\u03c1) = Eq [log p (W (n), (ek) n k=1 | \u03c0,\u03c1)]\u2212 Eq [log q(W (n))] .\nFocusing on the first term, recall that the log-likelihood is given by\nlog p (W (n), (ek) n k=1 | \u03c0,\u03c1)\n= n\u2211 k=1 K\u2211 r=1 ( 1{Jk=1}1{Wsk=r} + 1{Jk=3}1{Wtk=r} ) +\nn\u2211 k=1 K\u2211 r=1 K\u2211 m=1 1{Rk=1}1{Wsk=r}1{Wtk=m} log \u03c1m,r\n+ n\u2211 k=1 K\u2211 r=1 K\u2211 m=1 1{Rk=0}1{Wsk=r}1{Wtk=m} log(1\u2212 \u03c1m,r).\nTaking an expectation with respect to q gives that\nEq [log p (W (n), (ek) n k=1 | \u03c0,\u03c1)]\n= n\u2211 k=1 K\u2211 r=1 ( 1{Jk=1}Eq [ 1{Wsk=r} ] + 1{Jk=3}Eq [ 1{Wtk=r} ]) +\nn\u2211 k=1 K\u2211 r=1 K\u2211 m=1 1{Rk=1}Eq [ 1{Wsk=r}1{Wtk=m} ] log \u03c1m,r\n+ n\u2211 k=1 K\u2211 r=1 K\u2211 m=1 1{Rk=0}Eq [ 1{Wsk=r}1{Wtk=m} ] log(1\u2212 \u03c1m,r),\nand employing the mean-field family assumption,\nEq [log p (W (n), (ek) n k=1 | \u03c0,\u03c1)]\n= n\u2211 k=1 K\u2211 r=1 ( 1{Jk=1}Eq [ 1{Wsk=r} ] + 1{Jk=3}Eq [ 1{Wtk=r} ]) +\nn\u2211 k=1 K\u2211 r=1 K\u2211 m=1 1{Rk=1}Eq [ 1{Wsk=r} ] Eq [ 1{Wtk=m} ] log \u03c1m,r\n+ n\u2211\nk=1 K\u2211 r=1 K\u2211 m=1 1{Rk=0}Eq [ 1{Wsk=r} ] Eq [ 1{Wtk=m} ] log(1\u2212 \u03c1m,r)\n= n\u2211 k=1 K\u2211 r=1 ( 1{Jk=1}\u03c4sk,r + 1{Jk=3}\u03c4tk,r ) +\nn\u2211 k=1 K\u2211 r=1 K\u2211 m=1 1{Rk=1}\u03c4sk,r\u03c4tk,m log \u03c1m,r\n+ n\u2211 k=1 K\u2211 r=1 K\u2211 m=1 1{Rk=0}\u03c4sk,r\u03c4tk,m log(1\u2212 \u03c1m,r).\nFinally, the entropy term is given by\nEq [log q(W (n))] =Eq  \u2211 v\u2208V (n) K\u2211 r=1 1{Wv=r} log \u03c4v,r  = \u2211\nv\u2208V (n) K\u2211 r=1 Eq [ 1{Wv=r} ] log \u03c4v,r\n= \u2211\nv\u2208V (n) K\u2211 r=1 \u03c4v,r log \u03c4v,r.\nDerivation of the E-Step\nIn this section we derive the E-step of the variational EM algorithm (Step 1 of Algorithm 3). Recall that the E-step maximizes the ELBO with respect to the variational density q. We perform this optimization with coordinate ascent. From Blei et al. (2017), for every \u2113 = 1, . . . ,K, the optimal qw(Ww) satisfies\n\u03c4w,\u2113 = q \u22c6 w (Ww = \u2113) \u221d exp { Eq\u2212w [log p (Ww = \u2113 | (Wv)v \u0338=w, (ek)nk=1,\u03c0,\u03c1)] } ,\nwhich, by definition of conditional density, is proportional to\n\u221d exp { Eq\u2212w [log p (Ww = \u2113, (Wv)v \u0338=w, (ek) n k=1 | \u03c0,\u03c1)] } .\nHere, q\u2212w denotes the variational density on (Wv)v \u0338=w. Up to some constant C not depending on w, the log-likelihood term is given by\nlog p (Ww = \u2113, (Wv)v \u0338=w, (ek) n k=1 | \u03c0,\u03c1) = log \u03c0\u2113 + K\u2211 m=1 log \u03c1m,\u2113 \u2211\nk:sk=w\n1{Wtk=m}1{Rk=1}\n+ K\u2211\nm=1\nlog(1\u2212 \u03c1m,\u2113) \u2211\nk:sk=w\n1{Wtk=m}1{Rk=0}\n+ K\u2211 r=1 log \u03c1\u2113,r \u2211\nk:tk=w\n1{Wsk=r}1{Rk=1}\n+ K\u2211 r=1 log(1\u2212 \u03c1\u2113,r) \u2211\nk:tk=w\n1{Wsk=r}1{Rk=0} + C,\nand taking an expectation with respect to q\u2212w gives\nlog p (Ww = \u2113, (Wv)v \u0338=w, (ek) n k=1 | \u03c0,\u03c1) = log \u03c0\u2113 + K\u2211 m=1 log \u03c1m,\u2113 \u2211\nk:sk=w\n\u03c4tk,m1{Rk=1}\n+ K\u2211\nm=1\nlog(1\u2212 \u03c1m,\u2113) \u2211\nk:sk=w\n\u03c4tk,m1{Rk=0}\n+ K\u2211 r=1 log \u03c1\u2113,r \u2211\nk:tk=w\n\u03c4sk,r1{Rk=1}\n+ K\u2211 r=1 log(1\u2212 \u03c1\u2113,r) \u2211\nk:tk=w\n\u03c4sk,r1{Rk=0} + C.\nHence,\n\u03c4w,\u2113 = q \u22c6 w (Ww = \u2113) \u221d exp { Eq\u2212w [log p (Ww = \u2113, (Wv)v \u0338=w, (ek) n k=1 | \u03c0,\u03c1)] } \u221d\u03c0\u2113\nK\u220f m=1 \u03c1 \u2211 k:sk=w \u03c4tk,m1{Rk=1} m,\u2113 (1\u2212 \u03c1m,\u2113) \u2211 k:sk=w \u03c4tk,m1{Rk=0}\n\u00d7 K\u220f r=1 \u03c1 \u2211 k:tk=w \u03c4sk,r1{Rk=1} \u2113,r (1\u2212 \u03c1\u2113,r) \u2211 k:tk=w \u03c4sk,r1{Rk=0} .\n(14)\nThus, in the E-step, one cycles through (14) for each w \u2208 V (n) until the ELBO no longer meaningfully increases."
        }
    ],
    "title": "Modeling Random Networks with Heterogeneous Reciprocity",
    "year": 2023
}