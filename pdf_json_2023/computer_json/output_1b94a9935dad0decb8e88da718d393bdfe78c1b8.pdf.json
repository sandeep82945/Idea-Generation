{
    "abstractText": "LiDAR-based 3D point cloud recognition has benefited various applications. Without specially considering the LiDAR point distribution, most current methods suffer from information disconnection and limited receptive field, especially for the sparse distant points. In this work, we study the varying-sparsity distribution of LiDAR points and present SphereFormer to directly aggregate information from dense close points to the sparse distant ones. We design radial window self-attention that partitions the space into multiple non-overlapping narrow and long windows. It overcomes the disconnection issue and enlarges the receptive field smoothly and dramatically, which significantly boosts the performance of sparse distant points. Moreover, to fit the narrow and long windows, we propose exponential splitting to yield fine-grained position encoding and dynamic feature selection to increase model representation ability. Notably, our method ranks 1st on both nuScenes and SemanticKITTI semantic segmentation benchmarks with 81.9% and 74.8% mIoU, respectively. Also, we achieve the 3rd place on nuScenes object detection benchmark with 72.8% NDS and 68.5% mAP. Code is available at https: //github.com/dvlab-research/SphereFormer.git.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xin Lai"
        },
        {
            "affiliations": [],
            "name": "Yukang Chen"
        },
        {
            "affiliations": [],
            "name": "Fanbin Lu"
        },
        {
            "affiliations": [],
            "name": "Jianhui Liu"
        },
        {
            "affiliations": [],
            "name": "Jiaya Jia"
        }
    ],
    "id": "SP:2f87af0988f56b8985bf5df918044794613e77c2",
    "references": [
        {
            "authors": [
                "Inigo Alonso",
                "Luis Riazuelo",
                "Luis Montesano",
                "Ana C Murillo"
            ],
            "title": "3d-mininet: Learning a 2d representation from point clouds for fast and efficient 3d lidar semantic segmentation",
            "venue": "IEEE Robotics and Automation Letters,",
            "year": 2020
        },
        {
            "authors": [
                "Xuyang Bai",
                "Zeyu Hu",
                "Xinge Zhu",
                "Qingqiu Huang",
                "Yilun Chen",
                "Hongbo Fu",
                "Chiew-Lan Tai"
            ],
            "title": "Transfusion: Robust lidar-camera fusion for 3d object detection with transformers",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Jens Behley",
                "Martin Garbade",
                "Andres Milioto",
                "Jan Quenzel",
                "Sven Behnke",
                "Cyrill Stachniss",
                "Jurgen Gall"
            ],
            "title": "Semantickitti: A dataset for semantic scene understanding of lidar sequences",
            "venue": "In ICCV,",
            "year": 2019
        },
        {
            "authors": [
                "Holger Caesar",
                "Varun Bankiti",
                "Alex H Lang",
                "Sourabh Vora",
                "Venice Erin Liong",
                "Qiang Xu",
                "Anush Krishnan",
                "Yu Pan",
                "Giancarlo Baldan",
                "Oscar Beijbom"
            ],
            "title": "nuscenes: A multimodal dataset for autonomous driving",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Nicolas Carion",
                "Francisco Massa",
                "Gabriel Synnaeve",
                "Nicolas Usunier",
                "Alexander Kirillov",
                "Sergey Zagoruyko"
            ],
            "title": "End-toend object detection with transformers",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Liang-Chieh Chen",
                "Yukun Zhu",
                "George Papandreou",
                "Florian Schroff",
                "Hartwig Adam"
            ],
            "title": "Encoder-decoder with atrous separable convolution for semantic image segmentation",
            "venue": "In ECCV,",
            "year": 2018
        },
        {
            "authors": [
                "Qi Chen",
                "Lin Sun",
                "Ernest Cheung",
                "Alan L Yuille"
            ],
            "title": "Every view counts: Cross-view consistency in 3d object detection with hybrid-cylindrical-spherical voxelization",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Qi Chen",
                "Lin Sun",
                "Zhixin Wang",
                "Kui Jia",
                "Alan Yuille"
            ],
            "title": "Object as hotspots: An anchor-free 3d object detection approach via firing of hotspots",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Yukang Chen",
                "Yanwei Li",
                "Xiangyu Zhang",
                "Jian Sun",
                "Jiaya Jia"
            ],
            "title": "Focal sparse convolutional networks for 3d object detection",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Yukang Chen",
                "Jianhui Liu",
                "Xiaojuan Qi",
                "Xiangyu Zhang",
                "Jian Sun",
                "Jiaya Jia"
            ],
            "title": "Scaling up kernels in 3d cnns",
            "year": 2022
        },
        {
            "authors": [
                "Yukang Chen",
                "Jianhui Liu",
                "Xiangyu Zhang",
                "Xiaojuan Qi",
                "Jiaya Jia"
            ],
            "title": "Fully sparse voxelnet for 3d object detection and tracking",
            "venue": "In CVPR,",
            "year": 2023
        },
        {
            "authors": [
                "Ran Cheng",
                "Ryan Razani",
                "Ehsan Taghavi",
                "Enxu Li",
                "Bingbing Liu"
            ],
            "title": "2-s3net: Attentive feature fusion with adaptive feature selection for sparse semantic segmentation network",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Christopher Choy",
                "JunYoung Gwak",
                "Silvio Savarese"
            ],
            "title": "4d spatio-temporal convnets: Minkowski convolutional neural networks",
            "venue": "In CVPR, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Ruihang Chu",
                "Yukang Chen",
                "Tao Kong",
                "Lu Qi",
                "Lei Li"
            ],
            "title": "Icm-3d: Instantiated category modeling for 3d instance segmentation",
            "venue": "IEEE RAL,",
            "year": 2021
        },
        {
            "authors": [
                "Ruihang Chu",
                "Xiaoqing Ye",
                "Zhengzhe Liu",
                "Xiao Tan",
                "Xiaojuan Qi",
                "Chi-Wing Fu",
                "Jiaya Jia"
            ],
            "title": "Twist: Two-way inter-label self-training for semi-supervised 3d instance segmentation",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Xiangxiang Chu",
                "Zhi Tian",
                "Yuqing Wang",
                "Bo Zhang",
                "Haibing Ren",
                "Xiaolin Wei",
                "Huaxia Xia",
                "Chunhua Shen"
            ],
            "title": "Twins: Revisiting the design of spatial attention in vision transformers",
            "year": 2021
        },
        {
            "authors": [
                "Tiago Cortinhal",
                "George Tzelepis",
                "Eren Erdal Aksoy"
            ],
            "title": "Salsanext: Fast, uncertainty-aware semantic segmentation of lidar point clouds",
            "venue": "In International Symposium on Visual Computing,",
            "year": 2020
        },
        {
            "authors": [
                "Jiajun Deng",
                "Shaoshuai Shi",
                "Peiwei Li",
                "Wengang Zhou",
                "Yanyong Zhang",
                "Houqiang Li"
            ],
            "title": "Voxel R-CNN: towards high performance voxel-based 3d object detection",
            "venue": "In AAAI,",
            "year": 2021
        },
        {
            "authors": [
                "Xiaoyi Dong",
                "Jianmin Bao",
                "Dongdong Chen",
                "Weiming Zhang",
                "Nenghai Yu",
                "Lu Yuan",
                "Dong Chen",
                "Baining Guo"
            ],
            "title": "Cswin transformer: A general vision transformer backbone with cross-shaped windows",
            "year": 2021
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly",
                "Jakob Uszkoreit",
                "Neil Houlsby"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition",
            "year": 2021
        },
        {
            "authors": [
                "Lue Fan",
                "Ziqi Pang",
                "Tianyuan Zhang",
                "Yu-Xiong Wang",
                "Hang Zhao",
                "Feng Wang",
                "Naiyan Wang",
                "Zhaoxiang Zhang"
            ],
            "title": "Embracing Single Stride 3D Object Detector with Sparse Transformer",
            "venue": "In CVPR, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Kyle Genova",
                "Xiaoqi Yin",
                "Abhijit Kundu",
                "Caroline Pantofaru",
                "Forrester Cole",
                "Avneesh Sud",
                "Brian Brewington",
                "Brian Shucker",
                "Thomas Funkhouser"
            ],
            "title": "Learning 3d semantic segmentation with only 2d image supervision",
            "venue": "In 3DV,",
            "year": 2021
        },
        {
            "authors": [
                "Benjamin Graham",
                "Martin Engelcke",
                "Laurens van der Maaten"
            ],
            "title": "3d semantic segmentation with submanifold sparse convolutional networks",
            "venue": "In CVPR, 2018",
            "year": 2018
        },
        {
            "authors": [
                "Benjamin Graham",
                "Laurens van der Maaten"
            ],
            "title": "Submanifold sparse convolutional networks",
            "year": 2017
        },
        {
            "authors": [
                "Chenhang He",
                "Hui Zeng",
                "Jianqiang Huang",
                "Xian-Sheng Hua",
                "Lei Zhang"
            ],
            "title": "Structure aware single-stage 3d object detection from point cloud",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Yuenan Hou",
                "Xinge Zhu",
                "Yuexin Ma",
                "Chen Change Loy",
                "Yikang Li"
            ],
            "title": "Point-to-voxel knowledge distillation for lidar semantic segmentation",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Qingyong Hu",
                "Bo Yang",
                "Linhai Xie",
                "Stefano Rosa",
                "Yulan Guo",
                "Zhihua Wang",
                "Niki Trigoni",
                "Andrew Markham"
            ],
            "title": "Randla-net: Efficient semantic segmentation of large-scale point clouds",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Li Jiang",
                "Shaoshuai Shi",
                "Zhuotao Tian",
                "Xin Lai",
                "Shu Liu",
                "Chi-Wing Fu",
                "Jiaya Jia"
            ],
            "title": "Guided point contrastive learning for semi-supervised point cloud semantic segmentation",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Xin Lai",
                "Jianhui Liu",
                "Li Jiang",
                "Liwei Wang",
                "Hengshuang Zhao",
                "Shu Liu",
                "Xiaojuan Qi",
                "Jiaya Jia"
            ],
            "title": "Stratified transformer for 3d point cloud segmentation",
            "venue": "In CVPR, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Xin Lai",
                "Zhuotao Tian",
                "Li Jiang",
                "Shu Liu",
                "Hengshuang Zhao",
                "Liwei Wang",
                "Jiaya Jia"
            ],
            "title": "Semi-supervised semantic segmentation with directional context-aware consistency",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Xin Lai",
                "Zhuotao Tian",
                "Xiaogang Xu",
                "Yingcong Chen",
                "Shu Liu",
                "Hengshuang Zhao",
                "Liwei Wang",
                "Jiaya Jia"
            ],
            "title": "Decouplenet: Decoupled network for domain adaptive semantic segmentation",
            "venue": "In ECCV,",
            "year": 2022
        },
        {
            "authors": [
                "Alex H Lang",
                "Sourabh Vora",
                "Holger Caesar",
                "Lubing Zhou",
                "Jiong Yang",
                "Oscar Beijbom"
            ],
            "title": "Pointpillars: Fast encoders for object detection from point clouds",
            "venue": "In CVPR,",
            "year": 2019
        },
        {
            "authors": [
                "Yiming Li",
                "Tao Kong",
                "Ruihang Chu",
                "Yifeng Li",
                "Peng Wang",
                "Lei Li"
            ],
            "title": "Simultaneous semantic and collision learning for 6-dof grasp pose estimation",
            "venue": "In IROS. IEEE,",
            "year": 2021
        },
        {
            "authors": [
                "Venice Erin Liong",
                "Thi Ngoc Tho Nguyen",
                "Sergi Widjaja",
                "Dhananjai Sharma",
                "Zhuang Jie Chong"
            ],
            "title": "Amvnet: Assertion-based multi-view fusion network for lidar semantic segmentation",
            "year": 2012
        },
        {
            "authors": [
                "Jianhui Liu",
                "Yukang Chen",
                "Xiaoqing Ye",
                "Zhuotao Tian",
                "Xiao Tan",
                "Xiaojuan Qi"
            ],
            "title": "Spatial pruned sparse convolution for efficient 3d object detection",
            "venue": "In NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Minghua Liu",
                "Yin Zhou",
                "Charles R Qi",
                "Boqing Gong",
                "Hao Su",
                "Dragomir Anguelov"
            ],
            "title": "Less: Label-efficient semantic segmentation for lidar point clouds",
            "venue": "In ECCV,",
            "year": 2022
        },
        {
            "authors": [
                "Ze Liu",
                "Yutong Lin",
                "Yue Cao",
                "Han Hu",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Baining Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "year": 2021
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "year": 2017
        },
        {
            "authors": [
                "Wenjie Luo",
                "Yujia Li",
                "Raquel Urtasun",
                "Richard Zemel"
            ],
            "title": "Understanding the effective receptive field in deep convolutional neural networks",
            "venue": "In NeurIPS,",
            "year": 2016
        },
        {
            "authors": [
                "Jiageng Mao",
                "Minzhe Niu",
                "Haoyue Bai",
                "Xiaodan Liang",
                "Hang Xu",
                "Chunjing Xu"
            ],
            "title": "Pyramid R-CNN: towards better performance and adaptability for 3d object detection",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Jiageng Mao",
                "Yujing Xue",
                "Minzhe Niu",
                "Haoyue Bai",
                "Jiashi Feng",
                "Xiaodan Liang",
                "Hang Xu",
                "Chunjing Xu"
            ],
            "title": "Voxel transformer for 3d object detection",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Andres Milioto",
                "Ignacio Vizzo",
                "Jens Behley",
                "Cyrill Stachniss"
            ],
            "title": "Rangenet++: Fast and accurate lidar semantic segmentation",
            "venue": "IEEE/RSJ international conference on intelligent robots and systems (IROS),",
            "year": 2019
        },
        {
            "authors": [
                "Charles R Qi",
                "Hao Su",
                "Kaichun Mo",
                "Leonidas J Guibas"
            ],
            "title": "Pointnet: Deep learning on point sets for 3d classification and segmentation",
            "venue": "In CVPR,",
            "year": 2017
        },
        {
            "authors": [
                "Charles Ruizhongtai Qi",
                "Li Yi",
                "Hao Su",
                "Leonidas J Guibas"
            ],
            "title": "Pointnet++: Deep hierarchical feature learning on point sets in a metric space",
            "venue": "NeurIPS, 2017",
            "year": 2017
        },
        {
            "authors": [
                "Ryan Razani",
                "Ran Cheng",
                "Ehsan Taghavi",
                "Liu Bingbing"
            ],
            "title": "Lite-hdseg: Lidar semantic segmentation using lite harmonic dense convolutions",
            "venue": "In ICRA,",
            "year": 2021
        },
        {
            "authors": [
                "Shaoqing Ren",
                "Kaiming He",
                "Ross B. Girshick",
                "Jian Sun"
            ],
            "title": "Faster R-CNN: towards real-time object detection with region proposal networks",
            "venue": "In NeurIPS,",
            "year": 2015
        },
        {
            "authors": [
                "Damien Robert",
                "Bruno Vallet",
                "Loic Landrieu"
            ],
            "title": "Learning multi-view aggregation in the wild for large-scale 3d semantic segmentation",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Olaf Ronneberger",
                "Philipp Fischer",
                "Thomas Brox"
            ],
            "title": "U-net: Convolutional networks for biomedical image segmentation",
            "venue": "In MICCAI,",
            "year": 2015
        },
        {
            "authors": [
                "Shaoshuai Shi",
                "Chaoxu Guo",
                "Li Jiang",
                "Zhe Wang",
                "Jianping Shi",
                "Xiaogang Wang",
                "Hongsheng Li"
            ],
            "title": "PV-RCNN: pointvoxel feature set abstraction for 3d object detection",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Shaoshuai Shi",
                "Xiaogang Wang",
                "Hongsheng Li"
            ],
            "title": "Pointrcnn: 3d object proposal generation and detection from point cloud",
            "venue": "In CVPR,",
            "year": 2019
        },
        {
            "authors": [
                "Pei Sun",
                "Henrik Kretzschmar",
                "Xerxes Dotiwalla",
                "Aurelien Chouard",
                "Vijaysai Patnaik",
                "Paul Tsui",
                "James Guo",
                "Yin Zhou",
                "Yuning Chai",
                "Benjamin Caine"
            ],
            "title": "Scalability in perception for autonomous driving: Waymo open dataset",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Pei Sun",
                "Mingxing Tan",
                "Weiyue Wang",
                "Chenxi Liu",
                "Fei Xia",
                "Zhaoqi Leng",
                "Dragomir Anguelov"
            ],
            "title": "Swformer: Sparse window transformer for 3d object detection in point clouds",
            "year": 2022
        },
        {
            "authors": [
                "Shuyang Sun",
                "Xiaoyu Yue",
                "Song Bai",
                "Philip Torr"
            ],
            "title": "Visual parser: Representing part-whole hierarchies with transformers",
            "year": 2021
        },
        {
            "authors": [
                "Haotian Tang",
                "Zhijian Liu",
                "Shengyu Zhao",
                "Yujun Lin",
                "Ji Lin",
                "Hanrui Wang",
                "Song Han"
            ],
            "title": "Searching efficient 3d architectures with sparse point-voxel convolution",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Maxim Tatarchenko",
                "Jaesik Park",
                "Vladlen Koltun",
                "Qian- Yi Zhou"
            ],
            "title": "Tangent convolutions for dense prediction in 3d",
            "venue": "In CVPR,",
            "year": 2018
        },
        {
            "authors": [
                "Hugues Thomas",
                "Charles R Qi",
                "Jean-Emmanuel Deschaud",
                "Beatriz Marcotegui",
                "Fran\u00e7ois Goulette",
                "Leonidas J Guibas"
            ],
            "title": "Kpconv: Flexible and deformable convolution for point clouds",
            "venue": "In ICCV,",
            "year": 2019
        },
        {
            "authors": [
                "Zhuotao Tian",
                "Pengguang Chen",
                "Xin Lai",
                "Li Jiang",
                "Shu Liu",
                "Hengshuang Zhao",
                "Bei Yu",
                "Ming-Chang Yang",
                "Jiaya Jia"
            ],
            "title": "Adaptive perspective distillation for semantic segmentation. T-PAMI, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Zhuotao Tian",
                "Jiequan Cui",
                "Li Jiang",
                "Xiaojuan Qi",
                "Xin Lai",
                "Yixin Chen",
                "Shu Liu",
                "Jiaya Jia"
            ],
            "title": "Learning context-aware classifier for semantic segmentation",
            "year": 2023
        },
        {
            "authors": [
                "Zhuotao Tian",
                "Xin Lai",
                "Li Jiang",
                "Shu Liu",
                "Michelle Shu",
                "Hengshuang Zhao",
                "Jiaya Jia"
            ],
            "title": "Generalized few-shot semantic segmentation",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Hugo Touvron",
                "Matthieu Cord",
                "Matthijs Douze",
                "Francisco Massa",
                "Alexandre Sablayrolles",
                "Herve Jegou"
            ],
            "title": "Training data-efficient image transformers & distillation through attention",
            "venue": "In ICML,",
            "year": 2021
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "In NeurIPS,",
            "year": 2017
        },
        {
            "authors": [
                "Wenhai Wang",
                "Enze Xie",
                "Xiang Li",
                "Deng-Ping Fan",
                "Kaitao Song",
                "Ding Liang",
                "Tong Lu",
                "Ping Luo",
                "Ling Shao"
            ],
            "title": "Pvtv2: Improved baselines with pyramid vision transformer",
            "year": 2021
        },
        {
            "authors": [
                "Wenhai Wang",
                "Enze Xie",
                "Xiang Li",
                "Deng-Ping Fan",
                "Kaitao Song",
                "Ding Liang",
                "Tong Lu",
                "Ping Luo",
                "Ling Shao"
            ],
            "title": "Pyramid vision transformer: A versatile backbone for dense prediction without convolutions",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Bichen Wu",
                "Xuanyu Zhou",
                "Sicheng Zhao",
                "Xiangyu Yue",
                "Kurt Keutzer"
            ],
            "title": "Squeezesegv2: Improved model structure and unsupervised domain adaptation for road-object segmentation from a lidar point cloud",
            "venue": "In ICRA,",
            "year": 2019
        },
        {
            "authors": [
                "Chenfeng Xu",
                "Bichen Wu",
                "Zining Wang",
                "Wei Zhan",
                "Peter Vajda",
                "Kurt Keutzer",
                "Masayoshi Tomizuka"
            ],
            "title": "Squeezesegv3: Spatially-adaptive convolution for efficient pointcloud segmentation",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Jianyun Xu",
                "Ruixiang Zhang",
                "Jian Dou",
                "Yushi Zhu",
                "Jie Sun",
                "Shiliang Pu"
            ],
            "title": "Rpvnet: A deep and efficient range-pointvoxel fusion network for lidar point cloud segmentation",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Xu Yan",
                "Jiantao Gao",
                "Jie Li",
                "Ruimao Zhang",
                "Zhen Li",
                "Rui Huang",
                "Shuguang Cui"
            ],
            "title": "Sparse single sweep lidar point cloud segmentation via learning contextual shape priors from scene completion",
            "venue": "In AAAI,",
            "year": 2021
        },
        {
            "authors": [
                "Xu Yan",
                "Jiantao Gao",
                "Chaoda Zheng",
                "Chao Zheng",
                "Ruimao Zhang",
                "Shuguang Cui",
                "Zhen Li"
            ],
            "title": "2dpass: 2d priors assisted semantic segmentation on lidar point clouds",
            "venue": "In ECCV, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Xu Yan",
                "Chaoda Zheng",
                "Zhen Li",
                "Sheng Wang",
                "Shuguang Cui"
            ],
            "title": "Pointasnl: Robust point clouds processing using nonlocal neural networks with adaptive sampling",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Yan Yan",
                "Yuxing Mao",
                "Bo Li"
            ],
            "title": "SECOND: sparsely embedded convolutional detection",
            "year": 2018
        },
        {
            "authors": [
                "Jianwei Yang",
                "Chunyuan Li",
                "Pengchuan Zhang",
                "Xiyang Dai",
                "Bin Xiao",
                "Lu Yuan",
                "Jianfeng Gao"
            ],
            "title": "Focal selfattention for local-global interactions in vision transformers",
            "venue": "In NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Zetong Yang",
                "Yanan Sun",
                "Shu Liu",
                "Jiaya Jia"
            ],
            "title": "3dssd: Point-based 3d single stage object detector",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Zetong Yang",
                "Yanan Sun",
                "Shu Liu",
                "Jiaya Jia"
            ],
            "title": "3dssd: Point-based 3d single stage object detector",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Dongqiangzi Ye",
                "Zixiang Zhou",
                "Weijia Chen",
                "Yufei Xie",
                "Yu Wang",
                "Panqu Wang",
                "Hassan Foroosh"
            ],
            "title": "Lidarmultinet: Towards a unified multi-task network for lidar perception",
            "year": 2022
        },
        {
            "authors": [
                "Tianwei Yin",
                "Xingyi Zhou",
                "Philipp Krahenbuhl"
            ],
            "title": "Centerbased 3d object detection and tracking",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Yang Zhang",
                "Zixiang Zhou",
                "Philip David",
                "Xiangyu Yue",
                "Zerong Xi",
                "Boqing Gong",
                "Hassan Foroosh"
            ],
            "title": "Polarnet: An improved grid representation for online lidar point clouds semantic segmentation",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Hengshuang Zhao",
                "Jiaya Jia",
                "Vladlen Koltun"
            ],
            "title": "Exploring self-attention for image recognition",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Hengshuang Zhao",
                "Jianping Shi",
                "Xiaojuan Qi",
                "Xiaogang Wang",
                "Jiaya Jia"
            ],
            "title": "Pyramid scene parsing network",
            "venue": "In CVPR,",
            "year": 2017
        },
        {
            "authors": [
                "Wu Zheng",
                "Weiliang Tang",
                "Sijin Chen",
                "Li Jiang",
                "Chi- Wing Fu"
            ],
            "title": "CIA-SSD: confident iou-aware single-stage object detector from point cloud",
            "venue": "In AAAI,",
            "year": 2021
        },
        {
            "authors": [
                "Wu Zheng",
                "Weiliang Tang",
                "Li Jiang",
                "Chi-Wing Fu. SE"
            ],
            "title": "SSD: self-ensembling single-stage object detector from point cloud",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Yin Zhou",
                "Oncel Tuzel"
            ],
            "title": "Voxelnet: End-to-end learning for point cloud based 3d object detection",
            "venue": "In CVPR,",
            "year": 2018
        },
        {
            "authors": [
                "Benjin Zhu",
                "Zhengkai Jiang",
                "Xiangxin Zhou",
                "Zeming Li",
                "Gang Yu"
            ],
            "title": "Class-balanced grouping and sampling for point cloud 3d object detection",
            "year": 1908
        },
        {
            "authors": [
                "Xizhou Zhu",
                "Weijie Su",
                "Lewei Lu",
                "Bin Li",
                "Xiaogang Wang",
                "Jifeng Dai"
            ],
            "title": "Deformable detr: Deformable transformers for end-to-end object detection",
            "venue": "In ICLR,",
            "year": 2020
        },
        {
            "authors": [
                "Xinge Zhu",
                "Hui Zhou",
                "Tai Wang",
                "Fangzhou Hong",
                "Yuexin Ma",
                "Wei Li",
                "Hongsheng Li",
                "Dahua Lin"
            ],
            "title": "Cylindrical and asymmetrical 3d convolution networks for lidar segmentation",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Zhuangwei Zhuang",
                "Rong Li",
                "Kui Jia",
                "Qicheng Wang",
                "Yuanqing Li",
                "Mingkui Tan"
            ],
            "title": "Perception-aware multi-sensor fusion for 3d lidar semantic segmentation",
            "venue": "In ICCV,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Nowadays, point clouds can be easily collected by LiDAR sensors. They are extensively used in various industrial applications, such as autonomous driving and robotics. In contrast to 2D images where pixels are arranged densely and regularly, LiDAR point clouds possess the varyingsparsity property \u2014 points near the LiDAR are quite dense, while points far away from the sensor are much sparser, as shown in Fig. 2 (a).\nHowever, most existing work [12, 13, 24, 25, 55, 69\u201371] does not specially consider the the varying-sparsity point distribution of outdoor LiDAR point clouds. They inherit from 2D CNNs or 3D indoor scenarios, and conduct local operators (e.g., SparseConv [24, 25]) uniformly for all locations. This causes inferior results for the sparse distant points. As shown in Fig. 1, although decent performance\n(%) mIoU\nis yielded for the dense close points, it is difficult for these methods to deal with the sparse distant points optimally.\nWe note that the root cause lies in limited receptive field. For sparse distant points, there are few surrounding neighbors. This not only results in inconclusive features, but also hinders enlarging receptive field due to information disconnection. To verify this finding, we visualize the Effective Receptive Field (ERF) [40] of the given feature (shown with the yellow star) in Fig. 2 (d). The ERF cannot be expanded due to disconnection, which is caused by the extreme sparsity of the distant car.\nAlthough window self-attention [22, 30], dilated selfattention [42], and large-kernel CNN [10] have been proposed to conquer the limited receptive field, these methods do not specially deal with LiDAR point distribution, and remain to enlarge receptive field by stacking local operators as before, leaving the information disconnection issue still unsolved. As shown in Fig. 1, the method of cubic selfattention brings a limited improvement.\nIn this paper, we take a new direction to aggregate longrange information directly in a single operator to suit the varying-sparsity point distribution. We propose the module of SphereFormer to perceive useful information from points\nar X\niv :2\n30 3.\n12 76\n6v 1\n[ cs\n.C V\n] 2\n2 M\nar 2\n02 3\n50+ meters away and yield large receptive field for feature extraction. Specifically, we represent the 3D space using spherical coordinates (r, \u03b8, \u03d5) with the sensor being the origin, and partition the scene into multiple non-overlapping windows. Unlike the cubic window shape, we design radial windows that are long and narrow. They are obtained by partitioning only along the \u03b8 and \u03d5 axis, as shown in Fig. 2 (b). It is noteworthy that we make it a plugin module to conveniently insert into existing mainstream backbones.\nThe proposed module does not rely on stacking local operators to expand receptive field, thus avoiding the disconnection issue, as shown in Fig. 2 (e). Also, it facilitates the sparse distant points to aggregate information from the dense-point region, which is often semantically rich. So, the performance of the distant points can be improved significantly (i.e., +17.1% mIoU) as illustrated in Fig. 1.\nMoreover, to fit the long and narrow radial windows, we propose exponential splitting to obtain fine-grained relative position encoding. The radius r of a radial window can be over 50 meters, which causes large splitting intervals. It thus results in coarse position encoding when converting relative positions into integer indices. Besides, to let points at varying locations treat local and global information differently, we propose dynamic feature selection to make further improvements.\nIn total, our contribution is three-fold.\n\u2022 We propose SphereFormer to directly aggregate longrange information from dense-point region. It increases the receptive field smoothly and helps improve\nthe performance of sparse distant points.\n\u2022 To accommodate the radial windows, we develop exponential splitting for relative position encoding. Our dynamic feature selection further boosts performance.\n\u2022 Our method achieves new state-of-the-art results on multiple benchmarks of both semantic segmentation and object detection tasks."
        },
        {
            "heading": "2. Related Work",
            "text": ""
        },
        {
            "heading": "2.1. LiDAR-based 3D Recognition",
            "text": "Semantic Segmentation. Segmentation [6, 14, 15, 31, 32, 34, 49, 59\u201361, 82] is a fundamental task for vision perception. Approaches for LiDAR-based semantic segmentation can be roughly grouped into three categories, i.e., view-based, point-based, and voxel-based methods. Viewbased methods either transform the LiDAR point cloud into a range view [3, 43, 46, 67, 68], or use a bird-eye view (BEV) [79] for a 2D network to perform feature extraction. 3D geometric information is simplified.\nPoint-based methods [28, 30, 44, 45, 56, 58, 72] adopt the point features and positions as inputs, and design abundant operators to aggregate information from neighbors. Moreover, the voxel-based solutions [13, 24, 25] divide the 3D space into regular voxels and then apply sparse convolutions. Further, methods of [12,17,29,37,55,70,88] propose various structures for improved effectiveness. All of them focus on capturing local information. We follow this line\nof research, and propose to directly aggregate long-range information.\nRecently, RPVNet [69] combines the three modalities by feature fusion. Furthermore, 2DPASS [71] incorporates 2D images during training, and [48] fuses multi-modal features. Despite extra 2D information, the performance of these methods still lags behind compared to ours.\nObject Detection. 3D object detection frameworks can be roughly categorized into single-stage [11, 26, 36, 75, 83, 84] and two-stage [19, 41, 50, 51] methods. VoxelNet [85] extracts voxel features by PointNet [44] and applies RPN [47] to obtain the proposals. SECOND [73] is efficient thanks to the accelerated sparse convolutions. VoTr [42] applies cubic window attention to voxels. LiDARMultiNet [77] unifies semantic segmentation, panoptic segmentation, and object detection into a single multitask network with multiple types of supervision. Our experiments are based on CenterPoint [78], which is a widely used anchor-free framework. It is effective and efficient. We aim to enhance the features of sparse distant points, and our proposed module can be conveniently inserted into existing frameworks."
        },
        {
            "heading": "2.2. Vision Transformer",
            "text": "Recently, Transformer [64] become popular in various 2D image understanding tasks [5, 16, 20, 21, 38, 42, 54, 62, 63,65,66,74,80,87]. ViT [21] tokenizes every image patch and adopts a Transformer encoder to extract features. Further, PVT [66] presents a hierarchical structure to obtain a feature pyramid for dense prediction. It also proposes Spatial Reduction Attention to save memory. Also, Swin Transformer [38] uses window-based attention and proposes the shifted window operation in the successive Transformer block. Moreover, methods of [16, 20, 74] propose different designs to incorporate long-range dependencies. There are also methods [22, 30, 42, 53, 81] that apply Transformer into 3D vision. Few of them consider the point distribution of LiDAR point cloud. In our work, we utilize the varyingsparsity property, and design radial window self-attention to capture long-range information, especially for the sparse distant points."
        },
        {
            "heading": "3. Our Method",
            "text": "In this section, we first elaborate on radial window partition in Sec. 3.1. Then, we propose the improved position encoding and dynamic feature selection in Sec. 3.2 and 3.3."
        },
        {
            "heading": "3.1. Spherical Transformer",
            "text": "To model the long-range dependency, we adopt the window-attention [38] paradigm. However, unlike the cubic window attention [22, 30, 42], we take advantage of the\nvarying-sparsity property of LiDAR point cloud and present the SphereFormer module, as shown in Fig. 3.\nRadial Window Partition. Specifically, we represent LiDAR point clouds using the spherical coordinate system (r, \u03b8, \u03d5) with the LiDAR sensor being the origin. We partition the 3D space along the \u03b8 and \u03d5 axis. We, thus, obtain a number of non-overlapping radial windows with a long and narrow \u2019pyramid\u2019 shape, as shown in Fig. 3. We obtain the window index for the token at (ri, \u03b8i, \u03d5i) as\nwin indexi = (\u230a \u03b8i\n\u2206\u03b8 \u230b, \u230a\n\u03d5i \u2206\u03d5 \u230b), (1)\nwhere \u2206\u03b8 and \u2206\u03d5 denote the window size corresponding to the \u03b8 and \u03d5 dimension, respectively.\nTokens with the same window index would be assigned to the same window. The multi-head self-attention [64] is conducted within each window independently as follows.\nq\u0302 = f \u00b7Wq , k\u0302 = f \u00b7Wk, v\u0302 = f \u00b7Wv , (2)\nwhere f \u2208 Rn\u00d7c denotes the input features of a window, Wq,Wk,Wv \u2208 Rc\u00d7c are the linear projection weights, and q\u0302, k\u0302, v\u0302 \u2208 Rn\u00d7c are the projected features. Then, we split the projected features q\u0302, k\u0302, v\u0302 into h heads (i.e., Rn\u00d7(h\u00d7d)), and reshape them as q,k,v \u2208 Rh\u00d7n\u00d7d. For each head, we perform dot product and weighted sum as\nattnk = softmax(qk \u00b7 kTk ), (3)\nz\u0302k = attnk \u00b7 vk, (4)\nwhere qk,kk,vk \u2208 Rn\u00d7d denote the features of the k-th head, and attnk \u2208 Rn\u00d7n is the corresponding attention weight. Finally, we concatenate the features from all heads and apply the final linear projection with weight Wproj \u2208 Rc\u00d7c to yield the output z \u2208 Rn\u00d7c as\nz\u0302 = concat({z\u03020, z\u03021, ..., z\u0302h\u22121}). (5)\nz = z\u0302 \u00b7Wproj . (6)\nSphereFormer serves as a plugin module and can be conveniently inserted into existing mainstream models, e.g.,\nSparseConvNet [24,25], MinkowskiNet [13], local window self-attention [22, 30, 42]. In this paper, we find that inserting it into the end of each stage works well, and the network structure is given in the supplementary material. The resulting model can be applied to various downstream tasks, such as semantic segmentation and object detection, with strong performance as produced in experiments.\nSphereFormer is effective for the sparse distant points to get long-range information from the dense-point region. Therefore, the sparse distant points overcome the disconnection issue, and increase the effective receptive field.\nComparison with Cylinder3D. Although both Cylinder3D [88] and ours use polar or spherical coordinates to match LiDAR point distribution, there are two essential differences yet. First, Cylinder3D aims at a more balanced point distribution, while our target is to enlarge the receptive field smoothly and enable the sparse distant points to directly aggregate long-range information from the densepoint region. Second, what Cylinder3D does is replace the cubic voxel shape with the fan-shaped one. It remains to use local neighbors as before and still suffers from limited receptive field for the sparse distant points. Nevertheless, our method changes the way we find neighbors in a single operator (i.e., self-attention) and it is not limited to local neighbors. It thus avoids information separation between near and far objects and connects them in a natural way."
        },
        {
            "heading": "3.2. Position Encoding",
            "text": "For the 3D point cloud network, the input features have already incorporated the absolute xyz position. Therefore, there is no need to apply absolute position encoding. Also, we notice that Stratified Transformer [30] develops the contextual relative position encoding. It splits a relative position into several discrete parts uniformly, which converts the continuous relative positions into integers to index the positional embedding tables.\nThis method works well with local cubic windows. But in our case, the radial window is narrow and long, and its radius r can take even more than 50 meters, which could cause large intervals during discretization and thus coarse-\ngrained positional encoding. As shown in Fig. 4 (a), because of the large interval, key1 and key2 correspond to the same index. But there is still a considerable distance between them.\nExponential Splitting. Specifically, since the r dimension covers long distances, we propose exponential splitting for the r dimension as shown in Fig. 4 (b). The splitting interval grows exponentially when the index increases. In this way, the intervals near the query are much smaller, and the key1 and key2 can be assigned to different position encodings. Meanwhile, we remain to adopt the uniform splitting for the \u03b8 and \u03d5 dimensions. In notation, we have a query token qi and a key token kj . Their relative position (rij , \u03b8ij , \u03d5ij) is converted into integer index (idxrij , idx \u03b8 ij , idx \u03d5 ij) as\nidxrij =  \u2212max(0, \u2308log2( \u2212rij a )\u2309)\u2212 1 rij < 0 0 rij = 0\nmax(0, \u2308log2( rij a )\u2309) rij > 0 ,\nidx\u03b8ij = \u230a \u03b8ij\ninteval\u03b8 \u230b, idx\u03d5ij = \u230a\n\u03d5ij\ninteval\u03d5 \u230b,\nidxx = idxx + L\n2 , x \u2208 {r, \u03b8, \u03d5},\nwhere a is a hyper-parameter to control the starting splitting interval, and L is the length of the positional embedding tables. Note that we also add the indices with L2 to make sure they are non-negative.\nThe above indices (idxrij , idx \u03b8 ij , idx \u03d5 ij) are then used to index their positional embedding tables tr, t\u03b8, t\u03d5 \u2208 RL\u00d7(h\u00d7d) to find the corresponding position encoding prij ,p \u03b8 ij ,p \u03d5 ij \u2208 Rh\u00d7d, respectively. Then, we sum them up to yield the resultant positional encoding p \u2208 Rh\u00d7d, which then performs dot product with the features of qi and kj , respectively. The original Eq. (3) is updated to\np = prij + p \u03b8 ij + p \u03d5 ij ,\npos biask,i,j = qk,i \u00b7 pTk + kk,j \u00b7 p T k ,\nattnk = softmax(qk \u00b7 kTk + pos biask),\nwhere pos bias \u2208 Rh\u00d7n\u00d7n is the positional bias to the attention weight, qk,i \u2208 Rd means the the k-th head of the i-th query feature, and pk \u2208 Rd is the k-th head of the position encoding p.\nThe exponential splitting strategy provides smaller splitting intervals for near token pairs and larger intervals for distant ones. This operation enables a fine-grained position representation between near token pairs, and still maintains the same number of intervals in the meanwhile. Even though the splitting intervals become larger for distant token pairs, this solution actually works well since distant token pairs require less fine-grained relative position."
        },
        {
            "heading": "3.3. Dynamic Feature Selection",
            "text": "Point clouds scanned by LiDAR have the varyingsparsity property \u2014 close points are dense and distant points are much sparser. This property makes points at different locations perceive different amounts of local information. For example, as shown in Fig. 5, a point of the car (circled in green) near the LiDAR is with rich local geometric information from its dense neighbors, which is already enough for the model to make a correct prediction \u2013 incurring more global contexts might be contrarily detrimental. However, a point of bicycle (circled in red) far away from the LiDAR lacks shape information due to the extreme sparsity and even occlusion. Then we should supply long-range contexts as a supplement. This example shows treating all the query points equally is not optimal. We thus propose to dynamically select local or global features to address this issue.\nAs shown in Fig. 6, for each token, we incorporate not only the radial contextual information, but also local neighbor communication. Specifically, input features are projected into query, key and value features as Eq. (2). Then, the first half of the heads are used for radial window selfattention, and the remaining ones are used for cubic window self-attention. After that, these two features are concatenated and then linearly projected to the final output z for feature fusion. It enables different points to dynamically select local or global features. Formally, the Equations (3-5) are updated to\nattnradialk = softmax(q radial k \u00b7 k radial k\nT ),\nz\u0302radialk = attn radial k \u00b7 v radial k ,\nattncubick = softmax(q cubic k \u00b7 k cubic k\nT ),\nz\u0302cubick = attn cubic k \u00b7 v cubic k ,\nz\u0302 = concat({z\u0302radial0 , z\u0302radial1 , ..., z\u0302radialh/2\u22121, z\u0302 cubic h/2 , ..., z\u0302 cubic h\u22121 }),\nwhere qcubick ,k cubic k ,v cubic k \u2208 Rn cubic\u00d7d denote the query, key and value features for the k-th head with cubic window partition, and attncubick \u2208 Rn\ncubic\u00d7ncubic denotes the cubic window attention weight for the k-th head."
        },
        {
            "heading": "4. Experiments",
            "text": "In this section, we first introduce the experimental setting in Sec. 4.1. Then, we show the semantic segmentation and object detection results in Sec. 4.2 and 4.3. The ablation study and visual comparison are shown in Sec. 4.4 and 4.5. Our code and models will be made publicly available."
        },
        {
            "heading": "4.1. Experimental Setting",
            "text": "Network Architecture. For semantic segmentation, we adopt the encoder-decoder structure and follow U-Net [49] to concatenate the fine-grained encoder features in the decoder. We follow [88] to use SparseConv [24, 25] as our baseline model. There are a total of 5 stages whose channel numbers are [32, 64, 128, 256, 256], and there are two residual blocks at each stage. Our proposed module is stacked at the end of each encoding stage. For object detection, we adopt CenterPoint [78] as our baseline model, where the backbone possesses 4 stages whose channel numbers are [16, 32, 64, 128]. Our proposed module is stacked at the end of the second and third stages. Note that our proposed module incurs negligible extra parameters, and more details are given in the supplementary material.\nDatasets. Following previous work, we evaluate methods on nuScenes [4], SemanticKITTI [3], and Waymo Open Dataset [52] (WOD) for semantic segmentation. For object detection, we evaluate our methods on the nuScenes [4] dataset. The details of the datasets are given in the supplementary material.\nImplementation Details. For semantic segmentation, we use 4 GeForce RTX 3090 GPUs for training. We train the models for 50 epochs with AdamW [39] optimizer and \u2018poly\u2019 scheduler where power is set to\n0.9. The learning rate and weight decay are set to 0.006 and 0.01, respectively. Batch size is set to 16 on nuScenes, and 8 on both SemanticKITTI and Waymo Open Dataset. The window size is set to [120m, 2\u25e6, 2\u25e6] for (r, \u03b8, \u03d5) on both nuScenes and SemanticKITTI, and [80m, 1.5\u25e6, 1.5\u25e6] on Waymo Open Dataset. During data preprocessing, we confine the input scene to the range from [\u221251.2m,\u221251.2m,\u22124m] to [51.2m, 51.2m, 2.4m] on SemanticKITTI and [\u221275.2m,\u221275.2m,\u22122m] to [75.2m, 75.2m, 4m] on Waymo. Also, we set the voxel size to 0.1m on both nuScenes and Waymo, and 0.05m on SemanticKITTI.\nFor object detection, we adopt the OpenPCDet [57] codebase and follow the default CenterPoint [78] to set the training hyper-parameters. We set the window size to [120m, 1.5\u25e6, 1.5\u25e6]."
        },
        {
            "heading": "4.2. Semantic Segmentation Results",
            "text": "The results on SemanticKITTI test set are shown in Table 1. Our method yields 74.8% mIoU, a new state-of-theart result. Compared to the methods based on range images [43, 67] and Bird-Eye-View (BEV) [79], ours gives a result with over 20% mIoU performance gain. Moreover, thanks to the capability of directly aggregating long-range information, our method significantly outperforms the models based on sparse convolution [12,55,69,70,88]. It is also notable that our method outperforms 2DPASS [71] that uses extra 2D images in training by 1.9% mIoU.\nIn Tables 2 and 3, we also show the semantic segmentation results on nuScenes test and val set, respectively. Our method consistently outperforms others by a large margin, and achieves the 1st place on the benchmark. It is intriguing\nto note that our method is purely based on LiDAR data, and it works even better than approaches of [23, 71, 89] that use additional 2D information.\nMoreover, we demonstrate the semantic segmentation results on Waymo Open Dataset val set in Table 4. Our model outperforms the baseline model with a substantial gap of 3.3% mIoU. Also, it is worth noting that our method achieves a 9.3% mIoU performance gain for the far points, i.e., the sparse distant points."
        },
        {
            "heading": "4.3. Object Detection Results",
            "text": "Our method also achieves strong performance in object detection. As shown in Table 8, our method outperforms\nother published methods on nuScenes test set, and ranks 3rd on the LiDAR-only benchmark. It shows that directly aggregating long-range information is also beneficial for object detection. It also manifests the capability of our method to generalize to instance-level tasks."
        },
        {
            "heading": "4.4. Ablation Study",
            "text": "To testify the effectiveness of each component, we conduct an extensive ablation study and list the result in Table 5. The Experiment I (Exp. I for short) is our baseline model of SparseConv. Unless otherwise specified, we train the models on nuScenes train set and make evaluations on nuScenes val set for the ablation study. To comprehensively reveal the effect, we also report the performance at different distances, i.e., close (\u2264 20m), medium (> 20m & \u2264 50m), far (> 50m) distances.\nWindow Shape. By comparing Experiments I and II in Table 5, we can conclude that the radial window shape is beneficial. Further, the improvement stems mainly from better handling the medium and far points, where we yield 5.67% and 13.39% mIoU performance gain, respectively. This result exactly verifies the benefit of aggregating longrange information with the radial window shape.\nMoreover, we also compare the radial window shape with the cubic one proposed in [22, 30, 42]. As shown in Table 6, the radial window shape considerably outperforms the cubic one.\nBesides, we investigate the effect of window size as shown in Table 7. Setting it too small may make it hard to capture meaningful information, while setting it too large\nmay increase the optimization difficulty.\nExponential Splitting. Compared to Exp. IV, Exp. V improves with 1.36% more mIoU, which shows the effectiveness. Moreover, the consistent conclusion could be drawn from Experiments II and III, where we witness 3.88% and 4.43% more mIoU for the medium and far points, respectively. Also, we notice that with exponential splitting, all the close, medium, and far points are better dealt with.\nDynamic Feature Selection. From the comparison between Experiments III and V, we note that dynamic feature selection brings a 0.8% mIoU performance gain. Interestingly, we further notice that the gain mainly comes from the close points, which indicates that the close points may not rely too much on global information, since the dense local information is already enough for correct predictions for the dense close points. It also reveals the fact that points at varying locations should be treated differently. Moreover, the comparison between Exp. II and IV leads to consistent conclusion. Although the performance of medium and far decreases a little, the overall mIoU still increases, since their points number is much than that of the close points."
        },
        {
            "heading": "4.5. Visual Comparison",
            "text": "As shown in Fig. 7, we visually compare the baseline model (i.e., SparseConv) and ours. It visually indicates that with our proposed module, more sparse distant objects are recognized, which are highlighted with cyan boxes. More examples are given in the supplementary material."
        },
        {
            "heading": "5. Conclusion",
            "text": "We have studied and dealt with varying-sparsity LiDAR point distribution. We proposed SphereFormer to enable the sparse distant points to directly aggregate information from the close ones. We designed radial window self-attention, which enlarges the receptive field for distant points to intervene with close dense ones. Also, we presented exponential splitting to yield more detailed position encoding. Dynamically selecting local or global features is also helpful. Our method demonstrates powerful performance, ranking 1st on both nuScenes and SemanticKITTI semantic segmentation benchmarks and achieving the 3rd on nuScenes object detection benchmark. It shows a new way to further enhance 3D visual understanding. Our limitations are discussed in the supplementary material."
        }
    ],
    "title": "Spherical Transformer for LiDAR-based 3D Recognition",
    "year": 2023
}