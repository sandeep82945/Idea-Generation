{
    "abstractText": "Recognizing traffic signs is an essential component of intelligent driving systems\u2019 environment perception technology. In real-world applications, traffic sign recognition is easily influenced by variables such as light intensity, extreme weather, and distance, which increase the safety risks associated with intelligent vehicles. A Chinese traffic sign detection algorithm based on YOLOv4-tiny is proposed to overcome these challenges. An improved lightweight BECA attention mechanism module was added to the backbone feature extraction network, and an improved dense SPP network was added to the enhanced feature extraction network. A yolo detection layer was added to the detection layer, and k-means++ clustering was used to obtain prior boxes that were better suited for traffic sign detection. The improved algorithm, TSR-YOLO, was tested and assessed with the CCTSDB2021 dataset and showed a detection accuracy of 96.62%, a recall rate of 79.73%, an F-1 Score of 87.37%, and a mAP value of 92.77%, which outperformed the original YOLOv4-tiny network, and its FPS value remained around 81 f/s. Therefore, the proposed method can improve the accuracy of recognizing traffic signs in complex scenarios and can meet the real-time requirements of intelligent vehicles for traffic sign recognition tasks.",
    "authors": [
        {
            "affiliations": [],
            "name": "Felipe Jim\u00e9nez"
        },
        {
            "affiliations": [],
            "name": "Weizhen Song"
        }
    ],
    "id": "SP:e85880ed7e4287a53ec92c03f921e6e8ecd13fd7",
    "references": [
        {
            "authors": [
                "M.D. Radu",
                "I.M. Costea",
                "V.A. Stan"
            ],
            "title": "Automatic Traffic Sign Recognition Artificial Intelligence\u2014Deep Learning Algorithm",
            "venue": "In Proceedings of the 2020 12th International Conference on Electronics, Computers and Artificial Intelligence (ECAI), Bucharest, Romania,",
            "year": 2020
        },
        {
            "authors": [
                "J. Zhang",
                "X. Zou",
                "L.-D. Kuang",
                "J. Wang",
                "R.S. Sherratt",
                "X. Yu"
            ],
            "title": "A More Comprehensive Traffic Sign Detection Benchmark",
            "venue": "CCTSDB",
            "year": 2021
        },
        {
            "authors": [
                "Y. Pan",
                "V. Kadappa",
                "S. Guggari"
            ],
            "title": "Chapter 15\u2014Identification of Road Signs Using a Novel Convolutional Neural Network",
            "year": 2020
        },
        {
            "authors": [
                "M.N. Khan",
                "A. Das",
                "M.M. Ahmed",
                "S.S. Wulff"
            ],
            "title": "Multilevel Weather Detection Based on Images: A Machine Learning Approach with Histogram of Oriented Gradient and Local Binary Pattern-Based Features",
            "venue": "J. Intell. Transp. Syst",
            "year": 2021
        },
        {
            "authors": [
                "A. Hechri",
                "A. Mtibaa"
            ],
            "title": "Two-Stage Traffic Sign Detection and Recognition Based on SVM and Convolutional Neural Networks",
            "venue": "IET Image Process",
            "year": 2020
        },
        {
            "authors": [
                "Q. Yu",
                "Y. Zhou"
            ],
            "title": "Traffic Safety Analysis on Mixed Traffic Flows at Signalized Intersection Based on Haar-Adaboost Algorithm and Machine Learning",
            "venue": "Saf. Sci",
            "year": 2019
        },
        {
            "authors": [
                "X. Kuang",
                "W. Fu",
                "L. Yang"
            ],
            "title": "Real-Time Detection and Recognition of Road Traffic Signs Using MSER and Random Forests",
            "venue": "Int. J. Online Eng. 2018,",
            "year": 2018
        },
        {
            "authors": [
                "C. Hu",
                "X. He"
            ],
            "title": "Traffic Sign Detection",
            "venue": "Based on MSERs and SVM. Comput. Sci. 2022,",
            "year": 2022
        },
        {
            "authors": [
                "X. Dai",
                "X. Yuan",
                "G. Le",
                "L. Zhang"
            ],
            "title": "Detection method of traffic signs based on color pair and MSER in the complex environment",
            "venue": "J. Beijing Jiaotong Univ. 2018,",
            "year": 2018
        },
        {
            "authors": [
                "A. Krizhevsky",
                "I. Sutskever",
                "G.E. Hinton"
            ],
            "title": "ImageNet Classification with Deep Convolutional Neural Networks",
            "venue": "Commun. ACM 2017,",
            "year": 2017
        },
        {
            "authors": [
                "R. Girshick",
                "J. Donahue",
                "T. Darrell",
                "J. Malik"
            ],
            "title": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation",
            "venue": "In Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Columbus, OH, USA,",
            "year": 2014
        },
        {
            "authors": [
                "W. Liu",
                "D. Anguelov",
                "D. Erhan",
                "C. Szegedy",
                "S. Reed",
                "C.Y. Fu",
                "A.C. Berg"
            ],
            "title": "SSD: Single Shot Multibox Detector",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV), Amsterdam, The Netherlands,",
            "year": 2016
        },
        {
            "authors": [
                "S. Ren",
                "K. He",
                "R. Girshick",
                "J. Sun"
            ],
            "title": "Faster R-CNN: Towards Realtime Object Detection with Region Proposal Networks",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "year": 2017
        },
        {
            "authors": [
                "S. Ren",
                "K. He",
                "R. Girshick",
                "J. Sun"
            ],
            "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
            "venue": "In Advances in Neural Information Processing Systems; Curran Associates,",
            "year": 2015
        },
        {
            "authors": [
                "J. Redmon",
                "S. Divvala",
                "R. Girshick",
                "A. Farhadi"
            ],
            "title": "You Only Look Once: Unified, Real-Time Object Detection",
            "venue": "In Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV,",
            "year": 2016
        },
        {
            "authors": [
                "J. Redmon",
                "A. Farhadi"
            ],
            "title": "YOLO9000: Better, Faster, Stronger",
            "venue": "In Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, HI, USA,",
            "year": 2017
        },
        {
            "authors": [
                "J. Redmon",
                "A. Farhadi"
            ],
            "title": "YOLOv3: An Incremental Improvement",
            "venue": "arXiv 2018,",
            "year": 2018
        },
        {
            "authors": [
                "A. Bochkovskiy",
                "C.-Y. Wang",
                "H.-Y.M. Liao"
            ],
            "title": "YOLOv4: Optimal Speed and Accuracy of Object Detection",
            "venue": "arXiv 2020,",
            "year": 2020
        },
        {
            "authors": [
                "H. Zhang",
                "L. Qin",
                "J. Li",
                "Y. Guo",
                "Y. Zhou",
                "J. Zhang",
                "Z. Xu"
            ],
            "title": "Real-Time Detection Method for Small Traffic Signs Based on Yolov3",
            "venue": "IEEE Access 2020,",
            "year": 2020
        },
        {
            "authors": [
                "J. Zhang",
                "Z. Xie",
                "J. Sun",
                "X. Zou",
                "J. Wang"
            ],
            "title": "A cascaded R-CNN with multiscale attention and imbalanced samples for traffic sign detection",
            "venue": "IEEE Access 2020,",
            "year": 2020
        },
        {
            "authors": [
                "L. Cui",
                "P. Lv",
                "X. Jiang",
                "Z. Gao",
                "B. Zhou",
                "L. Zhang",
                "L. Shao",
                "M. Xu"
            ],
            "title": "Context-Aware Block Net for Small Object Detection",
            "venue": "IEEE Trans. Cybern. 2020,",
            "year": 2020
        },
        {
            "authors": [
                "C.-Y. Wang",
                "A. Bochkovskiy",
                "H.-Y.M. Liao"
            ],
            "title": "Scaled-YOLOv4: Scaling Cross Stage Partial Network",
            "venue": "In Proceedings of the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Nashville, TN, USA,",
            "year": 2021
        },
        {
            "authors": [
                "C. Liu",
                "S. Li",
                "F. Chang",
                "Y. Wang"
            ],
            "title": "Machine Vision Based Traffic Sign Detection Methods: Review, Analyses and Perspectives",
            "venue": "IEEE Access 2019,",
            "year": 2019
        },
        {
            "authors": [
                "T. Guofeng",
                "C. Huairong",
                "L. Yong",
                "Z. Kai"
            ],
            "title": "Traffic Sign Recognition Based on SVM and Convolutional Neural Network",
            "venue": "In Proceedings of the 2017 12th IEEE Conference on Industrial Electronics and Applications (ICIEA), Siem Reap, Cambodia,",
            "year": 2017
        },
        {
            "authors": [
                "S. Yucong",
                "G. Shuqing"
            ],
            "title": "Traffic Sign Recognition Based on HOG Feature Extraction",
            "venue": "J. Meas. Eng. 2021,",
            "year": 2021
        },
        {
            "authors": [
                "A. Madani",
                "R. Yusof"
            ],
            "title": "Traffic Sign Recognition Based on Color, Shape, and Pictogram Classification Using Sup-port Vector Machines",
            "venue": "Neural Comput. Appl. 2018,",
            "year": 2018
        },
        {
            "authors": [
                "S.B. Wali",
                "M.A. Abdullah",
                "M.A. Hannan",
                "A. Hussain",
                "S.A. Samad",
                "P.J. Ker",
                "M.B. Mansor"
            ],
            "title": "Vision-Based Traffic Sign Detection and Recognition Systems: Current Trends and Challenges",
            "venue": "Sensors 2019,",
            "year": 2093
        },
        {
            "authors": [
                "Z. Zuo",
                "K. Yu",
                "Q. Zhou",
                "X. Wang",
                "T. Li"
            ],
            "title": "Traffic Signs Detection Based on Faster R-CNN",
            "venue": "In Proceedings of the 2017 IEEE 37th International Conference on Distributed Computing Systems Workshops (ICDCSW), Atlanta, GA,",
            "year": 2017
        },
        {
            "authors": [
                "J. Li",
                "Z. Wang"
            ],
            "title": "Real-Time Traffic Sign Recognition Based on Efficient CNNs in the Wild",
            "venue": "IEEE Trans. Intell. Transp. Syst",
            "year": 2019
        },
        {
            "authors": [
                "H. Shan",
                "W. Zhu"
            ],
            "title": "A Small Traffic Sign Detection Algorithm Based on Modified SSD",
            "venue": "IOP Conf. Ser. Mater. Sci. Eng",
            "year": 2019
        },
        {
            "authors": [
                "C. Chen",
                "H. Wang",
                "Y. Zhao",
                "Y. Wang",
                "L. Li",
                "K. Li",
                "T. Zhang"
            ],
            "title": "A depth based traffic sign recognition algorithm",
            "venue": "Telecommun. Technol. 2021,",
            "year": 2021
        },
        {
            "authors": [
                "J. Hu",
                "L. Shen",
                "G. Sun"
            ],
            "title": "Squeeze-and-Excitation Networks",
            "venue": "In Proceedings of the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT,",
            "year": 2018
        },
        {
            "authors": [
                "Q. Wang",
                "B. Wu",
                "P. Zhu",
                "P. Li",
                "W. Zuo",
                "Q. Hu"
            ],
            "title": "ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks",
            "venue": "arXiv 2019,",
            "year": 2019
        },
        {
            "authors": [
                "J. Chen"
            ],
            "title": "Design of Fruit and Vegetable Electronic Scale System BASED on Deep Learning; Southwest Jiaotong University: Chengdu, China, 2021",
            "year": 2021
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recogni-tion",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "year": 2015
        },
        {
            "authors": [
                "D.K. Sharma"
            ],
            "title": "Information Measure Computation and Its Impact in MI COCO Dataset",
            "venue": "In Proceedings of the 2021 7th International Conference on Advanced Computing and Communication Systems (ICACCS), Coimbatore, India,",
            "year": 2021
        },
        {
            "authors": [
                "H. Satarkar",
                "N. Zagade",
                "S. Gupta",
                "S. Pundlik"
            ],
            "title": "Comparative Study between Segmentation",
            "venue": "Neural Networks on Pascal VOC Dataset. Int. Res. J. Eng. Technol. 2021,",
            "year": 2021
        },
        {
            "authors": [
                "K.P. Sinaga",
                "M.-S. Yang"
            ],
            "title": "Unsupervised K-Means Clustering Algorithm",
            "venue": "IEEE Access 2020,",
            "year": 2020
        },
        {
            "authors": [
                "S. Mehta",
                "C. Paunwala",
                "B. Vaidya"
            ],
            "title": "CNN Based Traffic Sign Classification Using Adam Optimizer",
            "venue": "In Proceedings of the 2019 International Conference on Intelligent Computing and Control Systems (ICCS), Madurai, India,",
            "year": 2019
        },
        {
            "authors": [
                "Z. Liu",
                "M. Qi",
                "C. Shen",
                "Y. Fang",
                "X. Zhao"
            ],
            "title": "Cascade Saccade Machine Learning Network with Hierarchical Classes for Traffic Sign Detection",
            "venue": "Sustain. Cities Soc",
            "year": 2021
        },
        {
            "authors": [
                "K. Ren",
                "L. Huang",
                "C. Fan"
            ],
            "title": "Real-time Small Traffic Sign Detection Algorithm based on Multi-scale Pixel Feature Fusion",
            "venue": "Signal Process",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "Citation: Song, W.; Suandi, S.A.\nTSR-YOLO: A Chinese Traffic Sign\nRecognition Algorithm for Intelligent\nVehicles in Complex Scenes. Sensors\n2023, 23, 749. https://doi.org/\n10.3390/s23020749\nAcademic Editor: Felipe Jim\u00e9nez\nReceived: 29 November 2022\nRevised: 30 December 2022\nAccepted: 4 January 2023\nPublished: 9 January 2023\nCopyright: \u00a9 2023 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\nKeywords: traffic sign; intelligent vehicle; YOLOv4-tiny; k-means++; CCTSDB2021 dataset\n1. Introduction\nTraffic sign recognition is a crucial component of intelligent vehicle driving systems and one of the most important research fields in computer vision [1]. Traffic sign recognition tasks are usually performed in natural scenes; however, extreme weather conditions (e.g., rain, snow, or fog) can obscure traffic signage information, and overexposure and dim light usually reduce the visibility of traffic signs. Furthermore, traffic signs are exposed all year, causing the surfaces of some to fade, become unclear, or become damaged. Complex and changing environments often affect the speed and accuracy of traffic sign recognition in intelligent transportation [2]. Therefore, it is now especially essential to study the problem of fast and accurate traffic sign detection in complex environments. Early recognition methods in traffic sign recognition used a sliding window strategy to traverse the entire image and generate many candidate regions. The candidate regions were then extracted with various types of hand-designed features, such as HOG (histogram of oriented gradient) [3], SIFT (scale-invariant feature transform) [4], and LBP (local binary pattern) [5]. These features were then fed into an efficient classifier, such as SVM (support vector machine) [6], Adaboost [7], or Random Forest [8], for detection and identification. However, traditional target detection methods require researchers to extract features manually and are not robust to changes in diversity. In addition, slidingwindow-based region selection strategies are not targeted and have high time complexity. Hu et al. [9] proposed a new approach for traffic sign detection based on maximally stable extremal regions (MSERs) and SVM, which had a high level of accuracy but only seven frames per second (FPS) of detection speed. Dai et al. [10] proposed using color to improve\nSensors 2023, 23, 749. https://doi.org/10.3390/s23020749 https://www.mdpi.com/journal/sensors\nSensors 2023, 23, 749 2 of 23\nthe recognition rate of traffic signs in varying brightness environments, achieving 78% accuracy and 11 FPS. Nevertheless, in real scenarios, real-time and accuracy are essential for traffic sign recognition. Therefore, conventional methods of target detection fall well short of the needs of intelligent traffic systems. The AlexNet [11] algorithm achieved great success for convolutional neural networks in 2012, making deep learning rapidly gain the attention of researchers in the field of artificial intelligence, including target detection. Girshick et al. proposed R-CNN (regions with CNN features) [12], the first deep-learning-based two-stage target detection algorithm, which provided a significant performance improvement compared to traditional algorithms. The algorithms that followed, such as SSD (single shot multi-box) [13], Fast R-CNN [14], Faster R-CNN [15], and the YOLO (you only look once) series [16\u201319], achieved higher accuracy in target localization and classification tasks. Zhang et al. [20] proposed the MSA_YOLOv3 algorithm for traffic sign recognition, with a mAP value of 0.86 and a detection speed of 9 FPS. Zhang et al. [21] proposed CMA R-CNN for traffic sign recognition, with a mAP value of 0.98 and a detection speed of only 3 FPS. Cui et al. [22] proposed CAB-s Net for traffic sign detection, with a mAP value of 0.89 and a detection speed of 27 FPS. However, these algorithms are frequently designed to extract more detailed features by constructing deeper network structures, resulting in models that are relatively large, are slow to detect, and require high amounts of hardware computing power and storage capacity, making them difficult to use in mobile and embedded devices. In order to accelerate the detection time of deep convolutional neural-network-based traffic sign detection methods, a lightweight convolutional neural-network-based target detection architecture is now used to recognize traffic signs. Regarding detection speed, YOLOv4-tiny [23] is a superior target detection model that outperforms the vast majority of current, complicated deep convolutional neural network models. However, the YOLOv4tiny algorithm\u2019s detection accuracy is relatively low. This paper proposes a Chinese traffic sign detection algorithm based on enhanced YOLOv4-tiny that can more effectively promote the transmission and sharing of different levels of information to improve the algorithm\u2019s detection accuracy and ensure its detection speed by optimizing the network. Compared to the YOLOv4-tiny algorithm, the following are the primary contributions of this study.\n\u2022 To address the issue that a complex background interferes with target recognition in the feature information extracted by the CSPDarknet53-tiny network, this paper embeds a BECA attention mechanism module in a CSP structure to improve the model\u2019s ability to extract and utilize key feature information while reducing the importance of useless features and to invest computational resources in different channels proportionally to the importance of the channels. \u2022 Since the YOLOv4-tiny enhanced feature extraction network is too simple, and the fusion of feature layers only reflects the stacking of a single feature layer after upsampling, resulting in a low utilization of feature information extracted from the backbone network and insufficient feature fusion, dense spatial pyramid pooling (Dense SPP) is introduced for multiscale pooling and the fusion of input feature layers to enrich the feature expression capability. \u2022 Based on the original network, the detection scale range is increased to improve the degree of matching for targets of various sizes. The bottom\u2013up fusion of deep semantic information with shallow semantic information is used to improve the feature information of small targets, predict small and far away traffic sign targets more accurately, and improve the accuracy of the network\u2019s localization and detection. \u2022 In order to accelerate the network\u2019s ability to detect traffic signs, k-means++ clustering is used to learn prior boxes that are more suitable for traffic sign detection. \u2022 The TSR-YOLO method proposed in this study has a higher mAP value of 8.23%, a higher precision value of 5.02%, a higher recall value of 1.6%, and a higher F-1 score of 3.04% compared to YOLOv4-tiny.\nSensors 2023, 23, 749 3 of 23\nThe rest of this paper is organized as follows. In Section 2, we briefly review the development of target detection, traffic sign detection, and related work. Section 3 describes our research methodology. Section 4 presents our experimental results and analysis. Section 5 summarizes our work and provides some suggestions for future work.\n2. Related Work\nTraffic sign detection is one of the most challenging and essential problems in autonomous vehicle-driving systems. Most early algorithms for identifying traffic signs used machine learning and template matching [24]. Deep-learning-based algorithms are widely used for high-precision traffic sign detection due to the rapid development of high-performance computers and the enormous explosion of data volume in recent years. Tong et al. [25] proposed a color-based support vector machine (SVM) algorithm for traffic sign recognition that first converted the RGB color space to HSV color space to determine the region of interest (ROI) and then extracted the histogram of oriented gradients (HOG) features and used an SVM to determine whether it was a traffic sign. Yu et al. [26] identified traffic signs using a color threshold segmentation method and morphological processing to eliminate the interference of the background region and increase the contours of the sign region and then used the HOG method to gather the gradient of each pixel point within a cell. Madani et al. [27] employed adaptive thresholding algorithms and support vector machine models to recognize and classify traffic signs based on boundary color and shape. Typically, the performance of such detection methods is dependent on the usefulness of the manual feature extraction, which requires shape features, color features, or hybrid features to obtain rich detail information of traffic signs. Detection results are also susceptible to objective natural factors, such as variations in light, extreme weather, and obstructions. Since the emergence of deep-learning techniques, numerous target detection algorithms based on deep learning have been applied to traffic sign detection [28]. In contrast to the above methods, deep-learning models can automatically extract features, avoiding the limitations of manual feature extraction, and their generalizability and robustness are relatively high. There are currently two types of CNN-based target identification algorithms: single-stage detectors based on regression and two-stage detectors based on candidate areas. Zuo et al. [29] used a two-stage target detection algorithm, Faster R-CNN, to detect traffic signs by conditionally scanning an image to generate a large number of candidate boxes, sending each candidate box to the network to extract a feature, sending that feature to a classifier for classification, and finally generating the correct class name. Li et al. [30] designed a detection model using the Faster R-CNN and MobileNet structures. It refined the localization of small traffic signs using color and shape information. A CNN with an asymptotic kernel was then used to classify traffic signs. The research results demonstrated that the proposed detector was able to detect different kinds of traffic signs. Unlike the two-stage target detection method, the single-stage target detection algorithm first uses a clustering algorithm to create a certain number of prior boxes. It then uses these prior boxes to find a region of interest, feeds the region of interest into a feature extraction network, and uses a regression method to determine the confidence probability of the object. This accelerates operation and allows for real-time detection. Shan et al. [31] used an SSD single-stage target identification method to detect traffic signs; the algorithm worked well with single-target, multi-target, and low-light images. Chen et al. [32] proposed employing the YOLOv3 method to overcome the problem of poor rate of traffic sign recognition due to complicated background interference and, ultimately, achieved accurate traffic sign recognition by fusing advanced network modules. In conclusion, detection methods based on deep learning can enable intelligent vehicles to better detect traffic signs in complex road scenarios. With the rapid development of intelligent vehicles, real-time and accuracy requirements for traffic sign detection and recognition have improved. This paper employs a single-stage deep learning detection\nSensors 2023, 23, 749 4 of 23\nmethod and proposes TSR-YOLO, a lightweight traffic sign detection model with high accuracy, low latency, and robustness to improve detection performance.\n3. The Proposed Method\nThis study creates an effective traffic sign identification algorithm and integrates the proposed TSR-YOLO model into a vehicle traffic sign perception system. This section begins with an overview of the smart car traffic sign recognition system, followed by a brief description of the YOLOv4-tiny network and a discussion of the YOLOv4-tiny network improvement method.\n3.1. The Traffic Sign Recognition System\nThis study demonstrates an intelligent vehicle traffic sign visual perception system with three main parts: a vision system, a traffic sign detection system, and an intelligent car display system. To be more specific, a vision system based on a monocular camera captured information in a vehicle\u2019s driving road environment in the form of video or image and then passed the information to a traffic sign detector, which detected the existence of traffic signs in the driving environment by the video sequence given by the vision system. If the traffic sign information was captured in the road environment, it was displayed on the HUD (head up display) flat-strip display. The responsibility of the traffic sign detection system was to detect the existence of traffic signs in the driving environment. It was a key component of the proposed system for identifying traffic signs. Therefore, efforts needed to be made to develop a system capable of detecting traffic signs rapidly and precisely in a complicated road environment. Figure 1 illustrates the proposed traffic sign recognition system\u2019s workflow.\nSensors 2023, 23, x FOR PEER REVIEW 4 of 24\nmethod and proposes TS - fi tection odel with high a curacy, low latency, and robustne s to improve detection perfor .\n3. The Proposed ethod This study creates an effective traffic sign identification algorith and integrates the proposed TSR-YOLO odel into a vehicle traffic sign perception system. This section begins with an overview of the smart car traffic sign recognition system, followed by a brief description of the YOLOv4-tiny net ork and a discussion of the YOLOv4-tiny network improvement method.\n3.1. The Traffic Sign Recognition System This study demonstrates an intelligent vehicle traffic sign visual perception system with three main parts: a vision system, a traffic sign detection system, and an intelligent car display system. To be more specific, a vision system based on a monocular camera captured information in a vehicle\u2019s driving road environment in the form of video or image and then passed the information to a traffic sign detector, which detected the existence of traffic signs in the driving environment by the video sequence given by the vision system. If the traffic sign information was captured in the road environment, it was displayed on the HUD (head up display) flat-strip display. The responsibility of the traffic sign detection system was to detect the existence of traffic signs in the driving environment. It was a key component of the proposed system for identifying traffic signs. Therefore, efforts needed to be made to develop a system capable of detecting traffic signs rapidly and precisely in a complicated road environment. Figure 1 illustrates the proposed traffic sign recognition system\u2019s workflow.\nTSR-YOLO Detector of traffic signs\nTraffic sign information\nData collection\nFront Camera\nVideos and Images\nDetect the presence of a traffic sign on a street\nDisplay on HUD and on-board computer\nDisplay if traffic sign is detected\nFigure 1. Traffic sign recognition system.\nA traffic sign recognition system can effectively remind drivers to pay attention to traffic sign information, such as prohibitions and warnings, to prevent violations caused by negligence. In our study, a monocular camera captured video sequences in real time. The camera was the \u201ceye\u201d for traffic sign detection and was connected to a computer system running an improved YOLOv4-tiny pretraining model. If the pretrained detector detected information containing traffic signs in the road environment, it passed the information to an intelligent vehicle display system for display on the HUD.\nFigure 1. Traffic sign recognition system.\nA traffic sign recognition system can effectively remind drivers to pay attention to traffic sign information, such as prohibitions and warnings, to prevent violations caused by negligence. In our study, a monocular camera captured video sequences in real time. The camera was the \u201ceye\u201d for traffic sign detection and was connected to a computer system running an improved YOLOv4-tiny pretraining model. If the pretrained detector detected information containing traffic signs in the road environment, it passed the information to an intelligent vehicle display system for display on the HUD.\n3.2. The YOLOv4-Tiny Network\nYOLOv4-tiny is a scaled-down version of YOLOv4. The main idea is to treat the target detection task as a regression problem, with the detected target location and classification results obtained directly through network model regression. Figure 2 depicts the network structure of YOLOv4-tiny. The YOLOv4-tiny network is divided into three components: the backbone (CSP-Darknet53-tiny), the neck (feature pyramid network, FPN), and the Yolo-head. (1) The backbone part is composed of a convolutional block (CBL), a maximum\nSensors 2023, 23, 749 5 of 23\npooling layer (maxpool), and a cross-stage partial (CSP) module, which is mainly used for prefeature extraction. (2) In the neck part, YOLOv4-tiny retains the feature pyramid network (FPN) structure of YOLOv4. The FPN structure can fuse the features between different network layers so that it can obtain both the rich semantic information of the deeper networks and the geometric detail information of the lower networks to enhance the feature extraction ability. (3) Two prediction branches are retained in the Yolo-head section, and the final prediction is performed using the feature fusion results obtained from the FPN module to form two prediction scales of 13 \u00d7 13 and 26 \u00d7 26. Because of its simple structure, small computation, and fast detection time, YOLOv4-tiny is suitable for intelligent vehicle environment-aware systems. Still, it is not very accurate in detecting small targets, such as traffic signs, which makes it difficult to adapt to the task of traffic sign recognition in complex scenes. Therefore, some improvements to YOLOv4-tiny are needed to make the algorithm capable of detecting traffic signs in complex scenarios.\nSensors 2023, 23, x FOR PEER REVIEW 5 of 24\n3.2. The YOLOv4-Tiny Network YOLOv4-tiny is a scaled-down version of YOLOv4. The main idea is to treat the target detection task as a regression problem, with the detected target location and classification results obtained directly through network model regression. Figure 2 depicts the network structure of YOLOv4-tiny. The YOLOv4-tiny network is divided into three components: the backbone (CSP-Darknet53-tiny), the neck (feature pyramid network, FPN), and the Yolo-head. (1) The backbone part is composed of a convolutional block (CBL), a maximum pooling layer (maxpool), and a cross-stage partial (CSP) module, which is mainly used for prefeature extraction. (2) In the neck part, YOLOv4-tiny retains the feature pyr mid ne work (FPN) structure of YOLOv4. The FPN structure can f s the features between different network layers so that it can obtain both the rich semantic information of the deeper networks nd the geometric detail information of the l wer netw rks to enha ce the fe ture extraction ability. (3) Two prediction branches are retained in the Yolo-head section, a d the final prediction is perfo med using the feature fusion results ob ai ed from the FPN module to form two prediction scales of 13 \u00d7 13 and 26 \u00d7 26. Because of its si ple structure, small computation, and ast detection time, YOLOv4-tiny is suitable for intelligent vehicle environment-aware systems. Still, it is not very accurat in detecting small targets, such as traffic signs, which makes it difficult to ad pt to the task of traffic sign recognition in complex scenes. Therefore, some improvements o YOLOv4tiny are needed to make the algorithm capable of d tecting traffic signs in complex scenarios.\n416*416*3\nInput\nCBL\nCBL\nCSP\nmaxpool\nCSP\nmaxpool\nCSP\nmaxpool\nCBL\nCBL CBL CONV\nCBL\nupsample\nConcat CBL CONV\nBackbone Neck Output\nFigure 2. Structure of the YOLOv4-tiny network.\n3.3. The Proposed TSR-YOLO Algorithm For the specific traffic sign detection task, we improved the YOLOv4-tiny algorithm\u2019s ability to extract features by adding an improved BECA attention mechanism module to a CSPDarknet53-tiny structure, combining an improved spatial-pyramid-pooling module with the FPN structure and adding a Yolo detection layer to the Yolo head. The CCTSDB2021 traffic sign dataset was grouped using the k-means++ algorithm to find the anchor boxes that the model used.\n3.3.1. The Improvement of CSPDarknet53-Tiny A color picture has three channels of RGB. After convolution by different convolution kernels, each channel produces new channels. The new channels\u2019 features reflect the image components on distinct convolutional kernels, which do not contribute equally to the\nFigure 2. Structure of the YOLOv4-tiny network.\n3.3.\nr t ifi fi i ti task, e improved the YOLOv4-tiny algorithm\u2019s ability to extract features by adding an i proved BEC at ention echanis o le to a CSPDarknet53-tiny structure, combining an i i l- ra id-pooling module\nith the FPN struct re and d ing a Yolo detection layer to the Yolo head. The CCTSDB2021 traffic sign dataset was grouped using the k-means++ algorithm to find the anchor box s that the model used.\n3.3.1. - i\nl l f R B. After convolution by different convolution kernels, each channel produces new chan els. The new chan els\u2019 features reflect he image comp nents on distinct convolutional kernels, which do not a ly to the task\u2019s crucial information. The performance of a network can be improved by blocking out irrelevant information and giving important information a higher weight value. In 2019, Hu et al. [33] proposed the SENet channel attention mechanism, which significantly enhanced the performance of convolutional neural network models. ECANet [34] is an improved lightweight channel attention mechanism compared to the SENet module. Global averaging pooling is performed before processing the features. Global averaging pooling sums and averages all weights of the same channel, which results in some high and low weights being averaged and a loss of information about the high weights. As a result, in this paper, we used Better-ECA [35], an improved ECA attention mechanism that incorporated\nSensors 2023, 23, 749 6 of 23\nmaximum global pooling (BECA). Figure 3 depicts the improved BECA channel attention mechanism\u2019s structure.\nSensors 2023, 23, x FOR PEER REVIEW 6 of 24\ntask\u2019s crucial information. The performance of a network can be improved by blocking out irrelevant information and giving important information a higher weight value. In 2019, Hu et al. [33] proposed the SENet channel attention mechanism, which significantly enhanced the performance of convolutional neural network models. ECANet [34] is an improved lightweight channel attention mechanism compared to the SENet module. Global averaging pooling is performed before processing the features. Global averaging pooling sums and averages all weights of the same channel, which results in some high and low weights being averaged and a loss of information about the high weights. As a result, in this paper, we used Better-ECA [35], an improved ECA attention mechanism that incorporated maximum global pooling (BECA). Figure 3 depicts the improved BECA channel attention mechanism\u2019s structure.\nGAP\nMAP\nX\nC\nH\nW\nW=1\u00d71\u00d7C\nU=1\u00d71\u00d7C\nZ=1\u00d71\u00d7C\n\u03c3\nL=1\u00d71\u00d7C\nC\nH\nW\nAdaptive Selection of Kernel Size:\nk=\u03a6(C)\nk X\nFigure 3. BECA structure.\n1. Feature compression In this step, global average pooling was utilized to compress the input H \u00d7 W \u00d7 C features into 1 \u00d7 1 \u00d7 C features W, while maximum global pooling was used to extract the maximum value of the channels to produce 1 \u00d7 1 \u00d7 C features U. The features acquired in the two parts were then subjected to a fusion operation, and their channel information on the corresponding channels was summed as shown in Equation (1): \ud835\udc4d = \ud835\udc4a + \ud835\udc48 (1) where \ud835\udc4a is the feature information of the global average pooling channel, and \ud835\udc48 is the feature information of the global maximum pooling channel. 2. Characteristic incentive\nA one-dimensional convolution with a convolution kernel of size \ud835\udc58 captured only the k-neighboring channels of the input features instead of all the channels. This could significantly reduce the parameters and computational costs. The convolved features were then activated by the sigmoid activation function to output the feature information of each channel, where the operation could be represented by Equation (2): \ud835\udc60 = \ud835\udf0e(\ud835\udc361\ud835\udc37 (\ud835\udc66)) (2) where \ud835\udf0e is the sigmoid activation function, \ud835\udc66 denotes the 1 \u00d7 1 \u00d7 C feature \ud835\udc4d being convolved, \ud835\udc361\ud835\udc37 denotes the one-dimensional convolution, and the size of the one-dimensional convolution kernel is indicated by k. k was obtained by Equation (3): \ud835\udc58 = \ud835\udf19(\ud835\udc36) = \ud835\udc59\ud835\udc5c\ud835\udc54 (\ud835\udc36)2 + 12 (3) where \ud835\udc36 denotes the given channel dimension, and \ud835\udc5c\ud835\udc51\ud835\udc51 is the nearest odd number after taking the absolute value calculation.\nFigure 3. BE structure.\n1. i\nIn this step, global average po ling was utilized to compres the input H \u00d7W \u00d7 C features into 1 \u00d7 C features , il i l l li t e tract the\naxi um value of the chan els to produce 1 \u00d7 1 t r s acq ired in the t o parts ere then subjected to a fusion operation, and their channel infor ation on the corresponding channels was summed as shown in Equation (1):\nZc = Wi + Ui (1)\nwhere Wi is the feature information of the global average pooling channel, and Ui is the feature information of the global maximum pooling channel.\n2. Characteristic incentive\nA one-dimensional convolutio with convolution kernel of size k captured nly the k-neighboring channels of the input features instead of all the chan els. This co ld significantly reduce the parameters and computational costs. The convolved features were then activated by the sigmoid activation function to output the feature information of each channel, where the operation could be represented by Equation (2):\ns = \u03c3(C1Dk(y)) (2)\nwhere \u03c3 is the sigmoid activation function, y denotes the 1 \u00d7 1 \u00d7 C feature Z being convolved, C1D denotes the one-dimensional convolution, and the size of the one-dimensional convolution kernel is indicated by k. k was obtained by Equation (3):\nk = \u03c6(C) = \u2223\u2223\u2223\u2223 log2(C)2 + 12 \u2223\u2223\u2223\u2223 odd\n(3)\nwhere C denotes the given channel dimension, and odd is the nearest odd number after taking the absolute value calculation.\n3. Feature recalibration\nThe weight information of each channel obtained in Step 2 was multiplied by the corresponding original channel features, thereby achieving the goal of recalibrating the original feature information by enhancing the task-critical channel information in all the channels and suppressing the unimportant channel information. The operation could be represented by Equation (4):\nX\u0303c = Lc\u00b7Xc (4)\nSensors 2023, 23, 749 7 of 23\nwhere Lc represents the weight coefficient of each channel, and Xc represents the original channel feature information for each channel. In this study, an improved lightweight channel attention mechanism was added to the CSP module of a CSPDarknet53-tiny network. This greatly improved the network\u2019s ability to extract important feature information while reducing the number of parameters and computations to improve the accuracy of the network\u2019s detection.\n3.3.2. The Improvement of the Feature Pyramid and Detection Network\nIn the traditional structure of a convolutional neural network, a fully connected layer is connected after the convolutional layer. Since the number of features in the fully connected layer is fixed, the size of the input image on the input side of the network is also fixed. In practical applications, the input image size is typically inadequate and must be cropped and stretched, which frequently distorts the image. Spatial pyramid pooling (SPP) [36] can generate fixed-scale features by processing input images of arbitrary sizes or scales and is robust to changes in the size and shape of an input image. Its structure is shown in Figure 4.\nSensors 2023, 23, x FOR PEER REVIEW 7 of 24\n3. Feature recalibration The weight information of each channel obtained in Step 2 was multiplied by the corresponding original channel features, thereby achieving the goal of recalibrating the original feature information by enhancing the task-critical channel information in all the channels and suppressing the unimportant channel information. The operation could be represented by Equation (4): \ud835\udc4b = \ud835\udc3f \u00b7 \ud835\udc4b (4) where \ud835\udc3f represents the weight coefficient of each channel, and \ud835\udc4b represents the original channel feature information for each channel. In this study, an improved lightweight channel attention mechanism was added to the CSP module of a CSPDarknet53-tiny network. This greatly improved the network\u2019s ability to extract important feature information while reducing the number of parameters and computations to improve the accuracy of the network\u2019s detection.\n3.3.2. The Improvement of the Feature Pyramid and Detection Network In the trad tional structure of a convolutional neural network, a fully conn cted layer is connected after the c nvolutional layer. Since the number of features in the fully connected layer is fixed, the size of the input image on the input sid of the network is also fixed. In ractical applications, the input image size is typically in dequate and must be cropp d an stretc ed, which frequently d storts the image. Spatial pyramid pooling (SPP) [36] can gener t ixed-scale features by processing input images of arbitrary sizes or scales and is robust to changes in the size and shape of an input image. Its structure is shown in Figure 4.\nMax Pooling\nInput Feature Map\n...\n...\nFully-connected layers\nW\u00d7H\u00d7C\n4\u00d7C 1\u00d7C 9\u00d7C\n14\u00d7C\nFigure 4. Spatial pyramid pooling.\nInspired by the idea of SPP and YOLOv3-spp [37], this study improved a traditional SPP module, and the improved structure is shown in Figure 5. The structure consisted of five branches. The first branch connected the input directly to the output, the second branch downsampled the input through a maximum pooling of size 3 \u00d7 3 and then output, the third branch downsampled the input through a maximum pooling of size 5 \u00d7 5 and then output, the fourth branch downsampled the input through a maximum pooling of size 7 \u00d7 7 and then output, and the fifth branch downsampled the input through a maximum pooling of size 9 \u00d7 9 and then output. Since the step size of the pooling layer was 1 and the padding operation was performed before the pooling operation, the length, width, and depth of the feature map output from these five branches were the same. Finally,\nFigure 4. Spatial pyramid pooling.\nInspired by the idea of SP and YOLOv3-sp [37], this study improved a traditional SPP module, and t i t t i in Figure 5. The structure con isted of five branches. The first branch connected the i t t output, the second branch downsampled the input hrough a maximum po ling of size 3 \u00d7 3 and then output, the third branch downsampled the input through a maximum po ling of size 5 \u00d7 5 and then output, the fourth branch downsampled the input through a maxi um pooling of size 7 \u00d7 7 and then output, and the fifth branch downsampled th input through a maxi um pooling f size 9 \u00d7 9 and the output. Since the step siz of the pooling layer was 1 and the padding operation was performed before the pooling peration, the length, width, and depth of the feature map output from these five branch s were the sam . Finally, these five feature maps were concatenated. This dense SPP network was added after the backbone network since YOLOv4-tiny disregards the fusion of multiscale local region features on the same convolutional layer. This dense SPP network converted the 13 \u00d7 13 \u00d7 512 feature maps generated by the 15th convolutional layer into 13 \u00d7 13 \u00d7 2560 feature maps. This structure achieved the fusion between feature maps of local and global features, and the multiscale fusion enhanced the characterization ability of the feature maps so that more features were passed to the next layer of the network. The number of input feature maps was then reduced from 2560 to 256 using 1 \u00d7 1 convolution to extract useful features from\nSensors 2023, 23, 749 8 of 23\nthe large number of relevant features, which were later pooled to different scales to improve the detection accuracy of traffic signs.\nSensors 2023, 23, x FOR PEER REVIEW 8 of 24\nthese five feature maps were concatenated. This dense SPP network was added after the backbone network since YOLOv4-tiny disregards the fusion of multiscale local region features on the same convolutional layer. This dense SPP network converted the 13 \u00d7 13 \u00d7 512 feature maps generated by the 15th convolutional layer into 13 \u00d7 13 \u00d7 2560 feature maps. This structure achieved the fusion between feature maps of local and global features, and the multiscale fusion enhanced the characterization ability of the feature maps so that more features were passed to the next layer of the network. The number of input feature maps was then reduced from 2560 to 256 using 1 \u00d7 1 convolution to extract useful features from the large number of rel vant features, which were later pooled to different scales to improve the detection accuracy of traffic signs.\nMulti-scale MaxPooling\nInput Feature Map\nConcatenate\n...\nOutput Feature Maps\nFeature Maps\nW\u00d7H\u00d7512\nW\u00d7H\u00d7512 W\u00d7H\u00d7512 W\u00d7H\u00d7512\nW\u00d7H\u00d72560\nW\u00d7H\u00d7512\nW\u00d7H\u00d7512\nFigure 5. Improved dense spatial pyramid pooling.\nWe enhanced the Yolo head module to increase the YOLOv4-tiny network\u2019s ability to identify traffic signs. Following the enhanced feature extraction network, a YOLO detection layer was added. To create this detection layer, we fine-tuned a second YOLO detection layer and added convolutional layers with channel sizes of 128, 256, 512, and 24. The final output of this detection layer was a high-dimensional feature map of 52 \u00d7 52 \u00d7 24, which enhanced the accuracy of target localization and prediction. YOLOv4-tiny can only generate feature maps with the dimensions of 13 \u00d7 13 \u00d7 24 and 26 \u00d7 26 \u00d7 24. With these improvements, the TSR-YOLO algorithm achieved first YOLO layer outputting feature maps of 13 \u00d7 13 \u00d7 24, second YOLO detection layer outputting feature maps of 26 \u00d7 26 \u00d7 24, and third YOLO detection layer outputting feature maps of 52 \u00d7 52 \u00d7 24. This method could better detect long-distance traffic signs in complex scenarios and solve the problem of inaccurate localization and prediction of YOLOv4-tiny when locating small targets at a far distance. The network\u2019s three YOLO detection layers were used to process and forecast the bounding boxes, objectness score, class predictions, and anchor boxes, where anchor boxes were used to identify the bounding boxes for each object in each class in the traffic sign recognition dataset. Because three detection layers were used and there were three classes of traffic signs in the dataset, the number of channels for each detection layer was calculated by the formula (class + 4 + 1) \u00d7 3 before designing each YOLO detection layer, and the channel size was set to 24. After completing the above improvements, the TSR-\nFigure 5. Improved dense spatial pyramid pooling.\ne enhanced the Yolo head module to increase the YOLOv4-tiny network\u2019s ability to identify traffic signs. Following the enhanced featur extraction network, a YOLO detection layer was added. To create this d tection layer, we fine-tuned a second YOLO detection layer and added convolutiona layers with channel sizes of 128, 256, 512, and 24. The final output of this detection layer was a high-dimensional fe ture map of 52 \u00d7 52 4, which en a ced the accura y of target localization and pre iction. YOLOv4-tiny ca only generate feature maps with the dimensions of 13 \u00d7 13 24 and 26 \u00d7 26 \u00d7 24. With these improvements, the TSR-YOLO algorit m achieved first YOLO layer outputting feature maps of 13 \u00d7 13 \u00d7 24, secon YOLO detecti n layer outputting feature maps of 26 \u00d7 26 \u00d7 24, and third YOLO detection layer outputting feature maps of 52 \u00d7 52 \u00d7 24. This method could better detect long-distance traffic signs in complex scenarios and solve the problem of inaccurate localization and prediction of YOLOv4-tiny when locating small targets at a far distance. The network\u2019s three YOLO detection layers were used to process and forecast the bounding boxes, objectness score, class predictions, and anchor boxes, where anchor boxes were used to identify the bounding boxes for each object in each class in the traffic sign recognition dataset. Because three detection layers were used and there were three classes of traffic signs in the dataset, the number of channels for each detection layer was calculated by the formula (class + 4 + 1) \u00d7 3 before designing each YOLO detection layer, and the channel size was set to 24. After completing the above improvements, the TSR-YOLO algorithm structure and the algorithm\u2019s detailed network configuration are given in Figure 6.\nSensors 2023, 23, 749 9 of 23\nSensors 2023, 23, x FOR PEER REVIEW 9 of 24\nYOLO algorithm structure and the algorithm\u2019s detailed network configuration are given in Figure 6.\n3.4. Anchor Boxes Using K-Means++ Clustering\nThe original YOLOv4-tiny model\u2019s anchor boxes were obtained by clustering the COCO dataset [38] and the Pascal VOC dataset [39]. By analyzing these datasets, we found that the targets in these datasets were more different in size and shape from those in the traffic sign dataset, and the background of the traffic sign dataset was more complex. As a result, the original anchor box size was unsuitable for Chinese traffic sign detection task and may harm the model\u2019s training results. Figure 7 shows samples from the Pascal VOC dataset and the CCTSDB2021 dataset.\n3.4. + Clustering i ti el\u2019s anchor boxes were obtained by clustering the t s t ataset [39]. By analyzing these datasets, we found t t t t t re ore dif erent in size and shape from those in the traffic sign dataset, an the backgro of t e traffic sign dataset as ore co plex. s a resul , the original anchor box size was s i i se traf ic sign detection task a lts. Figure 7 shows samples from the Pascal VOC\nat s t\n(a) Some samples of the Pascal VOC dataset\n(b) Some samples of the CCTSDB2021 dataset\nFigure 7. Some samples in the Pascal VOC and CCTSDB2021 datasets.\nA typical k-means clustering algorithm [40] was used in the original YOLOv4-tiny model for a dimensional clustering analysis of training images to obtain prior boxes. Nevertheless, the randomness of the k-means algorithm for the selection of initial clustering centers may have detrimental impacts on the clustering effect. The k-means++ clustering algorithm was used instead of the k-means clustering algorithm in this work to improve the accuracy with which the proposed target detection network predicted a target\u2019s location. The k-means++ clustering algorithm featured less clustering randomness, which reduced the bias of clustering results produced by the random selection of initial clustering centers. k-means++ was utilized to cluster the CCTSDB2021 dataset for Chinese traffic signs in order to generate more accurate and representative anchor boxes. The k-means++ technique for clustering works was as follows: 1. Determine the number of cluster centers k and the height and width set M of Chinese traffic signs in the given data. 2. Choose one point randomly from the set M to satisfy the initial clustering center \ud835\udc5e . 3. Determine \ud835\udc37(\ud835\udc65) the distance between each remaining point \ud835\udc65 in the set M and its\nnearest clustering center \ud835\udc5e . The greater the distance between the prior box and the next clustering center, the greater the probability \ud835\udc43(\ud835\udc65). This step should be repeated until k clustering centers are found. \ud835\udc37(\ud835\udc65) = 1 \u2212 \ud835\udc3c\ud835\udc42\ud835\udc48(\ud835\udc65, \ud835\udc5e ) (5)\n\ud835\udc43(\ud835\udc65) = \ud835\udc37(\ud835\udc65)\u2211 \ud835\udc37(\ud835\udc65)\u2208 (6)\nFigure 7. So e sa ples in the Pascal and TS B2021 datasets.\nt i l orith [40] as used in the original YOLOv4-tiny a dimensional clustering nalysis of training images to obtain prior boxes.\nNev rtheless, the randomness of the k-means algorithm for the s lection of in tial clustering ce t r i ff t. he k- eans + clustering algorith as used instead of the k-means clustering algorithm in this work to improve the accuracy with w ich the pro osed target detec ion network predicted a t rget\u2019s location. The k-m ans++ clustering algorithm featured less clu tering andomness, which reduced the bias of clu tering r sults produced by the random selection of initial clustering c nters. k-means++ was utilized to cluster he CCTSDB2021 dataset for Chinese traffic signs in order to generate more accurate and represent tive anchor boxes. The k-means++ technique for clustering works was as follows:\n1. eter ine the nu ber of cluster centers k and the height and width set M of Chinese traffic signs in the given data. 2. hoose one point rando ly fro the set M to satisfy the initial clustering center q1. 3. eter ine D(x) the distance between each remaining point x in the set M and its\nnearest clustering center qx. The greater the distance between the prior box and the next clustering center, the greater the probability P(x). This step should be repeated until k clustering centers are found.\nD(x) = 1\u2212 IOU(x, qx) (5)\nP(x) = D(x)2\n\u2211x\u2208N D(x)2 (6)\nIOU(x, qx) denotes the intersection ratio between the clustering center and each labeled box.\nSensors 2023, 23, 749 11 of 23\n4. Determine the distance D(x) between all points in the set M and the k cluster centers, and place the point in the cluster center category with the smallest distance. For the clustering results, recalculate each clustering category center Ci.\nCi = \u2211x\u2208Ci x |Ci|\n(7)\n5. When the cluster center Ci of each clustering category no longer changes, repeat Step 2 and output k cluster center results.\nThe CCTSDB2021 dataset was initially analyzed, and Figure 8 shows the annotated information of the samples in the dataset. The data for the clustering algorithm are in the annotated red box in Figure 8. Therefore, we give more detail about the data in the an-notated red box. The bndbox tag specifies the location of a traffic sign within an image, the xmin value specifies the horizontal coordinate of the upper-left corner of a traffic sign bounding box, and the ymin tag specifies the vertical coordinate of the upper-left corner of a traffic sign bounding box. The xmax tag specifies the horizontal coordinate of a sign\u2019s lower-right corner. The ymax tag specifies the vertical coordinate of a bounding box\u2019s lower right corner.\nSensors 2023, 23, x FOR PEER REVIEW 11 of 24\n\ud835\udc3c\ud835\udc42\ud835\udc48(\ud835\udc65, \ud835\udc5e ) denotes the intersection ratio between the clustering center and each labeled box. 4. Determine the distance D(x) between all points in the set M and the k cluster centers,\nand place the point in the cluster center category with the smallest distance. For the clustering results, recalculate each clustering category center \ud835\udc36 .\n\ud835\udc36 = \u2211 \ud835\udc65\u2208|\ud835\udc36 | (7) 5. When the cluster center \ud835\udc36 of each clustering category no longer changes, repeat Step\n2 and output k cluster center results. The CCTSDB2021 dataset was initial y analyzed, and Figure 8 shows the annotated\ninformation of the samples in the dat set. The data for the clustering al orithm are in the annotated red box in Figure 8. Therefore, we giv more detail about the data in the annotated r box. The bndbox tag specifies the location of a traffic sign within an image, the xmin value specifies the horizontal coordinate of the upper-left corner of a traffic sign bounding box, t e i t s ecifies the vertical c ordinate of the u per-left corner of a traffic sign bounding box. The x ax tag s ecifies t e orizo tal coor i ate of a sign\u2019s lower-right corner. The ymax tag specifies the vertical coordinate of a bounding box\u2019s lower right corner.\nFigure 8. Annotation information of the image.\nThe width and height of each traffic sign\u2019s bounding box were used as horizontal and vertical coordinates. The width and height were normalized with respect to the original image to obtain the distribution of the actual boxes of traffic signs in the CCTSDB2021 dataset. Following the above k-means++ algorithm steps, a cluster analysis was then performed using the CCTSDB2021 dataset, with k set to 9, and nine clustering results were obtained for the CCTSDB2021 dataset, where the black plus sign is the cluster center, as shown in Figure 8. The nine clustering centers in Figure 9 were (0.009375, 0.01805556), (0.0140625, 0.02638889), (0.02109375, 0.0375), (0.018, 0.04857143), (0.022, 0.06285714), (0.03203125, 0.05448718), (0.029, 0.07714286), (0.042, 0.1), and (0.07533351, 0.14571429), respectively. The final parameters of anchor boxes needed to be transformed according to the original image size. When the input image size was 416\u00d7416, the coordinate values of the clustering centers were multiplied by 416 to obtain the nine clustering centers in the\nFigure 8. A notation information of the image.\nThe width and height of each traffic sign\u2019s bounding box were used as horizontal and vertical coordinates. The width and height were normalized with respect to the original image to obtain the distribution of the actual boxes of traffic signs in the CCTSDB2021 dataset. Following the above k-means++ algorithm steps, a cluster analysis was then performed using the CCTSDB2021 dataset, with k set to 9, and nine clustering results were obtained for the CCTSDB2021 dataset, where the black plus sign is the cluster center, as shown in Figure 8. The nine clustering centers in Figure 9 were (0.009375, 0.01805556), (0.0140625, 0.02638889), (0.02109375, 0.0375), (0.018, 0.04857143), (0.022, 0.06285714), (0.03203125, 0.05448718), (0.029, 0.07714286), (0.042, 0.1), and (0.07533351, 0.14571429), respectively. The final parameters of anchor boxes needed to be transformed according to the original image size. When the input image size was 416 \u00d7 416, the coordinate values of the clustering centers were multiplied by 416 to obtain the nine clustering centers in the original image of (4, 8), (6, 11), (9, 16), (8, 20), (9, 26), (14, 23), (12, 32), (18, 42), and (32, 61). Large anchor boxes were used to predict big traffic signs, and small anchor boxes were used to predict small traffic signs. Thus, the prior boxes of (12, 32), (18, 42), and (32, 61) were used\nSensors 2023, 23, 749 12 of 23\nto predict the bounding box at the scale of 13 \u00d7 13; the prior boxes of (8, 20), (9, 26), and (14, 23) were used to predict the bounding box at the scale of 26 \u00d7 26; the remaining three prior boxes of (4, 8), (6, 11), and (9, 16) were used to predict the bounding box at the scale of 52 \u00d7 52.\nSensors 2023, 23, x FOR PEER REVIEW 12 of 24\noriginal image of (4, 8), (6, 11), (9, 16), (8, 20), (9, 26), (14, 23), (12, 32), (18, 42), and (32, 61). Large anchor boxes were used to predict big traffic signs, and small anchor boxes were used to predict small traffic signs. Thus, the prior boxes of (12, 32), (18, 42), and (32, 61) were used to predict the bounding box at the scale of 13 \u00d7 13; the prior boxes of (8, 20), (9, 26), and (14, 23) were used to predict the bounding box at the scale of 26 \u00d7 26; the remaining three prior boxes of (4, 8), (6, 11), and (9, 16) were used to predict the bounding box at the scale of 52 \u00d7 52.\nFigure 9. Distribution of clustering centers in the CCTSDB2021 dataset.\n3.5. Traffic Detection Using TSR-YOLO The traffic sign detection process based on TSR-YOLO included dataset preprocessing, model training, and detection of traffic signs, as shown in Figure 10. First, the CCTSDB2021 training dataset was preprocessed to improve traffic sign detection performance and prevent model overfitting. In the training phase, the dataset was first loaded, and the anchor boxes were generated using the k-means++ clustering algorithm; the training parameters were then set, and the TSR-YOLO network model was initialized; finally, the weights of the TSR-YOLO model were iteratively updated using the loss function to converge the loss function and obtain the model weights for traffic sign detection. An image or video was input during the traffic sign detection phase, the trained model weights were loaded, and traffic signs were predicted. At this period, the obtained prediction results contained multiple prediction boxes that overlapped. Redundant prediction boxes were removed using the non-maximum suppression (NMS) algorithm, and the final test results were output.\nFigure 9. Distr bution of clustering ce ers in the CCTSDB2021 dataset.\n3.5. Traffic Detection Using TSR-YOLO\nThe traffic sign detection process based on TSR-Y LO included dataset preprocessing, model training, a d detection of tra fic sign , as shown in F gure 10. First, the CCTSDB2021 training dat set was preprocessed to improve traffic s gn det ction pe mance and preve t model overfitting. I the training phase, the dataset as first loa ed, and the anchor boxes were generated using the k-means++ clustering algorithm; the training parameters were then set, and the TSR-YOLO network model was initialized; finally, the weights of the TSR-YOLO model were iteratively updated using the loss function to converge the loss function and obtain the model weights for traffic sign detection. An image or video was input during the traffic sign detection phase, the trained model weights were loaded, and traffic signs were predicted. At this period, the obtained prediction results contained multiple prediction boxes that overlapped. Redundant prediction boxes were removed using the non-maximum suppression (NMS) algorithm, and the final test results were output. Sensors 2023, 23, x FOR PEER REVIEW 13 of 24\nStart\nCCTSDB2021 DataSet\nData Preprocessing\nLoss Convergence?\nEnd\nTraining Dataset Loading\nAnchor boxes Setting\nParameters Setting\nNetwork Initialization\nModel Training\nTSR-YOLO model weights\nInput image or video\nModel Loading\nPrediction\nNMS\nOutput\nYes\nNo\nFigure 10. The traffic detection process based on TSR-YOLO.\n4. Experimental Section 4.1. Dataset\nIn the current research on traffic sign detection and recognition, the algorithm performance was primarily evaluated using well-known public transportation sign datasets, such as the GTSDB (German Traffic Sign Detection Benchmark), the BTSD (Belgian Traffic Sign Dataset), and the STSD (Swedish Traffic Sign Dataset) [41]. The aforementioned datasets are limited to European traffic signs, the samples are gathered primarily under optimal lighting settings, and there are substantial discrepancies between Chinese and European traffic signs [42]. We used the CCTSDB2021 dataset, which was composed of 423 films from traffic recorders at varied times, locations, and weather conditions, to accomplish real-time detection of Chinese traffic signs in difficult circumstances. The dataset included street traffic scenes, high-speed traffic scenes, rain traffic scenes, evening traffic scenes, and backlight traffic scenes. The diversity and coverage of the dataset were wellensured, which was more compatible with the task of Chinese traffic sign detection under complex scenarios. According to Table 1, the traffic signs in the CCTSDB2021 dataset were divided into three categories based on their respective meanings: prohibitive signs, warning signs, and mandatory signs. The prohibitive signs had a white background, a red circle, a red bar, and a black pattern, and their shapes were a circle, octagon, or equilateral triangle with the top angle pointing downward. The warning signs had a yellow background, a black border, and a black pattern, and their shape was an equilateral triangle with the top angle pointing upward. The mandatory signs had a blue background and a white pattern, and their bodies were composed of a circle, a rectangle, or a square. The training set for this dataset had 16,356 images, with 13,876 prohibitive signs, 4598 warning signs, and 8363 mandatory signs. This dataset\u2019s test set was composed of 1500 images, and the entire test set had 3228 traffic signs. In a ratio of 9:1, the training set was divided into a training set and a validation set.\nTable 1. Selected examples of three types of traffic signs in the CCTSDB2021 dataset.\nProhibitory Warning Mandatory\nFigure 10. The traffic detection process based on TSR-YOLO.\nSensors 2023, 23, 749 13 of 23\n4. Experimental Section 4.1. Dataset\nIn the current research on traffic sign detection and recognition, the algorithm performance was primarily evaluated using well-known public transportation sign datasets, such as the GTSDB (German Traffic Sign Detection Benchmark), the BTSD (Belgian Traffic Sign Dataset), and the STSD (Swedish Traffic Sign Dataset) [41]. The aforementioned datasets are limited to European traffic signs, the samples are gathered primarily under optimal lighting settings, and there are substantial discrepancies between Chinese and European traffic signs [42]. We used the CCTSDB2021 dataset, which was composed of 423 films from traffic recorders at varied times, locations, and weather conditions, to accomplish real-time detection of Chinese traffic signs in difficult circumstances. The dataset included street traffic scenes, high-speed traffic scenes, rain traffic scenes, evening traffic scenes, and backlight traffic scenes. The diversity and coverage of the dataset were well-ensured, which was more compatible with the task of Chinese traffic sign detection under complex scenarios. According to Table 1, the traffic signs in the CCTSDB2021 dataset were divided into three categories based on their respective meanings: prohibitive signs, warning signs, and mandatory signs. The prohibitive signs had a white background, a red circle, a red bar, and a black pattern, and their shapes were a circle, octagon, or equilateral triangle with the top angle pointing downward. The warning signs had a yellow background, a black border, and a black pattern, and their shape was an equilateral triangle with the top angle pointing upward. The mandatory signs had a blue background and a white pattern, and their bodies were composed of a circle, a rectangle, or a square. The training set for this dataset had 16,356 images, with 13,876 prohibitive signs, 4598 warning signs, and 8363 mandatory signs. This dataset\u2019s test set was composed of 1500 images, and the entire test set had 3228 traffic signs. In a ratio of 9:1, the training set was divided into a training set and a validation set.\nTable 1. Selected examples of three types of traffic signs in the CCTSDB2021 dataset.\nProhibitory Warning Mandatory\nSensors 2023, 23, x FOR PEER REVIEW 13 of 24 Start CCTSDB2021 DataSet Data Preprocessing Loss Convergence?\nEnd\nTraining Dataset Loading Anchor boxes Setting\nParameters Setting\nNetwork Initialization Model Training\nTSR-YOLO model weights\nInput image or video Model Loading Prediction NMS\nOutput\nYes\nNo\nFigure 10. The traffic detection process based on TSR-YOLO.\n4. Experimental Section 4.1. Dataset\nIn the current research on traffic sign detection and recognition, the algorithm performance was primarily evaluated using well-known public transportation sign datasets, such as the GTSDB (German Traffic Sign Detecti n Benchma k), the BTSD (Belgian Traffic Sign Dataset), a d the STSD (Swedish Traffic Sign Da aset) [41]. The aforementioned datasets are limited to European traffic signs, the samples are gathered primarily under optimal lighting settings, and there are substantial discrep ncies between Chinese and European traffic igns [42]. We us d the CCTSDB2021 dataset, which was composed of 423 films from r ffic recorders at vari d times, loc tions, and weather conditions, to accomplish real-time detection of Chinese traffic signs in difficult circumstances. The dataset included street traffic sc nes, high-speed traffic scenes, ra n traffic scenes, evening traffic scenes, and backlight tr ffic cen s. The diversity and coverage of the dataset were wellensured, which was more compatible with th task of Chinese traffic sign detection under complex scenarios. According to Table 1, the traffic signs in the CCTSDB2021 dataset were divided into three categories based on their respective meanings: prohibitive signs, warning signs, and mandatory signs. The prohibitive signs had a white background, a red circle, a red bar, and a black pattern, and their shapes were a circle, octagon, or equilateral triangle with the top angle pointing downward. The warning signs had a yellow background, a black border, and a black pattern, and their shape was an equilateral triangle with the top angle pointing upward. The mandatory signs had a blue background and a white pattern, and their bodies were composed of a circle, a rectangle, or a square. The training set for this dataset had 16,356 images, with 13,876 prohibitive signs, 4598 warning signs, and 8363 mandatory signs. This dataset\u2019s test set was composed of 1500 images, and the entire test set had 3228 traffic signs. In a ratio of 9:1, the training set was divided into a training set and a validation set.\nTable 1. Selected examples of three types of traffic signs in the CCTSDB2021 dataset.\nProhibitory Warning Mandatory\nSensors 2023, 3, x FOR PEER REVIEW 13 of 24 Start CCTSDB2021 DataSet Data Preprocessing Loss Convergence?\nEnd\nTraining Dataset Loading Anchor boxes Setting\nParameters Setting\nNetwork Initialization Model Training\nTSR-YOLO model weights\nInput image or video Model Loading Prediction NMS\nOutput\nYes\nNo\nFigure 10. The traffic detection process ba ed on TSR-YOLO.\n4. Experim ntal Section 4.1. Dataset\nIn the current search on traffic sign detection and recognitio , the algorithm performance was prima ily evaluated using well-know public transportation sig datasets, such as he GTSDB (G rma Traffic Sign Detection Be c ark), the BTSD (Belgian Traffic Sign Dataset), and the STSD ( wedish Traffic Sign Dataset) [41]. The aforementioned datasets are limited to European tr ffic sign , the samples ar gathered prima ily under optimal lig ting settings, and there are substantial discrepancies betwe n Chinese and European tr ffic sign [42]. We used the CCTSDB2021 dataset, which was comp sed of 423 films from traffic reco d rs at varied times, locati ns, a d weath r conditions, to accomplish real-time detection of Chinese traffic sign in difficult ircumstances. The dataset included street traffic scenes, high-speed traffic scenes, rain traffic scenes, evening traffic scenes, and backlight traffic scenes. The diversity and coverag of the dataset w re wellensured, which was more c mpatible with the task of Chinese traffic sign detection under complex sc narios. According to Table 1, the traffic sign in the CCTSDB2021 dataset w re divided into three cat gories based on their respectiv meanings: prohibitive sign , warning sign , a d mandatory sign . The prohibitive sign had a white background, a red circle, a red bar, and black pattern, and their shapes w re a circle, octagon, or equilateral tri ngle with the top angle pointing downward. The warning sign had a yellow background, a black border, and black pattern, and their shape was an equilateral tri ngle with the top angle pointing upward. The mandatory sign had a blue background a white pattern, and their bodies w re comp sed of a circle, a rectangle, or a square. The training set for this dataset had 16,356 images, with 13,876 prohibitive sign , 4598 warning sign , a d 8363 mandatory sign . This dataset\u2019s t st s t was comp sed of 1500 images, and the entir test set had 3228 traffic sign . I a ratio of 9:1, the training set was divi ed into a training set and validation set.\nTable 1. S lected examples of three typ s of traffic signs in the CCTSDB2021 dataset.\nProhibitory Warning Mandatory\nSensors 2023, 3, x FOR PEE REVIEW 13 of 24 Start CCTSDB2021 DataSet Data Preprocessing Loss Convergence?\nEnd\nTraining Dataset Loading Anchor boxes Setting\nParameters Set ing\nNetwork Initializat on Model Training\nTSR-YOLO model weights\nInput image or vid Model Loa ing Prediction NMS\nOutput\nYes\nNo\nFigure 10. The traffic detection pr cess ba ed on TSR-YOLO.\n4. Experim ntal Section 4.1. Dataset\nIn the current search on traffic sign detection and recog itio , the algorithm performance was prima ly evaluated using well-k o public transportation s g data ets, such as t e GTSDB ( erman Traffic Sign Detection B hmark), t e BTSD (Belgian Traffic Sign Dataset), a d the STSD ( wedish Traffic Sign Dataset) [41]. The aforem nti n d datasets are lim ted to Eur pean tr ffic signs, the samples r g thered prima ly unde optimal lighting settings, and there are subst ntial d screpa cies betwe n Chin se and European tr ffic signs [42]. We u ed th CCTSDB2021 dataset, which was comp sed of 423 films from traffic recorder at v ried t mes, locati s, a d weather conditions, to acc mplish real-time detection of Chinese traffic signs in difficult ircumstances. The dataset inclu ed street traffic scenes, high-spee traffic scenes, rain traffic scenes, evening traffic scenes, and backlight traffic scenes. The divers ty and coverag of the dataset w re wellensured, which was more c mpatible with th task of Chinese traffic ign detection under complex sc narios. According to Table 1, the traffic signs n the CCTSDB2021 dataset w re divid d into three cat gories based on their respectiv meanings: prohibitive s gns, warni g s gns, a d mandatory signs. The prohibitive s gns had a white background, a ed circle, a red bar, and black p ttern, and their shapes w re a circle, octagon, r equilateral tri ngle with the top angle pointing d wnward. The warning s gns had a yellow background, a black border, and black p ttern, and their shape wa an quilateral tri ngle with th top angle pointing upward. The mandatory signs had a blue ackground a white p ttern, and their bodies w re comp sed of a circle, a rectangle, or a square. The training set for this dataset had 16,356 images, with 13,876 prohibitive s gns, 4598 warning signs, a d 8363 mandatory signs. This dataset\u2019 t st t was comp sed of 1500 images, and the entir t st set had 3228 traffic signs. I a ratio of 9:1, the training set was divided into a trai ing set and validation set.\nTable 1. S lected exampl s of three typ s of traffic signs in the CCTSDB2021 dataset.\nProhibitory Warning Mandatory\nSensors 2023, 23, x FOR PEE R VIEW 13 of 24 Start CCTSDB2021 DataSet Data Preprocessing Loss Convergence?\nEnd\nTraining Dataset Loading Anchor boxes Setting\nParameters Setting\nNetwork Initialization Model Training\nTSR-YOLO model weights\nInput image or video Model Loading Prediction NMS\nOutput\nYes\nNo\nFigure 10. The t affic det tion proc ss based on TSR-YOLO.\n4. Experimental Section 4.1. Dataset\nIn the current resea h on tr ffic sign detect o and r cognition, he al ori m performance was pri rily evaluated using well-known public transportati sign datasets, such as t e GTSDB (Germa Tr ffic Sign Detect o B c m rk), the BTSD (B lgian Tr ffic Sign Dataset), and th STSD (Swedish raffic Sign Dataset) [41]. Th forementioned datasets r limi ed to Eur pean traffic signs, the amples ar athered p ima ily un er optimal ligh n settings, and th re re ubstantial di crep ncies b tw en Chi ese and European traffic signs [42]. We used th CCTSDB2021 dat set, which was composed f 423 films from t affic reco d rs at vari d tim , locatio , and we ther co itions, t acc mplish real-time det ction of Ch n se traffic igns in difficult circumstances. The dataset included street affic sce , high-speed traffic scene , rain t ffic scene , ev ni g traffic scene , a d backlight traffic scene . The div rsity and co ge of the dataset were llensured, which was more compatible with he task of C inese traff c sig d tect o under complex s enarios. According to Table 1, the traffic signs in the CCTSDB2021 dataset were divided nto three cat go i s based on their re pective meanings: proh bitive signs, warning signs, and mandatory signs. The prohibitive signs ad a white background, a red circle, a r d bar, and a black p ttern, and heir s apes were a circl , o tag n, or quil teral riangle with the top angl pointing downwar . The arning signs h d a yellow background, a black border, and a black p ttern, and their s ape was an equilater l riangle with the top angl pointing upward. The mandatory signs had a blue ackground and a white pattern, and their bodies wer compo d f a circle, a r ct ngle, or a squ re. Th training set for hi dataset had 16,356 images, with 13,876 prohibitive signs, 4598 warning signs, and 8363 mandatory signs. Thi dataset\u2019s test set was composed f 1500 images, nd th entire est set had 3228 tr ffic signs. In ratio of 9:1, he training se was divided nto a training set and a validation set.\nTable 1. Selected xamples of three yp s of traffic signs in the CCTSDB2021 dataset.\nProhibitory Warning Mandatory\nSensors 2023, 23, x FOR PEER REVIEW 13 of 24 Start CCTSDB2021 DataSet Data Preprocessing Loss Convergence?\nEnd\nTraining Dataset Loading Anchor boxes Setting\nParameters Setting\nNetwork Initialization Model Training\nTSR-YOLO model weights\nInput image or video Model Loading Prediction NMS\nOutput\nYes\nNo\nFigure 10. The traffic detection process base on TSR-YOLO.\n4. Experimental Section 4.1. Dataset\nIn the current research on traffic sign detection and recognition, the algorithm performance was primarily evaluated using well-known p blic transportation sign datasets, such as the GTSDB (Germ n Traffic Sign Detection Benchmark), the BTSD (Belgi n Traffic Sign Dataset), and the STSD (Swedish Traffic ign Dataset) [41]. The aforementioned datasets are li ited to European traffic signs, the s mples are athered primarily under optimal lighting settings, and t ere are substantial discrepancies between Chinese and European traffic signs [42]. We used the CCTSDB2021 dataset, which was composed of 423 films fro traffic r corders at varie times, locations, and weather conditio s, to ccomplish real-time detection of Chinese tr ffic signs in difficult circumsta ces. The dataset included street traffic cenes, high-speed traffic scenes, rain traffic scenes, evening traffic scenes, a d backlight traffic scenes. The diversity and coverage of th da aset were ellnsured, which as more compa ible with the task of Chinese traffic sign detection under omplex scenario . Accord ng to T ble 1, t e traffic sign i th CCTSDB2021 datas t w re divided into three categories based on their respectiv meanings: prohibitive signs, warning signs, and mandatory signs. The prohibitive signs ad a white back round, a red circle, a red bar, and a black pattern, and their s pes wer a ircle, octagon, or equilateral tri gle with the top angle pointing do nw rd. The w rning signs had yellow background, a black border, and a bl ck patt rn, their shape was equilater l triangle with the top angle pointing upward. The mandatory signs ad blue background and a white pattern, and their bodies were composed of a circle, a recta gle, or a squ re. The training set for this dataset had 16,356 images, with 13,87 prohibitive signs, 4598 warning signs, and 8363\nandatory signs. This dat set\u2019s test set was composed of 1500 images, an the entire test set had 3228 traffic signs. In a ratio of 9:1, the training set was divided i to a training set and a validation set.\nTable 1. Selected examples of three types of traffic signs in the CCTSDB2021 dataset.\nProhibitory Warning Mandatory\nSensors 2023, 23, x FOR PEER REVIEW 13 of 24 Start CCTSDB2021 DataSet Data Preprocessing Loss Convergence?\nEnd\nTraining Dataset Loading Anchor boxes Setti g\nParameters Setting\nNetwork Ini ialization Model Training\nTSR-YOLO model weights\nInput image or video Model Loading Prediction NMS\nOutput\nYes\nNo\nFigure 10. The traff c detection process bas d TSR-YOLO.\n4. Experimental Sectio 4.1. Dataset\nIn the curre t r sea ch on traffi sign dete tion and rec gnition, the lg ithm performance was primarily valu ted using w l-known public tra sportation ign datasets, such as the GTSDB (Germ Traffic Sign te tion Benchmark), the BTSD (Belgi n Traffic Sign Dataset), and he ST D (Swedish Traffic ign Da aset) [41]. Th foremen ioned daasets are limit d to European traffic sig s, the s ples are ath r d primarily und r optimal lighting settings, and ther re ubsta tial discrepancie betwee Chines and European traffic signs [42]. We u ed the CCTSDB 021 datas t, which was composed of 423 films from traffic recorders at va ied times, loca ions, an w ather conditi , ac mplish real-time detection of hin se traffic signs in difficult circumsta es. The dataset included street traffic s nes, high- p d traffic scenes, rain traffi sc n s, evening traffic scenes, and backlight tr ffic scenes. The div rsity and overage of th dataset were wellensured, which wa more compatible with the task of Chine e tr ffic sign d t ction und r complex scenarios. According to Table 1, he traffic signs in the CCTSDB2021 datas t w re divided into three categori s b sed on t eir respective meanings: prohib tive si s, warn ng gns, and mandatory signs. The prohibitive signs ad a whi e ackgrou , a red circl , a red bar, and a black pattern, th ir shapes were circle, octagon, or equil eral triangl wi h the top angle p inti downward. The ing signs h d a yellow backgr und, a b ac border, and a black pattern, and th ir shape was a equil t ral tri gl with the top angle pointing u ward. The m ndatory signs ad blue b ckgro nd and whit p ttern, their bodies were compos d of a circle, a rectangle, or a squ re. The tr ining for this dataset had 16,356 im ges, with 3 87 prohibitive signs, 4598 warn ng , a d 8363 mandatory signs. This dataset\u2019s test set was composed of 1500 images, and th entir test set had 3228 raffic signs. In a ratio of 9:1, the tr ini g set was divided in o a ra n g et and a valid tion set.\nTable 1. Selected ex mples of thr e types of traffic signs in h CCTSDB2021 dataset.\nProhibitory Warning Mandatory\nSensors 2023, 23, x FOR PEER REVIEW 14 of 24\n4.2. Experiment Configuration In this study, we built neural networks with the Pytorch deep-learning framework and trained them on a GPU server using the parameters in Table 2 for the experimental environment.\nTable 2. Experimental environmental parameters.\nExperimental Environment Environment Configuration Operating system Windows11\nCPU Intel(R) Core (TM) i7-10750 H CPU @ 2.60 GHz GPU NVIDIA GeForce RTX 2060\nProgramming language Python 3.10 Deep-learning framework Pytorch 1.12\nAcceleration platform CUDA11.3;cuDNN8.2\nWhen training the network model in the above experimental setting, no pretrained model was used. The input image size of the network was set to 416 \u00d7 416, and the weight parameters of the convolutional neural network were optimized using a stochastic gradient descent (SGD) optimizer. The learning rate was adjusted using cosine annealing LR. The TSR-YOLO model was updated and optimized to obtain the optimal model after several stable iterations of learning on the training set images. The main parameter settings for model training are shown in Table 3.\nTable 3. Experimental parameters of network training.\nAttribute Value epoch 500\nbatch size 16 initial learning rate 0.001\nmomentum 0.937 weight_decay 0.0005 input shape (416, 416)\nmosaic true mixup true lr_decay_type cos\n4.3. Evaluation Metrics Multiple evaluation criteria were employed to analyze the proposed modified Yolov4-tiny model from diverse angles in order to evaluate the detection performance of the algorithm in complex road scenarios more objectively. In this work, precision (P), recall (R), mean average precision (\ud835\udc5a\ud835\udc34\ud835\udc43), F-1 score, and frames per second (FPS) were utilized to evaluate the performance of the algorithm. FPS denoted the number of images processed per second in the evaluation metrics. While true positives (\ud835\udc47\ud835\udc43s) indicated that\nSensors 2023, 23, x FOR PEE R VIEW 14 of 2\n4.2. Experiment Co figuration In this study, we built neural networks with the Pytorch deep-l arning framework and trained th m on a GPU server using the parameters in Table 2 for the experim ntal environment.\nTable 2. Experimental viro mental par meters.\nExperim ntal Environme t Environme t Configuration Operating system Windows11\nCPU Intel(R) Core (TM) i7-10750 H CPU @ 2.60 GHz GPU NVIDIA GeForce RTX 2060\nProgramming la u ge Python 3.10 Deep-l arning framework Pytorch 1.12\nAcceleration platform CUDA11.3;cuDNN8.2\nWhen training the network model in the above experim ntal setting, no pretrained model was used. The input image size of the network was set to 416 \u00d7 416, and the weight parameters of the convoluti nal neural network were optimized using a stochastic gradient d scent (SGD) optimizer. The learning rate was adjusted using cosine anne li g LR. The TSR-YOLO model was updated nd optimized to ob ain the optimal odel aft r several stable iterations of learning o the training set images. The main p rameter settings for m del training are shown in Table 3.\nTable 3. Experimental par meters of network training.\nAttribu e Value epoch 500\nbatch size 16 initial learning rate 0.001\nmomentum 0.937 weight_decay 0.0005 input shape (416, 416)\nmosaic true mixup true lr_decay_type cos\n4.3. Evaluation Metrics Multiple evaluation criteria were employed to analyze the proposed modifie Yolov4-tiny model from diverse angles in order to valuate the det ction performance of the algorithm in complex road scenarios m re obj ctiv ly. In this work, p ecision (P), recall (R), mean average precision (\ud835\udc5a\ud835\udc34\ud835\udc43), F-1 score, and frames per second (FPS) were utilized to evaluate the performance of the algorithm. FPS denoted th number of images processed per second in the evaluation metrics. While tru positives (\ud835\udc47\ud835\udc43 ) indicated that\nSensors 2023, 23, x FOR PEE R VIEW 14 of 2\n4.2. Experiment Co figuration In this study, we b ilt neural netwo ks with the Pytorc deep-l arning framework and trained th m on a GPU server using the paramet rs in Table 2 for the expe im ntal environm t.\nTable 2. Experimental viro mental par me ers.\nExperim ntal Environme t Environme t C figuration Operating system Windows11\nCPU Intel(R) Core (TM) i7-10750 H CPU @ 2.60 GHz GPU NVIDIA GeForce RTX 2060\nProgramming l u ge Python 3.10 Deep-l arning framework Pytorch 1.12\nAcceleration p atform CUDA11.3;cu NN8.2\nWhen training the network model in the above experim ntal setting, no pretrai ed model was used. The input image size of the network was set to 416 \u00d7 416, and the weigh parameters of th convoluti nal ne ral network w re ptimized us ng a stochastic gradient d scent (SGD) optimizer. The learning rate was adjusted using cosine anne li g LR. The TSR-YOLO model was updated nd optimized to ob ain the optimal odel aft r several stable itera ions of learning o the training set image . The main p ra eter settings for m del training are shown in Table 3.\nTable 3. Experimental par me ers of network training.\nAttribu e Value epoch 500\nbatch size 16 initial lear ing rate 0.001\nmomentu 0.937 weight_decay 0.0005 input shape (416, 416)\nmosaic true mixup true lr_decay_type cos\n4.3. Evaluation Metrics Multiple eva uation criter a were employ d to analyze the proposed modifie Yolov4-tiny model fro diverse angles in order to valuate the det ction p rformance of the algori m in complex road sc narios m re obj ctively. In this work, p ecision (P), recall (R), mean average precision (\ud835\udc5a\ud835\udc34\ud835\udc43), F-1 score, and frames per second (FPS) were utilized to evaluate the performance f the alg ri m. FPS denoted th number of i ages processed per second in the evaluation metrics. While tru positives (\ud835\udc47\ud835\udc43 ) indicated that\nSensors 2023, 23, x FOR PEE R VIEW 14 of 2\n4.2. Experiment Co figuration In this study, we b ilt neural netwo ks with the Pytorc deep-l arning framewo k and trained th m on a GPU server using the paramet rs in T ble 2 for the exp im ntal enviro m t.\nTable 2. Experimental viro mental par meters.\nExperim ntal Environme t Environme t C figuration Operating system Windows11\nCPU Intel(R) Core (TM) i7-10750 H CPU @ 2.60 GHz GPU NVIDIA GeForce RTX 2060\nProgramming l u ge Python 3.10 Deep-l arning framewo k Pytorch 1.12\nAcceleration platform CUDA11.3;cu NN8.2\nWhen trai ing the e work model in the above exp rim ntal s ting, no pretrai ed model was used. The input image size of he network was set to 416 \u00d7 416, and the weigh parameters of th convoluti nal ne ral network were op imized us ng a stochastic gradient d scent (SGD) optimizer. The learning rate was adjusted using cosin anne li g LR. The TSR-YOLO model was update nd optimiz to ob ain the optimal m d l aft r several stable itera on of l arning on the training set images. The main p ra eter settings for m del training are shown T ble 3.\nTable 3. Experimental ar m ters of netwo k training.\nAttribu e Value epoch 500\nbatch size 16 initial lear ing ra e 0.001\nmomentu 0.937 weight_decay 0.0005 input shape (416, 416)\nmosaic true mixup true lr_decay_type cos\n4.3. Evaluation Metrics Multiple eva uation crit r a were employ d to analyze the proposed modified Yolov4-tiny model fro diverse angles in ord r to valuate the det ction p rformanc of the algorit m in complex road sc narios m re obj ctively. In this work, precis on (P), recall (R), mean average pr cision (\ud835\udc5a\ud835\udc34\ud835\udc43), F-1 score, and frames per second (FPS) were utilized to evaluate the performance of the alg ri m. FPS deno ed th number of i ages processed per second in th evaluatio m trics. While tru positives (\ud835\udc47\ud835\udc43 ) indicated that\nSensors 2023, 23, x FOR PEER REVIEW 14 of 24\n4.2. Experiment Configura on In this study, we bu lt neural n tworks with the Pytorch deep-learning framework and trained them on GPU s rver using th parameters in Tabl 2 for the xp rimental enviro ment.\nTable 2. Experimenta env ronmental para ters.\nExperimental E viro t Enviro ment Co figurati Operating system Windows11\nCPU Intel(R) Core (TM i7-10750 H CPU @ 2.60 G z GPU NVIDIA GeForc RTX 2060\nProgramming language Python 3.10 Deep-learning framework Pytorch 1.12\nAcceleration platform CUDA11.3;cu NN8.2\nWhen trai ing the network m d l in the above xp rimental setting, no pretrain d model was used. Th input image size of th ne work as s t o 416 \u00d7 , and the weight parameters of the conv luti nal neural e work e e optimized using a st chast grad - ent descent (SGD) op imizer. Th learning at w s adjuste using cosine an eali g LR. The TSR-YOLO model was up ated and op imize to obt in the p imal m del af er several stable iterations of e rning he tr i se images. Th main para et r s ttings for model training are shown in Tabl 3.\nTable 3. Experimenta ra ters of network raining.\nAttribu e Value epoch 500\nbatch size 16 initial learning at 0.001\nmomentum 0.937 weight_decay 0.0005 input shape (416, 416)\nmosaic true mixup true lr_decay_type cos\n4.3. Evaluation Metrics Multiple evaluation criteri were employed to analyze the prop sed m ifi Yolov4-tiny m del from diverse angl s in order to ev luate the detec ion performance of the algorithm in c mplex road s ena i s more objectively. In th s w rk, precisio (P), recall (R), me n averag precision (\ud835\udc5a\ud835\udc34\ud835\udc43), F-1 score, a d frames p r econd (FPS) were utilized to valuate the p rform nce of t e algorithm. FPS denoted the umber of imag s processed er s c nd in the evaluation metrics. Whi e true positives (\ud835\udc47\ud835\udc43 ) indicat that\nSensors 2023, 3, x FOR PEER R VIEW 14 of 24\n4.2. Experim nt Configuration In this study, we b ilt neural ne o ks with the Pytorc deep-learning framewo k and trained them on GPU server using the paramet rs T ble 2 fo th expe im ntal environm t.\nTable 2. Experim ntal enviro m tal para ters.\nExperimental Environme t Enviro me t C figuration Operating system Windows11\nCPU Intel(R) Core (TM) i7-10 50 H CPU @ 2.60 GHz GPU NVIDIA GeForce RTX 2060\nProgramming l ngu ge Python 3.10 Deep-learning framewo k Pytorch 1.12\nAcceleration platform CUDA11.3;cu NN8.2\nWhen training the ne work model in the ab ve experim ntal s tting, o pretrai d model was used. The input image siz of he network was set to 416 \u00d7 416, and the w igh parameters of th conv lutional ne ral network we e optimized us ng a st chastic gradient descent (SGD) optimizer. The learning rate was adjusted us cosine n ali g LR. The SR-YOLO model was update an optimiz to obtain he optimal del after several st ble itera ion f arning o the trai ing s t ima es. T m i para ter settings for model training are shown i T ble 3.\nTable 3. Experim ntal aram ters of netwo k tr ining.\nAttribute Value epoch 500\nbatch size 16 initial le r ing rate 0.001\nmomentu 0.937 weight_decay 0.00 5 input shape (416, )\nmosaic true mixup true lr_decay_type cos\n4.3. Evaluation Metrics Multiple eva uation crit r a were mploy d to analyze th r posed modifi d Yolov4-tiny model fro diverse a gl s in or r to aluate th detection p rformanc of the algorit m in complex r ad sc narios m re bject vely. In thi w k, pr ision (P), recall (R), mean aver g pr cision (\ud835\udc5a\ud835\udc34\ud835\udc43), F-1 score, and frames p r second (FPS) wer utilized to evaluate th performanc of t alg ri m. FPS deno d th number of i ages processed p r sec nd i th evaluatio m trics. W il rue posit v s (\ud835\udc47\ud835\udc43s) indicated that\nSensors 2023, 23, x FOR PEER REVIEW 14 of 24\n4.2. Experiment Configuration In this study, we built neural networks with the Pytorch deep-learning framework and trained them on a GPU server using the parameters in Table 2 for the experimental environment.\nTable 2. Experimental environmental parameters.\nExperimental Environment Environment Configuration Operating system Windows11\nCPU Intel(R) Core (TM) i7-10750 H CPU @ 2.60 GHz GPU NVIDIA GeForce RTX 2060\nProgramming language Python 3.10 Deep-learning framework Pytorch 1.12\nAcceleration platform CUDA11.3;cuDNN8.2\nWhen training the network model in the above experimental setting, no pretrained model was used. The input image size of the network was set to 416 \u00d7 416, and the weight parameters of the convolutional neural network were optimized using a stochastic gradient descent (SGD) optimizer. The learning rate was adjusted using cosine annealing LR. The TSR-YOLO model was updated and optimized to obtain the optimal model after several stable iterations of learning on the training set images. The main parameter settings for model training are shown in Table 3.\nTable 3. Experimental parameters of network training.\nAttribute Value epoch 500\nbatch size 16 initial learning rate 0.001\nmomentum 0.937 weight_decay 0.0005 input shape (416, 416)\nmosaic true mixup true lr_decay_type cos\n4.3. Evaluation Metrics Multiple evaluation criteria were employed to analyze the proposed modified Yolov4-tiny model from diverse angles in order to evaluate the detection performance of the algorithm in complex road scenarios more objectively. In this work, precision (P), recall (R), mean average precision (\ud835\udc5a\ud835\udc34\ud835\udc43), F-1 score, and frames per second (FPS) were utilized to evaluate the performance of the algorithm. FPS denoted the number of images processed per second in the evaluation metrics. While true positives (\ud835\udc47\ud835\udc43s) indicated that\nSensors 2023, 3, x FOR PEE REVIEW 14 of 24\n4.2. Experiment Configuration In this study, we built neural networks with the Pytorch deep-l arning framework and trained them on a GPU server using the parameters in Table 2 for the experim ntal environme t.\nTable 2. Experimental e viro mental parameters.\nExperim ntal Environme t Environme t Configuration Operating system Windows11\nCPU Intel(R) Core (TM) i7-10750 H CPU @ 2.60 GHz GPU NVIDIA GeForce RTX 2060\nProgramming la uage Python 3.10 Deep-learning framework Pytorch 1.12\nAcceleration platform CUDA11.3;cuDNN8.2\nWhen training the network model in the above experim ntal setting, no pretrained model was used. The input image siz of the network was set to 416 \u00d7 416, and the weight parameters of the convoluti nal neural network were optimized using a stochastic gradient descent (SGD) optimizer. The learning rate was adjusted using cosine a ne li g LR. The SR-YOLO model was update n optimized to obtain he optimal odel aft r several stable iterations f learning o the training set images. The main parameter settings for m del training are shown in Table 3.\nTable 3. Experimental parameters of network training.\nAttribute Value epoch 500\nbatch size 16 initial learning rate 0.001\nmomentum 0.937 weight_decay 0.0005 input shape (416, )\nmosaic true mixup true lr_decay_type cos\n4.3. Evaluation Metrics Multiple evaluation criteria were mployed to analyze th proposed modifie Yolov4-tiny model from diverse angles in order to evaluate the det ction performance of the algorithm in complex road scenarios m re obj ctiv ly. In this work, p ecision (P), recall (R), mean aver g precision (\ud835\udc5a\ud835\udc34\ud835\udc43), F-1 score, and frames per second (FPS) were utilized to evaluate the performance of the algorithm. FPS denot d the number of images processed p r second in the evaluation metrics. While tru positives (\ud835\udc47\ud835\udc43s) indicated that\nSensors 2023, 23, x FOR PEE R VIEW 14 of 2\n4.2. Experiment Co figuratio In this study, we built neural netwo ks with the Pytorc deep-l arning framework and trained th m on a GPU server using the parameters in Table 2 for th experim ntal enviro m t.\nTable 2. Experimental viro mental par me ers.\nExperim ntal Environme t Environme t Configuration Operating system Windows11\nCPU Intel(R) Core (TM) i7-10750 H CPU @ 2.60 GHz GPU NVIDIA GeForce RTX 2060\nProgrammin l u ge Python 3.10 Deep-l arning framework Pytorch 1.12\nAcceleration p atform CUDA11.3;cuDNN8.2\nWhen training the network model in the above experim ntal setting, no pretrained model was used. The input image size of the network was set to 416 \u00d7 416, and the weight parameters of th convoluti nal neural netwo k w re ptimiz d using a stochastic gradient d scent (SGD) optimizer. The lea ning rate was adjusted using cosine anne li g LR. The TSR-YOLO model was updated nd optimized to ob ain he opt mal odel ft r several stable iterations of lear ing on the trai ing set ima e . The main p rameter settings for m del training are shown in Table 3.\nTable 3. Experimental par me ers of network training.\nAttribu e Value epoch 500\nbatch size 16 initial lear ng rate 0.001\nmomentu 0.937 weight_decay 0.0005 input shape (416, 416)\nmosaic true mixup true lr_decay_type cos\n4.3. Evaluation Metrics Multiple evaluation criter a were employed to analyze the propos d modifie Yolov4-tiny model fro iverse angles in order to valuate the det ction p rformance f the algori m in complex road scenarios m re obj ctiv ly. In his work, p ecision (P), recall (R), mean average precision (\ud835\udc5a\ud835\udc34\ud835\udc43), F-1 score, and frames per second (FPS) were utilized to evalua e the performance f the alg ri m. FPS denoted th number of i ages processed per second in the evaluation metrics. While tru positives (\ud835\udc47\ud835\udc43 ) indicated that\nSensors 2023, 3, x FOR PEE R VIEW 14 of 24\n4.2. Experiment Co figu atio In this study, we built neural netwo ks with the Pytorc deep-l arning framework and trained them on a GPU server using th pa ameters in T ble 2 for the exp rim n al environm t.\nTable 2. Experimental viro ental parame ers.\nExperim ntal nvi onme t Environme t C figuration Operating system Windows11\nCPU Intel(R) Core (TM) i7-10750 H CPU @ 2.6 GHz GPU NVIDIA GeForce RTX 2060\nProgramming l u ge Python 3.10 Deep-l arning framework Pytorch 1.12\nAcceleration p atform CUDA11.3;cuDNN8.2\nWhen training the etwork model in the above experim ntal setting, no pretrai ed model was used. The input image size of the network was set o 416 \u00d7 416, and the weigh parameters of th convoluti nal ne ra netwo k w re ptimiz d us ng a stochastic gr dient descent (SGD) optimizer. The lea ning rate was adjusted using cosin anne li g LR. The TSR-YOLO model was update n optimized to ob a n th optimal od l af r several stable iterations of lear ing o the training se image . The main p ra ter settings for m del training are show i Table 3.\nTable 3. Experimental parame ers of network raining.\nAttribute Value epoch 500\nbatch size 16 initial lear ing rate 0.001\nmomentu 0.937 weight_decay 0.0005 input shape (416, )\nmosaic true mixup true lr_decay_type cos\n4.3. Evaluation Metrics Multiple eva uation criter a were employ d to analyze the proposed modifie Yolov4-tiny model fro divers angles in o der to valuate the d t ction p rformance of the algori m in complex road sc narios m re obj ctiv ly. In his work, p ecision (P), recall (R), mean average precision (\ud835\udc5a\ud835\udc34\ud835\udc43), F-1 score, and frames per second (FPS) w re utilized to evaluate the p rformance f the alg ri m. FPS denoted th number of i ages processed p r second in the evaluation metrics. While tru positives (\ud835\udc47\ud835\udc43s) indicated that\nSensors 2023, 23, x FOR PEER REVIEW 14 of 24\n4.2. Experiment Configura on In this study, we bu lt ne ral n works with th Pytorch deep-learning framework and tr ined them on a GPU s rver using th parameters in Tabl 2 for the xperimental environment.\nTable 2. Experimenta environmental para te s.\nExperimental E viron e Environment Co figurat Operating system Windows11\nCPU Intel(R) Core (TM i7-10750 H CPU @ 2.60 G z GPU NVIDIA GeForce RTX 2060\nProgramming language Python 3.10 Deep-learning framework Pytorch 1.12\nAcceleration platform CUDA11.3;cu NN8.2\nWhen training the ne work m d l in the above xperimental s t ing, no pretrain d model was used. Th input mage size of th ne w rk as s t o 416 \u00d7 416, and the weight parameters of the conv luti nal neural e work ere optimized using a st cha ti grad - ent descent (SGD) op imizer. Th learning at was adju te ing cosine ann ali g LR. The TSR-YOLO model was up ated and op imize to obtain the p imal model after several stable i erations of e rning he tr i se images. Th ma n para et r set ings for model training are shown in Tabl 3.\nTable 3. Experimenta p ra te s of network rai ing.\nAttribu e Value epoch 500\nbatch size 16 initial learning at 0.001\nmomentu 0.937 weight_decay 0.0005 input shape (416, 416)\nmosaic true mixup true lr_decay_type cos\n4.3. Evaluation Metrics Multiple evaluation cri eria were employed to analyze the prop sed m ifie Yolov4-tiny m del from div rse angl s i order to ev luate the de c ion performa c of the algorithm in c mplex r ad scena i s m re bjectively. In th s work, precisi (P), recall (R), mean averag p ecision (\ud835\udc5a\ud835\udc34\ud835\udc43), F-1 core, a d fr mes p r second (FPS) wer utilized to valuate the p rform nce of algorithm. FPS denoted the number of imag s processed er s cond in the evaluation metrics. Wh e tru po itives (\ud835\udc47\ud835\udc43s) indicat that\nSensors 2023, 23, x FOR PEE R VIEW 14 of 2\n4.2. Experiment Co figurat on In this study, we bu lt neural n works with the Pytorch deep-l arning framework and trained th m on a GPU s ver using th p ramet rs in Tabl 2 for the xp im nt l environment.\nTable 2. Experim nta viro m ntal par m ters.\nExperim ntal Environ Enviro me t Co figurati Operating system Windows11\nCPU Intel(R) Core (TM i7-10750 H CPU @ 2.60 G z GPU NVIDIA GeForce RTX 2060\nProgramming la u ge Python 3.10 Deep-l arning framework Pytorch 1.12\nAcceleration platform CUDA11.3;cuDNN8.2\nWhen training the network mod l in the above xperim ntal s tting, no pretrained model was used. The input image size of the ne w rk w s s t to 416 \u00d7 416, and the weight parameters of the conv luti nal n ural e w rk were optimiz d using a stochast c gradient d scent (SGD) op imizer. The learning at was adjusted using cosine ne li g LR. The TSR-YOLO model was up ated nd optimized to ob in the op imal del af r several stable iterations of e ni g th tr i s t images. The m p ramet r s ttings for m del training are shown in Tabl 3.\nTable 3. Experim nta par ters of network trai ing.\nAttribu e Value epoch 500\nbatch size 16 initial learning rat 0.001\nmomentum 0.937 weight_decay 0.0005 input shape (416, 416)\nmosaic true mixup true lr_decay_type cos\n4.3. Evaluation Metrics Multiple evaluation criteri were mployed to an lyz th pr p sed modified Yolov4-tiny m del from div rse angles in order to luate th det c ion perform ce o the algorithm in c mpl x road scena i s m re obj ctiv ly. I th s w rk, p ecis (P), recall (R), mean averag precision (\ud835\udc5a\ud835\udc34\ud835\udc43), F-1 core, a d frames p r second (FPS) were utilized to evaluate th p rform nce of the algorithm. FPS d noted th number of images processed er s cond in th valuation metrics. W le tru po i ives (\ud835\udc47\ud835\udc43 ) indicat d that\n4.2. Experiment Configuration\nIn this study, we built neural networks with the Pytorch deep-learning framework and trained them on a GPU server using the parameters in Table 2 for the experimental environment.\nSensors 2023, 23, 749 14 of 23\nTable 2. Experimental environmental parameters.\nExperimental Environment Environment Configuration\nOperating system Windows11 CPU Intel(R) Core (TM) i7-10750 H CPU @ 2.60 GHz GPU NVIDIA GeForce RTX 2060\nProgramming language Python 3.10 Deep-learning framework Pytorch 1.12\nAcceleration platform CUDA11.3;cuDNN8.2\nWhen training the network model in the above experimental setting, no pretrained model was used. The input image size of the network was set to 416 \u00d7 416, and the weight parameters of the convolutional neural network were optimized using a stochastic gradient descent (SGD) optimizer. The learning rate was adjusted using cosine annealing LR. The TSR-YOLO model was updated and optimized to obtain the optimal model after several stable iterations of learning on the training set images. The main parameter settings for model training are shown in Table 3.\nTable 3. Experimental parameters of network training.\nAttribute Value\nepoch 500 batch size 16\ninitial learning rate 0.001 momentum 0.937\nweight_decay 0.0005 input shape (416, 416)\nmosaic true mixup true lr_decay_type cos\n4.3. Evaluation Metrics\nMultiple evaluation criteria were employed to analyze the proposed modified Yolov4tiny model from diverse angles in order to evaluate the detection performance of the algorithm in complex road scenarios more objectively. In this work, precision (P), recall (R), mean average precision (mAP), F-1 score, and frames per second (FPS) were utilized to evaluate the performance of the algorithm. FPS denoted the number of images processed per second in the evaluation metrics. While true positives (TPs) indicated that the meaning of the identified traffic signs matched their actual meaning, false positives (FPs) occurred when traffic signs were identified, but the results of the detection contradicted the actual meaning of the signs. False negatives (FNs) were missed traffic signs by the model, P(R) was a function with R as a parameter, and \u201cclasses\u201d was the number of classes in the dataset. Average precision (AP) was the average accuracy of a single category\u2019s detection result. It showed how well the model worked at detecting the category\u2019s target. The mAP metric is often used to measure the accuracy of multicategory target detection. It was the average of the AP values of all the categories in the dataset and is one of the most important metrics for measuring how well target detection works. As a result, the metrics could be calculated using the equations below.\nPrecision = TP\nTP + FP (8)\nRecall = TP\nTP + FN (9)\nAP = \u222b 1\n0 P(R)d(R) (10)\nSensors 2023, 23, 749 15 of 23\nmAP = 1\nclasses\nclasses\n\u2211 i=1 \u222b 1 0 P(R)d(R) (11)\nF\u2212 1Score = 2 \u2217 (Precision \u2217 Recall) (Precision + Recall)\n(12)\n4.4. Experimental Results and Analyses 4.4.1. Evaluation Results\nUsing the CCTSDB2021 public dataset, the TSR-YOLO method was evaluated and compared to the original YOLOv4-tiny algorithm to produce more intuitive results. Table 4 displays the experimental detection results, as well as the evaluation metrics of AP, precision, recall, F-1 score, and mAP for each category in the dataset.\nThe mAP value of the TSR-YOLO algorithm for the CCTSDB2021 dataset was 8.23% higher than that of the original YOLOV4-tiny algorithm, as shown in Table 4, indicating that the algorithm proposed in this study had a high detection accuracy. Meanwhile, the F-1 score, recall, and precision of the TSR-YOLO algorithm were 3.04%, 1.60%, and 5.02% higher than those of YOLOV4-tiny, respectively. In addition, for the AP values of each class of the dataset, the AP values of the proposed algorithm for the prohibitive traffic sign class, the warning traffic sign class, and the mandatory traffic sign class were 92.51%, 93.54%, and 92.11%, respectively, which were improved by 11.46%, 8.98%, and 4.25%, respectively, compared with the original algorithm. The detection accuracy of the algorithm for each traffic sign class was improved to varying degrees, especially the detection accuracy of prohibitive traffic signs, which was greatly enhanced. To further compare the AP values in more detail, the PR curves for each category of these two algorithms are shown separately in Figure 11. The AP value for each category of traffic signs was the region contained by the PR curve and the coordinate axes. The AP value was higher and the performance was improved when the area that the angle and coordinate axes covered was larger. The figure also demonstrates that the TSR-YOLO model achieved a high detection performance in all three traffic sign categories, with considerable improvements in each category compared to YOLOv4-tiny, indicating that it was more capable of accurately identifying traffic signs in difficult settings. In order to evaluate the efficacy of the proposed algorithm, a relevant test was conducted in a natural scene using an in-car camera by selecting a video containing traffic signs for frame extraction and processing, processing the video into multiple images, and evaluating the processed images with the algorithm. Red identification boxes indicate prohibitive traffic signs, blue identification boxes represent mandatory traffic signs, and green identification boxes represent warning traffic signs in Figures 12 and 13.\nSensors 2023, 23, 749 16 of 23\nSensors 2023, 23, x FOR PEER REVIEW 16 of 24\nvalues in more detail, the PR curves for each category of these two algorithms are shown separately in Figure 11. The AP value for each category of traffic signs was the region contained by the PR curve and the coordinate axes. The AP value was higher and the performance was improved when the area that the angle and coordinate axes covered was larger. The figure also demonstrates that the TSR-YOLO model achieved a high detection performance in all three traffic sign categories, with considerable improvements in each category compared to YOLOv4-tiny, indicating that it was more capable of accurately identifying traffic signs in difficult settings.\n(a1) (b1)\n(a2) (b2)\n(a3) (b3)\nFigure 11. Comparison of PR curves for different categories, where (a1\u2013a3) are the PR curves for each type obtained by the YOLOv4-tiny algorithm, and (b1\u2013b3) are the PR curves for each type accepted by the TSR-YOLO model.\nFigure 11. Comparison of PR curves for different categories, where (a1\u2013a3) are the PR curves for each type obtained by the YOLOv4-tiny algorithm, and (b1\u2013b3) are the PR curves for each type accepted by the TSR-YOLO model.\nThese test images were captured on urban roads, as shown above, and were consistent with detecting traffic signs in complex scenarios. The detection precision of the algorithm described in this study was extremely high, and it achieved wide adoption. These test images were captured on the highway, as shown above, and the improved YOLOv4-tiny algorithm achieved full recognition with a very high detection accuracy. In conclusion, the method presented in this work could detect traffic signs in complicated environments.\nSensors 2023, 23, 749 17 of 23\nSensors 2023, 23, x FOR PEER REVIEW 17 of 24\nIn order to evaluate the efficacy of the proposed algorithm, a relevant test was conducted in a natural scene using an in-car camera by selecting a video containing traffic signs for frame extraction and processing, processing the video into multiple images, and evaluating the processed images with the algorithm. Red identification boxes indicate prohibitive traffic signs, blue identification boxes represent mandatory traffic signs, and green identification boxes represent warning traffic signs in Figures 12 and 13.\nFigure 12. Urban road test results.\nFigure 13. Highway test results.\nThese test images were captured on urban roads, as shown above, and were consistent with detecting traffic signs in complex scenarios. The detection precision of the algorithm described in this study was extremely high, and it achieved wide adoption. These test images were captured on the highway, as shown above, and the improved YOLOv4-tiny algorithm achieved full recognition with a very high detection accuracy. In conclusion, the method presented in this work could detect traffic signs in complicated environments.\n4.4.2. Performance Comparison The algorithm described in this study was thoroughly validated by comparing it to advanced traffic-sign-detecting methods using the CCTSDB2021 dataset. Multiple\nFigure 12. Urban road test results.\nSensors 2023, 23, x FOR PEER REVIEW 17 of 24\nIn order to evaluate the efficacy of the proposed algor thm, a relevant test was conducted in natural scene using an in-car cam ra by selecting a video conta nin traffic signs for frame extraction and processing, pr cessing the video into multiple images, and evaluating the processed imag s w th the algorithm. Red identificati n boxes indicate prohibitive traf sig s, blue identificatio boxes represent mandatory traffic signs, and green identification boxes represent warning traffic signs in Figures 12 and 13.\nFigure 12. Urban road test results.\nFigure 13. Highway test results.\nThese test images were captured n urban roads, as shown ab ve, and were consisten with detecting traffic signs in complex scenarios. The detection precision of the algorit m d cribed in this study was ex remely high, nd it achieved wide adoption. These test images were captured on the highway, as shown above, and the improved YOLOv4-tiny algorithm ac ieved full recognition with a very high dete tion accuracy. In conclusio , the method presented in this work could detect traffic signs in complicated environments.\n4.4.2. Performance Comparison Th algorithm described in this stu y was thoroughly validated by comparing it to advanced traffic-sign-detecting methods using the CCTSDB2021 dataset. Multiple\nFigure 13. Highway test results.\n4.4.2. Performance Comparison\nThe algorithm described in this study was thoroughly validated by comparing it to advanced traffic-sign-detecting methods using the CCTSDB2021 dataset. Multiple evaluation criteria were used in this experiment to perform a quantitative, all-around evaluation from many different points of view. The results of the comparison are shown in Table 5. The results of the detection are shown in Table 5. First, Faster R-CNN is a two-stage detection model that is relatively new. The algorithm had a high detection accuracy and could d tect traffic signs with precision, but its model was relatively vast and its detection speed was relatively slow. The YOLOv3, SSD, and YOLOv4 algorithms were the most representative one-stage model algorithms. Chen et al. proposed a more a v nced T-YOLO based on Yolov3, and from the table, we can see that the mAP value of this algorithm reached up to 97.30%, but the FPS value of this algorithm was only 19.30. Shan et al. changed an SSD model to further improve the algorithm\u2019s detection. Ren et al. combined a classical MobileNetv2 network with an SSD algorithm, which significantly improved the detection accuracy and speed, and the mAP value of YOLOv4 reached 95.8%; the size of\nSensors 2023, 23, 749 18 of 23\nthis model was 243.94 MB. It can be concluded that the detection accuracies of these models were relatively high. Nevertheless, the models were typically very large, and the detection speeds were slow, making them unsuitable for edge devices on smart automobiles in complicated scenarios for real-time detection. Yolov4-tiny is a great, light-weight detection model, and the speed of detection and the size of the model were better-suited for real-time detection on edge devices. However, this technique had low detection accuracy. TSRYOLO combined several optimization modules and improved the YOLOv4-tiny algorithm in terms of detecting traffic signs. In conclusion, the algorithm described in this research surpassed prior algorithms by balancing detection accuracy, detection speed, and model size and could better match the requirements of intelligent vehicle-sensing systems for the real-time detection of complicated road environments.\nThe TSR-YOLO algorithm was tested with YOLOv4-tiny in four complex environments selected from the CCTSDB2021 dataset, including a well-lit environment, a night environment, a rainy environment, and a snowy environment. The test results are depicted in Figures 14\u201317, where \u201cprohibitory\u201d represents prohibitive traffic signs, \u201cwarning\u201d represents warning traffic signs, and \u201cmandatory\u201d represents mandatory traffic signs. (a) and (c) represent the detection results of TSR-YOLO, whereas (b) and (d) represent the detection results of YOLOv4-tiny. Figure 14 shows that the method described in this paper worked well in places where there was enough light. In the first figure, the TSR-YOLO algorithm\u2019s detection accuracy for prohibitive traffic signs was 100% and 58%, while YOLOv4-tiny\u2019s detection accuracy was 98% and one of the traffic signs was not detected. The algorithm shown in this study was 14% more accurate than YOLOv4-tiny\u2019s detection accuracy, and the improvement was observable. The second figure shows that the TSR-YOLO algorithm and YOLOv4-tiny algorithm detected 100% of the warning traffic signs, but the YOLOv4-tiny algorithm incorrectly identified the background as a traffic sign. The algorithm in this work had more advantages in an environment with ideal lighting conditions. In a night environment, the detection results are shown in Figure 15. As shown above, the TSR-YOLO algorithm had a higher detection accuracy in the nighttime scenario than the YOLOv4-tiny method. However, the detection results of the two methods did not differ much, with an error rate of less than 3%. Figure 16 displays the results for detection on rainy days. Compared to the YOLOv4-tiny method, the detection results of the TSR-YOLO algorithm in rainy conditions were increased by around 7%, which considerably enhanced the detection accuracy. From the first image, the algorithm in this paper had more accurate target localization than YOLOv4-tiny in rainy environments. Detection results in a snowy climate are shown in Figure 17.\nSensors 2023, 23, 749 19 of 23Sensors 2023, 23, x FOR PEER REVIEW 19 of 24\n(a) (b)\n(c) (d)\nFigure 14. Comparison results of different algorithms in excellent lighting conditions, where (a,c) are the results of TSR-YOLO, (b,d) are the results of YOLOv4-tiny.\n(a) (b)\n(c) (d)\nFigure 15. Comparison results of different algorithms in a night scenario, where (a,c) are the results of TSR-YOLO, (b,d) are the results of YOLOv4-tiny.\nFigure 14. Comparison results of different algorithms in excellent lighting conditions, where (a,c) are the results of TSR-YOLO, (b,d) are the results of YOLOv4-tiny.\nSensors 2023, 23, x FOR PEER REVIEW 19 of 24\n(a) (b)\n(c) (d)\nFigur 14. Comparison results of diff rent algorithms in excelle t lighting conditions, where (a,c) are the results of TSR-YOLO, (b,d) are the results of YOLOv4-tiny.\n(a) (b)\n(c) (d)\nFigure 15. Comparison results of different algorithms in a night scenario, where (a,c) are the results of TSR-YOLO, (b,d) are the results of YOLOv4-tiny.\nFigure 15. Comparison results of different algorithms in a night scenario, where (a,c) are the results of TSR-YOLO, (b,d) are the results of YOLOv4-tiny.\nSensors 2023, 23, 749 20 of 23 Sensors 2023, 23, x FOR PEER REVIEW 20 of 24\nSensors 2023, 23, x FOR PEER REVIEW 20 of 24\nSensors 2023, 23, 749 21 of 23\nThe detection results of the TSR-YOLO algorithm differed greatly from those of the YOLOv4-tiny method, as depicted in the images above. The YOLOv4-tiny algorithm had a scenario of leakage detection with a relatively low detection rate. The technique presented in this research improved the detection accuracy by approximately 20% compared to the YOLOv4-tiny algorithm, and there was no leakage detection, which improved the accuracy of traffic sign detection. In a snowy environment, the TSR-YOLO algorithm outperformed the YOLOv4-tiny method. The above results demonstrate that TSR-YOLO had a significant effect on detection accuracy in normal lighting conditions, increasing it by 14%. In a snowy environment, the effect of the 20% boost was astounding. Even though TSR-YOLO did not improve the detection effect as dramatically as on sunny days and in snowy environments, it still improves the detection effect by 7% on rainy days. Due to noises such as rain, the detection results showed that TSR-YOLO could find traffic signs more accurately than the original algorithm. This shows that the algorithm was more robust. In a dark environment, the majority of the image backgrounds were black, background interference was minimal, and due to the effect of light, the traffic signs were clearer and easier to detect. Thus, the detection accuracies of TSR-YOLO and YOLOv4-tiny were nearly equivalent, and the rate of accuracy for each was close to 100%. In conclusion, the improved network was more adaptable to a complex natural environment and had improved localization and recognition accuracy.\n5. Discussion\nCompared to the YOLOv4-tiny algorithm, the TSR-YOLO algorithm significantly improved the detection accuracy for Chinese traffic sign recognition in complex scenarios, but the improved method still had limitations. First, TSR-YOLO could only roughly recognize traffic signs in three categories\u2014warning, prohibitive, and mandatory\u2014without fine-grained division, which was insufficient for scenarios requiring more precise traffic sign detection results. Second, this study only performed tests in four situations: a well-lit environment, a night environment, a rainy environment, and a snowy environment. It did not take into account all natural situations, such as other extreme weather conditions and conditions where traffic signs are blocked, faded, or broken. For future research, we plan to optimize the TSR-YOLO model to make it suitable for environments with greater complexity.\n6. Conclusions\nAn enhanced traffic sign detection algorithm based on YOLOv4-tiny was proposed to address the issue of the low accuracy of current lightweight networks at detecting traffic signs in complex circumstances. Some improvement strategies were offered based on YOLOv4-tiny. The k-means++ clustering technique first built suitable anchor boxes for a traffic sign dataset. A BECA module was then implemented to improve the model\u2019s ability to extract essential feature information in response to the fact that the extracted characteristics of the backbone network were mostly concentrated on a CSP module. In addition, a dense SPP module was added to the upgraded feature extraction network so that the convolutional neural network could fuse local and global features more effectively. Lastly, a Yolo detecting layer was added to more precisely detect and localize small targets at a great distance in a complicated environment, hence enhancing the algorithm\u2019s detection performance and achieving improved detection results. The experiments showed that, for the CCTSDB2021 dataset, the algorithm described in this paper was faster and more accurate than both YOLOv4-tiny and other excellent models. Therefore, the suggested network was more suited for real-time traffic sign detection on edge terminals deployed in intelligent vehicle-driving systems.\nAuthor Contributions: Writing\u2014original draft preparation, W.S.; writing\u2014review and editing, W.S. and S.A.S. All authors have read and agreed to the published version of the manuscript.\nSensors 2023, 23, 749 22 of 23\nFunding: This research received no external funding.\nInstitutional Review Board Statement: Not applicable.\nInformed Consent Statement: Not applicable.\nData Availability Statement: The data that support the findings of this study are openly available in CCTSDB 2021 at https://doi.org/10.22967/HCIS.2022.12.023.\nAcknowledgments: We would like to thank the anonymous reviewers for their helpful remarks.\nConflicts of Interest: The authors declare no conflict of interest.\nReferences 1. Radu, M.D.; Costea, I.M.; Stan, V.A. Automatic Traffic Sign Recognition Artificial Intelligence\u2014Deep Learning Algorithm. In\nProceedings of the 2020 12th International Conference on Electronics, Computers and Artificial Intelligence (ECAI), Bucharest, Romania, 25\u201327 June 2020; pp. 1\u20134.\n2. Zhang, J.; Zou, X.; Kuang, L.-D.; Wang, J.; Sherratt, R.S.; Yu, X. CCTSDB 2021: A More Comprehensive Traffic Sign Detection Benchmark. Hum. Cent. Comput. Inf. Sci. 2022, 12, 23. 3. Islam, K.T.; Wijewickrema, S.; Raj, R.G.; O\u2019Leary, S. Street Sign Recognition Using Histogram of Oriented Gradi-ents and Artificial Neural Networks. J. Imaging 2019, 5, 44. [CrossRef] [PubMed] 4. Pan, Y.; Kadappa, V.; Guggari, S. Chapter 15\u2014Identification of Road Signs Using a Novel Convolutional Neural Network. In Cognitive Informatics, Computer Modelling, and Cognitive Science; Sinha, G.R., Suri, J.S., Eds.; Academic Press: Cambridge, MA, USA, 2020; pp. 319\u2013337. ISBN 978-0-12-819443-0. 5. Khan, M.N.; Das, A.; Ahmed, M.M.; Wulff, S.S. Multilevel Weather Detection Based on Images: A Machine Learning Approach with Histogram of Oriented Gradient and Local Binary Pattern-Based Features. J. Intell. Transp. Syst. 2021, 25, 513\u2013532. [CrossRef] 6. Hechri, A.; Mtibaa, A. Two-Stage Traffic Sign Detection and Recognition Based on SVM and Convolutional Neural Networks. IET Image Process. 2020, 14, 939\u2013946. [CrossRef] 7. Yu, Q.; Zhou, Y. Traffic Safety Analysis on Mixed Traffic Flows at Signalized Intersection Based on Haar-Adaboost Algorithm and Machine Learning. Saf. Sci. 2019, 120, 248\u2013253. [CrossRef] 8. Kuang, X.; Fu, W.; Yang, L. Real-Time Detection and Recognition of Road Traffic Signs Using MSER and Random Forests. Int. J. Online Eng. 2018, 14, 34. [CrossRef] 9. Hu, C.; He, X. Traffic Sign Detection Based on MSERs and SVM. Comput. Sci. 2022, 49, 325\u2013330. 10. Dai, X.; Yuan, X.; Le, G.; Zhang, L. Detection method of traffic signs based on color pair and MSER in the complex environment. J. Beijing Jiaotong Univ. 2018, 42, 107\u2013115. (In Chinese) 11. Krizhevsky, A.; Sutskever, I.; Hinton, G.E. ImageNet Classification with Deep Convolutional Neural Networks. Commun. ACM 2017, 60, 84\u201390. [CrossRef] 12. Girshick, R.; Donahue, J.; Darrell, T.; Malik, J. Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation.\nIn Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Columbus, OH, USA, 23\u201328 June 2014; pp. 580\u2013587.\n13. Liu, W.; Anguelov, D.; Erhan, D.; Szegedy, C.; Reed, S.; Fu, C.Y.; Berg, A.C. SSD: Single Shot Multibox Detector. In Proceedings of the European Conference on Computer Vision (ECCV), Amsterdam, The Netherlands, 8\u201316 October 2016; pp. 21\u201337. 14. Ren, S.; He, K.; Girshick, R.; Sun, J. Faster R-CNN: Towards Realtime Object Detection with Region Proposal Networks. IEEE Trans. Pattern Anal. Mach. Intell. 2017, 39, 1137\u20131149. [CrossRef] 15. Ren, S.; He, K.; Girshick, R.; Sun, J. Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. In Advances in Neural Information Processing Systems; Curran Associates, Inc.: New York, NY, USA, 2015; Volume 28. 16. Redmon, J.; Divvala, S.; Girshick, R.; Farhadi, A. You Only Look Once: Unified, Real-Time Object Detection. In Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, USA, 27\u201330 June 2016; pp. 779\u2013788. 17. Redmon, J.; Farhadi, A. YOLO9000: Better, Faster, Stronger. In Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, HI, USA, 21\u201326 July 2017; pp. 7263\u20137271. 18. Redmon, J.; Farhadi, A. YOLOv3: An Incremental Improvement. arXiv 2018, arXiv:1804.02767. 19. Bochkovskiy, A.; Wang, C.-Y.; Liao, H.-Y.M. YOLOv4: Optimal Speed and Accuracy of Object Detection. arXiv 2020, arXiv:2004.10934. 20. Zhang, H.; Qin, L.; Li, J.; Guo, Y.; Zhou, Y.; Zhang, J.; Xu, Z. Real-Time Detection Method for Small Traffic Signs Based on Yolov3. IEEE Access 2020, 8, 64145\u201364156. [CrossRef] 21. Zhang, J.; Xie, Z.; Sun, J.; Zou, X.; Wang, J. A cascaded R-CNN with multiscale attention and imbalanced samples for traffic sign detection. IEEE Access 2020, 8, 29742\u201329754. [CrossRef] 22. Cui, L.; Lv, P.; Jiang, X.; Gao, Z.; Zhou, B.; Zhang, L.; Shao, L.; Xu, M. Context-Aware Block Net for Small Object Detection. IEEE\nTrans. Cybern. 2020, 52, 2300\u20132313. [CrossRef]\nSensors 2023, 23, 749 23 of 23\n23. Wang, C.-Y.; Bochkovskiy, A.; Liao, H.-Y.M. Scaled-YOLOv4: Scaling Cross Stage Partial Network. In Proceedings of the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Nashville, TN, USA, 20\u201325 June 2021; pp. 13029\u201313038. 24. Liu, C.; Li, S.; Chang, F.; Wang, Y. Machine Vision Based Traffic Sign Detection Methods: Review, Analyses and Perspectives. IEEE Access 2019, 7, 86578\u201386596. [CrossRef] 25. Guofeng, T.; Huairong, C.; Yong, L.; Kai, Z. Traffic Sign Recognition Based on SVM and Convolutional Neural Network. In Proceedings of the 2017 12th IEEE Conference on Industrial Electronics and Applications (ICIEA), Siem Reap, Cambodia, 18\u201320 June 2017; pp. 2066\u20132071. 26. Yucong, S.; Shuqing, G. Traffic Sign Recognition Based on HOG Feature Extraction. J. Meas. Eng. 2021, 9, 142\u2013155. [CrossRef] 27. Madani, A.; Yusof, R. Traffic Sign Recognition Based on Color, Shape, and Pictogram Classification Using Sup-port Vector Machines. Neural Comput. Appl. 2018, 30, 2807\u20132817. [CrossRef] 28. Wali, S.B.; Abdullah, M.A.; Hannan, M.A.; Hussain, A.; Samad, S.A.; Ker, P.J.; Mansor, M.B. Vision-Based Traffic Sign Detection and Recognition Systems: Current Trends and Challenges. Sensors 2019, 19, 2093. [CrossRef] 29. Zuo, Z.; Yu, K.; Zhou, Q.; Wang, X.; Li, T. Traffic Signs Detection Based on Faster R-CNN. In Proceedings of the 2017 IEEE\n37th International Conference on Distributed Computing Systems Workshops (ICDCSW), Atlanta, GA, USA, 5\u20138 June 2017; pp. 286\u2013288.\n30. Li, J.; Wang, Z. Real-Time Traffic Sign Recognition Based on Efficient CNNs in the Wild. IEEE Trans. Intell. Transp. Syst. 2019, 20, 975\u2013984. [CrossRef] 31. Shan, H.; Zhu, W. A Small Traffic Sign Detection Algorithm Based on Modified SSD. IOP Conf. Ser. Mater. Sci. Eng. 2019, 646, 012006. [CrossRef] 32. Chen, C.; Wang, H.; Zhao, Y.; Wang, Y.; Li, L.; Li, K.; Zhang, T. A depth based traffic sign recognition algorithm. Telecommun. Technol. 2021, 61, 76\u201382. 33. Hu, J.; Shen, L.; Sun, G. Squeeze-and-Excitation Networks. In Proceedings of the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, USA, 18\u201323 June 2018; pp. 7132\u20137141. 34. Wang, Q.; Wu, B.; Zhu, P.; Li, P.; Zuo, W.; Hu, Q. ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks. arXiv 2019, arXiv:1910.03151. 35. Chen, J. Design of Fruit and Vegetable Electronic Scale System BASED on Deep Learning; Southwest Jiaotong University: Chengdu, China, 2021. [CrossRef] 36. He, K.; Zhang, X.; Ren, S.; Sun, J. Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recogni-tion. IEEE Trans. Pattern Anal. Mach. Intell. 2015, 37, 1904\u20131916. [CrossRef] 37. Available online: https://github.com/ultralytics/yolov3 (accessed on 8 April 2018). 38. Sharma, D.K. Information Measure Computation and Its Impact in MI COCO Dataset. In Proceedings of the 2021 7th International\nConference on Advanced Computing and Communication Systems (ICACCS), Coimbatore, India, 19\u201320 March 2021; Volume 1, pp. 1964\u20131969.\n39. Satarkar, H.; Zagade, N.; Gupta, S.; Pundlik, S. Comparative Study between Segmentation Neural Networks on Pascal VOC Dataset. Int. Res. J. Eng. Technol. 2021, 8, 5. 40. Sinaga, K.P.; Yang, M.-S. Unsupervised K-Means Clustering Algorithm. IEEE Access 2020, 8, 80716\u201380727. [CrossRef] 41. Mehta, S.; Paunwala, C.; Vaidya, B. CNN Based Traffic Sign Classification Using Adam Optimizer. In Proceedings of the 2019 International Conference on Intelligent Computing and Control Systems (ICCS), Madurai, India, 15\u201317 May 2019; pp. 1293\u20131298. 42. Liu, Z.; Qi, M.; Shen, C.; Fang, Y.; Zhao, X. Cascade Saccade Machine Learning Network with Hierarchical Classes for Traffic Sign Detection. Sustain. Cities Soc. 2021, 67, 102700. [CrossRef] 43. Ren, K.; Huang, L.; Fan, C. Real-time Small Traffic Sign Detection Algorithm based on Multi-scale Pixel Feature Fusion. Signal Process. 2020, 36, 1457\u20131463. [CrossRef] 44. Liu, Y.; Shi, G.; Li, Y.; Zhao, Z. M-YOLO: Traffic Sign Detection Algorithm Applicable to Complex Scenarios. Symmetry 2022,\n14, 952. [CrossRef]\nDisclaimer/Publisher\u2019s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content."
        }
    ],
    "title": "TSR-YOLO: A Chinese Traffic Sign Recognition Algorithm for Intelligent Vehicles in Complex Scenes",
    "year": 2023
}