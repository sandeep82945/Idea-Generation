{
    "abstractText": "Synaptic plasticity or the ability of a brain to changes one or more of its functions or structures has generated and is sill generating a lot of interest from the scientific community especially neuroscientists. These interests especially went into high gear after empirical evidences were collected that challenged the established paradigm that human brain structures and functions are set from childhood and only modest changes were expected beyond. Early synaptic plasticity rules or laws to that regard include the basic Hebbian rule that proposed a mechanism for strengthening or weakening of synapses (weights) during learning and memory. This rule however did not account from the fact that weights must have bounded growth overtime. Thereafter, many other rules were proposed to complement the basic Hebbian rule and they also posses other desirable properties. In particular, a desirable property in synaptic plasticity rule is that the ambient system must account for inhibition which is often achieved if the rule used allows for a lower bound in synaptic weights. In this paper, we propose a synaptic plasticity rule inspired from the Allee effect, a phenomenon often observed in population dynamics. We show properties such such as synaptic normalization, competition between weights, de-correlation potential, and dynamic stability are satisfied. We show that in fact, an Allee effect in synaptic plasticity can be construed as an absence of plasticity.",
    "authors": [
        {
            "affiliations": [],
            "name": "Eddy Kwessi"
        }
    ],
    "id": "SP:6a98cc6bb19a3231d4c7297a10eb1e88377dde8c",
    "references": [
        {
            "authors": [
                "W.C. Allee"
            ],
            "title": "Principles of Animal Ecology",
            "venue": "W.B. Saounders Co., Philadelphia,",
            "year": 1949
        },
        {
            "authors": [
                "L. Assas",
                "S. Elaydi",
                "E. Kwessi",
                "G. Livadiotis",
                "D. Ribble"
            ],
            "title": "Competition models with allee effects",
            "venue": "Journal of Difference Equations and Applications,",
            "year": 2014
        },
        {
            "authors": [
                "L. Assas",
                "B. Dennis",
                "S. Elaydi",
                "E. Kwessi",
                "G. Livadiotis"
            ],
            "title": "Hierarchical competition models with the allee effect ii: the case of immigration",
            "venue": "Journal of Biological Dynamics,",
            "year": 2015
        },
        {
            "authors": [
                "L. Assas",
                "B. Dennis",
                "S. Elaydi",
                "E. Kwessi",
                "G. Livadiotis"
            ],
            "title": "A discrete-time hostparasitoid discrete model with an allee effect",
            "venue": "Journal of Biological Dynamics,",
            "year": 2015
        },
        {
            "authors": [
                "L. Assas",
                "S. Elaydi",
                "E. Kwessi",
                "G. Livadiotis",
                "D. Ribble"
            ],
            "title": "Hierarchical competition models with allee effects",
            "venue": "Journal of Biological Dynamics,",
            "year": 2015
        },
        {
            "authors": [
                "L. Assas",
                "B. Dennis",
                "S. Elaydi",
                "E. Kwessi",
                "G. Livadiotis"
            ],
            "title": "Stochastic modified beverton-holt model with allee effects ii: the cushing-henson conjecture",
            "venue": "Journal of Difference Equations and Applications,",
            "year": 2016
        },
        {
            "authors": [
                "M. Avery"
            ],
            "title": "A Message from Martha: The Extinction of the Passenger Pigeon and Its Relevance Today",
            "venue": "Bloomsbury,",
            "year": 2014
        },
        {
            "authors": [
                "M.F. Bear",
                "R.C. Malenka"
            ],
            "title": "Synaptic plasticity: Ltp and ltd",
            "venue": "Curr Opin Neurobiol,",
            "year": 1994
        },
        {
            "authors": [
                "E.L. Bienenstock",
                "L.N. Cooper",
                "P.W. Munro"
            ],
            "title": "Theory for the development of neuron selectivity: orientation specificity and binocular interaction in visual cortex",
            "venue": "Journal of neuroscience,",
            "year": 1982
        },
        {
            "authors": [
                "T.V. Bliss",
                "G.L. Collingridge"
            ],
            "title": "A synaptic model of memory: long-term potentiation in the hippocampus",
            "year": 1997
        },
        {
            "authors": [
                "T.V. Bliss",
                "T. Lomo"
            ],
            "title": "Long-lasting potentiation of synaptic transmission in the dentate area of the anaesthetized rabbit following stimulation of the perforant path",
            "venue": "J. of Physiol.,",
            "year": 1973
        },
        {
            "authors": [
                "J.S. Brashares",
                "J.R. Werner",
                "A.R.E. Sinclair"
            ],
            "title": "Social \u2018meltdown\u2019 in the demise of an island endemic: Allee effects and the vancouver island marmot",
            "venue": "Journal of Animal Biology,",
            "year": 2010
        },
        {
            "authors": [
                "T. Bussey"
            ],
            "title": "Pattern separation in the hippocampus: A role for adult neurogenesis and bdnf",
            "venue": "Neuroscience Research,",
            "year": 2011
        },
        {
            "authors": [
                "F. Courchamp",
                "L.. Berec",
                "J. Gascoigne"
            ],
            "title": "Allee effects in ecology and conservation",
            "year": 2008
        },
        {
            "authors": [
                "P. Dayan",
                "L.F. Abbott"
            ],
            "title": "Computational and Mathematical Modeling of Neural Systems",
            "year": 2001
        },
        {
            "authors": [
                "M. Delitala",
                "M. Ferraro"
            ],
            "title": "Is the allee effect relevant in cancer evolution and therapy",
            "venue": "AIMS Mathematics,",
            "year": 2020
        },
        {
            "authors": [
                "B. Dennis",
                "L. Assas",
                "S. Elaydi",
                "E. Kwessi",
                "G. Livadiotis"
            ],
            "title": "Allee effects and resilience in stochastic populations",
            "venue": "Theoretical Ecology,",
            "year": 2015
        },
        {
            "authors": [
                "J. Dorszewska",
                "W. Kozubski",
                "W. Waleszczyk",
                "M. Zabel",
                "K. Ong"
            ],
            "title": "Neuroplasticity in the pathology of neurodegenerative diseases",
            "venue": "Neural Plast.,",
            "year": 2020
        },
        {
            "authors": [
                "C.S. Drapaca"
            ],
            "title": "Mathematical modeling of a brain-on-a-chip: A study of the neuronal nitric oxide role in cerebral microaneurysms",
            "venue": "Emerging Science Journal,",
            "year": 2018
        },
        {
            "authors": [
                "S. Elaydi",
                "E. Kwessi",
                "G. Livadiotis"
            ],
            "title": "Hierarchical competition models with allee effect iii: Multispecies",
            "venue": "Journal of Biological Dynamics,",
            "year": 2018
        },
        {
            "authors": [
                "D.E. Feldman"
            ],
            "title": "Developmental synaptic plasticity: Ltp, ltd, and synapse formation and elimination",
            "year": 2009
        },
        {
            "authors": [
                "J.F. Fontanari",
                "L. Perlovsky"
            ],
            "title": "Allee effect on language evoluation",
            "venue": "6th International Conference,",
            "year": 2006
        },
        {
            "authors": [
                "E. Fuller"
            ],
            "title": "The Passenger Pigeon",
            "year": 2014
        },
        {
            "authors": [
                "J. Greensberg",
                "editor"
            ],
            "title": "A Feathered River Across the Sky: The Passenger Pigeon\u2019s Flight to Extinction",
            "venue": "Bloomsbury,",
            "year": 2014
        },
        {
            "authors": [
                "D.O. Hebb"
            ],
            "title": "The Organization of Behavior",
            "year": 1949
        },
        {
            "authors": [
                "J.A. Hutchings"
            ],
            "title": "Thresholds for impaired species recovery",
            "venue": "Proceedings of the Royal Society B,",
            "year": 2015
        },
        {
            "authors": [
                "K.E. Johnson",
                "G. Howard",
                "W. Mo",
                "M.K. Strasser",
                "E.A.B.F. Lima",
                "S. Huang",
                "A. Brock"
            ],
            "title": "Cancer cell population growth kinetics at low densities deviate from the exponential growth model and suggest an allee effect",
            "venue": "PLoS Biology,",
            "year": 2019
        },
        {
            "authors": [
                "A. Konstorum",
                "T. Hillen",
                "J. Lowengrub"
            ],
            "title": "Feedback regulation in a cancer stem cell model can cause an allee effect",
            "venue": "B. Mathematicakl Biology,",
            "year": 2016
        },
        {
            "authors": [
                "E. Kwessi"
            ],
            "title": "A consistent estimator of nontrivial stationary solutions of dynamic neural fields",
            "venue": "Stats, 4(1):122\u2013137,",
            "year": 2021
        },
        {
            "authors": [
                "E. Kwessi"
            ],
            "title": "Discrete dynamics of dynamic neural fields",
            "venue": "Frontiers in Computational Neuroscience,",
            "year": 2021
        },
        {
            "authors": [
                "P.Z. Liu",
                "R. Nusslock"
            ],
            "title": "Exercise and hippocampal neurogenesis: a dogma re-examined and lessons learned",
            "venue": "Neural Reg. Research,",
            "year": 2018
        },
        {
            "authors": [
                "P.Z. Liu",
                "R. Nusslock"
            ],
            "title": "Exercise-mediated neurogenesis in the hippocampus via bdnf",
            "venue": "Front Neurosci.,",
            "year": 2018
        },
        {
            "authors": [
                "Z. Neufeld",
                "W. von Witt",
                "D. Lakatos",
                "J. Wang",
                "B. Hegedus",
                "A. Czirok"
            ],
            "title": "The role of allee effect in modelling post resection recurrence of glioblastoma",
            "venue": "Plos Computational biology,",
            "year": 2017
        },
        {
            "authors": [
                "E. Oja"
            ],
            "title": "Simplified neuron model as a principal component analyzer",
            "venue": "Journal of Mathematical Biology,",
            "year": 1982
        },
        {
            "authors": [
                "A.K. Olson",
                "B.D. Eadie",
                "C. Ernst",
                "B.R. Christie"
            ],
            "title": "Environmental enrichment and voluntary exercise massively increase neurogenesis in the adult hippocampus via dissociable pathways",
            "venue": "Hippocampus, 16:250\u2013260,",
            "year": 2006
        },
        {
            "authors": [
                "T. Perala",
                "A. Kuparinen"
            ],
            "title": "Detection of allee effects in marine fishes: analytitical biases generated by data availability and model selection",
            "venue": "Proceedings of Royal Society B,",
            "year": 2017
        },
        {
            "authors": [
                "Terrence J. Sejnowski",
                "Gerald Tesauro"
            ],
            "title": "The hebb rule for synaptic plasticity",
            "venue": "Algorithms and implementations,",
            "year": 1989
        },
        {
            "authors": [
                "S.A. Siegelbaum",
                "E.R. Kandel"
            ],
            "title": "Learning-related synaptic plasticity",
            "venue": "Ltp and ltd. Curr Opin Neurobiol,",
            "year": 1991
        },
        {
            "authors": [
                "J.W. Swann",
                "M. Nishimura"
            ],
            "title": "Plasticity \u2014 dendritic abnormalities in epilepsy",
            "year": 2009
        },
        {
            "authors": [
                "G.W. Thickbroom",
                "F.L. Mastaglia"
            ],
            "title": "Plasticity in neurological disorders and challenges for noninvasive brain stimulation (nbs)",
            "venue": "J. Neuroeng Rehabil,",
            "year": 2009
        },
        {
            "authors": [
                "G. Voglis",
                "N. Tavernarakis"
            ],
            "title": "The role of synaptic ion channels in synaptic plasticity",
            "venue": "Embo Reports,",
            "year": 2006
        },
        {
            "authors": [
                "N. Wang",
                "L. Chen",
                "N. Cheng",
                "J. Zhang",
                "T. Tian",
                "W. Lu"
            ],
            "title": "Active calcium/calmodulindependent protein kinase ii (camkii) regulates nmda receptor mediated postischemic long-term potentiation (i-ltp) by promoting the interaction between camkii and nmda receptors in ischemia",
            "venue": "Neural Plast.,",
            "year": 2014
        },
        {
            "authors": [
                "V.C. Wimmer",
                "H.A. Lester",
                "S. Petrou"
            ],
            "title": "Ion channel mutations in familial epilepsy",
            "venue": "Encyclopedia of Basic Epilepsy Research,",
            "year": 2009
        },
        {
            "authors": [
                "J. Xu",
                "J. Kang"
            ],
            "title": "The mechanisms and functions of activity-dependent long-term potentiation of intrinsic excitability",
            "venue": "Rev. Neurosci.,",
            "year": 2005
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Synapses play an important role in the brain because they are junctions between nerves cells. As such, they facilitate diffusion of chemical substances called neurotransmitters from the brain to other parts of the body. During this diffusion, synapses are sometimes modified to adapt to the impulses and their transmission rate. These synaptic modifications may be due to lived experience and training and can occur at functional and structural levels. At a functional level, the brain may move functions from one area to other areas, often between damaged to undamaged ones. At a structural level, the brain may actually change its physical structure, mainly some synaptic structures as a result of external activities. Synapses modifications can in turn affect behavior and training, therefore, understanding the dichotomy between synaptic modifications and\n\u2217Corresponding author: Department of Mathematics, Trinity University, 1 Trinity Place, San Antonio, TX 78212, Email: ekwessi@trinity.edu\nar X\niv :2\n20 3.\n13 65\n0v 1\n[ q-\nbi o.\nN C\n] 2\n5 M\nar 2\nexperience and/or training is paramount if one wants to have an insight into some of our brain activities. Brain plasticity or neuroplasticity can be thought of as the ability of the brain to adapt to external activities by reorganizing some of its pathways or by modifying some of its synaptic structure. Changes in the brain were believed to occur only during infancy and early childhood so that the brain structure was believed to be mostly set by adulthood. Pioneers challenging this paradigm include James (1890), who suggested that in fact, at any age, the brain, just like any organic matter, has a great deal of plasticity. Early researchers on synaptic plasticity include Hebb (1949), who conjectured that on one hand, synapses from two neurons are often strengthened if impulses from one neuron contribute to the firing of another. On the other hand, synapses are weakened if non-coincidental neuronal firings occur. In essence, this rule infers that synaptic modifications are in direct relationship with experience and training, and consequently, the mechanisms underlying learning and memory can be understood via these synaptic modifications. In fact, there is ample empirical evidence consisting of transient and long lasting effects (long-term potentiation and depression) starting with Bliss and Lomo (1973) who experimented plasticity in rabbits. Plasticity was later experimented in selected regions of the brains like the hippocampus neocortex and cerebellum, see for instance Bear and Malenka (1994); Bliss and Lomo (1973); Bussey (2011); Feldman (2009); Liu and Nusslock (2018a,b); Olson et al. (2006); Siegelbaum and Kandel (1991); Xu and Kang (2005). Learning and memory are therefore manifestations of the brain\u2019s capacity to sustain recurrent changes. Moreover, it has been documented that the main excitatory neurotransmitter in the mammalian nervous system is L-glutamate, see Voglis and Tavernarakis (2006). This transmitter is usually detected at postsynaptic terminals by a coupling of G-protein and inotropic glutamate receptors who have been associated with learning and memory, see Bliss and Collingridge (1997). Plasticity also plays a role in various pathologies. For instance, following a stroke, evidence has emerged linking pathological neural plasticity or postischemic long-term potentiation with ischemia which is a deficiency of blood to the heart or to the brain, see Wang et al. (2014). Pathological neural plasticity also plays an important role in epilepsy, another pathology that is said to arise from malfunction of a mutated ion channel, leading to increased excitatory or decreased inhibitory currents, see Swann and Nishimura (2009); Wimmer et al. (2009). Other neurological disorders have been linked with plasticity, for instance Alzheimer\u2019s disease, Parkinson\u2019s disease, Huntington\u2019s disease, dystonia, see Dorszewska et al. (2020); Thickbroom and Mastaglia (2009). However, to understand plasticity at the functional level, one needs to go beyond mechanistic models as described above and find how plasticity relates neurons and/or network of neurons to the basic rules that govern its induction, see Dayan and Abbott (2001). This entails finding mechanisms relating the strengthening or weakening of synapses via neurotransmitters and (presynaptic) neurons. Many mathematical models or synaptic plasticity rules have been proposed to explain synaptic plasticity in supervised and unsupervised learning environments. In an unsupervised learning environment where the neurons network self-organizes, an activity is represented by a continuous variable (input) at the presynaptic level and linked to a postsynaptic activity variable (output) by dynamic weights. The relationship between these variables is a differential equation describing the change of weights overtime and include and is not limited to the Basic Hebbian rule (Sejnowski and Tesauro (1989)) and its variant the Covariance rule (Dayan and Abbott (2001)), the Bienestock-Cooper-Munro (BCM)\nrule (Bienenstock et al. (1982)), and the Oja rule (Oja (1982)). To avoid unbounded growth, an upper saturation limit is often imposed, for instance in BCM and Oja rules. A lower limit is needed to allow for inhibition. However, this lower limit is often given by the condition that the length of weights not be zero. In populations dynamics, there are rules for which the density or size of a population is both bounded above and below by nonnegative constants as in the Allee effect. The Allee effect was introduced by Allee (1949) and characterizes a phenomenon in population dynamics where there is a positive correlation between a population density or size and its per capita growth rate. In the literature, Allee effects are divided into strong and weak Allee effects, see for instance Hutchings (2015). The strong Allee effect occurs when a population has a critical density A below which it declines to extinction while the weak Allee effect occurs when a population lacks such a critical density, but at lower densities, the population growth rate arises with increasing densities. Since their inception, Allee effects have substantially been investigated and applied by researchers across the board. Ecology is probably the area where researchers have investigated it the most. Mathematical models of the Allee effects and their dynamics have been investigated for competing populations in Assas et al. (2014, 2015a,b,c); Elaydi et al. (2018). Stochastic models of the Allee effects were discussed in Assas et al. (2016). Models addressing Allee effects and conservation are discussed in Courchamp et al. (2008). Models addressing population resilience were proposed by Dennis et al. (2015). Some real life evidence of Allee effects have been documented in Courchamp et al. (2008); Perala and Kuparinen (2017). Possible extension of Allee effects to medicine have been proposed in Delitala et al. (2020); Fontanari and Perlovsky (2006); Johnson et al. (2019); Konstorum et al. (2016); Neufeld et al. (2017). It has been speculated that the passenger pigeon, who was once the must abundant bird in North America and now extinct, was subject to a phenomenon similar to an Allee effect, see for instance Avery (2014); Fuller (2014); Greensberg (2014). The endangered Vancouver Island marmots, who are on the brink of extinction, may be subject to an Allee effect as well, see for instance Brashares et al. (2010). Figure 1 below is an illustration of the different types of Allee effects, where A is the critical (Allee) density and K is the carrying capacity of the population.\n(a) (b) (c)\nTo our knowledge, the Allee effect has not yet been been discussed in combination with plasticity rules. In this paper, we aim to make a foray on the topic and we show that in fact, an Allee effect, when combined to the Oja rule, can be characterized as a drift toward an absence of plasticity. Moreover, the model we propose has the following key advantages.\n1. Unbounded growth is controlled.\n2. Multiplicative normalization is preserved.\n3. Competition between weights is induced.\n4. The model is general enough to account for multiple layers of pre-and postsynaptic neurons.\n5. Under specific conditions on the network parameters, stability of dynamical system is obtained.\nThe use of an Allee effect in neuroscience may have the potential to produce invaluable information that could highlight hidden features in plasticity and could potentially enrich the ever growing literature on the topic. The remainder of this paper is organized as follows: In Section 2, we introduce our idea of the an Allee effect postsynaptic neuron model. In Section 3, we discuss stability analysis of s single postsynaptic neuron model, with and without a plastic recurrent connection. Ensembles of postsynaptic neurons are tackled in Section 4."
        },
        {
            "heading": "2 Allee effect postsynaptic neuron model and motivation",
            "text": "Consider a system with L layers and let a L\u00d7Nu matrix u = ( u(1),u(2), \u00b7 \u00b7 \u00b7 ,u(L) ) repre-\nsent the presynaptic activities in the system. For 1 \u2264 ` \u2264 L, u(`) = ( u (`) 1 , u (` 2 ), \u00b7 \u00b7 \u00b7 , u (`) Nu ) represents presynaptic activities of Nu inputs or neurons within the `th layer of the system. Let a L \u00d7 Nv matrix v = ( v(1),v(2), \u00b7 \u00b7 \u00b7 ,v(L) ) be the postsynaptic activities\ngenerated by the presynaptic activities u, where v(`) = ( v (`) 1 , v (`) 2 , \u00b7 \u00b7 \u00b7 , v (`) Nv ) represents the postsynaptic activities of Nv neurons on the `th layer. Let W be an input synaptic block-matrix of weights representing the strengths of the synapses from the presynaptic neurons u to the postsynaptic neurons v. We note that W is an L\u00d7Nu\u00d7L\u00d7Nv blockmatrix with entries ( W(k,`) ) , for 1 \u2264 k, ` \u2264 L. Each block W(k,`) is a matrix with entries w (k,`) ij where 1 \u2264 j \u2264 Nv and 1 \u2264 i \u2264 Nu. To account for inter-connections between postsynaptic neurons, we will consider an L \u00d7 Nv \u00d7 L \u00d7 Nv recurrence block-matrix Z with entries Z(k,`), for 1 \u2264 k, ` \u2264 L, where each entry Z(k,`) is a matrix with entries z(k,`)mj for 1 \u2264 j,m \u2264 Nv. We let dw = L \u00d7 Nu \u00d7 L \u00d7 Nv and dz = L \u00d7 Nv \u00d7 L \u00d7 Nv and we define the length of a vector W in Rdw as \u2016W\u20162 = WT \u00b7W, where the dot stands for the dot or inner product in Rdw .\nik , and dz = 9 recurrent connections z (k,`) im .\nRemark 1. In the sequel, we will use invariably the dot product with the following understanding:"
        },
        {
            "heading": "WT \u00b7 u is an L\u00d7Nv matrix with entries",
            "text": "(WT \u00b7 u)(k)i = L\u2211 `=1 Nu\u2211 j=1 w (k,`) ij u (`) j , for 1 \u2264 k \u2264 L, 1 \u2264 i \u2264 Nv ."
        },
        {
            "heading": "ZT \u00b7 v is an L\u00d7Nv matrix with entries",
            "text": "(ZT \u00b7 v)(k)i = L\u2211 `=1 Nv\u2211 m=1 z (k,`) im v (`) m for 1 \u2264 k \u2264 L, 1 \u2264 i \u2264 Nv .\nConsequently, as sums of products of coordinates of vectors, entries for WT \u00b7u and ZT \u00b7v are themselves dot products and therefore enjoy their properties. Also in the sequel, matrices and vectors and will be represented by bold face symbols whereas scalars will be represented by normal font symbols.\nDefinition 2. The learning activity of a system or plasticity function of a system is a function given as\nL(W,u,v) = H(W,u,v)\u2212 \u03d5(W,u,v) , (2.1)\nwhere H(W,u,v) is a function referred to as the Hebbian function and \u03d5(W,u,v) is function referred to as the Hebbian modification function.\nDefinition 3. We define a synaptic plasticity rule as\n\u03c4 W\ndW\ndt = L(W,u,v) . (2.2)\nThe constant \u03c4 W represents a time scaling constant controlling the rate of change of W and \u03bbW =\n1 \u03c4 W represents the learning rate. The model in equation (2.2) is general enough\nto include many known synaptic plasticity rules. For instance,\n\u2022 If H(W,u,v) = vTu and \u03d5(W,u,v) = 0, we obtain the Basic Hebb rule, see Hebb (1949).\n\u2022 If H(W,u,v) = vTu and \u03d5(W,u,v) = \u03b8vu or \u03d5(W,u,v) = \u03b8uvT for some constants \u03b8u and \u03b8v, we obtain the Covariance rule, see Dayan and Abbott (2001).\n\u2022 If H(W,u,v) = vvTu and \u03d5(W,u,v) = \u03b8vvTu, for some constant \u03b8v, we obtain the Bienestock-Cooper-Munro (BCM) rule, see Bienenstock et al. (1982).\n\u2022 If H(W,u,v) = vTu and \u03d5(W,u,v) = \u03b1vTWv, for some positive constant \u03b1, then we obtain the Oja rule, see Oja (1982).\nTo model the dynamics of postsynaptic neurons v, we will use the firing-rate equation given as\n\u03c4v dv\ndt = \u2212v + T (W,Z,u,v) , (2.3)\nwhere \u03c4v represents the time scale of the firing-rate dynamics of v and T (W,Z,u,v) is a function representing the total activity in the system. This activity may consist of preand postsynaptic activities u and v, with feed-forward and/or feed-backward connections\nwith intensities (or weights) W, with or without recurrent connections with intensities Z. In the general literature, T (W,Z,u,v) is taken as a linear function of the pre-and postsynaptic activities u and v. That is, T (W,Z,u,v) = WT \u00b7 u + ZT \u00b7 v. It can also be a nonlinear function depending on an activation function G as T (W,Z,u,v) = G(WT \u00b7 u + ZT \u00b7 v) or two activation functions G1 and G2 as T (W,Z,u,v) = WT \u00b7 G1(u) + Z\nT \u00b7 G2(v). The activation function controls the rate of signals emitted by presynaptic neurons u and the recurrence rate of postsynaptic neurons v. It is common to use either the sigmoid function G(x) = (1 + e\u2212x)\u22121 or the Heaviside function G(x) = 0, x < 0, G(x) = 1, x > 1. We observe however that more general activation functions G can be considered, see for instance Kwessi (2021a). We note that a more complete and perhaps more realistic model for equation (2.3) should contain a diffusion term and a less trivial reaction term than v. For sake of simplicity and to maintain tractability, we will will not do so in the present discussion.\nRemark 4. It is important to note that here, the total activity, T (W,Z,u,v) would be zero if the the pre-and and postsynaptic activities u and v are canceling each other. This is obviously the case if u = 0 and v = 0. From equation (2.3) above, if T (W,Z,u,v) = 0, then v(t) = v0e\n\u2212\u03bbvt and thus approaches 0 overtime. Combining the latter with (2.2), it follows, for some constant presynaptic inputs u and for some generic constant matrix \u03a3 that\n\u2022 W(t) = \u03bbW \u03bbv [ e\u2212\u03bbvtvT0 u + \u03a3 ] for the Basic Hebb rule.\n\u2022 W(t) = \u03bbW \u03bbv [ e\u2212\u03bbvtvT0 (u\u2212 \u03b8v) + \u03a3 ] or W(t) = \u03bbW \u03bbv [ e\u2212\u03bbvtvT0 (\u03b8u \u2212 u) + \u03a3 ] for the\nCovariance rule.\n\u2022 W(t) = \u03bbW \u03bbv\n[( e\u2212\u03bbvt\n2 vT0 \u2212 \u03b8v ) e\u2212\u03bbvtvT0 u + \u03a3 ] for the BCM rule.\n\u2022 W(t) = ev T 0 v0 \u03b1 2\u03bbv e\u22122\u03bbvt [ vT0 u \u222b e\u2212\u03bbvt\u2212v T 0 v0 \u03b1 2\u03bbv e\u22122\u03bbvtdt+ \u03a3 ] for the Oja rule.\nWe can therefore infer that when T (W,Z,u,v) = 0, v approaches 0 overtime whereas W approaches a constant \u03a3. Moreover, if T (W,Z,u,v) = WT \u00b7 u + ZT \u00b7 v = 0, the constant \u03a3 must be zero or the presynaptic activities u must be zero.\nRemark 5. We also observe that T (W,Z,u,v) = WT \u00b7u + ZT \u00b7 v can be understood as the total potential energy in the system. Indeed, suppose W,Z,u, and v are vector fields over an open connected domain D . Let P be a path in D . If these fields are continuous\nover D and \u222b P WT \u00b7 du and \u222b P u \u00b7 dWT are path-independent, then the fields W and u are conservative. Consequently, there exist functions f1 and f2 such that W T = \u2207f1 and u = \u2207f2. Therefore, the potential energy due to the fields WT and u is\u222b P \u2207f1 \u00b7 du + \u222b P \u2207f2 \u00b7 dW = WT \u00b7 u .\nSimilarly, there exist functions g1 and g2 such that the potential energy due to the fields"
        },
        {
            "heading": "Z and v is \u222b",
            "text": "P \u2207g1 \u00b7 dv + \u222b P \u2207g2 \u00b7 dZT = ZT \u00b7 v .\nFrom equation (2.3), we can deduce that the steady state is attained when the postsynaptic activity is equal to the total potential energy in the system. This also suggests that postsynaptic activity increases if it is less than the system\u2019s potential energy and decreases otherwise.\nIn this paper, we will make the following considerations : for nonnegative constants A,K, in equation (2.2) , we put\nH(W,u,v) = [ 1\u2212 A(WTW)\u22121 ] vTu\n\u03d5(W,u,v) = K\u22121 [ 1\u2212 A(WTW)\u22121 ] vTWv,\nwhere 1 is a matrix with entries ones and with the same dimension as the matrix (WTW)\u22121. Equation (2.2) can now be written as\n\u03c4 W\ndW\ndt = vT\n( u\u2212K\u22121Wv ) ( 1\u2212 A(WTW)\u22121 ) . (2.4)\nLet us now give a motivation for the model in equation (2.4). Suppose that we have only one layer(L = 1) and a single postsynaptic neuron (Nv = 1). Therefore v = v will be a scalar. Let us assume further that there is no recurrent connection (Z = 0). Per Remark 1, WT \u00b7 u will be a scalar. Moreover, W is a 1\u00d7Nu vector, thus WT \u00b7W = \u2016W\u20162 is a positive scalar. Equation (2.4) becomes\n\u03c4 W\ndW\ndt = v\n( u\u2212K\u22121vW ) ( 1\u2212 A(\u2016W\u20162)\u22121 ) . (2.5)\nThus, if we take the dot product on both sides of equation (2.4) by WT and multiply by the constant 2, it becomes\n2\u03c4 W WT \u00b7 dW dt = \u03c4 W\nd \u2016W\u20162\ndt = 2v\n( WT \u00b7 u\u2212 v\nK \u2016W\u20162 )( 1\u2212 A \u2016W\u20162 ) . (2.6)\nConsequently, the system composed of equations (2.6) and (2.3) arrives at its steady\nstates, given by the equation of its v-isocline\n( dv\ndt = 0\n) v = T (W,u, v) and the equations\nof its \u2016W\u20162-isocline ( d\u2016W\u20162 dt = 0 ) given as v = 0, \u2016W\u20162 = A, and WT \u00b7u\u2212 v K \u2016W\u20162 = 0. In the linear case in particular, we have v = T (W,u, v) = WT \u00b7 u. Therefore, we obtain that either \u2016W\u20162 = A or \u2016W\u20162 = K when the steady states are reached. Similarly to a strong Allee effect, the lengths of weights \u2016W\u20162 below min {A,K} will decay to zero overtime, which means that postsynaptic activities will decrease towards a state of no activity at all. We note also that when A = 0, equation (2.4) becomes the Oja rule. In population dynamics in general, the term ( 1\u2212 A \u2016W\u2016\u22122 ) is used to account for the presence of sparse populations or mate limitation. Adding this term as in equation (2.6) induces the synaptic normalization in that when the weights are nonnegative, their growth is limited by the global treshholds A or K, see Figure 5\u20139 below, in the case of a single neuron. Moreover, convergence of \u2016W\u20162 toward K induces competition between weights and preserves dynamic stability. In conclusion, the model in equation (2.4) is a matrix equivalent of equation (2.5) when we account for multiple layers and multiple postsynaptic neurons. In light of these facts, we will combine equation (2.4) and (2.3) for the definition of an Allee plasticity rule.\nDefinition 6. An Allee effect is a synaptic plasticity rule having a treshhold under which, activities drift towards an absence of plasticity and above which activities eventually become stable.\nDefinition 7. Let W,Z,u, and v be given as above. Consider an activity function T (W,Z,u,v). We define an Allee plasticity rule with non plastic recurrent connections as the system of differential equations \u03c4 W dW dt = vT (u\u2212K\u22121Wv) ( 1\u2212 A(WTW)\u22121 ) \u03c4v dv\ndt = \u2212v + T (W,Z,u,v)\n. (2.7)\nDefinition 8. We define an Allee plasticity rule with plastic recurrent connections as the system of differential equations \u03c4 W dW dt = vT (u\u2212K\u22121Wv) ( 1\u2212 A(WTW)\u22121 ) \u03c4v dv dt = \u2212v + T (W,Z,u,v)\n\u03c4 Z\ndZ dt = R(W,Z,u,v)\n. (2.8)\nwhere R(W,Z,u,v) is the total recurrent (post-synaptic) activity and \u03c4 Z is a scaling constant. In the literature, two rules are often considered for R(W,Z,u,v), see for instance see Dayan and Abbott (2001).\n\u2022 Anti-Hebbian rule: R(W,Z,u,v) = \u2212vTv + \u03b2Z, for some constant \u03b2.\n\u2022 Goodall rule: R(W,Z,u,v) = I\u2212 (WT \u00b7u)v\u2212Z. This rule is often used because it produces de-corralated postsynaptic outputs and possess homoschedasticity properties.\nIn the next sections, we will discuss stability analysis of these plasticity rules."
        },
        {
            "heading": "3 Stability analysis of a single postsynaptic neuron model",
            "text": "Here, we let L = 1 and Nv = 1. In the presence of a single postsynaptic neuron, the matrix v is reduced to a constant v and the recurrent connection matrix Z is reduced to a single constant z. There are two cases to consider: firstly, there could be no recurrent connection between v and itself (z = 0), see Figure 3 (a) below. Secondly, there could be a recurrent connection with weight z 6= 0, see Figure 3 (b) below.\n(a) (b)"
        },
        {
            "heading": "3.1 Single postsynaptic neuron with no recurrent connection",
            "text": "In this case, we will have Z = 0 and v = v. For sake of simplification, we let T (W, 0,u, v) = T (W,u). As we observe above, we consider the scalar auxiliary Allee-type system given by  \u03c4 W d \u2016W\u20162 dt = 2v ( WT \u00b7 u\u2212 v K \u2016W\u20162 )( 1\u2212 A \u2016W\u20162 ) \u03c4v dv\ndt = \u2212v + T (W,u)\n. (3.1)\nRemark 9. The steady states of this system will be given by the v-isocline v = T (W,u) and the \u2016W\u20162-isocline v = 0, \u2016W\u20162 = A, or W T \u00b7 u \u2016W\u20162 = 1 K T (W,u). The latter equation has a geometric interpretation. Indeed, this means that within the sphere with radius r = \u2016W\u2016, 1 K T (W,u) is the scalar projection of u onto the vector W whereas 1 K T (W,u)W is the vector projection of u onto the vector W, see Figure 4 below.\nWe will use the notation x =: \u2016W\u20162 , y := v, u = \u2016u\u2016 cos(\u03b8), where \u03b8 is the angle between the vector W and u. Thus for suitable functions f1(x, u) and f2(x, u), this system is of the form \u03c4x dx dt = g1(x, y) := 2y ( f1(x, u)\u2212 yxK ) ( 1\u2212 A x ) \u03c4y dy\ndt = g2(x, y) := \u2212y + f2(x, u)\n. (3.2)\nLet us discuss the local stability of the steady state (A, f2(A, u)).\nTheorem 10. Consider the system (3.2). Put a0 = f1(A, u) and b0 = f2(A, y, u), c0 := \u2202f2(A, u)\n\u2202x .\n(i) The line y = 0 for all x > 0 is always a spiral sink.\n(ii) If b0 (a0K \u2212 b0A) < 0, then the steady state (A, b0) is asymptotically stable.\n(iii) If b0 (a0K \u2212 b0A) > 0, then (A, b0) is unstable (repeller).\n(iv) If (a0K \u2212 b0A) = 0, the steady state (A, b0) is a saddle.\nRemark 11. (a) We note that the classification of a steady state is independent of the sign of u because \u2206, tr(J), and det(J) all depend on u2.\n(b) We also note that dynamics of the system in the nonlinear case where T2(W,u, v) = W\u00b7G(u) for some nonlinear function G are similar to that of the linear case. Special care needs to be taken in the case of the steady state (x, 0). This would require, per remark 4 above, that G(u) = 0. Since G(x) > 0 for all x \u2208 R, (x, 0) cannot be a steady state. In the case of the Heaviside activation function, (x, 0) is a steady state only when the presynaptic activities are negative (in inhibition), otherwise, it is not a steady state.\nIn Figure 5 \u20139 below, we will illustrate the above results by plotting the time series of xt and yt, for t = 0, \u00b7 \u00b7 \u00b7 , 250. We let A and K take interchangeably the values 1.5 and 3 in Figure 5 , 6 , 8 . The starting points of the trajectories are (0.1,\u22122), (0.2,\u22120.9) in black color, (0.3, 1.1), (0.4, 1.5) in cyan color, (4.8, 1), (5, 2) in brown color, and (9.8,\u22122), (4.5,\u22122) in magenta color. In Figure 5 , 6 , 7 , 9 , we take u = 0.3. The solid blue curve represents\nthe x-isocline f1(x, u)\u2212 yx\nK = 0 which, in the linear case, is given as y = uK\u221a x . The solid\nblue line represents the x-isocline y = 0 and the solid red curve represents the y-isocline y = f2(x, u) which, in the linear case, is given as y = u \u221a x. The dots are the intersection between the x- and y-isoclines. In particular, the gray dot represents the origin (0, 0), the yellow dot has coordinates coordinates (A, u \u221a A), and is the intersection between the x-isocline x = A and the y-isocline y = u \u221a x. Likewise, the green dot is the intersection between the x-isocline x = K and the y-isocline y = u \u221a x with coordinates (K, u \u221a K). From these figures, we confirm the results above in that (A, u \u221a A) and (K, u \u221a K) are either attractors or saddle points. Figure 8 confirms that when u = 0, the line y = 0 is a sink."
        },
        {
            "heading": "3.2 Discussion",
            "text": "In the case of one neuron with non plastic recurrent connection, establishing stability of the steady states is relatively manageable compared to the case of a plastic connection. Figure 5 is similar to a typical case in ecology where the Allee threshold A is less than K. We clearly see that in the region below A, synaptic weights trajectories converge to zero, while postsynaptic neurons trajectories may be either in excitatory or inhibitory states. Interestingly, excitatory postsynaptic neurons never become inhibitory since they never cross the red curve. In that same region, inhibitory neurons become excitatory overtime but with decreasing synaptic weights. Figure 6 shows that if A > K, the Allee effect is no more guaranteed to occur below A or even below K. In fact, some neurons, whether in excitatory or inhibitory states would have decaying or increasing synaptic weights. This is the case for the trajectories in black and cyan color. Figure 7 is the case when A = K and there is an Allee effect. Figure 8 is an illustration of the situation where at some point in time, there is no presynaptic activity. Since postsynapatic where already in either excitatory or inhibitory modes, they will decay to zero rather quickly. Figure 9 is essentially the Oja rule and there is no Allee effect."
        },
        {
            "heading": "3.3 Single postsynaptic neuron with one constant recurrent connection",
            "text": "In this case, Z = z and v = v are constant with dZ\ndt = 0. As we observe above, we\nconsider the system given by \u03c4 W d\u2016W\u20162 dt = 2v ( WT \u00b7 u\u2212 v K \u2016W\u20162 ) ( 1\u2212 A\u2016W\u20162 ) \u03c4v dv\ndt = \u2212v + T2(W,u, z, v)\n. (3.3)\nUsing the notation x =: \u2016W\u20162 , y := v, u = \u2016u\u2016 cos(\u03b8), and given functions f1(x, u) and f2(x, y, u, v), this system is of the form \u03c4x dx dt = g1(x, y) := 2y ( f1(x, u)\u2212 yxK ) ( 1\u2212 A x ) \u03c4y dy\ndt = g2(x, y) := \u2212y + f2(x, y, u, z)\n. (3.4)\nThis system has similar dynamics to that of the system (3.2). In the linear case where f2(x, y, u, z) = u \u221a x+ zy, dy\ndt = (z\u2212 1)y+ u\n\u221a x = 0 if (1\u2212 z)y = u \u221a x. For z = 0, this is\nthe parabola (red solid curve) obtained in the previous case. When z approaches 1, this parabola becomes increasingly \u201csteeped\u201d and eventually explodes into the y-axis when z = 1. In the latter case, there is no steady state in the system since they are always given as intersections between the parabola and the vertical lines x = A and x = (1\u2212 z)K. In reality, there will be infinitely many points of intersection between the parabola and the vertical lines."
        },
        {
            "heading": "3.4 Single postsynaptic neuron with one plastic recurrent connection",
            "text": "For a single postsynaptic neuron with one plastic recurrent connection, we will have Z = z and v = v. In this paper, we will use Goodall\u2019s rule for its de-correlation properties. For a single neuron, we consider the given by \u03c4 W d\u2016W\u20162 dt = 2v ( WT \u00b7 u\u2212 v K \u2016W\u20162 ) ( 1\u2212 A\u2016W\u20162 ) \u03c4v dv dt = \u2212v + T2(W,u, z, v)\n\u03c4z dz\ndt = \u2212(WT \u00b7 u)v + 1\u2212 z\n. (3.5)\nLet us now discuss the steady states of the system above: Case 1: v = 0 Then, we would have T2(W,u, z, v) = 0, which as above, can only happen if u = 0 and v = 0. In this case, the third equation suggests that we must have z = 1. Thus, in the space formed by x = \u2016W\u20162 , y = v, and z, the line parallel to the x-axis with equation y = 0, z = 1 is a steady state. Case 2: v 6= 0,WT \u00b7 u = v\nK \u2016W\u20162\nThis condition is equivalent to v = K(WT \u00b7 u) \u2016W\u20162 . From the second equation in (3.5), we have v = WT \u00b7 u + zv, therefore, we deduce that z = 1\u2212 \u2016W\u2016 2\nK .\nUsing the third equation \u2212(WT \u00b7u)v+ 1\u2212 z = 0, it follows that z = 1\u2212 \u2016W\u2016 2\nK v2. Since\nthe values of z must be the same value, it follows that we should have v2 = 1. The latter entails having \u2016W\u20162 = K \u2223\u2223WT \u00b7 u\u2223\u2223 and z = 1 \u2212 \u2223\u2223WT \u00b7 u\u2223\u2223. We conclude that there are two steady states in the space formed by x = \u2016W\u20162 , y = v, and z, namely, the lines\nL1 : z = 1\u2212 x\nK , y = 1,\nL2 : z = 1\u2212 x\nK , y = \u22121 .\nCase 3: v 6= 0, \u2016W\u20162 = A In this case, v = WT \u00b7 u + zv and thus v = W T \u00b7 u\n1\u2212 z . We note from above that if v 6= 0,\nthen z 6= 1. It follows from the third equation of the system (3.5) that, z = 1\u2212(WT \u00b7u)v, and therefore, we can deduce that (1\u2212 z)(1\u2212 v2) = 0. Since z 6= 1, we must have v2 = 1. The latter implies that \u2223\u2223WT \u00b7 u\u2223\u2223 = |1\u2212 z|. Thus z = 1 \u00b1WT \u00b7 u. We conclude that there are two steady states in the space formed by x = \u2016W\u20162 , y = v, and z, namely, the points\nB1 = (A,\u22121, 1 + u \u221a A), B2 = (A, 1, 1\u2212 u \u221a A) .\nUsing the notation x =: \u2016W\u20162 , y := v, u = \u2016u\u2016 cos(\u03b8), and for given functions f1(x, u) and f2(x, y, z, u), the system (3.5) is of the form \u03c4x dx dt = g1(x, y, z) := 2y ( f1(x, u)\u2212 yxK ) ( 1\u2212 A x ) \u03c4y dy dt = g2(x, y, z) := \u2212y + f2(x, y, z, u)\n\u03c4z dz\ndt = g3(x, y, z) := f3(x, y, u) + 1\u2212 z\n. (3.6)\nOur first result in this section concerns the stability of the steady state line x > 0, y = 0, z = 1 and the points B1 and B2.\nTheorem 12. Consider the system (3.6), where f1(x, u) = u \u221a x, f2(x, y, z, u) = u \u221a x +\nzy, and f3(x, y, u) = \u2212uy \u221a x.\n(i) If u = 0, then the steady state is the line (x, 0, 1) for x > 0 and it is always stable.\n(ii) Suppose u > 0.\n(a) If u < 2 \u221a A\nK , then B1 is unstable and B2 is stable.\n(b) If u > 2 \u221a A\nK , then B1 and B2 are unstable.\n(iii) Suppose u < 0.\n(a) If \u22122 \u221a Amax {2A,K} \u2264 u, then B1 and B2 are stable. (b) If u < \u22122 \u221a Amax {2A,K}, then B1 and B2 are unstable. (c) If \u22122 \u221a Amin {2A,K} < u < \u22122 \u221a Amax {2A,K}, one of B1 or B2 is unstable\nand the other is stable.\nThe proof can be found in the Appendix. Our second result discusses the stability of the steady states L1 and L2. Theorem 13. Consider the system (3.6), where f1(x, u) = u \u221a x, f2(x, y, z, u) = u \u221a x +\nzy, and f3(x, y, u) = \u2212uy \u221a x. Put\n\u03b11 = 2\n( u\n2 \u221a x \u2212 1 K\n)( 1\u2212 1\nA\n) + 2 ( u \u221a x\u2212 2x\nK\n)( A\nx2\n) ,\n\u03b12 =\n( 2u \u221a x\u2212 4x\nK\n)( 1\u2212 A\nx\n) , \u03b13 = 0 ,\n\u03b21 = u\n2 \u221a x , \u03b22 = \u2212\nx\nK , \u03b23 = \u22121 ,\n\u03b31 = u\n2 \u221a x , \u03b32 = \u2212u\n\u221a x, \u03b33 = \u22121 .\nIn addition, we let\na2 = (\u03b11 + \u03b22 \u2212 1) , a1 = \u03b11(1\u2212 \u03b22) + \u03b22 \u2212 \u03b32 + \u03b12\u03b21 , a0 = \u03b11(\u03b32 \u2212 \u03b22) .\n(i) Suppose a0 = 0.\n(a) If a22 + 4a1 < 0 and a2 < 0, then L1 and L2 are stable. (b) Suppose a22 + 4a1 > 0.\n1. If a1 > 0 and a2 > 0, then L1 and L2 are unstable.\n2. If a1 < 0 and a2 > 0, then L1 and L2 are stable.\n3. If a1 > 0 and a2 < 0 or a1 < 0 and a2 > 0, then L1 and L2 are unstable.\n(ii) If a0 < 0, then L1 and L2 are unstable in two cases and stable in one.\n(iii) If a0 > 0, then L1 and L2 are unstable in case and stable two cases.\nThe proof can be found in the Appendix\nIllustration\nSince they are. any cases to consider, we will consider for simplicity just a couple of them for sake of simplicity. In the figures below, we show the dynamics of the system (3.5) above. We choose M = 20 different trajectories with length N = 5000. The initial state are chosen randomly as: since x = \u2016W\u20162 must be positive, we randomly select M starting points x0 in the interval (0, 5). The starting values y0 are chosen randomly as M/2 = 10 in the interval (\u22125, 0) and M/2 = 10 in (0, 5). The starting values z0 are chosen randomly as M/2 = 10 in the interval (\u221210, 0) and M/2 = 10 in (0, 10). The starting points (x0, y0, z0) are the white dots in the figures below. The green sphere is B1 while the blue sphere is B2.\nK\nand we observe that B2 is stable and B1 is unstable. The cubes\nR1 = [0, 4] \u00d7 [0, 10] \u00d7 [\u22125, 0] and R2 = [0, 4] \u00d7 [\u221220, 0] \u00d7 [\u22125, 10] are the Allee regions: starting trajectories will eventually converge to 0 in x leading to absence of plasticity.\nK\nand we observe that B1 and B2 are unstable. In fact, B1 is clearly\na repeller point whereas B1 is a saddle point. This case is less realistic since synaptic normalization is never achieved. In fact, the lengths of weights x increase without bound while the postsynaptic activities y, though different at early times, become increasing similar overtime (straight line). The Allee regions are the cubes R1 = [0, 2] \u00d7 [0, 10] \u00d7 [\u22125, 0] and R2 = [0, 2]\u00d7 [\u221240, 0]\u00d7 [\u22125, 10]. We observe that the gray trajectory starting just above R1 does not converge to 0 because it gets into the basin of attraction of L2 and increases thereafter."
        },
        {
            "heading": "3.5 Discussion",
            "text": "The first simulation shows that lengths of weights x either decrease to A = 4 or to 0. The second simulation is more nuanced in that some decrease to A = 2 first, then after a while they either decrease to 0 or increase. Others will first increase and then decrease to 0. Finally, some will increase without bound after initially decreasing to close to A = 2. What these simulations show is that the size of the Allee regions depend on the value of A. Clearly, if A = 0, there is no Allee region and the model is reduced to the Oja rule. An important observation is that since the fixed points B1 and B2 both depend on A, the first simulation shows that if A = 0 (Oja rule), then weight lengths x all decrease to 0, without any possibility of recovering. This means that, our system while stable in the longterm, represents adrift towards an absence of plasticity. In a sense, the parameter A must positive if we want to have more than a drift towards absence of plasticity for all trajectories. From above, we clearly see that there is advantage studying the length of weights rather than individual weights. The complexity of the dynamics is vastly reduced. This approach makes studying large amounts of layers mathematically possible while main-\ntaining interpretability of the results. The main drawback, as illustrated by the results above (Theorem 12 and 13) is that stability analysis, while feasible, still depends unfortunately of complicated quantities."
        },
        {
            "heading": "4 Multiple postsynaptic neurons model",
            "text": "In the single postsynaptic neuron model, we did not include recurrent connections. For a multiple postsynaptic model, we have to consider recurrent connections which themselves maybe fixed or plastic. Another important aspect to consider is that of a layered system where pre-and postsynaptic neurons are on different layers. It is entirely possible that this aspect may help reduce redundancies and correlations among output units."
        },
        {
            "heading": "4.1 Multiple output units with constant recurrent connections",
            "text": "In this case, Z and v are nonzero matrices with Z constant over time. In this case, we fix 1 \u2264 ` \u2264 L and 1 \u2264 j \u2264 Nv. The assumption here is still that we have Nu presynaptic neurons and Nv post-synaptic neurons per layer. Now we let W (`) j be 1 \u00d7 Nu vector of synaptic weights from the Nu presynaptic neurons u (`) on the `th layer to the jth postsynaptic neurons v (`) j on the `th layer. In this case, equation (3.3)\n \u03c4 W (`) j d \u2225\u2225\u2225W(`)j \u2225\u2225\u22252 dt = 2v (`) j ( [W (`) j ] T \u00b7 u(`) \u2212 v (`) j K (`) j \u2225\u2225\u2225W(`)j \u2225\u2225\u22252 )1\u2212 A(`)j\u2225\u2225\u2225W(`)j \u2225\u2225\u22252  \u03c4 v (`) j dv (`) j dt = \u2212v(`)j + T ( W (`) j ,u (`), z (`) j , v (`) j ) . (4.1)\nWe see that for these given ` and j, the system (4.1) has similar dynamics as the system is (3.3). However, there are other considerations to account for in this case. The system\u2019s parameters all depend on ` and j. The assumptions that time scale constants \u03c4\nW (`) j\nare\nthe same is not completely unrealistic, especially if the system evolves in a homogeneous ambient space. The same can be said of time scale constants \u03c4\nv (`) j\n. The thresholds A(`) and\nK(`) can either be the same or can vary, selected according to a chosen distribution. In the constant case, the dynamics of the system (4.1) is identical across all layers, thus the postsynaptic neurons will be perfectly correlated. In the case where these thresholds are not identical, of interest is understanding how and if the thresholds vectors A(`) = (A\n(`) j )\nand K(`) = (K (`) j ) for 1 \u2264 j \u2264 Nv, 1 \u2264 ` \u2264 L affect the correlation between postsynaptic neurons v(`) per layer. To illustrate the potential effect of thresholds, we will select select Nv = 150 samples of K from a truncated normal distribution N(\u00b5 = 0, \u03c32 = 100) over an interval [1.5, 30]. Likewise, we will select Nv samples A from an exponential distribution exp(\u03b8 = 0.5). These distributions are different enough to discriminate potential effect of thresholds A and K. The synaptic weights lengths x = \u2016W\u20162 will be initialized uniformly over the interval (0, 5). We fix the presynaptic length u = 0.3 and we let z\n(`) j = 0.4. The\npostsynaptic values v will be initialized uniformly within the interval [\u22122, 2]. We will\nobserve the Nv trajectories of v from t = 0 to t = 25, because not all of them will converge. In fact, for given Nv postsynaptic neurons, only a N \u2217 v \u2264 Nv will converge. We will therefore assess the correlation between these N\u2217v trajectories. In Figure 14 (a) below, the heat-map shows that the majority of the N\u2217v = 80 postsynaptic neurons v are highly correlated. Some of them, albeit a small number are de-correlated. It could be due to the randomness in the choice of the parameters above, or it could be due to the fact that the models itself reduces correlation, without any formal de-correlation mechanism like Goodall\u2019s. The boxplots in Figure 14 (b) show the evolution of the N\u2217v trajectories overtime. While the distribution of the N\u2217v = 80 trajectories differ significantly initially, they become increasingly similar over time, despite a few outliers. It also shows that the variance of outputs is constant overtime.\n(a) (b)\nTo ascertain whether the number of de-correlated postsynaptic neuron v is independent of the number Nv chosen, we introduce the de-correlation percentage. There are( N\u2217v 2 ) Spearman correlation coefficients. The de-correlation percentage is the proportion of these coefficients less than 0.2 (considered a weak correlation in the literature). Figure 15 shows that the de-correlation percentage is high when Nv is low, and decrease with increasing Nv."
        },
        {
            "heading": "4.2 Multiple output units with plastic recurrent connections",
            "text": "In this case, Z and v are matrices where Z is time-dependent. As Section 4.1 above, we will fix 1 \u2264 k, ` \u2264 L and 1 \u2264 j,m \u2264 Nv. Let z(k,`)mj represents the plastic weight connecting the jth postsynaptic neuron v\n(`) j on the `th layer with the mth postsynaptic\nneuron v (k) m on the kth layer. The system (3.5) becomes:\n \u03c4 W (`) j d \u2225\u2225\u2225W(`)j \u2225\u2225\u22252 dt = 2v (`) j ( [W (`) j ] T \u00b7 u(`) \u2212 v (`) j K (`) j \u2225\u2225\u2225W(`)j \u2225\u2225\u22252 )1\u2212 A(`)j\u2225\u2225\u2225W(`)j \u2225\u2225\u22252  \u03c4 v (`) j dv (`) j dt = \u2212v(`)j + T ( W (`) j ,u (`), z (k,`) mj , v (`) j ) \u03c4 z (k,`) mj dz (k,`) mj dt = \u2212 ( [W (`) j ] T \u00b7 u(`) ) v (`) j + 1\u2212 z (k,`) mj . (4.2)\nAs above, we may assume that the ambient space is homogeneous so that time scale constants \u03c4\nz (k,`) mj\nare the same. To obtain Figure 16 below, we used the same parameters\nas in Section 4, with the addition of plastic recurrent connections. The Heatmap in Figure 16 (a), shows that the N\u2217v = 107 postsynaptic neurons are less correlated than in the previous case above based on the prevalence of light red and light blue colors. Figure 16 (b) shows that the distribution of the trajectories stabilizes relatively quickly compared to the case above.\n(a) (b)"
        },
        {
            "heading": "4.3 Discussion",
            "text": "From the simulations above, we can draw a few observations: 1) The presence of the Allee effect term (1 \u2212 A \u2016W\u2016\u22122) in the model overall increases de-correlation, relatively speaking, see Figure 15 above. De-correlation is even increased\nwhen coupled with a de-correlation mechanism such as the Goodall\u2019s method. 2) We selected the thresholds A and K randomly, from noisy distributions so that any measured effects would be independent of their selection. Another important observation is that the initialization of v and Z does not seems to produce similar results as seen above, even when choosing from heavy-tailed distributions like N(0, 10) or a Student-t with low degrees of freedom. 3) The systems (4.1) and (4.2) are discussed in the context where the time scales are identical per layer. However, if they are chosen to be different and large, then the output units become highly correlated, within layers, reversing the de-correlation gains an Allee term would bring. 4) It seems as though the model, as written in (4.1) and (4.2) may be local to a single chosen layer. However, it is hardly the case given that one can consider that each layer has one single postsynaptic neurons, similarly to discrete dynamics of dynamic neural fields, see Kwessi (2021b)."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we have proposed a definition of the Allee effect in neuroplasticity that maintains the spirit of the Allee effect as originally proposed by Allee (1949). We have also proposed a learning rule that is more general than the Oja learning rule while preserving multiplication normalization, controlling unbounded growth and inducing competition between weights. The model has the advantage that it can accommodate single or multiple pre-and postsynaptic neurons, with and and without recurrent connections. Stability analysis was discussed with simulations to illustrate results. Absence of plasticity in the brain can be due to many factors and can be observed in many brain pathologies such as the Alzheimers, stroke, Parkinson\u2019s and Huntington disease. Using the firingrate equation to model post-synaptic activities could be a limiting factor in the model in that diffusion is not accounted for. A further improvement involving a diffusion term or a lattice differential equation would probably add more nuance and could be worthwhile. In Ecology, remedies to an Allee effect such as immigration have been proposed in Assas et al. (2015a), together with mathematical models explaining the process. This conservationism amounts in practice to adding either new offsprings from different population patches or to controlling predation. In the brain however, it is not clear how one would go about this. Often, neuroscientists focus on reactivating lost or dormant neurotransmitters by using new technologies such as brain implants. While applications of these brain implants have been numerous, mathematical models have lagged or at best, they have have been adaptation or reduction of the Hogkin-Huxley model, see Drapaca (2018). The model we proposed is different and is sensitive to external inputs (as in an Alllee-type model) and thus could be used to model the effects of brain implants. From a dynamical systems point of view, the model we propose is complex enough to accommodate a complex structure like the brain. Simulations do show that there are enough parameters in the model to capture a variety of phenomena related to neuroplasticity. From a mathematical point of view, studying the length of weights (rather than individual weights) coupled with other numerical quantities such as postsynaptic signals and their strengths reduces the complexity of the problem while maintaining interpretability of the results. From a purely scientific point of view, the model offers to merge notions\ndiscussed in ecology and neuroscience and it shows that these seemingly isolated areas from each other actually have similarities."
        },
        {
            "heading": "6 Appendix",
            "text": "6.1 Appendix A: Stability analysis of System (3.2)\nProof. Per remark 4 above, we have y = f2(x, u) = 0 for all x > 0 only when u = 0. Therefore the Jacobian matrix J(x, 0) is given as\nJ(x, 0) = 0 2f1(x, 0) ( 1\u2212 A x ) 0 \u22121  . The eigenvalues are therefore \u03bb1 = 0 and \u03bb2 = \u22121, which implies that the line y = 0 is always a spiral sink.\nThe partial derivatives of g1 and g2 are\n\u2202g1(x, y)\n\u2202x = 2y\n( \u2202f1(x, u)\n\u2202x \u2212 y K\n)( 1\u2212 A\nx\n) + 2y ( f1(x, u)\u2212 yx\nK )(A x2 ) ,\n\u2202g1(x, y)\n\u2202y =\n( 2f1(x, u)\u2212 4 yx\nK\n)( 1\u2212 A\nx\n) ,\n\u2202g2(x, y)\n\u2202x =\n\u2202f2(x, u)\n\u2202x ,\n\u2202g2(x, y) \u2202y = \u22121 + \u2202f2(x, u) \u2202y = \u22121 .\nWe are now concerned with the case when x = A and y = f2(A, y, u). For simplifications purposes, we put a0 = f1(A, u) and b0 = f2(A, u), c0 := \u2202f2(A, y, u)\n\u2202x . From above, the\nJacobian matrix is given as\nJ(A, b0) = 2 b0A ( a0 \u2212 b0 AK ) 0\n\u2212c0 \u22121  . Since the matrix J(A, b0) is a lower triangular matrix, the eigenvalues are there \u03bb1 = 2 b0 A ( a0 \u2212 b0 A K ) and \u03bb1 = \u22121. Thus if b0 (b0A\u2212 a0K) < 0, then \u03bb1 < 0 and \u03bb2 < 0, so the point (A, b0) is an attractor. If on the other hand b0 (a0K \u2212 b0A) \u2265 0, then \u03bb1 < 0 and \u03bb2 > 0, so the point (A, b0) is a repeller. Finally, if (a0K \u2212 b0A) = 0, then \u03bb1 < \u22121 and \u03bb2 = 0 and then (A, b0) is a saddle point.\nFinally, we discuss the case when the steady states are (x, y) with y = f2(x, u) and f1(x, u) = yx\nK . In this case,\ntr(J) = \u2202g1(x, y) \u2202x + \u2202g2(x, y) \u2202y\n= 2y ( \u2202f1 \u2202x \u2212 y K )( 1\u2212 A x ) \u2212 1 + \u2202f2 \u2202y\n= 2Kf1 x ( \u2202f1 \u2202x \u2212 f1 x )( 1\u2212 A x ) \u2212 1 + \u2202f2 \u2202y ,\nand\ndet(J) = \u2202g1(x, y)\n\u2202x\n\u2202g2(x, y) \u2202y \u2212 \u2202g2(x, y) \u2202x \u2202g1(x, y) \u2202y\n= 2Kf1 x ( \u2202f1 \u2202x \u2212 f1 x )( 1\u2212 A x )( \u22121 + \u2202f2 \u2202y ) + 2f1 \u2202f2 \u2202x ( 1\u2212 A x ) .\nThe eigenvalues are\n\u03bb1 = 1\n2\n[ tr(J)\u2212 \u221a \u2206 ]\n\u03bb2 = 1\n2\n[ tr(J) + \u221a \u2206 ] ,\nwhere\n\u2206 = [tr(J)]2 \u2212 4det(J)\n= [ 2Kf1 x ( \u2202f1 \u2202x \u2212 f1 x )( 1\u2212 A x ) \u2212 1 + \u2202f2 \u2202y ]2 \u2212 4 [ 2Kf1 x ( \u2202f1 \u2202x \u2212 f1 x )( 1\u2212 A x )( \u22121 + \u2202f2 \u2202y ) + 2f1 \u2202f2 \u2202x ( 1\u2212 A x\n)] = [ 2Kf1 x ( \u2202f1 \u2202x \u2212 f1 x )( 1\u2212 A x ) + 1\u2212 \u2202f2 \u2202y ]2 \u2212 8f1 \u2202f2 \u2202x ( 1\u2212 A x ) .\nIf \u2206 > 0 with det(J) > 0 and tr(J) > 0, then \u03bb2 > 0. In this case, \u221a\n\u2206 < |tr(J)| = tr(J) and it follows that \u03bb1 > 0. Consequently, we have a repeller (unstable point) at (x, y) such that y = f2(x, y, u) and f1(x, u) = yx\nK .\nIf \u2206 > 0 with det(J) > 0 and tr(J) < 0, then \u03bb1 < 0. In this case, \u221a\n\u2206 < |tr(J)| = \u2212tr(J) and thus \u03bb2 < 0. Consequently, we have an attractor (stable point) at (x, y) such that y = f2(x, y, u) and f1(x, u) = yx\nK .\nIf det(J) < 0, then \u2206 > 0. If, in addition tr(J) > 0, then \u03bb2 > 0. In this case\u221a \u2206 > |tr(J)| = tr(J) and it follows that \u03bb1 < 0. Consequently, we have a saddle at (x, y) such that y = f2(x, y, u) and f1(x, u) = yx\nK .\nIf det(J) < 0, then \u2206 > 0. If, in addition tr(J) < 0, then \u03bb1 < 0. Since \u221a\n\u2206 > |tr(J)| = \u2212tr(J). Therefore \u03bb2 > 0. Consequently, we have a saddle at (x, y) such that\ny = f2(x, y, u) and f1(x, u) = yx\nK .\nWe observe in particular that if x = A, then det(J) = 0 and one of the eigenvalues \u03bb1 or \u03bb2 is 0. The other eigenvalue is positive if \u2202f2 \u2202y > 1 and negative if \u2202f2 \u2202y < 1."
        },
        {
            "heading": "6.2 Important particular case",
            "text": "Let us discuss the linear case when T1(W,u) = u and T2(W,u, v) = W T \u00b7 u = \u2016W\u2016 \u00b7 \u2016u\u2016 cos(\u03b8), where \u03b8 is the angle between the vectors W and u. Therefore W \u00b7T1(W,u) = WT \u00b7 u. This leads to a modified Hebbian rule which becomes the Oja rule if A = 0. Using x := \u2016W\u20162, and u = \u2016u\u2016 cos(\u03b8), we will have f1(x, u) = f2(x, u) = u \u221a x. Since x and y have the same dimension, without loss of generality, we can let \u03c4x = \u03c4y = 1. The steady states for the system (3.3) are (x, 0) for x > 0, (A, u \u221a A) and (K, u \u221a K). From theorem 10 above, (x, 0), for x > 0, is always a spiral sink, which occurs only when u = 0. As for the steady state (A, u \u221a A), we have a0 = f1(A, u) = u \u221a A, b0 = f2(A, y, u) =\nu \u221a A, c0 = f2(A,y,u) \u2202x = u 2 \u221a A , and d0 = f1(A,y,u) \u2202y = 0. It follows that 1 + d0 > 0 and b0 (a0K \u2212 b0A) = b20 (K \u2212 A). Therefore, by Theorem ?? above, (A, u \u221a A) is a an attractor if K < A and a saddle if K > A. For the steady state (K, u \u221a K), we have\ntr(J) = 2Kf1 x ( \u2202f1 \u2202x \u2212 f1 x )( 1\u2212 A x ) \u2212 1 + \u2202f2 \u2202y\n= 2Ku \u221a x\nx\n( u\n2 \u221a x \u2212 u \u221a x x\n)( 1\u2212 A\nx\n) \u2212 1\n=\n( Ku2\nx \u2212 2Ku\n2\nx\n)( 1\u2212 A\nx\n) \u2212 1\n= \u2212 [ 1 + Ku2\nx\n( 1\u2212 A\nx\n)] .\nAnd with x = K, we will have: tr(J) = \u2212 [ 1 + u2 ( 1\u2212 A\nK\n)] .\nWe also have:\ndet(J) = 2Kf1 x ( \u2202f1 \u2202x \u2212 f1 x )( 1\u2212 A x )( \u22121 + \u2202f2 \u2202y ) + 2f1 \u2202f2 \u2202x ( 1\u2212 A x ) = 2Ku \u221a x\nx\n( u\n2 \u221a x \u2212 u \u221a x x\n)( 1\u2212 A\nx\n) (\u22121) + 2u \u221a x u\n2 \u221a x\n( 1\u2212 A\nx ) = (\u22121)\u2212Ku 2\nx\n( 1\u2212 A\nx\n) + u2 ( 1\u2212 A\nx ) = u2 ( 1\u2212 A\nx\n)( 1 + K\nx\n) .\nAnd with x = K, we will have det(J) = 2u2 (\n1\u2212 A K\n) .\nAlso,\n\u2206 = [ 2Kf1 x ( \u2202f1 \u2202x \u2212 f1 x )( 1\u2212 A x ) + 1\u2212 \u2202f2 \u2202y ]2 \u2212 8f1 \u2202f2 \u2202x ( 1\u2212 A x ) = [ \u2212Ku2\nx\n( 1\u2212 A\nx\n) + 1 ]2 \u2212 8u \u221a x u\n2 \u221a x\n( 1\u2212 A\nx ) = [ \u2212Ku2\nx\n( 1\u2212 A\nx\n) + 1 ]2 \u2212 4u2 ( 1\u2212 A\nx ) = [ 1\u2212 Ku 2\nx\n( 1\u2212 A\nx\n)]2 \u2212 4u2 ( 1\u2212 A\nx\n)\nFor x = K, we have\n\u2206 = [ 1\u2212 u2 ( 1\u2212 A\nK\n)]2 \u2212 4u2 ( 1\u2212 A\nK\n) .\nPutting \u03b1 = u2 ( 1\u2212 A\nK\n) , then \u2206 = \u03b12 \u2212 6\u03b1 + 1. The roots are\n\u03b11 = 6\u2212 \u221a 32\n2 , \u03b12 =\n6 + \u221a 32\n2 .\nIt follows that \u2206 > 0 if \u03b1 < \u03b11 or \u03b1 > \u03b12 and \u2206 < 0 if \u03b11 < \u03b1 < \u03b12. From Theorem ?? above, we conclude that x = K is either a saddle or an attractor.\n6.3 Appendix B: Stability analysis of System (3.6)"
        },
        {
            "heading": "6.3.1 Appendix B1",
            "text": "The steady states are obtained when we are on the x\u2212, y-, and z-isoclines. As above, on the x-isocline, we have either y = 0, or f1(x, u)\u2212 yx\nK = 0, or A = \u2016W\u20162. On the y-isocline,\nwe will have y = f2(x, y, z, u), and on the z-isocline, we will have z = 1\u2212 f3(x, y, u). Now we let the Jacobian matrix at a point (x, y, z) be\nJ := J(x, y, z) =  \u03b11 \u03b12 \u03b13 \u03b22 \u03b22 \u03b23\n\u03b31 \u03b32 \u03b33  . where\n\u03b11 = \u2202g1(x, y, z)\n\u2202x = 2y\n( \u2202f1(x, u)\n\u2202x \u2212 y K\n)( 1\u2212 A\nx\n) + 2y ( f1(x, u)\u2212 yx\nK )(A x2 )\n\u03b12 = \u2202g1(x, y, z)\n\u2202y =\n( 2f1(x, u)\u2212 4 yx\nK\n)( 1\u2212 A\nx\n) , \u03b13 = \u2202g1(x, y, z)\n\u2202z = 0\n\u03b21 = \u2202g2(x, y, z)\n\u2202x =\n\u2202f2(x, y, z, u)\n\u2202x , \u03b22 =\n\u2202g2(x, y, z) \u2202y = \u22121 + \u2202f2(x, y, z, u) \u2202y\n\u03b23 = \u2202g2(x, y, z)\n\u2202z =\n\u2202f2(x, y, z, u)\n\u2202z , \u03b31 =\n\u2202g3(x, y, z) \u2202x = \u2202f3(x, y, u) \u2202x\n\u03b32 = \u2202g3(x, y, z)\n\u2202y =\n\u2202f3(x, y, u)\n\u2202y , \u03b33 =\n\u2202g3(x, y, z)\n\u2202z = \u22121 .\nIn the linear case, we have f1(x, u) = u \u221a x, f2(x, y, z, u) = u \u221a x + zy. With Goodall\u2019s\nmodel, we will have f3(x, y, u) = \u2212uy \u221a x. It follows that\n\u03b11 = 2y\n( u\n2 \u221a x \u2212 y K\n)( 1\u2212 A\nx\n) + 2y ( u \u221a x\u2212 2yx\nK\n)( A\nx2 ) \u03b12 = ( 2u \u221a x\u2212 4yx\nK\n)( 1\u2212 A\nx\n) , \u03b13 = 0\n\u03b21 = u\n2 \u221a x , \u03b22 = z \u2212 1, \u03b23 = y\n\u03b31 = uy\n2 \u221a x\n= y\u03b21, \u03b32 = \u2212u \u221a x, \u03b33 = \u22121 .\nNow let us discuss the stability of the steady states.\nCase 1: Stability of the line (x, 0, 1), x > 0. In this case, we have\nA0 := J(x, 0, 1) =  0 2u \u221a x ( 1\u2212 A x ) 0 u 2 \u221a x 0 0\n0 \u2212u \u221a x \u22121\n .\nThe eigenvalues are \u03bb1 = \u22121, \u03bb2 = u \u221a\n1\u2212 A x , \u03bb3 = \u2212u\n\u221a 1\u2212 A\nx .\nSince y := v = 0 =\u21d2 u = 0, the eigenvalues are actually\n\u03bb1 = \u22121, \u03bb2 = 0, \u03bb3 = 0 .\nConsequently, the line(x, 0, 1) is always stable. Case 3: Stability of the points Bi, 1 \u2264 i \u2264 2. For B1, we know that x = A, y = \u22121 and z = 1 + u \u221a A. In this case,\n\u03b11 = \u22122 (\nu\u221a A + 2 K\n) ; \u03b12 = 0, \u03b13 = 0\n\u03b21 = u\n2 \u221a A , \u03b22 = u\n\u221a A; \u03b23 = \u22121\n\u03b31 = \u2212 u\n2 \u221a A\n; \u03b32 = \u2212u \u221a A, \u03b33 = \u22121 .\nReparametrizing as \u03b2 = u\n2 \u221a A\nand \u03b3 = u \u221a A, the Jacobian matrix is given as\nA31 :=  \u03b11 0 0 \u03b2 \u03b3 \u22121\n\u2212\u03b2 \u2212\u03b3 \u22121\n .\n\u03bb1 = \u03b11, \u03bb2 = 1\n2\n[ \u03b3 \u2212 1 + \u221a (\u03b3 \u2212 1)2 + 8\u03b3 ] , \u03bb3 = 1\n2\n[ \u03b3 \u2212 1\u2212 \u221a (\u03b3 \u2212 1)2 + 8\u03b3 ] .\nCase 311: u > 0. In this case, \u03b3 > 0, and therefore,\u221a\n(\u03b3 \u2212 1)2 + 8\u03b3 = \u221a \u03b32 + 6\u03b3 + 1 = \u221a (\u03b3 + 1)2 + 4\u03b3 \u2265 |\u03b3 + 1| = \u03b3 + 1.\nIt follows that 2\u03bb2 = \u03b3 \u2212 1 + \u221a\n(\u03b3 + 1)2 + 4\u03b3 \u2265 2\u03b3 > 0, and consequently, the point B1 is unstable.\nCase 312: u < 0. In this case, \u03b3 < 0, and therefore, \u03bb3 < 0. We also have\n0 \u2264 \u221a (\u03b3 \u2212 1)2 + 8\u03b3 \u2264 |\u03b3 \u2212 1| .\nIt follows that \u03b3 \u2212 1 \u2264 2\u03bb2 \u2264 \u03b3 \u2212 1 + |\u03b3 \u2212 1| .\nSince \u03b3 < 0 < 1, we conclude that\n\u03b3 \u2212 1 \u2264 2\u03bb2 \u2264 0.\nWe finally note that \u03bb1 = \u03b11 \u2264 0 if u \u2265 \u22122 \u221a A\nK . We note that\nmax { \u22122 \u221a A\nK ,\u22122 \u221a A 2A\n} = \u22122 \u221a Amin { 1\n2A ,\n1\nK\n} = \u22122 \u221a Amax {2A,K} .\nWe conclude that if \u22122 \u221a A K < 2 \u221a Amax {2A,K} < u < 0, the point B1 is stable and if not, it is unstable.\nFor B2, we know that x = A, y = 1 and z = 1\u2212 u \u221a A. In this case, we have\n\u03b11 = 2 ( u\u221a A \u2212 2 K ) ; \u03b12 = 0, \u03b13 = 0\n\u03b21 = u\n2 \u221a A , \u03b22 = \u2212u\n\u221a A; \u03b23 = \u22121\n\u03b31 = \u2212 u\n2 \u221a A\n; \u03b32 = \u2212u \u221a A, \u03b33 = \u22121 .\nThe Jacobian matrix is given as\nA32 :=  \u03b11 0 0 \u03b21 \u2212\u03b3 \u22121\n\u03b21 \u2212\u03b3 \u22121  . We find the eigenvalues to be\n\u03bb1 = 0 \u03bb2 = \u03b11, \u03bb3 = \u2212\u03b3 \u2212 1 .\nCase 321: u > 0. In this case, \u03bb3 < 0 since \u03b3 > 0. If u < 2 \u221a A\nK , then \u03bb2 = \u03b11 = 2\n( u\u221a A \u2212 2 K ) < 0. It\nfollows that B2 is stable, if not, it is unstable. Case 322: u < 0. Then \u03bb3 = \u2212\u03b3 \u2212 1 < 0 if \u03b3 > \u22121, that is, if u \u221a A > \u22121. Also, \u03bb2 = \u03b11 < 0 if u < 0. In conclusion\nif \u2212 1\u221a A\n= \u22122 \u221a A\n2A < \u22122 \u221a Amax {2A,K} < u < 0, the point B2 is stable."
        },
        {
            "heading": "6.3.2 Appendix B2",
            "text": "Case 2: Stability of the lines Li, 1 \u2264 i \u2264 2.\nFor L1, we know that y = 1 and z = 1\u2212 x\nK , and\n\u03b11 = 2\n( u\n2 \u221a x \u2212 1 K\n)( 1\u2212 1\nA\n) + 2 ( u \u221a x\u2212 2x\nK\n)( A\nx2 ) \u03b12 = ( 2u \u221a x\u2212 4x\nK\n)( 1\u2212 A\nx\n) , \u03b13 = 0\n\u03b21 = u\n2 \u221a x\n\u03b22 = \u2212 x\nK \u03b23 = \u22121\n\u03b31 = u\n2 \u221a x , \u03b32 = \u2212u\n\u221a x, \u03b33 = \u22121\nThe Jacobian matrix is given as\nA21 :=  \u03b11 \u03b12 0 \u03b21 \u03b22 \u22121\n\u03b21 \u03b32 \u22121\n .\nThe characteristic polynomial is P (\u2212\u03bb) = \u2212\u03bb3 + a2\u03bb2 + a1\u03bb+ a0. where\na2 = (\u03b11 + \u03b22 \u2212 1) a1 = \u03b11(1\u2212 \u03b22) + \u03b22 \u2212 \u03b32 + \u03b12\u03b21 a0 = \u03b11(\u03b32 \u2212 \u03b22)\nFrom Viete\u2019s theorem Vie\u0300te (1646), there are three cases to consider: Case 21: a0 = 0.\nIn this case, P (\u03bb) = \u03bb(\u2212\u03bb2 + a2\u03bb+ a1). The roots are\n\u03bb1 = 0, \u03bb2 = 1\n2\n[ a2 \u2212 \u221a a22 + 4a1 ] , \u03bb3 = 1\n2\n[ a2 + \u221a a22 + 4a1 ] .\nSuppose that a22 + 4a1 < 0. If a2 < 0, then \u03bb2 and \u03bb3 are complex conjugates eigenvalues with negative real parts, thus L1 is stable attractor. If a2 > 0, then L1 is unstable.\nSuppose that a22 +4a1 > 0. If a1 > 0 and a2 > 0, then \u03bb2 and \u03bb3 are real eigenvalues with \u03bb3 > 0, thus L1 is unstable. If a1 < 0 and a2 < 0, then \u03bb2 < 0 and \u03bb3 \u2264 12 [a2 + |a2|] = 0. It follows that L1 is stable. If a1 > 0 and a2 < 0 or a1 < 0 and a2 > 0, then one of the eigenvalues if positive, and thus L1 is unstable.\nCase 22: a0 < 0.\nThis means that we have 3 cases: 3 negative eigenvalues, 1 negative eigenvalue and 2 complex conjugate eigenvalues, or 2 positive and 1 negative eigenvalues. Clearly, in case the eigenvalues are all negative, L1 is stable. In the third case, L1 is unstable. In the second case, L1 may or may not be stable. Case 22: a0 > 0. This means that we also have 3 cases: 3 positive roots, 1 positive eigenvalues and 2 complex conjugate eigenvalues, 2 negative eigenvalues and one positive eigenvalue. In the first and third cases, L1 is unstable. In the second case, L1 may or may not be stable.\nFor L2, we know that y = \u22121 and z = 1\u2212 x\nK . The only difference between this case and\nthe previous is that here, \u03b11 = \u22122 ( u\n2 \u221a x\n+ 1\nK\n)( 1\u2212 1\nA\n) + 2 ( u \u221a x+ 2x\nK\n)( A\nx2 ) \u03b12 = \u2212 ( 2u \u221a x+ 4x\nK\n)( 1\u2212 A\nx\n) ."
        }
    ],
    "title": "Strong Allee effect synaptic plasticity rule in an unsupervised learning environment",
    "year": 2022
}