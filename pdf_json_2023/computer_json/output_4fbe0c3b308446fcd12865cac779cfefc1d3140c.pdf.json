{
    "abstractText": "Segmenting brain tumors in MR modalities is an important step in treatment planning. Recently, the majority of methods rely on Fully Convolutional Neural Networks (FCNNs) that have acceptable results for this task. Among various networks, the U-shaped architecture known as U-Net, has gained enormous success in medical image segmentation. However, absence of long-range association and the locality of convolutional layers in FCNNs can create issues in tumor segmentation with different tumor sizes. Due to the success of Transformers in natural language processing (NLP) as a result of using self-attention mechanism to model global information, some studies designed different variations of vision based UShaped Transformers. So, to get the effectiveness of U-Net we proposed TransDoubleU-Net which consists of double Ushaped nets for 3D MR Modality segmentation of brain images based on dual scale Swin Transformer for the encoder part and dual level decoder based on CNN and Transformers for better localization of features. The model's core uses the shifted windows multi-head self-attention of Swin Transformer and skip connections to CNN based decoder. The outputs are evaluated on BraTS2019 and BraTS2020 datasets and showed promising results in segmentation. INDEX TERMS Brain Tumor Segmentation, Vision Transformer, Swin Transformer, Dual Scale, U-Net, BraTS.",
    "authors": [
        {
            "affiliations": [],
            "name": "Marjan Vatanpour"
        },
        {
            "affiliations": [],
            "name": "Javad Haddadnia"
        }
    ],
    "id": "SP:0db70ec25f770340933ecb8c4015e30d2bd2ac32",
    "references": [
        {
            "authors": [
                "M. Monteiro"
            ],
            "title": "Multiclass semantic segmentation and quantification of traumatic brain injury lesions on head CT using deep learning: an algorithm development and multicentre validation study",
            "venue": "The Lancet Digital Health, vol. 2, no. 6, pp. e314-e322, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "O. Ronneberger",
                "P. Fischer",
                "T. Brox"
            ],
            "title": "U-net: Convolutional networks for biomedical image segmentation",
            "venue": "International Conference on Medical image computing and computer-assisted intervention, 2015: Springer, pp. 234-241.",
            "year": 2015
        },
        {
            "authors": [
                "M. Aghalari",
                "A. Aghagolzadeh",
                "M. Ezoji"
            ],
            "title": "Brain tumor image segmentation via asymmetric/symmetric UNet based on two-pathwayresidual blocks",
            "venue": "Biomedical Signal Processing and Control, vol. 69, p. 102841, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "G. Du",
                "X. Cao",
                "J. Liang",
                "X. Chen",
                "Y. Zhan"
            ],
            "title": "Medical image segmentation based on u-net: A review",
            "venue": "Journal of Imaging Science and Technology, vol. 64, pp. 1-12, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "M.Z. Alom",
                "C. Yakopcic",
                "M. Hasan",
                "T.M. Taha",
                "V.K. Asari"
            ],
            "title": "Recurrent residual U-Net for medical image segmentation",
            "venue": "Journal of Medical Imaging, vol. 6, no. 1, p. 014006, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "J. Yang",
                "J. Zhu",
                "H. Wang",
                "X. Yang"
            ],
            "title": "Dilated MultiResUNet: Dilated multiresidual blocks network based on U-Net for biomedical image segmentation",
            "venue": "Biomedical Signal Processing and Control, vol. 68, p. 102643, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "W. Chen",
                "B. Liu",
                "S. Peng",
                "J. Sun",
                "X. Qiao"
            ],
            "title": "S3D-UNet: separable 3D U-Net for brain tumor segmentation",
            "venue": "International MICCAI Brainlesion Workshop, 2018: Springer, pp. 358-368.",
            "year": 2018
        },
        {
            "authors": [
                "Z. Zhou",
                "M.M. Rahman Siddiquee",
                "N. Tajbakhsh",
                "J. Liang"
            ],
            "title": "Unet++: A nested u-net architecture for medical image segmentation",
            "venue": "Deep learning in medical image analysis and multimodal learning for clinical decision support: Springer, 2018, pp. 3- 11.",
            "year": 2018
        },
        {
            "authors": [
                "Y. Gao",
                "M. Zhou",
                "D.N. Metaxas"
            ],
            "title": "UTNet: a hybrid transformer architecture for medical image segmentation",
            "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention, 2021: Springer, pp. 61-71.",
            "year": 2021
        },
        {
            "authors": [
                "D. Bahdanau",
                "K. Cho",
                "Y. Bengio"
            ],
            "title": "Neural machine translation by jointly learning to align and translate",
            "venue": "arXiv preprint arXiv:1409.0473, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "C. Kaul",
                "S. Manandhar",
                "N. Pears"
            ],
            "title": "Focusnet: An attention-based fully convolutional network for medical image segmentation",
            "venue": "2019 IEEE 16th international symposium on biomedical imaging (ISBI 2019), 2019: IEEE, pp. 455-458.",
            "year": 2019
        },
        {
            "authors": [
                "J. Zhang",
                "Z. Jiang",
                "J. Dong",
                "Y. Hou",
                "B. Liu"
            ],
            "title": "Attention gate resU-Net for automatic MRI brain tumor segmentation",
            "venue": "IEEE Access, vol. 8, pp. 58533-58545, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "R. Arora",
                "B. Raman",
                "K. Nayyar",
                "R. Awasthi"
            ],
            "title": "Automated skin lesion segmentation using attention-based deep convolutional neural network",
            "venue": "Biomedical Signal Processing and Control, vol. 65, p. 102358, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "W. Weng",
                "X. Zhu"
            ],
            "title": "INet: convolutional networks for biomedical image segmentation",
            "venue": "Ieee Access, vol. 9, pp. 16591-16603, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S. Feng"
            ],
            "title": "CPFNet: Context pyramid fusion network for medical image segmentation",
            "venue": "IEEE transactions on medical imaging, vol. 39, no. 10, pp. 3008-3018, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "H. Park",
                "J. Paik"
            ],
            "title": "Pyramid attention upsampling module for object detection",
            "venue": "IEEE Access, vol. 10, pp. 38742-38749, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J. Chen"
            ],
            "title": "Transunet: Transformers make strong encoders for medical image segmentation",
            "venue": "arXiv preprint arXiv:2102.04306, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J.M.J. Valanarasu",
                "P. Oza",
                "I. Hacihaliloglu",
                "V.M. Patel"
            ],
            "title": "Medical transformer: Gated axialattention for medical image segmentation",
            "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention, 2021: Springer, pp. 36-46.",
            "year": 2021
        },
        {
            "authors": [
                "H. Cao"
            ],
            "title": "Swin-unet: Unet-like pure transformer for medical image segmentation",
            "venue": "arXiv preprint arXiv:2105.05537, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "W. Wang",
                "C. Chen",
                "M. Ding",
                "H. Yu",
                "S. Zha",
                "J. Li"
            ],
            "title": "Transbts: Multimodal brain tumor segmentation using transformer",
            "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention, 2021: Springer, pp. 109-119.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Xie",
                "J. Zhang",
                "C. Shen",
                "Y. Xia"
            ],
            "title": "Cotr: Efficiently bridging cnn and transformer for 3d medical image segmentation",
            "venue": "International conference on medical image computing and computer-assisted intervention, 2021: Springer, pp. 171-180.",
            "year": 2021
        },
        {
            "authors": [
                "A. Hatamizadeh"
            ],
            "title": "Unetr: Transformers for 3d medical image segmentation",
            "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2022, pp. 574-584.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Wu",
                "G. Wang",
                "Z. Wang",
                "H. Wang",
                "Y. Li"
            ],
            "title": "DI-Unet: Dimensional interaction self-attention for medical image segmentation",
            "venue": "Biomedical Signal Processing and Control, vol. 78, p. 103896, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "F. Milletari",
                "N. Navab",
                "S.-A. Ahmadi"
            ],
            "title": "V-net: Fully convolutional neural networks for volumetric medical image segmentation",
            "venue": "2016 fourth international conference on 3D vision (3DV), 2016: IEEE, pp. 565-571.",
            "year": 2016
        },
        {
            "authors": [
                "S. Bakas"
            ],
            "title": "Advancing the cancer genome atlas glioma MRI collections with expert segmentation labels and radiomic features",
            "venue": "Scientific data, vol. 4, no. 1, pp. 1-13, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "S. Bakas"
            ],
            "title": "Identifying the best machine learning algorithms for brain tumor segmentation, progression assessment, and overall survival prediction in the BRATS challenge",
            "venue": "arXiv preprint arXiv:1811.02629, 2018.",
            "year": 1811
        },
        {
            "authors": [
                "B.H. Menze"
            ],
            "title": "The multimodal brain tumor image segmentation benchmark (BRATS)",
            "venue": "IEEE transactions on medical imaging, vol. 34, no. 10, pp. 1993-2024, 2014.",
            "year": 1993
        },
        {
            "authors": [
                "H.A.S. Bakas",
                "A. Sotiras",
                "M. Bilello",
                "M. Rozycki",
                "J. Kirby"
            ],
            "title": "segmentation labels and radiomic features for the pre-operative scans of the TCGA- GBM collection",
            "venue": "the cancer imaging archive, 2017, doi: 10.7937/K9. This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3330958 This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/ 6 VOLUME XX, 2023",
            "year": 2017
        },
        {
            "authors": [
                "\u00d6. \u00c7i\u00e7ek",
                "A. Abdulkadir",
                "S.S. Lienkamp",
                "T. Brox",
                "O. Ronneberger"
            ],
            "title": "3D U-Net: learning dense volumetric segmentation from sparse annotation",
            "venue": "International conference on medical image computing and computer-assisted intervention, 2016: Springer, pp. 424-432.",
            "year": 2016
        },
        {
            "authors": [
                "Z. Zhang",
                "Q. Liu",
                "Y. Wang"
            ],
            "title": "Road extraction by deep residual u-net",
            "venue": "IEEE Geoscience and Remote Sensing Letters, vol. 15, no. 5, pp. 749-753, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "H. Peiris",
                "M. Hayat",
                "Z. Chen",
                "G. Egan",
                "M. Harandi"
            ],
            "title": "A volumetric transformer for accurate 3d tumor segmentation",
            "venue": "arXiv preprint arXiv:2111.13300, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "L. ZongRen",
                "W. Silamu",
                "W. Yuzhen",
                "W. Zhe"
            ],
            "title": "DenseTrans: Multimodal Brain Tumor Segmentation Using Swin Transformer",
            "venue": "IEEE Access, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "Z. Jiang",
                "C. Ding",
                "M. Liu",
                "D. Tao"
            ],
            "title": "Two-stage cascaded u-net: 1st place solution to brats challenge 2019 segmentation task",
            "venue": "Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries: 5th International Workshop, BrainLes 2019, Held in Conjunction with MICCAI 2019, Shenzhen, China, October 17, 2019, Revised Selected Papers, Part I 5, 2020: Springer, pp. 231- 241.",
            "year": 2019
        },
        {
            "authors": [
                "F. Isensee",
                "P.F. J\u00e4ger",
                "P.M. Full",
                "P. Vollmuth",
                "K.H. Maier-Hein"
            ],
            "title": "nnU-Net for brain tumor segmentation",
            "venue": "Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries: 6th International Workshop, BrainLes 2020, Held in Conjunction with MICCAI 2020, Lima, Peru, October 4, 2020, Revised Selected Papers, Part II 6, 2021: Springer, pp. 118-132.",
            "year": 2020
        },
        {
            "authors": [
                "B. Chen",
                "T. Dao",
                "E. Winsor",
                "Z. Song",
                "A. Rudra",
                "C. R\u00e9"
            ],
            "title": "Scatterbrain: Unifying sparse and low-rank attention",
            "venue": "Advances in Neural Information Processing Systems, vol. 34, pp. 17413-17426, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J.M.J. Valanarasu",
                "V.A. Sindagi",
                "I. Hacihaliloglu",
                "V.M. Patel"
            ],
            "title": "Kiu-net: Overcomplete convolutional architectures for biomedical image and volumetric segmentation",
            "venue": "IEEE Transactions on Medical Imaging, vol. 41, no. 4, pp. 965-976, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "O. Oktay"
            ],
            "title": "Attention u-net: Learning where to look for the pancreas",
            "venue": "arXiv preprint arXiv:1804.03999, 2018.",
            "year": 1804
        },
        {
            "authors": [
                "Y. Jiang",
                "Y. Zhang",
                "X. Lin",
                "J. Dong",
                "T. Cheng",
                "J. Liang"
            ],
            "title": "SwinBTS: A Method for 3D Multimodal Brain Tumor Segmentation Using Swin Transformer",
            "venue": "Brain Sciences, vol. 12, no. 6, p. 797, 2022.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "VOLUME XX, 2023 1\nI. INTRODUCTION Computer vision has been used for analysis of medical images as a result of the advancement of deep learning. Image segmentation is essential in medical image analysis since it is frequently the initial step in analyzing anatomical structures [1]. After the introduction of Ushaped network, U-Net [2], convolutional neural networks (CNNs) were chosen as the main method to do this task [3, 4]. Further, the performance of the U-Net was improved by some variants of it such as Res-UNet [5, 6], 3D U-Net [7] and U-Net++ [8]. Although CNNbased methods play a major role in segmenting medical images, they also have limitations. First, the convolution only collects data from nearby pixels and cannot explicitly express long-range and global semantic information interaction. Second, because convolution kernels are often fixed in size and shape, they cannot adapt to the input content [9]. By combining the CNN methods with the attention mechanism [10-12], using atrous convolutional layer [13, 14] and image pyramids [15, 16], these restrictions can be overcome to some extent. These methodologies, however, have drawbacks when it comes to representing long-range dependencies. Recently, Transform-based frameworks have been proposed as an alternate design, and it has performed well on a variety of computer vision tasks. Chen et\nal.[17] used the strong combination of Transformers and U-Net as an alternative method for segmenting medical images. Since Transformer training requires large-scale datasets, Valanarasu et al.[18] proposed a gated position-sensitive axial attention mechanism and LocalGlobal methodology for training the Transformer which was successful. Cao et al.[19] concluded that using skip connections for Transformer is effective, so they constructed an encoder-decoder architecture using skip connections based on Swin Transformer. For extraction of the volumetric spatial feature maps, 3D CNN is used in the encoder by Wang et al.[20] and Transformer is utilized to model global features. Gao et al.[9] suggested a U-shape hybrid Transformer network, which combines improvements in convolutional layers with a self-attention mechanism. The hybrid approach enables Transformer to be initialized into Conv-Nets without the need for pretraining and self-attention enables operations in encoder and decoder layers effectively achieve global dependencies. Vanilla Transformer treats equally each image position, but to reduce the computational costs and focus only on special part of image, Xie et al.[21] introduced a new attention mechanism in which only some key parts around a reference point are considered for the self-attention mechanism. For segmentation of 3D images, Hatamizadeh et al.[22] proposed a method\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n2 VOLUME XX, 2023\nthat learns representations of the input with the help of a Transformer as the encoder. The Transformer's self-attention mechanism simply investigates the feature maps spatial dimension connections, with no interaction with the channel dimension. So, Wu et al.[23] developed Dimensional Interactive (DI) self-attention that its combination with U-Net (DI-Unet) had better performance than Swin-Unet in large datasets. Inspired by these observations, in this study we have proposed a novel approach for brain tumor segmentation and our original contributions are:\n\u2022 TransDoubleU-Net as a novel architecture combining a dual-scale Swin Transformer.\n\u2022 Designing a dual-level decoder which consist of convolutional layers and transformers.\n\u2022 Combining the encoder with dual-level decoder using skip connections and Transformers."
        },
        {
            "heading": "II. MATERIAL AND METHODS",
            "text": ""
        },
        {
            "heading": "A) Architecture Overview",
            "text": "The architecture of the proposed approach is illustrated in Figure 1. The input has a size of H\u00d7W\u00d7D\u00d74 as we have 4 modalities for every subject. The proposed method consists of double U-Shaped networks which we call \u201cTransDoubleU-Net\u201d that every part is described as below. 1) Network Encoder The raw 3D image is fed to a pre-processing unit. After removing the backgrounds and resizing, the output is in size of H\u00d7W\u00d7D\u00d74. The encoder's main part is a DualSwin Transformer (Figure 2). In order to create the appropriate tokens for Swin Transformer, a patch partition layer is considered. The feature dimension of patch partitioning layer is 256 and 32 with patch resolutions of (!\n\"# )$and (! # )$ respectively, where s\ndenotes the patch size. A linear embedding layer is creating 3D tokens to Swin Transformer with dimensions of C=48 for patches of size S and 2C = 96 for patches of size 2S. The self-attention mechanism is computed in non-overlapping windows of size M=8. In every subsequent layer of the Transformer the windows are shifted by M/2. We can calculate the outputs of every stage in layer l and l+1 with the following: ?\u0302?% = \ud835\udc4a \u2212\ud835\udc40\ud835\udc46\ud835\udc34 ,\ud835\udc3f\ud835\udc41(\ud835\udc67%&')/\n+ \ud835\udc67%&' \ud835\udc67% = \ud835\udc40\ud835\udc3f\ud835\udc43,\ud835\udc3f\ud835\udc41(?\u0302?%)/ + ?\u0302?%\n?\u0302?%(' = \ud835\udc46\ud835\udc4a \u2212\ud835\udc40\ud835\udc46\ud835\udc34,\ud835\udc3f\ud835\udc41(\ud835\udc67%)/ + \ud835\udc67%\n\ud835\udc67%(' = \ud835\udc40\ud835\udc3f\ud835\udc43(\ud835\udc3f\ud835\udc41(?\u0302?%(')) + ?\u0302?%('.\n1\nThe outputs of W-MSA and SW-MSA are windowed multi-head self-attention and shifted window denoted as \ud835\udc67!\ud835\udc59 and \ud835\udc67!\ud835\udc59+1. The input for every windowed multi-head\nself-attention is passed through an LN and an MLP which they represent layer normalization and multilayer perceptron respectively. The W-MSA applies selfattention in windows. To overcome the problem of effective information between windows the SW-MSA will help. The input for SW-MSA will pass through a LN first. To compute the shifted window, a 3D cyclicshifting [22] is considered and the attention is computed as below: \ud835\udc34\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b(\ud835\udc44 \ud835\udc3e \ud835\udc49) = \ud835\udc46\ud835\udc5c\ud835\udc53\ud835\udc61\ud835\udc40\ud835\udc4e\ud835\udc65(\ud835\udc44\ud835\udc3e,/ \u221a\ud835\udc51 + \ud835\udc35) \u2217 \ud835\udc49 2 Where Q, K, V denote the query, key and values. d and B show dimension of query or key and bias matrix, respectively. For denoting a hierarchical representation, the tokens are reduced in every stage of the network, but the size of features are doubled with the help of a patch merge layer. Patch merge layer concatenates features of dimension 2\u00d72\u00d72 and then applying a linear layer to reduce the feature dimension by factor of 2. So the output resolution is reduced by a factor of 2 and the feature dimension is increased by a factor of 2 for every stage of the Swin Transformer. So, the output of every stage is (!\n# ), (! \"# ), (! -# ) and (! .# ) for one Swin Transformer\nwith patch size of C and (! \"# ), (! -# ), (! .# ) and ( ! '/#\n) for Swin Transformer with patch size of 2C."
        },
        {
            "heading": "2) Network Decoder",
            "text": "The decoders\u2019 blocks are shown in Figure 3 the decoder connects to encoder with the help of skip connections. For every connection a block named B consists of convolution layers is used for creating richer representation of features. The feature resolution is doubled and the number of features are halved with the help of block M. The 1\u00d71\u00d71 convolution layer helps with controlling the features dimension. From bottom to top of the decoder feature dimensions are halved except for the top layer where only the resolution of features are doubled for the direction with patch embedding size of C and quadrupled for patch embedding size of 2C. In the next part, after concatenation of the related parts with skip connections, the first, two bottom layers are concatenated properly through K and L blocks and converted to desired shape with block R, then fed to a vanilla Transformer. The same procedure is applied to the second, two layers from bottom of the architecture and the results are concatenated with the last step. The top most layer goes through a Transformer and after proper dimension reduction, it is concatenated with the last step result. The final segmented output is calculated via a 1\u00d71\u00d71 convolution layer."
        },
        {
            "heading": "B) Loss Function",
            "text": "The loss function that is used in this study is calculated as the sum of weighted cross-entropy and soft Dice loss function [24]:\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nVOLUME XX, 2023 3\n\ud835\udc3f01 = \u2212C\ud835\udc662\ud835\udc59\ud835\udc5c\ud835\udc54\ud835\udc663G 4\n25'\n3\n\ud835\udc3f67 (\ud835\udc4c \ud835\udc4cI) = 1\n\u2212 2 \ud835\udc41C \u2211 \ud835\udc4c8 :\ud835\udc4cI8 :;85' \u2211 \ud835\udc4c8 :\";85' +\u2211 \ud835\udc4cI8 :\";85'\n4\n:5'\n4\n\ud835\udc3f<=<>% = \ud835\udefc\ud835\udc3f01 + \ud835\udefd\ud835\udc3f67 5 Where the number of classes is denoted as N, K is voxel number, \ud835\udc4c\ud835\udc58 \ud835\udc5b and \ud835\udc4c!\ud835\udc58 \ud835\udc5b represents one-hot encoded label and the probability of output for class n at voxel k, respectively.\nIII. RESULTS"
        },
        {
            "heading": "A) Dataset",
            "text": "The datasets that are used in this work are provided by the Brain Tumor Segmentation: BraTS2019 and BraTS2020 challenge [25-28]. There are 335 and 369 patients for training in BraTS2019 and BraTS2020, respectively and both datasets consist of 125 cases for validation. Four modalities of brain MRI scans are included in each sample, which are named: T1-weighted (T1), post-contrast T1-weighted (T1ce), T2-weighted (T2) and Fluid Attenuated Inversion Recovery (FLAIR). The size of all the MR images is 240 \u00d7 240 \u00d7 155. The labels of both datasets are as follows: Label 0: Background, Label 1: Necrotic and non-enhancing tumor (NCR/NET), Label 2: Peritumoral edema (ED) and Label 4: GD-enhancing tumor.\nB) Implementation Details The proposed method is done in python 3.8 with PyTorch. The operating system is Windows 10 and CPU is Intel i7 11700k with 32 Gb of RAM and Nvidia RTX 3090 GPU. The model trained with Adam optimizer with initial learning rate of 0.0003 for 50 epochs, but other criteria such as early stopping, etc. were considered. To increase data diversity the following data augmentation is applied for all training cases: (1) random flip with probability of 0.5 across all axis, (2) random rotation which randomly rotates the images 0.2 degree, (3) random scaling and (4) random shift intensity was applied on every channel in [-0.1, 0.1] range."
        },
        {
            "heading": "C) Evaluation Metrics",
            "text": "The segmentation accuracy is evaluated by Dice score (DSC) and Hausdorff distance (95%) metrics. DSC is used to determine the overlap between the ground truth and the predictions and Hausdorff (HD) distance is used to compare ground truth images with segmentation\noutputs and to rate different segmentation outcomes in medical image segmentation.\nDSC= \",A B4( BA( \",A\n6 \ud835\udc3b\ud835\udc37 (\ud835\udc4b \ud835\udc4c) = \ud835\udc5a\ud835\udc4e\ud835\udc65 {\ud835\udc60\ud835\udc62\ud835\udc5d \ud835\udc56\ud835\udc5b\ud835\udc53 \ud835\udc51(\ud835\udc65 \ud835\udc66) \ud835\udc60\ud835\udc62\ud835\udc5d \ud835\udc56\ud835\udc5b\ud835\udc53 \ud835\udc51(\ud835\udc65 \ud835\udc66)} \ud835\udc65 \u2208 \ud835\udc4b \ud835\udc66 \u2208 \ud835\udc4c \ud835\udc66 \u2208 \ud835\udc4c \ud835\udc65 \u2208 \ud835\udc4b 7\nWhere TP, FN, FP are the number of True Positive, False Negative and False Positive voxels, respectively. And also sup, inf and d( , ) denote the supremum, infimum and the function that computes the distance between two points, while x and y represent the points on surface X of the ground truth and surface Y of the predicted regions respectively. Both metrics are measured for enhancing tumor (ET: Label 1), tumor core (TC: Label 1 + Label 4), and whole tumor (WT: Label 1+ Label 2+ Label 4)."
        },
        {
            "heading": "D) Quantitative Results",
            "text": "We start by doing a five-fold cross-validation evaluation on the BraTS2019 training set, which is a typical configuration used by many studies. Average DSC for our TransDoubleU-Net for ET, WT, and TC are 80.04%, 92.71%, and 85.69%, respectively. Additionally, we do tests on BraTS2019 validation dataset and compared the results with other existing SOTA approaches in Table I. The TransDoubleU-Net achieves the DSC of 78.97%, 91.44% and 83.36% for ET, WT and TC, respectively, which outperforms the previous SOTA methods. A significant improvement in segmentation has also been achieved in terms of HD. We also trained the TransDoubleU-Net on BraTS2020 training dataset and evaluated on validation dataset, the hyperparameters were the same. The comparison results with SOTA models can be seen in Table II. The TransDoubleU-Net achieves the DSC of 79.16%, 92.87% , 86.51% and HD of 19.953mm, 4.472mm, 7.885mm on ET, WT and TC, respectively. The quantitative results show that the TransDoubleUNet has considerable improvement in brain tumor segmentation compared with traditional CNN-based methods such as 3D U-Net [29], V-Net [24] and residual U-Net [30]. Additionally, it outperforms the models that used Transformer structure such as U-Netr [22], VTUNet [31] and TransBTS [20]. This reveals the benefit of using a Swin Transformer to represent global features and combining with a decoder based on convolution and base Transformer to detect where exactly are the target locations locally."
        },
        {
            "heading": "E) Qualitative Results",
            "text": "The result of TransDoubleU-Net segmentation and ground truth image is shown in Figure 4. The proposed method can achieve prominent results compared to ground truth mask. Although we can see clear errors in\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n4 VOLUME XX, 2023\nsome locations, but the difference is acceptable. The produced segmentation mask by TransDoubleU-Net confirms the efficacy of the dual scale Swin Transformer as the encoder and dual decoder for better localization of tumor.\nIV. DISCUSSION The performance of TransDoubleU-Net has been evaluated considering different folds of data against SOTA models. The average DSC and HD for BraTS2019 and BraTS2020 are 84.59%, 4.26mm and 86.18%, 10.77mm, respectively. The results available in Table I and Table II show that for all classes of ET, WT and TC the TransDoubleU-Net performs very well except ET compared to SOTA [32] in BraTS2020. Studying winners of BraTS2019 [33] and BraTS2020 [34], the proposed method performs comparable, except in ET in both DSC and HD. It is notable that in BraTS2019 the difference in TC is very small. According to results available in Table III and Figure 5, the efficacy of proposed architecture is clear in visual comparison of TransDoubleU-Net and its variation ablation studies. Performance boost of dual scale Swin Transformer is considered meaningful. The dual scale patches fed to Swin Transformers can help for better capturing of long-range dependencies and the windowed multi-head self-attention can embed contextual information of the images. It also helps in reducing the computational complexity of the model by considering windows of size M\u00d7M, but in order to capture the information of other windows, it will shift the windows by M/2 in every direction. The different patch sizes of encoder can implicitly induce the combination of local and global view of the inputs. The hierarchy of the encoder part is realized in reducing the resolution in every stage and increasing the feature dimension. The decoder part of the model holds the information of its own stage and the peer encoder stage concatenated together with convolution and instance normalization operations. This approach helps to decode the location of the tumor classes. It is important to notice the second level decoder for more detailed localization of tumor and it is somehow an answer to the dual scale encoder. For better evaluation of the impact of C and Patch size we performed another study in which different levels of Swin Transformer called L combined with variations of C and Patch size are compared. It is obvious that C=48, L=12 and Patch size = (2,4) can outperform other architectures. The reason for L is obvious as we can say the deeper Swin Transformers get the better results are achieved. Also, the feature dimension C can show better results if we collect more features from Swin Transformers (Table IV). Another study on second layer of decoder showed that if we combine the output of i=0 layer of the decoder with other decoder levels, the results may vary slightly and\nwe chose the best combination which was the {i=0 + i=(1+2) + i=(3+4)}(Table V). In Table VI the inference time for different architectural designs is available. It can be seen that as the design gets more complex, the results are better in terms of performance but there is a time delay in inference mode. It can induced from the results that the dual decoder can increase the testing time but improve performance results.\nV. CONCLUSION In this study a novel approach called TransDoubleU-Net for 3D semantic segmentation of brain tumor of MR images is proposed based on dual Swin Transformer as an encoder and a dual level decoder based on CNN and Transformers in order to better localization of features. In TransDoubleU-Net we leverage the benefits of hierarchical features of Swin Transformer and combine it with a dual-level decoder. The decoder does not evaluate the results of the encoder not only once but twice and it has proved for better performance. The method was evaluated on BraTS datasets and showed promising results. We believe that TransDoubleU-Net could be a fundamental method for further improvements in medical image segmentation. The method provides better performance in comparison with other related studies. The main reason is because of the dual U-shape networks which work in parallel and leveraging the Swin Transformer benefits. One the main limitations of this study is generalization which is mainly caused by the absence of more concrete datasets. It would be a good practice to provide a more sophisticated dataset for brain tumor segmentation, which we think is doable within near future. Another drawback of this study is testing and training time, which can be addressed in the future works, e.g. by using a modified version of attention, and using more powerful GPUs. For future works it would be of great help to visualize the experimental results of other studies to have a better understanding in terms of visual comparison. Also, we suggest using a modified version of attention such as Scatterbrain [35] for computational complexity reduction by combining sparse and low-rank attention, especially in the decoder part which can also help in reducing the testing time. Nevertheless, It may impact the final performance results but considering the reduction in memory resources dependency, it is a convenient approach. By considering such architecture, we may address some of the drawbacks mentioned before. Authors\u2019 Contributions All Authors has contributed equally in this study. All authors read and approved the final manuscript.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nVOLUME XX, 2023 5"
        }
    ],
    "title": "TransDoubleU-Net: Dual Scale Swin Transformer with Dual Level Decoder for 3D Multimodal Brain Tumor Segmentation",
    "year": 2023
}