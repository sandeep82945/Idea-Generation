{
    "abstractText": "We propose residual denoising diffusion models (RDDM), a novel dual diffusion process that decouples the traditional single denoising diffusion process into residual diffusion and noise diffusion. This dual diffusion framework expands the denoising-based diffusion models, initially uninterpretable for image restoration, into a unified and interpretable model for both image generation and restoration by introducing residuals. Specifically, our residual diffusion represents directional diffusion from the target image to the degraded input image and explicitly guides the reverse generation process for image restoration, while noise diffusion represents random perturbations in the diffusion process. The residual prioritizes certainty, while the noise emphasizes diversity, enabling RDDM to effectively unify tasks with varying certainty or diversity requirements, such as image generation and restoration. We demonstrate that our sampling process is consistent with that of DDPM and DDIM through coefficient transformation, and propose a partially path-independent generation process to better understand the reverse process. Notably, our RDDM enables a generic UNet, trained with only an l1 loss and a batch size of 1, to compete with state-of-the-art image restoration methods. We provide code and pre-trained models to encourage further exploration, application, and development of our innovative framework (https://github.com/nachifur/RDDM).",
    "authors": [
        {
            "affiliations": [],
            "name": "Jiawei Liu"
        },
        {
            "affiliations": [],
            "name": "Qiang Wang"
        },
        {
            "affiliations": [],
            "name": "Huijie Fan"
        },
        {
            "affiliations": [],
            "name": "Yinong Wang"
        },
        {
            "affiliations": [],
            "name": "Yandong Tang"
        },
        {
            "affiliations": [],
            "name": "Liangqiong Qu"
        }
    ],
    "id": "SP:c1b61a47753f286fc293dce90d20ab2f2a36e0e2",
    "references": [
        {
            "authors": [
                "Saeed Anwar",
                "Nick Barnes"
            ],
            "title": "Densely residual laplacian super-resolution",
            "venue": "IEEE TPAMI,",
            "year": 2020
        },
        {
            "authors": [
                "Arpit Bansal",
                "Eitan Borgnia",
                "Hong-Min Chu",
                "Jie S Li",
                "Hamid Kazemi",
                "Furong Huang",
                "Micah Goldblum",
                "Jonas Geiping",
                "Tom Goldstein"
            ],
            "title": "Cold diffusion: Inverting arbitrary image transforms without noise",
            "venue": "arXiv preprint arXiv:2208.09392,",
            "year": 2022
        },
        {
            "authors": [
                "Fan Bao",
                "Chongxuan Li",
                "Jiacheng Sun",
                "Jun Zhu",
                "Bo Zhang"
            ],
            "title": "Estimating the optimal covariance with imperfect mean in diffusion probabilistic models",
            "venue": "In Proc. ICML,",
            "year": 2022
        },
        {
            "authors": [
                "Fan Bao",
                "Chongxuan Li",
                "Jun Zhu",
                "Bo Zhang"
            ],
            "title": "Analytic-dpm: an analytic estimate of the optimal reverse variance in diffusion probabilistic models",
            "venue": "In Proc. ICLR,",
            "year": 2022
        },
        {
            "authors": [
                "Christopher M Bishop",
                "Nasser M Nasrabadi"
            ],
            "title": "Pattern recognition and machine learning, volume",
            "year": 2006
        },
        {
            "authors": [
                "Vladimir Bychkovsky",
                "Sylvain Paris",
                "Eric Chan",
                "Fr\u00e9do Durand"
            ],
            "title": "Learning photographic global tonal adjustment with a database of input / output image pairs",
            "venue": "In Proc. CVPR,",
            "year": 2011
        },
        {
            "authors": [
                "Chen Chen",
                "Qifeng Chen",
                "Jia Xu",
                "Vladlen Koltun"
            ],
            "title": "Learning to see in the dark",
            "venue": "In Proc. CVPR,",
            "year": 2018
        },
        {
            "authors": [
                "Zipei Chen",
                "Chengjiang Long",
                "Ling Zhang",
                "Chunxia Xiao"
            ],
            "title": "CANet: A Context-Aware Network for Shadow Removal",
            "venue": "In Proc. ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Yunjey Choi",
                "Youngjung Uh",
                "Jaejun Yoo",
                "Jung-Woo Ha"
            ],
            "title": "Stargan v2: Diverse image synthesis for multiple domains",
            "venue": "In Proc. CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Xiaodong Cun",
                "Chi Man Pun",
                "Cheng Shi"
            ],
            "title": "Towards ghost-free shadow removal via dual hierarchical aggregation network and shadow matting GAN",
            "venue": "In Proc. AAAI,",
            "year": 2020
        },
        {
            "authors": [
                "Mauricio Delbracio",
                "Peyman Milanfar"
            ],
            "title": "Inversion by direct iteration: An alternative to denoising diffusion for image restoration",
            "venue": "Transactions on Machine Learning Research,",
            "year": 2023
        },
        {
            "authors": [
                "Huiyu Duan",
                "Wei Shen",
                "Xiongkuo Min",
                "Yuan Tian",
                "Jae-Hyun Jung",
                "Xiaokang Yang",
                "Guangtao Zhai"
            ],
            "title": "Develop then rival: A human vision-inspired framework for superimposed image decomposition",
            "venue": "IEEE TMM,",
            "year": 2022
        },
        {
            "authors": [
                "Lan Fu",
                "Changqing Zhou",
                "Qing Guo",
                "Felix Juefei-Xu",
                "Hongkai Yu",
                "Wei Feng",
                "Yang Liu",
                "Song Wang"
            ],
            "title": "Auto-Exposure Fusion for Single-Image Shadow Removal",
            "venue": "In Proc. CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Lanqing Guo",
                "Chong Wang",
                "Wenhan Yang",
                "Siyu Huang",
                "Yufei Wang",
                "Hanspeter Pfister",
                "Bihan Wen"
            ],
            "title": "Shadowdiffusion: When degradation prior meets diffusion model for shadow removal",
            "venue": "In Proc. CVPR,",
            "year": 2023
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proc. CVPR,",
            "year": 2016
        },
        {
            "authors": [
                "Martin Heusel",
                "Hubert Ramsauer",
                "Thomas Unterthiner",
                "Bernhard Nessler",
                "Sepp Hochreiter"
            ],
            "title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium",
            "venue": "In Proc. NeurIPS,",
            "year": 2017
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "In Proc. NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Weipeng Hu",
                "Haifeng Hu"
            ],
            "title": "Domain discrepancy elimination and mean face representation learning for nir-vis face recognition",
            "venue": "IEEE Signal Processing Letters,",
            "year": 2021
        },
        {
            "authors": [
                "Xiaowei Hu",
                "Chi Wing Fu",
                "Lei Zhu",
                "Jing Qin",
                "Pheng Ann Heng"
            ],
            "title": "Direction-aware spatial context features for shadow detection and removal",
            "venue": "IEEE TPAMI,",
            "year": 2020
        },
        {
            "authors": [
                "Yeying Jin",
                "Aashish Sharma",
                "Robby T. Tan"
            ],
            "title": "DC-ShadowNet: Single-Image Hard and Soft Shadow Removal Using Unsupervised Domain-Classifier Guided Network",
            "venue": "In Proc. ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Yeying Jin",
                "Wenhan Yang",
                "Wei Ye",
                "Yuan Yuan",
                "Robby T Tan"
            ],
            "title": "Des3: Attention-driven self and soft shadow removal using vit similarity and color convergence",
            "venue": "arXiv preprint arXiv:2211.08089,",
            "year": 2022
        },
        {
            "authors": [
                "Yeying Jin",
                "Wenhan Yang",
                "Wei Ye",
                "Yuan Yuan",
                "Robby T Tan"
            ],
            "title": "Shadowdiffusion: Diffusion-based shadow removal using classifier-driven attention and structure preservation",
            "venue": "arXiv preprint arXiv:2211.08089,",
            "year": 2022
        },
        {
            "authors": [
                "Tero Karras",
                "Timo Aila",
                "Samuli Laine",
                "Jaakko Lehtinen"
            ],
            "title": "Progressive growing of gans for improved quality, stability, and variation",
            "venue": "In Proc. ICLR,",
            "year": 2018
        },
        {
            "authors": [
                "Diederik Kingma",
                "Tim Salimans",
                "Ben Poole",
                "Jonathan Ho"
            ],
            "title": "Variational diffusion models",
            "venue": "In Proc. NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Max Welling"
            ],
            "title": "Auto-encoding variational bayes",
            "venue": "In Proc. ICLR,",
            "year": 2014
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Max Welling"
            ],
            "title": "An introduction to variational autoencoders",
            "venue": "Found. Trends Mach. Learn.,",
            "year": 2019
        },
        {
            "authors": [
                "Orest Kupyn",
                "Tetiana Martyniuk",
                "Junru Wu",
                "Zhangyang Wang"
            ],
            "title": "Deblurgan-v2: Deblurring (orders-ofmagnitude) faster and better",
            "venue": "In Proc. ICCV,",
            "year": 2019
        },
        {
            "authors": [
                "Hieu Le",
                "Dimitris Samaras"
            ],
            "title": "Shadow removal via shadow image decomposition",
            "venue": "In Proc. ICCV,",
            "year": 2019
        },
        {
            "authors": [
                "Guan-Horng Liu",
                "Arash Vahdat",
                "De-An Huang",
                "Evangelos A Theodorou",
                "Weili Nie",
                "Anima Anandkumar"
            ],
            "title": "I2sb: Image-to-image schr\u00f6dinger bridge",
            "venue": "In Proc. ICML,",
            "year": 2023
        },
        {
            "authors": [
                "Jiawei Liu",
                "Qiang Wang",
                "Huijie Fan",
                "Wentao Li",
                "Liangqiong Qu",
                "Yandong Tang"
            ],
            "title": "A decoupled multi-task network for shadow removal",
            "venue": "IEEE TMM,",
            "year": 2023
        },
        {
            "authors": [
                "Jiawei Liu",
                "Qiang Wang",
                "Huijie Fan",
                "Jiandong Tian",
                "Yandong Tang"
            ],
            "title": "A shadow imaging bilinear model and three-branch residual network for shadow removal",
            "venue": "IEEE TNNLS, pp",
            "year": 2023
        },
        {
            "authors": [
                "Risheng Liu",
                "Long Ma",
                "Jiaao Zhang",
                "Xin Fan",
                "Zhongxuan Luo"
            ],
            "title": "Retinex-inspired unrolling with cooperative prior architecture search for low-light image enhancement",
            "venue": "In Proc. CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Xing Liu",
                "Masanori Suganuma",
                "Zhun Sun",
                "Takayuki Okatani"
            ],
            "title": "Dual residual networks leveraging the potential of paired operations for image restoration",
            "venue": "In Proc. CVPR,",
            "year": 2019
        },
        {
            "authors": [
                "Xingchao Liu",
                "Chengyue Gong",
                "Qiang Liu"
            ],
            "title": "Flow straight and fast: Learning to generate and transfer data with rectified flow",
            "venue": "In Proc. ICLR,",
            "year": 2023
        },
        {
            "authors": [
                "Zhihao Liu",
                "Hui Yin",
                "Yang Mi",
                "Mengyang Pu",
                "Song Wang"
            ],
            "title": "Shadow removal by a lightness-guided network with training on unpaired data",
            "venue": "IEEE TIP,",
            "year": 2021
        },
        {
            "authors": [
                "Ziwei Liu",
                "Ping Luo",
                "Xiaogang Wang",
                "Xiaoou Tang"
            ],
            "title": "Deep learning face attributes in the wild",
            "venue": "In Proc. ICCV,",
            "year": 2015
        },
        {
            "authors": [
                "Gunter Loffler",
                "Grigori Yourganov",
                "Frances Wilkinson",
                "Hugh R Wilson"
            ],
            "title": "fmri evidence for the neural representation of faces",
            "venue": "Nature neuroscience,",
            "year": 2005
        },
        {
            "authors": [
                "Andreas Lugmayr",
                "Martin Danelljan",
                "Andres Romero",
                "Fisher Yu",
                "Radu Timofte",
                "Luc Van Gool"
            ],
            "title": "Repaint: Inpainting using denoising diffusion probabilistic models",
            "venue": "In Proc. CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Ziwei Luo",
                "Fredrik K Gustafsson",
                "Zheng Zhao",
                "Jens Sj\u00f6lund",
                "Thomas B Sch\u00f6n"
            ],
            "title": "Image restoration with mean-reverting stochastic differential equations",
            "venue": "In Proc. ICML,",
            "year": 2023
        },
        {
            "authors": [
                "Chenlin Meng",
                "Yutong He",
                "Yang Song",
                "Jiaming Song",
                "Jiajun Wu",
                "Jun-Yan Zhu",
                "Stefano Ermon"
            ],
            "title": "Sdedit: Guided image synthesis and editing with stochastic differential equations",
            "venue": "In Proc. ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "Qiang Meng",
                "Shichao Zhao",
                "Zhida Huang",
                "Feng Zhou"
            ],
            "title": "Magface: A universal representation for face recognition and quality assessment",
            "venue": "In Proc. CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Seungjun Nah",
                "Tae Hyun Kim",
                "Kyoung Mu Lee"
            ],
            "title": "Deep multi-scale convolutional neural network for dynamic scene deblurring",
            "venue": "In Proc. CVPR,",
            "year": 2017
        },
        {
            "authors": [
                "Alexander Quinn Nichol",
                "Prafulla Dhariwal"
            ],
            "title": "Improved denoising diffusion probabilistic models",
            "venue": "In Proc. ICML,",
            "year": 2021
        },
        {
            "authors": [
                "Rui Qian",
                "Robby T Tan",
                "Wenhan Yang",
                "Jiajun Su",
                "Jiaying Liu"
            ],
            "title": "Attentive generative adversarial network for raindrop removal from a single image",
            "venue": "In Proc. CVPR,",
            "year": 2018
        },
        {
            "authors": [
                "Yuhui Quan",
                "Shijie Deng",
                "Yixin Chen",
                "Hui Ji"
            ],
            "title": "Deep learning for seeing through window with raindrops",
            "venue": "In Proc. ICCV,",
            "year": 2019
        },
        {
            "authors": [
                "K.F. Riley",
                "M.P. Hobson",
                "S.J. Bence"
            ],
            "title": "Mathematical Methods for Physics and Engineering: A Comprehensive Guide",
            "venue": "doi: 10.1017/CBO9780511810763",
            "year": 2006
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "High-resolution image synthesis with latent diffusion models",
            "venue": "In Proc. CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Chitwan Saharia",
                "Jonathan Ho",
                "William Chan",
                "Tim Salimans",
                "David J Fleet",
                "Mohammad Norouzi"
            ],
            "title": "Image super-resolution via iterative refinement",
            "venue": "IEEE TPAMI,",
            "year": 2022
        },
        {
            "authors": [
                "Jascha Sohl-Dickstein",
                "Eric Weiss",
                "Niru Maheswaranathan",
                "Surya Ganguli"
            ],
            "title": "Deep unsupervised learning using nonequilibrium thermodynamics",
            "venue": "In Proc. ICML, pp",
            "year": 2015
        },
        {
            "authors": [
                "Jiaming Song",
                "Chenlin Meng",
                "Stefano Ermon"
            ],
            "title": "Denoising diffusion implicit models",
            "venue": "In Proc. ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "Yang Song",
                "Stefano Ermon"
            ],
            "title": "Generative modeling by estimating gradients of the data distribution",
            "year": 2019
        },
        {
            "authors": [
                "Yang Song",
                "Jascha Sohl-Dickstein",
                "Diederik P Kingma",
                "Abhishek Kumar",
                "Stefano Ermon",
                "Ben Poole"
            ],
            "title": "Score-based generative modeling through stochastic differential equations",
            "venue": "In Proc. ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "Yang Song",
                "Prafulla Dhariwal",
                "Mark Chen",
                "Ilya Sutskever"
            ],
            "title": "Consistency models",
            "venue": "In Proc. ICML,",
            "year": 2023
        },
        {
            "authors": [
                "Maitreya Suin",
                "Kuldeep Purohit",
                "AN Rajagopalan"
            ],
            "title": "Spatially-attentive patch-hierarchical network for adaptive motion deblurring",
            "venue": "In Proc. CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Zhengzhong Tu",
                "Hossein Talebi",
                "Han Zhang",
                "Feng Yang",
                "Peyman Milanfar",
                "Alan Bovik",
                "Yinxiao Li"
            ],
            "title": "Maxim: Multi-axis mlp for image processing",
            "venue": "In Proc. CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Jifeng Wang",
                "Xiang Li",
                "Jian Yang"
            ],
            "title": "Stacked Conditional Generative Adversarial Networks for Jointly Learning Shadow Detection and Shadow Removal",
            "venue": "In Proc. CVPR,",
            "year": 2018
        },
        {
            "authors": [
                "Tao Wang",
                "Kaihao Zhang",
                "Tianrun Shen",
                "Wenhan Luo",
                "Bjorn Stenger",
                "Tong Lu"
            ],
            "title": "Ultra-high-definition low-light image enhancement: A benchmark and transformer-based method",
            "venue": "In Proc. AAAI,",
            "year": 2023
        },
        {
            "authors": [
                "Yufei Wang",
                "Renjie Wan",
                "Wenhan Yang",
                "Haoliang Li",
                "Lap-Pui Chau",
                "Alex Kot"
            ],
            "title": "Low-light image enhancement with normalizing flow",
            "venue": "In Proc. AAAI,",
            "year": 2022
        },
        {
            "authors": [
                "Zhendong Wang",
                "Xiaodong Cun",
                "Jianmin Bao",
                "Wengang Zhou",
                "Jianzhuang Liu",
                "Houqiang Li"
            ],
            "title": "Uformer: A general u-shaped transformer for image restoration",
            "venue": "In Proc. CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Chen Wei",
                "Wenjing Wang",
                "Wenhan Yang",
                "Jiaying Liu"
            ],
            "title": "Deep retinex decomposition for low-light enhancement",
            "venue": "In Proc. BMVC,",
            "year": 2018
        },
        {
            "authors": [
                "Jay Whang",
                "Mauricio Delbracio",
                "Hossein Talebi",
                "Chitwan Saharia",
                "Alexandros G Dimakis",
                "Peyman Milanfar"
            ],
            "title": "Deblurring via stochastic refinement",
            "venue": "In Proc. CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Hugh R Wilson",
                "Gunter Loffler",
                "Frances Wilkinson"
            ],
            "title": "Synthetic faces, face cubes, and the geometry of face space",
            "venue": "Vision research,",
            "year": 2002
        },
        {
            "authors": [
                "Jie Xiao",
                "Xueyang Fu",
                "Aiping Liu",
                "Feng Wu",
                "Zheng-Jun Zha"
            ],
            "title": "Image de-raining transformer",
            "venue": "IEEE TPAMI,",
            "year": 2022
        },
        {
            "authors": [
                "Ke Xu",
                "Xin Yang",
                "Baocai Yin",
                "Rynson WH Lau"
            ],
            "title": "Learning to restore low-light images via decompositionand-enhancement",
            "venue": "In Proc. CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Xiaogang Xu",
                "Ruixing Wang",
                "Chi-Wing Fu",
                "Jiaya Jia"
            ],
            "title": "Snr-aware low-light image enhancement",
            "venue": "In Proc. CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Wu Yuhui",
                "Pan Chen",
                "Wang Guoqing",
                "Yang Yang",
                "Wei Jiwei",
                "Li Chongyi",
                "Heng Tao Shen"
            ],
            "title": "Learning semantic-aware knowledge guidance for low-light image enhancement",
            "venue": "In Proc. CVPR,",
            "year": 2023
        },
        {
            "authors": [
                "Syed Waqas Zamir",
                "Aditya Arora",
                "Salman Khan",
                "Munawar Hayat",
                "Fahad Shahbaz Khan",
                "Ming-Hsuan Yang",
                "Ling Shao"
            ],
            "title": "Learning enriched features for real image restoration and enhancement",
            "venue": "In Proc. ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Syed Waqas Zamir",
                "Aditya Arora",
                "Salman Khan",
                "Munawar Hayat",
                "Fahad Shahbaz Khan",
                "Ming-Hsuan Yang",
                "Ling Shao"
            ],
            "title": "Multi-stage progressive image restoration",
            "venue": "In Proc. CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Syed Waqas Zamir",
                "Aditya Arora",
                "Salman Khan",
                "Munawar Hayat",
                "Fahad Shahbaz Khan",
                "Ming-Hsuan Yang",
                "Ling Shao"
            ],
            "title": "Learning enriched features for fast image restoration and enhancement",
            "venue": "IEEE TPAMI,",
            "year": 1934
        },
        {
            "authors": [
                "Kai Zhang",
                "Wangmeng Zuo",
                "Yunjin Chen",
                "Deyu Meng",
                "Lei Zhang"
            ],
            "title": "Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising",
            "venue": "IEEE TIP,",
            "year": 2017
        },
        {
            "authors": [
                "Richard Zhang",
                "Phillip Isola",
                "Alexei A Efros",
                "Eli Shechtman",
                "Oliver Wang"
            ],
            "title": "The unreasonable effectiveness of deep features as a perceptual metric",
            "venue": "In Proc. CVPR,",
            "year": 2018
        },
        {
            "authors": [
                "Yonghua Zhang",
                "Jiawan Zhang",
                "Xiaojie Guo"
            ],
            "title": "Kindling the darkness: A practical low-light image enhancer",
            "venue": "In Proc. ACMMM,",
            "year": 2019
        },
        {
            "authors": [
                "Yonghua Zhang",
                "Xiaojie Guo",
                "Jiayi Ma",
                "Wei Liu",
                "Jiawan Zhang"
            ],
            "title": "Beyond brightening low-light",
            "venue": "images. IJCV,",
            "year": 2021
        },
        {
            "authors": [
                "Yulun Zhang",
                "Yapeng Tian",
                "Yu Kong",
                "Bineng Zhong",
                "Yun Fu"
            ],
            "title": "Residual dense network for image restoration",
            "venue": "IEEE TPAMI,",
            "year": 2020
        },
        {
            "authors": [
                "Zhao Zhang",
                "Huan Zheng",
                "Richang Hong",
                "Mingliang Xu",
                "Shuicheng Yan",
                "Meng Wang"
            ],
            "title": "Deep color consistent network for low-light image enhancement",
            "venue": "In Proc. CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Chuanjun Zheng",
                "Daming Shi",
                "Wentian Shi"
            ],
            "title": "Adaptive unfolding total variation network for low-light image enhancement",
            "venue": "In Proc. ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Yurui Zhu",
                "Jie Huang",
                "Xueyang Fu",
                "Feng Zhao",
                "Qibin Sun",
                "Zheng-Jun Zha"
            ],
            "title": "Bijective mapping network for shadow removal",
            "venue": "In Proc. CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Yurui Zhu",
                "Zeyu Xiao",
                "Yanchi Fang",
                "Xueyang Fu",
                "Zhiwei Xiong",
                "Zheng-Jun Zha"
            ],
            "title": "Efficient model-driven network for shadow removal",
            "venue": "In Proc. AAAI,",
            "year": 2022
        },
        {
            "authors": [
                "Zhengxia Zou",
                "Sen Lei",
                "Tianyang Shi",
                "Zhenwei Shi",
                "Jieping Ye"
            ],
            "title": "Deep adversarial decomposition: A unified framework for separating superimposed images",
            "venue": "In Proc. CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Ozan \u00d6zdenizci",
                "Robert Legenstein"
            ],
            "title": "Restoring vision in adverse weather conditions with patch-based denoising diffusion models",
            "venue": "IEEE TPAMI,",
            "year": 2023
        },
        {
            "authors": [
                "res(It",
                "\u1fb1t \u00b7 T",
                "\u03b5\u03b8(It",
                "\u03b2\u0304t \u00b7 T ) (\u03b7"
            ],
            "title": "Image deblurring. We evaluate our method on the widely used deblurring dataset - GoPro (Nah et al., 2017) (3,214 images). Table 7(b) and Fig. 12 show that our method is competitive with the SOTA deblurring methods (e.g, MPRNet (Zamir et al., 2021), and Uformer-B",
            "venue": "(Wang et al.,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "In real-life scenarios, diffusion often occurs in complex forms involving multiple, concurrent processes, such as the dispersion of multiple gases or the propagation of different types of waves or fields. This leads us to ponder whether the denoising-based diffusion models (Ho et al., 2020; Song et al., 2021a) have limitations in focusing solely on denoising. Current diffusion-based image restoration methods (Lugmayr et al., 2022; Saharia et al., 2022; Rombach et al., 2022; Jin et al., 2022b; O\u0308zdenizci & Legenstein, 2023) extend the diffusion model to image restoration tasks by using degraded images as a condition input to implicitly guide the reverse generation process, without modifying the original denoising diffusion process (Ho et al., 2020; Song et al., 2021a). However, the reverse process starting from noise seems to be unnecessary, as the degraded image is already known. The forward process is non-interpretability for image restoration, as the diffusion process does not contain any information about the degraded image.\nIn this paper, we explore a novel dual diffusion process and propose Residual Denoising Diffusion Models (RDDM), which can tackle the non-interpretability of a single denoising process for image restoration. In RDDM, we decouple the previous diffusion process into residual diffusion and noise diffusion. Residual diffusion prioritizes certainty and represents a directional diffusion from the target image to the conditional input image, and noise diffusion emphasizes diversity and represents random perturbations in the diffusion process. Thus, our RDDM can unify different tasks that require different certainty or diversity, e.g., image generation and restoration. Compared to denoising-based diffusion models for image restoration, the residuals in RDDM clearly indicate the forward diffusion direction and explicitly guide the reverse generation process for image restoration.\nSpecifically, we redefine a new forward process that allows simultaneous diffusion of residuals and noise, wherein the target image progressively diffuses into a purely noisy image for image genera-\n*Corresponding author.\nar X\niv :2\n30 8.\n13 71\n2v 2\n[ cs\n.C V\n] 7\nO ct\n2 02\n3\ntion or a noise-carrying input image for image restoration, as shown in Fig. 1. Unlike the previous denoising diffusion model (Ho et al., 2020; Song et al., 2021a), which uses one coefficient schedule to control the mixing ratio of noise and images, our RDDM employs two independent coefficient schedules to control the diffusion speed of residuals and noise. We found that this independent diffusion property is also evident in the reverse generation process, e.g., readjusting the coefficient schedule within a certain range during testing does not affect the image generation results, and removing the residuals firstly, followed by denoising, can also produce semantically consistent images. Our RDDM is compatible with widely used denoising diffusion models, i.e., our sampling process is consistent with that of DDPM (Ho et al., 2020) and DDIM (Song et al., 2021a) by transforming coefficient schedules. In addition, our RDDM natively supports conditional inputs, enabling networks trained with only an \u21131 loss and a batch size of 1 to compete with state-of-the-art image restoration methods. We envision that our models can facilitate a unified and interpretable image-to-image distribution transformation methodology, highlighting that residuals and noise are equally important for diffusion models, e.g., the residual prioritizes certainty while the noise emphasizes diversity."
        },
        {
            "heading": "2 BACKGROUND",
            "text": "Denoising diffusion models (Ho et al., 2020; Sohl-Dickstein et al., 2015) aim to learn a distribution p\u03b8(I0) := \u222b p\u03b8(I0:T )dI1:T\n1 to approximate a target data distribution q(I0), where I0 are target images and I1, . . . , IT (T = 1000) are latents of the same dimension as I0. In the forward process, q(I0) is diffused into a Gaussian noise distribution using a fixed Markov chain,\nq(I1:T |I0) := \u220fT t=1q(It|It\u22121), q(It|It\u22121) := N (It; \u221a \u03b1tIt\u22121, (1\u2212 \u03b1t)I), (1)\nwhere \u03b11:T \u2208 (0, 1]T . q(It|It\u22121) can also be written as It = \u221a \u03b1tIt\u22121 + \u221a 1\u2212 \u03b1t\u03f5t\u22121. In fact, it is simpler to sampling It from I0 by reparameterization (Kingma & Welling, 2014; 2019),\nIt = \u221a \u03b1\u0304tI0 + \u221a 1\u2212 \u03b1\u0304t\u03f5, where \u03f5 \u223c N (0, I), \u03b1\u0304t := \u220ft s=1\u03b1s. (2)\nThe reverse process is also a Markov chain starting at p\u03b8(IT ) \u223c N (IT ;0, I), p\u03b8(I0:T ) := p\u03b8(IT ) \u220fT t=1p\u03b8(It\u22121|It), p\u03b8(It\u22121|It) := N (It\u22121;\u00b5\u03b8(It, t),\u03a3tI), (3) where p\u03b8(It\u22121|It) is a learnable transfer probability (the variance schedule \u03a3t is fixed). Ho et al. (2020) derive a simplified loss function from the maximum likelihood of p\u03b8(I0), i.e., L(\u03b8) := EI0\u223cq(I0),\u03f5\u223cN (0,I) [ \u2225\u03f5\u2212 \u03f5\u03b8(It, t)\u22252 ] . The estimated noise \u03f5\u03b8 can be used to represent \u00b5\u03b8 in p\u03b8(It\u22121|It), thus It\u22121 can be sampled from p\u03b8(It\u22121|It) step by step."
        },
        {
            "heading": "3 RESIDUAL DENOISING DIFFUSION MODELS",
            "text": "Our goal is to develop a dual diffusion process to unify and interpret image generation and restoration. We modify the representation of IT = \u03f5 in traditional DDPM to IT = Iin + \u03f5 in our RDDM,\n1To understand diffusion from an image perspective, we use I instead of x in DDPM (Ho et al., 2020).\nwhere Iin is a degraded image (e.g., a shadow, low-light, or blurred image) for image restoration and is set to 0 for image generation. This modification is compatible with the widely used denoising diffusion model, e.g., IT = 0 + \u03f5 is the pure noise (\u03f5) for generation. For image restoration, IT is a noisy-carrying degraded image (Iin + \u03f5), as shown in the third column in Fig. 1. The modified forward process from I0 to IT = Iin+ \u03f5 involves progressively degrading I0 to Iin, and injecting noise \u03f5. This naturally results in a dual diffusion process, a residual diffusion to model the transition from I0 to Iin and a noise diffusion. For example, the forward diffusion process from the shadow-free image I0 to the noisy carrying shadow image IT involves progressively adding shadows and noise, as shown in the second row in Fig. 1.\nIn the following subsections, we detail the underlying theory and the methodology behind our RDDM. Inspired by residual learning (He et al., 2016), we redefine each forward diffusion process step in Section 3.1. For the reverse process, we present a training objective to predict the residuals and noise injected in the forward process in Section 3.2. In Section 3.3, we propose three sampling methods, i.e., residual prediction (SM-Res), noise prediction (SM-N), and \u201cresidual and noise prediction\u201d (SM-Res-N)."
        },
        {
            "heading": "3.1 DIRECTIONAL RESIDUAL DIFFUSION PROCESS WITH PERTURBATION",
            "text": "To model the gradual degradation of image quality and the increment of noise, we define the single forward process step in our RDDM as follows:\nIt = It\u22121 + I t res, I t res \u223c N (\u03b1tIres, \u03b22t I), (4)\nwhere Itres represents a directional mean shift (residual diffusion) with random perturbation (noise diffusion) from state It\u22121 to state It, the residuals Ires in Itres is the difference between Iin and I0 (i.e., Ires = Iin \u2212 I0), and two independent coefficient schedules \u03b1t and \u03b2t control the residual and noise diffusion, respectively. In fact, it is simpler to sample It from I0 (like Eq. 2),\nIt =It\u22121 + \u03b1tIres + \u03b2t\u03f5t\u22121,where \u03f5t\u22121, \u03f5t\u22122 . . . \u03f5 \u223c N (0, I) =It\u22122 + (\u03b1t\u22121 + \u03b1t)Ires + ( \u221a \u03b22t\u22121 + \u03b2 2 t )\u03f5t\u22122\n= . . .\n=I0 + \u03b1\u0304tIres + \u03b2\u0304t\u03f5,\n(5)\nwhere \u03b1\u0304t = \u2211t i=1\u03b1i and \u03b2\u0304t = \u221a\u2211t i=1\u03b2 2 i . If t = T , \u03b1\u0304T = 1 and IT = Iin + \u03b2\u0304T \u03f5. \u03b2\u0304T can control the intensity of noise perturbation for image restoration (e.g., \u03b2\u03042T = 0.01 for shadow removal), while \u03b2\u03042T = 1 for image generation. The newly defined diffusion process via Eq. 5 has the sum-constrained variance, while DDPM has preserving variance (see Appendix A.4 and Fig. 7). From Eq. 4, the joint probability distributions in the forward process can be defined as:\nq(I1:T |I0, Ires) := \u220fT t=1q(It|It\u22121, Ires), q(It|It\u22121, Ires) := N (It; It\u22121 + \u03b1tIres, \u03b2 2 t I). (6)\nEq. 5 defines the marginal probability distribution q(It|I0, Ires) = N (It; I0 + \u03b1\u0304tIres, \u03b2\u03042t I). In fact, the forward diffusion of our RDDM is a mixture of three terms (i.e., I0, Ires, and \u03f5), extending beyond the widely used denoising diffusion model that is a mixture of two terms, i.e, I0 and \u03f5. A similar mixture form of three terms can be seen in several concurrent works, e.g., InDI (Delbracio & Milanfar, 2023), I2SB (Liu et al., 2023a), and IR-SDE (Luo et al., 2023)."
        },
        {
            "heading": "3.2 GENERATION PROCESS AND TRAINING OBJECTIVE",
            "text": "In the forward process (Eq. 5), residuals (Ires) and noise (\u03f5) are gradually added to I0, and then synthesized into It, while the reverse process from IT to I0 involves the estimation of the residuals and noise injected in the forward process. We can train a residual network I\u03b8res(It, t, Iin) to predict Ires and a noise network \u03f5\u03b8(It, t, Iin) to estimate \u03f5. Using Eq. 5, we obtain the estimated target images I\u03b80 = It \u2212 \u03b1\u0304tI\u03b8res \u2212 \u03b2\u0304t\u03f5\u03b8. If I\u03b80 and I\u03b8res are given, the generation process is defined as,\np\u03b8(It\u22121|It) := q\u03c3(It\u22121|It, I\u03b80 , I\u03b8res), (7)\nwhere the transfer probability q\u03c3(It\u22121|It, I0, Ires)2 from It to It\u22121 is,\nq\u03c3(It\u22121|It, I0, Ires) = N (It\u22121; I0 + \u03b1\u0304t\u22121Ires + \u221a \u03b2\u03042t\u22121 \u2212 \u03c32t It \u2212 (I0 + \u03b1\u0304tIres)\n\u03b2\u0304t , \u03c32t I), (8)\nwhere \u03c32t = \u03b7\u03b2 2 t \u03b2\u0304 2 t\u22121/\u03b2\u0304 2 t and \u03b7 controls whether the generation process is random (\u03b7 = 1) or deterministic (\u03b7 = 0). Using Eq. 7 and Eq. 8, It\u22121 can be sampled from It via:\nIt\u22121 = It \u2212 (\u03b1\u0304t \u2212 \u03b1\u0304t\u22121)I\u03b8res \u2212 (\u03b2\u0304t \u2212 \u221a \u03b2\u03042t\u22121 \u2212 \u03c32t )\u03f5\u03b8 + \u03c3t\u03f5t,where \u03f5t \u223c N (0, I). (9)\nWhen \u03b7 = 0 (i.e., \u03c3t = 0), the sampling process is deterministic,\nIt\u22121 = It \u2212 (\u03b1\u0304t \u2212 \u03b1\u0304t\u22121)I\u03b8res \u2212 (\u03b2\u0304t \u2212 \u03b2\u0304t\u22121)\u03f5\u03b8. (10)\nWe derive the following simplified loss function for training (Appendix A.1): Lres(\u03b8) := E [ \u03bbres \u2225\u2225Ires \u2212 I\u03b8res(It, t, Iin)\u2225\u22252] , L\u03f5(\u03b8) := E [\u03bb\u03f5 \u2225\u03f5\u2212 \u03f5\u03b8(It, t, Iin)\u22252] , (11) where the hyperparameters \u03bbres, \u03bb\u03f5 \u2208 {0, 1}, and the training input image It is synthesized using I0, Ires, and \u03f5 by Eq. 5. It can also be synthesized using Iin (replace I0 in Eq. 5 by I0 = Iin\u2212Ires),\nIt = Iin + (\u03b1\u0304t \u2212 1)Ires + \u03b2\u0304t\u03f5. (12)"
        },
        {
            "heading": "3.3 SAMPLING METHOD SELECTION STRATEGIES",
            "text": "For the generation process (from It to It\u22121), It and Iin are known, and thus Ires and \u03f5 can represent each other by Eq. 12. From Eq. 11 and Eq. 12, we propose three sampling methods as follows. SM-Res. When \u03bbres = 1 and \u03bb\u03f5 = 0, the residuals I\u03b8res are predicted by a network, while the noise \u03f5\u03b8 is represented as a transformation of I\u03b8res using Eq. 12. SM-N. When \u03bbres = 0 and \u03bb\u03f5 = 1, the noise \u03f5\u03b8 is predicted by a network, while the residuals I\u03b8res are represented as a transformation of \u03f5\u03b8 using Eq. 12. SM-Res-N. When \u03bbres = 1 and \u03bb\u03f5 = 1, both the residuals and the noise are predicted. To determine the optimal sampling method for real-world applications, we give empirical strategies and automatic selection algorithms in the following.\nEmpirical Research. Table 1 presents that the SM-Res shows better results for image restoration but offers a poorer FID for generation. On the other hand, the SM-N yields better frechet inception distance (FID in Heusel et al. (2017)) and inception scores (IS), but is ineffective in image restoration (e.g., PSNR 11.34 for shadow and 16.30 for low-light). This may be due to the inadequacy of using \u03f5\u03b8 to represent I\u03b8res in Eq. 12 for restoration tasks. We attribute these inconsistent results to the fact that residual predictions prioritize certainty, whereas noise predictions emphasize diversity. In our experiments, we use SM-Res for low-ligh enhancement, SM-N for image generation, and SM-Res-N for other image restoration tasks. For an unknown new task, we empirically recommend using SM-Res for tasks that demand higher certainty and SM-N for those requiring greater diversity.\nAutomatic Target Domain Prediction Strategies (ATDP). To automatically choose between SMRes or SM-N for an unknown task, we develop an automatic sampling selection algorithm in Appendix B.2. This algorithm requires only a single network and learns the hyperparameter in Eq. 11, enabling a gradual transition from combined residual and noise training (akin to SM-Res-N) to individual prediction (SM-Res or SM-N). This plug-and-play training strategy requires less than\n2Eq. 8 does not change q(It|I0, Ires) in Appendix A.2.\n1000 additional training iterations and is fully compatible with the current denoising-based diffusion methods (Ho et al., 2020). Our RDDM using ATDP has the potential to provide a unified and interpretable methodology for modeling, training, and inference pipelines for unknown target tasks.\nComparison with Other Prediction Methods. Our SM-N is similar to DDIM (Song et al., 2021a) (or DDPM (Ho et al., 2020)), which only estimates the noise, and is consistent with DDPM and DDIM by transforming the coefficient schedules in Eq. 9 (the proof in Appendix A.3),\n\u03b1\u0304t = 1\u2212 \u221a \u03b1\u0304tDDIM 3, \u03b2\u0304t = \u221a 1\u2212 \u03b1\u0304tDDIM , \u03c3 2 t = \u03c3 2 t (DDIM). (13)\nIn fact, current research has delved into numerous diffusion forms that extend beyond noise estimation. For example, IDDPM (Nichol & Dhariwal, 2021) proposes that it is feasible to estimate noise (\u03f5\u03b8), clean target images (I\u03b80 ), or the mean term (\u00b5\u03b8) to represent the transfer probabilities (i.e., p\u03b8(It\u22121|It) in Eq. 3). The score-based generative model (SGM) (Song & Ermon, 2019) and Schro\u0308dinger Bridge (I2SB (Liu et al., 2023a)) estimate the score of noisy data (i.e., the sum of residuals and noise \u2211t i=1I t res). ColdDiffusion (Bansal et al., 2022), InDI (Delbracio & Milanfar, 2023), and consistency models (Song et al., 2023) estimate the clean target images (I0). Rectified Flow (Liu et al., 2023d) predicts the residuals (Ires) to align with the image linear interpolation process without noise diffusion (i.e., IT = Iin). A detailed comparison can be found in Appendix A.5.\nThese previous/concurrent works choose to estimate the noise, the residual, the target image, or its linear transformation term. In contrast, we introduce residual estimation while also embracing noise for both generation and restoration. Residuals and noise have equal and independent status, which is reflected in the forward process (Eq. 5), the reverse process (Eq. 10), and the loss function (Eq. 11). This independence means that the noise diffusion can even be removed and only the residual diffusion retained to model the image interpolation process (when \u03b2\u0304T = 0 in Eq. 5, RDDM degenerates to Rectified Flow (Liu et al., 2023d)). In addition, this property derives an independent dual diffusion framework in Section 4."
        },
        {
            "heading": "4 DECOUPLED DUAL DIFFUSION FRAMEWORK",
            "text": "Upon examining DDPM from the perspective of RDDM, we discover that DDPM indeed involves the simultaneous diffusion of residuals and noise, which is evident as Eq. 43 becomes equivalent to Eq. 39 in Appendix A.3. We find that it is possible to decouple these two types of diffusion. Section 4.1 presents a decoupled forward diffusion process. In Section 4.2, we propose a partially path-independent generation process and decouple the simultaneous sampling into first removing the residuals and then removing noise (see Fig. 3(d) and Fig. 16). This decoupled dual diffusion framework sheds light on the roles of deresidual and denoising in the DDPM generation process."
        },
        {
            "heading": "4.1 DECOUPLED FORWARD DIFFUSION PROCESS",
            "text": "Our defined coefficients (\u03b1t, \u03b22t ) offer a distinct physical interpretation. In the forward diffusion process (Eq. 5), \u03b1t controls the speed of residual diffusion and \u03b22t regulates the speed of noise diffusion. In the reverse generation process (Eq. 10), \u03b1\u0304t and \u03b2\u0304t are associated with the speed of removing residual and noise, respectively. In fact, there\n3\u03b1\u0304tDDIM here is \u03b1t of DDIM (Song et al., 2021a). 4Our RDDM is implemented based on the popular diffusion repository github.com/lucidrains/denoisingdiffusion-pytorch. Differences in network structure and training details may lead to poorer FID. We have verified sampling consistency with DDIM (Song et al., 2021a) in Table 3(a) and Appendix A.3.\nare no constraints on \u03b1t and \u03b22t in Eq. 5, meaning that the residual diffusion and noise diffusion are independent of each other. Utilizing this decoupled property and the difference between these two diffusion processes, we should be able to design a better coefficient schedule, e.g., \u03b1t (linearly decreasing) and \u03b22t (linearly increasing) in Table 2. This aligns with the intuition that, during the reverse generation process (from T to 0), the estimated residuals become increasingly accurate while the estimated noise should also weaken progressively. Therefore, when t is close to 0, the deresidual pace should be faster and the denoising pace should be slower. Since our \u03b1t and \u03b22t represent the speed of diffusion, we name the curve in Fig. 6 (b-d) (see Appendix A.3) the diffusion speed curve."
        },
        {
            "heading": "4.2 PARTIALLY PATH-INDEPENDENT GENERATION PROCESS",
            "text": "In the original DDPM (Ho et al., 2020) or DDIM (Song et al., 2021a), when the \u03b1tDDIM schedule changes, it is necessary to retrain the denoising network because this alters the diffusion process (Rombach et al., 2022; Nichol & Dhariwal, 2021). As shown in Fig. 2(c)(d), directly changing the \u03b1tDDIM schedule causes denoising to fail. Here, we propose a path-independent generation process, i.e., modifying the diffusion speed curve does not cause the image generation process to fail. We try to readjust the diffusion speed curve in the generation process. First, we convert the \u03b1tDDIM schedule of a pre-trained DDIM into the \u03b1t and \u03b22t schedules of our RDDM using Eq. 13 (from Fig. 2(a) to Fig. 2(b). We then readjust the converted \u03b1t schedules using the normalized power function (P (x, a) in Fig. 2(f)), without touching the \u03b22t schedule that controls noise diffusion, as shown in Fig. 2(g). P (x, a) is defined as (a is a parameter of the power function),\nP (x, a) := xa/ \u222b 1 0 xadx,where x = t/T. (14)\nThese schedule modifications shown in Fig. 2 lead to the following key findings.\n1. Fig. 2(g) shows that modifying the residual diffusion speed curve (\u03b1t) leads to a drastic change in the generation results, probably due to I\u03b8res being represented as a transformation of \u03f5\u03b8 using Eq. 12.\n2. As the time condition t represents the current noise intensity in the denoising network (\u03f5\u03b8(It, t, 0)), modifying the noise diffusion speed curve (\u03b22t ) causes t to deviate from accurately indicating the current noise intensity, leading to denoising failure, as shown in Fig. 2(e).\nNonetheless, we believe that, corresponding to the decoupled forward diffusion process, there should also be a path-independent reverse generation process. To develop a path-independent generation process, we improve the generation process based on the above two key findings:\n1. Two networks are used to estimate I\u03b8res and \u03f5\u03b8 separately, i.e., SM-Res-N-2Net in Appendix B.2.\n5https://ux.xiaoice.com/beautyv3\n2. \u03b1\u0304t and \u03b2\u0304t are used for the time conditions embedded in the network, i.e., I\u03b8res(It, t, 0) \u2192 I\u03b8res(It, \u03b1\u0304t \u00b7 T, 0), \u03f5\u03b8(It, t, 0)\u2192 \u03f5\u03b8(It, \u03b2\u0304t \u00b7 T, 0). These improvements lead to a partially path-independent generation process, as evidenced by the results shown in Fig. 3(c).\nAnalysis of Partially Path-independence via Green\u2019s Theorem. \u201cPath-independence\u201d reminds us of Green\u2019s theorem in curve integration (Riley et al., 2006). From Eq. 10, we have:\nIt \u2212 It\u22121 = (\u03b1\u0304t \u2212 \u03b1\u0304t\u22121)I\u03b8res + (\u03b2\u0304t \u2212 \u03b2\u0304t\u22121)\u03f5\u03b8, dI(t) = I\u03b8res(I(t), \u03b1\u0304(t) \u00b7 T, 0)d\u03b1\u0304(t) + \u03f5\u03b8(I(t), \u03b2\u0304(t) \u00b7 T, 0)d\u03b2\u0304(t),\n(15)\nwhere I(t) = I(0) + \u03b1\u0304(t)Ires + \u03b2\u0304(t)\u03f5. Given inputs I(t) and \u03b1\u0304(t), the denoising network learns to approximate the noise \u03f5 in I(t) by estimating \u03f5\u03b8. If this network is trained well and robust enough, it should be able to avoid the interference of the residual terms \u03b1\u0304(t)Ires in I(t). This also applies to a robust residual estimation network. Thus, we have\n\u2202I\u03b8res(I(t), \u03b1\u0304(t) \u00b7 T ) \u2202\u03b2\u0304(t) \u2248 0, \u2202\u03f5\u03b8(I(t), \u03b2\u0304(t) \u00b7 T ) \u2202\u03b1\u0304(t) \u2248 0. (16)\nIf the equation in Formula 16 holds true, it serves as a necessary and sufficient condition for path independence in curve integration, which provides an explanation for why Fig. 3(c) achieves a partially path-independent generation process. The path-independent property is related to the network\u2019s resilience to disturbances and applies to disturbances that vary within a certain range. However, excessive disturbances can lead to visual inconsistencies, e.g., readjusting \u03b1t and \u03b22t to P (x, 5). Thus, we refer to this generative property as partially path-independent. We also investigated two reverse paths to gain insight into the implications of the proposed partial path independence. In the first case, the residuals are removed first, followed by the noise: I(T ) \u2212Ires\u2192 I(0) + \u03b2\u0304T \u03f5 \u2212\u03b2\u0304T \u03f5\u2192 I(0). The second case involves removing the noise first and then the residuals: I(T ) \u2212\u03b2\u0304T \u03f5\u2192 Iin\n\u2212Ires\u2192 I(0). The first case (Fig. 3(d2)) shows that removing residuals controls semantic transitions, while the second case (Fig. 3(d3)) shows that diversity is significantly reduced because noise is removed first. Fig. 3(d) validates our argument that residuals control directional semantic drift (certainty) and noise controls random perturbation (diversity). See Appendix B.4 for more details."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "Image Generation. We can convert a pre-trained6 DDIM (Song et al., 2021a) to RDDM by coefficient transformation using Eq. 13, and generate images by Eq. 9. Table 3(a) verifies that the quality of the generated images before and after the conversion is nearly the same7. We show the generated face images with 10 sampling steps in Fig. 4(a).\nImage Restoration. We extensively evaluate our method on several image restoration tasks, including shadow removal, low-light enhancement, deraining, and deblurring on 5 datasets. Notably, our RDDM uses an identical UNet and is trained with a batch size of 1 for all these tasks. In con-\n6https://huggingface.co/google/ddpm-celebahq-256 7The subtle differences in larger sampling steps may stem from errors introduced by numerical representation limitations during coefficient transformation, which may accumulate and amplify in larger sampling steps.\ntrast, SOAT methods often involve elaborate network architectures, such as multi-stage (Fu et al., 2021; Zhu et al., 2022b; Wang et al., 2022a), multi-branch (Cun et al., 2020), Transformer (Wang et al., 2023), and GAN (Kupyn et al., 2019), or sophisticated loss functions like the chromaticity (Jin et al., 2021), texture similarity (Zhang et al., 2019), and edge loss (Zamir et al., 2021). Table 3 and Fig. 4(b-c) show that our RDDM is competitive with the SOTA restoration methods. See Appendix B for more training details and comparison results.\nWe extend DDPM (Ho et al., 2020)/DDIM (Song et al., 2021a), initially uninterpretable for image restoration, into a unified and interpretable diffusion model for both image generation and restoration by introducing residuals. However, the residual diffusion process represents the directional diffusion from target images to conditional input images, which does not involve a priori information about the image restoration task, and therefore is not limited to it. Beyond image generation and restoration, we show examples of image inpainting and image translation to verify that our RDDM has the potential to be a unified and interpretable methodology for image-to-image distribution transformation. We do not intend to achieve optimal performance on all tasks by tuning all hyperparameters. The current experimental results show that RDDM 1) achieves consistent image generation performance with DDIM after coefficient transformation, 2) competes with state-of-theart image restoration methods using a generic UNet with only an \u21131 loss, a batch size of 1, and fewer than 5 sampling steps, and 3) has satisfactory visual results of image inpainting and image translation (see Fig. 4(d-e), Fig. 13, or Fig. 14), which have successfully validated the effectiveness of our RDDM."
        },
        {
            "heading": "6 RELATED WORK",
            "text": "Denoising diffusion models (e.g., DDPM (Ho et al., 2020), SGM (Song & Ermon, 2019; Song et al., 2021b), and DDIM (Song et al., 2021a)) were initially developed for image generation. Subsequent image restoration methods (Lugmayr et al., 2022; Rombach et al., 2022; Guo et al., 2023) based on DDPM and DDIM feed a degraded image as a conditional input to a denoising network, e.g., DvSR (Whang et al., 2022), SR3 (Saharia et al., 2022), and WeatherDiffusion (O\u0308zdenizci & Legenstein, 2023), which typically require large sampling steps and batch sizes. Additionally, the reverse process starting from noise in these methods seems unnecessary and inefficient for image restoration tasks. Thus, SDEdit (Meng et al., 2021a), ColdDiffusion (Bansal et al., 2022), InDI (Delbracio & Milanfar, 2023), and I2SB (Liu et al., 2023a) propose generating a clear image directly from a degraded image or noise-carrying degraded image. InDI (Delbracio & Milanfar, 2023) and I2SB (Liu et al., 2023a), which also present unified image generation and restoration frameworks, are the most closely related to our proposed RDDM. Specifically, the forward diffusion of InDI, I2SB, and our RDDM consistently employs a mixture of three terms (i.e., input images Iin, target images I0, and noise \u03f5), extending beyond the denoising-based diffusion model (Ho et al., 2020; Song et al., 2021a) which incorporates a mixture of two terms (i.e., I0 and \u03f5). However, InDI (Delbracio & Milanfar, 2023) and I2SB (Liu et al., 2023a) opt for estimating the target image or its linear transformation term to replace the noise estimation, akin to a special case of our RDDM (SM-Res). In contrast, we introduce residual estimation while also embracing noise for both generation and restoration tasks. Our RDDM can further extend DDPM (Ho et al., 2020), DDIM (Song et al., 2021a), InDI (Delbracio & Milanfar, 2023), and I2SB (Liu et al., 2023a) to independent double diffusion processes, and pave the way for the multi-dimensional diffusion process. We highlight that residuals and noise are equally important, e.g., the residual prioritizes certainty while the noise emphasizes diversity. In addition, our work is related to coefficient schedule design (Rombach et al., 2022; Nichol & Dhariwal, 2021), variance strategy optimization (Kingma et al., 2021; Nichol & Dhariwal, 2021; Bao et al., 2022b;a), superimposed image decomposition (Zou et al., 2020; Duan et al., 2022), curve integration (Riley et al., 2006), stochastic differential equations (Song et al., 2021b), and residual learning (He et al., 2016) for image restoration (Zhang et al., 2017; 2020; Anwar & Barnes, 2020; Zamir et al., 2021; Tu et al., 2022; Liu et al., 2023c). See Appendix A.5 for detailed comparison."
        },
        {
            "heading": "7 CONCLUSIONS AND DISCUSSIONS",
            "text": "We present a unified dual diffusion model called Residual Denoising Diffusion Models (RDDM) for image restoration and image generation. This is a three-term mixture framework beyond the previous denoising diffusion framework with two-term mixture. We demonstrate that our sampling process is\nconsistent with that of DDPM and DDIM through coefficient schedule transformation, and propose a partially path-independent generation process. Our experimental results on four different image restoration tasks show that RDDM achieves SOTA performance in no more than five sampling steps. We believe that our model and framework hold the potential to provide a unified methodology for image-to-image distribution transformation and pave the way for the multi-dimensional diffusion process. However, there are certain limitations and areas for further investigation that should be addressed: (a) a deeper understanding of the relationship between our RDDM and curve/multivariate integration, (b) a diffusion model trained with one set of pre-trained parameters to handle several different tasks, (c) implementing adaptive learning coefficient schedules to reduce the sampling steps while improving the quality of the generated images, and (d) constructing multi-dimensional latent diffusion models for multimodal fusion and exploring interpretable text-to-image frameworks."
        },
        {
            "heading": "APPENDIX",
            "text": ""
        },
        {
            "heading": "A DERIVATIONS AND PROOFS",
            "text": ""
        },
        {
            "heading": "A.1 PERTURBED GENERATION PROCESS",
            "text": "Fig. 5 shows the difference between the forward diffusion process of DDPM (Ho et al., 2020) and our RDDM. Unlike the noise diffusion of DDPM (Ho et al., 2020), ours is a directional residual diffusion process with perturbation. Next, we derive the reverse sampling formula.\nFor the reverse generation process from It to It\u22121, we can represent the transfer probabilities q(It\u22121|It, I0, Ires) by Bayes\u2019 rule:\nq(It\u22121|It, I0, Ires) = q(It|It\u22121, I0, Ires) q(It\u22121|I0, Ires) q(It|I0, Ires) , (17)\nwhere q(It\u22121|I0, Ires) = N (It\u22121; I0 + \u03b1\u0304t\u22121Ires, \u03b2\u03042t\u22121I) from Eq. 5, and q(It|It\u22121, I0, Ires) = q(It|It\u22121, Ires)8= N (It; It\u22121 + \u03b1tIres, \u03b22t I) from Eq. 6. Thus, we have (considering only the exponential term)\nq(It\u22121|It, I0, Ires) = N (It\u22121;\u00b5t(xt, I0, Ires),\u03a3t(xt, I0, Ires)I) \u221d exp ( \u22121 2 ( (It \u2212 It\u22121 \u2212 \u03b1tIres)2 \u03b22t + (It\u22121 \u2212 I0 \u2212 \u03b1\u0304t\u22121Ires)2 \u03b2\u03042t\u22121 )\u2212 (It \u2212 I0 \u2212 \u03b1\u0304tIres) 2 \u03b2\u03042t ) ) = exp ( \u22121 2 (( \u03b2\u03042t \u03b22t \u03b2\u0304 2 t\u22121 )I2t\u22121 \u2212 2( It \u2212 \u03b1tIres \u03b22t + \u03b1\u0304t\u22121Ires + I0 \u03b2\u03042t\u22121 )It\u22121 + C(It, I0, Ires)) ) ,\n(18)\nwhere the C(It, I0, Ires) term is not related to It\u22121. From Eq. 18, \u00b5t(xt, I0, Ires) and \u03a3t(xt, I0, Ires) are represented as follows,\n\u00b5t(xt, I0, Ires) = ( It \u2212 \u03b1tIres\n\u03b22t + \u03b1\u0304t\u22121Ires + I0 \u03b2\u03042t\u22121 )/ \u03b2\u03042t \u03b22t \u03b2\u0304 2 t\u22121\n(19)\n= \u03b2\u03042t\u22121 \u03b2\u03042t It + \u03b22t \u03b1\u0304t\u22121 \u2212 \u03b2\u03042t\u22121\u03b1t \u03b2\u03042t Ires + \u03b22t \u03b2\u03042t I0 (20) = It \u2212 \u03b1tIres \u2212 \u03b22t \u03b2\u0304t \u03f5, (21)\n\u03a3t(xt, I0, Ires) = \u03b22t \u03b2\u0304 2 t\u22121\n\u03b2\u03042t . (22)\n8Each step in Eq. 5 adds a new random Gaussian noise in the random forward diffusion. Thus for simplicity, we assume q(It|It\u22121, I0, Ires) = q(It|It\u22121, Ires), it follows that I0 is not important for It when It\u22121 presents as a condition.\nEq. 5 is used for the derivation from Eq. 20 to Eq. 21. Then, we define the generation process to start from p\u03b8(IT ) \u223c N (IT ;0, I),\np\u03b8(It\u22121|It) = q(It\u22121|It, I\u03b80 , I\u03b8res), (23)\nwhere I\u03b80 = It \u2212 \u03b1\u0304tI\u03b8res \u2212 \u03b2\u0304t\u03f5\u03b8 by Eq. 5. Here we only consider Lt\u22121 in (Ho et al., 2020),\nLt\u22121 = DKL(q(It\u22121|It, I0, Ires)||p\u03b8(It\u22121|It)) (24)\n= E [\u2225\u2225\u2225\u2225It \u2212 \u03b1tIres \u2212 \u03b22t\u03b2\u0304t \u03f5\u2212 (It \u2212 \u03b1tI\u03b8res \u2212 \u03b2 2 t \u03b2\u0304t \u03f5\u03b8) \u2225\u2225\u2225\u22252 ] , (25)\nwhere DKL denotes KL divergence. Ignoring the coefficients and the cross term < Ires \u2212 I\u03b8res, \u03f5\u2212 \u03f5\u03b8 >, we obtain the following simplified training objective,\nLres(\u03b8) + L\u03f5(\u03b8), (26)\nwhere (repeat Eq. 11 here) Lres(\u03b8) := E [ \u03bbres \u2225\u2225Ires \u2212 I\u03b8res(It, t, Iin)\u2225\u22252] , L\u03f5(\u03b8) := E [\u03bb\u03f5 \u2225\u03f5\u2212 \u03f5\u03b8(It, t, Iin)\u22252] , (27) and \u03bbres, \u03bb\u03f5 \u2208 {0, 1}."
        },
        {
            "heading": "A.2 DETERMINISTIC IMPLICIT SAMPLING",
            "text": "If q\u03c3(It\u22121|It, I0, Ires) is defined in Eq. 8, we have:\nq(It|I0, Ires) = N (It; I0 + \u03b1\u0304tIres, \u03b2\u03042t I). (28)\nProof. Similar to the evolution from DDPM (Ho et al., 2020) to DDIM (Song et al., 2021a), we can prove the statement with an induction argument for t from T to 1. Assuming that Eq. 28 holds at T , we just need to verify q(It\u22121|I0, Ires) = N (It\u22121; I0+ \u03b1\u0304t\u22121Ires, \u03b2\u03042t\u22121I) at t\u2212 1 from q(It|I0, Ires) at t using Eq. 28. Given:\nq(It|I0, Ires) = N (It; I0 + \u03b1\u0304tIres, \u03b2\u03042t I), (29) q\u03c3(It\u22121|It, I0, Ires) = N (It\u22121; I0 + \u03b1\u0304t\u22121Ires + \u221a \u03b2\u03042t\u22121 \u2212 \u03c32t\nIt \u2212 (I0 + \u03b1\u0304tIres) \u03b2\u0304t , \u03c32t I), (30)\nq(It\u22121|I0, Ires) := N (\u00b5\u0303t\u22121, \u03a3\u0303t\u22121) (31)\nSimilar to obtaining p(y) from p(x) and p(y|x) using Eq.2.113-Eq.2.115 in (Bishop & Nasrabadi, 2006), the values of \u00b5\u0303t\u22121 and \u03a3\u0303t\u22121 are derived as following:\n\u00b5\u0303t\u22121 = I0 + \u03b1\u0304t\u22121Ires + \u221a \u03b2\u03042t\u22121 \u2212 \u03c32t\n(I0 + \u03b1\u0304tIres)\u2212 (I0 + \u03b1\u0304tIres) \u03b2\u0304t = I0 + \u03b1\u0304t\u22121Ires, (32)\n\u03a3\u0303t\u22121 = \u03c3 2 t I+ (\n\u221a \u03b2\u03042t\u22121 \u2212 \u03c32t\n\u03b2\u0304t )2\u03b2\u03042t I = \u03b2 2 t\u22121I. (33)\nTherefore, q(It\u22121|I0, Ires) = N (It\u22121; I0 + \u03b1\u0304t\u22121Ires, \u03b2\u03042t\u22121I). In fact, the case (t = T ) already holds, thus Eq. 28 holds for all t.\nSimplifying Eq. 8. Eq. 8 can also be written as: It\u22121 = I0 + \u03b1\u0304t\u22121Ires + \u221a \u03b2\u03042t\u22121 \u2212 \u03c32t It \u2212 (I0 + \u03b1\u0304tIres)\n\u03b2\u0304t + \u03c3t\u03f5t, (34)\n=\n\u221a \u03b2\u03042t\u22121 \u2212 \u03c32t\n\u03b2\u0304t It + (1\u2212\n\u221a \u03b2\u03042t\u22121 \u2212 \u03c32t\n\u03b2\u0304t )I0 + (\u03b1\u0304t\u22121 \u2212\n\u221a \u03b1\u0304t\u03b2\u03042t\u22121 \u2212 \u03c32t\n\u03b2\u0304t )Ires + \u03c3t\u03f5t (35) = It \u2212 (\u03b1\u0304t \u2212 \u03b1\u0304t\u22121)Ires \u2212 (\u03b2\u0304t \u2212 \u221a \u03b2\u03042t\u22121 \u2212 \u03c32t )\u03f5+ \u03c3t\u03f5t, (36)\nwhere \u03f5t \u223c N (0, I). Eq. 36 is consistent with Eq. 9, and Eq. 5 is used for the derivation from Eq. 35 to Eq. 36."
        },
        {
            "heading": "A.3 COEFFICIENT TRANSFORMATION",
            "text": "For image generation, Iin = 0, thus Eq. 12 can also be written as: It = (\u03b1\u0304t \u2212 1)Ires + \u03b2\u0304t\u03f5 (37)\nIres = It \u2212 \u03b2\u0304t\u03f5 \u03b1\u0304t \u2212 1 . (38)\nIf the residuals I\u03b8res are represented as a transformation of \u03f5\u03b8 using Eq. 38, Eq. 9 is simplified to It\u22121 = It \u2212 (\u03b1\u0304t \u2212 \u03b1\u0304t\u22121)I\u03b8res \u2212 (\u03b2\u0304t \u2212 \u221a \u03b2\u03042t\u22121 \u2212 \u03c32t )\u03f5\u03b8 + \u03c3t\u03f5t (39)\n= It \u2212 (\u03b1\u0304t \u2212 \u03b1\u0304t\u22121) It \u2212 \u03b2\u0304t\u03f5\u03b8 \u03b1\u0304t \u2212 1\n\u2212 (\u03b2\u0304t \u2212 \u221a \u03b2\u03042t\u22121 \u2212 \u03c32t )\u03f5\u03b8 + \u03c3t\u03f5t (40)\n= 1\u2212 \u03b1\u0304t\u22121 1\u2212 \u03b1\u0304t It \u2212 ( 1\u2212 \u03b1\u0304t\u22121 1\u2212 \u03b1\u0304t\n\u03b2\u0304t \u2212 \u221a \u03b2\u03042t\u22121 \u2212 \u03c32t )\u03f5\u03b8 + \u03c3t\u03f5t (41)\n= \u221a \u03b1\u0304t\u22121DDIM\u221a \u03b1\u0304tDDIM It \u2212 ( \u221a \u03b1\u0304t\u22121DDIM \u221a 1\u2212 \u03b1\u0304tDDIM\u221a \u03b1\u0304tDDIM \u2212 \u221a 1\u2212 \u03b1\u0304t\u22121DDIM \u2212 \u03c32t )\u03f5\u03b8 + \u03c3t\u03f5t (42)\n= \u221a \u03b1\u0304t\u22121DDIM ( It \u2212 \u221a 1\u2212 \u03b1\u0304tDDIM \u03f5\u03b8\u221a \u03b1\u0304tDDIM ) + \u221a 1\u2212 \u03b1\u0304t\u22121DDIM \u2212 \u03c32t \u03f5\u03b8 + \u03c3t\u03f5t. (43)\nEq. 43 is consistent with Eq.12 in DDIM (Song et al., 2021a) by replacing \u03c32t with \u03c3 2 t (DDIM), and Eq. 13 is used for the derivation from Eq. 41 to Eq. 42. Thus, our sampling process is consistent with that of DDPM (Song et al., 2021a) and DDIM (Ho et al., 2020) by transforming coefficient schedules.\nWe present the pipeline of coefficient transformation in Algorithm 1. Fig. 6 shows the result of coefficient transformation. In Eq. 13, in addition to the coefficient \u03b1\u0304t, \u03b2\u03042t being replaced by \u03b1\u0304 t DDIM , the variance \u03c32t is also replaced with \u03c3 2 t (DDIM) to be consistent with DDIM (Song et al., 2021a) (\u03b7 = 0) and DDPM (Ho et al., 2020) (\u03b7 = 1). In fact, for DDIM (Song et al., 2021a) (\u03b7 = 0), the variance is equal to 0 and does not need to be converted. Therefore, we analyze the difference between the variance of our RDDM and the variance of DDPM (Ho et al., 2020) in Appendix A.4."
        },
        {
            "heading": "A.4 PERTURBED GENERATION PROCESS WITH SUM-CONSTRAINED VARIANCE",
            "text": "From Eq. 9, the variance of our RDDM (\u03b7 = 1) is\n\u03c32t (RDDM) = \u03b7 \u03b22t \u03b2\u0304 2 t\u22121\n\u03b2\u03042t . (44)\nWe replace \u03b2\u03042t by \u03b1\u0304 t DDIM using Eq. 13 and replace \u03b2 2 t by \u03b2\u0304 2 t \u2212 \u03b2\u03042t\u22121,\n\u03c32t (RDDM) = \u03b7\u03b1\u0304 t\u22121 DDIM (1\u2212 \u03b1\u0304t\u22121DDIM ) 1\u2212 \u03b1\u0304tDDIM (1\u2212 \u03b1\u0304 t DDIM \u03b1\u0304t\u22121DDIM ), (45)\nAlgorithm 1: Coefficient initialization, transformation, and adjustment. Input : The initial conditions \u03b1\u0304T = 1, \u03b2\u03042T > 0, T = 1000, and t \u2208 {1, 2, . . . T}. The\nhyperparameter \u03b7 = 1 for the random generation process and \u03b7 = 0 for deterministic implicit sampling. Variance modes have \u201cDDIM\u201d and \u201cDDIM\u2192RDDM\u201d. The coefficient adjustment mode Adjust=\u201cAlpha\u201d, \u201cBeta\u201d, or \u201cAlpha+Beta\u201d.\nOutput: The adjusted coefficients \u03b1\u0304\u2217t , \u03b2\u0304\u2217t and \u03c3\u2217t . // (a) Coefficient initialization of DDIM (Song et al., 2021a)\n1 \u03b2tDDIM \u2190 Linspace (0.0001, 0.02, T ) \u25b7 linear schedule (Song et al., 2021a) 2 \u03b1tDDIM \u2190 1\u2212 \u03b2tDDIM 3 \u03b1\u0304tDDIM \u2190 Cumprod (\u03b1tDDIM ) \u25b7 cumulative multiplication 4 \u03c32t (DDIM)\u2190 \u03b7 (1\u2212\u03b1\u0304t\u22121DDIM ) 1\u2212\u03b1\u0304tDDIM (1\u2212 \u03b1\u0304 t DDIM \u03b1\u0304t\u22121DDIM )\n// (b) Coefficient transformation from DDIM (Song et al., 2021a) to our RDDM 5 \u03b1\u0304t \u2190 1\u2212 \u221a \u03b1\u0304tDDIM \u25b7 Eq. 13\n6 \u03b2\u0304t \u2190 \u221a 1\u2212 \u03b1\u0304tDDIM \u25b7 Eq. 13 7 \u03c32t (RDDM)\u2190 \u03b7 (\u03b2\u03042t\u2212\u03b2\u0304 2 t\u22121)\u03b2\u0304 2 t\u22121\n\u03b2\u03042t // (c) Select variance schedule\n8 if Variance==\u201cDDIM\u201d then 9 \u03c3\u2217t \u2190 \u221a \u03c32t (DDIM) \u25b7 consistent sampling process with DDIM (Song et al., 2021a) and\nDDPM (Ho et al., 2020) 10 else if Variance==\u201cDDIM\u2192RDDM\u201d then 11 \u03c3\u2217t \u2190 \u221a \u03c32t (RDDM) \u25b7 sum-constrained variance schedule 12 end // (d) Coefficient adjustment 13 \u03b1t \u2190Power (1\u2212 t/T , 1) \u25b7 linearly decreasing by Eq. 14 14 \u03b22t \u2190Power (t/T , 1)\u00b7\u03b2\u03042T \u25b7 control the noise intensity in IT by \u03b2\u03042T 15 if Adjust==\u201cAlpha\u201d then 16 \u03b1\u0304\u2217t \u2190Cumsum (\u03b1t), \u03b2\u0304\u2217t \u2190 \u03b2\u0304t \u25b7 cumulative sum 17 else if Adjust==\u201cBeta\u201d then 18 \u03b1\u0304\u2217t \u2190 \u03b1\u0304t, \u03b2\u0304\u2217t \u2190 \u221a Cumsum(\u03b22t ) \u25b7 cumulative sum 19 else if Adjust==\u201cAlpha+Beta\u201d then 20 \u03b1\u0304\u2217t \u2190Cumsum (\u03b1t), \u03b2\u0304\u2217t \u2190 \u221a Cumsum(\u03b22t ) \u25b7 coefficient reinitialization 21 else 22 \u03b1\u0304\u2217t \u2190 \u03b1\u0304t, \u03b2\u0304\u2217t \u2190 \u03b2\u0304t 23 end 24 return \u03b1\u0304\u2217t , \u03b2\u0304 \u2217 t , \u03c3 \u2217 t \u25b7 sampling with adjusted coefficients by Eq. 9\nwhile the variance of DDPM (Ho et al., 2020) (\u03b7 = 1) is\n\u03c32t (DDIM) = \u03b7 (1\u2212 \u03b1\u0304t\u22121DDIM ) 1\u2212 \u03b1\u0304tDDIM (1\u2212 \u03b1\u0304 t DDIM \u03b1\u0304t\u22121DDIM ). (46)\nOur variance is much smaller than the variance of DDPM (Ho et al., 2020) because 0 < \u03b1\u0304t\u22121DDIM < \u03b11DDIM < 1 (e.g., \u03b1 1 DDIM = 0.02 in linear schedule (Song et al., 2021a)). Compared to \u03c32t (DDIM) \u2248 1 (Song et al., 2021b), the variance of our RDDM is sum-constrained, T\u2211\ni=1\n\u03c32t (RDDM) = T\u2211 i=1 \u03b7\u03b22t \u03b2\u03042t\u22121 \u03b2\u03042t \u2264 T\u2211 i=1 \u03b22t \u2264 1, (47)\nwhere \u2211T\ni=1 \u03b2 2 t = \u03b2\u0304 2 T = 1 for image generation. This is also consistent with the directional residual\ndiffusion process with perturbation defined in Eq. 5. A qualitative comparison of our RDDM (\u03b7 = 1) with DDIM (Song et al., 2021a) (\u03b7 = 0) and DDPM (Ho et al., 2020) (\u03b7 = 1) is shown in Fig. 7. Notably, for \u03b7 = 0, our RDDM is consistent with DDIM (Song et al., 2021a) (in Fig. 2(a)(b))."
        },
        {
            "heading": "A.5 COMPARISON WITH OTHER METHODS",
            "text": "The main difference is that to adapt the denoising diffusion, score, flow, or Schro\u0308dinger\u2019s bridge to image restoration, they choose the noise (Shadow Diffusion (Guo et al., 2023), SR3 (Saharia et al., 2022), and WeatherDiffusion (O\u0308zdenizci & Legenstein, 2023)), the residual (DvSR (Whang et al., 2022) and Rectified Flow (Liu et al., 2023d)), the target image (ColdDiffusion (Bansal et al., 2022), InDI (Delbracio & Milanfar, 2023), and consistency models (Song et al., 2023)), or its linear transformation term (I2SB (Liu et al., 2023a)), which is similar to a special case of our RDDM when it only predicts noise (SM-N) or residuals (SM-Res), while we introduce residual estimation but also embrace noise both for generation and restoration (SM-Res-N). We highlight that residuals and noise are equally important, e.g., the residual prioritizes certainty while the noise emphasizes diversity.\nDifferences from DDPM (Ho et al., 2020). 1) DDPM is not interpretable for image restoration, while our RDDM is a unified, interpretable diffusion model for both image generation and restoration. 2) Differences in the definition of the forward process lead to different variance strategies. Our RDDM has sum-constrained variance (much smaller than the variance of DDPM), while DDPM has preserving variance (Song et al., 2021b), as shown in Fig. 2(a)(b).\nDifferences from IDDPM (Nichol & Dhariwal, 2021). In the original DDPM (Ho et al., 2020), for the transfer probabilities p\u03b8(It\u22121|It) in Eq. 3, the mean \u00b5\u03b8(It, t) is learnable, while the variance \u03a3t is fixed. IDDPM (Nichol & Dhariwal, 2021) highlights the importance of estimating both the mean and variance, demonstrating that learning variances allow for fewer sampling steps with negligible differences in sample quality. However, IDDPM (Nichol & Dhariwal, 2021) still only\ninvolves denoising procedures, and crucially, IDDPM, like DDPM, is thus not interpretable for image restoration. In addition, IDDPM (Nichol & Dhariwal, 2021) proposes three alternative ways to parameterize \u00b5\u03b8(It, t), i.e., predict mean \u00b5\u03b8(It, t) directly with a neural network, predict noise \u03f5, or predict clean image I0. IDDPM (Nichol & Dhariwal, 2021) does not predict the clean image I0 and noise \u03f5 at the same time, while both the residuals and the noise are predicted for our SM-Res-N.\nThe essential difference is that, our RDDM contains a mixture of three terms (i.e., input images Iin, target images I0, and noise \u03f5) beyond DDPM/IDDPM (a mixture of two terms, i.e, I0 and \u03f5). We emphasize that residuals and noise are equally important: the residual prioritizes certainty, while the noise emphasizes diversity. Furthermore, our RDDM preserves the original DDPM generation framework by coefficient transformation (Eq. 13), enabling seamless transfer of improvement techniques from DDPM, such as variance optimization from IDDPM.\nDifferences from ColdDiffusion (Bansal et al., 2022). 1) ColdDiffusion aims to remove the random noise entirely from the diffusion model, and replace it with other transforms (e.g., blur, masking), while our RDDM still embraces noise diffusion. Notably, we argue that noise is necessary for generative tasks that emphasize diversity (see Table 1). In fact, since ColdDiffusion discards random noise, extra noise injection is required to improve generation diversity. 2) To simulate the degradation process for different restoration tasks, ColdDiffusion attempts to use a Gaussian blur operation for deblurring, a snowification transform for snow removal., etc. These explorations may lose generality and differ fundamentally from our residual learning. RDDM represents directional diffusion from target images to input images using residuals, without designing specific degradation operators for each task. Additionally, RDDM provides solid theoretical derivation, while ColdDiffusion lacks theoretical justification.\nDifferences from DvSR (Whang et al., 2022). Whang et al. (2022) indeed use residual. But they 1) predict the initial clean image from a blurring image via a traditional (non-diffusion) network, calculate the residuals between the ground truth of the clean image and the predicted clean image 2) use denoising-based diffusion models predict noise like DDPM (Ho et al., 2020) and use a linear transformation of the noise to represent the residuals. They treat the residual predictions as an image generation task, aiming to produce diverse and plausible outputs based on the initial predicted clean image. Beyond simply building a diffusion model on top of residuals, we redefine a new forward process that allows simultaneous diffusion of residuals and noise, wherein the target image progressively diffuses into a purely noise or a noise-carrying input image.\nDifferences from InDI (Delbracio & Milanfar, 2023) and I2SB (Liu et al., 2023a). We can conclude that the forward diffusion of InDI, I2SB, and our RDDM is consistent in the form of a mixture of three terms (i.e., input images Iin, target images I0, and noise \u03f5) beyond the denoisingbased diffusion (a mixture of two terms, i.e, I0 and \u03f5). Substituting Ires = Iin \u2212 I0 into Eq. 12 results in It = \u03b1\u0304tIin + (1 \u2212 \u03b1\u0304t)I0 + \u03b2\u0304t\u03f5. This resulted It has the same format as Eq.8 in InD (xt = ty+(1\u2212 t)x+ \u221a t\u03f5t\u03b7t), and is the same format as Eq.11 in I2SB. Similar to Eq. 13 (from our RDDM to DDPM/DDIM), transforming coefficients leads to complete consistency. However, our RDDM can further extend DDPM/DDIM, InD, and I2SB to independent double diffusion processes, and holds the potential to pave the way for the multi-dimensional diffusion process. From the initial stages of constructing a new forward process, our RDDM uses independent coefficient schedules to control the diffusion of residuals and noise. This provides a more general, flexible, and scalable framework, and inspires our partially path-independent generation process, demonstrated in Fig. 3 and Fig. 15(b-f) with stable generation across various diffusion rates and path variations."
        },
        {
            "heading": "B EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "B.1 TRAINING DETAILS",
            "text": "We use a UNet architecture9 for both residual prediction and noise prediction in our RDDM. The UNet settings remain consistent across all tasks, including the channel size (64) and channel multiplier (1,2,4,8). Detailed experimental settings can be found in Table 4. Training and testing for all experiments in Table 4 can be conducted on a single Nvidia GTX 3090.\n9Our RDDM is implemented by modifying https://github.com/lucidrains/denoising-diffusion-pytorch repository.\nImage Generation. For comparison with DDIM Song et al. (2021a), we convert the \u03b1tDDIM schedule of DDIM Song et al. (2021a) into the \u03b1t and \u03b22t schedules of our RDDM using Eq. 13 in Section 4.2 and Section 5. In fact, a better coefficient schedule can be used in our RDDM, e.g., \u03b1t (linearly decreasing) and \u03b22t (linearly increasing) in Table 2.\nImage Restoration. We extensively evaluate our method on several image restoration tasks, including shadow removal, low-light enhancement, image deraining, and image deblurring on 5 different datasets. For fair comparisons, the results of other SOTA methods are provided from the original papers whenever possible. For all image restoration tasks, the images are resized to 256, and the networks are trained with a batch size of 1. We use shadow masks and shadow images as conditions for shadow removal (similar to (Le & Samaras, 2019; Zhu et al., 2022a)), while other image restoration tasks use the degraded image as condition inputs. For low-light enhancement, we use histogram equalization for pre-processing. To cope with the varying tasks and dataset sizes, we only modified the number of training iterations, \u03b2\u03042T and sampling steps (5 steps for shadow removal and deraining, 2 steps for low-light and deblurring) as shown in Table 4. \u03b1t is initialized using a linearly decreasing schedule (i.e., P (1 \u2212 x, 1) in Eq. 14), while \u03b22t is initialized using a linearly decreasing schedule (i.e., P (x, 1)). The quantitative results were evaluated by the peak signal to noise ratio (PSNR), structural similarity (SSIM), and learned perceptual image patch similarity (LPIPS) (Zhang et al., 2018).\nNotably, our RDDM uses an identical UNet architecture and is trained with a batch size of 1 for all these tasks. In contrast, SOAT methods often involve elaborate network architectures, such as multi-stage (Fu et al., 2021; Zamir et al., 2021; Zhu et al., 2022b), multi-branch (Cun et al., 2020), and GAN (Wang et al., 2018; Kupyn et al., 2019; Qian et al., 2018), or sophisticated loss functions like the chromaticity (Jin et al., 2021), texture similarity (Zhang et al., 2019), and edge loss (Zamir et al., 2021).\nImage Inpainting and Image Translation. To increase the diversity of the generated images, conditional input images were not fed into the deresidual and denoising network (see Fig. 18)."
        },
        {
            "heading": "B.2 SAMPLING DETAILS",
            "text": "SM-Res or SM-N. We present the motivation, conceptualization, and implementation pipeline (Algorithm 2) of the Automatic Target Domain Prediction Algorithm (ATDP) as follows:\nStep 1. At the initial simultaneous training (similar to SM-Res-N), we do not know whether the network output (Iout) is residual or noise. Therefore, we set \u03bb\u03b8res = 0.5 to denote the probability that the output is residual (I\u03b8res), and 1\u2212 \u03bb\u03b8res is the probability that the output is noise (\u03f5\u03b8). Step 2. We then impose loss constraints on both residual and noise estimation weighted by the learned parameter (\u03bb\u03b8res), as follows:\nLauto(\u03b8) := \u03bb \u03b8 resE [\u2225\u2225Ires \u2212 I\u03b8res(It, t, Iin)\u2225\u22252]+ (1\u2212 \u03bb\u03b8res)E [\u2225\u03f5\u2212 \u03f5\u03b8(It, t, Iin)\u22252] . (48)\nAlgorithm 2: Training Pipeline Using ATDP. Input : A degraded input image Iin and its corresponding ground truth image I0. Gaussian\nSampling Method Network MAE(\u2193) SSIM(\u2191) PSNR(\u2191) SM-Res Residual network 4.76 0.959 30.72 SM-Res-N-2Net Residual network+noise network 4.67 0.962 30.91 SM-Res-N-1Net One network, only shared encoder 4.72 0.959 30.73 SM-Res-N-1Net One network 4.57 0.963 31.10\nThe joint loss functions Lauto(\u03b8) drive the network to gradually favor either residuals or noise based on the input. For example, in the image restoration task with deterministic input, it should be simpler for the network to estimate a clear image than noise. In contrast, for the image generation task with random noise input, it is simpler for the network to estimate the noise than a clear image.\nStep 3. To enable learning of \u03bb\u03b8res, we then include it in the network computation, allowing gradient transmission. Since \u03bb\u03b8res denotes the probability that the output is residual, the estimated residual I\u03b8res can be represented as \u03bb \u03b8 res \u00d7 Iout + (1 \u2212 \u03bb\u03b8res) \u00d7 f\u03f5\u2192res(Iout). f\u03f5\u2192res(\u00b7) represents the transformation from \u03f5 to Ires using Eq. 12. Similarly, \u03f5\u03b8 can be represented as \u03bb\u03b8res\u00d7fres\u2192\u03f5(Iout)+ (1\u2212 \u03bb\u03b8res)\u00d7 Iout. This is very similar to the cross-entropy loss function. Step 4. As training approaches are completed, our objective should be to estimate only noise (SMN) or residuals (SM-Res). By utilizing the learned \u03bb\u03b8res, we can determine when to switch from an adversarial-like process (residuals vs. noise in Step 2) to a single prediction (residuals or noise). This transition can be controlled, for instance, by setting a condition such as abs(\u03bb\u03b8res\u22120.5) \u2265 0.01. When the network\u2019s tendency to estimate residuals surpasses 51% probability, we set \u03bb\u03b8res to 1 and halt the gradient updates for \u03bb\u03b8res.\nThe experimental results were consistent with the empirical analysis in Section 3.3 and verified the effectiveness of ATDP. For instance, the initial simultaneous training switches to residual learning (SM-Res) for shadow removal and low-light enhancement in approximately 300 iterations, and to denoising learning (SM-N) for image generation in approximately 1000 iterations. To summarize, ATDP achieves the same inference cost as the current denoising-based diffusion methods (Ho et al., 2020) with the plug-and-play training strategy.\nSM-Res-N. Both the residuals and the noise are predicted, which can be implemented with two or one networks. SM-Res-N-2Net. If computational resources are sufficient, two separate networks can be trained for noise and residual predictions, and the optimal sampling method can be determined during testing. This setting easily obtains a well-suited network for the target task, and facilitates the exploration of the decoupled diffusion process and the partially path-independent generation process in Section 4. SM-Res-N-1Net. To avoid training two separate networks, another solution is to simply use a joint network (i.e., a shared encoder and decoder) to output 6 channels where the 0-3-th channels are residual and the 3-6-th channels are noise. This setting loses the decoupling property of RDDM, but can achieve dual prediction with a slight cost. Table 5 shows that the joint network (i.e., SM-Res-N-1Net+One network) achieves the best shadow removal results (MAE 4.57), even better than two independent networks (4.67). A network with the shared encoder (MAE 4.72) has a slight performance degradation compared to the independent two networks (4.67)."
        },
        {
            "heading": "B.3 MORE RESULTS",
            "text": "Shadow removal. We compare RDDM with DSC (Hu et al., 2020), FusionNet (Fu et al., 2021), BMNet (Zhu et al., 2022a) and DMTN (Liu et al., 2023b) on the ISTD dataset (Wang et al., 2018). The ISTD dataset (Wang et al., 2018) contains shadow images, shadow masks, and shadow-free image triplets (1,330 for training; 540 for testing). Table 3(b), Fig. 4(b), and Fig. 8 demonstrate the superiority of our method. In addition, we compare RDDM with more shadow removal methods (e.g., ST-CGAN (Wang et al., 2018), DHAN (Cun et al., 2020), CANet (Chen et al., 2021), LGShadowNet (Liu et al., 2021b), UnfoldingNet (Zhu et al., 2022b)) in Table 6.\nLow-light enhancement. We evaluate our RDDM on the LOL (Wei et al., 2018) (500 images) and SID-RGB (Xu et al., 2020) datasets (5,094 images), and compare our method with the current SOTA methods (Zhang et al., 2021; Liu et al., 2021a; Yuhui et al., 2023; Zhang et al., 2022; Xu et al., 2022; Zamir et al., 2022; Zheng et al., 2021). To unify and simplify the data loading pipeline for training, we only evaluate the RGB low-light image dataset (Wei et al., 2018; Xu et al., 2020), not the RAW datasets (e.g., FiveK (Bychkovsky et al., 2011)). Table 3(c), Fig. 4(c), and Fig. 9 show that our RDDM achieves the best SSIM and LPIPS (Zhang et al., 2018) and can recover better visual quality on the LOL (Wei et al., 2018) dataset. Table 7(a) shows the low-light enhancement results on the SID-RGB (Xu et al., 2020) dataset of different methods. Our RDDM outperforms the state-of-the-art SNR-Aware (Xu et al., 2022) by a 4.8% PSNR and a 34.2% SSIM improvement on the SID-RGB (Xu et al., 2020) dataset. Fig. 10 shows that our RDDM outperforms competitors in detail recovery (sharper text of 1st row), and color vibrancy (2nd & 3rd rows), avoiding issues like gray shading and detail blurring.\nImage deraining. We make a fair comparison with the current SOTA diffusion-based image restoration method - RainDiff128 (O\u0308zdenizci & Legenstein, 2023) (\u201c128\u201d denotes the 128\u00d7128 patch size for training) on the RainDrop dataset (Qian et al., 2018) (1119 images). RainDiff128 (O\u0308zdenizci &\nLegenstein, 2023) feeds the degraded input image as a condition to the denoising network, which requires 50 sampling steps to generate a clear image from the noise, while our RDDM requires only 5 sampling steps to recover the degraded image from the noise-carrying input image and outperforms RainDiff128 (O\u0308zdenizci & Legenstein, 2023), as shown in Table 3(d) and Fig. 11.\nImage deblurring. We evaluate our method on the widely used deblurring dataset - GoPro (Nah et al., 2017) (3,214 images). Table 7(b) and Fig. 12 show that our method is competitive with the SOTA deblurring methods (e.g, MPRNet (Zamir et al., 2021), and Uformer-B (Wang et al., 2022b)).\nImage Inpainting and Image Translation. We show more qualitative results of image inpainting (Fig. 13) and translation (Fig. 14(e))."
        },
        {
            "heading": "B.4 PARTIALLY PATH-INDEPENDENT GENERATION PROCESS",
            "text": "Fig. 15(b-f) provides evidence supporting the partially path-independent generation process, demonstrating the robustness of the generative process within a certain range of diffusion rates (step size per step) and path variation, e.g., converting DDIM (Song et al., 2021a) to a uniform diffusion speed in Fig. 15(c). However, excessive disturbances can result in visual inconsistencies, as depicted in Fig. 15(h)(i). Furthermore, Fig. 15(c) and Fig. 15(g) illustrate that even when the paths are the same, the variation in diffusion speed significantly impacts the quality of the generated images. This highlights the importance of carefully considering and controlling the diffusion speed and generation path during the generation process.\n(a) Remove residuals and noise simultaneously (c) First remove noise then residuals\n(b) First remove residuals then noise (d) First remove noise\n(b1) First remove\nresiduals\n(b2) Then remove\nnoise\nFigure 16: Special paths of the partially path-independent generation process. Two networks are used to estimate residuals and noise separately, i.e., I\u03b8res(It, \u03b1\u0304t \u00b7 T ), \u03f5\u03b8(It, \u03b2\u0304t \u00b7 T ) (\u03b7 = 0).\nWe also investigated two reverse paths to gain insight into the implications of the proposed partial path independence. In the first case, the residuals are removed first, followed by the noise:\nI(T ) \u2212Ires\u2192 I(0) + \u03b2\u0304T \u03f5 \u2212\u03b2\u0304T \u03f5\u2192 I(0), as shown in Fig. 16(b1)(b2). The second case involves removing the noise first and then the residuals: I(T ) \u2212\u03b2\u0304T \u03f5\u2192 Iin \u2212Ires\u2192 I(0). In the first case, images are successfully generated (as shown in Fig. 16(b)) which exhibit a striking similarity to the default images in Fig. 16(a). However, the second case shown in Fig. 16(c) fails to go from Iin to I(0) due to Iin = 0 in the generation task. Figure 16(d) shows the intermediate visualization results of removing the noise first."
        },
        {
            "heading": "B.5 ABLATION STUDIES",
            "text": "We have analyzed the sampling method in Table 1, the coefficient schedule in Table 2, and the network structure for SM-Res-N in Table 5.\nSampling Methods. We present the results for noise predictions only (SM-N) in Fig. 17. Fig. 17 (b) and (c) illustrate that estimating only the noise poses challenges as colors are distorted, and it becomes difficult to retain information from the input shadow image. We found that increasing sampling steps does not lead to improved results from Fig. 17 (b) to Fig. 17 (c), which may be an inherent limitation when estimating only the noise for image restoration. Actually, this is also reflected in DeS3 (Jin et al., 2022a) (a shadow removal method based on a denoising diffusion model), where DeS3 (Jin et al., 2022a) specifically designs the loss against color bias. Additionaly, training with batch size 1 may contribute to poor results of only predicting noise. However, estimating only the residuals (SM-Res) with batch size 1 does not exhibit such problems for image restoration, as\ndemonstrated in Fig. 17 (d)&(e) and Table 1, further demonstrating the merits of our RDDM. For image inpainting, SM-Res-N can generate more realistic face images compared to SM-N and SM-Res, as shown in Fig. 18(d-f). If computational resources are sufficient, to obtain better image quality for an unknown task, we suggest that two separate networks can be trained for noise and residual predictions, and the optimal sampling method can be determined during testing. If computational resources are limited, the sampling method can be determined empirically (see Section 3.3).\nNoise Perturbation Intensity. Table 8 shows that for image restoration, our RDDM with SM-Res or SM-Res-N is not sensitive to the noise intensity \u03b2\u03042T . For image generation, the diversity of the\ngenerated images decreases significantly as \u03b2\u03042T decreases from 1 in Fig. 19(c) to 0.01 in Fig. 19(b). The experiment is related to the mean face (Wilson et al., 2002; Loffler et al., 2005; Hu & Hu, 2021; Meng et al., 2021b) and could provide useful insights to better understand the generative process. Fig. 20 shows that modifying \u03b2\u03042T during testing (\u03b2\u0304 2 T : 1 \u2192 0.01) causes SM-N to fail to generate meaningful faces. SM-Res-N including deresidual and denoising networks can generate meaningful face images like SM-Res, indicating that the denoising network can perform denoising when modifying \u03b2\u03042T , but cannot obtain robust residuals (I \u03b8 res) in the sampling process by Eq. 12. In summary, the deresidual network is relatively robust to noise variations compared to the denoising network.\nResource efficiency. Due to fewer sampling steps, our RDDM inference time and performance is comparable to lllflow (Wang et al., 2022a), and LLFormer (Wang et al., 2023) (not diffusion-based). Compared to SR3 (Saharia et al., 2022), our RDDM (only res in Table 9(b)) has 10x fewer training iterations, 10x fewer parameters, 10x faster inference time, and 10% improvement in PSNR and SSIM\non the ISTD (Wang et al., 2018) dataset (shadow removal). For a fair comparison, priori shadow masks are used in SR3 (Saharia et al., 2022) with a batch size of 1. ShadowDiffusion (Guo et al., 2023) uses SR3 (Saharia et al., 2022) and Uformer (Wang et al., 2022b), which has a higher PSNR but is also expected to be more computationally expensive. Our RDDM with SM-Res requires only 4.8G of GPU memory for training. Experiments in shadow removal and low-light enhancement demonstrate the effectiveness of RDDM, enabling computationally-constrained researchers to utilize diffusion modeling for image restoration tasks.\nAccelerating Convergence. The residual prediction in our RDDM helps the diffusion process to be more certain, which can accelerate the convergence process, e.g., fewer training iterations and higher performance in Table 9(b).\nFailure case. We present some failure cases in Fig. 21."
        },
        {
            "heading": "C DISCUSSIONS, LIMITATIONS, AND FURTHER WORK",
            "text": "Limitations. Our primary focus has been on developing a unified prototype model for image restoration and generation, which may result in certain performance limitations when compared to taskspecific state-of-the-art methods. To further improve the performance of a specific task, potential avenues for exploration include using a UNet with a larger number of parameters, increasing the batch size, conducting more iterations, and implementing more effective training strategies, such as learning rate adjustments customized for different tasks. For the image generation task, although Table 2 showcases the development of an improved coefficient schedule, attaining state-of-the-art performance in image generation necessitates further investigation and experimentation. In summary, while we recognize the existing performance limitations for specific tasks, we are confident that our unified prototype model serves as a robust foundation for image restoration and generation.\nFurther Work. Here are some interesting ways to extend our RDDM.\n1. In-depth analysis of the relationship between RDDM and curve/multivariate integration. 2. Development of a diffusion model trained with one set of pre-trained parameters to handle\nseveral different tasks. 3. Implementing adaptive learning coefficient schedules to reduce the sampling steps while\nimproving the quality of the generated images. 4. Constructing interpretable multi-dimensional latent diffusion models for multimodal fu-\nsion, e.g., generating images using text and images as conditions. 5. Adaptive learning noise intensity (\u03b22T ) for an unknown new task.\nBroader Impacts. Our work establishes a seamless connection between denoising-based diffusion models and image restoration tasks, which serves the broader impact and potential of diffusion models in various fields. By introducing a directional residual diffusion approach with perturbations, our method holds promise for fine-tuning generation tasks. For instance, it can be applied to finetune classic clothing styles, model temporal variations with perturbations during face sculpting, or simulate realistic device aging processes. Furthermore, deterministic implicit sampling techniques (similar to DDIM (Song et al., 2021a) and GANs) can be employed to explore data compression and encryption.\nNevertheless, it is important to acknowledge the potential misuse and ethical concerns associated with data generation and encryption. For example, fake image videos of high-profile individuals raise ethical concerns, as they can be exploited to deceive, manipulate, or spread false information. Additionally, when it comes to encrypted data, our approach poses regulatory challenges. Encryption plays a crucial role in safeguarding sensitive information, but it can also hinder regulatory efforts aimed at combating illicit activities or ensuring public safety."
        }
    ],
    "title": "RESIDUAL DENOISING DIFFUSION MODELS",
    "year": 2023
}