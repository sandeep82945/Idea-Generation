{
    "abstractText": "Artificial Intelligence (AI) holds promise as a technology that can be used to improve government and economic policy-making. This paper proposes a new research agenda towards this end by introducing Social Environment Design, a general framework for the use of AI for automated policy-making that connects with the Reinforcement Learning, EconCS, and Computational Social Choice communities. The framework seeks to capture general economic environments, includes voting on policy objectives, and gives a direction for the systematic analysis of government and economic policy through AI simulation. We highlight key open problems for future research in AI-based policymaking. By solving these challenges, we hope to achieve various social welfare objectives, thereby promoting more ethical and responsible decision making.",
    "authors": [
        {
            "affiliations": [],
            "name": "Edwin Zhang"
        },
        {
            "affiliations": [],
            "name": "Sadie Zhao"
        },
        {
            "affiliations": [],
            "name": "Tonghan Wang"
        },
        {
            "affiliations": [],
            "name": "Safwan Hossain"
        },
        {
            "affiliations": [],
            "name": "Henry Gasztowtt"
        },
        {
            "affiliations": [],
            "name": "Stephan Zheng"
        },
        {
            "affiliations": [],
            "name": "David C. Parkes"
        },
        {
            "affiliations": [],
            "name": "Milind Tambe"
        },
        {
            "affiliations": [],
            "name": "Yiling Chen"
        }
    ],
    "id": "SP:860fcafa1ff940fdc97cba2a6f963a3675630cad",
    "references": [
        {
            "authors": [
                "J.P. Agapiou",
                "A.S. Vezhnevets",
                "E.A. Du\u00e9\u00f1ez-Guzm\u00e1n",
                "J. Matyas",
                "Y. Mao",
                "P. Sunehag",
                "R. K\u00f6ster",
                "U. Madhushani",
                "K. Kopparapu",
                "R Comanescu"
            ],
            "title": "Melting pot 2.0",
            "venue": "arXiv preprint arXiv:2211.13746,",
            "year": 2022
        },
        {
            "authors": [
                "K.J. Arrow"
            ],
            "title": "Social Choice and Individual Values",
            "year": 2012
        },
        {
            "authors": [
                "D. Aussel",
                "L. Brotcorne",
                "S. Lepaul",
                "L. von Niederh\u00e4usern"
            ],
            "title": "A trilevel model for best response in energy demand-side management",
            "venue": "European Journal of Operational Research,",
            "year": 2020
        },
        {
            "authors": [
                "J. Balaguer",
                "R. Koster",
                "A. Weinstein",
                "L. CampbellGillingham",
                "C. Summerfield",
                "M. Botvinick",
                "A. Tacchetti"
            ],
            "title": "Hcmd-zero: Learning value aligned mechanisms from data",
            "venue": "arXiv preprint arXiv:2202.10122,",
            "year": 2022
        },
        {
            "authors": [
                "S. Barocas",
                "M. Hardt",
                "A. Narayanan"
            ],
            "title": "Fairness and Machine Learning: Limitations and Opportunities",
            "year": 2023
        },
        {
            "authors": [
                "N. Basilico",
                "S. Coniglio",
                "N. Gatti"
            ],
            "title": "Methods for finding leader\u2013follower equilibria with multiple followers",
            "venue": "arXiv preprint arXiv:1707.02174,",
            "year": 2017
        },
        {
            "authors": [
                "M. Bichler",
                "M. Fichtl",
                "S. Heidekr\u00fcger",
                "N. Kohring",
                "P. Sutterer"
            ],
            "title": "Learning equilibria in symmetric auction games using artificial neural networks",
            "venue": "Nature machine intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "M. Bichler",
                "N. Kohring",
                "S. Heidekr\u00fcger"
            ],
            "title": "Learning equilibria in asymmetric auction games",
            "venue": "INFORMS Journal on Computing,",
            "year": 2023
        },
        {
            "authors": [
                "W. B\u00f6hmer",
                "V. Kurin",
                "S. Whiteson"
            ],
            "title": "Deep coordination graphs",
            "venue": "In Proceedings of the 37th International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "F. Brandt",
                "V. Conitzer",
                "U. Endriss",
                "J. Lang",
                "A.D. Procaccia"
            ],
            "title": "Handbook of computational social choice",
            "year": 2016
        },
        {
            "authors": [
                "G. Brero",
                "A. Eden",
                "D. Chakrabarti",
                "M. Gerstgrasser",
                "V. Li",
                "D.C. Parkes"
            ],
            "title": "Learning stackelberg equilibria and applications to economic design games",
            "venue": "arXiv preprint arXiv:2210.03852,",
            "year": 2022
        },
        {
            "authors": [
                "H.I. Calvete",
                "C. Gal\u00e9"
            ],
            "title": "Linear bilevel multi-follower programming with independent followers",
            "venue": "Journal of Global Optimization,",
            "year": 2007
        },
        {
            "authors": [
                "C. Cheng",
                "Z. Zhu",
                "B. Xin",
                "C. Chen"
            ],
            "title": "A multi-agent reinforcement learning algorithm based on stackelberg game",
            "year": 2017
        },
        {
            "authors": [
                "V. Conitzer",
                "T. Sandholm"
            ],
            "title": "Computing the optimal strategy to commit to",
            "venue": "In Proceedings of the 7th ACM conference on Electronic commerce,",
            "year": 2006
        },
        {
            "authors": [
                "M. Curry",
                "A. Trott",
                "S. Phade",
                "Y. Bai",
                "S. Zheng"
            ],
            "title": "Learning solutions in large economic networks using deep multi-agent reinforcement learning",
            "venue": "In Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems,",
            "year": 2023
        },
        {
            "authors": [
                "M.J. Curry",
                "U. Lyi",
                "T. Goldstein",
                "J.P. Dickerson"
            ],
            "title": "Learning revenue-maximizing auctions with differentiable matching",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2022
        },
        {
            "authors": [
                "J.M. de Figueiredo",
                "B.K. Richter"
            ],
            "title": "Advancing the Empirical Research on Lobbying",
            "venue": "Annual Review of Political Science,",
            "year": 2014
        },
        {
            "authors": [
                "Z. Duan",
                "H. Sun",
                "Y. Chen",
                "X. Deng"
            ],
            "title": "A scalable neural network for dsic affine maximizer auction design",
            "venue": "arXiv preprint arXiv:2305.12162,",
            "year": 2023
        },
        {
            "authors": [
                "P. D\u00fctting",
                "Z. Feng",
                "H. Narasimhan",
                "D.C. Parkes",
                "S.S. Ravindranath"
            ],
            "title": "Optimal auctions through deep learning: Advances in differentiable economics",
            "venue": "Journal of the ACM, Forthcoming",
            "year": 2023
        },
        {
            "authors": [
                "D.F. Engstrom",
                "D.E. Ho",
                "C.M. Sharkey",
                "Cu\u00e9llar",
                "M.-F"
            ],
            "title": "Government by Algorithm: Artificial Intelligence in Federal Administrative Agencies, February 2020",
            "venue": "URL https://papers.ssrn.com/ abstract=3551505",
            "year": 2020
        },
        {
            "authors": [
                "T. Fiez",
                "B. Chasnov",
                "L. Ratliff"
            ],
            "title": "Implicit learning dynamics in stackelberg games: Equilibria characterization, convergence analysis, and empirical study",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "J.N. Foerster",
                "G. Farquhar",
                "T. Afouras",
                "N. Nardelli",
                "S. Whiteson"
            ],
            "title": "Counterfactual multi-agent policy gradients",
            "venue": "In Thirty-Second AAAI Conference on Artificial Intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "J. Gan",
                "E. Elkind",
                "S. Kraus",
                "M. Wooldridge"
            ],
            "title": "Mechanism design for defense coordination in security games",
            "venue": "In Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems,",
            "year": 2020
        },
        {
            "authors": [
                "M. Gerstgrasser",
                "D.C. Parkes"
            ],
            "title": "Oracles & followers: Stackelberg equilibria in deep multi-agent reinforcement learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "C. Guestrin",
                "D. Koller",
                "R. Parr"
            ],
            "title": "Multiagent planning with factored mdps",
            "venue": "In Advances in neural information processing systems,",
            "year": 2002
        },
        {
            "authors": [
                "C. Guestrin",
                "M. Lagoudakis",
                "R. Parr"
            ],
            "title": "Coordinated reinforcement learning",
            "venue": "In ICML,",
            "year": 2002
        },
        {
            "authors": [
                "R. Hanson"
            ],
            "title": "Shall we vote on values, but bet on beliefs",
            "venue": "Journal of Political Philosophy,",
            "year": 2013
        },
        {
            "authors": [
                "A. Heywood"
            ],
            "title": "Political Ideologies: An Introduction",
            "year": 1998
        },
        {
            "authors": [
                "T.W. House"
            ],
            "title": "Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence",
            "year": 2023
        },
        {
            "authors": [
                "D. Ivanov",
                "I. Safiulin",
                "I. Filippov",
                "K. Balabaeva"
            ],
            "title": "Optimal-er auctions through attention",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "A.X. Jiang",
                "A.D. Procaccia",
                "Y. Qian",
                "N. Shah",
                "M. Tambe"
            ],
            "title": "Defender (mis) coordination in security games",
            "venue": "In Twenty-Third International Joint Conference on Artificial Intelligence,",
            "year": 2013
        },
        {
            "authors": [
                "C. Jin",
                "P. Netrapalli",
                "M. Jordan"
            ],
            "title": "What is local optimality in nonconvex-nonconcave minimax optimization",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Y. Kang",
                "T. Wang",
                "G. de Melo"
            ],
            "title": "Incorporating pragmatic reasoning communication into emergent language",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Y. Kang",
                "T. Wang",
                "Q. Yang",
                "X. Wu",
                "C. Zhang"
            ],
            "title": "Nonlinear coordination graphs",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "N. Kohring",
                "F.R. Pieroth",
                "M. Bichler"
            ],
            "title": "Enabling firstorder gradient-based learning for equilibrium computation in markets",
            "venue": "In International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "R. Koster",
                "J. Balaguer",
                "A. Tacchetti",
                "A. Weinstein",
                "T. Zhu",
                "O. Hauser",
                "D. Williams",
                "L. Campbell-Gillingham",
                "P. Thacker",
                "M Botvinick"
            ],
            "title": "Human-centred Mechanism Design with Democratic AI",
            "venue": "Nature Human Behaviour,",
            "year": 2022
        },
        {
            "authors": [
                "C. Li",
                "T. Wang",
                "C. Wu",
                "Q. Zhao",
                "J. Yang",
                "C. Zhang"
            ],
            "title": "Celebrating diversity in shared multi-agent reinforcement learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "R. Lowe",
                "Y. Wu",
                "A. Tamar",
                "J. Harb",
                "O.P. Abbeel",
                "I. Mordatch"
            ],
            "title": "Multi-agent actor-critic for mixed cooperative-competitive environments",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "H. Mao",
                "W. Liu",
                "J. Hao",
                "J. Luo",
                "D. Li",
                "Z. Zhang",
                "J. Wang",
                "Z. Xiao"
            ],
            "title": "Neighborhood cognition consistent multiagent reinforcement learning",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "P. Naghizadeh",
                "M. Liu"
            ],
            "title": "Voluntary participation in cyberinsurance markets",
            "venue": "In Workshop on the Economics of Information Security (WEIS),",
            "year": 2014
        },
        {
            "authors": [
                "D.C. Parkes",
                "M.P. Wellman"
            ],
            "title": "Economic reasoning and artificial intelligence",
            "venue": "doi: 10.1126/science.aaa8403. URL https://www.science.org/doi/abs/10",
            "year": 2015
        },
        {
            "authors": [
                "S. Patig"
            ],
            "title": "Measuring expressiveness in conceptual modeling",
            "venue": "In International Conference on Advanced Information Systems Engineering,",
            "year": 2004
        },
        {
            "authors": [
                "J. Perolat",
                "J.Z. Leibo",
                "V. Zambaldi",
                "C. Beattie",
                "K. Tuyls",
                "T. Graepel"
            ],
            "title": "A multi-agent reinforcement learning model of common-pool resource appropriation, 2017",
            "year": 2017
        },
        {
            "authors": [
                "A.D. Procaccia"
            ],
            "title": "Can approximation circumvent gibbardsatterthwaite",
            "venue": "In Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence,",
            "year": 2010
        },
        {
            "authors": [
                "T. Rashid",
                "M. Samvelyan",
                "C.S. Witt",
                "G. Farquhar",
                "J. Foerster",
                "S. Whiteson"
            ],
            "title": "Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "T. Rashid",
                "G. Farquhar",
                "B. Peng",
                "S. Whiteson"
            ],
            "title": "Weighted qmix: Expanding monotonic value function factorisation for deep multi-agent reinforcement learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "T. Sandholm"
            ],
            "title": "Automated Mechanism Design: A New Application Area for Search Algorithms",
            "venue": "Principles and Practice of Constraint Programming \u2013 CP",
            "year": 2003
        },
        {
            "authors": [
                "J. Schulman",
                "P. Moritz",
                "S. Levine",
                "M. Jordan",
                "P. Abbeel"
            ],
            "title": "High-dimensional continuous control using generalized advantage estimation",
            "venue": "arXiv preprint arXiv:1506.02438,",
            "year": 2015
        },
        {
            "authors": [
                "J. Schulman",
                "F. Wolski",
                "P. Dhariwal",
                "A. Radford",
                "O. Klimov"
            ],
            "title": "Proximal policy optimization algorithms",
            "venue": "arXiv preprint arXiv:1707.06347,",
            "year": 2017
        },
        {
            "authors": [
                "W. Shen",
                "P. Tang",
                "S. Zuo"
            ],
            "title": "Automated Mechanism Design via Neural Networks, May 2021",
            "venue": "URL http:// arxiv.org/abs/1805.03382",
            "year": 2021
        },
        {
            "authors": [
                "Z. Shi",
                "R. Yu",
                "X. Wang",
                "R. Wang",
                "Y. Zhang",
                "H. Lai",
                "B. An"
            ],
            "title": "Learning expensive coordination: An eventbased deep rl approach",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "A. Singh",
                "T. Jain",
                "S. Sukhbaatar"
            ],
            "title": "Learning when to communicate at scale in multiagent cooperative and competitive tasks",
            "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),",
            "year": 2019
        },
        {
            "authors": [
                "K. Son",
                "D. Kim",
                "W.J. Kang",
                "D.E. Hostallero",
                "Y. Yi"
            ],
            "title": "Qtran: Learning to factorize with transformation for cooperative multi-agent reinforcement learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "K. Tharakunnel",
                "S. Bhattacharyya"
            ],
            "title": "Leader-follower semi-markov decision problems: theoretical framework and approximate solution",
            "venue": "IEEE International Symposium on Approximate Dynamic Programming and Reinforcement Learning,",
            "year": 2007
        },
        {
            "authors": [
                "W. Thomson"
            ],
            "title": "Introduction to the Theory of Fair Allocation, pp. 261\u2013283",
            "year": 2016
        },
        {
            "authors": [
                "K. Wang",
                "L. Xu",
                "A. Perrault",
                "M.K. Reiter",
                "M. Tambe"
            ],
            "title": "Coordinating followers to reach better equilibria: Endto-end gradient descent for stackelberg games",
            "venue": "arXiv preprint arXiv:2106.03278,",
            "year": 2021
        },
        {
            "authors": [
                "T. Wang",
                "J. Wang",
                "C. Zheng",
                "C. Zhang"
            ],
            "title": "Learning nearly decomposable value functions via communication minimization",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "T. Wang",
                "H. Dong",
                "V. Lesser",
                "C. Zhang"
            ],
            "title": "ROMA: Multi-agent reinforcement learning with emergent roles",
            "venue": "In Proceedings of the 37th International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "T. Wang",
                "T. Gupta",
                "A. Mahajan",
                "B. Peng",
                "S. Whiteson",
                "C. Zhang"
            ],
            "title": "RODE: Learning roles to decompose multi-agent tasks",
            "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),",
            "year": 2021
        },
        {
            "authors": [
                "T. Wang",
                "L. Zeng",
                "W. Dong",
                "Q. Yang",
                "Y. Yu",
                "C. Zhang"
            ],
            "title": "Context-aware sparse deep coordination graphs",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "T. Wang",
                "P. D\u00fctting",
                "D. Ivanov",
                "I. Talgam-Cohen",
                "D.C. Parkes"
            ],
            "title": "Deep contract design via discontinuous piecewise affine neural networks",
            "venue": "arXiv preprint arXiv:2307.02318,",
            "year": 2023
        },
        {
            "authors": [
                "Y. Wang",
                "B. Han",
                "T. Wang",
                "H. Dong",
                "C. Zhang"
            ],
            "title": "Dop: Off-policy multi-agent decomposed policy gradients",
            "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),",
            "year": 2021
        },
        {
            "authors": [
                "M. Wen",
                "J. Kuba",
                "R. Lin",
                "W. Zhang",
                "Y. Wen",
                "J. Wang",
                "Y. Yang"
            ],
            "title": "Multi-agent reinforcement learning is a sequence modeling problem",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Y. Wen",
                "Y. Yang",
                "R. Luo",
                "J. Wang"
            ],
            "title": "Modelling bounded rationality in multi-agent interactions by generalized recursive reasoning",
            "venue": "arXiv preprint arXiv:1901.09216,",
            "year": 2019
        },
        {
            "authors": [
                "Y. Wen",
                "Y. Yang",
                "R. Luo",
                "J. Wang",
                "W. Pan"
            ],
            "title": "Probabilistic recursive reasoning for multi-agent reinforcement learning",
            "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),",
            "year": 2019
        },
        {
            "authors": [
                "J. Yang",
                "E. Wang",
                "R. Trivedi",
                "T. Zhao",
                "H. Zha"
            ],
            "title": "Adaptive incentive design with multi-agent meta-gradient reinforcement learning",
            "venue": "arXiv preprint arXiv:2112.10859,",
            "year": 2021
        },
        {
            "authors": [
                "Q. Yang",
                "W. Dong",
                "Z. Ren",
                "J. Wang",
                "T. Wang",
                "C. Zhang"
            ],
            "title": "Self-organized polynomial-time coordination graphs",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "H. Zhang",
                "Y. Chen",
                "D. Parkes"
            ],
            "title": "A general approach to environment design with one agent",
            "venue": "In Proceedings of the 21st International Joint Conference on Artificial Intelligence,",
            "year": 2002
        },
        {
            "authors": [
                "H. Zhang",
                "Y. Xiao",
                "L.X. Cai",
                "D. Niyato",
                "L. Song",
                "Z. Han"
            ],
            "title": "A multi-leader multi-follower stackelberg game for resource management in lte unlicensed",
            "venue": "IEEE Transactions on Wireless Communications,",
            "year": 2016
        },
        {
            "authors": [
                "S. Zheng",
                "A. Trott",
                "S. Srinivasa",
                "D.C. Parkes",
                "R. Socher"
            ],
            "title": "The AI Economist: Taxation policy design via two-level deep multiagent reinforcement learning, 2022",
            "venue": "URL https://www.science.org/doi/ abs/10.1126/sciadv.abk2607",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Economic policy formulation is a domain fraught with complexity, with traditional economic models providing limited foresight into the outcomes of policy decisions. Policymakers must not only understand the immediate implications of individual policies but also their aggregate and longterm effects. In addition, human policy-maker incentives are often not aligned with the interests of the general public, and may instead prioritize special interests or reelection (de Figueiredo & Richter, 2014). In light of this, AI-based approaches to policy design that can simulate economies and target different objectives, hold the potential for improved policy understanding and formulation (Zheng et al., 2022; Koster et al., 2022).\nIn an era where AI is gaining increasing attention in governments (House, 2023; Engstrom et al., 2020), it is timely to understand its potential influence on future policy-making.\n1Harvard University 2Founding 3Oxford University 4Asari AI 5Google Research. Correspondence to: Edwin Zhang <ezhang@g.harvard.edu>.\nFigure 1: The proposed framework. The process begins with voting, where human or AI players report preferences on social welfare objectives to a voting mechanism (1). This defines an objective for the Principal, who designs a parameterized N -player Partially Observable Markov Game (POMG) (2). The players are the same as the voters. This POMG unfolds over several timesteps T (3). Following the POMG, game state information is extracted to initiate a new round of voting, with the last POMG state used as the first game state of the new round. This whole process is repeated for \u03c4 timesteps.\nIdeally, such a framework should satisfy the following desiderata:\n1. Alignment of policy-makers to the values of constituents, whilst ensuring fair and equitable representation (Barocas et al., 2023).\n2. Sufficient model expressivity (Patig, 2004) to accurately represent the intricate governance structures found in the real world, capturing the subtleties and variances of socio-economic interactions.\n3. Balance expressiveness with computational tractability, making it feasible to scale to systems with a large number of agents.\n4. Build theoretical understanding, enabling systematic analysis and offering a new lens on complex economic models.\nIn this paper, we propose a new framework, Social Environment Design, that lays out an agenda in making progress\nar X\niv :2\n40 2.\n14 09\n0v 1\n[ cs\n.A I]\n2 1\nFe b\n20 24\ntowards these desiderata. In our framework, illustrated in Figure 1, we suggest addressing the concern of a misaligned policy-maker with \u201cVoting on Values (Hanson, 2013),\u201d coupled with a Principal policy-maker who seeks to achieve suggested policy goals. We capture the complexity of a general economic environment whilst maintaining computational tractability by modeling the economy as a Partially Observable Markov Game (POMG), which maintains a fixed observation space for each agent. Finally, we structure our framework as repeatedly finding Stackelberg Equilbria, enabling theoretical understanding by allowing reduction to simpler subproblems.\nIn advancing a research agenda situated in connection with the RL, EconCS, and Computational Social Choice communities, we discuss several open problems of practical and theoretical interest. By introducing this framework, we open a dialogue on AI\u2019s application to economic policy design, aspiring to someday help leverage AI to assist policymakers in enhancing economic resilience and governance effectiveness.\nIn summary, we list our core contributions : 1) We propose the Social Environment Design framework to enable future research in AI-led policymaking in complex economic systems; 2) We release a core implementation of our framework as a Sequential Social Dilemma Environment along with code; and 3) We provide a characterization of open problems, along with prospective solution concepts and algorithmic approaches to forward the dialogue on AI\u2019s application in economic policy design."
        },
        {
            "heading": "2. Preliminaries",
            "text": "Here we give some preliminaries on several foundational games and solution concepts. Definition 2.1. A (n+ 1)-player Stackelberg-Nash Game S = (n,m,X ,Y,u) comprises one player called the leader and n \u2208 N \\ {0} players called followers. In a StackelbergNash game, the leader first commits to an action x \u2208 X from action space X \u2282 Rm. Then, having observed the leader\u2019s action, each follower i \u2208 [n], responds with an action yi in their action space Yi \u2282 Rm. We define the followers\u2019 joint action space Y =\u00d7i\u2208[n] Yi. We refer to a collection of actions y = (y1, \u00b7 \u00b7 \u00b7 , yn) \u2208 Y as a followers\u2019 action profile, and to a collection (x, y) \u2208 X \u00d7 Y as an action profile.\nAfter all players choose an action, the leader receives payoff uo(x,y) \u2208 R, while each follower i \u2208 [n] receives payoff ui(x,y) \u2208 R. Each player i \u2208 [n] aims to maximize her payoff, and the leader aims to maximize her payoff assuming the followers will best respond.\nFixing the leader\u2019s action x \u2208 X , a Stackelberg Nash game S induces a lower-level Nash game GS =\n(n,m,Y,u\u22120(x, \u00b7)) among the followers. Definition 2.2. A Partially Observable Markov Game (POMG) M with n agents is a tuple (S,A, T, r,\u2126, B, \u03b3, \u00b50). Here, S is a shared state space for all agents; A = \u00d7i\u2208[n] Ai is the joint action space; T : S \u00d7 S \u00d7 A \u2192 [0, 1] is a stochastic transition function; r : S \u00d7 A \u2192 Rn is the reward function with r = (r1, \u00b7 \u00b7 \u00b7 , rn); \u2126 =\u00d7i\u2208[n] \u2126i is the joint observation space; B : \u2126\u00d7S\u00d7A \u2192 [0, 1] is the stochastic observation function; \u03b3 \u2208 [0, 1) is a discount factor; \u00b50 \u2208 \u2206(S) is the initial state distribution. An agent\u2019s behavior in this game is characterized by its policy \u03c0i : \u2126 \u2192 A, which maps observations to actions. Definition 2.3. A (n + 1)-player Stackelberg-Markov Game S = (n,m,\u03a6,\u03a0,u) comprises one player called the leader and n \u2208 N \\ {0} players called followers. In a Stackelberg-Markov game, the leader first commits to an action \u03d5 \u2208 \u03a6 from action space \u03a6 \u2282 Rm which induces a n-player low-level (Partially Observable) Markov Game M\u03d5 = (S,A\u03d5, T\u03d5, r\u03d5,\u2126\u03d5, B\u03d5, \u03b3\u03d5, \u00b5\u03d50 ). Then, having observed the leader\u2019s action, each follower i \u2208 [n], responds with an policy \u03c0i : \u2126 \u2192 Ai in their policy space \u03a0i. We define the followers\u2019 joint action space \u03a0 =\u00d7i\u2208[n] \u03a0i. We refer to a collection of policies \u03c0 = (\u03c01, \u00b7 \u00b7 \u00b7 , \u03c0n) \u2208 \u03a0 as a followers\u2019 policy profile.\nAfter all players choose an action, the leader receives payoff uo(\u03d5, \u03c0) \u2208 R, while each follower i \u2208 [n] receives payoff ui(\u03d5, \u03c0) = EM \u03d5,\u03c0[ \u2211\u221e t=0(\u03b3 \u03d5)tr\u03d5(st, at)] \u2208 R. Each player i \u2208 [n] aims to maximize her payoff, and the leader aims to maximize her payoff assuming the followers will best respond.\nFor all followers i \u2208 [n], we define the \u03b4-bestresponse correspondence BR\u03b4i (\u03d5, \u03c0\u2212i) = {\u03c0i \u2208 \u03a0i | ui(\u03d5, \u03c0) \u2265 max\u03c0i\u2208\u03a0i ui(\u03d5, (\u03c0i, \u03c0\u2212i)) \u2212 \u03b4} and the joint \u03b4-best-response correspondence BR\u03b4(\u03d5, \u03c0) = \u00d7i\u2208[n] BR\u03b4i (\u03d5, \u03c0\u2212i). Definition 2.4. A (\u03b5, \u03b4)-strong Stackelberg-MarkovNash equilibrium (SSMNE) in a Stackelberg-Markov game S = (n,m,\u03a6,\u03a0,u) is an action profile (\u03d5\u2217, \u03c0\u2217) \u2208 \u03a6\u00d7\u03a0 such that u0(\u03d5\u2217, \u03c0\u2217) \u2265 max\u03d5\u2208\u03a6 maxBR\u03b4 u0(\u03d5, \u03c0)\u2212 \u03b5 and ui(\u03d5\u2217, \u03c0\u2217) \u2265 max\u03c0i\u2208\u03a0i ui(\u03d5\u2217, (\u03c0i, \u03c0\u2217\u2212i))\u2212 \u03b4, for all i \u2208 [n]. Definition 2.5. A (One-Shot) Mechanism Design problem P = (n,m, T , S, t,u, u0, f) comprises of n agents, each i \u2208 [n] owns a private type ti \u2208 Ti from a set of possible types Ti \u2282 Rm .\nAn agent\u2019s preferences over outcomes s \u2208 S, for a set S of outcomes, can be expressed in terms of a utility function that is parameterized by the type. Let ui(s, ti) denote the utility of agent i for outcome s \u2208 S given type ti. A strategy (policy in RL) si : Ti \u2192 Ai is a complete decision\nrule, that defines the action an agent will select in every distinguishable state of the world. Let ai = si(ti) \u2208 Ai denote the action of agent i given type ti, where Ai is the set of all possible actions available to agent i.\nA mechanism M = ({Ai}i\u2208[n], g) defines the set of actions Ai available to each agent i, and an outcome rule g : \u00d7i\u2208[n] Ai \u2192 S, such that g(a) is the outcome implemented by the mechanism for action profile a = (a1, \u00b7 \u00b7 \u00b7 , an). u0 : M\u00d7\u00d7i\u2208[n] Ai \u2192 R is a principal objective function, where u0(M,a) represents the expected utility/revenue of the principal when mechanism designer chooses mechanism M and agents choose action profile a. Note that u0 is an alternative way of defining the social choice function. The goal of the mechanism designer is to design a mechanism M = ({Ai}i\u2208[n], g) \u2208 M that maximizes u0(M,a\n\u2217), where strategy profile a\u2217 = (a\u22171, \u00b7 \u00b7 \u00b7 , a\u2217n) is an (Nash, Bayesian-Nash, dominant-strategy) equilibrium to the game induced by M ."
        },
        {
            "heading": "3. Formal Definition of Social Environment Design Game",
            "text": "Definition 3.1. A Social Environment Design Game S = (\u03a6, P, \u03d50, D, \u03b4,\u0398,O, f) is a one-leader-nfollower onlinea Stackelberg-Markov Game, where\n\u2022 \u03a6 \u2286 Rk is the principal action space;\n\u2022 P : \u03a6 7\u2192 M\u03d5 is a policy implementation map that maps from a principal action \u03d5 \u2208 \u03a6 to a parameterized POMG M\u03d5 = (S,A\u03d5, T\u03d5, r\u03d5,\u2126\u03d5, B\u03d5, \u03b3\u03d5, \u00b5\u03d50 );\n\u2022 \u03d50 \u2208 \u03a6 is some initial action;\n\u2022 D : \u03a6 \u00d7 \u03a6 7\u2192 R\u22650 is a divergence measure on the leader action space;\n\u2022 \u03b4 > 0 is the divergence constraint;\n\u2022 \u0398 \u2286 R(n+1)\u00d7m is the type space.\n\u2022 O = {Oi}i\u2208[m] is some set of predefined social welfare functions, where each O maps \u03a6\u00d7\u03a0 7\u2192 R. We give examples of several possible choices of objectives below in Social Welfare Examples. \u03a0 here refers to the set of all possible policy profiles in the parameterized POMG;\n\u2022 f : \u0398 7\u2192 O is a social choice function representing the voting mechanism.\naHere, online means that the Stackelberg-Markov Game is repeatedly played, with the first state of a new round made equal to the final state of the last round.\nWe define the Social Environment Design Game formally as repeatedly finding a Stackelberg Equilibrium in a Markov Game (Gerstgrasser & Parkes, 2023; Brero et al., 2022), iterated over several rounds of voting.\nAt a high level, we frame the economic design problem as a Stackelberg game between the policy designer and economic participants. The economic participants first vote for a given objective, or values to optimize for. Subsequently, the Principal (leader) attempts to maximize this objective by designing the rules of an economic system, which induce an environment for the participants. We model this environment as a Partially Observable Markov Game (POMG) with theparticipants as the agents. We refer to this as the Social Environment Design Game because it generalizes mechanism design in a number of ways; e.g., it involves voting on goals, and it involves the design of an economic policy for an economic environment in which agents take actions and report types.\nFurther analysis and breakdown of Definition 3.1.\nFirst we make a note regarding our type space. Since we define the first row of a specific type instantiation \u03b8 \u2208 \u0398 to be the type of the principal, \u0398 has (n + 1) rows. We thus refer to \u03981 to be the type space of the principal and \u0398\u22121 to be the type space of all participants. In addition, \u0398 can be added to the state space of the POMG, which allows dynamic types that change over time in response to the state of the game. We do not allow the Principal to directly manipulate or observe the state space. Thus, we can embed the type space within the state space to hide it from the Principal. Even with elements of the POMG that the Principal does have control over, such as the state transition function T\u03d5, one can enforce hard constraints on how much power the Principal has to change the function explicitly through the divergence D or implicitly through the implementation map P .\nWe remark that both the infinite-horizon and finite-horizon version of the Social Environment Design Game can be considered. In contrast to standard Reinforcement Learning (RL), we do not need to introduce a discount factor for the infinite-horizon version, as we consider the Principal only maximizing for the objective at the current voting round since our model\u2019s objective changes at each round. This is perhaps a naive objective, and other continual objectives could instead be considered. Exploring the tradeoffs between the greedy objective and more complex continual objectives is left as an important open problem for future work. In the finite horizon case, we add an additional time horizon T to our game. We now proceed to a detailed breakdown of our game.\nThe Social Environment Design Game can be cleanly divided into a Voting Mechanism and Stackelberg Game,\nwhich is played with the Principal\u2019s objective determined by the Voting Mechanism.\nDefinition 3.2. The Voting Mechanism is defined as V = (O, f,\u0398).\nWe use the standard axiomatic model (Arrow, 2012), where O is the set of alternatives, f is the social choice function, and \u0398 is the type space, or set of all preference profiles. Intuitively, a specific agent i\u2019s type \u03b8i for row i in \u03b8 \u2208 \u0398, can be thought of as some latent vector which represents the agent\u2019s values. This type contains all information necessary for recovering a partial ordering over alternatives, a more specific way of defining preferences. The goal of the Voting Mechanism is then to define a objective for the Principal to optimize, given these types. To do so, we define the Voting Mechanism f and ask the players for a preference report \u03b8\u22121 \u2208 \u0398\u22121, which does not necessarily have to be truthful. It remains an open problem whether some notion of approximate incentive compatibility can be achieved by the principal. The Voting Mechanism then computes the objective O = f(\u03b81, \u03b8\u22121) as a result of the vote. Here, we set \u03b81 \u2208 \u03981 to be the preferences of the Principal. Importantly, the objective function includes the full \u03b8, which allows expressing preferences of the principal if one wished to encoded a form of \u201dmoral objectivity\u201d, or other biases. We also make this modeling choice for generality, as it allows our model to express mechanisms such as auctions where the objective of the principal may be entirely selfish and not depend at all on the participant\u2019s types.\nSocial Welfare Examples. Examples of social welfare functions that could be included in the voting set are the Utilitarian objective O(\u03d5, \u03c0) = \u2211 i J(\u03c0 \u03d5 i ), where J is the\nexpected discounted return J = \u2211\nt(\u03b3 \u03d5)tr\u03d5i (st, ai,t, a\u2212i,t),\n\u03c0 is the tuple of all agents \u03c0 = (\u03c0i)i\u2208[n], and \u03c0i are singular agents that map \u2126\u03d5i \u2192 A \u03d5 i . Other possible choices include\nthe Nash Welfare objective O = ( \u03a0iJ(\u03c0 \u03d5 i ) )1/n , as well as the Egalitarian objective O = mini J(\u03c0 \u03d5 i ). These are perhaps the most commonly discussed objectives, but bespoke or custom welfare functions could also be considered and added to the set of alternatives.\nDefinition 3.3. The Stackelberg Game I = (\u03a6, P,D, \u03b4, \u03d50) is a Stackelberg-Markov Game.\nThe Stackelberg Game game is played subsequently after the Voting Mechanism, and can be thought of as a single timestep of the full game. The Principal (leader) will choose action \u03d5 \u2208 \u03a6 which induces a parameterized Induced Economy M\u03d5 = (S,A\u03d5, T\u03d5, r\u03d5,\u2126\u03d5, B\u03d5, \u03b3\u03d5, \u00b5\u03d50 ) through the policy implementation map P : \u03d5 7\u2192 M\u03d5. Note that if agent preferences change over time, this can be modeled by adding agent types into the state space of the POMG. The transition function T would then be able to express changes\nin preferences over time. Thus, the objective of the leader in the Stackelberg Game game is to design a POMG, given the objective O decided prior in the Voting Mechanism:\nmax \u03d5 O(\u03d5, \u03c0)\ns.t. D(\u03d50, \u03d5) \u2264 \u03b4\n\u00b5 P (\u03d5) 0 = \u2206(sT ).\n(1)\nAgain, \u03c0 here is the tuple of all agents \u03c0 = (\u03c0i)i\u2208[n], and \u03c0i individual agents that map \u2126\u03d5i \u2192 A \u03d5 i . Our notation \u00b5 P (\u03d5) 0 denotes the \u00b50 of the tuple P (\u03d5), and \u2206(st) refers to a Delta Dirac distribution centered on sT . Therefore, the second constraint \u00b5P (\u03d5)0 = \u2206(sT ) forces the \u03d5 to choose a POMG that has the same initial state as the terminal state of the last round, so that continuity is kept between rounds.\nLastly, we remark that this constrained optimization can also be transformed into an unconstrained problem by using an additional reparameterization R : \u03be 7\u2192 \u03a6\u0302, where \u03be \u2208 \u039e := RL and \u03a6\u0302 := {\u03d5 | D(\u03d50, \u03d5) \u2264 \u03b4}. The optimization can then proceed in RL with no constraints. In this case, the Stackelberg game would reduce to I = (\u039e, P \u2032), where P \u2032 = P \u25e6 R. Definition 3.4. The Induced Economy is a Partially Observable Markov Game M\u03d5 = (S,A\u03d5, T\u03d5, r\u03d5,\u2126\u03d5, B\u03d5, \u03b3\u03d5, \u00b5\u03d50 ).\nFinally, the Induced Economy is defined as the POMG produced as the output of the principal. Agents within the POMG interact with one another and attempt to maximize their utility according to their true preferences. The n economic participants (followers) will play strategically in the parameterized POMG M\u03d5. At each step t of the game, every follower i chooses an action ai,t from their action space Ai, the game state evolves according to the joint action at = (a1,t, \u00b7 \u00b7 \u00b7 , an,t) and the transition function T , and agents receive observations and reward according to B and r. An agent\u2019s behavior in the game is characterized by its policy \u03c0i : \u2126 \u03d5 i \u2192 A \u03d5 i , which maps observations to actions. Each follower in the POMG M\u03d5 individually seeks to maximize its own (discounted) total return\u2211\nt(\u03b3 \u03d5)tr\u03d5i (st, ai,t, a\u2212i,t)."
        },
        {
            "heading": "4. Example: Apple Picking Game",
            "text": "In order to give a motivating example for how preference elicitation for the principal in Social Environment Design can be used to align policy-maker incentives, we have created a Sequential Social Dilemma Game inspired by the Harvest Game proposed in Perolat et al. (2017). The game is illustrated in Figure 2. The aim in Harvest Game is to collect apples, with each apple yielding a reward. If all apples\nin an area are harvested, they never grow back. The dilemma arises when individual self-interest drives rapid harvesting, which could permanently deplete resources. Thus, agents must sacrifice personal benefit and cooperate for collective well-being. One potential solution to this dilemma is through the use of a central government that taxes and redistributes apples. Thus, we have created a new game inspired by Zheng et al. (2022) in which a principal designs tax rates on apple collection and players vote on Utilitarian (productivity) vs. Egalitarian (equality) objectives for the principal. As players interact within this evolving environment, the principal faces the challenge of crafting policies that balance immediate economic incentives with sustainability goals. In order to achieve this, the principal must foster cooperation among players, guiding them towards the Pareto-Efficient equilibrium that the players have chosen.\nTo build intuition for how the Apple Picking Game maps onto the theoretical framework described in section 3, we give a more formal definition of the game here. Recall the definition of a Social Environment Design Game S = (\u03a6, P, \u03d50, D, \u03b4,\u0398,O, f). The action space for the environment designer \u03a6 is defined as tax weights 0 \u2264 \u03a6i \u2264 1, determining the percentage of income to be taxed for each bracket. For simplicity we evenly redistribute the taxes, and set the number of tax brackets to three. In this environment, the Principal can only change the reward function of the induced POMG, so M\u03d5 = (S,A, T, r\u03d5,\u2126, B, \u03b3, \u00b50). The\ntype of each agent is defined as (\u03c3, \u03b2), where \u03c3 refers to the selfishness of the agent and \u03b2 refers to the trust the agent holds in the Principal. In some sense this is a rough approximation for the political spectrum (Heywood, 1998). Finally, let ai be the number of apples collected for a given agent i. We can now define the policy implementation map P , which in this case reduces to the parameterization of the reward function:\nri(a, \u03d5) = \u03b2irextrinsic(a, \u03d5) + (1\u2212 \u03b2i)ri,intrinsic(a). (2)\nHere, the intrinsic reward is an average between the apples an agent collects and the apples all other agents collect within its field of view weighted by the selfishness of the agent. On the other hand, the extrinsic reward is the amount of apples after tax an agent collects plus an equal share of the redistributed total tax:\nri,intrinsic(a) = \u03c3ia+ (1\u2212 \u03c3i)( \u2211 j aj \u2212 a),\nrextrinsic(a, \u03d5) = (a\u2212 T (a, \u03d5)) + 1\nn \u2211 j T (aj , \u03d5),\nwhere tax T (a, \u03d5) = B\u22121\u2211 b=0 \u03d5b \u00b7 ((\u03c4b+1 \u2212 \u03c4b)1[a > \u03c4b+1]\n+ (a\u2212 \u03c4b)1[\u03c4b < a \u2264 \u03c4b+1]).\nHere, [\u03c4b, \u03c4b+1] refer to the tax brackets. Importantly, the principal can only incentivize agents through the extrinsic reward and cannot directly observe the intrinsic reward. The degree to which the principal holds power over the agent depends on the agent\u2019s trust or belief in the principal. We sample both selfishness and trust uniformly over [0, 1], and keep them fixed during training. Allowing them to change over time either randomly or in some fashion dependent on the performance of the Principal is left for future work. The objective space of the Principal is defined as O = {\u03b7 \u2211 i,t ai,t + (1 \u2212 \u03b7)mini \u2211 t a(i, t) | 0 \u2264 \u03b7 \u2264 1}, an interpolation between the Utilitarian and Egalitarian objective. A simple social choice function f(\u03c3) = \u03b7 can be defined as the average of agent selfishness: f(\u03c3) = 1 n \u2211 i \u03c3i. In this setting, we leave the Principal optimization unconstrained and thus do not need to define D or \u03b4. \u03d50 is initialized to 0, or no tax.\nWe run several tax periods per voting round, and at the end of each the principal decides on a new tax rate, for each bracket, for the next period - as well as calculating, applying and redistributing tax to the players for that entire period, delivered in the players\u2019 final extrinsic reward. We release a high quality version of our code designed for fast experimentation and further research in the supplementary material, and include environment hyperparameters and training details in Appendix A. We do not include results as the primary purpose of this paper is to propose a future research agenda\nand illustrate open problems within it. Preliminary results were promising, but further analysis is warranted."
        },
        {
            "heading": "5. Challenges and Open Problems",
            "text": "Based on the AI-led economic policy-making framework presented, the following key open problems of our framework are proposed for further exploration: Preference aggregation and democratic representation in voting mechanisms is a complex challenge that requires advanced algorithms to reflect collective preferences while respecting minority views, as well as ensuring that the simulated population is representative and their preferences correctly modeled. Modeling human behavior within the simulator is another key challenge, and points towards possibly incorporating bounded-rationality into MARL (Wen et al., 2019a) or role-based modeling (Wang et al., 2020; 2021b). To ensure responsible AI governance and accountability, responsible oversight mechanisms must be established. Furthermore, exploring socioeconomic interactions within these systems is critical, especially in understanding and deriving the conditions for convergence to and definition of the Principal\u2019s objective. As our framework is positioned within a continual learning setting, it is important to redefine what an optimal Principal looks like in this context. Finally, scaling laws of the framework should be analyzed in order to fully model real-world complexities. Can the framework handle simulating economies with thousands or millions of agents? What is the role of scale? When is simulation useful, and when does it fail? These remain important open problems for future research. We give a more detailed outline for several of the above problems for further consideration.\nPreference Aggregation and Democratic Representation.\nAggregation algorithms within the Voting Mechanism: The development of sophisticated algorithms that can effectively aggregate disparate and potentially conflicting preferences of diverse agent populations is a significant challenge. These algorithms must ensure that the outcomes represent collective preferences without overwhelming the minority views.\nIncorporating diverse decision-making models: The framework must be flexible enough to respect various cultural, ethical, and socioeconomic decision-making paradigms that different groups of agents might exhibit. In addition, such agents should imitate humans well, which we expand on further in the next section.\nModeling Human Behavior.\nBounded Rationality: Human decision-making in economic settings often demonstrates bounded rationality, where decisions are made based on satisficing rather than optimizing behavior. Further research is required to develop AI agents that can capture such nuances in human decision-making.\nCognitive and Behavioral Biases: Human economic behavior is influenced by a variety of cognitive biases. For instance, time-inconsistent preferences can lead to procrastination and problems with self-control, and loss aversion can skew risk preferences. AI agents within this framework need to capture these biases for accurate representation of human economic behavior.\nInteraction and Network Effects: Humans do not make economic decisions in isolation; their decisions are profoundly influenced by their interactions with others. This opens another avenue of research in modeling these network effects accurately within the agent behavior models. Higher order effects are oftentimes essential to understanding the behavior real-world systems."
        },
        {
            "heading": "AI Governance and Accountability.",
            "text": "Transparent decision-making processes: AI systems involved in policy-making need to have their decision-making processes be fully transparent. The creation of interpretable AI models that can provide explanations for suggested policies is essential for trust and accountability.\nLegal and ethical frameworks for AI decisions: There is an urgent need to establish legal and ethical frameworks that delineate the responsibilities and liabilities associated with AI-driven decision-making. These frameworks should set guidelines for what constitutes fair and lawful AI behavior in an economic context.\nOversight and human-AI collaboration: Establishing effective oversight mechanisms that involve both AI and human collaboration is critical. The role of human experts in supervising and guiding AI decisions, and their ability to intervene when AI-driven policies deviate from desired outcomes is still to be determined.\nConvergence to Desired Outcomes.\nExistence and characterization of forms of convergence or equilbria: Can we characterize the conditions under which an equilibrium will exist in such complex socioeconomic interactions. The uniqueness or multiplicity of equilibria and the conditions under which they are attained are also interesting to study. Also, conventional game-theoretic equilibria may not be the right object of study, as empirically these economic systems may never converge to a single, stable behavior.\nInfluence of dynamic changes on convergence points: The complex dynamics of economic systems call for a deep understanding of the sensitivity of equilibria to shocks and changes in the environment and agent behavior from variables that may have been unforeseen by the principal. Ensuring the robustness and stability of the principal to be able to recover from such shocks is also of importance.\nScaling Laws and Computational Efficiency.\nScaling up the model to larger systems: The proposed framework needs to be scaled to simulate economies of increasingly complexities. This comprises accommodating an increasing number of agents and more intricate interactions among them. Scaling laws of the model parameters and the computational resources required need to be examined.\nEfficient learning and decision-making algorithms: Efficient algorithms for learning agent behavior and optimizing the policy design are crucial for the practicality of the framework. Particularly, the principal must be sample efficient, as every step it takes induces an entire MARL optimization.\nMassive parallelization: To tackle real-world complex systems, embracing the advantage of high-performance computing is necessary. This includes implementing the framework with massively parallel computations for both the learning and the decision-making processes. Techniques for splitting these processes into smaller tasks that can be processed simultaneously, as well as the efficient management of these tasks, represent challenging aspects to be addressed."
        },
        {
            "heading": "6. Related Work",
            "text": "The concept of environment design was first proposed by Zhang et al. (2009) and focused on the single agent setting. In contrast, our framework resides between various strands of research, including but not limited to economic policy design, Stackelberg game learning, multi-agent reinforcement learning, mechanism design, and computational social choice. In this section, we delve into a comprehensive exploration of its connections with prior research."
        },
        {
            "heading": "6.1. Economic Policy Design and Simulation",
            "text": "Several approaches to automated economic policy design have been proposed in the past (Liu et al., 2022; Curry et al., 2023; Yang et al., 2021), and how usage of AI may span both participation in and design of economic systems (Parkes & Wellman, 2015). Here we cite several that are most related to our proposed framework and research agenda. Perhaps most related to our approach is the Human Centered Mechanism Design line of work from Koster et al. (2022); Balaguer et al. (2022). They propose learning mechanisms from behavioral models trained on human data, with the mechanism objective attempting to satisfy a majoritarian vote of the human participants. However, their work differs from ours in several key ways; firstly, they do not consider a fully general economic environment and limit their scope only to a generalization of the linear public goods setting. In other words, our framework encompasses Environment Design whilst theirs encompasses only Mechanism Design. Secondly, the voting that is defined within their framework is taken over actual mechanisms proposed by the designer and\nis by majority, whereas our voting is taken explicitly over Principal objectives and does not specify a majority vote, which allows potentially addressing issues such tyranny of the masses in future work. A more general game environment is illustrated in the AI Economist (Zheng et al., 2022), although they make strong assumptions on the goal of the Principal is and do not allow participants to adjust it."
        },
        {
            "heading": "6.2. Stackelberg Game",
            "text": "From the perspective of the Principal, it plays a Stackelberg game with agents of different types. Stackelberg games model many real-world problems that exhibit a hierarchical order of play by different players, including taxation (Zheng et al., 2022), security games (Jiang et al., 2013; Gan et al., 2020), and commercial decision-making (Naghizadeh & Liu, 2014; Zhang et al., 2016; Aussel et al., 2020). In the simplest case, a Stackelberg game contains one leader and one follower. For these games with discrete action spaces, Conitzer & Sandholm (2006) show that linear programming approaches can obtain Stackelberg equilibria in polynomial time in terms of the pure strategy space of the leader and follower. To find Stackelberg equilibria in continuous action spaces, Jin et al. (2020); Fiez et al. (2020) propose the notion of local Stackelberg equilibria and characterize them using first- and second-order conditions. Moreover, Jin et al. (2020) show that common gradient descent-ascent approaches can converge to local Stackelberg equilibria (except for some degenerate points) if the learning rate of the leader is much smaller than that of the follower. Fiez et al. (2020) give update rules with convergence guarantees. Different from these works, in this paper, we consider Stackelberg games with multiple followers.\nMore sophisticated than its single-follower counterpart, unless the followers are independent (Calvete & Gale\u0301, 2007), computing Stackelberg equilibria with multiple followers becomes NP-hard even when assuming equilibria with a special structure for the followers (Basilico et al., 2017). Recently, Wang et al. (2021a) propose to deal with an arbitrary equilibrium which can be reached by the follower via differentiating though it. Gerstgrasser & Parkes (2023) proposes a meta-learning framework among different policies of followers to enable fast adaption of the principal, which builds upon prior work done by Brero et al. (2022) who first introduced the Stackelberg-POMDP framework.\nMulti-agent reinforcement learning holds the promise to extend Stackelberg learning to more general and realistic problems. Tharakunnel & Bhattacharyya (2007) propose Leader-Follower Semi-Markov Decision Process to model the sequential Stackelberg learning problem. Cheng et al. (2017) propose Stackelberg Q-learning but without any convergence guarantee. Shu & Tian (2019); Shi et al. (2019) study leader-follower problems from an empirical perspec-\ntive, where the leader learns deep models to predict the followers\u2019 behavior."
        },
        {
            "heading": "6.3. Multi-Agent Reinforcement Learning",
            "text": "Another important component of the proposed framework is the followers\u2019 behavior learning. Deep multi-agent reinforcement learning algorithms have seen considerable advancements in recent years. Notable contributions such as COMA (Foerster et al., 2018), MADDPG (Lowe et al., 2017), PR2 (Wen et al., 2019b), and DOP (Wang et al., 2021d) address policy-based MARL challenges. These approaches leverage a (decomposed) centralized critic for computing gradients to decentralized actors. Conversely, valuebased algorithms decompose the joint Q-function into individual Q-functions, facilitating efficient optimization and decentralized implementation. Techniques like VDN (Sunehag et al., 2018), QMIX (Rashid et al., 2018), QTRAN (Son et al., 2019), and Weighted QMIX (Rashid et al., 2020) incrementally enhance the mixing network\u2019s representational capacity. Additional investigations explore MARL through coordination graphs (Guestrin et al., 2002b;a; Bo\u0308hmer et al., 2020; Kang et al., 2022; Wang et al., 2021c; Yang et al., 2022), communicative strategies (Singh et al., 2019; Mao et al., 2020; Wang et al., 2019; Kang et al., 2020), diversity (Li et al., 2021), and also expressive neural network architectures like Transformer (Wen et al., 2022), offering insights for participant learning without directly addressing human behavior modeling.\nFor modeling behavior, role-based learning frameworks (Wang et al., 2020; 2021b) are the most related to our work. They learn the roles of different agents autonomously and enhance learning efficiency by decomposing the task and learning sub-task-specific policies. However, these works are majorly studied in the setting of the Decentralized Partially Observable Markov Decision Process (Dec-POMDP), and are thus different from our work by two points: (1) The reward is shared among agents; and (2) The dynamics, including reward and transition dynamics, are static in these models. There likely would exist significant challenges in generalizing these to non-shared reward settings that are essential for many economic applications."
        },
        {
            "heading": "6.4. Computational Social Choice",
            "text": "Computational social choice is an interdisciplinary field combining computer science and social choice theory, focusing on the application of computational techniques to social choice mechanisms (such as voting rules or fair allocation procedures) and the theoretical analysis of these mechanisms with computational tools (Brandt et al., 2016). A fundamental component of the field is the study of manipulative behavior in elections and other collective decisionmaking processes, as well as the design of systems resistant\nto manipulation (Elkind et al., 2010; Procaccia, 2010). This area of study will likely inform the development of the Voting Mechanism, and thus merits much consideration in our framework. Additionally, computational social choice attempts to optimize the fair distribution of resources, often involving complex allocation problems (Thomson, 2016; Procaccia, 2016), another area for drawing inspiration from for development of human baselines to compare against the Principal. However, our work also differs significantly in considering individual values within elections. Computational social choice typically assumes a discrete set of alternatives. This requires voters to express their values through support of a candidate that shares similar values. On the other hand, our framework enables voters to directly report their values in a continuous type space \u03b8. This allows the voters to more precisely express values, without having to rely on a discrete set of candidates or policies who may not be exactly aligned with their personal \u03b8."
        },
        {
            "heading": "6.5. Automated Mechanism Design",
            "text": "Automated Mechanism Design has a rich history somewhat similar in motivation to ours, and was first introduced by Sandholm (2003), where search algorithms are used to computationally create specific rule sets (mechanisms) for games that lead to desirable outcomes even when participants act in self-interest. More recently, the work of automated mechanism design has been advanced through deep learning, in the framework known as differentiable economics. Du\u0308tting et al. (Forthcoming 2023) use deep neural network to learn the allocation and payment rules of auctions. Since then, a line of follow-up work has been introduced, extending the framework to make the architecture more powerful and general (Shen et al., 2021; Ivanov et al., 2022; Duan et al., 2023; Curry et al., 2022; Wang et al., 2023). Deep learning methods have also been explored in equilibrium calculation (Kohring et al., 2023; Bichler et al., 2023; 2021). While these techniques are applied to settings much less general than ours, architectural details may be useful in building a Principal."
        },
        {
            "heading": "7. Conclusion",
            "text": "In this paper, we present a theoretical framework for both policy design and simulation that merges economic policy design with AI to potentially help better inform economic policy-making. It is designed to tackle issues such as preference aggregation and counterfactual testing in complex economic systems. Significant challenges, including democratic representation and accountability in AI-driven systems, are highlighted. We hope to engage interdisciplinary expertise and foster collaborative innovation, and aspire to help create AI systems that not only enhance economic resilience and governance effectiveness but also uphold democratic ideals and ethical standards."
        },
        {
            "heading": "8. Impact Statement",
            "text": "This paper sets out an agenda in Social Environment Design, suggesting that AI holds promise in improving policy design by proposing a general framework that can simulate general, socioeconomic phenomena and scale to large settings. There are several relevant considerations that are important to take-up in advancing this framework towards adoption by policy-makers. For example, its effectiveness depends on capturing all pertinent stakeholders within a given scenario. Related, is to ensure that agent modeling is consistent with the diverse motivations and incentives of people, firms, and other entities. Lastly, any real-world trial of this initiative should engage vigorously and faithfully with non-technical stakeholders."
        },
        {
            "heading": "A. Environment Hyperparameters and Training Details",
            "text": "Here we give a detailed breakdown of several key hyperparameters and Training Details within our environment in section 4.\nWe use PPO (Schulman et al., 2017) player agents with parameter sharing and GAE (Schulman et al., 2015), collecting samples at a horizon shorter than the episode length to perform multiple policy update iterations per episode. The principal has separate, discrete, action subspaces for each tax bracket, and is also trained by standard PPO at the same time-scale as the player agents. We follow a two-phase curriculum with tax annealing, as suggested in Zheng et al. (2022). This annealing can be formalized as a constraint in the policy implementation map by simply bounding the maximum tax percentage that can be set. It is worth noting, however, that training the principal in this way is susceptible to issues of non-stationarity, and we refer to Yang et al. (2021) for a discussion on alternatives."
        }
    ],
    "title": "Social Environment Design",
    "year": 2024
}