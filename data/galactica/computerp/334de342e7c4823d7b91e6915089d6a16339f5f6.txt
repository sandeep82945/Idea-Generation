Spatio-temporal graph neural networks (STGNN) have become the most popular solution to traffic forecasting. While successful, they rely on the message passing scheme of GNNs to establish spatial dependencies between nodes, and thus inevitably inherit GNNs’ notorious inefficiency. Given these facts, in this paper, we propose an embarrassingly simple yet remarkably effective spatio-temporal learning approach, entitled SimST. Specifically, SimST approximates the efficacies of GNNs by two spatial learning techniques, which respectively model local and global spatial correlations. Moreover, SimST can be used alongside various temporal models and involves a tailored training strategy. We conduct experiments on five traffic benchmarks to assess the capability of SimST in terms of efficiency and effectiveness. Empirical results show that SimST improves the prediction throughput by up to 39 times compared to more sophisticated STGNNs while attaining comparable performance, which indicates that GNNs are not the only option for spatial modeling in traffic forecasting. 1. conclusion In this paper, we propose SimST, a simple, effective, and efficient spatio-temporal learning method for traffic forecasting. It approximates GNNs with two proposed spatial learning modules, involves a node-based batch sampling strategy, and is temporal-model-agnostic. Our study suggests that message passing is not the only effective way of modeling spatial relations in traffic forecasting; hence, we hope to spur the development of new model designs with both high efficiency and effectiveness in the community. a. datasets We conduct experiments on the traffic datasets of PeMSD41, PeMSD71, PeMSD81, LA2 and BAY2. • PEMS-04 (Song et al., 2020): The dataset refers to the traffic flow in San Francisco Bay Area. There are 307 sensors and the period ranges from Jan. 1 - Feb. 28, 2018. • PEMS-07 (Song et al., 2020): This dataset involves traffic flow readings collected from 883 sensors, and the time range is from May 1 - Aug. 6, 2017. • PEMS-08 (Song et al., 2020): The dataset contains traffic flow information collected from 170 sensors in the San Bernardino area from Jul. 1 - Aug. 31, 2016. • LA (Li et al., 2018): The dataset records the traffic speed collected from 207 loop detectors in the highway of Los Angeles County, ranging from Mar. 1 - Jun. 27, 2012. • BAY (Li et al., 2018): This dataset contains traffic speed information from 325 sensors in the Bay Area. It has 6 months of data ranging from Jan. 1 - Jun. 30, 2017. The datasets are split into three parts for training, validation, and testing with a ratio of 6:2:2 on PeMSD4, PeMSD7, and PeMSD8, and 7:1:2 on LA and BAY. The statistics of data partition are in Table 4. b. configuration of simst variants  c. additional performance comparison In Figure 6, we provide a visual comparison of throughput, accuracy, and parameter size between top-performing STGNNs and SimST. It can be seen that SimST variants are substantially more efficient and use much fewer parameters than state-of-the-art STGNNs, while achieving comparable performance. Additionally, we notice that adding an auxiliary feature – day of week can significantly improve the predictive accuracy, as shown in Table 6. The reason is that there is a significant difference in traffic patterns between weekdays and weekends. Therefore, this feature is a powerful prior knowledge for the model, and future research may consider incorporating this feature to enhance performance. d. wall-clock time comparison In Table 7, we provide the training and inference time per epoch of top-performing baselines and SimST variants, to enable straightforward efficiency comparison. We observe that SimST variants have significantly shorter inference time, thanks to their linear complexity w.r.t the number of nodes and small parameter size. As for training efficiency, although SimST-GRU applies a small actual batch size B∗ = 1, 024, which is necessary to achieve better predictive performance, its training time is still comparable to the fastest two baselines GWNET and AGCRN. Furthermore, we notice that methods based on ordinary differential equations (STGODE and STGNCDE) require significant training and inference time, which largely affects their practical applications in the real world. Moreover, since we need to perform ego-graph construction for the proximity modeling module, we provide details of the time spent here based on our current implementation: 41.9 seconds on PeMSD4, 206.5 seconds on PeMSD7, 28.8 seconds on PeMSD8, 284.1 seconds on LA, and 716.1 seconds on BAY. Note that this construction process only need to run once, because such process is deterministic and we can store the processed data and load them during training and inference. e. learning curve comparison In this section, we first scale up the parameter size of SimSTGRU from 130k to 340k (denoted as SimST-GRU-L), to ensure our method has comparable parameters to the baselines. Then in Figure 7, we compare the learning curves between top-performing baselines and SimST-GRU-L. It can be seen that SimST-GRU-L exhibits very fast convergence rate: it drops to a small validation MAE within a few epochs, and almost converges in around 20 epochs. In addition, we observe that the standard deviation of SimST is generally smaller than those in baselines, indicating the stability provided by our approach.