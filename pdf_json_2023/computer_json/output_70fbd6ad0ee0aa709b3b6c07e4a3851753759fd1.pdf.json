{
    "abstractText": "In recent years, the use of large convolutional kernels has become popular in designing convolutional neural networks due to their ability to capture long-range dependencies and provide large receptive fields. However, the increase in kernel size also leads to a quadratic growth in the number of parameters, resulting in heavy computation and memory requirements. To address this challenge, we propose a neighborhood attention (NA) module that upgrades the standard convolution with a self-attention mechanism. The NA module efficiently extracts long-range dependencies in a sliding window pattern, thereby achieving similar performance to large convolutional kernels but with fewer parameters. Building upon the NA module, we propose a lightweight single image super-resolution (SISR) network named TCSR. Additionally, we introduce an enhanced feed-forward network (EFFN) in TCSR to improve the SISR performance. EFFN employs a parameter-free spatial-shift operation for efficient feature aggregation. Our extensive experiments and ablation studies demonstrate that TCSR outperforms existing lightweight SISR methods and achieves state-ofthe-art performance. Our codes are available at https: //github.com/Aitical/TCSR.",
    "authors": [
        {
            "affiliations": [],
            "name": "Gang Wu"
        },
        {
            "affiliations": [],
            "name": "Junjun Jiang"
        },
        {
            "affiliations": [],
            "name": "Yuanchao Bai"
        },
        {
            "affiliations": [],
            "name": "Xianming Liu"
        }
    ],
    "id": "SP:c237eeca4ca14b8c0c5f45cb35a4b65b81faf065",
    "references": [
        {
            "authors": [
                "Eirikur Agustsson",
                "Radu Timofte"
            ],
            "title": "Ntire 2017 challenge on single image super-resolution: Dataset and study",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops,",
            "year": 2017
        },
        {
            "authors": [
                "Namhyuk Ahn",
                "Byungkon Kang",
                "Kyung-Ah Sohn"
            ],
            "title": "Fast, accurate, and lightweight super-resolution with cascading residual network",
            "venue": "In Proceedings of the European conference on computer vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Saeed Anwar",
                "Salman H. Khan",
                "Nick Barnes"
            ],
            "title": "A deep journey into super-resolution: A survey",
            "venue": "ACM Comput. Surv.,",
            "year": 2020
        },
        {
            "authors": [
                "Marco Bevilacqua",
                "Aline Roumy",
                "Christine Guillemot",
                "Marie-Line Alberi-Morel"
            ],
            "title": "Low-complexity single-image super-resolution based on nonnegative neighbor embedding",
            "venue": "In Proceedings of the British Machine Vision Conference (BMVC),",
            "year": 2012
        },
        {
            "authors": [
                "Tao Dai",
                "Jianrui Cai",
                "Yongbing Zhang",
                "Shu-Tao Xia",
                "Lei Zhang"
            ],
            "title": "Second-order attention network for single image super-resolution",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2019
        },
        {
            "authors": [
                "Xiaohan Ding",
                "Xiangyu Zhang",
                "Ningning Ma",
                "Jungong Han",
                "Guiguang Ding",
                "Jian Sun"
            ],
            "title": "Repvgg: Making vgg-style convnets great again",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "Xiaohan Ding",
                "Xiangyu Zhang",
                "Yizhuang Zhou",
                "Jungong Han",
                "Guiguang Ding",
                "Jian Sun"
            ],
            "title": "Scaling up your kernels to 31x31: Revisiting large kernel design in cnns",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Chao Dong",
                "Chen Change Loy",
                "Kaiming He",
                "Xiaoou Tang"
            ],
            "title": "Image super-resolution using deep convolutional networks",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,",
            "year": 2016
        },
        {
            "authors": [
                "Xiaoyi Dong",
                "Jianmin Bao",
                "Dongdong Chen",
                "Weiming Zhang",
                "Nenghai Yu",
                "Lu Yuan",
                "Dong Chen",
                "Baining Guo"
            ],
            "title": "Cswin transformer: A general vision transformer backbone with cross-shaped windows",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly",
                "Jakob Uszkoreit",
                "Neil Houlsby"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2021
        },
        {
            "authors": [
                "Guangwei Gao",
                "Wenjie Li",
                "Juncheng Li",
                "Fei Wu",
                "Huimin Lu",
                "Yi Yu"
            ],
            "title": "Feature distillation interaction weighting network for lightweight image super-resolution",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),",
            "year": 2022
        },
        {
            "authors": [
                "Jinjin Gu",
                "Chao Dong"
            ],
            "title": "Interpreting super-resolution networks with local attribution maps",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "Ali Hassani",
                "Steven Walton",
                "Jiachen Li",
                "Shen Li",
                "Humphrey Shi"
            ],
            "title": "Neighborhood attention transformer",
            "venue": "In arXiv preprint arXiv:2003.04297,",
            "year": 2022
        },
        {
            "authors": [
                "Jia-Bin Huang",
                "Abhishek Singh",
                "Narendra Ahuja"
            ],
            "title": "Single image super-resolution from transformed self-exemplars",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2015
        },
        {
            "authors": [
                "Zilong Huang",
                "Youcheng Ben",
                "Guozhong Luo",
                "Pei Cheng",
                "Gang Yu",
                "Bin Fu"
            ],
            "title": "Shuffle transformer: Rethinking 9 spatial shuffle for vision transformer",
            "venue": "In arXiv preprint arXiv:2106.03650,",
            "year": 2021
        },
        {
            "authors": [
                "Zheng Hui",
                "Xinbo Gao",
                "Yunchu Yang",
                "Xiumei Wang"
            ],
            "title": "Lightweight image super-resolution with information multidistillation network",
            "venue": "In Proceedings of the 27th ACM International Conference on Multimedia (ACM MM),",
            "year": 2019
        },
        {
            "authors": [
                "Zheng Hui",
                "Xiumei Wang",
                "Xinbo Gao"
            ],
            "title": "Fast and accurate single image super-resolution via information distillation network",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2018
        },
        {
            "authors": [
                "Longlong Jing",
                "Yingli Tian"
            ],
            "title": "Self-supervised visual feature learning with deep neural networks: A survey",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,",
            "year": 2021
        },
        {
            "authors": [
                "Jiwon Kim",
                "Jung Kwon Lee",
                "Kyoung Mu Lee"
            ],
            "title": "Accurate image super-resolution using very deep convolutional networks",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2016
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2015
        },
        {
            "authors": [
                "Wei-Sheng Lai",
                "Jia-Bin Huang",
                "Narendra Ahuja",
                "Ming- Hsuan Yang"
            ],
            "title": "Deep Laplacian pyramid networks for fast and accurate super-resolution",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2017
        },
        {
            "authors": [
                "Christian Ledig",
                "Lucas Theis",
                "Ferenc Huszar",
                "Jose Caballero",
                "Andrew Cunningham",
                "Alejandro Acosta",
                "Andrew P. Aitken",
                "Alykhan Tejani",
                "Johannes Totz",
                "Zehan Wang",
                "Wenzhe Shi"
            ],
            "title": "Photo-realistic single image super-resolution using a generative adversarial network",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2017
        },
        {
            "authors": [
                "Juncheng Li",
                "Zehua Pei",
                "Tieyong Zeng"
            ],
            "title": "From beginner to master: A survey for deep learning-based single-image super-resolution",
            "venue": "arXiv preprint arXiv:2109.14335,",
            "year": 2021
        },
        {
            "authors": [
                "Wenbo Li",
                "Kun Zhou",
                "Lu Qi",
                "Nianjuan Jiang",
                "Jiangbo Lu",
                "Jiaya Jia"
            ],
            "title": "LAPAR: linearly-assembled pixel-adaptive regression network for single image super-resolution and beyond",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2020
        },
        {
            "authors": [
                "Zhen Li",
                "Jinglei Yang",
                "Zheng Liu",
                "Xiaomin Yang",
                "Gwanggil Jeon",
                "Wei Wu"
            ],
            "title": "Feedback network for image superresolution",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2019
        },
        {
            "authors": [
                "Jingyun Liang",
                "Jiezhang Cao",
                "Guolei Sun",
                "Kai Zhang",
                "Luc Van Gool",
                "Radu Timofte"
            ],
            "title": "SwinIR: Image restoration using swin transformer",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops,",
            "year": 2021
        },
        {
            "authors": [
                "Bee Lim",
                "Sanghyun Son",
                "Heewon Kim",
                "Seungjun Nah",
                "Kyoung Mu Lee"
            ],
            "title": "Enhanced deep residual networks for single image super-resolution",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops,",
            "year": 2017
        },
        {
            "authors": [
                "Ze Liu",
                "Yutong Lin",
                "Yue Cao",
                "Han Hu",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Baining Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Zhuang Liu",
                "Hanzi Mao",
                "Chao-Yuan Wu",
                "Christoph Feichtenhofer",
                "Trevor Darrell",
                "Saining Xie"
            ],
            "title": "A convnet for the 2020s",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "David R. Martin",
                "Charless C. Fowlkes",
                "Doron Tal",
                "Jitendra Malik"
            ],
            "title": "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics",
            "venue": "In Proceedings Eighth IEEE International Conference on Computer Vision (ICCV),",
            "year": 2001
        },
        {
            "authors": [
                "Yusuke Matsui",
                "Kota Ito",
                "Yuji Aramaki",
                "Azuma Fujimoto",
                "Toru Ogawa",
                "Toshihiko Yamasaki",
                "Kiyoharu Aizawa"
            ],
            "title": "Sketch-based manga retrieval using manga109 dataset. Multim",
            "venue": "Tools Appl.,",
            "year": 2017
        },
        {
            "authors": [
                "Yiqun Mei",
                "Yuchen Fan",
                "Yuqian Zhou"
            ],
            "title": "Image superresolution with non-local sparse attention",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "Niki Parmar",
                "Prajit Ramachandran",
                "Ashish Vaswani",
                "Irwan Bello",
                "Anselm Levskaya",
                "Jonathon Shlens"
            ],
            "title": "Stand-alone self-attention in vision models",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2019
        },
        {
            "authors": [
                "Long Sun",
                "Jinshan Pan",
                "Jinhui Tang"
            ],
            "title": "Shufflemixer: An efficient convnet for image super-resolution",
            "venue": "In arXiv preprint arXiv:2205.15175,",
            "year": 2022
        },
        {
            "authors": [
                "Ying Tai",
                "Jian Yang",
                "Xiaoming Liu"
            ],
            "title": "Image superresolution via deep recursive residual network",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2017
        },
        {
            "authors": [
                "Longguang Wang",
                "Xiaoyu Dong",
                "Yingqian Wang",
                "Xinyi Ying",
                "Zaiping Lin",
                "Wei An",
                "Yulan Guo"
            ],
            "title": "Exploring sparsity in image super-resolution for efficient inference",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "Xintao Wang",
                "Ke Yu",
                "Shixiang Wu",
                "Jinjin Gu",
                "Yihao Liu",
                "Chao Dong",
                "Yu Qiao",
                "Chen Change Loy"
            ],
            "title": "ESRGAN: enhanced super-resolution generative adversarial networks",
            "venue": "ECCVW,",
            "year": 2018
        },
        {
            "authors": [
                "Zhou Wang",
                "Alan C. Bovik",
                "Hamid R. Sheikh",
                "Eero P. Simoncelli"
            ],
            "title": "Image quality assessment: from error visibility to structural similarity",
            "venue": "IEEE Trans. Image Process.,",
            "year": 2004
        },
        {
            "authors": [
                "Roman Zeyde",
                "Michael Elad",
                "Matan Protter"
            ],
            "title": "On single image scale-up using sparse-representations",
            "venue": "In Curves and Surfaces,",
            "year": 2012
        },
        {
            "authors": [
                "Xindong Zhang",
                "Hui Zeng",
                "Lei Zhang"
            ],
            "title": "Edge-oriented convolution block for real-time super resolution on mobile devices",
            "venue": "In Proceedings of ACM International Conference on Multimedia (ACM MM),",
            "year": 2021
        },
        {
            "authors": [
                "Yulun Zhang",
                "Kunpeng Li",
                "Kai Li",
                "Lichen Wang",
                "Bineng Zhong",
                "Yun Fu"
            ],
            "title": "Image super-resolution using very 10 deep residual channel attention networks",
            "venue": "In Proceedings of the European conference on computer vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Yulun Zhang",
                "Yapeng Tian",
                "Yu Kong",
                "Bineng Zhong",
                "Yun Fu"
            ],
            "title": "Residual dense network for image super-resolution",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2018
        },
        {
            "authors": [
                "Hang Zhao",
                "Orazio Gallo",
                "Iuri Frosio",
                "Jan Kautz"
            ],
            "title": "Loss functions for image restoration with neural networks",
            "venue": "In IEEE Transactions on computational imaging,",
            "year": 2016
        }
    ],
    "sections": [
        {
            "text": "Building upon the NA module, we propose a lightweight single image super-resolution (SISR) network named TCSR. Additionally, we introduce an enhanced feed-forward network (EFFN) in TCSR to improve the SISR performance. EFFN employs a parameter-free spatial-shift operation for efficient feature aggregation. Our extensive experiments and ablation studies demonstrate that TCSR outperforms existing lightweight SISR methods and achieves state-ofthe-art performance. Our codes are available at https: //github.com/Aitical/TCSR."
        },
        {
            "heading": "1. Introduction",
            "text": "Single image super-resolution (SISR) has enjoyed tremendous progress with the development of deep learning, especially in recent years. The goal of SISR is to reconstruct a high-resolution (HR) image from its low-resolution (LR) counterpart. The pioneering work SRCNN [8] first proposed a convolutional neural network (CNN) to learn the mapping from LR inputs to HR targets, and outperformed traditional SISR approaches by a large margin. Following [8], a series of well-designed CNN architectures [19,27,42] and visual attention mechanisms [5,32,41] were introduced\n*Corresponding author.\nto improve the CNN-based SISR performance. However, the above mentioned SISR methods rely heavily on complicated network architectures, which require substantial computational resources and are hard to be deployed on mobile and edge devices. Therefore, the design of efficient and lightweight SR models are highly demanded.\nFor practical SISR, many efforts have been made to reduce the number of parameters and floating-point operations (FLOPs) [2, 11, 16, 17, 24, 34, 36, 40]. Since both the number of parameters and FLOPs grow quadratically with respect to the kernel size, 3 \u00d7 3 convolution is widely adopted in CNN-based models. Recently, modern CNN architectures were exploited by revisiting the effectiveness of large kernels [7, 29]. Liu et al. [29] redesigned the basic residual block where the kernel size is crucial to the performance and 7\u00d7 7 kernel was utilized. Ding et al. [7] further extended the kernel size up to 31, which resulted in large effective receptive fields. Inspired by [7,29], we are interested in designing an efficient SISR method by taking advantage of large kernels while enjoying a lightweight architecture, in order to get the best of both worlds.\nSpecifically, in this paper, we propose a flexible and scalable neighborhood attention (NA) mechanism to substitute\nar X\niv :2\n30 3.\n14 32\n4v 1\n[ cs\n.C V\n] 2\n5 M\nar 2\n02 3\nfor standard convolutions. NA extracts feature relations in a sliding window pattern like standard convolutions. The corresponding feature extraction and aggregation compared to standard convolutions are shown in Fig. 1. The number of parameters in the standard convolution is coupled with the kernel size and grows quadratically, which makes it inefficient to leverage the large kernel size. Unlike convolutions, the proposed NA mechanism decouples the number of parameters from the feature aggregation, which can efficiently model the long-range relations for the large kernel size. By incorporating the proposed NA mechanism into CNNs, we propose a lightweight SISR network, named TCSR. TCSR adopts a shallow feature extraction module to extract features from an input LR image, and processes the extracted features with a feature aggregation module stacked by several NA blocks. In each NA block, we propose an enhanced feed-forward network (EFFN) following the NA module. Considering the FFN extracts the pixel-wise deep features separately and lacks the local feature aggregation, the proposed EFFN utilizes a spatial-shift operation leading to the effective local feature aggregation without extra computational cost. Finally, TCSR adopts a high-resolution reconstruction module based on 3 \u00d7 3 convolutions and a pixelshuffle layer, resulting in the super-resolved image.\nIn summary, this paper propose a sliding window-based NA mechanism that sheds a new light on base model design. The proposed NA mechanism can effectively realizes large kernel sizes with much smaller number of parameters and FLOPs than standard convolutions. Based on NA, we propose a lightweight SISR network, named TCSR. In TCSR, an EFFN with spatial-shift operations is further presented to achieve advanced feature enhancement. Extensive experiments demonstrate that the proposed TCSR achieves the state-of-the-art SISR performance and outperforms existing lightweight approaches, as illustrated in Fig. 2."
        },
        {
            "heading": "2. Related Works",
            "text": "In this section we will briefly introduce some related work about the image super-resolution, vision transformer, and modern convolutional networks.\nImage Super-Resolution. Deep learning-based methods for SISR tasks achieved great breakthroughs in recent years [3, 18, 23]. Dong et al. proposed the SRCNN which takes three convolutional layers to learn the LR to HR mapping directly and obtains promising results compared to the classical approaches. Subsequently, many well designed CNN-based architectures were exploited and proposed for SISR task and achieved further improvement [27,32,37,41,42]. Many efficient and lightweight SR models were proposed [2, 11, 16, 24, 34, 36, 40]. Liu et al. proposed the ShuffleMixer, which exploits the large kernel in SR network and utilizes the channel shuffle operation to reduce the number of learnable features. In addition, Liang et al. [26] proposed the SwinIR, which involved the Swin Transformer [28] to image restoration tasks and achieved promising results.\nModern Convolutional Network. In the last year, several work investigated some modern CNN-based architectures [7,29]. Liu et al. [29] revisited a modern design of the residual block and introduced larger kernels, where the 7\u00d77 kernel size is utilized. Furthermore, Ding et al. [7] exploited the kernel size upto 31. In [7], the stable and scalable architectures were proposed based on the re-parameterizing strategy [6], and a well-optimized implementation of the large kernel convolution was introduced, which makes it more practical. Compared to the standard 3\u00d73 convolution, large kernels bring larger receptive fields that significantly improve the capabilities of CNN-based networks.\nVision Transformer. Dosovitskiy et al. [10] first introduced the ViT, which proposed the Transformer-based architectures into vision tasks. Furthermore, Liu et al. [28] brought some inductive bias in CNNs into the Transformerbased architecture design, and proposed a local selfattention mechanism, named Shifted Window-based (Swin) Transformer. Swin partitions the input and applies selfattention into each partition separately, which reduces the computational cost and makes the Transformer-based architecture more practical for vision tasks. Based on Swin, how to extract more effective cross-region relation has attracted great attention [9, 15]. In addition, Ramachandran et al. [33] proposed the sliding window-based self-attention mechanism and made an attempt to substitute normal convolutions. Most recently, Hassani et al. [13] proposed the neighborhood attention module and given an efficient implementation of the sliding window-based self-attention.\nIn this paper, we attempt to exploit more large kernel design in lightweight SR network. In contrast to focusing on the architecture design, we exploit large kernel attention by a sliding window-based self-attention pattern, which ex-\ntracts long-range relation effectively while maintains inductive bias like the convolution. We first analyze the complementarity of the neighborhood attention (NA) against the normal convolution. As the aforementioned, NA contains inductive bias in the CNN, which can effectively extract the cross-region relation like the convolution by the dense feature extraction. Furthermore, we extend and propose the enhanced feed-forward network (EFFN). The proposed EFFN involves the spatial-shift operation to maintain more local feature aggregation along channel dimension."
        },
        {
            "heading": "3. Proposed Method",
            "text": "In this section, we provide a detailed description of our proposed TCSR. Firstly, we introduce the general framework for SISR tasks. Subsequently, we present the implementation details of the proposed NA and EFFN modules. Finally, we provide further comparisons between Swin, convolution, and NA modules."
        },
        {
            "heading": "3.1. Overall Architecture",
            "text": "As illustrated in Fig. 3, the proposed TCSR contains three components: the shallow feature extractor, the deep feature extraction module stacked by several NAT blocks, and the high-resolution reconstruction module. Given the LR input ILR \u2208 RH\u00d7W\u00d73where H , W are image height, width, respectively. The shallow feature extractor fs, based on a normal 3 \u00d7 3 convolutional layer, is firstly utilized to map the input image into the latent space and the primitive feature F \u2208 RH\u00d7W\u00d7C with C channel dimensions is ob-\ntained as follows:\nFs = fs(I LR). (1)\nThen Fs is sent into the deep feature extractor fd and deep the feature Fd is formulated as follows:\nFd = fd(Fs). (2)\nThe feature Fd is utilized for the final super-resolution by the HR reconstruction module, and the super-resolved image ISR is obtained as follows:\nISR = FHR(Fd), (3)\nwhere FHR presents the HR reconstruction module, based on the 3\u00d7 3 convolution and a pixelshuffle layer."
        },
        {
            "heading": "3.2. Neighborhood Attention Module",
            "text": "Self-Attention. Self-attention (SA) is an operation that employs a query (Q) and a set of key (K) and value (V) pairs. First, the dot product between the Q and K is computed and scaled, and the softmax function is utilized to obtain weighted coefficients. Then assembled feature can be obtained by combining the V with the coefficient. The SA is formulated as follows:\nSA(Q,K, V ) = SoftMax ( QKT\u221a dk +RPB ) V, (4) where d is the feature dimension, \u221a dk is the scale factor, and RPB is the learnable relative position bias. Furthermore, the multi-head self-attention is utilized, which translates the input into h independently features by learnable\nlinear projections, where h is the number of headers, and SA is applied in parallel against each projection.\nNeighborhood Attention. Based on SA, aggregated features can learn the relation between each pair of Q and K and easily obtain the long-range relation with a large scale of the key-value set. However, SA has a quadratic complexity to the number of tokens. To reduce the computational cost, the local attention mechanism is in demand. Considering the success of CNNs, the inductive bias in CNN is essential to modeling. To bridge the gap between the SA in transformer and the inductive bias in CNN, a sliding window-based local attention module is introduced here, which extracts the SA among local neighboring features around the target query pixel, named the neighborhood attention (NA).\nThe proposed NA applies SA with the sliding window like the normal convolutional layer. The analogy to the convolution with the kernel size k, for the (i, j)th pixel pi,j in the feature map, the key-value set is selected within local pixels around pi,j , noted as \u03c1ki,j . Let us take the 3\u00d73 kernel as the example, the corresponding key-value set is \u03c13i,j = {pi+1,j\u22121, pi+1,j , pi+1,j+1, pi,j\u22121, pi,j , pi,j+1, pi\u22121,j\u22121, pi\u22121,j , pi\u22121,j+1}.\nBased on the implementation of SA, the proposed NA is formulated as follows:\nNA(Qi,j ,K\u03c1ki,j , V\u03c1ki,j ) = SoftMax Qi,jKT\u03c1ki,j\u221a dk +RPB V\u03c1ki,j . (5)\nIt is worth noting that there is no patch splitting and patch embedding operation in the proposed NA. Feature extraction in NA is the same as the normal convolution with kernel size k, where we take 11 as the default kernel size, and more detailed ablations are presented in experiments."
        },
        {
            "heading": "3.3. Enhanced Feed-Forward Network",
            "text": "Following the feature aggregation NA module, a feedforward network (FFN) containing two linear layers with a non-linear activation layer is utilized. We can find that pixel-wise interaction is extracted by FFN, which lacks feature aggregation with local neighboring pixels. A vanilla way is to take convolutional layers, such as the normal 3 \u00d7 3 convolution, but more parameter count and computational costs are involved. To address this problem and bring the local feature aggregation into the FFN, as illustrated in Fig. 3(a), we propose the enhanced feed-forward network (EFFN) with the spatial-shift operation, which is parameter-free and no extra FLOPs cost.\nSpatial-Shift Operation. The spatial-shift operation manually exploits the feature aggregation against the channel dimension. As illustrated in Fig. 3(b), here we take the\nspatial-shift operation with 4 groups as the example. Given the input feature Fin \u2208 RH\u00d7W\u00d7C , we first uniformly separate it intoN thinner groups along channel dimension. Then each thinner grouped feature is shifted in different directions with the shift stride s. Here we take the same feature aggregation pattern as the normal 3\u00d7 3 convolution. In detail, given the input feature Fin, we uniformly split it into 8 groups along the channel dimension. Then each separated feature group is shifted in 8 directions with stride 1, and we take the constant value 0 as the default padding for borders.\nBy spatial-shift operation, local pixels are involved in the shifted feature among different channel groups."
        },
        {
            "heading": "3.4. Loss Function",
            "text": "For image SR tasks, MAE (Mean Absolute Error) and MSE (Mean Squared Error) are two commonly used loss functions. MAE loss, also known as L1 loss, measures the absolute differences between the super-resolved image and the HR target. It is frequently used because it is more robust to outliers than MSE and produces sharper edges in the output image [43]. In this paper, we adopt MAE loss to measure the differences between the SR images and the ground truth. Specifically, the loss function is:\nL1 = \u2016ISR \u2212 IHR\u20161, (6)\nwhere ISR and IHR are super-resolved image and the HR target, respectively."
        },
        {
            "heading": "3.5. Remark",
            "text": "Comparison between Conv, Swin, and NA. The proposed NA, a sliding window-based self-attention mechanism, brings the inductive bias in convolution to the vanilla self-attention module. Compared to the convolution, the parameter count of NA is independent of the kernel size, which is more flexible for extracting long-range relations. Local window-based self-attention is exploited by splitting the input into non-overlapping windows to reduce the computational cost of global self-attention. To obtain the crosswindow connection, Swin proposed and achieved promising results. Compared to the Swin, the proposed NA is a more flexible operation to obtain the cross-region relation like the normal convolution.\nComplexity Analysis. Given the input feature F \u2208 RH\u00d7W\u00d7C , whereH,W , andC are height, width, and channel dimension, respectively. The kernel size in convolution, the local window size in Swin, and the kernel size in NA are set as K. The number of headers in Swin and NA is set as 1, and the linear projection for Q, K, and V is contained. Results are presented in Tab. 1. One can find that NA has the same complexity as the Swin when they take the same spatial extent. Compared to the normal convolution, the computational cost of the NA grows slower than the convolution. If we take the channel dimension C = 64 as the example, the computational cost in the normal 3 \u00d7 3 convolution is near to the NA with K = 13."
        },
        {
            "heading": "4. Experiments",
            "text": ""
        },
        {
            "heading": "4.1. Experiment Setup",
            "text": "We take 800 images from DIV2K [1] and 2650 images from Flickr2K for training. Datasets for testing include Set5 [4], Set14 [39], B100 [30], Urban100 [14], and Manga109 [31] with the up-scaling factor 2, 3, and 4. We crop the image patches with the fixed size of 64 \u00d7 64 and set the batch size to 32 for training. The optimizer is ADAM [20] with the settings of \u03b21 = 0.9, \u03b22 = 0.999.\nWe compare the proposed TCSR with representative efficient SR models, including VDSR [19], LapSRN [21], DRRN [35], CARN [2], IMDN [16], LAPAR [24], SMSR [36], ECBSR [40], FDIWN [11], and ShuffleMixer [34] on \u00d74 up-scaling tasks. For comparison, we measure PSNR, and SSIM [38] on the Y channel of transformed YCbCr space."
        },
        {
            "heading": "4.2. Main Results",
            "text": "Quantitative Evaluation. Results of different SR models on five test datasets with scale 2, 3, and 4 are reported in Tab. 2. In addition to PSNR/SSIM, we also report the number of parameters. One can find that our base model TCSRB with 16 NAT blocks outperforms all CNN-based models and obtains comparable performance to SwinIR-light. When we take deeper architectures, our TCSR-L with 32 NAT blocks achieves new SOTA results on all test datasets. The promising performance demonstrates the effectiveness of the proposed TCSR, which contains both local feature aggregation and large receptive fields.\nQualitative Evaluation. Several visual results of VDSR [19], CARN [2], IMDN [16], SwinIR-light [26], and the proposed TCSR on \u00d74 up-scaling task are shown in Fig. 4. One can limpidly see that the proposed TCSR-L can recover the main structures with clear and accurate textures. Here we take the img092 in Urban100 as the example, results of some detail patches are shown in Fig. 5. Compared to the One can find that our TCSR-L obtains clear and accurate edges while some other methods cannot."
        },
        {
            "heading": "4.3. Ablation and Analysis",
            "text": "In this paper, our core contributions are to propose the sliding window-based self-attention mechanism NA and a enhance feed-forward network (EFFN). NA decouples the number of model parameters and feature aggregation, which can effectively build the long-range relation. The sliding window-based NA combines the self-attention mechanism with the inductive bias of the convolution. In addition, the proposed EFFN involve the local feature aggregation and advanced feature enhancement is obtained.\nIn this section, we present detailed ablations to better understand the effectiveness of these different components.\nKernel Size. In this study, we use a tiny TCSR model, which contains only 8 NAT blocks, as the benchmark. Compared to conventional convolutions, the proposed TCSR is scalable and flexible in its ability to exploit large kernel sizes. We train the tiny TCSR model with different kernel sizes, and the results are summarized in Tab. 3. Notably, we observe that performance improves as the kernel size increases, indicating the scalability and flexibility of TCSR for working with different kernel sizes. Specifically, the tiny TCSR model with kernel size 9 achieves comparable performance to well-designed CNN-based methods such as LAPAR [24], shufflemixer [34], and FDIWN [11].\nEnhanced Feed Forward Network. The FFN module contains a basic MLP, which captures pixel-wise interactions but lacks feature aggregation across pixels. To address this limitation, we incorporate a spatial-shift operation to enable local feature aggregation within the FFN module and propose the EFFN module.\nWe evaluate the performance of our proposed TCSR with and without the EFFN module across different kernel sizes and model sizes. The results are presented in Fig. 7 and Fig. 8, respectively. As shown in Fig. 7, our proposed EFFN module consistently outperforms the models without EFFN across all kernel sizes. This demonstrates the importance of local feature aggregation within the FFN module for enhancing feature representations. Specifically, we observe that when the kernel size is 7, the TCSR model with EFFN outperforms the TCSR model with kernel size 9 but without EFFN. This reveals that the spatial-shift operation can effectively extend the receptive fields by leveraging the NA module output for feature aggregation. Additionally, we take LAM [12] to analyze the receptive fields, shown in Fig. 6. There are more activated pixels around the target region, which demonstrates that the proposed EFFN can further improve the long-range relation modeling as well.\nFurther ablations on the performance of our proposed EFFN module across different model sizes are presented in Fig. 8. The results demonstrate that the EFFN module is scalable to model capacity. Notably, even the smallest TCSR model with 4 NAT blocks and kernel size 11 achieves comparable performance to many existing CNN-\nbased models, while the TCSR model with 8 NAT blocks outperforms them. This indicates the potential of large re-\nceptive fields in the SISR task. Moreover, our proposed EFFN module is generic and can be integrated into other Transformer-based SR models. Specifically, we replace the original FFN in SwinIR-light with our proposed EFFN module, which is trained using the same settings as the original SwinIR-light [26]. The results in Tab. 4 show that SwinIR-light with the EFFN module outperforms the original SwinIR-light on all five test\ndatasets. This further highlights the effectiveness of our proposed EFFN module for local feature aggregation and\nextended long-range modeling. In addition, the visualization of receptive fields in Fig. 9 reveals that our EFFN module yields larger activated regions than the original SwinIRlight, demonstrating its efficacy in enhancing feature representations.\nInference Comparison. More comparisons between SwinIR-light and TCSR on the computational cost and inference speed are presented in 5. The latency is tested\nFigure 7. Results of the proposed TCSR with or without spatial-shift operation against different kernel sizes on Set14 for scale 4.\nFigure 8. Results of the proposed TCSR with or without spatial-shift operation against different model capacities on Set14 for scale 4.\nTable 4. Results of SwinIR-light with or without spatial-shift operation.\nScale EFFN Set5 Set14 B100 Urban100 Manga109PSNR/SSIM PSNR/SSIM PSNR/SSIM PSNR/SSIM PSNR/SSIM\n\u00d74 w/o 32.44/0.8976 28.77/0.7858 27.69/0.7406 26.47/0.7980 30.92/0.9151w 32.45/0.8977 28.82/0.7869 27.71/0.7410 26.53/0.8002 30.97/0.9157\non a RTX3090 GPU. One can find that even SRResNet is the most complexity but the inference speed is faster than SwinIR-light and TCSRs. It is reasonable that the most widely utilized convolutional operation is optimized. Compared to the SwinIR-light, the prposed TCSR-B has the similar complexity while the SwinIR-light brings near 400% more time consuming. Because there are much timeconsuming image shift operations in SwinIR."
        },
        {
            "heading": "4.4. Discussion",
            "text": "In this section, we provide quantitative and qualitative comparisons and conduct thorough ablations to demonstrate the effectiveness of the proposed TCSR. As previously mentioned, TCSR can scale to larger kernel sizes while maintaining a lightweight model size, resulting in significant improvements in both subjective and objective results. Additionally, our EFFN further enhances performance across different kernel sizes and model sizes. We observe that performance improves as the kernel size increases, indicating the scalability and flexibility of TCSR for working with different kernel sizes. Furthermore, we perform an ablation study to investigate the effect of the EFFN in TCSR. Results of the ablation studies indicate that the proposed EFFN significantly improves the performance of benchmark SwinIR-light and the proposed TCSR, verifying its ability to enable more effective local feature aggregation and extend long-range modeling. The implementation of the proposed TCSR and its model weights are available on our project page for reproducibility and further research."
        },
        {
            "heading": "5. Limitation",
            "text": "In this paper, we attempt to exploit the large kernel design in lightweight SISR and provide a scalable TCSR architecture. However, the computational cost of the TCSR is relatively high, as shown in Tab. 5. We believe that welldesigned architectures for lightweight models are essential\nto achieve an advanced trade-off between effectiveness and efficiency. Although optimizing the architecture is beyond the scope of this paper, we are currently working on developing more efficient ways to exploit the large receptive fields. The proposed TCSR architecture is a general approach that can flexibly model large kernels. Despite its current application to the lightweight SISR task, we believe that it can be extended to address other image restoration tasks and be scaled to large models for future research."
        },
        {
            "heading": "6. Conclusion",
            "text": "In this paper, we proposed a new lightweight image super-resolution architecture named TCSR, which is a conv-like transformer architecture. TCSR combines the strengths of both convolution and self-attention mechanisms, leveraging the inductive bias of convolution for local feature aggregation in CNN and the long-range relation capabilities of self-attention. To further improve the feature enhancement capabilities of TCSR, we introduced an enhanced feed-forward network (EFFN) by utilizing the spatial-shift operation, which further improves the local feature aggregating and long-range modeling. Our extensive experiments demonstrate the effectiveness of TCSR, which outperforms existing lightweight SR networks. Moreover, we provide detailed ablation studies that reveal the scalability of TCSR. We believe that analyzing the difference between features extracted by convolution and self-attention and enhancing the fundamental architectures by interpolating convolution with self-attention is a promising research direction for the future. We hope that our work will inspire further exploration of modern architecture in the near future, leading to more significant improvements in the field of lightweight image super-resolution."
        }
    ],
    "title": "Incorporating Transformer Designs into Convolutions for Lightweight Image Super-Resolution",
    "year": 2023
}