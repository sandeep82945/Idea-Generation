{
    "abstractText": "Human drivers can seamlessly adapt their driving decisions across geographical locations with diverse conditions and rules of the road, e.g., left vs. right-hand traffic. In contrast, existing models for autonomous driving have been thus far only deployed within restricted operational domains, i.e., without accounting for varying driving behaviors across locations or model scalability. In this work, we propose AnyD, a single geographically-aware conditional imitation learning (CIL) model that can efficiently learn from heterogeneous and globally distributed data with dynamic environmental, traffic, and social characteristics. Our key insight is to introduce a high-capacity geo-location-based channel attention mechanism that effectively adapts to local nuances while also flexibly modeling similarities among regions in a data-driven manner. By optimizing a contrastive imitation objective, our proposed approach can efficiently scale across the inherently imbalanced data distributions and location-dependent events. We demonstrate the benefits of our AnyD agent across multiple datasets, cities, and scalable deployment paradigms, i.e., centralized, semi-supervised, and distributed agent training. Specifically, AnyD outperforms CIL baselines by over 14% in open-loop evaluation and 30% in closed-loop testing on CARLA.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ruizhao Zhu"
        },
        {
            "affiliations": [],
            "name": "Peng Huang"
        },
        {
            "affiliations": [],
            "name": "Eshed Ohn-Bar"
        },
        {
            "affiliations": [],
            "name": "Venkatesh Saligrama"
        }
    ],
    "id": "SP:a19e1ce520dc02dae8870047c9560a9d3c63e510",
    "references": [
        {
            "authors": [
                "P. Sun",
                "H. Kretzschmar",
                "X. Dotiwalla",
                "A. Chouard",
                "V. Patnaik",
                "P. Tsui",
                "J. Guo",
                "Y. Zhou",
                "Y. Chai",
                "B. Caine"
            ],
            "title": "Scalability in perception for autonomous driving: Waymo open dataset",
            "year": 2020
        },
        {
            "authors": [
                "X. Huang",
                "X. Cheng",
                "Q. Geng",
                "B. Cao",
                "D. Zhou",
                "P. Wang",
                "Y. Lin",
                "R. Yang"
            ],
            "title": "The apolloscape dataset for autonomous driving",
            "venue": "CVPRW,",
            "year": 2018
        },
        {
            "authors": [
                "B. Wilson",
                "W. Qi",
                "T. Agarwal",
                "J. Lambert",
                "J. Singh",
                "S. Khandelwal",
                "B. Pan",
                "R. Kumar",
                "A. Hartnett",
                "J.K. Pontes"
            ],
            "title": "Argoverse 2.0: Next generation datasets for self-driving perception and forecasting",
            "venue": "NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "H. Caesar",
                "V. Bankiti",
                "A.H. Lang",
                "S. Vora",
                "V.E. Liong",
                "Q. Xu",
                "A. Krishnan",
                "Y. Pan",
                "G. Baldan",
                "O. Beijbom"
            ],
            "title": "nuscenes: A multimodal dataset for autonomous driving",
            "venue": "CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "M. Bansal",
                "A. Krizhevsky",
                "A. Ogale"
            ],
            "title": "ChauffeurNet: Learning to drive by imitating the best and synthesizing the worst",
            "venue": "RSS,",
            "year": 2019
        },
        {
            "authors": [
                "F. Codevilla",
                "E. Santana",
                "A.M. L\u00f3pez",
                "A. Gaidon"
            ],
            "title": "Exploring the limitations of behavior cloning for autonomous driving",
            "venue": "ICCV,",
            "year": 2019
        },
        {
            "authors": [
                "J. Zhang",
                "R. Zhu",
                "E. Ohn-Bar"
            ],
            "title": "SelfD: Self-learning large-scale driving policies from the web",
            "venue": "CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "M. Bojarski",
                "D. Del Testa",
                "D. Dworakowski",
                "B. Firner",
                "B. Flepp",
                "P. Goyal",
                "L.D. Jackel",
                "M. Monfort",
                "U. Muller",
                "J. Zhang"
            ],
            "title": "End to end learning for self-driving cars",
            "venue": "In arXiv,",
            "year": 2016
        },
        {
            "authors": [
                "D. Chen",
                "V. Koltun",
                "P. Kr\u00e4henb\u00fchl"
            ],
            "title": "Learning to drive from a world on rails",
            "venue": "ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "N. Dvornik",
                "C. Schmid",
                "J. Mairal"
            ],
            "title": "Selecting relevant features from a multi-domain representation for few-shot classification",
            "venue": "ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Y. Wang",
                "J. Peng",
                "Z. Zhang"
            ],
            "title": "Uncertainty-aware pseudo label refinery for domain adaptive semantic segmentation",
            "venue": "ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "K. Saito",
                "Y. Ushiku",
                "T. Harada"
            ],
            "title": "Asymmetric tri-training for unsupervised domain adaptation",
            "venue": "ICML,",
            "year": 2017
        },
        {
            "authors": [
                "J. Yang",
                "S. Shi",
                "Z. Wang",
                "H. Li",
                "X. Qi"
            ],
            "title": "ST3D: Self-training for unsupervised domain adaptation on 3d object detection",
            "venue": "CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Z. Lin",
                "D. Ramanan",
                "A. Bansal"
            ],
            "title": "Streaming self-training via domain-agnostic unlabeled images",
            "venue": "arXiv,",
            "year": 2021
        },
        {
            "authors": [
                "Y. Liu",
                "W. Zhang",
                "J. Wang"
            ],
            "title": "Source-free domain adaptation for semantic segmentation",
            "venue": "CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "K. Wang",
                "X. Gao",
                "Y. Zhao",
                "X. Li",
                "D. Dou",
                "C.-Z. Xu"
            ],
            "title": "Pay attention to features, transfer learn faster cnns",
            "venue": "ICLR,",
            "year": 2019
        },
        {
            "authors": [
                "L. Lai",
                "Z. Shangguan",
                "J. Zhang",
                "E. Ohn-Bar"
            ],
            "title": "XVO: Generalized visual odometry via cross-modal self-training",
            "venue": "ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "X. Wang",
                "Z. Cai",
                "D. Gao",
                "N. Vasconcelos"
            ],
            "title": "Towards universal object detection by domain attention",
            "venue": "CVPR,",
            "year": 2019
        },
        {
            "authors": [
                "I. Kokkinos"
            ],
            "title": "Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory",
            "venue": "CVPR,",
            "year": 2017
        },
        {
            "authors": [
                "W.-H. Li",
                "X. Liu",
                "H. Bilen"
            ],
            "title": "Cross-domain few-shot learning with task-specific adapters",
            "venue": "CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "S.-A. Rebuffi",
                "H. Bilen",
                "A. Vedaldi"
            ],
            "title": "Learning multiple visual domains with residual adapters",
            "venue": "NeurIPS,",
            "year": 2017
        },
        {
            "authors": [
                "W.-H. Li",
                "X. Liu",
                "H. Bilen"
            ],
            "title": "Universal representation learning from multiple domains for few-shot classification",
            "venue": "ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "E. Perez",
                "F. Strub",
                "H. De Vries",
                "V. Dumoulin",
                "A. Courville"
            ],
            "title": "Film: Visual reasoning with a general conditioning layer",
            "venue": "AAAI,",
            "year": 2018
        },
        {
            "authors": [
                "P. Bateni",
                "R. Goyal",
                "V. Masrani",
                "F. Wood",
                "L. Sigal"
            ],
            "title": "Improved few-shot visual classification",
            "venue": "CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "J. Requeima",
                "J. Gordon",
                "J. Bronskill",
                "S. Nowozin",
                "R.E. Turner"
            ],
            "title": "Fast and flexible multi-task classification using conditional neural adaptive processes",
            "venue": "NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "S. Shalev-Shwartz",
                "A. Shashua"
            ],
            "title": "On the sample complexity of end-to-end training vs",
            "venue": "semantic abstraction training. In arXiv,",
            "year": 2016
        },
        {
            "authors": [
                "A. Dosovitskiy",
                "L. Beyer",
                "A. Kolesnikov",
                "D. Weissenborn",
                "X. Zhai",
                "T. Unterthiner",
                "M. Dehghani",
                "M. Minderer",
                "G. Heigold",
                "S. Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "In ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "A. Vaswani",
                "N. Shazeer",
                "N. Parmar",
                "J. Uszkoreit",
                "L. Jones",
                "A.N. Gomez",
                "\u0141. Kaiser",
                "I. Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "NeurIPS,",
            "year": 2017
        },
        {
            "authors": [
                "F. Codevilla",
                "M. Miiller",
                "A. L\u00f3pez",
                "V. Koltun",
                "A. Dosovitskiy"
            ],
            "title": "End-to-end driving via conditional imitation learning",
            "venue": "ICRA,",
            "year": 2018
        },
        {
            "authors": [
                "E. Ohn-Bar",
                "A. Prakash",
                "A. Behl",
                "K. Chitta",
                "A. Geiger"
            ],
            "title": "Learning situational driving",
            "venue": "CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "N. Hanselmann",
                "K. Renz",
                "K. Chitta",
                "A. Bhattacharyya",
                "A. Geiger"
            ],
            "title": "King: Generating safety-critical driving scenarios for robust imitation via kinematics gradients",
            "venue": "ECCV,",
            "year": 2022
        },
        {
            "authors": [
                "P. Wu",
                "X. Jia",
                "L. Chen",
                "J. Yan",
                "H. Li",
                "Y. Qiao"
            ],
            "title": "Trajectory-guided control prediction for end-to-end autonomous driving: A simple yet strong baseline",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "A. Hu",
                "G. Corrado",
                "N. Griffiths",
                "Z. Murez",
                "C. Gurau",
                "H. Yeo",
                "A. Kendall",
                "R. Cipolla",
                "J. Shotton"
            ],
            "title": "Model-based imitation learning for urban driving",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "J. Hawke",
                "R. Shen",
                "C. Gurau",
                "S. Sharma",
                "D. Reda",
                "N. Nikolov",
                "P. Mazur",
                "S. Micklethwaite",
                "N. Griffiths",
                "A. Shah"
            ],
            "title": "Urban driving with conditional imitation learning",
            "venue": "In ICRA,",
            "year": 2020
        },
        {
            "authors": [
                "M. M\u00fcller",
                "A. Dosovitskiy",
                "B. Ghanem",
                "V. Koltun"
            ],
            "title": "Driving policy transfer via modularity and abstraction",
            "venue": "CoRL,",
            "year": 2018
        },
        {
            "authors": [
                "W. Zeng",
                "W. Luo",
                "S. Suo",
                "A. Sadat",
                "B. Yang",
                "S. Casas",
                "R. Urtasun"
            ],
            "title": "End-to-end interpretable neural motion planner",
            "venue": "CVPR,",
            "year": 2019
        },
        {
            "authors": [
                "L.L. Li",
                "B. Yang",
                "M. Liang",
                "W. Zeng",
                "M. Ren",
                "S. Segal",
                "R. Urtasun"
            ],
            "title": "End-to-end contextual perception and prediction with interaction transformer",
            "venue": "IROS,",
            "year": 2020
        },
        {
            "authors": [
                "M.-F. Chang",
                "J. Lambert",
                "P. Sangkloy",
                "J. Singh",
                "S. Bak",
                "A. Hartnett",
                "D. Wang",
                "P. Carr",
                "S. Lucey",
                "D. Ramanan"
            ],
            "title": "Argoverse: 3d tracking and forecasting with rich maps",
            "year": 2019
        },
        {
            "authors": [
                "J. Houston",
                "G. Zuidhof",
                "L. Bergamini",
                "Y. Ye",
                "L. Chen",
                "A. Jain",
                "S. Omari",
                "V. Iglovikov",
                "P. Ondruska"
            ],
            "title": "One thousand and one hours: Self-driving motion prediction dataset",
            "venue": "CoRL,",
            "year": 2021
        },
        {
            "authors": [
                "X. Huang",
                "P. Wang",
                "X. Cheng",
                "D. Zhou",
                "Q. Geng",
                "R. Yang"
            ],
            "title": "The ApolloScape open dataset for autonomous driving and its application",
            "venue": "IEEE PAMI,",
            "year": 2020
        },
        {
            "authors": [
                "Z. Huang",
                "Z. Shangguan",
                "J. Zhang",
                "G. Bar",
                "M. Boyd",
                "E. Ohn-Bar"
            ],
            "title": "ASSISTER: Assistive navigation via conditional instruction generation",
            "venue": "ECCV,",
            "year": 2022
        },
        {
            "authors": [
                "V. Prabhu",
                "R.R. Selvaraju",
                "J. Hoffman",
                "N. Naik"
            ],
            "title": "Can domain adaptation make object recognition work for everyone",
            "year": 2022
        },
        {
            "authors": [
                "S. Sagawa",
                "P.W. Koh",
                "T. Lee",
                "I. Gao",
                "S.M. Xie",
                "K. Shen",
                "A. Kumar",
                "W. Hu",
                "M. Yasunaga",
                "H. Marklund"
            ],
            "title": "Extending the wilds benchmark for unsupervised adaptation",
            "venue": "In ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "R. Taori",
                "A. Dave",
                "V. Shankar",
                "N. Carlini",
                "B. Recht",
                "L. Schmidt"
            ],
            "title": "Measuring robustness to natural distribution shifts in image classification",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "J. Hu",
                "L. Shen",
                "G. Sun"
            ],
            "title": "Squeeze-and-excitation networks",
            "venue": "CVPR,",
            "year": 2018
        },
        {
            "authors": [
                "T. Chen",
                "S. Kornblith",
                "M. Norouzi",
                "G. Hinton"
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "ICML,",
            "year": 2020
        },
        {
            "authors": [
                "T. Bai",
                "J. Chen",
                "J. Zhao",
                "B. Wen",
                "X. Jiang",
                "A. Kot"
            ],
            "title": "Feature distillation with guided adversarial contrastive learning",
            "venue": "arXiv,",
            "year": 2020
        },
        {
            "authors": [
                "J. Li",
                "P. Zhou",
                "C. Xiong",
                "S.C. Hoi"
            ],
            "title": "Prototypical contrastive learning of unsupervised representations",
            "venue": "ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "P. Khosla",
                "P. Teterwak",
                "C. Wang",
                "A. Sarna",
                "Y. Tian",
                "P. Isola",
                "A. Maschinot",
                "C. Liu",
                "D. Krishnan"
            ],
            "title": "Supervised contrastive learning",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "J. Cui",
                "Z. Zhong",
                "S. Liu",
                "B. Yu",
                "J. Jia"
            ],
            "title": "Parametric contrastive learning",
            "venue": "ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Z. Mandi",
                "F. Liu",
                "K. Lee",
                "P. Abbeel"
            ],
            "title": "Towards more generalizable one-shot visual imitation learning",
            "venue": "ICRA,",
            "year": 2022
        },
        {
            "authors": [
                "B. McMahan",
                "E. Moore",
                "D. Ramage",
                "S. Hampson",
                "B.A. y Arcas"
            ],
            "title": "Communication-efficient learning of deep networks from decentralized data",
            "year": 2017
        },
        {
            "authors": [
                "D.A.E. Acar",
                "Y. Zhao",
                "R.M. Navarro",
                "M. Mattina",
                "P.N. Whatmough",
                "V. Saligrama"
            ],
            "title": "Federated learning based on dynamic regularization",
            "venue": "ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "D.A.E. Acar",
                "Y. Zhao",
                "R. Zhu",
                "R. Matas",
                "M. Mattina",
                "P. Whatmough",
                "V. Saligrama"
            ],
            "title": "Debiasing model updates for improving personalized federated training",
            "venue": "ICML,",
            "year": 2021
        },
        {
            "authors": [
                "T. Li",
                "A.K. Sahu",
                "M. Zaheer",
                "M. Sanjabi",
                "A. Talwalkar",
                "V. Smith"
            ],
            "title": "Federated optimization in heterogeneous networks",
            "venue": "MLSys,",
            "year": 2020
        },
        {
            "authors": [
                "S.P. Karimireddy",
                "S. Kale",
                "M. Mohri",
                "S. Reddi",
                "S. Stich",
                "A.T. Suresh"
            ],
            "title": "Scaffold: Stochastic controlled averaging for federated learning",
            "venue": "ICML,",
            "year": 2020
        },
        {
            "authors": [
                "J. Zhang",
                "Z. Huang",
                "E. Ohn-Bar"
            ],
            "title": "Coaching a teachable student",
            "venue": "CVPR,",
            "year": 2023
        },
        {
            "authors": [
                "T. Osa",
                "J. Pajarinen",
                "G. Neumann",
                "J.A. Bagnell",
                "P. Abbeel",
                "J. Peters"
            ],
            "title": "An algorithmic perspective on imitation learning",
            "venue": "In Foundations and Trends\u00ae in Robotics,",
            "year": 2018
        },
        {
            "authors": [
                "D. Chen",
                "B. Zhou",
                "V. Koltun",
                "P. Kr\u00e4henb\u00fchl"
            ],
            "title": "Learning by cheating",
            "venue": "CoRL,",
            "year": 2020
        },
        {
            "authors": [
                "O. Scheel",
                "L. Bergamini",
                "M. Wo\u0142czyk",
                "B. Osi\u0144ski",
                "P. Ondruska"
            ],
            "title": "Urban driver: Learning to drive from real-world demonstrations using policy gradients",
            "venue": "CoRL,",
            "year": 2021
        },
        {
            "authors": [
                "A. Behl",
                "K. Chitta",
                "A. Prakash",
                "E. Ohn-Bar",
                "A. Geiger"
            ],
            "title": "Label-efficient visual abstractions for autonomous driving",
            "venue": "IROS,",
            "year": 2020
        },
        {
            "authors": [
                "X. Liang",
                "T. Wang",
                "L. Yang",
                "E. Xing"
            ],
            "title": "CIRL: Controllable imitative reinforcement learning for vision-based self-driving",
            "venue": "ECCV,",
            "year": 2018
        },
        {
            "authors": [
                "Z. Zhang",
                "A. Liniger",
                "D. Dai",
                "F. Yu",
                "L. Van Gool"
            ],
            "title": "End-to-end urban driving by imitating a reinforcement learning coach",
            "venue": "ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "S. Ross",
                "G. Gordon",
                "D. Bagnell"
            ],
            "title": "A reduction of imitation learning and structured prediction to no-regret online learning",
            "venue": "AISTATS,",
            "year": 2011
        },
        {
            "authors": [
                "A. Prakash",
                "A. Behl",
                "E. Ohn-Bar",
                "K. Chitta",
                "A. Geiger"
            ],
            "title": "Exploring data aggregation in policy learning for vision-based urban autonomous driving",
            "venue": "CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "F. Codevilla",
                "A.M. Lopez",
                "V. Koltun",
                "A. Dosovitskiy"
            ],
            "title": "On offline evaluation of vision-based driving models",
            "venue": "ECCV,",
            "year": 2018
        },
        {
            "authors": [
                "K. Mangalam",
                "H. Girase",
                "S. Agarwal",
                "K.-H. Lee",
                "E. Adeli",
                "J. Malik",
                "A. Gaidon"
            ],
            "title": "It is not the journey but the destination: Endpoint conditioned trajectory prediction",
            "venue": "ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "K. Chitta",
                "A. Prakash",
                "B. Jaeger",
                "Z. Yu",
                "K. Renz",
                "A. Geiger"
            ],
            "title": "Transfuser: Imitation with transformer-based sensor fusion for autonomous driving",
            "venue": "PAMI,",
            "year": 2022
        },
        {
            "authors": [
                "S. Woo",
                "J. Park",
                "J.-Y. Lee",
                "I.S. Kweon"
            ],
            "title": "CBAM: Convolutional block attention module",
            "venue": "ECCV,",
            "year": 2018
        },
        {
            "authors": [
                "X. Wang",
                "F. Yu",
                "L. Dunlap",
                "Y.-A. Ma",
                "R. Wang",
                "A. Mirhoseini",
                "T. Darrell",
                "J.E. Gonzalez"
            ],
            "title": "Deep mixture of experts via shallow embedding",
            "venue": "ICML,",
            "year": 2018
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "CVPR,",
            "year": 2016
        },
        {
            "authors": [
                "T. Mikolov",
                "K. Chen",
                "G. Corrado",
                "J. Dean"
            ],
            "title": "Efficient estimation of word representations in vector space",
            "venue": "arXiv,",
            "year": 2013
        },
        {
            "authors": [
                "B. Kang",
                "Y. Li",
                "S. Xie",
                "Z. Yuan",
                "J. Feng"
            ],
            "title": "Exploring balanced feature spaces for representation learning",
            "venue": "ICLR,",
            "year": 2020
        },
        {
            "authors": [
                "A. Dosovitskiy",
                "G. Ros",
                "F. Codevilla",
                "A. Lopez",
                "V. Koltun"
            ],
            "title": "CARLA: An open urban driving simulator",
            "venue": "CoRL,",
            "year": 2017
        },
        {
            "authors": [
                "K. Ayush",
                "B. Uzkent",
                "C. Meng",
                "K. Tanmay",
                "M. Burke",
                "D. Lobell",
                "S. Ermon"
            ],
            "title": "Geographyaware self-supervised learning",
            "venue": "CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "H. Bilen",
                "A. Vedaldi"
            ],
            "title": "Universal representations: The missing link between faces, text, planktons, and cat breeds",
            "venue": "arXiv,",
            "year": 2017
        },
        {
            "authors": [
                "J. Bennett"
            ],
            "title": "OpenStreetMap",
            "venue": "Packt Publishing Ltd,",
            "year": 2010
        },
        {
            "authors": [
                "X. Huang",
                "P. Wang",
                "X. Cheng",
                "D. Zhou",
                "Q. Geng",
                "R. Yang"
            ],
            "title": "The apolloscape open dataset for autonomous driving and its application",
            "venue": "TPAMI,",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "Keywords: Global-scale Autonomous Driving, Imitation Learning, Transformer"
        },
        {
            "heading": "1 Introduction",
            "text": "Driving at scale involves a complex and nuanced decision-making process across diverse social and environmental conditions. For instance, a driving model for turning at intersections in San Francisco, CA may be able to generalize relatively well when deployed in Washington, DC, over 2400 miles away. However, when deployed just north of it in New York City, the model would discover that right turns on red, which have been banned in the city [1], could result in social disruption and potentially unsafe conditions at intersections. When deployed at intersections in Pittsburgh, PA, the model may begin accelerating at a green light only to be confounded by the frequent occurrence of the Pittsburgh left [2], resulting in frequent and uncomfortable braking. These examples illustrate how the lack of modeling of location-based traffic behavior and social norms can lead to potentially safety-critical consequences. Beyond city-level variability, failing to accurately account for state or country-level differences in traffic regulations and social norms can also have dire consequences, e.g., from the directionality of travel [3] to varying maximal speed limitations [4] or yielding expectations [5]. How can we design and learn models that flexibly accommodate the heterogeneous data encountered across challenging and diverse geographical, environmental, and social conditions?\nDespite recent advances in decision-making models for autonomous driving, models are often trained and evaluated within limited operational domains, i.e., a handful of geographical regions and social conditions (e.g., Waymo\u2019s service in Pheonix, AZ, and San Francisco, CA [6]). Autonomous driving benchmarks are often collected in a handful of cities and routes [7\u201310]. Existing frameworks for learning to drive (e.g., [11\u201315]) train a single policy without considering geo-location or policy adaptation. While such methods may be used to train and adapt different regional models, there are often many similarities which can be shared among the different locations and benefit potentially small and rare datasets with imbalanced distributions. In this work, we propose an approach for\n7th Conference on Robot Learning (CoRL 2023), Atlanta, USA.\nar X\niv :2\n30 9.\n12 29\n5v 2\n[ cs\n.C V\n] 2\n5 Se\np 20\n23\nlearning to modulate predictions across settings and locations, i.e., even in seemingly similar visual settings, within a single driving agent.\nWhile prior works have leveraged transfer learning [16\u201323], e.g., through access to unlabeled data of a target domain, or introduced internal layers that learn to adapt model output across various domains [24\u201328, 16, 29\u201331], these methods have exclusively focused on low-level object classification and detection tasks, and have not explicitly accounted for geographical priors or reasoning. In contrast, we study end-to-end models for learning safe perception and decision-making in intricate 3D navigation scenarios. In this case, to avoid a potential accident, perception and action characteristics must both be carefully tuned to consider geographical location when reasoning over traffic maneuvers and predicting social behavior. Moreover, the training process of our sensorimotor models may require order-of-magnitude higher sample complexity, i.e., due to the higher rarity of policy-level events and intricate maneuvers [32]. Thus, geo-aware model capacity should be explored jointly with approaches for efficient adaptation and parameter sharing, as we do in this work.\nContributions: We make three key contributions towards autonomous systems at scale: 1) We revisit current end-to-end driving models to identify limitations in learning from heterogeneous and distributed data sources. In particular, we build on recent advances in transformer-based models [33, 34] for learning high-capacity, geo-aware imitation learning agents that can adapt across geographical locations while sharing parameters and computation within a single network. 2) To facilitate efficient training across inherently imbalanced data distributions and maneuvers, we further generalize conditional imitation learning by designing a supervised contrastive loss over conditional commands and locations. 3) We combine three public autonomous driving datasets collected by different companies and platforms across 11 locations to extensively evaluate the impact of the proposed scalable learning framework. To understand generalization across diverse use-cases and model training regimens, we comprehensively analyze the benefits of our framework for various scalable deployment scenarios, including centralized (i.e., within a single company or server with shared raw data logs), distributed (i.e., with scalable federated computation), and semi-supervised (i.e., with unlabeled data) training."
        },
        {
            "heading": "2 Related Work",
            "text": "Learning to Drive from Demonstrations: Despite impressive recent advances in learning to drive, approaches often leverage simple navigation tasks, i.e., lane following, intersection turning, and basic collision avoidance (e.g., with CIL [12, 35, 15, 36\u201339, 13]), or short real-world routes in a handful of locations (e.g., [40, 11, 41, 42, 14, 43, 10, 44\u201346, 7]). We note that GPS localization in prior approaches may only be used to determine a next high-level command at an intersection [35], and not to learn regionally or socially appropriate decisions. Yet, training models among locations without such geo-awareness results in an ill-posed problem with ambiguous samples. Thus, our work can be seen as a natural generalization of goal-conditional imitation learning frameworks [35, 47, 13] to incorporate geographical information for learning a high-capacity and controllable model.\nDomain Adaptation: Model adaptation, i.e., from a source to a target domain with unlabeled data, has mostly focused on segmentation and detection tasks [16\u201322]. However, the robustness and reliability of current domain adaptation techniques at large scale have been repetitively questioned [48\u201350]. Moreover, the aforementioned techniques have not been previously studied within the more complex end-to-end training paradigm for decision-making models. Particularly relevant to our study are approaches that learn universal object detection models [24, 26] via self-attention and weighing feature channels based on the output of multiple parallel layers (i.e., adapters [51]). In contrast, our proposed cross-attention-based network architecture can more effectively fuse geographically-derived and visual features while also outperforming adapter-based methods [24].\nBenefits of Contrastive Learning: Researchers have been increasingly exploring the benefits of contrastive learning frameworks for learning generalized representations, even under imbalanced or long-tail settings [52\u201356]. Our main use-case inherently involves learning over diverse and imbalanced underlying data distributions. For instance, a Tesla may suddenly trigger a warning in a challenging scenario or an unsupported region, in which case small amounts of demonstration data from the driver may be collected and available for training. Moreover, although diverse and rare traffic scenarios can occur within any local city region or country, the underlying distribution of such events can significantly shift among locations. Recently, Mandi et al. demonstrated the benefits of unsupervised contrastive learning for improved imitation learning within simple robotic use-cases [57]. Instead, we demonstrate the benefits of supervised contrastive learning techniques (e.g., [55]) by designing a novel loss function for conditional imitation learning frameworks at scale.\nDistributed Learning to Drive: We comprehensively analyze our proposed approach across training paradigms suitable for scalable deployment in order to ensure the generalization of our findings. In particular, Federated Learning (FL) provides a natural framework for implementing AnyD in the real-world. The goal of FL to drive is to train a global model leveraging distributed data and models from different agents [58], i.e., where agents may avoid sharing raw driving logs and data due to various privacy and efficiency considerations. However, dealing with data heterogeneity among agents [59\u201362] remains a challenge. We demonstrate our novel geo-conditional mechanism to complement current federated learning algorithms. Somewhat surprisingly, our FL model variants result in outperformance compared to the centralized-trained counterparts due to the effective regional bias handling. We note that this is without having to share potentially sensitive geographical information, as our embedding matrix (defined in Sec. 3) is kept local and private in our implementation."
        },
        {
            "heading": "3 Method",
            "text": "We propose a geo-conditional agent (AnyD) which generalizes existing conditional imitation learning methods [35, 63] through two key aspects. First, we propose a novel network structure that leverages a multi-head transformer module for geo-aware adaptation of visual features across regions (Sec. 3.2). Second, we design a contrastive learning objective which regularizes training and\naddresses imbalances across locations and capture settings (Sec. 3.3). An overview of our approach is depicted in Fig. 2."
        },
        {
            "heading": "3.1 Problem Definition",
            "text": "Our objective is to learn a goal-directed agent that can effectively reason over varying traffic rules and social norms in complex and dynamic real-world settings. We leverage offline approaches relying on learning from driver demonstrations [64, 65, 40, 66, 67] as they can safely learn to map sensor observations to actions, i.e., as opposed to interactive methods [68, 69, 36, 70, 71]. As an example use-case, consider a deployed Tesla or Waymo fleet encountering challenging settings beyond its current constrained and geo-fenced deployment [72, 73]. Here, a human can take-over and demonstrate desired driving behavior which can subsequently be uploaded to a shared cloud server (i.e., centralized training) or updated to improve the model locally (i.e., federated training, we consider both cases in Sec. 3.4). However, current end-to-end agents that learn to drive in a data-driven manner, e.g., based on CIL [35, 12, 74, 13], do not differentiate among regional norms.\nGeo-Conditional Imitation Learning: We assume a dataset of demonstrations D = {(x,y)}Ni=1, i.e., measurements of x = (I, c, v,g) \u2208 X , where I \u2208 RW\u00d7H\u00d73 is an image of the current environment, v \u2208 R is the speed, c \u2208 N is a navigation command [35, 12], g \u2208 {0, 1}G is a region index encoded as a one-hot vector over a total G regions, and corresponding action labels y \u2208 Y based on human drivers. Consistently with prior work [75, 37, 76], we predict a waypoint-based label in the bird\u2019s eye view over the next five planned locations (2.5 seconds), such that y = {wt}5t=1 and wi \u2208 R2. The high-level waypoint output in the bird\u2019s eye view can also help standardize policy decisions across globally distributed platforms with heterogeneous sensor configurations [41]. In our work, we experiment with various definitions for g (manually defined city labels and unsupervised neighborhood-level labels, these do not require precise GPS localization). Moreover, we note AnyD does not rely on image-level perception labels or high-definition map information. As such, it can be trained based on cheaply collected GPS-based waypoint labels and benefit from rapid advancements in positioning technology (analysis of localization noise when training AnyD models can be found in the supplementary). We train a geographically-aware policy function \u03c0 : X \u2212\u2192 Y using supervised learning [12, 13]. Learning the policy \u03c0 in geo-conditional imitation learning requires carefully fusing image and geo-location information, i.e., as opposed to just basic concatenation. Next, we\nintroduce our network architecture which efficiently generalizes the branch-based architectures of CIL-based approaches [35]."
        },
        {
            "heading": "3.2 Geo-Conditional Transformer Module",
            "text": "To learn a scalable policy function, i.e., across cities, countries, and platforms, we design a single network which adapts its decisions based on an efficient multi-head module. The mechanism is motivated by transformer [34, 33, 77], with three main aspects. First, the queries are only conditioned on the part of the input, i.e., the region-based features. Second, we do not use spatial attention as in ViT-type architectures [33], but instead learn a low-dimensional channel weight vector which can be trained efficiently [77]. Third, while multi-head mechanisms have been used by prior methods [33], we propose to jointly predict a scalar weight for each head prior to the summation of the heads. This formulation is analogous to a mixture or adapter-based model [78, 28]. Our domain attention mechanism is implemented via a region token that enables the model to specialize the heads to specific domains or tasks and subsequently combine the heads based on the current appropriate region and decision. For instance, we identify the emergence of traffic rules, such as left vs. right-hand driving when inspecting the output of the learned heads in Sec. 4.\nOur model first extracts image features from the input image I. Subsequently, a multi-head transformer module computes channel weights for the features based on the current region definition g. Finally, the planner utilizes the re-weighted geo-aware features Fg and speed information v to generate waypoints for different commands. The command input c then selects the required waypoints y\u0302 for execution. The multi-head transformer module takes as input visual features extracted using a ResNet-34 encoder \u03d5 [79], F = \u03d5(I) \u2208 R8\u00d713\u00d7C and a regional embedding e = g\u22a4E (assuming a column vector g), extracted from a trainable embedding matrix E \u2208 RG\u00d7C [80]. We use C = 512 such that the output of the multi-head module is a 512-dimensional vector for weighting each channel in F and computing the geo-aware features Fg using the weighted and summed H output heads (Eqn. 3). The visual features are pooled (to accommodate the channel-wise attention), processed through a Fully Connected (FC) layer, and concatenated with a region token \u03b1, zI = [\u03b1,FC((Pool(F))] \u2208 R(C+1)\u00d7d, where d = 128 sets the number of hidden units. \u03b1 will be updated and used to weigh the multiple heads at the output of the module, as shown in Eqn. 3. Similarly, the region embedding zg = [\u03b1, FC(e)] \u2208 R(C+1)\u00d7d. The computation steps for the geo-aware transformer can then be summarized as:\nz = zI + Attention(LN(zI),LN(zg)) (1) z\u0302 = z + MLP(LN(z)) (2)\nFg = H\u2211 h=1 (\u03b1\u0302hz\u0302h,2:C+1)\u2297 F (3)\nwhere LN denotes Layer Normalization, \u2297 denotes channel-wise multiplication, and \u03b1\u0302 is the updated region token values. z in the second step is pooled before addition to make the shape consistent. We follow ViT [33] to compute attention as\nAttention(zI, zg) = softmax (QKT\u221a\nd\n) V (4)\nwhere Q = zgWQ, K = zIWK , V = zIWV and WQ \u2208 Rd\u00d7d, WK \u2208 Rd\u00d7d and WV \u2208 Rd\u00d7d are learned matrices. Unlike ViT, we do not merge the multiple heads by concatenating such that there are H outputs, each C+1-dimensional, i.e., z\u0302 \u2208 RH\u00d7(C+1). Here, the weights for the h-th head are stored at the first index of the head output vector, i.e., \u03b1\u0302h = z\u0302h,1. The adapted geo-aware features are then given to a command-conditional branch as shown in Fig. 2 for predicting the final waypoints. To optimize the network, we leverage a contrastive loss function over maneuvers and regional decisions, as discussed next."
        },
        {
            "heading": "3.3 Contrastive Imitation Learning",
            "text": "Loss Function: Standard supervised learning approaches for imitation learning leverage an L1 loss, i.e., behavior cloning, between predicted and demonstrated ground-truth waypoints [71, 12]. However, in our case of highly heterogeneous and imbalanced data over maneuvers and regions, this loss can result in poor performance and overfitting to local biases [81, 55]. In particular, the distribution of both conditional commands c and regions g can be highly skewed, with certain critical events (e.g., turns) occurring at a much lower frequency. While we employ a branched architecture [35, 13], a subset of the branches may be trained over a fraction of the total samples, i.e., with most updating the \u2018forward\u2019 branch. We hypothesize that such imbalances can introduce noisy predictions from poorly-trained branches. When adding the additional complexity of learning region-conditional policies, issues in data imbalance and heterogeneity compound. To tackle this practical safety-critical issue, Chen et al. [65] employed a privileged teacher (i.e., learned from complete ground truth observations of the 3D surroundings instead of raw images) that can be used for additional sampling and data augmentation. However, training such a privileged expert requires extensive annotation of realworld data, which is not scalable. Instead, we propose to introduce command and region-contrastive objectives as a simple and effective strategy for improving model optimization and providing more supervision when handling imbalanced data. In our analysis, we demonstrate the utility of this approach for both vanilla CIL and the proposed geo-CIL. As far as we are aware, we are the first to empirically analyze such benefits for imitation-learned driving agents at scale.\nWe propose to incorporate two additional terms in addition to the main behavior cloning loss, LBC . The total loss can be computed as\nL = LBC + \u03bbcLcmd\u2212ct + \u03bbgLg\u2212ct (5)\nwhere \u03bbc, \u03bbg are hyperparameters, Lcmd\u2212ct and Lg\u2212ct are contrastive losses. Next, we define each of the proposed loss terms.\nCommand Contrastive Loss: The branched conditional command architecture of CIL updates each branch based on a subset of the data samples in each batch B. As some commands are highly underrepresented in natural driving data, we propose a command contrastive loss that leverages predictions for other commands for the same sample as negative examples,\nLcmd\u2212ct = \u2212 1 |B| \u2211 i\u2208B log exp(d(y\u0302cii ,yi)/\u03c4)\u2211 c exp(d(y\u0302 c i ,yi)/\u03c4)\n(6)\nwhere the loss is computed over a single positive example with prediction outputted by the ground truth command branch ci, and the other predictions (i.e., hypothetical commands) as negatives. d is a similarity function (we use negative L2 distance) and \u03c4 \u2208 R+ is a scalar temperature parameter. The command contrastive loss is a natural extension supervised contrastive learning [55] to our case of conditional imitation learning, with two key differences. First, we do not apply it to the feature space (as commonly done) but instead to the output space of the network, i.e., in order to better model differences among maneuvers. Second, the command contrastive loss is not computed over all other samples in the same batch with different commands as in standard supervised contrastive loss [55]. While this practice may work for simple classification tasks, in our case such other samples tend to also involve different driving situations. We found leveraging other samples in this manner when training an imitation model to degrade policy performance, most likely due to the added learning complexity and ambiguity. A similar reasoning can be applied to improve optimization of the geoconditioned transformer module, as discussed next.\nGeo-Contrastive Loss: While driving behavior across cities and regions can often be similar, in practice, the cities in our employed datasets (detailed in Sec. 4) all have unique local characteristics effectively modeled. Thus, we also propose to incorporate a region (i.e., city)-based contrastive loss. We apply the loss within the transformer module over different output head weights \u03b1\u0302 \u2208 RH . Here, we follow standard contrastive loss implementation [55] and select the i-th sample as an anchor.\nDuring training, for the i-th sample in a batch, positive samples P(i) are defined within the same city, while negative samples N (i) from differing cities,\nLg\u2212ct = \u2212 1 |B| \u2211 i\u2208B 1 |P(i)| log \u2211 p\u2208P(i) exp(d(\u03b1\u0302i, \u03b1\u0302p)/\u03c4)\u2211 a\u2208A(i) exp(d(\u03b1\u0302i, \u03b1\u0302a)/\u03c4)\n(7)\nwhere A(i) \u2261 P(i) \u222aN (i) and d is a similarity function (we use negative L2 distance)."
        },
        {
            "heading": "3.4 Scalable Training Settings",
            "text": "We comprehensively analyze the training of our model using three different scalable deployment settings. First, in Centralized Learning (CL), agents are able to share all sensor data and geographical information with a centralized server. Consequently, the server conducts supervised learning of our AnyD model over all of the raw data. To further analyze model scalability, we implement a SemiSupervised Learning (SSL) model, which can leverage ample unlabeled data that may be available across locations (we follow [13]). Finally, we study the applicability of our findings within federated learning approaches, as sharing raw sensor data can be inefficient or even potentially undesirable. For instance, our AnyD agent may be distributed over numerous heterogeneous data sources with various constraints, i.e., local regulations, legal authorities, and privacy requirements or preferences. Thus, we also analyze a Federated Learning (FL) agent which does not require sharing raw and geographical information with a server. To fully understand the role of our proposed network structure within such paradigms, we optimize AnyD using two federated learning algorithms, FedAvg[58] and FedDyn[59]. We note that the geographical embedding matrix E which contains city-level information remains locally updated on each agent (i.e., akin to a form of local model personalization). In this manner, E reduces to a row vector as the embedding vector for the specific region (city in our implementation). We note that this results in the removal of the geo-contrastive loss term Lg\u2212ct in Eqn. 7. The supplementary contains additional details regarding our implementation."
        },
        {
            "heading": "4 Experiments",
            "text": "In this section, we first introduce our combined multi-city benchmark extracted from multiple publicly available datasets. Specifically, we present ablation studies for various model design choices and loss terms. To understand the benefits of the proposed framework on various training schemes, we also report analysis with three different training paradigms. This ensures our model and findings are relevant across real-world use-cases, e.g., with efficient distributed settings at large-scale."
        },
        {
            "heading": "4.1 Datasets and Metrics",
            "text": "To learn a global scale driving policy, we conduct training on data from three different datasets including Argoverse 2 (AV2) [9], nuScenes (nS) [10] and Waymo (Waymo) [7]. While these datasets do not have any official waypoints prediction benchmark, we extract these from the provided raw data logs. Specifically for each frame, we processing the raw data to get the future 2.5s as ground truth waypoints, current velocity, a front-view RGB image, navigational commands and a city-level information. The data spans 11 cities. We split the data into training, validation and testing data. Our split utilizes 190k, 20k, and 35k training samples for AV2, nS and Waymo datasets respectively. We follow standard evaluation using Average L2 Displacement Error (ADE) and Final L2 Displacement Error (FDE) over future waypoints in the BEV space. We also evaluate closed-loop policy performance using CARLA [82]. While CARLA benchmarks do not generally involve regional modeling, we simulate left-hand driving and town-varying behavior of agents. Our supplementary provides additional details and experiments, e.g., regarding unsupervised geo-location clustering mechanisms and closed-loop evaluation.\nModel and Loss Ablation: We study the underlying architecture of the model in Table 1. We find that replacing intermediate image-level heatmaps (used in several CIL-based baselines [65, 13]) with fully-connected layers provides improved reasoning for our diverse perspective settings (reducing ADE from 1.24 to 1.16). We also demonstrate our geo-conditional transformer framework with three heads to outperform other supervision choices, e.g., embedding concatenation and supervision as an auxiliary prediction task as in Ayush et al. [83] (1.09 vs. 1.15 ADE).\nAnyD also outperforms another attention-based method e.g., Hybrid ViT [33] on concatenated image-city features (1.09 vs. 1.20 ADE), which validates its efficiency on adaptation. Moreover, the proposed geo-conditional module can be used to increase the modeling capacity of the agent, and thus can scale beyond simple task supervision. We also find a holistic effect among the proposed loss terms, with a combination leading to the best results (1.93 vs 2.08 FDE for the vanilla behavior cloning loss). Fig. 3 depicts a case where AnyD handles diverse traffic regulations and social norms such as turning right (wider turn) in Singapore and yielding a \u2018Pittsburgh left.\u2019\nTraining Paradigms: Table 2 reports the impact of various model training schemes on ADE performance (additional details, including FDE-based analysis, are in the supplementary). We observe consistent improvements across paradigms and cities even with severe data imbalance. Moreover, leveraging unlabeled YouTube data for each city results in further gains, specifically for cities with lesser data (MTV, PAO, SGP, and PHX). For instance, MTV improves from 1.40 to 1.23 ADE due to the unlabeled data, showing the importance of this mechanism for our use-case. Overall, the model is shown to outperform the baseline of Zhang et al. [13], which does not leverage geo-location. Consistently with prior work, federated learning algorithms tend to under-perform their centralized counterparts. Yet, due to better handling of local biases AnyD is shown to benefit federated learning, surpassing centralized training (0.98 vs. 1.05 ADE).\nClosed-Loop Evaluation: The results of the closed-loop experiments on CARLA are shown in Table 3. We find consistent improvements in success rate, route completion, and infractions (the supplementary video shows qualitative examples where the baseline model struggles with left-hand driving). We compute both open and closed-loop metrics by saving the expert actions for the test sequences. AnyD outperforms the baseline planner [13] on both open-loop metrics (reducing ADE from 0.58 to 0.46) and closed-loop metrics (improving driving score by 38%, from 0.36 to 0.50). Nonetheless, overall success rates are quite low for our benchmark, as it contains significant behavior variability. This highlights the challenging nature of the region-aware decision-making task for current imitation learning models, either in the real-world or in simulation.\n5 Conclusion\nWe envision large-scale navigation agents that can seamlessly operate in heterogeneous and distributed locations. Towards this goal, our work introduces an efficient framework for training and adapting a universal high-capacity navigation agent across diverse locations and settings. Using our proposed agent, fleets of vehicles can increasingly grow their operation capacity to novel conditions, i.e., by involving humans and collecting both unlabeled or labeled demonstration data for policy training. Nonetheless, effectively incorporating geo-awareness into driving models\nremains a challenging and under-explored research problem."
        },
        {
            "heading": "6 Limitations",
            "text": "Despite the multiple publicly available datasets used in our experiments, the diversity in existing benchmarks is still limited, i.e., compared to the vast diversity of geo-locations and events that an agent may encounter in the real-world. Besides Singapore, which provides a challenging generalization use-case , data logs in current datasets are often captured over short drives and are biased towards the US. Thus, our framework requires further validation with larger-scale settings with increased diversity in the future. Here, while our approach for learning a unified model is motivated by human drivers that efficiently learn to adapt generalized skills across locations (including traffic direction), it can be potentially challenging to learn a single model across drastically differing locations. Finally, incorporating various explicit constraints and specifications (e.g., of local traffic rules) could also be studied in the future in order to enable efficient agent adaptation."
        },
        {
            "heading": "Acknowledgments",
            "text": "This research was supported by a Red Hat Research Grant, Army Research Office Grant W911NF2110246, National Science Foundation grants (CCF-2007350, CCF-1955981, and IIS2152077), and AFRL Contract no. FA8650-22-C-1039."
        },
        {
            "heading": "7 Supplementary",
            "text": "We provide additional implementation details, including network architecture and training protocol, as well as additional analysis, including ablative studies, results on CARLA, and additional qualitative examples."
        },
        {
            "heading": "7.1 Implementation Details",
            "text": "This section first provides details regarding our proposed network architecture and discuss differences with baseline models (Sec. 7.1.1). Next, we provide details regarding the processing of the driving datasets to construct the multi-city benchmark used throughout the analysis (Sec. 7.1.2). Finally, we discuss evaluation settings (Sec. 7.1.3) and training protocol (Sec. 7.1.4)."
        },
        {
            "heading": "7.1.1 Architecture and Baselines",
            "text": "We leverage an ImageNet-pretrained ResNet-34 [79] as our backbone \u03d5. All images are resized to 400\u00d7 225 prior to being inputted to the model. To leverage diverse camera viewpoints, we build on prior models in conditional imitation learning [65, 13] and train a direct image-to-BEV prediction model, i.e., without assuming a fixed known BEV perspective transform. Moreover, we also find the removal intermediate image-level heatmaps [65, 13] (and directly regressing the BEV waypoints) to improve model performance. Fig. 4 compares our proposed network architecture for image-toBEV planning to a standard baseline architecture (e.g., [13, 65]). Prior image-based models may utilize deconvolutional layers to obtain an image-aligned heatmap and followed by a soft-argmax (\u2018SA\u2019 in Fig. 4) and 2D waypoint projection to the BEV space. The projection can be implemented either using a homography (i.e., known extrinsic parameters [65, 38]) or with a learned projection layer [13]. In contrast, we find it beneficial to remove the intermediate image-level processing and directly predict BEV waypoints, as shown in Fig. 4(b). We replace the upsampling layers with a 3\u00d7 3 convolutional layer which fuses the image and speed-based features prior to inputting to three fully-connected final prediction layers. By removing unnecessary processing steps and enabling more expressive image-to-BEV mappings, the proposed planner architecture improves from 2.45 FDE to 2.17 FDE. This improvement comes with a minimal gain in parameters (23.8M vs. 24.1M). Incorporating the geo-conditional module with three adapters further improves trajectory prediction performance to 1.93 FDE with a 24.2M parameter model. We do not find it necessary to leverage more complex, e.g., GRU-based [38], prediction heads.\nBaselines: While related methods are often studied in simulation [38, 65], we provide several baselines by following publicly available implementations. In particular, we leverage the state-of-the-art monocular agent TCP [38]. To ensure meaningful comparison with single-frame models, we also remove the temporal refinement module. As TCP leverages a control prediction branch in addition to the waypoint prediction branch, we normalize the raw control signals among the different vehicle platforms across the multiple datasets to [0, 1]. When comparing with the semi-supervised learning scheme of SelfD [13], we leverage a 10-hour YouTube driving dataset with available city (i.e., region) descriptions. In this manner, we are able to use AnyD to pseudo-label videos that were taken within the 11 cities leveraged in our experiments. Subsequently, we mix the datasets to train a semi-supervised AnyD model, which is then evaluated over our multi-city benchmark."
        },
        {
            "heading": "7.1.2 Data Processing and Distributions",
            "text": "To obtain BEV waypoints for training, we standardize formats across three datasets, Argoverse 2 (AV2) [9], nuScenes (nS) [10] and Waymo (Waymo) [7]. The datasets provide post-processed world coordinates for each frame obtained from GPS and other mounted sensors, e.g., LiDAR [10]. We leverage these reported ego-poses to generate a waypoint prediction benchmark. For each frame, we use the global coordinate as an intermediate to get relative positions of the future 2.5s as ground truth waypoints. The conditional command (left, forward, or right) is inferred in a semi-automatic process. First, we extract the preliminary command by thresholding the curvature of the trajectory. However, this process cannot detect subtle maneuvers, such as lane changes, which are included in our dataset. Consequently, we manually verify and annotate the initial automatic command predictions. For our geo-conditional module, we do not require accurate GPS information as we quantize each latitude and longitude into a city-level cluster. Each dataset provides a front-view RGB image and speed (either from the raw CAN bus or from the positioning information), which are inputted into our model as observations.\nThe data spans 11 cities: Pittsburgh (PIT), Washington, DC (WDC), Miami (MIA), Austin (ATX), Palo Alto (PAO) and Detroit (DTW), Boston (BOS), Singapore (SGP), Phoenix (PHX), San Francisco (SFO) and Mountain View (MTV). The data distribution across different cities is shown in Fig. 5."
        },
        {
            "heading": "7.1.3 Evaluation Metrics and Settings",
            "text": "Open-Loop Evaluation: Average Displacement Error (ADE) and Final Displacement Error (FDE) are standard metrics for trajectory prediction [75]. We first compute the error within each city, and then average to obtain a balanced average metric. We also generate more fine-grained analysis by providing a breakdown over 11 semantic events in the dataset, as will be further discussed in Sec. 7.2. The extracted events include left turns, forward command, right turns, highway driving, heavy downtown traffic, red traffic lights, stop signs, uncontrolled intersections, pedestrian crossings, rain, and construction zones.\nCARLA Evaluation: The open-loop evaluation measures the distance of waypoint predictions with real-world human drivers under complex maneuvers, including yielding, merging, and irregular intersections. To further validate our proposed approach, we sought to evaluate AnyD in closed-loop settings where continuous predictions are made in order to navigate a vehicle to a destination along a route. While closed-loop real-world evaluation is challenging due to safety requirements, we leverage the CARLA [82] simulator. Yet, standard CARLA evaluation does not generally involves social and regional behavior that is dynamic across towns. Subsequently, models do not currently incorporate regional modeling, and GPS information is solely used to determine a command at intersections along a route. Hence, motivated by our 11-city real-world benchmark, we introduce a new benchmark where different towns have different traffic behavior. Our benchmark is defined over Town 1, Town 2, and Town 10. We have modified Town 2 for left-hand driving and added pedestrians and vehicles with more aggressive behaviors, e.g., with jaywalking, higher speeds, and closer proximity, to Town 10. We tune the autopilot\u2019s controller in order to generate optimal behavior under the novel\nsettings. When performing closed-loop evaluation in CARLA, we also compute a Driving Score (DS) [85], which is a product between the route completion and a penalty based on infractions."
        },
        {
            "heading": "7.1.4 Training Protocol",
            "text": "We study the role of our proposed network for scalable deployment use-cases using three training paradigms. To ensure standardized training across both centralized and federated training, we train the model using Stochastic Gradient Descent (SGD) [59]. In centralized training (where all the raw observation data is shared in a single server), we use a batch size of 48 and train for 7,500 iterations. We set the initial learning rate to 1e-1, learning rate decay as 0.997, and weight decay as 1e-3. The loss hyper-parameters are set as \u03bbc = 1e-3, \u03bbg = 1e-4, \u03bbd = 1e-4. For semi-supervised training settings, model training is done in three stages. We first download a set of online videos based on their tag which provides city-level information. We then train a supervised model using the same hyper-parameters as in centralized training above, and pseudo-label the unlabeled videos. We train three models from different initial seeds to compute a confidence score (i.e., variance) for each pseudo-label and filter low-confidence predictions. Subsequently, we train our model using the original training dataset combined with the large pseudo-labeled dataset. Here, we set the initial learning rate to 1e-3 and train the model for 500 iterations. Finally, for a fair comparison between the centralized training and the federated training settings, we train our federated model for 1,500 synchronous communication rounds. We treat each city as its own \u2018node\u2019 or \u2018device,\u2019 but do not share the private geo-embedding with the server (i.e., we aggregate all model parameters on the server using FedAvg [58] and FedDyn [59] excluding E). For each communication round, the model is updated for five local iterations with SGD (in this manner, total iterations remain at 7,500). We further note that we remove the geo-contrastive loss term Lg\u2212ct in the federated learning settings (as this information is not shared among the locations). We keep all other hyper-parameters fixed throughout the training settings."
        },
        {
            "heading": "7.2 Additional Ablation and Results",
            "text": "To supplement our findings in the main paper, we discuss four additional results. First, we provide supplementary analysis in terms of FDE (corresponding to Table 2 of the main paper with ADE), context and event-based performance evaluation (Sec. 7.2.1). Second, we perform additional ablation studies regarding the role of the number of heads in the multi-head module, impact of GPS noise over waypoints ground-truth in training, and clusters of the neighborhood-level models (Sec. 7.2.2). Third, we analyze an adaptation experiment to a novel city in Sec. 7.3 and show additional qualitative waypoint prediction results in Sec. 7.3.1. Finally, we perform closed-loop evaluation results using the introduced CARLA benchmark."
        },
        {
            "heading": "7.2.1 Additional Analysis",
            "text": "Final Displacement Error: For completeness, we report FDE results across training paradigms and models in Table 4. While FDE is more challenging as it emphasizes long-term prediction, we observe similar trends among the models and cities compared to the complementary ADE-based analysis in the main paper. Specifically, we demonstrate AnyD to improve over our baseline even using this harsher metric, i.e., from 2.55 to 1.93 average FDE. Semi-Supervised Learning (SSL) provides further gains compared to the centralized AnyD for most cities (excluding ATX and DTW, which show slight under-performance). When compared with Federated Learning (FL), the improvement is less pronounced compared to the gains observed with ADE and FL. While some cities are shown to significantly benefit FL over CL (e.g., BOS, SGP, PHX, SFO, MTV) others do not (e.g., MIA and ATX). ATX is a small dataset with limited diversity and high speed variability. While evaluation becomes less reliable, a low-shot learning setting can also be studied to understand such challenges in the future.\nEvent-Driven Analysis: Table 5 shows a breakdown of driving performance over different events and maneuvers. Here, we see a significant benefit for the introduced regional awareness, higher\nmodeling capacity, and more balanced contrastive objective. For instance, AnyD improves performance over the unevenly distributed commands (a ratio of 2 : 20 : 3 among left:forward:right in our dataset), for right command from 1.43 ADE with the planner and up to 1.19 ADE with AnyD. Other conditions, such as highway, downtown, crossings, and rain conditions all show improvements as well. These results suggest that the situational adapters are able to accommodate various conditions both within and across geo-locations."
        },
        {
            "heading": "7.2.2 Ablation Studies",
            "text": "Number of Heads : We investigated the impact of increasing the number of heads in Table 6 on the performance of the model in cities with different amounts of data. We observe that when cities have a sufficient amount of data (defined as more than 10,000 frames), the error rate decreases and remains consistently low as the number of heads increases. This can be attributed to the increased model capacity, allowing for more effective feature extraction and improved learning of city-specific patterns. However, when the training data is limited, increasing the number of heads can still result in worse performance on some of the cities, potentially due to overfitting the small data sample. Thus, efficiently increasing model capacity in small-data domains remains a challenge.\nFig. 6 studies the frequency of the head of the highest weight. Notably, our results indicate that the distribution in Singapore, where left-hand driving is practiced, differs from that of other cities. Additionally, the unique tropical scenery of Miami gives rise to a distinctive pattern as well.\nEffects of Geo-Conditional Transformer: Fig. 7 shows the visual effect of the attention pattern of the selected head (head with the highest weight) under the same observations, given different region embeddings. Given Boston embeddings, the head concentrates more on the objects in the middle and right portion of the image. While the Singapore embeddings guide attention towards the left portion of the image, which is attributable to the left-hand driving scenario in Singapore. We note\nthat our supplementary contains additional ablations regarding number of heads in the multi-head attention module and the impact of dataset size on training such higher capacity models.\nGround-Truth Noise Analysis: To understand the role of scalable real-world deployment and data collection, we analyze the impact of potential GPS error in Table 7. While our analyzed datasets carefully post-process the reported world coordinates, we envision AnyD deployed across more diverse and potentially noisy settings. We therefore report the performance degradation due to the addition of 1m and 3m Gaussian noise over the training waypoints. Specifically, we find that even with added noise, AnyD obtains decent prediction performance, outperforming prior state-of-the-art models that are trained with clean waypoints, e.g., 2.39 FDE with 3m noise vs. 2.45 FDE for the baseline [65, 68]).\nUnsupervised Clustering of Regions: In Table 8, we explore the benefit of finer-grained regional clustering choices, i.e., within each city, when defining g. This experiment can uncover potential\nbenefits from modeling intra-city settings. To achieve this, we utilize GPS trace data from OpenStreetMap (OSM)[86] and cluster cities into sub-regions based on traffic patterns. For example, MIA clustering results in semantic regions, e.g., downtown vs. beach areas, with large improvements in prediction performance for the finer-grained model (from 1.60 to 1.11 ADE). Similarly, WDC is clustered into downtown and highway regions, also benefiting performance (from 1.27 to 1.09 ADE). We note that while these examples suggest our model can further benefit model improvement, such clustering data may not be available for many locations, and thus cannot always be assumed.\nTable 8 further explores the benefits of finer-grained clustering on AnyD model performance, i.e., within city neighborhoods. To achieve this, we employ GPS trace data from OpenStreetMap (OSM) [86] and divide cities into sub-regions based on traffic patterns via K-means clustering. Fig. 8 shows example clustering results in MIA and WDC with respect to K = 3 and K = 10 clusters. For instance, the downtown areas of both two cities can be seen as clusters for both K = 3 and K = 10. While somewhat coarse in its clustering, we find that this privileged region information (which may not always be available) can introduce additional benefits when used as g during model training and evaluation."
        },
        {
            "heading": "7.3 Adaptation to a New City",
            "text": "In practice, a geo-aware model may be required to learn to drive in a previously unseen region with a significant domain gap. To understand the model performance of AnyD under such learning settings, we extract an additional city (Guangzhou, China) from a different dataset, ApolloScape [87]. In this case, a large domain gap occurs due to the differing social norms and traffic density in China. We mix the new city with the prior 11, and continue fine-tuning the model. Our results in Table 9 indicate that AnyD can learn to drive in the new city while also maintaining similar performance levels for the previously observed cities (i.e., without forgetting). In contrast, the baseline model, which does not incorporate the explicit geo-aware module, has higher error while also impacting performance on prior seen cities."
        },
        {
            "heading": "7.3.1 Qualitative Results",
            "text": "We show additional qualitative results in Fig. 9 and Fig. 10 (on the next page). As depicted in the figures, the AnyD generally provides better performance on challenging tasks of navigating across different regions and events, including turns and merging (requires reasoning over traffic directionality) as well as speed limits. Fig. 11 depicts failure cases, which show challenging conditions involving rare rule violations by other vehicles."
        }
    ],
    "title": "Learning to Drive Anywhere",
    "year": 2023
}