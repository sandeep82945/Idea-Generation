{
    "abstractText": "Natural language generation (NLG) is one of the most impactful fields in NLP, and recent years have witnessed its evolution brought about by large language models (LLMs). As the key instrument for writing assistance applications, they are generally prone to replicating or extending offensive content provided in the input. In low-resource data regime, they can also lead to repetitive outputs [1]. Usually, offensive content and repetitions are mitigated with post-hoc methods, including n-gram level blocklists, top-k and nucleus sampling. In this paper, we apply non-exact repetition suppression using token and sequence level unlikelihood loss, and further explore the framework of unlikelihood training objective in order to jointly endow the model with abilities to avoid generating offensive words and phrases from the beginning. Finally, with comprehensive experiments, we demonstrate that our proposed methods work exceptionally in controlling the repetition and content quality of LLM outputs.",
    "authors": [
        {
            "affiliations": [],
            "name": "Minghui Zhang"
        },
        {
            "affiliations": [],
            "name": "Weixin Cai"
        }
    ],
    "id": "SP:ea08ade56d991a33c231b1ac7fbc25f1c4e2996e",
    "references": [
        {
            "authors": [
                "A. Holtzman",
                "J. Buys",
                "L. Du",
                "M. Forbes",
                "Y. Choi"
            ],
            "title": "The curious case of neural text degeneration",
            "venue": "arXiv preprint arXiv:1904.09751, 2019.",
            "year": 1904
        },
        {
            "authors": [
                "T. Brants",
                "A.C. Popat",
                "P. Xu",
                "F.J. Och",
                "J. Dean"
            ],
            "title": "Large language models in machine translation",
            "venue": "2007.",
            "year": 2007
        },
        {
            "authors": [
                "T. Brown",
                "B. Mann",
                "N. Ryder",
                "M. Subbiah",
                "J.D. Kaplan",
                "P. Dhariwal",
                "A. Neelakantan",
                "P. Shyam",
                "G. Sastry",
                "A. Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in neural information processing systems, vol. 33, pp. 1877\u20131901, 2020.",
            "year": 1877
        },
        {
            "authors": [
                "S. Welleck",
                "I. Kulikov",
                "S. Roller",
                "E. Dinan",
                "K. Cho",
                "J. Weston"
            ],
            "title": "Neural text generation with unlikelihood training",
            "venue": "arXiv preprint arXiv:1908.04319, 2019.",
            "year": 1908
        },
        {
            "authors": [
                "A. Fan",
                "M. Lewis",
                "Y. Dauphin"
            ],
            "title": "Hierarchical neural story generation",
            "venue": "arXiv preprint arXiv:1805.04833, 2018.",
            "year": 1805
        },
        {
            "authors": [
                "A.K. Vijayakumar",
                "M. Cogswell",
                "R.R. Selvaraju",
                "Q. Sun",
                "S. Lee",
                "D. Crandall",
                "D. Batra"
            ],
            "title": "Diverse beam search: Decoding diverse solutions from neural sequence models",
            "venue": "arXiv preprint arXiv:1610.02424, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "Z. Fu",
                "W. Lam",
                "A.M.-C. So",
                "B. Shi"
            ],
            "title": "A theoretical analysis of the repetition problem in text generation",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, vol. 35, pp. 12848\u201312856, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "D.M. Ziegler",
                "N. Stiennon",
                "J. Wu",
                "T.B. Brown",
                "A. Radford",
                "D. Amodei",
                "P. Christiano",
                "G. Irving"
            ],
            "title": "Fine-tuning language models from human preferences",
            "venue": "arXiv preprint arXiv:1909.08593, 2019.",
            "year": 1909
        },
        {
            "authors": [
                "J. Devlin",
                "M.-W. Chang",
                "K. Lee",
                "K. Toutanova"
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805, 2018.",
            "year": 1810
        },
        {
            "authors": [
                "N. Reimers",
                "I. Gurevych"
            ],
            "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
            "venue": "arXiv preprint arXiv:1908.10084, 2019.",
            "year": 1908
        },
        {
            "authors": [
                "M. Lewis",
                "Y. Liu",
                "N. Goyal",
                "M. Ghazvininejad",
                "A. Mohamed",
                "O. Levy",
                "V. Stoyanov",
                "L. Zettlemoyer"
            ],
            "title": "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
            "venue": "arXiv preprint arXiv:1910.13461, 2019.",
            "year": 1910
        },
        {
            "authors": [
                "T. Kudo",
                "J. Richardson"
            ],
            "title": "Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing",
            "venue": "arXiv preprint arXiv:1808.06226, 2018.",
            "year": 1808
        },
        {
            "authors": [
                "R.J. Williams",
                "D. Zipser"
            ],
            "title": "A learning algorithm for continually running fully recurrent neural networks",
            "venue": "Neural computation, vol. 1, no. 2, pp. 270\u2013280, 1989.",
            "year": 1989
        },
        {
            "authors": [
                "C.-Y. Lin"
            ],
            "title": "Rouge: A package for automatic evaluation of summaries",
            "venue": "Text summarization branches out, pp. 74\u201381, 2004.",
            "year": 2004
        },
        {
            "authors": [
                "T. Zhang",
                "V. Kishore",
                "F. Wu",
                "K.Q. Weinberger",
                "Y. Artzi"
            ],
            "title": "Bertscore: Evaluating text generation with bert",
            "venue": "arXiv preprint arXiv:1904.09675, 2019.",
            "year": 1904
        },
        {
            "authors": [
                "T. Sellam",
                "D. Das",
                "A.P. Parikh"
            ],
            "title": "Bleurt: Learning robust metrics for text generation",
            "venue": "arXiv preprint arXiv:2004.04696, 2020.",
            "year": 2004
        }
    ],
    "sections": [
        {
            "text": "Keywords: unlikelihood loss, repetition suppression, content moderation\n1\nar X\niv :2\n30 4.\n10 61\n1v 2\n[ cs\n.C L\n] 5\nJ un\n2 02"
        },
        {
            "heading": "1. Introduction",
            "text": "Over the years, large language models have become more impactful as they are being applied to an increasing range of features and products [2, 3]. LLMs have been widely employed in various writing assistance applications, aiding users in generating high-quality, human-like text. However, despite their immense potential, LLMs often struggle with certain limitations that prevent them from further mimicking actual human-written content. Specifically, generations from LLMs sometimes contain repetitions [1] and controversial phrases (political topics, racial topics, etc.). Repetition refers to the tendency of LLMs to produce sentences or phrases that are either identical or very similar to each other within the generated text. This can lead to a lack of diversity in the output and cause the generated content to appear unnatural, monotonous, or even nonsensical. The problem of repetition can be particularly severe in low-resource data regimes where the model has limited training data to learn from, leading to a higher likelihood of generating repetitive content. Another significant challenge associated with LLMs is the generation of controversial or offensive phrases. LLMs are trained on vast amounts of text data from the internet, and as a result, they may inadvertently learn to generate content that is politically biased, racially insensitive, or otherwise offensive. This can pose a significant risk for applications that rely on LLMs for content generation, as it may lead to the dissemination of harmful or inappropriate content.\nCurrently, mainstream solutions for both problems are posthoc, rule-based methods, including n-gram level blocklists, top-k and nucleus sampling [1], which are suboptimal. Not only are they not effective enough - words may go through post-filters if they are not exact matches, but they also degrade the output by breaking its coherence.\nTo overcome these limitations, in this paper we first review the effectiveness of repetition removal method based on sentence embedding, and then we adopt token and sequence level unlikelihood training objective [4] as the non-exact repetition suppression solution. Furthermore, we extend the unlikelihood training objective and customize it to jointly address a broader range of problems. Finally, we demonstrate through experiments that our approach Pareto-dominates the baseline method and can help suppress repetition and moderate content with minimum impact to model performance."
        },
        {
            "heading": "2. Related Work",
            "text": "Various techniques have been proposed to tackle the repetition problem in language models. Some of these methods include modifying the decoding algorithm, such as using nucleus sampling [1], top-k sampling [5] and diverse beam\nsearch [6]. Another approach to suppress repetition is to penalize repeated n-grams during the decoding process. Holtzman et al. [1] acknowledge the problem of repetition and propose a new decoding strategy to mitigate it. Welleck et al. [4] try to tackle this problem by customizing training objectives. Fu et al. [7] provide a theoretical framework to systematically analyze the root cause of repetition and the reason behind existing techniques.\nContent moderation in LLMs aims to prevent the generation of offensive or controversial content. Several methods have been proposed to mitigate this issue, including the use of blocklists, adversarial training, reinforcement learning from human feedback [8], or rule-based filters. In practice, postprocessing filters like blocklists and offensiveness classifiers are still most economic and prevalent."
        },
        {
            "heading": "3. Non-Exact Repetition Suppression",
            "text": "Since its inception, BERT [9] has been used as a tool for encoding sentences and generating their representations. However, BERT isn\u2019t pre-trained to be natively support sentence similarity detection. SentenceBERT [10], being a solution to this issue, makes structural changes on top of BERT for training and inference respectively with cosine-similarity in mind. In practice, we calculate the similarity score for any two sentences in a paragraph generated by the LLM, and only keep one sentence in any pair with a score above a predetermined threshold. This SentenceBERT-based method is a great representative of repetition detection & removal approaches. Figure 1 contains the architectures of SentenceBERT during training (left) and inference (right).\nAs we discussed above in the introduction, repetition prevention is a more ideal way to address the issue than post-hoc repetition removal. We adopt the unlikelihood (UL) training objective in Welleck et al. [4]. Specifically, ordinary likelihood training objective minimizes the loss for ground truth tokens:\nLtLikelihood(p\u03b8(\u00b7|x<t)) = \u2212 log p\u03b8(xt|x<t) (1)\nWhere p\u03b8 denotes the language model with parameters \u03b8, and t is the current time step.\nIn unlikelihood training objective, higher loss is applied to a set of tokens, called negative candidates, and thus discourages the model from generating them. Token-level UL objective defines negative candidates CtUL\u2212token as previous tokens within the target (ground truth).\nLtUnlikelihood(p\u03b8(\u00b7|x<t), Ct) = \u2212 \u03b1 \u2211 c\u2208Ct log(1\u2212 p\u03b8(c|x<t))\u2212 log p\u03b8(xt|x<t) (2)\nCt = CtUL\u2212token = {x1, . . . , xt\u22121} \\ {xt} (3)\nCoefficient \u03b1 controls the proportion of the unlikelihood loss term.\nSequence-level UL objective follows the same principle, except that it defines negative candidates as previous duplicate n-grams within the predicted sequence.\nCt = CtUL\u2212seq = {xt} if (xt\u2212i, . . . , xt+j) \u2208 x<t\u2212i for any i+ j + 1 = n (4)"
        },
        {
            "heading": "4. Content Moderation",
            "text": "The unlikelihood training objectives, with both token-level UL loss and sequence-level UL loss, are not standalone loss functions. In fact, they propose a platform with the concept of negative candidates, and having repetition-related context can be seen as a special case. Based on this idea, we further generalize the scope of negative candidates and define a new loss term called \u201cblock loss\u201d. In block loss, the negative candidates are defined as blocklist phrases Bn that we have collected in advance.\nCt = Ctblock = 10\u22c3 n=2 Ctblock\u2212n (5)\nCtblock\u2212n = {xt} if (xt\u2212i, . . . , xt+j) \u2208 Bn for any i+ j + 1 = n\n(6)"
        },
        {
            "heading": "5. Experiments",
            "text": "First, we evaluate the performance of SentenceBERT-based post-processing methods. Then, we compare different aspects of unlikelihood training objectives. Lastly, we show the efficacy of our novel block loss in the setting of sequencelevel UL training objective."
        },
        {
            "heading": "5.1. Experimental Setup",
            "text": "We use a sequence-to-sequence text rewriting task to evaluate the approaches discussed in the sections above, with bullet points as inputs and paragraphs as outputs. In practice, this task could translate to writing assistant products or\nfeatures capable of generating fluent documents based on user-provided outlines.\nWe use pre-trained BART model [11] as the common starting point and apply different training objectives during finetuning. The baseline model is fine-tuned until convergence with regular likelihood loss. Another model is fine-tuned from the starting point with token-level UL objective. The last model is fine-tuned with sequence-level UL objective on top of the previous model. Following Welleck et al. [4], sequence-level UL objective is applied 50% of the time during fine-tuning, with the other 50% of the time using model\u2019s original token-level training objective.\nInputs are encoded by SentencePiece tokenizer [12]. Teacher forcing [13] is applied during training to ensure a faster and more stable convergence. We use beam search as the decoding strategy with beam size 5 across all experiments. All training is done on an Azure ML Compute instance with 4 Nvidia V100 16GB GPUs."
        },
        {
            "heading": "5.2. Data",
            "text": "Our data is in a simple format, as each data sample consists of one paragraph as ground truth and its corresponding list of bullet points in pure text format. The bullet is denoted by an asterisk (\u2217), and the sub-bullet is denoted by double asterisk (\u2217\u2217). The size of the training set, validation set, and test set are 45k, 6.6k, 1.4k respectively. Bullet points are summarized by human annotators from paragraphs in public Word documents corpus, and unqualified samples (too long, offensive, etc.) are removed in advance. Below is one example sample of the dataset:\n{\"bullet_points\": \"Evans, CMO of Subaru of America, Inc. (SOA) * 20+yrs experience * oversaw demand generation, brand awareness * customer engagement programs * SOA revenue increase ** 40% to 65%of parent company\",\n\"paragraph\": \"Evans boasts over 20 years of experience in the automotive industry, most recently as CMO of Subaru of America, Inc. (SOA). In that position, he oversaw demand generation, brand awareness and customer engagement programs that directly contributed to the company\u2019s highest years of sales growth in its history. SOA\u2019s revenue increased from 40% to 65% of parent company Fuji Heavy Industries\u2019 (FHI) overall revenue during Evans\u2019 tenure, with FHI\u2019s stock the best performer on the Nikkei.\"}\nFor the evaluation of sequence-level training objective with block loss, we use a subset of the original dataset, with 50% samples containing blocklist phrases & 50% regular samples, in order to better demonstrate the results and save computational cost."
        },
        {
            "heading": "5.3. Evaluation Metrics",
            "text": "We record an extensive range of metrics during evaluation in four categories: language model metrics, repetition metrics, summarization/translation-based metrics, and grammatical metrics. Below is a list of itemized metric descriptions.\nLanguage model metrics measure the basic capabilities of language models, namely how well they can predict the next token.\n\u2022 Perplexity (ppl)\n\u2022 Next-token prediction accuracy (acc)\nRepetition metrics measure the duplicativeness and uniqueness of generation on both token level and sequence level.\n\u2022 Repetition (rep-l): the fraction of next-token prediction that occur in previous l tokens\n\u2022 Wrong repetition (wrep-l): similar to rep-l, but only counts token repeats that are not equal to gold next token\n\u2022 Portion of duplicate 1-grams (seq-rep-1)\n\u2022 Portion of duplicate 4-grams (seq-rep-4)\n\u2022 Number of unique next-token predictions (uniq)\n\u2022 Number of unique tokens in the generated paragraph (uniq-seq)\nSummarization/translation-based metrics measure the semantic consistency between model\u2019s input and output.\n\u2022 ROUGE [14]: similarity between prediction & ground truth measured with overlap n-grams & longest common subsequence\n\u2022 BERTScore [15]: similarity between prediction & ground truth measured with BERT representations\n\u2022 BLEURT [16]: a transfer learning-based NLG metric\nGrammatical metrics measure the prevalence of grammar mistakes in model outputs, and thus monitor the impact on grammatical performance.\n\u2022 Percentage of sentences with grammar mistakes (percgrammar-wrong)\n\u2022 Number of grammar mistakes per sentence (countgrammar-wrong)"
        },
        {
            "heading": "5.4. Results",
            "text": "In table 1, SentenceBERT, as a post-hoc method, is Pareto dominated by unlikelihood objectives regardless of former\u2019s threshold choice, which justifies our transition from post processing to custom training objective. In table 2, tokenlevel UL objective significantly reduces token repetition, while additional training with sequence-level UL objective further reduces repetitive n-grams by a huge margin. Overall, unlikelihood training objectives help suppress repetition with minimum impact to performance.\nTable 5 lists a set of generations from different models given the same outline. Baseline output suffers from severe repetition problem whereas the outputs of models trained\nwith unlikelihood objectives do not, and thus provide a better text semantically and syntactically. Outline contains additional line breaks and indentations for better illustration. Ground truth (the original paragraph) is also attached for reference.\nIn terms of training time, as shown in table 3, token-level unlikelihood objective has a similar result as regular likelihood objective, which is expected since the former can reduce to the latter when \u03b1 = 0 in equation (2). In contrast, sequencelevel UL objectives are much more time-consuming. This order-of-magnitude difference between two levels can be explained by the iterative n-gram checks during the identification of sequence-level negative candidates.\nIn table 4, sequence-level UL objective with our novel block loss reduces the number of outputs containing unwanted blocklist phrases. Similarly, a set of generations from different models as well as corresponding ground truth are listed in table 6. The word \u201dabortion\u201d is in the blocklist and successfully avoided in the output of the model fine-tuned with sequence-level UL objective with block loss."
        },
        {
            "heading": "6. Conclusions & Future Work",
            "text": "In this paper, we explored a joint effort of non-exact repetition suppression and content moderation to address the limitations of LLMs in generating repetitive and offensive content. We first set up SentenceBERT as a baseline of repetition detection & post-processing methods. Then, we utilized multiple levels of unlikelihood training objectives to suppress repetition at the step of generation. Finally, we exploited and further generalized the unlikelihood training objective and brought it into the field of content moderation. We demonstrated that our proposed methods work exceptionally well in controlling the repetition and content quality of LLM outputs. The results showed that multi-level\nunlikelihood training objectives and our novel block loss greatly reduce token repetition, repetitive n-grams and offensive content while maintaining minimum impact on model performance.\nIn the future, we plan to explore more possibilities for the unlikelihood objective framework and negative candidates, as well as expanding the potential applications of our proposed methods in more complex NLP scenarios. Additional investigation could be performed on the impact of the size and prevalence of the blocklist."
        }
    ],
    "title": "Joint Repetition Suppression and Content Moderation of Large Language Models",
    "year": 2023
}