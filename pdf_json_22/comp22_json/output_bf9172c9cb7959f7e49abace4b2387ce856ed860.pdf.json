{
    "abstractText": "\u00a92022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. Abstract\u2014 We present an approach for safe trajectory planning, where a strategic task related to autonomous racing is learned sample-efficient within a simulation environment. A high-level policy, represented as a neural network, outputs a reward specification that is used within the cost function of a parametric nonlinear model predictive controller (NMPC). By including constraints and vehicle kinematics in the NLP, we are able to guarantee safe and feasible trajectories related to the used model. Compared to classical reinforcement learning (RL), our approach restricts the exploration to safe trajectories, starts with a good prior performance and yields full trajectories that can be passed to a tracking lowest-level controller. We do not address the lowest-level controller in this work and assume perfect tracking of feasible trajectories. We show the superior performance of our algorithm on simulated racing tasks that include high-level decision making. The vehicle learns to efficiently overtake slower vehicles and to avoid getting overtaken by blocking faster vehicles.",
    "authors": [
        {
            "affiliations": [],
            "name": "Rudolf Reiter"
        },
        {
            "affiliations": [],
            "name": "Jasper Hoffmann"
        },
        {
            "affiliations": [],
            "name": "Joschka Boedecker"
        },
        {
            "affiliations": [],
            "name": "Moritz Diehl"
        }
    ],
    "id": "SP:b0327530b023bbe89a9803b3d7db9b8b9c00b98a",
    "references": [
        {
            "authors": [
                "J. Betz",
                "H. Zheng",
                "A. Liniger",
                "U. Rosolia",
                "P. Karle",
                "M. Behl",
                "V. Krovi",
                "R. Mangharam"
            ],
            "title": "Autonomous vehicles on the edge: A survey on autonomous vehicle racing",
            "venue": "IEEE Open Journal of Intelligent Transportation Systems, vol. 3, pp. 458\u2013488, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "B. Paden",
                "M. \u010c\u00e1p",
                "S.Z. Yong",
                "D. Yershov",
                "E. Frazzoli"
            ],
            "title": "A survey of motion planning and control techniques for self-driving urban vehicles",
            "venue": "IEEE Transactions on Intelligent Vehicles, vol. 1, no. 1, pp. 33\u201355, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "J.B. Rawlings",
                "D.Q. Mayne",
                "M.M. Diehl"
            ],
            "title": "Model Predictive Control: Theory, Computation, and Design, 2nd ed",
            "year": 2017
        },
        {
            "authors": [
                "P. Wurman",
                "S. Barrett",
                "K. Kawamoto",
                "J. MacGlashan",
                "K. Subramanian",
                "T. Walsh",
                "R. Capobianco",
                "A. Devlic",
                "F. Eckert",
                "F. Fuchs",
                "L. Gilpin",
                "P. Khandelwal",
                "V. Kompella",
                "H. Lin",
                "P. MacAlpine",
                "D. Oller",
                "T. Seno",
                "C. Sherstan",
                "M. Thomure",
                "H. Kitano"
            ],
            "title": "Outracing champion gran turismo drivers with deep reinforcement learning",
            "venue": "Nature, vol. 602, pp. 223\u2013228, 02 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J.L. V\u00e1zquez",
                "M. Br\u00fchlmeier",
                "A. Liniger",
                "A. Rupenyan",
                "J. Lygeros"
            ],
            "title": "Optimization-based hierarchical motion planning for autonomous racing",
            "venue": "2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 2397\u20132403, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "K.P. Wabersich",
                "M.N. Zeilinger"
            ],
            "title": "A predictive safety filter for learning-based control of constrained nonlinear dynamical systems",
            "venue": "2021.",
            "year": 2021
        },
        {
            "authors": [
                "C. Greatwood",
                "A.G. Richards"
            ],
            "title": "Reinforcement learning and model predictive control for robust embedded quadrotor guidance and control",
            "venue": "Autonomous Robots, vol. 43, no. 7, pp. 1681\u2013 1693, Oct. 2019. [Online]. Available: http://link.springer.com/10. 1007/s10514-019-09829-4",
            "year": 2019
        },
        {
            "authors": [
                "B. Brito",
                "M. Everett",
                "J.P. How",
                "J. Alonso-Mora"
            ],
            "title": "Where to go next: Learning a subgoal recommendation policy for navigation in dynamic environments",
            "venue": "IEEE Robotics and Automation Letters, vol. 6, pp. 4616\u20134623, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "L. Brunke",
                "M. Greeff",
                "A.W. Hall",
                "Z. Yuan",
                "S. Zhou",
                "J. Panerati",
                "A.P. Schoellig"
            ],
            "title": "Safe learning in robotics: From learning-based control to safe reinforcement learning",
            "venue": "ArXiv, vol. abs/2108.06266, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J. Lubars",
                "H. Gupta",
                "S. Chinchali",
                "L. Li",
                "A. Raja",
                "R. Srikant",
                "X. Wu"
            ],
            "title": "Combining reinforcement learning with model predictive control for on-ramp merging",
            "venue": "2021.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Song",
                "D. Scaramuzza"
            ],
            "title": "Learning high-level policies for model predictive control",
            "venue": "2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 7629\u20137636, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "B. Zarrouki",
                "V. Kl\u00f6s",
                "N. Heppner",
                "S. Schwan",
                "R. Ritschel",
                "R. Vo\u00dfwinkel"
            ],
            "title": "Weights-varying mpc for autonomous vehicle guidance: a deep reinforcement learning approach",
            "venue": "2021 European Control Conference (ECC), 2021, pp. 119\u2013125.",
            "year": 2021
        },
        {
            "authors": [
                "A. Liniger",
                "J. Lygeros"
            ],
            "title": "A noncooperative game approach to autonomous racing",
            "venue": "IEEE Transactions on Control Systems Technology, vol. 28, no. 3, pp. 884\u2013897, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "R. Spica",
                "E. Cristofalo",
                "Z. Wang",
                "E. Montijano",
                "M. Schwager"
            ],
            "title": "A real-time game theoretic planner for autonomous two-player drone racing",
            "venue": "IEEE Transactions on Robotics, vol. 36, no. 5, pp. 1389\u20131403, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "M. Wang",
                "Z. Wang",
                "J. Talbot",
                "J.C. Gerdes",
                "M. Schwager"
            ],
            "title": "Gametheoretic planning for self-driving cars in multivehicle competitive scenarios",
            "venue": "IEEE Transactions on Robotics, vol. 37, no. 4, pp. 1313\u2013 1325, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S.L. Cleac\u2019h",
                "M. Schwager",
                "Z. Manchester"
            ],
            "title": "Algames: A fast augmented lagrangian solver for constrained dynamic games",
            "venue": "2021. [Online]. Available: https://arxiv.org/abs/2104.08452",
            "year": 2021
        },
        {
            "authors": [
                "W. Schwarting",
                "T. Seyde",
                "I. Gilitschenski",
                "L. Liebenwein",
                "R. Sander",
                "S. Karaman",
                "D. Rus"
            ],
            "title": "Deep latent competition: Learning to race using visual control policies in latent space",
            "venue": "2021. [Online]. Available: https://arxiv.org/abs/2102.09812",
            "year": 2021
        },
        {
            "authors": [
                "H.G. Bock",
                "K.J. Plitt"
            ],
            "title": "A multiple shooting algorithm for direct solution of optimal control problems",
            "venue": "Proceedings of the IFAC World Congress. Pergamon Press, 1984, pp. 242\u2013247.",
            "year": 1984
        },
        {
            "authors": [
                "S. Gros",
                "M. Zanon"
            ],
            "title": "Data-driven economic nmpc using reinforcement learning",
            "venue": "IEEE Transactions on Automatic Control, vol. 65, no. 2, p. 636\u2013648, Feb 2020. [Online]. Available: http://dx.doi.org/10.1109/TAC.2019.2913768",
            "year": 2020
        },
        {
            "authors": [
                "M. O\u2019Kelly",
                "H. Zheng",
                "D. Karthik",
                "R. Mangharam"
            ],
            "title": "F1tenth: An open-source evaluation environment for continuous control and reinforcement learning",
            "venue": "Proceedings of Machine Learning Research, vol. 123. [Online]. Available: https://par.nsf.gov/biblio/10221872",
            "year": 1872
        },
        {
            "authors": [
                "N. Li",
                "E. Goubault",
                "L. Pautet",
                "S. Putot"
            ],
            "title": "Autonomous racecar control in head-to-head competition using Mixed-Integer Quadratic Programming",
            "venue": "Opportunities and challenges with autonomous racing, 2021 ICRA workshop, Online, United States, May 2021. [Online]. Available: https://hal.archives-ouvertes.fr/hal-03749355",
            "year": 2021
        },
        {
            "authors": [
                "R. Reiter",
                "M. Diehl"
            ],
            "title": "Parameterization approach of the frenet transformation for model predictive control of autonomous vehicles",
            "venue": "Proceedings of the European Control Conference (ECC), 2021.",
            "year": 2021
        },
        {
            "authors": [
                "R. Reiter",
                "M. Kirchengast",
                "D. Watzenig",
                "M. Diehl"
            ],
            "title": "Mixed-integer optimization-based planning for autonomous racing with obstacles and rewards",
            "venue": "Proceedings of the IFAC Conference on Nonlinear Model Predictive Control (NMPC), 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S.H. Nair",
                "E.H. Tseng",
                "F. Borrelli"
            ],
            "title": "Collision avoidance for dynamic obstacles with uncertain predictions using model predictive control",
            "venue": "arXiv preprint arXiv:2208.03529, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "D. Kloeser",
                "T. Schoels",
                "T. Sartor",
                "A. Zanelli",
                "G. Frison",
                "M. Diehl"
            ],
            "title": "NMPC for racing using a singularity-free path-parametric model with obstacle avoidance",
            "venue": "Proceedings of the IFAC World Congress, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "R.S. Sutton",
                "A.G. Barto"
            ],
            "title": "Reinforcement learning: An introduction",
            "venue": "MIT press,",
            "year": 2018
        },
        {
            "authors": [
                "D. Silver",
                "G. Lever",
                "N. Heess",
                "T. Degris",
                "D. Wierstra",
                "M. Riedmiller"
            ],
            "title": "Deterministic policy gradient algorithms",
            "venue": "International conference on machine learning. PMLR, 2014, pp. 387\u2013395.",
            "year": 2014
        },
        {
            "authors": [
                "T. Haarnoja",
                "A. Zhou",
                "K. Hartikainen",
                "G. Tucker",
                "S. Ha",
                "J. Tan",
                "V. Kumar",
                "H. Zhu",
                "A. Gupta",
                "P. Abbeel"
            ],
            "title": "Soft actor-critic algorithms and applications",
            "venue": "arXiv preprint arXiv:1812.05905, 2018.",
            "year": 1812
        },
        {
            "authors": [
                "O. Yadan"
            ],
            "title": "Hydra - a framework for elegantly configuring complex applications",
            "venue": "Github, 2019. [Online]. Available: https: //github.com/facebookresearch/hydra",
            "year": 2019
        },
        {
            "authors": [
                "T. Akiba",
                "S. Sano",
                "T. Yanase",
                "T. Ohta",
                "M. Koyama"
            ],
            "title": "Optuna: A next-generation hyperparameter optimization framework",
            "venue": "Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, 2019, pp. 2623\u20132631.",
            "year": 2019
        },
        {
            "authors": [
                "R. Verschueren",
                "G. Frison",
                "D. Kouzoupis",
                "J. Frey",
                "N. van Duijkeren",
                "A. Zanelli",
                "B. Novoselnik",
                "T. Albin",
                "R. Quirynen",
                "M. Diehl"
            ],
            "title": "acados \u2013 a modular open-source framework for fast embedded optimal control",
            "venue": "Mathematical Programming Computation, Oct 2021.",
            "year": 2021
        },
        {
            "authors": [
                "G. Frison",
                "M. Diehl"
            ],
            "title": "HPIPM: a high-performance quadratic programming framework for model predictive control",
            "venue": "Proceedings of the IFAC World Congress, Berlin, Germany, July 2020.",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "\u00a92022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.\nI. INTRODUCTION\nMotion planning for autonomous racing is challenging due to the fact that vehicles operate at the performance limits. Furthermore, planning requires interactive and strategic, yet safe behavior. In this work we focus on strategic planning for fixed opponent policies with safety guarantees. Current research usually is based on either graph-based, samplingbased, learning-based or optimization-based planners [1], [2]. We propose a combination of model-predictive control (MPC) and a neural network (NN), trained by a reinforcement learning (RL) algorithm within simulation. MPC is a powerful optimization-based technique that is used commonly for solving trajectory planning and control problems. The use of efficient numerical solvers and the possibility to incorporate constraints directly [3] makes MPC attractive in terms of safety, explainability and performance. Nevertheless, in problems like interactive driving, it is difficult to model the behavior of other vehicles. In contrast to MPC, RL is an exploration-driven approach for solving optimal control problems. Instead of an optimization friendly model, RL only requires samples of the dynamics and can in theory optimize over arbitrary cost functions. The flexibility of RL comes at the cost of a high sample inefficiency that is often unfavorable for real-world applications, where data is expensive and\n1Department of Microsystems Engineering, University Freiburg, 79110 Freiburg, Germany {rudolf.reiter, moritz.diehl}@imtek.uni-freiburg.de\n2Neurorobotics Lab, University Freiburg, 79110 Freiburg, Germany {hoffmaja, jboedeck}@informatik.uni-freiburg.de\n3Department of Mathematics, University Freiburg, 79110 Freiburg, Germany rare. Furthermore, RL in the general setting lacks safetyguarantees. However, once the amount and quality of data is sufficient, the learned policies can show impressive results [4]. In this paper, we combine MPC and RL by using an MPC-inspired low-level trajectory planner to yield kinematic feasible and safe trajectories and use the high-level RL policy for strategic decision-making. We use the expression MPP (Parameterized Model Predictive Planner) to refer to a MPCbased planner, which outputs feasible reference trajectories that we assume to be tracked by a lowest-level control systems. This hierarchical approach is common in automotive software stacks [2], [5]. We use the MPP to formulate safetycritical constraints and basic time-optimal behavior but let the cost function be subject to changes by the high-level RL policy. Particularly, we propose an interface where the highlevel RL policy outputs a reference in the Frenet coordinate frame. With this approach, we start with a good prior strategy for known model parts and can guarantee safe behavior with respect to the chosen vehicle model and the prediction of the opponents. The structure of this paper is as follows. In Sec. II, we motivate our approach by a similar formulation named safety filter [6], in Sec. IV we describe the MPCbased planner and in Sec. V we explain the implementation of the high-level RL policy and how we train it. Finally, in Sec. VI we evaluate the algorithm, which we refer to as HILEPP (Hierarchical Learning-based Predictive Planner), in a multi-agent simulation which involves strategic decision making. Contribution: We contribute by a derivation and evaluation of a sample-efficient and safe motion planning algorithm for autonomous race-cars. It includes a novel cost function formulation for the interaction of MPC and RL with a strong prior performance, real-time applicability and high interpretability. Related work: Several works consider RL as a set-point generator for MPC for autonomous agents [7], [8]. As opposed to our approach, they focus on final target-points. Another research branch focuses on safety verification with a socalled \u201dsafety-filter\u201d [9]. For instance, in [6], a rudimentary MPC variant is proposed that considers constraints by using MPC as a verification module. Similarly, the authors of [10] use MPC as a correction of an RL policy, if a collision check fails. RL is also used with the intention of MPC weight tuning, such as in [11] for UAVs and in [12] for adaptive control in autonomous driving. Related research for motion planning of autonomous racing was recently surveyed in [1]. Several works focus on local planning without strategic considerations [4], [5], thus can not directly be used in multiar X iv :2 21 2. 01 60 7v 1 [ cs .R O ] 3 D ec 2 02 2\nagent settings. Other works use a game-theoretic framework [13] which, however, often limits the applicability due to its complexity. An algorithm for obtaining Nash equilibria is iterated best response (IBR), as shown for drone racing in [14] or for vehicle racing in [15]. However, IBR has high computation times. An algorithm aiming at the necessary KKT conditions of the generalized Nash equilibrium problem is presented in [16]. However, the resulting optimization problem is hard to solve. In [17], long term strategic behavior is learned through simulation without safety considerations."
        },
        {
            "heading": "II. BACKGROUND AND MOTIVATION",
            "text": "A trained neural network (NN) as function approximation for the policy \u03c0\u03b8(s), where \u03b8 \u2208 Rn\u03b8 is the learned parameter vector and s \u2208 Rns is the RL environment state, can generally not guarantee safety. Safety is related to constraints for states and controls that have to be satisfied at all times. Therefore, the authors in [6] propose an MPC-based policy \u03c0S : Rna \u2192 Rna that projects the NN output a \u2208 Rna to a safe control uS = \u03c0S(x, a), where it is guaranteed that uS \u2208 US \u2286 Rna . The safe set US is defined for a known (simple) system model x\u0307 = f(x, u) with states x and controls u and corresponding, often tightened, constraints. In this formulation, the input u has the same interpretation as the action a and the state x relates to the model inside the filter. Constraint satisfaction for states is expressed via the set membership x \u2208 X and for controls via u \u2208 U . The system model is usually transformed to discrete time via an integration function xi+1 = F (xi, ui) with step size \u2206t. When using direct multiple shooting [18] one obtains decision variables for the state X = [x0, . . . , xN ] \u2208 Rnx\u00d7(N+1) and for the controls u = [u0, . . . , uN\u22121] \u2208 Rnu\u00d7N . Since the optimization problem can only be formulated for a finite horizon, a control invariant terminal set St, needs to be included. The safety filter solves the following optimization problem\nminimize X,U\n\u2016u0 \u2212 a\u0304\u20162R\nsubject to x0 = x\u03040, xN \u2208 St, xi+1 = F (xi, ui), i = 0, . . . , N \u2212 1, xi \u2208 X , ui \u2208 U , i = 0, . . . , N \u2212 1 (1)\nand takes the first control u\u22170 of the solution (X \u2217, U\u2217) as output uS := u\u22170. The authors in [6] use the filter as a postprocessing safety adaption, while we propose to use this formulation as a basis for an online filter, even during learning, which makes it applicable for safety relevant environments. We do not require the same physical inputs to our filter, rather modifications to a more general optimization problem, similar to [19]. We propose a more general interface between the high-level RL policy and MPC, namely a more general cost function L(X,U, a), modified by action a. Our general version of the MPP as fundamental part of the algorithm\nsolves the optimization problem\nminimize X,U L(X,U, a)\nsubject to x0 = x\u03020, xN \u2208 St, xi+1 = F (xi, ui), i = 0, . . . , N \u2212 1, xi \u2208 X , ui \u2208 U , i = 0, . . . , N \u2212 1, (2)\nand takes the optimal state trajectory of the solution (X\u2217, U\u2217) as output Xref := X\u2217 of the MPP algorithm."
        },
        {
            "heading": "III. GENERAL METHOD",
            "text": "We aim at applying our algorithm to a multi-agent vehicle competition on a race-track. Our goal is to obtain a planner that performs time-optimal trajectory planning, avoids interactive obstacles and learns strategic behavior, such as blocking other vehicles in simulation. We assume fixed policies of the opponents and therefore, do not consider the interaction as a game-theoretical problem [20]. We use an obstacle avoidance rule, according to the autonomous racing competitions Roborace [21] and F1TENTH [22], where in a dueling situation the following vehicle (FV) is mainly responsible avoiding a crash, however the leading vehicle (LV) must not provoke a crash. Unfortunately, to the best of the authors knowledge, there is no rigorous rule for determining the allowed actions of dueling vehicles. However, we formalize the competition rules of F1TENTH similar to [23], where the LV only avoids an inevitable crash, which we state detailed in Sec. IV-B.2. A block diagram of our proposed algorithm is shown in Fig. 1, where we assume a multiagent environment with a measured state z \u2208 Rnz , which concatenates the ego agent states x, the obstacle/opponent vehicle states xob and the road curvature \u03ba(\u03b6i) on evaluation points \u03b6i. We include prior domain knowledge to get the\nhigh-level RL policy state s \u2208 Rns with the pre-processing function s = gs(z). For instance, we use relative distances of the opponents to the ego vehicle, instead of absolute values. An expansion function P = GP (a), with the high-level RL policy a = \u03c0\u03b8(s), is used to increase the dimension of the NN output in order to obtain a parametric cost function. The expansion function is used to include prior knowledge and to obtain an optimization-friendly cost function in the MPP."
        },
        {
            "heading": "IV. PARAMETERIZED MODEL PREDICTIVE PLANNER",
            "text": "Our core component MPP constitutes an MPC formulation that accounts for safety and strong initial racing performance. It comprises a vehicle model, safety constraints and a parameterized cost function, which we will explain in the following section.\nA. Vehicle Model\nWe use rear-wheel centered kinematic single-track vehicle models in the Frenet coordinate frame, as motivated in previous work [24]. The models are governed by the longitudinal force Fd that accounts for accelerating and braking, and the steering rate r which is the first derivative of the steering angle \u03b4. The most prominent resistance forces Fres(v) = cairv\n2+crollsign(v) are included. The air drag depends on the vehicle speed v with constant cair. The rolling resistance is proportional to sign(v) by the constant croll. We drop the sign function, since we only consider positive speed. As shown in previous work [24], [25], the Frenet transformation F(\u00b7) relates Cartesian states xC = [xe ye \u03d5]>, where xe and ye are Cartesian positions and \u03d5 is the heading angle, to the curvilinear states\nxF = F(xC) = [\u03b6 n \u03b1]>. (3) The Frenet states are related to the center lane \u03b3(\u03b6) = [\u03b3x(\u03b6) \u03b3y(\u03b6)], with signed curvature \u03ba(\u03b6) and tangent angle \u03d5\u03b3(\u03b6), where \u03b6 is the 1d-position of the closest point of the center lane, n is the lateral normal distance and \u03b1 is the difference of the vehicle heading angle to the tangent of the reference curve. Under mild assumptions [24], the Frenet transformation and its inverse\nxC = F\u22121(xF) = \u03b3x(\u03b6)\u2212 n sin(\u03d5\u03b3(\u03b6))\u03b3y(\u03b6) + n cos(\u03d5\u03b3(\u03b6)) \u03d5\u03b3(\u03b6)\u2212 \u03b1  (4) are well defined. We use indices \u201dego\u201d (or no index) and \u201dob-no\u201d to indicate the ego or the no-th opponent vehicle and summarize the states x = [ \u03b6 n \u03b1 v \u03b4 ]> and\ncontrols u = [ Fd r ]> . We formulate the Frenet frame vehicle model with parameters for mass m and length l as\nx\u0307 = f(x, u) =  v cos(\u03b1) 1\u2212n\u03ba(\u03b6) v sin(\u03b1) v l tan(\u03b4)\u2212 \u03ba(\u03b6)v cos(\u03b1) 1\u2212n\u03ba(\u03b6)\n1 m (Fd \u2212 Fres(v))\nr  . (5) The discrete states xk at sampling time k\u2206t are obtained by an RK4 integration function xk+1 = F (xk, uk,\u2206t)."
        },
        {
            "heading": "B. Safety Constraints",
            "text": "As stated in Sec. II, the MPP formulation should restrict trajectories Xref to be within model constraints. Since we assume known vehicle parameters and no measurement noise, this can be guaranteed for most limitations in a straight-forward way. Nevertheless, the interactive behavior of the opponent vehicles poses a serious challenge to the formulation. On one extreme, we could model the other vehicles robustly, which means we account for all possible maneuvers which yields quite conservative constraints. On the other extreme, with known parameters of all vehicles, one could model the opponent safety by \u201dleaving space\u201d for at least one possible motion of the opponent without a crash, thus not forcing a crash. The later leads to an hard bi-level optimization problem, since the feasibility problem, which is an optimization problem itself, is needed as constraint of the MPP. In this work, we aim at a heuristic explained in Sec. IV-B.2.\n1) Vehicle Limitations: Slack variables \u03c3 = [\u03c3v, \u03c3\u03b1, \u03c3n, \u03c3\u03b4, \u03c3a, \u03c3o]\n> \u2208 R6, with slacks for states \u03c3v, \u03c3\u03b1, \u03c3n, \u03c3\u03b4 , acceleration \u03c3a and obstacles \u03c3o are used to achieve numerically robust behavior. We use box constraints for states\nBx(\u03c3) := { x \u2223\u2223\u2223\u2223 \u2212\u03c3n + n \u2264n \u2264 n+ \u03c3n, (6a) \u2212\u03c3\u03b1 + \u03b1 \u2264\u03b1 \u2264 \u03b1+ \u03c3\u03b1, (6b)\n0 \u2264v \u2264 v + \u03c3v, (6c) \u2212\u03c3\u03b4 + \u03b4 \u2264\u03b4 \u2264 \u03b4 + \u03c3\u03b4 } , (6d)\nand controls\nBu := { u \u2223\u2223 F d \u2264 Fd \u2264 F d, r \u2264 r \u2264 r}. (7)\nFurther, we use a lateral acceleration constraints set\nBlat(\u03c3) :=\n{ x \u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223v2 tan(\u03b4)l \u2223\u2223\u2223\u2223 \u2264 alat + \u03c3a } , (8)\nto account for friction limits. 2) Obstacle Constraints: We approximate the rectangular shape of obstacles in the Cartesian coordinate frame by an ellipse and the ego vehicle by a circle [26]. We assume a predictor of an opponent vehicle i that outputs the expected Cartesian positions of the vehicle center pobik = [x obi e,k y obi e,k ] > \u2208 R2 with a constraint ellipse shape matrix \u03a3\u0302obik (x) \u2208 R2\u00d72 at time step k that depends on the (Frenet) vehicle state in x. The ellipse area is increased by \u03a3obi(x) = \u03a3\u0302obi(x) + I(rego + \u2206r)2 with radii of the ego covering circle rego and a safety distance \u2206r. Since the ego vehicle position p> = [xe ye] is measured at the rear axis and in order to have a centered covering circle, we project the rear position to the ego vehicle center pmid by\npmid = [ xe,mid ye,mid ] = P (xC) = [ xe + l 2 cos\u03d5\nye + l 2 sin\u03d5\n] (9)\nFor obstacle avoidance with respect to the ellipse matrix, we use the constraint set in compact notation\nBO(x ob,\u03a3ob, \u03c3) ={ x \u2208 R2 \u2223\u2223\u2223 \u2225\u2225P (F\u22121(x))\u2212 pob\u2225\u22252 (\u03a3ob(x))\u22121 \u2265 1\u2212 \u03c3o } .\n(10)\n3) Obstacle Prediction: The opponent prediction uses a simplified model with states xob = [\u03b6ob, nob, vob]> and assumes curvilinear motion depending on the initial estimated state x\u0302ob. With the constant acceleration force F obd , the ODE of the opponent estimator can be written as\n\u03b6\u0307ob = vob(t) cos(\u03b1\u0302ob)\n1\u2212 nob\u03ba(\u03b6ob) (11a)\nn\u0307ob = vob(t) sin(\u03b1\u0302ob) (11b)\nv\u0307ob = 1\nmob F obd . (11c)\nSince the FV is responsible for a crash, it generously predicts the LV by assuming constant velocity motion, where F obd is set to 0. The LV predicts the FV most insignificantly, which we realize by assuming a FV full stop with its maximum braking force F obd = F ob d . In any situation, this allows the FV to plan for at least one safe trajectory (i.e. a full stop), thus the LV does not \u201dprovoke\u201d a crash, as required in racing competition rules [21], [22]. Besides this minimum safety restrictions, interaction should be learned by the high-level RL policy. We simulate the system forward with a function \u03a6(), using steps of the RK4 integration function to obtain the predicted states [xob0 , . . . , x ob N ] = \u03a6(x\u0302\nob, \u03b1\u0302ob, F obd ). 4) Recursive feasibility: In order to guarantee safety for a finite horizon and constraints (6), (7) and (8), we refer to the concept of recursive feasibility and control invariant sets (CIS) [3]. A straight-forward CIS is the trivial set of zero velocity {x | v = 0}. An approximation to the CIS, which is theoretically not a CIS but which has shown good performance in practice, is the limited-velocity terminal set St := {x | \u03b1 = 0, v \u2264 vmax}. For long horizons, the influence of the terminal set vanishes."
        },
        {
            "heading": "C. Objective",
            "text": "For the parameterized cost function L(X,U, a), we propose a formulation with the following properties: \u2022 Simple structure for reliable and fast NLP iterations \u2022 Expressive behavior of the vehicle related to the strate-\ngic driving application \u2022 Low dimensional action vector a \u2022 Good initial performance with constant actions\nThe first property is achieved by restricting the cost function to a quadratic form. The second property is achieved by formulating the state reference in the Frenet coordinate frame. The final properties of a low dimensional action space and a good initial performance are achieved by interpreting the actions as reference lateral position nref and reference speed vref . By setting the reference speed, also the corresponding longitudinal state \u03b6ref,k of a curvilinear trajectory is defined by \u03b6ref,k = \u03b6\u0302 + k\u2206tvref . The\nreference heading angle miss-match \u03b1ref as well as the steering angle \u03b4ref are set to zero, with fixed weights w\u03b1 and w\u03b4 , since these weights are tuned for a smooth driving behavior. Setting the reference speed vref above maximum speed corresponds to time-optimal driving [27]. We compare the influence using references vref , nref with their associated weights wv, wn (HILEPP-II) in the action space or with fixed weights without using them in the action space (HILEPPI). We use the action-dependent stage cost matrix Qw(a) with Qw : Rna \u2192 Rnx\u00d7nx and a cost independent terminal cost Qt \u2208 Rnx\u00d7nx. We set the values of R, Q0 and Qt to values that correspond to driving smoothly and timeoptimal. With constant action inputs a = a\u0304, this leads to a strong initial performance in the beginning of training the high-level RL policy. With the action-dependent reference values \u03beref,k(a) \u2208 Rnx , we can write the expanding function as\nGP (a) : a\u2192 ( \u03beref,0(a), . . . , \u03beref,N (a), Qw(a) ) (12)\nwhich maps na to n2x(N + 1) + nx(N + 1) dimensions for cost matrices and reference values."
        },
        {
            "heading": "D. NLP formulation",
            "text": "We state the final NLP, using the vehicle model (5), the MPC path constraints for obstacle avoidance (10), vehicle limitations (6), (7) and (8) and the parametric cost functions of (2). The full objective, including slack variables \u039e = [\u03c30, . . . , \u03c3N ] \u2208 R6\u00d7N for each stage, associated L2 weights Q\u03c3,2 = diag(q\u03c3,2) \u2208 R6\u00d76 and L1 weights q\u03c3,1 \u2208 R6, reads as\nL(X,U, a,\u039e) = N\u22121\u2211 k=0 \u2016xk \u2212 \u03beref,k(a)\u20162Qw(a) + \u2016uk\u2016 2 R\n+ \u2016xN \u2212 \u03beref,N (a)\u20162Qt + N\u2211 k=0 \u2016\u03c3k\u20162Q\u03c3,2 + |q > \u03c3,1\u03c3k|. (13)\nTogether with the predictor for time step k of the j-th future opponent vehicle states, represented as bounding ellipses with the parameters pob,ji ,\u03a3 ob,j i , the parametric NLP can be written as\nminimize X,U,\u039e L(X,U, a,\u039e)\nsubject to x0 = x\u0302, \u039e \u2265 0, xN \u2208 St, xi+1 = F (xi, ui), i = 0, . . . , N \u2212 1, xi \u2208 Bx(\u03c3k) \u2229Blat(\u03c3k) i = 0, . . . , N, xi \u2208 Bob(pob,ji ,\u03a3ob,ji , \u03c3k) i = 0, . . . , N,\nj = 0, . . . , Nob,\nui \u2208 Bu, i = 0, . . . , N \u2212 1. (14)\nThe final MPP algorithm is stated in Alg. (1)."
        },
        {
            "heading": "V. Hierarchical Learning-based Predictive Planner",
            "text": "The MPP of Sec. IV plans safe and time-optimal, but not strategically. Therefore, we learn a policy \u03c0\u03b8 with RL that decides at each time step how to parameterize the\nAlgorithm 1: MPP input : action a, ego states x\u0302, Nob obstacle\nstates x\u0302ob\noutput: planned trajectory Xref 1 for j in range(Nob) do 2 if \u03b6\u0302ob \u2264 \u03b6\u0302 then 3 Consider opp. as FV: F obd \u2190 F obd 4 end 5 else 6 Consider opp. as LV F obd \u2190 0 7 end 8 Predict [xob0 , . . . , x ob N ] = \u03a6(x\u0302\nob, \u03b1\u0302ob, F obd ); 9 Compute constraint ellipses \u03a3ob,jk = \u03a30(\u03d5\nob,j); 10 end 11 Compute weights ( \u03b6ref,k, Qw ) \u2190 GP (a);\n12 Xref \u2190Solve NLP (2) with ( \u03b6ref,k, Qw ) ;\nMPP to achieve a strategic goal. Since we assume stationary opponent policies, we can apply standard, i.e. single-agent RL algorithms [20] and solve for a best response. In the following, we give a short theoretical background to policy gradient methods and then describe the training procedure in detail."
        },
        {
            "heading": "A. Policy gradient",
            "text": "RL requires a Markov Decision Process (MDP) framework. A MDP consists of a state space S, an action space A, a transition kernel P (s\u2032 | s, a), a reward function R : S \u00d7 A 7\u2192 R that describes how desirable a state is (equal to the negative cost) and a discount factor \u03b3 \u2208 [0, 1). The goal for a given MDP is finding a policy \u03c0\u03b8 : S 7\u2192 A that maximizes the expected discounted return\nJ(\u03c0\u03b8) = E [ \u221e\u2211 t=0 \u03b3tR(st, at) | s0 = s ] ,\nst \u223c P (st+1 | st, at), at \u223c \u03c0\u03b8(st). (15)\nwhere st is the state and at the action taken by the policy \u03c0\u03b8 at time step t. An important additional concept is the stateaction value function\nQ\u03c0 \u03b8 (s, a) = E [ \u221e\u2211 t=0 \u03b3tR(st, at) | s0 = s, a0 = a ] (16)\nthat is the expected value of the policy \u03c0\u03b8 starting in state s and taking an action a. In general, it is not possible to find an optimal policy \u03c0\u03b8 by directly optimizing \u03b8 in (15). The expectation in (15) might be computationally intractable or the exact transition probabilities P may be unknown. Thus, the policy gradient \u2207J(\u03c0\u03b8) is approximated by only using transition samples from the environment. We sample these transitions from a simulator, however, they could also come from real-world experiments. A particularly successful branch of policy gradient methods are actor-critic methods [28], where we train two neural networks, an actor \u03c0\u03b8 and a\ncritic Q\u03c6. The critic estimates the value of a chosen action and is trained by minimizing the temporal difference loss\nJQ(\u03c6) = Es,a,r,s\u2032\u223cD [( rt + \u03b3Q\u03c6\u2032 ( st+1, \u03c0 \u03b8(st+1) ) \u2212Q\u03c6(st) )2] .\n(17)\nThe trained critic is used to train the actor with the objective\nJ\u03c0(\u03b8) = Es,a,r,s\u2032\u223cD [ Q\u03c6(st, \u03c0 \u03b8(st)) ] . (18)\nTo derive the gradient for the policy \u03c0\u03b8 from (18) we can use the chain rule [29]\n\u2207\u03b8J\u03c0(\u03b8) = Es,a,r,s\u2032\u223cD [ \u2207\u03b8\u03c0\u03b8(s)\u2207aQ\u03c6(s, a) ] . (19)\nThe soft-actor critic method, introduced by [30] enhances the actor-critic method by adding an additional entropy term in to (18) and using the reparameterization trick to calculate the gradient. For a complete description we refer to [30]."
        },
        {
            "heading": "B. Training Environment",
            "text": "We reduce the RL state-space, based on domain knowledge which we put into the function gs(zk). The race track layout is approximated by finite curvature evaluations \u03ba(\u03b6 + di) at different longitudinal distances di relative to the ego vehicle position \u03b6, for i = 1, . . . , N\u03ba. For the RL ego state sego(zk) = [n, v, \u03b1]>, we include the lateral position n, the velocity v and the heading angle miss-match \u03b1. For opponent i, we additionally add the opponent longitudinal distance \u03b6ob \u2212 \u03b6 to the ego vehicle to state sobi = [\u03b6obi \u2212 \u03b6, nobi , vobi , \u03b1obi ]>. Combined, we get the following state definition for the RL agent\nsk = gs(zk) =\n[\u03ba(\u03b6 + di), . . . , \u03ba(\u03b6 + dN ), s > ego, s > ob1 , . . . , s > obNob\n]>. (20)\nWe propose a simple reward that encourages time-optimal and strategic driving: For driving time-optimal, we reward the progress on the race track by measuring the velocity of the ego vehicle projected point on the center line s\u0307k. For driving strategically, we reward ego vehicle overall rank. Combined, we get the reward function\nR(s, a) = s\u0307\n200 + Nob\u2211 i=1 1 \u03b6k>\u03b6 obi k . (21)\nAt each time step the high-level RL policy chooses a parameter for the MPP, thus the action space is the parameter space of the MPP. For training the high-level RL policy, an essential part is the simulation function of the environment znext = sim(z, \u03ba(\u00b7)), which we simulate for nepi episodes and a maximum of nscene steps. The road layout defined by \u03ba(\u03b6) is randomized within an interval [90.04, 0.04]m\u22121 before each training episode. The curvature is set together with initial random vehicle states z by a reset function (z, \u03ba(\u03b6)) = Z(). We use Alg. 2 for training and Alg. 3 for the final deployment of HILEPP.\nAlgorithm 2: HILEPP training input : number of episodes nepi, maximum scenario\nsteps nscene, reset function (z, \u03ba(\u03b6)) = Z(), reward function r(z)\noutput: learned policy \u03c0\u03b8(\u03b6) 1 for j in range(nepi) do 2 reset+randomize environment (z, \u03ba(\u03b6))\u2190 Z(); 3 for i in range(nscene) do 4 get NN input state s\u2190 gs(z); 5 get high-level action a\u2190 \u03c0\u03b8(s); 6 evaluate planner Xref \u2190MPP(a, z); 7 simulate environment znext = sim(Xref); 8 get reward (r, done)\u2190 R(znext, a); 9 RL update \u03b8 \u2190train(z, znext, r, a);\n10 if done then 11 exit loop 12 end 13 z\u2190 znext 14 end 15 end 16 return \u03c0\u03b8(s);\nAlgorithm 3: HILEPP deployment input : environment state z, trained policy \u03c0\u03b8(s) output: reference trajectory Xref\n1 compute NN input state s\u2190 gs(z); 2 compute high-level RL policy output a\u2190 \u03c0\u03b8(s); 3 return MPP output Xref \u2190MPP(z, a);"
        },
        {
            "heading": "VI. SIMULATED EXPERIMENTS",
            "text": "We evaluate (Alg. 3) and train (Alg. 2) HILEPP on three different scenarios that resemble racing situations (cf. Fig. 2). The first scenario overtaking constitutes three \u201dweaker\u201d, initially leading opponent agents, where \u201dweaker\u201d relates to the parameters of maximum accelerations, maximum torques and vehicle mass (cf. Tab. I). The second scenario blocking constitutes three \u201dstronger\u201d, initially subsequent opponents. The ego agent starts in between a stronger and a weaker opponent in a third mixed scenario. Each scenario is simulated for one minute, where the ego agent has to perform best related to the reward (21). We train the HILEPP agent with the different proposed action interfaces (I: A = {nref , vref}, II: A = {nref , vref , wn, wv}). Opponent agents, as well as\nthe ego agent base-line, are simulated with the state-of-theart MPP (Alg. 1) with a fixed action a that accounts for nonstrategic time-optimal driving with obstacle avoidance. We perform hyper-parameter (HP) search for the RL parameters with Hydra [31] and its Optuna HP optimizer extension by [32]. The search space was defined by [10\u22125, 10\u22123] for the learning rate, \u03c4 \u2208 [10\u22125, 10\u22122] for the polyak averaging of the target networks, {64, 128, 256} for the width of the hidden layers, {1, 2, 3} for the number of hidden layers and {128, 256} for the batch size. We used the average return of 30 evaluation episodes after training for 105 steps as the search metric. We trained on randomized scenarios for 5\u00b7105 steps with 10 different seeds on each scenario. For estimating the performance of the final policy, we evaluated the episode return (sum of rewards) on 100 episodes. We further compare our trained HILEPP against a pure RL policy that directly outputs the controls. The final experiments where run on a computing cluster where all 30 runs for one method where run on 8 GeForce RTX 2080 Ti with a AMD EPYC 7502 32-Core Processor with a training time of around 6 hours. We use the NLP solver acados [33] with HPIPM [34], RTI iterations and a partial condensing horizon of N2 ."
        },
        {
            "heading": "A. Results",
            "text": "In Fig. 3, we compare the training performance related to the reward (21) and in Fig. 4, we show the final performance of the two HILEPP formulations. HILEPP quickly outperforms the base-line MPP as well as the pure RL\nformulation. With a smaller action space, HILEPP-I learns faster, however, with more samples, HILEPP-II outperforms the smaller action space in two out of three scenarios (mixed and overtaking). Despite using state-of-the-art RL learning algorithms and an extensive HP search on GPU clusters, the pure RL agent can not even outperform the MPP baseline. Furthermore, the pure RL policy could not prevent crashes, whereas MPP successfully filters the actions within HILEPP to safe actions that do not cause safety violations. Notably, due to the struggle of the pure RL agent with lateral acceleration constraints, it has learned a less efficient strategy to drive slowly and just focus on blocking subsequent opponents in scenarios blocking and mixed. Therefore, pure RL could not perform efficient overtaking maneuvers in the overtaking scenario and yields insignificant returns (consequently excluded in Fig. 4). In Tab. III we show, that HILEPP is capable of planning trajectories with approximately 100Hz, which is sufficient and competitive for automotive motion planning [1]. A rendered plot of learned blocking is shown in Fig. 5, where also the time signals are shown of how the high-level RL policy sets the references of HILEPP-I. A rendered simulation for all three scenarios can be found on the website https://rudolfreiter. github.io/hilepp_vis/."
        },
        {
            "heading": "VII. CONCLUSIONS",
            "text": "We have shown a hierarchical planning algorithm for strategic racing. We use RL to train for strategies in simulated environments and have shown to outperform a basic timeoptimal and obstacle avoiding approach, as well as pure deep-learning based RL in several scenarios. The major drawback of our approach is the restrictive prediction and the\nstationary policy of opponents. More involved considerations would need to use multi-agent RL (MARL) based on Markov games, which is a challenging research area with still many open problems [20]. Both, an interactive prediction and MARL will be considered in future work."
        },
        {
            "heading": "ACKNOWLEDGMENT",
            "text": "This research was supported by DFG via Research Unit FOR 2401 and project 424107692 and by the EU via ELO-X 953348. The authors thank Christian Leininger, the SYSCOP and the Autonomous Racing Graz team for inspiring discussions."
        }
    ],
    "title": "A Hierarchical Approach for Strategic Motion Planning in Autonomous Racing",
    "year": 2022
}