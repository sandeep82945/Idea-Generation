A flaky test is a test case whose outcome changes without modification to the code of the test case or the program under test. These tests disrupt continuous integration, cause a loss of developer productivity, and limit the efficiency of testing. Many flaky test detection techniques are rerunning-based, meaning they require repeated test case executions at a considerable time cost, or are machine learning-based, and thus they are fast but offer only an approximate solution with variable detection performance. These two extremes leave developers with a stark choice. This paper introduces CANNIER, an approach for reducing the time cost of rerunning-based detection techniques by combining them with machine learning models. The empirical evaluation involving 89,668 test cases from 30 Python projects demonstrates that CANNIER can reduce the time cost of existing rerunningbased techniques by an order of magnitude while maintaining a detection performance that is significantly better than machine learning models alone. Furthermore, the comprehensive study extends existing work on machine learning-based detection and reveals a number of additional findings, including (1) the performance of machine learning models for detecting polluter test cases; (2) using the mean values of dynamic test case features from repeated measurements can slightly improve the detection performance of machine learning models; and (3) correlations between various test case features and the probability of the test case being flaky.Unlike Luo et al, they found causes related to networking and randomness to be the most prevalent. Bell et al. (2018) presented an automated technique, called DEFLAKER, for detecting NOD flaky tests. The key advantage of DEFLAKER over RERUN is that it does not require repeated test case executions. Instead, the technique takes advantage of a project’s history in a version control system. When a test case that passed on a previous version of the software now fails, and does not cover modified code, DEFLAKER labels it as flaky. Naturally, DEFLAKER requires a test suite run with code instrumentation to measure coverage. Detecting flaky tests using extra trees models with CANNIER-FRAMEWORK also requires an instrumented run to measure coverage and the other metrics in Table 1. In both cases, this test suite run introduces time overhead. However, DEFLAKER requires a run every time a change is made, whereas CANNIER-FRAMEWORK requires at least one to produce encodings for each test case that would likely remain relevant over a series of changes. Furthermore, DEFLAKER can only detect flaky tests after they fail. In contrast, the models trained by the CANNIER-FRAMEWORK can detect flaky tests preemptively. Pinto et al. (2020) and Bertolino et al. (2021) both presented machine learning-based flaky test detection techniques based purely on static features of the test case code. Both techniques encoded test cases using a bag-of-words approach. This represents test cases as sparse vectors where each element corresponds to the frequency of a particular identifier or keyword in its source code. Pinto et al used additional static features such as the number of lines of code. Bertolino et al used a k-nearest neighbor classifier (Keller et al. 1985) for the machine learning model and Pinto et al evaluated a range of different models, including random forest. They found random forest to yield the best detection performance, of which we use the extra trees variant in this paper’s study, having found it to be the most effective in our prior work (Parry et al. 2022a). Alshammari et al. (2021) presented FLAKEFLAGGER, a detection technique using a random forest model and encoding test cases with a feature set containing a mixture of static and dynamic test case metrics. Their evaluation showed that their feature set offered a 347% improvement in overall F1 score compared to Pinto et al’s purely static feature set at the cost of a single instrumented test suite run to measure the dynamic features. For this reason, we included both static and dynamic test metrics in our feature set instead of relying on purely static features. Shi et al. (2019) presented IFIXFLAKIES, a technique for automatically generating patches for victim flaky tests. Their approach uses delta-debugging (Zeller and Hildebrandt 2002) to identify a victim’s polluters and other test cases that may contain the statements needed to repair the victim, known as cleaners. CANNIER+PAIRWISE could provide a dropin replacement for this aspect of IFIXFLAKIES. However, we cannot say for certain if our approach would be faster than using delta-debugging because we have not yet evaluated it in this context. Lam et al. (2019) presented IDFLAKIES, a technique for detecting flaky tests and classifying them as either NOD or Victim. The overall process involves repeatedly executing a test suite in a modified order (e.g., shuffled) to identify flaky test cases. Following this, the tool enters a Classification stage where it attempts to determine the category of each flaky test. In this paper’s study, we evaluated the application of CANNIER to the Classification stage of this tool (CANNIER+IDFCLASS). Our empirical results demonstrated that CANNIER was able to significantly reduce the execution time overhead of the Classification stage at minimal detriment to its detection performance. 72 Page 46 of 52
Empir Software Eng (2023) 28:72 9 conclusion and futurework This paper expanded the existing work on machine learning-based flaky test detection and introduced CANNIER, an approach for significantly reducing the time cost of rerunningbased detection techniques by combining them with machine learning models. Initially, using a variety of machine learning pipelines and a feature set of 18 static and dynamic test case metrics, we performed a baseline evaluation of machine learning-based detection on our dataset of 89,668 test cases from 30 Python projects. We evaluated their performance with respect to detecting NOD flaky tests, victim flaky tests, and polluter test cases. Our results suggested that the performance of the machine learning models was lackluster and variable between projects. We then went on to investigate the impact of mean feature vectors on machine learning-based flaky test detection. We identified a positive relationship between the sample size to produce the mean feature vectors and the detection performance of the machine learning model. In the interest of model explainability, we applied the SHAP technique (Lundberg et al. 2020) to quantify the contribution of each individual feature to the output value of the model. While this technique can only reveal correlations and is not appropriate for inferring causality, we made several findings that support both the general intuition of developers and results from the flaky test literature. Finally, we evaluated CANNIER’s impact on three rerunning-based methods for flaky test detection RERUN, the Classification stage of IDFLAKIES, and PAIRWISE. We found that CANNIER was able to significantly reduce time cost at the expense of only a minor decrease in detection performance.