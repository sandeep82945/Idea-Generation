{
    "abstractText": "Developing Automatic Speech Recognition (ASR) for lowresource languages is a challenge due to the small amount of transcribed audio data. For many such languages, audio and text are available separately, but not audio with transcriptions. Using text, speech can be synthetically produced via text-tospeech (TTS) systems. However, many low-resource languages do not have quality TTS systems either. We propose an alternative: produce synthetic audio by running text from the target language through a trained TTS system for a higher-resource pivot language. We investigate when and how this technique is most effective in low-resource settings. In our experiments, using several thousand synthetic TTS text-speech pairs and duplicating authentic data to balance yields optimal results. Our findings suggest that searching over a set of candidate pivot languages can lead to marginal improvements and that, surprisingly, ASR performance can by harmed by increases in measured TTS quality. Application of these findings improves ASR by 64.5% and 45.0% character error reduction rate (CERR) respectively for two low-resource languages: Guaran\u0131\u0301 and Suba.",
    "authors": [
        {
            "affiliations": [],
            "name": "Nathaniel Robinson"
        },
        {
            "affiliations": [],
            "name": "Swetha Gangu"
        },
        {
            "affiliations": [],
            "name": "David R. Mortensen"
        },
        {
            "affiliations": [],
            "name": "Shinji Watanabe"
        }
    ],
    "id": "SP:01a5ad69cb8c111997e0d8804907fc11c478d846",
    "references": [
        {
            "authors": [
                "A. Michaud",
                "E. Castelli"
            ],
            "title": "Towards the automatic processing of yongning na (sino-tibetan): developing a\u2019light\u2019acoustic model of the target language and testing\u2019heavyweight\u2019models from five national languages",
            "venue": "4th International Workshop on Spoken Language Technologies for Under-resourced Languages (SLTU 2014), 2014, pp. 153\u2013160.",
            "year": 2014
        },
        {
            "authors": [
                "J. Shi",
                "J.D. Amith",
                "R. Castillo Garc\u0131\u0301a",
                "E. Guadalupe Sierra",
                "K. Duh",
                "S. Watanabe"
            ],
            "title": "Leveraging end-to-end ASR for endangered language documentation: An empirical study on yol\u00f3xochitl Mixtec",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. Online: Association for Computational Linguistics, Apr. 2021, pp. 1134\u20131145. [Online]. Available: https://aclanthology.org/2021.eacl-main.96",
            "year": 2021
        },
        {
            "authors": [
                "E. Morris"
            ],
            "title": "Automatic speech recognition for low-resource and morphologically complex languages",
            "venue": "Ph.D. dissertation, Rochester Institute of Technology, 2021, copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works; Last updated - 2021-06-16. [Online]. Available: https://www.proquest.com/ dissertations-theses/automatic-speech-recognition-low-resource/ docview/2538388272/se-2?accountid=9902",
            "year": 2021
        },
        {
            "authors": [
                "The Conversation"
            ],
            "title": "The story of how Swahili became Africa\u2019s most spoken language",
            "venue": "Feb 2022. [Online]. Available: https://nation.africa/kenya/news/ the-story-of-how-swahili-became-africa-s-most-spoken-language-3725834",
            "year": 2022
        },
        {
            "authors": [
                "K. Getao",
                "E. Miriti"
            ],
            "title": "Creation of a speech to text system for kiswahili",
            "venue": "5th World Congress of African Linguistics, 2006.",
            "year": 2006
        },
        {
            "authors": [
                "F. Jenner"
            ],
            "title": "Paraguay\u2019s Guaran\u0131\u0301 language is flourishing but its indigenous people are not",
            "venue": "May 2019. [Online]. Available: https://latinamericareports.com/ guarani-language-flourishing-not-indigenous-people/1941/",
            "year": 2019
        },
        {
            "authors": [
                "B. Racoma"
            ],
            "title": "Olusuba Language of Africa on the Verge of Extinction",
            "venue": "Mar. 2014. [Online]. Available: https://www. daytranslations.com/blog/olusuba-language-near-extinction/",
            "year": 2014
        },
        {
            "authors": [
                "N. Rossenbach",
                "A. Zeyer",
                "R. Schl\u00fcter",
                "H. Ney"
            ],
            "title": "Generating synthetic audio data for attention-based speech recognition systems",
            "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 7069\u20137073.",
            "year": 2020
        },
        {
            "authors": [
                "C. Du",
                "K. Yu"
            ],
            "title": "Speaker augmentation for low resource speech recognition",
            "venue": "ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020, pp. 7719\u20137723.",
            "year": 2020
        },
        {
            "authors": [
                "X. Zheng",
                "Y. Liu",
                "D. Gunceler",
                "D. Willett"
            ],
            "title": "Using synthetic audio to improve the recognition of out-of-vocabulary words in end-to-end asr systems",
            "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 5674\u20135678.",
            "year": 2021
        },
        {
            "authors": [
                "D.S. Park",
                "W. Chan",
                "Y. Zhang",
                "C.-C. Chiu",
                "B. Zoph",
                "E.D. Cubuk",
                "Q.V. Le"
            ],
            "title": "Specaugment: A simple data augmentation method for automatic speech recognition",
            "venue": "Interspeech 2019, Sep 2019. [Online]. Available: http://dx.doi. org/10.21437/Interspeech.2019-2680",
            "year": 2019
        },
        {
            "authors": [
                "C. Wang",
                "J.M. Pino",
                "J. Gu"
            ],
            "title": "Improving cross-lingual transfer learning for end-to-end speech recognition with speech translation",
            "venue": "INTERSPEECH, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "M. Stoian",
                "S. Bansal",
                "S. Goldwater"
            ],
            "title": "Analyzing asr pretraining for low-resource speech-to-text translation",
            "venue": "ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 7909\u20137913, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "S. Bansal",
                "H. Kamper",
                "K. Livescu",
                "A. Lopez",
                "S. Goldwater"
            ],
            "title": "Pre-training on high-resource speech recognition improves lowresource speech-to-text translation",
            "venue": "NAACL, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "J. Li",
                "R. Gadde",
                "B. Ginsburg",
                "V. Lavrukhin"
            ],
            "title": "Training neural speech recognition systems with synthetic speech augmentation",
            "venue": "2018. [Online]. Available: https://arxiv.org/abs/1811.00707",
            "year": 2018
        },
        {
            "authors": [
                "M. Mimura",
                "S. Ueno",
                "H. Inaguma",
                "S. Sakai",
                "T. Kawahara"
            ],
            "title": "Leveraging sequence-to-sequence speech synthesis for enhancing acoustic-to-word speech recognition",
            "venue": "2018 IEEE Spoken Language Technology Workshop (SLT), 2018, pp. 477\u2013484.",
            "year": 2018
        },
        {
            "authors": [
                "Y.-A. Chung",
                "Y. Wang",
                "W.-N. Hsu",
                "Y. Zhang",
                "R. Skerry- Ryan"
            ],
            "title": "Semi-supervised training for improving data efficiency in end-to-end speech synthesis",
            "venue": "ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2019, pp. 6940\u20136944.",
            "year": 2019
        },
        {
            "authors": [
                "D. Lin",
                "Y. Murakami",
                "T. Ishida"
            ],
            "title": "Towards language service creation and customization for low-resource languages",
            "venue": "Information, vol. 11, no. 2, 2020. [Online]. Available: https: //www.mdpi.com/2078-2489/11/2/67",
            "year": 2020
        },
        {
            "authors": [
                "A. Fazel",
                "W. Yang",
                "Y. Liu",
                "R. Barra-Chicote",
                "Y. Meng",
                "R. Maas",
                "J. Droppo"
            ],
            "title": "Synthasr: Unlocking synthetic data for speech recognition",
            "venue": "arXiv preprint arXiv:2106.07803, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A. \u00d6ktem",
                "M.A. Jaam",
                "E. DeLuca",
                "G. Tang"
            ],
            "title": "Gamayun language technology for humanitarian response",
            "venue": "2020 IEEE Global Humanitarian Technology Conference (GHTC), 2020, pp. 1\u20134.",
            "year": 2020
        },
        {
            "authors": [
                "P. Ogayo",
                "G. Neubig",
                "A.W. Black"
            ],
            "title": "Building African Voices",
            "venue": "2022. [Online]. Available: https://www.africanvoices.tech/",
            "year": 2022
        },
        {
            "authors": [
                "G.K. Anumanchipalli",
                "K. Prahallad",
                "A.W. Black"
            ],
            "title": "Festvox: Tools for creation and analyses of large speech corpora",
            "venue": "Workshop on Very Large Scale Phonetics Research, UPenn, Philadelphia, 2011, p. 70.",
            "year": 2011
        },
        {
            "authors": [
                "L. Chiruzzo",
                "P. Amarilla",
                "A. R\u0131\u0301os",
                "G. Gim\u00e9nez Lugo"
            ],
            "title": "Development of a Guarani - Spanish parallel corpus",
            "venue": "Proceedings of the 12th Language Resources and Evaluation Conference. Marseille, France: European Language Resources Association, May 2020, pp. 2629\u20132633. [Online]. Available: https://aclanthology.org/2020.lrec-1.320",
            "year": 2020
        },
        {
            "authors": [
                "S. Watanabe",
                "T. Hori",
                "S. Karita",
                "T. Hayashi",
                "J. Nishitoba",
                "Y. Unno",
                "N. Enrique Yalta Soplin",
                "J. Heymann",
                "M. Wiesner",
                "N. Chen",
                "A. Renduchintala",
                "T. Ochiai"
            ],
            "title": "Espnet: End-to-end speech processing toolkit",
            "venue": "Proc. Interspeech 2018, 2018, pp. 2207\u20132211. [Online]. Available: http://dx.doi.org/10.21437/ Interspeech.2018-1456",
            "year": 2018
        },
        {
            "authors": [
                "F. Boyer",
                "Y. Shinohara",
                "T. Ishii",
                "H. Inaguma",
                "S. Watanabe"
            ],
            "title": "A study of transducer based end-to-end asr with espnet: Architecture, auxiliary loss and decoding strategies",
            "venue": "2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2021, pp. 16\u201323.",
            "year": 2021
        },
        {
            "authors": [
                "P. Littell",
                "D.R. Mortensen",
                "K. Lin",
                "K. Kairis",
                "C. Turner",
                "L. Levin"
            ],
            "title": "Uriel and lang2vec: Representing languages as typological, geographical, and phylogenetic vectors",
            "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, 2017, pp. 8\u201314.",
            "year": 2017
        },
        {
            "authors": [
                "P. Wu",
                "J. Shi",
                "Y. Zhong",
                "S. Watanabe",
                "A.W. Black"
            ],
            "title": "Crosslingual transfer for speech processing using acoustic language similarity",
            "venue": "2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2021, pp. 1050\u20131057.",
            "year": 2021
        },
        {
            "authors": [
                "A. Conneau",
                "A. Baevski",
                "R. Collobert",
                "A. Mohamed",
                "M. Auli"
            ],
            "title": "Unsupervised Cross-Lingual Representation Learning for Speech Recognition",
            "venue": "Proc. Interspeech 2021, 2021, pp. 2426\u20132430.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "resource languages is a challenge due to the small amount of transcribed audio data. For many such languages, audio and text are available separately, but not audio with transcriptions. Using text, speech can be synthetically produced via text-tospeech (TTS) systems. However, many low-resource languages do not have quality TTS systems either. We propose an alternative: produce synthetic audio by running text from the target language through a trained TTS system for a higher-resource pivot language. We investigate when and how this technique is most effective in low-resource settings. In our experiments, using several thousand synthetic TTS text-speech pairs and duplicating authentic data to balance yields optimal results. Our findings suggest that searching over a set of candidate pivot languages can lead to marginal improvements and that, surprisingly, ASR performance can by harmed by increases in measured TTS quality. Application of these findings improves ASR by 64.5% and 45.0% character error reduction rate (CERR) respectively for two low-resource languages: Guaran\u0131\u0301 and Suba. Index Terms: speech recognition, text-to-speech, low-resource learning, multilingual NLP, language revitalization tools"
        },
        {
            "heading": "1. Problem statement and motivation",
            "text": "Applications of ASR systems such as digital assistants are becoming increasingly ubiquitous. Despite ASR being a crucial task for low-resource and endangered languages, most existing ASR projects cover high-resource languages and dialects from industrialized nations; most low-resource languages are left behind. This is troubling because speakers of low-resource languages can benefit significantly from ASR. ASR technologies could enable them to access digital information relating to education, politics, health conditions, natural disasters, etc. Endangered languages particularly need ASR for documentation, revitalization, and learning resources, since human audio transcription is prohibitively slow [1, 2].\nMost state-of-the-art ASR architectures for high-resource languages require large data sets, which take extensive time and resources to collect [3]. This work explores data augmentation for ASR via TTS for low resource languages, with Coastal Kiswahili (SWH), Guaran\u0131\u0301 (GRN), and Suba (SXB) as case studies, and Italian (ITA) as an additional example to demonstrate training trends. Kiswahili is a Bantu language and lingua franca in East and Central Africa. It is spoken by \u223c200 million people in Africa [4]. Though it is the most spoken African language, few Kiswahili speech technology developments have been made [3, 5]. Guaran\u0131\u0301 is South American language of the Tupi-Guaran\u0131\u0301 family with \u223c5.85 million speakers in Paraguay and for which digital resources are becoming increasingly necessary, though they remain limited [6]. Suba is an endangered\nBantu language with less than 10000 speakers in Kenya [7], for which speech technologies can help revitalization efforts.\nPrevious research [8, 9, 10] shows that TTS data can be used to augment ASR training. However, many low-resource languages that can benefit from ASR data augmentation do not have TTS systems. To this end, we ask: Can we use synthesized speech from a high-resource language\u2019s TTS system to improve ASR in a low-resource language?\nThis question prompts a few others. When augmenting, how much synthesized audio should be used? Which high-resource language should be used for TTS? In initial experiments to address these questions, we noticed that pivotlanguage TTS audio is noisy to human perception. This led us to experiment with TTS quality improvement. We contribute:\n\u2022 A novel study on the effects of TTS augmentation through a pivot language for low-resource ASR, with accompanying software and datasets\n\u2022 Experimentally backed recommendations for augmentation parameters: data amount and duplication, choice of pivot language, and TTS quality, with surprising findings suggesting language relatedness and impressionistic TTS quality may not improve performance\n\u2022 ASR improvement across languages, including 64.5% and 45.0% CERR1 for low-resource Guaran\u0131\u0301 and Suba"
        },
        {
            "heading": "2. Related work",
            "text": "Multiple data augmentation techniques exist for low-resource ASR. Popular approaches include SpecAugment [11], language models (LM), and incorporating text and untranscribed speech in addition to traditional waveform variation methods such as PSOLA, time-stretching and noise addition [3].\nWe are not the first to explore cross-lingual transfer for lowresource ASR. Pre-training weights on speech-to-text translation from a high-resource language can reduce ASR error rates [12], and high-resource ASR pre-training can conversely improve low-resource translation [13, 14]. Our work likewise seeks to improve ASR for languages where text data is more available than transcribed speech, but we do so via TTS, without pre-training or requiring translations or translation systems.\nResearchers have also explored using TTS to augment ASR training. [8, 9, 15] explored and implemented speaker augmentation to increase variability for synthesized speech, but [16] showed significant improvements in word error rate (WER) from a single speaker. [8] explored how LM, SpecAugment, and TTS augmentation affect WER. They found that these methods are independent of each other and that using TTS data yielded more improvements than LM and SpecAugment, with\n1We use reduction rates calculated as rateold\u2212ratenew rateold\nar X\niv :2\n20 7.\n09 88\n9v 1\n[ cs\n.C L\n] 2\n0 Ju\nl 2 02\n2\nthe best configuration combining them all. In all these works, researchers trained neural TTS models in the target language, requiring more than ten hours of high-quality data [17]. We build upon and differ from these works: because we use TTS systems trained on high-resource languages, our approach may apply to the thousands of low-resource languages for which tenhour audio data sets are not available [18].\nOther researchers have asked how much synthetic TTS data is appropriate, since acoustic differences between TTS data and authentic data can make it less effective from the same text [15, 19]. [15] found the best synthetic/authentic data balance on a LibriSpeech task was 50/50. \u00a74.1 shows similar findings in our novel setting of TTS from a high-resource pivot language."
        },
        {
            "heading": "3. Approach",
            "text": ""
        },
        {
            "heading": "3.1. Data setup",
            "text": "Authentic data We used Kiswahili audio with transcriptions from the Gamayun Swahili Minikit2 [20], which contains 4700 transcribed recordings, for training, validation, and testing. We obtain Italian and Guaran\u0131\u0301 data from Mozilla Common Voice3 Corpus 7.0. Our Suba corpus presents an extremely low-resource setting: 1178 sentences (1.7hrs) obtained from AfricanVoices [21].\nTTS Augmentation We feed authentic text to TTS models to generate synthetic audio which results into a synthetic textspeech pair. We used Microsoft TTS4 and Google Cloud5 neural models to obtain synthetic data. We employed this augmentation for ASR in four languages: Kiswahili, Guaran\u0131\u0301, Italian, and Suba. We outline the text corpora we used as TTS prompts for each language. Kiswahili: 14737 Kiswahili sentences from the Helsinki Swahili corpus [22], which contains news and political text. We selected these sentences to be phonetically diverse using Festvox tools [23]. Guaran\u0131\u0301: a uniformly random mixture from Guaran\u0131\u0301 Wikipedia6 and another Guaran\u0131\u0301 corpus [24]. Italian: text from unused recordings in the Mozilla data we downloaded for training. Suba: text from the Suba New Testament [25]. Further data details are in Table 1.\n2https://gamayun.translatorswb.org/data 3https://commonvoice.mozilla.org/en/datasets 4https://azure.microsoft.com/en-us/services/cognitive-services/\ntext-to-speech 5https://cloud.google.com/text-to-speech 6https://wortschatz.uni-leipzig.de/en/download/Guarani\nTransliteration We found that pivot language TTS systems make many pronunciation errors and hallucinate phones when Kiswahili text is input. This is not surprising because of orthographic diversity across languages (especially in the case of languages that use different alphabets, such as Kiswahili and Arabic). To remedy this in some experiments, we transliterated text into pivot language orthography using hand-crafted phone maps7. This generated empirically higher quality audio with TTS systems for those languages. (See \u00a74.3.) This is represented as the optional step \u201dTransliteration\u201d in Figure 1."
        },
        {
            "heading": "3.2. Experimental Setup",
            "text": "We combine different amounts of TTS data and authentic data to train an ASR system. We evaluate and test on authentic data only. We used ESPNet2 [26] with the default RNNTransducer [27] and RNNLM to train our End-to-End ASR systems. Figure 1 is a visual depiction of our approach."
        },
        {
            "heading": "4. Results",
            "text": "We explored effectiveness of TTS augmentation through a pivot language on three axes: synthetic data amount given authentic data, choice of pivot language, and TTS audio quality."
        },
        {
            "heading": "4.1. Synthetic data amount",
            "text": "We began our experiments by probing for the optimal amount of TTS augmented data. For these experiments we augmented Kiswahili speech data with Arabic, Italian, and Kiswahili TTS. We chose Arabic because of its historical influence on Kiswahili and Italian because its output for Kiswahili text sounded reasonably accurate to a proficient Kiswahili speaker. Results are\n7We release the exact mappings at https://github.com/n8rob/ Multilingual TTS Augmentation. Target language phones not present in the pivot language are approximated with close equivalents (e.g. representing Kiswahili\u2019s voiced post-alveolar affricate with Spanish\u2019s voiceless post-alveolar affricate).\nin Figures 2 and 3. Because low-resource languages can have varying amounts of transcribed audio data, we ran experiments using three authentic training sets of sizes 300, 1000, and 3900. Our test set of 400 utterances was kept constant throughout the experiments. In each of these settings, increasing the amount of TTS synthetic data improves error rates until it reaches a point where further increase degrades performance. This degradation occurs when the model overfits on the synthetic data and thus performs poorly when tested on authentic data. Figure 2 illustrates that in the case of each authentic data amount, using 4000 augmented pairs improves performance (or stagnates in the highest-resource case), and continuing to add synthetic data beyond that degrades performance.\nTo avoid this data imbalance and over-fitting, we duplicated authentic data in the setting with 1000 authentic pairs. Results\nare in Figure 3. We found that when beginning with 1000 transcribed recordings, duplicating authentic data eight times and augmenting with 8000 TTS synthetic pairs works best. This is a relevant example since many low-resource languages have roughly 1000 transcribed recordings, or 1-2 hours of speech, available. (E.g. our Guaran\u0131\u0301 and Suba data, see Table 1.) As shown in Figure 3, this policy reduces CER from 78.2% (no duplication or augmentation) to 20.0% for 74.4% CERR for Kiswahili with an Arabic TTS system. It works well for other language combinations as well: 31.4% CERR for Italian (with Finnish TTS), 64.5% CERR for Guaran\u0131\u0301 (with French TTS), and 45.0% CERR for Suba (with Spanish TTS)."
        },
        {
            "heading": "4.2. Choice of pivot language",
            "text": "Before conducting explorations related to pivot language choice, we analyzed the textual outputs from the lowest- and highest-resource experiments featured in Figure 2 beyond error rate. We tested whether there is a relationship between the pivot language or amount and the characters that the system learns to recognize. We did not find any noticeable pattern for phones that was tied to TTS augmentation. The only system that recognized any characters at a rate of \u2265 25% outside the average was the Kiswahili system with 3900 authentic pairs and 4000 Arabic TTS pairs, on 5 of the 26 Kiswahili characters.8\nWe experimented to test whether choosing a pivot language with high relatedness to the target language can improve results. We tested on three ASR target languages: Italian, Kiswahili, and Guaran\u0131\u0301. Results are in Table 2. We used pre-computed lang2vec distances from URIEL [28] to determine language relatedness. Following best practice recommendations [29], we relied primarily on geographical and genetic distance. We restricted our similarity search to the languages supported by Google Cloud TTS, and we added some language pairs to test other language characteristics.9\nIn our experiments featured in Table 2, language similarity did not appear to determine TTS augmentation suitability. The best-performing TTS system for Italian, for example, was Finnish, and for Guaran\u0131\u0301, French. Arabic performed best for Kiswahili, but this is likely because the data amount configuration was tuned for this pair. (Italian TTS also fares well.) It is possible that having a diversity of language characteristics provided by the TTS system is actually an advantage. We conclude that for any ASR target language, multiple TTS pivot languages should be tried to determine one that works well."
        },
        {
            "heading": "4.3. TTS quality",
            "text": "We sought to improve TTS quality in order to improve augmentation and ASR performance via (1) using Microsoft TTS rather than Google Cloud (higher quality but more time consuming), and (2) transliterating TTS text into pivot language orthogra-\n8The average described here is the average proportion of substitutions and deletions that occurred for each target language character. The five characters at least 25% outside the norm for one of the data configurations were s, u, i, l, j.\n9Google Cloud TTS does not support any languages geographically or genetically close to Guaran\u0131\u0301 or Kiswahili other than Spanish and Arabic, respectively. We selected Italian, Spanish, and Turkish (TUR) for Kiswahili to explore if their straightforward orthography systems would yield an advantage. For Italian, we chose Romanian as they are close geographically and genetically per URIEL and Spanish because of their linguistic proximity. We chose Afrikaans (AFR) for Guaran\u0131\u0301 since URIEL finds them phonologically close. We also tested languages that are closest to the average geographical/genetic distance from the target language: Finnish (FIN) for Italian and French (FRA) for Guaran\u0131\u0301.\nphy, as discussed in \u00a73.1. This technique seems in fact to increase TTS quality: in an A-B test on 20 recording comparisons between Kiswahili transliterated and not transliterated for Arabic TTS, transliterated audio was preferred 100% of the time by a proficient Kiswahili speaker. Surprisingly, higher quality of TTS augmentation data does not always aid ASR performance. See Table 3, where we performed these experiments on Kiswahili ASR with Arabic and Spanish TTS augmentation. Using transliterated Spanish TTS tends to have an advantage over lower-quality Spanish TTS, but higher quality Arabic TTS actually hurts performance consistently across starting amounts.\nA blind TTS quality test by a proficient Kiswahili speaker on 20 utterance comparisons gave the following quality scores: poor ARA:0, trans ARA:4, poor SPA:4, trans SPA:12. This is significant because although the Spanish TTS is clearly higher quality, it performs worse for ASR in the optimal settings in Table 2. Similarly, transliterated Arabic which is of higher quality actually hurts performance as compared to un-transliterated Arabic TTS. Spanish and Arabic scores in Table 3 are not directly comparable because of the difference in training set sizes.\nWe ran an ablation experiment with no authentic Kiswahili data: we trained solely on 4000 Arabic TTS files, both transliterated and not, with the same authentic test and validation sets. The higher-quality transliterated TTS performed slightly worse, with WER=102.5% and CER=72.9% compared to WER=99.6% and CER=72.7% for the lower-quality set.\nThese findings prompted questions as to how much TTS accuracy matters in ASR training. As another ablation, we augmented training data for Kiswahili ASR with 4000 Indonesian (IND) speech files, not resembling Kiswahili at all. Interestingly, in the setting with 3900 authentic pairs, this had a similar effect to using TTS and outscored multiple TTS examples (with CER=7.2% compared to CER=7.8% for ARA-TTS augmentation of the same amount). In the setting of 300 authentic pairs, augmentation quality is more relevant: Indonesian noise does not improve error rates significantly, but TTS audio does."
        },
        {
            "heading": "4.4. Application to low-resource languages",
            "text": "For many low-resource languages, only a small set of transcribed audio is available: on the order of 1000 utterances or 1-2 hours. Two such datasets are the CommonVoice Guaran\u0131\u0301 and African Voices Suba datasets. Employing our findings in practice, we augmented both of these datasets by duplicating the authentic data eight times and adding an equal amount of TTS synthetic data, following the recommended procedure based on Figure 3. See Table 4. This augmentation results in better ASR, with word error reduction rates (WERR) of 27.0% and 19.2% and CERR 64.5% and 45.0%."
        },
        {
            "heading": "5. Conclusion",
            "text": "We show that synthetic audio from a high-resource pivot language TTS system can be used to augment authentic datasets and improve ASR for low-resource languages. Our experiments suggest that performance improves best when several thousand TTS-generated synthetic pairs are used and authentic data is replicated to an equal amount, and when a search over potential pivot languages is conducted. Our experiments suggest, surprisingly, that measured TTS audio quality may not effect suitability for ASR training augmentation. These techniques improve ASR for low-resource languages Kiswahili, Guaran\u0131\u0301, and Suba (74.4%, 64.5%, and 45.0% CERR, respectively). They are also broadly applicable to the thousands of other lowresource languages often overlooked in speech technologies. This has promising implications of increased information access for speakers of these languages and for documentation and revitalization efforts for endangered languages like Suba.\nFuture work may involve methodological advances, such as including authentic speech from the TTS pivot language in training; or pretraining steps, such as training a model on noisy TTS synthetic data and then tuning on authentic data. This process could theoretically be enhanced by using pre-trained representations from multilingual self-supervised models such as XLSR [30]. Further investigation may also involve more rigorous analyses of optimal pivot languages, including comparisons of language recording acoustic features."
        },
        {
            "heading": "6. Acknowledgements",
            "text": "We thank Brian Yan, Alan W Black, Xinjian Li, Graham Neubig, and Rebeca Knapp for their contributions and support."
        },
        {
            "heading": "7. References",
            "text": "[1] A. Michaud, E. Castelli et al., \u201cTowards the automatic processing\nof yongning na (sino-tibetan): developing a\u2019light\u2019acoustic model of the target language and testing\u2019heavyweight\u2019models from five national languages,\u201d in 4th International Workshop on Spoken Language Technologies for Under-resourced Languages (SLTU 2014), 2014, pp. 153\u2013160.\n[2] J. Shi, J. D. Amith, R. Castillo Garc\u0131\u0301a, E. Guadalupe Sierra, K. Duh, and S. Watanabe, \u201cLeveraging end-to-end ASR for endangered language documentation: An empirical study on yolo\u0301xochitl Mixtec,\u201d in Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. Online: Association for Computational Linguistics, Apr. 2021, pp. 1134\u20131145. [Online]. Available: https://aclanthology.org/2021.eacl-main.96\n[3] E. Morris, \u201cAutomatic speech recognition for low-resource and morphologically complex languages,\u201d Ph.D. dissertation, Rochester Institute of Technology, 2021, copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works; Last updated - 2021-06-16. [Online]. Available: https://www.proquest.com/ dissertations-theses/automatic-speech-recognition-low-resource/ docview/2538388272/se-2?accountid=9902\n[4] The Conversation, \u201cThe story of how Swahili became Africa\u2019s most spoken language,\u201d Feb 2022. [Online]. Available: https://nation.africa/kenya/news/ the-story-of-how-swahili-became-africa-s-most-spoken-language-3725834\n[5] K. Getao and E. Miriti, \u201cCreation of a speech to text system for kiswahili,\u201d in 5th World Congress of African Linguistics, 2006.\n[6] F. Jenner, \u201cParaguay\u2019s Guaran\u0131\u0301 language is flourishing but its indigenous people are not,\u201d May 2019. [Online]. Available: https://latinamericareports.com/ guarani-language-flourishing-not-indigenous-people/1941/\n[7] B. Racoma, \u201cOlusuba Language of Africa on the Verge of Extinction,\u201d Mar. 2014. [Online]. Available: https://www. daytranslations.com/blog/olusuba-language-near-extinction/\n[8] N. Rossenbach, A. Zeyer, R. Schlu\u0308ter, and H. Ney, \u201cGenerating synthetic audio data for attention-based speech recognition systems,\u201d in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 7069\u20137073.\n[9] C. Du and K. Yu, \u201cSpeaker augmentation for low resource speech recognition,\u201d in ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020, pp. 7719\u20137723.\n[10] X. Zheng, Y. Liu, D. Gunceler, and D. Willett, \u201cUsing synthetic audio to improve the recognition of out-of-vocabulary words in end-to-end asr systems,\u201d in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 5674\u20135678.\n[11] D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk, and Q. V. Le, \u201cSpecaugment: A simple data augmentation method for automatic speech recognition,\u201d Interspeech 2019, Sep 2019. [Online]. Available: http://dx.doi. org/10.21437/Interspeech.2019-2680\n[12] C. Wang, J. M. Pino, and J. Gu, \u201cImproving cross-lingual transfer learning for end-to-end speech recognition with speech translation,\u201d in INTERSPEECH, 2020.\n[13] M. Stoian, S. Bansal, and S. Goldwater, \u201cAnalyzing asr pretraining for low-resource speech-to-text translation,\u201d ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 7909\u20137913, 2020.\n[14] S. Bansal, H. Kamper, K. Livescu, A. Lopez, and S. Goldwater, \u201cPre-training on high-resource speech recognition improves lowresource speech-to-text translation,\u201d in NAACL, 2019.\n[15] J. Li, R. Gadde, B. Ginsburg, and V. Lavrukhin, \u201cTraining neural speech recognition systems with synthetic speech augmentation,\u201d 2018. [Online]. Available: https://arxiv.org/abs/1811.00707\n[16] M. Mimura, S. Ueno, H. Inaguma, S. Sakai, and T. Kawahara, \u201cLeveraging sequence-to-sequence speech synthesis for enhancing acoustic-to-word speech recognition,\u201d in 2018 IEEE Spoken Language Technology Workshop (SLT), 2018, pp. 477\u2013484.\n[17] Y.-A. Chung, Y. Wang, W.-N. Hsu, Y. Zhang, and R. SkerryRyan, \u201cSemi-supervised training for improving data efficiency in end-to-end speech synthesis,\u201d in ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2019, pp. 6940\u20136944.\n[18] D. Lin, Y. Murakami, and T. Ishida, \u201cTowards language service creation and customization for low-resource languages,\u201d Information, vol. 11, no. 2, 2020. [Online]. Available: https: //www.mdpi.com/2078-2489/11/2/67\n[19] A. Fazel, W. Yang, Y. Liu, R. Barra-Chicote, Y. Meng, R. Maas, and J. Droppo, \u201cSynthasr: Unlocking synthetic data for speech recognition,\u201d arXiv preprint arXiv:2106.07803, 2021.\n[20] A. O\u0308ktem, M. A. Jaam, E. DeLuca, and G. Tang, \u201cGamayun - language technology for humanitarian response,\u201d in 2020 IEEE Global Humanitarian Technology Conference (GHTC), 2020, pp. 1\u20134.\n[21] P. Ogayo, G. Neubig, and A. W. Black, \u201cBuilding African Voices,\u201d 2022. [Online]. Available: https://www.africanvoices.tech/\n[22] A. Hurskainen and Department of World Cultures, University of Helsinki, Helsinki Corpus of Swahili 2.0 Annotated Version. Kielipankki, 2016. [Online]. Available: http://urn.fi/urn:nbn:fi: lb-2016011301\n[23] G. K. Anumanchipalli, K. Prahallad, and A. W. Black, \u201cFestvox: Tools for creation and analyses of large speech corpora,\u201d in Workshop on Very Large Scale Phonetics Research, UPenn, Philadelphia, 2011, p. 70.\n[24] L. Chiruzzo, P. Amarilla, A. R\u0131\u0301os, and G. Gime\u0301nez Lugo, \u201cDevelopment of a Guarani - Spanish parallel corpus,\u201d in Proceedings of the 12th Language Resources and Evaluation Conference. Marseille, France: European Language Resources Association, May 2020, pp. 2629\u20132633. [Online]. Available: https://aclanthology.org/2020.lrec-1.320\n[25] \u201cSuba New Testament | Suba language project website,\u201d Jan. 2018. [Online]. Available: https://subalanguage.com/en/ scripture-materials/suba-new-testament\n[26] S. Watanabe, T. Hori, S. Karita, T. Hayashi, J. Nishitoba, Y. Unno, N. Enrique Yalta Soplin, J. Heymann, M. Wiesner, N. Chen, A. Renduchintala, and T. Ochiai, \u201cEspnet: End-to-end speech processing toolkit,\u201d in Proc. Interspeech 2018, 2018, pp. 2207\u20132211. [Online]. Available: http://dx.doi.org/10.21437/ Interspeech.2018-1456\n[27] F. Boyer, Y. Shinohara, T. Ishii, H. Inaguma, and S. Watanabe, \u201cA study of transducer based end-to-end asr with espnet: Architecture, auxiliary loss and decoding strategies,\u201d in 2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2021, pp. 16\u201323.\n[28] P. Littell, D. R. Mortensen, K. Lin, K. Kairis, C. Turner, and L. Levin, \u201cUriel and lang2vec: Representing languages as typological, geographical, and phylogenetic vectors,\u201d in Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, 2017, pp. 8\u201314.\n[29] P. Wu, J. Shi, Y. Zhong, S. Watanabe, and A. W. Black, \u201cCrosslingual transfer for speech processing using acoustic language similarity,\u201d in 2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2021, pp. 1050\u20131057.\n[30] A. Conneau, A. Baevski, R. Collobert, A. Mohamed, and M. Auli, \u201cUnsupervised Cross-Lingual Representation Learning for Speech Recognition,\u201d in Proc. Interspeech 2021, 2021, pp. 2426\u20132430."
        }
    ],
    "title": "When Is TTS Augmentation Through a Pivot Language Useful?",
    "year": 2022
}