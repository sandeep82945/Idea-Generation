{
    "abstractText": "Real-world image denoising is an extremely important image processing problem, which aims to recover clean images from noisy images captured in natural environments. In recent years, diffusion models have achieved very promising results in the field of image generation, outperforming previous generation models. However, it has not been widely used in the field of image denoising because it is difficult to control the appropriate position of the added noise. Inspired by diffusion models, this paper proposes a novel general denoising diffusion model that can be used for real-world image denoising. We introduce a diffusion process with linear interpolation, and the intermediate noisy image is interpolated from the original clean image and the corresponding real-world noisy image, so that this diffusion model can handle the level of added noise. In particular, we also introduce two sampling algorithms for this diffusion model. The first one is a simple sampling procedure defined according to the diffusion process, and the second one targets the problem of the first one and makes a number of improvements. Our experimental results show that our proposed method with a simple CNNs Unet achieves comparable results compared to the Transformer architecture. Both quantitative and qualitative evaluations on real-world denoising benchmarks show that the proposed general diffusion model performs almost as well as against the state-of-the-art methods.",
    "authors": [
        {
            "affiliations": [],
            "name": "Cheng Yang"
        },
        {
            "affiliations": [],
            "name": "Lijing Liang"
        }
    ],
    "id": "SP:8c9b4481094dcfca6780aa1fd9631e8b31478134",
    "references": [
        {
            "authors": [
                "Chao Dong",
                "Chen Change Loy",
                "Kaiming He",
                "Xiaoou Tang"
            ],
            "title": "Image super-resolution using deep convolutional networks",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2014
        },
        {
            "authors": [
                "Uwe Schmidt",
                "Stefan Roth"
            ],
            "title": "Shrinkage fields for effective image restoration",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2014
        },
        {
            "authors": [
                "Jiwon Kim",
                "Jung Kwon Lee",
                "Kyoung Mu Lee"
            ],
            "title": "Accurate image super-resolution using very deep convolutional networks. 2016",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2015
        },
        {
            "authors": [
                "Seungjun Nah",
                "Tae Hyun Kim",
                "Kyoung Mu Lee"
            ],
            "title": "Deep multi-scale convolutional neural network for dynamic scene deblurring",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2017
        },
        {
            "authors": [
                "Viren Jain",
                "H. Sebastian Seung"
            ],
            "title": "Natural image denoising with convolutional networks",
            "venue": "In NIPS,",
            "year": 2008
        },
        {
            "authors": [
                "Maurits Malfait",
                "Dirk Roose"
            ],
            "title": "Wavelet-based image denoising using a markov random field a priori model",
            "venue": "IEEE transactions on image processing : a publication of the IEEE Signal Processing Society,",
            "year": 1997
        },
        {
            "authors": [
                "Harold Christopher Burger",
                "Christian J. Schuler",
                "Stefan Harmeling"
            ],
            "title": "Image denoising: Can plain neural networks compete with bm3d? 2012",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2012
        },
        {
            "authors": [
                "Pascal Vincent",
                "H. Larochelle",
                "Yoshua Bengio",
                "Pierre-Antoine Manzagol"
            ],
            "title": "Extracting and composing robust features with denoising autoencoders",
            "venue": "In International Conference on Machine Learning,",
            "year": 2008
        },
        {
            "authors": [
                "Yunjin Chen",
                "Thomas Pock"
            ],
            "title": "Trainable nonlinear reaction diffusion: A flexible framework for fast and effective image restoration",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2015
        },
        {
            "authors": [
                "Xiao-Jiao Mao",
                "Chunhua Shen",
                "Yubin Yang"
            ],
            "title": "Image restoration using very deep convolutional encoder-decoder networks with symmetric skip connections",
            "year": 2016
        },
        {
            "authors": [
                "Sungmin Cha",
                "Taesup Moon"
            ],
            "title": "Fully convolutional pixel adaptive image denoiser",
            "venue": "IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2019
        },
        {
            "authors": [
                "Jingwen Chen",
                "Jiawei Chen",
                "Hongyang Chao",
                "Ming Yang"
            ],
            "title": "Image blind denoising with generative adversarial network based noise modeling",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Ian J. Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron C. Courville",
                "Yoshua Bengio"
            ],
            "title": "Generative adversarial nets",
            "venue": "In NIPS,",
            "year": 2014
        },
        {
            "authors": [
                "K. Zhang",
                "Wangmeng Zuo",
                "Yunjin Chen",
                "Deyu Meng",
                "Lei Zhang"
            ],
            "title": "Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising",
            "venue": "IEEE Transactions on Image Processing,",
            "year": 2016
        },
        {
            "authors": [
                "K. Zhang",
                "Wangmeng Zuo",
                "Lei Zhang"
            ],
            "title": "Ffdnet: Toward a fast and flexible solution for cnn-based image denoising",
            "venue": "IEEE Transactions on Image Processing,",
            "year": 2017
        },
        {
            "authors": [
                "Shi Guo",
                "Zifei Yan",
                "K. Zhang",
                "Wangmeng Zuo",
                "Lei Zhang"
            ],
            "title": "Toward convolutional blind denoising of real photographs",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2019
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam M. Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you",
            "venue": "need. ArXiv,",
            "year": 2017
        },
        {
            "authors": [
                "Jean-Baptiste Cordonnier",
                "Andreas Loukas",
                "Martin Jaggi"
            ],
            "title": "On the relationship between self-attention and convolutional layers",
            "venue": "ArXiv, abs/1911.03584,",
            "year": 2019
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Prajit Ramachandran",
                "A. Srinivas",
                "Niki Parmar",
                "Blake A. Hechtman",
                "Jonathon Shlens"
            ],
            "title": "Scaling local self-attention for parameter efficient visual backbones",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "Hao Li",
                "Zhijing Yang",
                "Xiaobin Hong",
                "Ziying Zhao",
                "Junyang Chen",
                "Yukai Shi",
                "Jin shan Pan"
            ],
            "title": "Dnswin: Toward real-world denoising via continuous wavelet sliding-transformer",
            "venue": "Knowl. Based Syst.,",
            "year": 2022
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly",
                "Jakob Uszkoreit",
                "Neil Houlsby"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition",
            "year": 2010
        },
        {
            "authors": [
                "Ze Liu",
                "Yutong Lin",
                "Yue Cao",
                "Han Hu",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Baining Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Jingyun Liang",
                "Jie Cao",
                "Guolei Sun",
                "K. Zhang",
                "Luc Van Gool",
                "Radu Timofte"
            ],
            "title": "Swinir: Image restoration using swin transformer",
            "venue": "IEEE/CVF International Conference on Computer Vision Workshops (ICCVW),",
            "year": 2021
        },
        {
            "authors": [
                "Olaf Ronneberger",
                "Philipp Fischer",
                "Thomas Brox"
            ],
            "title": "U-net: Convolutional networks for biomedical image segmentation",
            "venue": "ArXiv, abs/1505.04597,",
            "year": 2015
        },
        {
            "authors": [
                "Zhendong Wang",
                "Xiaodong Cun",
                "Jianmin Bao",
                "Jianzhuang Liu"
            ],
            "title": "Uformer: A general u-shaped transformer for image restoration",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Achleshwar Luthra",
                "Harsh Sulakhe",
                "Tanish Mittal",
                "Abhishek Iyer",
                "Santosh Kumar Yadav"
            ],
            "title": "Eformer: Edge enhancement based transformer for medical image",
            "venue": "denoising. ArXiv,",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "P. Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "ArXiv, abs/2006.11239,",
            "year": 2020
        },
        {
            "authors": [
                "Jiaming Song",
                "Chenlin Meng",
                "Stefano Ermon"
            ],
            "title": "Denoising diffusion implicit models",
            "venue": "ArXiv, abs/2010.02502,",
            "year": 2020
        },
        {
            "authors": [
                "Robin San Roman",
                "Eliya Nachmani",
                "Lior Wolf"
            ],
            "title": "Noise estimation for generative diffusion models",
            "venue": "ArXiv, abs/2104.02600,",
            "year": 2021
        },
        {
            "authors": [
                "Arpit Bansal",
                "Eitan Borgnia",
                "Hong-Min Chu",
                "Jie Li",
                "Hamideh Kazemi",
                "Furong Huang",
                "Micah Goldblum",
                "Jonas Geiping",
                "Tom Goldstein"
            ],
            "title": "Cold diffusion: Inverting arbitrary image transforms without",
            "venue": "noise. ArXiv,",
            "year": 2022
        },
        {
            "authors": [
                "Shoufa Chen",
                "Pei Sun",
                "Yibing Song",
                "Ping Luo"
            ],
            "title": "Diffusiondet: Diffusion model for object detection",
            "venue": "ArXiv,",
            "year": 2022
        },
        {
            "authors": [
                "Mehdi Mirza",
                "Simon Osindero"
            ],
            "title": "Conditional generative adversarial",
            "venue": "nets. ArXiv,",
            "year": 2014
        },
        {
            "authors": [
                "Kihyuk Sohn",
                "Honglak Lee",
                "Xinchen Yan"
            ],
            "title": "Learning structured output representation using deep conditional generative models",
            "venue": "In NIPS,",
            "year": 2015
        },
        {
            "authors": [
                "Laurent Dinh",
                "David Krueger",
                "Yoshua Bengio"
            ],
            "title": "Nice: Non-linear independent components estimation",
            "venue": "CoRR, abs/1410.8516,",
            "year": 2014
        },
        {
            "authors": [
                "Pierre Charbonnier",
                "Laure Blanc-F\u00e9raud",
                "Gilles Aubert",
                "Michel Barlaud"
            ],
            "title": "Two deterministic half-quadratic regularization algorithms for computed imaging",
            "venue": "Proceedings of 1st International Conference on Image Processing,",
            "year": 1994
        },
        {
            "authors": [
                "A. Abdelhamed",
                "Stephen Lin",
                "M.S. Brown"
            ],
            "title": "A high-quality denoising dataset for smartphone",
            "venue": "cameras. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Tobias Pl\u00f6tz",
                "Stefan Roth"
            ],
            "title": "Benchmarking denoising algorithms with real photographs",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2017
        },
        {
            "authors": [
                "Zhou Wang",
                "Alan Conrad Bovik",
                "Hamid R. Sheikh",
                "Eero P. Simoncelli"
            ],
            "title": "Image quality assessment: from error visibility to structural similarity",
            "venue": "IEEE Transactions on Image Processing,",
            "year": 2004
        },
        {
            "authors": [
                "Richard Zhang",
                "Phillip Isola",
                "Alexei A. Efros",
                "Eli Shechtman",
                "Oliver Wang"
            ],
            "title": "The unreasonable effectiveness of deep features as a perceptual metric",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Yuda Song",
                "Yunfang Zhu",
                "Xin Du"
            ],
            "title": "Grouped multi-scale network for real-world image denoising",
            "venue": "IEEE Signal Processing Letters,",
            "year": 2020
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "CoRR, abs/1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Zongsheng Yue",
                "Qian Zhao",
                "Lei Zhang",
                "Deyu Meng"
            ],
            "title": "Dual adversarial network: Toward real-world noise removal and noise generation",
            "year": 2007
        },
        {
            "authors": [
                "Zongsheng Yue",
                "Hongwei Yong",
                "Qian Zhao",
                "Lei Zhang",
                "Deyu Meng"
            ],
            "title": "Variational denoising network: Toward blind noise modeling and removal",
            "year": 1908
        },
        {
            "authors": [
                "Syed Waqas Zamir",
                "Aditya Arora",
                "Salman Hameed Khan",
                "Munawar Hayat",
                "Fahad Shahbaz Khan",
                "Ming-Hsuan Yang",
                "Ling Shao"
            ],
            "title": "Learning enriched features for fast image restoration and enhancement",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Syed Waqas Zamir",
                "Aditya Arora",
                "Salman Hameed Khan",
                "Munawar Hayat",
                "Fahad Shahbaz Khan",
                "Ming-Hsuan Yang",
                "Ling Shao"
            ],
            "title": "Cycleisp: Real image restoration via improved data synthesis",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2020
        },
        {
            "authors": [
                "Syed Waqas Zamir",
                "Aditya Arora",
                "Salman Hameed Khan",
                "Munawar Hayat",
                "Fahad Shahbaz Khan",
                "Ming-Hsuan Yang",
                "Ling Shao"
            ],
            "title": "Multi-stage progressive image restoration",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "Shen Cheng",
                "Yuzhi Wang",
                "Haibin Huang",
                "Donghao Liu",
                "Haoqiang Fan",
                "Shuaicheng Liu"
            ],
            "title": "Nbnet: Noise basis learning for image denoising with subspace projection",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "Chi-Mao Fan",
                "Tsung-Jung Liu",
                "Kuan-Hsien Liu"
            ],
            "title": "Sunet: Swin transformer unet for image denoising",
            "venue": "IEEE International Symposium on Circuits and Systems (ISCAS),",
            "year": 2022
        },
        {
            "authors": [
                "Eirikur Agustsson",
                "Radu Timofte"
            ],
            "title": "Ntire 2017 challenge on single image super-resolution: Dataset and study",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),",
            "year": 2017
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Images are an important source of external information for humans and form the basis of human vision, containing a large amount of information about objects. However, during the process of image acquisition, transmission, and storage, images are often corrupted by unwanted signals, resulting in a degradation of image quality that can be detrimental to subsequent image processing operations. The purpose of real-world image denoising is to remove as much unnecessary or redundant noise as possible to ensure the integrity of the original information in the image and to facilitate the application of high-quality images to higher-level computer vision tasks. Real-world image denoising to obtain high quality images is the basis for correct recognition of image information and has a facilitating effect on the performance of other processing aspects of digital images. High-quality, clear images can help solve specific problems in medical imaging, autonomous driving, pattern recognition, and other areas. Therefore, real-world image denoising and image quality improvement are prerequisites for image recognition and various feature extraction.\nWith its powerful nonlinear adaptation capability, convolutional neural networks (CNNs) perform well in many low-level computer vision tasks [1, 2, 3, 4]. Traditional real-world image denoising algorithms use hand-crafted features and mathematical models to remove noise from an image. With CNNs, however, the network can learn these features on its own, without any prior knowledge of image processing techniques or statistical models. Convolutional neural networks can map real-world noisy images to noise-free images in image denoising methods, which is one of the classical algorithms for real-world image denoising, and the performance of convolutional neural networks [5] is comparable to that of the most advanced denoising algorithms based on wavelet transform and Markov random field [6]. There are two categories of image denoising methods based on convolutional neural networks: multi-layer perceptron models [7]and deep learning methods. Autoencoder [8] is a classical image denoising method based on the multi-layer perceptron model. The denoising autoencoder mentioned above can be used for pre-training deep neural networks just\nar X\niv :2\n30 5.\n04 45\n7v 1\n[ cs\n.C V\n] 8\nM ay\nlike ordinary autoencoders, but the denoising autoencoder can add noise to the original input as the input of the encoder, which can force the deep neural network to learn more robust invariant features so that the input can be more effective. TNRD [9] is a feed-forward deep neural network, also called trainable nonlinear reaction-diffusion model, which has achieved better denoising performance. Convolutional neural networks are used to extract features, which can reduce the influence of noise while extracting features, and then complete image inpainting by deconvolution to achieve the purpose of image denoising[10]. FC-AIDE [11] based on contextual pixel-level mapping, uses a fully convolutional enhanced supervised model to achieve more robust adaptability and improve image denoising performance through regularization methods. By exploiting the powerful capabilities of convolutional neural networks, these methods have achieved significant success in real-world image denoising.\nGCBD [12] uses the generative adversarial networks [13] for blind denoising, first a GAN network is trained to estimate the noise distribution on the input noisy image and generate noisy samples. Second, a paired training dataset is constructed using the noise patches collected in the first step, and then a deep CNN is trained to denoise the given noisy image. DnCNN [14] uses residual learning in the field of image denoising for the first time, and combines residual learning with batch normalization to speed up the training of the image denoising model and improve the denoising effect. DnCNN can effectively remove uniform Gaussian noise and suppress noise within a certain noise level. However, the real-world noise is usually due to the characteristics of color channel correlation and signal dependency, which cannot be handled by uniform Gaussian noise. In this case, FFDNet [15] tackles complex noise in real scenes by using noise level estimation as the input to the network, thus preserving more details in the image. CBDNet [16] proposes a framework that includes a noise estimation subnetwork and a non-blind denoising subnetwork, which combines the noise estimation and non-blind denoising models, and uses synthetic noise and real noise images for network training to improve denoising performance. Convolutional neural networks can automatically extract image features and reduce computational cost during the denoising process, while avoiding complicated computations during learning and inference. However, real-world image denoising methods based on convolutional neural networks have a remarkable effect on images with additive white Gaussian noise and struggle to deal well with real-world noise images. In addition, the convolution kernels of convolutional neural networks cannot adapt to the image content when long-term dependencies need to be established, resulting in global information loss.\nConvolutional neural networks excel at extracting local information, but have limitations in capturing long-range dependencies between global data. The introduction of self-attention mechanisms [17], residual feed-forward networks, and multi-head mechanisms in the Transformer architecture has revolutionized computer vision by effectively mining global interactions between textual information. Self-attention mechanisms [18, 19, 20] can replace convolutional networks for convolutional-like operations and can represent more long-range correlations, and scaling techniques for local self-attention models have outperformed even efficient convolutional models. Image Transformer [21] is a new image generator on the ImageNet dataset that performs well on ultra-high resolution tasks. ViT [22] is the representative work of transformer in the field of image recognition, which uses only the self-attention mechanism to achieve the current state-of-the-art recognition rate. Due to the excellent performance of transformer architecture on natural language and high-level vision tasks, Transformer has also been widely applied to real-world image denoising. Swin Transformer [23] is a transformer-based backbone network that adapts to multi-scale images while reducing computational complexity. SwinIR [24] consists of Transformer architecture, including three modules: shallow feature\nextraction, deep feature extraction, and high-quality image reconstruction, which can effectively remove the interference of severe noise while preserving high-frequency image details, resulting in sharper edges and more natural textures. SUNet [25] uses the Swin Transformer as a basic block and applies it to the UNet architecture for image denoising. Uformer [26] designs the local enhancement window transformer module and the skip connection mechanism to show excellent performance in real-world image denoising. Eformer [27] first uses the transformer for medical image denoising and constructs an encoder-decoder network for medical image denoising through the transformer module. The transformer structure solves the deficiency of convolutional neural networks in extracting the global information feature of the image, and the self-attention mechanism can produce a more interpretable model. However, transformer modules are usually very large and computationally intensive, which cannot be applied to most image restoration tasks involving high-resolution images, and the acquisition of local information by the transformer is not as strong as convolutional neural networks in some cases.\nRecent studies have shown that diffusion models [28, 29, 30, 31, 32] have achieved excellent performance in the field of generative modeling [13, 33, 34, 35], even surpassing previous classical methods. However, although diffusion models have been widely used in other fields, they are still not widely used in the field of real-world image denoising. This is mainly because the final step image in the traditional diffusion model is Gaussian noise, and it is difficult to precisely control the level of noise added during the forward process. While in the process of real-world image denoising, we hope that the last step image is the corresponding real-world noise image, and the level of added noise can be controlled, so as to gradually perform the denoising operation. Another reason is that real-world noise is extremely complex, which has various characteristics, including spatial and temporal dependence, frequency and color dependence, shot noise, pattern noise, and many other factors that contribute to the noise characteristics of different types of images. To address these problems, as shown in Figure 1, we propose a more general real-world image denoising diffusion model with linear interpolation, which achieves real-world image denoising through a forward process of gradual noise addition and a reverse process of gradual denoising operation. The intermediate noisy image is interpolated from the original clean image and the corresponding real-world noisy image, with the advantage that at time step t = 0, the image is the original clean image, and at time step t = T , the image corresponds to the real-world noisy image. Our proposed method controls the amount of added noise by the parameter \u03b1 = t/T , so that the noise added in the diffusion process is closer to the real world than Gaussian noise with standard distribution, thus achieving more effective image denoising. Compared with traditional methods, this model performs well in various scenarios and has broad application prospects. In general, our contributions can be summarized as follows.\n\u2022 Real-world image denoising using the diffusion model has not been studied, and we apply this method to real-world image denoising. It is noteworthy that our experimental results in image denoising show that using a simple CNNs Unet network alone can achieve denoising effects that approximate those of the Transformer architecture.\n\u2022 We propose a method using linear interpolation to precisely control the addition of noise to ensure that the added noise perfectly matches the noise image. With this method, the diffusion process can start with the original clean image and end with the corresponding real-world noise image instead of the pure Gaussian noise. This method provides new ideas for the field of image denoising and has the potential to achieve more precise and efficient image denoising operations in the future.\n\u2022 The effect of image denoising is strongly related to the sampling strategy adopted by the diffusion model. We propose an improved sampling algorithm with remarkable mathematical properties that can produce beneficial effects for real-world image denoising even when the original image cannot be accurately estimated. Compared with traditional sampling methods, our new method can achieve superior sampling results and bring new breakthroughs and innovations to the research and practice of image denoising."
        },
        {
            "heading": "2 Background",
            "text": "DDPM(Denoising Diffusion Probabilistic Model) [28] is a type of probabilistic generative model based on diffusion processes. It models at the pixel level and can generate high-quality images. DDPM combines the affine transformation network and the diffusion process network to model the image distribution. The model uses the reverse diffusion algorithm as an optimization method and continuously improves its generative capability by iteratively optimizing the network parameters.\nDDPM has a fixed Markov chain structure for the approximate posterior q(x1:T |x0), which is also called the forward process or diffusion process. This Markov chain gradually adds Gaussian noise to the data over time, using a variance schedule \u03b21, . . . , \u03b2T to control the amount of noise added at each point in the sequence:\nq (x1:T | x0) := T\u220f t=1 q (xt | xt\u22121) , q (xt | xt\u22121) := N ( xt; \u221a 1\u2212 \u03b2txt\u22121, \u03b2tI ) (1)\nThe joint distribution p\u03b8(x0:T ) is known as the reverse process in diffusion models, and it is defined as a Markov chain with learned Gaussian transitions. This chain starts with the prior distribution p(xT ) = N (xT ; 0, I), which is also a Gaussian distribution with mean 0 and covariance matrix I . The Markov chain then reduces the noise according to the learned Gaussian transformation, gradually evolving over time until it reaches the observed data x0:\np\u03b8 (x0:T ) := p (xT ) T\u220f t=1 p\u03b8 (xt\u22121 | xt) , p\u03b8 (xt\u22121 | xt) := N (xt\u22121;\u00b5\u03b8 (xt, t) ,\u03a3\u03b8 (xt, t)) (2)"
        },
        {
            "heading": "3 General Diffusion Model",
            "text": "The standard diffusion model that is commonly used in image generation tasks consists of two integral steps. First, a continuous random noise process is progressively added to the original image, resulting in a Gaussian distribution. A Unet model is then trained on this noisy image to estimate the added noise. Second, a pure Gaussian image is progressively denoised by subtracting the estimated noise from the image at each step using the same Unet model. This iterative process continues until the final version of the cleanly generated image is obtained. In our work, we present a more general diffusion model that takes into account matching the added noise to the real-world noisy image.\n3.1 Model Components and Training\nAlgorithm 1 Training 1: repeat 2: x0 \u223c q(x0), xT \u223c q(xT ) 3: t \u223c Uniform({0, . . . , T}) 4: \u03b1 = t/T 5: xt = (1\u2212 \u03b1)x0 + \u03b1xT 6: Take gradient descent step on\n\u2207\u03b8 \u2016x0 \u2212 S\u03b8 (xt, t)\u20162 7: until converged The study considers a clean image x0 \u2208 RN and its corresponding real-world noisy image xT \u2208 RN , where a parameter T determines the diffusion step. In Chapter 4 of the paper, the choice of T is discussed as a hyperparameter that significantly influences the outcome of the process. To control the amount of noise added to the image, a parameter \u03b1 is used, where \u03b1 = t/T . The Algorithm 1 ensures that the noisy image is intermediate between x0 and xT , which differs from the conventional diffusion model that adds noise until a pure Gaussian image is obtained. The proposed method results in an improved overall quality of the real-world denoised image, while allowing control of the added noise. It is essential that the output distribution xt varies continuously with respect to t, and the operator must satisfy this requirement:\nxt = (1\u2212 \u03b1)x0 + \u03b1xT (3)\nTo implement a sequential reverse diffusion process, we use a basic Unet network called S\u03b8. This network is specifically designed to accept two different inputs: the noisy image xt and the time step t, which allows the accurate estimation of the desired output x0. The proposed architecture of the network facilitates the progressive denoising of the image by using deep learning driven reverse diffusion processing.\nxt = (1\u2212 \u03b1)S\u03b8 (xt, t) + \u03b1xT (4)\nIn practical applications, a neural network parameterized by \u03b8 must be used. The objective function of this network is to accurately restore the image x0, and this is achieved by minimizing the corresponding problem through training:\nmin \u03b8 Ex0,xT\u223cpdata \u2016S\u03b8 (xt, t)\u2212 x0\u2016 (5)\nwhere x0 denotes a clean image randomly sampled from the distribution pdata, xT denotes a real-world noisy image drawn from the same distribution. The intermediate image, denoted as xt, is obtained by interpolating between x0 and xT . In the study, the norm used is represented by ||\u00b7||, which is specifically set to `2 within the experimental framework.\nInstead of using the L1 loss, our model is optimized with the robust Charbonnier loss [36] to better handle outliers and achieve better performance. The Charbonnier loss is defined as follows:\nLchar = \u221a \u2016IDenoised \u2212 IGT \u20162 + \u03b52 (6)\nwhere IGT represents the ground truth image and \u03b5 is an empirical value, set to 0.001 in this paper. Compared to the L1 loss, the Lchar loss makes the model more robust.\n3.2 Sampling Algorithm\nAlgorithm 2 Origin Sampling\n1: xT \u223c q(xT ) 2: for t = T, T \u2212 1 . . . , 1 do 3: \u03b1 = (t\u2212 1)/T 4: xt\u22121 = (1\u2212 \u03b1)S\u03b8 (xt, t) + \u03b1xT 5: end for 6: return x0\nAlgorithm 3 Improve Sampling\n1: xT \u223c q(xT ) 2: for t = T, T \u2212 1 . . . , 1 do 3: \u03b1t = t/T 4: \u03b1t\u22121 = (t\u2212 1)/T 5: x\u0303t = (1\u2212 \u03b1t)S\u03b8 (xt, t) + \u03b1txT 6: x\u0303t\u22121 = (1\u2212 \u03b1t\u22121)S\u03b8 (xt, t) + \u03b1t\u22121xT 7: xt\u22121 = xt \u2212 x\u0303t + x\u0303t\u22121 8: end for 9: return x0\nThe performance of real-world image denoising is highly dependent on the sampling strategy employed by the diffusion model, and the traditional method for generating an image using the diffusion model involves step-by-step sampling. This paper proposes a similar step-wise sampling algorithm for image denoising. In the forward process, a Unet is trained to estimate x0 for images with different noisy levels obtained by interpolating between clean and real-world noisy images. In the reverse process, the real-world noisy image is denoised step-by-step using S\u03b8. This corresponds to the standard sampling algorithm as described in Algorithm 2.\nThe accuracy of Unet is better for real-world image denoising when S\u03b8 (xt, t) becomes equivalent to x0. This is because as S\u03b8 approaches x0, the iterative results become more accurate for all steps of t. Therefore, the more accurate the estimation of x0, the better the performance of real-world image denoising. However, the accuracy of Unet is generally not very good, and the errors begin to accumulate, leading to the deviation of the iteration results from x0, resulting in poor image denoising performance.\nThe Unet is limited by the network architecture and data sizes, making it inaccurate in predicting x0, which leads to error accumulation and causes imprecise iterative denoising effects (experimentally demonstrated in Section 4 of this paper). To address this problem, the Algorithm 3 is proposed to perform denoising sampling. The experimental results confirm that it outperforms the traditional sampling Algorithm 2 in terms of performance.\nThis sampling algorithm has remarkable mathematical properties that enhance the real-world image denoising effect beyond the Algorithm 2. In particular, even if S\u03b8 does not accurately estimate x0, it can still produce a beneficial result for the real-world image denoising task. In the following section, we will elaborate on these special features and discuss their implications for improving real-world image denoising performance."
        },
        {
            "heading": "3.3 Detail of Algorithm 3",
            "text": "It is clear from the two algorithms that if S\u03b8 (xt, t) = x0 for all t < T , the two sampling algorithms can both denoise the real-world image accurately. In this section, we will analyze in detail the stability of these algorithms to errors in real-world image denoising.\nThe Algorithm 2 can be written as the following formula(7):\nxt\u22121 = (1\u2212 \u03b1)S\u03b8(xt, t) + \u03b1xT = [1\u2212 (t\u2212 1)/T ]S\u03b8(xt, t) + (t\u2212 1)/T \u00b7 xT = S\u03b8(xt, t) + (t\u2212 1)/T \u00b7 [xT \u2212 S\u03b8(xt, t)]\n(7)\nThe Algorithm 3 can be written as the following formula(8):\nxt\u22121 = xt \u2212 x\u0303t + x\u0303t\u22121 = xt \u2212 (1\u2212 \u03b1t)S\u03b8(xt, t)\u2212 \u03b1txT + (1\u2212 \u03b1t\u22121)S\u03b8(xt, t) + \u03b1t\u22121xT = xt \u2212 (1\u2212 t/T ) \u00b7 S\u03b8(xt, t)\u2212 t/T \u00b7 xT + [1\u2212 (t\u2212 1)/T ] \u00b7 S\u03b8(xt, t) + (t\u2212 1)/T \u00b7 xT = xt \u2212 1/T \u00b7 [xT \u2212 S\u03b8(xt, t)]\n(8)\nIt can be seen from Equation 7 that the Algorithm 2 is more dependent on the result of S\u03b8(xt, t), both the first and second terms of the formula are related to S\u03b8(xt, t), and the coefficient of xT \u2212 S\u03b8(xt, t) is t\u2212 1 times larger than that of the Algorithm 3, which will overestimate the error. If there is an error in the result of S\u03b8(xt, t), after several rounds of iteration, the error will be infinite, resulting in poor performance for real world image denoising.\nIn contrast, the Algorithm 3 is more robust to erroneous predictions of S\u03b8(xt, t). The first term has no direct relation to S\u03b8(xt, t) and refers only to the current noise image xt, the second term xT \u2212 S\u03b8(xt, t) is t\u2212 1 times smaller than that of Algorithm 2. The positive or negative in front of the second term is not important, it is just the accumulation of errors under quantitative analysis. From the Equation 7 and the Equation 8 we can conclude that the Algorithm 3 is less dependent on S\u03b8(xt, t) and can handle larger errors more efficiently. After numerous iterations, the performance of the Algorithm 3 surpasses that of the Algorithm 2, and we can draw the same conclusion from subsequent experiments."
        },
        {
            "heading": "4 Experiments",
            "text": "In this section, we will perform real-world image denoising experiments with our proposed method. First, we will introduce the real-world noisy image datasets and the evaluation index. Then, we will describe the experimental settings, including the training and testing settings and implementation details. Finally, we will compare our model with state-of-the-art methods on two real-world image denoising benchmarks."
        },
        {
            "heading": "4.1 Datasets and Evaluation Index",
            "text": "To comprehensively verify our proposed method, we use the following two real-world noisy image datasets:\nSIDD Dataset The SIDD [37] is a dataset for the evaluation of image denoising algorithms for smartphone cameras. It consists of 30,000 low-resolution (LR) images and their corresponding high-resolution (HR) counterparts, captured with five different smartphone cameras under different lighting conditions. The dataset comes with a predefined training and validation set, as well as an additional benchmark set for performance evaluation. For fast training and evaluation, we use the SIDD-Medium dataset (320 image pairs) for training and evaluate the method on the SIDD validation dataset1.\nDND Dataset The DND [38] consists of 50 pairs of low-light scene images and corresponding high-quality reference images that have been captured under different imaging conditions. The dataset is intended to facilitate the evaluation of image denoising methods in real-world scenarios. Each image pair includes a raw noisy image, accompanied by its ground-truth clean version. The dataset\u2019s diverse range of scenarios makes it well-suited for challenging real-world\ndenoising problems. However, the DND2 does not provide any additional training data for fine-tuning denoising networks and can only be evaluated online.\nIn terms of evaluation metrics, we quantitatively analyze real-world image denoising using structural similarity (SSIM) [39] and peak signal-to-noise ratio (PSNR) [40], which focus on pixel fields and are the most commonly used evaluation metrics in image restoration."
        },
        {
            "heading": "4.2 Implementation Detail and Competing Methods",
            "text": "Following the general training of GMSNet [41], we use the Adam optimizer [42] with \u03b21 = 0.9 and \u03b22 = 0.999 to train our model. To improve the trade-off between the size of the input patches and the available computing power, we set the batch size to 32 and the image patch size to 256\u00d7 256. The learning rate is 2\u00d7 10\u22124. For all experiments we use flipping and random rotation with angles of 90\u25e6, 180\u25e6 and 270\u25e6 for data augmentation. All experiments are performed in a Linux environment with PyTorch (1.12.0) running on a server with an NVIDIA RTX A6000 GPU. Nvidia CUDA 11.7 and cuDNN are used to accelerate the GPU computations. We use 500,000 iterations for the diffusion model, which takes about eight days to train.\nTo prove the superiority of our method, we compare it to state-of-the-art approaches on a real-world image denoising task. In particular, we evaluate its real-world image denoising performance on two test sets, the SIDD validation dataset [37] and the DND benchmark [38].\nSIDD Validation Dataset For the SIDD validation dataset, we follow the setting of DANet [43], which uses only the SIDD-Medium dataset for training. We use three sampling algorithms for image denoising: Origin Sampling Algorithm 2, Improved Sampling Algorithm 3, and Direct Sampling Algorithm, which uses only one step S\u03b8 (xt, t) = x0 for image denoising. To evaluate the effectiveness of our proposed method, we compare it with a baseline model without the diffusion model, denoted as Without DDPM. We also compare our method with previously developed state-of-the-art methods, including CBDNet [16], VDN [44], DANet [43], MIRNet [45], CycleISP [46], MPRNet [47], NBNet [48], GMSNet [41], Uformer [49], SwinIR [24].\nDND For the DND benchmark, we train our model on the SIDD Medium dataset and synthesize the noisy images provided by GMSNet [41] for a fair comparison. The synthesized noisy images are generated by DIV2K [50]. The high-resolution images in DIV2K are cropped into non-overlapping image patches, and then noise is added to these patches. We randomly sample 320 image patches from the SIDD-Medium dataset and 100 image patches from the synthetic noisy dataset for each training iteration. We also use three identical sampling algorithms, exactly the same as SIDD above and without DDPM, for image denoising, and use the same baselines as those implemented on the SIDD validation dataset for comparison purposes.\n1https://www.eecs.yorku.ca/k\u0303amel/sidd/benchmark.php 2https://noise.visinf.tu-darmstadt.de/benchmark/#results_srgb"
        },
        {
            "heading": "4.3 Comparison of Experimental Results",
            "text": "SIDD Validation Dataset As depicted in Table 1, the results indicate that our model of Direct Sampling achieves better performance than all the state-of-the-art methods. For example, our method compares favorably with the recently developed state-of-the-art SwinIR [24] method, where the PSNR and SSIM of the proposed results are 0.04 dB and 0.001 higher than those of SwinIR, respectively, justifying the effectiveness of our model. Compared with DANet [43], which uses a GAN [33], our model still achieves a 0.34 dB improvement in the PSNR index. Additionally, we present the visual comparison in Figure 2. We can see that our model recovers sharper and clearer images than those of the other methods, and is therefore more faithful to the ground truth, which verifies the effectiveness of the diffusion model for image denoising. We can also see that the performance of Improve Sampling is better than the Origin Sampling and Without DDPM methods, which has demonstrated the efficiency of the sampling algorithm 3. Although the performance of Improve Sampling is not better than Direct Sampling, this is not important and our experiments demonstrate the improvement of the diffusion model on the real-world image denoising performance.\nDND benchmark In Table 2, our model of Direct Sampling achieves a significant improvement over other stateof-the-art methods. Compared to MPRNet [47], our model shows performance gains of 0.42 dB and 0.008 in terms of PSNR and SSIM, respectively, justifying the notion that real-world image denoising via diffusion model makes our model more robust in handling information with different frequencies. Compared to the ViT-based methods [22], Uformer [49] and SwinIR [24], our model outperforms them by a PSNR margin of up to 0.10 dB. The qualitative results shown in Figure 3 verify that our model achieves a significant visual quality improvement over CycleISP [46] with realistic texture and clear structure. Our result also shows that Improve Sampling is better than Origin Sampling, and demonstrates the efficiency of the diffusion model, the result is the same as the SIDD validation dataset.\nTime Steps As shown in Table 3, diffusion steps have an effect on the denoising of real-world images. We perform detailed experiments on different steps on the DND benchmark, since the model must be retrained for different numbers of diffusion steps, our experiments only perform different training for the DND benchmark, and the sampling method is Improve Sampling. The performance of real-world image denoising is best when t = 70."
        },
        {
            "heading": "5 Conclusion",
            "text": "This study presents an effective method for real-world denoising based on a general diffusion model with linear interpolation. The proposed method combines the advantages of the simple Unet and the diffusion model, which not only takes advantage of the local receptive field of CNNs for processing large images, but also exploits the generative advantage of the diffusion model. Specifically, in the forward process, the method interpolates images with different noise levels to estimate their noise using a simple Unet. In the reverse process, the model estimates and removes the noise step by step, resulting in real image denoising. We also propose two sampling algorithms and compare the results of the two sampling algorithms with Direct Sampling. Although the results of the sampling algorithms are not better than Direct Sampling, this limitation could be attributed to the insufficient rigidity of the diffusion model and is not a major concern. More importantly, the proposed method shows significant improvements in real-world image denoising performance.\nLet\u2019s rethink, why does our method work? Through experiments, we have found that Direct Sampling is better than other sampling algorithms, we have also come to the conclusion that our proposed general diffusion model works in a manner similar to data augmentation. By interpolating noise images of different sizes, using the different time steps (T ) to control the degree of data augmentation, we can augment the dataset and use it to train a simple Unet. Data augmentation is widely used in machine learning as a technique to increase the size of training datasets, which can improve the generalization ability of models. Our method exploits this concept by using the diffusion process to generate new instances of noisy images with varying degrees of corruption. By using these augmented inputs during training, our model becomes more robust to variations in noise and can achieve better performance for real-world image denoising, which is also what makes Direct Sampling superior to other sampling algorithms.\nOf course, our proposed method has some limitations, one of which is the imprecision of the diffusion process. This can affect the performance of the two sampling methods we use and result in lower quality samples compared to Direct Sampling. Although we have made improvements to the sampling Algorithm 3, the results are still not as good as we would like. Addressing the imprecision of the diffusion process and optimizing the sampling algorithm will be a priority for future work on this topic. One possible approach to address this limitation would be to explore alternative diffusion models that provide greater precision and control over the samples generated. Another approach could be to improve the training process by incorporating additional loss functions or regularization techniques that encourage the model to generate higher quality samples.\nThe effectiveness of the method is demonstrated through extensive experiments, which include comparisons with state-of-the-art denoising methods on standard benchmark datasets. The results show that the proposed method achieves superior performance on various metrics, including PSNR and SSIM. Furthermore, the present work contributes to the field of image processing by advancing the understanding of how diffusion models can be integrated with CNNs to improve their performance in tasks such as denoising. The proposed method has potential applications in diverse domains, ranging from biomedical imaging to autonomous driving systems. We believe that our proposed method has the potential to be widely adopted in various domains due to its superior performance and versatility in handling different types of noise. Overall, we hope that our work will inspire further research in this area and lead to new innovations that can benefit society."
        },
        {
            "heading": "Acknowledgments",
            "text": "This work is supported in part by the National Key R&D Program of China (no. 2018AAA0100301), National Natural Science Foundation of China (no. 61976041), and Fundamental Research Funds for the Central Universities (DUT22LAB303)."
        }
    ],
    "title": "REAL-WORLD DENOISING VIA DIFFUSION MODEL",
    "year": 2023
}