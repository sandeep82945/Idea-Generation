{
    "abstractText": "In order to avoid the noise diffusion and amplification caused by traditional dehazing algorithms, a single image haze removal algorithm based on nonsubsampled contourlet transform (HRNSCT) is proposed. The HRNSCT removes haze only from the low-frequency components and suppresses noise in the high-frequency components of hazy images, preventing noise amplification caused by traditional dehazing algorithms. First, the nonsubsampled contourlet transform (NSCT) is used to decompose each channel of a hazy and noisy color image into low-frequency sub-band and high-frequency direction subbands. Second, according to the low-frequency sub-bands of the three channels, the color attenuation prior and dark channel prior are combined to estimate the transmission map, and use the transmission map to dehaze the low frequency sub-bands. Then, to achieve the noise suppression and details enhancement of the dehazed image, the high-frequency direction sub-bands of the three channels are shrunk, and those shrunk sub-bands are enhanced according to the transmission map. Finally, the nonsubsampled contourlet inverse transform is performed on the dehazed low-frequency sub-bands and enhanced high-frequency sub-bands to reconstruct the dehazed and noise-suppressed image. The experimental results show that the HRNSCT provides excellent haze removal and noise suppression performance and prevents noise amplification during dehazing, making it well suited for removing haze from noisy images. INDEX TERMS Image processing, image restoration, haze removal, nonsubsampled contourlet transform, noise suppression.",
    "authors": [
        {
            "affiliations": [],
            "name": "BOWEN ZHANG"
        },
        {
            "affiliations": [],
            "name": "MANLI WANG"
        },
        {
            "affiliations": [],
            "name": "XIAOBO SHEN"
        },
        {
            "affiliations": [],
            "name": "Manli Wang"
        }
    ],
    "id": "SP:83b3ef194b2196b2562a218fb00e7ff9d1bae394",
    "references": [
        {
            "authors": [
                "M.A. Khan",
                "M.I.U. Lali",
                "M. Sharif",
                "K. Javed",
                "K. Aurangzeb",
                "S.I. Haider",
                "A.S. Altamrah",
                "T. Akram"
            ],
            "title": "An optimized method for segmentation and classification of apple diseases based on strong correlation and genetic algorithm based feature selection",
            "venue": "IEEE Access, vol. 7, pp. 46261\u201346277, 2019, doi: 10.1109/ACCESS.2019.2908040.",
            "year": 2019
        },
        {
            "authors": [
                "C. Zhu",
                "Y. Peng"
            ],
            "title": "A boosted multi-task model for pedestrian detection with occlusion handling",
            "venue": "IEEE Trans. Image Process., vol. 24, no. 12, pp. 5619\u20135629, Dec. 2015, doi: 10.1109/TIP.2015.2483376.",
            "year": 2015
        },
        {
            "authors": [
                "J. Zhou",
                "D. Zhang",
                "P. Zou",
                "W. Zhang",
                "W. Zhang"
            ],
            "title": "Retinex-based Laplacian pyramid method for image defogging",
            "venue": "IEEE Access, vol. 7, pp. 122459\u2013122472, 2019, doi: 10.1109/ACCESS.2019.2934981.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Gao",
                "H. Hu"
            ],
            "title": "Foggy image enhancement based on multi-block coordinated single-scale retinex",
            "venue": "J. BeijingUniv. Aeronaut. Astronaut., vol. 45, no. 5, pp. 944\u2013951, 2019, doi: 10.13700/j.bh.1001-5965.2018.0528.",
            "year": 2019
        },
        {
            "authors": [
                "M. Zheng",
                "G. Qi",
                "Z. Zhu",
                "Y. Li",
                "H. Wei",
                "Y. Liu"
            ],
            "title": "Image dehazing by an artificial image fusion method based on adaptive structure decomposition",
            "venue": "IEEE Sensors J., vol. 20, no. 14, pp. 8062\u20138072, Jul. 2020, doi: 10.1109/JSEN.2020.2981719.",
            "year": 2020
        },
        {
            "authors": [
                "R. Fattal"
            ],
            "title": "Dehazing using color-lines,\u2019\u2019ACMTrans",
            "venue": "Graph., vol. 34,",
            "year": 2014
        },
        {
            "authors": [
                "C.-H. Yeh",
                "C.-H. Huang",
                "L.-W. Kang"
            ],
            "title": "Multi-scale deep residual learning-based single image haze removal via image decomposition",
            "venue": "IEEE Trans. Image Process., vol. 29, pp. 3153\u20133167, 2020, doi: 10.1109/TIP.2019.2957929.",
            "year": 2020
        },
        {
            "authors": [
                "K. Yuan",
                "J. Wei",
                "W. Lu",
                "N. Xiong"
            ],
            "title": "Single image dehazing via NIN-DehazeNet",
            "venue": "IEEE Access, vol. 7, pp. 181348\u2013181356, 2019, doi: 10.1109/ACCESS.2019.2958607.",
            "year": 1813
        },
        {
            "authors": [
                "C. Ancuti",
                "C.O. Ancuti",
                "C. De Vleeschouwer",
                "A.C. Bovik"
            ],
            "title": "Day and night-time dehazing by local airlight estimation",
            "venue": "IEEE Trans. Image Process., vol. 29, pp. 6264\u20136275, 2020, doi: 10.1109/TIP.2020.2988203.",
            "year": 2020
        },
        {
            "authors": [
                "J. Shin",
                "M. Kim",
                "J. Paik",
                "S. Lee"
            ],
            "title": "Radiance\u2013reflectance combined optimization and structure-guided L0-norm for single image dehazing",
            "venue": "IEEE Trans. Multimedia, vol. 22, no. 1, pp. 30\u201344, Jan. 2020, doi: 10.1109/TMM.2019.2922127.",
            "year": 2020
        },
        {
            "authors": [
                "W. Wang",
                "X. Yuan",
                "X. Wu",
                "Y. Liu"
            ],
            "title": "Fast image dehazing method based on linear transformation",
            "venue": "IEEE Trans. Multimedia, vol. 19, no. 6, pp. 1142\u20131155, Jun. 2017, doi: 10.1109/TMM.2017.2652069.",
            "year": 2017
        },
        {
            "authors": [
                "W. Lou",
                "Y. Li",
                "G. Yang",
                "C. Chen",
                "H. Yang",
                "T. Yu"
            ],
            "title": "Integrating haze density features for fast nighttime image dehazing",
            "venue": "IEEE Access, vol. 8, pp. 113318\u2013113330, 2020, doi: 10.1109/ACCESS.2020.3003444.",
            "year": 2020
        },
        {
            "authors": [
                "K. He",
                "J. Sun",
                "andX. Tang"
            ],
            "title": "Single image haze removal using dark channel prior",
            "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit., Miami, FL, USA, Jun. 2009, pp. 1956\u20131963.",
            "year": 2009
        },
        {
            "authors": [
                "Z. Wang",
                "G. Hou",
                "Z. Pan",
                "G. Wang"
            ],
            "title": "Single image dehazing and denoising combining dark channel prior and variational models",
            "venue": "IET Comput. Vis., vol. 12, no. 4, pp. 393\u2013402, Jun. 2018, doi: 10.1049/ietcvi.2017.0318.",
            "year": 2018
        },
        {
            "authors": [
                "K. Borkar",
                "S. Mukherjee"
            ],
            "title": "Single image dehazing by approximating and eliminating the additional airlight component",
            "venue": "Neurocomputing, vol. 400, pp. 294\u2013308, Aug. 2020, doi: 10.1016/j.neucom.2020.03.027.",
            "year": 2020
        },
        {
            "authors": [
                "T. Yu",
                "K. Song",
                "P. Miao",
                "G. Yang",
                "H. Yang",
                "C. Chen"
            ],
            "title": "Nighttime single image dehazing via pixel-wise alpha blending",
            "venue": "IEEE Access, vol. 7, pp. 114619\u2013114630, 2019, doi: 10.1109/ACCESS.2019.2936049.",
            "year": 2019
        },
        {
            "authors": [
                "S.E. Kim",
                "T.H. Park",
                "I.K. Eom"
            ],
            "title": "Fast single image dehazing using saturation based transmission map estimation",
            "venue": "IEEE Trans. Image Process., vol. 29, pp. 1985\u20131998, 2020, doi: 10.1109/TIP.2019.2948279.",
            "year": 1985
        },
        {
            "authors": [
                "H.-M. Hu",
                "H. Zhang",
                "Z. Zhao",
                "B. Li",
                "J. Zheng"
            ],
            "title": "Adaptive single image dehazing using joint local-global illumination adjustment",
            "venue": "IEEE Trans. Multimedia, vol. 22, no. 6, pp. 1485\u20131495, Jun. 2020, doi: 10.1109/TMM.2019.2944260.",
            "year": 2020
        },
        {
            "authors": [
                "G. Sahu",
                "A. Seal"
            ],
            "title": "Image dehazing based on luminance stretching",
            "venue": "Proc. Int. Conf. Inf. Technol. (ICIT), Bhubaneswar, India, Dec. 2019, pp. 388\u2013393, doi: 10.1109/ICIT48102.2019.00075.",
            "year": 2019
        },
        {
            "authors": [
                "T.M. Bui andW.Kim"
            ],
            "title": "Single image dehazing using color ellipsoid prior",
            "venue": "IEEE Trans. Image Process., vol. 27, no. 2, pp. 999\u20131009, Feb. 2018, doi: 10.1109/TIP.2017.2771158.",
            "year": 2018
        },
        {
            "authors": [
                "Q. Zhu",
                "J. Mai",
                "L. Shao"
            ],
            "title": "A fast single image haze removal algorithm using color attenuation prior",
            "venue": "IEEE Trans. Image Process., vol. 24, no. 11, pp. 3522\u20133533, Nov. 2015, doi: 10.1109/TIP.2015.2446191.",
            "year": 2015
        },
        {
            "authors": [
                "D. Berman",
                "T. Treibitz",
                "S. Avidan"
            ],
            "title": "Non-local image dehazing",
            "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Las Vegas, NV, USA, Jun. 2016, pp. 1674\u20131682, doi: 10.1109/CVPR.2016.185.",
            "year": 2016
        },
        {
            "authors": [
                "Y. Liu",
                "J. Shang",
                "L. Pan",
                "A. Wang",
                "M. Wang"
            ],
            "title": "A unified variational model for single image dehazing",
            "venue": "IEEE Access, vol. 7, pp. 15722\u201315736, 2019, doi: 10.1109/ACCESS.2019.2894525.",
            "year": 2019
        },
        {
            "authors": [
                "Q. Liu",
                "X. Gao",
                "L. He",
                "W. Lu"
            ],
            "title": "Single image dehazing with depth-aware non-local total variation regularization",
            "venue": "IEEE Trans. Image Process., vol. 27, no. 10, pp. 5178\u20135191, Oct. 2018, doi: 10.1109/TIP. 2018.2849928.",
            "year": 2018
        },
        {
            "authors": [
                "Q. Wu",
                "J. Zhang",
                "W. Ren",
                "W. Zuo",
                "X. Cao"
            ],
            "title": "Accurate transmission estimation for removing haze and noise from a single image",
            "venue": "IEEE Trans. Image Process., vol. 29, pp. 2583\u20132597, 2020, doi: 10.1109/TIP.2019.2949392.",
            "year": 2020
        },
        {
            "authors": [
                "Q. Wu",
                "W. Ren",
                "X. Cao"
            ],
            "title": "Learning interleaved cascade of shrinkage fields for joint image dehazing and denoising",
            "venue": "IEEE Trans. Image Process., vol. 29, pp. 1788\u20131801, 2020, doi: 10.1109/TIP.2019.2942504.",
            "year": 1801
        },
        {
            "authors": [
                "B. Cai",
                "X. Xu",
                "K. Jia",
                "C. Qing",
                "andD. Tao"
            ],
            "title": "DehazeNet: An end-to-end system for single image haze removal",
            "venue": "IEEE Trans. Image Process., vol. 25, no. 11, pp. 5187\u20135198, Nov. 2016, doi: 10.1109/TIP.2016.2598681.",
            "year": 2016
        },
        {
            "authors": [
                "B. Li",
                "X. Peng",
                "Z. Wang",
                "J. Xu",
                "D. Feng"
            ],
            "title": "AOD-Net: All-in-one dehazing network",
            "venue": "Proc. IEEE Int. Conf. Comput. Vis. (ICCV), Venice, Italy, Oct. 2017, pp. 4780\u20134788, doi: 10.1109/ICCV.2017.511.",
            "year": 2017
        },
        {
            "authors": [
                "K. Gan",
                "J. Zhao",
                "H. Chen"
            ],
            "title": "Multilevel image dehazing algorithm using conditional generative adversarial networks",
            "venue": "IEEE Access, vol. 8, pp. 55221\u201355229, 2020, doi: 10.1109/ACCESS.2020.2981944.",
            "year": 2020
        },
        {
            "authors": [
                "J. Zhang",
                "D. Tao"
            ],
            "title": "FAMED-Net: A fast and accurate multi-scale end-to-end dehazing network",
            "venue": "IEEE Trans. Image Process., vol. 29, pp. 72\u201384, 2020, doi: 10.1109/TIP.2019.2922837.",
            "year": 2020
        },
        {
            "authors": [
                "C. Li",
                "C. Guo",
                "J. Guo",
                "P. Han",
                "H. Fu",
                "R. Cong"
            ],
            "title": "PDR-Net: Perception-inspired single image dehazing network with refinement",
            "venue": "IEEE Trans. Multimedia, vol. 22, no. 3, pp. 704\u2013716, Mar. 2020, doi: 10.1109/TMM.2019.2933334.",
            "year": 2020
        },
        {
            "authors": [
                "H. Zhang",
                "V. Sindagi",
                "V.M. Patel"
            ],
            "title": "Joint transmission map estimation and dehazing using deep networks",
            "venue": "IEEE Trans. Circuits Syst. Video Technol., vol. 30, no. 7, pp. 1975\u20131986, Jul. 2020, doi: 10.1109/TCSVT.2019.2912145.",
            "year": 1975
        },
        {
            "authors": [
                "H. Dong",
                "J. Pan",
                "L. Xiang",
                "Z. Hu",
                "X. Zhang",
                "F. Wang",
                "M.-H. Yang"
            ],
            "title": "Multi-scale boosted dehazing network with dense feature fusion",
            "venue": "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Seattle, WA, USA, Jun. 2020, pp. 2154\u20132164, doi: 10.1109/CVPR42600. 2020.00223.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Shao",
                "L. Li",
                "W. Ren",
                "C. Gao",
                "N. Sang"
            ],
            "title": "Domain adaptation for image dehazing",
            "venue": "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Seattle, WA, USA, Jun. 2020, pp. 2805\u20132814, doi: 10.1109/CVPR42600.2020.00288.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Pang",
                "J. Nie",
                "J. Xie",
                "J. Han",
                "X. Li"
            ],
            "title": "BidNet: Binocular image dehazing without explicit disparity estimation",
            "venue": "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Seattle, WA, USA, Jun. 2020, pp. 5930\u20135939, doi: 10.1109/CVPR42600.2020.00597.",
            "year": 2020
        },
        {
            "authors": [
                "Z.J. Tian",
                "M.L. Wang",
                "J. Wu",
                "W.F. Gui",
                "W.Q. Wang"
            ],
            "title": "Mine image enhancement algorithm based on dual domain decomposition",
            "venue": "Acta Photonica Sinica, vol. 48, no. 5, pp. 107\u2013119, May 2019, doi: 10.3788/gzxb20194805.0510001.",
            "year": 2019
        },
        {
            "authors": [
                "A.L. Da Cunha",
                "J. Zhou",
                "M.N. Do"
            ],
            "title": "The nonsubsampled contourlet transform: Theory, design, and applications",
            "venue": "IEEE Trans. Image Process., vol. 15, no. 10, pp. 3089\u20133101, Oct. 2006, doi: 10.1109/TIP.2006.877507.",
            "year": 2006
        },
        {
            "authors": [
                "X. Wang",
                "W. Chen",
                "J. Gao",
                "C. Wang"
            ],
            "title": "Hybrid image denoising method based on non-subsampled contourlet transform and bandelet transform",
            "venue": "IET Image Process., vol. 12, no. 5, pp. 778\u2013784, May 2018, doi: 10.1049/iet-ipr.2017.0647.",
            "year": 2018
        },
        {
            "authors": [
                "K. He",
                "J. Sun",
                "X. Tang"
            ],
            "title": "Guided image filtering",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 35, no. 6, pp. 1397\u20131409, Jun. 2013, doi: 10.1109/TPAMI.2012.213. VOLUME 9, 2021 21719 B. Zhang et al.: Image Haze Removal Algorithm Based on Nonsubsampled Contourlet Transform",
            "year": 2013
        },
        {
            "authors": [
                "C.N. Ochotorena",
                "Y. Yamashita"
            ],
            "title": "Anisotropic guided filtering",
            "venue": "IEEE Trans. Image Process., vol. 29, pp. 1397\u20131412, 2020, doi: 10.1109/TIP.2019.2941326.",
            "year": 2020
        },
        {
            "authors": [
                "A. Sengupta",
                "A. Seal",
                "C. Panigrahy",
                "O. Krejcar",
                "A. Yazidi"
            ],
            "title": "Edge information based image fusion metrics using fractional order differentiation and sigmoidal functions",
            "venue": "IEEE Access, vol. 8, pp. 88385\u201388398, 2020, doi: 10.1109/ACCESS.2020.2993607.",
            "year": 2020
        },
        {
            "authors": [
                "N. Hautiere",
                "J. Tarel",
                "D. Aubert",
                "E. Dumont"
            ],
            "title": "Blind contrast enhancement assessment by gradient ratioing at visible edges",
            "venue": "Image Anal. Stereol., vol. 27, no. 2, pp. 87\u201395, Jun. 2008, doi: 10.5566/ias.v27.p87-95.",
            "year": 2008
        },
        {
            "authors": [
                "A. Mittal",
                "R. Soundararajan",
                "A.C. Bovik"
            ],
            "title": "Making a \u2018completely blind\u2019 image quality analyzer",
            "venue": "IEEE Signal Process. Lett., vol. 20, no. 3, pp. 209\u2013212, Mar. 2013, doi: 10.1109/LSP.2012.2227726.",
            "year": 2013
        },
        {
            "authors": [
                "J.-P. Tarel",
                "N. Hautiere",
                "L. Caraffa",
                "A. Cord",
                "H. Halmaoui",
                "D. Gruyer"
            ],
            "title": "Vision enhancement in homogeneous and heterogeneous fog",
            "venue": "IEEE Intell. Transp. Syst. Mag., vol. 4, no. 2, pp. 6\u201320, Apr. 2012, doi: 10.1109/MITS.2012.2189969.",
            "year": 2012
        },
        {
            "authors": [
                "C.O. Ancuti",
                "C. Ancuti",
                "R. Timofte",
                "C. De Vleeschouwer"
            ],
            "title": "O- HAZE: A dehazing benchmark with real hazy and haze-free outdoor images",
            "venue": "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW), Salt Lake City, UT, USA, Jun. 2018, pp. 8678\u20138867, doi: 10.1109/CVPRW.2018.00119.",
            "year": 2018
        },
        {
            "authors": [
                "W. Ren",
                "S. Liu",
                "H. Zhang",
                "J. Pan",
                "X. Cao",
                "M.H. Yang"
            ],
            "title": "Single image dehazing via multi-scale convolutional neural networks",
            "venue": "Proc. Eur. Conf. Comput. Vis., 2016, pp. 154\u2013169, doi: org/10.1007/978-3-319- 46475-6_10.",
            "year": 2016
        },
        {
            "authors": [
                "K. He",
                "J. Sun",
                "X. Tang"
            ],
            "title": "Single image haze removal using dark channel prior",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 33, no. 12, pp. 2341\u20132353, Dec. 2011, doi: 10.1109/TPAMI.2010.168.",
            "year": 2011
        },
        {
            "authors": [
                "A. Galdran"
            ],
            "title": "Image dehazing by artificial multiple-exposure image fusion",
            "venue": "Signal Process., vol. 149, pp. 135\u2013147, Aug. 2018, doi: 10.1016/j.sigpro.2018.03.008.",
            "year": 2018
        },
        {
            "authors": [
                "M.Z. Zhu",
                "B.W. He"
            ],
            "title": "Dehazing via graph cut",
            "venue": "Opt. Eng., vol. 56, no. 11, pp. 1\u201311, 2017, doi: 10.1117/1.OE.56.11.113105.",
            "year": 2017
        },
        {
            "authors": [
                "S. Salazar-Colores",
                "I. Cruz-Aceves",
                "J.M. Ramos-Arreguin"
            ],
            "title": "Single image dehazing using a multilayer perceptron",
            "venue": "J. Electron. Imag., vol. 27, no. 4, pp. 1\u201311, Jul. 2018, doi: 10.1117/1.JEI.27.4.043022.",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "INDEX TERMS Image processing, image restoration, haze removal, nonsubsampled contourlet transform, noise suppression.\nI. INTRODUCTION With the rapid development of the Internet of Things, big data, and cloud computing technologies, video devices are increasingly used in outdoor video surveillance systems. However, hazy weather significantly affects the image quality of the video surveillance systems and the subsequent image processing results, such as image segmentation, target recognition [1], and other tasks. Numerous haze removal algorithms have been proposed to improve the quality of hazeaffected images.\nMost existing dehazing algorithms are based on image enhancement [2], [3], physical models [4]\u2013[6] or machine learning [7], [8]. These algorithms can be divided into two categories, namely those with and without noise suppression.\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Madhu S. Nair .\nDehazing algorithms without noise suppression ability focus on estimating atmospheric light or a transmission map. For example, Ancuti et al. [9] used local atmospheric light to replace the global atmospheric light of traditional dehazing algorithms. Shin et al. [10] integrated radiation and reflection data to optimize the transmission map estimation.\nA fast image dehazing algorithm based on a simple linear transformation was presented by Wang et al. [11], the dehazing algorithm that used the linear transformation was faster than classic methods. Lou et al. improved the maximum reflectance prior (MRP) and used it to remove haze from nighttime images [12].\nThe transmission map in these dehazing algorithms is estimated based on a prior obtained from the image statistics; this topic is a research hotspot. The well-known dark channel prior (DCP) [13] refers to prior knowledge based on the statistics of haze-free images. Since it is a statistical law,\n21708 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ VOLUME 9, 2021\nit is inevitable that there is a deficiency in some specific scenes. The hazy image cannot include the sky area when the DCP is used to estimate a transmission map, and dehazed images by DCP are prone to produce halo effect. Therefore, numerousmethods [14]\u2013[18] that optimize or refine the transmission map estimated by the DCPwere developed. Sahu and Seal [19] presented an image dehazingmethod that calculated a transmissionmap of the non-sky and sky regions of the hazy image using DCP and luminance stretching, respectively. The luminance stretching method could restore hazy images that included the sky, and the dehazed images were clearer than those obtained from the DCP method. In addition to DCP, some scholars have successively proposed transmission map estimation methods [20]\u2013[26] with prior constraints, such as the color ellipsoid prior [20] and the color attenuation prior (CAP) [21].\nWith the development of deep machine learning methods, dehazing algorithms [27]\u2013[31], and [32] based on deep machine learning methods have also been developed.\nDong et al. [33] proposed a multi-scale boosted dehazing networkwith dense feature fusion to preserve spatial information in the U-Net architecture. The results showed that the UNet architecture was suitable for dehazing and provided good dehazing performance. Shao et al. [34] presented a dehazing net with a bidirectional translation network to achieve domain adaptation between the synthetic and real image domains. The dehazing net improved the performance of the dehazing model trained by synthetic hazy images. Shao et al. [34] proposed a binocular image dehazing network (BidNet) for removing haze from stereo images. The BidNet included a stereo transmission map estimation network and an atmospheric light estimation network and achieve dehazing for stereo images based on the physical scattering model.\nNonetheless, the methods of the above-mentioned documents except for [4], [30], and [31], do not consider noise interference in hazy images. The noise in hazy images is often amplified when these algorithms are used to remove haze, leading to the distortion of the dehazed images and the loss of details.\nGao and Hu et al. [4] introduced a noise term into the atmospheric scattering model and analyzed the reasons for the noise amplification caused by traditional dehazing algorithms. The authors proposed a depth-chromaticity regularization method based on the strong correlation between the depth and chromaticity of an image and the transmissionmap. The regularization method aimed to optimize the transmission map and the dehazed image and suppressed noise (noise in the original image and noise resulting from dehazing). In [30] and [31], dehazing algorithms with noise suppression were proposed.\nAlthough the dehazing algorithms in [4], [30], and [31] have noise suppression functions, they focus on reducing the influence of noise on the estimated transmission map and suppressing the noise amplified by the haze removal process. However, these algorithms may still amplify the noise during the dehazing process.\nThus, in this study, a single-image haze removal algorithm based on nonsubsampled contourlet transform (HRNSCT) is proposed to prevent noise diffusion and amplification during haze removal by decoupling the haze removal and noise suppression processes in the contourlet transform domain.\nThe highlights of this paper are as follows. 1. The HRNSCT algorithm decouples the haze removal and noise suppression of hazy images, minimizing the risk of noise amplification during the haze removal process of traditional algorithms.\n2. The transmission map and atmospheric light are estimated accurately by using the low-frequency sub-bands of the hazy image and combining the CAP and the DCP. This approach improves the estimated transmission map and avoids noise interference when estimating the transmission map and atmospheric light.\n3. The HRNSCT algorithm allows for dehazing only the low-frequency components of hazy images and provides a new approach for dehazing hazy images with noise.\n4. The noise in the dehazed image is suppressed by shrinking the high-frequency sub-bands of the original hazy image, providing a newmethod for the noise suppression of dehazing algorithms.\nThe remainder of this paper is organized as follows. TheHRNSCT framework is described in Section II, including image decomposition, scene depth, and atmospheric light estimation, transmission map estimation based on two priors, noise suppression, haze-free low-frequency sub-band recovery, noise suppression, high-frequency detail enhancement, and haze removal. The HRNSCT algorithm is described in Section III. The dehazing results of the HRNSCT are provided and compared with those of other algorithms in Section IV. The parameters of the HRNSCT are analyzed in Section V, and the conclusions are presented in Section VI.\nII. DECOUPLED FRAMEWORK OF HAZE REMOVAL AND NOISE SUPPRESSION As described in [36], hazemainly occurs in the low-frequency part of an image. A noise term n is added to the atmospheric scattering model, and the hazy image and the dehazed image are expressed as high-frequency components and lowfrequency components, respectively. The atmospheric scattering model is defined in Eq. (1):\nI = I l + Ih = J l \u00b7 t + A(1\u2212 t)+ Jh \u00b7 t + n, (1)\nwhere I l = J l \u00b7 t + A(1 \u2212 t), Ih = Jh \u00b7 t + n. I , I l , and Ih, respectively, are the hazy image, the low-frequency component, and the high-frequency component of the hazy image. J , J l , and Jh denote the target (dehazed) image, the lowfrequency component, and the high-frequency component of the dehazed image, respectively. n refers to Gaussian noise with zero mean. A represents the global atmospheric light. t = e\u2212\u03b2d(x,y) is the transmission map, d(x, y) refers to the scene depth, and \u03b2 denotes the scattering coefficient of the atmosphere.\nVOLUME 9, 2021 21709\nEquation (1) indicates that if A and t are known, and the interference of noise n can be eliminated, the dehazed image J can be obtained from the hazy image I . However, only the hazy image I is known in Eq. (1). If I can be decomposed into the low-frequency component I l and the high-frequency component Ih, I l contains almost all the haze and the contour structures of the image, and Ih includes the texture details and noise of the image. Subsequently, the haze is removed from the Il , and the noise is suppressed in the Ih, thereby decoupling the haze removal and noise suppression processes. After the image is decomposed, the haze removal process is converted into the estimation of the atmospheric light A and the transmission map. It is known from Refs. [11], [18] that A and t can be estimated using the CAP or the DCP. Nonetheless, both priors have advantages and disadvantages; thus, the two priors are both used to estimate A and t in this study.\nThe nonsubsampled contourlet transform (NSCT) is used to decompose the hazy image, and the CAP and DCP are combined to estimate the transmission map and atmospheric light. The framework of the HRNSCT is shown in Figure 1, where the red, green, and blue borders represent the r, g, b color channels of the color image.\nA. IMAGE DECOMPOSITION USING NSCT In [37] and [38], the nonsubsampled pyramid (NSP) transform and nonsubsampled directional filter bank (NSDFB) were used in the NSCT for multi-scale and multi-directional image decomposition. First, the image was decomposed into low-frequency and high-frequency components by the NSP.\nThen the high-frequency components were divided into different directional sub-bands in the frequency domain by the NSDFB. The multi-scale decomposition and frequencydomain segmentation are shown in Figure 2. After N iterations, a low-frequency sub-band and \u2211n j=1 2\nj high-frequency sub-bands were obtained. Since neither the NSP nor the NSDFB performs nonsubsampled operations, each sub-band matrix of the NSCT has the same size as the original input image.\nFor the convenience of description, the symbol TNSCT () is used to indicate the NSCT operation. An image I is decomposed by the NSCT using the expression in Eq. (2), where Ccl denotes the low-frequency sub-bands of the three channels of the color image, and Ccl = I c l . C c h,j,s represent the highfrequency directional sub-bands of the three channels, which contain the texture details and almost all noise in the color image. L and Sj refer to the maximum decomposition scale of the NSCT and the number of directions on the given scale, respectively. c denotes the r, g, b channels of the color image.{ Ccl ,C c h,j,s, j = 1, 2, . . . ,L, s = 1, 2, . . . , Sj } denotes a set of all contourlet sub-bands of the image.\nTNSCT (Ic) = { Ccl ,C c h,j,s, j = 1, 2, . . . ,L, s = 1, 2, . . . , Sj } ,\nc \u2208 {r, g, b} (2)"
        },
        {
            "heading": "B. SCENE DEPTH AND ATMOSPHERIC LIGHT ESTIMATION",
            "text": "In [21], the CAP of hazy images was presented. The CAP indicates that the haze density of a hazy image is closely related to the brightness and saturation of the image. The scene depth d(x, y) and the haze density Hd (x, y) of the hazy\n21710 VOLUME 9, 2021\nimage are positively correlated with the difference between the brightness v(x, y) and saturation s(x, y) of the hazy image in the hue, saturation, value (HSV) color space. The deeper the scene depth in the hazy image, the greater the haze density is. In addition, the difference between brightness and saturation increases with increasing haze density. The proportional relationship between the scene depth of the hazy image and the haze density is defined in Eq. (3):\nd(x, y) \u221d H(x, y) \u221d v(x, y)\u2212 s(x, y), (3)\nwhere d(x, y) and H(x, y) denote the scene depth and haze density, respectively. v(x, y) and s(x, y) denote the brightness and saturation, respectively, of the hazy image in the HSV color space, and (x, y) is the pixel coordinate of the hazy image.\nA linear estimation model of the scene depth of a hazy image based on the CAP was developed in [18]:\nd(x, y) = \u03b80 + \u03b81v(x, y)+ \u03b82s(x, y)+ \u03b5(x, y), (4)\nwhere \u03b80, \u03b81, and \u03b82 represent the unknown linear coefficients of the model. \u03b5(x, y) refers to a random error term of the model with a Gaussian distribution with mean 0 and variance \u03c3 2. In [18], themodel (Eq. (4)) is trained using the hazy image set; the parameters of the trained model are \u03b80 = 0.121779, \u03b81 = 0.959710, \u03b82 = \u22120.780245, and \u03c3 = 0.041337. These parameters are used to estimate the scene depth of hazy images in this study.\nEquation (1) indicates that the global atmospheric light A and transmission map t(x, y) have to be known to remove\nhaze from the low-frequency components of the hazy image. According to Eqs. (3) and (4), the greater the v(x, y)\u2212s(x, y), the farther the scene depth distance is, and the brighter the area corresponding to the depth map is. Therefore, all grey values of the depth map are sorted; the pixel positions of the first 0.1% of the gray values of the depth map are marked, and the maximum of these pixel values in the low-frequency component I l is regarded as the atmospheric light A. When the atmospheric light is computed, and the depth map is estimated using Eq. (4), white objects in the image can be mistaken for the atmospheric light. The basic scene depth db(x, y) is replaced by the regional scene depth d r(x, y) when computing A to improve the robustness of the atmospheric light estimation. d r(x, y) is obtained by filtering db(x, y) using a local minimum filter. The local minimum filter operator is defined in Eq. (5):\nd r(x, y) = Fmin (x,y)\u2208wr db(x, y), (5)\nwhere wr denotes a r \u00d7 r neighborhood window centered on (x, y), and Fmin denotes the minimum filter operator.\nWe mark the pixel position of the first 0.1% of the grey values of d r(x, y) and determine the maximum value at the corresponding position of I l as the atmospheric light A.\nC. TRANSMISSION MAP ESTIMATION BASED ON TWO PRIORS Themost common priors used for estimating the transmission map include the CAP and the DCP. Since both have advantages and disadvantages, both priors are used to estimate the transmittance transmission map.\nAccording to the CAP, I l is converted from the RGB color space to the HSV color space. Equation (4) is used to obtain the scene depth estimate db(x, y) of I l . Because the edges and details of the original hazy image I retained by db(x, y) are not fine enough, halos may occur in the edge contour area of the dehazed image. Therefore, db(x, y) is filtered with a guided filter to refine the transmission map.\nIn [39] and [40], it is assumed during the use of the guided filter that the output image Q and the guide image G are locally linear, i.e., in the square window wk centered on k , Q is the linear transformation of G, as defined in Eq. (6):\nQi = akGi + bk , \u2200i \u2208 wk , (6)\nwhere ak and bk are the linear coefficients in the local window wk , whose value can be estimated by the cost function (Eq. (7)):\nIn Eq. (7), \u03bb refers to the balance factor to prevent ak from becoming too large. Il represents the filtered image. According to the linear ridge regression model, ak and bk can be calculated by Eq. (7): ak = 1 |w| \u2211 i\u2208wk GiI li \u2212 \u00b5k I\u0304 lk \u03c3 2k + \u03bb\nbk = I\u0304 lk \u2212 ak\u00b5k ,\n(7)\nVOLUME 9, 2021 21711\nwhere \u00b5k and \u03c3 2k are the mean and variance of the guide image G in the local window wk . |w| denotes the number of pixels in the local window wk , and I\u0304 lk = 1|wk | \u2211 i\u2208wk I li is the mean value of the filtered image in the local window wk . When the scene depth is filtered, the sum image of the low-frequency sub-bands and the high-frequency directional sub-bands corresponding to the first decomposition scale of the hazy image is regarded as the guide image to avoid the influence of noise in the hazy image and include the sufficient details in the guide image. In other words, the calculation result of Eq. (8) is used as the guide image. In Eq. (8), Ccl denotes the low-frequency sub-bands, C c h,1,s denotes the high-frequency directional sub-bands corresponding to the first decomposition scale, and S1 represents the number of decomposition directions corresponding to the first decomposition scale of the hazy image.\nG = Ccl + S1\u2211 s=1 Cch,1,s, c \u2208 {r, g, b}, (8)\nThe final refined scene depth estimate dg(x, y) is obtained by filtering db(x, y); G is used as the guide image of the guided filter. For ease of description, the symbol Fg() is used to represent the guided filter operator, db(x, y) and G denote the image to be filtered and the guide image, respectively. This filtering process is expressed as Eq. (9):\ndg(x, y) = Fg(G, db(x, y)), (9)\nAfter dg(x, y) is obtained, the transmission map t1(x, y) is computed using Eq. (10):\nt1(x, y) = exp(\u2212\u03b2dg(x, y)), (10)\nwhere \u03b2 denotes the attenuation coefficient. The recommended range of \u03b2 is [0.8,2.5]; the greater the value of \u03b2, the smaller the value of the transmission map is.\nFigure 3 shows the estimated basic scene depth db(x, y), regional scene depth d r(x, y), final scene depth dg(x, y), and the transmission map t1(x, y) of the hazy image from left to right.\nSubsequently, the transmission map t2(x, y) is estimated using the DCP to compensate for the defects of the estimate obtained from the CAP. The DCP is an empirical result obtained from the statistics of many haze-free outdoor images. The grey values of one of the three channels of the haze-free images in the RGB space are always close to zero. In other words, theminimumgrey values of the three channels r, g, b tend to zero. Hence, the real scene image without sky\nareas satisfies the dark primary color defined in Eq. (11):\nJdark = min (x,y)\u2208 (x,y) ( min c\u2208{r,g,b} (Jc(x, y))), (11)\nwhere (x, y) represents the local window centered on (x, y), and Jc is one of the color channels of the haze-free image J . In the non-sky and haze-free image, the dark primary color Jdark tends to zero according to the DCP. According to the DCP and the atmospheric scattering model, the transmission map t\u0303(x, y) can be estimated by Eq. (12) when the atmospheric light is known.\nt\u0303(x, y) = 1\u2212 \u03b1 \u00b7 min (x,y)\u2208 (x,y) ( min c\u2208{r,g,b}\n(Icl (x, y))), (12)\nwhere \u03b1 denotes a weight factor that controls the residual haze. The range of \u03b1 is 0 < \u03b1 < 1, and the recommended value is 0.95.\nSimilar to the basic scene depth estimation db(x, y), t\u0303(x, y) also has the disadvantage of insufficient details. Therefore, t\u0303(x, y) is also filtered by a guided filter, and G is used as the guide image to obtain an improved estimate of the transmission map t2(x, y), i.e., t2(x, y) = Fg(G, t\u0303(x, y)). Furthermore, the same guide image G is used for filtering t2(x, y) and dg(x, y) to minimize inconsistencies in the details of the two transmission maps estimated by the two priors.\nThe two transmission maps t1(x, y) and t2(x, y) are fused using Eq. (13) to obtain the final transmission map t(x, y). In Eq. (13), \u00b5 represents the balance factor that determines the contribution of each transmission map to the final transmission map t(x, y). The value range of\u00b5 is the interval [0,1].\nt(x, y) = \u00b5 \u00b7 t1(x, y)+ (1\u2212 \u00b5) \u00b7 t2(x, y), (13)\nD. HAZE-FREE LOW-FREQUENCY SUB-BAND RECOVERY If A and t(x, y) are known, the value range of t(x, y) is limited to [0.05,1.0] to eliminate the very low and very high grey values in the low-frequency sub-bands of the haze-free image. Thus, the low-frequency sub-bands of the haze-free image are obtained by Eq. (14). Let C\u0302 l = I ld , i.e., the dehazed lowfrequency sub-bands are regarded as the low-frequency subbands of J l .\nI ld (x, y) = I l(x, y)\u2212 A\nmin(max(t(x, y), 0.05), 1.0) + A, (14)\nE. NOISE SUPPRESSION AND HIGH-FREQUENCY DETAIL ENHANCEMENT According to Eq. (1), the remaining task after removing haze from I l is to remove noise from Ih and obtain the high-frequency component of Jh. First, the high-frequency sub-bands { Cch,j,s, j = 1, 2, . . . ,L, s = 1, 2, . . . , Sj } need be shrunk to remove or suppress noise in the hazy image. Then, the high-frequency sub-bands after threshold shrinkage are compensated for the difference relative to Ccl according to the transmission map.\nThreshold shrinkage is a classic noise suppression method. When a hard threshold function is used to remove noise,\n21712 VOLUME 9, 2021\nthe denoised image has a relatively high peak signal-to-noise ratio, and the edges are emphasized.\nA hard threshold function is used for noise suppression in the high-frequency sub-bands to reduce the loss of details in the dehazed image. The hard threshold function is defined in Eq. (15):\nC\u0303 c h,j,s =\n{ Cch,j,s, \u2223\u2223\u2223Cch,j,s\u2223\u2223\u2223 > kjTj,s\u03c3 c , 0, otherwise,\nj \u2208 1, 2, . . . ,L, s \u2208 Sj, c \u2208 {r, g, b} (15)\nwhere kj denotes threshold scale coefficient of the jth decomposition scale. Tj,s represents the frequency-domain root mean square of the high-frequency sub-bands, which calculated using Eq. (16). \u03c3 c denotes the noise variance of the channels of the hazy image, and the other symbols have the same meaning as in Eq. (2).\nTj,s =\n\u221a\u221a\u221a\u221a\u221a M\u2211m=1 N\u2211 n=1 |F(CMzj,s )|2\nM \u00d7 N , (16)\nwhere MZ is an M \u00d7 N matrix with the same number of rows and columns as I c(x, y); the values are defined in Eq. (17). CMZj,s denotes the decomposition sub-bands of MZ , which are decomposed by the NSCT. The same parameters (scale and direction filter number) as in the decomposed Ic(x, y) are used. F() and | \u00b7 | denote the two-dimensional Fourier transform (FT) and the absolute value operator, respectively.\nMZ (m, n) = { 1, m = M/2, n = N/2 0, otherwise , (17)\n\u03c3 c is calculated by the empirical formula of the wavelet noise estimation (Eq. (18)), wheremed() refers to the median operator, | \u00b7 | denotes the absolute value operator, and W cd1 denotes the diagonal high-frequency coefficient that is obtained using single-scale \u2018\u2018sym8\u2019\u2019 wavelet decomposition of I c(x, y).\n\u03c3 c = med( \u2223\u2223W cd1\u2223\u2223) 0.6745 , c \u2208 {r, g, b}, (18)\nAfter the shrunk high-frequency sub-bands C\u0303ch,j,s have been obtained, details enhancement is performed using Eq. (19) according to the transmission map t(x, y) to compensate for the attenuation of C\u0303ch,j,s relative to C c l .\nC\u0302 c h,j,s =\nC\u0303 c h,j,s\nmin(max(t(x, y), 0.1), 0.9) , c \u2208 {r, g, b}, (19)\nwhere C\u0302ch,j,s denotes the enhanced high-frequency sub-bands.\nF. RECONSTRUCTION OF THE DEHAZED IMAGE After performing low-frequency haze removal, highfrequency noise suppression, and high-frequency details enhancement, the noise- and haze-suppressed image Id is obtained by applying the nonsubsampled contourlet inverse\ntransformation. For convenience of expression, the symbol T\u22121NSCT denotes the nonsubsampled contourlet inverse transformation; this process is defined in Eq. (20).\nId = T\u22121NSCT ({ C\u0302 c l , C\u0302 c h,j,s, j=1, 2, . . . ,L, s=1, 2, . . . , Sj }) ,\nc \u2208 {r, g, b} (20)\nAfter the dehazed image Id has been reconstructed, a Gamma correction is performed to increase the grey values of low-brightness pixels. The gamma correction transform is defined as:\nId,g = (Id)\u03b3 , (21)\nwhere the range of \u03b3 is 0 < \u03b3 \u2264 1; a value of 0.8 is used in this study.\nIII. IMPLEMENTATION OF THE HRNSCT ALGORITHM The HRNSCT algorithm includes four steps. First, the nonsubsampled contourlet decomposes the hazy image. Second, in the contourlet domain, the CAP and DCP are used to estimate the transmission map and atmospheric light and obtain the haze-free low-frequency sub-bands. The high-frequency sub-bands of the hazy image are shrunk in the contourlet domain for noise suppression, followed by the enhancement of these sub-bands according to the transmission map. Finally, the nonsubsampled contourlet inverse transform is performed to reconstruct the haze-free image in the RGB space, and the Gamma correction is applied to brighten the dark areas.\nThe details of the HRNSCT are defined in Algorithm 1.\nAlgorithm 1 Image Haze Removal AlgorithmBased on Nonsubsampled Contourlet Transform Input: Hazy color image I Initialization: Assign values to all parameters Step 1While c \u2208 {r, g, b}, do\n1. Decompose I c using NSCT Step 2 1. Compute t1(x, y) and t2(x, y) 2. Obtain the dehazed low-frequency image I ld (x, y) Step 3 1. Compute \u03c3 cand \u03c3\u0302j,s\n2. Reduce Cch,j,s 3. Obtain the enhanced high-frequency sub-band C\u0302ch,j,s\nStep 4 1. Reconstruct the dehazed image Id using C\u0302cl , C\u0302 c h,j,s\n2. Correct Id using gamma correction End\nOutput: Dehazed image Id,g\nIV. NUMERICAL EXPERIMENTS An edge quality metric is often used in image fusion [41]. In this study, the ratio of the increased number of visible edges in the dehazed image to visible edges of the original image Rve, the mean visibility levelMvl [42], and the noise variance Nv are used to evaluate the dehazed images of hazy images without or with low-density noise. In contrast, the noise variance, i.e., the natural image quality evaluator (NIQE) [43],\nVOLUME 9, 2021 21713\nand the structural similarity index measure (SSIM) are used to evaluate the dehazed images of hazy images with highdensity noise.\nThe noise variance is calculated using the empirical formula of wavelet noise (Eq. (18)). Rve and Mvl are defined in Eq. (22) and Eq. (23), and the SSIM is defined in Eq. (24).\nRve = nr \u2212 n0 n0 , (22)\nMvl = exp  1 nr \u2211 i\u2208\u2118r log ri  , (23) where nr and n0 denote the number of visible edges in the dehazed image and the original hazy image. ri is the gradient ratio of the dehazed image and the original hazy image. \u2118r represents the number of visible edges in the dehazed image.\nSSIM (x, y) = (2\u00b5x\u00b5y + C1)(2\u03c3xy + C2)\n(\u00b52x + \u00b52y + C1)(\u03c3 2x + \u03c3 2y + C2) , (24)\nwhere x and y are the reference image and comparison image, \u00b5x and \u00b5y denote the mean values of x and y, respectively, \u03c3x and \u03c3y refer to the variances of x and y, respectively, \u03c3xy represents the covariances of x and y, respectively, C1 and C2 are constants to avoid a zero denominator, where C1 = (K1L)2 andC2 = (K2L)2; L is the range of grey values. In this study, L = 255, K1 = 0.01, and K2 = 0.03. All the codes are implemented in Matlab R2014b, and the experiments are conducted on a PC runningWindows 10 with 16 G of RAM and a 2.7 GHz i7 CPU.\nIn the experiment, hazy images were selected from the image sets in [4], [21], FRIDA [44], O-HAZE [45], I-HAZE [45], [46], and [47], and the other hazy imagesare selected from real hazy images.\nThree types of experiments were conducted to verify the performance of low-frequency dehazing, high-frequency noise reduction, and comprehensive performance for dehazing and noise reduction of HRNSCT.\nA. LOW-FREQUENCY DEHAZING In the experiment, the low-frequency dehazing performance is evaluated visually and using objective indicators. The parameters of the HRNSCT are as follows: the scale of the NSCT decomposition image is 3; the number of directional filter banks corresponding to each scale is 2, 4, and 8; \u03bb = 0.015, r = 15, \u03b2 = 2.2, \u00b5 = 0.5, kj = [0 3 3 4], and \u03b3 = 0.6.\nA comparison of the original and dehazed images T1-T3 is shown in Figure 4. The results of the evaluation indicators are shown in Figure 5.\nFigure 4 shows that the HRNSCT significantly reduces the amount of haze in the images. In the dehazed images, the amount of residual haze is greater in the distant areas than nearby. However, most areas are haze-free and show no halos. The results indicate that it is feasible to achieve haze removal in the image by removing haze only in the low-frequency domain.\nAs shown in Fig. 5 (a), the noise variance of the dehazed images T1 and T3 is lower than that of the original images, however, that of dehazed image T2 is slightly higher than that of T2, but that of dehazed image T2 is less 0.9. The result shows that the HRNSCT does not result in significant noise diffusion or amplification during haze removal.\nFigure 5 (b) shows that the Rve of the dehazed images is greater than 1, indicating that the number of visible edges in\n21714 VOLUME 9, 2021\nthe dehazed images is more than twice that of the original hazy images. The visibility indicator Mvl is greater than 1, showing a significant increase in the contrast of the dehazed image.\nThe results in Figures 4 and 5 demonstrate that the HRNSCT algorithm achieves excellent haze removal performance only by removing the haze from the low-frequency subbands.\nB. HIGH-FREQUENCY NOISE SUPPRESSION The objective of this experiment was to verify that the HRNSCT can decouple the haze removal and noise suppression in the contourlet transform domain. The HRNSCT was applied to remove haze of 60 hazy images with different Gaussian noise.\nThe noise variance of the original and dehazed images is shown in Figure 6.\nThe noise variances of the dehazed images and the original hazy images are considerably different. Those of the dehazed images are close to 1. The results indicate that the shrink of the high-frequency directional sub-bands of the original hazy images successfully suppresses the noise in the dehazed images.\nThe original images T61 and T62 with different noise levels and the dehazed images are shown in Figure 7. The\nenlargements of the dehazed images show that more detail is lost, and the edges become less clear as the noise variance increases. The reason is that the noise conceals the details in the original hazy images, making it impossible to restore these details at a high noise variance.\nFigures 4 to 7 indicate that the HRNSCT algorithm achieves the decoupling of the haze removal and noise suppression by removing haze from the low-frequency sub-bands and suppressing noise in the high-frequency sub-bands.\nC. COMPREHENSIVE PERFORMANCE VERIFICATION OF THE HRNSCT The HRNSCT, HRCAP [21], DehazeNet (deep learning method) [27], AMEF [48], DHGC [49], DHMP [50], and MSW [51] are used to dehaze the test images, and the performances of the seven algorithms are compared using visual evaluations and the objective indicators.\nThe dehazing results of the seven algorithms for the lownoise hazy images T69 and T70 and the composite noisefree hazy image T71 are shown in Figure 8. T69 has dense haze; thus, the denominator in the indicator Rve (the number of visible edges n0 in the original hazy image) approaches zero. Therefore, the dehazing results of the 7 algorithms for T69 are only evaluated visually.\nThe results of the evaluation indicators of the dehazed image T71 are listed in Table 1, and those of T72 are listed in Table 2.\nThe results in Fig. 8 show that all seven algorithms provide good dehazing performance for removing light haze or medium haze. However, there are great significant performance differences for removing dense haze.\nAs shown in Fig. 8 (a), HRCAP, DehazeNet, and AMEF provide low performance for removing dense haze. The DHMP results in color distortion, the DHGC causes spots,\nVOLUME 9, 2021 21715\nand the dehazing result of MSW shows striping. The HRNSCT provides the best dehazing performance.\nIn Figure 8(b), the overall brightness of the dehazed images obtained from the HRCAP, DHGC, and MSW is low, and there are several dark areas. The performances of the DehazeNet, AMEF, DHMP, and HRNSCT are similar. However, the enlargement shows that the images obtained from the AMEF and HRNSCT have more detail than those of the other algorithms. Therefore, the AMEF and HRNSCT provide the best dehazing performances for T70.\nA shown in Figure 8(c), the DHGC provides the best dehazing performance for T71; however, the results obtained from the HRNSCT and the other five algorithms are almost indistinguishable visually.\nIn Tables 1 and 2, \u2018Original Nv\u2032 denotes the noise variance of the original hazy images. Nv,Rve, andMvl denote the eval-\nuation indicators of the dehazed images. The bold underlined numbers represent the optimal values. The\u2193 symbol indicates that the smaller the value, the better the result is, and vice versa for the \u2191 symbol. Table 1 shows that the Nv value of the HRNSCT algorithm is optimal, indicating that the noise level of the dehazed image obtained from the HRNSCT was the lowest. HRNSCT achieved the optimal results for all indicators for a comparison with HRCAP, DehazeNet, DHMP, and MSW. Comparing with AMEF and DHGC, the Nv and Rve values of the HRNSCT were better than those of the AMEF, and the Nv andMvl values of the HRNSCT were better than those of the DHGC. Therefore, the dehazing performance of HRNSCT is better than that of the other six algorithms for T70.\nAs shown in Table 2, the Nv values of the HRCAP, DehazeNet, and HRNSCT algorithms were optimal and identical. The Rve andMvl values of the HRNSCT algorithm were better than those of the HRCAP, DehazeNet, AMEF, and DHMP. The Nv and Mvl values of the HRNSCT algorithm were better than those of the DHGC and MSW. Therefore, comparing the other six algorithms, the dehazing performance of HRNSCT is better than that of the other six algorithms for T71.\nThe results in Figure 8 and Tables 1 and 2 demonstrate that the proposed HRNSCT algorithm has similar dehazing performance to the DHGC and MSW algorithms for haze removal from images with low or no noise.\nSubsequently, to further verify the performances of HRNSCT and the other algorithms for removing haze from noisy images, the seven algorithms were used to remove haze from hazy images T72, T73, and T90 with Gaussian noise. The dehazing results of the seven algorithms are shown in Figure 9.\nIn Figure 9, comparing with the original hazy images, dehazing effects of all dehazing images of T72, T73 and T90 are obvious. However, the enlargements show that the HRCAP, DehazeNet, AMEF, DHGC, and DHMP result in noise amplification during haze removal from noisy images.\nTheMSW and HRNSCT algorithms provide the best noise suppression performance. However, the MSW algorithm produces spots in the dehazed images. Hence, the visual comparison of the dehazed images of the 7 algorithms indicates that the HRNSCT algorithm provides the best performance for removing haze from noisy images.\nBecause the Rve and Mvl indicators are sensitive to noise, the noise variance Nv, the SSIM and NIQE are used to evaluate the dehazing image. The evaluation indicators of the dehazing images T72, T73, and T90 are shown in Figure 10.\nIn Figure 10(a), the smaller the Nv value, the lower the noise level of the dehazed image is. The Nv values of the dehazed images obtained from the HRCAP, DehazeNet, AMEF, DHGC, and DHMP algorithms were larger than those of the original images, indicating that these five algorithms increase the noise variance. In contrast, the Nv values of the dehazed images obtained from the MSW and HRNSCT algorithms were reduced. The Nv value was lowest for the\n21716 VOLUME 9, 2021\nHRNSCT. These results indicate that the HRNSCT algorithm provides the best noise suppression performance, followed by the MSW.\nIn Figure 10(b), the larger the SSIM value, the higher the similarity is between the original image and the dehazed image. The SSIM values are lower for the dehazed images than the original images for the six algorithms, whereas those of the HRNSCT are higher in the dehazed images. This finding indicates that the HRNSCT algorithm causes the least image distortion among the seven dehazing algorithms.\nIn Figure 10(c), the smaller the NIQE value, the better the image quality is. The NIQE values are higher for the dehazed images than the original images for the five algorithms, whereas those of the MSW and HRNSCT are lower. This result shows that the MSW and HRNSCT algorithms increase the image quality after dehazing, whereas the other five algorithms decrease the image quality during the haze removal from noisy images. The HRNSCT provides the low-\nest NIQE values, demonstrating that it provides the best haze removal performance for the T71, T73, and T90 hazy images with noise.\nThe results presented in Figures 9 and 10 show the superiority of the HRNSCT over the other algorithms for removing haze from noisy images.\nThe computational complexity of the HRNSCT depends primarily on the NSP and NSDFB of the NSCT. The NSDFB comprises a larger proportion of the NSCT operation than the NSP. Therefore, the HRNSCT has the same computational complexity as the NSDFB and a computational complexity of O(n2) for the NSDFB for an image of size n \u00d7 n. Hence, the computational complexity of the HRNSCT is O(n2) for each channel of the color image.\nThe average runtimes of the seven algorithms for dehazing 60 hazy images with an average size of 512\u00d7 480 are shown in Figure 11.\nVOLUME 9, 2021 21717\nThe order of the average runtime of the 7 algorithms is MSW < HRCAP < DHGC < AMEF < DHMP < DehazeNet < HRNSCT. The HRNSCT has the longest average runtime because it is implemented in the Matlab environment. The HRNSCT algorithm can be improved by increasing its processing speed using program optimization or GPU hardware acceleration in practical applications.\nV. PARAMETER ANALYSIS AND DISCUSSION The proposed HRNSCT algorithm provides the best dehazing and noise reduction performances in comparison with comparable algorithms. We analyze the role of the main parameters of the HRNSCT algorithm in the following section. The parameter \u03b1 plays a role in preserving the haze in the DCP estimation of the transmission map. The balance factor \u00b5 controls the contribution of the CAP and DCP to the final transmission map. The parameter \u03b3 makes the role of improvement low grey value pixels and compression high grey value pixels. Threshold proportional coefficient kj plays a role in setting the threshold. The larger the kj, the stronger the noise suppression effect. Nonetheless, if kj is large, which will lead to the loss of weak details. The recommended range of kj is [4,6] when the noise variance of the hazy image is less than 2, other recommended ranges are [2,4]. The details of these parameters are not repeated here. The following focuses on analyzing the roles of the balance factor \u00b5 and the atmospheric attenuation coefficient \u03b2. Figure 12 shows the results of using different \u00b5 values for haze removal from image T91 (from the O-HAZE dataset). The dehazed image becomes brighter as the \u00b5 value increases, but the residual haze increases, indicating that the\nsmaller the value of\u00b5, the stronger the impact of the DCP and the weaker the impact of the CAP in the transmission map.\nThe evaluation indicators of images dehazed with different \u00b5 values are provided in Figure 13.\nAs the \u00b5 value increases, Nv decreases, and the rate of increase decreases, with few changes after reaching a \u00b5value of 0.5. The NIQE shows a similar trend, exhibiting few changes after reaching a \u00b5 value of 0.8. The SSIM value increases with an increase in the \u00b5 value and stabilizes after reaching a \u00b5 value of 0.5.\nThe visual evaluation and the evaluation indicators suggest that the optimal \u00b5 value range is [05,0.8].\nFigure 14 shows the haze removal results of image T74 for \u00b5 = 0.5 and different \u03b2 values. As the \u03b2 value increases, the dehazing performance of the HRNSCT and the clarify of the images increases. However, a very large \u03b2 value will darken the images. Therefore, the visual quality of the dehazed image should be considered comprehensively when the parameter values are changed. The experiment results show that the suitable range of the \u03b2 value is [0.8,2.2].\nVI. CONCLUSION TheHRNSCT algorithmwas proposed for haze removal from noisy images. Based on theoretical analyses and numerical experiments, the following conclusions can be drawn.\nThe HRNSCT algorithm has excellent noise- and hazesuppression performance, only by dehazing the lowfrequency sub-bands and shrinking the high-frequency subbands of the original image. The HRNSCT algorithm decouples the haze removal and noise suppression processes in the contourlet transform domain. This approach prevents the noise amplification caused by traditional dehazing algorithms during the haze removal process. Since most hazy images include noise, the HRNSCT algorithm is well suited for dehazing noisy images.\n21718 VOLUME 9, 2021\nAlthough the HRNSCT algorithm provided excellent results for simultaneous haze removal and noise suppression, it currently has a relatively long runtime. Thus, it is necessary to optimize and accelerate the HRNSCT algorithm in the future. Alternatively, similar algorithms based on deep learning should be developed and designed along with the idea of decoupling haze removal and noise suppression of the HRNSCT algorithm.\nREFERENCES [1] M. A. Khan, M. I. U. Lali, M. Sharif, K. Javed, K. Aurangzeb, S. I. Haider,\nA. S. Altamrah, and T. Akram, \u2018\u2018An optimized method for segmentation and classification of apple diseases based on strong correlation and genetic algorithm based feature selection,\u2019\u2019 IEEE Access, vol. 7, pp. 46261\u201346277, 2019, doi: 10.1109/ACCESS.2019.2908040. [2] C. Zhu and Y. Peng, \u2018\u2018A boosted multi-task model for pedestrian detection with occlusion handling,\u2019\u2019 IEEE Trans. Image Process., vol. 24, no. 12, pp. 5619\u20135629, Dec. 2015, doi: 10.1109/TIP.2015.2483376. [3] J. Zhou, D. Zhang, P. Zou, W. Zhang, and W. Zhang, \u2018\u2018Retinex-based Laplacian pyramid method for image defogging,\u2019\u2019 IEEE Access, vol. 7, pp. 122459\u2013122472, 2019, doi: 10.1109/ACCESS.2019.2934981. [4] Y. Gao and H. Hu, \u2018\u2018Foggy image enhancement based on multi-block coordinated single-scale retinex,\u2019\u2019 J. BeijingUniv. Aeronaut. Astronaut., vol. 45, no. 5, pp. 944\u2013951, 2019, doi: 10.13700/j.bh.1001-5965.2018.0528. [5] M. Zheng, G. Qi, Z. Zhu, Y. Li, H. Wei, and Y. Liu, \u2018\u2018Image dehazing by an artificial image fusion method based on adaptive structure decomposition,\u2019\u2019 IEEE Sensors J., vol. 20, no. 14, pp. 8062\u20138072, Jul. 2020, doi: 10.1109/JSEN.2020.2981719. [6] R. Fattal, \u2018\u2018Dehazing using color-lines,\u2019\u2019ACMTrans. Graph., vol. 34, no. 1, pp. 13:1\u201313:14, Nov. 2014, doi: 10.1145/2651362. [7] C.-H. Yeh, C.-H. Huang, and L.-W. Kang, \u2018\u2018Multi-scale deep residual learning-based single image haze removal via image decomposition,\u2019\u2019 IEEE Trans. Image Process., vol. 29, pp. 3153\u20133167, 2020, doi: 10.1109/TIP.2019.2957929. [8] K. Yuan, J. Wei, W. Lu, and N. Xiong, \u2018\u2018Single image dehazing via NIN-DehazeNet,\u2019\u2019 IEEE Access, vol. 7, pp. 181348\u2013181356, 2019, doi: 10.1109/ACCESS.2019.2958607. [9] C. Ancuti, C. O. Ancuti, C. De Vleeschouwer, and A. C. Bovik, \u2018\u2018Day and night-time dehazing by local airlight estimation,\u2019\u2019 IEEE Trans. Image Process., vol. 29, pp. 6264\u20136275, 2020, doi: 10.1109/TIP.2020.2988203. [10] J. Shin, M. Kim, J. Paik, and S. Lee, \u2018\u2018Radiance\u2013reflectance combined optimization and structure-guided L0-norm for single image dehazing,\u2019\u2019 IEEE Trans. Multimedia, vol. 22, no. 1, pp. 30\u201344, Jan. 2020, doi: 10.1109/TMM.2019.2922127. [11] W. Wang, X. Yuan, X. Wu, and Y. Liu, \u2018\u2018Fast image dehazing method based on linear transformation,\u2019\u2019 IEEE Trans. Multimedia, vol. 19, no. 6, pp. 1142\u20131155, Jun. 2017, doi: 10.1109/TMM.2017.2652069. [12] W. Lou, Y. Li, G. Yang, C. Chen, H. Yang, and T. Yu, \u2018\u2018Integrating haze density features for fast nighttime image dehazing,\u2019\u2019 IEEE Access, vol. 8, pp. 113318\u2013113330, 2020, doi: 10.1109/ACCESS.2020.3003444. [13] K. He, J. Sun, andX. Tang, \u2018\u2018Single image haze removal using dark channel prior,\u2019\u2019 in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., Miami, FL, USA, Jun. 2009, pp. 1956\u20131963. [14] Z. Wang, G. Hou, Z. Pan, and G. Wang, \u2018\u2018Single image dehazing and denoising combining dark channel prior and variational models,\u2019\u2019 IET Comput. Vis., vol. 12, no. 4, pp. 393\u2013402, Jun. 2018, doi: 10.1049/ietcvi.2017.0318. [15] K. Borkar and S. Mukherjee, \u2018\u2018Single image dehazing by approximating and eliminating the additional airlight component,\u2019\u2019 Neurocomputing, vol. 400, pp. 294\u2013308, Aug. 2020, doi: 10.1016/j.neucom.2020.03.027. [16] T. Yu, K. Song, P. Miao, G. Yang, H. Yang, and C. Chen, \u2018\u2018Nighttime single image dehazing via pixel-wise alpha blending,\u2019\u2019 IEEE Access, vol. 7, pp. 114619\u2013114630, 2019, doi: 10.1109/ACCESS.2019.2936049. [17] S. E. Kim, T. H. Park, and I. K. Eom, \u2018\u2018Fast single image dehazing using saturation based transmission map estimation,\u2019\u2019 IEEE Trans. Image Process., vol. 29, pp. 1985\u20131998, 2020, doi: 10.1109/TIP.2019.2948279. [18] H.-M. Hu, H. Zhang, Z. Zhao, B. Li, and J. Zheng, \u2018\u2018Adaptive single image dehazing using joint local-global illumination adjustment,\u2019\u2019 IEEE Trans. Multimedia, vol. 22, no. 6, pp. 1485\u20131495, Jun. 2020, doi: 10.1109/TMM.2019.2944260.\n[19] G. Sahu and A. Seal, \u2018\u2018Image dehazing based on luminance stretching,\u2019\u2019 in Proc. Int. Conf. Inf. Technol. (ICIT), Bhubaneswar, India, Dec. 2019, pp. 388\u2013393, doi: 10.1109/ICIT48102.2019.00075. [20] T.M. Bui andW.Kim, \u2018\u2018Single image dehazing using color ellipsoid prior,\u2019\u2019 IEEE Trans. Image Process., vol. 27, no. 2, pp. 999\u20131009, Feb. 2018, doi: 10.1109/TIP.2017.2771158. [21] Q. Zhu, J. Mai, and L. Shao, \u2018\u2018A fast single image haze removal algorithm using color attenuation prior,\u2019\u2019 IEEE Trans. Image Process., vol. 24, no. 11, pp. 3522\u20133533, Nov. 2015, doi: 10.1109/TIP.2015.2446191. [22] D. Berman, T. Treibitz, and S. Avidan, \u2018\u2018Non-local image dehazing,\u2019\u2019 in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Las Vegas, NV, USA, Jun. 2016, pp. 1674\u20131682, doi: 10.1109/CVPR.2016.185. [23] Y. Liu, J. Shang, L. Pan, A. Wang, and M. Wang, \u2018\u2018A unified variational model for single image dehazing,\u2019\u2019 IEEE Access, vol. 7, pp. 15722\u201315736, 2019, doi: 10.1109/ACCESS.2019.2894525. [24] Q. Liu, X. Gao, L. He, and W. Lu, \u2018\u2018Single image dehazing with depth-aware non-local total variation regularization,\u2019\u2019 IEEE Trans. Image Process., vol. 27, no. 10, pp. 5178\u20135191, Oct. 2018, doi: 10.1109/TIP. 2018.2849928. [25] Q. Wu, J. Zhang, W. Ren, W. Zuo, and X. Cao, \u2018\u2018Accurate transmission estimation for removing haze and noise from a single image,\u2019\u2019 IEEE Trans. Image Process., vol. 29, pp. 2583\u20132597, 2020, doi: 10.1109/TIP.2019.2949392. [26] Q. Wu, W. Ren, and X. Cao, \u2018\u2018Learning interleaved cascade of shrinkage fields for joint image dehazing and denoising,\u2019\u2019 IEEE Trans. Image Process., vol. 29, pp. 1788\u20131801, 2020, doi: 10.1109/TIP.2019.2942504. [27] B. Cai, X. Xu, K. Jia, C. Qing, andD. Tao, \u2018\u2018DehazeNet: An end-to-end system for single image haze removal,\u2019\u2019 IEEE Trans. Image Process., vol. 25, no. 11, pp. 5187\u20135198, Nov. 2016, doi: 10.1109/TIP.2016.2598681. [28] B. Li, X. Peng, Z. Wang, J. Xu, and D. Feng, \u2018\u2018AOD-Net: All-in-one dehazing network,\u2019\u2019 in Proc. IEEE Int. Conf. Comput. Vis. (ICCV), Venice, Italy, Oct. 2017, pp. 4780\u20134788, doi: 10.1109/ICCV.2017.511. [29] K. Gan, J. Zhao, and H. Chen, \u2018\u2018Multilevel image dehazing algorithm using conditional generative adversarial networks,\u2019\u2019 IEEE Access, vol. 8, pp. 55221\u201355229, 2020, doi: 10.1109/ACCESS.2020.2981944. [30] J. Zhang and D. Tao, \u2018\u2018FAMED-Net: A fast and accurate multi-scale end-to-end dehazing network,\u2019\u2019 IEEE Trans. Image Process., vol. 29, pp. 72\u201384, 2020, doi: 10.1109/TIP.2019.2922837. [31] C. Li, C. Guo, J. Guo, P. Han, H. Fu, and R. Cong, \u2018\u2018PDR-Net: Perception-inspired single image dehazing network with refinement,\u2019\u2019 IEEE Trans. Multimedia, vol. 22, no. 3, pp. 704\u2013716, Mar. 2020, doi: 10.1109/TMM.2019.2933334. [32] H. Zhang, V. Sindagi, and V. M. Patel, \u2018\u2018Joint transmission map estimation and dehazing using deep networks,\u2019\u2019 IEEE Trans. Circuits Syst. Video Technol., vol. 30, no. 7, pp. 1975\u20131986, Jul. 2020, doi: 10.1109/TCSVT.2019.2912145. [33] H. Dong, J. Pan, L. Xiang, Z. Hu, X. Zhang, F. Wang, and M.-H. Yang, \u2018\u2018Multi-scale boosted dehazing network with dense feature fusion,\u2019\u2019 in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Seattle, WA, USA, Jun. 2020, pp. 2154\u20132164, doi: 10.1109/CVPR42600. 2020.00223. [34] Y. Shao, L. Li, W. Ren, C. Gao, and N. Sang, \u2018\u2018Domain adaptation for image dehazing,\u2019\u2019 in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Seattle, WA, USA, Jun. 2020, pp. 2805\u20132814, doi: 10.1109/CVPR42600.2020.00288. [35] Y. Pang, J. Nie, J. Xie, J. Han, and X. Li, \u2018\u2018BidNet: Binocular image dehazing without explicit disparity estimation,\u2019\u2019 in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Seattle, WA, USA, Jun. 2020, pp. 5930\u20135939, doi: 10.1109/CVPR42600.2020.00597. [36] Z. J. Tian, M. L. Wang, J. Wu, W. F. Gui, and W. Q. Wang, \u2018\u2018Mine image enhancement algorithm based on dual domain decomposition,\u2019\u2019 Acta Photonica Sinica, vol. 48, no. 5, pp. 107\u2013119, May 2019, doi: 10.3788/gzxb20194805.0510001. [37] A. L. Da Cunha, J. Zhou, and M. N. Do, \u2018\u2018The nonsubsampled contourlet transform: Theory, design, and applications,\u2019\u2019 IEEE Trans. Image Process., vol. 15, no. 10, pp. 3089\u20133101, Oct. 2006, doi: 10.1109/TIP.2006.877507. [38] X. Wang, W. Chen, J. Gao, and C. Wang, \u2018\u2018Hybrid image denoising method based on non-subsampled contourlet transform and bandelet transform,\u2019\u2019 IET Image Process., vol. 12, no. 5, pp. 778\u2013784, May 2018, doi: 10.1049/iet-ipr.2017.0647. [39] K. He, J. Sun, and X. Tang, \u2018\u2018Guided image filtering,\u2019\u2019 IEEE Trans. Pattern Anal. Mach. Intell., vol. 35, no. 6, pp. 1397\u20131409, Jun. 2013, doi: 10.1109/TPAMI.2012.213.\nVOLUME 9, 2021 21719\n[40] C. N. Ochotorena and Y. Yamashita, \u2018\u2018Anisotropic guided filtering,\u2019\u2019 IEEE Trans. Image Process., vol. 29, pp. 1397\u20131412, 2020, doi: 10.1109/TIP.2019.2941326. [41] A. Sengupta, A. Seal, C. Panigrahy, O. Krejcar, and A. Yazidi, \u2018\u2018Edge information based image fusion metrics using fractional order differentiation and sigmoidal functions,\u2019\u2019 IEEE Access, vol. 8, pp. 88385\u201388398, 2020, doi: 10.1109/ACCESS.2020.2993607. [42] N. Hautiere, J. Tarel, D. Aubert, and E. Dumont, \u2018\u2018Blind contrast enhancement assessment by gradient ratioing at visible edges,\u2019\u2019 Image Anal. Stereol., vol. 27, no. 2, pp. 87\u201395, Jun. 2008, doi: 10.5566/ias.v27.p87-95. [43] A. Mittal, R. Soundararajan, and A. C. Bovik, \u2018\u2018Making a \u2018completely blind\u2019 image quality analyzer,\u2019\u2019 IEEE Signal Process. Lett., vol. 20, no. 3, pp. 209\u2013212, Mar. 2013, doi: 10.1109/LSP.2012.2227726. [44] J.-P. Tarel, N. Hautiere, L. Caraffa, A. Cord, H. Halmaoui, and D. Gruyer, \u2018\u2018Vision enhancement in homogeneous and heterogeneous fog,\u2019\u2019 IEEE Intell. Transp. Syst. Mag., vol. 4, no. 2, pp. 6\u201320, Apr. 2012, doi: 10.1109/MITS.2012.2189969. [45] C. O. Ancuti, C. Ancuti, R. Timofte, and C. De Vleeschouwer, \u2018\u2018OHAZE: A dehazing benchmark with real hazy and haze-free outdoor images,\u2019\u2019 in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW), Salt Lake City, UT, USA, Jun. 2018, pp. 8678\u20138867, doi: 10.1109/CVPRW.2018.00119. [46] W. Ren, S. Liu, H. Zhang, J. Pan, X. Cao, and M. H. Yang, \u2018\u2018Single image dehazing via multi-scale convolutional neural networks,\u2019\u2019 in Proc. Eur. Conf. Comput. Vis., 2016, pp. 154\u2013169, doi: org/10.1007/978-3-31946475-6_10. [47] K. He, J. Sun, and X. Tang, \u2018\u2018Single image haze removal using dark channel prior,\u2019\u2019 IEEE Trans. Pattern Anal. Mach. Intell., vol. 33, no. 12, pp. 2341\u20132353, Dec. 2011, doi: 10.1109/TPAMI.2010.168. [48] A. Galdran, \u2018\u2018Image dehazing by artificial multiple-exposure image fusion,\u2019\u2019 Signal Process., vol. 149, pp. 135\u2013147, Aug. 2018, doi: 10.1016/j.sigpro.2018.03.008. [49] M. Z. Zhu and B. W. He, \u2018\u2018Dehazing via graph cut,\u2019\u2019 Opt. Eng., vol. 56, no. 11, pp. 1\u201311, 2017, doi: 10.1117/1.OE.56.11.113105. [50] S. Salazar-Colores, I. Cruz-Aceves, and J. M. Ramos-Arreguin, \u2018\u2018Single image dehazing using a multilayer perceptron,\u2019\u2019 J. Electron. Imag., vol. 27, no. 4, pp. 1\u201311, Jul. 2018, doi: 10.1117/1.JEI.27.4.043022. [51] X. Liu, H. Zhang, Y.-M. Cheung, X. You, and Y. Y. Tang, \u2018\u2018Efficient single image dehazing and denoising: An efficient multi-scale correlated wavelet approach,\u2019\u2019 Comput. Vis. Image Understand., vol. 162, pp. 23\u201333, Sep. 2017, doi: 10.1016/j.cviu.2017.08.002.\nBOWEN ZHANG was born in Jiaozuo, China. He is currently pursuing the bachelor\u2019s degree with the School of Computer Science and Engineering, Nanjing University of Science and Technology. He has published a number of articles in image processing journals. His research interests include image dehazing and image enhancement.\nMANLI WANG was born in Jiaozuo, China, in 1981. He received the B.S. degree in electrical engineering and automation, the M.S. degree in control theory and control engineering fromHenan Polytechnic University, and the Ph.D. degree from the School of Mechanical Electronic and Information Engineering, China University of Mining and Technology, Beijing, China, in 2020. He has published a number of articles in image processing journals. His research interests include image\ndenoising and image enhancement.\nXIAOBO SHEN received the B.Sc. and Ph.D. degrees from the School of Computer Science and Engineering, Nanjing University of Science and Technology, in 2011 and 2017, respectively. He is currently a Full Professor with the School of Computer Science and Engineering, Nanjing University of Science and Technology, China. He has authored over 30 technical papers in prominent journals and conferences, such as IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING\nSYSTEMS, IEEE TRANSACTIONS ON IMAGE PROCESSING, IEEE TRANSACTIONS ON CYBERNETICS, NIPS, ACM MM, AAAI, and IJCAI. His primary research interests include multi-view learning, multi-label learning, network embedding, and hashing.\n21720 VOLUME 9, 2021"
        }
    ],
    "title": "Image Haze Removal Algorithm Based on Nonsubsampled Contourlet Transform",
    "year": 2021
}