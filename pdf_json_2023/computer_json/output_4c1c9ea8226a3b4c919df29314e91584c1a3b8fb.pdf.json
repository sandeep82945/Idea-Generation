{
    "abstractText": "Object tracking based on the fusion of visible and thermal images, known as RGB-T tracking, has gained increasing attention from researchers in recent years. How to achieve a more comprehensive fusion of information from the two modalities with fewer computational costs has been a problem that researchers have been exploring. Recently, with the rise of prompt learning in computer vision, we can better transfer knowledge from visual large models to downstream tasks. Considering the strong complementarity between visible and thermal modalities, we propose a tracking architecture based on mutual prompt learning between the two modalities. We also design a lightweight prompter that incorporates attention mechanisms in two dimensions to transfer information from one modality to the other with lower computational costs, embedding it into each layer of the backbone. Extensive experiments have demonstrated that our proposed tracking architecture is effective and efficient, achieving state-of-the-art performance while maintaining high running speeds. (The code is available at https://github.com/HusterYoung/MPLT).",
    "authors": [
        {
            "affiliations": [],
            "name": "Yang Luo"
        },
        {
            "affiliations": [],
            "name": "Xiqing Guo"
        },
        {
            "affiliations": [],
            "name": "Hui Feng"
        },
        {
            "affiliations": [],
            "name": "Lei Ao"
        }
    ],
    "id": "SP:f2e9e986b042ac4c2453d300367b45cb65e7b540",
    "references": [
        {
            "authors": [
                "H. Dai",
                "C. Ma",
                "Z. Liu",
                "Y. Li",
                "P. Shu",
                "X. Wei",
                "L. Zhao",
                "Z. Wu",
                "D. Zhu",
                "W. Liu"
            ],
            "title": "SAMAug: Point Prompt Augmentation for Segment Anything Model.",
            "venue": "ArXiv Preprint ArXiv:2307.01187",
            "year": 2023
        },
        {
            "authors": [
                "Y. Gao",
                "C. Li",
                "Y. Zhu",
                "J. Tang",
                "T. He",
                "F. Wang"
            ],
            "title": "Deep Adaptive Fusion Network for High Performance RGBT Tracking.",
            "venue": "IEEE/CVF International Conference on Computer Vision Workshop (ICCVW),",
            "year": 2019
        },
        {
            "authors": [
                "C. Guo",
                "D. Yang",
                "C. Li",
                "P. Song"
            ],
            "title": "Dual Siamese Network for RGBT Tracking via Fusing Predicted Position Maps.",
            "venue": "The Visual Computer",
            "year": 2022
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep Residual Learning for Image Recognition.",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2016
        },
        {
            "authors": [
                "T. Hui",
                "Z. Xun",
                "F. Peng",
                "J. Huang",
                "X. Wei",
                "J. Dai",
                "J. Han",
                "S. Liu"
            ],
            "title": "Bridging Search Region Interaction With Template for RGB-T Tracking.",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "M. Jia",
                "L. Tang",
                "B-C. Chen",
                "C. Cardie",
                "S. Belongie",
                "B. Hariharan",
                "S-N. Lim"
            ],
            "title": "Visual Prompt Tuning.",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "C. Li",
                "X. Liang",
                "Y. Lu",
                "N. Zhao",
                "J. Tang"
            ],
            "title": "RGB-T Object Tracking:Benchmark and Baseline.",
            "venue": "Pattern Recognition",
            "year": 2019
        },
        {
            "authors": [
                "C. Li",
                "L. Liu",
                "A. Lu",
                "Q. Ji",
                "J. Tang"
            ],
            "title": "Challenge-Aware RGBT Tracking.",
            "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "C. Li",
                "W. Xue",
                "Y. Jia",
                "Z. Qu",
                "B. Luo",
                "J. Tang",
                "D. Sun"
            ],
            "title": "LasHeR: A Large-Scale High-Diversity Benchmark for RGBT Tracking.",
            "venue": "IEEE Transactions on Image Processing",
            "year": 2022
        },
        {
            "authors": [
                "C. Li",
                "N. Zhao",
                "Y. Lu",
                "C. Zhu",
                "J. Tang"
            ],
            "title": "Weighted Sparse Representation Regularized Graph Learning for RGB-T Object Tracking.",
            "venue": "In Proceedings of the 25th ACM International Conference on Multimedia,",
            "year": 2017
        },
        {
            "authors": [
                "C. Long Li",
                "A. Lu",
                "A. Hua Zheng",
                "Z. Tu",
                "J. Tang"
            ],
            "title": "Multi-Adapter RGBT Tracking.",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops,",
            "year": 2019
        },
        {
            "authors": [
                "I. Loshchilov",
                "F. Hutter"
            ],
            "title": "Decoupled Weight Decay Regularization.",
            "venue": "ArXiv Preprint ArXiv:1711.05101",
            "year": 2017
        },
        {
            "authors": [
                "A. Lu",
                "C. Li",
                "Y. Yan",
                "J. Tang",
                "B. Luo"
            ],
            "title": "RGBT Tracking via Multi-Adapter Network with Hierarchical Divergence Loss.",
            "venue": "IEEE Transactions on Image Processing",
            "year": 2021
        },
        {
            "authors": [
                "A. Lu",
                "C. Qian",
                "C. Li",
                "J. Tang",
                "L. Wang"
            ],
            "title": "DualityGated Mutual Condition Network for RGBT Tracking.",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems",
            "year": 2022
        },
        {
            "authors": [
                "Y. Luo",
                "M. Dong",
                "X. Guo",
                "J. Yu"
            ],
            "title": "RGB-T Tracking Based on Mixed Attention.",
            "venue": "ArXiv Preprint ArXiv:2304.04264",
            "year": 2023
        },
        {
            "authors": [
                "H. Nam",
                "B. Han"
            ],
            "title": "Learning Multi-Domain Convolutional Neural Networks for Visual Tracking.",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2016
        },
        {
            "authors": [
                "A. Paszke",
                "S. Gross",
                "F. Massa",
                "A. Lerer",
                "J. Bradbury",
                "G. Chanan",
                "T. Killeen",
                "Z. Lin",
                "N. Gimelshein",
                "L. Antiga"
            ],
            "title": "Pytorch: An Imperative Style, High-Performance Deep Learning Library.",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2019
        },
        {
            "authors": [
                "K. Simonyan",
                "A. Zisserman"
            ],
            "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition.",
            "venue": "ArXiv Preprint ArXiv:1409.1556",
            "year": 2014
        },
        {
            "authors": [
                "Z. Tang",
                "T. Xu",
                "X-J. Wu"
            ],
            "title": "A Survey for Deep RGBT Tracking.",
            "venue": "arXiv. http://arxiv.org/abs/2201.09296",
            "year": 2022
        },
        {
            "authors": [
                "S. Woo",
                "J. Park",
                "J-Y. Lee",
                "IS. Kweon"
            ],
            "title": "Cbam: Convolutional Block Attention Module.",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Y. Xiao",
                "M. Yang",
                "C. Li",
                "L. Liu",
                "J. Tang"
            ],
            "title": "AttributeBased Progressive Fusion Network for RGBT Tracking.",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
            "year": 2022
        },
        {
            "authors": [
                "T. Yang",
                "Y. Zhu",
                "Y. Xie",
                "A. Zhang",
                "C. Chen",
                "M. Li"
            ],
            "title": "Aim: Adapting Image Models for Efficient Video Action Recognition.",
            "venue": "ArXiv Preprint ArXiv:2302.03024",
            "year": 2023
        },
        {
            "authors": [
                "B. Ye",
                "H. Chang",
                "B. Ma",
                "S. Shan",
                "X. Chen"
            ],
            "title": "Joint Feature Learning and Relation Modeling for Tracking: A OneStream Framework.",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "H. Zhang",
                "L. Zhang",
                "L. Zhuo",
                "J. Zhang"
            ],
            "title": "Object Tracking in RGB-T Videos Using Modal-Aware Attention Network and Competitive Learning.",
            "year": 2020
        },
        {
            "authors": [
                "L. Zhang",
                "M. Danelljan",
                "A. Gonzalez-Garcia",
                "J van de. Weijer",
                "F. Shahbaz Khan"
            ],
            "title": "Multi-Modal Fusion for End-to-End RGB-T Tracking.",
            "venue": "IEEE/CVF International Conference on Computer Vision Workshop (ICCVW),",
            "year": 2019
        },
        {
            "authors": [
                "T. Zhang",
                "H. Guo",
                "Q. Jiao",
                "Q. Zhang",
                "J. Han"
            ],
            "title": "Efficient RGB-T Tracking via Cross-Modality Distillation.",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "T. Zhang",
                "X. Liu",
                "Q. Zhang",
                "J. Han"
            ],
            "title": "SiamCDA: Complementarity-and Distractor-Aware RGB-T Tracking Based on Siamese Network.",
            "venue": "IEEE Transactions on Circuits and Systems for Video Technology",
            "year": 2021
        },
        {
            "authors": [
                "X. Zhang",
                "P. Ye",
                "S. Peng",
                "J. Liu",
                "K. Gong",
                "G. Xiao"
            ],
            "title": "SiamFT: An RGB-Infrared Fusion Tracking Method via Fully Convolutional Siamese Networks.",
            "venue": "IEEE Access",
            "year": 2019
        },
        {
            "authors": [
                "X. Zhang",
                "P. Ye",
                "S. Peng",
                "J. Liu",
                "G. Xiao"
            ],
            "title": "DSiamMFT: An RGB-T Fusion Tracking Method via Dynamic Siamese Networks Using Multi-Layer Feature Fusion.",
            "venue": "Signal Processing: Image Communication",
            "year": 2020
        },
        {
            "authors": [
                "J. Zhu",
                "S. Lai",
                "X. Chen",
                "D. Wang",
                "H. Lu"
            ],
            "title": "Visual Prompt Multi-Modal Tracking.",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Y. Zhu",
                "C. Li",
                "B. Luo",
                "J. Tang",
                "X. Wang"
            ],
            "title": "Dense Feature Aggregation and Pruning for RGBT Tracking.",
            "venue": "In Proceedings of the 27th ACM International Conference on Multimedia,",
            "year": 2019
        },
        {
            "authors": [
                "Y. Zhu",
                "C. Li",
                "J. Tang",
                "B. Luo"
            ],
            "title": "Quality-Aware Feature Aggregation Network for Robust RGBT Tracking.",
            "venue": "IEEE Transactions on Intelligent Vehicles",
            "year": 2021
        },
        {
            "authors": [
                "Y. Zhu",
                "C. Li",
                "J. Tang",
                "B. Luo",
                "L. Wang"
            ],
            "title": "RGBT Tracking by Trident Fusion Network.",
            "venue": "IEEE Transactions on Circuits and Systems for Video Technology",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "Introduction",
            "text": "With the gradual maturity and popularity of thermal-infrared imaging devices, object tracking based on the fusion of visible and thermal images (RGB-T tracking) has garnered increasing attention from researchers. By incorporating the thermal modality, RGB-T tracking effectively addresses issues related to sensitivity to illumination changes and susceptibility to rain, fog, and other interferences that are commonly encountered in single-modality(visible) tracking. It has found widespread applications in industries such as autonomous driving, intelligent security, and robotics (Xiao et al. 2022). However, researchers have been continuously striving to improve the effectiveness and efficiency of fusing the two modalities to better handle various challenges that may arise during the tracking process, such as illumination variations, fast motion, occlusions, and thermal crossover.\nRecently, with the emergence of prompt learning methods in the field of natural language processing, it has become possible to transfer knowledge from pre-trained large models to downstream tasks more quickly and efficiently. Similarly, in the computer vision domain, researchers have also explored prompt learning methods(Jia et al. 2022). Zhu et al.(J. Zhu et al. 2023) applied the concept of prompt learning to RGB-T tracking in their work ViPT. The core idea is to freeze the backbone that is pre-trained on RGB images and\nintegrate the information from the second modality into each layer of the frozen backbone using a set of modality complementarity prompters, as shown in Fig 1(a).\nWhile this method requires training a very small number of parameters, the frozen layers still consume computational resources during training and inference. Therefore, the savings in GPU memory space and computational burden are relatively limited. Additionally, since only unidirectional prompts are used, and the prompter simply calculates weights for pixels within the tokens, this method is actually insufficient for modal fusion.\nTherefore, we believe that there exists a better form of modality fusion for RGB-T tracking based on prompt learning. The main challenges lie in balancing the complexity of the modules, training difficulty, and efficiency. Therefore, we have designed a novel approach, as shown in Fig 1(b), based on the idea of mutual prompt learning. In our approach, we introduce two sets of lightweight modules in each layer of the backbone. The fused information is propagated downwards layer by layer through updating strategies, making full and effective use of pre-trained weights to integrate multi-level information from low-level details to high-\nlevel semantics across different modalities. Ultimately, this approach enhances the dominant modality information adaptively while suppressing noise from the inferior modality.\nTo be more specific, we extend the ViT backbone into a dual-branch Siamese architecture. After embedding the images of the two modalities separately, we employ two sets of lightweight prompters, which consist of token attention and spatial attention concatenated together. These modules learn the weights of each token in different modalities and the weights of different spatial positions within each token. Next, we multiply the obtained weights with the original tokens and add them to the corresponding layer's output of the other modality's backbone. Additionally, we add the output of the previous level's prompter to the above, resulting in the prompt output of this layer. We then add this prompt output to the output of the backbone at this layer, which serves as the input for the next layer in the backbone. This process enables multi-level information exchange and integration within the backbone.\nLastly, we explored an online template updating strategy based on classification confidence score and a prediction box correction method based on Kalman filtering to adapt to challenges such as significant appearance variations and severe occlusions during the tracking process. These strategies further enhance the robustness of the tracker.\nThe evaluation results on multiple publicly available RGB-T datasets demonstrate that our proposed tracker achieves state-of-the-art performance with a relatively small number of trainable parameters.\nThe main contributions of this article can be summarized as follows: \u2022Proposed a RGB-T tracking framework, called MultiModal Mutual Prompt Learning Tracker (MPLT). By establishing bidirectional modal information interaction channels, it enables the complementary fusion of different modality images during the feature extraction stage, thereby achieving adaptive and precise enhancement of modal information. \u2022Designed a more efficient Multi-Modal Visual Information Prompter (MVIP). It achieves high-quality information fusion by adaptively generating weights from another modality through the concatenation of two attention mechanisms and adding them to the current modality. \u2022As a relatively versatile multi-modal fusion tracking architecture, our proposed method can be easily extended to other modal fusion tracking scenarios beyond visible modality."
        },
        {
            "heading": "Related Works",
            "text": "In this section, we give a brief introduction to RGB-T tracking and visual prompt learning."
        },
        {
            "heading": "RGB-T Tracking",
            "text": "Currently, there are two main paths followed by RGB-T tracking methods. The first path, based on MDNet(Nam and Han 2016), is adopted by methods such as ((Xiao et al. 2022; Long Li et al. 2019; Lu et al. 2021; 2022; Y. Zhu et al. 2021; Li et al. 2020; Y. Zhu et al. 2019; Gao et al. 2019; Y. Zhu et al. 2022; H. Zhang et al. 2020)). These methods first generate candidate boxes (RoIs) from the search frame, then use specific fusion structures to merge features from different modalities within the RoIs. Finally, they perform binary classification and regress the bbox based on the fused features. Li et al. (Long Li et al. 2019) designed a MultiAdapter Network that extracts features at three levels: modality-shared features, modality-specific features, and instance-level features. Zhu et al.(Y. Zhu et al. 2022) proposed a three-branch architecture to integrate fused modality features and two modality-specific features, achieving robust target representation. Lu et al. (Lu et al. 2022) proposed a network with a dual-gate structure that utilizes discriminative information from one modality to guide the feature learning of another modality, effectively reducing noise from low-quality modalities. Xiao et al.(Xiao et al. 2022), focusing on the specific challenges of RGBT attributes, employed specific fusion strategies and incorporated three independent Transformer encoders and decoders into each branch to achieve self-enhancement within modalities and interaction across modalities. One important drawback of these methods is that the aspect ratio of the RoI regions is fixed and local. They cannot flexibly adapt to changes in the target's shape and fail to include sufficient background information for feature learning. As a result, the feature interaction between different modality RoIs may be insufficient, leading to inadequate modeling of the global context. This limitation also restricts the mutual enhancement and complementarity between the two modalities(Hui et al. 2023).\nThe second main path in RGB-T tracking is the Siamese architecture, which is widely acclaimed in visual tracking due to its efficient end-to-end training(Tang, Xu, and Wu 2022). RGB-T tracking based on the Siamese architecture((X. Zhang et al. 2019; 2020; Hui et al. 2023; Luo et al. 2023; Guo et al. 2022; T. Zhang et al. 2021; 2023)) typically involves designing separate feature extraction branches for visible and thermal modalities. Modal fusion modules are introduced in the backbone or after the backbone to fuse the extracted features. The fused features are then fed into the head for classification/regression or directly used for predicting the corners of the target bounding box. These methods generally rely on offline training and do not have an online learning phase. Early Siamese-based RGB-T tracking methods mostly used VGG(Simonyan and Zisserman 2014) or ResNet(He et al. 2016) as the feature extraction backbone. While they achieved high speeds, their accuracy often fell behind MDNet-based methods. In recent years, with the\nrapid development of the Transformer architecture, more and more RGB-T tracking methods have introduced Transformers as the feature extraction backbone. Hui et al.(Hui et al. 2023) and Luo et al.(Luo et al. 2023) extended the Transformer backbone into the Siamese architecture and achieved modal fusion by inserting multiple cross-attention/self-attention modules into the backbone or placing them after the backbone. Although these methods achieved high accuracy, the stacking of multiple attention modules introduced a large number of parameters, resulting in higher overall complexity of the network."
        },
        {
            "heading": "Visual Prompt Learning",
            "text": "Prompt, as a form of auxiliary information, has been added to the text to help pre-trained models better adapt to specific downstream tasks. It has been widely applied in the field of NLP(Dai et al. 2023). Recently, researchers have begun to explore the introduction of prompt learning in the computer vision domain. VPT(Jia et al. 2022) was among the first to explore the feasibility of prompt leaning in the visual domain. By freezing the backbone parameters and introducing a small number of learnable parameters in the input space, it achieved comparable downstream performance to full finetuning. AIM(Yang et al. 2023) introduced the idea of prompt learning in the domain of video action recognition. By introducing adaptor-based prompters in both spatial and temporal dimensions, it achieved high performance with a small number of trainable parameters. ViPT(J. Zhu et al. 2023) introduced prompt leaning in multi-modal object tracking by integrating limited multi-modal data into a baseline model pre-trained on a large number of RGB images, effectively improving the overall tracking robustness. The aforementioned works based on visual prompt learning all employed the method of freezing the backbone while training the prompt modules. In contrast, in this paper, we adopt the method of Full Finetune + Prompt Leaning. This is because we found in our experiments that freezing the backbone parameters had a limited effect in reducing computational and memory space usage. Training both the backbone and\nprompt simultaneously allows us to provide additional guidance to the backbone while fine-tuning, which better adapts to the downstream data compared to freezing the backbone.\nMethod\nThe overall architecture of the proposed tracking method is shown in Fig 2, consisting of three components: a dualbranch Transformer backbone, a multi-modal mutual prompting structure, and a localization head. Below, we will provide a detailed description of the workflow of this method.\nBaseline Tracking Model\nThe basic form of single-object tracking (SOT) involves taking the target region from the initial frame as the template \ud835\udc4d\ud835\udc45\ud835\udc3a\ud835\udc35 \u2208 \u211d \ud835\udc3b\ud835\udc67\u00d7\ud835\udc4a\ud835\udc67\u00d73\uff0cand searching for the template target in the subsequent frame \ud835\udc4b\ud835\udc45\ud835\udc3a\ud835\udc35 \u2208 \u211d \ud835\udc3b\ud835\udc65\u00d7\ud835\udc4a\ud835\udc65\u00d73 to locate the target by bounding it.\nFor Transformer-based SOT models like the baseline(Ye et al. 2022)model, the first step is to convert \ud835\udc4d\ud835\udc45\ud835\udc3a\ud835\udc35 , \ud835\udc4b\ud835\udc45\ud835\udc3a\ud835\udc35 into patches of size \ud835\udc43 \u00d7 \ud835\udc43 through embedding:\n{\ud835\udc4d\ud835\udc45\ud835\udc3a\ud835\udc35, \ud835\udc4b\ud835\udc45\ud835\udc3a\ud835\udc35} \u2192 {\ud835\udc4d\ud835\udc45\ud835\udc3a\ud835\udc35 \ud835\udc43 \u2208 \u211d\ud835\udc41\ud835\udc67\u00d7\ud835\udc37, \ud835\udc4b\ud835\udc45\ud835\udc3a\ud835\udc35 \ud835\udc43 \u2208 \u211d\ud835\udc41\ud835\udc4b\u00d7\ud835\udc37} (1)\nWhere \ud835\udc41\ud835\udc67 = \ud835\udc3b\ud835\udc67\ud835\udc4a\ud835\udc67/\ud835\udc43 2 \uff0c \ud835\udc41\ud835\udc4b = \ud835\udc3b\ud835\udc4b\ud835\udc4a\ud835\udc4b/\ud835\udc43 2 \uff0c \ud835\udc37 = \ud835\udc432 \u00d7 \ud835\udc36(\ud835\udc36 is the number of image channels, here is 3). Next, \ud835\udc4d\ud835\udc43\uff0c\ud835\udc4b\ud835\udc43 are concatenated and fed into the backbone to learn features and facilitate interaction between the template and search regions:\n\ud835\udc3b\ud835\udc45\ud835\udc3a\ud835\udc35 = \ud835\udc36\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc4e\ud835\udc61(\ud835\udc4d\ud835\udc45\ud835\udc3a\ud835\udc35 , \ud835\udc4b\ud835\udc45\ud835\udc3a\ud835\udc35) (2)\n\ud835\udc35\ud835\udc45\ud835\udc3a\ud835\udc35 = \u210e(\ud835\udc53(\ud835\udc3b\ud835\udc45\ud835\udc3a\ud835\udc35)) (3) Where \ud835\udc3b\ud835\udc45\ud835\udc3a\ud835\udc35 \u2208 \u211d\n\ud835\udc41\u00d7\ud835\udc37 , \ud835\udc41 = \ud835\udc41\ud835\udc4b + \ud835\udc41\ud835\udc4d, \ud835\udc53 represent the ViT backbone, \u210e represent the localization head, \ud835\udc35\ud835\udc45\ud835\udc3a\ud835\udc35 represent the final output bounding box."
        },
        {
            "heading": "Mutual Prompt Learning for RGB-T Tracking",
            "text": "Overview In our approach, in addition to the visible modality, we also introduce the thermal modality. Therefore, we have:\n{\ud835\udc4d\ud835\udc47\ud835\udc3c\ud835\udc45, \ud835\udc4b\ud835\udc47\ud835\udc3c\ud835\udc45} \u2192 {\ud835\udc4d\ud835\udc47\ud835\udc3c\ud835\udc45 \ud835\udc43 \u2208 \u211d\ud835\udc41\ud835\udc67\u00d7\ud835\udc37, \ud835\udc4b\ud835\udc47\ud835\udc3c\ud835\udc45 \ud835\udc43 \u2208 \u211d\ud835\udc41\ud835\udc4b\u00d7\ud835\udc37} (4)\n\ud835\udc3b\ud835\udc47\ud835\udc3c\ud835\udc45 = \ud835\udc36\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc4e\ud835\udc61(\ud835\udc4d\ud835\udc47\ud835\udc3c\ud835\udc45, \ud835\udc4b\ud835\udc47\ud835\udc3c\ud835\udc45) (5)\nMoreover, we extend the backbone into a dual-branch Siamese architecture, where \ud835\udc53\ud835\udc45\ud835\udc3a\ud835\udc35 represents the backbone for extracting RGB features, and \ud835\udc53\ud835\udc47\ud835\udc3c\ud835\udc45 represents the backbone for extracting thermal features. Let \ud835\udc38\ud835\udc45\ud835\udc3a\ud835\udc35 \ud835\udc59 denote the encoder in the l-th layer of \ud835\udc53\ud835\udc45\ud835\udc3a\ud835\udc35, and \ud835\udc38\ud835\udc47\ud835\udc3c\ud835\udc45 \ud835\udc59 denote the encoder in the l-th layer of \ud835\udc53\ud835\udc47\ud835\udc3c\ud835\udc45 (with a total of L layers). \ud835\udc3b\ud835\udc45\ud835\udc3a\ud835\udc35 \ud835\udc59 represents the output of \ud835\udc38\ud835\udc45\ud835\udc3a\ud835\udc35 \ud835\udc59 , and \ud835\udc3b\ud835\udc47\ud835\udc3c\ud835\udc45 \ud835\udc59 represents the output of \ud835\udc38\ud835\udc47\ud835\udc3c\ud835\udc45 \ud835\udc59 :\n\ud835\udc3b\ud835\udc45\ud835\udc3a\ud835\udc35 \ud835\udc59 = \ud835\udc38\ud835\udc59(\ud835\udc3b\ud835\udc45\ud835\udc3a\ud835\udc35 \ud835\udc59\u22121 ), \ud835\udc59 = 1,2, \u2026 , \ud835\udc3f (6)\nBefore the input to the first layer of the encoder, the tokens from the visible modality and the thermal modality (denoted as \ud835\udc3b\ud835\udc45\ud835\udc3a\ud835\udc35 \ud835\udc56\ud835\udc5b\ud835\udc56\ud835\udc61 and \ud835\udc3b\ud835\udc47\ud835\udc3c\ud835\udc45 \ud835\udc56\ud835\udc5b\ud835\udc56\ud835\udc61, respectively) are first passed through an Initial Multi-Modal Visual Information Prompter (IMVIP). The outputs of the IMVIP are then added to the tokens of the other modality:\n\ud835\udc3b\ud835\udc45\ud835\udc3a\ud835\udc35 0 = \ud835\udc3b\ud835\udc45\ud835\udc3a\ud835\udc35 \ud835\udc56\ud835\udc5b\ud835\udc56\ud835\udc61 + \ud835\udc43\ud835\udc61\ud835\udc56\n\ud835\udc43\ud835\udc61\ud835\udc56 = \ud835\udc3c\ud835\udc40\ud835\udc49\ud835\udc3c\ud835\udc43(\ud835\udc3b\ud835\udc45\ud835\udc3a\ud835\udc35 \ud835\udc56\ud835\udc5b\ud835\udc56\ud835\udc61\uff0c\ud835\udc3b\ud835\udc47\ud835\udc3c\ud835\udc45 \ud835\udc56\ud835\udc5b\ud835\udc56\ud835\udc61)\n\ud835\udc3b\ud835\udc47\ud835\udc3c\ud835\udc45 0 = \ud835\udc3b\ud835\udc47\ud835\udc3c\ud835\udc45 \ud835\udc56\ud835\udc5b\ud835\udc56\ud835\udc61 + \ud835\udc43\ud835\udc5f\ud835\udc56\n\ud835\udc43\ud835\udc5f\ud835\udc56 = \ud835\udc3c\ud835\udc40\ud835\udc49\ud835\udc3c\ud835\udc43(\ud835\udc3b\ud835\udc47\ud835\udc3c\ud835\udc45 \ud835\udc56\ud835\udc5b\ud835\udc56\ud835\udc61\uff0c\ud835\udc3b\ud835\udc45\ud835\udc3a\ud835\udc35 \ud835\udc56\ud835\udc5b\ud835\udc56\ud835\udc61 ) (7)\nWhere \ud835\udc43\ud835\udc61\ud835\udc56 and \ud835\udc43\ud835\udc5f\ud835\udc56 represent the output of the IMVIP. In the subsequent layers of the encoder, we have:\n\ud835\udc3b\ud835\udc45\ud835\udc3a\ud835\udc35 \ud835\udc59 = \ud835\udc3b\ud835\udc45\ud835\udc3a\ud835\udc35 \ud835\udc59\u22121 + \ud835\udc43\ud835\udc61 \ud835\udc59\n\ud835\udc43\ud835\udc61 \ud835\udc59 = \ud835\udc40\ud835\udc49\ud835\udc3c\ud835\udc43(\ud835\udc3b\ud835\udc45\ud835\udc3a\ud835\udc35 \ud835\udc59\u22121 , \ud835\udc43\ud835\udc61 \ud835\udc59\u22121, \ud835\udc3b\ud835\udc47\ud835\udc3c\ud835\udc45 \ud835\udc59\u22121)\n\ud835\udc3b\ud835\udc47\ud835\udc3c\ud835\udc45 \ud835\udc59 = \ud835\udc3b\ud835\udc47\ud835\udc3c\ud835\udc45 \ud835\udc59\u22121 + \ud835\udc43\ud835\udc5f \ud835\udc59\n\ud835\udc43\ud835\udc5f \ud835\udc59 = \ud835\udc40\ud835\udc49\ud835\udc3c\ud835\udc43(\ud835\udc3b\ud835\udc47\ud835\udc3c\ud835\udc45 \ud835\udc59\u22121, \ud835\udc43\ud835\udc5f \ud835\udc59\u22121, \ud835\udc3b\ud835\udc45\ud835\udc3a\ud835\udc35 \ud835\udc59\u22121 ) (8)\nWhere \ud835\udc59 = 1,2, \u2026 , \ud835\udc3f , \ud835\udc40\ud835\udc49\ud835\udc3c\ud835\udc43 represents the Multi-Modal Visual Information Prompter, and \ud835\udc43\ud835\udc61 \ud835\udc59 and \ud835\udc43\ud835\udc5f \ud835\udc59 represent the output of the MVIP in the l-th layer(with \ud835\udc43\ud835\udc61 0 = \ud835\udc43\ud835\udc61\ud835\udc56 and \ud835\udc43\ud835\udc5f 0 = \ud835\udc43\ud835\udc5f\ud835\udc56). Lastly, the features from the two branches, after mutual prompt learning, are concatenated. Then, they are passed through a linear layer to reduce the channel dimensionality before being fed into the localization head for classification prediction and target box regression:\n\ud835\udc35 = \u210e (\ud835\udc37\ud835\udc45(\ud835\udc36\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc4e\ud835\udc61(\ud835\udc3b\ud835\udc45\ud835\udc3a\ud835\udc35 \ud835\udc3f , \ud835\udc3b\ud835\udc45\ud835\udc3a\ud835\udc35 \ud835\udc3f ))) (9)\nWhere \ud835\udc37\ud835\udc45 represents the linear layer for dimensionality reduction. The details of the localization head \u210e can be referred to OSTrack(Ye et al. 2022). Multi-Modal Visual Information Prompter In VPT(Jia et al. 2022), several forms of visual prompts were explored, and the authors empirically demonstrated that converting\nthe prompts into token form and inserting it into the original token sequence yielded the best performance in downstream tasks. It is well known that the parameter count of the Transformer structure is related to the number of tokens, so this form of prompt insertion comes with a higher computational burden. In our task, we use two modalities of images with the same size for prompt learning. Therefore, considering efficiency, we adopt the second visual prompt method mentioned in(Jia et al. 2022) , which directly superimposes the prompt information on the original tokens. This approach does not increase the token count while achieving comparable performance in downstream tasks to the first method.\nIn terms of the specific structure of the prompter, the proposed modules can be divided into two subclasses. The first one is the Initial Multi-Modal Visual Information Prompter (IMVIP), which has only two input branches (as shown in Equation 7). The second one is the Multi-Modal Visual Information Prompter (MVIP), which is designed for intermediate layers and has three input branches (as shown in Equation 8). Since the basic structure of the two types of prompter is the same except for the number of branches, we will now focus on providing a detailed introduction to the MVIP module.\nAs shown in Fig 3, the MVIP module has three input branches: the output of the encoder of current modality from the previous layer, the output of the encoder of the other modality from the previous layer, and the output of the previous MVIP. Inspired by CBAM(Woo et al. 2018), the workflow of the MVIP module consists of three steps: (1)Spatial attention operations are performed on each of the three input branches. Specifically, for a particular branch, let's assume the input tokens are represented by \ud835\udc3b, average pooling and max pooling are first applied along the \ud835\udc37 dimension:\n\ud835\udc4a\ud835\udc4e\ud835\udc63\ud835\udc5f \ud835\udc60 = \ud835\udc4e\ud835\udc63\ud835\udc5f\u2212\ud835\udc5d\ud835\udc5c\ud835\udc5c\ud835\udc59(\ud835\udc3b, \ud835\udc51\ud835\udc56\ud835\udc5a~\ud835\udc37) (10)\n\ud835\udc4a\ud835\udc5a\ud835\udc4e\ud835\udc65 \ud835\udc60 = \ud835\udc5a\ud835\udc4e\ud835\udc65\u2212\ud835\udc5d\ud835\udc5c\ud835\udc5c\ud835\udc59(\ud835\udc3b, \ud835\udc51\ud835\udc56\ud835\udc5a~\ud835\udc37) (11)\nNext, \ud835\udc4a\ud835\udc4e\ud835\udc63\ud835\udc5f \ud835\udc60 and \ud835\udc4a\ud835\udc5a\ud835\udc4e\ud835\udc65 \ud835\udc60 undergo channel dimension reduction in the \ud835\udc41 dimension using a 1\u00d71 convolutional layer \ud835\udc54\ud835\udc601 . This projects the features into a lower-dimensional latent embedding. The resulting features are then passed through a ReLU layer for non-linear enhancement before being projected back to the original dimension using another 1\u00d71 convolutional layer \ud835\udc54\ud835\udc602. This generates a weight map with the same size as the token dimension \ud835\udc37:\n\ud835\udc4a\ud835\udc60\ud835\udc5d\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc4e\ud835\udc59 =\n\ud835\udc54\ud835\udc602 (\ud835\udc5f\ud835\udc52\ud835\udc59\ud835\udc62(\ud835\udc54\ud835\udc601(\ud835\udc4a\ud835\udc4e\ud835\udc63\ud835\udc5f \ud835\udc60 ))) + \ud835\udc54\ud835\udc602 (\ud835\udc5f\ud835\udc52\ud835\udc59\ud835\udc62(\ud835\udc54\ud835\udc601(\ud835\udc4a\ud835\udc5a\ud835\udc4e\ud835\udc65 \ud835\udc60 ))) (12)\nFinally, \ud835\udc4a\ud835\udc60\ud835\udc5d\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc4e\ud835\udc59 is multiplied element-wise with the original tokens to obtain the tokens with redistributed weights in the spatial dimension:\n\ud835\udc3b\u2217 = \ud835\udc3b \u2217 \ud835\udc4a\ud835\udc60\ud835\udc5d\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc4e\ud835\udc59 (13)\n(2)Token attention operations are performed on each of the three input branches. Specifically, for a particular branch, the average and maximum values are computed along the \ud835\udc41 dimension of the tokens:\n\ud835\udc4a\ud835\udc4e\ud835\udc63\ud835\udc5f \ud835\udc61 = \ud835\udc5a\ud835\udc52\ud835\udc4e\ud835\udc5b(\ud835\udc3b, \ud835\udc51\ud835\udc56\ud835\udc5a~\ud835\udc41) (14)\n\ud835\udc4a\ud835\udc5a\ud835\udc4e\ud835\udc65 \ud835\udc61 = max(\ud835\udc3b, \ud835\udc51\ud835\udc56\ud835\udc5a~\ud835\udc41) (15)\nThen, \ud835\udc4a\ud835\udc4e\ud835\udc63\ud835\udc5f \ud835\udc61 \u2208 \u211d1\u00d7\ud835\udc37 and \ud835\udc4a\ud835\udc5a\ud835\udc4e\ud835\udc65 \ud835\udc61 \u2208 \u211d1\u00d7\ud835\udc37 are concatenated along the first dimension, and a 7\u00d77 convolutional layer \ud835\udc54\ud835\udc61 with padding is used to reduce the first dimension back to 1, resulting in a \ud835\udc37-dimensional weight map \ud835\udc4a\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b:\n\ud835\udc4a\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b = \ud835\udc54\ud835\udc61 \u00b7 \ud835\udc36\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc4e\ud835\udc61(\ud835\udc4a\ud835\udc4e\ud835\udc63\ud835\udc5f , \ud835\udc4a\ud835\udc5a\ud835\udc4e\ud835\udc65) (16)\nThen, the weight map is multiplied element-wise with the input tokens to obtain the token sequence with reweighted values:\n\ud835\udc3b\u2217 = \ud835\udc3b \u2217 \ud835\udc4a\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc5b (17)\n(3) Execute the spatial fovea operation on the token of current modality, which first applies a \u03bb-smoothed spatial softmax across all the spatial dimensions, and produces the enhanced embeddings \ud835\udc3b\ud835\udc45\ud835\udc3a\ud835\udc35 \u2217 by applying the channel-wise spatial attention-like mask \ud835\udc4a\ud835\udc53\ud835\udc5c\ud835\udc63\ud835\udc52\ud835\udc4e over \ud835\udc3b\ud835\udc45\ud835\udc3a\ud835\udc35 \u2217 . Then, the tokens from the other two branches are added to obtain the output of MVIP\uff1a\n\ud835\udc43\ud835\udc59 = \ud835\udc4a\ud835\udc53\ud835\udc5c\ud835\udc63\ud835\udc52\ud835\udc4e \u00b7 \ud835\udc3b1 \u2217 + \ud835\udc3b2 \u2217 + \ud835\udc43\ud835\udc59\u22121 \u2217 (18)\nWhere \ud835\udc3b1 \u2217 , \ud835\udc3b2 \u2217 represent the outputs of the tokens from two different modalities after the two attention operations, and \ud835\udc43\ud835\udc59\u22121 \u2217 represents the output of the previous MVIP after the two attention operations. Discussion Regardless of token attention or spatial attention, their purpose is to adaptively allocate weights to different tokens or pixel positions within tokens, assuming that the image information is redundant. This aims to enhance the target information and suppress noise. In previous works on multi-modal image fusion using visual prompts, such as ViPT(J. Zhu et al. 2023), the prompters were designed considering only the pixel positions within tokens, and the prompter structure was relatively simple, which failed to accurately extract the modal information for prompting. This led to poor performance of the model in challenging sequences that require effective utilization of complementary thermal modal information, such as low-illumination conditions (as shown in Table 2). Additionally, considering the complementary nature of information between visible and thermal modalities, there are cases where the visible modality needs supplementation from the thermal modality, and vice versa. Therefore, the design of visual prompts should not be unidirectional but rather bidirectional. A mechanism based on mutual prompt learning for modality information transfer can more fully capture various complementary situations."
        },
        {
            "heading": "Inference",
            "text": "Object tracking, as a real-time task, also considers temporal information. Therefore, in the inference stage, to incorporate temporal information, we designed a simple online template\nAlgorithm 1: Online Template Update and Kalman Filter Based Prediction Correction\nInput: The predicted bbox and classification confidence of the last n frames. Parameter: n, thru, thrb Output: Bbox to be corrected.\nwhile Current frame number<Total frame number do\nRecord the predicted bbox and classification confidence of the last n frames if Current confidence>thru then\nRe-crop the template\nelse Do nothing if Current confidence<thrb then\nAccording to the prediction of frame 0~n-1, use the Kalman filter to predict the bbox of the nth frame, and use it to correct the network prediction results.\nelse Do nothing\nend while\nupdate method based on the confidence scores from the classification head. Additionally, we combined it with Kalman filtering to correct low-confidence prediction results. The specific algorithm is presented in Algorithm 1."
        },
        {
            "heading": "Experiments",
            "text": "This section mainly introduces the specific implementation method of MPLT, evaluates its performance on multiple datasets, and conducts ablation experiments on the involved modules/methods to verify its effectiveness. Implementation Details Our model is implemented using PyTorch (Paszke et al. 2019) and trained using two RTX 3090 GPUs. The batchsize is set to 24, and each epoch samples 60,000 images. We train the model for a total of 15 epochs on LasHeR training set. The learning rate for the backbone is set to 7.5e-5, while the learning rate for other parts is set to 7.5e-4. After the 10th epoch, the learning rate is decayed by a factor of 10. We use the AdmW(Loshchilov and Hutter 2017) optimizer for iteration with a weight decay of 1e-4. The input image sizes for the network are as follows: search frame is 256\u00d7256, and template frame is 128\u00d7128. The structure and parameter settings of the loss function are the same as OSTrack (Ye et al. 2022) . The template update and prediction correction consider 16 frames, and the confidence thresholds are set to thru=0.91 and thrb=0.25. Additionally, for evaluation metrics, we use commonly used precision/success rate(PR/SR) metrics and set the center location error (CLE) threshold to the conventional value of 20 pixels."
        },
        {
            "heading": "Evaluation on RGBT234 Dataset",
            "text": "The RGBT234 dataset(Li et al. 2019) consists of 234 sequences with approximately 116.7K frames. From Table 1, it can be observed that our proposed algorithm achieves the best tracking performance among all state-of-the-art RGBT tracking algorithms. It outperforms the second and thirdranked algorithms by 1.3%/2% & 2.7%/3.5% in terms of PR and SR respectively. Compared to the baseline model, our model achieved an improvement of 15.5% in terms of PR and 10.8% in terms of SR. These results provide strong evidence for the effectiveness of our algorithm."
        },
        {
            "heading": "Evaluation on LasHeR Dataset",
            "text": "The LasHeR dataset(Li et al. 2022) is currently the largest RGB-T tracking dataset with precise annotation and alignment. The dataset is divided into training and testing sets, and it presents a higher level of tracking difficulty compared to the RGBT234 dataset. We evaluate the trackers on 245 test video sequences in terms of precision plot and success plot. The results are reported in Fig 4. Our tracker achieves state-of-the-art performance in terms of PR and SR. It outperforms the second and third-ranked trackers by 1.5%/0.8% and 6.7%/4.7% respectively. Compared to the baseline model, our model achieved an improvement of 22.2% in terms of PR and 17.6% in terms of SR. These results further validate the effectiveness of our algorithm.\nAttribute-Based Evaluation In order to better evaluate the complementary fusion capability of our proposed tracking approach across two modalities, we selected 5 challenging attributes from the LasHeR dataset, namely low illumination(LI), high illumination(HI), thermal crossover(TC), frame loss(FL), and fast motion(FM). Comparisons with the second to the fourth-ranked trackers in terms of SR metrics revealed that our proposed model achieved the best performance in these challenges, providing ample evidence of its ability to fuse complementary information between the two modalities, and proving its high robustness even in scenarios with target blurring or loss, too."
        },
        {
            "heading": "Evaluation on RGBT210 Dataset",
            "text": "The RGBT210(Li et al. 2017) dataset consists of 210 sequences with approximately 104.8K frames. As shown in Table 3, our tracker achieved the best performance in terms of PR/SR metrics for RGBT210 dataset."
        },
        {
            "heading": "Ablation Study",
            "text": "In this section, each component of MPLT will be analyzed separately to validate their effectiveness. Variants Comparison We further explored different variant versions of MPLT. (1)MPLT-Full: Full MPLT model. (2) MPLT-w/o MVIP: Remove all MVIP modules, Concatenate the features from the two backbones directly, and feed them into the localization head. (3) MPLT-w/o SA: Remove all spatial attention operations in MVIP. (4) MPLT-w/o TA: Remove all token attention operations in MVIP. (5) MPLTw/o TU: Remove template update operation. (6) MPLT-w/o KF: Remove the prediction refinement step based on Kalman filtering. As shown in Table 4, It can be observed that the modules/methods we designed have improved the tracking performance to varying degrees.\nFreeze or Unfreeze\uff1f To investigate the impact of freezing the backbone on our method, we introduced a control group called MPLT-F. The network architecture of MPLT-F is the same as MPLT, but the parameters of the backbone will be frozen. Similarly,ViPT-UF in the control group represents unfreezing the backbone parameters of ViPT.\nAs shown in Table 5, freezing the backbone parameters significantly reduces the performance of our model, but it still outperforms ViPT, which is based on unidirectional prompt learning. This demonstrates the effectiveness of the prompt structure we designed. Additionally, it is worth noting that freezing the backbone does not significantly reduce the GPU memory utilization compared to trainable parameters. This implies that full finetuning remains a cost-effective choice. Lastly, compared to TBSI, which uses stacked cross-attention modules to fuse tokens, our model is more efficient in terms of parameter count and FLOPS."
        },
        {
            "heading": "Visualization",
            "text": "To demonstrate the robustness of our model in challenging tracking scenarios, we visualized the attention map between the template and the search region, as shown in Fig 5. Compare to the baseline model, our model accurately matches the corresponding regions between the template and the search image in high/low-illumination conditions and thermal-crossover scenarios, etc. Additionally, it effectively suppresses noise from low-quality modalities.\nConclusion\nIn this work, we propose MPLT, an RGB-T tracking method based on multi-modal mutual prompt learning. MPLT effectively extracts and fuses complementary information from different modalities while maintaining a low computational cost. It successfully transfers the object tracking foundation model trained on single-modal images to downstream tasks, achieving high performance. Extensive experiments provide ample evidence that our method is both effective and efficient."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported by The National Key Research and Development Program of China (No.2022YFB3901800, No.2022YFB3901805)."
        },
        {
            "heading": "Guo, C.; Yang, D.; Li, C.; and Song, P. 2022. \u201cDual Siamese Network for RGBT Tracking via Fusing Predicted Position Maps.\u201d",
            "text": "The Visual Computer 38 (7): 2555\u201367.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. \u201cDeep Residual Learning for Image Recognition.\u201d In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 770\u201378.\nHui, T.; Xun, Z.; Peng, F.; Huang, J.; Wei, X.; Wei, X.; Dai, J.; Han, J.; and Liu, S. 2023. \u201cBridging Search Region Interaction With Template for RGB-T Tracking.\u201d In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 13630\u201339.\nJia, M.; Tang, L.; Chen, B-C.; Cardie, C.; Belongie, S.; Hariharan, B.; and Lim, S-N. 2022. \u201cVisual Prompt Tuning.\u201d In European Conference on Computer Vision, 709\u201327. Springer.\nLi, C.; Liang, X.; Lu, Y.; Zhao, N.; and Tang, J. 2019. \u201cRGB-T Object Tracking:Benchmark and Baseline.\u201d Pattern Recognition 96: 106977.\nLi, C.; Liu, L.; Lu, A.; Ji, Q.; and Tang, J. 2020. \u201cChallenge-Aware RGBT Tracking.\u201d In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XXII 16, 222\u201337. Springer.\nLi, C.; Xue, W.; Jia, Y.; Qu, Z.; Luo, B.; Tang, J.; and Sun, D. 2022. \u201cLasHeR: A Large-Scale High-Diversity Benchmark for RGBT Tracking.\u201d IEEE Transactions on Image Processing 31: 392\u2013404.\nLi, C.; Zhao, N.; Lu, Y.; Zhu, C.; and Tang, J. 2017. \u201cWeighted Sparse Representation Regularized Graph Learning for RGB-T Object Tracking.\u201d In Proceedings of the 25th ACM International Conference on Multimedia, 1856\u201364.\nLong Li, C.; Lu, A.; Hua Zheng, A.; Tu, Z.; and Tang, J. 2019. \u201cMulti-Adapter RGBT Tracking.\u201d In Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops, 0\u20130.\nLoshchilov, I.; and Hutter, F. 2017. \u201cDecoupled Weight Decay Regularization.\u201d ArXiv Preprint ArXiv:1711.05101.\nLu, A.; Li, C.; Yan, Y.; Tang, J.; and Luo, B. 2021. \u201cRGBT Tracking via Multi-Adapter Network with Hierarchical Divergence Loss.\u201d IEEE Transactions on Image Processing 30: 5613\u201325.\nLu, A.; Qian, C.; Li, C.; Tang, J.; and Wang, L. 2022. \u201cDualityGated Mutual Condition Network for RGBT Tracking.\u201d IEEE Transactions on Neural Networks and Learning Systems.\nLuo, Y.; Dong, M.; Guo, X.; and Yu, J. 2023. \u201cRGB-T Tracking Based on Mixed Attention.\u201d ArXiv Preprint ArXiv:2304.04264.\nNam, H.; and Han, B. 2016. \u201cLearning Multi-Domain Convolutional Neural Networks for Visual Tracking.\u201d In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 4293\u20134302. Las Vegas, NV, USA: IEEE.\nPaszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.; Chanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; and Antiga, L. 2019. \u201cPytorch: An Imperative Style, High-Performance Deep Learning Library.\u201d Advances in Neural Information Processing Systems 32.\nSimonyan, K.; and Zisserman, A. 2014. \u201cVery Deep Convolutional Networks for Large-Scale Image Recognition.\u201d ArXiv Preprint ArXiv:1409.1556.\nTang, Z.; Xu, T.; and Wu, X-J. 2022. \u201cA Survey for Deep RGBT Tracking.\u201d arXiv. http://arxiv.org/abs/2201.09296.\nWoo, S.; Park, J.; Lee, J-Y.; and Kweon, IS. 2018. \u201cCbam: Convolutional Block Attention Module.\u201d In Proceedings of the European Conference on Computer Vision (ECCV), 3\u201319.\nXiao, Y.; Yang, M.; Li, C.; Liu, L.; and Tang, J. 2022. \u201cAttributeBased Progressive Fusion Network for RGBT Tracking.\u201d Proceedings of the AAAI Conference on Artificial Intelligence 36 (3): 2831\u201338.\nYang, T.; Zhu, Y.; Xie, Y.; Zhang, A.; Chen, C.; and Li, M. 2023. \u201cAim: Adapting Image Models for Efficient Video Action Recognition.\u201d ArXiv Preprint ArXiv:2302.03024.\nYe, B.; Chang, H.; Ma, B.; Shan, S.; and Chen, X. 2022. \u201cJoint Feature Learning and Relation Modeling for Tracking: A OneStream Framework.\u201d In European Conference on Computer Vision, 341\u201357. Springer.\nZhang, H.; Zhang, L.; Zhuo, L.; and Zhang, J. 2020. \u201cObject Tracking in RGB-T Videos Using Modal-Aware Attention Network and Competitive Learning.\u201d Sensors 20 (2): 393.\nZhang, L.; Danelljan, M.; Gonzalez-Garcia, A.; Weijer, J van de.; and Shahbaz Khan, F. 2019. \u201cMulti-Modal Fusion for End-to-End RGB-T Tracking.\u201d In 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW), 2252\u201361. Seoul, Korea (South): IEEE.\nZhang, T.; Guo, H.; Jiao, Q.; Zhang, Q.; and Han, J. 2023. \u201cEfficient RGB-T Tracking via Cross-Modality Distillation.\u201d In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 5404\u201313.\nZhang, T.; Liu, X.; Zhang, Q.; and Han, J. 2021. \u201cSiamCDA: Complementarity-and Distractor-Aware RGB-T Tracking Based on Siamese Network.\u201d IEEE Transactions on Circuits and Systems for Video Technology 32 (3): 1403\u201317.\nZhang, X.; Ye, P.; Peng, S.; Liu, J.; Gong, K.; and Xiao, G. 2019. \u201cSiamFT: An RGB-Infrared Fusion Tracking Method via Fully Convolutional Siamese Networks.\u201d IEEE Access 7: 122122\u201333.\nZhang, X.; Ye, P.; Peng, S.; Liu, J.; and Xiao, G. 2020. \u201cDSiamMFT: An RGB-T Fusion Tracking Method via Dynamic Siamese Networks Using Multi-Layer Feature Fusion.\u201d Signal Processing: Image Communication 84: 115756.\nZhu, J.; Lai, S.; Chen, X.; Wang, D.; and Lu, H. 2023. \u201cVisual Prompt Multi-Modal Tracking.\u201d In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 9516\u2013 26.\nZhu, Y.; Li, C.; Luo, B.; Tang, J.; and Wang, X. 2019. \u201cDense Feature Aggregation and Pruning for RGBT Tracking.\u201d In Proceedings of the 27th ACM International Conference on Multimedia, 465\u201372.\nZhu, Y.; Li, C.; Tang, J.; and Luo, B. 2021. \u201cQuality-Aware Feature Aggregation Network for Robust RGBT Tracking.\u201d IEEE Transactions on Intelligent Vehicles 6 (1): 121\u201330.\nZhu, Y.; Li, C.; Tang, J.; Luo, B.; and Wang, L. 2022. \u201cRGBT Tracking by Trident Fusion Network.\u201d IEEE Transactions on Circuits and Systems for Video Technology 32 (2): 579\u201392."
        }
    ],
    "title": "RGB-T Tracking via Multi-Modal Mutual Prompt Learning",
    "year": 2023
}