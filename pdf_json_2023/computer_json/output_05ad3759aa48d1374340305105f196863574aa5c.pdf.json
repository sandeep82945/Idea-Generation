{
    "abstractText": "Identifying traffic accidents in driving videos is crucial to ensuring the safety of autonomous driving and driver assistance systems. To address the potential danger caused by the long-tailed distribution of driving events, existing traffic accident detection (TAD) methods mainly rely on unsupervised learning. However, TAD is still challenging due to the rapid movement of cameras and dynamic scenes in driving scenarios. Existing unsupervised TAD methods mainly rely on a single pretext task, i.e., an appearance-based or future object localization task, to detect accidents. However, appearance-based approaches are easily disturbed by the rapid movement of the camera and changes in illumination, which significantly reduce the performance of traffic accident detection. Methods based on future object localization may fail to capture appearance changes in video frames, making it difficult to detect ego-involved accidents (e.g., out of control of the ego-vehicle). In this paper, we propose a novel memoryaugmented multi-task collaborative framework (MAMTCF) for unsupervised traffic accident detection in driving videos. Different from previous approaches, our method can more accurately detect both ego-involved and non-ego accidents by simultaneously modeling appearance changes and object motions in video frames through the collaboration of optical flow reconstruction and future object localization tasks. Further, we introduce a memoryaugmented motion representation mechanism to fully explore the interrelation between different types of motion representations and exploit the high-level features of normal traffic patterns stored in memory to augment motion representations, thus enlarging the difference from anomalies. Experimental results on recently published large-scale dataset demonstrate that our method achieves better performance compared to previous stateof-the-art approaches.",
    "authors": [
        {
            "affiliations": [],
            "name": "Rongqin Liang"
        },
        {
            "affiliations": [],
            "name": "Yingxin Yi"
        }
    ],
    "id": "SP:ac0c77e1bcc56a9c745baab539556687a3c64266",
    "references": [
        {
            "authors": [
                "Z. Yuan",
                "X. Song",
                "L. Bai",
                "Z. Wang",
                "W. Ouyang"
            ],
            "title": "Temporal-channel transformer for 3d lidar-based video object detection for autonomous driving",
            "venue": "IEEE Trans. Circuits Syst. Video Technol., vol. 32, no. 4, pp. 2068\u20132078, 2022. 12",
            "year": 2022
        },
        {
            "authors": [
                "L. Claussmann",
                "M. Revilloud",
                "D. Gruyer",
                "S. Glaser"
            ],
            "title": "A review of motion planning for highway autonomous driving",
            "venue": "IEEE Trans. Intell. Transp. Syst., vol. 21, no. 5, pp. 1826\u20131848, 2020.",
            "year": 1826
        },
        {
            "authors": [
                "M. Jeong",
                "B.C. Ko",
                "J.-Y. Nam"
            ],
            "title": "Early detection of sudden pedestrian crossing for safe driving during summer nights",
            "venue": "IEEE Trans. Circuits Syst. Video Technol., vol. 27, no. 6, pp. 1368\u20131380, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "L. Yue",
                "M.A. Abdel-Aty",
                "Y. Wu",
                "A. Farid"
            ],
            "title": "The practical effectiveness of advanced driver assistance systems at different roadway facilities: System limitation, adoption, and usage",
            "venue": "IEEE Trans. Intell. Transp. Syst., vol. 21, no. 9, pp. 3859\u20133870, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Yuan",
                "D. Wang",
                "Q. Wang"
            ],
            "title": "Anomaly detection in traffic scenes via spatial-aware motion reconstruction",
            "venue": "IEEE Trans. Intell. Transp. Syst., vol. 18, no. 5, pp. 1198\u20131209, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "W. Liu",
                "W. Luo",
                "D. Lian",
                "S. Gao"
            ],
            "title": "Future frame prediction for anomaly detection \u2013 a new baseline",
            "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recogn., 2018, pp. 6536\u20136545.",
            "year": 2018
        },
        {
            "authors": [
                "Z. Liu",
                "Y. Nie",
                "C. Long",
                "Q. Zhang",
                "G. Li"
            ],
            "title": "A hybrid video anomaly detection framework via memory-augmented flow reconstruction and flow-guided frame prediction",
            "venue": "Proc. IEEE Int. Conf. Comput. Vis., 2021, pp. 13 588\u201313 597.",
            "year": 2021
        },
        {
            "authors": [
                "Z. Zhou",
                "X. Dong",
                "Z. Li",
                "K. Yu",
                "C. Ding",
                "Y. Yang"
            ],
            "title": "Spatio-temporal feature encoding for traffic accident detection in vanet environment",
            "venue": "IEEE Trans. Intell. Transp. Syst., vol. 23, no. 10, pp. 19 772\u201319 781, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "G. Sun",
                "Z. Liu",
                "L. Wen",
                "J. Shi",
                "C. Xu"
            ],
            "title": "Anomaly crossing: New horizons for video anomaly detection as cross-domain few-shot learning",
            "venue": "arXiv preprint arXiv:2112.06320, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Zhong",
                "X. Chen",
                "Y. Hu",
                "P. Tang",
                "F. Ren"
            ],
            "title": "Bidirectional spatiotemporal feature learning with multiscale evaluation for video anomaly detection",
            "venue": "IEEE Trans. Circuits Syst. Video Technol., vol. 32, no. 12, pp. 8285\u20138296, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Yao",
                "M. Xu",
                "Y. Wang",
                "D.J. Crandall",
                "E.M. Atkins"
            ],
            "title": "Unsupervised traffic accident detection in first-person videos",
            "venue": "Proc. IEEE Int. Conf. Intell. Rob. Syst., 2019, pp. 273\u2013280.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Yao",
                "X. Wang",
                "M. Xu",
                "Z. Pu",
                "Y. Wang",
                "E. Atkins",
                "D.J. Crandall"
            ],
            "title": "Dota: Unsupervised detection of traffic anomaly in driving videos",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 45, no. 1, pp. 444\u2013459, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "D. Gong",
                "L. Liu",
                "V. Le",
                "B. Saha",
                "M.R. Mansour",
                "S. Venkatesh",
                "A. v. d. Hengel"
            ],
            "title": "Memorizing normality to detect anomaly: Memoryaugmented deep autoencoder for unsupervised anomaly detection",
            "venue": "Proc. IEEE Int. Conf. Comput. Vis., 2019, pp. 1705\u20131714.",
            "year": 2019
        },
        {
            "authors": [
                "J. Fang",
                "J. Qiao",
                "J. Bai",
                "H. Yu",
                "J. Xue"
            ],
            "title": "Traffic accident detection via self-supervised consistency learning in driving scenarios",
            "venue": "IEEE Trans. Intell. Transp. Syst., vol. 23, no. 7, pp. 9601\u20139614, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Lu",
                "K.M. Kumar",
                "S. s. Nabavi",
                "Y. Wang"
            ],
            "title": "Future frame prediction using convolutional vrnn for anomaly detection",
            "venue": "Proc. IEEE Int. Conf. Adv. Video Signal-Based Surveill., 2019, pp. 1\u20138.",
            "year": 2019
        },
        {
            "authors": [
                "J.T. Zhou",
                "L. Zhang",
                "Z. Fang",
                "J. Du",
                "X. Peng",
                "Y. Xiao"
            ],
            "title": "Attentiondriven loss for anomaly detection in video surveillance",
            "venue": "IEEE Trans. Circuits Syst. Video Technol., vol. 30, no. 12, pp. 4639\u20134647, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "S. Li",
                "J. Fang",
                "H. Xu",
                "J. Xue"
            ],
            "title": "Video frame prediction by deep multi-branch mask network",
            "venue": "IEEE Trans. Circuits Syst. Video Technol., vol. 31, no. 4, pp. 1283\u20131295, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M. Hasan",
                "J. Choi",
                "J. Neumann",
                "A.K. Roy-Chowdhury",
                "L.S. Davis"
            ],
            "title": "Learning temporal regularity in video sequences",
            "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recogn., 2016, pp. 733\u2013742.",
            "year": 2016
        },
        {
            "authors": [
                "Y. Yao",
                "M. Xu",
                "C. Choi",
                "D.J. Crandall",
                "E.M. Atkins",
                "B. Dariush"
            ],
            "title": "Egocentric vision-based future vehicle localization for intelligent driving assistance systems",
            "venue": "Proc. IEEE Int. Conf. Robot. Autom., 2019, pp. 9711\u20139717.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Li",
                "R. Liang",
                "W. Wei",
                "W. Wang",
                "J. Zhou",
                "X. Li"
            ],
            "title": "Temporal pyramid network with spatial-temporal attention for pedestrian trajectory prediction",
            "venue": "IEEE Trans. Netw. Sci. Eng., vol. 9, no. 3, pp. 1006\u20131019, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "R. Liang",
                "Y. Li",
                "X. Li",
                "Y. Tang",
                "J. Zhou",
                "W. Zou"
            ],
            "title": "Temporal pyramid network for pedestrian trajectory prediction with multi-supervision",
            "venue": "Proc. AAAI Conf. Art. Intel., vol. 35, 2021, pp. 2029\u20132037.",
            "year": 2021
        },
        {
            "authors": [
                "Y.S. Chong",
                "Y.H. Tay"
            ],
            "title": "Abnormal event detection in videos using spatiotemporal autoencoder",
            "venue": "Proc. Adv. Neural Networks, 2017, pp. 189\u2013196.",
            "year": 2017
        },
        {
            "authors": [
                "A. Adam",
                "E. Rivlin",
                "I. Shimshoni",
                "D. Reinitz"
            ],
            "title": "Robust real-time unusual event detection using multiple fixed-location monitors",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 30, no. 3, pp. 555\u2013560, 2008.",
            "year": 2008
        },
        {
            "authors": [
                "C. Piciarelli",
                "C. Micheloni",
                "G.L. Foresti"
            ],
            "title": "Trajectory-based anomalous event detection",
            "venue": "IEEE Trans. Circuits Syst. Video Technol., vol. 18, no. 11, pp. 1544\u20131554, 2008.",
            "year": 2008
        },
        {
            "authors": [
                "Y. Benezeth",
                "P.-M. Jodoin",
                "V. Saligrama",
                "C. Rosenberger"
            ],
            "title": "Abnormal events detection based on spatio-temporal co-occurences",
            "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recogn., 2009, pp. 2458\u20132465.",
            "year": 2009
        },
        {
            "authors": [
                "V. Mahadevan",
                "W. Li",
                "V. Bhalodia",
                "N. Vasconcelos"
            ],
            "title": "Anomaly detection in crowded scenes",
            "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recogn., 2010, pp. 1975\u20131981.",
            "year": 2010
        },
        {
            "authors": [
                "K.-W. Cheng",
                "Y.-T. Chen",
                "W.-H. Fang"
            ],
            "title": "Video anomaly detection and localization using hierarchical feature representation and gaussian process regression",
            "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recogn., 2015, pp. 2909\u20132917.",
            "year": 2015
        },
        {
            "authors": [
                "X. Mo",
                "V. Monga",
                "R. Bala",
                "Z. Fan"
            ],
            "title": "Adaptive sparse representations for video anomaly detection",
            "venue": "IEEE Trans. Circuits Syst. Video Technol., vol. 24, no. 4, pp. 631\u2013645, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "W. Luo",
                "W. Liu",
                "S. Gao"
            ],
            "title": "Remembering history with convolutional lstm for anomaly detection",
            "venue": "IEEE Int. Conf. on Multimedia & Expo, 2017, pp. 439\u2013444.",
            "year": 2017
        },
        {
            "authors": [
                "W. Luo",
                "W. Liu",
                "S. Gao"
            ],
            "title": "A revisit of sparse coding based anomaly detection in stacked rnn framework",
            "venue": "Proc. IEEE Int. Conf. Comput. Vis., 2017, pp. 341\u2013349.",
            "year": 2017
        },
        {
            "authors": [
                "M. Ravanbakhsh",
                "M. Nabi",
                "E. Sangineto",
                "L. Marcenaro",
                "C. Regazzoni",
                "N. Sebe"
            ],
            "title": "Abnormal event detection in videos using generative adversarial nets",
            "venue": "Proc. IEEE Int. Conf. Image Processing, 2017, pp. 1577\u20131581.",
            "year": 2017
        },
        {
            "authors": [
                "Y. Fan",
                "G. Wen",
                "D. Li",
                "S. Qiu",
                "M.D. Levine",
                "F. Xiao"
            ],
            "title": "Video anomaly detection and localization via gaussian mixture fully convolutional variational autoencoder",
            "venue": "Comput. Vis. Image Underst., vol. 195, pp. 102 920\u2013102 932, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "M.I. Georgescu",
                "R.T. Ionescu",
                "F.S. Khan",
                "M. Popescu",
                "M. Shah"
            ],
            "title": "A background-agnostic framework with adversarial training for abnormal event detection in video",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 44, no. 9, pp. 4505\u20134523, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "S. Zhang",
                "M. Gong",
                "Y. Xie",
                "A.K. Qin",
                "H. Li",
                "Y. Gao",
                "Y.- S. Ong"
            ],
            "title": "Influence-aware attention networks for anomaly detection in surveillance videos",
            "venue": "IEEE Trans. Circuits Syst. Video Technol., vol. 32, no. 8, pp. 5427\u20135437, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "T.-N. Nguyen",
                "J. Meunier"
            ],
            "title": "Anomaly detection in video sequence with appearance-motion correspondence",
            "venue": "Proc. IEEE Int. Conf. Comput. Vis., 2019, pp. 1273\u20131283.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Zhao",
                "B. Deng",
                "C. Shen",
                "Y. Liu",
                "H. Lu",
                "X.-S. Hua"
            ],
            "title": "Spatiotemporal autoencoder for video anomaly detection",
            "venue": "Proc. ACM Int. Conf. Multi., 2017, p. 1933\u20131941.",
            "year": 2017
        },
        {
            "authors": [
                "M. Ye",
                "X. Peng",
                "W. Gan",
                "W. Wu",
                "Y. Qiao"
            ],
            "title": "Anopcn: Video anomaly detection via deep predictive coding network",
            "venue": "Proc. ACM Int. Conf. Multi., 2019, p. 1805\u20131813.",
            "year": 2019
        },
        {
            "authors": [
                "X. Zeng",
                "Y. Jiang",
                "W. Ding",
                "H. Li",
                "Y. Hao",
                "Z. Qiu"
            ],
            "title": "A hierarchical spatio-temporal graph convolutional neural network for anomaly detection in videos",
            "venue": "IEEE Trans. Circuits Syst. Video Technol., vol. 33, no. 1, pp. 200\u2013212, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "Y. Tang",
                "L. Zhao",
                "S. Zhang",
                "C. Gong",
                "G. Li",
                "J. Yang"
            ],
            "title": "Integrating prediction and reconstruction for anomaly detection",
            "venue": "Pattern Recognit. Lett., vol. 129, pp. 123\u2013130, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "N.-C. Ristea",
                "N. Madan",
                "R.T. Ionescu",
                "K. Nasrollahi",
                "F.S. Khan",
                "T.B. Moeslund",
                "M. Shah"
            ],
            "title": "Self-supervised predictive convolutional attentive block for anomaly detection",
            "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recogn., 2022, pp. 13 576\u201313 586.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Yuan",
                "J. Fang",
                "Q. Wang"
            ],
            "title": "Incrementally perceiving hazards in driving",
            "venue": "Neurocomputing, vol. 282, pp. 202\u2013217, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "J. Weston",
                "S. Chopra",
                "A. Bordes"
            ],
            "title": "Memory networks",
            "venue": "Proc. Int. Conf. Learn. Represent., 2015.",
            "year": 2015
        },
        {
            "authors": [
                "Q. Wu",
                "Z. Lan",
                "K. Qian",
                "J. Gu",
                "A. Geramifard",
                "Z. Yu"
            ],
            "title": "Memformer: A memory-augmented transformer for sequence modeling",
            "venue": "Proc. Find. Assoc. Comput. Linguist.: AACL-IJCNLP, 2022, pp. 308\u2013318.",
            "year": 2022
        },
        {
            "authors": [
                "H. Park",
                "J. Noh",
                "B. Ham"
            ],
            "title": "Learning memory-guided normality for anomaly detection",
            "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recogn., 2020, pp. 14 372\u201314 381.",
            "year": 2020
        },
        {
            "authors": [
                "E. Ilg",
                "N. Mayer",
                "T. Saikia",
                "M. Keuper",
                "A. Dosovitskiy",
                "T. Brox"
            ],
            "title": "Flownet 2.0: Evolution of optical flow estimation with deep networks",
            "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recogn., 2017.",
            "year": 2017
        },
        {
            "authors": [
                "K. He",
                "G. Gkioxari",
                "P. Dollar",
                "R. Girshick"
            ],
            "title": "Mask r-cnn",
            "venue": "Proc. IEEE Int. Conf. Comput. Vis., 2017.",
            "year": 2017
        },
        {
            "authors": [
                "A. Vaswani",
                "N. Shazeer",
                "N. Parmar",
                "J. Uszkoreit",
                "L. Jones",
                "A.N. Gomez",
                "\u0141. Kaiser",
                "I. Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Proc. Adv. Neural Inf. Process. Syst., 2017, pp. 5998\u20136008.",
            "year": 2017
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980, 2014.",
            "year": 2014
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014Traffic accident detection, Future object localization, Optical flow reconstruction, Transformer, Memory.\nI. INTRODUCTION\nIN recent years, autonomous driving technology [1], [2] andadvanced driver assistance systems [3], [4] have achieved rapid development, bringing great convenience to human travel. At the same time, the everyday incidence of traffic accidents continually motivates efforts to increase the safety of driving systems in natural driving scenarios, especially in the\nRongqin Liang, Yuanman Li, Yingxin Yi and Xia Li are with Guangdong Key Laboratory of Intelligent Information Processing, College of Electronics and Information Engineering, Shenzhen University, Shenzhen 518060, China (email: 1810262064@email.szu.edu.cn; yuanmanli@szu.edu.cn; 2210433112@email.szu.edu.cn; lixia@szu.edu.cn. Corresponding author: Yuanman Li).\nJiantao Zhou is with the State Key Laboratory of Internet of Things for Smart City, and also with the Department of Computer and Information Science, University of Macau. e-mail: jtzhou@um.edu.mo.\ndetection of traffic accidents. Traffic accident detection (TAD) aims to detect abnormal traffic patterns in driving videos. Accurate TAD methods assist to reduce traffic accidents, increase road safety, shorten traffic recovery times and so on.\nMany researchers in computer vision have investigated the detection of anomalous events from dashboard-mounted cameras [5]\u2013[8]. These methods can be mainly divided into supervised and unsupervised approaches. While supervised methods [8], [9] have recently made great progress, the longtailed distribution of driving events means that it may not be possible to collect all types of traffic accidents as training data, making it difficult to accurately detect traffic accidents outside the distribution of the dataset, potentially raising the risk of serious accidents. Therefore, to overcome the difficulty of modeling all possible driving events, we focus on exploring an unsupervised TAD approach in driving videos.\nWith the rapid development of deep learning technology, unsupervised video anomaly detection (VAD) methods [10]\u2013 [13] have significantly advanced in surveillance scenarios. However, in driving scenarios, high-speed moving vehicles cause videos with dynamically changing backgrounds, rendering these approaches not well-extended or even ineffective for driving scenarios [12], [14]. Existing unsupervised TAD methods mainly model the normal traffic pattern by building a single pretext task, i.e., appearance-based [15]\u2013[18] or futureobject-localization-based [19]\u2013[21] TAD methods, and treat observed events that deviate from the normal pattern as anomalies. Among them, appearance-based methods [6], [18], [22] focus on detecting differences between predicted or reconstructed video frames and natural frames, while future-objectlocalization-based methods [11], [12] aim to detect anomalous motion by computing the variance of observed objects in predicted positions. Despite significant advances in these single-pretext-task-based methods, accurate detection of traffic accidents remains challenging. First, when the camera moves rapidly in the driving scene, appearance-based methods are easily disturbed by many factors such as dynamic background and illumination change, which may lead to misjudgment of TAD models. Second, although future-object-localization based methods avoid the difficulty of predicting whole frames, they generally fail to detect traffic accidents involving the egovehicle but not involving other objects (e.g., out of control of the ego-vehicle) due to their inability to capture appearance changes of video frames. Therefore, how to accurately detect both ego-involved (i.e., traffic accidents involving the egoar X iv :2 30 7. 14 57\n5v 1\n[ cs\n.C V\n] 2\n7 Ju\nl 2 02\n3\n2 vehicle) and non-ego (i.e., traffic accidents involving observed objects) accidents is very important for TAD in driving videos. Besides, traffic accident detection is essentially about detecting outliers that distinguish them from normal traffic patterns. Therefore, it is important to model normal traffic patterns and improve the sensitivity to abnormal patterns for accurate traffic accident detection.\nIn this work, we argue that collaborating on optical flow reconstruction and future object localization tasks helps to more accurately detect both ego-involved and non-ego accidents. First, optical flow characterizes the appearance changes of video frames, which helps to detect ego-involved accidents. Second, accurate future object localization helps to detect abnormal object motion, which promotes the detection of nonego accidents. Additionally, optical flow in driving scenes also reflects the ego motion of the dashboard-mounted camera, which helps to better model the motion states of observed objects. Furthermore, the motion state of observed objects reflects the local motion cues of video frames, which is potentially beneficial to modeling the appearance changes.\nTo fulfill the insights mentioned above, we propose a novel memory-augmented multi-task collaborative framework (MAMTCF) for unsupervised TAD in driving videos. First, we propose an unsupervised TAD framework that collaborates on optical flow reconstruction and future object localization tasks. Compared to existing TAD methods based on a single pretext task, our framework simultaneously modeling appearance changes and object motions in video frames, which helps detecting both ego-involved and non-ego accidents, achieving remarkable performance gains. In addition, we propose a memory-augmented motion representation (MAMR) mechanism to model the interrelation between different types of motion representations, and utilize the high-level features of normal traffic patterns stored in memory to reconstruct motion representations. This enlarges the distinction from representations of abnormal traffic patterns and makes traffic accidents easier to detect. Specifically, in the training phase, the MAMTCF is applied to train both the optical flow reconstruction and future object localization tasks. In the inference phase, we obtain an anomaly score for a driving video frame based on the reconstruction error of optical flow and the variance of predicted positions of observed objects. The main contributions of our work can be summarized as follows:\n1) We present a novel multi-task collaborative framework for unsupervised TAD. Compared to previous single-pretexttask-based TAD methods, our framework models both appearance changes and object motions in video frames by collaborating on optical flow reconstruction and future object localization tasks. This collaboration promotes the detection of both ego-involved and non-ego accidents, greatly improving the detection of traffic accidents. 2) We further propose a memory-augmented motion representation mechanism to fully explore the interrelation between different types of motion representations and reconstruct motion representations utilizing the high-level features of normal traffic patterns stored in memory. Such reconstructed motion representations help increase differences from anomalies, which benefits the detection\nof traffic accidents. 3) The proposed framework achieves state-of-the-art perfor-\nmance on the recently published large-scale benchmark, providing a promising direction for unsupervised traffic accident detection in driving videos.\nThe remainder of this paper is organized as follows. Section II gives a brief review of related works. Section III details our proposed MAMTCF for traffic accident detection in driving videos. Extensive experimental results are presented in Section IV, and we finally draw a conclusion in Section V."
        },
        {
            "heading": "II. RELATED WORKS",
            "text": "A. Video Anomaly Detection (VAD) in Surveillance Videos\nAnomaly detection in surveillance videos aims to detect abnormal events occurring in the surveillance perspective. The main difference between VAD and TAD is that the background in surveillance videos is fixed, while the background in dashcam videos changes dynamically.\nTraditional VAD methods [23]\u2013[28] mainly extract handcrafted features, followed by normality modeling to detect anomalies. For instance, Adam et al. [23] designed multiple local, low-level feature (e.g., optical flow) monitors to detect abnormal events in the scene. Although traditional VAD methods demonstrate the importance of modeling normality, they rely on carefully handcrafted features and struggle to robustly handle various abnormal events.\nWith the rapid development of deep neural networks, researchers have recently proposed many deep-learning-based VAD methods. Among them, reconstruction-based methods and prediction-based methods are the two main paradigms of VAD methods. Reconstruction-based methods [29]\u2013[34] typically train a generative model to reconstruct normal data, expecting the model to exhibit large reconstruction errors for abnormal data. For example, ConvLSTM-AE [29] integrated a convolutional neural network (ConvNet) and a convolutional long short-term memory network (ConvLSTM) with an autoencoder to learn the regularity of appearance and motion at ordinary moments. Ravanbakhsh et al. [31] employed generative adversarial networks (GANs) to learn an internal representation of scene normality to detect anomalies through reconstruction errors in appearance and motion representations. However, due to the potentially generalization capability of generative models, they can even reconstruct abnormal data, which can lead to the detection of some abnormal events being missed. To alleviate this problem, some researchers have proposed prediction-based VAD methods [15]\u2013[17], [35]\u2013[38], which mainly rely on prediction errors to evaluate anomalies. For instance, Conv-VRNN [15] introduced a sequence generation model based on Variational Autoencoder (CVAE) for future frame prediction with ConvLSTM. DMMNet [17] provided a flexible masking network for motion and appearance fusion on video frame prediction. However, prediction-based methods may not be robust to noise in real surveillance videos, which can lead to a rapid reduction in detection performance. Additionally, some approaches [7], [39], [40] attempt to combine the two paradigms of reconstruction and prediction. For\n3 instance, SSPCAB [40] integrated reconstruction-based functionality into a self-supervised predictive architecture building block. Although the aforementioned methods have achieved promising performance in the VAD task for surveillance videos, they are difficult to directly apply to traffic accident detection in a driving scenario. This is because the front and background of the video change dynamically due to the rapidly moving dashboard-mounted camera."
        },
        {
            "heading": "B. Traffic Accident Detection (TAD) in Dashcam Videos",
            "text": "Traditional traffic accident detection methods [5], [41] mainly extract handcrafted features and classify them using a Bayesian model. Among them, Yuan et al. [5] measured the abnormality of motion orientation and magnitude and fused the measurements using a Bayesian model to obtain a detection result. However, these methods are computationally complex, sensitive to handcrafted features, and lack robustness when applied to various traffic accidents.\nWith the advances in deep learning in computer vision, deep-learning-based traffic accident detection [6], [8], [9], [11], [12], [14] has attracted the attention of researchers. Existing TAD methods mainly detect traffic accidents in an unsupervised manner through a single pretext task, i.e., appearance-based or future-object-localization-based TAD methods. Among them, appearance-based approaches [6], [22] focus on detecting the difference between the predicted or reconstructed frame and the natural frame. For instance, Liu et al. [6] introduced appearance and motion constraints to facilitate the prediction of future frames for normal events and thus help identify anomalous events that do not conform to expectations. Another part of the works [11], [12] applied future object localization to detect abnormal events. For instance, Yao et al. [12] predict the future locations of objects over a short horizon in a driving scenario and then monitor prediction accuracy and consistency metrics as evidence of anomalies. Though previous single-pretext-taskbased methods have achieved promising performance, these approaches still have inherent limitations, e.g., appearancebased methods are prone to greatly reduce performance due to the rapid movement of dashboard-mounted cameras, while methods based on future object localization may fail to capture changes in appearance, which may lead to failure to detect ego-involved accidents. Recently, Fang et al. [14] attempted to collaborate frame prediction and future object localization tasks to absorb the merits of them. They proposed the SSCTAD framework to detect traffic accidents by analyzing the inconsistency of video frames, object locations, and scene spatial relationship structures between different frames of driving videos. Different from SSC-TAD, our framework collaborates on the tasks of optical flow reconstruction and future object localization. The proposed MAMR mechanism not only fully explores the inherent interrelation between different types of motion representations but is also more sensitive to abnormal patterns.\nBesides, some researchers have recently proposed supervised methods [8], [9] for TAD in driving videos. For instance, Zhou et al. [8] proposed a coarse-to-fine supervised TAD\nmethod based on spatio-temporal feature encoding with a multi-layer neural network. Sun et al. [9] proposed a traffic anomaly detection method based on cross-domain few-shot learning. However, since driving events follow a long-tailed distribution, it may not be possible to collect all types of traffic accidents. This may increase the risk of the supervised TAD approach to cope with unknown events. Therefore, in our work, we focus on exploring an unsupervised TAD approach to avoid collecting all types of traffic incidents as training data."
        },
        {
            "heading": "C. Memory Networks",
            "text": "Memory modules in neural networks [42], [43] have recently gained much attention as a type of read-write global memory. For instance, Memformer [43] utilizes an external dynamic memory to encode and retrieve past information for efficient sequence modeling. More recently, some works [7], [13], [44] have applied memory networks to the VAD task in surveillance videos. For example, Park et al. [44] proposed using a memory module with an update scheme, where items in the memory record the normal patterns of the training data. HF 2-VAD [7] introduces a multi-level memory module in an autoencoder with skip connections to memorize normal patterns for optical flow reconstruction, so that abnormal events can be sensitively identified through reconstruction errors. In this work, we make the first attempt to use a memory network to memorize normal traffic patterns for traffic accident detection in driving videos."
        },
        {
            "heading": "III. THE PROPOSED APPROACH: MAMTCF",
            "text": "The overall framework of MAMTCF model is illustrated in Fig. 1. It primarily consists of three components: 1) a feature extraction module to extract different types of motion representations; 2) a memory-augmented motion representation (MAMR) mechanism to collaborate on different types of motion representations and output the memory-augmented motion representations; 3) a multi-task decoder to reconstruct optical flow and predict future bounding boxes of objects. Note that the whole framework is trained on normal data only. In the inference phase, both the reconstruction and prediction errors are used for traffic accident detection.\nIn the following sections, we introduce the feature extraction module first, then the MAMR mechanism, followed by the multi-task decoder, and finally show how to use our model for traffic accident detection in driving videos."
        },
        {
            "heading": "A. Feature Extraction",
            "text": "Modeling changes in appearance of video frames as well as the motion states of observed objects in the scene is beneficial for detecting both ego-involved and non-ego traffic accidents. In our work, we first apply the feature extraction module to extract different types of motion representations. As shown in Fig. 1, we primarily extract representations of two types of inputs, i.e., the optical flow of video frames and the bounding boxes of objects in observed scenes.\nPractically, the optical flow of video frames not only contains the appearance changes of the scene but also reflects the\n4 Optical Flow Encoder\nLocation Encoder\nR oIPool\nC oncat\nOptical Flow Decoder\nLocation Decoder\nGT\nScores\nInference Training Add\nFeature Extraction\nMulti-Task Decoder\nMemory-Augmented Motion Representation\nFig. 1. The framework of our MAMTCF algorithm. MAMTCF primarily consists of a feature extraction module, a memory-augmented motion representation mechanism, and a multi-task decoder. 1) First, the optical flow of driving video frames and the bounding boxes of observed objects in the scenes are encoded into different types of motion representations through the feature extraction module; 2) then, a memory-augmented motion representation (MAMR) mechanism collaborates on different types of motion representations and outputs the memory-augmented motion representations; 3) the memory-augmented motion representations are passed through the multi-task decoder to reconstruct optical flow and predict future bounding boxes of objects, respectively; 4) finally, the anomaly score is obtained by fusing the reconstruction error of the optical flow and variance of predicted bounding boxes.\nego motion of the ego-vehicle. To capture both the appearance changes and the ego motion simultaneously, we define the representation of optical flow from frame t to t + 1 as the global motion at time step t, which can be written as:\nFt = \u03d5 (It; \u03a6f ) , (1)\nwhere \u03a6f is the parameter of the optical flow encoder \u03d5(\u00b7), It denotes the optical flow from frame t to t+1, which is obtained by pre-trained FlowNet 2.0 [45]. Without loss of generality, we adopt the encoder in HF 2-VAD [7] as the optical flow encoder \u03d5(\u00b7) in our experiments.\nMoreover, the bounding boxes of observed objects in the scene reflect the motion states of the objects. Thus, we define the representation of bounding boxes of objects in observed scenes as the object motion, which can be formulated as:\nXt = \u03c6 ( X1:N1:t ; \u03a6x ) , (2)\nwhere \u03a6x represents the parameter of the location encoder \u03c6(\u00b7), X1:N1:t denotes bounding boxes of N objects in observed t scenes, which is obtained by pre-trained Mask-RCNN [46]. In our experiments, the location encoder consists of a Fully Connected layer (FC) followed by a Gated Recurrent Neural network (GRU). In addition, to better perceive the motion states of objects, we further utilize the optical flow of objects to encode their current motion features. Specifically, similar to [11], [14], we extract the motion features of objects from the precomputed optical flow field using a region-of-interest pooling (RoIPool) operation with bilinear interpolation. The object motion Xt can be updated as follows:\nXt := Xt +MLP ( RoIPool ( It, X 1:N t ) ;\u03a6m ) , (3)\nwhere \u03a6m is the parameter of the Multilayer Perceptron (MLP). After obtaining the global motion Ft and the object motion Xt, our model can perceive both the appearance changes of video frames and the motion states of objects in the scene, enabling our method to detect both ego-involved and non-ego traffic accidents."
        },
        {
            "heading": "B. Memory-Augmented Motion Representation (MAMR) Mechanism for Collaborating Multi-Task",
            "text": "Collaborating on different types of motion representations helps to detect different types of traffic incidents, i.e., global motion focuses on appearance changes of video frames to detect ego-involved accidents, while object motion emphasizes the motion states of the observed objects to detect non-ego accidents. Intuitively, the simplest way to fuse these two types of motion representations is to concatenate them and decode them for different pretext tasks. However, on the one hand, global motion and object motion have an inherent interrelation. The global motion reflects the ego motion of the dashboardmounted camera, which helps to better model the object motion of the observed object and is beneficial to the detection of non-ego accidents. Correspondingly, object motion reflects the local motion cues of the video frames, which potentially contribute to characterize the global motion of video frames and thus promote the detection of ego-involved accidents. On the other hand, traffic accident detection is essentially about detecting outliers to distinguish them from normal traffic patterns. However, the potential generalization ability of the autoencoder leads to the possibility that it may learn shared patterns with abnormal traffic patterns [13], thereby blurring the distinctions between normal and abnormal motion representations. Consequently, this could lead to a failure to detect traffic accidents. Therefore, simply concatenating different types of motion representations is not a good strategy. This is also verified by experimental results in Table IV.\nIn our work, we specially design a memory-augmented motion representation mechanism to collaborate different types of motion representations. As shown in Fig. 2, our MAMR mechanism mainly composes an inter-motion layer and a memoryaugmented motion layer. The former models the interrelation between different types of motion representations while the latter reconstructs motion representations using normal traffic patterns stored in memory, thereby increasing differences from anomalies.\n1) Inter-motion layer: To model the interrelation between global motion and object motion, we first design a self-\n5 Inter-M otion Layer PE\nLinear Linear\nLinear Memory Memory-Augmented Motion Layer FeedForw ard N etw ork\nFig. 2. Illustration of the propose MAMR mechanism. The MAMR mechanism mainly consists of a inter-motion layer, memory-augmented motion layer, and feedforward network. The inter-motion layer explores the interrelation between global motion and object motion, while the memory-augmented motion layer retrieves high-level features of normal traffic patterns stored in memory and reconstructs motion representations with normal traffic patterns.\nattention-based inter-motion layer for better modeling appearance changes and motion states of observed objects in video frames. Specifically, as shown in Fig. 2, we first add a positional encoding to global motion and object motion to encode the relative relation of them, which can be formulated as:\nMt = Concat(Ft, Xt), Mt := Mt + PE(Mt), (4)\nwhere Mt is the concatenating representation of global motion Ft and object motion Xt, PE(\u00b7) denotes the widely used hardcoded position embedding strategy [47]. Then, we apply a self attention mechanism to model the interrelation between global motion and object motion, which can be formulated as:\nQm,Km, Vm = MtWQ,MtWK ,MtWV ,\nAt = MHAttn (Qm,Km) ,\nHt = Mt + LN(Softmax (At)Vm),\n(5)\nwhere WQ,WK , and WV are the parameters corresponding to the Query Qm, Key Km, and Value Vm of Mt, MHAttn(\u00b7) denotes the multi-head self-attention, LN(\u00b7) is the layer normalization, and Ht models the interrelation between global motion and object motion.\n2) Memory-augmented motion layer: Traffic accident detection is essentially about detecting outliers to distinguish them from normal traffic patterns. Intuitively, after modeling the interrelation between global motion and object motion, we can directly decode motion representations for multi-task. However, the autoencoder may learn some common features between normal and abnormal traffic patterns [13], thereby reducing the distinctions between normal and abnormal motion representations, which could result in missed detection of traffic accidents. Therefore, we further propose a cross-attentionbased memory-augmented motion layer. By utilizing the highlevel features of normal traffic patterns stored in memory to reconstruct the motion representations, we can augment the modeling of normal traffic patterns and improve sensitivity to abnormal patterns.\nSpecifically, as shown in Fig. 2, the memory is designed as a matrix M \u2208 RM\u00d7C that contains M slots of high-level features of normal traffic patterns with a fixed dimension C.\nNote that the memory M is randomly initialized at the beginning of training. For each input Ht, the memory-augmented motion layer needs to read the memory to retrieve relevant high-level features of normal traffic patterns. We leverage the cross-attention mechanism to achieve this function:\nQh,Kn, Vn = HtW \u2032 Q,MW \u2032 K ,MW \u2032 V ,\nAn = MHAttn (Qh,Kn) ,\nHn = Ht + LN(HardShrinkage (An)Vn),\n(6)\nwhere memory slot vectors are projected into Keys Kn and Values Vn with parameters W \u2032 K and W \u2032\nV , and the Ht is projected into the Query Qh with parameter W \u2032\nQ. Hn denotes the reconstructed motion representations. Note that, to make the stored high-level features of normal traffic patterns more representative, we utilize the hard shrinkage operation in [13] to promote the sparsity of the memory M:\na\u0302i = h (ai;\u03bb) = ReLU (ai \u2212 \u03bb) \u00b7 ai\n|ai \u2212 \u03bb|+ \u03b5 , (7)\nwhere \u03b5 is a very small positive scalar, ai \u2208 An, i \u2208 (1,M), \u03bb denotes the shrinkage threshold, and ReLU is the ReLU activation. The hard shrinkage operation encourages the model to reconstruct Ht with fewer but more relevant memory items, thus prompting the learning of high-level features that are more representative of normal traffic patterns in the memory. In addition, we minimize a sparsity regularizer on A\u0302n during training to promote the sparsity of the memory, similar to [13]:\nLs = M\u2211 i=1 \u2212a\u0302i \u00b7 log (a\u0302i), (8)\nwhere a\u0302i \u2208 A\u0302n and i \u2208 (1,M). Finally, we apply a feedforward network after the memoryaugmented motion layer, which can be formulated as:\nM \u2032\nt = Hn + LN (FFN (Hn;\u0398f )) , (9)\nwhere FFN(\u00b7) is a block of two fully connected layers, \u0398f denotes the parameters of FFN(\u00b7), and M \u2032t is the memoryaugmented motion representation. We can observe that our MAMR mechanism not only models the interrelation between different types of motion representations, but also utilizes the\n6 high-level features of normal traffic patterns to reconstruct motion representations, enhancing the sensitivity of our framework to abnormal traffic accidents."
        },
        {
            "heading": "C. Multi-Task Decoder for Traffic Accident Detection",
            "text": "After obtaining the memory-augmented motion representation, we can easily decode it for multiple tasks, i.e., optical flow reconstruction and future object localization. During the inference phase, unsupervised TAD is implemented based on the reconstruction error and prediction bias of multi-tasks.\n1) Optical flow reconstruction: As shown in Fig. 1, we utilize the multi-task decoder to reconstruct the optical flow of video frames and predict the future bounding boxes of observed objects in the scene. Specifically, the motion representation M \u2032\nt is passed through the optical flow decoder to reconstruct the optical flow, which can be formulated as:\nF \u2032 t , X \u2032 t = Split(M \u2032 t ),\nI \u2032 t = \u03d5dec(Ft);\u0398f ), (10)\nwhere Split(\u00b7) operation represents the partition of M \u2032t by channel, and F \u2032 t , X \u2032\nt denote the memory-augmented global motion and object motion, respectively. \u0398f is the parameters of the optical flow decoder \u03d5dec(\u00b7), and I \u2032\nt denotes the reconstructed optical flow. In our experiments, the decoder in HF 2-VAD [7] is utilized as the optical flow decoder. Note that, to better reconstruct the optical flow, we replace the convolutional layer of the penultimate layer in the decoder with the SSPCAB module [40]. This module integrates the reconstruction-based functionality into a self-supervised prediction architecture building block, as detailed in [40].\nIn fact, in driving scenarios, the moving velocity is one of the important factors leading to traffic accidents. Therefore, in our work, we not only supervise the consistency of the optical flow but also emphasize the consistency of the reconstructed motion. The loss function for the reconstructed optical flow is as follows:\nLf = Lmotion + Lrecon\n= \u221a (Ix \u2212 I \u2032x) 2 + ( Iy \u2212 I \u2032y )2 + |It \u2212 I \u2032 t |, (11)\nwhere (Ix, Iy) = It denotes the the offset of the image in x and y directions. The first term Lmotion of formula 11 emphasizes motion consistency, while the last term Lrecon supervises the reconstructed optical flow.\n2) Future object localization: For the future object localization task, we utilize the location decoder to recurrently decode the motion representation M \u2032\nt to future bounding boxes of objects, which can be written as:\nht = g(M \u2032 t ; \u03bem),\nht+1 = GRU (et, ht; \u03beg) ,\net+1 = MLP (ht+1; \u03bee),\nY\u0302 1:Nt+1 = MLP (ht+1; \u03bey) ,\n(12)\nwhere \u03be\u2217 is the learnable parameters, and the projection head g(\u00b7) consist of a linear layer followed by a ReLU activation layer. In addition, e0 is initialized with zeros. Y\u0302 1:Nt+1 denotes the\npredicted bounding boxes of objects at timestep t+ 1, where t ranges from t to T \u2212 1.\nFurther, the loss function for the predicted bounding box can be defined as:\nLmse = T\u2211 i=t+1 1 N N\u2211 n=1 d ( Y ni , Y\u0302 n i ) , (13)\nwhere d(\u00b7) calculates the Euclidean distance. Combined with the loss function of the reconstructed optical flow, our final loss is formulated as:\nLtotal = \u03bb1Lf + \u03bb2Lmse + \u03bb3Ls, (14)\nwhere \u03bb1, \u03bb2, and \u03bb3 are the coefficients of different losses. 3) Traffic accident detection: In this section, we present how our multi-task collaborative framework detects traffic accidents in driving videos during the inference phase.\nSpecifically, we fuse the traffic anomaly score based on motion consistency with the variance of the predicted bounding box. We calculate the reconstruction error of the motion and the variance of the predicted bounding box separately as follows:\nSe = \u221a (Ix \u2212 I \u2032x)2 + (Iy \u2212 I \u2032 y) 2,\nSl =max {1:N} (mean {bbox}\n((STD([|Y 1:Nt,t\u2212j \u2212 Y\u0302 1:Nt,t\u2212j |] j=\u03b4 j=1))))\n(15)\nwhere Se denotes the motion reconstruction error, and Sl represents the variance of \u03b4 bounding boxes at time t predicted from time t \u2212 1, t \u2212 2, ..., t \u2212 \u03b4. Se focuses on appearance changes of video frames, which helps to detect ego-involved traffic accidents. Sl emphasizes the motion behavior of objects in the scene, which is beneficial to detect non-ego accidents. Note that we calculate the variance by STD(\u00b7) for the topleft and bottom-right coordinates of the bounding box (i.e., bbox = xmin, ymin, xmin, ymin) and take the average as the prediction error for that object at time t. Given that objects in a scene with an accident have a relatively large corresponding prediction error, therefore, we take the maximum prediction error in the scene as the anomaly score of the driving video at time t.\nFurthermore, we fuse the motion reconstruction error Se with the variance of the bounding box Sl to obtain the final traffic anomaly score Sf , which can be formulated as:\nSf = Norm (\u03b1Norm (Se) + (1\u2212 \u03b1)Norm (Sl)) , (16)\nwhere Norm(\u00b7) denotes the max-min normalization, and \u03b1 \u2208 [0, 1] represents the fusion coefficient."
        },
        {
            "heading": "IV. EXPERIMENTS AND DISCUSSIONS",
            "text": "In this section, we evaluate the performance of our proposed method, which is performed on a platform with one NVIDIA 3090 GPU. All experiments were implemented using the PyTorch framework. Our source code and trained models will be publicly available upon acceptance.\n7\nA. Implementation Details\nIn the experiment, we resize the extracted optical flow of video frames to 64 \u00d7 64. Besides, we observe 5 frames of previous bounding boxes and predict 10 frames of future bounding boxes. The dimension of global motion and object motion is set to 512, and the size of the ROI pooling operation is set to 5 \u00d7 5. We set the layer of the MAMR mechanism to 3 (i.e., L = 3), and empirically set the head number of self-attention and cross-attention to 8. The slot number M is empirically set as 1000, and \u03bb in the hard shrinkage operation is set to 3/M . In the training phase, we empirically set the coefficients \u03bb1 and \u03bb2 of the final loss to be 1.0 and 1.0, respectively. Following prior works [7], [40], we set the coefficient \u03bb3 to 0.0002 in practice. We optimize the loss function (14) using Adam algorithm [48] with a batch size of 128, learning rate of 1e-4, betas of 0.9 and 0.999, weight decay of 5e-4, and train our framework for 100 epochs. During inference, we experimentally set the fusion coefficient \u03b1 to 0.4."
        },
        {
            "heading": "B. Dataset",
            "text": "For the sake of fairness, we follow prior works [8], [12] and evaluate our method on a recently publicly available dataset named DoTA [12]. DoTA is the first traffic anomaly video dataset that provides detailed spatio\u2013temporal annotations of anomalous objects for traffic accident detection in driving scenarios. It contains temporal, spatial, and categorical annotations of accidents for frames and objects in video. The DoTA consists of 4677 video clips with the resolution of 1280\u00d7720, the majority of which come from two YouTube channels that provide traffic accident videos for driver education purposes. It includes a variety of dashcam videos from different areas under different weather and lighting conditions. Each video is annotated with an anomaly start and end time, which separates it into three parts: the precursor, which is the normal video that precedes the anomaly, the accident frames, and the postaccident frames. Moreover, each anomaly participant is labeled with a track ID, and their bounding box is labeled from anomaly start to anomaly end. Besides, each video is assigned to one of 9 categories, which we summarize in Table I.\nIn our experiments, we ensure fairness of comparison by adopting a data partitioning consistent with [8], [12], where DoTA is randomly divided into 3,275 training videos and 1,402 testing videos. During training, we only use the precursor frames from each video."
        },
        {
            "heading": "C. Evaluation Setups",
            "text": "1) Metrics: Following prior works [12], [13], [30], we use Area under ROC curve (AUC) metrics to evaluate the performance of different traffic accident detection models.\nArea under ROC curve (AUC): Performance was evaluated by adopting the area under a standard frame-level receiver operating characteristic curve (ROC), with true positive rate (TPR) as the vertical axis and false positive rate (FPR) as the horizontal axis. The larger AUC prefers a better performance.\n2) Baselines: To verify the superiority of the proposed framework, we compare with the following state-of-the-art methods. ConvAE [18]: The spatio-temporal autoencoder-based model reconstructs the input and computes an anomaly score based on the reconstruction error. In the experiments, we compare two variants of ConvAE: one that reconstructs grayscale images and another that reconstructs optical flow. ConvLSTMAE [22]: A method combines CNN and LSTM to model spatial and temporal features. In the experiments, we also compare two variants of reconstructing grayscale images and reconstructing optical flow. AnoPred [6]: A VAD method based on frame prediction takes the first four continuous RGB frames as input and applies UNet to predict a future RGB frame. In the experiments, we compare two variants: one that predicts the whole RGB frame and another that predicts only the RGB image of the foreground object. FOL-STD [11]: A traffic accident detection method is based on future object localization. In our experiments, we compare three variants: using only the bounding boxes of objects as input, using the bounding boxes of objects and the corresponding optical flow as input, and using the bounding boxes of objects, the optical flow, and the ego motion as input. FOL-Ensemble [12]: An ensemble approach. In this method, two models are trained separately, i.e., the method FOLSTD [11] based on future object localization and the method AnoPred [6] based on frame prediction. The anomaly score of each object is then mapped to per pixel score in each frame by a late fusion strategy. FOL: Our approach without the optical flow reconstruction task and MAMR mechanism. FLOW: Our approach without future object localization tasks and MAMR mechanism.\n8"
        },
        {
            "heading": "D. Quantitative Results",
            "text": "1) Overall results: We compare our proposed MAMTCF with all the above methods in terms of AUC metrics. Table II summarizes the results of different algorithms and their corresponding inputs for each variant. Based on these results, we draw the following conclusions:\n\u2022 Overall, our method outperforms all the previous methods in terms of AUC. \u2022 Our method outperforms traffic accident detection methods based on a single pretext task, i.e., appearancebased and future-object-localization-based TAD methods. For instance, compared to ConvAE [18] which is the reconstruction-based method and has an AUC of 66.3, our method (76.6) achieves a relative improvement of 15.5% on AUC. Similarly, compared to AnPred [6] which is a prediction-based method and has an AUC of 67.5, our method achieves a relative improvement of 13.5%. Besides, our method achieves a relative improvement of 9.9% compared to the previous best future-objectlocalization-based method FOL-STD [11] (69.7). Furthermore, our method also outperforms the ensemble model FOL-Ensemble [12], which is the previous best method, by achieving a relative improvement of 4.9%. This demonstrate that our proposed multi-task collaborative framework to model both the appearance changes of video frames and the motion states of objects in the scene is indeed helpful for TAD in driving videos. \u2022 In addition, our method achieves a 10.9% relative improvement compared to FOL-STD [11] under the same input conditions, i.e., the extracted bounding box of the object and the optical flow of the video frame (Box + Flow). This indicates that our proposed multi-task collaborative framework can make better use of data and is more suitable for traffic accident detection tasks.\n2) Per-class results: To investigate the detection capability of our proposed framework for ego-involved and non-ego traffic accidents, we further compare the detection performance of different methods for these two types of accidents. Table III summarizes the detection AUC performance of different approaches for 9 categories of traffic accidents on the DoTA dataset. We present results for ego-involved accidents and non-\nego traffic accidents (marked by \u2217) separately. Additionally, we report the average performance of each method in the last column. In general, our method outperforms previous singlepretext-task-based methods in terms of average AUC for both ego-involved and non-ego accidents. For example, in non-ego accident detection, our method improves the average AUC by 12.9% compared to the previous best single-pretext-task-based method, FOL-STD. In ego-involved accident detection, our approach achieves a relative improvement of 6.6% compare to previous best future-object-localization-based method, FOLSTD, and a relative improvement of 4.1% compare to previous prediction-based method, AnPred. These results indicate that the proposed multi-task collaborative framework can indeed promote the detection of both ego-involved and non-ego traffic accidents. Notably, our method also greatly outperforms FOLEnsemble in terms of the average AUC of non-ego accident detection. Although FOL-Ensemble achieves comparable AUC performance on average to our method on ego-involved accident detection, it integrates two separately trained singlepretext-task-based TAD methods using a late fusion strategy. Such ensemble models undoubtedly increase the difficulty of traffic accident detection and potentially limit its application in autonomous driving and driver assistance systems. Besides, our method has better detection capacity in most accident categories, especially non-ego accidents. The main reason for this is that our proposed MAMR mechanism fully explores the interrelation between different types of motion representations and augments the motion representations by exploiting the high-level features of normal traffic patterns stored in memory, thus increasing the difference from anomalies.\n9 # 012 # 031 # 060 # 090# 050\n# 018 # 043 # 052 # 070 # 110\n# 006 # 027 # 047 # 060 # 084\n# 012 # 077 # 089# 025 # 039\n# 009 # 030 # 056 # 069 # 080\nFig. 3. The visualization of anomaly score curves for traffic accident detection of different variants on the DoTA dataset. The first row of each case shows the extracted video frames of the driving video, where the red boxes mark the object involved in the accident. The second rows show the anomaly score curves of different methods on the corresponding whole videos. Better viewed in color."
        },
        {
            "heading": "E. Qualitative Results",
            "text": "In this subsection, we provide visual examples in Fig. 3 to illustrate that our multi-task collaborative framework MAMTCF can better detect ego-involved and non-ego traffic accidents than single-pretext-task-based TAD methods.\nOverall, as shown in Fig. 3, our method performs better than the TAD methods based on a single pretext task,\ni.e., future object localization (FOL) and optical flow reconstruction (FLOW) tasks, when comparing the anomaly score curves of the different methods with the ground truth curves. Specifically, we show five traffic accident types as examples from top to bottom: a) The ego-vehicle collides with another vehicle moving laterally in the same direction. b) The egovehicle collides with an abnormal vehicle traveling in the\n10\nopposite direction. c) The ego-vehicle collides with another vehicle turning into the road. d) The other vehicle collides with another vehicle crossing the road. e) The ego-vehicle is out of control. From the above visualization results of different types of traffic accidents, we can summarize as follows. First, the FOL-based TAD method focus on detecting anomalies in the motion trajectory of observed objects. However, these types of approaches may not be able to detect ego-involved anomalies when there are no objects in the scene (as shown in the last row of Fig. 3). Second, the FLOW-based method concentrate on detecting changes in appearance of video frames but may cause false detection or missed detection of traffic accidents due to the rapid movement of dashboard-mounted cameras (as shown in 5 traffic accident types in Fig. 3). Finally, our memory-augmented multi-task collaborative framework can absorb the advantages of these two types of single pretext tasks based methods to detect both ego-involved and non-ego traffic accidents."
        },
        {
            "heading": "F. Ablation Investigation",
            "text": "In this subsection, we conduct ablation experiments to investigate how our proposed MAMTCF framework impacts the detection of traffic accident.\n1) Variants of our architecture: We evaluate each component of the proposed framework by performing a series of ablation experiments, including pretext tasks (i.e., FOL and FLOW tasks) and fusion strategies for collaborating multitasks. The experimental results are summarized in Table IV. Note that in our variant, the FOL-based TAD method uses Sl in formula (15) to calculate the final anomaly scores, while the FLOW-based method uses Se to compute the final anomaly scores. From the results reported in Table IV, we have the following conclusions. First, the multi-task based methods significantly outperform those based on a single pretext task, namely FOL and FLOW. The main reason is that the FOLbased method focuses on the motion states of observed objects and helps detect non-ego accidents, while the FLOW-based method focuses on appearance changes and helps detect egoinvolved accidents. Therefore, combining the two pretext tasks promotes to detect both types of traffic accidents. Second, modeling the interrelation between global motion and object motion is beneficial for modeling both appearance changes in video frames and motion states of observed objects. By using Transformers instead of simple concatenation, we can observe further improvements in the performance of traffic accident detection. Third, the best fusion strategy for collaborating multi-tasks is our proposed MAMR mechanism. This is because we further augment the motion representation with high-level features of normal traffic patterns stored in memory, thus increasing the difference from anomalies.\n2) Hyperparameters analysis: To ensure the rationality of our approach\u2019s hyperparameter settings, we further conduct ablation experiments on the number M of memory slots, the threshold \u03bb for hard shrinkage operations, and the layer L of MAMR mechanism. Fig. 4 illustrates the results of hyperparameter experiments for memory in the MAMR mechanism. To investigate the robustness of the proposed method to memory size M , we conduct experiments with different numbers of\nmemory slots, and the corresponding AUC performances are shown in Fig. (a). From the results, we can conclude that our method is robust to changes in memory slot size and achieves good accident detection performance even with small memory sizes. With an increase in the number of memory slots, the performance of accident detection improves slightly, and best performance is achieved when M = 1000. However, when the number of slots continues to increase, the performance of accident detection slightly decreases. One reason for this is that the memory size is too large, which may causes the model to overfit. Fig. (b) shows the effect of the hard shrinkage operation on the MAMR mechanism at different threshold settings. Note that when the threshold \u03bb = 0, we apply the softmax operation instead of the hard shrinkage operation. Fig. (b) shows that the hard shrinkage operation performs slightly better than the softmax operation because it improves the sparsity of the memory, thus encouraging to reconstruct motion representations with fewer but more relevant memory items. Additionally, the best accident detection performance is achieved when \u03bb = 3/M . When \u03bb > 3/M , the memory items may be too sparse and result in a slight drop in performance. In addition, we conduct experiments on the layer L of MAMR mechanism to explore their impact on traffic accident detection. The experimental results are summarized in Table V. One observation from these experiments is that increasing the layer of MAMR mechanism not only increases the parameters of the model but may also compromise the performance of accident detection. Based on the experimental results, we finally adopted L = 3 in our experiments."
        },
        {
            "heading": "G. Disscusion",
            "text": "In this subsection, we discuss some limitations of our approach. First, we experimentally find that our method may fail to accurately detect traffic accidents in scenarios with small changes in the motion of ego-vehicle. Fig. 5 shows several cases where our approach fails. Specifically, the first scenario\n11\nshows the ego-vehicle colliding with a stationary vehicle and coming to a stop. The second scenario shows the ego-vehicle colliding with a pedestrian on the road and continuing to move. The third scenario shows the ego-vehicle colliding with an oncoming vehicle and then coming to a stop. Analyzing the anomaly score curve in Fig 5, we can attribute the main reason why our method fails in the above scenario to two factors. On the one hand, object detection algorithms may fail in some scenarios (e.g., pedestrians appearing suddenly in the dark or bright light from the oncoming vehicle), which can invalidate future object localization algorithms. On the other hand, due to the slow movement of the vehicle or complex illumination in the dark, the optical flow of video frames changes little (i.e., the appearance changes are not obvious), which leads to the failure of TAD method based on optical flow reconstruction to detect traffic accidents. This also explains the poor performance of our method on detecting ego-involved ST, OC, and VP accident categories in Table III. Although our method outperforms the single-pretext-task-based TAD methods in most scenarios, it is clear that more efforts are needed by the community in the future to explore a general traffic accident detection method for the above scenarios.\nIn addition, most existing unsupervised traffic accident detection methods (including ours) are based on a two-stage strategy. In the first stage, trained models are used to extract features such as optical flow or bounding boxes of objects. The second stage then trains the pretext task for traffic accident\ndetection. This two-stage approach not only increases the time required for accident detection but also suffers from the error accumulation problem, which limits the application of traffic accident detection algorithms. Therefore, in the future, we will focus on exploring one-stage unsupervised traffic accident detection methods."
        },
        {
            "heading": "V. CONCLUSION",
            "text": "In this paper, we have proposed a novel memory-augmented multi-task collaborative framework for unsupervised traffic accident detection in driving videos. Different from previous TAD methods based on a single pretext task, our method collaborates optical flow reconstruction with future object localization tasks to better detect both ego-involved and nonego accidents. Furthermore, the proposed MAMR mechanism fully explores the interrelation between different types of motion representations, and augments the motion representations with high-level features of normal traffic patterns stored in memory, thus broadening the distinction from anomalies. Both quantitative and qualitative experimental results on recently published large-scale dataset have demonstrated the superiority of our approach in various situations."
        }
    ],
    "title": "A Memory-Augmented Multi-Task Collaborative Framework for Unsupervised Traffic Accident Detection in Driving Videos",
    "year": 2023
}