{
    "abstractText": "Recent work in visual representation learning for robotics demonstrates the viability of learning from large video datasets of humans performing everyday tasks. Leveraging methods such as masked autoencoding and contrastive learning, these representations exhibit strong transfer to policy learning for visuomotor control. But, robot learning encompasses a diverse set of problems beyond control including grasp affordance prediction, language-conditioned imitation learning, and intent scoring for human-robot collaboration, amongst others. First, we demonstrate that existing representations yield inconsistent results across these tasks: masked autoencoding approaches pick up on low-level spatial features at the cost of highlevel semantics, while contrastive learning approaches capture the opposite. We then introduceVoltron, a framework for languagedriven representation learning from human videos and associated captions. Voltron trades off language-conditioned visual reconstruction to learn low-level visual patterns, and visually-grounded language generation to encode high-level semantics. We also construct a new evaluation suite spanning five distinct robot learning problems \u2013 a unified platform for holistically evaluating visual representations for robotics. Through comprehensive, controlled experiments across all five problems, we find thatVoltron\u2019s languagedriven representations outperform the prior state-of-the-art, especially on targeted problems requiring higher-level features.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Siddharth Karamcheti"
        },
        {
            "affiliations": [],
            "name": "Suraj Nair"
        },
        {
            "affiliations": [],
            "name": "Annie Chen"
        },
        {
            "affiliations": [],
            "name": "Thomas Kollar"
        },
        {
            "affiliations": [],
            "name": "Chelsea Finn"
        },
        {
            "affiliations": [],
            "name": "Dorsa Sadigh"
        },
        {
            "affiliations": [],
            "name": "Percy Liang"
        }
    ],
    "id": "SP:cd313dec287c7f94b6a59f1c7fb378778465b1cc",
    "references": [
        {
            "authors": [
                "Armen Aghajanyan",
                "Bernie Huang",
                "Candace Ross",
                "Vladimir Karpukhin",
                "Hu Xu",
                "Naman Goyal",
                "Dmytro Okhonko",
                "Mandar Joshi",
                "Gargi Ghosh",
                "Mike Lewis",
                "Luke Zettlemoyer."
            ],
            "title": "CM3: A Causal Masked Multimodal Model of the Internet",
            "venue": "arXiv preprint arXiv:2201.07520 (2022).",
            "year": 2022
        },
        {
            "authors": [
                "Sichun Xu",
                "Mengyuan Yan."
            ],
            "title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances",
            "venue": "arXiv preprint arXiv:2204.01691 (2022).",
            "year": 2022
        },
        {
            "authors": [
                "Nematzadeh",
                "Sahand Sharifzadeh",
                "Mikolaj Binkowski",
                "Ricardo Barreira",
                "Oriol Vinyals",
                "Andrew Zisserman",
                "Karen Simonyan."
            ],
            "title": "Flamingo: a Visual Language Model for Few-Shot Learning",
            "venue": "arXiv preprint arXiv:2204.14198 (2022).",
            "year": 2022
        },
        {
            "authors": [
                "Dilip Arumugam",
                "Siddharth Karamcheti",
                "Nakul Gopalan",
                "Lawson L.S. Wong",
                "Stefanie Tellex."
            ],
            "title": "Accurately and Efficiently Interpreting Human-Robot Instructions of Varying Granularities",
            "venue": "Robotics: Science and Systems (RSS).",
            "year": 2017
        },
        {
            "authors": [
                "Shikhar Bahl",
                "Abhi Gupta",
                "Deepak Pathak."
            ],
            "title": "Human-toRobot Imitation in the Wild",
            "venue": "Robotics: Science and Systems (RSS).",
            "year": 2022
        },
        {
            "authors": [
                "Tirthankar Bandyopadhyay",
                "Kok Sung Won",
                "Emilio Frazzoli",
                "David Hsu",
                "Wee Sun Lee",
                "Daniela Rus."
            ],
            "title": "Intention-Aware Motion Planning",
            "venue": "InWorkshop for the Algorithmic Foundations of Robotics (WAFR).",
            "year": 2013
        },
        {
            "authors": [
                "Hangbo Bao",
                "Li Dong",
                "FuruWei."
            ],
            "title": "BEiT: BERT Pre-Training of Image Transformers",
            "venue": "International Conference on Learning Representations (ICLR).",
            "year": 2022
        },
        {
            "authors": [
                "Iz Beltagy",
                "Matthew E. Peters",
                "Arman Cohan."
            ],
            "title": "Longformer: The Long-Document Transformer",
            "venue": "arXiv preprint arXiv:2004.05150 (2020).",
            "year": 2020
        },
        {
            "authors": [
                "Elad Ben-Zaken",
                "Shauli Ravfogel",
                "Yoav Goldberg."
            ],
            "title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models",
            "venue": "Association for Computational Linguistics (ACL).",
            "year": 2022
        },
        {
            "authors": [
                "Jeannette Bohg",
                "AntonioMorales",
                "TamimAsfour",
                "andDanica Kragic"
            ],
            "title": "Data-Driven Grasp Synthesis\u2014A Survey",
            "venue": "IEEE Transactions on Robotics (T-RO)",
            "year": 2013
        },
        {
            "authors": [
                "Taori",
                "Armin W. Thomas",
                "Florian Tram\u00e8r",
                "Rose E. Wang",
                "William Wang",
                "Bohan Wu",
                "Jiajun Wu",
                "Yuhuai Wu",
                "Sang Michael Xie",
                "Michihiro Yasunaga",
                "Jiaxuan You",
                "Matei Zaharia",
                "Michael Zhang",
                "Tianyi Zhang",
                "Xikun Zhang",
                "Yuhui Zhang",
                "Lucia Zheng",
                "Kaitlyn Zhou",
                "Percy Liang"
            ],
            "title": "On the Opportunities and Risks",
            "year": 2021
        },
        {
            "authors": [
                "Fran\u00e7ois Chaumette",
                "Seth A. Hutchinson."
            ],
            "title": "Visual servo control",
            "venue": "I. Basic approaches. IEEE Robotics & Automation Magazine 13 (2006), 82\u201390.",
            "year": 2006
        },
        {
            "authors": [
                "Annie S. Chen",
                "Suraj Nair",
                "Chelsea Finn."
            ],
            "title": "Learning Generalizable Robotic Reward Functions from \"In-The-Wild\" Human Videos",
            "venue": "Robotics: Science and Systems (RSS).",
            "year": 2021
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton."
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "International Conference on Machine Learning (ICML). 1597\u20131607.",
            "year": 2020
        },
        {
            "authors": [
                "Meier-Hellstern",
                "D. Eck",
                "J. Dean",
                "Slav Petrov",
                "Noah Fiedel."
            ],
            "title": "PaLM: Scaling Language Modeling with Pathways",
            "venue": "arXiv (2022).",
            "year": 2022
        },
        {
            "authors": [
                "Marc G. Bellemare"
            ],
            "title": "DeepMDP: Learning Continuous La",
            "year": 2019
        },
        {
            "authors": [
                "P. Abbeel"
            ],
            "title": "Multimodal Masked Autoencoders Learn",
            "year": 2022
        },
        {
            "authors": [
                "Sato",
                "Jianbo Shi",
                "Mike Zheng Shou",
                "Antonio Torralba",
                "Lorenzo Torresani",
                "Mingfei Yan",
                "Jitendra Malik."
            ],
            "title": "Ego4D: Around theWorld in 3,000 Hours of Egocentric Video",
            "venue": "Computer Vision and Pattern Recognition (CVPR).",
            "year": 2022
        },
        {
            "authors": [
                "Danijar Hafner",
                "Timothy P. Lillicrap",
                "Jimmy Ba",
                "Mohammad Norouzi."
            ],
            "title": "Dream to Control: Learning Behaviors by Latent Imagination",
            "venue": "International Conference on Learning Representations (ICLR).",
            "year": 2020
        },
        {
            "authors": [
                "Kris K. Hauser."
            ],
            "title": "Recognition, prediction, and planning for assisted teleoperation of freeform tasks",
            "venue": "Autonomous Robots (AURO) (2012), 241\u2013254.",
            "year": 2012
        },
        {
            "authors": [
                "Kaiming He",
                "Xinlei Chen",
                "Saining Xie",
                "Yanghao Li",
                "Piotr Doll\u00e1r",
                "Ross B. Girshick."
            ],
            "title": "Masked Autoencoders Are Scalable Vision Learners",
            "venue": "Computer Vision and Pattern Recognition (CVPR).",
            "year": 2022
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun."
            ],
            "title": "Deep Residual Learning for Image Recognition",
            "venue": "Computer Vision and Pattern Recognition (CVPR).",
            "year": 2016
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Kevin Gimpel."
            ],
            "title": "Gaussian Error Linear Units (GELUs)",
            "venue": "arXiv preprint arXiv:1606.08415 (2016).",
            "year": 2016
        },
        {
            "authors": [
                "Guy Hoffman",
                "Cynthia Breazeal."
            ],
            "title": "Cost-Based Anticipatory Action Selection for Human\u2013Robot Fluency",
            "venue": "IEEE Transactions on Robotics (T-RO) 23 (2007), 952\u2013961.",
            "year": 2007
        },
        {
            "authors": [
                "Neil Houlsby",
                "Andrei Giurgiu",
                "Stanislaw Jastrzebski",
                "Bruna Morrone",
                "Quentin de Laroussilhe",
                "Andrea Gesmundo",
                "Mona Attariyan",
                "Sylvain Gelly."
            ],
            "title": "Parameter-Efficient Transfer Learning for NLP",
            "venue": "arXiv (2019).",
            "year": 2019
        },
        {
            "authors": [
                "Edward J. Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Weizhu Chen."
            ],
            "title": "LoRA: LowRank Adaptation of Large Language Models",
            "venue": "arXiv preprint arXiv:2106.09685 (2021).",
            "year": 2021
        },
        {
            "authors": [
                "Sergey Ioffe",
                "Christian Szegedy."
            ],
            "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
            "venue": "International Conference on Machine Learning (ICML). 448\u2013456.",
            "year": 2015
        },
        {
            "authors": [
                "Andrew Jaegle",
                "Felix Gimeno",
                "Andrew Brock",
                "Andrew Zisserman",
                "Oriol Vinyals",
                "Jo\u00e3o Carreira."
            ],
            "title": "Perceiver: General Perception with Iterative Attention",
            "venue": "International Conference on Machine Learning (ICML).",
            "year": 2021
        },
        {
            "authors": [
                "Stephen James",
                "Andrew J. Davison."
            ],
            "title": "Q-Attention: Enabling Efficient Learning for Vision-based Robotic Manipulation",
            "venue": "IEEE Robotics and Automation Letters (RA-L) 7 (2022), 1612\u20131619.",
            "year": 2022
        },
        {
            "authors": [
                "Stephen James",
                "KentaroWada",
                "Tristan Laidlow",
                "Andrew J. Davison."
            ],
            "title": "Coarse-to-Fine Q-Attention: Efficient Learning for Visual Robotic Manipulation via Discretisation",
            "venue": "Computer Vision and Pattern Recognition (CVPR). 13729\u201313738.",
            "year": 2022
        },
        {
            "authors": [
                "Shervin Javdani",
                "Henny Admoni",
                "Stefania Pellegrinelli",
                "Siddhartha S Srinivasa",
                "J Andrew Bagnell."
            ],
            "title": "Shared autonomy via hindsight optimization for teleoperation and teaming",
            "venue": "International Journal of Robotics Research (IJRR) 37 (2018), 717\u2013742.",
            "year": 2018
        },
        {
            "authors": [
                "Rico Jonschkowski",
                "Oliver Brock."
            ],
            "title": "Learning state representations with robotic priors",
            "venue": "Autonomous Robots 39 (2015), 407\u2013428.",
            "year": 2015
        },
        {
            "authors": [
                "Siddharth Karamcheti",
                "Laurel Orr",
                "Jason Bolton",
                "Tianyi Zhang",
                "Karan Goel",
                "Avanika Narayan",
                "Rishi Bommasani",
                "Deepak Narayanan",
                "Tatsunori Hashimoto",
                "Dan Jurafsky",
                "Christopher D. Manning",
                "Christopher Potts",
                "Christopher R\u00e9",
                "Percy Liang"
            ],
            "title": "Mistral - A Journey towards Reproducible Language",
            "year": 2021
        },
        {
            "authors": [
                "Siddharth Karamcheti",
                "Megha Srivastava",
                "Percy Liang",
                "Dorsa Sadigh."
            ],
            "title": "LILA: Language-Informed Latent Actions",
            "venue": "Conference on Robot Learning (CoRL).",
            "year": 2021
        },
        {
            "authors": [
                "Apoorv Khandelwal",
                "Luca Weihs",
                "Roozbeh Mottaghi",
                "Aniruddha Kembhavi."
            ],
            "title": "Simple but Effective: CLIP Embeddings for Embodied AI",
            "venue": "Computer Vision and Pattern Recognition (CVPR). 14809\u201314818.",
            "year": 2021
        },
        {
            "authors": [
                "Diederik Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "International Conference on Learning Representations (ICLR).",
            "year": 2015
        },
        {
            "authors": [
                "Ilya Kostrikov",
                "Denis Yarats",
                "Rob Fergus."
            ],
            "title": "Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels",
            "venue": "International Conference on Learning Representations (ICLR).",
            "year": 2021
        },
        {
            "authors": [
                "Michael Laskin",
                "Kimin Lee",
                "Adam Stooke",
                "Lerrel Pinto",
                "P. Abbeel",
                "A. Srinivas."
            ],
            "title": "Reinforcement Learning with Augmented Data",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS).",
            "year": 2020
        },
        {
            "authors": [
                "Juho Lee",
                "Yoonho Lee",
                "Jungtaek Kim",
                "Adam R. Kosiorek",
                "Seungjin Choi",
                "Yee Whye Teh."
            ],
            "title": "Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks",
            "venue": "International Conference on Machine Learning (ICML).",
            "year": 2018
        },
        {
            "authors": [
                "S. Levine",
                "Chelsea Finn",
                "Trevor Darrell",
                "P. Abbeel."
            ],
            "title": "End-toEnd Training of Deep Visuomotor Policies",
            "venue": "Journal of Machine Learning Research (JMLR) 17 (2016).",
            "year": 2016
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C. Lawrence Zitnick."
            ],
            "title": "Microsoft COCO: Common objects in context",
            "venue": "European Conference on Computer Vision (ECCV). 740\u2013755.",
            "year": 2014
        },
        {
            "authors": [
                "Hao Liu",
                "Lisa Lee",
                "Kimin Lee",
                "Pieter Abbeel."
            ],
            "title": "InstructRL: Simple yet Effective Instruction-Following Agents with Multimodal Transformer",
            "venue": "arXiv preprint arXiv:2210.13431 (2022).",
            "year": 2022
        },
        {
            "authors": [
                "Jiasen Lu",
                "Dhruv Batra",
                "Devi Parikh",
                "Stefan Lee."
            ],
            "title": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS).",
            "year": 2019
        },
        {
            "authors": [
                "Jiasen Lu",
                "Christopher Clark",
                "Rowan Zellers",
                "Roozbeh Mottaghi",
                "Aniruddha Kembhavi."
            ],
            "title": "Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks",
            "venue": "International Conference on Learning Representations (ICLR).",
            "year": 2023
        },
        {
            "authors": [
                "Corey Lynch",
                "Pierre Sermanet."
            ],
            "title": "Grounding Language in Play",
            "venue": "arXiv preprint arXiv:2005.07648 (2020).",
            "year": 2020
        },
        {
            "authors": [
                "Yecheng Jason Ma",
                "Shagun Sodhani",
                "Dinesh Jayaraman",
                "Osbert Bastani",
                "Vikash Kumar",
                "Amy Zhang."
            ],
            "title": "VIP: Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training",
            "venue": "arXiv preprint arXiv:2210.00030 (2022).",
            "year": 2022
        },
        {
            "authors": [
                "Jeffrey Mahler",
                "Jacky Liang",
                "Sherdil Niyaz",
                "Michael Laskey",
                "Richard Doan",
                "Xinyu Liu",
                "Juan Aparicio Ojea",
                "Ken Goldberg"
            ],
            "title": "Dex-Net 2.0: Deep Learning to Plan Robust Grasps with Synthetic Point Clouds and Analytic Grasp Metrics",
            "venue": "In Robotics: Science and Systems (RSS)",
            "year": 2017
        },
        {
            "authors": [
                "Ajay Mandlekar",
                "Danfei Xu",
                "Josiah Wong",
                "Soroush Nasiriany",
                "Chen Wang",
                "Rohun Kulkarni",
                "Li Fei-Fei",
                "Silvio Savarese",
                "Yuke Zhu",
                "Roberto Mart\u00edn-Mart\u00edn."
            ],
            "title": "What Matters in Learning from Offline Human Demonstrations for Robot Manipulation",
            "venue": "Conference on Robot Learning (CoRL).",
            "year": 2021
        },
        {
            "authors": [
                "Dipendra K. Misra",
                "John Langford",
                "Yoav Artzi."
            ],
            "title": "Mapping Instructions and Visual Observations to Actions with Reinforcement Learning",
            "venue": "Empirical Methods in Natural Language Processing (EMNLP).",
            "year": 2017
        },
        {
            "authors": [
                "Suraj Nair",
                "Eric Mitchell",
                "Kevin Chen",
                "Brian Ichter",
                "Silvio Savarese",
                "Chelsea Finn."
            ],
            "title": "Learning Language-Conditioned Robot Behavior from Offline Data and Crowd-Sourced Annotation",
            "venue": "Conference on Robot Learning (CoRL).",
            "year": 2021
        },
        {
            "authors": [
                "Suraj Nair",
                "Aravind Rajeswaran",
                "Vikash Kumar",
                "Chelsea Finn",
                "Abhinav Gupta."
            ],
            "title": "R3M: A Universal Visual Representation for Robot Manipulation",
            "venue": "arXiv preprint arXiv:2203.12601 (2022).",
            "year": 2022
        },
        {
            "authors": [
                "Sharan Narang",
                "HyungWon Chung",
                "Yi Tay",
                "William Fedus",
                "Thibault F\u00e9vry",
                "Michael Matena",
                "Karishma Malkan",
                "Noah Fiedel",
                "Noam M. Shazeer",
                "Zhenzhong Lan",
                "Yanqi Zhou",
                "Wei Li",
                "NanDing",
                "JakeMarcus",
                "Adam Roberts",
                "Colin Raffel"
            ],
            "title": "Do Transformer Modifications Transfer Across Implementations and Applications",
            "year": 2021
        },
        {
            "authors": [
                "Jyothish Pari",
                "Nur Muhammad (Mahi) Shafiullah",
                "Sridhar Pandian Arunachalam",
                "Lerrel Pinto"
            ],
            "title": "The Surprising Effectiveness of Representation Learning for Visual Imitation",
            "venue": "In Robotics: Science and Systems (RSS)",
            "year": 2022
        },
        {
            "authors": [
                "Simone Parisi",
                "Aravind Rajeswaran",
                "Senthil Purushwalkam",
                "Abhinav Kumar Gupta."
            ],
            "title": "The Unsurprising Effectiveness of Pre-Trained Vision Models for Control",
            "venue": "arXiv preprint arXiv:2203.03580 (2022).",
            "year": 2022
        },
        {
            "authors": [
                "Ofir Press",
                "Noah A. Smith",
                "Mike Lewis."
            ],
            "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
            "venue": "International Conference on Learning Representations (ICLR).",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark",
                "Gretchen Krueger",
                "Ilya Sutskever."
            ],
            "title": "Learning Transferable Visual Models From Natural Language Supervision",
            "venue": "International Conference on",
            "year": 2021
        },
        {
            "authors": [
                "Ilija Radosavovic",
                "Tete Xiao",
                "Stephen James",
                "P. Abbeel",
                "Jitendra Malik",
                "Trevor Darrell."
            ],
            "title": "Real-World Robot Learning with Masked Visual Pre-training",
            "venue": "Conference on Robot Learning (CoRL).",
            "year": 2022
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "arXiv preprint arXiv:1910.10683 (2019).",
            "year": 2019
        },
        {
            "authors": [
                "Maithra Raghu",
                "Thomas Unterthiner",
                "Simon Kornblith",
                "Chiyuan Zhang",
                "Alexey Dosovitskiy."
            ],
            "title": "Do Vision Transformers See Like Convolutional Neural Networks",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS).",
            "year": 2021
        },
        {
            "authors": [
                "Machel Reid",
                "Yutaro Yamada",
                "Shixiang Shane Gu"
            ],
            "title": "Can Wikipedia Help Offline Reinforcement Learning",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "Victor Sanh",
                "Lysandre Debut",
                "Julien Chaumond",
                "Thomas Wolf."
            ],
            "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
            "venue": "arXiv preprint arXiv:1910.01108 (2019).",
            "year": 2019
        },
        {
            "authors": [
                "Ashutosh Saxena",
                "Justin Driemeyer",
                "A. Ng."
            ],
            "title": "Robotic Grasping of Novel Objects using Vision",
            "venue": "International Journal of Robotics Research (IJRR) 27 (2008), 157\u2013173.",
            "year": 2008
        },
        {
            "authors": [
                "Christoph Schuhmann",
                "Richard Vencu",
                "Romain Beaumont",
                "Robert Kaczmarczyk",
                "Clayton Mullis",
                "Aarush Katta",
                "Theo Coombes",
                "Jenia Jitsev",
                "Aran Komatsuzaki."
            ],
            "title": "LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs",
            "venue": "arXiv preprint arXiv:2111.02114 (2021).",
            "year": 2021
        },
        {
            "authors": [
                "Pierre Sermanet",
                "Corey Lynch",
                "Yevgen Chebotar",
                "Jasmine Hsu",
                "Eric Jang",
                "Stefan Schaal",
                "Sergey Levine."
            ],
            "title": "Time-Contrastive Networks: Self-Supervised Learning from Video",
            "venue": "International Conference on Robotics and Automation (ICRA). 1134\u20131141.",
            "year": 2018
        },
        {
            "authors": [
                "Rutav Shah",
                "Vikash Kumar."
            ],
            "title": "RRL: Resnet as representation for Reinforcement Learning",
            "venue": "International Conference on Machine Learning (ICML).",
            "year": 2021
        },
        {
            "authors": [
                "Dandan Shan",
                "Jiaqi Geng",
                "Michelle Shu",
                "David F. Fouhey."
            ],
            "title": "Understanding Human Hands in Contact at Internet Scale",
            "venue": "Computer Vision and Pattern Recognition (CVPR). 9866\u20139875.",
            "year": 2020
        },
        {
            "authors": [
                "Lin Shao",
                "Toki Migimatsu",
                "Q. Zhang",
                "Karen Yang",
                "Jeannette Bohg."
            ],
            "title": "Concept2Robot: Learning Manipulation Concepts from Instructions and Human Demonstrations",
            "venue": "Robotics: Science and Systems (RSS).",
            "year": 2020
        },
        {
            "authors": [
                "Piyush Sharma",
                "Nan Ding",
                "Sebastian Goodman",
                "Radu Soricut."
            ],
            "title": "Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning",
            "venue": "Association for Computational Linguistics (ACL).",
            "year": 2018
        },
        {
            "authors": [
                "Noam M. Shazeer."
            ],
            "title": "GLU Variants Improve Transformer",
            "venue": "arXiv preprint arXiv:2002.05202 (2020).",
            "year": 2020
        },
        {
            "authors": [
                "Mohit Shridhar",
                "Lucas Manuelli",
                "Dieter Fox."
            ],
            "title": "CLIPort: What and Where Pathways for Robotic Manipulation",
            "venue": "Conference on Robot Learning (CoRL).",
            "year": 2021
        },
        {
            "authors": [
                "Mohit Shridhar",
                "Lucas Manuelli",
                "Dieter Fox."
            ],
            "title": "PerceiverActor: A Multi-Task Transformer for Robotic Manipulation",
            "venue": "Conference on Robot Learning (CoRL).",
            "year": 2022
        },
        {
            "authors": [
                "Amanpreet Singh",
                "Ronghang Hu",
                "Vedanuj Goswami",
                "Guillaume Couairon",
                "Wojciech Galuba",
                "Marcus Rohrbach",
                "Douwe Kiela."
            ],
            "title": "FLAVA: A Foundational Language And Vision Alignment Model",
            "venue": "InComputer Vision and Pattern Recognition (CVPR). 15617\u2013 15629.",
            "year": 2022
        },
        {
            "authors": [
                "Laura Smith",
                "Nikita Dhawan",
                "Marvin Zhang",
                "P. Abbeel",
                "Sergey Levine."
            ],
            "title": "AVID: Learning Multi-Stage Tasks via Pixel-Level Translation of Human Videos",
            "venue": "Robotics: Science and Systems (RSS).",
            "year": 2020
        },
        {
            "authors": [
                "A. Srinivas",
                "Michael Laskin",
                "P. Abbeel."
            ],
            "title": "CURL: Contrastive Unsupervised Representations for Reinforcement Learning",
            "venue": "International Conference on Machine Learning (ICML).",
            "year": 2020
        },
        {
            "authors": [
                "Krishna Srinivasan",
                "Karthik Raman",
                "Jiecao Chen",
                "Michael Bendersky",
                "Marc Najork."
            ],
            "title": "WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning",
            "venue": "ACM Special Interest Group on Information Retreival (SIGIR).",
            "year": 2021
        },
        {
            "authors": [
                "Simon Stepputtis",
                "J. Campbell",
                "Mariano Phielipp",
                "Stefan Lee",
                "Chitta Baral",
                "H.B. Amor."
            ],
            "title": "Language-Conditioned Imitation Learning for Robot Manipulation Tasks",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS).",
            "year": 2020
        },
        {
            "authors": [
                "Robin Strudel",
                "Ricardo Garcia Pinel",
                "Ivan Laptev",
                "Cordelia Schmid."
            ],
            "title": "Segmenter: Transformer for Semantic Segmentation",
            "venue": "International Conference on Computer Vision (ICCV). 7242\u20137252.",
            "year": 2021
        },
        {
            "authors": [
                "Jianlin Su",
                "Yu Lu",
                "Shengfeng Pan",
                "Bo Wen",
                "Yunfeng Liu."
            ],
            "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding",
            "venue": "arXiv preprint arXiv:2104.09864 (2021).",
            "year": 2021
        },
        {
            "authors": [
                "Stefanie Tellex",
                "Thomas Kollar",
                "Steven Dickerson",
                "Matthew R Walter",
                "Ashis Gopal Banerjee",
                "Seth J Teller",
                "Nicholas Roy."
            ],
            "title": "Understanding Natural Language Commands for Robotic Navigation andMobile Manipulation",
            "venue": "InAssociation for the Advancement of Artificial Intelligence (AAAI).",
            "year": 2011
        },
        {
            "authors": [
                "Zhan Tong",
                "Yibing Song",
                "Jue Wang",
                "Limin Wang."
            ],
            "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for SelfSupervised Video Pre-Training",
            "venue": "InAdvances in Neural Information Processing Systems (NeurIPS).",
            "year": 2022
        },
        {
            "authors": [
                "Hugo Touvron",
                "Matthieu Cord",
                "Alexandre Sablayrolles",
                "Gabriel Synnaeve",
                "Herv\u00e9 J\u00e9gou."
            ],
            "title": "Going deeper with Image Transformers",
            "venue": "International Conference on Computer Vision (ICCV). 32\u201342.",
            "year": 2021
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention Is All You Need",
            "venue": "arXiv preprint arXiv:1706.03762 (2017).",
            "year": 2017
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Yinggong Zhao",
                "Victoria Fossum",
                "David Chiang."
            ],
            "title": "Decoding with Large-Scale Neural Language Models Improves Translation",
            "venue": "Empirical Methods in Natural Language Processing (EMNLP). 1387\u20131392.",
            "year": 2013
        },
        {
            "authors": [
                "Ke-Jyun Wang",
                "Yun-Hsuan Liu",
                "Hung-Ting Su",
                "Jen-Wei Wang",
                "YuSiang Wang",
                "Winston H. Hsu",
                "Wen-Chin Chen."
            ],
            "title": "OCIDRef: A 3D Robotic Dataset With Embodied Language For Clutter Scene Grounding",
            "venue": "Association for Computational Linguistics (ACL).",
            "year": 2021
        },
        {
            "authors": [
                "Lee E. Weiss",
                "Arthur C. Sanderson",
                "Charles P. Neuman."
            ],
            "title": "Dynamic sensor-based control of robots with visual feedback",
            "venue": "IEEE Robotics and Automation Letters (RA-L) 3 (1987), 404\u2013417.",
            "year": 1987
        },
        {
            "authors": [
                "Ross Wightman."
            ],
            "title": "PyTorch Image Models",
            "venue": "https://github.com/ rwightman/pytorch-image-models.",
            "year": 2019
        },
        {
            "authors": [
                "Tete Xiao",
                "Ilija Radosavovic",
                "Trevor Darrell",
                "Jitendra Malik."
            ],
            "title": "Masked Visual Pre-training for Motor Control",
            "venue": "arXiv preprint arXiv:2203.06173 (2022).",
            "year": 2022
        },
        {
            "authors": [
                "Fisher Yu",
                "Yinda Zhang",
                "Shuran Song",
                "Ari Seff",
                "Jianxiong Xiao."
            ],
            "title": "LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop",
            "venue": "arXiv preprint arXiv:1506.03365 (2015).",
            "year": 2015
        },
        {
            "authors": [
                "Seyedhosseini",
                "Yonghui Wu"
            ],
            "title": "CoCa: Contrastive Cap",
            "year": 2022
        },
        {
            "authors": [
                "Tamara L. Berg"
            ],
            "title": "Modeling Context in Referring Expressions",
            "year": 2016
        },
        {
            "authors": [
                "Alberto Rodriguez"
            ],
            "title": "Robotic pick-and-place of novel ob",
            "year": 2017
        },
        {
            "authors": [
                "Sergey Levine"
            ],
            "title": "Learning Invariant Representations",
            "year": 2021
        },
        {
            "authors": [
                "H.S. Torr",
                "Li Zhang"
            ],
            "title": "Rethinking Semantic Segmenta",
            "year": 2021
        },
        {
            "authors": [
                "Nair"
            ],
            "title": "2022], finding that while language is again superior, higher-level features perform better. This is preliminary evidence that even for individual evaluation domains (e.g., single-task visuomotor control), there is no silver bullet; different types of representations perform differently",
            "venue": "We present results on the Adroit Visumotor Control environments",
            "year": 2022
        },
        {
            "authors": [
                "Both MVP",
                "R3M [Radosavovic"
            ],
            "title": "2022] only evaluate frozen visual representations, following a precedent set by a long tradition of work in self-supervised learning from the computer vision community",
            "venue": "[Chen et al. 2020; Radford et al",
            "year": 2022
        },
        {
            "authors": [
                "Geiger"
            ],
            "title": "Voltron expects a dataset of videos and associated language narrations, there is a wealth of visually diverse and relevant data that does not subscribe to this type signature:: datasets of standalone images from curated datasets [Deng et al",
            "year": 2022
        },
        {
            "authors": [
                "Touvron"
            ],
            "title": "2021] for scaling down the magnitude of residual connections; prior work has found this to have a powerful stabilizing effect during pretraining [Karamcheti et al. 2021a]. We also provide pseudocode for implementing the various modifications in Figure 8 (bottom); these modifications are all simple and transferable across Transformer implementations. Furthermore, as part of the no-language implementation in \u00a76, we ablate the effects",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Good words are worth much, and cost little.\n\u2014 George Herbert\nRealizing a future of ubiquitous, broadly capable robots is predicated on systems capable of generalizable perception and interaction [Weiss et al. 1987; Chaumette andHutchinson 2006; Levine et al. 2016]. Towards this goal, recentwork in robotics present approaches for learning visual representations to bootstrap learning for visuomotor control [Parisi et al. 2022; Nair et al. 2022; Radosavovic et al. 2022]. Critically, these approaches show that we can learn such representations from real-world videos of human behavior \u2013 specifically, egocentric video datasets such as Something-Something-v2 and Ego4D [Goyal et al. 2017; Grauman et al. 2022] \u2013 instead of solely relying on in-domain robotics data that is scarce and expensive. While prior work has developed and evaluated representations for visuomotor control, robot learning is an expansive discipline, 1Project Page: https://sites.google.com/view/voltron-robotics Model Artifacts & Pretraining Code: https://github.com/siddk/voltron-robotics Evaluation Suite: https://github.com/siddk/voltron-evaluation\nspanning a diverse spectrum of problems: predicting grasp proposals from visual input [Saxena et al. 2008; Mahler et al. 2017], languageconditioned imitation learning [Tellex et al. 2011] and belief/intent tracking for human-robot interaction [Hauser 2012; Javdani et al. 2018], amongst others. Broadening our focus to problems beyond learning for control enables us to develop flexible, generalizable representations that capture both low-level spatial reasoning and high-level semantic understanding \u2013 a flexibility that is a key prerequisite to realizing a foundation model for robotics [Bommasani et al. 2021]. Thus, we ask: how can we learn visual representations that generalize across the diverse spectrum of problems in robot learning?\nRecent approaches for learning visual representations for robotics use pretraining objectives that reflect different inductive biases for what the learned representations should capture. Masked Visual Pretraining [MVP; Radosavovic et al. 2022] proposes using masked autoencoding [He et al. 2022] to prioritize visual reconstruction from heavily masked video frames, encoding representations that facilitate per-pixel reconstruction. Separately, Reusable Representations for Robotic Manipulation [R3M; Nair et al. 2022] eschews pixel reconstruction for two contrastive learning objectives: time contrastive learning [Sermanet et al. 2018] and video-language alignment. These approaches show strong performance on imitation learning in simulated and real-world settings, with sizeable improvements over strong alternatives such as ResNet or CLIP features [He et al. 2016; Radford et al. 2021]; however, they have not been evaluated beyond these settings. As a first contribution, we evaluate these representations on problems beyond control and identify inconsistent evaluation performance, with huge penalties depending on the approach and specific application. MVP performs well on problems such as grasp affordance prediction, but struggles with higher-level problems such as language-conditioned imitation. R3M instead excels at the higher-level problems, but degrades completely on problems such as grasp affordance prediction.\nMotivated by this, we presentVoltron, a framework for languagedriven visual representation learning for robotics that learns representations that capture both low-level and high-level features, empirically outperforming prior approaches over all applications. Voltron models take videos and associated language captions as input to a masked autoencoding pipeline, reconstructing one (or more) frames from a masked context. The novelty of our framework is in how we use language supervision. Depending on a tunable probability \ud835\udefc , we either condition on (\ud835\udefc = 0), or generate (\ud835\udefc > 0) the associated caption. Explicitly conditioning on words in different contexts allows for low-level pattern recognition at the local, spatial level, while generating language from our learned visual\nar X\niv :2\n30 2.\n12 76\n6v 1\n[ cs\n.R O\n] 2\n4 Fe\nb 20\n23\nKaramcheti et. al.\nencoding allow us to infer higher-level features around affordances and intents. Furthermore, guided by the hypothesis that language is especially useful in describing change, we study dual-frame contexts consisting of the initial and current observation in multi-timestep tasks. Altogether, we examine three differentVoltron variants:V \u2013 Cond (Language Conditioning: single frame, \ud835\udefc = 0), V \u2013 Dual (Adding Context: dual-frame conditioning, \ud835\udefc = 0), and V \u2013 Gen (Adding Language Generation: dual-frame, \ud835\udefc = 0.5 \u2013 we find that \ud835\udefc = 1 with no language-conditioning at all hurts performance).\nTo evaluate Voltron and other visual representation learning approaches, we assemble a new evaluation suite (depicted in Figure 1) spanning five problem domains within robotics: 1) dense segmentation for grasp affordance prediction [Zeng et al. 2017], 2) object detection from referring expressions (e.g., \u201cthe blue coffee mug to the left of the plate\u201d) in cluttered scenes [Wang et al. 2021], 3) imitation learning for visuomotor control (in simulation) [Nair et al. 2022], 4) learning multi-task language-conditioned policies for real-world manipulation [Stepputtis et al. 2020] (on a real-world Franka Emika fixed-arm manipulator), and 5) zero-shot intent scoring [Javdani et al. 2018; Chen et al. 2021]. We choose these tasks for their broad coverage; tasks such as grasp affordance prediction and referring expression grounding require reasoning over low-level spatial features, while language-conditioned imitation and intent scoring require a deeper understanding of semantics.\nThrough experiments controlling for pretraining data and model capacity, we show that the simplest Voltron representations (from V \u2013 Cond) strictly outperform both MVP and R3M representations across all evaluation domains. Furthermore, by adapting our models to learn from multiple frame contexts and that favor generation (e.g., withV \u2013 Dual andV \u2013 Gen), we show that we can further boost performance on evaluations requiring higher-level features such as with language-conditioned policy learning (on a real robot) and intent scoring. Though language-conditioning offers universal performance gains, there are tradeoffs between Voltron\nmodels; adding language generation hurts performance on some control tasks, even though its necessary for strong performance on intent scoring. Furthermore, Voltron with single-frame language conditioning performs well on non-episodic tasks (e.g., grasping), but underperforms multi-frame models on control tasks. There is not yet a silver bullet \u2013 a single representation strong on all tasks \u2013 but the ability to balance tradeoffs between encoding low and high-level features offers a net win over restrictions of past work.\nContributions. 1)WepresentVoltron, a framework for languagedriven visual representation learning. Through controlled experiments and comprehensive ablations we demonstrate thatVoltron\u2019s representations strictly outperform the prior art across 2) a new evaluation suite composed of five distinct problem domains within robotics. Finally, 3) we analyze the tradeoffs between different Voltron models that balance different types of feature learning, outlining several directions for future work. We release all models, the evaluation suite, code (pretraining and adaptation), and preprocessed data (https://sites.google.com/view/voltron-robotics).\nLimitations. We do not have access to the compute resources to train models of the same scale and data used in prior work [Radosavovic et al. 2022; Nair et al. 2022]. Instead, we carefully reproduce MVP and R3M \u2013 the current state-of-the-art approaches \u2013 by pretraining on the Something-Something-v2 dataset [Goyal et al. 2017], further controlling for batch ordering, model capacity, and other sources of randomness (full details are in \u00a74). However, for full context we also include results from the official release artifacts from both these works, as well as other methods such as CLIP [Radford et al. 2021], though we note these results in gray or with dashed lines as to indicate they are not directly comparable."
        },
        {
            "heading": "2 Related Work",
            "text": "Voltron is situated within a rich body of work in visual representation learning for robotics and multimodal pretraining.\nVoltron: Language-Driven Representations for Robotics\nVisual Representation Learning for Robotics. An emerging body of work in robot learning studies learning visual state representations for control. A wealth of prior approaches learn representations from in-domain data taken directly from the target environment (and corresponding task); these techniques range from using data augmentation [Laskin et al. 2020; Srinivas et al. 2020; Kostrikov et al. 2021; Pari et al. 2022] to modeling forward dynamics [Gelada et al. 2019; Hafner et al. 2020] to using task-specific information [Jonschkowski and Brock 2015; Zhang et al. 2021]. Unlike these approaches, we move beyond task-specific data, instead leveraging large, accessible datasets such as videos of humans performing everyday tasks. Work in this paradigm has exploded in recent years. A number of approaches find that existing representations such as features from models trained on ImageNet [Deng et al. 2009], or features from CLIP [Radford et al. 2021] enable more efficient learning [Shah and Kumar 2021; Khandelwal et al. 2021]. More recently, multiple approaches have shown increased dividends in applying such representations to visuomotor control, for example by combining features at different layers of pretrained ResNets [Parisi et al. 2022] or by pretraining such representations on human videos, conjecturing that such data captures features useful for robotic manipulation [Nair et al. 2022; Xiao et al. 2022; Radosavovic et al. 2022; Ma et al. 2022]. However, missing from these approaches is a notion of semantics; works such as MVP [Xiao et al. 2022; Radosavovic et al. 2022] purely learn to perform masked reconstruction from a single image, and even works that leverage some temporal and linguistic signals do so in a limited way [Nair et al. 2022; Ma et al. 2022]. Instead, our work is motivated by the hypothesis that language understanding \u2013 both via conditioning\nand generation \u2013 is an essential component of learning generalizable visual representations. It is not enough that a representation summarizes an observation; instead, for generalization to new contexts and behaviors, it must capture how observations (and changes thereof) relate to higher-level semantic abstractions.\nVoltron aims to do this with its language-driven representation learning objective: by jointly modeling sequences of frames and language, we enable a range of capabilities, from producing representations of single images in isolation, to providing the capability to generate language grounded in visual contexts. We demonstrate the benefits of language-driven learning in our evaluation (see \u00a75): in head-to-head comparisons, Voltron models strictly outperform prior approaches across all evaluation domains.\nLearningMultimodal FoundationModels.Ourwork draws further inspiration from a wave of progress in multimodal foundation models such as CLIP, Multimodal Masked Autoencoders (M3AE), Flamingo, CoCa, and Gato, amongst many others [Radford et al. 2021; Geng et al. 2022; Alayrac et al. 2022; Yu et al. 2022; Reed et al. 2022; Lu et al. 2023; Aghajanyan et al. 2022]. These approaches highlight the myriad benefits of multimodal pretraining: language supervision works to enrich visual representations (even in the absence of language downstream), while visual supervision similarly enriches language representations [Lu et al. 2019; Singh et al. 2022]. Of the many capabilities afforded by these models, many have applications in embodied AI and robotics. CLIP representations have shown to be effective in applications to various robotics tasks [Shridhar et al. 2021; Khandelwal et al. 2021; Cui et al. 2022], while multimodal transformer models have proven effective initializations for training control policies [Reid et al. 2022; Liu et al.\nKaramcheti et. al.\n2022]. These approaches are similar toVoltron in their joint use of visual and language inputs; whereVoltron differs, however, is in our novel representation learning objective that balances language conditioning and generation, enabling learning representations that transfer to a wide range of applications within robotics.\n3 Voltron \u2013 Language-Driven Learning We assume access to a dataset of videos paired with natural language annotations; in each video-language pair (\ud835\udc63, \ud835\udc50), language can take the form of a caption (e.g., \u201cpeels the carrot\u201d in Figure 2), narration, or even coarse textual label of a behavior. We assume each video \ud835\udc63 \u2208 R\ud835\udc47\u00d7\ud835\udc3b\u00d7\ud835\udc4a \u00d7\ud835\udc36 consists of a sequence of frames \ud835\udc63 = [\ud835\udc5c1, . . . , \ud835\udc5c\ud835\udc47 ], where each frame \ud835\udc5c\ud835\udc56 \u2208 R\ud835\udc3b\u00d7\ud835\udc4a \u00d7\ud835\udc36 is RGB-encoded. We tokenize and one-hot encode each utterance into a vocabulary \ud835\udc49 of cardinality |\ud835\udc49 |, padding to a max length \ud835\udc3f such that \ud835\udc50 \u2208 R\ud835\udc3f\u00d7|\ud835\udc49 | . We define a <NULL> token (separate from the <PAD> token) as a placeholder for an empty language context. Furthermore, following the MAE work, we define a visual masking function Mask(\ud835\udc63,\ud835\udefe) \u2192 (\ud835\udc63visible \u2208 R(1\u2212\ud835\udefe ) (\ud835\udc47\u00d7\ud835\udc3b\u00d7\ud835\udc4a \u00d7\ud835\udc36) , \ud835\udc63masked \u2208 R\ud835\udefe (\ud835\udc47\u00d7\ud835\udc3b\u00d7\ud835\udc4a \u00d7\ud835\udc36) ) that partitions the regions of a video into a set of visible and masked-out regions subject to a fixed masking ratio \ud835\udefe . We sample a mask once, and apply it uniformly across all frames in the video to prevent leakage [Tong et al. 2022]; if the masks were sampled independently, a masked region in one frame could be visible in another, allowing the encoder to \u201ccheat\u201d by looking ahead.\n3.1 Voltron \u2013 Core Components AVoltron model comprises 1) a multimodal encoder that takes in a visual context and (optional) language utterance producing a dense representation, 2) a visual reconstructor that attempts to reconstruct the masked-out visual context from the encoder\u2019s representation of what is visible, and 3) a language generator that predicts the language annotation for the video given the encoded visual context. The visual reconstructor and language generator crucially act to shape the representations by first erasing portions of a (\ud835\udc63, \ud835\udc50) pair, then attempting to reconstruct the missing parts; we show in our experiments (see \u00a75) that this bottleneck helps focus on more lowlevel features when we favor reconstruction over generation, and more high-level, semantic features when we favor generation over reconstruction. We step through each component below.\nMultimodal Encoder: E\ud835\udf03 (\ud835\udc63,\ud835\udc62) \u2192 \u210e \u2208 R\ud835\udc46\u00d7\ud835\udc51 The multimodal encoder (Figure 2; lower half in blue and orange) is the core of a Voltron model. It takes as input (\ud835\udc63,\ud835\udc62) where \ud835\udc63 \u2208 {\ud835\udc63visible, \ud835\udc63} denotes either the masked or unmasked (full) visual context respectively, and \ud835\udc62 represents a (possibly <NULL>) utterance to condition on. As output, the encoder produces a dense representation \u210e \u2208 R\ud835\udc46\u00d7\ud835\udc51 where \ud835\udc46 denotes the number of encoded regions, and \ud835\udc51 is a hyperparameter denoting the dimensionality of the representation. Keeping with the original MAE work, we divide each image \ud835\udc5c\ud835\udc56 \u2208 R\ud835\udc3b\u00d7\ud835\udc4a \u00d7\ud835\udc36 into a set of non-overlapping regions \ud835\udc45, where each region is a \ud835\udc5d \u00d7 \ud835\udc5d patch; this results in |\ud835\udc45 | = \ud835\udc3b\ud835\udc4a /\ud835\udc5d2 regions. Given a \ud835\udc58-frame context, \ud835\udc46 = (1 \u2212 \ud835\udefe)\ud835\udc58 |\ud835\udc45 |.\nVisual Reconstructor: R\ud835\udf03 (\u210e) \u2192 \ud835\udc63masked \u2208 R\ud835\udefe (\ud835\udc58\u00d7\ud835\udc3b\u00d7\ud835\udc4a \u00d7\ud835\udc36) The visual reconstructor (Figure 2; upper half in orange) takes as input the encoded representation of the visible visual context\n\u210e = E\ud835\udf03 (\ud835\udc63visible, \ud835\udc50). It attempts to reconstruct the missing visual regions \ud835\udc63masked, conditioned on language context \ud835\udc50 , producing a prediction \ud835\udc63masked. Following prior work, the elements of \ud835\udc63masked are the normalized pixel targets from the original image. We use mean-squared error as the reconstruction loss Lreconstruct (\ud835\udf03 ).\nLanguage Generator: G\ud835\udf03 (\u210e) \u2192 \ud835\udc50 \u2208 R\ud835\udc3f\u00d7\ud835\udc36\nThe language generator (Figure 2; upper half in red) takes the encoded representation of the visible context and the <NULL> language token, \u210e = E\ud835\udf03 (\ud835\udc63visible, <NULL>). It generates the language annotation, producing \ud835\udc50 \u2208 R\ud835\udc3f\u00d7|\ud835\udc49 | , with each of the \ud835\udc3f elements corresponding to a probability distribution over the vocabulary. We use the negative log-likelihood (cross-entropy) of the annotation \ud835\udc50 under the generator as our loss Lgenerate.\nThe language generator crucially takes the <NULL> token as input instead of the annotation \ud835\udc50; inputting the same \ud835\udc50 that the generator is trying to output can lead to trivial collapse where the encoder learns to memorize the tokens to aid the generator. As a result, for each example during training we need to either condition or generate language; this further motivates the parameter \ud835\udefc in Figure 2 and in the training objective."
        },
        {
            "heading": "3.2 Balancing Reconstruction & Generation",
            "text": "The Voltron learning objective trades off language-conditioned reconstruction and visually-grounded language generation to shape the features captured by the encoder\u2019s learned representation. The reconstruction objective prioritizes low-level spatial information conducive to filling in missing textures, colors, or edges; likewise, the generation objective captures higher-level semantic information, encouraging the encoder to encode features that are predictive of the language caption. We make this tradeoff explicit by minimizing the following loss, characterized by the parameter \ud835\udefc \u2208 [0, 1]:\nL(\ud835\udf03 ) = Lreconstruct (\ud835\udf03 ) + Lgenerate (\ud835\udf03 )\n=  MSE(\ud835\udc63masked, R\ud835\udf03 (E\ud835\udf03 (\ud835\udc63visible, \ud835\udc50))) if \ud835\udc67 = 0 MSE(\ud835\udc63masked, R\ud835\udf03 (E\ud835\udf03 (\ud835\udc63visible, <NULL>))) if \ud835\udc67 = 1\n+ NLL(\ud835\udc50,G\ud835\udf03 (E\ud835\udf03 (\ud835\udc63visible, <NULL>))) and \ud835\udc67 \u223c Bernoulli(\ud835\udefc)\nFor each example (\ud835\udc63, \ud835\udc50) seen at training, we draw \ud835\udc67 \u223c Bernoulli(\ud835\udefc): with \ud835\udc67 = 0 we condition on the original language utterance, while with \ud835\udc67 = 1, we generate the original language utterance, conditioning the encoder on the <NULL> token. We limit our exploration in this work to at most two frame contexts \ud835\udc58 = 2 due to computational cost; even four frame contexts exceed the memory on the compute available to us. In selecting the two frame contexts, we sample at least five frames from each video clip in our dataset (with random intervals between). We enforce a heuristic such that the first frame in each dual-frame context comes from the first 20% of the clip, with the other frame appearing in the remaining 80%.\nDriven by the hypothesis that different values of \ud835\udefc and framecontexts \ud835\udc58 shape the balance of low-level and high-level features in our representations, we evaluate three different instantiations of the Voltron framework (as mentioned in \u00a71):\nVoltron: Language-Driven Representations for Robotics\n\u2022 V \u2013 Cond: \ud835\udefc = 0, \ud835\udc58 = 1 single-frame conditioning. \u2022 V \u2013 Dual: \ud835\udefc = 0, \ud835\udc58 = 2 dual-frame conditioning; a contextaware model identical to V \u2013 Cond but trained on dualframe pairs (initial frame, random subsequent frame). \u2022 V \u2013 Gen: \ud835\udefc = 0.5, \ud835\udc58 = 2; condition and generate with equal probability, trained on dual-frame contexts as above.\nNote that we do not evaluate \ud835\udefc = 1; we find through preliminary experiments that some language-conditioning is always helpful."
        },
        {
            "heading": "4 Implementation & Reproducibility",
            "text": "In addition to our framework, a core contribution of this work is a comprehensive set of controlled experiments. To do this, we reimplement both MVP and R3M using code released by the authors, controlling for the pretraining data (at the level of the individual frames seen per epoch) and model capacity. Baselines \u2013 Preliminaries. Throughout this work, we have mentioned both MVP and R3M in terms of their tradeoffs; here, we make their pretraining objectives explicit. Both prior approaches use video datasets, but only learn single-frame encoders, choosing to use the video structure in different ways (detailed below). Of the two approaches, we note that only R3M uses language supervision.\nMVP follows a masked autoencoding backbone, similar to that depicted in Figure 2 (without language conditioning). MVP does not offer any special consideration to the temporal structure of videos, instead treating each frame in the dataset as as standalone input. Given a single frame, MVP masks out regions subject to a fixed mask ratio \ud835\udefe (same as in Voltron), encoding the visible context with a Transformer encoder, then attempting to reconstruct the missing context with a separate Transformer decoder \u2013 also using mean-squared error for reconstruction.\nR3M is different in that it does not contain a reconstruction component, instead combining two contrastive objectives on top of a single-frame visual encoder \u2013 time contrastive learning [Sermanet et al. 2018] and image-language temporal alignment [Radford et al. 2021; Nair et al. 2021]. These objectives explicitly use the temporal structure of videos. Given an encoding of a visual context, the timecontrastive objective seeks to maximize the score of encodings between frames close together in time (e.g., within a few frames of each other), contrasted against frames from the same video that are further away. R3M also uses language supervision. Given a separate encoder that fuses a language caption with the encoding dualframes contexts (consisting of an initial and subsequent frame) the image-language alignment objective attempts to assign scores that capture \u201ctask progress:\u201d the score of a subsequent frame occurring later in a video subject to a language caption should be higher than the score of a frame occurring earlier. The two key differences between Voltron and R3M are 1) using visual reconstruction as a dense objective vs. time contrastive learning, and 2) explicitly conditioning on or generating language inVoltron vs. matching visual and language embeddings as a contrastive objective. Pretraining Dataset Construction. For all models in this work, we use Something-Something-v2 [Sth-Sth; Goyal et al. 2017] as our pretraining dataset, motivated by prior work [Shao et al. 2020; Chen et al. 2021; Xiao et al. 2022]. All models see the exact same image frames. We extract 5 frames per video, per training epoch to ensure we are learning from multiple visual inputs of the same\ncontext and to facilitate R3M\u2019s time contrastive learning objective [Sermanet et al. 2018]; we serialize the processed frames, and store index files with the video/frame indices per epoch. Data-Equivalent Reproductions. Though prior works release trained model artifacts, they do not provide sufficient details for reproduction, such as the exact frames sampled from videos, preprocessing applied, or hardware/compute used. We thus reimplement MVP and R3M in a controlled setting on Sth-Sth using the released code from the original papers where possible and clarifying additional details with the authors directly as needed. We implement all models with a Vision Transformer (ViT) backbone and additionally implement R3M with a ResNet-50 backbone based on discussions with the authors of the original work. They suggested that there may be slight differences in the inductive bias of ResNets vs. Vision Transformers [Raghu et al. 2021] that would be worth investigating. We use the ViT-Small/16 variant, with patch size \ud835\udc5d \u00d7 \ud835\udc5d = 16 \u00d7 16 and a Transformer with 12 blocks, 6 attention heads per block, and hidden dimension \ud835\udc51 = 384 [Wightman 2019]. We refer to our reproductions as \u201cR-MVP,\u201d \u201cR-R3M (ViT-S),\u201d and \u201cR-R3M (RN-50).\u201d\nWe pretrain all models in this work on TPU v3-8 compute, generously granted to us by the TPU Research Cloud program (TRC). We run 400 epochs of training for all models with a batch size of 1024, each epoch comprised of a pass through 844K frames (168K clips in Sth-Sth, 5 frames per clip). We do not use dropout or data augmentation. All code and reproducibility details are in our open-source code repositories, linked from our project page. Additional Comparisons. Thoughwe lack the compute resources to train on models on the same scale data, we further contextualize our results by evaluating the official R3M and MVP models released in the original works. We note that the released R3M model uses the entirety of the Ego4D dataset [Grauman et al. 2022], comprised of over 3000 hours of videos, spanning 3.6M individual clips (comprisingmore than 20x the data we use in this work). The released MVP also uses Ego4D, but add Sth-Sth, Epic-Kitchens, and more [Damen et al. 2018; Shan et al. 2020], while also scaling models up to 86M and 307M parameters, (4-10x the size of ViT-Small). We also evaluate OpenAI\u2019s CLIP model (ViT-Base) as a strong baseline that leverages language supervision. We refer to these models as \u201cR3M (Ego4D),\u201d \u201cMVP (EgoSoup),\u201d and \u201cCLIP (ViT-B),\u201d following naming conventions from the original work and denote them with gray text and dashed lines in plots. Voltron Architecture Details.Voltron follows the masked autoencoding pipeline detailed above, with simple extensions for incorporating language. We implement the Voltron encoder E\ud835\udf03 by jointly embedding the language \ud835\udc62 and visual inputs \ud835\udc63visible with a Transformer [Vaswani et al. 2017]. We initialize language embeddings from DistilBERT [Sanh et al. 2019], learning a separate linear projection into the encoder\u2019s embedding space, similar to R3M. For the visual reconstructor R\ud835\udf03 and language generator G\ud835\udf03 , we use a separate Transformer with a small addition to enable language generation. In a standard MAE decoder, patches are generated independently, attending to all patch embeddings from the encoder. To enable generation, we append a causal (lower triangular) attention mask for preventing our language decoder from \u201cpeeking\u201d at the future inputs to generate (visualized by the red triangle in Figure 2). This is akin to prefix language modeling [Raffel et al. 2019]; all\nKaramcheti et. al.\nTable 1: Summary of Evaluation Suite & Results.While some of our evaluation domains use language input, grasp affordance prediction and single-task visuomotor control do not. WhileVoltron models obtain strong performance over all applications, R-R3M and R-MVP exhibit variable performance depending on the application subset.\nInput Format Train Dataset Size Best Model Best Baseline\nGrasp \u00a75.1 Single Frame 1470 V \u2013 Cond R-MVP Referring Expressions \u00a75.2 Single Frame, Language Expression 259,839 V \u2013 Cond R-R3M (ViT) Single-Task Control \u00a75.3 Frame History \ud835\udc5b \u2208 [5, 10, 25] Demos V \u2013 Dual R-R3M (RN-50) Language-Conditioned Imitation \u00a75.4 Frame History, Language Instruction 100 = 5 x 20 Demos V \u2013 Dual / V \u2013 Gen R-R3M (ViT) Intent Scoring \u00a75.5 Frame History, Language Intent N/A (Zero-Shot) V \u2013 Gen N/A\nembeddings can attend to the visual inputs (as in a traditional MAE decoder), but language embeddings can only attend to the preceding language input.\nVoltron uses a combination of different language objectives on top of the standardMAE pipeline, adding complexity. To help ensure stable and reliable training, we follow best practices from the NLP community and make a series of small changes to the Transformer architecture including: 1) switching the default LayerNorm to rootmean square normalization [Zhang and Sennrich 2019; Narang et al. 2021] (stability, no learned parameters), 2) switching from the default GELU to the more performant SwishGLU activation [Shazeer 2020; Chowdhery et al. 2022] (performance), and 3) adopting LayerScale for scaling down the magnitude of each residual connection [Touvron et al. 2021; Karamcheti et al. 2021a] (prevents overflow). To ensure that any gains in evaluation performance stem from our insights around language-driven learning rather than this modified architecture, we run an ablation experiment in \u00a76. We find that these changes do not change downstream evaluation results, but significantly improve training stability. We present further details, including a sketch of the implementation differnces in \u00a7B.1. Adapting Representations. Unfortunately, there is not yet a standard for extracting representations from learned Vision Transformer encoders, especially for those trained via masked autoencoding. However, Zhai et al. [2022] suggest that multiheaded attention pooling [MAP; Lee et al. 2018] is a strong and versatile approach. We choose to use MAP as the sole feature extraction approach in all our ViT experiments, finding it to universally improve performance for all ViT models, relative to the \u201cdefault\u201d extraction approaches suggested in prior work. Notably, we find that just switching to MAP-based extraction over the procedure used in the original MVP work almost doubles success rate on visuomotor control tasks; we provide results from this analysis in \u00a7D.2. We note that we use MAP when evaluating CLIP (ViT-Base/16) and MVP (EgoSoup) for the fairest and strongest possible comparison."
        },
        {
            "heading": "5 Evaluation Suite: Construction & Results",
            "text": "We outline our evaluation suite (Table 1) comprised of five problem domains within robotics. Each evaluation consists of adaptation data and evaluation metrics. The adaptation data consists of visual input(s) (as RGB frames) and in some cases, language (e.g., an instruction for language-conditioned imitation). We evaluate representations fromVoltron and various baseline models by freezing the pretrained vision and language encoders, instead adapting evaluation-specific \u201cheads\u201d(lightweight networks) on top of the extracted representations. We choose evaluations that represent\nFigure 3: Grasp Affordance Prediction [ARC Grasping; Zeng et al. 2017]. Given objects in cluttered bins, segment the image corresponding to \u201cgraspable\u201d (green), vs. \u201cnon-graspable\u201d (red) regions; note that these regions are labeled for use with suction grippers.\ndomains that capture different types of understanding; in the following sections, we motivate the role of each application and provide experimental results."
        },
        {
            "heading": "5.1 Grasp Affordance Prediction",
            "text": "We consider the problem of grasp affordance prediction: given an image of a set of objects (e.g., on a cluttered workspace), predict a dense segmentation mask corresponding to \u201cgraspable\u201d and \u201cnongraspable\u201d locations for a suction-based gripper.\nMotivation. Grasp affordance prediction from visual input is a foundational task in robot learning, and is often a key component of many modular systems [Bohg et al. 2013; Correll et al. 2016]. Including this evaluation allows us to probe the low-level spatial features retained by various representations.\nVoltron: Language-Driven Representations for Robotics\nEvaluation Details.We specifically consider the problem as formulated in the Amazon Robotics Challenge Grasping Dataset (ARCGrasping) introduced by Zeng et al. [2017]. We choose this dataset over alternatives as it is readily available and consists of 1800+ images of multiple real-world objects in cluttered bins (Figure 3; left). We focus on the RGB-only, suction-grasping split of the dataset. We implement models for grasp affordance prediction following recent work on semantic segmentation with Transformers [Zheng et al. 2021; Strudel et al. 2021; Bao et al. 2022], specifically by introducing a Progressive Upsampling (SETR-PUP) head on top of our frozen visual features. We omit results from all ResNet models \u2013 R-R3M (RN-50) and R3M (Ego4D); unfortunately, training with simple PUPstyle on the final ResNet-50 7 \u00d7 7 spatial grid did not converge, possibly indicating a need for more complex architectures with significant added parameters (beyond the scope of this work). As this task only takes a single frame as input, we do not evaluateV \u2013 Dual andV \u2013 Gen. Following the original work, we report average precision at various confidences: Top-1 precision, Top-1% precision, and Top-5% precision. We select models via 5-fold cross validation. This task does not have a language component. We provide additional details around the adaptation procedure in Appendix E and the open-source code repositories.\nExperimental Results. Looking at Table 2, representations from MVP and Voltron models perform well across the board, while contrastive representations (e.g., from CLIP and R-R3M) perform quite poorly. Interestingly, V \u2013 Cond outperforms R-MVP and MVP (EgoSoup) on this task, despite the absence of language input, demonstrating that language supervision during pretraining can improve low-level feature learning, even relative to larger-scale models trained on much more data."
        },
        {
            "heading": "5.2 Referring Expression Grounding",
            "text": "Given a cluttered scene and language expression, the goal is to predict a bounding box around an object (e.g., \u201cthe blue black pen on the front left of the orange can\u201d in Figure 4; middle). Motivation. Capturing object-centric priors and high-level semantics around properties such as color and spatial relationships is crucial across the entire robotics stack. More importantly, this is a language-conditioned task, allowing us to evaluate the impact of pretraining with language supervision. Evaluation Details. We use the OCID-Ref Dataset [Wang et al. 2021] grounded in scenes that are representative of robotics settings; other datasets such as RefCoCo [Yu et al. 2016] are grounded in more global scenes (e.g., multiple humans playing frisbee on a field) that are less informative for robot learning. OCID-Ref also\nKaramcheti et. al.\nprovides splits based on the clutter level of the underlying scene, letting us further evaluate robustness. We regress bounding box coordinates directly from our frozen features using a shallow MLP. All approaches condition on language (see expressions in Figure 4), using the given language encoder where possible. This means using the multimodal encoder forV \u2013 Cond and the default learned text encoder for CLIP or R3M. However, for approaches that only learn visual representations (e.g., MVP), we append pretrained language features from DistilBERT \u2013 the same language model used to initializeVoltron. We note again that we omit ResNet results; though this task did not require upsampling, we find trained models obtained no better than random performance, again indicating a need for a more sophisticated adaptation architecture (beyond the scope of this work). We report average precision at 0.25 IoU for each split following the evaluation procedure outlined in Wang et al. [2021]. We provide additional details around the adaptation procedure in Appendix E and the open-source code repositories.\nExperimental Results. Results for each model across the various clutter splits are in Table 3. Voltron models are especially strong, vastly outperforming R-MVP by 40% and R-R3M by over 25% on all splits, showing that multimodal pretraining \u2013 even just conditioning on language when optimizing for masked reconstruction \u2013 can lead to substantial gains on downstream multimodal tasks. We isolate the massive performance gains ofVoltron models over prior work due to the multimodal encoder that learns fused embeddings of vision and language, allowing language to shape the visual representations during pretraining. In contrast, R3M, and CLIP models learn independent text encodings that are only fused post-hoc, during adaptation. This is even worse for MVP: these models need to learn to fuse their strong visual embeddings with the language embeddings from a completely different model (DistilBERT)."
        },
        {
            "heading": "5.3 Single-Task Visuomotor Control",
            "text": "Motivation. Imitation learning for visuomotor control has been the de-facto evaluation for prior work [Parisi et al. 2022; Nair et al.\n2022; Radosavovic et al. 2022], giving us the closest comparison to the evaluations used in MVP and R3M. This evaluation focuses on sample-efficient generalization, measuring how well visual representations help in learning policies from limited demonstrations \ud835\udc5b \u2208 {5, 10, 25}. This evaluation takes place in simulation. Evaluation Details. We look at policy learning in the Franka Kitchen simulation environments as defined by Nair et al. [2022]. This domain consists of 5 tasks, with 2 distinct camera viewpoints (Figure 5). We learn shallow MLP policy heads via behavioral cloning that predict 9-DoF joint velocities (7 joints, 2 gripper) from our (frozen) visual features and proprioceptive state. We follow the R3M evaluation, reporting average success rates for each setting with \ud835\udc5b demonstrations across the 5 tasks, 2 viewpoints, and 3 random seeds. We train separate policies per task, with no language conditioning \u2013 using the exact code provided by Nair et al. [2022]. Additional details are in Appendix E and the open-source code. Experimental Results.Most approaches perform similarly across the various number of training demonstrations (Figure 5; right). However, we see some promising trends; Voltron models perform better than both baselines, with approaches that learn from multiple frame contextsV \u2013 Dual andV \u2013 Gen showing significant improvements over single-frame approaches. Yet, the absolute success rates are low; learning for control is difficult, and while good visual representations can help, learning closed-loop policies from limited data remains an open challenge."
        },
        {
            "heading": "5.4 Language-Conditioned Imitation (Real)",
            "text": "Given a dataset of language instructions (e.g. \u201cthrow the bag of chips away\u201d) paired with demonstrations (on a real robot in a realworld tabletop setting), learn an instruction following policy via behavioral cloning. Figure 6 depicts the real-world environment. Motivation. A large body of work looks at learning languageconditioned policies for human-robot collaborative settings [Arumugam et al. 2017; Stepputtis et al. 2020; Lynch and Sermanet 2020; Karamcheti et al. 2021b; Ahn et al. 2022]. This evaluation gets at\nVoltron: Language-Driven Representations for Robotics\nthe robustness and reliability of learned representations, with the goal of validating different approaches in real-robot settings.\nEvaluation Details. We construct a \u201cstudy desk\u201d environment (Figure 6) with five prototypical \u201ctasks\u201d: 1) closing the drawer, 2) throwing the green bag of chips in the trash can, 3) discarding the used coffee pods, 4) moving the cyan coffee mug to the purple plate, and 5) moving the same mug to the yellow plate. For each task, we collect 20 teleoperated demonstrations at 10 Hz, randomly resetting the scene between episodes. We adopt the keyframe-based action space proposed in James and Davison [2022] for learning. This approach heuristically breaks a demonstration into 4-5 \u201cwaypoints\u201d (end-effector poses) that are used as action targets during behavior cloning; during policy execution, we plan min-jerk trajectories from the current position to the predicted waypoint, feeding the subsequent state and visual observation back to our policy [James et al. 2022; Shridhar et al. 2022]. To collect diverse instructions, we prompt ChatGPT [version dated Jan 9th, 2023; OpenAI 2022] with simple task descriptions, asking it to generate diverse language instructions, collecting 25 utterances total (20 train, 5 held-out) per task.2 We parameterize our policy similarly to \u00a75.3, adding a shallow MLP on top of the extracted (frozen) visual representations [Misra et al. 2017]. This task is language-conditioned; as in OCIDRef, we use the given language encoders for each approach where possible, appending DistilBERT features to pure visual representations otherwise. We report success rates with partial credit \u2013 0.25 points for achieving each of the following \u201cmilestones\u201d: reaching an object, interacting with it, transporting it, and completing the task. We provide additional details in Appendix E, and include videos of policy rollouts on the project page.\n2ChatGPT Prompt (additional details and generated instructions on project page): I\u2019m trying to train a robot assistant that can follow diverse language instructions. One task requires moving an empty chip bag (a green bag of those jalapeno chips) to the garbage. Can you generate 25 natural-sounding instructions (e.g., \u201cthrow away the chips\u201d)?\nExperimental Results. Looking at success rates of the various representations (Figure 6; top right) we see an exaggerated version of the trends exhibited in the single-task control setting; Voltron models obtain an extra boost in performance across the board given that this task is language-conditioned, highlighting the strength of its fused representations. Similarly, R-R3M models exhibit the next best performance. Due to time and shared resource constraints, we do not run out MVP (EgoSoup), R3M (Ego4D), or CLIP (ViT-B/16), though we expect similar trends as in the last evaluation."
        },
        {
            "heading": "5.5 Qualitative: Zero-Shot Intent Scoring",
            "text": "We perform a qualitative evaluation for the problem of languagebased intent scoring; given a language expression describing an intent or behavior (e.g., \u201copening the faucet\u201d) and a corresponding video (that may or may not show the described behavior), predict an \u201calignment score\u201d for each frame of a video. This alignment score should capture how well the current visual context matches the described behavior \u2013 ideally reflecting calibrated confidence over time (an example language/video is shown in Figure 7; left).\nMotivation. This evaluation is motivated by two active areas of research: reward learning from language and demonstrations [Smith et al. 2020; Shao et al. 2020; Chen et al. 2021; Bahl et al. 2022], and belief modeling for human-robot collaboration [Hoffman and Breazeal 2007; Hauser 2012; Bandyopadhyay et al. 2013] This evaluation probes for the ability to reason over intents and visual behaviors jointly, without the need for additional data.\nEvaluation Details. This is a qualitative evaluation that focuses on measuring how well existing approaches \u201ctrack\u201d progress conditioned on a language intent over time. Doing this zero-shot means that we can only evaluate models that can produce alignment scores given language and visual context: 1) CLIP (ViT-B/16) through cosine similarity of learned vision and text representations, 2) R3M (Ego4D) through the \u201cvideo-language alignment\u201d head, and 3) our\nKaramcheti et. al.\nV \u2013 Gen model (by measuring the likelihood of a given language utterance conditioned on visual context under the language generator). Given a video of an agent performing some behavior described in language (e.g., \u201copening the faucet\u201d), we estimate and plot scores under each model across a sequence of video frames. We use videos from WHiRL [Bahl et al. 2022] of humans and robots performing the same tasks from different views; we choose to evaluate intent scoring for both agents to better capture the robustness and transfer potential for these approaches in similar real-world settings.\nExperimental Results. The two curves in Figure 7 show the predicted scores over time for the language intent \u201copening the faucet.\u201d Even though it has never been trained for this task, we find that V \u2013 Gen is able to coherently predict not only the exact frames corresponding to \u201ckeypoints\u201d in each video (e.g., touching the handle, observing when the water starts running), but is also capable of measuring partial progress \u2013 akin to a shaped, dense reward; however, both R3M (Ego4D) and CLIP (ViT-B/16) fail at this task, predicting random scores with high variance across sequential time steps. Note that the intent scores are not perfect; after turning the faucet on for the human video, predicted scores remain high, while for the robot, the scores taper off. It is not clear why this happens, but given a small amount of adaptation data, one could ensure consistent behavior. We provide more examples from WHiRL in \u00a7C.5, and additional evaluation details in Appendix E."
        },
        {
            "heading": "6 Ablations, Extensions, & Further Analysis",
            "text": "The comparative results across the various evaluation problem domains paintVoltron\u2019s language-driven representations in a favorable light relative to MVP and R3M baselines. Yet, there remain key questions that we address in this section: is language supervision actually driving these results? Why generative language modeling over masked language modeling? WillVoltron scale? Ablation: The Impact of Language Supervision. The second row of Table 4 shows a subset of evaluation results across three different problem domains when training a \u201cno-language\u201d variant of\ntheV \u2013 Cond architecture \u2013 this variant is in essence an alternate version of a masked autoencoder that uses the small architecture modifications we added for training stability in \u00a74. As such, it also serves as an architecture ablation when compared to the R-MVP results, enabling us to isolate the impact of the small stability modifications described in \u00a74. Indeed, the results confirm our hypotheses: first, removing language results in a definitive drop in performance across all evaluation applications. Second, the respective results for each evaluation application are on par with the corresponding results for the R-MVP model, demonstrating that the performance ofVoltron models does not stem from the architecture. We delve further into this ablation in \u00a7C.1. Ablation: Generative vs. Masked Language Modeling. Looking at theVoltron objective, a natural question to ask is why we chose language generation over masked language modeling. Furthermore, recent and concurrent work propose learning multimodal masked autoencoders (M3AE) both within and outside of robotics [Geng et al. 2022; Liu et al. 2022], showing promising results in learning visual representations for image classification tasks, amongst others. To assess the differences, we choose to reproduce the M3AE model in a manner similar to our reproduction of MVP and R3M; we keep the same Something-Something-v2 pretraining data, adopting the exact procedure described in Geng et al. [2022], then evaluating the resulting representations on the same subset of evaluation domains as in the prior ablation (third row of Table 4). Surprisingly, we see drastic drops in performance across the board. Looking at the pretraining curves, we identify a possible reason for this failure: in optimizing M3AE on Sth-Sth, we see the language modeling loss go to zero almost immediately, leading to overfitting. A possible explanation is that the masked language modeling conditioned on visual contexts in datasets annotated with short, predictable narrations leads to degenerate representations, while generative language modeling is not susceptible to the same types of collapse; looking at ways to mitigate this seems like a promising direction for future work. Explicit details around pretraining and evaluating R-M3AE, with an in-depth discussion are in \u00a7C.2.\nVoltron: Language-Driven Representations for Robotics\nExtension: Scaling Up. Prior approaches have shown gains in scaling model capacity; here, we present preliminary evidence that Voltron models behave similarly. For each evaluation in \u00a75, we evaluate a ViT-Base variant of V \u2013 Cond (86M parameters vs. the 22M in the ViT-Small). We see universal improvement: Top5% precision for grasping (Table 2; middle row) increases by 15%, expression grounding accuracy improves (Table 3; middle row), as does performance on control. Extension:Robustness toReal-WorldDistractors. Factors such as lighting conditions, time of day, and accidental environment perturbations (e.g., a colleague knocking over the camera) can have a profound impact on performance of robotic systems, especially if learned representations are not robust. We run a limited \u201crobustness\u201d evaluation after training language-conditioned policies from the demonstrations described in \u00a75.4. Success rates before and after introducing visual distractors for two of the \u201cmeta-tasks\u201d are in Figure 6 (bottom right).3 We find thatVoltron and R-MVP models are robust to even the most extreme distractors \u2013 seemingly a benefit of per-patch masking coupled with MAP-based extraction."
        },
        {
            "heading": "7 Discussion & Conclusion",
            "text": "We propose Voltron, a framework for language-driven representation learning that balances conditioning and generation to shape the balance of low and high-level features captured. We introduce an evaluation suite spanning five diverse problems within robotics for holistically evaluating visual representations. Through controlled experiments and ablations, we validate the strengths of our representations; across all evaluation tasks,Voltron models that balance language conditioning and generation strictly outperform prior approaches such as R3M and MVP, and in many cases show performance competitive with or exceeding that of approaches that use orders of magnitude more data or more expressive models.\nYet, while language is a pivotal source of supervision, there are still key questions to answer. Why is language-based pretraining helpful on tasks that have nothing to do with language? Why not try to learn one model that can encode both low-level and high-level features, without tradeoffs? While there is not a silver bullet yet we hope that future work takes a deep, grounded look at these questions, identifying what existing representations capture \u2013 and more importantly, what they miss. Our hope is thatVoltron serves as a starting point; a flexible, unified framework for future improvements in visual representation learning for robotics. 3We try five distractors spanning simple changes such as swapping the purple textbook in the background for a green one, to more extreme distractors such as playing a clip from \u201cVoltron, the Animated Series\u201d on a tablet in the middle of the workspace. Videos are on the project page."
        },
        {
            "heading": "Acknowledgments",
            "text": "This work would not have been possible without the support of entire communities of students, engineers, and various domain experts; our gratitude cannot be understated. We would specifically like to thank Shyamal Buch, David Hall, Sasha Khazatsky, and John Thickstun for their invaluable advice and suggestions around pretraining and evaluation. We further thank Dilip Arumugam, Masha Itkina, Minae Kwon, Tyler Lum, Vivek Myers, and Karl Pertsch for their feedback on earlier drafts.\nToyota Research Institute (\u201cTRI\u201d) provided funds to support this work. This project was additionally supported by the Office of Naval Research (ONR). Parts of this research \u2013 specifically model pretraining \u2013 was supported with Cloud TPUs from Google\u2019s TPU Research Cloud (TRC). Siddharth Karamcheti is grateful to be supported by the Open Philanthropy Project AI Fellowship. Annie Chen is supported by the NSF Graduate Research Fellowship (NSF GRFP). Finally, we thank the members of the Stanford ILIAD, IRIS, and NLP groups for valuable discussions and their unwavering support.\nKaramcheti et. al."
        },
        {
            "heading": "Overview",
            "text": "In the appendices below, we provide additional details around the implementation, pretraining, and adaptation procedures described in the main text, in addition to delving deeper into various discussions. Finally, we add additional results and visualizations that further complement the findings from the main text.\nWe provide open-source code for loading and using pretraining models, hosted links for our preprocessing splits (including the actual batches seen during training), and a separate, standalone open-source code repository for our evaluation suite. Our hope is that the evaluation suite especially is general and easy to use for downstream work on evaluating learned representations. The full manifest of resources are as follows:\n\u2022 Project Page (videos & additional links): https://sites.google.com/view/voltron-robotics \u2022 Open-Source Modeling Repository (pretraining code for all approaches, loading models): https://github.com/siddk/voltron-robotics \u2022 Open-Source Evaluation Suite (general API for evaluating on different problem domains): https://github.com/siddk/voltron-evaluation\nAll model and automated evaluation code is in PyTorch; however, the evaluation code can be easily overridden to suit your needs.\nAn overview of each appendix can be found below. We further indicate which parts of the appendices are best viewed here in the text or on the project page; for videos and visualizations, we highly recommend navigating to the latter.\nAppendix A \u2013 Motivating Questions\nWe index a list of \u201cmotivating\u201d questions that may arise from reading the main text and that we expand on further here (e.g., \u201cwhy only evaluate frozen representations\u201d). Our answers here are direct, and in many cases link to actual experiments further on in the appendices.\nAppendix B \u2013 Voltron Implementation\nWe provide code and other implementation details around the modifications to the Transformer architecture described in the Implementation and Reproducibility Section (see \u00a74) of the main text, along with additional details around the released models and data artifacts from this work. The section is structured as follows:\n\u00a7B.1 \u2013Voltron Transformer Implementation Side-by-side comparisons of theVoltron and \u201cstandard\u201d Vision Transformer blocks.\n\u00a7B.2 \u2013 Jointly Processing Vision & Language\nAdditional details around encoding multimodal inputs (e.g., position encoding, modality tokens, etc.).\n\u00a7B.3 \u2013 Pretraining Curves\nVoltron pretraining loss curves (reconstruction error, language modeling error) over training; useful for characterizing the behavior of downstream models (and the trade-offs between the losses).\n\u00a7B.4 \u2013 Index of Released Artifacts\nWe release pretrained Voltron models \u2013 V \u2013 Cond, V \u2013 Dual, V \u2013 Gen \u2013 in addition to intermediate checkpoints to facilitate future work. We also release the largerV \u2013 Cond model (ViT-Base).\nAppendix C \u2013 Additional Results & Visualization\nWe report additional results and visualizations from experiments mentioned in the main text, as well as other experiments that further support our conclusions.\n\u00a7C.1 \u2013 Analysis: Impact of Language-Conditioning on Reconstruction Loss\nWe revisit the language vs. no-language ablation from the main text, looking at pretraining curves to help explain why language is so helpful as a supervision signal. We find that language-conditioning significantly lowers reconstruction loss, allowing models to pick up on more low-level features.\n\u00a7C.2 \u2013 Analysis: Generative vs. Masked Language Modeling\nWe look further at the masked language modeling ablation from the main text, via the reproduction of Multimodal Masked Autoencoders [M3AE; Geng et al. 2022]. We find in the pretraining curves high evidence of overfitting with masked models early in training, impacting the learned representations.\nKaramcheti et. al.\n\u00a7C.3 \u2013 Results: Adroit Visuomotor Control\nWe present results on the Adroit Visumotor Control environments from Nair et al. [2022], finding that while language is again superior, higher-level features perform better. This is preliminary evidence that even for individual evaluation domains (e.g., single-task visuomotor control), there is no silver bullet; different types of representations perform differently.\n\u00a7C.4 \u2013 Qualitative: Real-Robot Language-Conditioned Policy Rollouts\nVisualizations of real-world policy rollouts from the various representation learning approaches.\n\u00a7C.5 \u2013 Qualitative: Additional Intent Scoring Visualizations\nAdditional intent scoring visualizations using videos from the WHiRL dataset [Bahl et al. 2022].\nAppendix D \u2013 Data-Equivalent Reproductions & Reproducibility\nWe add additional discussion around the reproductions of MVP and R3M on the Something-Something-v2 dataset: \u00a7D.1 \u2013 Additional Preprocessing Discussion\nAdditional discussion of how we preprocess Something-Something-v2 [Sth-Sth; Goyal et al. 2017] for pretraining, with a comparison of how prior work such as MVP source and process pretraining data.\n\u00a7D.2 \u2013 Multiheaded Attention Pooling \u2013 Feature Extraction\nDetailed explanation of the Multiheaded Attention Pooling [MAP; Lee et al. 2018] feature extraction strategy, with analysis and results comparing to alternative methods.\nAppendix E \u2013 Adapting Representations for Evaluation\nWe provide further descriptions of the adaptation pipeline for each of the five evaluation domains.\nVoltron: Language-Driven Representations for Robotics"
        },
        {
            "heading": "A Motivating Questions",
            "text": "Q1. From the results, some Voltron models outperform larger models such as MVP-Base trained on significantly more data, even on tasks that do not necessarily need language information. How do you make sense of this?\nWe find that in many of our evaluation domains, especially domains with episodic tasks such as single-task and language-conditioned imitation learning, it is important to discern differences across frames in the same overall visual context, or otherwise pay attention to small visual distinctions. Looking at the original MVP work [Radosavovic et al. 2022], we see that the original pretraining datasets are compiled by sampling frames from various video datasets once, in a single-step procedure, at low sampling rates. For many datasets (such as Sth-Sth and Ego4D), this means only seeing 1-2 frames per video clip in total during training.\nIn contrast, when we sample data from Sth-Sth, we ensure to sample at least 5 frames per clip, per epoch; while the aggregate amount of diverse contexts is much lower than in the original MVP work, seeing multiple frames per context seems to significantly help learning, and not just forVoltron models! On the tasks whereVoltron models outperform MVP (EgoSoup) (with a larger ViT-Base encoder), we also see commensurate gains in our reproductions R-MVP and R-R3M. For example, R-MVP is at par with or only slightly less performant than MVP (EgoSoup) on grasp affordance prediction and single-task control. We offer further discussion in \u00a7D.1.\nQ2. Why don\u2019t you evaluate models trained with \ud835\udefc = 1 (pure language generation)?\nIn preliminary experiments, we partially pretrained variants of V \u2013 Gen with values \ud835\udefc = 0.25, 0.5, 0.75; we focused on evaluating the downstream performance of these representations in the context of the single task visuomotor control evaluation. With \ud835\udefc = 0.75 we observed significant performance degradation on control tasks; furthermore, looking at the pretraining loss curves, we saw the reconstruction error plateau early in training. We found that \ud835\udefc = 0.5 balanced learning, and allowed us to continue to push reconstruction error down while also pushing the language generator loss (cross-entropy) lower; with \ud835\udefc = 0.25, we saw the opposite trend as with \ud835\udefc = 0.75.\nThese results are to be taken with a grain of salt, given the limited pretraining duration. However, we worry that with \ud835\udefc = 1, we might suffer doubly for 1) never conditioning on language, which is so clearly helpful from our results, and 2) potentially fall into the same failure mode as the R-M3AE multimodal masked autoencoder from Section \u00a76 in the main text, overfitting to the language loss. In general,V \u2013 Gen with \ud835\udefc = 0.5 already converges to a substantially higher reconstruction loss asV \u2013 Cond andV \u2013 Dual, as shown in the pretraining curves in \u00a7B.3. That being said, it is a promising avenue for future work to understand if this is inherent or a problem with the specific optimization procedure we used \u2013 perhaps changing the relative scaling of the two losses over the course of pretraining may mitigate this issue, or even adaptively clipping the gradient updates depending on the relative contribution of the visual reconstructor or language generator.\nQ3. Why does language during pretraining help for downstream tasks that don\u2019t use language?\nConsider a masked visual input of a \u201cblack, curved object above a wooden surface.\u201d Given this information \u2013 and this information alone \u2013 what is a plausible reconstruction? There are myriad objects that fit those percepts \u2013 a black, curved object: we could lbe looking at the side of a bowl, the handle of a briefcase, the arm of a chair or stool, or in general, any number of possible options. A masked autoencoder optimizing for reconstruction must embed in the representation of this input as many of the features possible to enable good downstream reconstruction loss. It needs to model everything, as the visual context is under-specified and ambiguous. This compressed bottleneck is core to learning a masked autoencoder, but the unfortunate byproduct of this \u2013 in light of a vast world of possibilities \u2013 are representations that try to capture everything they possibly can.\nContrast this with a world in which you are told that the same visual context is associated with the language caption \u201clifting a black coffee mug on the table.\u201d What changes? The posterior over possible objects collapses down to the narrow slice of possibilities captured by \u201cblack coffee mug\u201d; under this new set of possibilities, what does the encoder focus on? What type of black coffee mug is on the table? If it is being lifted, how is it being lifted? From what part of the object \u2013 the handle (seen in frame), or somewhere else? What are the features that help further reconstruct the black coffee mug? The other nearby surfaces \u2013 what is the mug resting on (a wooden table? the wooden arm of a chair?), is it at an angle? The additional visual context \u2013 what type of scene are we in \u2013 a living room, a coffeehouse? What else can I specifically encode that helps me reconstruct this cup in high-fidelity? The edges of the cup, its texture, the way the light is reflecting off of it in this particular visible context?\nConditioning on a language description both simplifies and focuses what I need to represent. My encoded features are no longer general enough to cover the full range of objects that could follow from the visible context alone; instead, I can use that same capacity to represent this specific context, as denoted by language. The encoder can focus on all of things left unspecified by language \u2013 arguably, the very things we want a visual encoder for robotics to represent. Because we know that it is a \u201cblack coffee mug,\u201d we can encode features around different types of black coffee mugs as a first level, and at a second level, go deeper, and actually model the low-level features that are not tied to semantics, but tied to core, perceptual primitives: the texture of the mug, the edges/boundaries of the object relative to other objects, even the way light reflects off of the surface. These are the features that help in tasks like grasp affordance prediction (the edges of objects), and when we learn joint representations of language and vision, the features that help with localization (grounding referring expressions) and detection. Though speculative, we can attempt to make this concrete with results: if language is indeed reducing the space over plausible reconstructions (and focusing the encoder), we might expect lower reconstruction error when language-conditioning vs. when we condition\nKaramcheti et. al.\nsolely on the visual context alone. This is exactly what we show in \u00a7C.1, and a hint at whyVoltron is able to perform so strongly downstream (even without language input). The simple presence of language during pretraining refocuses the features in our representations.\nQ4. Why only evaluate frozen representations? Why not fully finetune the backbones for each downstream evaluation?\nBoth MVP and R3M [Radosavovic et al. 2022; Nair et al. 2022] only evaluate frozen visual representations, following a precedent set by a long tradition of work in self-supervised learning from the computer vision community [Chen et al. 2020; Radford et al. 2021; He et al. 2022]. There are two reasons for the validity of evaluating frozen representations. First, the hope is that evaluating frozen representations (via adapted per-evaluation \u201cheads\u201d on top) help us isolate the relative impact of what the representations contain \u2013 otherwise, the separation between the standalone representations and the downstream evaluation parameters (and the co-mingling of the two when optimizing all weights via gradient descent) becomes much less clear. Second, for many of the evaluations we look at, we have extremely small amounts of data \u2013 on the order of 1000 examples for grasp affordance prediction, 10 - 20 demonstrations for single task and language-conditioned imitation. There is a valid fear that full-finetuning the sizeable visual encoders vs. just the adaptation parameters (< 50K parameters) could lead to extreme overfitting. In general, finetuning large-scale Transformers from minimal data is an active area of research in and of itself, with work like adapters, low-rank approximations, and partial finetuning [Houlsby et al. 2019; Hu et al. 2021; Ben-Zaken et al. 2022].\nQ5. Assuming pretraining datasets of (video, language) pairs feels restrictive; is there a way to leverage other sources of data? While Voltron expects a dataset of videos and associated language narrations, there is a wealth of visually diverse and relevant data that does not subscribe to this type signature:: datasets of standalone images from curated datasets [Deng et al. 2009; Geiger et al. 2012; Yu et al. 2015], curated images paired with language captions as in Conceptual Captions [Lin et al. 2014; Sharma et al. 2018], and large in-the-wild datasets of images paired with text scraped from the internet [Schuhmann et al. 2021; Srinivasan et al. 2021; Schuhmann et al. 2022].\nLuckily (though beyond the scope of this initial work), incorporating this data into the existingVoltron learning pipeline is straightforward; for image data without language, we can simply \u201cannotate\u201d each example with an empty <NULL> token in the worst case, or alternatively, with some minimal textual metadata (e.g., a class label, dataset descriptor, or even a URL if available). To accommodate for training on variable length image contexts, a naive solution would be adopting frame dropout or padding; there are myriad ways to do this efficiently \u2013 from Perceiver-based resampling of large patch sequences [Jaegle et al. 2021; Alayrac et al. 2022] to different position encoding schemes [Su et al. 2021; Press et al. 2022], to more efficient attention variants [Beltagy et al. 2020].\nVoltron: Language-Driven Representations for Robotics\nB Voltron Implementation & Artifacts\nWe provide complete implementation details for the variousVoltron models, from the small modifications to the Transformer block for added pretraining stability, to the added structural components for embedding multimodal (vision and language) inputs. All of these details are made explicit in our code release, linked on our project page.\nB.1 Voltron Transformer Implementation As mentioned in \u00a74, we perform a series of modifications to the typical Transformer block used in prior work in the Vision Transformer and Masked Autoencoding literature to help with pretraining stability; these changes are motivated by recent work from the NLP community on training stable and performant Transformer models [Narang et al. 2021; Karamcheti et al. 2021a; Chowdhery et al. 2022].\nWe show the side-by-side comparison of the \u201cstandard\u201d Transformer block implementation vs. the Voltron Transformer block in Figure 8. The changes are three-fold:\n\u2022 Using Root Mean-Square Normalization [Zhang and Sennrich 2019] over the default LayerNorm; not only does RMSNorm have fewer parameters, but it has been shown to increase stability and performance [Narang et al. 2021]. \u2022 Using the SwishGLU activation [Shazeer 2020; Chowdhery et al. 2022] over the default GELU [Hendrycks and Gimpel 2016]. \u2022 Using LayerScale [Touvron et al. 2021] for scaling down the magnitude of residual connections; prior work has found this to have a powerful stabilizing effect during pretraining [Karamcheti et al. 2021a].\nWe also provide pseudocode for implementing the various modifications in Figure 8 (bottom); these modifications are all simple and transferable across Transformer implementations. Furthermore, as part of the no-language implementation in \u00a76, we ablate the effects of these modifications on performance; we find that these modifications do not change downstream performance, but significantly increase pretraining stability, following our initial motivation.\nKaramcheti et. al."
        },
        {
            "heading": "B.2 Jointly Processing Vision & Language",
            "text": "To incorporate language into the typical masked autoencoding pipeline, we add a series of small structural changes to handle 1) multi-modality, 2) sharing a Transformer decoder for both visual reconstruction and language generation, and 3) handling position encoding for both visual patch embeddings and textual tokens.\nMultimodal Encoder.We make the following adjustments to enable a Transformer encoder to embed multiple modalities. First, we project both our learned \u201cpatch embeddings\u201d (obtained as in a standard ViT, by learning a linear transformation of our flattened RGB patches of size \ud835\udc5d \u00d7 \ud835\udc5d \u00d7 3) and our pretrained language embeddings to the same space R\ud835\udc51 , where \ud835\udc51 is the Transformer dimensionality (e.g., \ud835\udc51 = 384 for a ViT-Small). While we learn our patch embedding end-to-end, we initialize our language embeddings from a pretrained (and frozen) DistilBERT model [Sanh et al. 2019]; this is following R3M [Nair et al. 2022]. We pad each language annotation \ud835\udc50 in our dataset to a maximum length \ud835\udc3f = 20 tokens, additionally storing a binary length mask to ensure that each Transformer block does not attend to padding.\nOnce projected into the Transformer\u2019s embedding space, we add learned modality embeddings (e.g., an embedding for <IMG> and <LANG>) to each of the respective inputs; we find that this better allows the Transformer to reason over different modalities. We initialize these learnable embeddings via a truncated normal distribution, with scale \ud835\udf0e = 0.02, following how other special embeddings are initialized in the MAE and Vision Transformer literature [He et al. 2022].\nThe final step is for handling multi-frame contexts; we learn a set of frame index embeddings (e.g., for FRAME-1, FRAME-2, etc.) and add these to the corresponding patch embeddings \u2013 i.e. we add the FRAME-i embedding to all patch embeddings from the first frame and so on. This further allows us to distinguish individual frame patches from one another.\nAt this point, we concatenate the full sequence of flattened visual patch embeddings and language token embeddings, and feed them through the stack of Transformer blocks that form the multimodal encoder. This output is fed to the decoder, in the same fashion as a traditional masked autoencoder.\nShared Transformer for Reconstruction & Generation. As mentioned in \u00a74, we make one crucial change to the standard Transformer decoder in a masked autoencoder to additionally allow for language generation: namely adding a prefix mask over the language inputs [Raffel et al. 2019]. The goal of this mask (as stated in the main text) is to prevent information leakage when decoding; this mask selectively zeroes out dependencies in the multiheaded attention during training such that when generating language given a visual context, each language embedding at a given timestep \ud835\udc61 can only attend to prior generated language at timesteps < \ud835\udc61 , as well as the entire visual context. This masking operates in the same way as the original decoder masking described in Vaswani et al. [2013]; the attention scores for all \u201cinvalid\u201d inputs (> \ud835\udc61 ) are set to 0, restricting the model from incorporating future predictions as it processes the sequence.\nApart from this, the only other change we make to the MAE decoder is learning a separate set of modality embeddings (as described in the prior section) \u2013 i.e. embeddings for <IMG-DECODER> and <LANG-DECODER>; the reason for this is that the Decoder sees a series of <MASK> embeddings representing the \u201cunseen\u201d visible context to reconstruct, as well as the new language context to generate (recall that because of the \ud835\udefc gating, the language generator never sees language embeddings from the encoder). We add these to the corresponding embeddings fed to the decoder, then resume the standard MAE decoding pipeline (reconstructing visual patches), and the language generation pipeline (autoregressively generating the original annotation).\nPosition Encoding.We follow standard pratice in the masked autoencoding literature (and the same practice used by MVP), as position encode each of the patch embeddings subject to a fixed (deterministic) 2D sinusoidal embedding that reflects both vertical and horizontal positioning of each patch within a grid \u2013 this is taken directly from the original MAE codebase. To encode text, we use a similar strategy, using a 1D sinusoidal embedding added to each token embedding in a sequence."
        },
        {
            "heading": "B.3 Pretraining Curves",
            "text": "To further contextualize our results and enrich some of the discussion \u00a76 (and further on in the appendices), we include the pretraining loss curves for each of the three Voltron models we train in this work \u2013V \u2013 Cond,V \u2013 Dual, andV \u2013 Gen. The reconstruction error curves for the three models can be found in Figure 9. In general, we find that the \u201ctrade-off\u201d between language-conditioned reconstruction and visually-grounded language generation is made concrete in the pretraining loss \u2013 both purely language-conditioned models (V \u2013 Cond, V \u2013 Dual with \ud835\udefc = 0) converge to fairly low reconstruction error; however,V \u2013 Gen (with \ud835\udefc = 0.5) converges to a much higher reconstruction error \u2013 due to the tension between optimizing for both reconstruction and language generation. We additionally note that adding even simple, dual-frame contexts enables lower reconstruction error \u2013 even with the ViT-Small models, on the Sth-Sth dataset.\nVoltron: Language-Driven Representations for Robotics\nB.4 Index of Released Artifacts All of the following are linked in our code release and project page:\n\u2022 Checkpoints forV \u2013 Cond,V \u2013 Dual, andV \u2013 Gen after 400 epochs of training on Sth-Sth. \u2022 Checkpoints for our reproductions R-MVP and R-R3M (both with a ViT-S and RN-50 backbone). \u2022 All index files (serialized frames/order seen during training) for reproducible pretraining. \u2022 Intermediate checkpoints every 20 epochs for each of the threeVoltron models \u2013 along with optimizer states. \u2022 Checkpoints for the ViT-Base variant of V \u2013 Cond (86M parameters vs. 22M for a ViT-Small).\nThe modeling code release additionally provides documentation and scripts for 1) training these models from scratch, and 2) downloading and extracting representations from the pretrained models. The evaluation code release provides a unified API for the various problems we evaluate on in this work.\nKaramcheti et. al."
        },
        {
            "heading": "C Additional Results & Visualizations",
            "text": "We present additional results and visualizations to further support our claims from the main text. We provide additional discussion of 1) the impact of language supervision (in the context of pretraining reconstruction loss), 2) a further discussion of masked vs. generative language modeling as an objective, with an analysis of pretraining language modeling loss, 3) additional single task control results on the Adroit dexterous manipulation environments, 4) qualitative trajectory rollouts from theV \u2013 Gen language-conditioned imitation policy, and 5) additional qualitative intent scoring results."
        },
        {
            "heading": "C.1 Analysis: Impact of Language-Conditioning on Reconstruction Loss",
            "text": "As part of the ablation experiments in \u00a76, we evaluate the impact of language-supervision during pretraining via a no-language ablation, training a single-frame masked autoencoder with the Voltron Transformer architecture as described in \u00a7B.1; this resulting model does not condition on language at all, but is otherwise identical to V \u2013 Cond. In the main text, we evaluated the corresponding no-language model on a subset of evaluation tasks, showing a noticeable drop in performance across every evaluated application (even those without language input) \u2013 thereby showing concrete evidence as to the value of language-driven pretraining. Here we expand on those results by characterizing the behavior of bothV \u2013 Cond and the no-language ablation thereof in terms of their pretraining behavior.\nFigure 10 shows the reconstruction error for both V \u2013 Cond (yellow) and the no-language ablation (gray) over the course of pretraining. There are two noticeable properties of these curves: first, V \u2013 Cond converges to a substantially lower reconstruction error than the same model trained without language. Second,V \u2013 Cond is able to learn faster, showing a steeper decline in reconstruction error earlier on in training. Taken together, these curves suggest that language-conditioning is able to focus feature learning in a way that allows the learned visual encoder to better encode masked contexts \u2013 especially considering that the visual reconstructor is by definition not language-conditioned. Furthermore, from the aggregate evaluation results, the features learned as a result somehow generalize better across the board, from low-level tasks like grasp affordance prediction, to high-level tasks such as control.\nVoltron: Language-Driven Representations for Robotics"
        },
        {
            "heading": "C.2 Analysis: Generative vs. Masked Language Modeling",
            "text": "Later in \u00a76, we raise the question: why generative (autoregressive) language modeling over masked language modeling? To help contextualize this choice, we look at recent work on combining masked autoencoders (for vision) with masked language modeling (for text), through multimodal masked autoencoders [M3AE; Geng et al. 2022]. We reimplement this M3AE model, pretraining on the same Sth-Sth dataset used throughout this work, following the same standard of quality as for R-MVP and R-R3M. When we evaluate the corresponding R-M3AE model, we notice substantially worse performance across all evaluation domains; in the main text we attributed this to overfitting during pretraining \u2013 here, we provide that concrete evidence.\nFigure 11 shows the language model perplexity over time for both the R-M3AE, and the V \u2013 Gen model (trained with \ud835\udefc = 0.5). Perplexity (PPL) = exp(NLL) is a monotonic function of the cross-entropy loss; lower values are \u201cbetter\u201d with a lower bound value of 1.0. Almost immediately, the R-M3AE model overfits to the masked language modeling task, hitting a \u201cperfect\u201d perplexity of 1 (loss of 0.0) within the first 20 epochs. Contrast this withV \u2013 Gen that learns to gradually lower perplexity of the entire course of training, almost driving down to a PPL of 1.0 by the 400th epoch. We attribute R-M3AE\u2019s poor performance to this extremely early overfitting of the language loss, again echoing the hypothesis that language generation is slightly more robust to these settings \u2013 predict short language captions given visual context \u2013 than a masked language modeling objective. We note that this pretraining data (Sth-Sth) is significantly different than the data used to train the original M3AE model in Geng et al. [2022]; the original M3AE work used Conceptual Captions 12M [Sharma et al. 2018], a rich dataset of images paired with long, descriptive captions. Further work on extending M3AE models as in Liu et al. [2022] further pretrain on text-only datasets such as Wikipedia and Toronto Books [Devlin et al. 2019] suggesting the need for diverse, broad coverage text when training (multimodal) masked language models."
        },
        {
            "heading": "C.3 Results: Adroit Visuomotor Control",
            "text": "To supplement our single-task visuomotor control results, we run out evaluations on the Adroit dexterous manipulation tasks from the R3M paper [Nair et al. 2022]. The two tasks we evaluate on, depicted in Figure 12 (left) consist of controlling a high degree-of-freedom robotic hand (24-DoF) for the task of 1) relocating a ball on the table to a specified target position, and 2) reorienting a pen within the hand to reach a target orientation. Given the innate difficulty of controlling a high-dimensional dexterous robotic hand over a 9-DoF fixed arm manipulator, these tasks are evaluated with \ud835\udc5b \u2208 [25, 50, 100] demonstrations instead of \ud835\udc5b \u2208 [5, 10, 25] as with the Franka Kitchen evaluation. In general, learning policies in this environment is difficult, especially from limited data.\nLooking to the results we see that on this environment,V \u2013 Gen and R-R3M models tend to be the most performant, in contrast with the Franka Kitchen results which favored V \u2013 Cond and V \u2013 Dual (the reconstruction-leaning models). Interestingly, this flipped trend seems to suggest that even within single-task control, different tasks and environments seems to prefer different visual features to perform well \u2013 in this case, the more high-level features under models such as R-R3M and V \u2013 Gen seem to be preferred. In a way, this makes sense; unlike with Franka Kitchen, the actual background objects and interactions thereof \u2013 turning knobs, opening microwaves, or sliding doors with clearly marked handles \u2013 seem more sensitive to low-level features (where on the microwave is the handle, which knob of the various possible needs to be turned). In Adroit however, these tasks are on clean backgrounds, with individual objects; the high-level behaviors instead that are more important (e.g., \u201cis the ball getting closer to the target location?\u201d). It would be an interesting direction for future work\nKaramcheti et. al.\nto further profile other \u201ccommon\u201d visuomotor control tasks along this axis, to get a better understanding of what visual representations must capture to be useful in general tasks \u2013 to the extent of predicting ahead of time what features would be useful to aid in solving a task."
        },
        {
            "heading": "C.4 Qualitative: Real-Robot Language-Conditioned Policy Rollouts",
            "text": "While the experimental results in \u00a75 capture the quantitative success rates of various methods for language-conditioned imitation, they do not paint a picture of how these policies behave. In Figure 13 we show three different rollouts for the best-performingV \u2013 Gen model: a task success (in-distribution), a task failure (in-distribution), and an example rollout from the visual distractor split. With the waypoint-based action space described in \u00a75, we generally see smooth motions; however, the failure mode of these policies are \u201coscillations\u201d (Figure 13; middle) where the policy collapses to predicting the same two waypoints repeatedly. We supplement these visualizations with full videos of rollouts from each representation learning approach \u2013 these are all on our project page."
        },
        {
            "heading": "C.5 Qualitative: Additional Intent Scoring Visualizations",
            "text": "Figure 14 presents additional intent scoring qualitative visualizations for two other tasks from the WHiRL dataset [Bahl et al. 2022] \u2013 specifically \u201clifting the lid off a pot\u201d and \u201cstacking cups.\u201d In both scenarios, we see similar behavior to the results from \u00a7V of the main text:V \u2013 Gen shows a propensity for not only tracking the key progress points in the videos for both human and robot agents, but also providing a dense and smooth measure of intermediate progress. Both CLIP (ViT-Base) and R3M (Ego4D) unfortunately predict high-variance scores, seemingly random across the video.\nVoltron: Language-Driven Representations for Robotics"
        },
        {
            "heading": "D Data-Equivalent Reproductions & Reproducibility",
            "text": "In this section we provide additional discussion around two aspects of the reproduction and pretraining procedure discussed in \u00a74: 1) preprocessing, and specifically the importance of selecting multiple images from the same context, and 2) how to operationalize the representations from the visual encoder for downstream learning."
        },
        {
            "heading": "D.1 Additional Preprocessing Discussion",
            "text": "We described our preprocessing approach in \u00a74: following the R3M paper, we sample five frames from each video clip for each epoch of pretraining. Seeing multiple frames from the same visual context is minimally necessary for the R3M time-contrastive learning objective, but we posit in this discussion (following the questions in Appendix A) that repeatedly sampling from the same visual context \u2013 even with a reconstruction objective \u2013 allows for picking up on finer-grained changes within a context. The best evidence we have for this is in looking at how prior work constructs their pretraining datasets.\nThe original MVP work [Xiao et al. 2022; Radosavovic et al. 2022] constructs static datasets of images by iterating through the various video clips in their pretraining datasets \u2013 Sth-Sth, Ego4D [Grauman et al. 2022], 100 Days of Hands [Shan et al. 2020] \u2013 at a fixed rate, usually from 0.2 to 1 frames per second. Given video clip lengths of 2 seconds, this means that in aggregate these pretraining datasets comprise maybe 2-3 frames sampled from the same clip, if that. Contrast that with this work and R3M, sampling multiple frames from each video clip for every pretraining epoch (for 400 epochs). This not only means that we are seeing the same context repeatedly, but also that we are seeing different views of the same context; this can help tune reconstruction towards picking up on finer-grained features (e.g., if a high-capacity model is able to memorize prior contexts given enough repetition).\nThis offers a (again, speculative) explanation of why Voltron models outperform MVP (EgoSoup) models that are both higher-capacity and trained on orders of magnitude more data \u2013 but definitely requires further experiments to prove. In the meantime, it seems as though taking steps to use as much of the pretraining datasets we have access to as possible is in our best interest.\nKaramcheti et. al."
        },
        {
            "heading": "D.2 Multiheaded Attention Pooling \u2013 Extracting Representations",
            "text": "There is a critical difference between pretraining visual representations and identifying the \u201cright\u201d way to use these representations for downstream adaptation tasks. Especially for Vision Transformers trained as part of a masked autoencoder \u2013 as mentioned at the end of Section \u00a74 of the main text \u2013 identifying a method for extracting information from the learned representations is an open problem. The main text states \u2013 by fiat \u2013 that we use multiheaded attention pooling [MAP; Lee et al. 2018] as suggested by Zhai et al. [2022] to operationalize our learned representations for our downstream tasks. Here, we further contextualize that decision with a description of alternative approaches, as well as comparative results (Table 5) that show the superiority of MAP-based \u201cfeature extraction\u201d (referring to the process of taking the output of a Vision Transformer and producing a dense, summary vector for downstream learning) over alternative approaches.\nMVP and prior work in masked autoencoding with Vision Transformers [He et al. 2022] make an interesting choice when it comes to extracting features: during pretraining, these works append a dummy <CLS> token to the input of the encoder and decoder in the masked autoencoding pipeline (depicted in Figure 15). This \u201cfree\u201d embedding is motivated by how Vision Transformers for supervised learning (e.g., classification) are parameterized: in these settings, after encoding an input image, the <CLS> embedding is used as (the sole) input to a linear projection into label space, thus obtaining supervision from the global loss function (e.g., the cross-entropy loss for classification). Crucially, the <CLS> embedding in these cases gets direct supervision during training. However, in the masked autoencoding setting, this <CLS> embedding is just passed through the various Transformer layers of the encoder and decoder, never obtaining any direct or indirect\nVoltron: Language-Driven Representations for Robotics\nsupervision; while it does attend to all other patch embeddings as a byproduct of the multiheaded attention mechanism, there is no guarantee that this embedding captures or summarize all the useful information necessary.\nInstead, recent work from the same authors of the original Vision Transformer [Zhai et al. 2022] eschew the <CLS> embedding completely during training, instead identifying that two other strategies \u2013 mean-pooling all the patch embeddings output by the encoder, or using multiheaded attention pooling [Lee et al. 2018] \u2013 are almost always preferable. As an aside \u2013 this work is what motivates Voltron models to also do away with the <CLS> embedding.\nMultiheaded attention pooling (MAP) can be thought of as a form of cross-attention with a learned query. Starting with a randomly initialized query vector (or optionally, set of query vectors), a MAP block implements a shallow multiheaded attention operation, using the initialized query vector to cross-attend over the patch embeddings output by the Vision Transformer \u2013 the resulting output is a \u201cweighted\u201d combination of the individual patch embeddings that is shaped on a per-adaptation basis. We evaluate MAP-based extraction against mean-pooling and any other \u201cdefault\u201d strategy (e.g., the <CLS> embedding used in MVP, the learned dense representation under CLIP) in Table 5. We find that MAP universally outperforms all other strategies on the Franka Kitchen control tasks (with \ud835\udc5b = 10 demonstrations), informing our usage of MAP as the sole feature extraction approach throughout this work. Notably, we find that MAP-based extraction when applied to the original model MVP (EgoSoup) released in the original work almost doubles success rate on downstream control tasks. We even find that simple mean-pooling over patches outperforms the <CLS> embedding, further motivating alternate strategies.\nKaramcheti et. al."
        },
        {
            "heading": "E Adapting Representations for Evaluation",
            "text": "The description of the adaptation pipeline described in \u00a75 outlines all major details for the adaptation experiments for each evaluation domain; the role of this section is to clarify any potentially ambiguous details, and further motivate some of the choices we make in implementing each evaluation. In general, all of the details for adapting representations for each evaluation in the same manner used in this work are in the released evaluation code repository that provides a unified harness for evaluating arbitrary visual representations on all evaluation domains used in this work \u2013 this codebase is also linked from our project page.\nIn general, for each evaluation domain, we keep the adaptation architecture as simple as possible, and optimization parameters simple as well. For all applications we use an AdamW optimizer [Kingma and Ba 2015] with the default learning rate of 1e-3, and weight decay of 0.01.\nGrasp Affordance Prediction.We implement the adaptation head for the grasp affordance prediction task following recent work in learning segmentation heads on top of vision transformer features, specifically following the procedure outlined in Segmentation Transformers via Progressive Upsampling (SETR-PUP) [Zheng et al. 2021]. A PUP block is straightforward \u2013 we first extract all patch embeddings from the output of our Vision Transformer encoder, using a shallow MAP block with the same number of seed vectors as patches output by the encoder. We then reshape the extracted features into a grid, then stack a series of 4 upsampling blocks (channel depths of [128, 64, 32, 16], ReLU activation) that consist of a 2D convolution followed by a bilinear upsampling, until we recover a grid of the same size of the original image. We finally apply a spatial softmax, predicting distributions over each of the possible labels (\u201cgraspable,\u201d \u201cnon-graspable,\u201d \u201cbackground\u201d), and compute our loss per-pixel. We optimize with a batch size of 64, for 50 epochs in total. Given the small size of the dataset, we find that there is a great deal of variance across random initializations; we report results by running 5-fold cross-validation, taking the model with the best performance across validation folds to compute final test statistics.\nReferring Expression Grounding. We use a simple adaptation head for referring expression grounding that extracts a single dense representation from our learned encoder via a shallow MAP block with a single seed vector (the default extractor for obtaining a vector representation of a visual input). For representations that are not language-conditioned, we concatenate this vector with the language embedding under the appropriate model \u2013 e.g., the CLIP text embedding for CLIP (ViT-Base) \u2013 or the DistilBERT language embedding for pure visual models (e.g., MVP). We then feed this context through a 4-layer MLP (hidden dimensions of [512, 128, 128, 64], GELU activation) that directly predicts bounding box coordinates as (\ud835\udc65,\ud835\udc66,width, height). We use a Huber loss to compute error. We optimize with a batch size of 512, for 10 epochs in total, using the provided validation set for model selection.\nSingle-Task Visuomotor Control.We first extract a dense representation using a shallow MAP block (as described above), then follow the exact procedure for evaluating both Franka Kitchen and Adroit policy learning as described in the R3M work [Nair et al. 2022]. Namely, we concatenate the visual representation with the robot\u2019s proprioceptive state, followed by a BatchNorm layer [Ioffe and Szegedy 2015]. These are then fed to a 2-layer MLP (\ud835\udc51 = 256) that directly predicts action targets for computing mean-squared error against the ground-truth actions. Following R3M, we run 20,000 gradient steps with a batch size of 32, evaluating the models online every 5000 steps on a heldout set of 50 environments (fixed seed) \u2013 we report success rate subject to the best performing model from the online evaluation. We run three seeds for each combination of viewpoint, number of demonstrations, and task.\nReal-World Language-Conditioned Imitation. The full set of language instructions generated by ChatGPT can be found on our project page. For adaptation, we first extract a representation as with the referring expression evaluation by using a shallow MAP block, and concatenating the corresponding language embedding as appropriate. We concatenate this fused vector with the robot\u2019s proprioceptive state, and pass the corresponding embedding to a BatchNorm layer. Then, following recent work on real-world imitation learning [Mandlekar et al. 2021], we only train a shallow 2-layer MLP with (\ud835\udc51 = 64) to predict action targets for computing mean-squared error against the ground-truth waypoint actions. We optimize with a batch size of 256, and train for 10 epochs. As policy evaluation in the real-world is expensive \u2013 especially for the five approaches we evalaute \u2013 we uniformly choose the last epoch checkpoint to perform evaluation rollouts.\nQualitative: Zero-Shot Intent Scoring. This is a zero-shot evaluation with no adaptation data, only applicable to the representation learning models capable of \u201cscoring\u201d joint vision-language contexts: V \u2013 Gen, CLIP (ViT-Base), and R3M (Ego4D). We download videos from the WHiRL dataset off of the WHiRL website: https://human2robot.github.io/. To generate plots, we sample frames at 2 FPS from each video, center cropping and resizing each frame prior to passing it to each model."
        }
    ],
    "title": "Language-Driven Representation Learning for Robotics",
    "year": 2023
}