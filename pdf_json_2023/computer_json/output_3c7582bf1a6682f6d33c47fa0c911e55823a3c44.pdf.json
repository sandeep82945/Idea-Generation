{
    "abstractText": "Despite the remarkable progress in deep generative models, synthesizing high-resolution and temporally coherent videos still remains a challenge due to their highdimensionality and complex temporal dynamics along with large spatial variations. Recent works on diffusion models have shown their potential to solve this challenge, yet they suffer from severe computationand memory-inefficiency that limit the scalability. To handle this issue, we propose a novel generative model for videos, coined projected latent video diffusion model (PVDM), a probabilistic diffusion model which learns a video distribution in a lowdimensional latent space and thus can be efficiently trained with high-resolution videos under limited resources. Specifically, PVDM is composed of two components: (a) an autoencoder that projects a given video as 2D-shaped latent vectors that factorize the complex cubic structure of video pixels and (b) a diffusion model architecture specialized for our new factorized latent space and the training/sampling procedure to synthesize videos of arbitrary length with a single model. Experiments on popular video generation datasets demonstrate the superiority of PVDM compared with previous video synthesis methods; e.g., PVDM obtains the FVD score of 639.7 on the UCF-101 long video (128 frames) generation benchmark, which improves 1773.4 of the prior state-of-the-art.",
    "authors": [
        {
            "affiliations": [],
            "name": "Sihyun Yu"
        },
        {
            "affiliations": [],
            "name": "Kihyuk Sohn"
        },
        {
            "affiliations": [],
            "name": "Subin Kim"
        },
        {
            "affiliations": [],
            "name": "Jinwoo Shin"
        }
    ],
    "id": "SP:27fc6c64fb15dfe0339ca0384c312a4253d0db93",
    "references": [
        {
            "authors": [
                "Dinesh Acharya",
                "Zhiwu Huang",
                "Danda Pani Paudel",
                "Luc Van Gool"
            ],
            "title": "Towards high resolution video generation with progressive growing of sliced Wasserstein GANs",
            "venue": "arXiv preprint arXiv:1810.02419,",
            "year": 2018
        },
        {
            "authors": [
                "Daniel Adiwardana",
                "Minh-Thang Luong",
                "David R So",
                "Jamie Hall",
                "Noah Fiedel",
                "Romal Thoppilan",
                "Zi Yang",
                "Apoorv Kulshreshtha",
                "Gaurav Nemade",
                "Yifeng Lu"
            ],
            "title": "Towards a human-like open-domain chatbot",
            "venue": "arXiv preprint arXiv:2001.09977,",
            "year": 2020
        },
        {
            "authors": [
                "Martin Arjovsky",
                "Soumith Chintala",
                "L\u00e9on Bottou"
            ],
            "title": "Wasserstein generative adversarial networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Gedas Bertasius",
                "Heng Wang",
                "Lorenzo Torresani"
            ],
            "title": "Is space-time attention all you need for video understanding",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell",
                "Sandhini Agarwal",
                "Ariel Herbert-Voss",
                "Gretchen Krueger",
                "Tom Henighan",
                "Rewon Child",
                "Aditya Ramesh",
                "Daniel Ziegler",
                "Jeffrey Wu",
                "Clemens Winter",
                "Chris Hesse",
                "Mark Chen",
                "Eric Sigler",
                "Mateusz Litwin",
                "Scott Gray",
                "Benjamin Chess",
                "Jack Clark",
                "Christopher Berner",
                "Sam McCandlish",
                "Alec Radford",
                "Ilya Sutskever",
                "Dario Amodei"
            ],
            "title": "Language models are fewshot learners",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Eric R Chan",
                "Connor Z Lin",
                "Matthew A Chan",
                "Koki Nagano",
                "Boxiao Pan",
                "Shalini De Mello",
                "Orazio Gallo",
                "Leonidas J Guibas",
                "Jonathan Tremblay",
                "Sameh Khamis"
            ],
            "title": "Efficient geometry-aware 3d generative adversarial networks",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Aidan Clark",
                "Jeff Donahue",
                "Karen Simonyan"
            ],
            "title": "Adversarial video generation on complex datasets",
            "venue": "arXiv preprint arXiv:1907.06571,",
            "year": 2019
        },
        {
            "authors": [
                "Prafulla Dhariwal",
                "Heewoo Jun",
                "Christine Payne",
                "Jong Wook Kim",
                "Alec Radford",
                "Ilya Sutskever"
            ],
            "title": "Jukebox: A generative model for music",
            "venue": "arXiv preprint arXiv:2005.00341,",
            "year": 2020
        },
        {
            "authors": [
                "Prafulla Dhariwal",
                "Alex Nichol"
            ],
            "title": "Diffusion models beat GANs on image synthesis",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Patrick Esser",
                "Robin Rombach",
                "Bj\u00f6rn Ommer"
            ],
            "title": "Taming transformers for high-resolution image synthesis",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Gereon Fox",
                "Ayush Tewari",
                "Mohamed Elgharib",
                "Christian Theobalt"
            ],
            "title": "StyleVideoGAN: A temporal generative model using a pretrained StyleGAN",
            "venue": "arXiv preprint arXiv:2107.07224,",
            "year": 2021
        },
        {
            "authors": [
                "Songwei Ge",
                "Thomas Hayes",
                "Harry Yang",
                "Xi Yin",
                "Guan Pang",
                "David Jacobs",
                "Jia-Bin Huang",
                "Devi Parikh"
            ],
            "title": "Long video generation with time-agnostic vqgan and time-sensitive transformer",
            "venue": "In European Conference on Computer Vision, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Ian Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "Generative adversarial nets",
            "venue": "Advances in neural information processing systems,",
            "year": 2014
        },
        {
            "authors": [
                "Cade Gordon",
                "Natalie Parde"
            ],
            "title": "Latent neural differential equations for video generation",
            "venue": "In NeurIPS 2020 Workshop on Pre-registration in Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Shuyang Gu",
                "Dong Chen",
                "Jianmin Bao",
                "Fang Wen",
                "Bo Zhang",
                "Dongdong Chen",
                "Lu Yuan",
                "Baining Guo"
            ],
            "title": "Vector quantized diffusion model for text-to-image synthesis",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "William Harvey",
                "Saeid Naderiparizi",
                "Vaden Masrani",
                "Christian Weilbach",
                "Frank Wood"
            ],
            "title": "Flexible diffusion modeling of long videos",
            "venue": "arXiv preprint arXiv:2205.11495,",
            "year": 2022
        },
        {
            "authors": [
                "Yingqing He",
                "Tianyu Yang",
                "Yong Zhang",
                "Ying Shan",
                "Qifeng Chen"
            ],
            "title": "Latent video diffusion models for high-fidelity video generation with arbitrary lengths",
            "venue": "arXiv preprint arXiv:2211.13221,",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Ho",
                "William Chan",
                "Chitwan Saharia",
                "Jay Whang",
                "Ruiqi Gao",
                "Alexey Gritsenko",
                "Diederik P. Kingma",
                "Ben Poole",
                "Mohammad Norouzi",
                "David J. Fleet",
                "Tim Salimans"
            ],
            "title": "Imagen video: High definition video generation with diffusion models",
            "venue": "arXiv preprint arXiv:2210.02303,",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan Ho",
                "Tim Salimans"
            ],
            "title": "Classifier-free diffusion guidance",
            "venue": "arXiv preprint arXiv:2207.12598,",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Ho",
                "Tim Salimans",
                "Alexey Gritsenko",
                "William Chan",
                "Mohammad Norouzi",
                "David J Fleet"
            ],
            "title": "Video diffusion models",
            "venue": "arXiv preprint arXiv:2204.03458,",
            "year": 2022
        },
        {
            "authors": [
                "Tobias H\u00f6ppe",
                "Arash Mehrjou",
                "Stefan Bauer",
                "Didrik Nielsen",
                "Andrea Dittadi"
            ],
            "title": "Diffusion models for video prediction and infilling",
            "venue": "arXiv preprint arXiv:2206.07696,",
            "year": 2022
        },
        {
            "authors": [
                "Emmanuel Kahembwe",
                "Subramanian Ramamoorthy"
            ],
            "title": "Lower dimensional kernels for video discriminators",
            "venue": "Neural Networks,",
            "year": 2020
        },
        {
            "authors": [
                "Nal Kalchbrenner",
                "A\u00e4ron Oord",
                "Karen Simonyan",
                "Ivo Danihelka",
                "Oriol Vinyals",
                "Alex Graves",
                "Koray Kavukcuoglu"
            ],
            "title": "Video pixel networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Andrej Karpathy",
                "George Toderici",
                "Sanketh Shetty",
                "Thomas Leung",
                "Rahul Sukthankar",
                "Li Fei-Fei"
            ],
            "title": "Large-scale video classification with convolutional neural networks",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2014
        },
        {
            "authors": [
                "Tero Karras",
                "Timo Aila",
                "Samuli Laine",
                "Jaakko Lehtinen"
            ],
            "title": "Progressive growing of GANs for improved quality, stability, and variation",
            "venue": "International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Tero Karras",
                "Miika Aittala",
                "Samuli Laine",
                "Erik H\u00e4rk\u00f6nen",
                "Janne Hellsten",
                "Jaakko Lehtinen",
                "Timo Aila"
            ],
            "title": "Aliasfree generative adversarial networks",
            "venue": "arXiv preprint arXiv:2106.12423,",
            "year": 2021
        },
        {
            "authors": [
                "Tero Karras",
                "Samuli Laine",
                "Miika Aittala",
                "Janne Hellsten",
                "Jaakko Lehtinen",
                "Timo Aila"
            ],
            "title": "Analyzing and improving 9 the image quality of StyleGAN",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Subin Kim",
                "Sihyun Yu",
                "Jaeho Lee",
                "Jinwoo Shin"
            ],
            "title": "Scalable neural video representations with learnable positional features",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Max Welling"
            ],
            "title": "Auto-encoding variational bayes",
            "venue": "In International Conference on Learning Representations,",
            "year": 2014
        },
        {
            "authors": [
                "Zhifeng Kong",
                "Wei Ping",
                "Jiaji Huang",
                "Kexin Zhao",
                "Bryan Catanzaro"
            ],
            "title": "Diffwave: A versatile diffusion model for audio synthesis",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Kushal Lakhotia",
                "Evgeny Kharitonov",
                "Wei-Ning Hsu",
                "Yossi Adi",
                "Adam Polyak",
                "Benjamin Bolte",
                "Tu-Anh Nguyen",
                "Jade Copet",
                "Alexei Baevski",
                "Adelrahman Mohamed"
            ],
            "title": "Generative spoken language modeling from raw audio",
            "venue": "arXiv preprint arXiv:2102.01192,",
            "year": 2021
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Shitong Luo",
                "Wei Hu"
            ],
            "title": "Diffusion probabilistic models for 3d point cloud generation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Kangfu Mei",
                "Vishal M. Patel"
            ],
            "title": "Vidm: Video implicit diffusion models",
            "venue": "In AAAI Conference on Artificial Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Andres Munoz",
                "Mohammadreza Zolfaghari",
                "Max Argus",
                "Thomas Brox"
            ],
            "title": "Temporal shift GAN for large scale video generation",
            "venue": "In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Aaron Oord",
                "Yazhe Li",
                "Igor Babuschkin",
                "Karen Simonyan",
                "Oriol Vinyals",
                "Koray Kavukcuoglu",
                "George Driessche",
                "Edward Lockhart",
                "Luis Cobo",
                "Florian Stimberg"
            ],
            "title": "Parallel wavenet: Fast high-fidelity speech synthesis",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Ben Poole",
                "Ajay Jain",
                "Jonathan T Barron",
                "Ben Mildenhall"
            ],
            "title": "Dreamfusion: Text-to-3d using 2d diffusion",
            "venue": "arXiv preprint arXiv:2209.14988,",
            "year": 2022
        },
        {
            "authors": [
                "Ruslan Rakhimov",
                "Denis Volkhonskiy",
                "Alexey Artemov",
                "Denis Zorin",
                "Evgeny Burnaev"
            ],
            "title": "Latent video transformer",
            "venue": "arXiv preprint arXiv:2006.10704,",
            "year": 2020
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Prafulla Dhariwal",
                "Alex Nichol",
                "Casey Chu",
                "Mark Chen"
            ],
            "title": "Hierarchical text-conditional image generation with clip latents",
            "venue": "arXiv preprint arXiv:2204.06125,",
            "year": 2022
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "High-resolution image synthesis with latent diffusion models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Masaki Saito",
                "Eiichi Matsumoto",
                "Shunta Saito"
            ],
            "title": "Temporal generative adversarial nets with singular value clipping",
            "venue": "In IEEE International Conference on Computer Vision,",
            "year": 2017
        },
        {
            "authors": [
                "Masaki Saito",
                "Shunta Saito",
                "Masanori Koyama",
                "Sosuke Kobayashi"
            ],
            "title": "Train sparsely, generate densely: Memoryefficient unsupervised training of high-resolution temporal GAN",
            "venue": "International Journal of Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Tim Salimans",
                "Ian Goodfellow",
                "Wojciech Zaremba",
                "Vicki Cheung",
                "Alec Radford",
                "Xi Chen"
            ],
            "title": "Improved techniques for training GANs",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2016
        },
        {
            "authors": [
                "Younggyo Seo",
                "Kimin Lee",
                "Fangchen Liu",
                "Stephen James",
                "Pieter Abbeel"
            ],
            "title": "Autoregressive latent video prediction with high-fidelity image generator",
            "venue": "In IEEE International Conference on Image Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Uriel Singer",
                "Adam Polyak",
                "Thomas Hayes",
                "Xi Yin",
                "Jie An",
                "Songyang Zhang",
                "Qiyuan Hu",
                "Harry Yang",
                "Oron Ashual",
                "Oran Gafni"
            ],
            "title": "Make-a-video: Text-to-video generation without text-video data",
            "venue": "In International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Ivan Skorokhodov",
                "Sergey Tulyakov",
                "Mohamed Elhoseiny"
            ],
            "title": "StyleGAN-V: A continuous video generator with the price, image quality and perks of StyleGAN2",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Ivan Skorokhodov",
                "Sergey Tulyakov",
                "Yiqun Wang",
                "Peter Wonka"
            ],
            "title": "Epigraf: Rethinking training of 3d gans",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Jascha Sohl-Dickstein",
                "Eric Weiss",
                "Niru Maheswaranathan",
                "Surya Ganguli"
            ],
            "title": "Deep unsupervised learning using nonequilibrium thermodynamics",
            "venue": "In International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Jascha Sohl-Dickstein",
                "Eric Weiss",
                "Niru Maheswaranathan",
                "Surya Ganguli"
            ],
            "title": "Deep unsupervised learning using nonequilibrium thermodynamics",
            "venue": "In International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Jiaming Song",
                "Chenlin Meng",
                "Stefano Ermon"
            ],
            "title": "Denoising diffusion implicit models",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Yang Song",
                "Stefano Ermon"
            ],
            "title": "Generative modeling by estimating gradients of the data distribution",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Yang Song",
                "Jascha Sohl-Dickstein",
                "Diederik P Kingma",
                "Abhishek Kumar",
                "Stefano Ermon",
                "Ben Poole"
            ],
            "title": "Score-based generative modeling through stochastic differential equations",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Khurram Soomro",
                "Amir Roshan Zamir",
                "Mubarak Shah"
            ],
            "title": "UCF101: A dataset of 101 human actions classes from videos in the wild",
            "venue": "arXiv preprint arXiv:1212.0402,",
            "year": 2012
        },
        {
            "authors": [
                "Yu Tian",
                "Jian Ren",
                "Menglei Chai",
                "Kyle Olszewski",
                "Xi Peng",
                "Dimitris N Metaxas",
                "Sergey Tulyakov"
            ],
            "title": "A good image generator is what you need for high-resolution video synthesis",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Du Tran",
                "Lubomir Bourdev",
                "Rob Fergus",
                "Lorenzo Torresani",
                "Manohar Paluri"
            ],
            "title": "Learning spatiotemporal features with 3D convolutional networks",
            "venue": "In IEEE International Conference on Computer Vision,",
            "year": 2015
        },
        {
            "authors": [
                "Sergey Tulyakov",
                "Ming-Yu Liu",
                "Xiaodong Yang",
                "Jan Kautz"
            ],
            "title": "MoCoGAN: Decomposing motion and content for video generation",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Thomas Unterthiner",
                "Sjoerd van Steenkiste",
                "Karol Kurach",
                "Raphael Marinier",
                "Marcin Michalski",
                "Sylvain Gelly"
            ],
            "title": "Towards accurate generative models of video: A new metric & challenges",
            "venue": "arXiv preprint arXiv:1812.01717,",
            "year": 2018
        },
        {
            "authors": [
                "Arash Vahdat",
                "Karsten Kreis",
                "Jan Kautz"
            ],
            "title": "Score-based generative modeling in latent space",
            "venue": "In Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Aaron van den Oord",
                "Oriol Vinyals",
                "Koray Kavukcuoglu"
            ],
            "title": "Neural discrete representation learning",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Carl Vondrick",
                "Hamed Pirsiavash",
                "Antonio Torralba"
            ],
            "title": "Generating videos with scene dynamics",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2016
        },
        {
            "authors": [
                "Dirk Weissenborn",
                "Oscar T\u00e4ckstr\u00f6m",
                "Jakob Uszkoreit"
            ],
            "title": "Scaling autoregressive video models",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Wei Xiong",
                "Wenhan Luo",
                "Lin Ma",
                "Wei Liu",
                "Jiebo Luo"
            ],
            "title": "Learning to generate time-lapse videos using multi-stage dynamic generative adversarial networks",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Wilson Yan",
                "Yunzhi Zhang",
                "Pieter Abbeel",
                "Aravind Srinivas"
            ],
            "title": "VideoGPT: Video generation using VQ-VAE and transformers",
            "venue": "arXiv preprint arXiv:2104.10157,",
            "year": 2021
        },
        {
            "authors": [
                "Ruihan Yang",
                "Prakhar Srivastava",
                "Stephan Mandt"
            ],
            "title": "Diffusion probabilistic modeling for video generation",
            "venue": "arXiv preprint arXiv:2203.09481,",
            "year": 2022
        },
        {
            "authors": [
                "Sihyun Yu",
                "Jihoon Tack",
                "Sangwoo Mo",
                "Hyunsu Kim",
                "Junho Kim",
                "Jung-Woo Ha",
                "Jinwoo Shin"
            ],
            "title": "Generating videos with dynamics-aware implicit generative adversarial networks",
            "venue": "In The Tenth International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Vladyslav Yushchenko",
                "Nikita Araslanov",
                "Stefan Roth"
            ],
            "title": "Markov decision process for video generation",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops,",
            "year": 2019
        },
        {
            "authors": [
                "Qinsheng Zhang",
                "Yongxin Chen"
            ],
            "title": "Fast sampling of diffusion models with exponential integrator",
            "venue": "arXiv preprint arXiv:2204.13902,",
            "year": 2022
        },
        {
            "authors": [
                "Richard Zhang",
                "Phillip Isola",
                "Alexei A Efros",
                "Eli Shechtman",
                "Oliver Wang"
            ],
            "title": "The unreasonable effectiveness of deep features as a perceptual metric",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Recent progresses of deep generative models have shown their promise to synthesize high-quality, realistic samples in various domains, such as images [9,27,41], audio [8,31,32], 3D scenes [6, 38, 48], natural languages [2, 5], etc. As a next step forward, several works have been actively focusing on the more challenging task of video synthesis [12, 18, 21, 47, 55, 67]. In contrast to the success in other domains, the generation quality is yet far from real-world videos, due to the high-dimensionality and complexity of videos that contain complicated spatiotemporal dynamics in high-resolution frames.\nInspired by the success of diffusion models in handling complex and large-scale image datasets [9, 40], recent approaches have attempted to design diffusion models for videos [16, 18, 21, 22, 35, 66]. Similar to image domains, these methods have shown great potential to model video distribution much better with scalability (both in terms of spatial resolution and temporal durations), even achieving photorealistic generation results [18]. However, they suffer from severe computation and memory inefficiency, as diffusion models require lots of iterative processes in input space to synthesize samples [51]. Such bottlenecks are much more amplified in video due to a cubic RGB array structure.\nMeanwhile, recent works in image generation have proposed latent diffusion models to circumvent the computation and memory inefficiency of diffusion models [15, 41, 59]. Instead of training the model in raw pixels, latent diffusion models first train an autoencoder to learn a low-dimensional latent space succinctly parameterizing images [10, 41, 60] and then models this latent distribution. Intriguingly, the approach has shown a dramatic improvement in efficiency for synthesizing samples while even achieving state-of-the-art generation results [41]. Despite their appealing potential, however, developing a form of latent diffusion model for videos is yet overlooked.\nContribution. We present a novel latent diffusion model for videos, coined projected latent video diffusion model (PVDM). Specifically, it is a two-stage framework (see Figure 1 for the overall illustration):\n\u2022 Autoencoder: We introduce an autoencoder that represents a video with three 2D image-like latent vectors by factorizing the complex cubic array structure of videos. Specifically, we propose 3D\u2192 2D projections of videos at each spatiotemporal direction to encode 3D video pixels as three succinct 2D latent vectors. At a high level, we design one latent vector across the temporal direction to parameterize the common contents of the video (e.g., background), and the latter two vectors to encode the motion of a video. These 2D latent vectors are beneficial for achieving high-quality and succinct encoding of videos, as well as enabling compute-efficient diffusion model architecture design due to their image-like structure.\nar X\niv :2\n30 2.\n07 68\n5v 2\n[ cs\n.C V\n] 3\n0 M\nar 2\n02 3\n\u2022 Diffusion model: Based on the 2D image-like latent space built from our video autoencoder, we design a new diffusion model architecture to model the video distribution. Since we parameterize videos as imagelike latent representations, we avoid computation-heavy 3D convolutional neural network architectures that are conventionally used for handling videos. Instead, our architecture is based on 2D convolution network diffusion model architecture that has shown its strength in handling images. Moreover, we present a joint training of unconditional and frame conditional generative modeling to generate a long video of arbitrary lengths.\nWe verify the effectiveness of our method on two popular datasets for evaluating video generation methods: UCF101 [54] and SkyTimelapse [64]. Measured with Inception score (IS; higher is better [44]) on UCF-101, a representative metric of evaluating unconditional video generation, PVDM achieves the state-of-the-art result of 74.40 on UCF101 in generating 16 frames, 256\u00d7256 resolution videos. In terms of Fr\u00e9chet video distance (FVD; lower is better [58]) on UCF-101 in synthesizing long videos (128 frames) of 256\u00d7256 resolution, it significantly improves the score from 1773.4 of the prior state-of-the-art to 639.7. Moreover, compared with recent video diffusion models, our model shows a strong memory and computation efficiency. For instance, on a single NVIDIA 3090Ti 24GB GPU, a video diffusion model [21] requires almost full memory (\u224824GB) to train at 128\u00d7128 resolution with a batch size of 1. On the other hand, PVDM can be trained with a batch size of 7 at most per this GPU with 16 frames videos at 256\u00d7256 resolution.\nTo our knowledge, the proposed PVDM is the first latent diffusion model designed for video synthesis. We believe our work would facilitate video generation research towards efficient real-time, high-resolution, and long video synthesis under the limited computational resource constraints."
        },
        {
            "heading": "2. Related work",
            "text": "Video generation. Video generation is one of the longstanding goals in deep generative models. Many prior works have attempted to solve the problem and they mostly fall into three categories. First, there exists numerous attempts to extend image generative adversarial networks (GANs) [13] to generate videos [1,7,11,14,23,36,42,43,46,47,55,57,62,67, 68]; however, GANs often suffer from mode collapse problem and these methods are difficult to be scaled to complex, large-scale video datasets. Other approaches have proposed learning the distribution via training autoregressive models [12, 24, 39, 63, 65] using Transformers [61]. They have shown better mode coverage and video quality than GANbased approaches, but they require expensive computation and memory costs to generate longer videos [47]. Finally, recent works have attempted to build diffusion models [19] for videos [16, 21, 22, 46, 66], achieving state-of-the-art results, yet they also suffer from significant computation and memory inefficiency. Our method also takes an approach to diffusion models for videos; however, we consider the generative modeling in low-dimensional latent space to alleviate these bottlenecks of diffusion models.\nDiffusion models. Diffusion models [19, 49], which are categorized as score-based generative models [52,53], model the data distribution by learning a gradual iterative denoising process from the Gaussian distribution to the data distribution. Intriguingly, they show a strong promise in generating high-quality samples with wide mode coverage, even outperforming GANs in image synthesis [9] and enabling zero-shot text-to-image synthesis [40]. Not limited to images, diffusion models have shown their promise in other data domains, including point clouds [34], audio [31], etc. However, diffusion models suffer from severe computation inefficiency for data sampling due to the iterative denoising process in high-dimensional data space. To alleviate this problem, sev-\neral works have proposed an effective sampling process from trained diffusion models [51, 69] or learning the data distribution from low-dimensional latent space that amortizes the data [15, 41, 59]. We take the latter approach for designing a computation-efficient video diffusion model. Diffusion models for videos. Following the remarkable success of diffusion models in image domains, several works [16, 18, 21, 22, 35, 66] have extended them for video generation. Intriguingly, they often show much better results than prior works, even can be scaled-up to complex datasets, and achieves successful generation results on challenging text-tovideo synthesis task [18]. Despite their potential on modeling videos, scaling them for synthesizing high-resolution, long video is not straightforward due to a huge computation bottleneck caused by an unfavorable dimension increase of video data as 3D RGB arrays. We tackle this problem by modeling the video distribution in the low-dimensional latent space. Triplane representations. Several recent works in 3Daware generation [6, 48] have demonstrated that highdimensional 3D voxels can be effectively parameterized with 2D triplane latent representations without sacrificing the encoding quality. In particular, prior works have proposed GAN architectures that synthesize a 3D scene by generating three image-like latents as an approximation of complex 3D voxels. In contrast, we explore the effectiveness of triplane representations for encoding videos, and we build diffusion models instead of GANs with such representations."
        },
        {
            "heading": "3. Projected latent video diffusion model",
            "text": "We first formalize the problem of video generative modeling. Consider a dataset D= {xi}Ni=1 of size N , where each x\u2208D is sampled from the unknown data distribution pdata(x). Here, each x \u2208 R3\u00d7S\u00d7H\u00d7W is a video clip consisting of S frames at H \u00d7W (spatial) resolution. In video generative modeling, the goal is to learn a model distribution pmodel(x) that matches pdata(x) using D.\nTo accomplish the goal, we build a method based on diffusion models\u2014a type of generative model that models the data distribution pdata(x) by learning the reverse process of\nthe Markov diffusion process starting from pdata(x) to the Gaussian prior distribution N (0x, Ix). Remarkably, diffusion models are shown to synthesize high-quality samples without mode collapse problems and can be scaled to model complex datasets. However, diffusion models directly operate in raw input space; unlike other data domains (e.g., images), designing diffusion models for videos is challenging due to their cubic complexity and high-dimensionality as 3D tensors of RGB values. Our key contribution is to mitigate this issue by proposing diffusion models operating on a novel low-dimensional latent space that succinctly parameterizes videos by breaking down the complex 3D structure of video pixels into three 2D structures.\nIn the rest of this section, we describe our projected latent video diffusion model (PVDM) in detail. In Section 3.1, we provide an overview of (latent) diffusion models. In Section 3.2, we explain how we design PVDM in detail. Finally, in Section 3.3, we describe our training objective and sampling strategy to generate longer videos."
        },
        {
            "heading": "3.1. Latent diffusion models",
            "text": "At a high level, diffusion models learn the target distribution pdata(x) by learning a gradual denoising process from Gaussian prior distribution to reach pdata(x). Formally, diffusion models consider the reverse process p\u03b8(xt\u22121|xt) of the Markov diffusion process q(xt|xt\u22121) of a fixed length T > 0 starting from p(x0) := pdata(x) to p(xT ) :=N (0x, Ix). More specifically, q(xt|xt\u22121) is formalized as the following normal distribution with a pre-defined 0 < \u03b21, . . . , \u03b2T < 1 and \u03b1\u0304t := \u220ft i=1(1\u2212 \u03b2i):\nq(xt|xt\u22121) := N (xt; \u221a\n1\u2212 \u03b2txt\u22121, \u03b2tIx), q(xt|x0) = N (xt; \u221a \u03b1\u0304tx0, (1\u2212 \u03b1\u0304t)Ix).\nUsing the reparameterization trick [30], Ho et al. [19] shows the corresponding p\u03b8(xt\u22121|xt) can be learned as a denoising autoencoder \u03b8(xt, t) that denoises a noisy sample xt, which is trained with the following noise-prediction objective:\nEx0, ,t [ || \u2212 \u03b8(xt, t)||22 ] where xt = \u221a \u03b1\u0304tx0 + \u221a 1\u2212 \u03b1\u0304t ,\nAlgorithm 1 projected latent video diffusion model (PVDM)\n1: for ` = 1 to L do . Iteratively generate video clips x`. 2: Sample the random noise z`T \u223c p(zT ). 3: for t = T to 1 do 4: if ` = 1 then 5: Unconditional score t = \u03b8(z`t,0, t). 6: else 7: Conditional score t = \u03b8(z`t, z `\u22121 0 , t). 8: end if 9: Sample \u223c N (0z, Iz).\n10: Compute z`t\u22121 = 1\u221a\n1\u2212\u03b2t\n( z`t\u2212 \u03b2t\u221a 1\u2212\u03b1\u0304t t ) +\u03c3t .\n11: end for 12: Decode the `-th clip x` = g\u03c8(z`0). 13: end for 14: Output the generated video [x1, . . . ,xL].\nand p\u03b8(xt\u22121|xt) can be approximately formulated as the following normal distribution with a small enough \u03b2t [50]:\np\u03b8(xt\u22121|xt) := N ( xt\u22121;xt \u2212\n\u03b2t\u221a 1\u2212 \u03b1\u0304t \u03b8(xt, t), \u03c3 2 t\n) ,\nwith the variances \u03c32t := \u03b2t as pre-defined hyperparameters. The main drawback of diffusion models is severe computation and memory inefficiency. To generate the sample, one should operate p\u03b8(xt\u22121|xt) in high-dimensional input space X repeatedly (e.g., T = 1000 in Ho et al. [19]). To tackle this issue, several works [15, 41, 59] have proposed latent diffusion models to learn the distribution in low-dimensional latent space Z that succinctly encodes the data, which is typically learned with autoencoders [41]. Specifically, latent diffusion models train the denoising autoencoder \u03b8(zt, t) in Z instead of X i.e., learning p\u03b8(zt\u22121|zt), so that x is generated by first sampling z and then decoding to x with the decoder. Due to the significant dimension reduction from x to z, one can dramatically reduce the computation for sampling the data. Inspired by their success, we also model video distribution with latent diffusion models."
        },
        {
            "heading": "3.2. Designing efficient latent video diffusion model",
            "text": "Autoencoder. To represent a video x as a low-dimensional latent vector z, we train an autoencoder composed of an encoder f\u03c6 : X \u2192 Z with f\u03c6(x) = z and a decoder g\u03c8 : Z \u2192 X with g\u03c8(z) = x\u0303 so that x\u0303 becomes x. Motivated by VQGAN [10] for compressing images perceptually via latent codes [37], we encode videos with discrete latent codes by minimizing the sum of two terms: pixel-level reconstruction loss and the negative of perceptual similarity (LPIPS; [70]); we provide more details in Appendix A.\nConventional video autoencoders have mostly relied on frame-wise 2D convolutional networks [45] or 3D convolutional networks [65] to compress a given video x. While\nthese approaches are fairly effective in amortizing x as a low-dimensional latent vector, they encode x as a 3D latent vector, which overlooks the temporal coherency and requiring the diffusion model architecture to deal with a 3D tensor and may cause a computation overhead. Instead, we take a different approach: given x, we construct z as three 2D latent vectors zs, zh, zw, i.e., z := [zs, zh, zw], zs \u2208 RC\u00d7H\u2032\u00d7W \u2032 , zh \u2208 RC\u00d7S\u00d7W \u2032 , zw \u2208 RC\u00d7S\u00d7H\u2032 , where C is a latent dimension and H \u2032 = H/d, W \u2032 = W/d for d > 1. Here, we denote each zs, zh, zw the concatenation of latent codes zshw, z h sw, z w sh \u2208 RC (respectively); e.g., for zs, we write\nzs := [zshw] for 1 \u2264 h \u2264 H \u2032, 1 \u2264 w \u2264W \u2032.\nWe design zs to capture the common content across time in x (e.g., background), and the latter two latent vectors zh, zw to encode the underlying motion in x by learning the representations across two spatial axes of videos. Specifically, [zs, zh, zw] is computed with the encoder f\u03c6, where f\u03c6 is a composition of a video-to-3D-latent mapping fshw\u03c6shw and a 3D-to-2D-latents projection fs\u03c6s \u00d7 f h \u03c6h \u00d7 fw\u03c6w (with \u03c6 := (\u03c6shw,\u03c6s,\u03c6h,\u03c6w); see Figure 2 for the illustration). More specifically, we compute z from x as follows:\nu := fshw\u03c6shw(x), where u = [ushw] \u2208 R C\u00d7S\u00d7H\u2032\u00d7W \u2032 ,\nzshw := f s \u03c6s(u1hw, . . . , uShw), 1\u2264h\u2264H \u2032, 1\u2264w\u2264W \u2032, zhsw := f h \u03c6h(us1w, . . . , usH\u2032w), 1\u2264 s\u2264S, 1\u2264w\u2264W \u2032,\nzwsh := f w \u03c6w(ush1, . . . , ushW \u2032), 1\u2264 s\u2264S, 1\u2264h\u2264H \u2032.\nWith the latent vector z of x from f\u03c6, we construct the decoder g\u03c8 that (a) computes a 3D latent grid v from z and (b) reconstructs x from v, where v is computed as follows:\nv = (vshw) \u2208 R3C\u00d7S\u00d7H \u2032\u00d7W \u2032 , vshw := [zhw, zsw, zsh].\nWe use video Transformer (e.g., TimeSformer [4]) for fshw\u03c6shw(x) and g\u03c8, and a small Transformer [61] for projections fs\u03c6s , f h \u03c6h , fw\u03c6w . While our autoencoder design requires slightly more parameters and computations for encoding videos from additional projections, it provides dramatic computation efficiency for training (and sampling) of a diffusion model in latent space. In particular, conventional video autoencoder design requires O(SHW ) dimensions of latent codes for encoding videos, and thus diffusion models to utilize 3D convolution layers and self-attention layers, which lead the computation overhead to be O(SHW ) and O((SHW )2), respectively. In contrast, we represent a video as image-like latent vectors with O(HW +SW +SH) latent codes; such latent representations enables more computeefficient diffusion model design by utilizing 2D convolution layers (O(HW +SW +SH)) and self-attention layers (O((HW +SW +SH)2)) based on modifying popular architectures used for image diffusion models [19].\nThe intuition behind our overall autoencoder design is that videos are temporally coherent signals and share the common contents across the temporal axis; capturing the common content as zs can dramatically reduce the number of parameters for encoding videos [29]. Moreover, due to the high temporal coherency of videos, the temporal variation is often not very large; we empirically verify our succinct representation of motions as two spatial grids zh, zw does not hurt the encoding quality (see Section 4.3). Latent diffusion model. Recall that we encode the video x as three 2D latent vectors z = [zs, zh, zw]; it is enough to model the distribution p(zs, zh, zw) for learning pdata(x). To train a denoising autoencoder for [zs, zh, zw], we design the neural network architecture based on utilizing popular 2D convolutional U-Net architecture used for training diffusion models for image generation [9] instead of 3D convolutional networks [21]. Specifically, we use a single U-Net (i.e., shared parameters) to denoise each zs, zh, zw. To handle the dependency among zs, zh, zw to model the joint distribution p(zs, zh, zw), we add attention layers that operates to the intermediate features of zs, zh, zw from shared U-Net. We remark that such 2D convolutional architecture design is more computation-efficient than a na\u00efve 3D convolutional U-Nets for videos, which is possible due to the \u201cimage-like\u201d structure and a reduced dimension of our latent vectors from using less latent codes for encoding videos.\n1StyleGAN-V results are from https://universome.github. io/stylegan-v."
        },
        {
            "heading": "3.3. Generating longer videos with PVDM",
            "text": "Note that videos are sequential data; unlike our setup that assumes all videos x \u2208 D have the same length S, the length of real-world videos varies, and generative video models should be able to generate videos of arbitrary length. However, we only learn the distribution of the fixed-length video clips pdata(x); to enable long video generation, one can consider learning a conditional distribution p(x2|x1) of two consecutive video clips [x1,x2] of x1,x2 \u2208 R3\u00d7S\u00d7H\u00d7W and sequentially generate future clips given the current ones.\nA straightforward solution is to have two separate models to learn the unconditional distribution pdata(x) and the conditional distribution p(x2|x1). Instead of having an extra model, we propose to train a single diffusion model to jointly learn an unconditional distribution p(x) and the conditional distribution p(x2|x1). It can be achieved by training a conditional diffusion model p(x2|x1) with introducing a null frame (i.e., x1 = 0) for a joint learning of p(x) [20]. More specifically, we consider training of a denoising autoencoder \u03b8(z 2 t , z 1 0, t) in the latent space with the following objective:\nE(x10,x20), ,t [ \u03bb|| \u2212 \u03b8(z2t , z10, t)||22\n+ (1\u2212 \u03bb)|| \u2212 \u03b8(z2t ,0, t)||22 ] ,\nwhere z10 = f\u03c6(x 1 0), z 2 0 = f\u03c6(x 2 0), z 2 t = \u221a \u03b1\u0304tz 2 0 + \u221a\n1\u2212 \u03b1\u0304t , and \u03bb\u2208 (0, 1) is a hyperparameter that balances a learning between unconditional and conditional distribution.\nAfter training, one can generate the long video as follows: sample an initial video clip x1\u223c p\u03b8(x), and repeat generating next clip x`+1\u223c p\u03b8(x`+1|x`) conditioned on the previous clip. Finally, we obtain a long video by concatenating all generated clips [x1, . . . ,xL] of arbitrary length L > 1. See Algorithm 1 for details and Figure 3 for results."
        },
        {
            "heading": "4. Experiments",
            "text": "We validate the superiority of our PVDM framework under two representative datasets: UCF-101 [54] and SkyTimelapse [64]. In Section 4.1, we provide the experimental setup and evaluation details that we used for experiments. In Section 4.2, we present the main qualitative and quantitative video synthesis results. Finally, in Section 4.3, we perform an ablation studies to verify the effect of components and efficiencies of our method."
        },
        {
            "heading": "4.1. Experiment setup",
            "text": "Datasets. We train our PVDM and compare with baselines on well-known video datasets used for video synthesis: UCF101 [54] and SkyTimelapse [64]. Following the experimental setup used in recent video generation methods [47, 67], we preprocess these datasets as video clips of length 16 or 128 frames, where each frame is resized to 256\u00d7256 resolution. For training each model, we use the train split of the dataset for all of the experiments. We provide the detailed description of datasets in Appendix B.1.\nEvaluation. For evaluation metrics for quantitative comparison, we use and report Inception score (IS) [44] and Fr\u00e9chet video distance (FVD) [58]. We use the clip length of 16 for the evaluation of IS, following the prior experiment setups in unconditional video generation. For FVD to evaluate UCF101 and SkyTimelapse, we used a fixed protocol proposed by StyleGAN-V that removes the potential bias from the existence of long videos in the dataset (we provide more details of metrics in Appendix B.2). We consider two different clip lengths (16 and 128) for FVD, where we denote FVD16 and FVD128 as the FVD score measured on video clips of lengths 16 and 128, respectively. We use 2,048 real/fake video clips for evaluating FVD16 and FVD128, and generate 10,000 video clips for the evaluation of IS. Baselines. Following the setup in StyleGAN-V, one of the state-of-the-art video generation methods, we mainly compare PVDM with the following recent video synthesis methods: VideoGPT [65], MoCoGAN [57], MoCoGAN-HD [55], DIGAN [67], and StyleGAN-V [47]. Moreover, we perform an additional comparison between PVDM and previous approaches on IS values on UCF-101: MoCoGAN, ProgressiveVGAN [1], LDVD-GAN [23], VideoGPT, TGANv2 [43], DVD-GAN [7], DIGAN, VDM [21], and TATS [12]. All reported values are collected from the recent prior works: StyleGAN-V, DIGAN and TATS, unless otherwise specified. In particular, we compare the memory and computation efficiency with VDM. We provide more details of each method and how they are implemented in Appendix C.\nTraining details. We use Adam [30] and AdamW optimizer [33] for training of an autoencoder and a diffusion model, respectively, where other training details mostly follow the setups in latent diffusion models for images [41]. For the autoencoder architecture, we use TimeSformer [4] to both encoder and decoder, where we use Transformer [61] architecture for 3D-to-2D projection mapping. We use two configurations for diffusion models, (denoted by PVDM-S and PVDM-L, respectively); please refer to Appendix D for more training details and model configurations."
        },
        {
            "heading": "4.2. Main results",
            "text": "Qualitative results. Figure 4 illustrates the video synthesis results from PVDM on UCF-101 and SkyTimelapse: our method shows realistic video generation results in both scenarios, namely, including the motion of the small object (Figure 4a) or the large transition of the over frames (Figure 4b). We also note that such training is achieved under high-fidelity (256\u00d7256 resolution) videos. Moreover, our method also has the capability to synthesize videos in the complex UCF-101 dataset with a plausible quality, as shown in Figure 4a, while other baselines often fail on such challenging dataset datasets [47, 67].2\nQuantitative results. Table 1 and 2 summarize the quantitative comparison between our method and prior video generation methods: PVDM consistently outperforms other methods measured with diverse metrics. In particular, compared with VDM [21], which trains a diffusion model on video pixels, our method shows a better IS even though VDM uses more data (uses train+test split) than our setup (uses train split only). Intriguingly, our method shows a notable\n2We provide the illustrations of synthesized videos from our method and comparison with other baselines in the following project website: https: //sihyun.me/PVDM.\nimprovement on UCF, a complicated, multi-class dataset, which shows the potential of our method to model complex video distribution. We also note that our method also achieves state-of-the-art results in 128 frame videos, which demonstrates the effectiveness of our method in generating longer videos. We provide the qualitative comparison with other video generation methods in Appedix E. Long video generation. We visualize the long video generation results of our PVDM in Figure 3. As shown in this Figure, our PVDM has the powerful capability of generating long videos (128 frames) with 256\u00d7256 resolution frames while maintaining temporal coherency across timesteps. Here, we emphasize that our method produces long videos not only on fine-grained datasets with monotonic motion (e.g., SkyTimelapse) but also on UCF-101, which is a complex dataset that contains various dynamic motions. We also note that synthesizing long videos on such a complex UCF101 dataset has been regarded as a challenging task in prior state-of-the-art video generation approaches that target long video generation [12,47,67], and thus they often fail to generate temporally coherent and realistic videos on these datasets. We remark that the superiority of our method on long video synthesis is also verified quantitatively as FVD128 in Table 1: our method significantly outperforms prior methods on this metric on UCF-101 as 1773.4\u2192 639.7."
        },
        {
            "heading": "4.3. Analysis",
            "text": "Reconstruction quality. Figure 5 and Table 4 summarize the results of the reconstructed videos from our autoencoder. We consider the following metrics to measure the quality: peak signal-to-noise-ratio (PSNR) for reconstruction quality and R-FVD for perceptual similarity. Here, R-FVD indicates FVD between reconstructions and the ground-truth real videos. Measured with these metrics, our method consis-\ntently shows accurate reconstructions similar to ground-truth videos. In particular, our method shows a small enough RFVD, which validates the effectiveness of our latent space in compressing videos while preserving perceptual similarity. We also provide a more extensive analysis, including a comparison with other popular autoencoders in Appendix F. Comparison with VDM. Compared with a recent video diffusion model (VDM) [21], our model achieves great computation and memory efficiency in generating samples due to the use of 2D instead of 3D convolution networks and the low-dimensionality of latent vector that encodes highdimensional video pixels. Following the model configurations of VDM as described in [21], Table 5 compares the maximum batch size that can be allocated in training as well as the memory and time to synthesize a single video at a 256\u00d7256 resolution. Under the same sampler setup, PVDM achieves \u224817.6\u00d7 better computation efficiency: our method requires \u22487.88 seconds to generate a video with 256\u00d7256 resolution and length of 16, while VDM requires >2 minutes to generate such videos. Moreover, our PVDM shows at most 3.5\u00d7 better memory efficiency; it can be trained with videos at 256\u00d7256 resolution and length of 16 and synthesize longer videos (e.g., length 128) under the limited memory constraint (24GB), yet VDM cannot be trained and generate these videos under these computational resources. We also provide the comparison with two recent autoregressive video synthesis methods (TATS and VideoGPT): PVDM still achieves superior efficiencies in both time and memory."
        },
        {
            "heading": "5. Conclusion",
            "text": "We proposed PVDM, a latent diffusion model for video generation. Our key idea is based on proposing an image-like 2D latent space that effectively parameterizes a given video so that the given video data distribution can be effectively learned via diffusion models in latent space. We hope our method will initiate lots of intriguing directions in effectively scaling video synthesis methods."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank Jaehyung Kim, Jihoon Tack, Younggyo Seo, and Mostafa Dehghani for their helpful discussions, and Lu Jiang and David Salesin for the proofreading our manuscript. We also appreciate Ivan Skorokhodov for providing baseline results and pre-trained checkpoints. This work was mainly supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2021-0-02068, Artificial Intelligence Innovation Hub; No.2019-0-00075, Artificial Intelligence Graduate School Program (KAIST)). This work is in part supported by Google Research grant and Google Cloud Research Credits program."
        },
        {
            "heading": "A. Detailed description of training objective",
            "text": "In what follows, we describe the training objective that we used for our autoencoder consists of the encoder f\u03c6 and the decoder g\u03c8. We use the similar objective L(\u03c6,\u03c8) used in VQGAN [41] and TATS [12], except using vector-quantization regularization:\nL(\u03c6,\u03c8) := Lpixel(\u03c6,\u03c8) + \u03bb1LLPIPS(\u03c6,\u03c8) + \u03bb2 max h LGAN(\u03c6,\u03c8)\nwhere Lpixel, LLPIPS, LGAN, LVQ(\u03c6,\u03c8) denote pixel-level `1-reconstruction loss, negative perceptual similarity [70], and adversarial objective [13] with a discriminator network h, respectively. Unlike prior works that uses KL-regularization [41] or VQ-regularization [12] to control the latent, we find simply adding tanh activation after projection network works well in training latent diffusion without without degradation autoencoder training.\nFor LGAN, we use the sum of hinge-loss and feature similarity between real samples and their reconstructions, following the objective used in TATS to train the autoencoder. Following the training setup in VQGAN, we choose \u03bb1 = 1 and \u03bb3 = 0.25 as a constant and set \u03bb2 = 0 before the other terms converge and \u03bb2 = 0.25 after the convergence. We find early stopping is crucial in training our autoencoder with LGAN; we stop the training by tracking the R-FVD values during training."
        },
        {
            "heading": "B. Detailed description of experiment setup",
            "text": "B.1. Datasets\nUCF-101 [54] is a video dataset composed of diverse human actions. Specifically, the dataset contains 101 classes, and each video has a 320\u00d7240 resolution. There are 13,320 videos in total, where we used only the train split (9,357 videos) for training and the rest of the test split for the evaluation, following the common practice in evaluating unconditional video generation on UCF-101 [67]. All videos are center-cropped and resized to 256\u00d7 256 resolution. SkyTimelapse [64] is a collection of sky time-lapse total of 5,000 videos. We use the same data preprocessing following the official link.3 Moreover, similar to UCF-101, all videos are center-cropped and resized to 256\u00d7 256 resolution. We use the train split for both training the model and the evaluation, following the evaluation setup used in StyleGAN-V [47].\nB.2. Metrics\nFor a fair comparison with prior works, we carefully choose the setups for quantitative evaluation. For measuring the Inception score (IS; [44]) on UCF-101 [54], we use the C3D network [56] trained on the Sports-1M dataset [25] and finetuned on UCF-101. Following the representative setups for evaluating IS for videos, also used in recent state-of-the-art methods [12, 67], we generate 10,000 samples to measure the score.\nIn the case of Fr\u00e9chet video distance (FVD; [58]), we used the fixed protocol proposed by StyleGAN-V [47]. Unlike predominant setups that first preprocess the given video dataset as a fixed-length video clip and then sample real samples for calculating real statistics, the paper proposes first to sample the video data and randomly choose the fixed-length video clip. Such a different protocol is inspired by the observation from the previous evaluation procedure\u2014if the dataset contains extremely long videos, the statistics can be easily biased due to the large number of clips from this video. We sample 2,048 samples (or the size of the real data if it is smaller) for calculating real statistics and 2,048 samples for evaluating fake statistics.\n3https://github.com/weixiong-ur/mdgan"
        },
        {
            "heading": "C. Detailed description of the baselines",
            "text": "In this section, we describe the main idea of video generation methods that we used for the evaluation.\n\u2022 VGAN [62] replaces 2D convolutional networks to 3D convolutional networks to extend image generative adversarial network (GAN; [13]) architecture for video generation.\n\u2022 TGAN [42] extends Wasserstein GAN [3] for images based on separating the motion and the content generator.\n\u2022 MoCoGAN [57] decomposes motion and the content of the video for video generation via having a separate content generator and an autoregressive motion generator.\n\u2022 ProgressiveVGAN [1] Inspired by GAN-based high-resolution image generation via progressive architecture design [26], ProgressiveGAN also generates the video progressively in both spatial and temporal directions.\n\u2022 LDVD-GAN [23] mitigates computation inefficiency of video GANs with a discriminator using low-dimensional kernels.\n\u2022 VideoGPT [65] is a two-stage model: it encodes videos as a sequence of discrete latent vectors using VQ-VAE [60] and learns the autoregressive model with these sequences via Transformer [61].\n\u2022 TGANv2 [43] proposes a computation-efficient video GAN based on designing submodules for a generator and a discriminator.\n\u2022 DVD-GAN [7] uses two discriminators for identifying real/fake of each spatial and temporal dimension, where the input of the temporal discriminator is low-resolution for efficiency.\n\u2022 MoCoGAN-HD [55] proposes to use a strong image generator for high-resolution image synthesis: they generate videos via modeling trajectories in the latent space of the generator.\n\u2022 DIGAN [67] proposes a video GAN based on exploiting the concept of implicit neural representations and computationefficient discriminators.\n\u2022 StyleGAN-V [47] also introduces neural-representation-based video GAN to learn a given video distribution with a computation-efficient discriminator.\n\u2022 TATS [12] proposes a new VQGAN [10] for videos and trains an autoregressive model to learn the latent distribution.\n\u2022 VDM [21] extends image diffusion models by proposing a new U-Net architecture based on 3D convolutional layers."
        },
        {
            "heading": "D. More details on training setup",
            "text": "For all experiments, we use a batch size of 24 and a learning rate 1e-4 for training autoencoders. We train the model until both FVD and PSNR converge. We use 4-layer Transformer for 3D-to-2D projections, where the number of heads is set to 4, the hidden dimension is set to 384, and the hidden dimension of the multilayer perceptron (MLP) in Transformer is to be 512. We set the dimension of the latent codebook to 4. For training diffusion models, we set a learning rate as 1e-4 with a batch size of 64. For more hyperparameters, such as the base channel and the depth of U-Net architecture, we adopt a similar setup to LDMs [41]. In particular, we set the codebook channel C to 4 and the patch size to 4\u00d7 4\u00d7 1, so that a 256\u00d7 256\u00d7 16\u00d7 3 dimension video is encoded to a (32\u00d7 32 + 32\u00d7 16 + 32\u00d7 16)\u00d7 4 = 8,192 dimension vector. Throughout the experiments, we consider two model configurations: PVDM-S (small model) and PVDM-L (large model), where we summarize the detailed configurations and hyperparameters in Table 6."
        },
        {
            "heading": "E. Qualitative comparison of generated results",
            "text": "We provide the qualitative comparison of the first frame generated from baselines in Figure 6; note that we provide the comparison of videos on our anonymized website. Compared with other recent video synthesis methods, our PVDM shows overall high-quality synthesis results, especially on the complex dataset, UCF-101.\nTable 8. Autoencoder performance (R-FVD) with different model sizes. # Enc. and # Dec. denote the number of parameters for encoder and decoder, respectively.\n# Enc.+# Dec. R-FVD \u2193\n11M+10M 154.7 23M+17M 67.76 35M+27M 63.34"
        },
        {
            "heading": "F. More analysis in our autoencoder architecture",
            "text": "Ablation study on our autoencoder. We also conduct an ablation study of autoencoder on UCF-101 and report it in Table 7 (we adjust the model sizes to be equal): Our autoencoder components are particularly effective in capturing perceptual details with \u201c2D-shaped\u201d latents z, as shown by better R-FVD than other variants. We also find the performance of 2D autoencoder degrades a lot for small dim(z) (8,192) as 2D AE has no temporal layers and thus is hard to capture the temporal coherence with compact latents. We also find removing popular vector quantization used for training autoencoders is not necessary in improving our autoencoder quality and does not prevents diffusion model training; it was enough to clip the range of latents by adding activation functions after the encoder ouputs (e.g., tanh). Autoencoder performance with various model sizes. Table 8 summarizes the autoencoder performance w.r.t. varying model sizes. It shows our autoencoder is scalable and it works even fairly well using \u2248 2/3 number of parameters."
        },
        {
            "heading": "G. Discussion with concurrent work",
            "text": "The concurrent work, MagicVideo [71], is also a latent video diffusion model. However, unlike our PVDM, they encode videos in a frame-wise manner, resulting in an unfavorable increase of latent dimension with length and thus limiting the scalability. Another concurrent work, LVDM [17] uses 3D CNN to encode video clip and design a latent diffusion model, but they still deal with cubic-shaped latent tensors in designing diffusion models. Make-a-video [46] extends a popular image diffusion model for video synthesis. They also avoid using 3D convolutions, yet they deal with 3D RGB arrays directly and are still difficult to be trained on high-resolution, long videos. As a result, it leads the framework to heavily rely on complex spatiotemporal interpolation modules to generate high-dimensional videos."
        },
        {
            "heading": "H. Limitations and future work",
            "text": "While PVDM shows promise in efficient and effective video synthesis, there are many possible future directions for further improvement as there still exists a gap between the real and the generated videos. For instance, designing diffusion model architectures specialized for our triplane latents to model latent distribution better or investigating better latent structures than our triplane idea to encode video more efficiently would be interesting directions. While we have not conducted experiments in large-scale video datasets to perform challenging text-to-video generation due to the limited resources, we strongly believe our framework also works well in this task and leave it for the future work."
        }
    ],
    "title": "Video Probabilistic Diffusion Models in Projected Latent Space",
    "year": 2023
}