{
    "abstractText": "Learning operator mapping between infinite-dimensional Banach spaces via neural networks has attracted a considerable amount of attention in recent years. In this work, we propose an interfaced operator network (IONet) to solve parametric elliptic interface PDEs, where different coefficients, source terms and boundary conditions are considered as input features. To capture the discontinuities of both input functions and output solutions across the interface, IONet divides the entire domain into several separate sub-domains according to the interface, and leverages multiple branch networks and truck networks. Each branch network extracts latent representations of input functions at a fixed number of sensors on a specific sub-domain, and each truck network is responsible for output solutions on one sub-domain. In addition, tailored physics-informed loss of IONet is proposed to ensure physical consistency, which greatly reduces the requirement for training datasets and makes IONet effective without any paired input-output observations in the interior of the computational domain. Extensive numerical studies show that IONet outperforms existing state-of-the-art deep operator networks in terms of accuracy, efficiency, and versatility.",
    "authors": [
        {
            "affiliations": [],
            "name": "Sidi Wua"
        },
        {
            "affiliations": [],
            "name": "Aiqing Zhua"
        },
        {
            "affiliations": [],
            "name": "Yifa Tanga"
        },
        {
            "affiliations": [],
            "name": "Benzhuo Lua"
        }
    ],
    "id": "SP:e8f0ac98b608ed19f2415732b78e915b75b5a169",
    "references": [
        {
            "authors": [
                "M. Sussman",
                "E. Fatemi"
            ],
            "title": "An efficient interface-preserving level set redistancing algorithm and its application to interfacial incompressible fluid flow",
            "venue": "SIAM Journal on scientific computing 20 (4) ",
            "year": 1999
        },
        {
            "authors": [
                "E.A. Fadlun",
                "R. Verzicco",
                "P. Orlandi",
                "J. Mohd-Yusof"
            ],
            "title": "Combined immersed-boundary finite-difference methods for three-dimensional complex flow simulations",
            "venue": "Journal of computational physics 161 (1) ",
            "year": 2000
        },
        {
            "authors": [
                "Y. Liu",
                "M. Sussman",
                "Y. Lian",
                "M.Y. Hussaini"
            ],
            "title": "A moment-of-fluid method for diffusion equations on irregular domains in multi-material systems",
            "venue": "Journal of Computational Physics 402 ",
            "year": 2020
        },
        {
            "authors": [
                "L. Wang",
                "H. Zheng",
                "X. Lu",
                "L. Shi"
            ],
            "title": "A Petrov-Galerkin finite element interface method for interface problems with Bloch-periodic boundary conditions and its application in phononic crystals",
            "venue": "Journal of Computational Physics 393 ",
            "year": 2019
        },
        {
            "authors": [
                "J.S. Hesthaven"
            ],
            "title": "High-order accurate methods in time-domain computational electromagnetics: A review",
            "venue": "Advances in imaging and electron physics 127 ",
            "year": 2003
        },
        {
            "authors": [
                "B. Lu",
                "Y. Zhou",
                "M. Holst",
                "J. McCammon"
            ],
            "title": "Recent progress in numerical methods for the Poisson-Boltzmann equation in biophysical applications",
            "venue": "Communications in Computational Physics 3 (5) ",
            "year": 2008
        },
        {
            "authors": [
                "N. Ji",
                "T. Liu",
                "J. Xu",
                "L.Q. Shen",
                "B. Lu"
            ],
            "title": "A finite element solution of lateral periodic Poisson\u2013Boltzmann model for membrane channel proteins",
            "venue": "International journal of molecular sciences 19 (3) ",
            "year": 2018
        },
        {
            "authors": [
                "J. Philip"
            ],
            "title": "Flow in porous media",
            "venue": "Annual Review of Fluid Mechanics 2 (1) ",
            "year": 1970
        },
        {
            "authors": [
                "Y. Khoo",
                "J. Lu",
                "L. Ying"
            ],
            "title": "Solving parametric pde problems with artificial neural networks",
            "venue": "European Journal of Applied Mathematics 32 (3) ",
            "year": 2021
        },
        {
            "authors": [
                "Y. Zhu",
                "N. Zabaras",
                "P.-S. Koutsourelakis",
                "P. Perdikaris"
            ],
            "title": "Physics-constrained deep learning for high-dimensional surrogate modeling and uncertainty quantification without labeled data",
            "venue": "Journal of Computational Physics 394 ",
            "year": 2019
        },
        {
            "authors": [
                "I. Babu\u0161ka"
            ],
            "title": "The finite element method for elliptic equations with discontinuous coefficients",
            "venue": "Computing 5 (3) ",
            "year": 1970
        },
        {
            "authors": [
                "J.H. Bramble",
                "J.T. King"
            ],
            "title": "A finite element method for interface problems in domains with smooth boundaries and interfaces",
            "venue": "Advances in Computational Mathematics 6 ",
            "year": 1996
        },
        {
            "authors": [
                "Y. Chen",
                "S. Hou",
                "X. Zhang"
            ],
            "title": "A bilinear partially penalized immersed finite element method for elliptic interface problems with multidomain and triple-junction points",
            "venue": "Results in Applied Mathematics 8 ",
            "year": 2020
        },
        {
            "authors": [
                "B. Zhang",
                "J. DeBuhr",
                "D. Niedzielski",
                "S. Mayolo",
                "B. Lu",
                "T. Sterling"
            ],
            "title": "DASHMM accelerated adaptive fast multipole Poisson-Boltzmann solver on distributed memory architecture",
            "venue": "Communications in Computational Physics 25 (4) ",
            "year": 2019
        },
        {
            "authors": [
                "L. Mu",
                "J. Wang",
                "X. Ye",
                "S. Zhao"
            ],
            "title": "A new weak Galerkin finite element method for elliptic interface problems",
            "venue": "Journal of Computational Physics 325 ",
            "year": 2016
        },
        {
            "authors": [
                "T. Liu",
                "M. Chen",
                "B. Lu"
            ],
            "title": "Efficient and qualified mesh generation for Gaussian molecular surface using adaptive partition and piecewise polynomial approximation",
            "venue": "SIAM Journal on Scientific Computing 40 (2) ",
            "year": 2018
        },
        {
            "authors": [
                "C.S. Peskin"
            ],
            "title": "The immersed boundary method",
            "venue": "Acta numerica 11 ",
            "year": 2002
        },
        {
            "authors": [
                "R.J. LeVeque",
                "Z. Li"
            ],
            "title": "The immersed interface method for elliptic equations with discontinuous coefficients and singular sources",
            "venue": "SIAM Journal on Numerical Analysis 31 (4) ",
            "year": 1994
        },
        {
            "authors": [
                "Z. Chen",
                "Y. Xiao",
                "L. Zhang"
            ],
            "title": "The adaptive immersed interface finite element method for elliptic and maxwell interface problems",
            "venue": "Journal of Computational Physics 228 (14) ",
            "year": 2009
        },
        {
            "authors": [
                "R.P. Fedkiw",
                "T. Aslam",
                "B. Merriman",
                "S. Osher"
            ],
            "title": "A non-oscillatory eulerian approach to interfaces in multimaterial flows (the ghost fluid method)",
            "venue": "Journal of computational physics 152 (2) ",
            "year": 1999
        },
        {
            "authors": [
                "R. Egan",
                "F. Gibou"
            ],
            "title": "xGFM: Recovering convergence of fluxes in the ghost fluid method",
            "venue": "Journal of Computational Physics 409 ",
            "year": 2020
        },
        {
            "authors": [
                "D. Bochkov",
                "F. Gibou"
            ],
            "title": "Solving elliptic interface problems with jump conditions on cartesian grids",
            "venue": "Journal of Computational Physics 407 ",
            "year": 2020
        },
        {
            "authors": [
                "K. Xia",
                "M. Zhan",
                "G.-W. Wei"
            ],
            "title": "Mib method for elliptic equations with multi-material interfaces",
            "venue": "Journal of computational physics 230 (12) ",
            "year": 2011
        },
        {
            "authors": [
                "I. Babu\u0161ka",
                "U. Banerjee"
            ],
            "title": "Stable generalized finite element method (sgfem)",
            "venue": "Computer methods in applied mechanics and engineering 201 ",
            "year": 2012
        },
        {
            "authors": [
                "H. Liu",
                "L. Zhang",
                "X. Zhang",
                "W. Zheng"
            ],
            "title": "Interface-penalty finite element methods for interface problems in h1",
            "venue": "h (curl), and h (div), Computer Methods in Applied Mechanics and Engineering 367 ",
            "year": 2020
        },
        {
            "authors": [
                "A. Taleei",
                "M. Dehghan"
            ],
            "title": "Direct meshless local Petrov\u2013Galerkin method for elliptic interface problems with applications in electrostatic and elastostatic",
            "venue": "Computer Methods in Applied Mechanics and Engineering 278 ",
            "year": 2014
        },
        {
            "authors": [
                "F. Gholampour",
                "E. Hesameddini",
                "A. Taleei"
            ],
            "title": "A global rbf-qr collocation technique for solving two-dimensional elliptic problems involving arbitrary interface",
            "venue": "Engineering with Computers 37 (4) ",
            "year": 2021
        },
        {
            "authors": [
                "M. Ahmad"
            ],
            "title": "S",
            "venue": "ul Islam, E. Larsson, Local meshless methods for second order elliptic interface problems with sharp corners, Journal of Computational Physics 416 ",
            "year": 2020
        },
        {
            "authors": [
                "\u00d6. Oru\u00e7"
            ],
            "title": "An efficient meshfree method based on pascal polynomials and multiple-scale approach for numerical solution of 2-d and 3-d second order elliptic interface problems",
            "venue": "Journal of Computational Physics 428 ",
            "year": 2021
        },
        {
            "authors": [
                "Z. Wang",
                "Z. Zhang"
            ],
            "title": "A mesh-free method for interface problems using the deep learning approach",
            "venue": "Journal of Computational Physics 400 ",
            "year": 2020
        },
        {
            "authors": [
                "C. He",
                "X. Hu",
                "L. Mu"
            ],
            "title": "A mesh-free method using piecewise deep neural network for elliptic interface problems",
            "venue": "Journal of Computational and Applied Mathematics 412 ",
            "year": 2022
        },
        {
            "authors": [
                "H. Guo",
                "X. Yang"
            ],
            "title": "Deep unfitted nitsche method for elliptic interface problems",
            "venue": "Communications in Computational Physics 31 (4) ",
            "year": 2022
        },
        {
            "authors": [
                "Q. Sun",
                "X. Xu"
            ],
            "title": "H",
            "venue": "Yi, Dirichlet-neumann learning algorithm for solving elliptic interface problems ",
            "year": 2023
        },
        {
            "authors": [
                "S. Wu",
                "B. Lu"
            ],
            "title": "INN: Interfaced neural networks as an accessible meshless approach for solving interface PDE problems",
            "venue": "Journal of Computational Physics 470 ",
            "year": 2022
        },
        {
            "authors": [
                "J. Berg",
                "K. Nystr\u00f6m"
            ],
            "title": "A unified deep artificial neural network approach to partial differential equations in complex geometries",
            "venue": "Neurocomputing 317 ",
            "year": 2018
        },
        {
            "authors": [
                "A.D. Jagtap",
                "G.E. Karniadakis"
            ],
            "title": "Extended physics-informed neural networks (xpinns): A generalized space-time domain decomposition based deep learning framework for nonlinear partial differential equations",
            "venue": "Communications in Computational Physics 28 (5) ",
            "year": 2020
        },
        {
            "authors": [
                "Z. Liu",
                "W. Cai",
                "Z.-Q. John Xu"
            ],
            "title": "Multi-scale deep neural network (MscaleDNN) for solving poisson-boltzmann equation in complex domains",
            "venue": "Communications in Computational Physics 28 (5) ",
            "year": 2020
        },
        {
            "authors": [
                "J. Ying",
                "J. Liu",
                "J. Chen",
                "S. Cao",
                "M. Hou",
                "Y. Chen"
            ],
            "title": "Multi-scale fusion network: A new deep learning structure for elliptic interface problems",
            "venue": "Applied Mathematical Modelling 114 ",
            "year": 2023
        },
        {
            "authors": [
                "W.-F. Hu",
                "T.-S. Lin",
                "M.-C. Lai"
            ],
            "title": "A discontinuity capturing shallow neural network for elliptic interface problems",
            "venue": "Journal of Computational Physics 469 ",
            "year": 2022
        },
        {
            "authors": [
                "M.-C. Lai",
                "C.-C. Chang",
                "W.-S. Lin",
                "W.-F. Hu",
                "T.-S. Lin"
            ],
            "title": "A shallow ritz method for elliptic problems with singular sources",
            "venue": "Journal of Computational Physics 469 ",
            "year": 2022
        },
        {
            "authors": [
                "Y.-H. Tseng",
                "T.-S. Lin",
                "W.-F. Hu",
                "M.-C. Lai"
            ],
            "title": "A cusp-capturing pinn for elliptic interface problems",
            "venue": "Journal of Computational Physics ",
            "year": 2023
        },
        {
            "authors": [
                "D.J. Lucia",
                "P.S. Beran",
                "W.A. Silva"
            ],
            "title": "Reduced-order modeling: new approaches for computational physics",
            "venue": "Progress in aerospace sciences 40 (1-2) ",
            "year": 2004
        },
        {
            "authors": [
                "A. Quarteroni",
                "A. Manzoni",
                "F. Negri"
            ],
            "title": "Reduced basis methods for partial differential equations: an introduction",
            "venue": "Vol. 92, Springer",
            "year": 2015
        },
        {
            "authors": [
                "A.J. Majda",
                "D. Qi"
            ],
            "title": "Strategies for reduced-order models for predicting the statistical responses and uncertainty quantification in complex turbulent dynamical systems",
            "venue": "SIAM Review 60 (3) ",
            "year": 2018
        },
        {
            "authors": [
                "Z. Long",
                "Y. Lu"
            ],
            "title": "B",
            "venue": "Dong, Pde-net 2.0: Learning pdes from data with a numeric-symbolic hybrid deep network, Journal of Computational Physics 399 ",
            "year": 2019
        },
        {
            "authors": [
                "L. Lu",
                "P. Jin",
                "G. Pang",
                "Z. Zhang",
                "G.E. Karniadakis"
            ],
            "title": "Learning nonlinear operators via deeponet based on the universal approximation theorem of operators",
            "venue": "Nature machine intelligence 3 (3) ",
            "year": 2021
        },
        {
            "authors": [
                "P. Jin",
                "S. Meng",
                "L. Lu"
            ],
            "title": "Mionet: Learning multiple-input operators via tensor product",
            "venue": "SIAM Journal on Scientific Computing 44 (6) ",
            "year": 2022
        },
        {
            "authors": [
                "L. Lu",
                "X. Meng",
                "S. Cai",
                "Z. Mao",
                "S. Goswami",
                "Z. Zhang",
                "G.E. Karniadakis"
            ],
            "title": "A comprehensive and fair comparison of two neural operators (with practical extensions) based on fair data",
            "venue": "Computer Methods in Applied Mechanics and Engineering 393 ",
            "year": 2022
        },
        {
            "authors": [
                "S. Wang",
                "H. Wang",
                "P. Perdikaris"
            ],
            "title": "Learning the solution operator of parametric partial differential equations with physics-informed deeponets",
            "venue": "Science advances 7 (40) ",
            "year": 2021
        },
        {
            "authors": [
                "W. Littman",
                "G. Stampacchia",
                "H.F. Weinberger"
            ],
            "title": "Regular points for elliptic equations with discontinuous coefficients",
            "venue": "Annali della Scuola Normale Superiore di Pisa-Classe di Scienze 17 (1-2) ",
            "year": 1963
        },
        {
            "authors": [
                "T. Chen",
                "H. Chen"
            ],
            "title": "Universal approximation to nonlinear operators by neural networks with arbitrary activation functions and its application to dynamical systems",
            "venue": "IEEE Transactions on Neural Networks 6 (4) ",
            "year": 1995
        },
        {
            "authors": [
                "A.G. Baydin",
                "B.A. Pearlmutter",
                "A.A. Radul",
                "J.M. Siskind"
            ],
            "title": "Automatic differentiation in machine learning: a survey",
            "venue": "Journal of Marchine Learning Research 18 ",
            "year": 2018
        },
        {
            "authors": [
                "M. Raissi",
                "P. Perdikaris",
                "G.E. Karniadakis"
            ],
            "title": "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations",
            "venue": "Journal of Computational Physics 378 ",
            "year": 2019
        },
        {
            "authors": [
                "M. Zhu",
                "H. Zhang",
                "A. Jiao",
                "G.E. Karniadakis",
                "L. Lu"
            ],
            "title": "Reliable extrapolation of deep neural operators informed by physics or sparse observations",
            "venue": "Computer Methods in Applied Mechanics and Engineering 412 ",
            "year": 2023
        },
        {
            "authors": [
                "S. Lanthaler",
                "S. Mishra",
                "G.E. Karniadakis"
            ],
            "title": "Error estimates for deeponets: A deep learning framework in infinite dimensions",
            "venue": "Transactions of Mathematics and Its Applications 6 (1) ",
            "year": 2022
        },
        {
            "authors": [
                "Y. Lu",
                "H. Chen",
                "J. Lu",
                "L. Ying",
                "J.H. Blanchet"
            ],
            "title": "Machine learning for elliptic pdes: Fast rate generalization bound, neural scaling law and minimax optimality",
            "venue": "in: The Tenth International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Y. Shin",
                "J. Darbon",
                "G. Em Karniadakis"
            ],
            "title": "On the convergence of physics informed neural networks for linear second-order elliptic and parabolic type PDEs",
            "venue": "Communications in Computational Physics 28 (5) ",
            "year": 2020
        },
        {
            "authors": [
                "T. Luo",
                "H. Yang"
            ],
            "title": "Two-layer neural networks for partial differential equations: Optimization and generalization theory, arXiv preprint arXiv:2006.15733",
            "year": 2006
        },
        {
            "authors": [
                "S. Wu",
                "A. Zhu",
                "Y. Tang",
                "B. Lu"
            ],
            "title": "Convergence of physics-informed neural networks applied to linear second-order elliptic interface problems",
            "venue": "Communications in Computational Physics 33 (2) ",
            "year": 2023
        },
        {
            "authors": [
                "T. De Ryck",
                "S. Lanthaler",
                "S. Mishra"
            ],
            "title": "On the approximation of functions by tanh neural networks",
            "venue": "Neural Networks 143 ",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Learning operator mapping between infinite-dimensional Banach spaces via neural networks has attracted a considerable amount of attention in recent years. In this work, we propose an interfaced operator network (IONet) to solve parametric elliptic interface PDEs, where different coefficients, source terms and boundary conditions are considered as input features. To capture the discontinuities of both input functions and output solutions across the interface, IONet divides the entire domain into several separate sub-domains according to the interface, and leverages multiple branch networks and truck networks. Each branch network extracts latent representations of input functions at a fixed number of sensors on a specific sub-domain, and each truck network is responsible for output solutions on one sub-domain. In addition, tailored physics-informed loss of IONet is proposed to ensure physical consistency, which greatly reduces the requirement for training datasets and makes IONet effective without any paired input-output observations in the interior of the computational domain. Extensive numerical studies show that IONet outperforms existing state-of-the-art deep operator networks in terms of accuracy, efficiency, and versatility.\nKeywords: Parametric elliptic interface problems; Interfaced operator network; Neural networks."
        },
        {
            "heading": "1. Introduction",
            "text": "Elliptic interface problems find extensive applications across a wide spectrum of fields such as fluid mechanics [1, 2], mechanics of materials [3, 4], electromagnetics [5], biomimetics [6, 7], and flow in porous media [8]. Accurate modeling and rapid evaluation of these differential equations are critical in both scientific and engineering realms. A lot of computational tasks arising in science and engineering often involve repeated evaluation of an expensive forward model for numerous statistically similar inputs. These tasks, known as parametric PDE problems, encompass various areas such as inverse problems, control and optimization, risk assessment, and uncertainty quantification [9, 10]. When dealing with parametric PDE with discontinuous coefficients across certain interfaces, i.e., parametric interface problems, the low global regularity of the solution and the irregular geometry of the interface give rise to additional challenges, particularly for problems with non-smooth interfaces containing geometric singularities such as sharp edges, tips and cusps.\nConsider an open and bounded domain \u2126 \u2282 Rd with a Lipschitz boundary \u2202\u2126. There is an interface \u0393 that separates the domain \u2126 into two disjoint sub-domains, i.e., \u21261 and \u21262. For illustration, a sketch of the computational domain considered in 2D is shown in Fig. 1. In this paper, we will develop deep learning methods to solve\n\u2217Corresponding author. Email addresses: bzlu@lsec.cc.ac.cn\nPreprint submitted to Elsevier\nar X\niv :2\n30 8.\n14 53\n7v 1\n[ m\nat h.\nN A\n] 2\n8 A\nug 2\n02 3\nthe following parametric second-order linear elliptic interface problem:\n\u2212\u2207 \u00b7 (a\u2207u) + bu = f , in \u2126 \\ \u0393, (1a)\nJuK = gD, on \u0393, (1b)\nJa\u2207u \u00b7 nK = gN , on \u0393, (1c)\nu = h, on \u2202\u2126, (1d)\nwhere n denotes the outward unit normal vectors of \u0393 (from \u21261 to \u21262), J\u00b7K denotes the jump across the interface, for a point x\u03b3 \u2208 \u0393,\nJuK(x\u03b3) : = lim x\u2208\u21262 x\u2192x\u03b3 u(x) \u2212 lim x\u2208\u21261 x\u2192x\u03b3 u(x),\nJa\u2207u \u00b7 nK(x\u03b3) : = lim x\u2208\u21262 x\u2192x\u03b3 a(x)\u2207u(x) \u00b7 n \u2212 lim x\u2208\u21261 x\u2192x\u03b3 a(x)\u2207u(x) \u00b7 n.\nHere, the coefficient a(x) : \u2126 \u2192 R is continuous and positive in each of the sub-domains \u21261 and \u21262, and is discontinuous across the interface \u0393; the coefficient b(x) : \u2126 \u2192 R and source f (x) : \u2126 \u2192 R is continuous in each of the sub-domains and may be discontinuous across the interface \u0393. We will also consider a nonlinear example, i.e., replacing bu here with b(u). The latent solution u(x) : \u2126 \u2192 R to this problem is piece-wise smooth but with low global regularity, or even discontinuous across the interface. The goal of this paper is to provide quick and accurate approximation of the operator that maps any PDE parameter (e.g., the coefficient a(x), the source term f (x)) to the corresponding solution u of the given interface problem (1).\nClassical numerical methods for solving elliptic interface problems can be roughly divided into two categories: interface-fitted methods and interface-unfitted methods. The first type of approach is suitable for solving PDE problems defined in complex domains. The methods that fall under this category include the classical finite element method (FEM) [11, 12, 13], boundary element method (BEM) [14], weak Galerkin method [15], etc. These methods require the mesh surface to be aligned with the interface to maintain optimum convergence behavior. This alignment ensures that the interface conditions are correctly applied at the interface, which in turn enhances the accuracy of numerical solutions. However, employing such methods results in significant, non-trivial computational costs to generate interface-fitted meshes of irregular domains or interfaces [16], especially when high accuracy is required. To alleviate the burden of mesh generation, many works employ interface-unfitted meshes (e.g., a uniform Cartesian mesh) to discretize the computational domain and enforce jump conditions by modifying finite difference stencils or finite element basis near the interface, for example, the immersed boundary method (IBM)\n[17], immersed interface method (IIM) [18], immersed finite element method [19], ghost fluid method (GFM) [20] and its improvement (xGFM) [21], a cartesian grid finite volume approach (FVM) [22], matched interface and boundary method [23], extended finite element methods (XFEM) [24, 25], and references therein.\nBesides the mesh-based methods, there are also many efforts focusing on meshless or meshfree numerical methods for interface problems, such as direct meshless local Petrov-Galerkin method [26], global RBF-QR collection method [27], local RBF meshless methods [28], meshless method based on pascal polynomials and multiple-scale approach [29], etc. In addition, there is a growing interest in utilizing neural network-based methods to solve elliptic interface problems. [30] employed a shallow neural network to remove the inhomogeneous boundary conditions and developed a deep Ritz-type approach for solving the interface problem with continuous solutions. Based on the observation that the solution to the interface problem is usually piece-wise continuous, the combination of deep learning and domain decomposition methods has received a lot of attention. The approximate solution to Eq. (1) can be obtained by minimizing a loss function derived from the least squares principle [31] or the variational principle [32, 33]. Adaptively setting appropriate penalty weights among different terms in the loss function could accelerate overall training and improve accuracy [34, 35, 36]. Moreover, utilizing specially designed neural network structures is another way to improve the performance of neural models. [37, 38] introduced multi-scale features into neural networks for solving the Poisson-Boltzmann equation. [39, 40, 41] augmented an extra feature input to the neural network to capture the discontinuity of the solution to elliptic interface problems.\nAlthough these numerical methods have been shown to be effective to some extent, they are employed to solve a given instance of the elliptic interface problem (1), where the coefficients a(x) and b(x), the forcing terms f (x), the interface conditions gD(x) : \u0393 \u2192 R and gN(x) : \u0393 \u2192 R, and the boundary condition h(x) : \u2202\u2126 \u2192 R are given in advance. In other words, these methods treat a PDE with different parameters as different tasks, each of which needs to be solved end-to-end, which is computationally expensive and time-consuming. To address these challenges, one approach is to employ a reduced-order model that leverages a set of high-fidelity solution snapshots to construct rapid emulators [42, 43]. However, the validity of this method relies on the assumption that the solution set is contained approximately in a low-dimensional manifold, which could potentially lead to compromised accuracy and generalization performance [44, 45].\nRecently, as an emerging paradigm in scientific machine learning, several operator neural networks, such as PDE-Nets [46], deep operator network (DeepONet) [47] and Fourier neural operator (FNO) [48], have been developed to directly learn the solution mapping between two infinite-dimensional function spaces. The ability of neural networks to learn from data makes them particularly well-suited for this task. Such methods have great potential of developing fast forward and inverse solvers for PDE problems and have shown good performance in building surrogate models for many types of PDEs, including the Burgers\u2019 equation [47, 49], Navier-Stokes equations [48, 50], Darcy flow [50], diffusion-reaction PDE [51], and so on.\nDespite the aforementioned success, these operator learning methods cannot be applied directly to solving parametric interface problems due to the following two reasons: First, to work with the input function numerically, we typically need to discretize the input functions and evaluate them at a set of locations, which discards the irregularity of the input on the interface. Second, the global regularity of solutions to interface problems is usually very low, even discontinuous [52], but these operator networks are inefficient at capturing the irregularity of the solutions to interface problems. These can limit their ability to accurately represent and learn the complex behavior\nassociated with interface problems.\nTo address these limitations, in this paper we propose a novel meshfree method for approximation of the solution operator for parametric elliptic interface problems. Different from existing operator networks, we divide the entire domain into several separate sub-domains according to the interface, and leverage multiple branch networks and truck networks. Specifically, each branch network encodes the input function at a fixed number of sensors on one sub-domain, and each truck network is responsible for output solutions on one sub-domain. Such an architecture allows the model to accommodate irregularities in input functions and solutions. In addition, tailored physics-informed loss is proposed to ensure physical consistency, which greatly reduces the requirement for training datasets and makes the network effective without any paired input-output observations in the interior of the computational domain. The proposed method circumvents mesh generation and numerical discretization on the interface, thus easily handling problems in irregular domains, and provides quick simulation given different input functions by being trained only once. Herein, we name this neural operator as Interfaced Operator Network (IONet). Numerical results show that IONet exhibits good numerical performance and better accuracy, as well as generalization properties for various input parameters, compared with state-of-the-art models.\nThe rest of this paper is organized as follows: In Section 2, we review the basic idea of the operator network and the DeepONet method. In Section 3, we introduce the proposed interfaced neural network in detail. Then in Section 4, we investigate the performance of the proposed methods in four typical numerical examples. Finally, we conclude the paper and discuss some future directions in Section 5."
        },
        {
            "heading": "2. Learning operators with neural networks",
            "text": "In this section, we brief overview of the DeepONet model architecture [47] that can learn nonlinear operators between infinite function space, and two of its extensions, i.e., Multi-input operator network (MIONet) [49] and Physics-informed DeepONet (PI-DeepONet) [51].\nLet V and U be two Banach spaces, and G be an operator that maps between these two infinite-dimensional function spaces, i.e., G : V \u2192 U. We assume that for each v(y) : y \u2192 R in V, there is a unique corresponding output function u in U that can be represented as G(v)(x) : x \u2192 R. Analogously, in the context of parametric PDE problems, V and U are denoted as the input function space and the solution space, respectively. Following the original works of [47, 51], an unstacked DeepONet G\u03b8 is trained to approximate the target solution operator G, where G\u03b8 prediction of a function (an input parameter) v \u2208 V evaluated at a point x in the domain of G(v) can be expressed as\nG\u03b8(v)(x) = Nb(v(y1), v(y2), . . . , v(ym))T\ufe38 \ufe37\ufe37 \ufe38 branch net Nt(x)\ufe38\ufe37\ufe37\ufe38 trunk net + b0\ufe38\ufe37\ufe37\ufe38 bias\n= K\u2211 k=1 bktk + b0,\nwhere \u03b8 denotes all the trainable parameters, i.e., the set consisting of the parameters in branch network Nb and trunk network Nt as well as bias b0 \u2208 R. Here, [b1, b2, . . . , bK]T \u2208 RK denotes the output of Nb as a feature embedding of input function v, and [t1, t2, . . . , tK]T \u2208 RK represents the output of Nt, and {y1, y2, . . . , ym} is a collection of fixed point locations, the so-called \u201csensors\u201d, where we discretize the input function v. It is noted that DeepONet are capable of approximating any continuous operators [53, 47], making them a powerful tool in area of scientific computation.\nMIONet [49] extends the architecture and approximation theory of DeepONet to the case of operators defined\non multiple Banach spaces. Let G be a multi-input operator defined on the product of Banach spaces:\nG : V1 \u00d7V2 \u00d7 . . . \u00d7Vq \u2192U,\nwhere V1,V2, . . . ,Vq are q different input Banach spaces, which can be defined on different regions, U denotes the output Banach space. Then, when we employ a MIONet G\u03b8 to approximate the operator G, a given input function (v1, v2, . . . , vq) \u2208 V1\u00d7V2\u00d7 . . .\u00d7Vq, G\u03b8(v1, v2, . . . , vq) prediction at a point x in the domain of G(v1, v2, . . . , vq) is formulated as\nG\u03b8(v1, v2, . . . , vq)(x) = S Nb1 (v1)\ufe38 \ufe37\ufe37 \ufe38 branch1 \u2299Nb2 (v2)\ufe38 \ufe37\ufe37 \ufe38 branch2 \u2299 . . . \u2299 Nbq (vq)\ufe38 \ufe37\ufe37 \ufe38 branchq \u2299Nt(x)\ufe38\ufe37\ufe37\ufe38 trunk  + b0\ufe38\ufe37\ufe37\ufe38 bias\n= K\u2211 k=1 tk q\u220f i=1 bik + b0.\nHere, S is the summation of all the components of a vector, \u2299 represents the Hadamard product. Each input function vi is projected onto finite-dimensional spaces Rmi as vi := [vi(yi1), vi(y i 2), . . . , vi(y i mi )] T in the same manner as DeepONet, where {yij} mi j=1 is the set of sensors in the domain of vi. Similarly, [b i 1, b i 2, . . . , b i K] and [t1, t2, . . . , tK] denote the output of sub-branch network Nbi (vi) and trunk network Nt(x), respectively. In the framework of DeepONet, the data-driven (DD) approach was adopted to train network and determine the parameter \u03b8. Precisely, the training dataset is in the form of paired input-output observations, and the trainable parameters \u03b8 can be identified by minimizing the following empirical loss function of the general form\nLoss(\u03b8) = 1\nNP N\u2211 n=1 P\u2211 p=1 \u2223\u2223\u2223G\u03b8(vn)(xn,p) \u2212G(vn)(xn,p)\u2223\u2223\u22232 , where {vn}Nn=1 denotes N input functions that sampled from the parameter spaceV, and for each input function of DeepONet, the training data points {xn,p}Pp=1 are randomly sampled form the computational domain of G(vn) and can be set to vary for different n.\nFurthermore, PI-DeepONet [51] reduces the requirement for paired observed data and ensures that the learned operator network adheres to fundamental physical constraints by using automatic differentiation [54] to formulate appropriate regularization mechanisms. To better illustrate this, let us consider a generic parametric PDE expressed as: L(v, u) = 0, in \u2126,\nu = h, on \u2202\u2126,\nwhere v and u denote the input function and latent solution, respectively. Drawing motivation from physicsinformed neural network (PINN) [55], the trainable parameter \u03b8 of PI-DeepONet can be optimized by minimizing the residuals of the governing equations and the corresponding boundary conditions through the use of automatic differentiation [54] in the training phase. Specifically, the physics-informed loss function of PI-DeepONet can be formulated as\nLoss(\u03b8) = \u03bbrLossr(\u03b8) + \u03bbbLossb(\u03b8).\nHere, \u03bbr and \u03bbb are non-negative weight, the loss term\nLossr(\u03b8) = 1\nNPr N\u2211 n=1 Pr\u2211 p=1 \u2223\u2223\u2223L(vn,G\u03b8(vn))(xrn,p)\u2223\u2223\u2223\nenforces the operator network to satisfy underlying physical constraints, and\nLossb(\u03b8) = 1\nNPb N\u2211 n=1 Pb\u2211 p=1 \u2223\u2223\u2223G\u03b8(vn)(xbn,p) \u2212 h(xbn,p)\u2223\u2223\u2223 penalizes the violation of the boundary conditions, where {xrn,p}Prp=1 and {xbn,p} Pb p=1 denote the training data points randomly sampled from the interior and boundary of the domain \u2126, respectively."
        },
        {
            "heading": "3. Interfaced operator network",
            "text": "In this section, we discuss how to use deep learning-based methods to numerically solve parametric interface problems. The main idea of our new method is to approach the potential solution operator through multiple suboperators while remaining consistent with the potential physical constraints. For the sake of simplicity, we will introduce our method in the case of two sub-domains. Note that this setting can be easily generalized to a multidomain scenario, depending on the number of distinct domains involved. Specifically, we consider Eq. (1) as a parametric interface problem of general form. For an illustration, we consider to learn a solution operator G mapping form the coefficient a(x) to the solution u(x) of Eq. (1), i.e.,\nG : a(x)\u2192 u(x)."
        },
        {
            "heading": "3.1. Network architecture of IONet",
            "text": "To preserve the inherent discontinuity of interface problems, we decompose the computational domain into two sub-domains according to the interface and leverage two operator networks that share some parameters, each of which is responsible for the solution in one sub-domain. In particular, we define the following IONet to approximate the operator G:\nG\u03b8(a)(x) =  G1\u03b8(a)(x), if x \u2208 \u21261, G2\u03b8(a)(x), if x \u2208 \u21262, (2)\nwhere a is the input function and x denotes the location where the output function is evaluated. Note that input functions are discretized and evaluated at a set of sensors typically. To retain the irregularity of the input function on the interface, we divide the set of sensors according to the interface and use two branch networks, denoted as Nb1 and Nb2 , for extracting latent representations of input functions on the corresponding sub-domain. Similar to vanilla DeepONet [47], in each sub-operator Gi\u03b8, we use a trunk network denoted as N it for extracting continuous input coordinates at which the output functions are evaluated. Finally, following MIONet [49], we merge the outputs of all sub-networks via a Hadamard product and a summation and add a bias in the last stage. More specifically, The sub-operator in (2) is constructed as follows:\nGi\u03b8(a)(x) = S Nb1 (a(y11), \u00b7 \u00b7 \u00b7 , a(y1m1 ))\ufe38 \ufe37\ufe37 \ufe38 branch1 \u2299Nb2 (a(y21), \u00b7 \u00b7 \u00b7 , a(y2m2 ))\ufe38 \ufe37\ufe37 \ufe38 branch2 \u2299N it (x)\ufe38\ufe37\ufe37\ufe38 trunk  + bi0\ufe38\ufe37\ufe37\ufe38 bias\n= K\u2211 k=1 tikb1kb2k + b i 0.\n(3)\nHere, \u03b8 denotes the trainable parameters in this architecture. For i = 1, 2, {yij} mi j=1 represents the collection of sensors for evaluating a(x) in \u2126i, [bi1, bi2, . . . , biK] and [ti1, t i 2, . . . , t i K] denote the output features of the branch networksNbi\nand the trunk network N it , respectively. The network architecture of IONet is schematically visualized on the left side of Fig. 2. To demonstrate the capability and performance alone, we apply the simplest feedforward Neural Networks (FNNs) with tanh activation as the branch and trunk networks in this paper, and we note that other neural networks such as ResNet and CNN can be chosen as the sub-networks in IONet according to specific problems.\nNext, we show that IONet is able to approximate arbitrary continuous operators with discontinuous inputs and\noutputs. For later analysis, we define the following space\nX(\u2126) = H0(\u2126) \u22c2 H2(\u21261) \u22c2 H2(\u21262)\nequipped with the norm\n\u2225u\u2225X(\u2126) = \u2225u\u2225H2(\u21261) + \u2225u\u2225H2(\u21262) .\nThen, the approximation theorem of IONet is given as follows.\nTheorem 1. Let \u2126 \u2282 Rd be a bounded domain, \u21261 \u2282 \u2126 be an open domain with boundary \u0393 = \u2202\u21261 and \u21262 = \u2126 \\\u21261. Assume G : L\u221e(\u2126) \u22c2 C(\u21261) \u22c2 C(\u21262)\u2192 X(\u2126) is a continuous operator and K \u2282 L\u221e(\u2126) \u22c2 C(\u21261) \u22c2 C(\u21262) is a compact set. Then for any \u03b5 > 0, there exist positive integers m1, m2, p, tanh FNNs Nb1 : Rm1 \u2192 Rp, Nb2 : Rm2 \u2192 Rp, N1t , N2t : Rd \u2192 Rp, and y11, \u00b7 \u00b7 \u00b7 , y1m1 \u2208 \u21261, y21, \u00b7 \u00b7 \u00b7 , y2m2 \u2208 \u21262, such that\nsup a\u2208K \u2225\u2225\u2225\u2225G(a)(\u00b7) \u2212 S (Nb1 (a(y11), \u00b7 \u00b7 \u00b7 , a(y1m1 )) \u2299 Nb2 (a(y21), \u00b7 \u00b7 \u00b7 , a(y2m2 )) \u2299 N it (\u00b7))\u2225\u2225\u2225\u2225H2(\u2126i) \u2264 \u03b5, where index i = 1, 2, S is the summation of all the components of a vector, \u2299 is the Hadamard product.\nProof. The proof can be found in Appendix A."
        },
        {
            "heading": "3.2. Physics-informed loss function of IONet",
            "text": "Similar to DeepONet, a data-driven approach can be adopted to train the network and optimize the parameters\n\u03b8 by minimizing the following mean square error loss:\nLoperator(\u03b8) = 1\nNoPo No\u2211 n=1 Po\u2211 p=1 \u2223\u2223\u2223G\u03b8(ano)(xon,p) \u2212G(ano)(xon,p)\u2223\u2223\u22232 , (4) where {ano}Non=1 denotes No input functions that sampled from the parameter space; for n = 1, \u00b7 \u00b7 \u00b7 ,No, the training data points {xon,p}Pop=1 \u2282 \u2126 denotes the set of locations to evaluate the output function and can be set to vary for different n; G(ano)(xon,p) and G\u03b8(ano)(xon,p) are evaluated values of output functions of the solution operator G and IONet G\u03b8 at location xon,p when ano is the input function, respectively.\nThe above data-driven approach based on the assumption that there exists enough labeled data to train the\nmodel, i.e., we need a large corpus of paired input-output training data of the following form{( ano(y 1 1), \u00b7 \u00b7 \u00b7 , ano(y1m1 ), a n o(y 2 1), \u00b7 \u00b7 \u00b7 , ano(y2m2 ), x o n,p,G(a n o)(x o n,p) )}\nn=1,\u00b7\u00b7\u00b7 ,No, p=1,\u00b7\u00b7\u00b7 ,Po .\nHowever, the cost of experimental data acquisition and high-quality numerical simulation is generally expensive, and we are inevitably faced with limited or even intractable training data for many practical scenarios. In this section, we introduce a physics-informed loss function of IONet inspired by PINN and PI-DeepONet. By adding physics constraints to restrict the IONet output function to be consistent with the given interface PDE (1), the proposed IONet is capable of learning the solution operator of parametric interface problems even without any labeled training data (except for boundary and interface conditions).\nNext, we demonstrate the use of IONet to solve parametric interface problem (1) with coefficient a(x) as input\nfunction. Define\nLri (\u03b8) := N\u2211\nn=1 Pi\u2211 p=1 \u2223\u2223\u2223\u2223\u2223\u2223\u2212\u2207 \u00b7 (an(x)\u2207G\u03b8(an)(x)) \u2223\u2223\u2223\u2223\u2223 x=xin,p + b(xin,p)G\u03b8(a n)(xin,p) \u2212 f (xin,p) \u2223\u2223\u2223\u2223\u2223\u22232 , where {xin,p}Pip=1 \u2282 \u2126i with i = 1, 2 are randomly sampled from the computational domain. In addition, let L\u0393(\u03b8) = L\u0393D + L\u0393N , where\nL\u0393D (\u03b8) = N\u2211\nn=1 P\u03b3\u2211 p=1 \u2223\u2223\u2223JG\u03b8(an)K(x\u03b3n,p) \u2212 gD(x\u03b3n,p)\u2223\u2223\u22232 = N\u2211 n=1 P\u03b3\u2211 p=1 \u2223\u2223\u2223G2\u03b8(an)(x\u03b3n,p) \u2212G1\u03b8(an)(x\u03b3n,p) \u2212 gD(x\u03b3n,p)\u2223\u2223\u22232 , and\nL\u0393N (\u03b8) = N\u2211\nn=1 P\u03b3\u2211 p=1 \u2223\u2223\u2223Jan\u2207G\u03b8(an) \u00b7 nK(x\u03b3n,p) \u2212 gN(x\u03b3n,p)\u2223\u2223\u22232\n= N\u2211 n=1 P\u03b3\u2211 p=1 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 limx\u2208\u21262x\u2192x\u03b3n,p an(x)\u2207G2\u03b8(an)(x \u03b3 n,p) \u00b7 n \u2212 lim x\u2208\u21261 x\u2192x\u03b3n,p an(x)\u2207G1\u03b8(an)(x \u03b3 n,p) \u00b7 n \u2212 gN(x\u03b3n,p) \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 2 .\nHere, {x\u03b3n,p} P\u03b3 p=1 represents a set of training points sampled from the interface \u0393 for n-th input function. Let\nLb(\u03b8) := N\u2211\nn=1 Pb\u2211 p=1 \u2223\u2223\u2223G\u03b8(an)(xbn,p) \u2212 h(xbn,p)\u2223\u2223\u2223 , where {xbn,p}Pbp=1 denotes training data points randomly sampled from the boundary \u2202\u2126. By the definitions of IONet (2) and (3), the output function G\u03b8(an) has a continuous representation in each sub-domain:\nG\u03b8(an)(x) =  S ( Nb1 (an(y11), \u00b7 \u00b7 \u00b7 , an(y1m1 )) \u2299 Nb2 (a n(y21), \u00b7 \u00b7 \u00b7 , an(y2m2 )) \u2299 N 1 t (x) ) + b10, if x \u2208 \u21261, S ( Nb1 (an(y11), \u00b7 \u00b7 \u00b7 , an(y1m1 )) \u2299 Nb2 (a n(y21), \u00b7 \u00b7 \u00b7 , an(y2m2 )) \u2299 N 2 t (x) ) + b20, if x \u2208 \u21262.\nAnd automatic differentiation [54] in a deep learning framework can then be employed to calculate the derivatives of G\u03b8(an) at xin,p provided the trunk networks N it are smooth enough. Then, we are ready to define the physicsinformed loss function of IONet as follows:\nLphysics(\u03b8) = \u03bb1Lr1 (\u03b8) + \u03bb2Lr2 (\u03b8) + \u03bb3L\u0393(\u03b8) + \u03bb4Lb(\u03b8), (5)\nwhere Lr1 (\u03b8) and Lr2 (\u03b8) are to approximately restrict the IONet output function to obey the given governing PDE (1a), while Lb(\u03b8) and L\u0393(\u03b8) penalize IONet for violating the boundary (1d) and interface conditions (1b) and (1c), respectively. Such physics-informed loss function of IONet is schematically depicted in the right side of Fig. 2. Finally, we combine loss functions (4) and (5) and minimize the following composite loss function to obtain the parameter \u03b8 of IONet:\nL(\u03b8) = \u03bbpLphysics(\u03b8) + \u03bboLoperator(\u03b8). (6)"
        },
        {
            "heading": "4. Numerical Results",
            "text": "In this section, we provide a series of numerical studies for solving parametric elliptic interface problems to demonstrate the effectiveness of IONet. Throughout all benchmarks, branch networks and trunk networks in IONet are tanh FNNs. All operator network models are trained via stochastic gradient descent using Adam optimizer with default settings. The learning rate is set to exponential decay with a decay-rate of 0.95 every #Epoch/100, where #Epoch denotes the number of maximum iterations. Without trying to find the best hyper-parameter settings, the weights among different terms in empirical loss function (5) are set to \u03bb1 = \u03bb2 = \u03bb3 = 1 and \u03bb4 = 100, respectively. In this section, IONet is trained by minimizing loss function (6) with \u03bbp = 1 and \u03bbo = 0 by default, denoted as \u201cIONet\u201d or \u201cPI-IONet\u201d. In particular, we denote IONet trained using data-driven approach, i.e., \u03bbp = 0 and \u03bbo = 1, as \u201cDD-IONet\u201d. If no otherwise specified, the training data points for evaluating loss functions are sampled uniformly and randomly in the computational domain, while the locations for evaluating output solutions are the equispaced grid points. All experiments are tested on one NVIDIA Tesla V100 GPU."
        },
        {
            "heading": "4.1. Parametric elliptic interface problems in one dimension",
            "text": "Example 1. As the first example, we investigate the capability of the suggested method to handle non-zero jump conditions in elliptic interface problem (1) defined on an interval \u2126 = [0, 1] with an interface point at x\u0393 = 0.5:\n\u2212\u2207 \u00b7 (a(x)\u2207u(x)) = 0, x \u2208 \u2126,\ngD(x\u0393) = 1, gN(x\u0393) = 0,\nu(0) = 1, u(1) = 0.\n(7)\nThe latent solution of this example is discontinuous across the interface, and our goal in this case is to learn a solution operator G mapping from discontinuous coefficient functions a(x) to the latent solutions u(x). To make the input function\na(x) =  a1(x), x \u2208 [0, 0.5]\na2(x), x \u2208 (0.5, 1]\nstrictly positive, we let ai(x) = a\u0303(x) \u2212 minx a\u0303(x) + 1, where a\u0303(x) is sampled from a zero-mean Gaussian random field (GRF) [47] with the following covariance kernel\nkl(x1, x2) = exp(\u2212\u2225x1 \u2212 x2\u22252/2l2)\nusing a length scale of l = 0.25 (see the left panel of Fig. 3 for an illustration).\nWe randomly sample 10000 and 1000 input functions a(x) for training and testing, respectively, where ai(x) with i = 1, 2 are sampled independently from the given distribution. The sensors of the input functions are 100 equidistant grid points in the interval [0, 1]. To obtain the reference solutions, for each input function, we solve Eq. (7) using the matched interface and boundary (MIB) method [23] on a mesh of size 1 by 1000, spanning the interval [0, 1].\nIn this example, we chose to represent the latent solution operator using IONet, where branch and trunk networks are 5-layer FNNs with 100 units per layer. To demonstrate the versatility of IONet in addressing parametric\ninterface problems, we examine the performance of both DD-IONet with relu activation function and PI-IONet with tanh activation function in solving Eq. (7). Furthermore, we investigate the performance of two state-of-theart models, vanilla DeepONet (DD-DeepONet) [47] and physics-informed DeepONet (PI-DeepONet) [51], under almost the same hyper-parameter setting. Note that paired input-output measurements of each training sample used to train data-driven models, i.e., DD-IONet and DD-DeepONet, are generated by solving the given Eq. (1) via the MIB method. In all cases, we train the neural models for 100K iterations. The mean and the standard deviation of relative L2 errors between the predicted solution and the reference solution are presented in Table 1. As can be seen from this table, the relative L2 error of DD-IONet is measured at 1.12 \u00d7 10\u22123 is significantly less than that of DD-DeepONet (8.74 \u00d7 10\u22122). In addition, in the absence of any paired training data (except for boundary and interface conditions), the predicted solution of PI-IONet, trained by minimizing the physics-informed loss function (5), yields a \u223c70% improvement in prediction accuracy over that of PI-DeepONet. A visualization of numerical solutions is presented in Fig. 3. In particular, we present a comparison between the reference and the numerical solutions for five randomly sampled input functions from the test dataset. As can be seen from the second column, PI-DeepONet fails to yield accurate results; the numerical results of DDDeepONet can match the reference solutions well away from the interface (x\u0393 = 0.5), but there are large errors near the interface. The main reason is due to the fact that the output function space of DeepONet is a continuous function space, which limits its effectiveness in capturing discontinuities in solution functions to interface problems. The numerical results for DD-IONet and PI-IONet are displayed in the third column, where it can be seen that the numerical solutions naturally maintain the discontinuous nature of the numerical solution at the interface, demonstrating excellent agreement with the reference solutions. These numerical results show that IONet is more adaptable to the irregularities in the input function and the solutions, which exhibits a greater ability to represent the solutions of interface problems compared to the conventional DeepONet architecture.\nWe also compare the computational performance of our proposed method with that of the MIB solver. A comparison of the computational costs required for MIB and IONet for various mesh sizes (corresponding to the number of test data points Ntest in IONet) is recorded in Table 2. It can be observed that the test time of the MIB method increases significantly as the mesh size increases, while that of IONet is nearly independent of the test data size. In particular, when the mesh size reaches 2000, the test time of IONet is two orders of magnitude faster than MIB. The immunity to fluctuating mesh sizes showcases the ability of IONet to handle diverse and large datasets without compromising its performance. Its efficient processing capabilities make it exceptionally well-suited for scenarios where time constraints and computational resources are crucial factors.\nExample 2. Next, we consider Eq. (1) in case of the latent solution is continuous, while the derivative of the\nsolution is discontinuous across the interface:\n\u2212\u2207 \u00b7 (a\u2207u(x)) = f (x), x \u2208 \u2126,\ngD(x\u0393) = gN(x\u0393) = 0,\nu(0) = u(1) = 0.\n(8)\nThe computational domain is taken as \u2126 = [0, 1], and x\u0393 = 0.5 and the coefficient a(x) is a piece-wise constant defined by\na(x) =  1, if x \u2208 [0, 0.5], 2, if x \u2208 (0.5, 1].\nIn this example, the source term f is the input parameter, and our goal is to learn an operator G mapping from f to the solution u of Eq. (8).\nTo obtain the dataset, we model the input function f (x) using zero-mean GRFs. In particular, considering that input functions f may be discontinuous across the interface, we use two GRFs with length scales l = 0.2 and l = 0.1 to independently simulate f1 := f |\u21261 and f2 := f |\u21262 , respectively. Some trajectories of input functions are displayed in the first column of Fig. 4. Since the analytical solution of Eq. (8) is unknown, for each input function in our experiments, we employ the MIB method on a 1 \u00d7 1000 grid to obtain the baseline solution. Then, we construct the training and testing datasets with the same settings as demonstrated in Example 1.\nIn this example, we compare the performance of IONet, DD-DeepONet, and PI-DeepONet under almost the same settings. Here, IONet is trained by minimizing the residual of the interface equation and its corresponding boundary conditions. Table 3 displays the network architectures used to approximate the solution operator and records errors of the three models after 40K gradient descent iterations. Evidently, IONet improves the performance of DeepONet and PI-DeepONet. The mean relative L2 errors of numerical solutions and their gradients obtained by IONet are significantly less than those of PI-DeepONet and DD-DeepONet. Moreover, some visualizations of numerical results obtained by IONet and PI-DeepONet are shown in Fig. 4. As one can see from this figure, the numerical solution obtained by IONet align more consistently with the reference solution, even considering the continuous nature of the ground truth of Eq. 8. These results suggest that IONet effectively models the solution operator for interface problems.\nIt is remarked that other advantages of DeepONet also hold true for IONet, for example, the capability of providing accurate predictions for out-of-distribution test data [51, 56]. Fig. 5 depicts the trajectories of regenerated test input samples and the corresponding reference and numerical solution obtained by the MIB method and IONet, respectively. Note that the training dataset used to train the IONet model is generated using l1 = 0.2 and l2 = 0.1 in this figure, while the test input functions are generated using l1 = l2 = 0.3 (first row) or l1 = l2 = 0.15 (second row). Nevertheless, IONet is still able to obtain accurate numerical results. Specifically, in the test phase, when\nl1 = l2 = 0.3, the average relative L2 error is measured to be 3.73 \u00d7 10\u22123; and when l1 = l2 = 0.15, the average relative L2 error increases slightly to 9.58 \u00d7 10\u22123. These results further highlight the robustness and generalization capability of IONet."
        },
        {
            "heading": "4.2. Parametric elliptic interface problems in two dimensions",
            "text": "Example 3. To further investigate the capability of IONet, we consider a parametric interface problem (1) with\na sharp and complicated interface \u0393 which is given as\nx1(\u03d1) = 0.65cos(\u03d1)3, x2(\u03d1) = 0.65sin(\u03d1)3, 0 \u2264 \u03d1 \u2264 \u03c0,\nHere, the source term f is the input parameter of the target solution operator. In this example, we model the input function in the following way:\nfi(x) := f (x) \u2223\u2223\u2223 \u2126i =\npi1 [1 + 10(x21 + x 2 2)] 2 \u2212\npi2(x 2 1 + x 2 2)\n[1 + 10(x21 + x 2 2)] 3\nwhere (pi1, p i 2) comes from [50, 100] \u00d7 [1550, 1650] with i = 1, 2. The computational domain is a regular square \u2126 = [\u22121, 1] \u00d7 [\u22121, 1] (see the left panel of Fig. 6 for an illustration). The coefficient a(x) is a piece-wise constant, which is given by a(x)|\u21261 = 2 and a(x)|\u21262 = 1. The interface conditions on \u0393 are set as\ngD(x) = 1\n1 + 10(x21 + x 2 2) ,\ngN(x) = 0,\nand h(x) on boundary \u2202\u2126 is stated as\nh(x) = 2\n1 + 10(x21 + x 2 2) .\nOne can observe that if we take f1(x) = f2(x) with (p11, p 1 2) = (p 2 1, p 2 2) = (80, 1600), then Eq. (1) has following exact solution [34]\nu(x) =  1 1 + 10(x21 + x 2 2) , in \u21261,\n2 1 + 10(x21 + x 2 2) , in \u21262.\nTo this end, we randomly sample some pairs of input functions from the given data distribution except for (p11, p 1 2) = (p21, p 2 2) = (80, 1600), which is reserved for testing purposes. The sensors for the input functions in each subdomain are shown on the left side of Fig. 6. Specifically, there are 17 and 60 sensors in \u21261 and \u21262, respectively. Take advantage of being mesh-free, IONet can easily handle problems in irregular domains.\nIn this case, we investigate the error tendency under different training data sizes. Specifically, we vary the number of training functions, denoted as #Samples, from 10 to 320. The branch networks and trunk networks in IONet are 5-layer FNNs with 50 units per hidden layer. We train IONet by minimizing the physics-informed loss function (6) for 40K parameter updates. The variation of the L\u221e- and the relative L2-errors of numerical solutions against #Samples are shown in the right panel of Fig. 6. It is observed that increasing #Samples results in more accurate numerical solutions, eventually reaching a plain where the error reduction levels off. In addition, although IONet is a typical stochastic method as the initial values of parameters in the network and the SGD optimizer are random, it does not significantly affect the numerical results when #Samples is sufficiently large. As shown in Fig. 7, the numerical solution is in excellent agreement with the exact solution, and the relative L2 error is measured at 2.05 \u00d7 10\u22123. These results demonstrate the consistent and reliable performance of IONet in generating accurate\nnumerical results, even in scenarios where the interface is complicated.\nExample 4. This example aims to highlight the effectiveness of the proposed IONet for handling the parametric interface problem with variable boundary conditions. Computational domain \u2126 := [0, 1] \u00d7 [0, 1], and the interface is defined as \u0393 := {x := (x1, x2) | x2 = 0.5, x \u2208 \u2126}. Without losing generality, we defined \u21261 := {x | x2 > 0.5, x \u2208 \u2126} and \u21262 := {x | x2 < 0.5, x \u2208 \u2126}. Specifically, the interface problem takes the following specific form:\n\u2212\u2207 \u00b7 (a\u2207u(x)) = 0, x \u2208 \u2126,\nu(x) = h(x), x \u2208 \u2202\u2126 \\ \u0393, (9)\nwith interface conditions gD = 0 and gN = 0. The coefficient a is set to be 1 and 2 in \u21261 and \u21262, respectively (see Fig. 8 for an illustration). Here, we aim to learn the solution operator mapping boundary conditions h(x) to the solution u(x) to Eq. (9).\nWe use a GRF with periodic boundary conditions to model boundary conditions\nhi(x) := h(x)|\u2126i\u2229\u2202\u2126 \u223c N(0, 103(\u2212\u2206 + 100I)\u22122)\nwith i = 1, 2 independently. Specifically, we randomly sample 500 and 100 input functions as training and test datasets, respectively. For each test input function, the reference solutions are obtained by solving Eq. (9) using the finite element method (FEM) on a uniform grid of 257 \u00d7 257. In this study, we approximate the solution operator with an IONet, whose branch and trunk networks are 5- layer FNN with 120 units per hidden layer. We train the model through 100K iterations of stochastic gradient descent. The left panel of Fig. 9 shows the variation of the relative L2 error between reference and numerical\nsolutions as the number of sensors, denoted as #Sensor, increases. It is shown that when #Sensors is less than or equal to 64, the error decreases rapidly with the increase in the number of sensors, but it tends to level off as the sensor count is further augmented. This could be caused by optimization errors. Moreover, the errors in the two subdomains exhibit close proximity to one another, indicating the effectiveness of the proposed method in balancing errors across the subdomains. The histogram illustrating the distribution of test errors across all samples in the test dataset is displayed in the right panel of Fig. 9. Among all the testing samples, although the highest testing error is 1.10 \u00d7 10\u22121, 98% of the testing samples have relative errors within 0.1. Next, we fix #Sensors = 128. With almost the same setup, we train the DD-DeepONet model with paired\ninput-output measurements obtained by FEM, and train IONet by minimizing the physics-informed empirical loss (5). The average relative L2 errors between the reference solution and the numerical solution are recorded in Table 4. IONet achieves more accurate predictions compared to DeepONet, even in the absence of any paired inputoutput measurements within the computational domain. An example of a numerical solution obtained by IONet is shown in Fig. 10. It is evident that the predictions derived from the IONet model exhibit consistency with the reference solution."
        },
        {
            "heading": "4.3. Parametric elliptic interface problems in three dimensions",
            "text": "Example 5. In this case, our primary objective is to emphasize the robustness of IONet in effectively solving the Poisson-Boltzmann equation (PBE), a commonly encountered nonlinear elliptic interface problem. The PBE is a prevalent implicit continuum model utilized in the estimation of biomolecular electrostatic potentials \u03a6(x) and similar equations occur in various applications, including electrochemistry and semiconductor physics. The molecule is represented by a series of Nm charges qi at positions ci, where qi = ziec, zi \u2208 R, i = 1, . . . ,Nm. Specifically, we choose a real molecule (PDBID: ADP) with Nm = 39 atoms as an example. Without loss of generality, the molecule is translated from the average coordinate center of all atoms to the center of \u2126 = [\u221210, 10]3. Then, in the special case of 1 : 1 electrolyte, the PBE can be formulated for dimensionless potential u(x) = eck\u22121B T \u22121\u03a6(x) as follows:\n\u2212\u2207 \u00b7 (\u03f5(x)\u2207u(x)) + \u03ba\u03042(x) sinh(u(x)) = \u03b1 Nm\u2211 i=1 zi\u03b4(x \u2212 ci), x \u2208 \u2126,\nJu(x)K = 0, x \u2208 \u0393,\nJ\u03f5(x)\u2202u(x) \u2202n K = 0, x \u2208 \u0393,\nu(x) = \u03b1\n4\u03c0\u03f5(x) Nm\u2211 i=1 zi e\u2212\u03ba\u2225x\u2212ci\u2225 \u2225x \u2212 ci\u2225 , x \u2208 \u2202\u2126,\n(10)\nwhere \u03b4(\u00b7) is the Dirac delta function, the permittivity \u03f5(x) takes the values of \u03f5m\u03f50 and \u03f5s\u03f50 in the molecular region \u21261 and the solution region \u21262, respectively. The modified Debye-Hu\u0308ckel takes the values \u03ba\u0304 = 0 in \u21261 and \u03ba\u0304 = \u221a \u03f5m\u03f50\u03ba in\u21262, and constant \u03b1 = e2c kBT . Here, constants \u03f50, ec, \u03b2, \u03ba and T represent the vacuum dielectric constant, fundamental charge, Boltzmann\u2019s constant, Debye-Hu\u0308ckel constant and absolute temperature, respectively. Our goal is to learn an operator G mapping from the permittivity \u03f5(x) to the solution u(x) to PBE. Note that \u03f5 has a piece-wise constant nature, allowing us to directly utilize the function values as inputs for IONet without requiring sensor-based discretization.\nTo numerically solve PBE (10), we use a solution decomposition scheme to overcome the singular difficulty\ncaused by the Dirac delta distributions. Similar to our former work [34], u is decomposed as\nu(x) = G\u0304(x) + u\u0304(x).\nHere,\nG\u0304(x) = \u03b1\n4\u03c0\u03f5m\u03f50 Nm\u2211 i=1 zi \u2225x \u2212 ci\u2225 , \u2207G\u0304(x) = \u2212 \u03b1 4\u03c0\u03f5m\u03f50 Nm\u2211 i=1 zi x \u2212 ci \u2225x \u2212 ci\u22253 .\nRestricted to \u21261, u\u0304(x) satisfies the following PDE\n\u2212\u2207 \u00b7 (\u03f5m\u03f50\u2207u\u0304(x)) = 0, x \u2208 \u21261,\n\u2212\u2207 \u00b7 (\u03f5s\u03f50\u2207u\u0304(x)) + \u03ba\u03042 sinh(u\u0304(x)) = 0, x \u2208 \u21262,\nJu\u0304(x)K = G\u0304(x), x \u2208 \u0393,\nJ\u03f5(x)\u2202u\u0304(x) \u2202n K = \u03f5m\u03f50 \u2202G\u0304(x) \u2202n , x \u2208 \u0393,\nu\u0304(x) = \u03b1\n4\u03c0\u03f5s\u03f50 Nm\u2211 i=1 zi e\u2212\u03ba\u2225x\u2212ci\u2225 \u2225x \u2212 ci\u2225 , x \u2208 \u2202\u2126.\nIn the field of molecular simulation calculations, the solvation free energy\nE = kBT\n2 Nm\u2211 i=1 ziu\u0304(ci)\nis a physical quantity of interest in addition to the potential distribution.\nThe training dataset comprises 1000 input functions (constants) uniformly and randomly selected from the space (\u03f5m, \u03f5s) \u2208 [1, 2] \u00d7 [80, 100], while the test dataset is composed of equidistant grid points arranged in a 6 \u00d7 6 grid within this space. For each test sample, we solved PBE (10) using FEM to generate a reference solution. The locations for evaluating loss function are chosen from the mesh points of the interface-fitted mesh used in FEM for a consistent interface with FEM. Specifically, the number of mesh points are 1407, 3403, 2743, and 2402 in \u21261, \u21262, \u0393, and \u2202\u2126, respectively. Note that the surface of molecular ADP is described by point clouds generated by FEM in our previous work [7], and the unit outward normal vector for each point on the interface \u0393 is approximated by taking the average of the outward normal directions of all elements that contain the corresponding point. In the training phase, we randomly select one-tenth of the grid points in each region as training points for each input function.\nAll the branch and trunk networks within IONet consist of a 5-layer FNN with 150 units in each layer. After 50K iterations, the error of the predicted potential u(x) is validated against all test input functions, resulting an average relative L2 error of 1.69 \u00d7 10\u22122 \u00b1 2.32 \u00d7 10\u22124. Some specific assessments between the reference solution obtained by FEM and the predicted solution obtained by IONet are presented in Table 5. For this example, the final relative L2 errors of the potential and energy could reach the order of \u223c 10\u22122 and \u223c 10\u22123, respectively. A visual comparison of the reference and predicted surface potentials of the protein ADP is shown in Fig. 11. These findings further emphasize the capability of IONet to effectively handle parametric interface problems within irregular domains, even without any paired input-output measurements, except for boundary conditions. Although our\ncurrent work demonstrates the effectiveness of IONet in solving PBE (10) with a real small-molecule ADP, further research is needed to investigate the computational efficiency of IONet and other neural network-based methods in solving large-scale computational problems in biophysics, such as solving PBE with real macromolecules. We will postpone this part of the work to future research."
        },
        {
            "heading": "4.4. Parametric elliptic interface problems in six dimensions",
            "text": "Example 6. Our final example aims to highlight the ability of the proposed framework to handle highdimensional parametric interface problems. Here, we consider Eq. (1) defined on a 6-dimension sphere of radius 0.6 domain \u2126 enclosing another smaller 6-dimension sphere of radius 0.5 as the interior domain \u21261. Our goal is to learn the solution operator mapping from the source terms f to the latent solution of Eq. (1), i.e., G : f (x)\u2192 u(x), where f has the following specific forms\nf (x) =  \u2212 p1 6\u220f i=1 exp(xi), x \u2208 \u21261, \u2212 p2 6\u220f\ni=1\nsin(xi), x \u2208 \u21262,\nwhere (p1, p2) randomly sample from [1, 10] \u00d7 [\u221210,\u22121]. For the problem setup, the coefficient\na(x) =  1, x \u2208 \u21261,\n10\u22123, x \u2208 \u21262,\nhas a large contrast (a1/a2 = 103), the boundary condition is given as h(x) = \u220f6 i=1 sin(xi), the interface conditions\nare chosen as gD(x) = \u220f6 i=1 sin(xi) \u2212 \u220f6 i=1 exp(xi), and\ngN(x) = 5 3 10\u22123 6\u2211 i=1 xi cos(xi) 6\u220f j=1, j,i sin(x j)  \u2212  6\u2211 i=1 xi  6\u220f j=1 exp(x j)  . Note that, when we take f with p1 = 6 and p2 = \u22126, Eq. (1) exists an exact solution [39]\nu(x) =  6\u220f i=1 exp(xi), x \u2208 \u21261, 6\u220f i=1 sin(xi), x \u2208 \u21262.\nIn this study, we train IONet G\u03b8 with different scale of architecture to approximate the solution operator, and then test the model in case of input f with p1 = 6 and p2 = \u22126. Here, we use a set of randomly sampled points in the computational region as the sensors for discreting the input functions. For each combination of depth and width of FNNs, we train IONet for 40K iterations using 100 randomly sampled input functions. Table 6 shows the mean and standard deviation of the L\u221e errors and the relative L2 errors in the IONet solution over the whole domain. We observe that the accuracy of the numerical solution improves as the expressivity of the network increases. The final L\u221e error and relative L2 error are about 0.7% and 0.2%, respectively, with low deviations. These results indicate that IONet has the potential to achieve high performance in dealing with high-dimensional output solutions of parametric elliptic interface problems, even in scenarios involving high-contrast coefficients."
        },
        {
            "heading": "5. Conclusions",
            "text": "In this work, we have investigated deep neural network-based operator learning methods and proposed the interfaced operator network (IONet) to tackle parametric elliptic interface problems. The main contribution is that we first combine the domain-decomposed method with the operator learning methods and employ multiple branch networks and truck networks to explicitly handle the discontinuities across the interface in the input and output functions. In addition, we introduce customized physics-informed loss designed to constrain the physical consistency of the proposed model. This strategy reduces the requirement for training data and empowers the IONet to remain effective even in the absence of paired input-output training data. We also provide theory and numerical experiments to demonstrate that the proposed IONet is effective and reliable for approximating the solution operator of parametric interface problems. In our simulations, we systematically studied the effects of different factors on the accuracy of INO and existing state-of-the-art operator networks. The results show that the proposed IONet is more robust and accurate in dealing with many kinds of parametric interface problems due to its discontinuity-preserving architecture.\nDespite the preliminary success, there are still many more issues that deserve further investigation. For instance, one limitation is the absence of treating geometry configuration as an input function in our current work. Integrating the geometry configuration into IONet could potentially enhance its capabilities and further broaden its range of applications. Additionally, we have not yet obtained the convergence rate for IONet, which would provide valuable insights into the model\u2019s accuracy and stability. Inspired by the works on the error estimates for DeepONets [57] and generalization performance analysis of deep learning for PDEs [58, 59, 60, 61], including interface problems [62], it is interesting to improve the convergence properties and the error estimation of IONet for solving parametric elliptic interface problems. Finally, IONet can be viewed as a specific DeepONet preserving discontinuity, and we are also interested in exploring the feasibility of integrating recent advancements and extensions from DeepONet (e.g., DeepONet with proper orthogonal decomposition [50], DeepONet based on latent\nrepresentations and autoencoders [63], DeepONet using laplace transform [64] ) into IONet. This exploration aims to enhance the efficiency and precision of IONet, enabling its application to complex, large-scale computational engineering problems."
        },
        {
            "heading": "Acknowledgments",
            "text": "This work was supported by the National Natural Science Foundation of China (Grant Nos. 11771435,\n22073110 and 12171466)."
        },
        {
            "heading": "Appendix A. Proof of Theorem 1",
            "text": "Here we provide the proof of Theorem 1, which relies on the universal approximation theorem of FNNs (see\ne.g., [65]) and the tensor product decomposition of operators [49].\nProof. Denote Ki = {a|\u2126i | a \u2208 K} \u2282 C(\u2126i) where i = 1, 2. Consider an operator G\u0303 mapping from K1 \u00d7 K2 to X(\u2126) as\nG\u0303(a1, a2) = G(a), where a(x) =  a1(x), if x \u2208 \u21261 a2(x), if x \u2208 \u21262 \u2208 K. (A.1)\nObserving the fact that the function values evaluated at given points is equivalent to the piece-wise linear functions of Faber-Schauder basis, and by corollary 2.6 in [49], for any \u03b5 > 0, there exist positive integers m1, m2, p and continuous functions g1, j \u2208 C(Rm1 ), g2, j \u2208 C(Rm2 ) and u j \u2208 X(\u2126) with j = 1, \u00b7 \u00b7 \u00b7 , p and y11, \u00b7 \u00b7 \u00b7 , y1m1 \u2208 \u21261, y21, \u00b7 \u00b7 \u00b7 , y2m2 \u2208 \u21262, such that\nsup a1\u2208K1, a2\u2208K2 \u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225G\u0303(a1, a2)(\u00b7) \u2212 p\u2211\nj=1\ng1, j(a1(y11), \u00b7 \u00b7 \u00b7 , a1(y1m1 ))g2, j(a2(y 2 1), \u00b7 \u00b7 \u00b7 , a2(y2m2 ))u j(\u00b7) \u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225 X(\u2126) \u2264 \u03b5 2 . (A.2)\nSince Ki is a collection of continuous functions, and\u2126 is bounded, we have that for i = 1, 2, Ai := {(ai(y1i ), \u00b7 \u00b7 \u00b7 , ai(y mi i ))|ai \u2208 Ki} \u2208 Rmi is bounded and there exists a cuboid containing Ai. Consequently, by the approximation theorems of tanh FNN [65], for any \u03b4 > 0, there exist tanh FNNs Nbi : Rmi \u2192 Rp, N it : \u2126i \u2192 Rp, such that\u2225\u2225\u2225[Nbi ] j \u2212 gi, j\u2225\u2225\u2225C\u221e(Ai) \u2264 \u03b4, \u2225\u2225\u2225[N it ] j \u2212 u j|\u2126i\u2225\u2225\u2225H2(\u2126i) \u2264 \u03b4, i = 1, 2, j = 1, \u00b7 \u00b7 \u00b7 , p, where [N] j denotes the j-th component of FNN N . Denote\nM j = max{ \u2225\u2225\u2225g1, j\u2225\u2225\u2225C\u221e(A1) , \u2225\u2225\u2225g2, j\u2225\u2225\u2225C\u221e(A2) \u2225\u2225\u2225u j\u2225\u2225\u2225X(\u2126)}, j = 1, \u00b7 \u00b7 \u00b7 , p.\nSubsequently we can choose a sufficiently small \u03b4 such that\n\u2225\u2225\u2225\u2225 p\u2211 j=1 [Nb1 ] j(a1(y11), \u00b7 \u00b7 \u00b7 , a1(y1m1 ))[Nb2 ] j(a2(y 2 1), \u00b7 \u00b7 \u00b7 , a2(y2m2 ))[N i t ] j(\u00b7)\n\u2212 p\u2211\nj=1\ng1, j(a1(y11), \u00b7 \u00b7 \u00b7 , a1(y1m1 ))g2, j(a2(y 2 1), \u00b7 \u00b7 \u00b7 , a2(y2m2 ))u j(\u00b7) \u2225\u2225\u2225\u2225 H2(\u2126i)\n\u2264 p\u2211\nj=1\n(3M2j \u03b4 + 3M j\u03b4 2 + \u03b43) \u2264 \u03b5\n2 .\n(A.3)\nFinally, combining definition (A.1), estimates (A.2) and (A.3), we conclude that for i = 1, 2,\nsup a\u2208K \u2225\u2225\u2225\u2225G(a)(\u00b7) \u2212 S (Nb1 (a(y11), \u00b7 \u00b7 \u00b7 , a(y1m1 )) \u2299 Nb2 (a(y21), \u00b7 \u00b7 \u00b7 , a(y2m2 )) \u2299 N it (\u00b7))\u2225\u2225\u2225\u2225H2(\u2126i) \u2264 sup\na\u2208K \u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225G\u0303(a|\u21261 , a|\u21262 )(\u00b7) \u2212 p\u2211\nj=1\ng1, j(a(y11), \u00b7 \u00b7 \u00b7 , a(y1m1 ))g2, j(a(y 2 1), \u00b7 \u00b7 \u00b7 , a(y2m2 ))u j(\u00b7) \u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225 H2(\u2126i)\n+ sup a\u2208K \u2225\u2225\u2225\u2225\u2225\u2225 p\u2211 j=1 [Nb1 ] j(a(y11), \u00b7 \u00b7 \u00b7 , a(y1m1 ))[Nb2 ] j(a(y 2 1), \u00b7 \u00b7 \u00b7 , a(y2m2 ))[N i t ] j(\u00b7)\n\u2212 p\u2211\nj=1\ng1, j(a(y11), \u00b7 \u00b7 \u00b7 , a(y1m1 ))g2, j(a(y 2 1), \u00b7 \u00b7 \u00b7 , a(y2m2 ))u j(\u00b7) \u2225\u2225\u2225\u2225\u2225\u2225 H2(\u2126i)\n\u2264\u03b5 2 + \u03b5 2 = \u03b5,\nwhich completes the proof."
        }
    ],
    "title": "Solving parametric elliptic interface problems via interfaced operator network",
    "year": 2023
}