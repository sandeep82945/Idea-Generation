{
    "abstractText": "The Gaia Astrometric Verification Unit\u2013Global Sphere Reconstruction (AVU\u2013GSR) Parallel Solver aims to find the astrometric parameters for \u223c10 stars in the Milky Way, the attitude and the instrumental specifications of the Gaia satellite, and the global parameter \u03b3 of the post Newtonian formalism. The code iteratively solves a system of linear equations, A\u00d7 x = b, where the coefficient matrix A is large (\u223c10 \u00d7 10 elements) and sparse. To solve this system of equations, the code exploits a hybrid implementation of the iterative PC-LSQR algorithm, where the computation related to different horizontal portions of the coefficient matrix is assigned to separate MPI processes. In the original code, each matrix portion is further parallelized over the OpenMP threads. To further improve the code performance, we ported the application to the GPU, replacing the OpenMP parallelization language with OpenACC. In this port, \u223c95% of the data is copied from the host to the device at the beginning of the entire cycle of iterations, making the code compute bound rather than data-transfer bound. The OpenACC code presents a speedup of \u223c1.5 over the OpenMP version but further optimizations are in progress to obtain higher gains. The code runs on multiple GPUs and it was tested on the CINECA supercomputer Marconi100, in anticipation of a port to the pre-exascale system Leonardo, that will be installed at CINECA in 2022.",
    "authors": [
        {
            "affiliations": [],
            "name": "Valentina Cesarea"
        },
        {
            "affiliations": [],
            "name": "Ugo Becciani"
        },
        {
            "affiliations": [],
            "name": "Alberto Vecchiato"
        },
        {
            "affiliations": [],
            "name": "Mario Gilberto Lattanzi"
        },
        {
            "affiliations": [],
            "name": "Fabio Pitari"
        },
        {
            "affiliations": [],
            "name": "Mario Raciti"
        },
        {
            "affiliations": [],
            "name": "Giuseppe Tudisco"
        },
        {
            "affiliations": [],
            "name": "Marco Aldinucci"
        },
        {
            "affiliations": [],
            "name": "Beatrice Bucciarelli"
        }
    ],
    "id": "SP:b6ebb72780369c63a2b9c0577590504037ca33bc",
    "references": [
        {
            "authors": [
                "M Aldinucci"
            ],
            "title": "Practical parallelization of scientific",
            "year": 2021
        },
        {
            "authors": [
                "S. Balay",
                "S. Abhyankar",
                "M.F. Adams",
                "J. Brown",
                "P. Brune",
                "K. Buschelman",
                "L. Dalcin",
                "A. Dener",
                "V. Eijkhout",
                "W.D. Gropp",
                "D. Kaushik",
                "M.G. Knepley",
                "D.A. May",
                "L.C. McInnes",
                "R.T. Mills",
                "T. Munson",
                "K. Rupp",
                "P. Sanan",
                "B.F. Smith",
                "S. Zampini",
                "H. Zhang"
            ],
            "title": "PETSc Users Manual",
            "year": 2021
        },
        {
            "authors": [
                "S. Balay",
                "S. Abhyankar",
                "M.F. Adams",
                "J. Brown",
                "P. Brune",
                "K. Buschelman",
                "L. Dalcin",
                "A. Dener",
                "V. Eijkhout",
                "W.D. Gropp",
                "D. Kaushik",
                "M.G. Knepley",
                "D.A. May",
                "L.C. McInnes",
                "R.T. Mills",
                "T. Munson",
                "K. Rupp",
                "P. Sanan",
                "B.F. Smith",
                "S. Zampini",
                "H. Zhang"
            ],
            "title": "PETSc Web page",
            "year": 2021
        },
        {
            "authors": [
                "S. Balay",
                "W.D. Gropp",
                "L.C. McInnes",
                "B.F. Smith"
            ],
            "title": "Efficient management of parallelism in object oriented numerical software libraries",
            "venue": "Modern Software Tools in Scientific Computing,",
            "year": 1997
        },
        {
            "authors": [
                "O. Baur",
                "G. Austen"
            ],
            "title": "A parallel iterative algorithm for largescale problems of type potential field recovery from satellite data, in: Proceedings Joint CHAMP/GRACE Science Meeting, GeoForschungsZentrum Potsdam",
            "year": 2005
        },
        {
            "authors": [
                "U. Becciani",
                "E. Sciacca",
                "M. Bandieramonte",
                "A. Vecchiato",
                "B. Bucciarelli",
                "M.G. Lattanzi"
            ],
            "title": "Solving a very large-scale sparse linear system with a parallel algorithm in the gaia mission",
            "venue": "in: 2014 International Conference on High Performance Computing Simulation (HPCS),",
            "year": 2014
        },
        {
            "authors": [
                "S. Bertone",
                "A. Vecchiato",
                "B. Bucciarelli",
                "M. Crosta",
                "M.G. Lattanzi",
                "L. Bianchi",
                "M.C. Angonin",
                "C. Le Poncin-Lafitte"
            ],
            "title": "Application of time transfer functions to Gaia\u2019s global astrometry. Validation on DPAC simulated Gaia-like observations",
            "venue": "A&A 608,",
            "year": 2017
        },
        {
            "authors": [
                "G. Bin",
                "S. Wu",
                "M. Shao",
                "Z. Zhou"
            ],
            "title": "Irn-mlsqr: An improved iterative reweight norm approach to the inverse problem of electrocardiography incorporating factorization-free preconditioned lsqr",
            "venue": "J. Electrocardiol",
            "year": 2020
        },
        {
            "authors": [
                "L. Borriello",
                "F. Dalessandro",
                "F. Murgolo",
                "G. Prezioso"
            ],
            "title": "Hipparcos-the reduction chain of observations and double star recognition using an image processing approach",
            "venue": "Mem. Soc. Astron. Ital",
            "year": 1986
        },
        {
            "authors": [
                "A.G.A. Brown",
                "A. Vallenari",
                "T. Prusti",
                "J.H.J. de Bruijne",
                "C. Babusiaux",
                "M. Biermann",
                "O.L. Creevey",
                "D.W. Evans",
                "L Eyer"
            ],
            "title": "Gaia early data release",
            "venue": "A&A 650,",
            "year": 2021
        },
        {
            "authors": [
                "M. Crosta",
                "A. Geralico",
                "M.G. Lattanzi",
                "A. Vecchiato"
            ],
            "title": "General relativistic observable for gravitational astrometry in the context of the Gaia mission and beyond",
            "venue": "Phys. Rev. D",
            "year": 2017
        },
        {
            "authors": [
                "L. Flores",
                "V. Vidal",
                "G. Verd\u00fa"
            ],
            "title": "Gpu based algorithms in ct imaging",
            "venue": "AMGP",
            "year": 2016
        },
        {
            "authors": [
                "H. Guo",
                "H. Zhao",
                "J. Yu",
                "X. He",
                "X. Song"
            ],
            "title": "Xray luminescence computed tomography using a hybrid proton propagation model and lasso-lsqr algorithm",
            "venue": "J. Biophotonics ,",
            "year": 2021
        },
        {
            "authors": [
                "J.L. Gustafson"
            ],
            "title": "Reevaluating amdahl\u2019s law",
            "venue": "Commun. ACM",
            "year": 1988
        },
        {
            "authors": [
                "A. Hees",
                "C. Le Poncin-Lafitte",
                "D. Hestroffer",
                "P. David"
            ],
            "title": "Local tests of gravitation with Gaia observations of Solar System Objects, in: Recio-Blanco",
            "venue": "Astrometry and Astrophysics in the Gaia Sky,",
            "year": 2018
        },
        {
            "authors": [
                "H. Huang",
                "J.M. Dennis",
                "L. Wang",
                "P. Chen"
            ],
            "title": "A scalable parallel lsqr algorithm for solving large-scale linear system for tomographic problems: a case study in seismic tomography",
            "venue": "Procedia Comput. Sci",
            "year": 2013
        },
        {
            "authors": [
                "H. Huang",
                "L. Wang",
                "E.J. Lee",
                "P. Chen"
            ],
            "title": "An mpi-cuda implementation and optimization for parallel sparse equations and least squares (lsqr)",
            "venue": "Procedia Comput. Sci",
            "year": 2012
        },
        {
            "authors": [
                "N.R. Jaffri",
                "L. Shi",
                "U. Abrar",
                "A. Ahmad",
                "J. Yang"
            ],
            "title": "Electrical resistance tomographic image enhancement using mrnsd and lsqr",
            "venue": "in: Proceedings of the 2020 5th International Conference on Multimedia Systems and Signal Processing,",
            "year": 2020
        },
        {
            "authors": [
                "F. Joulidehsar",
                "A. Moradzadeh",
                "F.D. Ardejani"
            ],
            "title": "An improved 3d joint inversion method of potential field data using cross-gradient constraint and lsqr method",
            "venue": "Pure Appl. Geophys",
            "year": 2018
        },
        {
            "authors": [
                "D.M. Krolikowski",
                "A.L. Kraus",
                "A.C. Rizzuto"
            ],
            "title": "Gaia EDR3 Reveals the Substructure and Complicated Star Formation History of the Greater Taurus-Auriga Star-forming Complex",
            "venue": "AJ 162,",
            "year": 2021
        },
        {
            "authors": [
                "S.X. Liang",
                "Y.J. Jiao",
                "W.X. Fan",
                "B.Z. Yang"
            ],
            "title": "2019a. 3d inversion of magnetic data based on lsqr method and correlation coefficient self constrained",
            "venue": "Progress in Geophysics",
            "year": 2019
        },
        {
            "authors": [
                "S.X. Liang",
                "Q. Wang",
                "Y.J. Jiao",
                "G.Z. Liao",
                "G. Jing"
            ],
            "title": "Lsqr - analysis and evaluation of the potential field inversion using lsqr method",
            "venue": "Geophysical and Geochemical Exploration",
            "year": 2019
        },
        {
            "authors": [
                "S.T. Ling",
                "Z.G. Jia",
                "X. Lu",
                "B. Yang"
            ],
            "title": "Matrix lsqr algorithm for structured solutions to quaternionic least squares problem",
            "venue": "Comput. Math. Appl",
            "year": 2019
        },
        {
            "authors": [
                "J.S. Liu",
                "F.T. Liu",
                "J. Liu",
                "T.Y. Hao"
            ],
            "title": "Parallel lsqr algorithms used in seismic tomography",
            "venue": "Chin. J. Geophys",
            "year": 2006
        },
        {
            "authors": [
                "S. Naghibzadeh",
                "A.J. van der Veen"
            ],
            "title": "Radioastronomical least squares image reconstruction with iteration regularized krylov subspaces and beamforming-based prior conditioning",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2017
        },
        {
            "authors": [
                "C.C. Paige",
                "M.A. Saunders"
            ],
            "title": "Lsqr: An algorithm for sparse linear equations and sparse least squares",
            "venue": "ACM Trans. Math. Softw. (TOMS)",
            "year": 1982
        },
        {
            "authors": [
                "C.C. Paige",
                "M.A. Saunders"
            ],
            "title": "Algorithm 583: Lsqr: Sparse linear equations and least squares problems",
            "venue": "ACM Trans. Math. Softw. (TOMS)",
            "year": 1982
        },
        {
            "authors": [
                "H. Penghui",
                "L. Houbiao"
            ],
            "title": "A note on the least squares qr (lsqr",
            "venue": "algorithm. Math. Numer. Sin",
            "year": 2020
        },
        {
            "authors": [
                "L. Reichel",
                "Q. Ye"
            ],
            "title": "A generalized lsqr algorithm",
            "venue": "Numer. Linear Algebra Appl",
            "year": 2008
        },
        {
            "authors": [
                "H. Van der Marel"
            ],
            "title": "On the \u201cgreat circle reduction\u201d in the data analysis for the astrometric satellite HIPPARCOS",
            "year": 1988
        },
        {
            "authors": [
                "A. Vecchiato",
                "B. Bucciarelli",
                "M.G. Lattanzi",
                "U. Becciani",
                "L. Bianchi",
                "U. Abbas",
                "E. Sciacca",
                "R. Messineo",
                "R. De March"
            ],
            "title": "The global sphere reconstruction (GSR). Demonstrating an independent implementation of the astrometric core solution for Gaia",
            "venue": "A&A 620,",
            "year": 2018
        },
        {
            "authors": [
                "A. Vecchiato",
                "M.G. Lattanzi",
                "B. Bucciarelli",
                "M. Crosta",
                "F. de Felice",
                "M. Gai"
            ],
            "title": "Testing general relativity by micro-arcsecond global astrometry",
            "venue": "A&A 399,",
            "year": 2003
        },
        {
            "authors": [
                "A. Yoo",
                "A.H. Baker",
                "R. Pearce",
                "V.E. Henson"
            ],
            "title": "A scalable eigensolver for large scale-free graphs using 2d graph partitioning",
            "venue": "in: Proceedings of 2011 International Conference for High Performance Computing,",
            "year": 2011
        }
    ],
    "sections": [
        {
            "text": "The Gaia Astrometric Verification Unit\u2013Global Sphere Reconstruction (AVU\u2013GSR) Parallel Solver aims to find the astrometric parameters for \u223c108 stars in the Milky Way, the attitude and the instrumental specifications of the Gaia satellite, and the global parameter \u03b3 of the post Newtonian formalism. The code iteratively solves a system of linear equations, A\u00d7 x = b, where the coefficient matrix A is large (\u223c1011 \u00d7 108 elements) and sparse. To solve this system of equations, the code exploits a hybrid implementation of the iterative PC-LSQR algorithm, where the computation related to different horizontal portions of the coefficient matrix is assigned to separate MPI processes. In the original code, each matrix portion is further parallelized over the OpenMP threads. To further improve the code performance, we ported the application to the GPU, replacing the OpenMP parallelization language with OpenACC. In this port, \u223c95% of the data is copied from the host to the device at the beginning of the entire cycle of iterations, making the code compute bound rather than data-transfer bound. The OpenACC code presents a speedup of \u223c1.5 over the OpenMP version but further optimizations are in progress to obtain higher gains. The code runs on multiple GPUs and it was tested on the CINECA supercomputer Marconi100, in anticipation of a port to the pre-exascale system Leonardo, that will be installed at CINECA in 2022.\nKeywords: Massively parallel algorithms, astrometry, methods: numerical, Galaxy: stellar content"
        },
        {
            "heading": "1. Introduction",
            "text": "The ESA\u2019s Gaia mission1 has provided, in the eight years since its launch on 19th December 2013, a map both in the position and in the velocity dimensions of \u223c1 billion of stars in the Milky Way, about 1% of its total content, with micro-arcsecond accuracy. Two of the main objectives of the mission are the investigation of the formation and the evolution of our galaxy (e.g. Krolikowski et al. 2021) and the test of Einstein\u2019s theory of General Relativity (GR) (e.g. Hees et al. 2018). Indeed, thanks to the accuracy of the measurements, Gaia can detect the bending of the light around massive objects and the consistency of this effect with the predictions of GR can be verified.\nIn the Gaia\u2019s early third data release (EDR3) (Brown et al., 2021) a catalogue of parallaxes, sky positions, and\n\u2217Corresponding author Email addresses: valentina.cesare@inaf.it (Valentina\nCesare), ugo.becciani@inaf.it (Ugo Becciani), alberto.vecchiato@inaf.it (Alberto Vecchiato), mario.lattanzi@inaf.it (Mario Gilberto Lattanzi), f.pitari@cineca.it (Fabio Pitari), mario.raciti@inaf.it (Mario Raciti), giuseppe.tudisco@inaf.it (Giuseppe Tudisco), marco.aldinucci@unito.it (Marco Aldinucci), beatrice.bucciarelli@inaf.it (Beatrice Bucciarelli)\n1https://sci.esa.int/web/gaia\nproper motions of \u223c1.468\u00d7109 stars was published and the complete Gaia\u2019s third data release (DR3) was published in June 2022. The application that we present in this work, called Gaia Astrometric Verification Unit\u2013Global Sphere Reconstruction (AVU\u2013GSR) Parallel Solver, is developed under the Data Processing and Analysis Consortium (DPAC), and it aims to find these parameters for the so-called primary stars of the global astrometric sphere of the Gaia mission, comprising \u223c108 stars. Moreover, it will constrain the attitude and the instrumental settings of the Gaia satellite and the parameter \u03b3 of the Parametrized Post-Newtonian (PPN) formalism of relativistic gravity theories to describe the space-time and to test GR against alternative theories of gravity (Vecchiato et al., 2003).\nThe astrometric model of the observations produces a set of non-linear equations, one for each observation of the Gaia satellite, as a function of the unknowns. To render this system of equations computationally solvable, the observation equations are linearized around an appropriate starting point. The resulting linearized system can be written in matrix form as:\nA\u00d7 x = b, (1)\nwhere A is the coefficient matrix, x the vector of the un-\nPreprint submitted to Journal of LATEX Templates December 23, 2022\nar X\niv :2\n21 2.\n11 67\n5v 1\n[ as\ntr o-\nph .I\nM ]\n2 2\nD ec\n2 02\nknowns, and b the vector of the known terms. The system matrix A is large and sparse (\u223c1011\u00d7108 elements), where the number of rows is the number of observations of the stellar parameters and the number of columns is the number of unknowns to solve. During computation only the non-zero elements of A are considered, and the new matrix contains \u223c1011 \u00d7 101 elements. A single star is observed \u223c103 times, on average. Since the number of equations is much larger than the number of unknowns, the system has to be solved in the least-squares sense, adopting a modified version of the iterative LSQR algorithm (Paige and Saunders, 1982a,b), where the iterations stop when either a given convergence criterion or a maximum number of iterations is achieved.\nThe latest version of the code is written in C/C++ and is hybridly parallelized with MPI + OpenMP (Becciani et al., 2014). Each MPI process deals with the computation of a horizontal portion of the coefficient matrix, a subset of the total number of observations, and the calculation in each MPI process is further parallelized over the OpenMP threads.\nHere we present a preliminary port of this application to GPUs, where we replace the OpenMP part with the parallelization language OpenACC, finalized to a more optimized port with CUDA to pre-exascale systems. The OpenACC code runs on multiple GPUs and currently presents a moderate gain in performance over the OpenMP version, which might be improved with the future optimizations both with OpenACC and CUDA. We performed the performance tests on the MPI + OpenMP and MPI + OpenACC versions of the application on the CINECA supercomputer Marconi100 (M100).\nThe contents of the paper are as follows: Section 2 briefly describes the usage of the LSQR algorithm in the literature; Section 3 presents the structure of the coefficient matrix of the system of equations; Section 4 details the present version of the code parallelized with MPI + OpenMP, in production on M100; Section 5 describes the platform on which the code that is in production runs and presents the performance of a typical execution of the inproduction code; Section 6 presents the current port to the GPU with MPI + OpenACC; Section 7 describes the tests, performed on M100, that compare the performance of the MPI + OpenMP and MPI + OpenACC applications; Section 8 concludes the paper, also introducing the future work with CUDA aimed to further accelerate the code."
        },
        {
            "heading": "2. Related work",
            "text": "The main algorithm of this application is based on the LSQR, an iterative Krylov subspace algorithm conceived to solve large scale ill-posed problems while maintaining numerical stability (Paige and Saunders, 1982a,b). Typically, the LSQR algorithm is employed to solve, in the least-squares sense, a system of equations with a large\nand sparse coefficient matrix that has not a unique solution (e.g. Paige and Saunders 1982a,b; Reichel and Ye 2008; Jaffri et al. 2020; Penghui and Houbiao 2020). To mention some examples, this algorithm is exploited in the following contexts: (I) geophysics, to locate underground gravitational and magnetic anomalies (Joulidehsar et al., 2018; Liang et al., 2019a,b); (II) medicine, in electrocardiography (Bin et al., 2020) and X-ray tomography (Guo et al., 2021); (III) industry, in electrical resistance tomography (Jaffri et al., 2020); (IV) astronomy, for radioastronomical image reconstruction (Naghibzadeh and van der Veen, 2017), and in the application presented in this paper.\nThe LSQR algorithm was also adopted for the data reduction of the High Precision Parallax Collecting Satellite (Hipparcos) Space Astrometry Mission, the first astrometric space mission defined within the ESA scientific program and precursor of the Gaia mission (Borriello et al., 1986; Van der Marel, 1988; Becciani et al., 2014). In several research fields, this algorithm is used to solve an inverse problem, one of the most important issues in mathematical sciences, that consists of estimating the parameters of a model from a set of observational data (e.g. Liang et al. 2019b; Bin et al. 2020; Guo et al. 2021). This is also the target of the Gaia AVU\u2013GSR application.\nThe LSQR algorithm usually adopts a preconditioning technique (e.g. Ling et al. 2019; Bin et al. 2020) to improve its convergence speed, that could consist of properly normalizing the coefficient matrix of the system before starting the iterations and multiplying this normalization factor with the solution obtained only at the end of the computation. This allows the LSQR algorithm to find an equally accurate solution in \u223c60-70% of the time compared to other standard algorithms such as the conventional iterative reweight norm method (Bin et al., 2020). In the Gaia AVU-GSR code, we preconditioned the system of equations by dividing the parameters of each column of the coefficients matrix by the norm of the column itself. We stored these normalization factors in an array p with a number of elements equal to the number of columns of the coefficient matrix and we multiplied this array with the solution and standard error arrays at the end of all the iterations. The preconditioned LSQR algorithm can be abbreviated as PC-LSQR.\nTypically, the systems of equations that exploit the LSQR algorithm require parallelism to be solved in reasonable timescales and to overcome possible problems due to memory limits, given the large size of their coefficient matrices (Huang et al., 2012). There are several existing implementations of the parallel LSQR algorithm. For example, the implementation of Baur and Austen (2005) exploits repeated vector-vector operations and was applied to the data taken by the CHAMP, GRACE, and GOCE satellites, and the implementations of Liu et al. (2006) and Huang et al. (2013) were employed in seismic tomography. Another example is provided by the Portable, Extensible Toolkit for Scientific Computation (PETSc) libraries (Balay et al., 1997, 2021a,b), optimized scientific li-\nbraries that support MPI and GPU (CUDA and OpenCL) parallelism and even hybrid MPI + GPU parallelization paradigms.\nThe main issue for these libraries is that they are not optimized for sparse matrices with particular patterns of non-zero elements (Yoo et al., 2011) and, thus, they are not suitable to solve systems of equations, such as the one of the Gaia AVU\u2013GSR application, that are based on a very peculiar sparsity scheme, as described in Section 3. When dealing with such large systems, optimization is essential: assuming a system that converges in 141000 iterations, with 4.23 s per iteration, representative of a typical execution of the Gaia AVU-GSR code (see Section 5), saving 1 s per iteration means saving 2350 hours, that is the \u223c24% of the initial execution time. These percentages might vary for systems with different sizes. For this reason, we did not adopt an existing implementation of the LSQR algorithm but defined a customized implementation, which exploits a preconditioning technique and an ad-hoc algorithm to compress the sparse coefficient matrix of the system of equations.\nWe ported our application to a GPU environment, using the OpenACC parallel programming model. Some implementations of GPU-ported LSQR algorithm were created for solving problems in specific fields, e.g. seismic (Huang et al., 2012) or medical (Flores et al., 2016) tomography. In particular, Huang et al. (2012) implemented a MPI\u2013CUDA version of the LSQR algorithm with excellent performance: 17.6x speedup over the serial CPU algorithm and 3.8x speedup over the MPI\u2013CPU algorithm, besides obtaining a better performance than the PETSc implementation and very good strong and weak scaling behaviours2."
        },
        {
            "heading": "3. Coefficient matrix structure",
            "text": "As mentioned in Section 1, this application solves a system of linear equations given by Eq. (1), where the coefficient matrix A contains \u223c1011\u00d7108 elements. To solve the system, we may choose between two fully-relativistic astrometric models (Bertone et al., 2017; Crosta et al., 2017), whose unknowns are (Becciani et al., 2014):\n1. the astrometric unknowns, i.e. the parallax \u03c9, the right ascension \u03b1, the declination \u03b4, and the two components of the proper motion along these two directions, (\u00b5\u03b1, \u00b5\u03b4), of every star; 2. the attitude unknowns, given by a proper B-spline representation of the Rodrigues parameters that describe the satellite attitude during the entire duration of the mission;\n2The strong scaling is the capability of a parallel software to compute a fixed-size problem faster with more computing resources (Amdahl, 1967). Instead, the weak scaling is the capability of a parallel software to maintain constant the computation time of a problem with a size proportional to the amount of computing resources (Gustafson, 1988).\n3. the instrumental unknowns;\n4. the global unknown \u03b3 of the PPN formalism;\nwith a total of \u223c108 unknowns. To solve the system of equations, we adopted a hybrid implementation of PC-LSQR, an iterative conjugategradient type algorithm that can solve an overdetermined system of linear equations in the least-squares sense. The most computationally demanding part of this algorithm, and of the entire application, consists of the call, at each iteration i, of the aprod function in one of two possible modes. In the mode 1, aprod computes the product A \u00d7 xi, where xi is the i-th approximation of the solution of the system. In the mode 2, aprod computes the product AT \u00d7yi, where yi is proportional to the iterative estimation of the vector of the residuals, y\u2032 i = b\u2212A\u00d7xi, and b is the vector of the known terms. The vector of the residuals is the quantity that has to be minimized in the least-squares sense up to the convergence of the algorithm. In fact, the convergence is considered achieved when y\u2032 i goes below a predefined tolerance tol, set to the machine precision (10\u221216 on M100). To find a unique solution, an additional number of constraint equations have to be set.\nFigure 1 shows the flowchart of the entire code. First of all, the quantities employed in the calculations, such as the coefficient matrix A and the known terms array b, are imported from external binary files, converted in binary format from original FITS files with an external program. Then, we calculate the preconditioning array p and we normalize each column of A by the elements of this vector (Section 2). From the normalized coefficient matrix and the known terms array, we calculate, in each MPI process, the initial solution of the system of equations, with the aprod 2 function. The initial solution is then reduced among the different MPI processes. Then, the LSQR algorithm starts. The LSQR algorithm consists of a while loop that terminates when either the convergence condition or, in case convergence cannot be achieved, a maximum number of iterations are reached. At each iteration i of the LSQR algorithm, the aprod 1 and 2 functions are called to provide the iterative estimates of the known terms b and of the unknowns x, respectively, locally to each MPI process. The b and the x arrays are then reduced among the MPI processes. At the end of each iteration, we also compute the standard errors on the unknowns (variances) and the possible correlations between the unknowns (covariances). After the end of the LSQR cycle, we re-multiply the solution and the error on the solution by the preconditioning vector p. The algorithm concludes with the print of the solution, its standard error, and the covariances to binary files, that are converted to FITS format with an external program.\nEach observation equation (each row of the coefficient matrix A) contains the astrometric, the attitude, the instrumental, and the global parameters (vertical stripes on the coefficient matrix in Figure 2). The astrometric coefficients represent the \u223c90% of A and they are equal to\nNAstro \u00d7 NStars \u00d7 NObs, where NAstro = 5 is the number of astrometric parameters per star, NStars \u2208 [106, 108] is the number of stars, and NObs \u223c 1011 is the number of observations. The non-zero astrometric coefficients of A are organized in a block-diagonal structure (left part of the coefficient matrix in Figure 2), where the rows of each block contain the astrometric parameters observed for a certain star. Specifically, the first block is related to star number 0, the second block to star number 1, and so forth.\nAfter the astrometric section, each row of the attitude part of the matrix A contains NAtt = 12 parameters different from zero, organized in NAxes = 3 blocks of NParAxis = 4 elements separated by NDFA elements equal to zero. NAxes represents the number of axes of the satellite attitude, in this case equal to 3, whereas NDFA is the number of degrees of freedom carried by each of the three axes. After the attitude section, each row of the instrumental part of the matrix A contains NInstr = 6 elements different from zero, distributed without any predefined order, and the last columns of A contain the coefficients of the global unknowns. In our case, we only considered NGlob = 1 global parameter, which is the parameter \u03b3 of the PPN formalism. The left part of Figure 2 summarizes the structure of the system matrix A.\nThe coefficient matrix A is large and sparse, since the ratio between the number of elements equal to zero and different from zero is much larger than one. With a large matrix of \u223c1011 \u00d7 108 elements, solving the system could not be performed on any existing infrastructure, even with an efficient parallelization scheme. For this reason, the matrix is compressed through an ad-hoc algorithm (Becciani et al., 2014) and only the non-zero elements of A are considered during computation. In this way, the matrix size reduces from \u223c1011 \u00d7 108 to \u223c1011 \u00d7 101. The new matrix, Ad, where \u201cd\u201d stands for \u201cdense\u201d, is stored in a 1D double-precision array that contains, for each observation, Npar = 24 non-zero parameters (NAstro = 5 astrometric coefficients, NAtt = 12 attitude coefficients, NInstr = 6 instrumental coefficients, and NGlob = 1 global coefficient).\nTo solve the system of equations with Ad, we defined a map between the indexes of the elements in Ad and in the original matrix A. The 1D single-precision array M i contains the number of the star and the index of the first attitude element in the matrix A, for each observation, i.e. for each row of A. Given the regular structures of the astrometric and of the attitude sections in the matrix A, described above, these two indexes are sufficient to retrieve the original positions of all the astrometric and the attitude coefficients in A. Instead, the 1D single-precision array Ic contains the indexes pointing to the positions that all the instrumental elements of Ad had in A, since the instrumental part does not follow a regular pattern. Instead, the unknowns and the known terms are stored in two 1D double-precision arrays, x and b, respectively."
        },
        {
            "heading": "4. Parallel code structure: MPI + OpenMP",
            "text": "The application is parallelized with MPI + OpenMP. Figure 2 summarizes the parallelization scheme of the system of equations (Eq. 1) on four MPI processes, represented with four different colours, in a single node of a computer cluster.\nThe computation associated with different horizontal portions of the matrix Ad, i.e. with different subsets of the total number of observations, is assigned to separate\nMPI processes. The number of observations handled by each MPI process is stored in a 1D single-precision array, N [nproc], where nproc is the total number of MPI processes on which the code runs.\nTo optimize the code, the data are distributed among the MPI processes as evenly as possible. To perform this task, a small portion of the data, i.e. the attitude, the instrumental, and the global portions of the unknown vector x, is replicated on each MPI process. The \u201cAtt+Instr+Glob \u2013 Replica portion\u201d part in Figure 2 represents this replicated portion of the solution array. The majority of the data, i.e. the astrometric section of x, is distributed, as we can see from the four \u201cAstrometric\u201d blocks in Figure 2, colour-coded as the four MPI processes. This scheme reduces the number of communications between the different MPI processes. At every iteration of the LSQR algorithm, a reduce operation is performed to combine the results, i.e. the replicated parts of the i-th iterative estimates of both the known terms and the solution arrays, bi and xi, from the different MPI processes (Figures 1 and 2).\nThe reason for distributing the astrometric part and replicating the attitude, instrumental, and global parts on the MPI processes instead of distributing the entire system is due to the different arrangements of the coefficients in the four parts of the system. The coefficients of the\nastrometric section are regularly distributed in the system matrix following a block-diagonal structure, which makes it easy to assign their computation to different MPI processes. Instead, the other three sections do not follow a specific regular pattern, which makes more difficult to rearrange the code to distribute their computation among the MPI processes. Replicating the computation of the attitude + instrumental + global parts on the MPI processes simplifies the writing of the code while implying only a minor loss in performance, since these sections represent only the \u223c10% of the total.\nThe observations treated by each MPI process are further parallelized over the OpenMP threads, where the index of the thread, tid, goes from 0 to nth\u22121, where nth is the total number of threads set at runtime (see lines omp.3 and omp.4 of Algorithm 2). In the aprod 1, the parallelization with OpenMP is managed with the #pragma omp for directive, placed on the for loop at line omp.4 of Algorithm 1. In the aprod 2, we defined a single-precision 1D array, N t, that stores the number of observations dealt by each OpenMP thread tid, similarly to the array N for each MPI process of rank pid. The for loop that manages the computation in each OpenMP thread of ID tid iterates from N t[tid][0] to N t[tid][1], where N t[tid][0] and N t[tid][1] are the first and the last observation in the tid-\nth thread (see line omp.5 of Algorithm 2)."
        },
        {
            "heading": "5. The Marconi100 cluster and the production of the AVU\u2013GSR code",
            "text": "The application has been in production since 2014 under an agreement between Istituto Nazionale di Astrofisica (INAF) and CINECA, with the support of the Italian Space Agency (ASI). The code has run on all the Tier0 systems of CINECA and it is now employed by the Coordination Unit 3 (CU3) of DPAC for the AVU-GSR tasks. The entire process of AVU-GSR is managed by the Data Processing Center of Turin (DPCT) that is supervised by the Aerospace Logistics Technology Engineering Company (ALTEC) in collaboration with the Astrophysics Observatory of Turin (INAF-OATO).\nThe application is currently running on the CINECA supercomputer M100 which has 980 compute nodes with the following features:\n1. 2 sockets of 16 physical cores each, of the type IBM POWER9 AC922, with a processor speed of 3.1 GHz. Each physical core corresponds to 4 virtual cores, with a total of 128 (2 \u00d7 16 \u00d7 4) virtual cores per node;\n2. 4 GPUs of the type NVIDIA Volta V100, with a memory of 16 GB each, connected with Nvlink 2.0;\n3. 256 GB of RAM.\nSo far, we have worked with a subset of the total number of observations. One of the most recent runs, which is representative of a typical execution, dealt with \u223c8.4\u00d7106 stars observed multiple times, with a total number of observations equal to \u223c1.8\u00d7109. The execution converged after \u223c141000 iterations, with a time per iteration equal to \u223c4.23 s. This results in a total elapsed time of te ' 166 hours. The run was executed on 2 nodes of M100, on a total of nproc = 32 MPI processes (16 per node). For each MPI process, the code ran on nth = 2 OpenMP threads. The run occupied a memory of \u223c10.6 GB per MPI process, correspondent to a total memory of \u223c340 GB. With these features, the total computational time corresponds to tc = te \u00d7 nproc \u00d7 nth = 10624 hours. The astrometric unknowns were retrieved with a micro-arcsecond resolution (Vecchiato et al., 2018)."
        },
        {
            "heading": "6. From the CPU to the GPU: OpenACC",
            "text": "To further improve the performance of the application, we ported it to the GPU by replacing OpenMP with OpenACC (see Aldinucci et al., 2021, for the description of a semi-automatic methodology to parallelize scientific applications with the OpenACC parallelization model)."
        },
        {
            "heading": "6.1. Multi\u2013GPU computation",
            "text": "The code runs on multiple GPUs, depending on the number of MPI processes. The MPI processes in each node are assigned to the GPUs of the node in a round-robin fashion, according to the command at line 3 of Algorithm 3.\nFigure 3 represents the parallelization scheme of the coefficient matrix on four nodes of a computer cluster with four MPI processes and four GPUs per node (e.g. M100). The figure shows that the computation related to each MPI process is assigned to a different GPU. Running on a number of MPI processes per node equal to the number of GPUs per node, like in this example, represents the optimal configuration, as better explained in Section 7.1."
        },
        {
            "heading": "6.2. Data transfers",
            "text": "The GPU has\u223c102 times more cores than the CPU and it is ideal to parallelize the computation of a large amount of data. When dealing with GPU programming, it is important to proper manage the data transfers from the host (CPU) to the device (GPU) (H2D) and vice versa (D2H). Indeed, if a code parallelized on the GPU becomes datatransfer bound, namely the data transfers dominate the computation, its performance could worsen compared to the same code parallelized on the CPU. Managing the data copies is particularly important in iterative constructs, where it is essential to move as much data as possible at the beginning and/or at the end of the entire iteration cycle, and to reduce the copies at every step of the algorithm.\nThe data movements between the CPU and the GPU are regulated by particular directives placed in strategical points of the code. Specifically, we employed the following directives (see lines 4, 10, 12, 14, and 16 of Algorithm 3):\n1. #pragma acc enter data copyin(), that copies H2D the arrays listed within the round brackets;\n2. #pragma acc exit data copyout(), that copies D2H the arrays listed within the round brackets.\nIn our port, we transfer \u223c95% of the data H2D at the beginning of the LSQR iterative cycle. This is possible since these quantities are not modified during the computation on the GPU. Particularly, among these quantities there are (see line 4 of Algorithm 3):\n1. Ad, the 1D double-precision array containing only the non-zero elements of the coefficient matrix A (the dense coefficient matrix);\n2. M i, the 1D single-precision array containing the number of the star and the index of the first attitude element for each observation of the original matrix A;\n3. Ic, the 1D single-precision array with the indexes that the instrumental elements of the dense matrix Ad had in the original matrix A; 4. N , the 1D single-precision array containing the number of observations assigned to each MPI process.\nAlgorithm 1: aprod 1 with OpenMP and OpenACC\naprod 1 with OpenMP\nomp.1 #pragma omp parallel private(pid,sum) shared(N,x,Ad,b) omp.2 { omp.3 #pragma omp for omp.4 for i\u2190 0 to N [pid] do omp.5 sum = 0.0 // Astrometric sect. omp.6 k = i\u00d7Npar omp.7 for j \u2190 0 to NAstro do omp.8 sum = sum+ Ad[k]x[j + offset[i]] omp.9 k++\n// Attitude sect. omp.10 k = i\u00d7Npar +NAstro omp.11 for j1 \u2190 0 to NAxes do omp.12 k2 = j1 \u00d7NDFA + offset[i] omp.13 for j2 \u2190 0 to NParAxis do omp.14 sum = sum+ Ad[k]x[j2 + k2] omp.15 k++\n// Instrumental sect. omp.16 k = i\u00d7Npar +NAstro +NAtt omp.17 for j \u2190 0 to NInstr do omp.18 sum = sum+ Ad[k]x[F(i, j) + offset] omp.19 k++\n// Global sect. omp.20 k = i\u00d7Npar +NAstro +NAtt +NInstr omp.21 for j \u2190 0 to NGlob do omp.22 sum = sum+ Ad[k]x[j + offset] omp.23 k++\nomp.24 b[i] = b[i] + sum\nomp.25 } omp.26 Constraints computation\naprod 1 with OpenACC\n// Astrometric sect. acc.1 #pragma acc parallel private(sum) acc.2 { acc.3 #pragma acc loop acc.4 for i\u2190 0 to N [pid] do acc.5 sum = 0.0 acc.6 k = i\u00d7Npar acc.7 for j \u2190 0 to NAstro do acc.8 sum = sum+ Ad[k + j]x[j + offset[i]]\nacc.9 b[i] = b[i] + sum\nacc.10 } // Attitude sect. acc.11 #pragma acc parallel private(sum) acc.12 { acc.13 #pragma acc loop acc.14 for i\u2190 0 to N [pid] do acc.15 sum = 0.0 acc.16 k = i\u00d7Npar +NAstro acc.17 for j1 \u2190 0 to NAxes do acc.18 k1 = j1 \u00d7NParAxis acc.19 k2 = j1 \u00d7NDFA + offset[i] acc.20 for j2 \u2190 0 to NParAxis do acc.21 sum = sum+ Ad[k + j2 + k1]x[j2 + k2]\nacc.22 b[i] = b[i] + sum\nacc.23 } // Instrumental sect. acc.24 #pragma acc parallel private(sum) acc.25 { acc.26 #pragma acc loop acc.27 for i\u2190 0 to N [pid] do acc.28 sum = 0.0 acc.29 k = i\u00d7Npar +NAstro +NAtt acc.30 for j \u2190 0 to NInstr do acc.31 sum = sum+ Ad[k + j]x[F(i, j) + offset]\nacc.32 b[i] = b[i] + sum\nacc.33 } // Global sect. acc.34 #pragma acc parallel private(sum) acc.35 { acc.36 #pragma acc loop acc.37 for i\u2190 0 to N [pid] do acc.38 sum = 0.0 acc.39 k = i\u00d7Npar +NAstro +NAtt +NInstr acc.40 for j \u2190 0 to NGlob do acc.41 sum = sum+ Ad[k + j]x[j + offset]\nacc.42 b[i] = b[i] + sum\nacc.43 }\nacc.44 Constraints computation\nAlgorithm 2: aprod 2 with OpenMP and OpenACC\naprod 2 with OpenMP\nomp.1 #pragma omp parallel private(pid,tid,nth) shared(N,x,Ad,b) omp.2 { // ID number of the OpenMP thread omp.3 tid = omp get thread num() // Number of OpenMP threads omp.4 nth = omp get num threads(); omp.5 for i\u2190N t[tid][0] to N t[tid][1] do\n// Astrometric sect. omp.6 k = i\u00d7Npar omp.7 for j \u2190 0 to NAstro do\nomp.8 x[j + offset[i]] = x[j + offset[i]] + Ad[k]b[i] omp.9 k++\n// Attitude sect. omp.10 k = i\u00d7Npar +NAstro omp.11 for j1 \u2190 0 to NAxes do omp.12 k2 = j1 \u00d7NDFA + offset[i] omp.13 for j2 \u2190 0 to NParAxis do omp.14 x[j2 + k2] = x[j2 + k2] + Ad[k]b[i] omp.15 k++\n// Instrumental sect. omp.16 k = i\u00d7Npar +NAstro +NAtt omp.17 for j \u2190 0 to NInstr do omp.18 x[F(i, j)+offset] = x[F(i, j)+offset]+Ad[k]b[i] omp.19 k++\n// Global sect. omp.20 k = i\u00d7Npar +NAstro +NAtt +NInstr omp.21 for j \u2190 0 to NGlob do omp.22 x[j + offset] = x[j + offset] + Ad[k]b[i] omp.23 k++\nomp.24 } omp.25 Constraints computation\nacc.1 aprod 2 with OpenACC acc.2 #pragma acc parallel acc.3 {\nacc.4 #pragma acc loop acc.5 for i\u2190 0 to N [pid] do\n// Astrometric sect. acc.6 k = i\u00d7Npar acc.7 for j \u2190 0 to NAstro do acc.8 #pragma acc atomic acc.9 x[j + offset[i]] = x[j + offset[i]] + Ad[k + j]b[i]\n// Attitude sect. acc.10 k = i\u00d7Npar +NAstro acc.11 for j1 \u2190 0 to NAxes do acc.12 k1 = j1 \u00d7NParAxis acc.13 k2 = j1 \u00d7NDFA + offset[i] acc.14 for j2 \u2190 0 to NParAxis do acc.15 #pragma acc atomic acc.16 x[j2 + k2] = x[j2 + k2] + Ad[k + j2 + k1]b[i]\n// Instrumental sect. acc.17 k = i\u00d7Npar +NAstro +NAtt acc.18 for j \u2190 0 to NInstr do acc.19 #pragma acc atomic acc.20 x[F(i, j)+offset] = x[F(i, j)+offset]+Ad[k+j]b[i]\n// Global sect. acc.21 k = i\u00d7Npar +NAstro +NAtt +NInstr acc.22 for j \u2190 0 to NGlob do acc.23 #pragma acc atomic acc.24 x[j + offset] = x[j + offset] + Ad[k + j]b[i]\nacc.25 }\nacc.26 Constraints computation\nThere is no reason to copy these data back to the CPU at the end of the entire cycle of iterations. For a typical execution as the one presented in Section 5, that occupies a total memory of \u223c340 GB, the quantities Ad, M i, and Ic occupy a memory of \u223c246 GB, \u223c27 GB, and \u223c41 GB, respectively, whereas the amount of memory occupied by the array N is negligible compared to the total.\nThe only arrays that need to be copied both H2D and D2H at every step of the LSQR algorithm are x, containing the unknowns of the problem, and b containing the known terms of the system of equations (see lines 10, 12, 14, and 16 of Algorithm 3), since they are both updated at each iteration. The H2D and the D2H copies of x and b are performed both for the aprod 1 and for the aprod 2 functions but this does not imply a substantial slowdown of the code, since these vectors only represent the \u223c5% of the total memory. Indeed, for the execution presented above, the x and b arrays occupy a memory of \u223c0.23 GB and \u223c14 GB, respectively.\nManaging the data copies in this way, that is moving the \u223c95% of the data H2D before the LSQR cycle and only the \u223c5% of the data H2D and D2H at each step of the LSQR, the time employed in memory transfers is much less than the time employed in calculations, namely the application is compute bound rather than data-transfer bound. This point is better addressed in Section 7.3."
        },
        {
            "heading": "6.3. Parallelization",
            "text": "We defined the aprod modes 1 and 2 in two separate functions that we both ported to the GPU. In aprod 1, we\nreorganized the structure of the code: instead of computing the astrometric, the attitude, the instrumental, and the global sections within a single external for loop iterating on the number of observations assigned to each MPI process (Algorithm 1, left column), we repeated the external loop four times, once for every section (Algorithm 1, right column). Then, we enclosed each of the newly arranged sections in a #pragma acc parallel private(sum) directive, defining four parallel regions (lines acc.1\u201310, acc.11\u2013 23, acc.24\u201333, and acc.34\u201343 of Algorithm 1). The #pragma acc parallel directive starts a parallel execution on the current device and, to be effective, it requires an analysis by the programmer to ensure safe parallelism of the region of the code that is enclosed within the scope defined by the directive. The variable sum is declared within the private clause (lines acc.1, acc.11, acc.24, and acc.34 of Algorithm 1), such that each GPU thread has a local copy of it. In each parallel region, we parallelized the most external for loop with the #pragma acc loop directive (lines acc.3, acc.13, acc.26, and acc.36 of Algorithm 1).\nWe inserted the entire aprod 2 in a #pragma acc parallel region (Algorithm 2, lines acc.2\u201325) and we parallelized the most external for loop with a #pragma acc loop directive (Algorithm 2, line acc.4). With OpenACC, the variable tid, that identifies the OpenMP thread in the CPU code, becomes unnecessary: therefore the for loop at line omp.5 of Algorithm 2 does not iterate anymore from N t[tid][0] to N t[tid][1], but from 0 to N [pid], where pid is rank of the MPI process. On the lines of code that compute the operation x = x + A \u00d7 b in the astrometric, the attitude, the instrumental, and the global sections\nAlgorithm 3: Structure of the entire Gaia AVU\u2013GSR application in OpenACC\n1 Import data from files 2 Preconditioning 3 acc set device num(pid%acc get num devices(acc device default),acc device default) 4 #pragma acc enter data copyin(Ad,M i,Ic,N) 5 #pragma acc enter data copyin(x,b) 6 aprod 2 call (calculation of the initial solution) 7 #pragma acc exit data copyout(x,b) 8 MPI reduce\n// LSQR algorithm 9 while (conv. cond. | | max itn. reached) do 10 #pragma acc enter data copyin(x,b) 11 aprod 1 call 12 #pragma acc exit data copyout(x,b) 13 MPI reduce 14 #pragma acc enter data copyin(x,b) 15 aprod 2 call 16 #pragma acc exit data copyout(x,b) 17 MPI reduce 18 Variances and covariances computation\n19 Print solution to files\n(lines acc.9, acc.16, acc.20, and acc.24 of Algorithm 2), we put a #pragma acc atomic directive, preventing different GPU threads to simultaneously write the same elements of the array x."
        },
        {
            "heading": "6.4. Compilation and optimization of the code",
            "text": "To compile the application, we employed the PGI compiler3. Specifically, we prepared a Makefile and we compiled with the -acc, -fast, and -ta=tesla:maxregcount:32 options, where:\n1. -acc enables the OpenACC directives;\n2. -fast includes a set of flags to optimize the code;\n3. maxregcount:n specifies that the GPU employs a maximum number of registers equal to n. In particular, we set n to 32, since it optimizes the usage of the compute resources and the memory occupancy of a NVIDIA Volta V100 GPU, the device on which the code is tested, and it provides a speedup of the application. For better details, see Section 7.4.\nThe application uses the CFITSIO4, a library of C and Fortran subroutines for reading and writing data files in FITS data format. Indeed, before the start of the LSQR iterations, the application converts the input data from FITS to binary format, read by the LSQR, and after the end of the LSQR iterations the solution is written to a binary file then converted to FITS format (Figure 1).\n3https://www.pgroup.com/resources/docs/19.1/x86/\npgi-ref-guide/index.htm 4https://heasarc.gsfc.nasa.gov/fitsio/"
        },
        {
            "heading": "7. Performance tests",
            "text": "We aim to build a GPU version of the Gaia AVU-GSR application that accelerates compared to the current MPI + OpenMP code. In this section, we compare the performance of the OpenMP and the OpenACC codes. To perform this task, we simulated a complete system of stars in the Milky Way, the attitude and the instrumental settings of the Gaia satellite, and the global parameter \u03b3 of the PPN formalism, as described in Section 3.\nWe ran the performance tests on the CINECA supercomputer M100, described in Section 5. On M100, the memory of one node is of 256 GB and the memory of the 4 GPUs in each node is of 4\u00d7 16 GB. For this reason, we cannot run the tests for the OpenACC code by simulating a system that occupies a memory larger than 64 GB per node. For safety reasons, to avoid memory overflows due to the GPU architecture, we decided to lower this limit to 40 GB per node. The OpenMP code is not subject to this limitation but we needed to set the same amount of memory in the two applications to compare their performance.\nFor a complete exploration of the performance of the two applications, we ran the tests both on a single node and on an increasing number of nodes, setting a system with a fixed amount of memory. We also ran the tests on more nodes, setting an amount of memory proportional to the number of nodes. Specifically, the three tests were run with the following configurations:\n1. Fixed memory, intra-node: on 1 node of M100, on an increasing number of MPI processes, setting the parameters of the system such that it occupies a mem-\nory of 10 GB.5;\n2. Fixed memory, inter-nodes: on an increasing number of nodes of M100, up to 16 nodes, with 4 MPI processes per node and a system that occupies a memory of 40 GB;\n3. Proportional memory, inter-nodes: on an increasing number of nodes of M100, up to 16 nodes, with 4 MPI processes per node, setting a system that occupies a memory of 40 GB per node (40 GB on one node, 80 GB on two nodes, and so forth).\nThe tests are illustrated in Figure 4. The code ran up to convergence of the LSQR algorithm, to increase the statistical meaning of the time measurements. On the vertical axis of all plots in Figure 4, we show the mean execution time of one LSQR iteration. The error on these measurements, represented as error bars, is provided by the standard deviation of the times of all the LSQR iterations.\nIn the intra-node plot (Figure 4a), the bottom horizontal axis represents the number of MPI processes, set to {1,2,4,8,16,32}. For the OpenMP version of the code, the top horizontal axis shows the number of OpenMP threads assigned to each MPI process. In particular, the number of OpenMP threads is set such that the product of the number of MPI processes and the number of OpenMP threads is equal to 32, the number of physical cores in one node (Section 5). Since we set the number of MPI processes to {1,2,4,8,16,32}, the number of OpenMP threads is set to {32,16,8,4,2,1}.\nTo optimize each run in the intra-node and in the internodes tests, we set the number of MPI processes per socket (--ntasks-per-socket) to the number of MPI processes per node (--ntasks-per-node) divided by two, except when we only consider one MPI process per node, since M100 is a two-socket platform. As specified in Section 5, each physical CPU core of M100 corresponds to 4 virtual cores. We required that each MPI process on each node was allocated on a different physical CPU core of the node since, as experienced on POWER9 architecture and suggested by the CINECA support for our specific case, allocating more MPI processes on different virtual cores of the same physical CPU causes a slowdown of the code. To obtain this configuration, we set the number of virtual cores to allocate for each MPI process (--cpus-per-task) such that the product between --ntasks-per-node and --cpus-per-task is equal to 128, the total number of virtual cores per node of M100. As detailed in the previous paragraph, to exploit all the physical CPUs in each node, we set the number of OpenMP threads (OMP NUM THREADS)\n5In this test, the memory is not set to 40 GB but to 10 GB because the codes are run on a number of MPI processes from 1 to 32. Since the OpenACC code executed on one MPI process runs on only one GPU, that has a memory of 16 GB, we cannot set a memory of 40 GB. It is logical to set the memory to 40 GB (the limit that we chose for 4 GPUs) divided by 4 (the number of GPUs in the node).\nsuch that the product of --ntasks-per-node and OMP NUM THREADS is equal to 32.\nBoth the OpenMP and the OpenACC codes are launched with a --map-by socket: PE=n specification. This map defines the number of physical cores assigned to each MPI process in one socket and is set to the total number of physical cores present in a socket (16 on M100) divided by the number of MPI processes per socket assigned for the run. Setting this variable prevents each MPI process to run on more virtual cores belonging to the same physical core in each socket. This map is fundamental for the OpenMP code, that runs on the CPU, but it is basically irrelevant for the OpenACC code, that mainly runs on the GPU.\nIn the inter-nodes plots (Figures 4b and 4c), the bottom horizontal axis represents, instead, the number of nodes on which the codes run. Since the two inter-nodes tests run on 4 MPI processes per node, the OpenMP code is always parallelized on 8 OpenMP threads per MPI process. We ran the two inter-nodes test on {1,2,4,8,16} nodes, and thus on {4,8,16,32,64} total MPI processes."
        },
        {
            "heading": "7.1. Fixed memory",
            "text": "Figures 4a and 4b show the intra-node and the internodes performance tests of the code, respectively, with a fixed memory configuration. Both plots demonstrate that the OpenACC code is more performant than the OpenMP code almost along the entire range of considered computational resources.\nIn the intra-node case, the gain in performance over the OpenMP version increases when the OpenACC code runs on a number of MPI processes larger or equal to 4. This is due to the fact that, in these configurations, the code runs on the four GPUs of the node, whereas when the number of MPI processes is set to 1 and 2, the code only exploits 1 and 2 GPUs, respectively (line 3 of Algorithm 3). When running on a number of MPI processes \u2265 4, the time of one iteration of the OpenACC code remains nearly constant. For this reason, we could say that the optimal configuration to run the OpenACC code is on 4 MPI processes per node, since we obtain the best performance employing the minor number of computing resources: in this setting, all the GPUs of the node are exploited and only one MPI process is assigned to each GPU. This is the configuration on which the GPU code will run when in production.\nConcerning the MPI + OpenMP code, we observe that the response time decreases when the MPI processes increase and the OpenMP threads decrease. Indeed, the OpenMP parallelization is only employed within the two aprod functions, whereas MPI parallelizes the entire structure of the code, combining among the MPI processes the results obtained from the two aprod functions at each step of the LSQR algorithm. In particular, we can observe from Figure 4a that the optimal configuration for the OpenMP code is to run on 16 MPI processes and 2 OpenMP threads.\nWe note that this configuration was employed for the inproduction run presented as an example in Section 5 and for all the other production runs. Computing the speedup of the OpenACC code over the OpenMP code as the ratio between the iteration times achieved in the two optimal settings, 4 MPI processes for the OpenACC code and 16 MPI processes + 2 OpenMP threads for the OpenMP code, we obtain a factor of \u03b7 = 1.20 \u00b1 0.02. In general, from 4 MPI processes on, the ratio between the average times of one LSQR iteration of the OpenMP and the OpenACC codes is nearly constant, around 1.3, and reaches a maximum of 1.5 when comparing the two values on 32 MPI processes.\nIn the inter-nodes case, the ratio between the OpenMP and the OpenACC average times remains nearly constant along the entire range of nodes on which the codes were run. This is explained by the fact that the two codes always run in the same configuration, on 4 MPI processes per node. Specifically, the OpenACC code always runs in its optimal setting. Specifically, the average ratio, with its dispersion, is 1.39 \u00b1 0.06, consistent with the results obtained for the intra-node case.\nWe note that the first point of the plot in Figure 4b corresponds to a run of the two codes on 4 MPI processes on one node and has an ordinate, i.e. the time for one iteration, \u223c4 times larger than the ordinate of the third point of the plot in Figure 4a, that runs in the same configuration. This is explained by the fact that the inter-nodes test computes a system that occupies an amount of memory 4 times larger than the memory occupied by the system in the intra-node test, as specified in the numbered list in Section 7."
        },
        {
            "heading": "7.1.1. Strong scaling",
            "text": "We investigate the strong scaling of the OpenMP and the OpenACC applications across the nodes. However, porting the Gaia code to the GPU is not intended to improve its scalability compared to the CPU code but its performance, to obtain scientific results in more reasonable timescales. In fact, we do not expect the scalability of the OpenACC code to substantially change compared to the OpenMP code, since the two applications have the same structure.\nThe similar scaling behaviour of the OpenMP and the OpenACC codes is confirmed by Figure 5a, that corresponds to Figure 4b, where the iteration time is replaced by the speedup, computed as:\nS = t1 tn . (2)\nIn Eq. (2), t1 is the time of one average iteration of the LSQR algorithm on one node and tn is the iteration time on an increasing number of nodes. The error bars are calculated by propagating the uncertainties on t1 and tn. For comparison, we show as a black dashed line the ideal speedup relation.\nFor both codes, the perfect scaling is achieved only up to 2 nodes and for a larger number of nodes the two scalability curves depart from the one-to-one line. Specifically, on 16 nodes, the OpenMP and the OpenACC codes reach a maximum speedup of 9.91 and 9.57, respectively, which translates to a parallel efficiency of 9.91/16 = 0.62 and 9.57/16 = 0.60. This moderate scalability can be explained by the non-parallelizable parts of the two applications, such as atomic operations, and by the communications among the MPI processes scheduled across the nodes."
        },
        {
            "heading": "7.2. Proportional memory",
            "text": "Figure 4c shows the inter-nodes performance test of the codes where the system occupies a memory proportional to the amount of computational resources. Also in this case, the two codes always run on 4 MPI processes per node and the OpenACC code is always in its optimal configuration. The mean gain of the OpenACC code over the OpenMP code, 1.44\u00b10.02, is in agreement with the results obtained in the two fixed memory tests."
        },
        {
            "heading": "7.2.1. Weak scaling",
            "text": "We investigate the weak scaling of the OpenMP and of the OpenACC applications across the nodes. Figure 5b corresponds to Figure 4c, where the iteration time is replaced by the scaled speedup. The scaled speedup and its error are calculated as in Section 7.1.1. For comparison, we show as a black dashed line the S = 1 relation, where S is the scaled speedup (Eq. 2), representing the ideal trend of weak scalability, as stated by the Gustafson\u2019s law (Gustafson, 1988). The plot shows that the weak scalability curves of the OpenMP and the OpenACC codes are in agreement with each other and that the weak scaling is quite well satisfied for both applications, since the minimum scaled speedup is of 0.95 for the OpenMP code and of 0.94 for the OpenACC code. The mean iteration time passes from a minimum of 1.60 s (OpenMP) and 1.12 s (OpenACC) when the codes run on one node to a maximum of 1.69 s (OpenMP) and 1.19 s (OpenACC) when the codes run on 16 nodes."
        },
        {
            "heading": "7.3. Detailed analysis of the speedup",
            "text": "We now investigate the origin of the speedup of the OpenACC code over the OpenMP code by comparing the elapsed times of the different regions of the two applications and by evaluating how much the memory transfers H2D/D2H of the OpenACC code affect its performance. Specifically, we compare a run of the OpenACC and of the OpenMP codes in their optimal configurations on one node of M100 (4 MPI processes for the OpenACC code and 16 MPI processes + 2 OpenMP threads for the OpenMP code). In this analysis, we do not consider the regions before the first call of the aprod 2 function, to calculate the initial solution, and of the region after the end of the LSQR iteration cycle (see Figure 1), since they are comparable between the two codes, given that they are not\nported to the GPU in the OpenACC application. The simulated systems occupy a global memory of 10 GB, as in the intra-node performance test.\nFigure 6 shows the output of the NVIDIA Nsight Systems profiler tool6 for the run of the OpenACC code. We considered a run with 4 iterations of the LSQR algorithm. The NVIDIA Nsight Systems profiler is an analysis tool to visualize all the regions of a GPU-ported application: (I) computed by a GPU kernel (blue regions with the name of the correspondent kernel), (II) that involve memory transfers H2D (green regions) and D2H (purple regions), and (III) still computed on the CPU (white regions). This\n6https://developer.nvidia.com/nsight-systems\ntool is particularly useful to suggest how to optimize an in-development application, like the Gaia AVU-GSR one. Specifically, Figure 6 shows the relevant portion of the output of the OpenACC code, from line 4 to line 17 of Algorithm 3, to be compared with the correspondent part of the OpenMP code.\nThe most expensive computation regions are the executions of the aprod 1 and 2 functions. Figure 7a shows a zoom-in of one iteration of the run of the OpenACC code, illustrated in Figure 6, where the aprod 1 and aprod 2 kernels, identified with the blue areas labelled as \u201cb plus...\u201d (aprod 1) and \u201cx plus...\u201d (aprod 2), are more visible. The times employed by these regions are directly measured from the profiler, for the OpenACC code (see Figures 7b\nand 7c, where these times are highlighted on the corresponding kernels), and with the MPI Wtime() function, for the OpenMP code.\nIn the OpenACC code, the aprod 1 computation is divided into four kernels, one for the astrometric, one for the attitude, one for the instrumental, and one for the global part of the system of equations, as we can see in Figure 7b and in the right part of Algorithm 1. The total elapsed time of these four regions of the aprod 1 is of ta1,ACC \u223c 0.15 s (see Figure 7b), and the correspondent time for the OpenMP code is of ta1,OMP \u223c 0.12 s. This clearly means that the speedup of the OpenACC application over the OpenMP code is not due to the aprod 1 function but that, instead, the OpenACC code looses in performance compared to the OpenMP counterpart in executing this function. Specifically, the ratio between the OpenMP and OpenACC times of the aprod 1 is of \u223c0.8.\nFrom Figure 7c we can see that the aprod 2 kernel employs ta2,ACC \u223c 0.064 s for the OpenACC code. Instead, in the OpenMP code, the aprod 2 function employs ta2,OMP \u223c 0.23 s. In this case, the OpenACC code clearly accelerates compared to the OpenMP code, with a speedup of \u223c3.6. Dividing the sum of the times of the aprod 1 and 2 regions for the OpenMP and the OpenACC codes, we obtain a speedup of \u223c1.6, a bit larger than the one found in the previous sections. This is explained by the fact that in the OpenACC code we loose some time in copying the data H2D and D2H for every iteration, operation that is not performed in the OpenMP code.\nIn Figure 7 and in Algorithm 3, we can see that we copy twice the x and b arrays both H2D and D2H at each iteration of the LSQR algorithm. These four copies em-\nploy a total time of tMem \u223c 0.04 s, that is smaller than the computation times of both the aprod 1 and 2 regions. The white regions, namely all the operations still performed on the CPU that include minor I/O and the reduce operations among the MPI processes, employ a total time of tCPU \u223c 0.064 s, comparable to the corresponding time in the OpenMP code. We estimate the total speedup as:\n\u03b7\u2032 = ta1,OMP + ta2,OMP + tCPU\nta1,ACC + ta2,ACC + tMem + tCPU \u223c 1.3, (3)\nwhich is consistent with the value found in Section 7.1. This speedup is mainly due to the acceleration of the computation of the aprod 2 region.\nWe have seen that, for each iteration of the OpenACC code, the time involved in data copies is of tMem \u223c 0.04 s, whereas the time involved in kernel computation is of\nta1,ACC + ta2,ACC \u223c 0.15 + 0.064 \u223c 0.21 s. The data copies represent the \u223c18.7% of the time employed in kernel computation, which means that the code is compute bound rather than data-transfer bound. This is a consideration only for one iteration. Yet, if we observe the left part of Figure 6, we can see that for the entire run the 89.0% of the time is due to kernel computation and only the 11.0% of the time is due to memory transfers. The compute bound consideration can thus be extended to the entire application."
        },
        {
            "heading": "7.4. GPU utilization",
            "text": "The OpenACC code runs on 32 GPU registers. This parameter is particularly important for a GPU code. On\na NVIDIA V100 GPU, setting the number of registers to 32 might be a logical and optimal choice, since the NVIDIA V100 architecture is organized such that groups of 32 registers see the same cache memory and are subject to the same operation in a Single Instruction Multiple Data (SIMD)-like fashion. In the software, this is encoded in the size of a warp, a logical block of 32 threads that always perform the same operations simultaneously. Each warp is directly mapped on each block of 32 registers.\nTo verify whether 32 GPU registers actually correspond to the optimal configuration, we exploited the NVIDIA Nsight Compute profiler tool7. With this profiler, we compared the Speed Of Light metric, that calculates the percentage of utilization of the compute (SM) and of the memory resources of the GPU compared to the theoretical max-\n7https://developer.nvidia.com/nsight-compute\nimum, of four configurations, where we set the number of registers to 32, 64, 128, and 42 (green, light blue, purple, and orange bars in Figure 8, respectively), three numbers multiples of 32 and one number that is not a multiple of 32. Figure 8 refers to a system that occupies 10 GB of memory and that runs on 1 GPU.\nWhereas with 32 registers \u223c80% of the available compute and memory performance of the GPU are utilized, for the other three cases these reduce to \u223c45% and are comparable to each other. For 32 registers, the resources of the device are better exploited. A higher Speed Of Light metric corresponds to a better performance of the code: whereas the mean iteration times for the 64-, 128-, and 42-registers cases are of 1.30 s, 1.27 s, and 1.25 s, the mean iteration time for the 32-registers case is of 1.05 s, which implies a speedup of 1.24, 1.21, and 1.19 compared to the other three configurations."
        },
        {
            "heading": "8. Conclusions and Future Work",
            "text": "We ported to the GPU with OpenACC the Gaia AVU\u2013 GSR solver, that aims to find the positions and the proper motions of \u223c108 stars in our galaxy, besides the attitude and the instrumental specifications of the Gaia satellite and the parameter \u03b3 of the PPN formalism. The application, originally parallelized on the CPU with MPI+OpenMP, solves, with the iterative LSQR algorithm, a system of linear equations, where the coefficient matrix is large and sparse.\nThe main target of this analysis is to explore the feasibility of porting this application to a GPU environment through a preliminary work based on the OpenACC library. This study, along with the investigation of the performance improvement, is propaedeutic to the final goal of a CUDA port and to a better optimization of the algorithm, to exploit at most the GPU architecture.\nTo perform this preliminary port, we replaced the OpenMP part with the OpenACC language. In the OpenACC port, we moved the \u223c95% of the data H2D before the start of the LSQR cycle to limit the copies per iteration, which makes the code compute bound rather than data-transfer bound.\nWe compared the performance of the OpenMP and the OpenACC applications on M100 by running systems that occupy a memory both constant and proportional to the amount of computing resources. The OpenACC code presents a speedup of \u223c1.2\u20131.5 over the OpenMP code, and its optimal configuration is obtained by running on 4 MPI processes per node, which allows to exploit all the GPUs of the node assigning a single MPI process per GPU. With a speedup of \u223c1.3, a typical execution of the AVU\u2013 GSR solver, as the one presented in Section 5, passes from an elapsed time of \u223c166 hours to \u223c128 hours, saving the \u223c23% of the total time, in agreement with the estimate presented in Section 2. This speedup is mainly driven by the port of the aprod 2 function to the GPU, which accelerates \u223c3.6 times over the CPU version. We point out that, to control the GPUs, the MPI processes are the most logical choice compared to the OpenMP threads. A OpenMP + OpenACC version of this code would follow a completely different structure compared to the MPI + OpenMP application and it would be limited to run on only one node, which is not possible for systems with a size even much smaller than the expected final data set of\nGaia. The proportional memory test shows that both the OpenMP and the OpenACC applications satisfy the weak scalability, since the average time of one LSQR iteration maintains nearly constant with the number of nodes, proportional to the memory occupied by the system.\nTo exploit at best the compute and the memory resources of the GPU, we compiled the OpenACC code with 32 GPU registers, which entail the optimal performance compared to a different number of registers.\nAdditional analyses to further accelerate the code are already underway or planned. A possible way to explore is the asynchronous computation of the CPU and the GPU code regions. Some tests have demonstrated that, with the current structure of the code, we do not obtain a significant gain in performance if we asynchronously run, for example, the aprod 1 and 2 regions, ported to the GPU with OpenACC, and their respective constraints sections, run on the CPU. This is due to the fact that, although running on the CPU, the two constraints regions are \u223c100- 1000 faster than the aprod 1 and 2 functions. For the same reason, also a port of these two regions to the GPU would not significantly improve the code speed.\nAnother possibility would be to reduce the copies of the x and b vectors at every iteration. However, the main future aim for this application is to port it to the GPU by replacing OpenACC with CUDA and by further optimizing the algorithm. CUDA is a low-level parallelization language that would imply a reorganization of some parts of the code to better match the GPU architecture, which would entail a more efficient parallelization and, thus, a higher speedup. This CUDA port is already in progress and the preliminary results are very optimistic. The acceleration factor over the OpenMP application might be around 10. With the CUDA port, we might also explore the advantages of the asynchronous computation of different GPU regions and of CPU and GPU regions.\nWe began to define the CUDA code by following the architecture of the V100 GPUs on M100 and we plan to readjust it to fit the architecture of the A100 GPUs that will be present on the pre-exascale system Leonardo, a supercomputer of CINECA that will be operational at the end of 2022. Given that the memory and the number of streaming multiprocessors of the A100 GPUs of Leonardo will be larger than on the V100 GPUs of M100, we would expect a further speedup of the code when it will run on\nthis system. This is the object of a paper in preparation. To conclude, the preliminary tests presented in this work provide essential information about the potential performance and scaling properties of the GPU-ported Gaia AVU-GSR code in perspective of exascale systems. These properties could be extrapolated to a class of codes that analogously solve an inverse problem for a large-sized system, that, as we have seen in Section 2, we can retrieve in several contexts. However, these tests have to be extended to provide more exhaustive information. The largest system that we computed in the tests presented in this paper occupies a memory of 640 GB (40 GB per node on 16 nodes) and contains \u223c3 \u00d7 109 observations of the Milky Way stars, \u223c2 orders of magnitude less than in the expected final data set of the Gaia mission, i.e. \u223c1011 observations. A system with \u223c1011 observations will occupy a memory of \u223c 10 11\n3\u00d7109 \u00d7 640 = 21333 GB, which will require 356 nodes of M100 to be solved, by running with 60 GB per node, nearly the maximum allowed for a GPU code. To better explore the behaviour of this code and of other LSQR-based applications on exascale systems, we aim to extend the presented tests up to a larger amount of computing resources, for systems with an increasing size up to realistic science cases. This is what we plan to do with the CUDA code both on M100 and on the pre-exascale cluster Leonardo.\nCRediT authorship contribution statement\nValentina Cesare: Software, Validation, Formal analysis, Investigation, Writing - Original Draft, Visualization. Ugo Becciani: Term, Conceptualization, Software, Validation, Writing - Review & Editing, Supervision, Project administration. Alberto Vecchiato: Software, Writing - Review & Editing, Supervision. Mario Gilberto Lattanzi: Writing - Review & Editing, Supervision, Funding acquisition. Fabio Pitari: Software, Investigation, Writing - Review & Editing. Mario Raciti: Software, Investigation, Writing - Review & Editing. Giuseppe Tudisco: Software, Investigation, Writing - Review & Editing. Marco Aldinucci: Methodology, Writing - Review & Editing. Beatrice Bucciarelli: Writing - Review & Editing, Supervision.\nDeclaration of competing interest\nThe authors declare the following financial interests/ personal relationships which may be considered as potential competing interests: Mario Gilberto Lattanzi has patent Gaia AVU\u2013GSR parallel solver licensed to Mario Gilberto Lattanzi, Alberto Vecchiato, Beatrice Bucciarelli, Roberto Morbidelli, Ugo Becciani, Valentina Cesare. Alberto Vecchiato has patent Gaia AVU\u2013GSR parallel solver licensed to Mario Gilberto Lattanzi, Alberto Vecchiato, Beatrice Bucciarelli, Roberto Morbidelli, Ugo Becciani, Valentina Cesare. Beatrice Bucciarelli has patent Gaia\nAVU\u2013GSR parallel solver licensed to Mario Gilberto Lattanzi, Alberto Vecchiato, Beatrice Bucciarelli, Roberto Morbidelli, Ugo Becciani, Valentina Cesare. Ugo Becciani has patent Gaia AVU\u2013GSR parallel solver licensed to Mario Gilberto Lattanzi, Alberto Vecchiato, Beatrice Bucciarelli, Roberto Morbidelli, Ugo Becciani, Valentina Cesare. Valentina Cesare has patent Gaia AVU\u2013GSR parallel solver licensed to Mario Gilberto Lattanzi, Alberto Vecchiato, Beatrice Bucciarelli, Roberto Morbidelli, Ugo Becciani, Valentina Cesare.\nData availability\nThe data that has been used is confidential."
        },
        {
            "heading": "Acknowledgements",
            "text": "We sincerely thank the referee, whose comments largely improved and clarified the presentation of our results. We sincerely thank Dr. Aswin Kumar of NVIDIA, one of the mentors of the CINECA GPU Hackathon Digital Event of 2021, for the precious indications provided during the event that allowed to achieve the current speedup of the OpenACC code over the OpenMP code and for his help with the usage of the NVIDIA profilers. We also thank Dr. Massimiliano Guarrasi of CINECA, for the time that he dedicated to explain the basis of GPU architecture, and the organizers of the CINECA course \u201cProgramming paradigms for GPU devices\u201d, held on 9th\u221211th June 2021, for their availability to deepen the fundamental aspects of GPU programming that were exploited to parallelize our application in a more efficient way.\nFunding\nThis work was supported by the Italian Space Agency (ASI) [Grant Number: 2018-24-HH.0], in support of the Italian participation to the Gaia mission. This work was also supported by Consorzio Interuniversitario Nazionale per l\u2019Informatica, under the project EUPEX, EC H2020 RIA, EuroHPC-02-2020 [Grant Agreement: 101033975]."
        },
        {
            "heading": "Appendix A. Supplementary data",
            "text": "Supplementary material related to this article can be found online at https://doi.org/10.1016/j.ascom.2022. 100660. Supplementary material contains: Configuration options set in the SLURM scripts to run the performance tests of the code."
        }
    ],
    "title": "The Gaia AVU\u2013GSR parallel solver: preliminary studies of a LSQR\u2013based application in perspective of exascale systems",
    "year": 2022
}