{
    "abstractText": "Visible-infrared person re-identification (V-I ReID) seeks to match images of individuals captured over a distributed network of RGB and IR cameras. The task is challenging due to the significant differences between V and I modalities, especially under real-world conditions, where images are corrupted by, e.g, blur, noise, and weather. Despite their practical relevance, deep learning (DL) models for multimodal V-I ReID remain far less investigated than for single and cross-modal V to I settings. Moreover, state-of-art V-I ReID models cannot leverage corrupted modality information to sustain a high level of accuracy. In this paper, we propose an efficient model for multimodal V-I ReID \u2013 named Multimodal Middle Stream Fusion (MMSF) \u2013 that preserves modality-specific knowledge for improved robustness to corrupted multimodal images. In addition, three state-of-art attention-based multimodal fusion models are adapted to address corrupted multimodal data in V-I ReID, allowing for dynamic balancing of the importance of each modality. The literature typically reports ReID performance using clean datasets, but more recently, evaluation protocols have been proposed to assess the robustness of ReID models under challenging real-world scenarios, using data with realistic corruptions. However, these protocols are limited to unimodal V settings. For realistic evaluation of multimodal (and cross-modal) V-I person ReID models, we propose new challenging corrupted datasets for scenarios where V and I cameras are co-located (CL) and not co-located (NCL). Finally, the benefits of our Masking and Local Multimodal Data Augmentation (ML-MDA) strategy are explored to improve the robustness of ReID models to multimodal corruption. Our experiments on clean and corrupted versions of the SYSU-MM01, RegDB, and ThermalWORLD datasets indicate the multimodal V-I ReID models that are more likely to perform well in realworld operational conditions. In particular, our ML-MDA is an important strategy for a V-I person ReID system to sustain high accuracy and robustness when processing corrupted multimodal images. The multimodal ReID models provide the best accuracy and complexity trade-off under both CL and NCL settings and compared to state-of-art unimodal ReID systems, except for the ThermalWORLD dataset due to its low-quality I. Our MMSF model outperforms every method under CL and NCL camera scenarios. GitHub code: https://github.com/art2611/MREiD-UCD-CCD.git.",
    "authors": [
        {
            "affiliations": [],
            "name": "Arthur Josi"
        },
        {
            "affiliations": [],
            "name": "Mahdi Alehdaghi"
        },
        {
            "affiliations": [],
            "name": "Rafael M. O. Cruz"
        },
        {
            "affiliations": [],
            "name": "Eric Granger"
        }
    ],
    "id": "SP:7250688c386c6c4dba736f5401a21a27479c004e",
    "references": [
        {
            "authors": [
                "M. Alehdaghi",
                "A. Josi",
                "R. Cruz",
                "E. Granger"
            ],
            "title": "Visible-infrared person reidentification using privileged intermediate information",
            "venue": "arxiv:2209.09348.",
            "year": 2022
        },
        {
            "authors": [
                "T. Baltru\u0161aitis",
                "C. Ahuja",
                "Morency",
                "L.-P."
            ],
            "title": "Multimodal machine learning: A survey and taxonomy",
            "venue": "TPAMI. IEEE.",
            "year": 2018
        },
        {
            "authors": [
                "A. Bhuiyan",
                "Y. Liu",
                "P. Siva",
                "M. Javan",
                "I.B. Ayed",
                "E. Granger"
            ],
            "title": "Pose guided gated fusion for person re-identification",
            "venue": "WACV.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Chang",
                "C. Jung",
                "J. Sun",
                "F. Wang"
            ],
            "title": "Siamese dense network for reflection removal with flash and no-flash image pairs",
            "venue": "IJCV. Springer.",
            "year": 2020
        },
        {
            "authors": [
                "J. Chen",
                "Q. Yang",
                "J. Meng",
                "Zheng",
                "W.-S.",
                "Lai",
                "J.-H."
            ],
            "title": "Contour-guided person re-identification",
            "venue": "PRCV.",
            "year": 2019
        },
        {
            "authors": [
                "M. Chen",
                "Z. Wang",
                "F. Zheng"
            ],
            "title": "Benchmarks for corruption invariant person reidentification",
            "venue": "arxiv:2111.00880.",
            "year": 2021
        },
        {
            "authors": [
                "S. Choi",
                "S. Lee",
                "Y. Kim",
                "T. Kim",
                "C. Kim"
            ],
            "title": "Hi-cmd: hierarchical cross-modality disentanglement for visible-infrared person re-identification",
            "venue": "CVPR.",
            "year": 2020
        },
        {
            "authors": [
                "D. Ciregan",
                "U. Meier",
                "J. Schmidhuber"
            ],
            "title": "Multi-column deep neural networks for image classification",
            "venue": "CVPR.",
            "year": 2012
        },
        {
            "authors": [
                "D. Fu",
                "D. Chen",
                "J. Bao",
                "H. Yang",
                "L. Yuan",
                "L. Zhang",
                "D. . . Chen"
            ],
            "title": "Unsupervised pre-training for person reidentification",
            "year": 2021
        },
        {
            "authors": [
                "V. Gabeur",
                "A. Nagrani",
                "C. Sun",
                "K. Alahari",
                "C. Schmid"
            ],
            "title": "Masking modalities for cross-modal video retrieval",
            "venue": "WACV.",
            "year": 2022
        },
        {
            "authors": [
                "R. Geirhos",
                "C.R. Temme",
                "J. Rauber",
                "H.H. Sch\u00fctt",
                "M. Bethge",
                "F.A. Wichmann"
            ],
            "title": "Generalisation in humans and deep neural networks",
            "venue": "NIPS.",
            "year": 2018
        },
        {
            "authors": [
                "Y. Gong",
                "Z. Zeng",
                "L. Chen",
                "Y. Luo",
                "B. Weng",
                "F. Ye"
            ],
            "title": "A person re-identification data augmentation method with adversarial defense effect",
            "venue": "arxiv:2101.08783.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Gu",
                "K. Yang",
                "S. Fu",
                "S. Chen",
                "X. Li",
                "I. Marsic"
            ],
            "title": "Hybrid attention based multimodal network for spoken language classification",
            "venue": "Proceedings of the conference. association for computational linguistics. meeting.",
            "year": 2018
        },
        {
            "authors": [
                "X. Hao",
                "Y. Zhu",
                "S. Appalaraju",
                "A. Zhang",
                "W. Zhang",
                "B. Li",
                "M. Li"
            ],
            "title": "Mixgen: A new multi-modal data augmentation",
            "venue": "arxiv:2206.08358.",
            "year": 2022
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "CVPR.",
            "year": 2016
        },
        {
            "authors": [
                "S. He",
                "H. Luo",
                "P. Wang",
                "F. Wang",
                "H. Li",
                "W. Jiang"
            ],
            "title": "Transreid: Transformer-based object re-identification",
            "venue": "arxiv:2102.04378.",
            "year": 2021
        },
        {
            "authors": [
                "D. Hendrycks",
                "S. Basart",
                "N. Mu",
                "S. Kadavath",
                "F. Wang",
                "E. Dorundo"
            ],
            "title": "The many faces of robustness: A critical analysis of out-of-distribution generalization",
            "year": 2021
        },
        {
            "authors": [
                "D. Hendrycks",
                "T. Dietterich"
            ],
            "title": "Benchmarking neural network robustness to common corruptions and perturbations. arxiv:1903.12261",
            "year": 2019
        },
        {
            "authors": [
                "D. Hendrycks",
                "X. Liu",
                "E. Wallace",
                "A. Dziedzic",
                "R. Krishnan",
                "D. Song"
            ],
            "title": "Pretrained transformers improve out-ofdistribution robustness",
            "venue": "arxiv:2004.06100.",
            "year": 2020
        },
        {
            "authors": [
                "D. Hendrycks",
                "N. Mu",
                "E.D. Cubuk",
                "B. Zoph",
                "J. Gilmer",
                "B. Lakshminarayanan"
            ],
            "title": "Augmix: A simple data processing method to improve robustness and uncertainty",
            "venue": "arxiv:1912.02781.",
            "year": 2019
        },
        {
            "authors": [
                "A. Hermans",
                "L. Beyer",
                "B. Leibe"
            ],
            "title": "In defense of the triplet loss for person reidentification",
            "venue": "arxiv:1703.07737.",
            "year": 2017
        },
        {
            "authors": [
                "J. Hong",
                "M. Kim",
                "J. Choi",
                "Y.M. Ro"
            ],
            "title": "Watch or listen: Robust audio-visual speech recognition with visual corruption modeling and reliability scoring",
            "year": 2023
        },
        {
            "authors": [
                "A.A. Ismail",
                "M. Hasan",
                "F. Ishtiaq"
            ],
            "title": "Improving multimodal accuracy through modality pre-training and attention",
            "venue": "arxiv:2011.06102.",
            "year": 2020
        },
        {
            "authors": [
                "A. Josi",
                "M. Alehdaghi",
                "R.M. Cruz",
                "E. Granger"
            ],
            "title": "Multimodal data augmentation for visual-infrared person reid with corrupted data",
            "venue": "WACV.",
            "year": 2023
        },
        {
            "authors": [
                "H.R.V. Joze",
                "A. Shaban",
                "M.L. Iuzzolino",
                "K. Koishida"
            ],
            "title": "Mmtm: multimodal transfer module for cnn fusion",
            "venue": "CVPR.",
            "year": 2020
        },
        {
            "authors": [
                "S.D. Khan",
                "H. Ullah"
            ],
            "title": "A survey of advances in vision-based vehicle reidentification",
            "year": 2019
        },
        {
            "authors": [
                "V.V. Kniaz",
                "V.A. Knyaz",
                "J. Hladuvka",
                "W.G. Kropatsch",
                "V. Mizginov"
            ],
            "title": "Thermalgan: Multimodal color-to-thermal image translation for person re-identification in multispectral dataset",
            "venue": "ECCV workshops.",
            "year": 2018
        },
        {
            "authors": [
                "Z. Lian",
                "B. Liu",
                "J. Tao"
            ],
            "title": "Ctnet: Conversational transformer network for emotion recognition",
            "venue": "TASLP. IEEE.",
            "year": 2021
        },
        {
            "authors": [
                "V. Lohweg",
                "U. M\u00f6nks"
            ],
            "title": "Fuzzy-patternclassifier based sensor fusion for machine conditioning. Sensor fusion and its applications",
            "year": 2010
        },
        {
            "authors": [
                "H. Luo",
                "Y. Gu",
                "X. Liao",
                "S. Lai",
                "W. Jiang"
            ],
            "title": "Bag of tricks and a strong baseline for deep person re-identification",
            "venue": "CVPR workshops.",
            "year": 2019
        },
        {
            "authors": [
                "S. Lai",
                "J. Gu"
            ],
            "title": "A strong baseline and batch normalization neck for deep person re-identification",
            "year": 2019
        },
        {
            "authors": [
                "W. Luo",
                "J. Xing",
                "A. Milan",
                "X. Zhang",
                "W. Liu",
                "Kim",
                "T.-K."
            ],
            "title": "Multiple object tracking: A literature review",
            "venue": "Artificial intelligence. Elsevier.",
            "year": 2021
        },
        {
            "authors": [
                "D. Mekhazni",
                "A. Bhuiyan",
                "G. Ekladious",
                "E. Granger"
            ],
            "title": "Unsupervised domain adaptation in the dissimilarity space for person ReID",
            "venue": "ECCV.",
            "year": 2020
        },
        {
            "authors": [
                "C. Michaelis",
                "B. Mitzkus",
                "R. Geirhos",
                "E. Rusak",
                "O. Bringmann",
                "A.S. Ecker",
                "W. . . Brendel"
            ],
            "title": "Benchmarking robustness in object detection: Autonomous driving when winter is coming",
            "year": 2019
        },
        {
            "authors": [
                "A. Mittal",
                "A.K. Moorthy",
                "A.C. Bovik"
            ],
            "title": "Blind/referenceless image spatial quality evaluator",
            "venue": "ASILOMAR.",
            "year": 2011
        },
        {
            "authors": [
                "R. M\u00fcller",
                "S. Kornblith",
                "G.E. Hinton"
            ],
            "title": "When does label smoothing help? NeurIPS",
            "year": 2019
        },
        {
            "authors": [
                "Y. Nakamura",
                "Y. Ishii",
                "Y. Maruyama",
                "T. Yamashita"
            ],
            "title": "Few-shot adaptive object detection with cross-domain cutmix",
            "venue": "ACCV.",
            "year": 2022
        },
        {
            "authors": [
                "D.T. Nguyen",
                "H.G. Hong",
                "K.W. Kim",
                "K.R. Park"
            ],
            "title": "Person recognition system based on a combination of body images from visible light and thermal cameras",
            "venue": "Sensors. Multidisciplinary Digital Publishing",
            "year": 2017
        },
        {
            "authors": [
                "S. Poria",
                "E. Cambria",
                "R. Bajpai",
                "A. Hussain"
            ],
            "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
            "venue": "(Vol. 37, pp. 98\u2013125). Elsevier.",
            "year": 2017
        },
        {
            "authors": [
                "A. Rahate",
                "R. Walambe",
                "S. Ramanna",
                "K. Kotecha"
            ],
            "title": "Multimodal co-learning: challenges, applications with datasets, recent advances and future directions",
            "venue": "Elsevier.",
            "year": 2022
        },
        {
            "authors": [
                "S. Raschka"
            ],
            "title": "Model evaluation, model selection, and algorithm selection in machine learning",
            "venue": "arxiv:1811.12808.",
            "year": 2018
        },
        {
            "authors": [
                "E. Ristani",
                "C. Tomasi"
            ],
            "title": "Features for multi-target multi-camera tracking and reidentification",
            "year": 2018
        },
        {
            "authors": [
                "E. Rusak",
                "L. Schott",
                "R.S. Zimmermann",
                "J. Bitterwolf",
                "O. Bringmann",
                "M. Bethge",
                "W. Brendel"
            ],
            "title": "A simple way to make neural networks robust against diverse image corruptions",
            "venue": "ECCV.",
            "year": 2020
        },
        {
            "authors": [
                "P.C. Sen",
                "M. Hajra",
                "M. Ghosh"
            ],
            "title": "Supervised classification algorithms in machine learning: A survey and review",
            "venue": "Emerging technology in modelling and graphics: Proceedings of IEM graph 2018.",
            "year": 2020
        },
        {
            "authors": [
                "C. Sharma",
                "S.R. Kapil",
                "D. Chapman"
            ],
            "title": "Person re-identification with a locally aware transformer",
            "venue": "arxiv:2106.03720.",
            "year": 2021
        },
        {
            "authors": [
                "C. Shorten",
                "T.M. Khoshgoftaar"
            ],
            "title": "A survey on image data augmentation for deep learning",
            "venue": "Journal of big data. SpringerOpen",
            "year": 2019
        },
        {
            "authors": [
                "C.G. Snoek",
                "M. Worring",
                "A.W. Smeulders"
            ],
            "title": "Early versus late fusion in semantic video analysis",
            "venue": "ICMI.",
            "year": 2005
        },
        {
            "authors": [
                "V. Somers",
                "C. De Vleeschouwer",
                "A. Alahi"
            ],
            "title": "Body part-based representation learning for occluded person reidentification",
            "venue": "WACV.",
            "year": 2023
        },
        {
            "authors": [
                "A. Stylianou",
                "R. Souvenir",
                "R. Pless"
            ],
            "title": "Visualizing deep similarity networks",
            "venue": "WACV.",
            "year": 2019
        },
        {
            "authors": [
                "L. Su",
                "C. Hu",
                "G. Li",
                "D. Cao"
            ],
            "title": "Msaf: Multimodal split attention fusion",
            "venue": "arxiv:2012.07175.",
            "year": 2020
        },
        {
            "authors": [
                "L. Sun",
                "B. Liu",
                "J. Tao",
                "Z. Lian"
            ],
            "title": "Multimodal cross-and self-attention network for speech emotion recognition",
            "venue": "ICASSP.",
            "year": 2021
        },
        {
            "authors": [
                "C. Szegedy",
                "V. Vanhoucke",
                "S. Ioffe",
                "J. Shlens",
                "Z. Wojna"
            ],
            "title": "Rethinking the inception architecture for computer vision",
            "venue": "CVPR.",
            "year": 2016
        },
        {
            "authors": [
                "G. Wang",
                "T. Zhang",
                "J. Cheng",
                "S. Liu",
                "Y. Yang",
                "Z. Hou"
            ],
            "title": "Rgb-infrared cross-modality person re-identification via joint pixel and feature alignment",
            "venue": "ICCV.",
            "year": 2019
        },
        {
            "authors": [
                "X. Wang",
                "K. Shu",
                "H. Kuang",
                "S. Luo",
                "R. Jin",
                "J. Liu"
            ],
            "title": "The role of spatial alignment in multimodal medical image fusion using deep learning for diagnostic problems",
            "venue": "ICIMH.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Wang"
            ],
            "title": "Survey on deep multi-modal data analytics: Collaboration, rivalry, and fusion",
            "venue": "TOMM. ACM New York, NY.",
            "year": 2021
        },
        {
            "authors": [
                "Z. Wang",
                "C. Li",
                "A. Zheng",
                "R. He",
                "J. Tang"
            ],
            "title": "Interact, embed, and enlarge: Boosting modality-specific representations for multi-modal person re-identification",
            "venue": "Proceedings of the AAAI conference on artificial",
            "year": 2022
        },
        {
            "authors": [
                "Z. Wang",
                "Y. Zheng",
                "Chuang",
                "Y.-Y",
                "S. Satoh"
            ],
            "title": "Learning to reduce duallevel discrepancy for infrared-visible person",
            "year": 2019
        },
        {
            "authors": [
                "X. Wei",
                "T. Zhang",
                "Y. Li",
                "Y. Zhang",
                "F. Wu"
            ],
            "title": "Multi-modality cross attention network for image and sentence matching",
            "venue": "CVPR.",
            "year": 2020
        },
        {
            "authors": [
                "A. Wu",
                "Zheng",
                "W.-S.",
                "Yu",
                "H.-X.",
                "S. Gong",
                "J. Lai"
            ],
            "title": "Rgb-infrared cross-modality person re-identification",
            "venue": "ICCV.",
            "year": 2017
        },
        {
            "authors": [
                "Q. Xie",
                "Luong",
                "M.-T.",
                "E. Hovy",
                "Q.V. Le"
            ],
            "title": "Self-training with noisy student improves imagenet classification",
            "venue": "CVPR.",
            "year": 2020
        },
        {
            "authors": [
                "N. Xu",
                "W. Mao",
                "P. Wei",
                "D. Zeng"
            ],
            "title": "Mda: Multimodal data augmentation framework for boosting performance on sentiment/emotion classification tasks",
            "venue": "IEEE intelligent systems. IEEE.",
            "year": 2020
        },
        {
            "authors": [
                "K. Xuan",
                "L. Xiang",
                "X. Huang",
                "L. Zhang",
                "S. Liao",
                "D. Shen",
                "Q. Wang"
            ],
            "title": "Multimodal mri reconstruction assisted with spatial alignment network",
            "venue": "TMI. 10.1109/ TMI.2022.3164050",
            "year": 2022
        },
        {
            "authors": [
                "M. Ye",
                "X. Lan",
                "Z. Wang",
                "P.C. Yuen"
            ],
            "title": "Bidirectional center-constrained top-ranking for visible thermal person re-identification",
            "venue": "TIFS. IEEE.",
            "year": 2019
        },
        {
            "authors": [
                "M. Ye",
                "J. Shen",
                "G. Lin",
                "T. Xiang",
                "L. Shao",
                "S.C. Hoi"
            ],
            "title": "Deep learning for person re-identification: A survey and outlook",
            "venue": "TPAMI.",
            "year": 2021
        },
        {
            "authors": [
                "S.S.A. Zaidi",
                "M.S. Ansari",
                "A. Aslam",
                "N. Kanwal",
                "M. Asghar",
                "B. Lee"
            ],
            "title": "A survey of modern deep learning based object detection models",
            "venue": "DSP. Elsevier.",
            "year": 2022
        },
        {
            "authors": [
                "Q. Zhang",
                "C. Lai",
                "J. Liu",
                "N. Huang",
                "J. Han"
            ],
            "title": "Fmcnet: Feature-level modality compensation for visible-infrared person re-identification",
            "venue": "CVPR.",
            "year": 2022
        },
        {
            "authors": [
                "S. Zhang",
                "S. Zhang",
                "T. Huang",
                "W. Gao",
                "Q. Tian"
            ],
            "title": "Learning affective features with a hybrid deep model for audio\u2013visual emotion recognition",
            "venue": "TCSVT. IEEE.",
            "year": 2017
        },
        {
            "authors": [
                "A. Zheng",
                "Z. Wang",
                "Z. Chen",
                "C. Li",
                "J. Tang"
            ],
            "title": "Robust multi-modality person re-identification",
            "venue": "Proceedings of the AAAI conference on artificial intelligence.",
            "year": 2021
        },
        {
            "authors": [
                "Z. Zhong",
                "L. Zheng",
                "G. Kang",
                "S. Li",
                "Y. Yang"
            ],
            "title": "Random erasing data augmentation",
            "venue": "Proceedings of the AAAI conference on",
            "year": 2020
        },
        {
            "authors": [
                "Z. Zou",
                "Z. Shi",
                "Y. Guo",
                "J. Ye"
            ],
            "title": "Object detection in 20 years: A survey",
            "venue": "arxiv:1905.05055.",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "Keywords: Deep Neural Networks, Multimodal Fusion, Corrupted Images, Data Augmentation, Visual-Infrared Person Re-Identification.\n1\nar X\niv :2\n30 5.\n00 32\n0v 1"
        },
        {
            "heading": "1 Introduction",
            "text": "Real-world video monitoring and surveillance applications (e.g., recognizing individuals in airports, and vehicles in traffic) are challenging problems that rely on object detection (Zaidi et al., 2022; Zou et al., 2019), tracking (W. Luo et al., 2021), classification (Sen et al., 2020), and reidentification (ReID) (Khan & Ullah, 2019; Ye et al., 2021). Person ReID aims to recognize individuals over a set of distributed non-overlapping cameras. State-of-art ReID systems based on, e.g., deep Siamese networks (Fu et al., 2021; Sharma et al., 2021; Somers et al., 2023), typically learn an embedding through various metric learning losses, which seeks to make image pairs with the same identity closer, and image pairs with different identities more distant in the embedding space. Despite the recent advances with deep learning (DL) models, person ReID remains a challenging task due to the non-rigid structure of the human body, the different viewpoints/poses with which a person can be observed, image corruption, and the variability of capture conditions (e.g., illumination, scale, contrast) (Bhuiyan et al., 2020; Mekhazni et al., 2020).\nVisible-infrared (V-I) person ReID aims to recognize individuals of interest across a network of RGB and IR cameras. Unlike visible cameras, infrared ones allow night-time recognition. This has motivated research on cross-modal recognition, to provide methods for V-I person ReID from night-time to day-time, or vice-versa (Ye et al., 2021). In addition, a V-I person ReID approach has been proposed for a multimodal recognition (Nguyen et al., 2017), where the I modality is used in conjunction with the V, improving accuracy due to its different data encoding and perception under low light conditions. In fact, a V-I ReID can allow training a single model remains accurate over diverse capture conditions. A VI ReID model should however conserve modality specific-features instead of focusing mostly on modality-shared ones (Baltrus\u030caitis et al., 2018), which is often absent, or not explicitly addressed by state-of-art approaches. Furthermore, RGB and IR cameras may be co-located (CL) or not co-located (NCL), and variation in camera configuration affects the spacial alignment of V-I images, which is likely influencing ReID (as it is known to\nimpact other tasks) (X. Wang et al., 2021; Xuan et al., 2022).\nArtificially corrupted datasets (M. Chen et al., 2021; Hendrycks & Dietterich, 2019; Michaelis et al., 2019) are important for evaluating V-I person ReID models, yet public datasets are often collected in controlled environments that cannot cover the range of real-world scenarios (Poria et al., 2017). As highlighted by Rahate et al. (2022), there is a need to create multimodal real-world datasets that contain corrupted modalities. Apart from the recent approach using corrupted audiovisual data in emotion recognition Hong et al. (2023), the V-I ReID evaluation set proposed in our preliminary work (Josi et al., 2023) is, to our best knowledge, the only existing dataset for corrupted evaluation for visual multimodal learning. However, the dataset in (Josi et al., 2023) is only evaluated for a simple architecture, and does not consider the correlation in the corruption from one camera to another. For example, corruption due to weather conditions should similarly occur on a V-I pair from co-located V-I cameras.\nNeglecting to evaluate ReID models on corrupted data can result in large and unexpected performance gaps at deployment. To reduce this gap, one can attempt to restore corrupted input images during test time (Chang et al., 2020), at the expense pipeline complexity by restoring the data before proceeding to the main ReID task. Using more complex DL models has been shown to improve performance on corrupted image data in object detection (Michaelis et al., 2019) and image classification (Xie et al., 2020). For instance, vision transformer models (Han et al., 2020) have been shown some robustness to image corruption (Hendrycks et al., 2020). In particular, the TransReID model S. He et al. (2021) provides state-of-art person ReID performance when facing corrupted data (M. Chen et al., 2021). However, such complex models limit the potential for realtime ReID applications. Using more diverse training data can improve the robustness of deep ReID models to corrupted data (Xie et al., 2020), and does not increase the model\u2019s complexity at test time. Data augmentation (Shorten & Khoshgoftaar, 2019) also avoids the costs of data collection and annotation.\nThis paper focuses on the following research questions. How can efficient V-I ReID models be developed considering CL or NCL scenarios?\nHow can these V-I models be trained, thanks to augmented multimodal data, to provide better robustness to real-world image corruptions than state-of-art models like TransReID? In this paper, a cost-effective V-I ReID model named Multimodal Middle Stream Fusion (MMSF) is proposed to explicitly preserve and exploit both modalityspecific and modality-shared knowledge, thereby improving robustness to corrupted images. In addition, three state-of-art attention-based models are adapted from the areas of sentiment analysis, emotion recognition, and action recognition for similarity matching, as needed for person ReID. Attention approaches are expected to address image corruptions through a dynamic feature selection, dealing with the varying availability of modality information. However, these models mainly focus on modality-shared features, eventually losing some capacity to discriminate.\nEssential for the evaluation of both multimodal and cross-modal V-I person ReID models, corrupted V-I datasets are proposed for uncorrelated and correlated cases, named respectively uncorrelated corrupted dataset (UCD) and correlated corrupted dataset (CCD). These two sets allow for a robust evaluation of models based on 20 V and 19 I different corrupted conditions. Improving from our preliminary work, corruptions are correlated or not to suit NCL and CL camera configurations. In our experiments, we validate ReID models using clean and corrupted versions of the SYSU-MM01 (Wu et al., 2017) (NCL), RegDB (Nguyen et al., 2017) (CL), and ThermalWORLD (Kniaz et al., 2018) (CL) datasets. Our preliminary work in (Josi et al., 2023) introduced the Masking and Local Multimodal Data Augmentation (ML-MDA) strategy that improves the accuracy and robustness to strong corruptions using simple fusion architecture. The strategy is further assessed in this paper, and expected to train models leveraging the complementary knowledge among modalities while dynamically balancing the importance of individual modalities in final predictions. Main contributions: (1) A novel MMSF architecture is proposed for V-I ReID that allows preserving both modalityspecific and -shared features. This aspect is shown to be essential for both CL and NCL settings but is not addressed most of the time. Additionally, three state-of-art attention-based models are\nadapted to similarity matching, and evaluated for V-I person ReID. These models are detailed in Section 3. (2) For realistic evaluation of V-I person ReID models, challenging UCD and CCD datasets are designed (see Section 4). (3) The ML-MDA strategy presented Section 5 is introduced for training DL models for V-I ReID multimodal that are robust to corruption. (4) Our empirical results (see Section 6) on clean and corrupted versions of the challenging SYSUMM01, RegDB, and ThermalWORLD datasets provides insight about cost-effective DL models to adopt for V-I ReID, and their dependency on dataset properties and CL/NCL scenarios. Results also indicate that our V-I ReID models can outperform TransReID and related state-ofart models on clean and corrupted data in terms of accuracy and complexity."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Multimodal fusion",
            "text": "Fusion approach and spatial alignment. To better handle or analyze a given problem, not being restricted to a single source of information is usually a powerful strategy (Baltrus\u030caitis et al., 2018; Y. Wang, 2021). As well-known approaches, one can think of late (Snoek et al., 2005) or sensor (Lohweg & Mo\u0308nks, 2010) fusions. The former considers independent learning and feature extraction for each modality before making a decision. Such fusions are easy to implement, as models can be trained independently and added to a system through minor adjustments. However, a model cannot learn the correlation between the modalities (S. Zhang et al., 2017), like spatially related information. The latter (i.e., sensor fusion) stacks modalities together before any feature extraction, allowing inter-modality correlations to be mined and used by the model but considerably increasing the input dimension. Also, no spatial alignment may make modality correlations harder to find by the model (X. Wang et al., 2021).\nIntermediate or model-level fusion techniques consider fusing modalities during the feature extraction and before the decision layer (Baltrus\u030caitis et al., 2018), increasing the semantic information contained in features before fusion and eventually making correlations easier to find.\nHowever, where spatial information continuously disappears through the network (L.-C. Chen et al., 2018), it is unclear how much remains at each step and how it may impact a model. From experiments provided by X. Wang et al. (2021) on fusion location and data alignment, it is important to differentiate spatially aligned and unaligned data as models may have really distinct behaviors. Model level fusion. Model-level fusion considers fusing modality representations of a deep learning model somewhere in between the sensor representations and the feature vectors. Coordinated modality representation is seen by Baltrus\u030caitis et al. (2018) as a challenging but promising fusion direction for model-level fusion approaches. Exchanging modality knowledge allows it and seems very practical as correlations may be mined by a model and as one modality may be more or less informative. However, they raise the models\u2019 lack of ability to conserve supplementary information and not only exploit complementary information.\nIn practice, attention-based multimodal approaches allow modality knowledge exchange, as it is the case for the Multimodal Transfer Module (MMTM) proposed by Joze et al. (2020). The module refactors the channels of each modality regarding how the intra- and inter-modality channels correlate. Based on the MMTM concept and inspired by H. Zhang et al. (2020) that used the split operation to improve the dynamic channel selection, Su et al. (2020) presented the MSAF approach. The dynamic channel refactoring in such multimodal models may allow for fine-grained feature selection and limit corruption impact. Unlike previous approaches, modality attention Gu et al. (2018), later updated by Ismail et al. (2020), provides soft attention weights for each modality to balance modality importance in the final embedding based on their discriminating capabilities. Again, such attention sounds to be a great approach to tackling punctually corrupted data. However, those attention models do not explicitly work at conserving the modalityspecific knowledge, missing the point raised by Baltrus\u030caitis et al. (2018).\nSome transformers architectures tackle this aspect, conserving modality-specific knowledge through modality-specific streams and selfattention, and modality-shared knowledge thanks to modality-shared streams and cross-attention\n(Lian et al., 2021; Sun et al., 2021; Wei et al., 2020). However, transformer architectures are known to be complex and heavy Han et al. (2020), which do not align with video-surveillance challenges, requiring close to real-time algorithms. Multimodal person ReID. Most approaches for person ReID (Ye et al., 2021) focus on the unimodal (RGB) (H. Luo, Gu, et al., 2019; Ristani & Tomasi, 2018) and cross-modal (Alehdaghi et al., 2022; Ye et al., 2021; Q. Zhang et al., 2022) settings. Few only focused on combining multimodal information. For example, J. Chen et al. (2019) used the contour information. Bhuiyan et al. (2020) used pose information. However, for those approaches, the additional modality is built from the exploitation of the main modality, which would be similarly affected by image corruption and consequently not so helpful in this regard.\nUsing another sensor to extract a supplementary modality allows to have a distinct encoding, likely differently affected by corruptions. For example, the infrared and near-infrared are shown to be beneficial for person ReID (Z. Wang et al., 2022; Zheng et al., 2021), but leveraging the knowledge from three modalities might not be realistic for a real-world surveillance setting, asking for large models architectures.\nNguyen et al. (2017) represents the only approach where visible and infrared modalities only are integrated into a joint representation space. Infrared and visual features are concatenated, produced from independently trained CNNs, and used for pairwise matching at test time. This simple model attained an impressive performance on the RegDB dataset. However, RegDB data is captured with only one camera per modality, RGB-IR cameras are co-located with only a single tracklet of ten images per modality and individual, and except for their low resolutions, captured images present no specific corruptions. For these reasons, the RegDB dataset is less consistent with a real-world scenario. In fact, the development of person ReID models that are effective in uncontrolled real-world scenarios remains an open problem (Hendrycks et al., 2021)."
        },
        {
            "heading": "2.2 Image corruption and augmentation strategies",
            "text": "Data augmentation (DA) consists in multiplying the available training dataset by punctually\napplying transformations on training images, like flips, rotations, and scaling (Ciregan et al., 2012). This way, a model usually benefits from increased robustness to image variations and improved generalization performance. According to Geirhos et al. (2018), training a model on a given corruption is only sometimes helpful over other types of degradation. Yet, Rusak et al. (2020) showed that a well-tuned DA can help the model to perform well over multiple types of image corruption through Gaussian and Speckle noise augmentation. Hendrycks et al. (2019) proposed the Augmix strategy, for which multiple variations of an image are obtained through randomly applied transformations, variations that get mixed together. Random Erasing occludes parts of the images punctually by replacing pixels with random values (Zhong et al., 2020). Previous strategies allow a large variety of augmented images, simulating eventually real-world data and hence inducing higher generalization performance.\nFocusing on person ReID, M. Chen et al. (2021) proposed the CIL learning strategy to improve systems performance under corrupted data. Their strategy is partly based on two local DA methods \u2013 self-patch mixing and soft random erasing. The former replaces some of the pixels in a patch with random values, while the latter superposes a randomly selected patch from an image at a random position on this same image. Gong et al. (2021) show interesting improvements through local and global grayscale patch DA on RGB images. However, the previous strategies are limited to single modality stream models, even though the latter shows how grayscale data may reinforce the visible modality features using DA.\nMultimodal data augmentation strategies have presented encouraging results for image-text emotion recognition (Xu et al., 2020) or visionlanguage representation learning (Hao et al., 2022). Also, Nakamura et al. (2022) proposed a visible-thermal cross-domain DA for few shots object thermal detection, working at closing the domain gap by augmenting data through hetero modality objects added on the main modality images. However, to our best knowledge, our preliminary work (Josi et al., 2023) is the first to propose MDA with V-I person ReID applications through ML-MDA. Still, this MDA has only been investigated on a simple fusion model, which does not assure its generalization to more developed\nfusion architectures. Also, the evaluation is limited to corruptions set that do not consider eventual correlations between corruptions for NCL or CL cameras, which is tackled in this work."
        },
        {
            "heading": "3 Multimodal Fusion for V-I ReID",
            "text": "The main objective of our study is to find how modalities should be fused to be robust to data corruption while conserving great performances on clean data. Hence, plural multimodal models are studied, all trained and evaluated following a pairwise matching scheme (Fig. 1).\nFrom our preliminary work (Josi et al., 2023), the learned concatenation model is now used as a baseline, referred to as Baseline C. Baseline S stands as our second baseline with the same architecture but an element-wise sum fusion of the feature vectors instead of a concatenation.\nThe selection of modality-shared and modality-specific features remains unclear in most models, whereas the importance of the conservation of both feature types has been highlighted by Baltrus\u030caitis et al. (2018). Hence, the multimodal middle stream fusion (MMSF) is proposed and first presented. Three attention-based models follow as attention should handle corruption well through a dynamic feature selection regarding each input. Still, the attention could also allow a modality corruption to degrade the hetero modality and require investigation. The three models are extracted from the literature and specially adapted to pairwise matching and, more precisely, to the person ReID task.\nFusion approaches are not restricted to a specific backbone, but ResNet-18 (K. He et al., 2016) backbones are used for illustration purposes. Each model is optimized using the batch hard triplet loss (Hermans et al., 2017) LBH tri, and crossentropy with regularization via label smoothing (Szegedy et al., 2016) LCE ls. We follow the usual optimization process (Ye et al., 2021), except for the cross-entropy. Indeed, regularization via label smoothing is used by M. Chen et al. (2021) is is better at addressing corruption.\nV-T Anchor\nMultimodal CNN Backbone\nSimilarity Matching\nMultimodal CNN\nBackbone\nV-T Negative\nShared Weights\nV-T Positive\n/\nV-T Query\nMultimodal CNN Backbone ltimodal CNN ackbone\nSimilarity Matching\nV-T Gallery\nultimodal CNN\nBackbone\n(a) Training (b) Testing\nFig. 1: Representation of the multimodal person ReID (a) learning while using the triplet loss and (b) inference."
        },
        {
            "heading": "3.1 Multimodal middle stream fusion",
            "text": "Assuring the conservation of the modality supplementary information, while taking advantage of the modality-shared information, we propose the Multimodal Middle Stream Fusion (MMSF).\nThe model comprises two independent modality-specific CNN streams focused on the modality-specific information and a middle CNN stream that exploits the modality-shared information (Fig. 2). Each stream is independent and optimized through its specific loss functions, allowing it not to influence a stream representation from direct knowledge exchanges among streams. FlV \u2208 RH\u00d7W\u00d7C and FlI \u2208 RH\u00d7W\u00d7C are the visible and infrared feature maps before convolution blocks l \u2208 N. For a fusion before layer l, the middle stream takes Fm = F l V + F l I as input and pursues the feature extraction from this fused representation. Its middle stream size varies regarding l value, being a partial backbone starting at layer l."
        },
        {
            "heading": "3.2 Attention-based models",
            "text": ""
        },
        {
            "heading": "3.2.1 Modality attention network",
            "text": "Modality Attention Network (MAN) (Gu et al., 2018) is an attention-based multimodal approach that dynamically weights feature vectors from each modality before fusing them. This model\nseems meaningful to explore as the dynamic weighting of each modality feature vector should help handle corrupted data. Since the model architecture has been adapted for our person ReID study, its architecture is presented in Fig. 3.\nTwo backbones first extract each visible fV \u2208 Rd and infrared fI \u2208 Rd modality features, with d \u2208 R. The obtained vectors are concatenated and passed through a modality attention module, which learns to generate soft attention weights. The soft weights allow the model to give more importance to the discriminant modality features in the final embedding. To do so, the concatenation of the two embeddings goes through two dense layers and a final softmax \u03c3 regression, which produces the soft weights SV \u2208 R for the visible and SI \u2208 R for the infrared modalities. Soft weights are produced as follows:\n[SV,SI] = \u03c3(W2tanh(W1[fV, fI] T +b1)+b2) (1)\nwhere W1 \u2208 Rk\u00d7d and W2 \u2208 R1\u00d7k are weight matrix, k \u2208 R being an hyper-parameter, b1 \u2208 Rk\u00d72, and b2 \u2208 R1\u00d72 are biases.\nThanks to soft attention weights, visible and infrared original features are then weighted, respectively noted fV w and fI w. For the visible modality, fV w = SV \u00d7 fV, and for the infrared modality, fI w = SI\u00d7fI. Then, the predicted output vector y\u0302 is computed by passing the concatenation\nor the element-wise sum of the fV w and fI w vectors through a final softmax layer for classification.\nAs a consequence of the CL and NCL camera scenarios and the induced spatial alignment, which might influence the feature vector\u2019s composition, we also consider the element-wise sum fusion of the feature vectors in this work. Concatenation conserves each feature definition while fusing, but doubles the feature vector dimension. Summation makes the fused vector of the original feature vector size but may erase knowledge if the embedded concepts are not aligned."
        },
        {
            "heading": "3.2.2 Multimodal transfer module",
            "text": "The Multimodal Transfer Module (MMTM) (Joze et al., 2020) is an approach that focuses on channel attention to refactor the feature maps from two or more modality CNN streams regarding the spatial statistics of each. As the refactoring is done dynamically and based on the statistics of each given input, such attention should also be helpful while facing corrupted data. Two similar backbones are used to extract the features from each V and I representation. Two modules\nare used for our architecture (Fig. 4), after the third and the fourth convolution blocks, allowing for intermediate and high-level feature refactoring. For a given layer l \u2208 N, the visible and the infrared modality feature maps are respectively noted FlV \u2208 RH\u00d7W\u00d7C and FlI \u2208 RH\u00d7W\u00d7C , with H \u2208 R, W \u2208 R and C \u2208 R being respectively the feature maps height, width and channel size. The feature map from each stream is first squeezed with a global average pooling layer over the spatial dimension, leading to two linear vectors of channel descriptors. Those vectors are concatenated and passed through a dense layer, following equation (2), to obtain the joint representation Jl \u2208 RCJ .\nJl = W([AvgPool(FlV); AvgPool(F l I)]) + b (2)\nwhere W \u2208 RCJ\u00d7C2 is a weight matrix, b \u2208 RCJ the bias of the dense layer, and CJ = C\n2/4 to limit the model capacity and increase the generalization power (Joze et al., 2020). Then, an excitation signal is produced with a distinct dense and softmax activation layer applied for each modality to the shared channel descriptor Jl. Finally, this excitation signal is broadcasted through the spatial dimension for each modality with an elementwise product, following equations (3), forming the final weighted feature maps FlV\nw \u2208 RH\u00d7W\u00d7C and FlI w \u2208 RH\u00d7W\u00d7C .\nFlV w = 2\u00d7 \u03c3(WVJl + bV) FlV FlI w = 2\u00d7 \u03c3(WIJl + bI) FlI\n(3)\nwhere WV \u2208 RC\u00d7CJ and WI \u2208 RC \u2032\u00d7CJ are weight matrix and bV \u2208 RC , bI \u2208 RC the bias of\nthe dense layers. \u03c3 stands for the sigmoid function. The element-wise product is represented by ."
        },
        {
            "heading": "3.2.3 Multimodal split attention fusion",
            "text": "The Multimodal Split Attention Fusion module (MSAF) proposed by Su et al. (2020) also works from the channel attention principle. Modules are applied at the same locations for this model (Fig. 4). Let us describe the MSAF module. First, the visible and infrared feature maps FlV \u2208 RH\u00d7W\u00d7C and FlI \u2208 RH\u00d7W\u00d7C are split into n \u2208 R visible and infrared sub feature maps, respectively noted SlV \u2208 RH\u00d7W\u00d7 C n and SlI \u2208 RH\u00d7W\u00d7 C n . The n splits from each modality are element-wise summed and fed to a global average pooling layer to get a global channel descriptor per modality noted JlV \u2208 R C n and JlI \u2208 R C n . Then, the global channel descriptor from each modality is element-wise summed and passed through a dense layer, followed by a batch normalization and a ReLU activation to catch the inter-channel correlations, forming the common channel descriptor Jl \u2208 RCn . From Jl, n excitation signals are produced per modality, using a dense layer and a softmax activation on Jl for each original feature map split. These excitation signals are then broadcasted through the spatial dimension for each split with an element-wise product, following equations 4, forming the final weighted splits SlVi w \u2208 RH\u00d7W\u00d7Cn and SlIi w \u2208 RH\u00d7W\u00d7Cn .\nSlVi w = \u03c3(WViJ l V + bVi) SlVi\nSlIi w = \u03c3(WTiJ l I + bTi) SlTi\n(4)\nThe n excited splits are concatenated together for each modality to get the final weighted feature\nmaps FlVi w \u2208 RH\u00d7W\u00d7C and FlIi w \u2208 RH\u00d7W\u00d7C . One can notice that the model needs fewer parameters than the MMTM approach, thanks to the feature map splits."
        },
        {
            "heading": "4 Corrupted Datasets",
            "text": "To better simulate real-world conditions while evaluating a model, the focus has been on corrupted test sets over the last few years (M. Chen et al., 2021; Hendrycks & Dietterich, 2019; Michaelis et al., 2019). However, those benchmark test sets were proposed for single modality settings, whereas our objective is to evaluate the value of V-I multimodal models. As both the V and the I modalities encode from visual cues, corruptions that affect the visual modality may also affect the infrared modality, such as occlusions or weatherrelated corruptions. From this observation, the 20 visible corruptions from M. Chen et al. (2021) are extended to the infrared domain in this work, allowing us to provide two corrupted datasets. Those two datasets are suited to the co-located (CL) and the not co-located (NCL) settings.\nIn the following sections, the three clean datasets are first detailed. A presentation of the used modality corruptions follows. Finally, our two corrupted datasets are detailed."
        },
        {
            "heading": "4.1 Clean datasets",
            "text": "The three used datasets present distinct statistics (Tab. 1) suited to build and draw a strong study. SYSU-MM01 (Wu et al., 2017) gather 4 visible and 2 infrared cameras, with 491 distinct individuals, 29033 RGB, and 15712 IR images. The V and I cameras are not co-located, so the scene\u2019s spatial\ndescription varies from one modality to another for a given V-I image pair. RegDB (Nguyen et al., 2017) is a much smaller dataset, with one camera only per modality, the V and I cameras being co-located. A single 10 images tracklet is available per identity and camera. Hence, RegDB 412 identities lead to 4120 images per modality. ThermalWorld1 (Kniaz et al., 2018) has only its training part available, leading us to 409 distinct identities. 16 co-located cameras per modality captured each 8125 image. However, the infrared images are of terrible quality, with a BRISQUE (Mittal et al., 2011) value of 60.25, much higher than RegDB and SYSU-MM01 ones, being at 38.81 and 40.52 respectively.\n1Download link obtained from github ThermalGAN issues."
        },
        {
            "heading": "4.2 Modality corruptions",
            "text": "M. Chen et al. (2021); Hendrycks and Dietterich (2019) used 20 corruptions of the visual modality, which were regrouped into four distinct types - noise, weather, blur, and digital. In this work, the used corruptions are the same for the visual modality. However, the I modality can also be affected by multiple corruptions, which is considered. In fact, 19 of the corruptions affecting the visual modality can also apply to the infrared with a few slight adjustments (Corruptions taxonomy figure and corruptions adjustments table in the appendix 1.A).\nFirst, the current luminosity does not impact the IR modality, so brightness corruption is not used for this modality. Then, different noises, like Gaussian, Shot, Impulse, and Speckle, are applied similarly, except each noise is turned into grayscale values to respect the infrared modality single color channel encoding. Spatter and frost are two other corruptions that needed to be grayscaled before being applied to the infrared images. Indeed, bluecolored water or brown-colored dirt was applied for spatter, and frozen blue masks for frost. As a last adjustment, the saturation is expressed differently for the I modality, visually brightening the object of interest eventually if this one is too close to the camera, instead of modeling color intensity for the visual modality. Finally, all other corruptions were applied similarly for the V and I modalities."
        },
        {
            "heading": "4.3 Uncorrelated corruption dataset",
            "text": "The Uncorrelated Corruption Dataset (UCD) is proposed as a first way to evaluate the models\u2019 corruption robustness. To build UCD, the corruptions are randomly and independently selected and applied on each modality for a given V-I test pair, making it highly challenging. The camera\ncorruption independence from V to I modality is suited for a NCL camera setting, as it is the case for SYSU-MM01. Indeed, for example, a visible indoor and an outdoor infrared camera would lead to weather appearing only on the infrared camera or to blur, impacting one camera only while the other is impacted independently. As applied corruptions are most of the time distinct from one modality to another under UCD, it should allow each modality to compensate for the corrupted features from the other. Hence, this setting should be a great way to evaluate the models\u2019 ability to select the information of interest from one or another modality. !"
        },
        {
            "heading": "4.4 Correlated corruption dataset",
            "text": "One can expect some corruption to be correlated from one camera to another, corruption type-wise as intensity-wise. As a brief example, the rain is expected to appear on both visible and infrared cameras simultaneously, especially if those are co-located. However, some other types of\ncorruption, such as image saturation, are camera dependent and would happen punctually on one camera with no correlation with the other. The CCD dataset is proposed from these observations, suited for CL cameras and gathering the following characteristics (Tab. 2).\nAt first, weather-related corruptions such as fog, rain, frost, and snow appear much correlated, so the weather from one camera is assumed to appear with the same level of corruption on the other. Spatter expresses the water or dirt splashes on the cameras, which has a great chance to happen on both cameras considering co-located cameras, but with a level that might differ; the level is selected randomly and independently. Similar behaviors for blur-related corruptions would also make sense in real-world conditions if cameras are co-located since those corruptions are a consequence of camera settings, like exposure time or focus, for example, but which also mostly depends on the current scene. Because each modality\ncamera might be more or less reactive regarding the situation, we consider that blur-related corruptions (i.e., defocus, gaussian, glass, zoom, motion blurs) affect the two modalities simultaneously with an intensity level that can differ. The intensity level is randomly and independently selected except for motion blur corruption. Indeed, infrared cameras usually have a higher exposure time than visible cameras, making those more affected by motion blur. Consequently, the level is always selected as equal or superior for the infrared modality compared to the visible one.\nConcerning the ten remaining corruptions, those are much related to data encoding and can affect visible or infrared cameras independently. The hetero-modality is consequently corrupted if the selected corruption lies in the correlated corruptions. Otherwise, we randomly apply another corruption among the uncorrelated ones to the hetero-modality. Considering modalities as always corrupted is an extreme scenario, which is attractive to frame models\u2019 behaviors but not entirely realistic. Hence, the UCD-X dataset is proposed. In this configuration, X% of the corrupted pairs affected by uncorrelated corruptions are formed with one of the two modalities remaining clean. In practice, we fixed it at 50%, but this value can be tweaked to make the datasets more or less challenging for further experiments."
        },
        {
            "heading": "5 Multimodal Data Augmentation",
            "text": "The explored models are based on co-learning, allowing each modality stream to adapt to the other one (Baltrus\u030caitis et al., 2018; Rahate et al., 2022). We propose a new MDA approach, the Masking and Local Multimodal Data Augmentation (ML-MDA), for better learning of the models. In practice, ML-MDA is based on two components: multimodal soft random erasing (MS-REA) and modality masking (Fig. 7). Those two data augmentations are used together during the learning process to make the learned co-learning model robust and accurate in a challenging inference environment."
        },
        {
            "heading": "5.1 Multimodal soft random erasing",
            "text": "Making a multimodal model focus on modalityspecific features is challenging, as the model usually mainly focuses on shared features (Baltrus\u030caitis et al., 2018). Augmenting the multimodal data with local occlusions may help the model to emphasize modality-specific feature importance, as some features will be available only from one or another modality. The soft random erasing (M. Chen et al., 2021) (S-REA) (Fig. 7.a.) uses local occlusions to learn the model not to rely only on the most important features, but consider unimodal learning and consequently not exploit this aspect.\nThe MS-REA data augmentation is proposed to close this gap. Instead of replacing a proportion of the pixels in a given image patch with random pixel values for the visible modality only as S-REA, MS-REA applies a patch on both the visible and infrared modalities. Grayscaled random pixel values are used for patches on the thermal modality to respect the infrared thermal image definition, encoded on one channel, and potentially aligning better with real-world corruptions. The spatial patch location is randomly and independently selected from the visible and infrared images for a given V-I pair. To close the occlusion gap brought by the applied patches through MS-REA, the model must learn how to select each modality feature when partial information is available from each modality. Such behavior is expected to extend well to real-world corruption."
        },
        {
            "heading": "5.2 Modality masking",
            "text": "A modality might be punctually unavailable or primarily uninformative. Thus, the model shall learn how to cancel a modality to reduce its impact on the final prediction. The modality masking approach is expected to allow it by punctually replacing one or another modality with an entirely blank image. Instead of masking the multimodal representation as it has been done by Gabeur et al. (2022), a representation is extracted from the masked input, so the model has to learn how to cancel its influence on the final results. This also forces the model to focus more on modality-specific features since one modality only contains all the meaningful knowledge for ReID. This DA should supposedly complement the previously presented MS-REA approach by balancing each modality\u2019s importance in the final embedding regarding each modality level of corruption, whereas MS-REA should learn the model to select the features within each modality better. MS-REA should also make models\u2019 put more emphasis on the modality-specific features, this time thanks to the independent occlusions locations on each image."
        },
        {
            "heading": "6 Results and Discussion",
            "text": ""
        },
        {
            "heading": "6.1 Experimental methodology",
            "text": "Data division. SYSU-MM01 and RegDB datasets have well-established V-I cross-modal protocols (G. Wang et al., 2019; Z. Wang et al., 2019; Ye et al., 2019), but multimodal protocols were not existing prior to our preliminary work (Josi et al., 2023). Following them again, 395 and 96 identities from SYSU-MM01 are respectively used for the training and the testing set. For RegDB, 412 identities are divided into two identical sets of 206 individuals for learning and testing. The SYSU-MM01 train/test ratio is kept\nfor ThermalWORLD, leading to 325 training identities and 84 for testing. A 5-fold validation (Raschka, 2018) is performed over the data used for training, using folds of respectively 79, 41, and 65 distinct identities for SYSU-MM01, RegDB, and ThermalWORLD. Data augmentation. Our proposed multimodal extensions MS-REA is used with the same appearance augmentation probability as S-REA (M. Chen et al., 2021). Modality Masking is applied randomly on one or another modality, with equiprobability, and occurs with a default probability of 1/8. When used on unimodal models, the CIL (M. Chen et al., 2021) DA is used the same way as the original authors. For the RegDB dataset only, the validation set is given the same DA as the training set as the maximum performances were reached in the early epochs otherwise. This way, better convergence was observed, allowing learning complex cues by the model. Pre-processing. A data normalization is done at first by re-scaling RBG and IR images to 144 \u00d7 288. Random cropping with zero padding and horizontal flips are adopted for base DA. Those parameters were proposed by Ye et al. (2021) on RegDB and SYSU-MM01 datasets. The same normalization is kept under ThermalWORLD for consistency among protocols. Performance measures. The mean Average Precision (mAP), and the mean Inverse Penalty (mINP) are used as performance measures, commonly used for person ReID (Ye et al., 2021). The mAP is the mean computed over all query image ratio of retrieved matches over total matches. However, mAP does not reflect the worst-case scenario, unlike the mINP measure, which applies a penalty on the hardest matches, making it a great complementary measure. Hyperparameters. The hyperparameters values in our models were set based on the default AGW\n(Ye et al., 2021) baseline. The SGD is used for training optimization, combined with a Nesterov momentum of 0.9 and a weight decay of 5e \u2212 4. Our models are trained through 100 epochs. Early stopping is applied based on validation mAP performances. The learning rate is initialized at 0.1 and follows a warming-up strategy (H. Luo, Jiang, et al., 2019). The batch size is 32, with 8 distinct individuals and 4 images per individual. The paired image is selected by default for RegDB and ThermalWORLD. For the SYSU-MM01 dataset, the images from the hetero modality are randomly selected through the available ones to form a pair for a given identity. Losses. The Batch Hard triplet loss (Hermans et al., 2017) LBH tri and the cross-entropy with regularization via Label smoothing (Szegedy et al., 2016) LCE ls are used as loss functions for our models. Indeed, the former is widely used in person ReID approaches (Choi et al., 2020; G. Wang et al., 2019; Ye et al., 2021), so the same margin value is fixed at 0.3, and the latter is part of the CIL implementation (M. Chen et al., 2021). The total loss corresponds to the sum of both losses. The batch hard triplet loss aims at reducing the distance in the embedding space for the hardest positives while increasing the distance for the hardest negatives. The regularization with label smoothing reduces the gap between logits, making the model less confident in predictions and hence improving generalization (Mu\u0308ller et al., 2019). Models details. MMSF is used with l = 4 for NCL and l = 0 for CL cameras (Appendix 1.B). The influence of concatenation or sum of the feature vectors is explored in the Appendix 1.C and allowed to converge to use MMTM S (Sum) and MSAF C (Concatenation) for RegDB, and MMTM C and MSAF S for ThermalWORLD and SYSU-MM01. Leave-one-out query strategy. The LeaveOne-Out Query (LOOQ) strategy, proposed in our preliminary work (Josi et al., 2023), is used the same way in this study. The LOOQ treats the extreme but meaningful case in which one would have only a unique image of the person to ReID and multiple footages containing images of this same person in the gallery. Every pair of images is alternatively used as a probe set while all the other pairs join the gallery. While an interesting evaluation strategy, this also allows us to respect the\noriginal dataset statistics (Tab. 1) by authorizing the number of used gallery images per individual to vary."
        },
        {
            "heading": "6.2 Scenario with not co-located cameras",
            "text": "Not co-located (NCL) V-I cameras imply that a pair of images for a given individual is built from two distinct viewpoints. Consequently, images in a given V-I pair will not be spatially aligned from one modality to another. Having two viewpoints for a given V-I pair should allow more cues and be more discriminant to ReID than a co-located (CL) setting. Indeed, if the person is occluded or partially visible from one camera modality, for example, the hetero-modality camera might have a better view and compensate for the missing features. However, correlations from one modality to another may be harder to find for NCL cameras as the scene appears much different between modalities (X. Wang et al., 2021). For example, the spatial information remaining in the features when the fusion is done may act as noise for the model due to the absence of alignment.\nSince various corruptions can impact either modality, a multimodal model might be disturbed by the supplementary modality and could consequently be less able to ReID than a well-trained single-modal model. The upcoming study is proposed to determine whether or not the multimodal framework is worthwhile given the above statements and to seek the best approach to follow."
        },
        {
            "heading": "6.2.1 Robustness to corruption",
            "text": "Multimodal models are compared while evaluated on each clean, UCD, CCD, and CCD-50 version of the SYSU-MM01 evaluation data. Clean data is important as a reference, observing performances under the best-case scenario. UCD and CCD should complete each other. The former will allow observing how the models can adapt and select information from differently corrupted VI inputs. The latter will present how the models can deal with similarly corrupted inputs, which should make the task harder as it should happen more often that the same features for a given pair get corrupted from V to I. Finally, the CCD-50 should be the easiest evaluation set, with 50% of\nthe pairs having one over two modalities remaining clean. This last set should allow observing if some models better deal with punctual unilateral corruption. A) Natural models corruption robustness.\nTo begin with, the models are trained without any data augmentation technique and evaluated on the original and corrupted versions of SYSU-MM01 (Upper half Tab. 3). The considered unimodal models are fine-tuned ResNet-18 models, trained from visible (unimodal V) or infrared (unimodal I) modality only.\nBefore seeking the models\u2019 robustness to corruption, observing good performance on clean evaluation data is essential. In practice, each multimodal approach improves over the unimodal models. From the unimodal to the multimodal setting, the greatest improvement comes between the unimodal visible and the proposed MMSF approach, improving the mAP and mINP percentile point (PP) by 11, 08 and 39, 77, respectively. The impressive performance improvement shows how the infrared modality and the NCL cameras through clean SYSU-MM01 strongly benefit the multimodal ReID.\nFor each corrupted test set (i.e., UCD, CCD, and CCD-50), as both modalities can impact the ReID either way, one can observe here that each multimodal model (learned without a specific strategy) is less efficient than the unimodal V specialist. Indeed, the unimodal V model reaches 32.16% mAP, followed by MAN at 27.76% mAP for SYSU-MM01-UCD, for example.\nFocusing on the corrupted datasets, the multimodal models globally reach lower performances from UCD to CCD as expected, with, for example, the MMSF model being respectively at 22.23% or 17.70% mAP. From CCD to CCD-50, one can see that some models seem to react better to unilateral corruptions, as the mAP improvement in PP for the proposed MMSF is about 13.45, for MSAF about 12.81, and for Baseline C about 6.89.\nAmong multimodal models, the ranking is inconsistent, from the clean to the corrupted setting. On clean data, the proposed MMSF model presents the highest performances, with mAP about 97.80% and mINP about 80.93%, closely followed by the attention-based MSAF approach,\nreaching 96.77% mAP and 77.27% mINP. On corrupted data, MAN and MSAF appear as better at handling corruption than MMSF.\nFrom there, it would be hard to advise one or another model with the aim of ReID under real-world conditions. Indeed, the evaluated multimodal models are shown not to learn how to select the right modality information in the face of corruption without using a corruption-dedicated learning strategy."
        },
        {
            "heading": "B) DA impact on models robustness.",
            "text": "Performances for the unimodal specialists learned using CIL and the multimodal models learned using ML-MDA are presented in the bottom half of Tab. 3.\nThe CIL use for unimodal models increases the models\u2019 performances on clean data. Indeed, especially for the unimodal infrared model, mAP and mINP are respectively improved by 1.27 PP and 4.97 PP for example. ML-MDA has an impact that is model dependant. Still, most models conserve similar mAPs, except for the baseline sum and MAN that see it considerably improve.\nConsidering corrupted evaluation sets, using DA brings an impressive corruption robustness improvement to every model. The unimodal V model under UCD improves, for example, from 32.16% to 52.37% mAP, or the baseline C model from 24.35% to 61.94% mAP under the CCD set. Similar improvements are happening under each corrupted setting.\nUsing ML-MDA, the multimodal models\u2019 performances are now ahead of the unimodal ones by a strong margin on corrupted datasets. Indeed, the models learn to select information from each corrupted modality way better. Also, its usage brings more consistency from the clean to the corrupted setting, making it essential to handle real-world conditions. For example, the proposed MMSF model was, and remains, the most discriminant approach under clean data but is now the second-best approach on corrupted datasets. In contrast, this one came in the fifth position at best without DA.\nOne may wonder if the multimodal models benefit more from clean data pairs than the unimodal specialists. Results from CCD to CCD-50 (that has 50% of its pairs containing one clean modality) should help for this analysis. In fact, the mAP gap from unimodal V to MSAF increases\nby 0.62 percentage points from CCD to CCD-50, and decreases from unimodal V to MMSF by 0.48 points. Hence, the multimodal setting seems to benefit globally as much from the clean data pairs as the unimodal V model. However, as 50% of VI pair have a clean image, it means 25% of the data is clean for the unimodal V, which shows, in a way, that the multimodal models are benefiting less from a clean modality but keep up with the unimodal V model thanks to the doubled amount of clean pairs. For deeper analysis, each corruption impact and unilateral corruption are further explored in the next section, so as a qualitative analysis through class activation maps (CAMs) generation (Appendix 1.E.)."
        },
        {
            "heading": "6.2.2 Specific corruption impact",
            "text": "Corruption can sometimes be one-sided, as with NCL cameras or digital corruption. Hence, we may wonder whether some corruptions of the infrared modality will make the unimodal V model advantageous against multimodal models. This question is also raised for visible corruptions and the unimodal I specialist against multimodal models. To answer those, performances of the unimodal visible and thermal models and those of MSAF and MMSF are observed regarding each corruption (Tab. 4) while corrupting only either modality. The MSAF and the proposed MMSF models are selected as those that performed the best over the evaluated multimodal models.\nWeather-related corruptions are the most challenging over the infrared modality, reflected in the lower MSAF and MMSF performances under those data alterations. Compared to the unimodal V model, MSAF is under for 4 corruptions and MMSF for 5 (in red), and both are, on average, much higher for the other. When the RGB modality only is corrupted, MMTM and MSAF models globally conserve a great performance margin over the unimodal I model without corruption. Indeed, it only happens twice among the 20 V corruptions, with contrast and saturation, that those two multimodal models get under unimodal I (in blue). This leads us to affirm that unimodal corruptions are globally very well handled by the multimodal models that can extract some interesting cues from the corrupted modality while not getting regrettably impacted on the clean modality input most of the time.\nComparing the proposed MMSF to the proposed MSAF model, we may observe that MMSF deals better with most corruptions, except for the very challenging ones. Indeed, the I weather alterations are very challenging, and one can see the snow corruption leading, for example, the MSAF model to 81.77% mAP, against 46.99% mAP for MMSF. In fact, strong corruptions may completely alter 2/3 of the MMSF fused embedding (Corrupted modality stream feature and the modality shared one), whereas the MSAF attention may simply refactor features in the corrupted modality so that they do not influence too much the final embedding. For weaker corruptions, having a specific stream to mine the right cues while having a specific stream that exploits the correlations among modalities is better."
        },
        {
            "heading": "6.2.3 Comparison with state-of-art",
            "text": "The multimodal baseline, MSAF, and the proposed MMSF models get compared in terms of complexity and accuracy against the unimodal V and the state-of-art unimodal models LightMBN and TransREID (Fig. 8). The Appendix 1.D provides detailed performance and additional comparison. Unimodal models are learned using CIL\nDA, and the multimodal models using our MLMDA. The accuracy is obtained over both SYSUMM01 clean and CCD evaluation sets and gathered Fig. 8 along with the models\u2019 number of parameters (params) and FLOPs.\nPerformance-wise, each multimodal model is more interesting on clean data than the best unimodal approach, LightMBN. For the best ReID overall, MMSF is the best model, although it performs slightly under MSAF regarding mAP on corrupted data. In fact, MSAF would be favored for a highly challenging environment, especially when facing strong unilateral corruption.\nComplexity-wise, LightMBN is the best model to adopt but comes with a considerable performance decrease from our MMSF, the gap being about 3.32 mAP PP and 15.59 mINP PP on clean data."
        },
        {
            "heading": "6.2.4 Discussion",
            "text": "Experiments over the SYSU-MM01 dataset give us an excellent overview of the multimodal power under the NCL configuration. The main conclusions are as follows:\n\u2022 The proposed ML-MDA is essential for the multimodal models to handle corruption. This way,\nmodels learn how to select the right information from each modality and not get disturbed by noisy features. \u2022 For the best ReID, the proposed MMSF should be used in priority, followed by MSAF, and finally by the unimodal LightMBN models if the memory resources do not allow it. \u2022 The high multimodal accuracy on corrupted data is to highlight as both modalities always get corrupted through the UCD evaluation set, making the task highly challenging. \u2022 The multimodal setting appears as a much better answer to data corruption than the transformer-based approach TransReID, both regarding complexity and accuracy. In fact, TransReID performs less than expected from its performances without DA (M. Chen et al., 2021), the CIL strategy making, for example, the LightMBN more interesting."
        },
        {
            "heading": "6.3 Scenario with co-located cameras",
            "text": "The spatial alignment brought by co-located VI cameras should make the correlations from\none modality to another easier to find for a model.However, this might not make much difference for fusions that come late in the model, as the spatial information will be much diminished and supposedly replaced by semantic information. Also, a corrupted V-I input brings some disequilibrium in how each modality contains relevant information, which should perturb the multimodal models and eventually influence the correlation benefits of spatial alignments. Previous assumptions are explored in the next sections."
        },
        {
            "heading": "6.3.1 Robustness to corruption",
            "text": ""
        },
        {
            "heading": "A) Natural models corruption robustness.",
            "text": "To begin with, the RegDB and ThermalWORLD models are learned without the use of data augmentation, and their performances are respectively gathered in the upper half Tab. 6 and 5.\nThe models must be robust to corrupted data but must also be accurate on clean data at first. Indeed, an optimal model would perform well under the two scenarios. On clean data, the multimodal models are improving over the unimodal visible and Thermal specialists, except for the ThermalWORLD sum model. Precisely, our MMSF model comes first for the two datasets, both regarding mAP and mINP.\nOn corrupted evaluation sets, RegDB presents a unimodal visible accuracy considerably ahead of every multimodal model, showing the multimodal model\u2019s lack of adaptation while facing corrupted data. Indeed, the unimodal V model is, for example, at 45.43% mAP, when the following approach is our MMSF model reaching only 38.99% mAP under the CCD-50 set. In reverse, ThermalWORLD observes a considerable improvement with the multimodal setting. Indeed, the unimodal model is behind every multimodal approach for each corrupted dataset version. The most significant improvement comes from the proposed MMSF model again, reaching 40.01% mAP, whereas the unimodal V reaches 35.28% mAP.\nThe lousy thermal modality quality makes the ThermalWORLD dataset distinct from RegDB, which can explain why the multimodal models naturally better handle corruptions. Indeed, this might seem counter-intuitive as a lower quality modality should help less for the ReID, but this\nmore likely indicates that the challenging ThermalWORLD learning environment helps the multimodal models to handle corruption better. This learning configuration forces the models to learn how to adapt regarding each input quality. Under this assumption, higher corruption robustness can be expected from MDA strategies since it works on related concepts by synthetically bringing noisy samples into the learning process.\nAs a supplementary observation, the gap from unimodal to multimodal models performance is much lower under the CL datasets than under SYSU-MM01 and its NCL cameras. Here, the highest performance improvement in PP from the visible to the best multimodal model is about 0, 69 mAP and 2, 65 mINP for RegDB, and about 2, 1 mAP and 1, 77 mINP for ThermalWORLD. In comparison, the gap in PP was about 11.8 mAP and 39.77 mINP for SYSU-MM01. This performance gap change might result from the CL cameras concerning RegDB and ThermalWORLD, the additional modality bringing fewer supplementary cues than the NCL setting, as an expected consequence of the spatial alignment. For ThermalWORLD, the gap change is likely also due to the terrible thermal modality quality (BRISQUE value Tab. 1), reflected in the mAP gap from the unimodal V to the unimodal T model, being of 31.21 PP. For RegDB, the unique camera per modality probably influence this aspect as well, making the problem easier, leading to almost maxed-out performances that do not allow similar improvement through the multimodal setting. B) DA impact on models robustness.\nModels performances while considering data augmentation strategies are presented lower half Tab. 6 and 5 respectively for RegDB and ThermalWORLD.\nMoving from no use of DA to its usage leads to impressive performance improvements on corrupted data. Where the RegDB multimodal models performed lower than the unimodal visible model using no DA, all multimodal models learned with our ML-MDA become way ahead of the visible model. The greatest improvement comes from unimodal V to our proposed MMSF model, which increases the mAP by 16.91 PP for CCD and 15.82 PP for CCD-50.\nFor corrupted versions of ThermalWORLD, for which multimodal models already had better performances than unimodal specialists before DA, the performance gap significantly increases with ML-MDA usage. Considering CCD evaluation, for example, the gap from unimodal V to the best approach being MMSF is about 7.92 mAP, where it was about 1.89 mAP percentage points without DA.\nThe massive multimodal corruption robustness improvement from the proposed multimodal data augmentation on the two datasets makes it a crucial approach. With it, the MMSF model becomes the best working approach for RegDB, followed by MMTM. In fact, modalities are both corrupted most of the time, so the attention through MMTM and MSAF probably becomes tough to adjust for the models. MMSF does not allow another modality to bring additional noise in its modalityspecific streams and consequently better benefits from each input. Also, its central stream can focus only on the encoding of the modality correlations and eventually improve the ReID even more."
        },
        {
            "heading": "6.3.2 Comparison with state-of-the-art",
            "text": "For CL cameras, multimodal MMSF and MMTM models get compared to the state-of-art unimodal models under both RegDB and ThermalWORLD Clean and CCD evaluation sets. The accuracy is put in perspective of the models\u2019 complexity through their number of parameters (params) and FLOPs (Fig. 9). Only the CCD evaluation set is considered as this configuration is the most adapted to CL cameras (Section 4) and should allow drawing the main conclusions.\nFor RegDB (Fig. 9a), the best-performing model is our proposed MMSF model in terms of accuracy, both on clean and corrupted data. The model is followed by LightMBN and then by MMTM. Hence, the complexity and accuracy trade-off comes between LightMBN and the MMSF model, MMSF being the best way for a strong ReID, and LightMBN for a lighter but lesser efficient approach.\nFocusing on ThermalWORLD (Fig. 9b), the story is different. Despite the same CL camera configuration as RegDB, the two compared multimodal models are much less accurate than the unimodal LightMBN and TransReID models while having more parameters and needing\nmAP Clean\nmINP Clean\nmAP CCD\nmINP CCD\nParams\nFLOPs\n99.25\n99.5\n99.75\n98.5\n99.0\n99.5\n59.25\n63.5\n67.75\n15.25\n20.5\n25.75\n30.75\n54.5\n78.25\n5.0\n10.0\n15.0\nUnimodal V +CIL TransReID +CIL\nLightMBN +CIL Baseline C +MDA\nMMTM +MDA MMSF +MDA\n(a) RegDB\nmAP Clean\nmINP Clean\nmAP CCD\nmINP CCD\nParams\nFLOPs\n88.25\n90.5\n92.75\n50.75\n58.5\n66.25\n57.5\n63.0\n68.5\n11.25\n15.5\n19.75\n30.75\n54.5\n78.25\n5.0\n10.0\n15.0\nUnimodal V +CIL TransReID +CIL\nLightMBN +CIL Baseline C +MDA\nMMTM +MDA MMSF +MDA\n(b) ThermalWORLD\nFig. 9: Complexity and accuracy trade-off using clean and CCD evaluation sets. Dashed lines and plain lines are, respectively, unimodal and multimodal approaches. Measures marked with \u2019\u2193\u2019 should be minimized for an optimized model.\nmore FLOPs than LightMBN. This large gap in behavior from RegDB to ThermalWORLD comes from the latter dataset\u2019s infrared quality again. Still, for a similar ResNet-18 backbone architecture through Unimodal V, we observe that the multimodal models are more accurate. This shows how the multimodal models can benefit from the additional modality even if this one is of low quality, but that it is not enough to compare with LightMBN and TransReID discriminant power. Finally, among TransReID and LightMBN models, it is again an accuracy and complexity trade-off. Heavier but more discriminant is TransReID for ThermalWORLD, and much lighter but also less discriminant is LightMBN."
        },
        {
            "heading": "6.3.3 Discussion",
            "text": "The previous analysis under the CL setting from RegDB and ThermalWORLD datasets allowed us to reinforce some observations from the NCL setting and draw additional conclusions that are as follows:\n\u2022 The proposed ML-MDA data augmentation is crucial for a multimodal model to handle challenging data in NCL and CL settings well. Also, models still benefit much from the MDA when the original dataset is challenging, as observed through ThermalWORLD. \u2022 Our MMSF model deals substantially better with clean and corrupted data than every other approach, including TransReID, despite its highlighted corruption dealing (M. Chen et al., 2021). The early fusion likely allows the model to better apprehend and disentangle the corrupted features from the clean ones between modalities. Considered attention approaches exchange stream information later in the process and consequently have already lost an essential part of the modality correlations. Plus, they do not have modality-specific streams as MMSF, whereas it assures that the final embedding conserves features from a good modality definition and also ensure the model does not only focus on modality-shared features.\n\u2022 The deficient infrared data quality of the ThermalWORLD dataset does not allow the multimodal setting to compare with unimodal stateof-art."
        },
        {
            "heading": "7 Conclusion",
            "text": "Real-world surveillance and especially person ReID is a complex task that requires models to handle complex and abstract concepts, handle data corruption and remain lightweight. To address these challenges, the multimodal setting can be a powerful tool, as an additional modality brings supplementary information that can help to reach higher accuracy while it allows reaching competitive complexity thanks to lightened backbones. However, real-world conditions and the subsequent data corruptions (e.g., weather, blur, illumination) have to be considered. To this aim, our study proposes a strong V-I multimodal evaluation through the first V-I corrupted evaluation sets (UCD and CCD) for multimodal (and cross-modal) V-I person ReID, tackling the lack of multimodal real-world datasets (Rahate et al., 2022). Precisely, 20 visible and 19 infrared corruptions are considered, 3 datasets, 2 camera settings (NCL and CL), 2 state-of-art person ReID models, a MDA, 6 multimodal models, comprising 3 attention-based, 2 baselines, and our proposed MMSF architecture.\nExperiments on the clean and proposed corrupted datasets converge to present the proposed ML-MDA as a must-use to make any multimodal model way more robust to real-world events. The multimodal models observe a larger margin of improvement from the NCL rather than the CL scenario as a consequence of the additional information provided by the NCL complementary view. Still, the benefits of plural modalities are unequivocal for both scenarios, the TransReID model being way more complex and less accurate than plural multimodal approaches (except under really low-quality infrared through the ThermalWORLD dataset). Especially, among multimodal approaches, our MMSF model comes ahead of every considered model for the two scenarios, highlighting the importance of considering modalityspecific features not tackled in attention SOA models.\nTo extend this work, vision-based MDA could be further explored as it showed great benefits but\nremains not much investigated in the literature. Also, the proposed MMSF has shown weakness while facing strongly and unilaterally corrupted data, which has less impact on attention-based models. Hence, adding the right attention modules may allow getting the best of both worlds. Finally, different backbones could be explored for a better accuracy/complexity ratio.\nDeclarations\n\u2022 Acknowledgements: This research was supported by Nuvoola AI Inc., the Natural Sciences and Engineering Research Council of Canada, and by Compute Canada (www.computecanada.ca). \u2022 Availability of data and materials. SYSU-MM01. A signed dataset release agreement must be sent to wuancong@gmail.com and wuanc@mail.sysu.edu.cn to obtain a download link. RegDB. The dataset can be downloaded by submitting a copyright form this website. ThermalWORLD. A part of the dataset is available only. It can be downloaded from this link, obtained from GitHub ThermalGAN issues. \u2022 Code availability. GitHub link."
        },
        {
            "heading": "Appendix A Details",
            "text": "regarding infrared corruptions\nFurther details are provided Tab. A1 concerning the way infrared corruptions were obtained from the existing visible ones. Also, a figure gathering an example of each 19 infrared corruptions is presented Fig. A1.\nTable A1: Applied corruption adjustments to extend Visible (V) corruptions to the Infrared (I) modality. V corruptions that get grayscaled to perform I corruptions appear in red.\nType V corruption I corruption\nN o is\ne\nGaussian noise Each noise is used similarly but is first grayscaled.\nShot noise Impulse noise Speckle noise\nB lu\nr\nDefocus blur\nNo change in the way blurs are extended to infrared. Glass blur Motion blur Zoom blur Gaussian blur\nW ea\nth er\nSnow Brightness is not used for Infrared. Spatter (water or dirt splash) and frost get grayscaled. Others are similarly applied. Frost Fog Rain Brightness Spatter\nD ig\nit a l\nContrast Digital corruptions are the same except for saturation. Saturation for infrared make close objects brighter. Elastic trsf Pixelate JPEG compr Saturation"
        },
        {
            "heading": "Appendix B MMSF",
            "text": "optimization\nB.1 MMSF and not co-located cameras\nThe MMSF model fuses the features from each visible and infrared backbone in its middle stream (Sec. 3.2 in main document). The proper fusion location has to be determined. The fusion can\neither be early in the process, fusing directly original images by element-wise sum, or later by fusing feature maps from each modality stream the same way for a given layer. Intuitively, as the cameras are not co-located and, as a consequence, the images not spatially aligned for the SYSU dataset, an early fusion in the middle stream might result in a noised fused representation. Indeed, the model in early stages might not be able to extract meaningful representation and adapt them according to the used fusion. In reverse, later-stage feature maps have a superior degree of abstraction and should suit better such fusion. Also, considering a corrupted evaluation setting, corruptions may increase the representation gap from one modality to another and thus eventually make the model further benefit from a later fusion. Still, as earlier representation gather more information and hence more potential correlations from one modality to another, one can only be assured of where to fuse data in the middle stream with an empirical study.\nObtained results are gathered in Tab. B2. As expected, fusing at later stages for the middle stream leads to a more discriminant final representation. Indeed, performances in both mAP and mINP gradually improve from fusing at k = 0 to fusing at k = 4 for clean data. For example, the mAP improves by 1, 18% and the mINP by 7, 27% from k = 0 to k = 4 respectively. Also, for the UCD corrupted setting, mAP improves by 2, 09% and 0, 24% mINP for the same k values. Later fusion is more beneficial to the model, confirming the drawn hypothesis on NCL data. Most complex cases are similarly handled by all configurations on corrupted data according to the mINP witch evolves from 10.27% to 10.51% mINP for k = 0 to k = 4 respectively. .\nTable B2: MMSF performances regarding the fusion location in the middle stream, and for the clean and UCD SYSU datasets.\nModel SYSU SYSU-UCD mAP mINP mAP mINP\nMMSF 0 96.59 73.11 63.73 10.27 MMSF 1 97.27 77.01 64.28 9.57 MMSF 2 97.28 77.37 63.43 9.60 MMSF 3 97.76 79.91 64.81 10.38 MMSF 4 97.77 80.38 65.82 10.51\nFig. A1: Taxonomy of the 19 thermal corruptions, all applied with an intensity level 3.\nB.2 MMSF and co-located cameras\nThe MMSF model under CL cameras may behave differently than under NCL cameras due to the alignment of the visible and thermal images in a given pair. In fact, earlier fusion should allow more correlation findings as the feature representation is less compressed than later in the process. Plus, the spatial alignment should make the sum of the feature maps relevant even in the early process. Still, an earlier MMSF fusion comes with a more complex architecture since it requires more layers in the central stream, which needs to be kept in mind.\nPerformances regarding the fusion location l for RegDB and ThermalWORLD datasets are presented Tab. B3. As expected, it is interesting to observe the performance decrease from l = 0 to l = 4 on RegDB for clean and CCD-50 data. In practice, the mAP decreases by 2.46%, and the mINP by 3.74% on CCD-50 dataset. ThermalWORLD results are not following this same scheme, as the results on clean data are the highest for l = 1, followed by l = 2 l = 4 and l = 0. The model act as an in-between the NCL and CL settings, which probably comes from the thermal modality being of terrible quality, messing with the expected impact of spatial alignment. Still,\nthe RegBD model acts similarly as the ThermalWORLD one on corrupted data, performing the best through earlier fusions. In fact, earlier fusion may allow the model to get less impacted by corrupted features as the model can directly find and discard them while benefiting from the most correlations.\nTable B3: MMSF performances regarding the fusion location in the middle stream, and for the clean and CCD-50 versions of RegDB and ThermalWORLD datasets.\nModel Clean CCD-50 mAP mINP mAP mINP\nR eg\nD B MMSF 0 99.95 99.69 74.25 33.24 MMSF 1 99.93 99.60 73.16 30.36 MMSF 2 99.93 99.66 73.17 30.68 MMSF 3 99.93 99.64 72.55 29.74 MMSF 4 99.94 99.64 71.79 29.50\nT W\nO R\nL D MMSF 0 86.10 44.50 62.77 14.24 MMSF 1 86.27 45.96 62.27 13.59 MMSF 2 86.28 45.26 62.06 13.44 MMSF 3 86.14 44.24 61.21 12.98 MMSF 4 86.58 44.89 61.30 12.95"
        },
        {
            "heading": "Appendix C Element-wise",
            "text": "sum or concatenation\nC.1 Fusion with co-located cameras\nThe absence of spatial alignment may favor the concatenation over the element-wise sum of the feature vectors or vice-versa. Indeed, a summation might require the vector information to be aligned from one modality to another, not to erase it. Unlike element-wise sum, concatenation conserves features from each modality the same way and could consequently better fit with NCL configuration. Also, in the case of corrupted data, corruption should also tend to make the produced modality-specific feature vector representation different and hence favored concatenation as well. Still, this is only hypothetical as the information may be only semantic and aligned at this point of the data encoding.\nTo confirm or invalidate the previous assumptions, the baseline, MMTM, and MSAF models are compared in terms of mAP and mINP regarding a sum or a concatenation of the feature vectors, and on clean and UCD SYSU-MM01 datasets (Table C4). The corrupted UCD set is only considered here as uncorrelated corruptions are the most suited for the NCL configuration and as it should allow answering the previous hypothesis. Models were trained using ML-MDA, but the ML-MDA is beyond this section\u2019s scope, only used as a tool (for now) to bring consistency from clean to corrupted evaluation. Observing the baseline results, it seems beneficial to concatenate the features as expected while looking at clean data results. Indeed, concatenation improves mINP by 1.52% while conserving similar mAPs. However, performances under the UCD dataset show that summing is more beneficial, slightly improving the mAP and mINP respectively by 0.60 and 0, 21%. These results on corrupted data are going against our hypothesis, as concatenation was expected to overpass summation under UCD. Observing attention MMTM and MSAF models results on clean and UCD data; the former considerably improves from summation to concatenation, whereas the second considerably decreases. Hence, the concatenation or summation\nof the features at such a level of abstraction probably allows the model to deal with the absence of spacial alignment and to align features according to the fusion used. Consequently, the best feature vector fusion strategy is model dependent and needs to be assessed experimentally. For MMTM and MSAF, the upcoming NCL analysis will consider only their best fusion version MMTM C and MSAF S.\nTable C4: Baseline, MMTM, and MSAF performances on SYSU-MM01 dataset while summing (S) or concatenating (C) their feature vectors. Clean and UCD evaluation only are considered since UCD respects the most NCL corruptions (Section 4.3 main document).\nModel Clean UCD mAP mINP mAP mINP\nBaseline S 96.54 74.49 64.00 9.72 Baseline C 96.77 76.01 63.40 9.51\nMMTM S 94.97 68.33 63.29 9.45 MMTM C 95.81 74.23 64.41 11.49\nMSAF S 96.36 73.70 67.78 10.09 MSAF C 96.04 71.13 66.20 9.68\nC.2 Fusion with co-located cameras\nThe best strategy between element-wise sum and concatenation of the feature maps was shown to be model-dependent for NCL cameras (Section C.1). Unlike NCL cameras, CL ones bring spacial alignment that might impact the preferred fusion differently. An empirical analysis is provided Tab. C5 to determine which fusion to follow and if it remains model dependent by applying it on the baseline, MMTM and MSAF models. In practice, where it behaves similarly for each dataset by favoring the fusion by concatenation for the baseline models, it becomes more complex for MMTM and MSAF models. Indeed, the MMTM and the MSAF models, which exchange information between the visible and thermal CNN streams, seem not to follow a specific rule again. More than being model-dependent, performances appear as being data-dependent. For example, MMTM S performs better under both clean and corrupted RegDB settings, whereas it is MMTM C for\nThermalWORLD. It is important to notice that the performance gap can be important from sum to concatenation, making such analysis important while seeking the right way to fuse feature vectors in a model. Models performing best for MMTM and MSAF are kept for the rest of CL cameras study.\nTable C5: Baseline, MMTM and MSAF performances on RegDB and ThermalWORLD datasets while summing (S) or concatenating (C) the feature vectors as fusion.\nModel Clean CCD-50 mAP mINP mAP mINP\nR eg\nD B\nBaseline S 99.87 99.37 63.89 17.34 Baseline C 99.90 99.45 64.15 18.08\nMMTM S 99.84 99.24 67.27 20.17 MMTM C 99.80 99.12 63.92 17.97\nMSAF S 99.84 99.19 59.22 13.26 MSAF C 99.88 99.33 63.87 18.28\nT W\nO R\nL D\nBaseline S 82.18 36.89 55.68 10.55 Baseline C 86.34 43.24 58.01 11.02\nMMTM S 86.17 45.50 59.60 10.91 MMTM C 87.82 47.95 60.51 12.36\nMSAF S 87.62 50.02 60.78 10.93 MSAF C 87.73 48.00 60.57 12.01"
        },
        {
            "heading": "Appendix D Detailed",
            "text": "complexity and accuracy trade-off\nThe accuracy and complexity analysis is provided in the main document Section 6.2.3 for NCL and 6.3.2 for CL cameras. However, detailed performances and complexity was not provided. Hence, this section focus on the detailed models performances for NCL and CL cameras at first and finally present the complexity in terms of parameters and FLOPs for each and every considered model.\nD.1 Accuracy with not co-located cameras\nState-of-art unimodal models, along with the unimodal V model, are compared to the baseline C, MMSF, and MSAF multimodal approaches learned using our ML-MDA (Table D6). Unimodal models are evaluated while being learned with and without the CIL strategy. As a first observation, multimodal models are all considerably over the unimodal models in terms of both mAP and mINP on clean data. Indeed, the highest improvement in mAP and mINP from the best unimodal model performances is respectively about 3.32% and 15.59%. Then, if we compare the multimodal models among themselves, MMSF comes first by improving mAP of the baseline by 1.00% while it improves its mINP by 4.37%. Surprisingly, MSAF is below the baseline\u2019s mINP by 2.31% while conserving its mAP.\nLooking now at the UCD performances, the best working model is MSAF with 67.78% mAP and 10.09% mINP. Then, LightMBN and MMSF are pretty equivalent, with respectively mAPs about 67.80% and 65.82% but mINPs about 8.23% and 10.51%. From the previous observations, both the MSAF and MMSF models can be used to improve over the state-of-art unimodal models, considering both clean and corrupted data. However, the benefits from the proposed MMSF are higher than the ones from the MSAF approach. Still, if the real-world conditions were expected as tough, MSAF would eventually be favored. However, if conditions were varying or tending to be clean, MMSF should be used.\nD.2 Accuracy with co-located cameras\nThe multimodal models trained using our MLMDA are compared with state-of-art unimodal frameworks learned using CIL DA under the CL setting Table D7. First, observing performances on RegDB clean data, the multimodal baseline C and the proposed MMSF are ahead of the unimodal models. MMSF improving LightMBN mAP and mINP respectively from 99.89 to 99.92 and 99.45 to 99.57. If we observe corrupted performances, only the proposed MMSF can improve over the\nTable D6: Multimodal comparison with the state-of-art unimodal models. MDA refer to our ML-MDA approach.\nSYSU Model Clean UCD\nmAP mINP mAP mINP\nN o\nD A Unimodal V 86.25 39.97 32.36 1.91\nTransReID 94.33 64.79 52.03 3.60 LightMBN 94.45 64.06 40.90 2.13\nC IL Unimodal V 86.64 42.78 51.64 3.83 TransReID 93.20 62.02 61.38 7.20 LightMBN 94.07 61.95 67.80 8.23\nM D A Baseline C 96.77 76.01 63.01 9.59 MSAF 96.36 73.70 67.78 10.09 MMSF 97.77 80.38 65.82 10.51\nbest unimodal model LightMBN + CIL, increasing the mAP by 4.97% on CCD and by 4.85 on CCD-50. Hence, our MMSF model is the way to go for both clean and corrupted data under the CL configuration performance-wise.\nAbout ThermalWORLD, models behave really differently. The TransReID and LightMBN models perform much better than the best multimodal approach MSAF on clean data. Indeed, for example, TransReID reaches 95.86% mAP when MSAF reaches 87.82% mAP. In fact, the slight 0.87% mAP improvement from the Unimodal V to the MSAF model shows how hard the multimodal setting benefits from the bad thermal modality. This is confirmed by the results under corrupted settings, as the best multimodal approach MMSF is 12.86% and 13.44% mAP below the TransReID approach for CCD and CCD-50 respectively. Consequently, favoring stronger unimodal models is a better strategy when the supplementary modality is far behind in terms of quality.\nD.3 Models complexity\nThanks to the additional modality and knowledge, a multimodal setting might allow the use of lighter backbones than a given unimodal pipeline while matching or even improving accuracy. From the previous experiments, the multimodal accuracy comes ahead unimodal approaches, but a complexity analysis remains needed, and is provided Tab.\nD8. The analysis is presented regarding the models\u2019 number of parameters and the FLOPs needed to compute a single input.\nFirst, one can observe that the TransReID complexity appeals at first sight, much heavier through 102.0M parameters than any other models, followed by MMSF with l = 0 and its 34.6M parameters. The lighter model is LightMBN, being more than ten times lighter than TransReID with 7.6M parameters. Based on the obtained results for NCL, the multimodal setting improves much the ReID accuracy. Especially, LightMBN comes first among unimodal approaches but is way less performing than MSAF and MMSF. In practice, the proposed MMSF works the best (l = 4 for NCL) under NCL cameras and should be used if resources allow it, requiring 2.31 GFLOPs and 31.9M parameters. Otherwise, MSAF would be the next model to go with 1.54 GFLOPs and 22.5M parameters, finally followed by the unimodal LightMBN approach with 2.09GFLOPs and 7.6M parameters.\nConsidering the CL setting, the proposed MMSF (l = 0) model is ahead, followed directly by the LightMBN model performance-wise. Similarly LightMBN comes with less complexity than MMSF, thus making a compromise between precision and complexity."
        },
        {
            "heading": "Appendix E Qualitative analysis",
            "text": "Models learned through ML-MDA were compared over clean and corrupted data in terms of performances Section 6.2.1 in the main document. However, observing what the models are focusing on to discriminate and ReID would be a great way to draw additional conclusions, or at least to better understand why a model is better than another. To this end, adapted for pairwise matching algorithms, similarity based Class Activation Maps (CAMs) from Stylianou et al. (2019) is used. It is important to notice that MMSF CAMs are produced from its two modality specific streams only, and that the shared modality stream cannot be analysed from this CAM technique for the NCL cameras. Indeed, CAMs could be determined for the middle stream but there would be no way to dissociate from which spatial part of the V or I modality comes the shared activation.\nTable D7: RegDB and ThermalWORLD - Comparison with SOTA\nModel Clean CCD CCD-50 mAP mINP mAP mINP mAP mINP\nR eg\nD B Unimodal V 99.26 96.64 45.15 7.01 45.42 6.20 TransReID 99.34 97.35 45.64 5.69 48.60 7.01 LightMBN 99.90 99.41 32.40 7.01 33.63 3.85 Unimodal V + CIL 99.65 98.41 55.76 10.9 58.53 14.8 TransReID + CIL 99.69 98.57 58.74 12.8 60.48 16.2 LightMBN + CIL 99.89 99.41 66.55 21.5 69.40 26.2 Baseline C + ML-MDA 99.90 99.45 59.06 14.6 64.15 18.0 MMTM + ML-MDA 99.84 99.2!4 63.34 17.8 67.27 20.1 MMSF + ML-MDA 99.95 99.69 71.52 30.4 74.25 33.2\nT h\ner m\na lW\nO R\nL D Unimodal V 86.44 49.44 28.06 3.86 35.27 5.18 TransReID 95.86 77.98 65.47 17.2 68.66 20.0 LightMBN 93.02 65.94 37.34 5.60 44.01 6.70 Unimodal V + CIL 86.95 48.07 52.85 7.97 56.33 10.6 TransReID + CIL 94.79 73.82 73.61 23.1 76.21 25.8 LightMBN + CIL 93.20 66.14 71.30 19.7 73.62 21.4 Baseline C + ML-MDA 86.34 43.24 56.10 9.93 58.01 11.02 MMTM + ML-MDA 87.82 47.95 58.12 11.53 60.51 12.36 MMSF + ML-MDA 86.10 44.50 60.75 13.33 62.77 14.24\nTable D8: Size (Number of parameters) and computation complexity regarding FLOPs.\nModel No params (M) FLOPs (G)\nUnimodal V or I 11.3 0.51 TransReID 102.0 19.55 LightMBN 7.6 2.09\nBaseline 22.5 1.54 MAN 22.5 1.54 MMTM 23.8 1.54 MSAF 22.5 1.54 MMSF l=0 34.6 3.09 MMSF l=4 31.9 2.31\nTo put visualizations in perspective, models ranking performance-wise on clean data start from MMSF, followed by MAN, Baseline C, MSAF, and MMTM. Observing Fig. E2a., one can see that activation on the V modality is more or less similar from one model to another, focusing mainly on the torso. Actually, MMSF might appear a bit less accurate but tend to focus on the same region. However, it seems that the most discriminant models consider both the torso and the legs of the person concerning the I modality. Indeed, from MMSF (6) to MAN (3), Baseline C (2), MSAF\n(5) and finally to MMTM (4), legs activation just decreases.\nSwitching to corrupted data, models ranking was the following under CCD: MSAF, MMSF, MMTM, Baseline C, MAN. Looking at Fig. E2b. one may observe that the best working models focus both on the short and on the t-shirt of the individual concerning the V modality. About the I modality, it is harder to interpret, as the added snow made the focus of the models much less accurate, which in fact correlate well with the snow corruption impact on the thermal modality (Tab. 4). In fact, for I, both MMSF and MSAF focus on waist, but MMSF adding feet where MSAF adds shoulders to it. Also, MMTM seems much perturbed, as its attention is not so much on the person, and baseline C with MAN are both looking pretty fuzzy, mainly looking at the whole back of the individual.\nIf we look at corruptions that seem to less affect each modalities, with Fig. E2c., one can see that the thermal modality (gaussian noise) is much better apprehended by each model. For the most discriminant ones, MMSF (6), MSAF (5) and MMTM (4), it is interesting to again observe the importance of feet in the ReID process.\n(1) (2) (3) (4) (5) (6)\n(1) (2) (3) (4) (5) (6)\n(1) (2) (3) (4) (5) (6)\n(a)\n(b)\n(c)\nFig. E2: Three examples of similarity based CAMs using SYSU-MM01 (a) clean V-I pairs, (b) snow corrupted V-I pairs, (c) differently corrupted V (spatter) and I (gaussian noise) pairs. CAMs are computed from (2) baseline C, (3) MAN, (4) MMTM, (5) MSAF, and (6) MMSF. (1) is the reference V-I pair."
        }
    ],
    "title": "Fusion for Visual-Infrared Person ReID in Real-World Surveillance Using Corrupted Multimodal Data - First version submitted to IJCV",
    "year": 2023
}