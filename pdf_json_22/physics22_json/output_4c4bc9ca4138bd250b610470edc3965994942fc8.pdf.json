{
    "abstractText": "F orthcoming astronomical surv e ys are e xpected to detect new sources in such large numbers that measuring their spectroscopic redshift measurements will not be practical. Thus, there is much interest in using machine learning to yield the redshift from the photometry of each object. We are particularly interested in radio sources (quasars) detected with the Square Kilometre Array and have found Deep Learning, trained upon a large optically selected sample of quasi-stellar objects, to be ef fecti ve in the prediction of the redshifts in three external samples of radio-selected sources. Ho we ver, the requirement of nine different magnitudes, from the near-infrared, optical, and ultra-violet bands, has the effect of significantly reducing the number of sources for which redshifts can be predicted. Here, we explore the possibility of using machine learning to impute the missing features. We find that for the training sample simple imputation is sufficient, particularly replacing the missing magnitude with the maximum for that band, thus presuming that the non-detection is at the sensitivity limit. For the test samples, ho we ver, this does not perform as well as multi v ariate imputation, which suggests that many of the missing magnitudes are not limits, but have indeed not been observed. From e xtensiv e testing of the models, we suggest that the imputation is best restricted to two missing values per source. Where the sources o v erlap on the sk y, in the worst case, this increases the fraction of sources for which redshifts can be estimated from 46 per cent to 80 per cent, with > 90 per cent being reached for the other samples.",
    "authors": [
        {
            "affiliations": [],
            "name": "S. J. Curran"
        }
    ],
    "id": "SP:9c8f8d54a95c39b6f27b48ed24718978a5b97342",
    "references": [],
    "sections": [
        {
            "text": "MNRAS 512, 2099\u20132109 (2022) https://doi.org/10.1093/mnras/stac660 Advance Access publication 2022 March 10\nQuasar photometric redshifts from incomplete data using deep learning\nS. J. Curran \u2039 School of Chemical and Physical Sciences, Victoria University of Wellington, PO Box 600, Wellington 6140, New Zealand\nAccepted 2022 March 7. Received 2022 March 7; in original form 2021 December 7\nA B S T R A C T F orthcoming astronomical surv e ys are e xpected to detect new sources in such large numbers that measuring their spectroscopic redshift measurements will not be practical. Thus, there is much interest in using machine learning to yield the redshift from the photometry of each object. We are particularly interested in radio sources (quasars) detected with the Square Kilometre Array and have found Deep Learning, trained upon a large optically selected sample of quasi-stellar objects, to be ef fecti ve in the prediction of the redshifts in three external samples of radio-selected sources. Ho we ver, the requirement of nine different magnitudes, from the near-infrared, optical, and ultra-violet bands, has the effect of significantly reducing the number of sources for which redshifts can be predicted. Here, we explore the possibility of using machine learning to impute the missing features. We find that for the training sample simple imputation is sufficient, particularly replacing the missing magnitude with the maximum for that band, thus presuming that the non-detection is at the sensitivity limit. For the test samples, ho we ver, this does not perform as well as multi v ariate imputation, which suggests that many of the missing magnitudes are not limits, but have indeed not been observed. From e xtensiv e testing of the models, we suggest that the imputation is best restricted to two missing values per source. Where the sources o v erlap on the sk y, in the worst case, this increases the fraction of sources for which redshifts can be estimated from 46 per cent to 80 per cent, with > 90 per cent being reached for the other samples.\nKey words: methods: statistical \u2013 techniques: photometric \u2013 galaxies: active \u2013 galaxies: photometry \u2013 infrared: galaxies \u2013 ultraviolet: galaxies.\n1\nG f t N m p z D 2 b t a b e M o e g r\nw n\nt L ( p e\nt s o b c t r S a o o u d p\n1\n\u00a9 P C p\nD ow nloaded from https://academ ic.oup.com /m nras/article/512/2/2099/6546165 by Indian Institute of Technology Patna user on 19 January 2024 I N T RO D U C T I O N iven that a large number of sources are expected to be detected by orthcoming continuum surv e ys with the ne xt generation of large elescopes (e.g. Norris et al. 2011 ; Ananna et al. 2017 ; Luken, orris & Park 2019 ), there is currently much interest in using achine learning techniques to determine their redshifts from their hotometry. These generally utilize the u \u2212 g , g \u2212 r , r \u2212 i , and i \u2212 colours as features to train and validate upon sources in the Sloan igital Sky Survey (SDSS, e.g. Richards et al. 2001 ; Weinstein et al. 004 ; Maddox et al. 2012 ; Han et al. 2016 ). The addition of other ands, specifically the near-infrared (NIR) W 1 and W 2 bands and he ultra-violet (UV) NUV and FUV bands, can greatly impro v e the ccuracy of the predictions, reducing the scatter (see Section 2.1.3 ) etween the predicted and actual redshifts to \u03c3 z(norm) \u223c 0.1 (Bovy t al. 2012 ; Brescia et al. 2013 ; Salvato, Ilbert & Hoyle 2019 ; Curran, oss & Perrott 2021 ). This is similar to the scatter reached by state-\nf-the-art template fitting of the SEDs (Hildebrandt et al. 2010 ; Beck t al. 2017 , 2021 , and references therein), although the samples are enerally smaller ( n < \u223c 10 3 , cf. > \u223c 10 4 ) and often require the prior emoval of outliers.\nFrom our previous results (Curran & Moss 2019 ; Curran 2020 ), e (Curran et al. 2021 ) suggested that the wide range of magitudes was required to co v er the redshifting of rest-frame fea-\nE-mail: scurran.astro@gmail.com\nt a m\n2022 The Author(s). ublished by Oxford University Press on behalf of Royal Astronomical Society. Th ommons Attribution License ( http:// creativecommonsorg/licenses/ by/4.0/ ), which rovided the original work is properly cited.\nures, for example, the \u03bb \u223c 1 \u03bcm inflection and the \u03bb = 1216 \u00c5 yman-break. This signifies a commonality between the data-driven machine learning) methods and the physically moti v ated (temlate fitting) methods of photometric redshift estimation (Salvato t al. 2019 ).\nThe requirement of a measurement in each of the nine bands has he effect of reducing the sample size. For example, of the 100 000 trong training sample only \u2248 70 per cent have the full complement f measurements in all of the FUV , NUV , u , g , r , i , z, W 1, W 2 ands (Curran et al. 2021 ). This worsens for the test data, which omprise three external samples of radio-selected surveys, chosen o test the potential of using an SDSS trained model to predict the edshifts of Square Kilometre Array (SKA) (pathfinder) data (see ection 2.2 ). The common practice is to remo v e sources for which ll of the photometry is not available 1 , which can have the effect f dramatically reducing the sample size (e.g. up to half of one f our training sets). In this paper, we explore the possibility of sing machine learning methods to mitigate the effect of missing ata, reducing the number of sources for which we cannot obtain hotometric redshifts.\nExceptions are Bovy et al. ( 2012 ), who use the probability distribution in he feature\u2013redshift space of the other sources to fill in the missing values, nd Carvajal et al. ( 2021 ), who assume the detection limit of that band for the issing values.\nis is an Open Access article distributed under the terms of the Creative permits unrestricted reuse, distribution, and reproduction in any medium,\nTable 1. The number of missing magnitude measurements in each of the bands of the 100 337 strong SDSS QSO sample. As expected, the numbers are larger for the non-SDSS photometry.\nFUV NUV u g r i z W 1 W 2\n10 658 20 300 2923 1301 1230 1256 1258 4040 4156\n2\n2\n2\nF o\nf t E F e 2 p u f A f a t u p\nD ow nloaded from https: ANALYSIS .1 The training data .1.1 The data or the training data, we extracted the first 100 337 quasi-stellar bjects (QSOs) with accurate spectroscopic redshifts ( \u03b4z/ z < 0.01)\nrom the SDSS Data Release 12 (DR12, Alam et al. 2015 ). We hen matched the nearest source within 6 arcsec in the NASA/IPAC xtragalactic Data base (NED), usually resulting in a single match. or these, the photometry was scraped from NED, WISE (Wright t al. 2010 ), the Two Micron All Sky Survey (2MASS, Skrutskie et al. 006 ), and GALEX (Bianchi, Shiao & Thilker 2017 ) databases. As er Curran ( 2020 ) and Curran et al. ( 2021 ), in order to ensure a niform magnitude measure between the SDSS and other samples, or each QSO we added the PSF flux densities associated with the B magnitudes, which fell within log 10 \u03bd = \u00b10.05 of the central requency of the band. Within each band range, the fluxes were then veraged before being converted to a magnitude. This method, rather han just using the SDSS magnitudes directly, gives the option of sing the SDSS data to train other samples, for which direct SDSS hotometry may not be available.\ndata. Where the minimum and maximum are not shown and\n//academ ic.oup.com\nFigure 1. The distribution of each of the magnitudes for various imputation methods. Note that the un-imputed maximum, mean, median, and most frequent distributions are coincident apart from the spikes introduced by these methods.\nW t w H o\n2\nG w d\nm T\nu ( a O\nr t\n2 ht tps://scikit -learn.org/stable/\nD ow nloaded from https://academ ic.oup.com /m nras/article/512/2/2099/6546165 by Indian Institute of Technology Patna user on 19 January 2024\nExtensi ve testing sho wed that use of all FUV , NUV , u , g , r , i , z, 1, W 2 bands gave the most accurate photometric redshifts, with he addition of the WISE W 3, W 4 photometry having little benefit, hile significantly decreasing the sample size (Curran et al. 2021 ). o we ver, e ven with the exclusion of the W 3, W 4 bands, there were nly 72 276 QSOs that had all of the required photometry (Table 1 ). .1.2 Data imputation iven that only 72 per cent of the available training data are complete, e explored various methods for replacing (imputing) the missing ata: (i) Univariate (single) imputation: the most straightforward ethod, which replaces the missing magnitude with a single value. his can be done via: (1) Simple imputation: Replacing the missing value with the mean, median or most frequently occurring value. An arbitrary constant may also be assigned.\n(2) Maximum value imputation: The missing value is replaced by the detection limit (Carvajal et al. 2021 ), which we implemented by assuming that this was given by the largest value of the magnitude in question.\n(3) Hot-deck imputation: The missing magnitudes are replaced by random values. We did this by generating a random number with a value between the minimum and maximum of that particular magnitude.\n(ii) Model-based imputation: Each missing value is modelled sing the other non-missing features of the data set. A feature column magnitude) is the output, with the remainder of the magnitudes cting as the inputs (Little & Rubin 1986 ; van Buuren & Groothuisudshoorn 2011 ). We tested:\n(1) Multivariate (multiple) imputation: A regression is fit for a known output and used to predict the empty values, which is then iterated for each feature until the maximum specified number of iterations is reached. We used the IterativeImputer function of sklearn , 2 with a maximum number of iterations set to 1000, in order to reach the early stopping criterion.\n(2) k-nearest neighbours (kNN) imputation: For each missing value, the Euclidean distance is found for k nearest neighbours for which the feature has a value. The neighbour features can either be weighted uniformly or by the inverse of the Euclidean distance (Troyanskaya et al. 2001 ). This was implemented using the KNNImputer function of sklearn , for which we found k \u223c 10 nearest neighbours uniform weighting to perform the best, and so we tested this for imputed models which used k = 3, 10, and 20.\nSince the aim is to predict the unavailable source redshifts, we emo v ed the redshifts from the data before imputing to ensure that hey did not contribute to the machine learning. We summarize\nMNRAS 512, 2099\u20132109 (2022)\nM\nFigure 2. The upper extremes of the magnitudes of the un-imputed data with suspected outliers.\nt o s l t o t s d t v (\n2\nb N w s r\n3\nm m 4\n( a s f\n8 v o W d s i i d i t a t r\n\u03bc\nt\n\u03c3\na\n\u03c3\nt \u2212 \u2248 t w m e m\n\u03c3\no 0 D \u03c3 N\n2\n2\nA f d\nD ow nloaded from https://academ ic.oup.com /m nras/article/512/2/2099/6546165 by Indian Institute of Technology Patna user on 19 January 2024\nhe results in Table 2 and in Fig. 1 , we show the distributions f the un-imputed 3 and imputed magnitudes. From the figure, we ee that, as expected, the imputed non-SDSS photometry showed arger deviations from the measured magnitudes, most likely due o the larger number of missing values (Table 1 ). Ho we ver, this nly appears to be particularly severe for the NUV data, which have he most missing values. We also note that the maximum values in ome bands may be due to outliers. Showing the top ends of the istributions in detail (Fig. 2 ), we see that this certainly appears to be he case for the i and WISE bands. We therefore apply the maximum alue imputation, but limiting i \u2264 23 and W 1 \u2264 19 and W2 \u2264 19 maximum truncated in Table 2 ). .1.3 Deep learning\nAs described in Curran et al. ( 2021 ), the Deep Learning model, uilt with the TensorFlow 4 platform, outperformed both the k - earest Neighbour (kNN) and Decision Tree Regression algorithms, ith self-v alidation gi ving a regression coef ficient of the leastquares linear fit between the predicted and measured redshifts of = 0.94. As with other studies, we used the colours as features\nNRAS 512, 2099\u20132109 (2022)\nWe use the term un-imputed to refer to the full 100 337 samples with the issing values included and non-imputed for the 72 276 sources with all nine agnitudes. https://www .tensorflow .org\nr t\n5\nm\nSection 1 ), although we noted that using the raw magnitudes gave similar performance. Since replacing missing magnitudes is more traightforward than changing colours, we use the magnitudes as eatures here.\nUpon imputing the data, we trained the model on a random 0 per cent portion, using the same Deep Learning model as preiously \u2013 two Rectified Linear Unit (ReLu) function layers and ne hyperbolic tangent (tanh) layer comprising 200 neurons each. e then validated the model on the remaining 20 per cent of the ata, comparing the photometric (predicted) redshifts with the pectroscopic (measured) redshifts, which had been returned to the mputed data set. In order to account for the small differences n the results inherent between each trial, we then reshuffled the ata and repeated the process 99 times. We summarize the results n Table 3 , using statistics calculated for the difference between he photometric and spectroscopic redshifts, z \u2261 z spec \u2212 z phot , nd its normalized counterpart, z( norm ) \u2261 z spec \u2212z phot\nz spec + 1 . Specifically, he mean difference between the photometric and spectroscopic edshifts,\nz \u2261 1 N N \u2211 i= 1 z,\nhe standard deviation,\nz \u2261 \u221a \u221a \u221a \u221a 1\nN N \u2211 i= 1 z 2 ,\nnd the median absolute deviation (MAD), MAD \u2261 1 . 48 \u00d7 median \u2223\u2223z spec \u2212 z phot\n\u2223\u2223 . We confirm that using the magnitudes directly performs as well as he FUV \u2212 NUV , NUV \u2212 u , u \u2212 g , g \u2212 r , r \u2212 i , i \u2212 z, z \u2212 W 1, and W 1 W 2 colours, where the regression coefficient was r \u2248 0.94, \u03c3 z 0.235 and \u03c3 MAD \u2248 0.092 (Curran et al. 2021 ). 5 We also see that he best performing imputation method is the maximum truncated , ith r = 0 . 933 , \u03c3 z = 0 . 315, and \u03c3MAD = 0 . 127. This uses a similar ethod as Carvajal et al. ( 2021 ), who assume the detection limit for ach missing value. According to their chosen metric, the normalized edian absolute deviation (NMAD),\nNMAD \u2261 1 . 48 \u00d7 median \u2223\u2223\u2223\u2223 z spec \u2212 z phot z spec + 1 \u2223\u2223\u2223\u2223 ,\nur model performs slightly better than theirs (all have \u03c3 NMAD > .06, cf. our 0.052). As previously noted (Curran et al. 2021 ), eep Learning gives better results across all metrics, including NMAD , than standard machine learning algorithms (namely, k - earest Neighbour and Decision Tree Regression).\n.2 The test data\n.2.1 The data\ns stated abo v e, our goal is to develop photometric redshift models or radio sources detected with the SKA and its pathfinders. As escribed in Curran & Moss ( 2019 ), finding large catalogues of adio sources with spectroscopic redshifts is a challenge, and just hree sizable data bases were found:\nThese are given as approximations since they are from a single run of the odel.\nTable 3. The mean results of the SDSS validation (20 067 sources) for the various imputation methods o v er randomized 100 trials, quoted with \u00b11 \u03c3 . For the non-imputed data there are 14 253 validation sources.\nImputation r Un-normalized Normalized \u03bc z \u03c3 z \u03c3MAD \u03bc z(norm) \u03c3 z(norm) \u03c3NMAD\nNone 0.934 \u00b1 0.002 0.003 \u00b1 0.032 0.234 \u00b1 0.003 0.105 \u00b1 0.012 \u2212 0.008 \u00b1 0.015 0.115 \u00b1 0.005 0.048 \u00b1 0.006 Uni v ariate Mean 0.918 \u00b1 0.002 0.003 \u00b1 0.034 0.351 \u00b1 0.004 0.132 \u00b1 0.012 \u2212 0.017 \u00b1 0.017 0.162 \u00b1 0.006 0.055 \u00b1 0.005 Median 0.919 \u00b1 0.002 0.002 \u00b1 0.032 0.349 \u00b1 0.004 0.129 \u00b1 0.011 \u2212 0.016 \u00b1 0.015 0.162 \u00b1 0.006 0.054 \u00b1 0.005 Most 0.918 \u00b1 0.002 0.002 \u00b1 0.032 0.352 \u00b1 0.004 0.130 \u00b1 0.010 \u2212 0.016 \u00b1 0.015 0.166 \u00b1 0.006 0.055 \u00b1 0.004 Max 0.927 \u00b1 0.002 0.004 \u00b1 0.035 0.332 \u00b1 0.005 0.124 \u00b1 0, 015 \u2212 0.014 \u00b1 0.017 0.155 \u00b1 0.009 0.051 \u00b1 0.007 Max trun. 0.933 \u00b1 0.003 0.003 \u00b1 0.037 0.315 \u00b1 0.007 0.127 \u00b1 0.017 \u2212 0.014 \u00b1 0.016 0.154 \u00b1 0.009 0.052 \u00b1 0.005 Random 0.909 \u00b1 0.002 0.006 \u00b1 0.032 0.369 \u00b1 0.005 0.134 \u00b1 0.011 \u2212 0.166 \u00b1 0.015 0.175 \u00b1 0.007 0.056 \u00b1 0.004 Multi v ariate 0.916 \u00b1 0.002 0.000 \u00b1 0.032 0.355 \u00b1 0.005 0.130 \u00b1 0.012 \u2212 0.018 \u00b1 0.015 0.167 \u00b1 0.008 0.055 \u00b1 0.005 kNN k = 3 0.903 \u00b1 0.002 \u2212 0.004 \u00b1 0.030 0.380 \u00b1 0.004 0.138 \u00b1 0.009 \u2212 0.022 \u00b1 0.013 0.180 \u00b1 0.007 0.058 \u00b1 0.004 k = 10 0.909 \u00b1 0.002 \u2212 0.003 \u00b1 0.029 0.369 \u00b1 0.004 0.132 \u00b1 0.011 \u2212 0.021 \u00b1 0.013 0.175 \u00b1 0.006 0.056 \u00b1 0.005 k = 20 0.912 \u00b1 0.002 0.001 \u00b1 0.026 0.364 \u00b1 0.004 0.129 \u00b1 0.010 \u2212 0.019 \u00b1 0.012 0.172 \u00b1 0.007 0.054 \u00b1 0.004\nFigure 4. Histograms of the total number of missing magnitudes per source for the three test samples.\n( h e\n( e\n6\nr \u2018\nTable 4. The statistics for the test samples. N is the number of sources for which the magnitude is av ailable, follo wed by the percentage of the total. This is followed by the minimum, maximum, mean, median, and most frequently measured value of the magnitude.\nMag. N per cent Min Max Mean Median Most\nFIRST, n = 9016 FUV 6912 77 16 .07 25 .03 20 .78 20 .85 21 .38 NUV 6699 74 14 .83 25 .40 20 .57 20 .52 21 .43 u 8457 94 14 .22 28 .45 19 .76 19 .65 18 .88 g 8672 95 14 .49 26 .34 19 .30 19 .32 18 .87 r 8587 95 14 .38 22 .18 19 .02 19 .07 18 .87 i 8564 95 14 .45 22 .06 18 .85 18 .92 18 .85 z 8563 95 14 .35 22 .55 18 .75 18 .83 18 .81 W 1 8563 95 9 .84 18 .34 14 .96 15 .05 15 .84 W 2 8552 95 8 .90 17 .97 14 .05 14 .14 14 .81\nLARGESS, n = 1608 FUV 1265 79 16 .02 24 .40 21 .23 21 .23 20 .27 NUV 1373 85 15 .79 24 .92 20 .96 21 .02 19 .73 u 1549 96 15 .65 30 .36 20 .13 20 .10 20 .97 g 1583 98 14 .27 24 .16 19 .67 19 .72 20 .38 r 1582 98 14 .09 22 .21 19 .34 19 .43 18 .74 i 1579 98 13 .61 21 .49 19 .14 19 .26 18 .85 z 1579 98 13 .61 23 .30 19 .01 19 .13 18 .81 W 1 1468 91 10 .12 17 .77 15 .02 15 .16 14 .61 W 2 1466 91 8 .89 17 .04 14 .17 14 .32 15 .15\nOCARS, n = 3033 FUV 1434 47 13 .51 24 .90 20 .22 20 .31 20 .82 NUV 2035 67 13 .56 24 .66 20 .01 20 .01 21 .15 u 1170 39 12 .78 27 .47 19 .30 19 .18 18 .26 g 1255 41 12 .45 26 .34 18 .86 18 .78 18 .74 r 1374 45 12 .77 26 .28 18 .57 18 .55 18 .75 i 1264 42 11 .67 24 .09 18 .45 18 .48 18 .70 z 1205 40 12 .69 23 .13 18 .30 18 .34 18 .61 W 1 2562 84 7 .26 17 .56 14 .20 14 .33 13 .77 W 2 2562 84 6 .26 17 .23 13 .30 13 .39 13 .54\n\u2018\nD ow nloaded from https://academ ic.oup.com /m nras/article/512/2/2099/6546165 by Indian Institute of Technology Patna user on 19 January 2024\n(i) The Faint Ima g es of the Radio Sky at Twenty-Centimeters FIRST, Becker, White & Helfand 1995 ; White et al. 1997 ), which as 18 273 sources with redshifts from the SDSS DR14 QSOs (P \u0302 aris t al. 2018 ). Of these, 9016 are classed as QSOs in NED.\n(ii) The Large Area Radio Galaxy Evolution Spectroscopic Survey LARGESS). Of the 10 685 sources with optical redshifts (Ching t al. 2017 ), 6 1608 are classified as quasars in NED. Although termed\nThose with redshift reliability flag q \u2265 3, where q = 3 designates \u2018a easonably confident redshift\u2019, and the maximum q = 5 designates an extremely reliable redshift from a good-quality spectrum\u2019.\nS\n( a s v\nradio-loud\u2019, these have a similar distribution of radio fluxes as the DSS sample (Curran et al. 2021 ) and so we refer to these as QSOs. (iii) The Optical Characteristics of Astrometric Radio Sources OCARS) catalogue of very long baseline interferometry (VLBI) strometry sources (Ma et al. 2009 ; Malkin 2018 ). Of the 3663 ources, 2404 are classified as quasars, but given that these are ery strong radio calibration sources, we assume that all of the\nMNRAS 512, 2099\u20132109 (2022)\nM\nTable 5. The mean results for the various imputation methods o v er 10 trials, quoted with \u00b11 \u03c3 .\nImputation n r Un-normalized Normalized \u03bc z \u03c3 z \u03c3MAD \u03bc z(norm) \u03c3 z(norm) \u03c3NMAD\nFIRST None 5147 0.949 \u00b1 0.004 0.031 \u00b1 0.019 0.206 \u00b1 0.006 0.116 \u00b1 0.005 0.008 \u00b1 0.009 0.094 \u00b1 0.002 0.052 \u00b1 0.002 Uni v ariate Mean 9016 0.752 \u00b1 0.014 0.109 \u00b1 0.052 0.554 \u00b1 0.005 0.176 \u00b1 0.014 0.014 \u00b1 0.024 0.202 \u00b1 0.009 0.075 \u00b1 0.007 Median 9016 0.740 \u00b1 0.033 0.121 \u00b1 0.039 0.564 \u00b1 0.032 0.169 \u00b1 0.012 0.018 \u00b1 0.015 0.206 \u00b1 0.005 0.072 \u00b1 0.005 Most 9016 0.683 \u00b1 0.048 0.180 \u00b1 0.054 0.611 \u00b1 0.038 0.184 \u00b1 0.010 0.035 \u00b1 0.021 0.208 \u00b1 0.011 0.081 \u00b1 0.005 Max 9016 0.771 \u00b1 0.012 \u2212 0.141 \u00b1 0.026 0.597 \u00b1 0.028 0.172 \u00b1 0.009 \u2212 0.078 \u00b1 0.009 0.299 \u00b1 0.018 0.070 \u00b1 0.004 Random 9016 0.680 \u00b1 0.030 0.026 \u00b1 0.072 0.648 \u00b1 0.025 0.191 \u00b1 0.009 \u2212 0.024 \u00b1 0.027 0.276 \u00b1 0.108 0.081 \u00b1 0.005 Multi v ariate 9016 0.904 \u00b1 0.005 0.020 \u00b1 0.025 0.359 \u00b1 0.010 0.146 \u00b1 0.009 \u2212 0.004 \u00b1 0.011 0.155 \u00b1 0.006 0.060 \u00b1 0.001 kNN k = 3 9016 0.839 \u00b1 0.017 0.069 \u00b1 0.051 0.046 \u00b1 0.021 0.159 \u00b1 0.014 0.009 \u00b1 0.021 0.170 \u00b1 0.003 0.066 \u00b1 0.006 k = 10 9016 0.872 \u00b1 0.012 0.054 \u00b1 0.027 0.409 \u00b1 0.019 0.149 \u00b1 0.007 0.006 \u00b1 0.004 0.160 \u00b1 0.005 0.063 \u00b1 0.001 k = 20 9016 0.890 \u00b1 0.007 0.022 \u00b1 0.028 0.382 \u00b1 0.011 0.144 \u00b1 0.006 \u2212 0.006 \u00b1 0.013 0.155 \u00b1 0.005 0.060 \u00b1 0.002\nLARGESS None 1046 0.913 \u00b1 0.005 0.042 \u00b1 0.022 0.292 \u00b1 0.008 0.128 \u00b1 0.005 0.008 \u00b1 0.012 0.116 \u00b1 0.009 0.061 \u00b1 0.002 Uni v ariate Mean 1608 0.720 \u00b1 0.020 0.080 \u00b1 0.053 0.610 \u00b1 0.022 0.172 \u00b1 0.011 \u2212 0.005 \u00b1 0.023 0.271 \u00b1 0.031 0.077 \u00b1 0.006 Median 1608 0.723 \u00b1 0.012 0.075 \u00b1 0.033 0.605 \u00b1 0.012 0.175 \u00b1 0.020 \u2212 0.008 \u00b1 0.018 0.271 \u00b1 0.021 0.079 \u00b1 0.010 Most 1608 0.641 \u00b1 0.024 0.105 \u00b1 0.049 0.683 \u00b1 0.025 0.196 \u00b1 0.020 \u2212 0.005 \u00b1 0.022 0.305 \u00b1 0.025 0.088 \u00b1 0.009 Max 1608 0.809 \u00b1 0.013 \u2212 0.036 \u00b1 0.026 0.531 \u00b1 0.027 0.171 \u00b1 0.009 \u2212 0.041 \u00b1 0.013 0.297 \u00b1 0.021 0.074 \u00b1 0.003 Random 1608 0.621 \u00b1 0.025 0.024 \u00b1 0.046 0.742 \u00b1 0.040 0.190 \u00b1 0.008 \u2212 0.034 \u00b1 0.019 0.361 \u00b1 0.029 0.085 \u00b1 0.005 Multi v ariate 1608 0.829 \u00b1 0.012 0.028 \u00b1 0.025 0.488 \u00b1 0.017 0.157 \u00b1 0.006 \u2212 0.013 \u00b1 0.011 0.224 \u00b1 0.021 0.068 \u00b1 0.002 kNN k = 3 1608 0.814 \u00b1 0.010 0.023 \u00b1 0.024 0.511 \u00b1 0.018 0.162 \u00b1 0.007 \u2212 0.017 \u00b1 0.012 0.249 \u00b1 0.019 0.071 \u00b1 0.003 k = 10 1608 0.816 \u00b1 0.016 0.031 \u00b1 0.028 0.506 \u00b1 0.026 0.013 \u00b1 0.007 \u2212 0.013 \u00b1 0.015 0.242 \u00b1 0.027 0.070 \u00b1 0.003 k = 20 1608 0.811 \u00b1 0.010 0.044 \u00b1 0.048 0.513 \u00b1 0.022 0.161 \u00b1 0.007 \u2212 0.009 \u00b1 0.021 0.238 \u00b1 0.034 0.071 \u00b1 0.003\nOCARS None 649 0.867 \u00b1 0.010 0.048 \u00b1 0.043 0.339 \u00b1 0.003 0.144 \u00b1 0.013 0.008 \u00b1 0.025 0.159 \u00b1 0.014 0.066 \u00b1 0.007 Uni v ariate Mean 3033 0.482 \u00b1 0.020 0.181 \u00b1 0.015 0.823 \u00b1 0.012 0.476 \u00b1 0.017 0.015 \u00b1 0.015 0.337 \u00b1 0.009 0.218 \u00b1 0.009 Median 3033 0.483 \u00b1 0.011 0.114 \u00b1 0.055 0.821 \u00b1 0.013 0.478 \u00b1 0.016 \u2212 0.017 \u00b1 0.026 0.350 \u00b1 0.011 0.217 \u00b1 0.008 Most 3033 0.421 \u00b1 0.018 0.190 \u00b1 0.047 0.807 \u00b1 0.011 0.518 \u00b1 0.022 \u2212 0.002 \u00b1 0.008 0.224 \u00b1 0..08 0.238 \u00b1 0.006 Max 3033 0.346 \u00b1 0.048 0.281 \u00b1 0.165 0.943 \u00b1 0.049 0.638 \u00b1 0.060 0.041 \u00b1 0.073 0.411 \u00b1 0.032 0.313 \u00b1 0.039 Random 3033 0.282 \u00b1 0.014 0.070 \u00b1 0.100 1.043 \u00b1 0.040 0.720 \u00b1 0.039 \u2212 0.055 \u00b1 0.046 0.478 \u00b1 0.024 0.321 \u00b1 0.018 Multi v ariate 3033 0.597 \u00b1 0.011 0.240 \u00b1 0.050 0.722 \u00b1 0.009 0.379 \u00b1 0.013 0.050 \u00b1 0.024 0.284 \u00b1 0.008 0.183 \u00b1 0.007 kNN k = 3 3033 0.506 \u00b1 0.008 0.238 \u00b1 0.040 0.813 \u00b1 0.011 0.466 \u00b1 0.011 0.043 \u00b1 0.018 0.326 \u00b1 0.013 0.221 \u00b1 0.004 k = 10 3033 0.513 \u00b1 0.017 0.314 \u00b1 0.032 0.785 \u00b1 0.010 0.438 \u00b1 0.018 0.078 \u00b1 0.015 0.295 \u00b1 0.002 0.215 \u00b1 0.009 k = 20 3033 0.540 \u00b1 0.026 0.310 \u00b1 0.028 0.767 \u00b1 0.015 0.444 \u00b1 0.023 0.077 \u00b1 0.017 0.290 \u00b1 0.008 0.212 \u00b1 0.011\nTable 6. The statistics for the OCARS quasars with at least one SDSS magnitude (1420 sources), cf. Table 4 .\nMag. n per cent Min Max Mean Median Most\nFUV 946 67 13 .51 24 .90 20 .09 20 .15 20 .39 NUV 1059 75 13 .57 24 .66 19 .83 19 .71 20 .50 u 1170 82 12 .78 27 .47 19 .30 19 .18 18 .26 g 1255 88 12 .45 26 .34 18 .86 18 .78 18 .74 r 1374 97 12 .77 26 .28 18 .57 18 .55 18 .75 i 1264 89 11 .67 24 .09 18 .45 18 .48 18 .70 z 1205 85 12 .69 23 .13 18 .30 18 .34 18 .61 W 1 1269 89 8 .31 17 .56 14 .19 14 .33 14 .65 W 2 1268 89 7 .05 16 .55 13 .25 13 .34 14 .78\nn o\nb t\n2 i c w s 7 h\n2\nW S m n i w Q t r\nD ow nloaded from https://academ ic.oup.com /m nras/article/512/2/2099/6546165 by Indian Institute of Technology Patna user on 19 January 2024\non-galaxies are active galactic nuclei (AGNs), giving a sample size f 3033.\nAgain, the requirement of measured magnitudes in all nine ands has the effect of cutting the samples, leaving 65 per cent of he LARGESS sample, 57 per cent of the FIRST sample, and just\nNRAS 512, 2099\u20132109 (2022)\n1 per cent of the OCARS sample. The large fraction of missing data n the latter is due to a mismatch between this and the SDSS\u2019s sky o v erage, which is restricted to the northern sky (Fig. 3 ). In Fig. 4 , e show the distribution of the number of missing magnitudes per ource, from which we see that OCARS is most likely to have 5\u2013 magnitudes missing, in addition to a large proportion of sources aving no magnitude measurements at all.\n.2.2 Data imputation and machine learning\ne impute the missing values using the same methods as for the DSS data (Section 2.1.2 ). The only method we exclude is the aximum truncated, since the maximum values for the test data do\not indicate the presence of extreme outliers (Table 4 ). Following the mputation and upon returning the spectroscopic redshifts to the data, e train the model using the non-imputed SDSS photometry (72 276 SOs), which we then validate on each test sample, again comparing he predicted with the measured redshifts (Table 5 ). We see that eplacing the missing magnitudes via machine learning methods\nTable 7. As Table 5 , but for the OCARS sources with at least one SDSS magnitude measurement.\nImputation n r Un-normalized Normalized \u03bc z \u03c3 z \u03c3MAD \u03bc z(norm) \u03c3 z(norm) \u03c3NMAD\nOCARS ( \u22650 SDSS match) Uni v ariate Mean 1420 0.546 \u00b1 0.023 0.232 \u00b1 0.060 0.804 \u00b1 0.014 0.263 \u00b1 0.023 0.039 \u00b1 0.028 0.310 \u00b1 0.014 0.125 \u00b1 0.010 Median 1420 0.544 \u00b1 0.022 0.218 \u00b1 0.056 0.801 \u00b1 0.026 0.267 \u00b1 0.011 0.032 \u00b1 0.026 0.314 \u00b1 0.023 0.128 \u00b1 0.006 Most 1420 0.529 \u00b1 0.026 0.191 \u00b1 0.075 0.816 \u00b1 0.025 0.320 \u00b1 0.015 0.017 \u00b1 0.033 0.317 \u00b1 0.007 0.153 \u00b1 0.008 Max 1420 0.574 \u00b1 0.030 \u2212 0.032 \u00b1 0.058 0.826 \u00b1 0.035 0.292 \u00b1 0.012 \u2212 0.074 \u00b1 0.027 0.397 \u00b1 0.023 0.127 \u00b1 0.006 Random 1420 0.458 \u00b1 0.028 0.032 \u00b1 0.073 0.923 \u00b1 0.056 0.346 \u00b1 0.065 \u2212 0.060 \u00b1 0.033 0.428 \u00b1 0.044 0.157 \u00b1 0.010 Multi v ariate 1420 0.737 \u00b1 0.004 0.156 \u00b1 0.033 0.635 \u00b1 0.011 0.213 \u00b1 0.011 0.031 \u00b1 0.016 0.248 \u00b1 0.014 0.095 \u00b1 0.006 kNN k = 3 1420 0.699 \u00b1 0.017 0.174 \u00b1 0.059 0.674 \u00b1 0.015 0.232 \u00b1 0.017 0.034 \u00b1 0.029 0.256 \u00b1 0.014 0.150 \u00b1 0.008 k = 10 1420 0.715 \u00b1 0.014 0.189 \u00b1 0.040 0.652 \u00b1 0.013 0.214 \u00b1 0.009 0.040 \u00b1 0.019 0.240 \u00b1 0.010 0.099 \u00b1 0.004 k = 20 1420 0.686 \u00b1 0.030 0.199 \u00b1 0.061 0.679 \u00b1 0.030 0.227 \u00b1 0.014 0.041 \u00b1 0.010 0.248 \u00b1 0.013 0.105 \u00b1 0.008\n( s t a m w a m\nw s t L v i e o c\n3\n3\n3\nT w t t c s w m ( ( l\na m r s o c d m\n3\nA a u e t m b fl S s \u223c r r\nr r p W i L l f r i o\nw a d b e a r b t d u i\n3\nI m\nD ow nloaded from https://academ ic.oup.com /m nras/article/512/2/2099/6546165 by Indian Institute of Technology Patna user on 19 January 2024 multi v ariate and kNN imputation) significantly outperforms the imple (uni v ariate and random) methods, although replacement with he maximum value remains the best of these. The effectiveness of pplying the maximum value to the SDSS data likely stems from any of the missing values actually being due to sensitivity limits, hereas for data drawn independently of the SDSS (i.e. OCARS), larger fraction of the missing data are expected to be due to noneasurements. In order to place the OCARS data on a more equal par ith the other test samples, we now consider only the OCARS ources with at least one SDSS magnitude, thus being more likely o hav e o v erlapping coordinates (Table 6 ). Applying the Deep earning to this data (Table 7 ), we see again that the multiariate method is the best, although in general the results are nferior to those of the FIRST and LARGESS samples. Howver, e ven with the requirement of at least one SDSS magnitude, nly 46 per cent of the OCARS sources have the full magnitude omplement. DISCUSSION .1 Results .1.1 General raining and validating on the 100 337 strong SDSS sample, for hich 28 per cent of sources have at least one missing magniude, we find all of the imputation methods to be ef fecti ve, with he randomly selected 20 067 validation sources giving regression oefficients of r > \u223c 0 . 9 between the predicted and measured redhifts. These compare fa v ourably with r = 0.91 to 0.94 obtained hen using the source colours are features. The best-performing ethod was replacement by the maximum for the band in question r = 0.93), similar to the imputation method of Carvajal et al. 2021 ), who assume that each missing value is at the detection imit.\nTraining on the 72 276 QSOs that have all nine magnitudes nd testing on the three other, radio-selected, catalogues, we find ulti v ariate imputation to perform the best, although we only achieve = 0 . 60 for the OCARS sample. Ho we ver, only 47 per cent of the ources have an SDSS magnitude, due to the full-sky distribution f OCARS, and considering only these increases the regression oefficient to r = 0 . 74. This is still low, which we believe is ue to only 46 per cent of that 47 per cent having all of the nine agnitudes.\n.1.2 OCARS\ns discussed abo v e, the machine learning performs poorly when pplied to the OCARS data. This may not be surprising since, nlike the LARGESS and FIRST data (Ching et al. 2017 ; P \u0302 aris t al. 2018 ), OCARS is compiled independently of the SDSS. Also, he mean radio flux density for the OCARS sources is an order of\nagnitude higher than the others. These, unlike OCARS, appear to e truncated at the lower flux densities ( S radio < \u223c 1 mJy), suggesting a ux limitation (Curran et al. 2021 ). Hence, both the LARGESS and DSS data may be more representative of future continuum radio urv e ys, which are expected to be sensitive to flux densities of S radio\n0.1 mJy (e.g. Norris et al. 2011 ). We should, ho we ver, not yet ule out using the SDSS to train the NIR-optical-UV photometry of adio-loud sources.\nIn Fig. 5 , we show how the imputation affects the Deep Learning esults for the OCARS quasars as more missing magnitudes are eplaced. For all cases, we see that the redshift accuracy is relatively oor at z < \u223c 0 . 5, which was also noted for the non-imputed data. e (Curran et al. 2021 ) suggested that this was due to limitations n the FUV magnitudes, which are required to accurately trace the yman break at z \u223c 0, and in Fig. 6 , we see that the FUV has the argest fraction of missing values at low redshift. We also see that the raction of missing values increases across all bands with increasing edshift, thus requiring a larger degree of imputation. This is evident n Fig. 5 , where higher redshifts are attainable with the imputation f more missing values, although at a cost of inaccuracy. Imputing all of the missing magnitudes, we would retain the\nhole training sample. Knowing that the higher redshifts are less ccurate, we could, in principle, assign a confidence in the prediction ependent upon the redshift. But since in practice, the redshifts will e unknown a priori, we would require a means of approximately stimating these. Both Curran & Moss ( 2019 ) and Li et al. ( 2021 ) use two-step approach, first splitting the sources into low- and highedshift samples. Specifically, Curran & Moss used the correlation etween the W 2 magnitude and redshift (Glowacki et al. 2017 ), o obtain an approximate redshift before proceeding with a more etailed determination. Ho we ver, as sho wn in Fig. 7 , there is a large ncertainty and any W 2\u2013redshift relation will already be incorporated nto the Deep Learning.\n.2 Imputation limits\nn Fig. 8 , we show the degradation in the performance as more issing values are replaced for the three test data sets. While these\nMNRAS 512, 2099\u20132109 (2022)\nM\nFigure 5. The photometric versus the spectroscopic redshift for different numbers of imputed magnitudes (1\u20133, 1\u20135, and 1\u20138) per source for the OCARS sources with at least one SDSS match. The grey-scale shows the source distribution with the key on the bottom right showing the number within each pixel. The binning is for an equal number of sources in each bin ( n /10), with the error bars showing \u00b11 \u03c3 . The broken line shows z phot = z spec and the inset shows the distribution of z. Note that this is from a single Deep Learning trial and so the values should be considered approximate only.\na i D s s i h\nFigure 6. The fraction of missing magnitudes in each band (top panel) and the number of sources (bottom panel) for the OCARS sources with at least one SDSS match.\nFigure 7. The W 2 magnitude versus redshift for the SDSS sample. The binning of the 96 181 sources is for equal z spec spacing, with the error bars on the abscissa showing the range and the error bars on the ordinate \u00b11 \u03c3 .\ns n s t r w t w r\na i\nD ow nloaded from https://academ ic.oup.com /m nras/article/512/2/2099/6546165 by Indian Institute of Technology Patna user on 19 January 2024\nre examples of the o v erall performance, in order to obtain real nsight into how far it is sensible to impute, we require the mean eep Learning metrics for each number of missing magnitudes per ource: For the FIRST sample (Fig. 9 , top panels), we see that most ources have less than three missing magnitudes, and if we limit the mputation to this number, the predicted redshifts retain a relatively igh quality.\nNRAS 512, 2099\u20132109 (2022)\nFor the LARGESS sample (Fig. 9 , middle panels) we see a similar ituation, although the decrease in the number of sources as the umber of missing values increases is more gradual. Nevertheless, ince we start with a smaller sample, there are only 55 sources with hree missing magnitudes, falling to 16 with four. Again, the best esults cluster at less than three imputed magnitudes per source, ith the error bars indicating considerably less scatter between he Deep Learning runs. Note that there are only two sources ith six imputed magnitudes, resulting in the large errors, e.g. = 0 . 4 \u00b1 0 . 9.\nFor the OCARS sample (Fig. 9 , bottom panels), again we see large drop in performance when more than two magnitudes are mputed. We note that in all three cases, the normalized median\nFigure 8. The spread in z as more missing magnitudes are imputed for each source using the multi v ariate method. From a single Deep Learning run, the standard deviations are approximate.\na t m i h S s h S s w c t m\nt o\n7\nm c t\n9 f v\ns\n\u03c3 0\n\u03c3 2 e e 2 c M e\n4\nT t t t m r d w m m m p s > t <\ns p m w fi i v i d w o o b T u S t\nD ow nloaded from https://academ ic.oup.com /m nras/article/512/2/2099/6546165 by Indian Institute of Technology Patna user on 19 January 2024\nbsolute deviation (the metric chosen by Carvajal et al. 2021 ) appears o be the clearest tracer of the degradation of the performance as more issing magnitudes are imputed. As discussed in Section 2.2.1 , and llustrated in Fig. 3 , ho we ver, around half of the OCARS sources ave no SDSS magnitudes due to their location in the sky. Since the KA and its pathfinders are restricted to surv e ying the southern ky, we can expect very little overlap with the SDSS, thus not aving all of the magnitudes required to predict redshifts. Ho we ver, kyMapper (Wolf et al. 2018 ), 7 which is currently surv e ying the outhern sky, uses similar bands to the SDSS ( u , v, g , r , i , z), and so e expect a model trained upon SDSS data to still be applicable. Of ourse, once the SkyMapper catalogue becomes sufficiently large, he photometry from these sources can also be used to build a\nodel. In order to demonstrate the performance of the test data with up to wo missing magnitudes, from the original un-imputed data, we retain nly those with less than three missing values before performing\nhttps://skymapper.anu.edu.au\np f\nulti v ariate imputation. These are then trained using the same model, reated using the non-imputed SDSS data, on all three sets. We show he results in Fig. 10 , from which we see:\n(i) FIRST: r \u2248 0.93 and \u03c3 NMAD \u2248 0.055, while retaining 3 per cent of the data, compared to r = 0 . 95 and \u03c3NMAD = 0 . 052 or the 57 per cent remaining when the sources with any missing alues are excluded.\n(ii) LARGESS: r \u2248 0.87 and \u03c3 NMAD \u2248 0.063 for 94 per cent of the ample, compared to r = 0 . 91 and \u03c3NMAD = 0 . 061 for 65 per cent.\n(iii) OCARS, with at least one SDSS magnitude, r \u2248 0.84 and NMAD \u2248 0.068 for 80 per cent, compared to r = 0 . 87 and \u03c3NMAD = . 066 for 46 per cent.\nThe NMAD values are within the range of those in the literature, NMAD = 0.029 (Laurino et al. 2011 ), 0.012\u20130.058 (Brescia et al. 013 ), 0.060\u20130.065 (Ananna et al. 2017 ), 0.014\u20130.401 (Duncan t al. 2018 ), 0.026 (D\u2019Isanto & Polsterer 2018 ), > 0.016 (Beck t al. 2021 ), 0.091 (Carvajal et al. 2021 ) and > 0.079 (Li et al. 021 ). Unlike these, ho we ver, our predictions are from an optially selected training model applied to radio selected samples. oreo v er, these include sources with up to two missing magnitudes ach.\nC O N C L U S I O N S\nhe requirement of all nine FUV , NUV , u , g , r , i , z, W 1, W 2 bands o optimally predict photometric redshifts using machine learning echniques can cause a significant reduction in the number of sources o which the model is applicable. We therefore explore various ethods of imputation to replace the missing data, comparing the esults with those of the non-imputed data. For the SDSS training ata, 28 per cent of the sources have incomplete photometry, and e find simple imputation to be ef fecti ve, particularly replacing the issing magnitude with the maximum value for that band. This ethod is similar to that of Carvajal et al. ( 2021 ), who assume that issing data are at the detection limit. All of the tested methods erform well, ho we ver, gi ving a regression coefficient of the leastquares linear fit between the predicted and measured redshifts of r 0.9 and a normalized median absolute deviation, which is found o be the best metric to quantify the effect of imputation, of \u03c3 NMAD 0.06. Our aim is to use Deep Learning to train models for radio-selected ources similar to those expected to be detected by the SKA and its athfinders. For our three test data sets, the number of sources with issing magnitudes ranges from 35 per cent to 79 per cent, which ould clearly benefit from ef fecti ve imputation. Testing these, we nd simple imputation to produce inferior models to multi v ariate mputation, which uses machine learning to replace the missing alues based upon the features in the data. We suggest that this s since a large fraction of the missing SDSS data will indeed be ue to sensitivity limits (in particular the u , g , r , i , z magnitudes), hereas for the other data sets, these may just not have been bserved. This will certainly be the case for the OCARS sources, f which only about half o v erlap the same region of sky surveyed y the SDSS with only half again having all of the magnitudes. his will also be an issue for the SKA and its pathfinders, although sing SkyMapper, which will observe similar optical bands as the DSS o v er the southern sky, to obtain the photometry will address his. Testing v arious le vels of imputation, we note a steep decrease in erformance when more than two missing magnitudes are replaced, or all of the test data sets. Also, the fraction of sources with more\nMNRAS 512, 2099\u20132109 (2022)\nM\nFigure 9. The performance of the Deep Learning for different numbers of imputed magnitudes per source for the FIRST (top panels) & LARGESS (middle panels) QSOs and OCARS quasars with at least one SDSS magnitude (bottom panels). The error bars show the \u00b11 \u03c3 from the mean of 10 trials.\nt i p w b o s\nl S t\np f ( S w v c D g o p\nD ow nloaded from https://academ ic.oup.com /m nras/article/512/2/2099/6546165 by Indian Institute of Technology Patna user on 19 January 2024\nhan two missing values is small. We therefore suggest limiting the mputation to this number per source and, applying this, we find the erformance to be similar to that of the non-imputed data, although ith significant increases in sample sizes. We also find the results to e comparable to those in the literature, in which all, but one, use nly complete data and all of which are tested upon the same data et used to train the model. Of the three training sets, two (FIRST & LARGESS) do have a arge o v erlap with the SDSS and, with mean radio flux densities of radio \u223c 20 mJy (Curran et al. 2021 ), these may be representative of he radio continuum sources which will be detected with the SKA\nNRAS 512, 2099\u20132109 (2022)\nathfinders (e.g. Norris et al. 2011 ). The other training set, OCARS, orms a VLBI calibration catalogue and so has much higher fluxes S radio = 340 mJy). Furthermore, OCARS only o v erlaps with the DSS o v er one half of the sky. Ho we ver, if we select OCARS sources ith at least one SDSS magnitude, imputing up to two missing alues returns 80 per cent of the sample while yielding an NMAD omparable to those discussed abo v e. The fact that application of the eep Learning method on imputed data from disparate data bases ives similarly good results makes us confident in the applicability f such techniques to radio sources detected with the SKA and its athfinders.\nF m o l\nA\nI T b C\nA D o\nD\nD\nR\nA A B\nB\nB B B B\nC\nC C C C D D G\nH\nH L L L\nL M M\nM N P R S S T\nv W W W W\nT\nD ow nloaded from https://academ ic.oup.com /m nras/article/512/2/2099/6546165 by Indian Institute of Technology Patna user on 19 January 2024 igure 10. The results a single Deep Learning run on the test data sets issing up to two magnitudes per source. The binning is for an equal number f sources in each bin (100) with the error bars showing \u00b11 \u03c3 . The broken ine shows z phot = z spec and the inset the distribution of z. C K N OW L E D G E M E N T S\nwish to thank the referee for their prompt and helpful comments. his research has made use of the NASA/IPAC Extragalactic Data ase (NED), which is operated by the Jet Propulsion Laboratory, alifornia Institute of Technology, under contract with the National\neronautics and Space Administration and NASA\u2019s Astrophysics ata System Bibliographic Service. This research has also made use f NASA\u2019s Astrophysics Data System Bibliographic Service.\nATA AVAI LABI LI TY\nata and SDSS TensorFlow training model available on request.\nEFERENCES\nlam S. et al., 2015, ApJS , 219, 12 nanna T. T. et al., 2017, ApJ , 850, 66 eck R., Dobos L., Budav \u0301ari T., Szalay A. S., Csabai I., 2017, Astron.\nComput. , 19, 34 eck R., Szapudi I., Flewelling H., Holmberg C., Magnier E., 2021, MNRAS ,\n500, 1633 ecker R. H., White R. L., Helfand D. J., 1995, ApJ , 450, 559 ianchi L., Shiao B., Thilker D., 2017, ApJS , 230, 24 ovy J. et al., 2012, ApJ , 749, 41 rescia M., Cavuoti S., D\u2019Abrusco R., Longo G., Mercurio A., 2013, ApJ,\n772, 140 arvajal R., Matute I., Afonso J., Amarantidis S., Barbosa D., Cunha P.,\nHumphrey A., 2021, A New Window on the Radio Emission from Galaxies, Galaxy Clusters and Cosmic Web: Current Status and Perspectives hing J. H. Y. et al., 2017, MNRAS , 464, 1306 urran S. J., 2020, MNRAS , 493, L70 urran S. J., Moss J. P., 2019, A&A , 629, A56 urran S. J., Moss J. P., Perrott Y. C., 2021, MNRAS , 503, 2639 \u2019Isanto A., Polsterer K. L., 2018, A&A, 609, 111 uncan K. J. et al., 2018, MNRAS , 473, 2655 lowacki M., Allison J. R., Sadler E. M., Moss V. A., Jarrett T. H., 2017,\nMNRAS, preprint ( arXiv:1709.08634 ) an B., Ding H.-P., Zhang Y.-X., Zhao Y.-H., 2016, Res. Astron. Astrophys. ,\n16, 74 ildebrandt H. et al., 2010, A&A , 523, A31 aurino O., D\u2019Abrusco R., Longo G., Riccio G., 2011, MNRAS, 418, 2165 i C. et al., 2021, MNRAS , 509, 2289 ittle R. J. A., Rubin D. B., 1986, Statistical Analysis with Missing Data.\nJohn Wiley & Sons, New York, NY uken K. J., Norris R. P., Park L. A. F., 2019, PASP , 131, 108003 a C. et al., 2009, IERS Tech. Note, 35, 1 addox N., Hewett P. C., P \u0301eroux C., Nestor D. B., Wisotzki L., 2012,\nMNRAS , 424, 2876 alkin Z., 2018, ApJS , 239, 20 orris R. P. et al., 2011, Publ. Astron. Soc. Aust. , 28, 215  \u0302 aris I. et al., 2018, A&A, 613, A51 ichards G. T. et al., 2001, AJ , 122, 1151 alvato M., Ilbert O., Hoyle B., 2019, Nat. Astron. , 3, 212 krutskie M. F. et al., 2006, AJ , 131, 1163 royanskaya O., Cantor M., Sherlock G., Brown P., Hastie T., Tibshirani R.,\nBotstein D., Altman R. B., 2001, Bioinformatics, 17, 520 an Buuren S., Groothuis-Oudshoorn K., 2011, J. Stat. Softw., 45, 1 einstein M. A. et al., 2004, ApJS , 155, 243 hite R. L., Becker R. H., Helfand D. J., Gregg M. D., 1997, ApJ , 475, 479 olf C. et al., 2018, Publ. Astron. Soc. Aust. , 35, 10 right E. L. et al., 2010, AJ , 140, 1868\nhis paper has been typeset from a T E X/L A T E X file prepared by the author.\nMNRAS 512, 2099\u20132109 (2022)"
        }
    ],
    "title": "Quasar photometric redshifts from incomplete data using deep learning",
    "year": 2022
}