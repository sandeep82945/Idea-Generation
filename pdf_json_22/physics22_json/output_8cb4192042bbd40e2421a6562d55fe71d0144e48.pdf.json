{
    "abstractText": "Compressed fluorescence lifetime imaging (Compressed-FLIM) is a novel Snapshot compressive imaging (SCI) method for single-shot widefield FLIM. This approach has the advantages of high temporal resolution and deep frame sequences, allowing for the analysis of FLIM signals that follow complex decay models. However, the precision of CompressedFLIM is limited by reconstruction algorithms. To improve the reconstruction accuracy of Compressed-FLIM in dealing with large-scale FLIM problem, we developed a more effective combined prior model 3DTGp V_net, based on the Plug and Play (PnP) framework. Extensive numerical simulations indicate the proposed method eliminates reconstruction artifacts caused by the Deep denoiser networks. Moreover, it improves the reconstructed accuracy by around 4dB (peak signal-to-noise ratio; PSNR) over the state-of-the-art TV+FFDNet in test data sets. We conducted the single-shot FLIM experiment with different Rhodamine reagents and the results show that in practice, the proposed algorithm has promising reconstruction performance and more negligible lifetime bias.",
    "authors": [
        {
            "affiliations": [],
            "name": "Chao Ji"
        },
        {
            "affiliations": [],
            "name": "Xing Wang"
        },
        {
            "affiliations": [],
            "name": "Kai He"
        },
        {
            "affiliations": [],
            "name": "Yanhua Xue"
        },
        {
            "affiliations": [],
            "name": "Yahui Li"
        },
        {
            "affiliations": [],
            "name": "Liwei Xin"
        },
        {
            "affiliations": [],
            "name": "Wei Zhao"
        },
        {
            "affiliations": [],
            "name": "Jinshou TianID"
        },
        {
            "affiliations": [],
            "name": "Liang Sheng"
        }
    ],
    "id": "SP:e9d1d0da1346b6eb12d3f8965d29845cba505eb0",
    "references": [
        {
            "authors": [
                "Y Ouyang",
                "Y Liu",
                "ZM Wang",
                "Z Liu",
                "M. Wu"
            ],
            "title": "FLIM as a Promising Tool for Cancer Diagnosis and Treatment Monitoring",
            "venue": "Nano-Micro Lett",
            "year": 2021
        },
        {
            "authors": [
                "L. Marcu"
            ],
            "title": "Fluorescence Lifetime Techniques in Medical Applications",
            "venue": "Ann Biomed Eng",
            "year": 2012
        },
        {
            "authors": [
                "Z Wang",
                "P Stamatoglou",
                "Z Li",
                "M Ald\u00e9n",
                "M. Richter"
            ],
            "title": "Ultra-high-speed PLIF imaging for simultaneous visualization of multiple species in turbulent flames",
            "venue": "Opt Express",
            "year": 2017
        },
        {
            "authors": [
                "B Zhou",
                "C Brackmann",
                "Q Li",
                "Z Wang",
                "P Petersson",
                "Z Li"
            ],
            "title": "Distributed reactions in highly turbulent premixed methane/air flames. Combustion and Flame",
            "year": 2015
        },
        {
            "authors": [
                "GO Fruhwirth",
                "S Ameer-Beg",
                "R Cook",
                "T Watson",
                "T Ng",
                "F. Festy"
            ],
            "title": "Fluorescence lifetime endoscopy using TCSPC for the measurement of FRET in live cells",
            "venue": "Opt Express",
            "year": 2010
        },
        {
            "authors": [
                "S Isbaner",
                "N Karedla",
                "D Ruhlandt",
                "SC Stein",
                "A Chizhik",
                "I Gregor"
            ],
            "title": "Dead-time correction of fluorescence lifetime measurements and fluorescence lifetime imaging",
            "venue": "Opt Express",
            "year": 2016
        },
        {
            "authors": [
                "RV Krishnan",
                "H Saitoh",
                "H Terada",
                "VE Centonze",
                "B. Herman"
            ],
            "title": "Development of a multiphoton fluorescence lifetime imaging microscopy system using a streak camera",
            "venue": "Review of Scientific Instruments",
            "year": 2003
        },
        {
            "authors": [
                "AC Ulku",
                "C Bruschini",
                "IM Antolovic",
                "Y Kuo",
                "R Ankri",
                "S Weiss"
            ],
            "title": "A 512 \u00d7 512 SPAD Image Sensor With Integrated Gating for Widefield FLIM",
            "venue": "IEEE J Select Topics Quantum Electron",
            "year": 2019
        },
        {
            "authors": [
                "V Zickus",
                "M-L Wu",
                "K Morimoto",
                "V Kapitany",
                "A Fatima",
                "A Turpin"
            ],
            "title": "Fluorescence lifetime imaging with a megapixel SPAD camera and neural network lifetime estimation",
            "venue": "PMID:",
            "year": 2098
        },
        {
            "authors": [
                "Y Ma",
                "Y Lee",
                "C Best-Popescu",
                "L. Gao"
            ],
            "title": "High-speed compressed-sensing fluorescence lifetime imaging microscopy of live cells",
            "venue": "Proc Natl Acad Sci USA",
            "year": 2004
        },
        {
            "authors": [
                "SV Venkatakrishnan",
                "CA Bouman",
                "B. Wohlberg"
            ],
            "title": "Plug-and-Play priors for model based reconstruction",
            "venue": "IEEE Global Conference on Signal and Information Processing. Austin, TX, USA: IEEE;",
            "year": 2013
        },
        {
            "authors": [
                "X. Yuan"
            ],
            "title": "Generalized alternating projection based total variation minimization for compressive sensing",
            "venue": "IEEE International Conference on Image Processing (ICIP). Phoenix, AZ, USA: IEEE;",
            "year": 2016
        },
        {
            "authors": [
                "T Ehret",
                "P. Arias"
            ],
            "title": "Implementation of the VBM3D Video Denoising Method and Some Variants",
            "venue": "[cs]",
            "year": 2020
        },
        {
            "authors": [
                "Y Liu",
                "X Yuan",
                "J Suo",
                "DJ Brady",
                "Q. Dai"
            ],
            "title": "Rank Minimization for Snapshot Compressive Imaging",
            "venue": "IEEE Trans Pattern Anal Mach Intell",
            "year": 2019
        },
        {
            "authors": [
                "K Zhang",
                "W Zuo",
                "L. Zhang"
            ],
            "title": "FFDNet: Toward a Fast and Flexible Solution for CNN based Image Denoising",
            "venue": "IEEE Trans on Image Process",
            "year": 2018
        },
        {
            "authors": [
                "M Tassano",
                "J Delon",
                "T. Veit"
            ],
            "title": "FastDVDnet: Towards Real-Time Deep Video Denoising Without Flow Estimation",
            "venue": "[cs, eess]. 2020 [cited",
            "year": 2021
        },
        {
            "authors": [
                "X Yuan",
                "Y Liu",
                "J Suo",
                "Q. Dai"
            ],
            "title": "Plug-and-Play Algorithms for Large-Scale Snapshot Compressive Imaging",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Seattle, WA, USA: IEEE;",
            "year": 2020
        },
        {
            "authors": [
                "H Qiu",
                "Y Wang",
                "D. Meng"
            ],
            "title": "Effective Snapshot Compressive-spectral Imaging via Deep Denoising and Total Variation Priors",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Nashville, TN, USA: IEEE;",
            "year": 2021
        },
        {
            "authors": [
                "J Yao",
                "D Qi",
                "C Yang",
                "F Cao",
                "Y He",
                "P Ding"
            ],
            "title": "Multichannel-coupled compressed ultrafast photography",
            "venue": "J Opt",
            "year": 2020
        },
        {
            "authors": [
                "K Bredies",
                "K Kunisch",
                "T. Pock"
            ],
            "title": "Total Generalized Variation",
            "venue": "SIAM J Imaging Sci",
            "year": 2010
        },
        {
            "authors": [
                "H Zhang",
                "L Wang",
                "B Yan",
                "L Li",
                "A Cai",
                "G. Hu"
            ],
            "title": "Constrained Total Generalized p-Variation Minimization for Few-View X-Ray Computed Tomography Image Reconstruction",
            "venue": "Zeng L, editor. PLoS ONE",
            "year": 2016
        },
        {
            "authors": [
                "Y Le Montagner",
                "E Angelini",
                "J-C. Olivo-Marin"
            ],
            "title": "Video reconstruction using compressed sensing measurements and 3d total variation regularization for bio-imaging applications",
            "venue": "IEEE International Conference on Image Processing",
            "year": 2012
        },
        {
            "authors": [
                "C Ji",
                "J Tian",
                "L Sheng",
                "K He",
                "L Xin",
                "X Yan"
            ],
            "title": "Reconstruction of compressed video via non-convex minimization",
            "venue": "AIP Advances",
            "year": 2020
        },
        {
            "authors": [
                "S Cai",
                "K Liu",
                "M Yang",
                "J Tang",
                "X Xiong",
                "M. Xiao"
            ],
            "title": "A new development of non-local image denoising using fixed-point iteration for non-convex lp sparse optimization",
            "venue": "Hatt M, editor. PLoS ONE",
            "year": 2085
        },
        {
            "authors": [
                "W Zuo",
                "D Meng",
                "L Zhang",
                "X Feng",
                "D. Zhang"
            ],
            "title": "A Generalized Iterated Shrinkage Algorithm for Non-convex Sparse Coding",
            "venue": "IEEE International Conference on Computer Vision. Sydney, Australia:",
            "year": 2013
        }
    ],
    "sections": [
        {
            "text": "Compressed fluorescence lifetime imaging (Compressed-FLIM) is a novel Snapshot com-\npressive imaging (SCI) method for single-shot widefield FLIM. This approach has the advan-\ntages of high temporal resolution and deep frame sequences, allowing for the analysis of\nFLIM signals that follow complex decay models. However, the precision of Compressed-\nFLIM is limited by reconstruction algorithms. To improve the reconstruction accuracy of\nCompressed-FLIM in dealing with large-scale FLIM problem, we developed a more effective\ncombined prior model 3DTGp V_net, based on the Plug and Play (PnP) framework. Extensive numerical simulations indicate the proposed method eliminates reconstruction artifacts\ncaused by the Deep denoiser networks. Moreover, it improves the reconstructed accuracy\nby around 4dB (peak signal-to-noise ratio; PSNR) over the state-of-the-art TV+FFDNet in\ntest data sets. We conducted the single-shot FLIM experiment with different Rhodamine\nreagents and the results show that in practice, the proposed algorithm has promising recon-\nstruction performance and more negligible lifetime bias."
        },
        {
            "heading": "1. Introduction",
            "text": "Widefield fluorescence lifetime imaging (FLIM) is widely used in biomedical diagnostics and flow quantitative measurements, such as cancer diagnosis and treatment monitoring [1, 2], identifying species concentration from reactive-flow systems [3], and understanding the transient evolutionary behavior of eddies in highly turbulent flames [4]. Most of these examples are non-repeatable transient events that demand a single-shot widefield measurement method. However, performing high precision widefield lifetime measurements and quantitative analyses have always been a significant challenge in this field.\nThe traditional widefield FLIM approaches, including time-correlated single-photon count-\ning (TCSPC) [5, 6], streak camera [7], and single-photon avalanche diode (SPAD) [8, 9] possess high temporal resolution. Nevertheless, they require repeated measurements to obtain the widefield fluorescence lifetime. Recently, a snapshot compressive imaging (SCI) method,\nPLOS ONE | https://doi.org/10.1371/journal.pone.0271441 August 12, 2022 1 / 14\na1111111111\nOPEN ACCESS\nCitation: Ji C, Wang X, He K, Xue Y, Li Y, Xin L, et al. (2022) Compressed fluorescence lifetime imaging via combined TV-based and deep priors. PLoS ONE 17(8): e0271441. https://doi.org/ 10.1371/journal.pone.0271441\nEditor: Li Zeng, Chongqing University, CHINA\nReceived: February 11, 2022\nAccepted: June 30, 2022\nPublished: August 12, 2022\nPeer Review History: PLOS recognizes the benefits of transparency in the peer review process; therefore, we enable the publication of all of the content of peer review and author responses alongside final, published articles. The editorial history of this article is available here: https://doi.org/10.1371/journal.pone.0271441\nCopyright: \u00a9 2022 Ji et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.\nData Availability Statement: All relevant data are within the manuscript and its Supporting Information files\nFunding: The authors received no specific funding for this work.\ncompressed ultrafast photography (CUP), has emerged as a potential solution for snapshot widefield FLIM [10]. Compared to traditional methods, CUP is the only passive 2D technology with picosecond to femtosecond time resolution, which can acquire complete 2D transient processes within a snapshot.\nThe CUP system is a combination of the streak camera and compressive sensing methods.\nThe typical CUP process is to map 3D encoded data onto a 2D detection array, and then restore the original information through compressed sensing algorithms. However, the data reconstruction step of CUP is a complex task. Significantly, the reconstruction quality of the image deteriorates rapidly with increasing sequence depth. To solve this issue, numerous algorithms have been designed through the exploration of underlying sparsity structures. Plug and Play(PnP) [11] is a typical SCI framework that allows the matching of flexible state-of-the-art forward models with advanced priors or denoising models. On this basis, GAP-TV has become a popular low memory and fast SCI algorithm that combines generalized alternating projection (GAP) and Total Variation (TV) [12]. Denoiser based on block similarity such as blockmatching and 3D filtering (BM3D) [13] and weighted nuclear norm minimization (WNNM) [14] enjoy more effective sparsity representation than TV. However, these methods have high computational complexity and often take several hours, while the TV algorithm only takes a few minutes. As a result, BM3D and WNNM are rarely used in Compressed-FLIM, especially when real-time imaging is required.\nIn contrast to conventional denoisers, Deep denoiser networks such as FFDNet [15] and\nFastDVDnet [16, 17] resolve the common sparsity representation problem in local similarity and motion compensation while enjoying fast computing speed. However, due to limited priors with the training sets, Deep denoiser networks are required to extract artifacts in the reconstruction process, leading to confusing results. To take advantage of both the Deep denoiser network and TV model, H. Qiu et al. proposed a combined denoiser TV+FFDNet and achieved superior performance to previous algorithms [18].\nInspired by combined priors, we further explore a more effective combination of traditional denoisers and Deep denoiser networks. In this paper, we devise a 3DTGp V denoiser by exploring the underlying sparsity of signals in space-time and the superiority of the non-convex \u2113p(0 <p< 1) norm in minimizing convergence. Meanwhile, by further combining the video denoising network FastDVDnet, we develop a novel combination prior, named 3DTGp V_net. We make various simulations based on the CUP framework and determine that the proposed 3DTGp V_net prior offers a ~4dB improvement in peak signal-to-noise ratio (PSNR) compared with the TV+FFDNet prior in runner test sets. Meanwhile, the reconstruction artifacts caused by Deep denoiser networks are successfully eliminated. Besides, we conduct a widefield Compressed-FLIM experiment and obtain 70 consecutive high-resolution images within a single snapshot. Compared with the lifetime bias of the reconstructed data with TV+FFDNet, our method provides higher lifetime evaluation accuracy."
        },
        {
            "heading": "2. Principle of compressed-FLIM",
            "text": "A schematic diagram of the compressed ultrafast photography-FLIM (Compressed-FLIM) is illustrated in Fig 1. It comprises three parts: generation of widefield fluorescence signals, data acquisition, and data reconstruction. Unlike the previous scheme [10], we use a transmissive mask rather than the reflective a digital mirror device (DMD) as the spatial encoder."
        },
        {
            "heading": "2.1 Generation of widefield fluorescence signals",
            "text": "A 515nm femtosecond laser (200fs) beam passes a cylindrical lens into a laser sheet. The laser sheet illuminates a Rhodamine water solution. Behind the Rhodamine solution, a 515nm filter\nPLOS ONE | https://doi.org/10.1371/journal.pone.0271441 August 12, 2022 2 / 14\nCompeting interests: The authors have declared that no competing interests exist.\nis positioned to filter excitation light. The fluorescence signals pass through a pre-designed circular mask (M1) cut with a cross to highlight spatial recognition, generating shaped fluorescence signals. The diameter of M1 is 35mm."
        },
        {
            "heading": "2.2 Data acquisition",
            "text": "After passing through a lens, the shaped signals are divided into two beams by a beam splitter (BS). One sub-signal is directly detected with an external charge-coupled device (CCD) image sensor (Hamamatsu C11440). The other is spatially encoded through a binary mask M2 and recorded by a Streak Camera (XIOPM 5200). The layout of M2 is a random pattern with a pixel resolution of 250 \u00d7 250, and the size of a single-pixel is 20 \u00d7 20\u03bcm. To ensure entire imaging of the targets, the slit of the Streak Camera is fully open (~5 mm), and the image plane of the Streak Camera is adjusted at M2.\nIn the acquisition section of Streak Camera, the encoded signal undergoes photoelectric\nconversion at the cathode, then the electric signals at different times are deflected by the slope voltage to various positions on the fluorescent screen. Finally, photons are emitted and collected by the internal CCD (512 \u00d7 512 binned pixels; 4 \u00d7 4 binning). The size of the binned pixels is 26 \u00d7 26 \u03bcm.\nA DMD is the typical encoder in the CUP system. However, for weak fluorescence acquisi-\ntion, the fixed binary mask [19] significantly improves the signal-to-noise ratio (SNR) by its transmission characteristics. We randomly generated multiple groups of coding layouts through MATLAB, and selected the best coding layout through simulation results. The signals transmittance rate is ultimately set to 25%."
        },
        {
            "heading": "2.3 Data reconstruction",
            "text": "The fluorescence signals can be regarded as a data cube I(x, y, t). In the external CCD view, the cube is directly integrated along the time direction, and the measured data from the CCD can\nhttps://doi.org/10.1371/journal.pone.0271441.g001\nPLOS ONE | https://doi.org/10.1371/journal.pone.0271441 August 12, 2022 3 / 14\nbe expressed as Ec = R I(x, y, t)dt. From the perspective of Streak Camera, operator T carries out spatial coding of the cube, and operator S executes the shearing of signals from the coding cube to the tilted coding cube. Ultimately, the accumulation of tilted coding cubes along the time direction is represented by operator C. The entire data acquisition process in Streak Camera view can be described as Es = TSCI(x, y, t). Data reconstruction is an ill-condition inverse process. Adding sparsity constraints to the least-squares method realizes the stable reconstruction of the algorithm. The optimization problem of CUP-FLIM can be expressed as:\nargmin I k TSCI x; y; t\u00f0 \u00de Es k 2 2 \u00fem k\nZ\nI x; y; t\u00f0 \u00dedt Ec k 2 2 \u00fel\u03c6 I\u00f0 \u00de \u00f01\u00de\nwhere the first and the second terms are fidelity terms with data collected by the Streak Camera and external CCD, respectively. The last term \u03c6(I) represents the prior used to impose sparsity features to signals while \u03bc and \u03bb are weight parameters. In the next section, we will describe the implementation process of the proposed algorithm and the innovative sophisticated prior.\n3. Reconstruction algorithm\n3.1 3DTGpV priors Prior plays a key role in the reconstruction algorithms of compressed sensing. The \u21130 norm prior is the sparsest representation, as it counts the number of nonzero entries in signals. However, it is extremely challenging to process numerically. For solving the dilemma of algorithms without convergence, Donoho. et al. verified the approximate equivalence of the \u21131 and \u21130 norms [20].\nFormally, the \u21131 norm minimization can be expressed as\nargmin u k Au b k2 2 \u00felkuk 1 \u00f02\u00de\nIn the research area of images, by considering the spatial smoothing properties of natural\nsignals, the generalized form of \u21131 norm total variation (TV) has been proved to be far sparser when applying the minimization principle of image gradient shown in Fig 2A. The TV prior obeys\nFTV u\u00f0 \u00de \u00bc X\ni;j;t\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi\njui;j;t ui 1;j;tj 2 \u00fe jui;j;t ui;j 1;tj\n2 \ufffd \ufffd 2\nr\n: \u00f03\u00de\nNext, we will briefly introduce three generalized forms (3DTV, TGV, TpV) based on the TV prior that strengthen sparsity representation.\nWe define\nDi \u00bc jui;j;t ui 1;j;t j\n\u00f0a\u00de\n;Dj \u00bc jui;j;t ui;j 1;tj\n\u00f0b\u00de\n;Dt \u00bc jui;j;t ui;j;t 1j\n\u00f0c\u00de\n: \u00f04\u00de\n3.1.1 Stretch in the spatial domain\u2013TGV. Total generalized variation (TGV) is a second-\norder gradient minimization proposed by Kunisch. et al. [21]. It incorporates more adjacent elements than TV and regards the second-order gradient of images as the sparse coefficients, as Fig 2B illustrates. For mathematical imaging problems, TGV is an effective approach that enhances the details of high-frequency signals and eliminates staircasing effects [22]. It can be\nPLOS ONE | https://doi.org/10.1371/journal.pone.0271441 August 12, 2022 4 / 14\nexpressed as\nFG u\u00f0 \u00de \u00bc X\ni;j;t\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi\nDi\u00fe1 Di \ufffd2 \u00fe Dj\u00fe1 Dj \ufffd \ufffd22\nr\n: \u00f05\u00de\n3.1.2 Stretch in the time domain\u2013 3DTV. The TV prior merely considers image similar-\nity in continuous 2D space but ignores the similarity of adjacent elements [23] in the time direction. 3DTV introduces the 3D sparsity constraint of fluorescence signals shown in Fig 2C, and can be represented as\nF3D u\u00f0 \u00de \u00bc X\ni;j;t\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi\nD2i \u00fe D2j \u00fe D2t \ufffd \ufffd 2\nr\n: \u00f06\u00de\nFurthermore, given that the time-domain correlation decreases with the increase of motion\nscale, we add a time-domain weight parameter \u03c4 (0\ufffd \u03c4\ufffd 1) to flexibly balance the relevancy between motion scale and time-domain correlation. Therefore, Eq (6) can be rewritten as\nF3D u\u00f0 \u00de \u00bc X\ni;j;t\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi\nD2i \u00fe D2j \u00fe tD2t \ufffd \ufffd 2\nr\n: \u00f07\u00de\n3.1.3 \u2113p(0 <p< 1) norm of gradient\u2013TpV. The \u2113p (0 <p< 1) norm is defined as Xn\ni\u00bc1 jxij\np \ufffd \ufffd1=p\n, which is closer to \u21130 norm than \u21131 norm in mathematical form, thus\napproaching the sparsest solution. Our previous work has proved that non-convex optimization algorithm based on \u2113p norm has more vital sparsity constraints even when it belongs to non-convex optimization problem [24]. With superior sparsity performance, the TpV prior eliminates artifacts and achieves superior reconstruction results [25, 26]. The TpV prior is expressed as\nFp u\u00f0 \u00de \u00bc X\ni;j;t\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi\nDpi \u00fe D p j\n\ufffd p q\n\u00f08\u00de\nhttps://doi.org/10.1371/journal.pone.0271441.g002\nPLOS ONE | https://doi.org/10.1371/journal.pone.0271441 August 12, 2022 5 / 14\nBy combining the different merits of the three TV-based priors, we proposed the following\n3DTGpV prior:\nC u\u00f0 \u00de \u00bc X\ni;j;t\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi\nDi\u00fe1 Di \ufffdp \u00fe Dj\u00fe1 Dj \ufffd \ufffdp \u00fe t Dt\u00fe1 Dt \ufffdpp\nr\n: \u00f09\u00de\n3.2 PnP- 3DTGpV_net algorithms In this section, we propose a novel algorithm based on the PnP-framework 3DTGp V_net prior by combining 3DTGp V prior and Deep denoiser network.\nWe will introduce the overall algorithm flow presented in Fig 3 and Algorithm 1. The sparse\nsignals of interest u are rebuilt by determining the minimum solution from the following constrained formula\nu t\u00f0 \u00de; v t\u00f0 \u00de\u00f0 \u00de \u00bc arg minu;v 1\n2 k u v k2 2 \u00felC v\u00f0 \u00de\nsubject to Asu \u00bc bs; Acu \u00bc bc ; \u00f010\u00de\nwhere bs denotes data measured by Streak Camera. bc signifies data measured by the external CCD, As and As represent the corresponding projection matrices, respectively, v is an auxiliary variable, and \u03bb is an added weight.\nAccording to the generalized alternating projection (GAP) algorithm [12], we update the\nfidelity and prior term separately. Fig 3 displays the workflow of the PnP-3DTGp V_net algorithm. For each iteration stage, we first apply the Euclidean projection for updating u(t):\nu t\u00f0 \u00de \u00bc v t 1\u00f0 \u00de \u00fe As> AsAs>\u00f0 \u00de 1 bs Asv t 1\u00f0 \u00de\u00f0 \u00de \u00fe \u03bcAc> AcAc>\u00f0 \u00de 1 bc Acv t 1\u00f0 \u00de\u00f0 \u00de\n\ufffd \ufffd\n1\u00fe \u03bc \u00f011\u00de\nThe update of v is a denoising problem, we execute the denoising process by using 3DTGpV and FastDVDnet [16], respectively\nFor the 3DTGpV update section\nv t\u00f0 \u00de1 \u00bc u t\u00f0 \u00de C > z t\u00f0 \u00de \ufffd ; \u00f012\u00de\nz t\u00f0 \u00de \u00bc clip z t 1\u00f0 \u00de \u00fe 1 a C v t 1\u00f0 \u00de \ufffd ; l 2\n\ufffd \ufffd\n; \u00f013\u00de\nhttps://doi.org/10.1371/journal.pone.0271441.g003\nPLOS ONE | https://doi.org/10.1371/journal.pone.0271441 August 12, 2022 6 / 14\nwhere\nclip s;T\u00f0 \u00de :\u00bc s; if jsj \ufffd T\nTsign s\u00f0 \u00de; otherwise \u00f014\u00de\n(\nFor the Deep denoiser network update section\nv t\u00f0 \u00de2 \u00bc FastDvDnet v t\u00f0 \u00de 1 \ufffd \ufffd \u00f015\u00de\nAlgorithm 1. PnP- 3DTGp V_net framework.\nInput As, Ac, bs, bc, Given p 2 (0,1):\nInitialize v0 = As\u2019bs, \u03bc = 0.1,\u03bb = 0.07\nfor iteration in range(0, 250):\n1. Update streak camera\u2019s reconstruction data u t\u00f0 \u00desc by u t\u00f0 \u00desc \u00bc v t 1\u00f0 \u00de \u00fe As> AsAs>\u00f0 \u00de 1 bs Av t 1\u00f0 \u00de\u00f0 \u00de;\n2. Update CCD\u2019s reconstruction data u t\u00f0 \u00deCCD by u t\u00f0 \u00deCCD \u00bc v t 1\u00f0 \u00de \u00fe Ac> AcAc>\u00f0 \u00de 1 bc Acv t 1\u00f0 \u00de\u00f0 \u00de;\n3. Update ut by ut \u00bc x t\u00f0 \u00desc \u00fe \u03bcu t\u00f0 \u00de CCD\n\ufffd \ufffd =\u00f0 1\u00fe \u03bc\u00de;\n4. 3DTGpV denoising: Update v t\u00f0 \u00de 1 by v t\u00f0 \u00de 1 \u00bc u t\u00f0 \u00de C > z t\u00f0 \u00de\u00f0 \u00de where z t\u00f0 \u00de \u00bc clip z t 1\u00f0 \u00de \u00fe 1\na C v t 1\u00f0 \u00de\u00f0 \u00de; l 2\n\ufffd ;\n5. Deep network denoising: Update v t\u00f0 \u00de2 by v t\u00f0 \u00de 2 \u00bc FastDvDnet v t\u00f0 \u00de 1 \ufffd \ufffd ;\n6. Update v(t) by v t\u00f0 \u00de \u00bc v t\u00f0 \u00de2\nObtain reconstruction result: u"
        },
        {
            "heading": "4 Simulation results",
            "text": "In the simulation, we compare the reconstruction performances of six priors (TV, 3DTGpV, BM3D, TV+FFDNet, TV+FastDVDnet, and 3DTGp V_net) by applying widely-used drop and runner datasets. Each dataset comprises 30 video clips. For initialization, we set v(0) = AsTbs, z(0) = 0, \u03bb = 0.07, \u03bc = 0.1, and \u03c4 = 0.2. Each algorithm performs 250 iterations independently. For related Deep network, we directly use the FFDNet model and parameters from https:// github.com/cszn/KAIR. Besides, the FastDVDnet model and parameters are from https:// github.com/m-tassano/fastdvdnet. The drop and runner datasets are from https://github.com/ zsm1211/PnP-SCI/tree/master/dataset/simdata/benchmark.\nFigs 4 and 5 present the reconstructed frames (seen in Visualization 1 and Visualization\n2) restored with different priors using the two above-mentioned datasets. The 3DTGpV prior achieves superior detailed than the TV prior. Although BM3D has a more rigorous denoising ability than the TV-based methods, excessive smoothing leads to the loss of image detail. The combined priors based on TV and Deep denoiser networks (TV+FFDNet and TV+FastDVDnet) provide better reconstruction contrast and detail than traditional priors but they expose unsatisfactory artifacts in the reconstructed images. Our proposed combined prior 3DTGp V_net succeed in eliminating artifacts, leading to more accurate representations of the original images.\nPLOS ONE | https://doi.org/10.1371/journal.pone.0271441 August 12, 2022 7 / 14\nWe evaluate the quality of the reconstructed images by two indicators: peak signal-to-noise\nratio (PSNR) and structural similarity (SSIM).\nThe PSNR can be calculated by\nPSNR \u00bc 10 \ufffd log 10 2552 \ufffdmn Pm 1n 1\ni\u00bc0 \u00bdx\u00f0i; j\u00de y\u00f0i; j\u00de\ufffd2\n0\nB B @\n1\nC C A \u00f016\u00de\nwhere x and y represents the original image and the reconstructed image, respectively.m and n indicates the height and width of the image, respectively.\nhttps://doi.org/10.1371/journal.pone.0271441.g004\nhttps://doi.org/10.1371/journal.pone.0271441.g005\nPLOS ONE | https://doi.org/10.1371/journal.pone.0271441 August 12, 2022 8 / 14\nThe SSIM can be calculated by\nSSIM\u00f0x; y\u00de \u00bc 2mxmy \u00fe c1 \ufffd \ufffd 2sxy \u00fe c2 \ufffd \ufffd\nm2x \u00fe m 2 y \u00fe c1 \ufffd \ufffd s2x \u00fe s 2 y \u00fe c2 \ufffd \ufffd \u00f017\u00de\nwhere \u03bcx and \u03bcy represents the mean value of the original image and the reconstructed image, respectively. s2x and s 2 y are the corresponding variance. \u03c3xy means the covariance.\nTable 1 presents the average PSNR and SSIM results. We can conclude that the 3DTGp V_net prior outperforms the other priors in both PSNR and SSIM. Significantly, the proposed prior improve the reconstructed accuracy by approximately 4dB (PSNR) over the state-of-theart TV+FFDNet in runner test sets."
        },
        {
            "heading": "5. Experiments",
            "text": "In the experiments, we record widefield fluorescence data of Rhodamine 6G and Rhodamine B by CUP. The results are reconstructed using both the PnP\u2014TV+FFDNet algorithm and the proposed PnP - 3DTGp V_net algorithm. We set the CUP time resolution to be 330 ps. The reconstruction process is implemented in Ubuntu 20.04 with an NVIDIA GeForce GTX 1650Ti GPU.\nFig 6A presents the streak camera measurement data for Rhodamine 6G. The scanning direction of the data is from top to bottom. Fig 6B and 6C show the widefield fluorescence data rebuilt by the PnP-TV+FFDNet and PnP - 3DTGp V_net algorithms, respectively. Also, the reconstructed movies are shown in Visualization 3 and Visualization 4. By comparing the two sets of data, it is apparent that our proposed algorithm achieves smoother reconstruction results with fewer artifacts.\nIn Fig 7A, the measured data of Rhodamine B is displayed. It has a shorter glow duration\nthan Rhodamine 6G. The corresponding rebuild results are shown in Fig 7B and 7C, and the movies are presented in Visualization 5 and Visualization 6. The results indicate that the proposed algorithm achieves better detail reconstruction in various fluorescence environments.\nTo further analyze the measurement accuracy of widefield fluorescence lifetime, we imple-\nment the exponential fitting with the least square method, based on a mono-exponential decay model for each pixel [27].\nThe measured decay h(t) can be expressed as\nh t\u00f0 \u00de \u00bc irf t\u00f0 \u00de \ufffd Aexp t t\n\ufffd \ufffd\n\u00fe \u03b5 \u00f018\u00de\nwhere A represents the amplitude, \u03c4 denotes the lifetime, \u03b5 signifies noise, and irf (t) is the instrument response function (IRF) of the measurement system. Since the full width at half\nhttps://doi.org/10.1371/journal.pone.0271441.t001\nPLOS ONE | https://doi.org/10.1371/journal.pone.0271441 August 12, 2022 9 / 14\nhttps://doi.org/10.1371/journal.pone.0271441.g006\nhttps://doi.org/10.1371/journal.pone.0271441.g007\nPLOS ONE | https://doi.org/10.1371/journal.pone.0271441 August 12, 2022 10 / 14\nmaximum (FWMH) of the laser pulse is 200fs, irf (t) can be regarded as a delta function for fluorescence decays with nanosecond lifetimes.\nFig 8A and 8B display the two groups of 2D lifetime images rebuilt using the PnP-TV+-\nFFDNet and PnP-3DTGp V_net algorithms, respectively. Besides, Fig 9 shows the reconstruction lifetime bias. For Rhodamine 6G (R6G), the mean lifetime and standard deviation of the proposed algorithm are 3.91 ns and 0.57 ns, respectively. Also, the corresponding values for PnP-TV+FFDNet are 4.41 ns and 1.1 ns. For Rhodamine B (RB), the mean lifetime and standard deviation of the proposed algorithm are 1.68 ns and 0.52 ns, while for PnP-TV+FFDNet they are 1.72 ns and 0.54 ns.\nIn the slit-scanning mode of the Streak Camera, we re-obtain non-superimposed fluores-\ncence lifetime data as a reference. The single exponential fitting results of Rhodamine 6G and Rhodamine B are 3.62 ns and 1.51 ns, respectively. These improved results demonstrate that our proposed PnP-3DTGp V_net algorithm produces a bias that is 0.29 ns and 0.17 ns lower than the PnP-TV+FFDNet algorithm."
        },
        {
            "heading": "6. Conclusion",
            "text": "In this study, we propose 3DTGp V_net, a highly effective Compressed-FLIM combined prior. Results from numerous simulations and experiments confirm that our proposed method has better reconstruction performance than the existing algorithms and presents higher evaluation accuracy for wide-field FLIM. Besides, this study further confirms that combined priors can\nhttps://doi.org/10.1371/journal.pone.0271441.g008\nPLOS ONE | https://doi.org/10.1371/journal.pone.0271441 August 12, 2022 11 / 14\neffectively complement the advantages of traditional priors and Deep denoiser networks to improve the reconstruction performance of compressive video imaging technology. Lastly, it is noted that our algorithm is a general framework and demanding to relevant SCI systems.\nSupporting information\nS1 Video. Drop. (AVI)\nS2 Video. Runner. (AVI)\nS3 Video. R6G(3DTGpV_net). (AVI)\nS4 Video. R6G(TV+FFDnet). (AVI)\nhttps://doi.org/10.1371/journal.pone.0271441.g009\nPLOS ONE | https://doi.org/10.1371/journal.pone.0271441 August 12, 2022 12 / 14\nS5 Video. RB(3DTGpV_net). (AVI)\nS6 Video. RB(TV+FFDnet). (AVI)"
        },
        {
            "heading": "Acknowledgments",
            "text": "The authors would also like to thank Prof. Jinshou Tian and Prof. Liang Sheng for their valuable help and contribution.\nAuthor Contributions\nConceptualization: Chao Ji, Liang Sheng.\nData curation: Kai He, Yahui Li, Liang Sheng.\nFormal analysis: Yahui Li.\nFunding acquisition: Yanhua Xue, Liwei Xin, Wei Zhao.\nInvestigation: Kai He, Wei Zhao.\nMethodology: Xing Wang, Liang Sheng.\nProject administration: Kai He, Wei Zhao.\nResources: Liwei Xin, Wei Zhao.\nSoftware: Yanhua Xue.\nSupervision: Yanhua Xue, Liwei Xin.\nVisualization: Liwei Xin.\nWriting \u2013 original draft: Chao Ji.\nWriting \u2013 review & editing: Xing Wang, Yahui Li, Jinshou Tian, Liang Sheng."
        }
    ],
    "title": "Compressed fluorescence lifetime imaging via combined TV-based and deep priors",
    "year": 2022
}