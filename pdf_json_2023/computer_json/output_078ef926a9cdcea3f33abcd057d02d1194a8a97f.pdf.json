{
    "abstractText": "Deep neural networks (DNNs) are nowadays ubiquitous in many domains such as computer vision. However, due to their high latency, the deployment of DNNs hinges on the development of compression techniques such as quantization which consists in lowering the number of bits used to encode the weights and activations. Growing concerns for privacy and security have motivated the development of data-free techniques, at the expanse of accuracy. In this paper, we identity the uniformity of the quantization operator as a limitation of existing approaches, and propose a data-free non-uniform method. More specifically, we argue that to be readily usable without dedicated hardware and implementation, non-uniform quantization shall not change the nature of the mathematical operations performed by the DNN. This leads to search among the continuous automorphisms of (R+,\u00d7), which boils down to the power functions defined by their exponent. To find this parameter, we propose to optimize the reconstruction error of each layer: in particular, we show that this procedure is locally convex and admits a unique solution. At inference time, we show that our approach, dubbed PowerQuant, only require simple modifications in the quantized DNN activation functions. As such, with only negligible overhead, it significantly outperforms existing methods in a variety of configurations.",
    "authors": [
        {
            "affiliations": [],
            "name": "Edouard Yvinec"
        },
        {
            "affiliations": [],
            "name": "Kevin Bailly"
        }
    ],
    "id": "SP:74eb18ba240bcc243ad24a73235873ab1ac9a8d2",
    "references": [
        {
            "authors": [
                "Ron Banner",
                "Yury Nahshan",
                "Daniel Soudry"
            ],
            "title": "Post training 4-bit quantization of convolutional networks for rapid-deployment",
            "venue": "NeurIPS, pp",
            "year": 2019
        },
        {
            "authors": [
                "Yash Bhalgat",
                "Jinwon Lee",
                "Markus Nagel",
                "Tijmen Blankevoort",
                "Nojun Kwak"
            ],
            "title": "Lsq+: Improving low-bit quantization through learnable offsets and better initialization",
            "venue": "CVPR Workshops,",
            "year": 2020
        },
        {
            "authors": [
                "Yaohui Cai",
                "Zhewei Yao",
                "Zhen Dong",
                "Amir Gholami",
                "Michael W Mahoney",
                "Kurt Keutzer"
            ],
            "title": "Zeroq: A novel zero shot quantization framework",
            "year": 2020
        },
        {
            "authors": [
                "Wenlin Chen",
                "James Wilson"
            ],
            "title": "Compressing neural networks with the hashing trick",
            "venue": "ICML, pp. 2285\u20132294,",
            "year": 2015
        },
        {
            "authors": [
                "Vladimir Chikin",
                "Vladimir Kryzhanovskiy"
            ],
            "title": "Channel balancing for accurate quantization of winograd convolutions",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Yoni Choukroun",
                "Eli Kravchik",
                "Fan Yang",
                "Pavel Kisilev"
            ],
            "title": "Low-bit quantization of neural networks for efficient inference",
            "venue": "ICCV Workshops,",
            "year": 2019
        },
        {
            "authors": [
                "Guo Cong",
                "Qiu Yuxian",
                "Leng Jingwen",
                "Gao Xiaotian",
                "Zhang Chen",
                "Liu Yunxin",
                "Yang Fan",
                "Zhu Yuhao",
                "Guo Minyi"
            ],
            "title": "Squant: On-the-fly data-free quantization via diagonal hessian approximation",
            "year": 2022
        },
        {
            "authors": [
                "Andrew R Conn",
                "Katya Scheinberg",
                "Ph L Toint"
            ],
            "title": "On the convergence of derivative-free methods for unconstrained optimization. Approximation theory and optimization: tributes to MJD",
            "year": 1997
        },
        {
            "authors": [
                "J. Deng",
                "W. Dong"
            ],
            "title": "ImageNet: A Large-Scale Hierarchical Image Database",
            "year": 2009
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova. Bert"
            ],
            "title": "Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805,",
            "year": 2018
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "year": 2021
        },
        {
            "authors": [
                "Jun Fang",
                "Ali Shafiee",
                "Hamzah Abdel-Aziz",
                "David Thorsley",
                "Georgios Georgiadis",
                "Joseph H Hassoun"
            ],
            "title": "Post-training piecewise linear quantization for deep neural networks",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan Frankle",
                "Michael Carbin"
            ],
            "title": "The lottery ticket hypothesis: Finding sparse, trainable neural networks",
            "year": 2018
        },
        {
            "authors": [
                "Sahaj Garg",
                "Anirudh Jain",
                "Joe Lou",
                "Mitchell Nahmias"
            ],
            "title": "Confounding tradeoffs for neural network quantization",
            "venue": "arXiv preprint arXiv:2102.06366,",
            "year": 2021
        },
        {
            "authors": [
                "Amir Gholami",
                "Sehoon Kim",
                "Zhen Dong",
                "Zhewei Yao",
                "Michael W Mahoney",
                "Kurt Keutzer"
            ],
            "title": "A survey of quantization methods for efficient neural network inference",
            "venue": "arXiv preprint arXiv:2103.13630,",
            "year": 2021
        },
        {
            "authors": [
                "Yunchao Gong",
                "Liu Liu",
                "Ming Yang",
                "Lubomir Bourdev"
            ],
            "title": "Compressing deep convolutional networks using vector quantization",
            "venue": "arXiv preprint arXiv:1412.6115,",
            "year": 2014
        },
        {
            "authors": [
                "Zbigniew Hajduk"
            ],
            "title": "High accuracy fpga activation function implementation for neural",
            "venue": "networks. Neurocomputing,",
            "year": 2017
        },
        {
            "authors": [
                "Kun Han",
                "Yuxuan Wang",
                "DeLiang Wang",
                "William S Woods",
                "Ivo Merks",
                "Tao Zhang"
            ],
            "title": "Learning spectral mapping for speech dereverberation and denoising",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,",
            "year": 2015
        },
        {
            "authors": [
                "Song Han",
                "Huizi Mao",
                "William J Dally"
            ],
            "title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding",
            "year": 2016
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "CVPR, pp",
            "year": 2016
        },
        {
            "authors": [
                "Horst Herrlich"
            ],
            "title": "Axiom of choice, volume 1876",
            "year": 2006
        },
        {
            "authors": [
                "Geoffrey Hinton",
                "Oriol Vinyals",
                "Jeff Dean"
            ],
            "title": "Distilling the knowledge in a neural network",
            "venue": "NeurIPS,",
            "year": 2014
        },
        {
            "authors": [
                "Gao Huang",
                "Zhuang Liu",
                "Laurens Van Der Maaten",
                "Kilian Q Weinberger"
            ],
            "title": "Densely connected convolutional networks",
            "year": 2017
        },
        {
            "authors": [
                "Benoit Jacob",
                "Skirmantas Kligys",
                "Bo Chen",
                "Menglong Zhu",
                "Matthew Tang",
                "Andrew Howard",
                "Hartwig Adam",
                "Dmitry"
            ],
            "title": "Kalenichenko. Quantization and training of neural networks for efficient integer-arithmetic-only inference",
            "year": 2018
        },
        {
            "authors": [
                "Yongkweon Jeon",
                "Baeseong Park",
                "Se Jung Kwon",
                "Byeongwook Kim",
                "Jeongin Yun",
                "Dongsoo Lee"
            ],
            "title": "Biqgemm: matrix multiplication with lookup table for binary-coding-based quantized dnns",
            "venue": "SC20: International Conference for High Performance Computing, Networking, Storage and Analysis,",
            "year": 2020
        },
        {
            "authors": [
                "Yongkweon Jeon",
                "Chungman Lee",
                "Eulrang Cho",
                "Yeonju Ro"
            ],
            "title": "Mr. biq: Post-training non-uniform quantization based on minimizing the reconstruction error",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Sehoon Kim",
                "Amir Gholami",
                "Zhewei Yao",
                "Michael W Mahoney",
                "Kurt Keutzer"
            ],
            "title": "I-bert: Integeronly bert quantization",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "Raghuraman Krishnamoorthi"
            ],
            "title": "Quantizing deep convolutional networks for efficient inference: A whitepaper",
            "venue": "arXiv preprint arXiv:1806.08342,",
            "year": 2018
        },
        {
            "authors": [
                "Maximilian Lam",
                "Michael Mitzenmacher",
                "Vijay Janapa Reddi",
                "Gu-Yeon Wei",
                "David Brooks"
            ],
            "title": "Tabula: Efficiently computing nonlinear activation functions for secure neural network inference",
            "venue": "arXiv preprint arXiv:2203.02833,",
            "year": 2022
        },
        {
            "authors": [
                "Yuhang Li",
                "Xin Dong",
                "Wei Wang"
            ],
            "title": "Additive powers-of-two quantization: An efficient nonuniform discretization for neural networks",
            "venue": "arXiv preprint arXiv:1909.13144,",
            "year": 2019
        },
        {
            "authors": [
                "Zhikai Li",
                "Liping Ma",
                "Mengjuan Chen",
                "Junrui Xiao",
                "Qingyi Gu"
            ],
            "title": "Patch similarity aware data-free quantization for vision transformers",
            "venue": "arXiv preprint arXiv:2203.02250,",
            "year": 2022
        },
        {
            "authors": [
                "Daisuke Miyashita",
                "Edward H Lee",
                "Boris Murmann"
            ],
            "title": "Convolutional neural networks using logarithmic data representation",
            "venue": "arXiv preprint arXiv:1603.01025,",
            "year": 2016
        },
        {
            "authors": [
                "Pavlo Molchanov",
                "Arun Mallya",
                "Stephen Tyree",
                "Iuri Frosio",
                "Jan Kautz"
            ],
            "title": "Importance estimation for neural network pruning",
            "year": 2019
        },
        {
            "authors": [
                "Markus Nagel",
                "Mart van Baalen"
            ],
            "title": "Data-free quantization through weight equalization and bias correction",
            "venue": "ICCV, pp",
            "year": 2019
        },
        {
            "authors": [
                "John A Nelder",
                "Roger Mead"
            ],
            "title": "A simplex method for function minimization",
            "venue": "The computer journal,",
            "year": 1965
        },
        {
            "authors": [
                "Michael JD Powell"
            ],
            "title": "An efficient method for finding the minimum of a function of several variables without calculating derivatives",
            "venue": "The computer journal,",
            "year": 1964
        },
        {
            "authors": [
                "Charbel Sakr",
                "Steve Dai",
                "Rangha Venkatesan",
                "Brian Zimmer",
                "William Dally",
                "Brucek Khailany"
            ],
            "title": "Optimal clipping and magnitude-aware differentiation for improved quantization-aware training",
            "venue": "In ICML,",
            "year": 2022
        },
        {
            "authors": [
                "Mark Sandler",
                "Andrew Howard"
            ],
            "title": "Mobilenetv2: Inverted residuals and linear bottlenecks",
            "venue": "CVPR, pp",
            "year": 2018
        },
        {
            "authors": [
                "Mingxing Tan",
                "Quoc V Le"
            ],
            "title": "Efficientnet: Rethinking model scaling for convolutional neural networks",
            "venue": "ICML, pp",
            "year": 2019
        },
        {
            "authors": [
                "Hugo Touvron",
                "Matthieu Cord",
                "Matthijs Douze",
                "Francisco Massa",
                "Alexandre Sablayrolles",
                "Herv\u00e9 J\u00e9gou"
            ],
            "title": "Training data-efficient image transformers & distillation through attention",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Mart van Baalen",
                "Brian Kahne",
                "Eric Mahurin",
                "Andrey Kuzmin",
                "Andrii Skliar",
                "Markus Nagel",
                "Tijmen Blankevoort"
            ],
            "title": "Simulated quantization, real power savings",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel Bowman"
            ],
            "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP,",
            "year": 2018
        },
        {
            "authors": [
                "Jiaxiang Wu",
                "Cong Leng",
                "Yuhang Wang",
                "Qinghao Hu",
                "Jian Cheng"
            ],
            "title": "Quantized convolutional neural networks for mobile devices",
            "year": 2016
        },
        {
            "authors": [
                "Shoukai Xu",
                "Haokun Li",
                "Bohan Zhuang",
                "Jing Liu",
                "Jiezhang Cao",
                "Chuangrun Liang",
                "Mingkui Tan"
            ],
            "title": "Generative low-bitwidth data free quantization",
            "venue": "ECCV, pp",
            "year": 2020
        },
        {
            "authors": [
                "Edouard Yvinec",
                "Arnaud Dapogny",
                "Kevin Bailly"
            ],
            "title": "To fold or not to fold: a necessary and sufficient condition on batch-normalization layers folding. IJCAI, 2022a",
            "year": 2022
        },
        {
            "authors": [
                "Edouard Yvinec",
                "Arnaud Dapogny",
                "Matthieu Cord",
                "Kevin Bailly"
            ],
            "title": "Spiq: Data-free per-channel static input quantization",
            "venue": "arXiv preprint arXiv:2203.14642,",
            "year": 2022
        },
        {
            "authors": [
                "Dongqing Zhang",
                "Jiaolong Yang",
                "Dongqiangzi Ye",
                "Gang Hua"
            ],
            "title": "Lq-nets: Learned quantization for highly accurate and compact deep neural networks",
            "year": 2018
        },
        {
            "authors": [
                "Sai Qian Zhang",
                "Bradley McDanel",
                "HT Kung",
                "Xin Dong"
            ],
            "title": "Training for multi-resolution inference using reusable quantization terms",
            "venue": "In Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Xiangguo Zhang",
                "Haotong Qin",
                "Yifu Ding",
                "Ruihao Gong",
                "Qinghua Yan",
                "Renshuai Tao",
                "Yuhang Li",
                "Fengwei Yu",
                "Xianglong Liu"
            ],
            "title": "Diversifying sample generation for accurate data-free quantization",
            "year": 2021
        },
        {
            "authors": [
                "Yichi Zhang",
                "Zhiru Zhang",
                "Lukasz Lew"
            ],
            "title": "Pokebnn: A binary pursuit of lightweight accuracy",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Ritchie Zhao",
                "Yuwei Hu",
                "Jordan Dotzel",
                "Chris De Sa",
                "Zhiru Zhang"
            ],
            "title": "Improving neural network quantization without retraining using outlier channel splitting",
            "year": 2019
        },
        {
            "authors": [
                "Zhou"
            ],
            "title": "HOW TO PERFORM MATRIX MULTIPLICATION WITH POWERQUANT The proposed PowerQuant method preserves the multiplication operations, i.e. a multiplication in the floating point space remains a multiplication in the quantized space (integers). This allows one to leverage current implementations of uniform quantization available on most hardware Gholami et al",
            "year": 2016
        },
        {
            "authors": [
                "Bhalgat"
            ],
            "title": "ResNet50 as it can also be quantized using asymmetric quantization although in our research, we only applied asymmetric quantization to SilU and GeLU based architectures. We included these results in the appendix of the revised article",
            "year": 2020
        },
        {
            "authors": [
                "SQuant Cong"
            ],
            "title": "2022) the author claim that it is better to minimize the absolute sum of errors rather than the sum of absolute errors and achieve good performance in data-free quantization",
            "venue": "I IMPROVEMENT WITH RESPECT TO QAT In the introduction,",
            "year": 2022
        },
        {
            "authors": [
                "Sakr"
            ],
            "title": "2022), both using dynamic quantization (i.e. estimating the ranges of the activations",
            "year": 2022
        },
        {
            "authors": [
                "Lam"
            ],
            "title": "However these considerations are hardware agnostic",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Deep neural networks (DNNs) tremendously improved algorithmic solutions for a wide range of tasks. In particular, in computer vision, these achievements come at a consequent price, as DNNs deployment bares a great energetic price. Consequently, the generalization of their usage hinges on the development of compression strategies. Quantization is one of the most promising such technique, that consists in reducing the number of bits needed to encode the DNN weights and/or activations, thus limiting the cost of data processing on a computing device.\nExisting DNN quantization techniques, for computer vision tasks, are numerous and can be distinguished by their constraints. One such constraint is data usage, as introduced in Nagel et al. (2019), and is based upon the importance of data privacy and security concerns. Data-free approaches such as Banner et al. (2019); Cai et al. (2020); Choukroun et al. (2019); Fang et al. (2020); Garg et al. (2021); Zhao et al. (2019); Nagel et al. (2019), exploit heuristics and weight properties in order to perform the most efficient weight quantization without having access to the training data. As compared to data-driven methods, the aforementioned techniques are more convenient to use but usually come with higher accuracy loss at equivalent compression rates. Data-driven methods performance offer an upper bound on what can be expected from data-free approaches and in this work, we aim at further narrowing the gap between these methods.\nTo achieve this goal, we propose to leverage a second aspect of quantization: uniformity. For simplicity reasons, most quantization techniques such as Nagel et al. (2019); Zhao et al. (2019); Cong et al. (2022) perform uniform quantization, i.e. they consist in mapping floating point values to an evenly spread, discrete space. However, non-uniform quantization can theoretically provide a closer fit to the network weight distributions, thus better preserving the network accuracy. Previous work on non-uniform quantization either focused on the search of binary codes (Banner et al., 2019;\nar X\niv :2\n30 1.\n09 85\n8v 1\n[ cs\n.C V\n] 2\n4 Ja\nn 20\n23\nHubara et al., 2016; Jeon et al., 2020; Wu et al., 2016; Zhang et al., 2018) or leverage logarithmic distribution (Miyashita et al., 2016; Zhou et al., 2017). However, these approaches map floating point multiplications operations to other operations that are hard to leverage on current hardware (e.g. bit-shift) as opposed to uniform quantization which maps floating point multiplications to integer multiplications (Gholami et al., 2021; Zhou et al., 2016). To circumvent this limitation and reach a tighter fit between the quantized and original weight distributions, in this work, we propose to search for the best possible quantization operator that preserves the nature of the mathematical operations. We show that this search boils down to the space defined by the continuous automorphisms of (R\u2217+,\u00d7), which is limited to power functions defined by their exponent. We optimize the value of this parameter by minimizing the error introduced by quantization. This allows us to reach superior accuracy, as illustrated in Fig 1. To sum it up, our contributions are:\n\u2022 We search for the best quantization operator that do not change the nature of the mathematical operations performed by the DNN, i.e. the automorphisms of (R\u2217+,\u00d7). We show that this search can be narrowed down to finding the best exponent for power functions.\n\u2022 We find the optimal exponent parameter to more closely fit the original weight distribution compared with existing (e.g. uniform and logarithmic) baselines. To do so, we propose to optimize the quantization reconstruction error. We show that this problem is locally convex and admits a unique solution.\n\u2022 In practice, we show that the proposed approach, dubbed PowerQuant, only requires simple modifications in the quantized DNN activation functions. Furthermore, we demonstrate through extensive experimentation that our method achieves outstanding results on various and challenging benchmarks with only negligible computational overhead."
        },
        {
            "heading": "2 RELATED WORK",
            "text": ""
        },
        {
            "heading": "2.1 QUANTIZATION",
            "text": "In this section, we provide a background on the current state of DNNs quantization. Notice that while certain approaches are geared towards memory footprint reduction (e.g. without quantizing inputs and activations) (Chen et al., 2015; Gong et al., 2014; Han et al., 2016; Zhou et al., 2017), in what follows, we essentially focus on methods that aim at reducing the inference time. In particular, motivated by the growing concerns for privacy and security, data-free quantization methods (Banner et al., 2019; Cai et al., 2020; Choukroun et al., 2019; Fang et al., 2020; Garg et al., 2021; Zhao et al., 2019; Nagel et al., 2019; Cong et al., 2022) are emerging and have significantly improved over the recent years.\nThe first breakthrough in data-free quantization (Nagel et al., 2019) was based on two mathematical ingenuities. First, they exploited the mathematical properties of piece-wise affine activation func-\ntions (such as e.g. ReLU based DNNs) in order to balance the per-channel weight distributions by iteratively applying scaling factors to consecutive layers. Second, they proposed a bias correction scheme that consists in updating the bias terms of the layers with the difference between the expected quantized prediction and the original predictions. They achieved near full-precision accuracy in int8 quantization. Since this seminal work, two main categories of data-free quantization methods have emerged. First, data-generation based methods, such as Cai et al. (2020); Garg et al. (2021), that used samples generated by Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) as samples to fine-tune the quantized model through knowledge distillation (Hinton et al., 2014). Nevertheless, these methods are time-consuming and require significantly more computational resources. Other methods, such as Banner et al. (2019); Choukroun et al. (2019); Fang et al. (2020); Zhao et al. (2019); Nagel et al. (2019); Cong et al. (2022), focus on improving the quantization operator but usually achieve lower accuracy. One limitation of these approaches is that they are essentially restricted to uniform quantization, while considering non-uniform mappings between the floating point and low-bit representation might be key to superior performance."
        },
        {
            "heading": "2.2 NON-UNIFORM QUANTIZATION",
            "text": "Indeed, in uniform settings, continuous variables are mapped to an equally-spaced grid in the original, floating point space. Such mapping introduces an error: however, applying such uniform mapping to an a priori non-uniform weight distribution is likely to be suboptimal in the general case. To circumvent this limitation, non-uniform quantization has been introduced (Banner et al., 2019; Hubara et al., 2016; Jeon et al., 2020; Wu et al., 2016; Zhang et al., 2018; Miyashita et al., 2016; Zhou et al., 2017) and (Zhang et al., 2021a; Li et al., 2019). We distinguish two categories of non-uniform quantization approaches. First, methods that introduce a code-base and require very sophisticated implementations for actual inference benefits such as Banner et al. (2019); Hubara et al. (2016); Jeon et al. (2020); Wu et al. (2016); Zhang et al. (2018). Second, methods that simply modify the quantization operator such as Miyashita et al. (2016); Zhou et al. (2017). In particular, (Zhang et al., 2021a) propose a log-quantization technique. Similarly, Li et al. (Li et al., 2019) use log quantization with basis 2. In both cases, in practice, such logarithmic quantization scheme changes the nature of the mathematical operations involved, with multiplications being replaced by bit shifts. Nevertheless, one limitation of this approach is that because the very nature of the mathematical operations is intrinsically altered, in practice, it is hard to leverage without dedicated hardware and implementation. Instead of transforming floating point multiplications in integer multiplications, they change floating point multiplications into bit-shifts or even look up tables (LUTs). Some of these operations are very specific to some hardware (e.g. LUTs are thought for FPGAs) and may not be well supported on most hardware. Conversely, in this work, we propose a non-uniform quantization scheme that preserves the nature of the mathematical operations by mapping floating point multiplications to standard integer multiplications. As a result, our approach boils down to simple modifications of the computations in the quantized DNN, hence allowing higher accuracies than uniform quantization methods while leading to straightforward, ready-to-use inference speed gains. Below we describe the methodology behind the proposed approach."
        },
        {
            "heading": "3 METHODOLOGY",
            "text": "Let F be a trained feed forward neural network with L layers, each comprising a weight tensor Wl. Let Q be a quantization operator such that the quantized weights Q(Wl) are represented on b bits. The most popular such operator is the uniform one. We argue that, despite its simplicity, the choice of such a uniform operator is responsible for a significant part of the quantization error. In fact, the weights Wl most often follow a bell-shaped distribution for which uniform quantization is ill-suited: intuitively, in such a case, we would want to quantize more precisely the small weights on the peak of the distribution. For this reason, the most popular non-uniform quantization scheme is logarithmic quantization, outputting superior performance. Practically speaking, however, it consists in replacing the quantized multiplications by bit-shift operations. As a result, these methods have limited adaptability as the increment speed is hardware dependent.\nTo adress this problem, we look for the non-uniform quantization operators that preserve the nature of matrix multiplications. Formally, taking aside the rounding operation in quantization, we want to\ndefine the space Q of functions Q such that \u2200Q \u2208 Q,\u2203Q\u22121 \u2208 Q s.t. \u2200x, y Q\u22121(Q(x) \u2217Q(y)) = x\u00d7 y (1)\nwhere \u2217 is the intern composition law of the quantized space and \u00d7 is the standard multiplication, and Q, Q\u22121 are the quantization and de-quantization operators, respectively. In the case of uniform quantization and our work \u2217 will be the multiplication while in other non-uniform works it often corresponds to other operations that are harder to leverage, e.g. bit-shift. Recall that, for now, we omit the rounding operator. The proposed PowerQuant method consists in the search for the best suited operator Q for a given trained neural network and input statistics."
        },
        {
            "heading": "3.1 AUTOMORPHISMS OF (R\u2217+,\u00d7)",
            "text": "Let Q be a transformation from R+ to R+. In this case, \u2217, the intern composition law in quantized space in (1), simply denote the scalar multiplication operator \u00d7 and (1) becomes Q(x) \u00d7 Q(y) = Q(x \u00d7 y) \u2200x, y \u2208 R2+. In order to define a de-quantization operation, we need Q\u22121 to be defined i.e. Q is bijective. Thus, by definition, Q is a group automorphism of (R\u2217+,\u00d7). Thus, quantization operators that preserve the nature of multiplications are restricted to automorphisms of (R\u2217+,\u00d7). The following lemma further restricts the search to power functions. Lemma 1. The set of continuous automorphisms of (R\u2217+,\u00d7) is defined by the set of power functions Q = {Q : x 7\u2192 xa|a \u2208 R}.\nA proof of this result can be found in Appendix A. For the sake of clarity, we will now include the rounding operation in the quantization operators.\nQ = { Qa :W 7\u2192 \u230a (2b\u22121 \u2212 1) sign(W )\u00d7 |W | a\nmax |W |a \u230b \u2223\u2223\u2223a \u2208 R} (2) where W is a tensor and all the operations are performed element-wise. As functions of W , the quantization operators defined in equation 2 are (signed) power functions. Fig 2 illustrates the effect of the power parameter a on quantization (vertical bars). Uniform quantization and a = 1 are equivalent and correspond to a quantization invariant to the weight distribution. For a < 1, the quantization is more fine-grained on weight values with low absolute value and coarser on high absolute values. Conversely, for a > 1, the quantization becomes more fine-grained on high absolute values. We now define the search protocol in the proposed search space Q."
        },
        {
            "heading": "3.2 AUTOMORPHISM SEARCH AS A MINIMIZATION PROBLEM",
            "text": "We propose to use the error introduced by quantization on the weights as a proxy on the distance between the quantized and the original model.\nReconstruction Error Minimization: The operator Qa is not a bijection. Thus, quantization introduces a reconstruction error summed over all the layers of the network, and defined as follows:\n(F, a) = L\u2211 l=1 \u2225\u2225Wl \u2212Q\u22121a (Qa(Wl))\u2225\u2225p (3)\nwhere \u2016 \u00b7 \u2016p denotes the Lp vector norm (in practice p = 2, see appendix B) and the de-quantization operator Q\u22121a is defined as:\nQ\u22121a (W ) = sign(W )\u00d7 \u2223\u2223\u2223\u2223W \u00d7 max |W |2b\u22121 \u2212 1 \u2223\u2223\u2223\u2223 1a (4) In practice, the problem of finding the best exponent a\u2217 = argmina (F, a) in (3) is a locally convex optimization problem (Appendix C.1) which has a unique minimum (see Appendix C.2). We find the optimal value for a using the Nelder\u2013Mead method (Nelder & Mead, 1965) which solves problems for which derivatives may not be known or, in our case, are almost-surely zero (due to the rounding operation). In practice, more recent solvers are not required in order to reach the optimal solution (see Appendix D). Lastly, we discuss the limitations of the proposed metric in Appendix H."
        },
        {
            "heading": "3.3 FUSED DE-QUANTIZATION AND ACTIVATION FUNCTION",
            "text": "Based on equation 2, the quantization process of the weights necessitates the storage and multiplication of W along with a signs tensor, which is memory and computationally intensive. For the weights, however, this can be computed once during the quantization process, inducing no overhead during inference. As for activations, we do not have to store the sign of ReLU activations as they are always positive. In this case, the power function has to be computed at inference time (see algorithm 2). However, it can be efficiently computed Kim et al. (2021), using Newton\u2019s method to approximate continuous functions in integer-only arithmetic. This method is very efficient in practice as it converges in 2 steps for low bit representations (four steps for int32). Thus, PowerQuant leads to significant accuracy gains with limited computational overhead. Conversely, for non-ReLU feed forward networks such as EfficientNets (SiLU) or Image Transformers (GeLU), activations are signed. This can be tackled using asymmetric quantization which consists in the use of a zero-point. In general, asymmetric quantization allows one to have a better coverage of the quantized values support. In our case, we use asymmetric quantization to work with positive values only. Formally, for both SiLU and GeLU, the activations are analytically bounded below by CSiLU = 0.27846 and CGeLU = 0.169971 respectively. Consequently, assuming a layer with SiLU activation with input x and weights W , we have:\nQ\u22121a (Qa(x+ CSiLU)Qa(W )) \u2248 ((x+ CSiLU)aW a) 1 a = xW + CSiLUW (5)\nThe bias term CSiLUW induces a very slight computation overhead which is standard in asymmetric quantization. We provide a detailed empirical evaluation of this cost in Appendix G. Using the adequate value for the bias corrector, we can generalize equation 5 to any activation function \u03c3. The quantization process and inference with the quantized DNN are summarized in Algorithm 1 and 2. The proposed representation is fully compatible with integer multiplication as defined in Jacob et al. (2018), thus it is fully compatible with integer only inference (see appendix F for more details).\nAlgorithm 1 Weight Quantization Algorithm Require: trained neural network F with L layers to quantize, number of bits b a\u2190 solver(min{error(F, a)}) . in practice we use the Nelder\u2013Mead method for l \u2208 {1, . . . , L} do\nWsign \u2190 sign(Wl) . save the sign of the scalar values in W Wl \u2190Wsign \u00d7 |Wl|a . power transformation s\u2190 max |Wl|\n2b\u22121\u22121 . get quantization scale Q :Wl 7\u2192 \u230a Wl s \u2309 and Q\u22121 :W 7\u2192Wsign \u00d7 |W \u00d7 s| 1 a . qdefine Q and Q\u22121\nend for"
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "In this section, we empirically validate our method. First, we discuss the optimization of the exponent parameter a of PowerQuant using the reconstruction error, showing its interest as a proxy for the quantized model accuracy from an experimental standpoint. We show that the proposed approach preserves this reconstruction error significantly better, allowing a closer fit to the original weight distribution through non-uniform quantization. Second, we show through a variety of benchmarks that\nAlgorithm 2 Simulated Inference Algorithm Require: trained neural network F quantized with L layers, input X and exponent a\u2217\nfor l \u2208 {1, . . . , L} do X \u2190 Xa\u2217 . X is assumed positive (see equation (5)) XQ \u2190 bXsXe . where sX is a scale in the input range O \u2190 Fl(XQ) . O contains the quantized output of the layer\nX \u2190 ( \u03c3(O) sXsW ) 1 a\u2217\n. where \u03c3 is the activation function and sW the weight scale end for return X\nthe proposed approach significantly outperforms state-of-the-art data-free methods, thanks to more efficient power function quantization with optimized exponent. Third, we show that the proposed approach comes at a negligible cost in term of inference speed."
        },
        {
            "heading": "4.1 DATASETS AND IMPLEMENTATION DETAILS",
            "text": "We validate the proposed PowerQuant method on ImageNet classification (Deng et al., 2009) (\u2248 1.2M images train/50k test). In our experiments we used pre-trained MobileNets (Sandler et al., 2018), ResNets (He et al., 2016), EfficientNets (Tan & Le, 2019) and DenseNets (Huang et al., 2017). We used Tensorflow implementations of the baseline models from official repositories, achieving standard baseline accuracies. The quantization process was done using Numpy library. Activations are quantized as unsigned integers and weights are quantized using a symmetric representation. We fold batch-normalization layers as in Yvinec et al. (2022a).\nWe performed ablation study using the uniform quantization operator over weight values from Krishnamoorthi (2018) and logarithmic quantization from Miyashita et al. (2016). For our comparison with state-of-the-art approaches in data-free quantization, we implemented the more complex quantization operator from SQuant (Cong et al., 2022). To compare with strong baselines, we also implement bias correction (Nagel et al., 2019) (which measures the expected difference between the outputs of the original and quantized models and updates the biases terms to compensate for this difference) as well as input weight quantization (Nagel et al., 2019)."
        },
        {
            "heading": "4.2 EXPONENT PARAMETER FITTING",
            "text": "Fig 3 illustrates the evolution of both the accuracy of the whole DNN and the reconstruction error summed over all the layers of the network, as functions of the exponent parameter a. Our target is the highest accuracy with respect to the value of a: however, in a data-free context, we only have access to the reconstruction error. Nevertheless, as shown on Fig 3, these metrics are strongly anticorrelated. Furthermore, while the reconstruction curve is not convex it behaves well for simplex based optimization method such as the Nelder-Mead method. This is due to two properties: locally convex (Appendix C.1) and has a unique minimum (Appendix C.2).\nEmpirically, optimal values a\u2217 for the exponent parameter are centered on 0.55, which approximately corresponds to the first distribution in Fig 2. Still, as shown on Table 1 we observe some variations on the best value for a which motivates the optimization of a for each network and bitwidth. Furthermore, our results provide a novel insight on the difference between pruning and quantization. In the pruning literature (Han et al., 2015; Frankle & Carbin, 2018; Molchanov et al., 2019), the baseline method consists in setting the smallest scalar weight values to zero and keeping unchanged the highest non-zero values, assuming that small weights contribute less to the network prediction. In a similar vein, logarithmic or power quantization with a > 1 roughly quantizes (almost zeroing it out) small scalar values to better preserve the precision on larger values. In practice, in our case, lower reconstruction errors, and better accuracies, are achieved by setting a < 1: this suggests that the assumption behind pruning can\u2019t be straightforwardly applied to quantization, where in fact we argue that finely quantizing smaller weights is paramount to preserve the patterns learned at each layer, and the representation power of the whole network.\nAnother approach that puts more emphasis on the nuances between low valued weights is logarithmic based non-uniform quantization. In Table 1 and Appendix E, we compare the proposed power method to both uniform and logarithmic approaches. By definition, the proposed power method necessarily outperforms the uniform method in every scenario as uniform quantization is included in the search space. For instance, in int4, the proposed method improves the accuracy by 13.22 points on ResNet 50. This improvement can also be attributed to a better input quantization of each layer, especially on ResNet 50 where the gap in the reconstruction error (over the weights) is smaller."
        },
        {
            "heading": "4.3 COMPARISON WITH DATA-FREE QUANTIZATION METHODS",
            "text": "In table 2, we report the performance of several data-free quantization approaches on ResNet 50. Although no real training data is involved in these methods, some approaches such as ZeroQ (Cai et al., 2020), DSG (Zhang et al., 2021b) or GDFQ (Xu et al., 2020) rely on data generation (DG) in order to calibrate parameters of the method or to apply fine-tuning to preserve the accuracy through quantization. As shown in table 2, in the W8/A8 setup, the proposed PowerQuant method outperforms other data-free solutions, fully preserving the accuracy of the floating point model. The gap is even wider on the more challenging low bit quantization W4/A4 setup, where the PowerQuant improves the accuracy by 1.93 points over SQuant (Cong et al., 2022) and by 14.88 points over GDFQ. This shows the effectiveness of the method on ResNet 50. We provide more results on DenseNet (Huang et al., 2017), MobileNet (Sandler et al., 2018), Efficient Net (Tan & Le, 2019) in Appendix J. These results demonstrate the versatility of the method on both large and very compact convnets. In summary, the proposed PowerQuant vastly outperforms other data-free quantization schemes.\nLast but not least, when compared to recent QAT methods such as OCTAV Sakr et al. (2022), PowerQuant achieves competitive results on both ResNets and MobileNets using either both static or dynamic quantization. This is remarkable since PowerQuant does not involve any fine-tuning of the network. We provide more details on this benchamrk in Appendix I. In what follows, we evaluate PowerQuant on recent transformer architectures for both image and language applications."
        },
        {
            "heading": "4.4 EVALUATION ON TRANSFORMER ARCHITECTURES",
            "text": "In Table 3, we quantized the weight tensors of a ViT Dosovitskiy et al. (2021) with 85M parameters and baseline accuracy\u2248 78 as well as DeiT T,S and B Touvron et al. (2021) with baseline accuracies 72.2, 79.9 and 81.8 and \u2248 5M, \u2248 22M, \u2248 87M parameters respectively. Similarly to ConvNets, the image transformer is better quantized using PowerQuant rather than standard uniform quantization schemes such as DFQ. Furthermore, more complex and recent data-free quantization schemes such as SQuant, tend to under-perform on the novel Transformer architectures as compared to ConvNets. This is not the case for PowerQuant which maintains its very high performance even in low bit representations. This is best illustrated on ViT where PowerQuant W4/A8 out performs both DFQ and SQuant even when they are allowed 8 bits for the weights (W8/A8) by a whopping 4.91 points. The proposed PowerQuant even outperforms methods dedicated to transformer quantization such as PSAQ Li et al. (2022) on every image transformer tested.\nWe further compare the proposed power quantization, in W4/A8, on natural language processing (NLP) tasks and report results in Table 4. We evaluate a BERT model (Devlin et al., 2018) on GLUE (Wang et al., 2018) and report both the original (reference) and our reproduced (baseline) results. We compare the three quantization processes: uniform, logarithmic and PowerQuant. Similarly to computer vision tasks, the power quantization outperforms the other methods in every instances which further confirms its ability to generalize well to transformers and NLP tasks. In what follows, we show experimentally that our approach induces very negligible overhead at inference time, making this accuracy enhancement virtually free from a computational standpoint."
        },
        {
            "heading": "4.5 INFERENCE COST AND PROCESSING TIME",
            "text": "The ACE metrics was recently introduced in Zhang et al. (2022) to provide a hardware-agnostic measurement of the overhead computation cost in quantized neural networks. In Table 5, we evaluate the cost in the inference graph due to the change in the activation function. We observe very similar results to Table 17. The proposed changes are negligible in terms of computational cost on all tested networks. Furthermore, DenseNet has the highest cost due to its very dense connectivity. On the other hand, using this metric it seems that the overhead cost due to the zero-point technique from section 3.3 for EfficientNet has no significant impact as compared to MobileNet and ResNet. In addition, we provide a more detailed discussion on the inference and processing cost of PowerQuant on specific hardware using dedicated tools in Appendix K."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this paper, we pinpointed the uniformity of the quantization as a limitation of existing datafree methods. To address this limitation, we proposed a novel data-free method for non-uniform quantization of trained neural networks for computer vision tasks, with an emphasis on not changing the nature of the mathematical operations involved (e.g. matrix multiplication). This led us to search among the continuous automorphisms of (R\u2217+,\u00d7), which are restricted to the power functions x\u2192 xa. We proposed an optimization of this exponent parameter based upon the reconstruction error between the original floating point weights and the quantized ones. We show that this procedure is locally convex and admits a unique solution. At inference time, the proposed approach, dubbed PowerQuant, involves only very simple modifications in the quantized DNN activation functions. We empirically demonstrate that PowerQuant allows a closer fit to the original weight distributions compared with uniform or logarithmic baselines, and significantly outperforms existing methods in\na variety of benchmarks with only negligible computational overhead at inference time. In addition, we also discussed and addressed some of the limitations in terms of optimization (per-layer or global) and generalization (non-ReLU networks).\nFuture work involves the search of a better proxy error as compared with the proposed weight reconstruction error as well as the extension of the search space to other internal composition law of R+ that are suited for efficient calculus and inference."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This work has been supported by the french National Association for Research and Technology (ANRT), the company Datakalab (CIFRE convention C20/1396) and by the French National Agency (ANR) (FacIL, project ANR-17-CE33-0002). This work was granted access to the HPC resources of IDRIS under the allocation 2022-AD011013384 made by GENCI."
        },
        {
            "heading": "A PROOF OF LEMMA 1",
            "text": "In this section, we provide a simple proof for lemma 1 as well as a discussion on the continuity hypothesis.\nProof. We have that \u2200x \u2208 R+, Q(x) \u00d7 Q(0) = Q(0) and \u2200x \u2208 R+, Q(x) \u00d7 Q(1) = Q(x) which induces that Q is either the constant 1 or Q(0) = 0 and Q(1) = 1. Because Q is an automorphism we can eliminate the first option. Now, we will demonstrate that Q is necessarily a power function. Let n be an integer, then\nQ(xn) = Q(x)\u00d7Q(xn\u22121) = Q(x)2 \u00d7Q(xn\u22122) = \u00b7 \u00b7 \u00b7 = Q(x)n. (6)\nSimilarly, for fractions, we get Q(x 1 n )\u00d7 \u00b7 \u00b7 \u00b7 \u00d7Q(x 1n ) = Q(x)\u21d4 Q(x 1n ) = Q(x) 1n . Assuming Q is continuous, we deduce that for any rational a \u2208 R, we have\nQ(xa) = Q(x)a (7)\nIn order to verify that the solution is limited to power functions, we use a reductio ad absurdum. Assume Q is not a power function. Therefore, there exists (x, y) \u2208 R2+ and a \u2208 R such that Q(x) 6= xa and Q(y) = ya. By definition of the logarithm, there exists b such that xb = y. We get the following contradiction, from (7),{\nQ(xb a ) = Q(ya) = ya Q(xb a ) = Q(xab) = Q(xa) b 6= ( xab = ya ) (8) Consequently, the suited functions Q are limited to power functions i.e. Q = {Q : x 7\u2192 xa|a \u2208 R}.\nWe would also like to put the emphasis on the fact that there are other Automorphisms of (R,\u00d7). However, the construction of such automorphisms require the axiom of choice Herrlich (2006). Such automorphisms are not applicable in our case which is why the key constraint is being an automorphism rather than the continuous property."
        },
        {
            "heading": "B NORM SELECTION",
            "text": "In the minimization objective, we need to select a norm to apply. In this section, we provide theoretical arguments in favor of the l2 vector norm. Let F be a feed forward neural network with L layers to quantize, each defined by a set of weights Wl = (wl)i,j \u2208 Rnl\u00d7ml and bias bl \u2208 Rnl . We note (\u03bb(i)l )i the eigenvalues associated with Wl. We want to study the distance d(F, Fa) between the predictive function F and its quantized version Fa defined as\nd(F, Fa) = max x\u2208D \u2016F (x)\u2212 Fa(x)\u2016p (9)\nwhere D is the domain of F . We prove that minimizing the reconstruction error with respect to a is equivalent to minimizing d(F, Fa) with respect to a. Assume L = 1 for the sake of simplicity and we drop the notation l. With the proposed PowerQuant method, we minimize the vector norm\n\u2016W \u2212Q\u22121a (Qa(W ))\u2016pp = \u2211 i<=n max j<=m |wi,j \u2212Q\u22121a (Qa(wi,j))|p (10)\nFor p = 2, the euclidean norm is equal to the spectral norm, thus minimizing \u2016W\u2212Q\u22121a (Qa(W ))\u20162 is equivalent to minimizing d(F, Fa) for L = 1. However, we know that minimizing for another value of pmay result in a different optimal solution and therefore not necessarily minimize d(F, Fa).\nIn the context of data-free quantization, we want to avoid uncontrollable changes on F , which is why we recommend the use of p = 2."
        },
        {
            "heading": "C MATHEMATICAL PROPERTIES",
            "text": "C.1 LOCAL CONVEXITY\nWe prove that the minimization problem defined in equation 3 is locally convex around the solution a\u2217. Formally we prove that x 7\u2192 \u2225\u2225x\u2212Q\u22121a (Qa(x))\u2225\u2225p (11)\nis locally convex around a\u2217 defined as argmina \u2225\u2225x\u2212Q\u22121a (Qa(x))\u2225\u2225p.\nLemma 2. The minimization problem defined as\nargmin a {\u2225\u2225x\u2212Q\u22121a (Qa(x))\u2225\u2225p} (12) is locally convex around any solution a\u2217.\nProof. We recall that \u2202x a\n\u2202a = x a log(x). The function \u2225\u2225x\u2212Q\u22121a (Qa(x))\u2225\u2225 is differentiable. We assume x \u2208 R, then we can simplify the sign functions (assume x positive without loss of generality) and note y = max |x|, then\n\u2202Q\u22121a (Qa(x)) \u2202a = \u2202 \u2223\u2223\u2223\u230a(2b\u22121 \u2212 1)xaya \u230b ya2b\u22121\u22121 \u2223\u2223\u2223 1a \u2202a . (13)\nThis simplifies to\n\u2202Q\u22121a (Qa(x))\n\u2202a = y\n\u2202\n( bB( xy )ac\nB\n) 1 a\n\u2202a , (14)\nwith B = 2b\u22121\u22121. By using the standard differentiation rules, we know that the rounding operator has a zero derivative a.e.. Consequently we get,\n\u2202Q\u22121a (Qa(x))\n\u2202a = \u2212a2y\n \u230a B ( x y )a\u230b B  1 a log  \u230a B ( x y )a\u230b B  . (15) Now we can compute the second derivative of Q\u22121a (Qa(x)),\n\u22022Q\u22121a (Qa(x))\n\u2202a2 = a4y\n \u230a B ( x y )a\u230b B  1 a log2  \u230a B ( x y )a\u230b B  . (16) From this expression, we derive the second derivative, using the property (f \u25e6 g)\u2032\u2032 = f \u2032\u2032 \u25e6 g\u00d7 g\u20322 +\nf \u2032 \u25e6 g \u00d7 g\u2032\u2032 and the derivatives | \u00b7 | 1 p \u2032 = x|x| 1 p \u22122 p and | \u00b7 | 1 p \u2032\u2032 = 1\u2212pp2 |x| 1 p x2 , then for any xi \u2208 x\n\u22022 \u2223\u2223xi \u2212Q\u22121a (Qa(xi))\u2223\u2223\n\u2202a2 = 1\u2212 p p2 |xi \u2212Q\u22121a (Qa(xi)| 1 p\n(xi \u2212Q\u22121a (Qa(xi))2\n( \u2202Q\u22121a (Qa(x))\n\u2202a )2 + (xi \u2212Q\u22121a (Qa(xi))|xi \u2212Q\u22121a (Qa(xi)| 1 p\u22122\np\n\u22022Q\u22121a (Qa(x))\n\u2202a2\n(17)\nWe now note the first term in the previous addition T1 = 1\u2212pp2 |xi\u2212Q\u22121a (Qa(xi)|\n1 p\n(xi\u2212Q\u22121a (Qa(xi))2\n( \u2202Q\u22121a (Qa(x))\n\u2202a\n)2 and\nthe second term as a product of T2 = (xi\u2212Q\u22121a (Qa(xi))|xi\u2212Q \u22121 a (Qa(xi)|\n1 p \u22122\np times T3 = \u22022Q\u22121a (Qa(x))\n\u2202a2 . We know that T1 > 0 and T3 > 0, consequently, and T2 is continuous in a. At a\u2217 the terms with |xi \u2212 Q\u22121a (Qa(xi)) | are negligible in comparison with \u22022Q\u22121a (Qa(x)) \u2202a2 and ( \u2202Q\u22121a (Qa(x)) \u2202a )2 . Consequently, there exists an open set around a\u2217 where T1 > |T2|T3, and \u22022|xi\u2212Q\u22121a (Qa(xi))|\n\u2202a2 > 0. This concludes the proof.\nC.2 UNIQUENESS OF THE SOLUTION\nIn this section we provide the elements of proof on the uniqueness of the solution of the minimization of the quantization reconstruction error. Lemma 3. The minimization problem over x \u2208 RN defined as\nargmin a {\u2225\u2225x\u2212Q\u22121a (Qa(x))\u2225\u2225p} (18) has almost surely a unique global minimum a\u2217. Proof. We assume that x can not be exactly quantized, i.e. mina {\u2225\u2225x\u2212Q\u22121a (Qa(x))\u2225\u2225p} > 0 which is true almost everywhere. We use a reductio ad absurdum and assume that there exist two optimal solutions a1 and a2 to the optimization problem. We expand the expression\u2225\u2225x\u2212Q\u22121a (Qa(x))\u2225\u2225p and get\u2225\u2225x\u2212Q\u22121a (Qa(x))\u2225\u2225p = \u2225\u2225\u2225\u2225\u2225x\u2212 \u2223\u2223\u2223\u2223\u230a(2b\u22121 \u2212 1) sign(x)\u00d7 |x|amax |x|a \u230b max |x|a 2b\u22121 \u2212 1 \u2223\u2223\u2223\u2223 1a sign(x) \u2225\u2225\u2225\u2225\u2225 .p (19)\nWe note the rounding term Ra and get\u2225\u2225x\u2212Q\u22121a (Qa(x))\u2225\u2225p = \u2225\u2225\u2225\u2225\u2225x\u2212 \u2223\u2223\u2223\u2223Ramax |x|a2b\u22121 \u2212 1 \u2223\u2223\u2223\u2223 1a sign(x) \u2225\u2225\u2225\u2225\u2225 p . (20)\nAssume Ra1 = Ra2 = R, the minimization problem argmina \u2225\u2225\u2225\u2225x\u2212 \u2223\u2223\u2223Rmax |x|a2b\u22121\u22121 \u2223\u2223\u2223 1a sign(x)\u2225\u2225\u2225\u2225 p is convex and has a unique solution, thus a1 = a2. Now assume Ra1 6= Ra2 .\nLet\u2019s denoteD(R) the domain of power values a over which we have \u230a (2b\u22121 \u2212 1) sign(x)\u00d7|x| a\nmax |x|a\n\u230b = R.\nIf there is a value a outside of D(Ra1) \u222a D(Ra2) such that R\u2032 has each of its coordinate strictly between the coordinates of Ra1 and Ra2 , then, without loss of generality, assume that at least half of the coordinates of Ra1 are further away from the corresponding coordinates of x than one quantization step. This implies that there exists a value a\u2032 in D(R\u2032) such that\n\u2225\u2225x\u2212Q\u22121a\u2032 (Qa\u2032(x))\u2225\u2225p <\u2225\u2225x\u2212Q\u22121a1 (Qa1(x))\u2225\u2225p. which goes against our hypothesis. Thus, there are up to N possible values for R that minimize the problem which happens iff x satisfies at least one coordinate can be either ceiled or floored by the rounding. The set defined by this condition has a zero measure."
        },
        {
            "heading": "D SOLVER FOR MINIMIZATION",
            "text": "In the main article we state that we can use Nelder-Mead (Nelder & Mead, 1965) solver to find the optimal a\u2217. We tested several other solvers and report the results in Table 6. The empirical results show that basically any popular solver can be used, and that the Nelder-Mead solver is sufficient for the minimization problem."
        },
        {
            "heading": "E COMPARISON BETWEEN LOG, NAIVE AND POWER QUANTIZATION COMPLEMENTARY RESULTS",
            "text": "To complement the results provided in the main paper on ResNet 50, we list in Table 7 more quantization setups on ResNet 50 as well as DenseNet 121. To put it in a nutshell, The proposed power\nquantization systematically achieves significantly higher accuracy and lower reconstruction error than the logarithmic and uniform quantization schemes. On a side note, the poor performance of the logarithmic approach on DenseNet 121 can be attributed to the skewness of the weight distributions. Formally, ResNet 50 and DenseNet 121 weight values show similar average standard deviations across layers (0.0246 and 0.0264 respectively) as well as similar kurtosis (6.905 and 6.870 respectively). However their skewness are significantly different: 0.238 for ResNet 50 and more than twice as much for DenseNet 121, with 0.489. The logarithmic quantization, that focuses on very small value is very sensible to asymmetry which explains the poor performance on DenseNet 121. In contrast, the proposed method offers a robust performance in all situations."
        },
        {
            "heading": "F HOW TO PERFORM MATRIX MULTIPLICATION WITH POWERQUANT",
            "text": "The proposed PowerQuant method preserves the multiplication operations, i.e. a multiplication in the floating point space remains a multiplication in the quantized space (integers). This allows one to leverage current implementations of uniform quantization available on most hardware Gholami et al. (2021); Zhou et al. (2016). However, while PowerQuant preserves multiplications it doesn\u2019t preserve additions which are significantly less costly than multiplications. Consequently, in order to infer under the PowerQuant transformation, instead of accumulating the quantized products, as done in standard quantization Jacob et al. (2018), one need to accumulate the powers of said products. Formally, let\u2019s consider two quantized weights w1, w2 and their respective quantized inputs x1, x2. The standard accumulation would be performed as followsw1x1+w2x2. In the case of PowerQuant, this would be done as (w1x1) 1 a +(w2x2) 1 a . Previous studies on quantization have demonstrated that such power functions can be computed with very high fidelity at almost no latency cost Kim et al. (2021)."
        },
        {
            "heading": "G OVERHEAD COST OF ZERO-POINTS IN ACTIVATION QUANTIZATION",
            "text": "The overhead cost introduced in equation 5 is well known in general in quantization as it arises from asymmetric quantization. Nonetheless, we share here (as well as in the article) some empirical values.\nThese are empirical results from our own implementation. We include ResNet50 as it can also be quantized using asymmetric quantization although in our research, we only applied asymmetric quantization to SilU and GeLU based architectures. We included these results in the appendix of the revised article. It is worth noting that according to LSQ+ Bhalgat et al. (2020), asymmetric quantization can be achieved at virtually not run-time cost."
        },
        {
            "heading": "H LIMITATIONS OF THE RECONSTRUCTION ERROR METRIC",
            "text": "In the proposed PowerQuant method, we fit the parameter a based on the reconstruction error over all the weights, i.e. over all layers in the whole network. Then, we perform per-channel quantization layer by layer independently. However, if the final objective is to minimize the reconstruction error from equation (3), a more efficient approach would consist in fitting the parameter a separately for each layer. We note a\u2217l such that for every layer l we have\na\u2217l = argmin a {\u2225\u2225Wl \u2212Q\u22121a (Qa(Wl))\u2225\u2225p} (21) Then the network (F, (al)\u2217) quantized with a per-layer fit of the power parameter will satisfy\nL\u2211 l=1 \u2225\u2225Wl \u2212Q\u22121al (Qal(Wl))\u2225\u2225p < L\u2211 l=1 \u2225\u2225Wl \u2212Q\u22121a (Qa(Wl))\u2225\u2225p (22) if and only if their exists at least one l such that al 6= a. Consequently, if the reconstruction error was a perfect estimate of the resulting accuracy, the per-layer strategy would offer an even higher accuracy than the proposed PowerQuant method. Unfortunately, the empirical evidence, in table 9, shows that the proposed PowerQuant method achieves better results in every benchmark. This observation demonstrates the limits of the measure of the reconstruction error. We explain this phenomenon by the importance of inputs and activations quantization. This can be seen as some form of overfitting the parameters al on the weights which leads to poor performance on the activation quantization and prediction. In the general sens, this highlights the limitations of the reconstruction error as a proxy for maximizing the accuracy. Previous results can be interpreted in a similar way. For instance, in SQuant Cong et al. (2022) the author claim that it is better to minimize the absolute sum of errors rather than the sum of absolute errors and achieve good performance in data-free quantization.\nI IMPROVEMENT WITH RESPECT TO QAT\nIn the introduction, we argued that data-driven quantization schemes performance define an upperbound on data-free performance. Our goal was to narrow the resulting gap between these methods. In Table 10, we report the evolution in the gap between data-free and data-driven quantization techniques. These empirical results validate the significant improvement of the proposed method at narrowing the gap between data-free and data-driven quantization methods by 26.66% to 29.74%.\nTable 10: Performance Gap as compared to Data-driven techniques on ResNet 50 quantization in W4/A4. The relative gap improvement to the state-of-the-art SQuant [6], is measured as gs\u2212gp\ngs\nwith gs = \u2217\u2212SQuant\u2217 and\ngp = \u2217\u2212PowerQuant\u2217 where \u2217 is the performance of a data-driven method\ndata-driven method SQuant PowerQuant relative gap OCTAV Sakr et al. (2022) (ICML) 8,72% 6,15% +29,47%\nSQ van Baalen et al. (2022) (CVPR) 8,64% 6,07% +29,74% WinogradQ Chikin & Kryzhanovskiy (2022) (CVPR) 9,55% 7,00% +26,66%\nMr BiQ Jeon et al. (2022) (CVPR) 8,74% 6,17% +29,38%\nIn order to complete our comparison to QAT methods, we considered the short-re-training (30 epochs) regime from OCTAV in Table 11. We can draw two observations from this comparison. First, on ResNet 50, OCTAV achieves remarkable results by reach near full-precision accuracy. Still the proposed method does not fall too far back with only 5.31 points lower accuracy while being data-free. Second, on very small models such as MobileNet V2, using a strong quantization operator rather than a short re-training leads to a huge accuracy improvement as PowerQuant achieves 45.18 points higher accuracy. This is also the finding of the author in OCTAV, as they conclude that models such as MobileNet tend to be very challenging to quantize using static quantization and short re-training.\nIn Table 12, we draw a comparison between the proposed PowerQuant and the QAT method OCTAV Sakr et al. (2022), both using dynamic quantization (i.e. estimating the ranges of the activations onthe-fly depending on the input). As expected, the use of dynamic ranges has a considerable influence on the performance of both quantization methods. As can be observed the QAT method OCTAV achieved very impressive results and even outperforming the full-precision model on ResNet 50. Nevertheless, it is on MobileNet that the influence of dynamic ranges is the most impressive. For OCTAV, we observe a boost of almost 71 points going from almost random predictions to near exact full-precision accuracy. It is to be noted that PowerQuant does not fall shy in front of these performances, as using static quantization we still manage to preserve some of the predictive capability of the model. Furthermore, using dynamic quantization, Powerquant achieves similar accuracies than OCTAV while not involving any fine-tuning, contrary to OCTAV.\nAll in all, we can conclude that the proposed data-free method manages to hold close results to a state-of-the-art QAT method in some context. An interesting future work could be the extension of PowerQuant as a QAT method and possibly learning the power parameter a that we use in our quantization operator."
        },
        {
            "heading": "J COMPARISON TO STATE-OF-THE-ART DATA-FREE QUANTIZATION ON OTHER CONVNETS",
            "text": "In addition to our evaluation on ResNet, we propose some complementary results on DenseNet in Table 13 as well as the challenging and compact architectures MobileNet and EfficientNet in Table\n14 as well as weights only for Bert in Table 16. In table 13, we report the performance of other data-free quantization processes on DenseNet 121. The OMSE method (Choukroun et al., 2019) is a post-training quantization method that leverages validation examples during quantization, thus cannot be labelled as data-free. Yet, we include this work in our comparison as they show strong performance in terms of accuracy at a very low usage of real data. As showcased in table 13, the proposed PowerQuant method almost preserves the floating point accuracy in W8/A8 quantization. Additionally, on the challenging W4/A4 setup, our approach improves the accuracy by a remarkable 12.30 points over OMSE and 17.54 points over SQuant. This is due to the overall better efficiency of non-uniform quantization, that allows a theoretically closer fit to the weight distributions of each DNN layer. The results on MobileNet and EfficientNet from Table 14 confirm our previous findings. We observe a significant boost in performance from PowerQuant as compared to the other very competitive data-free solutions."
        },
        {
            "heading": "K OVERHEAD COST DISCUSSION",
            "text": "In this section, we provide more empirical results on the inference cost of the proposed method. Table 17 shows the inference time of DNNs quantized with our approach (which only implies modifications of the activation function and a bias correction-see Section 3.3). For DenseNet, ResNet and MobileNet V2, the baseline activation function is the ReLU, which is particularly fast to compute.\nNevertheless, our results show that our approach leads to only increasing by 1% the whole inference time on most networks. More precisely, in the case of ResNet 50, the change in activation function induces a slowdown of 0.15%. The largest runtime increase is obtained on DenseNet with a 3.4% increase. Lastly, note that our approach is also particularly fast and efficient on EfficientNet B0, which uses SiLU activation, thanks to the bias correction technique introduced in Section 3.3. Overall, the proposed approach can be easily implemented and induces negligible overhead in inference on GPU. To furthermore justify the practicality of the proposed quantization process, we recall that the only practicality concern that may arise is on the activation function as the other operations are strictly identical to standard uniform quantization. According to Kim et al. (2021) efficient power functions can be implemented for generic hardware as long as they support standard integer arithmetic, i.e. as long as they support uniform quantization. When it comes to Field-Programmable Gate Array (FPGA), activation functions are implemented using look-up tables (LUT) as detailed in Hajduk (2017). More precisely, they are pre-computed using Pade\u0301 approximation which are quotients of polynomial functions. Consequently the proposed approach would simply change the polynomial values but not the inference time as it would still rely on the same number of LUTs.\nIn general, activation functions that are non-linear can be very effectively implemented in quantization runtime Lam et al. (2022). However these considerations are hardware agnostic. In order to circumvent this limitation and address any concerns to our best, we conducted a small study using\nthe simulation tool nntool from GreenWaves, a risc-v chips manufacturer that enables to simulate inference cost of quantized neural networks on their gap unit. We tested a single convolutional layer with bias and relu activation plus our power quantization operation and reported the number of cycles and operations. These results demonstrate that even without any optimization the proposed method has a marginal computational cost on MCU inference which corroborates our previous empirical results. We would like to put the emphasis on the fact that this cost could be further reduced via optimizing the computation of the power function using existing methods such as Kim et al. (2021). Similarly, we measure the empirical time required to perform the proposed quantization method on several neural networks and report the results in table 19. These results show that the proposed PowerQuant method offers outstanding trade-offs in terms of compression and accuracy at virtually no cost over the processing and inference time as compared to other data-free quantization methods. For instance, SQuant is a sophisticated method that requires heavy lifting in order to efficiently process a neural network. On a CPU, it requires at least 100 times more time to reach a lower accuracy than the proposed method as we will showcase in our comparison to state-of-the-art quantization schemes."
        }
    ],
    "title": "POWERQUANT: AUTOMORPHISM SEARCH FOR NON- UNIFORM QUANTIZATION",
    "year": 2023
}