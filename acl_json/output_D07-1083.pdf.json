{
    "abstractText": "This paper proposes a framework for semi-supervised structured output learning (SOL), specifically for sequence labeling, based on a hybrid generative and discriminative approach. We define the objective function of our hybrid model, which is written in log-linear form, by discriminatively combining discriminative structured predictor(s) with generative model(s) that incorporate unlabeled data. Then, unlabeled data is used in a generative manner to increase the sum of the discriminant functions for all outputs during the parameter estimation. Experiments on named entity recognition (CoNLL-2003) and syntactic chunking (CoNLL-2000) data show that our hybrid model significantly outperforms the stateof-the-art performance obtained with supervised SOL methods, such as conditional random fields (CRFs).",
    "authors": [
        {
            "affiliations": [],
            "name": "Jun Suzuki"
        },
        {
            "affiliations": [],
            "name": "Akinori Fujino"
        },
        {
            "affiliations": [],
            "name": "Hideki Isozaki"
        }
    ],
    "id": "SP:49906c2d8d88ad14cb6894ff4cbb3655aa8282ce",
    "references": [
        {
            "authors": [
                "Y. Altun",
                "D. McAllester",
                "M. Belkin."
            ],
            "title": "Maximum Margin Semi-Supervised Learning for Structured Variables",
            "venue": "InProc. of NIPS*2005.",
            "year": 2005
        },
        {
            "authors": [
                "R. Ando",
                "T. Zhang."
            ],
            "title": "A High-Performance Semi-Supervised Learning Method for Text Chunking",
            "venue": "Proc. of ACL-2005,",
            "year": 2005
        },
        {
            "authors": [
                "U. Brefeld",
                "T. Scheffer."
            ],
            "title": "Semi-Supervised Learning for Structured Output Variables",
            "venue": "In",
            "year": 2006
        },
        {
            "authors": [
                "H.L. Chieu",
                "Hwee T. Ng."
            ],
            "title": "Named Entity Recognition with a Maximum Entropy Approach",
            "venue": "Proc. of CoNLL-2003",
            "year": 2003
        },
        {
            "authors": [
                "A.P. Dempster",
                "N.M. Laird",
                "D.B. Rubin"
            ],
            "title": "Maximum Likelihood from Incomplete Data via the EM Algorithm",
            "venue": "Journal of the Royal Statistical Society, Series B,",
            "year": 1977
        },
        {
            "authors": [
                "39:1\u201338. R. Florian",
                "A. Ittycheriah",
                "H. Jing",
                "T. Zhang"
            ],
            "title": "Named Entity Recognition through Classifier Combination",
            "venue": "InProc. of CoNLL-2003",
            "year": 2003
        },
        {
            "authors": [
                "A. Fujino",
                "N. Ueda",
                "K. Saito."
            ],
            "title": "A Hybrid Generative/Discriminative Approach to Semi-Supervised Classifier Design",
            "venue": "InProc. of AAAI-05, pages 764\u2013 769.",
            "year": 2005
        },
        {
            "authors": [
                "Y. Grandvalet",
                "Y. Bengio."
            ],
            "title": "Semi-Supervised Learning by Entropy Minimization",
            "venue": "InProc. of NIPS*2004, pages 529\u2013536.",
            "year": 2004
        },
        {
            "authors": [
                "F. Jiao",
                "S. Wang",
                "C.-H. Lee",
                "R. Greiner",
                "D. Schuurmans."
            ],
            "title": "Semi-Supervised Conditional Random Fields for Improved Sequence Segmentation and Labeling",
            "venue": "Proc. of COLING/ACL-2006",
            "year": 2006
        },
        {
            "authors": [
                "T. Kudo",
                "Y. Matsumoto."
            ],
            "title": "Chunking with Support Vector Machines",
            "venue": "InProc. of NAACL 2001",
            "year": 2001
        },
        {
            "authors": [
                "J. Lafferty",
                "A. McCallum",
                "F. Pereira"
            ],
            "title": "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data",
            "year": 2001
        },
        {
            "authors": [
                "W. Li",
                "A. McCallum."
            ],
            "title": "Semi-Supervised Sequence Modeling with Syntactic Topic Models",
            "venue": "Proc. of AAAI-2005,",
            "year": 2005
        },
        {
            "authors": [
                "D.C. Liu",
                "J. Nocedal"
            ],
            "title": "On the Limited Memory BFGS Method for Large Scale Optimization",
            "year": 1989
        },
        {
            "authors": [
                "K. Nigam",
                "A. McCallum",
                "S. Thrun",
                "T. Mitchell"
            ],
            "title": "Text Classification from Labeled and Unlabeled Documents using EM.Machine Learning",
            "year": 2000
        },
        {
            "authors": [
                "R. Raina",
                "Y. Shen",
                "A.Y. Ng",
                "A. McCallum."
            ],
            "title": "Classification with Hybrid Generative/Discriminative Models",
            "venue": "InProc. of NIPS*2003.",
            "year": 2003
        },
        {
            "authors": [
                "F. Sha",
                "F. Pereira."
            ],
            "title": "Shallow Parsing with Conditional Random Fields",
            "venue": "InProc. of HLT/NAACL-2003",
            "year": 2003
        },
        {
            "authors": [
                "A. Smith",
                "T. Cohn",
                "M. Osborne."
            ],
            "title": "Logarithmic Opinion Pools for Conditional Random Fields",
            "venue": "Proc. of ACL-2005,",
            "year": 2005
        },
        {
            "authors": [
                "C. Sutton",
                "M. Sindelar",
                "A. McCallum"
            ],
            "title": "Reducing Weight Undertraining in Structured Discriminative Learning",
            "venue": "InProc. of HTL-NAACL",
            "year": 2006
        },
        {
            "authors": [
                "J. Suzuki",
                "E. McDermott",
                "H. Isozoki."
            ],
            "title": "Training Conditional Random Fields with Multivariate Evaluation Measure",
            "venue": "InProc. of COLING/ACL-2006",
            "year": 2006
        },
        {
            "authors": [
                "E.F. Tjong Kim Sang",
                "S. Buchholz."
            ],
            "title": "Introduction to the CoNLL-2000 Shared Task: Chunking",
            "venue": "Proc. of CoNLL-2000 and LLL-2000",
            "year": 2000
        },
        {
            "authors": [
                "E.T. Tjong Kim Sang",
                "F. De Meulder."
            ],
            "title": "Introduction to the CoNLL-2003 Shared Task: LanguageIndependent Named Entity Recognition",
            "venue": "In",
            "year": 2003
        },
        {
            "authors": [
                "T. Zhang",
                "F. Damerau",
                "D. Johnson"
            ],
            "title": "Text Chunking based on a Generalization of Winnow",
            "year": 2002
        },
        {
            "authors": [
                "X. Zhu",
                "Z. Ghahramani",
                "J. Lafferty."
            ],
            "title": "SemiSupervised Learning using Gaussian Fields and Harmonic Functions",
            "venue": "InProc.of ICML-2003, pages 912\u2013 919. 800",
            "year": 2003
        }
    ],
    "sections": [
        {
            "text": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp. 791\u2013800, Prague, June 2007. c\u00a92007 Association for Computational Linguistics\nThis paper proposes a framework for semi-supervised structured output learning (SOL), specifically for sequence labeling, based on a hybrid generative and discriminative approach. We define the objective function of our hybrid model, which is written in log-linear form, by discriminatively combining discriminative structured predictor(s) with generative model(s) that incorporate unlabeled data. Then, unlabeled data is used in a generative manner to increase the sum of the discriminant functions for all outputs during the parameter estimation. Experiments on named entity recognition (CoNLL-2003) and syntactic chunking (CoNLL-2000) data show that our hybrid model significantly outperforms the stateof-the-art performance obtained with supervised SOL methods, such as conditional random fields (CRFs)."
        },
        {
            "heading": "1 Introduction",
            "text": "Structured output learning (SOL) methods, which attempt to optimize an interdependent output space globally, are important methodologies for certain natural language processing (NLP) tasks such as part-of-speech tagging, syntactic chunking (Chunking) and named entity recognition (NER), which are also referred to as sequence labeling tasks. When we consider the nature of these sequence labeling tasks, a semi-supervised approach appears to be more natural and appropriate. This is because the number of features and parameters typically become extremely large, and labeled examples can only sparsely cover the parameter space, even if thousands of labeled ex-\namples are available. In fact, many attempts have recently been made to develop semi-supervised SOL methods (Zhu et al., 2003; Li and McCallum, 2005; Altun et al., 2005; Jiao et al., 2006; Brefeld and Scheffer, 2006).\nWith the generative approach, we can easily incorporate unlabeled data into probabilistic models with the help of expectation-maximization (EM) algorithms (Dempster et al., 1977). For example, the Baum-Welch algorithm is a well-known algorithm for training a hidden Markov model (HMM) of sequence learning. Generally, with sequence learning tasks such as NER and Chunking, we cannot expect to obtain better performance than that obtained using discriminative approaches in supervised learning settings.\nIn contrast to the generative approach, with the discriminative approach, it is not obvious how unlabeled training data can be naturally incorporated into a discriminative training criterion. For example, the effect of unlabeled data will be eliminated from the objective function if the unlabeled data is directly used in traditional i.i.d. conditionalprobability models. Nevertheless, several attempts have recently been made to incorporate unlabeled data in the discriminative approach. An approach based on pairwise similarities, which encourage nearby data points to have the same class label, has been proposed as a way of incorporating unlabeled data discriminatively (Zhu et al., 2003; Altun et al., 2005; Brefeld and Scheffer, 2006). However, this approach generally requires joint inference over the whole data set for prediction, which is not practical as regards the large data sets used for standard sequence labeling tasks in NLP. Another discriminative approach to semi-supervised SOL involves the incorporation of an entropy regularizer (Grand-\n791\nvalet and Bengio, 2004). Semi-supervised conditional random fields (CRFs) based on a minimum entropy regularizer (SS-CRF-MER) have been proposed in (Jiao et al., 2006). With this approach, the parameter is estimated to maximize the likelihood of labeled data and the negative conditional entropy of unlabeled data. Therefore, the structured predictor is trained to separate unlabeled data well under the entropy criterion by parameter estimation.\nIn contrast to these previous studies, this paper proposes a semi-supervised SOL framework based on a hybrid generative and discriminative approach. A hybrid approach was first proposed in a supervised learning setting (Raina et al., 2003) for text classification. (Fujino et al., 2005) have developed a semi-supervised approach by discriminatively combining a supervised classifier with generative models that incorporate unlabeled data. We extend this framework to the structured output domain, specifically for sequence labeling tasks. Moreover, we reformalize the objective function to allow the incorporation of discriminative models (structured predictors) trained from labeled data, since the original framework only considers the combination of generative classifiers. As a result, our hybrid model can significantly improve on the state-of-the-art performance obtained with supervised SOL methods, such as CRFs, even if a large amount of labeled data is available, as shown in our experiments on CoNLL2003 NER and CoNLL-2000 Chunking data. In addition, compared with SS-CRF-MER, our hybrid model has several good characteristics including a low calculation cost and a robust optimization in terms of a sensitiveness of hyper-parameters. This is described in detail in Section 5.3."
        },
        {
            "heading": "2 Supervised SOL: CRFs",
            "text": "This paper focuses solely on sequence labeling tasks, such as named entity recognition (NER) and syntactic chunking (Chunking), as SOL problems. Thus, letx=(x1, . . . , xS)\u2208X be an input sequence, andy=(y0, . . . , yS+1)\u2208Y be a particular output sequence, wherey0 andyS+1 are special fixed labels that represent the beginning and end of a sequence.\nAs regards supervised sequence learning, CRFs are recently introduced methods that constitute flexible and powerful models for structured predictors based on undirected graphical models that have been\nglobally conditioned on a set of inputs (Lafferty et al., 2001). Let\u03bb be a parameter vector and f(ys\u22121, ys, x) be a (local) feature vector obtained from the corresponding positions given x. CRFs define the conditional probability,p(y|x), as being proportional to a product of potential functions on the cliques. That is,p(y|x) on a (linear-chain) CRF can be defined as follows:\np(y|x; \u03bb) = 1 Z(x)\nS+1\u220f\ns=1\nexp(\u03bb \u00b7 f (ys\u22121, ys, x)).\nZ(x) = \u2211\ny \u220fS+1 s=1 exp(\u03bb \u00b7 f(ys\u22121, ys, x)) is a nor-\nmalization factor over all output values,Y, and is also known as the partition function.\nFor parameter estimation (training), given labeled dataDl = {(xk, yk)}Kk=1, the Maximum a Posteriori (MAP) parameter estimation, namely maximizing log p(\u03bb|Dl), is now the most widely used CRF training criterion. Thus, we maximize the following objective function to obtain optimal\u03bb:\nLCRF(\u03bb) = \u2211\nk\n[ \u03bb \u00b7 \u2211\ns\nf s \u2212 log Z(x k)\n] + log p(\u03bb), (1)\nwheref s is an abbreviation of (ys\u22121, ys, x) and p(\u03bb) is a prior probability distribution of\u03bb. A gradient-based optimization algorithm such as LBFGS (Liu and Nocedal, 1989) is widely used for maximizing Equation (1). The gradient of Equation (1) can be written as follows:\n\u2207LCRF(\u03bb) = \u2211\nk\nEp\u0303(yk,xk;\u03bb) [\u2211\ns\nf s ]\n\u2212 \u2211\nk\nEp(Y|xk;\u03bb) [\u2211\ns\nf s ] +\u2207 log p(\u03bb).\nCalculatingEp(Y|x,\u03bb) as well as the partition function Z(x) is not always tractable. However, for linear-chain CRFs, a dynamic programming algorithm similar in nature to the forward-backward algorithm in HMMs has already been developed for an efficient calculation (Lafferty et al., 2001).\nFor prediction, the most probable output, that is, y\u0302 = arg maxy\u2208Y p(y|x; \u03bb), can be efficiently obtained by using the Viterbi algorithm."
        },
        {
            "heading": "3 Hybrid Generative and Discriminative Approach to Semi-Supervised SOL",
            "text": "In this section, we describe our formulation of a hybrid approach to SOL and a parameter estimation method for sequence predictors. We assume\nthat we have a set of labeled and unlabeled data, D = {Dl,Du}, whereDl = {(xn, yn)}Nn=1 and Du = {xm}Mm=1.\nLet us assume that we haveI-units of discriminative models,pDi , andJ-units of generative models, pGj . Our hybrid model for a structured predictor is designed by the discriminative combination of several joint probability densities ofx andy, p(x, y). That is, the posterior probability of our hybrid model is defined by providing the log-values ofp(x, y) as the features of a log-linear model, such that:\nR(y|x;\u039b,\u0398,\u0393)\n=\n\u220f i pDi (x, y; \u03bbi) \u03b3i \u220f j pGj (x, y; \u03b8j) \u03b3j\n\u2211 y \u220f i pDi (x, y; \u03bbi) \u03b3i \u220f j pGj (x, y; \u03b8j) \u03b3j\n=\n\u220f i pDi (y|x; \u03bbi)\u03b3i \u220f j pGj (x, y; \u03b8j) \u03b3j\n\u2211 y \u220f i pDi (y|x; \u03bbi)\u03b3i \u220f j pGj (x, y; \u03b8j) \u03b3j .\n(2)\nHere, \u0393 = {{\u03b3i}Ii=1, {\u03b3j} I+J j=I+1} represents the discriminative combination weight of each model where\u03b3i,\u03b3j\u2208 [0, 1]. Moreover,\u039b={\u03bbi}Ii=1 and\u0398= {\u03b8j}Jj=1 represent model parameters of individual models estimated from labeled and unlabeled data, respectively. UsingpD(x,y) = pD(y|x)pD(x), we can derive the third line from the second line, where pDi (x; \u03bbi)\n\u03b3i for all i are canceled out. Thus, our hybrid model is constructed by combining discriminative models,pDi (y|x; \u03bbi), with generative models, pGj (x, y; \u03b8j).\nHereafter, let us assume that our hybrid model consists of CRFs for discriminative models,pDi , and HMMs for generative models,pGj , shown in Equation (2), since this paper focuses solely on sequence modeling. For HMMs, we consider a first order HMM defined in the following equation:\np(x, y|\u03b8) = S+1\u220f\ns=1\n\u03b8ys\u22121,ys\u03b8ys,xs ,\nwhere \u03b8ys\u22121,ys and \u03b8ys,xs represent the transition probability between statesys\u22121 andys and the symbol emission probability of thes-th position of the corresponding input sequence, respectively, where \u03b8yS+1,xS+1 = 1.\nIt can be seen that the formalization in the loglinear combination of our hybrid model is very similar to that of LOP-CRFs (Smith et al., 2005). In fact, if we only use a combination of discriminative\nmodels (CRFs), which is equivalent to\u03b3j = 0 for all j, we obtain essentially the same objective function as that of the LOP-CRFs. Thus, our framework can also be seen as an extension of LOP-CRFs that enables us to incorporate unlabeled data."
        },
        {
            "heading": "3.1 Discriminative Combination",
            "text": "For estimating the parameter\u0393, let us assume that we already have discriminatively trained models on labeled data,pDi (y|x;\u03bbi). We maximize the following objective function for estimating parameter \u0393 under a fixed\u0398:\nLHySOL(\u0393|\u0398) = \u2211\nn\nlog R(yn|xn;\u039b,\u0398,\u0393)+log p(\u0393). (3)\nwherep(\u0393) is a prior probability distribution of\u0393. The value of\u0393 providing a global maximum of LHySOL(\u0393|\u0398) is guaranteed under an arbitrary fixed value in the\u0398 domain, sinceLHySOL(\u0393|\u0398) is a concave function of\u0393. Thus, we can easily maximize Equation (3) by using a gradient-based optimization algorithm such as (bound constrained) L-BFGS (Liu and Nocedal, 1989)."
        },
        {
            "heading": "3.2 Incorporating Unlabeled Data",
            "text": "We cannot directly incorporate unlabeled data for discriminative training such as Equation (3) since the correct outputsy for unlabeled data are unknown. On the other hand, generative approaches can easily deal with unlabeled data as incomplete data (data with missing variabley) by using a mixture model. A well-known way to achieve this incorporation is to maximize the log likelihood of unlabeled data with respect to the marginal distribution of generative models as\nL(\u03b8) = \u2211\nm\nlog \u2211\ny\np(xm, y; \u03b8).\nIn fact, (Nigam et al., 2000) have reported that using unlabeled data with a mixture model can improve the text classification performance.\nAccording to Bayes\u2019 rule, p(y|x; \u03b8) \u221d p(x, y; \u03b8), the discriminant functions of generative classifiers are provided by generative models p(x, y; \u03b8). Therefore, we can regardL(\u03b8) as the logarithm of the sum of discriminant functions for all missing variablesy of unlabeled data. Following this view, we can directly incorporate unlabeled data into our hybrid model by maximizing the\ndiscriminant functionsg of our hybrid model in the same way as for a mixture model as explained above. Thus, we maximize the following objective function for estimating the model parameters\u0398 for generative models of unlabeled data:\nG(\u0398|\u0393) = \u2211\nm\nlog \u2211\ny\ng(xm, y;\u0398) + log p(\u0398). (4)\nwherep(\u0398) is a prior probability distribution of\u0398. Here, the discriminant functiong of outputy given\ninput x in our hybrid model can be obtained by the\nnumerator on the third line of Equation (2), since the denominator does not affect the determination ofy, that is,\ng(x, y;\u0398) = \u220f\ni\npDi (y|x; \u03bbi)\u03b3i \u220f\nj\npGj (x, y; \u03b8j) \u03b3j .\nUnder a fixed\u0393, we can estimate the local maximum of G(\u0398|\u0393) around the initialized value of\u0398 by an iterative computation such as the EM algorithm (Dempster et al., 1977). Let\u0398\u2032\u2032 and\u0398\u2032 be estimates of\u0398 in the next and current steps, respectively. Using Jensen\u2019s inequality,log a \u2264 a \u2212 1, we obtain aQ-function that satisfies the inequality G(\u0398\u2032\u2032|\u0393)\u2212G(\u0398\u2032|\u0393)\u2265Q(\u0398\u2032\u2032,\u0398\u2032;\u0393)\u2212Q(\u0398\u2032,\u0398\u2032;\u0393), such that\nQ(\u0398\u2032\u2032,\u0398\u2032;\u0393)\n= \u2211\nj\n\u03b3j \u2211\nm\n\u2211\ny\nR(y|xm;\u039b,\u0398\u2032,\u0393) log pGj (xm, y;\u0398\u2032\u2032)\n+ log p(\u0398\u2032\u2032). (5)\nSinceQ(\u0398\u2032,\u0398\u2032;\u0393) is independent of\u0398\u2032\u2032, we can improve the value ofG(\u0398|\u0393) by computing\u0398\u2032\u2032 to maximizeQ(\u0398\u2032\u2032,\u0398\u2032;\u0393). We can obtain a\u0398 estimate by iteratively performing this update while G(\u0398|\u0393) is hill climbing.\nAs shown in Equation (5),R is used for estimating the parameter\u0398. The intuitive effect of maximizing Equation (4) is similar to performing \u2018softclustering\u2019. That is, unlabeled data is clustered with respect to theR distribution, which also includes information about labeled data, under the constraint of generative model structures."
        },
        {
            "heading": "3.3 Parameter Estimation Procedure",
            "text": "According to our definition, the\u0398 and \u0393 estimations are mutually dependent. That is, the parameters of the hybrid model,\u0393, should be estimated\n1.Given training set:Du = {xm}Mm=1 and Dl = {D\u2032l = {(xk, yk)}Kk=1,D\u2032\u2032l = {(xn, yn)}Nn=1} 2.Compute\u039b, usingD\u2032l. 3.Initialize\u0393(0), \u0398(0) andt \u2190 0. 4.Perform the following until|\u0398\n(t+1)\u2212\u0398(t)| |\u0398(t)| < \u00b2.\n4.1. Compute\u0398(t+1) to maximize Equation (4)\nusing Equation (3) with a fixed\u0398, while the parameters of the generative models,\u0398, should be estimated using Equation (4) with a fixed\u0393. As a solution to our parameter estimation, we search for the \u0398 and\u0393 that maximizeLHySOL(\u0393|\u0398) andG(\u0398|\u0393) simultaneously. For this search, we compute\u0398 and \u0393 by maximizing the objective functions shown in Equations (4) and (3) iteratively and alternately. We summarize the algorithm for estimating these model parameters in Figure 1.\nNote that during the\u0393 estimation (procedure 4.2 in Figure 1),\u0393 can be over-fitted to the labeled training data if we use the same labeled training data as used for the\u039b estimation. There are several possible ways to reduce this over-fit. In this paper, we select one of the simplest; we divide the labeled training dataDl into two distinct setsD\u2032l andD\u2032\u2032l . Then,D\u2032l andD\u2032\u2032l are individually used for estimating\u039b and \u0393, respectively. In our experiments, we divide the labeled training dataDl so that 4/5 is used forD\u2032l and the remaining 1/5 forD\u2032\u2032l ."
        },
        {
            "heading": "3.4 Efficient Parameter Estimation Algorithm",
            "text": "Let NR(x) represent the denominator of Equation (2), that is the normalization factor ofR. We can rearrange Equation (2) as follows:\nR(y|x;\u039b,\u0398,\u0393) = \u220f s \u220f i [ V Di,s ]\u03b3i \u220f j [ V Gj,s ]\u03b3j NR(x) \u220f i [Zi(x)]\u03b3i , (6)\nwhereV Di,s represents the potential function of the s-th position of the sequence in thei-th CRF and V Gj,s represents the probability of thes-th position in the j-th HMM, that is,V Di,s = exp(\u03bbi \u00b7 f s) and V Gj,s = \u03b8ys\u22121,ys\u03b8ys,xs , respectively. See the Appendix for the derivation of Equation (6) from Equation (2).\nTo estimate\u0393(t+1), namely procedure 4.2 in Figure 1, we employ the derivatives with respect to\u03b3i and\u03b3j shown in Equation (6), which are the parameters of the discriminative and generative models, respectively. Thus, we obtain the following derivatives with respect to\u03b3i:\n\u2202LHySOL(\u0393|\u0398) \u2202\u03b3i\n= \u2211\nn\nlog pDi (y n|xn) +\n\u2211\nn\nlog ZDi (x n)\n\u2212 \u2211\nn\nER(Y|xn;\u039b,\u0398,\u0393) [\u2211\ns\nlog V Di,s ] .\nThe first and second terms are constant during iterative procedure 4 in our optimization algorithm shown in Figure 1. Thus, we only need to calculate these values once at the beginning of procedure 4. Let\u03b1s(y) and\u03b2s(y) represent the forward and backward state costs at positions with output y for corresponding inputx. Let Vs(y, y\u2032) represent the products of the total value of the transition cost betweens\u22121 ands with labelsy andy\u2032 in the corresponding input sequence, that is,Vs(y, y\u2032) =\u220f\ni[V D i,s(y, y\n\u2032)]\u03b3i \u220f\nj [V G j,s(y, y \u2032)]\u03b3j . The third term, which indicates the expectation of potential functions, can be rewritten in the form of a forwardbackward algorithm, that is,\nER(Y|x;\u039b,\u0398,\u0393) [\u2211\ns\nlog V Di,s ]\n= 1\nZR(x)\n\u2211\ns\n\u2211\ny,y\u2032\n\u03b1s\u22121(y)Vs(y, y\u2032)\u03b2s(y \u2032) log V Di,s(y, y \u2032),\n(7)\nwhereZR(x) represents the partition function of our hybrid model, that is,ZR(x)=NR(x) \u220f i[Zi(x)]\n\u03b3i . Hence, the calculation of derivatives with respect to \u03b3i is tractable since we can incorporate the same forward-backward algorithm as that used in a standard CRF.\nThen, the derivatives with respect to\u03b3j , which are the parameters of generative models, can be written as follows:\n\u2202LHySOL(\u0393|\u0398) \u2202\u03b3j\n= \u2211\nn\nlog pGj (x n, yn)\u2212\n\u2211\nn\nER(Y|xn;\u039b,\u0398,\u0393) [\u2211\ns\nlog V Gj,s ] .\nAgain, the second term, which indicates the expectation of transition probabilities and symbol emission probabilities, can be rewritten in the form of a forward-backward algorithm in the same manner as\n\u03b3i, where the only difference is thatV Di,s is substituted byV Gj,s in Equation (7).\nTo estimate\u0398(t+1), which is procedure 4.1 in Figure 1, the same forward-backward algorithm as used in standard HMMs is available since the form of our Q-function shown in Equation (5) is the same as that of standard HMMs. The only difference is that our method uses marginal probabilities given byR instead of thep(x, y; \u03b8) of standard HMMs.\nTherefore, only a forward-backward algorithm is required for the efficient calculation of our parameter estimation process. Note that even though our hybrid model supports the use of a combination of several generative and discriminative models, we only need to calculate the forward-backward algorithm once for each sample during optimization procedures 4.1 and 4.2. This means that the required number of executions of the forward-backward algorithm for our parameter estimation is independent of the number of models used in the hybrid model.\nIn addition, after training, we can easily merge all the parameter values in a single parameter vector. This means that we can simply employ the Viterbialgorithm for evaluating unseen samples, as well as that of standard CRFs, without any additional cost."
        },
        {
            "heading": "4 Experiments",
            "text": "We examined our hybrid model (HySOL) by applying it to two sequence labeling tasks, named entity recognition (NER) and syntactic chunking (Chunking). We used the same Chunking and \u2018English\u2019 NER data as those used for the shared tasks of CoNLL-2000 (Tjong Kim Sang and Buchholz, 2000) and CoNLL-2003 (Tjong Kim Sang and Meulder, 2003), respectively.\nFor the baseline method, we performed a conditional random field (CRF), which is exactly the same training procedure described in (Sha and Pereira, 2003) with L-BFGS. Moreover, LOP-CRF (Smith et al., 2005) is also compared with our hybrid model, since the formalism of our hybrid model can be seen as an extension of LOP-CRFs as described in Section 3. For CRF, we used the Gaussian prior as the second term on the RHS in Equation (1), where \u03b42 represents the hyper-parameter in the Gaussian prior. In contrast, for LOP-CRF and HySOL, we used the Dirichlet priors as the second term on the\nRHS in Equations (3), and (4), where\u03be and\u03b7 are the hyper-parameters in each Dirichlet prior."
        },
        {
            "heading": "4.1 Named Entity Recognition Experiments",
            "text": "The English NER data consists of 203,621, 51,362 and 46,435 words from 14,987, 3,466 and 3,684 sentences in training, development and test data, respectively, with four named entity tags, PERSON, LOCATION, ORGANIZATION and MISC, plus the \u2018O\u2019 tag. The unlabeled data consists of 17,003,926 words from 1,029,122 sentences. These data sets are exactly the same as those provided for the shared task of CoNLL-2003.\nWe slightly extended the feature set of the supplied data by adding feature types such as \u2018word type\u2019, and word prefix and suffix. Examples of \u2018word type\u2019 include whether the word is capitalized, contains digit or contains punctuation, which basically follows the baseline features of (Sutton et al., 2006) without regular expressions. Note that, unlike several previous studies, we did not employ additional information from external resources such as gazetteers. All our features can be automatically extracted from the supplied data.\nFor LOP-CRF and HySOL, we used four base discriminative models trained by CRFs with different feature sets. Table 1 shows the feature sets we used for training these models. The design of these feature sets was derived from a suggestion in (Smith et al., 2005), which exhibited the best performance in the several feature division. Note that the CRF for the comparison method was trained by using all fea-\nture types, namely the same as\u03bb4.\nAs we explained in Section 3.3, for training HySOL, the parameters of four discriminative models,\u039b, were trained from 4/5 of the labeled training data, and\u0393 were trained from remaining 1/5. For the features of the generative models, we used all of the feature types shown in Figure 1. Note that one feature type corresponds to one HMM. Thus, each HMM maintains to consist of a non-overlapping feature set since each feature type only generates one symbol per state."
        },
        {
            "heading": "4.2 Syntactic Chunking Experiments",
            "text": "CoNLL-2000 Chunking data was obtained from the Wall Street Journal (WSJ) corpus: sections 15-18 as training data (8,936 sentences and 211,727 words), and section 20 as test data (2,012 sentences and 47,377 words), with 11 different chunk-tags, such as NP and VP plus the \u2018O\u2019 tag, which represents the region outside any target chunk.\nFor LOP-CRF and HySOL, we also used four base discriminative models trained by CRFs with different feature sets. Table 2 shows the feature set we used in the Chunking experiments. We used the feature set of the supplied data without any extenion of additional feature types.\nTo train HySOL, we used the same unlabeled data as used for our NER experiments (17,003,926 words from the Reuters corpus). Moreover, the division of the labeled training data and the feature set of the generative models were derived in the same manner as our NER experiments (see Section 4.1). That is, we divided the labeled training data into 4/5 for estimating\u039b and 1/5 for estimating\u0393; one feature type shown in Table 2 is assigned in one generative model.\nmethods (hyper-params) F\u03b2=1 (gain) Sent (gain)\nCRF (\u03b42=10.0) 93.87 - 59.84 - (4/5 labeled data,\u03b42=10.0) 93.70 (-0.17)58.85 (-0.99) LOP-CRF (\u03be\u2032=0.1) 93.91 (+0.04)60.34 (+0.50) HySOL (\u03be\u2032=1.0,\u03b7\u2032=0.0001)94.30 (+0.43) 61.73 (+1.89) (w/o prior) 94.17 (+0.30)61.23 (+1.39) w/o pGj \u2200j (\u03be\u2032=1.0) 93.84 (-0.03)59.74 (-0.10)\nTable 4: Chunking performance (CoNLL-2000)"
        },
        {
            "heading": "5 Results and Discussion",
            "text": "We evaluated the performance in terms of the F\u03b2=1 score, which is the evaluation measure used in CoNLL-2000 and 2003, and sentence accuracy, since all the methods in our experiments optimize sequence loss. Tables 3 and 4 show the results of the NER and Chunking experiments, respectively. The F\u03b2=1 and \u2018Sent\u2019 columns show the performance evaluated using the F\u03b2=1 score and sentence accuracy, respectively.\u03b42, \u03be and\u03b7, which are the hyperparameters in Gaussian or Dirichlet priors, are selected from a certain value set by using a development set1, that is,\u03b42 \u2208 {0.01, 0.1, 1, 10, 100, 1000}, \u03be \u2212 1 = \u03be\u2032 \u2208 {0.01, 0.1, 1, 10} and\u03b7 \u2212 1 = \u03b7\u2032 \u2208 {0.00001, 0.0001, 0.001, 0.01}. The second rows of CRF in Tables 3 and 4 represent the performance of base discriminative models used in HySOL with all the features, which are trained with 4/5 of the labeled training data. The third rows of HySOL show the performance obtained without using generative models (unlabeled data). The model itself is essentially the same as LOP-CRFs. However the performance in the third HySOL rows was consistently lower than that of LOP-CRF since the discriminative models in HySOL are trained with 4/5 labeled data.\nAs shown in Tables 3 and 4, HySOL signifi1Chunking (CoNLL-2000) data has no common develop-\nment set. Thus, our preliminary examination employed by using 4/5 labeled training data with the remaining 1/5 as development data to determine the hyper-parameter values.\n(a) NER (b) Chunking\nFigure 2: Changes in the performance and the convergence condition value (procedure 4 in Figure 1) of HySOL.\ncantly improved the performance of supervised setting, CRF and LOP-CRF, as regards both NER and Chunking experiments."
        },
        {
            "heading": "5.1 Impact of Incorporating Unlabeled Data",
            "text": "The contributions provided by incorporating unlabeled data in our hybrid model can be seen by comparison with the performance of the first and third r ws in HySOL, namely a 2.64 point F-score and a 2.96 point sentence accuracy gain in the NER experiments and a 0.46 point F-score and a 1.99 point sentence accuracy gain in the Chunking experiments.\nWe believe there are two key ideas that enable the unlabeled data in our approach to exhibit this improvement compared with the the state-of-the-art performance provided by discriminative models in supervised settings. First, unlabeled data is only used for optimizing Equation (4) to obtain a similar e fect to \u2019soft-clustering\u2019, which can be calculated without information about the correct output. Second, by using a combination of generative models, we can enhance the flexibility of the feature design for unlabeled data. For example, we can handle arbitrary overlapping features, similar to those used in discriminative models, for unlabeled data by assigning one feature type for one generative model as in our experiments."
        },
        {
            "heading": "5.2 Impact of Iterative Parameter Estimation",
            "text": "Figure 2 shows the changes in the performance and the convergence condition value of HySOL during parameter estimation iteration in our NER and Chunking experiments, respectively. As shown in the figure, HySOL was able to reach the conver-\ngence condition in a small number of iterations in our experiments. Moreover, the change in the performance remains quite stable during the iteration. However, theoretically, our optimization procedure is not guaranteed to converge in the\u0393 and\u0398 space, since the optimization of\u0398 has local maxima. Even if we were unable to meet the convergence condition, we were easily able to obtain model parame-\nters by performing a sufficient fixed number of itera-\ntions, and then select the parameters when Equation\n(4) obtained the maximum objective value."
        },
        {
            "heading": "5.3 Comparison with SS-CRF-MER",
            "text": "When we consider semi-supervised SOL methods, SS-CRF-MER (Jiao et al., 2006) is the most competitive with HySOL, since both methods are defined based on CRFs. We planned to compare the perfor-\nmance with that of SS-CRF-MER in our NER and\nChunking experiments. Unfortunately, we failed to\nimplement SS-CRF-MER since it requires the use of a slightly complicated algorithm, called the \u2018nested\u2019 forward-backward algorithm.\nAlthough, we cannot compare the performance, our hybrid approach has several good characteristics compared with SS-CRF-MER. First, it requires a higher order algorithm, namely a \u2018nested\u2019 forwardbackward algorithm, for the parameter estimation of unlabeled data whose time complexity isO(L3S2) for each unlabeled data, whereL andS represent the output label size and unlabeled sample length, respectively. Thus, our hybrid approach is more scalable for the size of unlabeled data, since HySOL only needs a standard forward-backward algorithm whose time complexity isO(L2S). In fact, we still have a question as to whether SS-CRF-MER is really scalable in practical time for such a large amount of unlabeled data as used in our experiments, which is about 680 times larger than that of (Jiao et al., 2006). Scalability for unlabeled data will become really important in the future, as it will be natural to use millions or billions of unlabeled data for further improvement. Second, SS-CRFMER has a sensitive hyper-parameter in the objective function, which controls the influence of the unlabeled data. In contrast, our objective function only has a hyper-parameter of prior distribution, which is widely used for standard MAP estimation. Moreover, the experimental results shown in Tables 3 and\nF\u03b2=1 additional resources ASO-semi 89.31 unlabeled data (27M words) (Ando and Zhang, 2005) (Florian et al., 2003) 88.76 their own large gazetteers, 2M-word labeled data (Chieu and Ng, 2003) 88.31 their own large gazetteers, very elaborated features HySOL 88.14 unlabeled data (17M words) supplied gazetters HySOL 87.20 unlabeled data (17M words)\nF\u03b2=1 additional resources ASO-semi 94.39 unlabeled data (Ando and Zhang, 2005) (15M words: WSJ) HySOL 94.30 unlabeled data (17M words: Reuters) (Zhang et al., 2002) 94.17 full parser output (Kudo and Matsumoto, 2001)93.91 \u2013\n4 indicate that HySOL is rather robust with respect to the hyper-parameter since we can obtain fairly good performance without a prior distribution."
        },
        {
            "heading": "5.4 Comparison with Previous Top Systems",
            "text": "With respect to the performance of NER and Chunking tasks, the current best performance is reported in (Ando and Zhang, 2005), which we refer to as \u2018ASO-semi\u2019, as shown in Figures 5 and 6. ASOsemi also incorporates unlabeled data solely for the additional information in the same way as our method. Unfortunately, our results could not reach their level of performance, although the size and source of the unlabeled data are not the same for certain reasons. First, (Ando and Zhang, 2005) does not describe the unlabeled data used in their NER experiments in detail, and second, we are not licensed to use the TREC corpus including WSJ unlabeled data that they used for their Chunking experiments (training and test data for Chunking is derived from WSJ). Therefore, we simply used the supplied unlabeled data of the CoNLL-2003 shared task for both NER and Chunking. If we consider the advantage of our approach, our hybrid model incorporating generative models seems rather intuitive, since it is sometimes difficult to find out a design of effective auxiliary problems for the target problem.\nInterestingly, the additional information obtained\nF\u03b2=1 (gain)\nHySOL (\u03be\u2032=0.1,\u03b7\u2032=0.0001) 87.20 - + w/ F-score opt. (Suzuki et al., 2006)88.02 (+0.82) + unlabeled data (17M\u2192 27M words) 88.41 (+0.39) + supplied gazetters 88.90 (+0.49) + add dev. set for estimating\u0393 89.27 (+0.37)\nF\u03b2=1 (gain) HySOL (\u03be\u2032=0.1,\u03b7\u2032=0.0001) 94.30 -\n+ w/ F-score opt. (Suzuki et al., 2006)94.36 (+0.06)\nTable 8: The HySOL performance with the F-score optimization technique on Chunking (CoNLL-2000) experiments\nfrom unlabeled data appear different from each other. ASO-semi uses unlabeled data for constructing auxiliary problems to find the \u2018shared structures\u2019 of auxiliary problems that are expected to improve the performance of the main problem. Moreover, it is possible to combine both methods, for example, by incorporating the features obtained with their method in our base discriminative models, and then construct a hybrid model using our method. Therefore, there may be a possibility of further improving the performance by this simple combination.\nIn NER, most of the top systems other than ASO-semi boost performance by employing external hand-crafted resources such as large gazetteers. This is why their results are superior to those obtained with HySOL. In fact, if we simply add the gazetteers included in CoNLL-2003 supplied data as features, HySOL achieves 88.14."
        },
        {
            "heading": "5.5 Applying F-score Optimization Technique",
            "text": "In addition, we can simply apply the F-score optimization technique for the sequence labeling tasks proposed in (Suzuki et al., 2006) to boost the HySOL performance since the base discriminative models pD(y|x) and discriminative combination, namely Equation (3), in our hybrid model basically uses the same optimization procedure as CRFs. Tables 7 and 8 show the F-score gain when we apply the F-score optimization technique. As shown in the Tables, the F-score optimization technique can easily improve the (F-score) performance without any additional resources or feature engineering.\nIn NER, we also examined HySOL with additional resources to observe the performance gain. The third row represents the performance when we add approximately 10M words of unlabeled data (total 27M words)2 that are derived from 1996/11/15-\n30 articles in Reuters corpus. Then, the fourth and\nfifth rows represent the performance when we add\nthe supplied gazetters in the CoNLL-2003 data as\nfeatures, and adding development data as training data of\u0393. In this case, HySOL achieved a comparable performance to that of the current best system, ASO-semi, in both NER and Chunking exper-\niments even though the NER experiment is not a\nfair comparison since we added additional resources\n(gazetters and dev. set) that ASO-semi does not use\nin training."
        },
        {
            "heading": "6 Conclusion and Future Work",
            "text": "We proposed a framework for semi-supervised SOL based on a hybrid generative and discriminative approach. Experimental results showed that incorporating unlabeled data in a generative manner has the power to further improve on the state-of-the-art performance provided by supervised SOL methods such as CRFs, with the help of our hybrid approach, which discriminatively combines with discriminative models. In future we intend to investigate more appropriate model and feature design for unlabeled data, which may further improve the performance achieved in our experiments.\nAppendix\nLet V Di,s = exp(\u03bb \u00b7 f s) andV Gj,s = \u03b8ys\u22121,ys\u03b8ys,xs . Equation (6) can be obtained by the following rearrangement of Equation (2) :\nR(y|x;\u039b,\u0398,\u0393)\n=\n\u220f i pDi (y|x, \u03bbi)\u03b3i \u220f j pGj (x, y, \u03b8j) \u03b3j\n\u2211 y \u220f i pDi (y|x, \u03bbi)\u03b3i \u220f j pGj (x, y, \u03b8j) \u03b3j\n= 1 NR(x) \u220f\ni\n[\u220f s V Di,s\nZi(x)\n]\u03b3i\u220f\nj\n[\u220f\ns\nV Gj,s ]\u03b3j\n= 1 NR(x) \u220f\ni [Zi(x)]\u03b3i\n\u220f\ni\n[\u220f\ns\nV Di,s ]\u03b3i\u220f\nj\n[\u220f\ns\nV Gj,s ]\u03b3j\n= 1 NR(x) \u220f\ni [Zi(x)]\u03b3i\n\u220f\ns\n\u220f\ni\n[ V Di,s ]\u03b3i \u220f\nj\n[ V Gj,s ]\u03b3j .\n2In order to keep the consistency of POS tags, we reattached POS tags of the supplied data set and new 10M words of unlabeled data using a POS tagger trained from WSJ corpus."
        }
    ],
    "title": "Semi-Supervised Structured Output Learning Based on a Hybrid Generative and Discriminative Approach",
    "year": 2007
}