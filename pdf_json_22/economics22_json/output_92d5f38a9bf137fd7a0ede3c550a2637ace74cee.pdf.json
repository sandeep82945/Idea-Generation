{
    "abstractText": "Advanced health economic analysis techniques currently performed in Microsoft Excel, such as incorporating heterogeneity, time-dependent transitions and a value of information analysis, can be easily transferred to R. Often the outputs of survival analyses (such as Weibull regression models) will estimate the impacts of correlated patient characteristics on patient outcomes, and are utilised directly as inputs for health economic decision models. This tutorial provides a step-by-step guide of how to conduct such analyses with a Markov model developed in R, and offers a comparison with established analyses performed in Microsoft Excel. This is done through the conversion of a previously published Microsoft Excel case study of a hip replacement surgery cost-effectiveness model. We hope that this paper can act as a facilitator in switching decision models from Microsoft Excel to R for complex health economic analyses, providing open-access code and data, suitable for future adaptation. Nichola R. Naylor, Jack Williams are joint first authors. * Nichola R. Naylor nichola.naylor@lshtm.ac.uk 1 Department of Health Services Research and Policy, London School of Hygiene & Tropical Medicine, London, UK 2 HCAI, Fungal, AMR, AMU and Sepsis Division, UK Health Security Agency, London, UK 3 Department of Statistical Science, University College London, London, UK 4 Mathematical Sciences Research Centre, Queen\u2019s University Belfast, Belfast, UK Key Points for Decision Makers First tutorial for linking survival analyses to Markov models and performing sensitivity analyses (including a value of information analysis) in R. Intended for users of Microsoft Excel with downloadable resources across both types of software, with a practical example for total hip replacement prosthesis strategies. Provides adaptable open-access resources to be used as frameworks for future health economic evaluation models in R.",
    "authors": [
        {
            "affiliations": [],
            "name": "Nichola R. Naylor"
        },
        {
            "affiliations": [],
            "name": "Jack Williams are"
        }
    ],
    "id": "SP:f631ff0f0651bdaf9ffc5689c3cc7c2b31d55dcf",
    "references": [
        {
            "authors": [
                "D Incerti",
                "H Thom",
                "G Baio",
                "JP. Jansen"
            ],
            "title": "R you still using excel? the advantages of modern software tools for health technology assessment. value Health",
            "year": 2019
        },
        {
            "authors": [
                "G Baio",
                "A. Heath"
            ],
            "title": "When simple becomes complicated: why excel should lose its place at the top table. Glob",
            "venue": "Reg Health Technol. Assess",
            "year": 2017
        },
        {
            "authors": [
                "C Hollman",
                "M Paulden",
                "P Pechlivanoglou",
                "C. McCabe"
            ],
            "title": "A comparison of four software programs for implementing decision analytic cost-effectiveness models. PharmacoEconomics",
            "year": 2017
        },
        {
            "authors": [
                "N Nathan Green",
                "F Lamrock",
                "NR Naylor",
                "J Williams",
                "A. Briggs"
            ],
            "title": "Health economic evaluation using Markov models in R for microsoft excel users: a tutorial (2022, submitted)",
            "year": 2022
        },
        {
            "authors": [
                "A Briggs",
                "M Sculpher",
                "J Dawson",
                "R Fitzpatrick",
                "D Murray",
                "H. Malchau"
            ],
            "title": "The use of probabilistic decision models in technology assessment: the case of total hip replacement",
            "venue": "Appl Health Econ Health Policy",
            "year": 2004
        },
        {
            "authors": [],
            "title": "2022. https:// github. com/ Excel-R- tutor ials",
            "venue": "Exten sions_ Paper. Accessed",
            "year": 2022
        },
        {
            "authors": [
                "N. Stephen"
            ],
            "title": "Version control with Git and SVN. https:// suppo rt. rstud io. com/ hc/ en- us/ artic les/ 20053 2077Versi on- Contr ol- withGit- and",
            "venue": "SVN. Accessed",
            "year": 2022
        },
        {
            "authors": [
                "Dragulescu AA"
            ],
            "title": "xlsx) files. https:// mran. revol ution analy tics. com/ snaps",
            "venue": "web/ packa ges/ xlsx/ vigne ttes/ xlsx. pdf",
            "year": 2014
        },
        {
            "authors": [
                "H. Wickham"
            ],
            "title": "Reshaping data with the reshape package",
            "venue": "J Stat Softw",
            "year": 2007
        },
        {
            "authors": [
                "H. Wickham"
            ],
            "title": "Elegant graphics for data analysis. Media",
            "year": 2009
        },
        {
            "authors": [
                "A Briggs",
                "M Sculpher",
                "K. Claxton"
            ],
            "title": "Decision modelling for health economic evaluation. 2022",
            "venue": "https:// www. herc. ox. ac. uk/ downl oads/ decis ion- model ling- for- health- econo mic- evalu ation. Accessed",
            "year": 2022
        },
        {
            "authors": [
                "A Briggs",
                "M Sculpher",
                "K. Claxton"
            ],
            "title": "Decision modelling for health economic evaluation",
            "year": 2006
        },
        {
            "authors": [
                "F Alarid-Escudero",
                "EM Krijkamp",
                "Enns EA",
                "A Yang",
                "M Hunink",
                "P Pechlivanoglou",
                "H. Jalal"
            ],
            "title": "An introductory tutorial on cohort state-transition models in R using a cost-effectiveness analysis example. 2020",
            "venue": "https:// www. arXiv prepr int arXiv:",
            "year": 2001
        },
        {
            "authors": [
                "C Rothery",
                "M Strong",
                "HE Koffijberg",
                "A Basu",
                "S Ghabri",
                "S Knies",
                "E. Fenwick"
            ],
            "title": "Value of information analytical methods: report 2 of the ISPOR value of information analysis emerging good practices task force",
            "venue": "Value Health",
            "year": 2020
        },
        {
            "authors": [
                "M Strong",
                "JE Oakley",
                "A. Brennan"
            ],
            "title": "Estimating multiparameter partial expected value of perfect information from a probabilistic sensitivity analysis sample: a nonparametric regression approach",
            "venue": "Med Decis Making",
            "year": 2014
        },
        {
            "authors": [
                "A Heath",
                "N Kunst",
                "C Jackson",
                "M Strong",
                "F Alarid-Escudero",
                "JD Goldhaber-Fiebert",
                "H. Jalal"
            ],
            "title": "Calculating the expected value of sample information in practice: considerations from 3 case Studies",
            "venue": "Med Decis Making",
            "year": 2020
        },
        {
            "authors": [
                "N Kunst",
                "ECF Wilson",
                "D Glynn",
                "F Alarid-Escudero",
                "G Baio",
                "A. Brennan"
            ],
            "title": "Collaborative Network for Value of, I. Computing the expected value of sample information efficiently: practical guidance and recommendations for four model-Based methods",
            "venue": "Value Health",
            "year": 2020
        },
        {
            "authors": [
                "R Smith",
                "P. Schneider"
            ],
            "title": "Making health economic models Shiny: a tutorial",
            "venue": "Wellcome Open Res",
            "year": 2020
        },
        {
            "authors": [
                "A Filipovi\u0107-Pierucci",
                "K Zarca"
            ],
            "title": "Durand-Zalesk I. Markov models for health economic evaluations: the R package Heemod",
            "venue": "arXiv preprint",
            "year": 2017
        },
        {
            "authors": [
                "H. Wickham"
            ],
            "title": "ggplot2: elegant graphics for data analysis SpringerVerlag New York",
            "year": 2009
        },
        {
            "authors": [
                "SHEPRD. Signposting Healt"
            ],
            "title": "Economic Packages in R for Decision Modelling (SHEPRD)",
            "venue": "2021. https:// hermes- sheprd. netli fy. app/. Accessed 6 Oct",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Vol.:(0123456789)\nNichola R. Naylor, Jack Williams\u00a0are joint first authors.\n* Nichola R. Naylor nichola.naylor@lshtm.ac.uk\n1 Department of\u00a0Health Services Research and\u00a0Policy, London School of\u00a0Hygiene & Tropical Medicine, London, UK\n2 HCAI, Fungal, AMR, AMU and\u00a0Sepsis Division, UK Health Security Agency, London, UK\n3 Department of\u00a0Statistical Science, University College London, London, UK\n4 Mathematical Sciences Research Centre, Queen\u2019s University Belfast, Belfast, UK\nKey Points for Decision Makers\nFirst tutorial for linking survival analyses to Markov models and performing sensitivity analyses (including a value of information analysis) in R.\nIntended for users of Microsoft Excel with downloadable resources across both types of software, with a practical example for total hip replacement prosthesis strategies.\nProvides adaptable open-access resources to be used as frameworks for future health economic evaluation models in R."
        },
        {
            "heading": "1 Introduction",
            "text": "The benefits of utilising R for health economic evaluations are becoming well documented [1\u20133]. Whilst Microsoft (MS) Excel and TreeAge are visual graphical user interfaces and therefore useful software for learning purposes, R (alongside other programming language-based software such as MATLAB) has higher efficiency, transparency and adaptability in comparison [1\u20133].\nThe foundation of many such health economic evaluations is often the Markov model. Markov models can quantify the impact of interventions on transitions between health states, as well as the costs and outcomes associated with the differing course of actions. Intervention impacts on health outcomes, conditional on patient characteristics, can be quantified through standard survival analysis techniques. Subsequently, these impacts can be fed through Markov models to appropriately account for heterogeneity across subpopulations of interest. Additionally, decomposition techniques can be utilised to allow for covariance to be maintained during probabilistic sensitivity analyses.\nPrevious health economic evaluation tutorials for R generally run through how to create deterministic and probabilistic Markov models in R [4]. However, a comparison of more advanced modelling techniques, such as modelling heterogeneity through the inclusion of survival\nanalysis results whilst conducting value of information (VOI) analyses in R compared to MS Excel, has yet to be done. This tutorial first introduces a case study of hip replacement surgery, for which an MS Excel model has been published [5]. This case study is then used to demonstrate how to integrate survival analyses within sensitivity analyses using R instead of MS Excel. This is then followed by instructions on how to conduct analyses for the expected value of perfect information (EVPI) and the expected value of partially perfect information (EVPPI), also known as the expected value of perfect parameter information, within R. We outline how to conduct these analyses using mainly base R functions. By focusing on using basic R functions, rather than specific health economics R packages, it reduces reliance on \u201cblack boxes\u201d and increases the potential for adaptability to suit need."
        },
        {
            "heading": "2 General Tips",
            "text": ""
        },
        {
            "heading": "2.1 Setting Up Projects",
            "text": "Throughout this tutorial code, data available from the GitHub repository are cited [6]. Once the folder has been downloaded (which can be done through the \u201ccode\u201d button) or linked via another interface, such as RStudio, you can run the R project. Clicking on the Git \u201cR Project\u201d item automatically sets your working directory to be the equivalent to where the project is based, allowing you to read in data also within that folder. There are many blogs and guides on how to link Git and RStudio using R projects [7]. The RProject is the health economic evaluation model. The folder containing the RProject files acts as the \u201cMS Excel file\u201d, where the separate csv files and R scripts within that folder are similar to having different MS Excel sheets within the file, housing different data tables or analysis functions for the model."
        },
        {
            "heading": "2.2 Reading in\u00a0Data",
            "text": "In this tutorial, we read in data files that contain life table data and outputs of survival analyses. For csv files, base R (i.e. R \u201cas is\u201d) allows you to read in data using the \u2018read. csv()\u2019 function, where the file path relative to the working directory can be specified within the brackets. For example \u2018read.csv(\"life-table.csv\", header=TRUE)\u2019 reads in the life table csv data in the working directory, specifying that the first row of the csv file is the header row. For other types of data files, there are often already packages that deal with directly importing such data (e.g. the \u2018xlsx\u2019 package importing MS Excel \u2018.xls\u2019 and \u2018.xlsx\u2019 files [8])."
        },
        {
            "heading": "2.3 Graphics Packages",
            "text": "This tutorial uses base R wherever possible, reducing reliance on packages, and often allows for better understanding for each step. However, we do use the reshape2 [9] and ggplot2 [10] packages. reshape2 allows for easy data manipulation (e.g. converting data between long and wide formats), whilst ggplot2 allows users to create attractive plots and diagrams that suit need (e.g. plotting multiple variables on multiple panels)."
        },
        {
            "heading": "3 Total Hip Replacement: A\u00a0Case Study for\u00a0an\u00a0Advanced Health Economic Evaluation",
            "text": "Cost utility models for the total hip replacement case study are available online, performed in MS Excel [11, 12]. These are a simplified version of a previously published economic model, developed for education purposes [5]. This tutorial will use the same case study when demonstrating how to utilise R when conducting more advanced health economic modelling. Within this example, a new procedure that reduces the risk of revision surgery in a cohort of patients undergoing a primary total hip replacement (THR) procedure is compared to standard treatment. Potential health states and state transitions are displayed in Fig.\u00a01.\nAll of the R relevant materials used in this tutorial can be found within a corresponding GitHub repository [6]. The corresponding MS Excel files are available for download within this repository; or originally from the Briggs et\u00a0al. corresponding webpage [11]. Specifically, \u201cTHR_Model.R\u201d and \u201cTHR_Model_VOI.R\u201d scripts are equivalent to \u201cEx57sol.xls\u201d and \u201cEx66bsol.xls\u201d, respectively. The Markov model process or probabilistic analysis will not be described in detail here, as these are covered in detail elsewhere [4].\nIn this tutorial, we follow the structure outlined in Fig.\u00a02, referring to the stated subsection headings and relating these to equivalent MS Excel processes. This structure can be utilised in other models outside of the case study."
        },
        {
            "heading": "3.1 Set\u2011Up Model Structure",
            "text": "In the MS Excel model, there are different worksheets that house inputs, intermediate values and outputs, including \u201cParameters\u201d listing the main parameters of the model and \u201cStandard\u201d listing the health states and tracing the cohort across these health states over time for a standard prosthesis (see in Fig.\u00a01 for health states included). Information from different sheets is then combined to produce outputs presented on the analysis sheet. In R, we focus on one script that\nfollows the sections described in Fig.\u00a02 (see \u2018THR_Model.R\u2019 [6]) to define inputs and produce outputs. Numbered sections refer to subheadings within the relevant R script.\nTo set up the model in R, we first need to load packages, which will be used later for plotting data. However, it is good practice to group and load all necessary packages at the top of the script, avoiding potential issues when running parts of your code [13]. We load reshape2 and ggplot2, which are needed to plot the outputs [9, 10].\nThere are two ways in which age and sex influence transitions within this case study, with mean ages of 40, 60 and 80 years for male and female individuals of interest. The first is through background mortality and the second through the impact on the risk of failure of primary THR. External data\nin the form of life tables (\u201clife-table.csv\u201d) and a parametric Weibull regression analysis (\u201chazardfunction.csv\u201d and \u201ccov55. csv\u201d containing coefficient and covariance data, respectively) need to be fed into the model. In MS Excel, these are simply added/viewed as worksheets. It is also good practice to read in data utilised within the R script at the beginning, after packages. By having a dedicated section at the beginning of your script dedicated to external code and data, any errors due to reading in such information are detected early and allow for easy testing of other portions of code reliant on these external factors.\nA benefit of using R, over MS Excel\u2019s RAND-type functions, is that you can \u201cset the seed\u201d easily within your script so that when you draw from a sample (random sampling), R returns the same values every time. This is achieved by entering set.seed(#), where # is set to an integer of your choice (e.g. 100) and corresponds with the draw you will obtain. By setting the seed, you can ensure consistency in result reporting and model checking.\nThe THR model script therefore begins with:\ni. Loading libraries, such as ggplot2 [10]:\nwhere the initial \u201cif\u201d statement checks to see if the user has the package installed before calling it to be used, and if not, installs the package.\nii. Reading in data files, such as the life tables:\nif (!require(ggplot2))install.packages (\ufffd ggplot2 \ufffd)\nlibrary(ggplot2),\nlife.tables <\u2212 read.csv(\u201clife \u2212 table.csv\u201d, header = TRUE)\niii. Setting the seed:\nStructural parameters then need to be defined, through declaring state names ('state.names') and numbers ('n. states'), initial cohort distribution across health states ('seed'), number of cycles (in the case study, 'cycles' are set to 60 and 'cycle.v' being a vector from 1 to 60), discount rates for costs (cDR) and outcomes (oDR), using assignments performed in a previous tutorial [4]. Discount rates can be included in models in various ways. One approach is to define a vector of discount weights that can be easily multiplied by resulting outcome vectors or matrices over time (see Fig.\u00a03a for code and Fig.\u00a03b for output examples). A discount rate of 1.5% for outcomes and 6% for costs are used in the case study."
        },
        {
            "heading": "3.2 Define Deterministic Parameters",
            "text": "Focusing on setting the deterministic model parameters, the shape and scale parameters for those to be used in probabilistic analyses later, means naming and assigning values to variables and health states as seen in Table\u00a01.\nTable\u00a01 shows that only four variables are deterministic; the cost of primary surgery and successful surgery (set to zero as these are the same values across comparators and we are interested in incremental analyses) and the cost of a standard prosthesis (\u00a3394) and a new prosthesis (\u00a3579). The remainder are probabilistic, with distributions presented in Table\u00a01.\nset.seed(1234)"
        },
        {
            "heading": "3.3 Defining the\u00a0Shape and\u00a0Scale Values for\u00a0Probabilistic Parameters",
            "text": "The parameter values in the probabilistic sensitivity analysis (PSA) distributions in Table\u00a01 can be determined by the mean and standard error of the parameters if known. For example, for the cost of revision where the mean cost is \u00a35294, and the standard error is \u00a31487:\nThis process is performed for the other probabilistic values, apart from for the risk of revision and the risk of death. The former first needs to integrate the results of a survival analysis. Once these values are estimated, they are stored in a list ('params'), so that they can be passed to the model function, later on."
        },
        {
            "heading": "3.4 Defining Hazard Coefficient and\u00a0Covariance Values",
            "text": "To utilise the Weibull regression results, we first save the coefficient values (which represents the hazard ratios for each coefficient) within the list of parameter values (\u2018params\u2019). This is stored as a vector within the \u2018params\u2019 list, and includes the constant, age, sex and new prosthesis coefficients. We can also save the covariance matrix associated with this particular survival analysis:\na.cRevision <\u2212(mn.cRevision\u2215se.cRevision)2\nb.cRevision <\u2212 se.cRevision2\nmn.cRevision\nparams$cRevision <\u2212c(shape = a.cRevision, scale = b.cRevision)\nparams$coeff <\u2212hazard.function$coefficient\nparams$cov <\u2212cov\nThis is similar to using the Name functionality within MS Excel (under \u201cFormula\u201d and \u201cDefine Name\u201d) to label parameter values and/or tables within sheets, which can then be utilised in formulae instead of referring to the cell number itself. An advantage of using R for this process over MS Excel is, as it is a script language, users can see which \u2018named values\u2019 come from which source, and in what order they are assigned and used, instead of having various sheets for which the inter-sheet dependency can be opaque, unless thoroughly annotated."
        },
        {
            "heading": "3.5 Sampling",
            "text": "To incorporate heterogeneity in the risks of revision and death due to age and sex, whilst performing sensitivity analyses, a function needs to be created to produce a list (\u2018sample.output\u2019). This list houses samples for each parameter/ parameter group of interest. This is equivalent to sampling every parameter value, for every iteration of the PSA, for all of the parameters included in the PSA, and storing in an Excel Sheet1 so that it is clear which values are being used in each PSA simulation. The function allows users to specify age (as a numeric) and sex (as a dummy variable that is 0 for female individuals and 1 for male individuals), the list of parameters and the number of simulation runs:\npsa.sampling <\u2212function(age = 60,male = 0,\nparams = params, sim.runs = 1000){\u2026}\nFirst, the covariance matrix stored in the section above can undergo a Cholesky decomposition to allow for the generation of correlated variables. For a further description of the theory behind this process, refer to Chapter\u00a04, Briggs et\u00a0al. [12]. Step-by-step calculations for the Cholesky decomposition in R are available in the Electronic Supplementary Material (ESM) to show the workings.2 However, we use the handy \u2018t()\u2019 and \u2018chol() functions available in base R (i.e. no further packages are needed to perform these operations). The transpose function (\u2018t()\u2019) is needed as the Cholesky decomposition function (\u2018chol()\u2019) returns the Upper Triangular Decomposition, whereas for our purposes we want the Lower Triangular Decomposition. We therefore transpose our input, run \u2018chol()\u2019, and transpose the output to produce a matrix named \u2018cholm\u2019:\nThe \u2018cholm\u2019 matrix can then be used in a sampling function to generate random numbers that follow the same covariance as indicated by the survival analysis. First, an empty matrix (\u2018temp.values\u2019) where there are five independent standard normal variates (representing the coefficients for the five survival analysis variables \u2018lngamma, cons, age, male and NP1\u2019) for each simulation is created. For each simulation run, another matrix ('Tz') can be created that is the product of the decomposition matrix (\u2018cholm\u2019) and the generated \u2018temp.values\u2019 matrix. This can then be added to the mean coefficient values (\u2018params$coeff\u2019). The resulting\ncholm <\u2212t ( chol ( t ( params$cov )))\nTable 1 THR parameters (adapted from [11] parameters table)\nTHR total hip replacement\nName Value Standard error Distribution Description\nTransition probability variables \u00a0omrPTHR 0.02 Beta(2, 98) Operative mortality rate following primary THR \u00a0omrRTHR 0.02 Beta(2, 98) Operative mortality rate following revision THR \u00a0Rrr 0.04 Beta(2, 96) Re-revision risk\nResource cost parameters \u00a0cPrimary \u00a30 Cost of a primary THR procedure \u00a0cSuccess \u00a30 Cost of one cycle in a \u2018success\u2019 state (primary or revision) \u00a0cStandard \u00a3394 Cost of standard prosthesis \u00a0cNP1 \u00a3579 Cost of new prosthesis 1 \u00a0cRevision \u00a35294 1487 Gamma(12.67, 417.67) Cost of one cycle in the Revision THR state\nUtility of Markov states per cycle \u00a0uSuccessP 0.85 0.03 Beta(119.57, 21.10) Utility score for having had a successful primary THR \u00a0uSuccessR 0.75 0.04 Beta(87.14, 29.05) Utility score for having a successful revision THR \u00a0uRevision 0.3 0.03 Beta(69.70, 162.63) Utility score during the revision period\n1 The outputted R list (\u2018sample.output\u2019) equivalent to the parameter trial data in the \u201cSimulation\u201d sheet in the corresponding MS Excel model.\n2 The <Hazard function> sheet within the downloadable MS Excel model equivalent utilises individual calculations to perform this process.\nmatrix (\u2018coeff.table\u2019) gives five variables that are correlated in line with the survival analyses results. An exponentiation process to correctly interpret the results of the Weibull model is then performed, as depicted in Fig.\u00a04a. This is based on the following formulae [12], where the time-dependent transition probability per time-step (\u2018tp(t)\u2019) is given by:\nwhere lambda and gamma values are the shape and scale parameters in the Weibull distribution (see Fig.\u00a04b).\nOther probabilistic cost parameters are sampled according to the distributions outlined in Table\u00a01. To incorporate background mortality based on life tables we need two data points: age at that cycle and mortality risk at that specific age. A process on how to do this has been previously described [4], with an additional method of doing this given in the ESM.\ntp(t) = 1 \u2212 exp { [ (t \u2212 1) ] \u2212 t }"
        },
        {
            "heading": "3.6 Total Hip Replacement Model Function",
            "text": "Once we have the samples for relevant parameters/parameter groups, they can be fed through the main model function (model.THR(), see ESM). The main model processes are similar to those previously described [4, 14]). To integrate the heterogeneous risks of revision and mortality within the function, simply update the transitions within the transition arrays (such as \u2018tm.SP0\u2019, which is a transition array for standard procedure where the third dimension represents each cycle) as shown in Fig.\u00a05a, b. In this example, we set age to 60 years and male sex to 0, the code sits within a function and thus the outputs printed here are exemplary of what occurs within the function.\nThe model function returns a vector of values containing the costs and quality-adjusted life-years of the standard and new prosthesis procedures and the incremental costeffectiveness ratio value of the comparison."
        },
        {
            "heading": "3.7 Running the\u00a0Simulations, Estimating NMB and\u00a0Performing Subgroup Analyses Across All Subgroups",
            "text": "Once the parameters have been sampled and the model function defined, it is a case of creating a blank data frame, which has columns representing model outputs, and rows representing the number of simulations (like having a blank sheet where each row gets filled with each simulation result in MS Excel). This can then be filled utilising a for loop, where each value of the sample outputs, created in section (5), is utilised within the model function (see Table\u00a01 of the ESM for an example of the resulting output \u201csimulation.outputs\u201d).\nThe incremental costs and effects for each simulation run can be transformed into net monetary benefit (NMB) using\nwillingness-to-pay thresholds (WTPs), compared to zero (with NMB >0 flagged as cost effective) and the probability of the new procedure being cost effective calculated by averaging over all simulations the number of times the procedure is deemed cost effective. This can all be wrapped into a function (\u2018p.CE\u2019):\np.CE <\u2212function(WTP, simulation.results){ nmb <\u2212simulation.results \ufffd , \ufffd\ufffdinc.qalys\ufffd\ufffd \ufffd \u2217 WTP\u2212simulation.results \ufffd , \ufffd\ufffdinc.cost\ufffd\ufffd \ufffd\nCE\u27e8\u2212nmb\u27e90 probCE <\u2212mean(CE]\nreturn(probCE)}\nwhere the function is run across different WTP values and the related probability of cost effectiveness calculated, both values are then stored in a data frame. The processes outlined above can then be performed on specific age and sex subgroups, and stored within lists and/or arrays. The resulting outputs can be plotted, utilising \u2018ggplot()\u2019 allows graphs to be easily altered to suit users\u2019 needs [10], see the ESM for the code and plots related to graphical outputs."
        },
        {
            "heading": "4 VOI Analysis",
            "text": "In this section, we demonstrate the steps involved in the VOI analyses, with comparisons between R code and MS Excel directly (i.e. the cells within the spreadsheet), or between R code and MS Excel\u2019s Visual Basic for Applications code. There are two key methods for quantifying the VOI for parameter uncertainty: by calculating the EVPI and by calculating the EVPPI (for more information on the theory behind these methods, see Briggs et\u00a0al. [12]). The structure of the VOI R script is shown in Fig.\u00a06."
        },
        {
            "heading": "4.1 Set Up Model Script and\u00a0Source THR Model Functions",
            "text": "The \u2018THR_Model_VOI.R\u2019 script is an extension of the \u2018THR_Model.R script\u2019 [6], using the same sampled data and main model function. A benefit of using R over MS Excel, is the ability to easily and efficiently source data and functions from other scripts within projects, simply by using \u2018source(\"THR_Model.R\")\u2019 at the beginning of the VOI model script."
        },
        {
            "heading": "4.2 Set VOI Population Parameters",
            "text": "When we estimate the VOI, this is often done for one individual in the model. Of course, there will be many people eligible for a particular treatment, each year. The EVPI and EVPPI for an individual can be multiplied by the effective population; the number of eligible patients per annum and the expected lifetime of the technology. For the hip replacement example, we assume an effective technology life of 10 years with 40,000 new patients eligible for treatment each year. A 6% discount rate is used on the effective population, over a 10-year time horizon. The total (discounted) effective population is 312,067 (the sum of the annual discounted population over 10 years, thus e.g., if the discount rate was 0% this would be 400,000)."
        },
        {
            "heading": "4.3 EVPI at\u00a0the\u00a0Population Level",
            "text": "The EVPI represents the value of eliminating the uncertainty in the model parameters, and thereby assuring that with perfect information, the correct decision is made. In contrast, when a decision is made with the current information, there may still be uncertainty around the cost effectiveness in probabilistic analyses, meaning there is a possibility that the wrong decision is made, which would result in a loss of health benefits and/or resources. A full comparison between the R and the MS Excel Visual Basic for Applications codes used for the case study, with annotated comparisons, is available in the ESM, and the full EVPPI R code is available in the ESM. The corresponding MS Excel workbook (\u201cExcelVersion_Ex66bsol.xlsx\u201d) is available to download [6]).\nThe EVPI is estimated using the results of the PSA. The costs and effects of each PSA simulation are converted into NMB, for any given WTP value. Then, the mean NMB is taken for the two treatments, with the highest being the treatment of choice. Across each individual simulation, the highest NMB possible from either treatment is also recorded, which represents the correct decision being made in each simulation. The average of all these values gives the NMB under perfect information, which is then multiplied by the effective population.\nThis can all be wrapped into a function in R (\u2018est.EVPI. pop\u2019), which takes the PSA results (\u2018simulation.results\u2019) and creates a data frame of NMB values (\u2018nmb.table\u2019) for the two treatments, across all simulations. An example of the calculations performed within the EVPI function is shown in Table\u00a02. For example, at a WTP threshold of \u00a32500 per quality-adjusted life-year gained, the highest NMB under current information is \u00a336,117 (for the new prosthesis). In contrast, under perfect information, the NMB is \u00a336,135. This gives an EVPI per person of \u00a318.36, which multiplied by the effective population gives an EVPI of \u00a35,730,624."
        },
        {
            "heading": "4.4 Set Up the\u00a0EVPPI Inner and\u00a0Outer Loop Framework and\u00a0NMB Function",
            "text": "The population EVPI is an upper limit on returns to future research. However, of crucial importance is which particular parameters (or groups of related parameters) are most important in terms of having the greatest VOI. The EVPPI approaches are designed to look at just that. They work by looking at the VOI of the remaining parameters of the model if we assume perfect information for the parameter of interest.\nThe EVPPI analyses often use a nested, double-loop Monte Carlo method, although alternative methods are able to approximate EVPPI using regression modelling and probabilistic analysis results [15, 16]. The double-loop method involves the parameter of interest being sampled in an outer loop, and all other parameters being sampled within the inner loop. For the case study, 100 inner and 100 outer loops are performed. For the EVPPI, each inner loop requires an estimation of the NMB of the two treatments, under the fixed parameter value of interest (i.e. the partial parameter).\nIn R, a similar function (named \u2018nmb.function\u2019) does this, by taking a vector of WTPs, and creates a matrix to estimate NMB, for each inner loop simulation, at each WTP. In contrast, in the MS Excel model, this is performed at just one WTP value, and uses the same structure as used to estimate the average NMB in the EVPI calculations. In both cases, the NMB for each treatment is stored from each inner loop."
        },
        {
            "heading": "4.5 Calculate EVPPI Values for\u00a0WTP Values",
            "text": "In the \u2018evppi.results.SP0\u2019 and \u2018evppi.results.NP1\u2019 data frames in R, each row represents the mean NMB across all inner loop simulations. This is equivalent to cell AS6:AT105 in MS Excel, which is shown in Fig.\u00a07 for comparison to the R code presented in the ESM. Each row represents the mean NMB (the average derived from all inner loop simulations), for each outer loop. In R, this is done across multiple columns, for each WTP. In MS Excel, this is done for just one WTP value.\nOnce all outer loop simulations have been performed, we can calculate the mean NMB across all outer loops simulations (\u2018current.info1\u2019 and \u2018current.info2\u2019 in R, cells AS4:AT4 in MS Excel and Fig.\u00a07). The treatment with the highest NMB is then chosen (\u2018current.info\u2019 in R, in MS Excel, the equivalent is taken as part of the formula in cell AX7).\nNext, using a similar approach taken to calculate EVPI, the highest NMB value across each simulation (i.e. each row) needs to be taken, to consider the decision made with perfect parameter information. In MS Excel, this is simply the higher value of the SP0 and NP1 columns (AS6:AT105, Fig.\u00a0 7). The higher value is selected and shown in AU6:AU105 (Fig.\u00a07). In R, the same calculation is done, but across multiple WTP values. First, an array is created, which in this case is essentially two data frames that contain the NMB from all outer loop simulations, across each WTP value. The apply function is then used to take the maximum NMB for either treatment (i.e. for any given simulation, at each WTP), to create a vector of maximum NMB values (\u2018perf.info.sims\u2019).\nOnce the maximum NMB of either treatment is available for each iteration at each WTP, we can take the mean NMB across all simulations, for each WTP value (\u2018perf.info\u2019 in R, AU4 in MS Excel). Finally, the EVPPI results are the difference between this partially perfect information, and current information, and then multiplied by the effective population. This is shown in AX7 in MS Excel (Fig.\u00a07). In R, this is the data.frame \u2018evppi.results\u2019, which is returned from the \u2018gen. evppi.function\u2019. In this example, the R function returns the EVPPI at each of the WTP values, whereas in MS Excel, the value returned is for only one WTP value.\nAn example of how the EVPPI is calculated is shown in Table\u00a03, for one partial parameter (re-revision risk) and at the \u00a32500 WTP threshold only. Each row represents an outer loop. Once a re-revision risk value has been selected in the outer loop (in the first outer loop, it is 0.0414), then 100 inner loop simulations are performed with all other remaining parameters sampled across each inner loop. The mean NMB across inner loops, for the first outer loop, is estimated for each treatment, and reported. This is \u00a336,287.04 for the standard prosthesis, and \u00a336,305.70 for the new prosthesis.\nEVPI expected value of perfect information, NMB net monetary benefit a NMB for Standard Prosthesis and New Prosthesis is equivalent to X6:X105 and Y6:Y105 in the MS Excel \u201cSimulation\u201d sheet respectively, whilst in the R code, the \u2018apply\u2019 function estimates this (saved as \u2018av.current\u2019). b The NMB for the optimal treatment appears in AK6:AK105 in MS Excel, and \u2018max.nmb\u2019 in R, and the EVPI results are presented as AK1 and \u2018EVPI.indiv\u2019 in MS Excel and R respectively."
        },
        {
            "heading": "5 34536 34603 34603",
            "text": ""
        },
        {
            "heading": "4 35645 35717 35717",
            "text": ""
        },
        {
            "heading": "3 36976 36960 36976",
            "text": ""
        },
        {
            "heading": "2 36626 36617 36626",
            "text": ""
        },
        {
            "heading": "1 37759 37881 37881",
            "text": "The mean for each treatment is then estimated across all outer loops (\u00a336290.53 for the new prosthesis and \u00a336306.65 for the new prosthesis). The EVPPI for the re-revision risk is then estimated, taking the NMB under perfect information (\u00a336306.83), and subtracting the highest NMB for either treatment (\u00a336306.65), to give the difference between perfect parameter information and current information. This gives an EVPPI of \u00a30.18, which multiplied by the effective population gives a population EVPPI of \u00a355,985."
        },
        {
            "heading": "4.6 Run EVPPI Simulations",
            "text": "Now that we have functions to be able to process the results of the inner loops, and the outer loops to estimate the EVPPI, we can create the structure to run the EVPPI analyses. In R, we use three nested loops. The first loop (\u2018j\u2019) selects the\nparameter of interest for the \u2018partial\u2019 perfect information. The number of \u2018j\u2019 loops are set to the number of partial parameters included in the EVPPI analyses. In this instance, we look at six parameter groups. Next, the outer loop (\u2018a\u2019) will select a value for the partial parameter of interest, which will remain fixed within each inner loop. With each new outer loop, the parameter value for the partial parameter will change. The final loop is the inner loop (\u2018b\u2019), in which all other model parameters are sampled (except the \u2018partial\u2019 parameter selected in the outer loop, which remains fixed).\nOnce the inner loop has completed, the mean NMB for each treatment will be estimated using the \u2018nmb.function\u2019 function. A new outer loop will proceed, in which a new parameter value for the partial parameter will be selected, and then the inner loops will be performed. Once all outer loops have been performed, the \u2018gen.evppi.results\u2019 function"
        },
        {
            "heading": "5 0.0171 36312.57 36312.62 36312.62",
            "text": ""
        },
        {
            "heading": "4 0.055 36273.3 36301.98 36301.98",
            "text": ""
        },
        {
            "heading": "3 0.0166 36313.17 36312.78 36313.17",
            "text": ""
        },
        {
            "heading": "2 0.0603 36268.01 36300.55 36300.55",
            "text": ""
        },
        {
            "heading": "1 0.0414 36287.04 36305.70 36305.70",
            "text": "will estimate the EVPPI for this particular partial parameter. Finally, once the EVPPI for a parameter has been completed, a new partial parameter will be selected, and the process repeated, until all EVPPI results, for each partial parameter, have been performed. Note that in MS Excel, the selection of each outer loop partial parameter is performed in Visual Basic for Applications (see ESM).\nThe results of the EVPPI loops can be plotted. Note that in MS Excel, the EVPPI is only calculated at one WTP threshold at a time, and therefore, the values for each parameter are only shown at the particular WTP selected. Using R, we can plot the EVPPI over a range of WTP thresholds, see ESM for the code and plots related to graphical outputs."
        },
        {
            "heading": "5 Discussion and\u00a0Conclusions",
            "text": "Taking the example of THR surgery from previously published MS Excel models, we provide a guide to building a Markov model in R that accounts for heterogeneity across population subgroups. We also provide a demonstration of how VOI analyses can be developed in R, with the R code compared directly to the corresponding Visual Basic for Applications code.\nWe also provide a demonstration of an economic model parameterised using a regression-based survival analysis. This allows the economic model to account for the heterogeneity that may exist in the survival analysis, creating samples based on covariates and covariance estimated through a parametric Weibull regression analysis. By building a flexible model that allows us to select the age and sex within the model function, we demonstrate how subgroup analyses can be easily performed, and integrated with such analyses. Survival analyses could easily precede the economic evaluation, with both being performed in R, allowing both the statistical and economic analyses to be performed using the same software. This avoids issues with reading in outputs externally, but also allows for all analyses and outputs to be updated and rerun simultaneously.\nWhen comparing the time taken to run EVPPI loops, a total of 100 inner and 100 outer loops were performed for each of the six parameter groups included in the EVPPI analysis. This was approximately 11 times quicker in R than MS Excel, even when MS Excel calculated the EVPPI for only one WTP threshold, whereas this is calculated for 501 WTP values in R (\u00a30\u2013\u00a350,000, in increments of \u00a3100). The speed advantage of R when running multiple simulations has already been noted [3]. Furthermore, alternative regressionbased methods are available in R, which can be used to estimate EVPPI without the use of inner and outer loops (i.e. a double loop) [16]. The original MS Excel case study for which this R tutorial is based on does not utilise expected value of sample information techniques and therefore these\nanalyses were not considered here. This is however an important tool in decision analyses, and other R guidance for such VOI analyses are available elsewhere [17, 18].\nOther benefits of R over MS Excel highlighted by this direct comparison tutorial include: the ability to easily source other models and data, and the ability to readily conduct and store EVPPI results over multiple WTP thresholds. Additionally, by example, we highlight the ease of publishing and citing economic evaluation models in R open access through repositories such as Github [6]. Such models can also be adapted into Shinyapps and R packages [19, 20]. Whilst this tutorial utilises mainly base R, we introduce the concept of loading and using packages through the use of reshape and ggplot2 [9, 21]. A compilation of specific health economics packages can be found elsewhere [22].\nIn a world where the coronavirus disease 2019 pandemic and potential subsequent global recessions could lead to smaller healthcare budgets and funding available for research, VOI analyses within the healthcare technology appraisal process can provide a formal framework in quantifying the potential costs and benefits of gathering further information used in relevant health economic models. Additionally, \u2018open source\u2019 health economic modelling will increase general transparency and adaptability in the field [4].\nWe hope that this tutorial paper will help guide MS Excel users who may want, or need, to transfer their model to R. The publicly available code can provide a template for individuals to develop their own models that are able to capture heterogeneity amongst model subgroups, and are able to evaluate the VOI for a particular decision.\nSupplementary Information The online version contains supplementary material available at https:// doi. org/ 10. 1007/ s40273- 022- 01203-0.\nDeclarations\nFunding Financial support for this study was partially provided by an LSHTM seed funding grant. The views expressed are those of the author(s) and are not necessarily those of author-affiliated institutions, including the National Institute for Health Research, the UK Health Security Agency or the Department of Health and Social Care.\nConflicts of interest/Competing interests Nichola R. Naylor, Jack Williams, Nathan Green, Felicity Lamrock and Andrew Briggs have no conflicts of interest that are directly relevant to the content of this article.\nEthics approval Not applicable.\nConsent to participate Not applicable.\nConsent for publication Not applicable.\nAvailability of data and material The data used for this tutorial paper are available at https:// github. com/ Excel-R- tutor ials/ Exten sions_ Paper.\nCode availability The code is available at https:// github. com/ ExcelR- tutor ials/ Exten sions_ Paper. as well as the data.\nAuthors\u2019 contributions JW: conceptualisation, writing original draft, validation, writing review and editing. NN: conceptualisation, writing original draft, validation, writing review and editing. NG: validation, writing review and editing. FL: validation, writing review and editing. AB: resources, writing review and editing.\nOpen Access This article is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License, which permits any non-commercial use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by- nc/4. 0/."
        }
    ],
    "title": "Extensions of Health Economic Evaluations in R for Microsoft Excel Users: A Tutorial for Incorporating Heterogeneity and Conducting Value of Information Analyses",
    "year": 2022
}