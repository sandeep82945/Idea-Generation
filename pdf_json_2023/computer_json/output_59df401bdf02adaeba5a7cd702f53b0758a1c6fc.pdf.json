{
    "abstractText": "Survival prediction is crucial for cancer patients as it provides early prognostic information for treatment planning. Recently, deep survival models based on deep learning and medical images have shown promising performance for survival prediction. However, existing deep survival models are not well developed in utilizing multi-modality images (e.g., PET-CT) and in extracting region-specific information (e.g., the prognostic information in Primary Tumor (PT) and Metastatic Lymph Node (MLN) regions). In view of this, we propose a merging-diverging learning framework for survival prediction from multimodality images. This framework has a merging encoder to fuse multi-modality information and a diverging decoder to extract region-specific information. In the merging encoder, we propose a Hybrid Parallel Cross-Attention (HPCA) block to effectively fuse multi-modality features via parallel convolutional layers and cross-attention transformers. In the diverging decoder, we propose a Region-specific Attention Gate (RAG) block to screen out the features related to lesion regions. Our framework is demonstrated on survival prediction from PET-CT images in Head and Neck (H&N) cancer, by designing an X-shape merging-diverging hybrid transformer network (named XSurv). Our XSurv combines the complementary information in PET and CT images and extracts the region-specific prognostic information in PT and MLN regions. Extensive experiments on the public dataset of HEad and neCK TumOR segmentation and outcome prediction challenge (HECKTOR 2022) demonstrate that our XSurv outperforms state-of-the-art survival prediction methods.",
    "authors": [
        {
            "affiliations": [],
            "name": "Mingyuan Meng"
        },
        {
            "affiliations": [],
            "name": "Lei Bi"
        },
        {
            "affiliations": [],
            "name": "Michael Fulham"
        },
        {
            "affiliations": [],
            "name": "Dagan Feng"
        },
        {
            "affiliations": [],
            "name": "Jinman Kim"
        }
    ],
    "id": "SP:b65b26def241301ab6049ab7095ae6f640b8d1b2",
    "references": [
        {
            "authors": [
                "D.M. Parkin",
                "F. Bray",
                "J. Ferlay",
                "P. Pisani"
            ],
            "title": "Global cancer statistics, 2002",
            "venue": "CA: a cancer journal for clinicians 55(2), 74\u2013108",
            "year": 2005
        },
        {
            "authors": [
                "X. Wang",
                "B.B. Li"
            ],
            "title": "Deep learning in head and neck tumor multiomics diagnosis and analysis: review of the literature",
            "venue": "Frontiers in Genetics 12, 624820",
            "year": 2021
        },
        {
            "authors": [
                "M Bogowicz"
            ],
            "title": "Comparison of PET and CT radiomics for prediction of local tumor control in head and neck squamous cell carcinoma",
            "venue": "Acta oncologica 56(11), 1531-1536",
            "year": 2017
        },
        {
            "authors": [
                "B Gu"
            ],
            "title": "Prediction of 5-year progression-free survival in advanced nasopharyngeal carcinoma with pretreatment PET/CT using multi-modality deep learning-based radiomics",
            "venue": "Frontiers in oncology 12, 899351",
            "year": 2022
        },
        {
            "authors": [
                "V Andrearczyk"
            ],
            "title": "Overview of the HECKTOR Challenge at MICCAI 2020: Automatic Head and Neck Tumor Segmentation in PET/CT",
            "venue": "Andrearczyk, V., et al. (eds.) HECKTOR 2020. LNCS, vol. 12603, pp. 1-21. Springer, Cham",
            "year": 2021
        },
        {
            "authors": [
                "V Andrearczyk"
            ],
            "title": "Overview of the HECKTOR Challenge at MICCAI 2021: Automatic Head and Neck Tumor Segmentation and Outcome Prediction in PET/CT Images",
            "venue": "Andrearczyk, V., et al. (eds.) HECKTOR 2021. LNCS, vol. 13209, pp. 1-37. Springer, Cham",
            "year": 2022
        },
        {
            "authors": [
                "V. Andrearczyk",
                "et al. Overview of the HECKTOR Challenge at MICCAI 2022"
            ],
            "title": "Automatic Head and Neck Tumor Segmentation and Outcome Prediction in PET/CT",
            "venue": "Andrearczyk, V., et al. (eds.) HECKTOR 2022. LNCS, vol. 13626, pp. 1-30. Springer, Cham",
            "year": 2023
        },
        {
            "authors": [
                "R.J. Gillies",
                "P.E. Kinahan",
                "H. Hricak"
            ],
            "title": "Radiomics: Images are more than pictures, they are data",
            "venue": "Radiology 278(2), 563\u2013577",
            "year": 2016
        },
        {
            "authors": [
                "D.R. Cox"
            ],
            "title": "Regression models and life-tables",
            "venue": "Journal of the Royal Statistical Society: Series B (Methodological) 34(2), 187-202",
            "year": 1972
        },
        {
            "authors": [
                "P. Deepa",
                "C. Gunavathi"
            ],
            "title": "A systematic review on machine learning and deep learning techniques in cancer survival prediction",
            "venue": "Progress in Biophysics and Molecular Biology 174, 62-71",
            "year": 2022
        },
        {
            "authors": [
                "N. Saeed",
                "I. Sobirov",
                "R. Al Majzoub",
                "M. Yaqub"
            ],
            "title": "TMSS: An End-to-End TransformerBased Multimodal Network for Segmentation and Survival Prediction",
            "venue": "Wang, L., et al. (eds.) MICCAI 2022. LNCS, vol. 13437, pp. 319-329. Springer, Cham",
            "year": 2022
        },
        {
            "authors": [
                "H Zheng"
            ],
            "title": "Multi-transSP: Multimodal Transformer for Survival Prediction of Nasopharyngeal Carcinoma Patients",
            "venue": "Wang, L., et al. (eds.) MICCAI 2022. LNCS, vol. 13437, pp. 234-243. Springer, Cham",
            "year": 2022
        },
        {
            "authors": [
                "P Afshar"
            ],
            "title": "From handcrafted to deep-learning-based cancer radiomics: challenges and opportunities",
            "venue": "IEEE Signal Processing Magazine 36(4), 132-160",
            "year": 2019
        },
        {
            "authors": [
                "N Saeed"
            ],
            "title": "An ensemble approach for patient prognosis of head and neck tumor using multimodal data",
            "venue": "Andrearczyk, V., et al. (eds.) HECKTOR 2021. LNCS,",
            "year": 2021
        },
        {
            "authors": [
                "Naser",
                "M.A"
            ],
            "title": "Progression free survival prediction for head and neck cancer using deep learning based on clinical and PET/CT imaging data",
            "venue": "Andrearczyk, V., et al. (eds.) HECKTOR 2021. LNCS, vol. 13209, pp. 287-299. Springer, Cham",
            "year": 2022
        },
        {
            "authors": [
                "M. Meng",
                "L. Bi",
                "D. Feng",
                "J. Kim"
            ],
            "title": "Radiomics-enhanced Deep Multi-task Learning for Outcome Prediction in Head and Neck Cancer",
            "venue": "Andrearczyk, V., et al. (eds.) HECKTOR 2022. LNCS, vol. 13626, pp. 135-143. Springer, Cham",
            "year": 2023
        },
        {
            "authors": [
                "A Diamant"
            ],
            "title": "Deep learning in head & neck cancer outcome prediction",
            "venue": "Scientific reports 9, 2764",
            "year": 2019
        },
        {
            "authors": [
                "N Fujima"
            ],
            "title": "Prediction of the local treatment outcome in patients with oropharyngeal squamous cell carcinoma using deep learning analysis of pretreatment FDG-PET images",
            "venue": "BMC Cancer 21, 900",
            "year": 2021
        },
        {
            "authors": [
                "Y Wang"
            ],
            "title": "Deep learning based time-to-event analysis with PET, CT and joint PET/CT for head and neck cancer prognosis",
            "venue": "Computer Methods and Programs in Biomedicine 222, 106948",
            "year": 2022
        },
        {
            "authors": [
                "T Zhou"
            ],
            "title": "M^2Net: Multi-modal Multi-channel Network for Overall Survival Time Prediction of Brain Tumor Patients",
            "venue": "Martel, A.L., et al. (eds.) MICCAI 2020. LNCS, vol. 12263, pp. 221\u2013231. Springer, Cham",
            "year": 2020
        },
        {
            "authors": [
                "D\u2019Souza",
                "N.S"
            ],
            "title": "Fusing Modalities by Multiplexed Graph Neural Networks for Outcome Prediction in Tuberculosis",
            "venue": "Wang, L., et al. (eds.) MICCAI 2022. LNCS, vol. 13437, pp. 287-297. Springer, Cham",
            "year": 2022
        },
        {
            "authors": [
                "W Tang"
            ],
            "title": "MMMNA-Net for Overall Survival Time Prediction of Brain Tumor Patients",
            "venue": "Annual International Conference of the IEEE Engineering in Medicine & Biology Society, pp. 3805-3808",
            "year": 2022
        },
        {
            "authors": [
                "X. Wang",
                "R. Girshick",
                "A. Gupta",
                "K. He"
            ],
            "title": "Non-local neural networks",
            "venue": "IEEE conference on Computer Vision and Pattern Recognition, pp. 7794-7803",
            "year": 2018
        },
        {
            "authors": [
                "M Meng"
            ],
            "title": "DeepMTS: Deep multi-task learning for survival prediction in patients with advanced nasopharyngeal carcinoma using pretreatment PET/CT",
            "venue": "IEEE Journal of Biomedical and Health Informatics 26(9), 4497-4507",
            "year": 2022
        },
        {
            "authors": [
                "M. Meng",
                "Y. Peng",
                "L. Bi",
                "J. Kim"
            ],
            "title": "Multi-task deep learning for joint tumor segmentation and outcome prediction in head and neck cancer",
            "venue": "Andrearczyk, V., et al. (eds.) HECKTOR 2021. LNCS, vol. 13209, pp. 160-167. Springer, Cham",
            "year": 2022
        },
        {
            "authors": [
                "V Andrearczyk"
            ],
            "title": "Multi-task deep segmentation and radiomics for automatic prognosis in head and neck cancer",
            "venue": "Rekik, I., et al. (eds.) PRIME 2021. LNCS, vol. 12928, pp. 147-156. Springer, Cham",
            "year": 2022
        },
        {
            "authors": [
                "Z Liu"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "IEEE/CVF International Conference on Computer Vision, pp. 10012-10022",
            "year": 2021
        },
        {
            "authors": [
                "W Liu"
            ],
            "title": "PHTrans: Parallelly aggregating global and local representations for medical image segmentation",
            "venue": "Wang, L., et al. (eds.) MICCAI 2022. LNCS, vol. 13435, pp. 235-244. Springer, Cham",
            "year": 2022
        },
        {
            "authors": [
                "J Schlemper"
            ],
            "title": "Attention gated networks: Learning to leverage salient regions in medical images",
            "venue": "Medical image analysis 53, 197-207",
            "year": 2019
        },
        {
            "authors": [
                "M.F. Gensheimer",
                "B. Narasimhan"
            ],
            "title": "A scalable discrete-time survival model for neural networks",
            "venue": "PeerJ 7, e6257",
            "year": 2019
        },
        {
            "authors": [
                "F Milletari"
            ],
            "title": "V-Net: fully convolutional neural networks for volumetric medical image segmentation",
            "venue": "International Conference on 3D Vision, pp. 565-571",
            "year": 2016
        },
        {
            "authors": [
                "Lin",
                "T.Y"
            ],
            "title": "Focal loss for dense object detection",
            "venue": "IEEE International Conference on Computer Vision, pp. 2980-2988",
            "year": 2017
        },
        {
            "authors": [
                "Van Griethuysen",
                "J.J"
            ],
            "title": "Computational radiomics system to decode the radiographic phenotype",
            "venue": "Cancer research 77(21), e104-e107",
            "year": 2017
        },
        {
            "authors": [
                "L Rebaud"
            ],
            "title": "Simplicity is All You Need: Out-of-the-Box nnUNet followed by Binary-Weighted Radiomic Model for Segmentation and Outcome Prediction in Head and Neck PET/CT",
            "venue": "Andrearczyk, V., et al. (eds.) HECKTOR 2022. LNCS, vol. 13626, pp. 121-134. Springer, Cham",
            "year": 2023
        },
        {
            "authors": [
                "M Eisenmann"
            ],
            "title": "Biomedical image analysis competitions: The state of current participation practice",
            "venue": "arXiv preprint, arXiv:2212.08568",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Keywords: Survival Prediction, Transformer, Head and Neck Cancer."
        },
        {
            "heading": "1 Introduction",
            "text": "Head and Neck (H&N) cancer refers to malignant tumors in H&N regions, which is among the most common cancers worldwide [1]. Survival prediction, a regression task that models the survival outcomes of patients, is crucial for H&N cancer patients: it provides early prognostic information to guide treatment planning and potentially improves the overall survival outcomes of patients [2]. Multi-modality imaging of\n2\nPositron Emission Tomography \u2013 Computed Tomography (PET-CT) has been shown to benefit survival prediction as it offers both anatomical (CT) and metabolic (PET) information about tumors [3, 4]. Therefore, survival prediction from PET-CT images in H&N cancer has attracted wide attention and serves as a key research area. For instance, HEad and neCK TumOR segmentation and outcome prediction challenges (HECKTOR) have been held for the last three years to facilitate the development of new algorithms for survival prediction from PET-CT images in H&N cancer [5-7].\nTraditional survival prediction methods are usually based on radiomics [8], where handcrafted radiomics features are extracted from pre-segmented tumor regions and then are modeled by statistical survival models, such as the Cox Proportional Hazard (CoxPH) model [9]. In addition, deep survival models based on deep learning have been proposed to perform end-to-end survival prediction from medical images, where pre-segmented tumor masks are often unrequired [10]. Deep survival models usually adopt Convolutional Neural Networks (CNNs) to extract image features, and recently Visual Transformers (ViT) have been adopted for its capabilities to capture longrange dependency within images [11, 12]. These deep survival models have shown the potential to outperform traditional survival prediction methods [13]. For survival prediction in H&N cancer, deep survival models have achieved top performance in the HECKTOR 2021/2022 and are regarded as state-of-the-art [14-16]. Nevertheless, we identified that existing deep survival models still have two main limitations.\nFirstly, existing deep survival models are underdeveloped in utilizing complementary multi-modality information, such as the metabolic and anatomical information in PET and CT images. For survival prediction in H&N cancer, existing methods usually use single imaging modality [17, 18] or rely on early fusion (i.e., concatenating multimodality images as multi-channel inputs) to combine multi-modality information [11, 14-16, 19]. In addition, late fusion has been used for survival prediction in other diseases such as gliomas and tuberculosis [20, 21], where multi-modality features were extracted by multiple independent encoders with resultant features fused. However, early fusion has difficulties in extracting intra-modality information due to entangled (concatenated) images for feature extraction, while late fusion has difficulties in extracting inter-modality information due to fully independent feature extraction. Recently, Tang et al. [22] attempted to address this limitation by proposing a Multi-scale Non-local Attention Fusion (MNAF) block for survival prediction of glioma patients, in which multi-modality features were fused via non-local attention mechanism [23] at multiple scales. However, the performance of this method heavily relies on using tumor segmentation masks as inputs, which limits its generalizability. Secondly, although deep survival models have advantages in performing end-toend survival prediction without requiring tumor masks, this also incurs difficulties in extracting region-specific information, such as the prognostic information in Primary Tumor (PT) and Metastatic Lymph Node (MLN) regions. To address this limitation, recent deep survival models adopted multi-task learning for joint tumor segmentation and survival prediction, to implicitly guide the model to extract features related to tumor regions [11, 16, 24-26]. However, most of them only considered PT segmentation and ignored the prognostic information in MLN regions [11, 24-26]. Meng et al. [16] performed survival prediction with joint PT-MLN segmentation and achieved\n3\none of the top performances in HECKTOR 2022. However, this method extracted entangled features related to both PT and MLN regions, which incurs difficulties in discovering the prognostic information in PT-/MLN-only regions. In this study, we design an X-shape merging-diverging hybrid transformer network (named XSurv, Fig. 1) for survival prediction in H&N cancer. Our XSurv has a merging encoder to fuse complementary anatomical and metabolic information in PET and CT images and has a diverging decoder to extract region-specific prognostic information in PT and MLN regions. Our technical contributions in XSurv are three folds: (i) We propose a merging-diverging learning framework for survival prediction. This framework is specialized in leveraging multi-modality images and extracting regionspecific information, which potentially could be applied to many survival prediction tasks with multi-modality imaging. (ii) We propose a Hybrid Parallel Cross-Attention (HPCA) block for multi-modality feature learning, where both local intra-modality and global inter-modality features are learned via parallel convolutional layers and cross-attention transformers. (iii) We propose a Region-specific Attention Gate (RAG) block for region-specific feature extraction, which screens out the features related to lesion regions. Extensive experiments on the public dataset of HECKTOR 2022 [7] demonstrate that our XSurv outperforms state-of-the-art survival prediction methods, including the top-performing methods in HECKTOR 2022."
        },
        {
            "heading": "2 Method",
            "text": "Fig. 1 illustrates the overall architecture of our XSurv, which presents an X-shape architecture consisting of a merging encoder for multi-modality feature learning and a diverging decoder for region-specific feature extraction. The encoder includes two PET-/CT-specific feature learning branches with HPCA blocks (refer to Section 2.1), while the decoder includes two PT-/MLN-specific feature extraction branches with RAG blocks (refer to Section 2.2). Our XSurv performs joint survival prediction and segmentation, where the two decoder branches are trained to perform PT/MLN seg-\n4\nmentation and provide PT-/MLN-related deep features for survival prediction (refer to Section 2.3). Our XSurv also can be enhanced by leveraging the radiomics features extracted from the XSurv-segmented PT/MLN regions (refer to Section 2.4). Our implementation is provided at https://github.com/MungoMeng/Survival-XSurv."
        },
        {
            "heading": "2.1 PET-CT Merging Encoder",
            "text": "Assuming !!\"#$ , !%&'( , and !!)\"%% are three architecture parameters, each encoder branch consists of !!\"#$ Conv blocks, !%&'( Hybrid Parallel Self-Attention (HPSA) blocks, and !!)\"%% HPCA blocks. Max pooling is applied between blocks and the features before max pooling are propagated to the decoder through skip connections. As shown in Fig. 2(a), HPCA blocks perform parallel convolution and cross-attention operations. The convolution operations are realized using successive convolutional layers with residual connections, while the cross-attention operations are realized using Swin Transformer [27] where the input \"*# (from the same encoder branch) is projected as # and the input \"!)\"%% (from the other encoder branch) is projected as $ and %. In addition, Conv blocks perform the same convolution operations as HPCA blocks but discard cross-attention operations; HPSA blocks share the same overall architecture with HPCA blocks but perform self-attention within the input \"*# (i.e., the \"*# is projected as #, $ and %). Conv and HPSA blocks are used first and then followed by HPCA blocks, which enables the XSurv to learn both intra- and intermodality information. In this study, we set !!\"#$, !%&'(, and !!)\"%% as 1, 1, and 3, as this setting achieved the best validation results (refer to the supplementary materials). Other architecture details are also presented in the supplementary materials.\nThe idea of adopting convolutions and transformers in parallel has been explored for segmentation [28], which suggests that parallelly aggregating global and local information is beneficial for feature learning. In this study, we extend this idea to multi-modality feature learning, which parallelly aggregates global inter-modality and local intra-modality information via HPCA blocks, to discover inter-modality interactions while preserving intra-modality characteristics.\n5"
        },
        {
            "heading": "2.2 PT-MLN Diverging Decoder",
            "text": "As shown in Fig. 1, each decoder branch is symmetric to the encoder branch and thus includes a total of (!!\"#$+!%&'(+!!)\"%%) Conv blocks. The features propagated from skip connections are fed into RAG blocks for feature diverging before entering the Conv blocks in two decoder branches, where the output of the former Conv block is upsampled and concatenated with the output of the RAG block. As shown in Fig. 2(b), RAG blocks generate three softmax-activated spatial attention maps &+,, &-./, and &0 that correspond to PT, MLN, and background regions. These attention maps are computed based on the contextual information provided by the gating signals '+, and '-./ (which are the outputs of the former Conv blocks in the PT and MLN branches). The attention maps &+, and &-./ are multiplied with the features \"%1*2 that are propagated from skip connections, which spatially diverge the features \"%1*2 into PT- and MLN-related features \"+, and \"-./. Different from the vanilla Attention Gate (AG) block [29], RAG blocks leverage the gating signals from two decoder branches and generate mutually exclusive (softmax-activated) attention maps.\nThe output of the last Conv block in the PT/MLN branch is fed into a segmentation head, which generates PT/MLN segmentation masks using a sigmoid-activated 1\u00d71\u00d71 convolutional layer. In addition, the outputs of all but not the first Conv blocks in the PT/MLN branches are fed into global averaging pooling layers to derive PT-/MLNrelated deep features. Finally, all deep features are fed into a survival prediction head, which maps the deep features into a survival score using two fully-connected layers with dropout, L2 regularization, and sigmoid activation."
        },
        {
            "heading": "2.3 Multi-task Learning",
            "text": "Following existing multi-task deep survival models [11, 16, 24-26], our XSurv is endto-end trained for survival prediction and PT-MLN segmentation using a combined loss: \u2112 = \u211234)$ + +(\u2112+, + \u2112-./), where the + is a parameter to balance the survival prediction term \u211234)$ and the PT/MLN segmentation terms \u2112+,/-./. We follow [15] to adopt a negative log-likelihood loss [30] as the \u211234)$. For the \u2112+,/-./, we adopt the sum of Dice [31] and Focal [32] losses. The loss functions are detailed in the supplementary materials. The + is set as 1 in the experiments as default."
        },
        {
            "heading": "2.4 Radiomics Enhancement",
            "text": "Our XSurv also can be enhanced by leveraging radiomics features (denoted as RadioXSurv). Following [16], radiomics features are extracted from the XSurv-segmented PT/MLN regions via Pyradiomics [33] and selected by Least Absolute Shrinkage and Selection Operator (LASSO) regression. The process of radiomics feature extraction is provided in the supplementary materials. Then, a CoxPH model [9] is adopted to integrate the selected radiomics features and the XSurv-predicted survival score to make the final prediction. In addition, clinical indicators (e.g., age, gender) also can be integrated by the CoxPH model.\n6"
        },
        {
            "heading": "3 Experimental Setup",
            "text": ""
        },
        {
            "heading": "3.1 Dataset and Preprocessing",
            "text": "We adopted the training dataset of HECKTOR 2022 (refer to https://hecktor.grandchallenge.org/), including 488 H&N cancer patients acquired from seven medical centers [7], while the testing dataset was excluded as its ground-truth labels are not released. Each patient underwent pretreatment PET/CT and has clinical indicators. We present the distributions of all clinical indicators in the supplementary materials. Recurrence-Free Survival (RFS), including time-to-event in days and censored-or-not status, was provided as ground truth for survival prediction, while PT and MLN annotations were provided for segmentation. The patients from two centers (CHUM and CHUV) were used for testing and other patients for training, which split the data into 386/102 patients in training/testing sets. We trained and validated models using 5-fold cross-validation within the training set and evaluated them in the testing set.\nWe resampled PET-CT images into isotropic voxels where 1 voxel corresponds to 1 mm3. Each image was cropped to 160\u00d7160\u00d7160 voxels with the tumor located in the center. PET images were standardized using Z-score normalization, while CT images were clipped to [\u22121024, 1024] and then mapped to [\u22121, 1]. In addition, we performed univariate and multivariate Cox analyses on the clinical indicators to screen out the prognostic indicators with significant relevance to RFS (P<0.05)."
        },
        {
            "heading": "3.2 Implementation Details",
            "text": "We implemented our XSurv using PyTorch on a 12GB GeForce GTX Titan X GPU. Our XSurv was trained for 12,000 iterations using an Adam optimizer with a batch size of 2. Each training batch included the same number of censored and uncensored samples. The learning rate was set as 1e-4 initially and then reset to 5e\u22125 and 1e\u22125 at the 4,000th and 8,000th training iteration. Data augmentation was applied in real-time during training to minimize overfitting, including random affine transformations and random cropping to 112\u00d7112\u00d7112 voxels. Validation was performed after every 200 training iterations and the model achieving the highest validation result was preserved. In our experiments, one training iteration (including data augmentation) took roughly 4.2 second, and one inference iteration took roughly 0.61 second."
        },
        {
            "heading": "3.3 Experimental Settings",
            "text": "We compared our XSurv to six state-of-the-art survival prediction methods, including two traditional radiomics-based methods and four deep survival models. The included traditional methods are CoxPH [9] and Individual Coefficient Approximation for Risk Estimation (ICARE) [34]. For traditional methods, radiomics features were extracted from the provided ground-truth tumor regions and selected by LASSO regression. The included deep survival models are Deep Multi-Task Logistic Regression and CoxPH ensemble (DeepMTLR-CoxPH) [14], Transformer-based Multimodal net-\n7\nworks for Segmentation and Survival prediction (TMSS) [11], Deep Multi-task Survival model (DeepMTS) [24], and Radiomics-enhanced DeepMTS (Radio-DeepMTS) [16]. DeepMTLR-CoxPH, ICARE, and Radio-DeepMTS achieved top performance in HECKTOR 2021 and 2022. For a fair comparison, all methods took the same preprocessed images and clinical indicators as inputs. Survival prediction and segmentation were evaluated using Concordance index (C-index) and Dice Similarity Coefficient (DSC), which are the standard evaluation metrics in the challenges [6, 7, 35]. We also performed two ablation studies on the encoder and decoder separately: (i) We replaced HPCA/HPSA blocks with Conv blocks and compared different strategies to combine PET-CT images. (ii) We removed RAG blocks and compared different strategies to extract PT/MLN-related information."
        },
        {
            "heading": "4 Results and Discussion",
            "text": "The comparison between our XSurv and the state-of-the-art methods is presented in Table 1. Our XSurv achieved a higher C-index than all compared methods, which demonstrates that our XSurv has achieved state-of-the-art performance in survival prediction of H&N cancer. When radiomics enhancement was adopted in XSurv and DeepMTS, our Radio-XSurv also outperformed the Radio-DeepMTS and achieved the highest C-index. Moreover, the segmentation results of multi-task deep survival models (TMSS, DeepMTS, and XSurv) are also reported in Table 1. Our XSurv achieved higher DSCs than TMSS and DeepMTS, which demonstrates that our XSurv can locate PT and MLN more precisely and this infers that our XSurv has better learning capability. We attribute these performance improvements to the use of our proposed merging-diverging learning framework, HPCA block, and RAG block, which can be evidenced by ablation studies.\nThe ablation study on the PET-CT merging encoder is shown in Table 2. We found that using PET alone resulted in a higher C-index than using both PET-CT with early or late fusion. This finding is consistent with Wang et al. [19]\u2019s study, which suggests that early and late fusion cannot effectively leverage the complementary information in PET-CT images. As we have mentioned, early and late fusion have difficulties in extracting intra- and inter-modality information, respectively. Our encoder first adopts\n8\nConv/HPSA blocks to extract intra-modality information and then leverages HPCA blocks to discover their interactions, which achieved the highest C-index. For PT and MLN segmentation, our encoder also achieved the highest DSCs, which indicates that our encoder also can improve segmentation. In addition, MNAF blocks [22] were compared and showed poor performance. This is likely attributed to the fact that leveraging non-local attention at multiple scales has corrupted local spatial information, which degraded the segmentation performance and distracted the model from PT and MLN regions. To relieve this problem, in Tang et al.\u2019s study [22], tumor segmentation masks were fed into the model as explicit guidance to tumor regions. However, it is intractable to have segmentation masks at the inference stage in clinical practice.\nThe ablation study on the PT-MLN diverging decoder is shown in Table 3. We found that, even without adopting AG, using a dual-branch decoder for PT and MLN segmentation resulted in a higher C-index than using a single-branch decoder, which demonstrates the effectiveness of our diverging decoder design. Adopting vanilla AG [29] or RAG in the dual-branch decoder further improved survival prediction. Compared to the vanilla AG, our RAG contributed to a larger improvement, and this enabled our decoder to achieve the highest C-index. In the supplementary materials, we visualized the attention maps produced by RAG blocks, where the attention maps can precisely locate PT/MLN regions and screen out PT-/MLN-related features. For PT and MLN segmentation, using a single-branch decoder for PT- or MLN-only segmentation achieved the highest DSCs. This is expected as the model can leverage all its capabilities to segment only one target. Nevertheless, our decoder still achieved the second-best DSCs in both PT and MLN segmentation with a small gap.\n9"
        },
        {
            "heading": "5 Conclusion",
            "text": "We have outlined an X-shape merging-diverging hybrid transformer network (XSurv) for survival prediction from PET-CT images in H&N cancer. Within the XSurv, we propose a merging-diverging learning framework, a Hybrid Parallel Cross-Attention (HPCA) block, and a Region-specific Attention Gate (RAG) block, to learn complementary information from multi-modality images and extract region-specific prognostic information for survival prediction. Extensive experiments have shown that the proposed framework and blocks enable our XSurv to outperform state-of-the-art survival prediction methods on the well-benchmarked HECKTOR 2022 dataset.\nAcknowledgement. This work was supported by Australian Research Council (ARC) under Grant DP200103748."
        }
    ],
    "title": "Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer",
    "year": 2023
}