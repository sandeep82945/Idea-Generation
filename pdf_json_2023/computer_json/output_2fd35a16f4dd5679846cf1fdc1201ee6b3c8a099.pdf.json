{
    "abstractText": "Leonardo Banchi, 2, \u2217 Jason Luke Pereira, Sharu Theresa Jose, and Osvaldo Simeone Department of Physics and Astronomy, University of Florence, via G. Sansone 1, I-50019 Sesto Fiorentino (FI), Italy INFN Sezione di Firenze, via G. Sansone 1, I-50019, Sesto Fiorentino (FI), Italy Department of Computer Science, University of Birmingham Birmingham, B15 2TT, UK King\u2019s Learning and Information Processing lab (KCLIP) & Centre for Intelligent Information Processing Systems (CIIPS)& Department of Engineering, King\u2019s College London, Strand, London, WC2R 2LS, UK",
    "authors": [
        {
            "affiliations": [],
            "name": "Leonardo Banchi"
        },
        {
            "affiliations": [],
            "name": "Jason Luke Pereira"
        },
        {
            "affiliations": [],
            "name": "Sharu Theresa Jose"
        },
        {
            "affiliations": [],
            "name": "Osvaldo Simeone"
        }
    ],
    "id": "SP:d7acecb12a76025d6b02f801fa10d1ebabcaeca4",
    "references": [
        {
            "authors": [
                "J. Biamonte",
                "P. Wittek",
                "N. Pancotti",
                "P. Rebentrost",
                "N. Wiebe",
                "S. Lloyd"
            ],
            "title": "Quantum machine learning",
            "venue": "Nature 549, 195 ",
            "year": 2017
        },
        {
            "authors": [
                "V. Gebhart",
                "R. Santagati",
                "A.A. Gentile",
                "E.M. Gauger",
                "D. Craig",
                "N. Ares",
                "L. Banchi",
                "F. Marquardt",
                "L. Pezz\u00e8",
                "C. Bonato"
            ],
            "title": "Learning quantum systems",
            "venue": "Nature Reviews Physics 5, 141 ",
            "year": 2023
        },
        {
            "authors": [
                "O. Simeone"
            ],
            "title": "An introduction to quantum machine learning for engineers, Foundations and Trends\u00ae",
            "venue": "in Signal Processing 16,",
            "year": 2022
        },
        {
            "authors": [
                "C. Ciliberto",
                "M. Herbster",
                "A.D. Ialongo",
                "M. Pontil",
                "A. Rocchetto",
                "S. Severini",
                "L. Wossnig"
            ],
            "title": "Quantum machine learning: a classical perspective",
            "venue": "Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences 474, 20170551 ",
            "year": 2018
        },
        {
            "authors": [
                "V. Dunjko",
                "H.J. Briegel"
            ],
            "title": "Machine learning & artif",
            "venue": "intell. in the quantum domain: a review of recent progress, Rep. Prog. Phys. 81, 074001 ",
            "year": 2018
        },
        {
            "authors": [
                "S. Shalev-Shwartz",
                "S. Ben-David"
            ],
            "title": "Understanding machine learning: From theory to algorithms (Cambridge university",
            "year": 2014
        },
        {
            "authors": [
                "L. Banchi",
                "J. Pereira",
                "S. Pirandola"
            ],
            "title": "Generalization in quantum machine learning: A quantum information standpoint",
            "venue": "PRX Quantum 2, 040321 ",
            "year": 2021
        },
        {
            "authors": [
                "S. Arunachalam",
                "R. de Wolf"
            ],
            "title": "Guest column: A survey of quantum learning theory",
            "venue": "ACM Sigact News 48,",
            "year": 2017
        },
        {
            "authors": [
                "H.-Y. Huang",
                "M. Broughton",
                "M. Mohseni",
                "R. Babbush",
                "S. Boixo",
                "H. Neven",
                "J.R. McClean"
            ],
            "title": "Power of data in quantum machine learning",
            "venue": "Nature Communications 12, 2631 ",
            "year": 2021
        },
        {
            "authors": [
                "H.-Y. Huang",
                "R. Kueng",
                "J. Preskill"
            ],
            "title": "Information-theoretic bounds on quantum advantage in machine learning",
            "venue": "Physical Review Letters 126, 190505 ",
            "year": 2021
        },
        {
            "authors": [
                "M.C. Caro",
                "E. Gil-Fuster",
                "J.J. Meyer",
                "J. Eisert",
                "R. Sweke"
            ],
            "title": "Encoding-dependent generalization bounds for parametrized quantum circuits",
            "venue": "Quantum 5, 582 ",
            "year": 2021
        },
        {
            "authors": [
                "M.C. Caro",
                "H.-Y. Huang",
                "M. Cerezo",
                "K. Sharma",
                "A. Sornborger",
                "L. Cincio",
                "P.J. Coles"
            ],
            "title": "Generalization in quantum machine learning from few training data",
            "venue": "Nature communications 13, 4919 ",
            "year": 2022
        },
        {
            "authors": [
                "Y. Du",
                "Y. Yang",
                "D. Tao",
                "M.-H. Hsieh"
            ],
            "title": "Demystify problem-dependent power of quantum neural networks on multi-class classification",
            "venue": "arXiv preprint arXiv:2301.01597 ",
            "year": 2022
        },
        {
            "authors": [
                "A. Abbas",
                "D. Sutter",
                "C. Zoufal",
                "A. Lucchi",
                "A. Figalli",
                "S. Woerner"
            ],
            "title": "The power of quantum neural networks",
            "venue": "Nature Computational Science 1, 403 ",
            "year": 2021
        },
        {
            "authors": [
                "T. Haug",
                "K. Bharti",
                "M. Kim"
            ],
            "title": "Capacity and quantum geometry of parametrized quantum circuits",
            "venue": "PRX Quantum 2, 040309 ",
            "year": 2021
        },
        {
            "authors": [
                "C. Gyurik"
            ],
            "title": "V",
            "venue": "Dunjko, et al., Structural risk minimization for quantum linear classifiers, Quantum 7, 893 ",
            "year": 2023
        },
        {
            "authors": [
                "H.-Y. Huang",
                "M. Broughton",
                "J. Cotler",
                "S. Chen",
                "J. Li",
                "M. Mohseni",
                "H. Neven",
                "R. Babbush",
                "R. Kueng"
            ],
            "title": "J",
            "venue": "Preskill, et al., Quantum advantage in learning from experiments, Science 376, 1182 ",
            "year": 2022
        },
        {
            "authors": [
                "L. Banchi",
                "Q. Zhuang",
                "S. Pirandola"
            ],
            "title": "Quantum-enhanced barcode decoding and pattern recognition",
            "venue": "Physical Review Applied 14, 064026 ",
            "year": 2020
        },
        {
            "authors": [
                "G. Carleo",
                "I. Cirac",
                "K. Cranmer",
                "L. Daudet",
                "M. Schuld",
                "N. Tishby",
                "L. Vogt-Maranto",
                "L. Zdeborov\u00e1"
            ],
            "title": "Machine learning and the physical sciences",
            "venue": "Reviews of Modern Physics 91, 045002 ",
            "year": 2019
        },
        {
            "authors": [
                "M. Benedetti",
                "D. Garcia-Pintos",
                "O. Perdomo",
                "V. Leyton-Ortega",
                "Y. Nam",
                "A. Perdomo-Ortiz"
            ],
            "title": "A generative modeling approach for benchmarking and training shallow quantum circuits",
            "venue": "npj Quantum Information 5, 45 ",
            "year": 2019
        },
        {
            "authors": [
                "H. Situ",
                "Z. He",
                "Y. Wang",
                "L. Li",
                "S. Zheng"
            ],
            "title": "Information Sciences 538",
            "venue": "193 ",
            "year": 2020
        },
        {
            "authors": [
                "J. Romero",
                "A. Aspuru-Guzik"
            ],
            "title": "Variational quantum generators: Generative adversarial quantum machine learning for continuous distributions",
            "venue": "Advanced Quantum Technologies 4, 2000003 ",
            "year": 2021
        },
        {
            "authors": [
                "C.D. Scott",
                "R.D. Nowak"
            ],
            "title": "Learning minimum volume sets",
            "venue": "Journal of Machine Learning Research 7, 665 ",
            "year": 2006
        },
        {
            "authors": [
                "N. Ezzell",
                "E.M. Ball",
                "A.U. Siddiqui",
                "M.M. Wilde",
                "A.T. Sornborger",
                "P.J. Coles",
                "Z. Holmes"
            ],
            "title": "Quantum mixed state compiling",
            "venue": "Quantum Science and Technology 8, 035001 ",
            "year": 2023
        },
        {
            "authors": [
                "P. Braccia",
                "L. Banchi",
                "F. Caruso"
            ],
            "title": "Quantum noise sensing by generating fake noise",
            "venue": "Physical Review Applied 17, 024002 ",
            "year": 2022
        },
        {
            "authors": [
                "M. Benedetti",
                "E. Grant",
                "L. Wossnig",
                "S. Severini"
            ],
            "title": "Adversarial quantum circuit learning for pure state approximation",
            "venue": "New Journal of Physics 21, 043023 ",
            "year": 2019
        },
        {
            "authors": [
                "P. Braccia",
                "F. Caruso",
                "L. Banchi"
            ],
            "title": "How to enhance quantum generative adversarial learning of noisy information",
            "venue": "New Journal of Physics 23, 053024 ",
            "year": 2021
        },
        {
            "authors": [
                "L. Maccone"
            ],
            "title": "Entropic information-disturbance tradeoff",
            "venue": "Europhysics Letters 77, 40002 ",
            "year": 2007
        },
        {
            "authors": [
                "V. Vovk",
                "A. Gammerman"
            ],
            "title": "and G",
            "venue": "Shafer, Algorithmic learning in a random world ",
            "year": 2023
        },
        {
            "authors": [
                "S. Park",
                "O. Simeone"
            ],
            "title": "Quantum conformal prediction for reliable uncertainty quantification in quantum machine learning",
            "venue": "arXiv preprint arXiv:2304.03398 ",
            "year": 2023
        },
        {
            "authors": [
                "K. Fujii",
                "K. Nakajima"
            ],
            "title": "Harnessing disordered-ensemble quantum dynamics for machine learning",
            "venue": "Physical Review Applied 8, 024030 ",
            "year": 2017
        },
        {
            "authors": [
                "P. Mujal"
            ],
            "title": "R",
            "venue": "Mart\u0301\u0131nez-Pe\u00f1a, J. Nokkala, J. Gar\u0107\u0131a-Beni, G. L. Giorgi, M. C. Soriano, and R. Zambrini, Opportunities in quantum reservoir computing and extreme learning machines, Advanced Quantum Technologies 4, 2100027 ",
            "year": 2021
        },
        {
            "authors": [
                "H.-Y. Huang",
                "R. Kueng",
                "J. Preskill"
            ],
            "title": "Predicting many properties of a quantum system from very few measurements",
            "venue": "Nature Physics 16, 1050 ",
            "year": 2020
        },
        {
            "authors": [
                "S. Lloyd",
                "M. Schuld",
                "A. Ijaz",
                "J. Izaac",
                "N. Killoran"
            ],
            "title": "Quantum embeddings for machine learning",
            "venue": "arXiv preprint arXiv:2001.03622 ",
            "year": 2020
        },
        {
            "authors": [
                "S. Kimmel",
                "C.Y.-Y. Lin",
                "G.H. Low",
                "M. Ozols",
                "T.J. Yoder"
            ],
            "title": "Hamiltonian simulation with optimal sample complexity",
            "venue": "npj Quantum Information 3, 13 ",
            "year": 2017
        },
        {
            "authors": [
                "M. Schuld"
            ],
            "title": "Supervised quantum machine learning models are kernel methods",
            "venue": "arXiv preprint arXiv:2101.11020 ",
            "year": 2021
        },
        {
            "authors": [
                "M.A. Nielsen",
                "I.L. Chuang"
            ],
            "title": "Quantum Computation and Quantum Information",
            "year": 2000
        },
        {
            "authors": [
                "R. Konig",
                "R. Renner",
                "C. Schaffner"
            ],
            "title": "The operational meaning of min-and max-entropy",
            "venue": "IEEE Transactions on Information theory 55, 4337 ",
            "year": 2009
        },
        {
            "authors": [
                "P.L. Bartlett",
                "A. Montanari",
                "A. Rakhlin"
            ],
            "title": "Deep learning: a statistical viewpoint",
            "venue": "Acta numerica 30, 87 ",
            "year": 2021
        },
        {
            "authors": [
                "M. Ragone",
                "P. Braccia",
                "Q.T. Nguyen",
                "L. Schatzki",
                "P.J. Coles",
                "F. Sauvage",
                "M. Larocca",
                "M. Cerezo"
            ],
            "title": "Representation theory for geometric quantum machine learning",
            "venue": "arXiv preprint arXiv:2210.07980 ",
            "year": 2022
        },
        {
            "authors": [
                "Y. Du",
                "Z. Tu",
                "X. Yuan",
                "D. Tao"
            ],
            "title": "Efficient measure for the expressivity of variational quantum algorithms",
            "venue": "Physical Review Letters 128, 080506 ",
            "year": 2022
        },
        {
            "authors": [
                "M. Schuld",
                "R. Sweke",
                "J.J. Meyer"
            ],
            "title": "Effect of data encoding on the expressive power of variational quantum-machinelearning models",
            "venue": "Physical Review A 103, 032430 ",
            "year": 2021
        },
        {
            "authors": [
                "A. Canatar",
                "E. Peters",
                "C. Pehlevan",
                "S.M. Wild",
                "R. Shaydulin"
            ],
            "title": "Bandwidth enables generalization in quantum kernel models",
            "venue": "arXiv preprint arXiv:2206.06686 ",
            "year": 2022
        },
        {
            "authors": [
                "S. Sachdev"
            ],
            "title": "Quantum phase transitions",
            "venue": "Physics world 12, 33 ",
            "year": 1999
        },
        {
            "authors": [
                "X.-Y. Dong"
            ],
            "title": "F",
            "venue": "Pollmann, X.-F. Zhang, et al., Machine learning of quantum phase transitions, Physical Review B 99, 121104 ",
            "year": 2019
        },
        {
            "authors": [
                "P. Huembeli",
                "A. Dauphin",
                "P. Wittek"
            ],
            "title": "Identifying quantum phase transitions with adversarial neural networks",
            "venue": "Physical Review B 97, 134109 ",
            "year": 2018
        },
        {
            "authors": [
                "B.S. Rem",
                "N. K\u00e4ming",
                "M. Tarnowski",
                "L. Asteria",
                "N. Fl\u00e4schner",
                "C. Becker",
                "K. Sengstock",
                "C. Weitenberg"
            ],
            "title": "Identifying quantum phase transitions using artificial neural networks on experimental data",
            "venue": "Nature Physics 15, 917 ",
            "year": 2019
        },
        {
            "authors": [
                "H.-Y. Huang",
                "R. Kueng",
                "G. Torlai",
                "V.V. Albert",
                "J. Preskill"
            ],
            "title": "Provably efficient machine learning for quantum manybody problems",
            "venue": "Science 377, eabk3333 ",
            "year": 2022
        },
        {
            "authors": [
                "R. Horodecki",
                "P. Horodecki",
                "M. Horodecki",
                "K. Horodecki"
            ],
            "title": "Quantum entanglement",
            "venue": "Reviews of modern physics 81, 865 ",
            "year": 2009
        },
        {
            "authors": [
                "K. Kawaguchi",
                "Y. Bengio"
            ],
            "title": "and L",
            "venue": "Kaelbling, Mathematical aspects of deep learning ",
            "year": 2022
        },
        {
            "authors": [
                "S. Arora",
                "R. Ge",
                "Y. Liang",
                "T. Ma",
                "Y. Zhang"
            ],
            "title": "Generalization and equilibrium in generative adversarial nets (gans)",
            "venue": "International Conference on Machine Learning ",
            "year": 2017
        },
        {
            "authors": [
                "P. Zhang",
                "Q. Liu",
                "D. Zhou",
                "T. Xu",
                "X. He"
            ],
            "title": "On the discrimination-generalization tradeoff in gans",
            "venue": "arXiv preprint arXiv:1711.02771 ",
            "year": 2017
        },
        {
            "authors": [
                "O. Simeone"
            ],
            "title": "Machine learning for engineers",
            "year": 2022
        },
        {
            "authors": [
                "P.L. Bartlett",
                "S. Mendelson"
            ],
            "title": "Rademacher and gaussian complexities: Risk bounds and structural results",
            "venue": "Journal of Machine Learning Research 3, 463 ",
            "year": 2002
        },
        {
            "authors": [
                "A. Jacot",
                "F. Gabriel",
                "C. Hongler"
            ],
            "title": "Neural tangent kernel: Convergence and generalization in neural networks",
            "venue": "Advances in Neural Information Processing Systems , Vol. 31, edited by S. Bengio, H. Wallach, H. Larochelle, K. Grau- 57 man, N. Cesa-Bianchi, and R. Garnett ",
            "year": 2018
        },
        {
            "authors": [
                "N. Golowich",
                "A. Rakhlin",
                "O. Shamir"
            ],
            "title": "Size-independent sample complexity of neural networks",
            "venue": "Information and Inference: A Journal of the IMA 9, 473 ",
            "year": 2020
        },
        {
            "authors": [
                "K.P. Murphy"
            ],
            "title": "Probabilistic machine learning: an introduction (MIT press, 2022)",
            "year": 2022
        },
        {
            "authors": [
                "M. Arjovsky",
                "S. Chintala",
                "L. Bottou"
            ],
            "title": "Wasserstein generative adversarial networks",
            "venue": "International conference on machine learning ",
            "year": 2017
        },
        {
            "authors": [
                "S. Pirandola",
                "U.L. Andersen",
                "L. Banchi",
                "M. Berta",
                "D. Bunandar",
                "R. Colbeck",
                "D. Englund",
                "T. Gehring",
                "C. Lupo"
            ],
            "title": "C",
            "venue": "Ottaviani, et al., Advances in quantum cryptography, Advances in optics and photonics 12, 1012 ",
            "year": 2020
        },
        {
            "authors": [
                "S.-H. Tan",
                "B.I. Erkmen",
                "V. Giovannetti",
                "S. Guha",
                "S. Lloyd",
                "L. Maccone",
                "S. Pirandola",
                "J.H. Shapiro"
            ],
            "title": "Quantum illumination with gaussian states",
            "venue": "Physical review letters 101, 253601 ",
            "year": 2008
        },
        {
            "authors": [
                "L. Maccone",
                "C. Ren"
            ],
            "title": "Quantum radar",
            "venue": "Physical Review Letters 124, 200503 ",
            "year": 2020
        },
        {
            "authors": [
                "Q. Zhuang",
                "S. Pirandola"
            ],
            "title": "Ultimate limits for multiple quantum channel discrimination",
            "venue": "Physical Review Letters 125, 080505 ",
            "year": 2020
        },
        {
            "authors": [
                "A.S. Holevo"
            ],
            "title": "Statistical decision theory for quantum systems",
            "venue": "Journal of multivariate analysis 3, 337 ",
            "year": 1973
        },
        {
            "authors": [
                "C.W. Helstrom",
                "C.W. Helstrom"
            ],
            "title": "Quantum detection and estimation theory",
            "venue": "Vol. 3 ",
            "year": 1976
        },
        {
            "authors": [
                "P. Hausladen",
                "W.K. Wootters"
            ],
            "title": "A pretty good measurement for distinguishing quantum states",
            "venue": "Journal of Modern Optics 41, 2385 ",
            "year": 1994
        },
        {
            "authors": [
                "H. Barnum",
                "E. Knill"
            ],
            "title": "Reversing quantum dynamics with near-optimal quantum and classical fidelity",
            "venue": "Journal of Mathematical Physics 43, 2097 ",
            "year": 2002
        },
        {
            "authors": [
                "A. Montanaro"
            ],
            "title": "Pretty simple bounds on quantum state discrimination",
            "venue": "arXiv preprint arXiv:1908.08312 ",
            "year": 2019
        },
        {
            "authors": [
                "W. Matthews",
                "S. Wehner",
                "A. Winter"
            ],
            "title": "Distinguishability of quantum states under restricted families of measurements with an application to quantum data hiding",
            "venue": "Communications in Mathematical Physics 291, 813 ",
            "year": 2009
        },
        {
            "authors": [
                "W.K. Wootters",
                "W.H. Zurek"
            ],
            "title": "A single quantum cannot be cloned",
            "venue": "Nature 299, 802 ",
            "year": 1982
        },
        {
            "authors": [
                "A.S. Holevo"
            ],
            "title": "Bounds for the quantity of information transmitted by a quantum communication channel",
            "venue": "Problemy Peredachi Informatsii 9, 3 ",
            "year": 1973
        },
        {
            "authors": [
                "A. A\u0107\u0131n",
                "E. Bagan",
                "M. Baig",
                "L. Masanes",
                "R. Munoz-Tapia"
            ],
            "title": "Multiple-copy two-state discrimination with individual measurements",
            "venue": "Physical Review A 71, 032338 ",
            "year": 2005
        },
        {
            "authors": [
                "R. Ash"
            ],
            "title": "Information Theory",
            "venue": "Dover books on advanced mathematics ",
            "year": 1990
        },
        {
            "authors": [
                "W. Ren",
                "W. Li",
                "S. Xu",
                "K. Wang",
                "W. Jiang",
                "F. Jin",
                "X. Zhu",
                "J. Chen",
                "Z. Song"
            ],
            "title": "P",
            "venue": "Zhang, et al., Experimental quantum adversarial learning with programmable superconducting qubits, Nature Computational Science 2, 711 ",
            "year": 2022
        },
        {
            "authors": [
                "L. Banchi"
            ],
            "title": "Robust quantum classifiers via nisq adversarial learning",
            "venue": "Nature Computational Science 2, 699 ",
            "year": 2022
        },
        {
            "authors": [
                "G. De Palma",
                "M. Marvian",
                "D. Trevisan",
                "S. Lloyd"
            ],
            "title": "The quantum wasserstein distance of order 1",
            "venue": "IEEE Transactions on Information Theory 67, 6627 ",
            "year": 2021
        },
        {
            "authors": [
                "G. De Palma",
                "M. Marvian",
                "C. Rouz\u00e9",
                "D.S. Fran\u00e7a"
            ],
            "title": "Limitations of variational quantum algorithms: a quantum optimal transport approach",
            "venue": "PRX Quantum 4, 010309 ",
            "year": 2023
        },
        {
            "authors": [
                "P. Derbeko",
                "R. El-Yaniv",
                "R. Meir"
            ],
            "title": "Explicit learning curves for transduction and application to clustering and compression algorithms",
            "venue": "Journal of Artificial Intelligence Research 22, 117 ",
            "year": 2004
        },
        {
            "authors": [
                "J. Haah",
                "A.W. Harrow",
                "Z. Ji",
                "X. Wu",
                "N. Yu"
            ],
            "title": "Sample-optimal tomography of quantum states",
            "venue": "Proceedings of the forty-eighth annual ACM symposium on Theory of Computing ",
            "year": 2016
        },
        {
            "authors": [
                "A. Gily\u00e9n",
                "S. Lloyd",
                "I. Marvian",
                "Y. Quek",
                "M.M. Wilde"
            ],
            "title": "Quantum algorithm for petz recovery channels and pretty good measurements",
            "venue": "Physical Review Letters 128, 220502 ",
            "year": 2022
        },
        {
            "authors": [
                "S. Lloyd",
                "M. Mohseni",
                "P. Rebentrost"
            ],
            "title": "Quantum principal component analysis",
            "venue": "Nature Physics 10, 631 ",
            "year": 2014
        },
        {
            "authors": [
                "A. Barenco",
                "A. Berthiaume",
                "D. Deutsch",
                "A. Ekert",
                "R. Jozsa",
                "C. Macchiavello"
            ],
            "title": "Stabilization of quantum computations by symmetrization",
            "venue": "SIAM Journal on Computing 26, 1541 ",
            "year": 1997
        },
        {
            "authors": [
                "A. Monr\u00e0s"
            ],
            "title": "G",
            "venue": "Sent\u0301\u0131s, and P. Wittek, Inductive Supervised Quantum Learning, Phys. Rev. Lett. 118, 190503 ",
            "year": 2017
        },
        {
            "authors": [
                "R.M. Dudley"
            ],
            "title": "The sizes of compact subsets of hilbert space and continuity of gaussian processes",
            "venue": "Journal of Functional Analysis 1, 290 ",
            "year": 1967
        },
        {
            "authors": [
                "Q.-W. Zeng",
                "H.-Y. Ge",
                "C. Gong",
                "N.-R. Zhou"
            ],
            "title": "Conditional quantum circuit born machine based on a hybrid quantum\u2013 classical framework",
            "venue": "Physica A: Statistical Mechanics and its Applications 618, 128693 ",
            "year": 2023
        },
        {
            "authors": [
                "C. Zoufal",
                "A. Lucchi",
                "S. Woerner"
            ],
            "title": "Quantum generative adversarial networks for learning and loading random distributions",
            "venue": "npj Quantum Information 5, 103 ",
            "year": 2019
        },
        {
            "authors": [
                "Y. Du",
                "Z. Tu",
                "B. Wu",
                "X. Yuan",
                "D. Tao"
            ],
            "title": "Theory of quantum generative learning models with maximum mean discrepancy",
            "venue": "arXiv preprint arXiv:2205.04730 ",
            "year": 2022
        },
        {
            "authors": [
                "S. Chakrabarti",
                "H. Yiming",
                "T. Li",
                "S. Feizi",
                "X. Wu"
            ],
            "title": "Quantum wasserstein generative adversarial networks",
            "venue": "Advances in Neural Information Processing Systems , Vol. 32, edited by H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett ",
            "year": 2019
        },
        {
            "authors": [
                "P.-L. Dallaire-Demers",
                "N. Killoran"
            ],
            "title": "Quantum generative adversarial networks",
            "venue": "Physical Review A 98, 012324 ",
            "year": 2018
        },
        {
            "authors": [
                "K. Gili",
                "M. Mauri",
                "A. Perdomo-Ortiz"
            ],
            "title": "Evaluating generalization in classical and quantum generative models",
            "venue": "arXiv preprint arXiv:2201.08770 ",
            "year": 2022
        },
        {
            "authors": [
                "S.T. Jose",
                "O. Simeone"
            ],
            "title": "Transfer learning for quantum classifiers: An information-theoretic generalization analysis",
            "venue": "58 2023 IEEE Information Theory Workshop (ITW) ",
            "year": 2023
        },
        {
            "authors": [
                "S. Aaronson"
            ],
            "title": "Read the fine print",
            "venue": "Nature Physics 11, 291 ",
            "year": 2015
        },
        {
            "authors": [
                "T.J. Elliott",
                "C. Yang",
                "F.C. Binder",
                "A.J. Garner",
                "J. Thompson",
                "M. Gu"
            ],
            "title": "Extreme dimensionality reduction with quantum modeling",
            "venue": "Physical Review Letters 125, 260501 ",
            "year": 2020
        },
        {
            "authors": [
                "R. Raz"
            ],
            "title": "Exponential separation of quantum and classical communication complexity",
            "venue": "Proceedings of the thirty-first annual ACM symposium on Theory of computing ",
            "year": 1999
        },
        {
            "authors": [
                "U. Haagerup"
            ],
            "title": "The best constants in the khintchine inequality",
            "venue": "Studia Mathematica 70, 231 ",
            "year": 1981
        },
        {
            "authors": [
                "F. Lust-Piquard"
            ],
            "title": "In\u00e9galit\u00e9s de Khintchine dans Cp (1 < p <\u221e)",
            "venue": "CR Acad. Sci. Paris 303, 289 ",
            "year": 1986
        },
        {
            "authors": [
                "E. Candes",
                "B. Recht"
            ],
            "title": "Exact matrix completion via convex optimization",
            "venue": "Communications of the ACM 55, 111 ",
            "year": 2012
        },
        {
            "authors": [
                "J.A. Tropp"
            ],
            "title": "An introduction to matrix concentration inequalities, Foundations and Trends\u00ae in Machine Learning",
            "year": 2015
        },
        {
            "authors": [
                "D. Cohen",
                "A. Kontorovich",
                "G. Wolfer"
            ],
            "title": "Learning discrete distributions with infinite support",
            "venue": "Advances in Neural Information Processing Systems 33, 3942 ",
            "year": 2020
        },
        {
            "authors": [
                "Y. Bai",
                "T. Ma",
                "A. Risteski"
            ],
            "title": "Approximability of discriminators implies diversity in gans",
            "venue": "arXiv preprint arXiv:1806.10586 ",
            "year": 2018
        },
        {
            "authors": [
                "H.V. Poor"
            ],
            "title": "An introduction to signal detection and estimation (Springer",
            "venue": "Science & Business Media,",
            "year": 1998
        },
        {
            "authors": [
                "M. Fanizza",
                "M. Skotiniotis",
                "J. Calsamiglia",
                "R. Mu\u00f1oz-Tapia"
            ],
            "title": "and G",
            "venue": "Sent\u0301\u0131s, Universal algorithms for quantum data learning, EPL 140, 28001 ",
            "year": 2022
        },
        {
            "authors": [
                "A. Hayashi",
                "M. Horibe",
                "T. Hashimoto"
            ],
            "title": "Quantum pure-state identification",
            "venue": "Phys. Rev. A 72, 052306 ",
            "year": 2005
        },
        {
            "authors": [
                "G. Sent\u0301\u0131s",
                "J. Calsamiglia",
                "R. Mu\u00f1oz-Tapia",
                "E. Bagan"
            ],
            "title": "Quantum learning without quantum memory",
            "venue": "Sci Rep 2,",
            "year": 2012
        },
        {
            "authors": [
                "B. He",
                "J.A. Bergou"
            ],
            "title": "Programmable unknown quantum-state discriminators with multiple copies of program and data: A Jordan-basis approach",
            "venue": "Phys. Rev. A 75, 032316 ",
            "year": 2007
        },
        {
            "authors": [
                "M. Fanizza",
                "A. Mari",
                "V. Giovannetti"
            ],
            "title": "Optimal Universal Learning Machines for Quantum State Discrimination",
            "venue": "IEEE Transactions on Information Theory 65, 5931 ",
            "year": 2019
        },
        {
            "authors": [
                "M. Larocca",
                "N. Ju"
            ],
            "title": "D",
            "venue": "Gar\u0107\u0131a-Mart\u0301\u0131n, P. J. Coles, and M. Cerezo, Theory of overparametrization in quantum neural networks, Nature Computational Science 3, 542 ",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n30 9.\n11 61\n7v 1\n[ qu\nan t-\nph ]\n2 0\nSe p\n20 23\nStatistical Complexity of Quantum Learning\nLeonardo Banchi,1, 2, \u2217 Jason Luke Pereira,2 Sharu Theresa Jose,3 and Osvaldo Simeone4\n1Department of Physics and Astronomy, University of Florence, via G. Sansone 1, I-50019 Sesto Fiorentino (FI), Italy\n2INFN Sezione di Firenze, via G. Sansone 1, I-50019, Sesto Fiorentino (FI), Italy 3Department of Computer Science, University of Birmingham Birmingham, B15 2TT, UK\n4King\u2019s Learning and Information Processing lab (KCLIP) & Centre for Intelligent Information Processing Systems (CIIPS)&\nDepartment of Engineering, King\u2019s College London, Strand, London, WC2R 2LS, UK\nRecent years have seen significant activity on the problem of using data for the purpose of learning properties of quantum systems or of processing classical or quantum data via quantum computing. As in classical learning, quantum learning problems involve settings in which the mechanism generating the data is unknown, and the main goal of a learning algorithm is to ensure satisfactory accuracy levels when only given access to data and, possibly, side information such as expert knowledge. This article reviews the complexity of quantum learning using information-theoretic techniques by focusing on data complexity, copy complexity, and model complexity. Copy complexity arises from the destructive nature of quantum measurements, which irreversibly alter the state to be processed, limiting the information that can be extracted about quantum data. For example, in a quantum system, unlike in classical machine learning, it is generally not possible to evaluate the training loss simultaneously on multiple hypotheses using the same quantum data. To make the paper selfcontained and approachable by different research communities, we provide extensive background material on classical results from statistical learning theory, as well as on the distinguishability of quantum states. Throughout, we highlight the differences between quantum and classical learning by addressing both supervised and unsupervised learning, and we provide extensive pointers to the literature.\nContents\n1. Introduction and Summary 3 1.1. Scope 3 1.2. Examples 3 1.3. Learning Settings 4 1.4. Inductive vs. Transductive Learning 6 1.5. Architectures 7\n1.5.1. Inductive Learning 7 1.5.2. Transductive Learning 8\n1.6. Optimality Gap and Generalization Error 9 1.6.1. Known Training States 9 1.6.2. Unknown Training States 10 1.7. Overview of Results with Unconstrained Operations 11 1.8. Overview of Results with Constrained Operations 14 1.9. Applications 15\n1.9.1. Parametric Quantum Circuits 15 1.9.2. Learning to Classify Phases of Matter 16 1.9.3. Learning to Classify Entanglement 16\n1.10. Organization of the Paper 17\n2. Classical Statistical Learning Theory 17 2.1. Supervised Learning 18 2.2. Generalization Error in Supervised Learning 19\n\u2217 leonardo.banchi@unifi.it\n2 2.3. Model Selection via Structural Risk Minimization 20\n2.4. Unsupervised Learning 21\n2.5. Generalization Error in Unsupervised Learning 22\n3. Quantum State Discrimination 23\n3.1. Single-Shot Discrimination with Fixed Measurements 24\n3.2. Single-Shot Discrimination with Optimized Measurements 25\n3.3. Multiple Shot Discrimination via the Majority Rule 25\n3.4. Multiple Shot Discrimination via Expected Value of an Observable 26\n3.4.1. Problem Formulation 26\n3.4.2. Decision Observables with Fixed Operator Structure 27\n3.4.3. Norm-Constrained Decision Observables 27\n3.4.4. Decision Observables from the Representer Theorem 28\n4. Learning To Discriminate Unknown Quantum States 29\n4.1. Generalization Analysis 30\n4.2. Tomography-based Quantum State Classification 30\n4.3. Discrimination with fixed measurements 32\n4.4. Learning with decision observables 33\n4.5. Learning with Helstrom measurements 33\n4.6. Learning with the representer theorem 35\n4.7. Inductive vs. Transductive Strategies 36\n5. Supervised Learning for Quantum Classification 37\n5.1. Known Quantum State and Unconstrained Complexity 38\n5.1.1. Lowering the Training Error via the Majority Rule 38\n5.2. Unknown Quantum States and Unconstrained Complexity 39\n5.3. Information theoretic understanding of training/testing errors 40\n5.3.1. Learning with imperfect copies 41\n5.4. Learning with Observables 42\n5.5. Learning with Kernels 43\n5.6. Parameterized Quantum Circuits as Classifiers 44\n6. Unsupervised Learning for Quantum Generative Modelling 46\n6.1. Learning Tasks 46\n6.2. Quantum Generative Learning Models 46\n6.3. Excess Risk of Quantum Generative Modelling 47\n7. Conclusions and Outlook 47\nA. Useful mathematical results 49\n1. Bounds on Rademacher Complexities 50\nB. Derivation of the upper bounds 52\n1. Unsupervised Learning under IPM 52\n2. Supervised Learning with Unconstrained POVM 52\n3. Rademacher Complexity of the Majority Rule with Unconstrained Local Measurements 53\n4. Supervised Learning with Constrained Observables 53\nC. Generalization error with Fourier-like embeddings 53\nD. On Learning to Discriminate: Average-Case Error vs. Worst-Case Error 54\nReferences 55\n3"
        },
        {
            "heading": "1. Introduction and Summary",
            "text": "In this section, we provide an introduction to the topic of statistical complexity of quantum learning and we highlight some key results that will be elaborated on in the rest of the paper."
        },
        {
            "heading": "1.1. Scope",
            "text": "Quantum information theory addresses the implications of quantum mechanics on the representation, storage, and transmission of information in microscopic physical systems. Mathematically, it centers on the characterization of probabilistic and statistical aspects of observations made on quantum systems through the lens of information and uncertainty quantification. In this context, recent years have seen significant activity on the problem of using observations \u2013 i.e., data \u2013 for the purpose of learning properties of quantum systems or of processing classical or quantum data via quantum means [1\u20136]. In this article, we study the complexity of quantum learning using information-theoretic techniques. As in classical learning, quantum learning problems involve settings in which the mechanism generating the data is unknown, and the main goal of a learning algorithm is to ensure satisfactory accuracy levels when only given access to data and, possibly, side information such as expert knowledge. The complexity of quantum learning is a multi-faceted concept, and in this article we focus on the following three aspects:\n\u2022 Data complexity: As in classical machine learning, limitations on the amount of available data \u2013 which may be classical or quantum \u2013 play a key role in determining the achievable accuracy levels for data-driven methods. Data complexity refers to the requirements in terms of data set size on the performance of a learning algorithm. Throughout the paper, we will use the letter N to indicate the number of training data examples.\n\u2022 Copy complexity: The second form of complexity hinges on the destructive nature of quantum measurements, which irreversibly alter the state to be processed. Because of this, only limited information about quantum data can be extracted. This stands in contrast to classical data which, barring computational complexity constraints, can be accessed, copied, and processed an arbitrary number of times. In particular, with quantum data, the number of copies available of a given quantum state determines the amount of information that can be extracted from the state. For example, in a quantum system, unlike in classical machine learning, it is generally not possible to evaluate the training loss simultaneously on multiple hypotheses using the same quantum data, since each evaluation irreversibly modifies the quantum training data. The copy complexity reflects the requirements in terms of copies of a quantum state that are needed to ensure given accuracy levels. We will use the letter S to indicate the number of copies of each training data example and V to denote the number of copies of a test state.\n\u2022 Model complexity: Quantum algorithms are implemented via quantum circuits whose complexity can be measured in terms of number of qubits and number of gates. For a quantum learning algorithm to be successful, it is necessary to strike a balance between the expressivity enabled by a more complex model and trainability, which may be impaired by the adoption of larger models with more tunable parameters.\nClassical learning theory has traditionally focused on developing results that are agnostic to the distribution of the data, typically depending only on the complexity of the model [7]. In contrast, more recent information-theoretic analyses center on the interplay between learning algorithm, data distribution, and model complexity. In the context of quantum learning, data-dependent analyses are particularly useful in offering greater physical insights. In fact, since such analyses explicitly depend on the characteristics of the quantum data, they can reveal important physical aspects such as the features of quantum states that determine the data, copy, and/ or model complexity of learning. Accordingly, in this article, we will quantitatively study data, copy, and model complexity by adopting an information-theoretic viewpoint. To make the paper self-contained, we will provide extensive background material on classical results from statistical learning theory, as well as on the distinguishability of quantum states. Most of this material will be based on papers, including [8\u201318], but new applications will also be presented."
        },
        {
            "heading": "1.2. Examples",
            "text": "In this article, we consider both supervised and unsupervised quantum learning problems. To illustrate the wide range of applications for the problems under study, this subsection lists some examples in both categories.\n4 Supervised learning: In supervised learning problems, one is given data in the form of an input \u2013 classical or quantum \u2013 state and of a desired output, which is typically classical. Some examples of applications are as follows.\n\u2022 Quantum classification of classical data: Consider classical data x, e.g., images or text, that need to be classified. A quantum computer can be used for this purpose by first embedding the classical data x into a quantum state \u03c1(x), and by then using a quantum measurement to classify the resulting states. The embedding \u03c1(x) is produced by a quantum circuit that may be optimized based on data along with the measurement, and the output of the circuit is a classification label y.\n\u2022 Quantum classification of quantum sensed data: Consider now the problem of detecting some physical property y on the basis of quantum data collected by a quantum sensor. For example, quantum light may be used to illuminate a sample x under study, some photons are dispersed in the environment, while others enter the detector. Here, quantum data is described by the quantum state \u03c1(x) of the system collected by the detector, and y represent some feature of the illuminated physical sample x that we want to extract. Entangled light was shown to enhance the classification of sensed images [19].\n\u2022 Classification of quantum phases of matter. The phase of a many-body quantum system can be determined on the basis of the ground state, or some other equilibrium state, of the system, with classical input x modeling the tunable external parameters, such as magnetic fields, coupling strengths among particles and the temperature [20].\n\u2022 Detecting entanglement. Given a quantum state, one may be interested in assessing their entanglement structure, e.g., whether they are separable or entangled for a certain binary partition. This can be done by applying a quantum algorithm on a state \u03c1(x), where the inputs x model all the parameters in the (unitary) transformation used to create the resulting state.\n\u2022 Classification of noisy states. Consider a source that emits two possible states, which are then perturbed by an environment. The goal is to determine which of the two possible states was emitted by the source based on access to the perturbed state \u03c1.\nUnsupervised learning: Unsupervised learning is a vast field that includes tasks as different as clustering and generative modelling. Classically, unsupervised learning refers generically to the task of inferring properties of the unknown data generation mechanism. In particular, in generative modeling, the ultimate goal is generating new data by sampling from the data distribution. Focusing on generative tasks, examples of unsupervised learning problems are as follows.\n\u2022 Generating classical data: Quantum models can be used to approximately sample from an unknown classical distribution. In this case, the statistical description of the model is expressed as a quantum state, which has to be learnt from data. After training, new classical data are generated by applying measurements onto that state [21\u201323].\n\u2022 Loading a classical distribution into a quantum state: Given an unknown data distribution P (x), it may be of\ninterest to generate a quantum superposition |\u03c8\u3009 =\u2211x \u221a\nP (x) |x\u3009 that \u201cloads\u201d such distribution on a quantum system [24]. This can enable the processing of classical distributions using quantum hardware.\n\u2022 Quantum state approximation: Given copies of a quantum state \u03c1, one may be interested in optimizing a quantum circuit that can produce systems in a state \u03c1\u0302 that is approximately equal to \u03c1, with applications in quantum state compilation [25], quantum noise sensing [26], approximation of unknown pure [27] and mixed states [25, 28]."
        },
        {
            "heading": "1.3. Learning Settings",
            "text": "In this article, we study both supervised and unsupervised learning problems. A learner is given classical and/ or quantum data, and the goal is to extract information from data that can be used for tasks such as inference and data generation. As illustrated in the first row of Table I, in classical machine learning, a data point (x, y) encompasses a classical input x, e.g., a vector of numbers, and a desired classical output y. Each pair is assumed to be generated from an unknown data distribution P (x, y). The learner has access to information in the form of N training pairs (xn, yn) with n = 1, ..., N and to a test input x. The learner can copy this information at will to compute any number of\ntimes on the training data and test input. Note that, in the case of unsupervised learning, the classical label y is not defined. For quantum learning problems, a data point (x, \u03c1(x), y) generally encompasses: a classical input x; a quantum input state \u03c1(x); and a classical output y. Note that not all three elements must be present for all problems. For instance, in some settings, such as the classification of quantum sensed data or that of noisy states mentioned in the previous subsection, a data point may not include input x. Furthermore, as mentioned, for unsupervised learning, the classical label y is not present. The classical input x and the classical output y \u2013 jointly distributed according to an unknown joint distribution P (x, y) \u2013 can be processed and copied at will, while for the training quantum input state \u03c1(xn) we must distinguish between the following situations:\n\u2022 Known training quantum states : In the first situation, the quantum states \u03c1(xn) from the training set are known, in the sense that classical descriptions of all density matrices \u03c1(xn) exist, either on the basis of a theoretical model or because of a prior full state tomography. Training may not involve any quantum hardware, as arbitrary copies of the training states can be made within a classical computer, just as in the classical setting. Accordingly, the performance of the learner is limited by the number N of available examples, as for classical machine learning.\n\u2022 Unknown training quantum states, known state-preparation mechanism: In the second type of problem, the training quantum states \u03c1(xn) are unknown, even when the classical inputs xn are available, and the learner can only gather partial information about the states via measurements. However, a physical mechanism for creating training and test states is known, so a learner can repeat the same state-preparation procedure to create a number, S, of copies of each training state \u03c1(xn), and V copies of the test state. Given the trade-off between disturbance and the amount of information extracted from a state by a measurement [29], the number of copies available for the training and test states determine the degree to which the learner can access information about the training and test data, respectively. Therefore, in this case, the performance of the learner is limited not only by the number N of available examples, but also by the number of copies of each training state, S and of the test state, V .\n\u2022 Unknown training quantum states, unknown state-preparation mechanism: In this last type of problem, not only are the states \u03c1(xn) unknown, but the mechanism for creating them is also not available. For instance, in the problem of classifying noisy states described in the previous section, each state is generally different in the sense that the specific perturbation applied to the state is typically not known. This extreme case may be reduced to the previous one by setting S = 1, i.e., each training state is treated as an individual copy. In this case, it may still be meaningful to allow for a number V of copies of the test state, which may be created by the unknown physical mechanism at test time.\nLearning settings with unknown quantum states and known state-preparation mechanism are more general than those with known quantum states. This is in the sense that, as the number of copies available for each quantum state \u03c1(xn) or \u03c1(x) grows, the learner can obtain a description of the corresponding states, which may hence considered as known. Accordingly, studying scenarios with known states is often useful as a stepping stone to address more challenging problems with unknown states.\n6 Referring to Table I, a learner has generally access to\n\u2022 a finite-copy, or S-copy training set SS of N training data points (xn, \u03c1(xn)\u2297S , yn), for n = 1, . . . , N , comprising classical input xn, S copies of the unknown quantum state \u03c1(xn), and a classical output yn;\n\u2022 and one or more test inputs (x, \u03c1(x)\u2297V ), for which one has V copies of the quantum state \u03c1(x), to which the learner wish to assign a classical label y.\nWhen the training states \u03c1(xn) are known, the learner is said to have access to the abstract training set, denoted as S, which contains a classical description of the states \u03c1(xn). The abstract training set can be thought of, conceptually, as containing an infinite number of copies of the states."
        },
        {
            "heading": "1.4. Inductive vs. Transductive Learning",
            "text": "We can distinguish two main ways to infer the output y given the input test data and the training data, with the second being more general than the first.\n\u2022 Inductive learning: As illustrated in Figure 1(a), inductive learning follows a two-step procedure, in which training data is only used in the first phase, and is no longer needed to make decisions on a new input.\n\u2013 In the training phase, inductive learning methods use training data to obtain a general inference operation f \u2208 F , among a chosen function class F , which is classically described and stored in a classical memory. The inference operation may, e.g., amount to the specification of a quantum circuit, or of a classical description of quantum states. Selection of the inference operation f is typically done by comparing the performance of operation f on the same available training data. For unknown states, quantum measurements require the \u201cconsumption\u201d of different copies of the training states, and hence comparisons are inherently stochastic unless the number of copies S is arbitrarily large.\n\u2013 During the testing phase, the inference operation f is applied to the test example to determine the output y.\nAccordingly, induction refers to the extraction of a general, classically describable, rule from examples. Importantly, the rule f can be applied to any number of test examples, and training examples are no longer needed after the training phase.\n\u2022 Transductive learning: As illustrated in Figure 1(b), transductive learning strategies jointly process an S-copy version SS of the training data set S and the test example. Accordingly, producing the output y for different test examples (x, \u03c1(x)\u2297V ) generally requires distinct copies of the training data. This is unlike inductive methods for which training data is no longer used for testing. We will specifically target transductive strategies for the general case in which the quantum states \u03c1(x) are unknown. In this case, one needs to use the available copies of each training example as efficiently as possible, and the generality of transductive strategies may offer advantages in this regard. Transductive strategies may be \u201cuniversal\u201d, in the sense that they amount to a single quantum operation that applies to any training data set and test input to produce a prediction. As a note of terminology, one may still define an inference function for transductive learning, although such a function is not inferred from data, but rather enacted as a result of the single quantum operation on the training set and test example. More generally, as shown in Figure 1(c), there are transductive strategies that also have inductive elements, storing some learned information in a classical memory. For such strategies, the inductive learning step consumes additional copies of the training set, allowing the quantum operation to be tailored to the given problem.\nWe end with two remarks on inductive and transductive learning. First, we observe that in classical machine learning, transductive schemes are similarly defined (see, e.g., [30]). Using classical transductive concepts, transduction may be useful to improve statistical efficiency in the case in which the quantum states \u03c1(x) are known. For example, it could be used, in conjunction with conformal prediction strategies [30], to enhance calibration (see also [31]). Second, in principle, it is possible to replace the classical memory with a quantum memory in the strategies of Figs. 1(a,c). However, in spite of the separation between training and testing, since a quantum memory is destroyed after each measurement, there is no reusable information and this approach can be cast in the purely transductive scheme of Figure 1(b)."
        },
        {
            "heading": "1.5. Architectures",
            "text": "Table II provides examples of inductive and transductive quantum learning architectures, which are briefly reviewed next. We emphasize that transductive techniques only apply to the case of unknown states, in which copies of the training states are needed to process each new input."
        },
        {
            "heading": "1.5.1. Inductive Learning",
            "text": "The following are popular instance of inductive learning.\n\u2022 Quantum neural networks use a trainable parametric quantum circuit followed by a fixed measurement to predict the outcome y. The label y can be extracted by means of a single-shot measurement, via a classical post-processing of multiple shots, or via the expectation value of a fixed observable [3]. In this case, the model can be described classically via the circuit structure and the circuit parameters, while the circuit evaluation is done\n8 in a quantum hardware. Evaluation may be also implemented on classical hardware for shallow circuits and few qubits. Training follows the inductive learning procedure of finding the optimal circuit parameters \u2013 along with possibly other hyperparameters such as the circuit structure. This is done via classical optimization. A typical approach is to leverage gradient descent schemes, whereby the gradient is evaluated by taking measurements of the output of the circuit, and variations thereof, thus requiring copies of the training data for each gradient evaluation. Furthermore, at testing time, the circuit with learnt parameters is employed to process, separately, any number of new inputs, without requiring access to the training data.\n\u2022 Quantum reservoir computing and extreme learning machines represent an experimentally friendly hybrid approach for quantum machine learning, which exploits the dynamics of a fixed complex quantum system to process the inputs, followed by trainable post-processing [32, 33], whose parameters can be stored in a classical memory. Training and testing follow the same general inductive learning approach as for quantum neural networks.\n\u2022 Classical shadows represent a powerful technique that has been applied to different learning problems with quantum inputs. Notably, in the original formulation[34], the task was the regression-like problem of predicting expectation values of a few observables, depending on the outcomes of random measurements. Following an inductive learning method, training consists of a two-step process, with the first step consisting of the collection of partial classical information about the unknown training states \u03c1(x), followed by a classical post-processing of the measurement outcomes. It should be mentioned that alternatives involving a quantum memory have also been proposed [11, 18]."
        },
        {
            "heading": "1.5.2. Transductive Learning",
            "text": "Some instance of transductive learning architectures are as follows.\n\u2022 The Helstrom classifier is the optimal binary quantum state detector under the most general and unrestricted set of measurements [8, 35]. It has an explicit expression in terms of the training data, and hence, for the case of known training states, it can be inductively applied to any test data. However, approximating the classifier for unknown states can benefit from a transductive learning architecture that jointly processes all training and testing data. Such an architecture was proposed that hinges on the state exponentiation and phase estimation algorithms, with an algorithmic complexity that scales logarithmically with the dimension of the Hilbert space.[35, 36]. In this architecture, there are no learnt parameters to be stored in either quantum or classical memories, and each classification requires processing different copies of the test state \u03c1(x) as well as different copies of the training states.\n\u2022 Quantum kernel methods produce a prediction y for a new test state \u03c1(x) by carrying out a comparison, using a quantum routine, with all examples in the training set[10, 37]. Accordingly, following a transductive learning approach, training data can only be leveraged to process a single test example. The comparisons between test and training states produce classical outputs that are combined with the training labels in order to obtain the final decision y. To elaborate, define as k(x, x\u2032) = Tr[\u03c1(x)\u03c1(x\u2032)] the kernel function that measures the similarity of states \u03c1(x) and \u03c1(x\u2032). Note that, when states \u03c1(x) and \u03c1(x\u2032) are unknown, estimating k(x, x\u2032) requires access to copies of the states. With this definition, the prediction produced by quantum kernel methods is f(x) = \u2211\nn \u03b1nk(xn, x), where xn represents the n-th training input and the weight parameters \u03b1n are obtained during training from the kernel matrix k(xn, xm) between pairs of training data (xn, xm). Accordingly, training requires copies of the training data, but also providing a prediction for a new input (x, \u03c1(x)) requires having access to copies of the test state \u03c1(x) and of the training states \u03c1(xn). Overall, quantum kernel methods are transductive, but with an inductive component for estimating the classical coefficients \u03b1n.\nAlthough it may naively appear better to focus on inductive strategies, since they can can be applied an arbitrary number of times, it has been shown that there are transductive strategies for principal component analysis of n-qubit states that require exponentially fewer (in n) samples than inductive strategies that measure each state separately. Note that this approach is more restrictive than the most general inductive strategy. Nonetheless, this result shows that transductive strategies can be advantageous if the dimension of the inputs is large and we do not have many copies of the test inputs [18].\n9"
        },
        {
            "heading": "1.6. Optimality Gap and Generalization Error",
            "text": "In this section, we discuss how to define and evaluate the performance of quantum learning algorithms as a function of the number of training samples, N , of the number of copies S available for each training state, and of the number of copies V of the test state. As we detail next, we consider two different regimes.\n\u2022 Known training states : In the first regime, complete knowledge of the training states is assumed, which can be viewed as the limit S \u2192 \u221e of an infinite number of copies of the respective states. This setting can be mapped to a classical problem, with inputs given by the classical description \u03c1(x) rather than x, and one can use tools from classical statistical learning theory to define and bound the errors in terms of the training data set size N .\n\u2022 Unknown training states : In the second regime, we assume that the quantum states are unknown and the average number of copies S is finite. When the state-preparation mechanism is known, one may assume that gathering new data, namely increasing N , is more costly than increasing the number of copies of each available data, i.e., increasing S, when a state-preparation mechanism is known. Conversely, in order to address the case in which the state-preparation mechanism is not known, one can study the special case in which a single sample, S = 1, is available for the training samples."
        },
        {
            "heading": "1.6.1. Known Training States",
            "text": "In supervised learning, as part of the problem definition, one specifies a loss function to gauge the quality of a prediction. The overall performance of a learning algorithm, as well as its design, typically hinge on different types of averages of the loss function. Machine learning operates in the absence of information about the data-generation mechanism, apart from the available training data and, possibly, domain knowledge. If the data generation mechanism, denoted as P , were known, one could define an average loss LP(f), where the average is taken with respect to any randomness in the data generation process \u2013 as dictated by P \u2013 and in the processing steps. The average loss LP(f) is also known as the test loss. As illustrated in Figure 2, in the ideal case in which one can evaluate the test loss LP(f), it is in principle possible to optimize the inference function by minimising the loss LP(f). The minimum average loss LP(f\u2217) attained for the optimal inference function f\u2217 represents the performance level accrued by an optimal processing of the test input given knowledge of the mechanism P . Given any, generally suboptimal, inference function f , the difference between the minimum average loss and the function\u2019s average loss is defined as the optimality gap\nE(f) = LP(f)\u2212 LP(f\u2217). (1)\nThe optimality gap, illustrated in Figure 2, gauges the suboptimality of function f with respect to the optimal inference function f\u2217. In this subsection, we focus on the case in which the data-generation mechanism is not known, i.e., the distribution P (x, y) is not known, but the training states are known. In this case, one can estimate the average loss LP(f) via an average over the abstract training set S. We refer to this estimate as the dataset loss, since it relies on the availability of what may be considered as a classical dataset \u2013 the abstract data set S \u2013 that contains samples from distribution P (x, y) along with a classical description of the training set. In this setting, the dataset loss is an empirical loss since it is obtained as an average over classical data points obtained from empirical observations. By minimizing the dataset loss L(f,S), the learner obtains an optimized inference function fS , which is generally different from the optimal inference function f\u2217. The inference function fS achieves the learner\u2019s average loss LP(fS). Note that the learner\u2019s average loss can be expressed in terms of the optimality gap as\nLP(fS) = LP(f\u2217) + E(fS). (2)\nWe are interested in studying how the optimality gap E(fS) for the learned inference function changes as a function of the amount of training data N and of the the model complexity. In this regard, it is observed that, as the size of the data set, N , grows arbitrarily large, the optimality gap tends to zero for a well-designed learning strategy. In fact, in this regime, the learner can acquire full information about the distribution P (x, y) and the training loss tends to the average loss, implying that the optimized function fS approximates increasingly well the ideal function f\u2217. An important performance metric for an inference function f is the generalization error\nG(f) = LP(f)\u2212 L(f,S) (3)\n10\nthat quantifies how well the training loss L(f,S) approximates the average loss LP(f). A smaller generalization error indicates that the learner can \u201ctrust\u201d the training loss as a design criterion used as a proxy for the unknown average loss LP(f). The average loss LP(fS) of the inference function fS can be decomposed in terms of the generalization error as\nLP(fS) = L(fS ,S) + G(fS). (4)\nFrom Eqs. (2) and (4), as illustrated in Figure 2, an inference function fS that either minimizes the optimality gap, or that simultaneously minimize the generalization error and the training error, ensures a small average loss."
        },
        {
            "heading": "1.6.2. Unknown Training States",
            "text": "When the training states are unknown, information about the data-generation mechanism P at the learner is limited to an S-copy version SS of the abstract data set. In this subsection, we elaborate on the notions of optimality gap and generalization error in this setting. In general, the goal of the learner to implement an inference function, denoted as fSS with as small an average loss as possible. It is useful to start by elaborating on two asymptotic regimes.\n\u2022 Infinite number of copies (S \u2192 \u221e): In the asymptotic limit S \u2192 \u221e of an infinite number of copies, we return to the setting of Section 1.6.1, as the learner acquires the abstract data set and so perfect knowledge of the dataset loss L(f,S). Therefore, the optimized function fSS tends to the function fS designed in the case of known training states, and the performance is limited only by the number of data points, N .\n\u2022 Infinite number of data points (N \u2192 \u221e): In contrast, if we let N \u2192 \u221e and keep S fixed, the learner observes an infinite number of copies of any state \u03c1(x) that has a non-zero probability under the joint distribution P (x, y), while also acquiring complete information about P (x, y). This implies that a well-designed learning algorithm returns a function fSS that coincides with the ideal function f\u2217. In this case, the optimality gap tends to zero, irrespective of the value of the number of copies, S.\nIn a general non-asymptotic regime, in the absence of knowledge of the training states, the learner may not carry out intermediate measurements, and so it may not have access to any empirical estimate of the average loss. We refer to whatever loss function the learner minimizes to obtain the optimized inference function fSS as the training loss, regardless of whether it uses any empirical data. We now elaborate on how inductive and transductive learning schemes may operate in this regime.\n11\n\u2022 Inductive learning: Following an inductive learning approach, the learner may carry out measurements on the training set to construct a classical set of observations and define an empirical loss over that set. This loss may be considered \u201cdoubly-empirical\u201d, in the sense that it is conceptually obtained by approximating the average loss LP(f) via two empirical averages. The first average is taken with respect to the N samples of the abstract training set, S, yielding the abstract dataset loss L(f,S), while the second is applied with respect to the S copies available for each training state.\n\u2022 Transductive learning: An alternative approach consists in using a transductive strategy whereby a fixed \u2013 or partially programmable, for strategies with inductive elements \u2013 circuit is applied jointly to both the new test input and the S-copy training set. In this case, as we will show in Section 5, whilst the learner never directly gains any empirical, i.e., measurement-based, knowledge of the dataset loss, the inference function fSS enacted by the learner to test a new input can approximate the inference function fS that minimizes the dataset loss.\nOwing to the availability of a finite number of copies, S, of the training quantum states, for any finite value of N , there is generally a difference \u2013 which may be positive or negative \u2013 between the test loss LP(fSS ) attained by the inference function fSS optimized by the learner and the test loss LP(fS) that the learner would have obtained with known training states as in the setting studied in the previous subsection. As illustrated in Figure 2, we refer to this difference as the excess testing error, which is defined as\nEStest = LP(f S S )\u2212 LP(fS). (5)\nIn a similar way, the gap in performance between the functions fS and fSS can be evaluated in terms of the dataset loss, yielding the knowledge gap\nESS = L(f S S ,S) \u2212 L(fS ,S). (6)\nThe knowledge gap is always positive and depends on our learning strategy. With these definitions, we can decompose the average loss for the inference function fSS as\nLP(f S S ) = LP(f\u2217) + E(fSS ) (7)\n= L(fS ,S) + G(fS) + EStest, (8) = L(fS ,S) + G(fSS ) + ESS , (9)\nThese decompositions highlight the additional error terms EStest and E S S caused by the availability of an S-copy training set in lieu of the abstract training set. The average loss LP(fSS ) can be in principle analyzed by using any of the decompositions highlighted above. Different techniques may indeed approach the problem in distinct ways depending on the particular optimization protocol being implemented. For instance, one may attempt to analyze the generalization error G(fS) of the inference function fS , alongside the excess testing error EStest, while leveraging the third decomposition; or one may study the generalization error G(fSS ) of the inference function fSS , along with the knowledge gap ESS , by using the last decomposition."
        },
        {
            "heading": "1.7. Overview of Results with Unconstrained Operations",
            "text": "In the next two subsections, we review results concerning the analysis of the optimality gap and generalization errors that will be further analyzed in the rest of the paper. Throughout these sections, we focus on classification problems with two possible equiprobable classes of quantum states, indexed by the label y = \u00b11 with probabilities P (y) = 1/2. We adopt the standard linear 01 loss, that takes value 1 when the predicted class y\u0302 of a quantum state \u03c1(x) differs from the true class y, and takes value 0 otherwise. As such, the 01 loss has a natural interpretation as the probability of misclassification. We also assume that the classical input x is not available to the learner. As discussed, the classifier has access to V copies of a given test state \u03c1(x)\u2297V . Since as long as the probability of misclassification with a single copy is no larger than 1/2 \u2212 O(V \u22121) a majority rule over the V copies ensures that average 01 loss decreases exponentially with V (see Section 5), we will focus here on the V = 1 case. Furthermore, in this subsection, we consider general classification strategies, without imposing any constraints on the allowed quantum and classical operations, while in the next subsection we discuss the role played by restrictions on the class of feasible models. We begin this subsection by analyzing the ideal case in which the learner knows the overall data-generation mechanism P , including the classical distribution P (x, y) and the training states \u03c1(xn). In this regime, no training data\n12\nare required. Then, we address the learning problems presented in the previous subsection with known or unknown training states. 1) Known data-generation mechanism P, known states: In this first setting, detailed in Section 3, we assume that the learner knows the statistical description P (x, y) of the classical input x and output y, as well as the quantum states \u03c1(x) in the training set. Thanks to the linearity of the 01 loss, this setting reduces to the discrimination of the average states\n\u03c1\u0304y =\n\u222b\nP (x|y)\u03c1(x)dx (10)\nthat describe the state of the system for classes y = + and y = \u2212. Note that the average in (10) captures the lack of knowledge at the learner concerning the classical input x. When the average states are not distinguishable, namely \u03c1\u0304+ = \u03c1\u0304\u2212, for instance because the two state classes uniformly populate the state space, it may still be possible to use copies to make the average states distinguishable \u2013 see Section 1.9.3 for an example concerning the classification of entanglement. Intuitively, the more distinct states \u03c1\u0304+ and \u03c1\u0304\u2212 are, the easier it is to classify them. Under the 01 loss, the minimum average, or testing, loss LP(f\u2217) for V = 1 is given by\nLP(f\u2217) = 1 2 \u2212 1 4 \u2016\u03c1\u0304+ \u2212 \u03c1\u0304\u2212\u20161\n= 1\u2212 2\u2212Hmin(Y |Q)\n\u2264 1\u2212 2I(Y :Q)\u2212H(Y ), (11)\nwhich is conventionally expressed, as in the first equality, in terms of the trace distance \u2016\u03c1\u0304+\u2212 \u03c1\u0304\u2212\u20161 between the mixed states \u03c1\u0304+ and \u03c1\u0304\u2212 (see, e.g., [38]). The second equality shows that the minimum average loss can also be written using an information theoretic quantity, namely the so-called conditional quantum min-entropy Hmin(Y |Q) of the classical label Y given the quantum state Q representing the test input \u03c1\u0304y. The conditional quantum min-entropy is a variant of the von Neumann conditional entropy that relies on an alternative relative entropy metric [39]. Intuitively, it captures the residual uncertainty on the classical label y given access to one copy of the corresponding input quantum test state \u03c1\u0304y. The inequality in (11) demonstrates that the minimum average loss can be also bounded in terms of the von Neumann mutual information I(Y :Q), which is a measure of correlation between test input quantum state and classical label y. Accordingly, the minimum average loss decreases as more information can be extracted about label y from state \u03c1\u0304y. The optimal measurement for binary classification, which attains the minimum average loss LP , is known as the Holevo-Helstrom (HH) measurement, and it involves the eigendecomposition of the difference \u03c1\u0304+ \u2212 \u03c1\u0304\u2212 (see [39] for generalizations). 2) Unknown data-generation mechanism P, known states: In this second, more challenging, setting, studied in Section 5, the learner does not fully know the data-generation mechanism P . Specifically, it is not aware of the true joint classical distribution P (x, y), but it knows the quantum states \u03c1(x) in the training set. Accordingly, the learner has access to the abstract training set S = {(xn, yn, \u03c1(xn))}Nn=1 of N data samples, where the states \u03c1(xn) are known. Given the available information, the learner can choose a measurement that minimizes the training loss L(f,S). The resulting optimized measurement setting fS for binary classification turns out to be the HH measurement for the empirical quantum states\n\u03c1\u0304Sy = 1\nNy\nN\u2211\nn=1\n\u03b4y,yn\u03c1(xn) (12)\nfor the two classes y = \u00b1, where N\u00b1 is the number of samples with yn = \u00b1 in the training set (\u03b4y,yn equals 1 if y = yn and zero otherwise). The densities (12) provide an empirical estimate of the true per-class densities (10). Note that these matrices can be computed at the learner given the learner\u2019s knowledge of the quantum states \u03c1(xn) in the training set. Using the decomposition (4), i.e., LP(fS) = L(fS ,S) + G(fS), the average loss LP(fS) obtained by the optimized function fS can be related to the generalization error G(fS). Accordingly, a smaller generalization error, for a fixed training loss, guarantees a smaller average loss. Intuitively, the generalization error must decrease with the data set size N , as the empirical density matrices (12) approximate increasingly well the true density matrices (10). Furthermore, for a fixed value N , the quality of this approximation must depend on how much the density matrices \u03c1(x) change with input x. In fact, a larger variability generally requires more observations, i.e., a larger N , in order to ensure an accurate estimate of the per-class density matrices (10).\n13\nTo formalize this intuition, it turns out that a useful definition of correlation between input x and state \u03c1(x) is given by the Re\u0301nyi quantum mutual information\nIS1/2(X :Q) = 2 log2\n\nTr\n\u221a \u221a \u221a \u221a 1\nN\nN\u2211\nn=1\n\u03c1(xn)2\n\n . (13)\nNote that this mutual information is evaluated with respect to the empirical distribution of the data points in the abstract training set S. With this definition, the generalization error can be shown to decrease with the training data set size N as\nG(fS) / O\n\n\n\u221a\n2I S 1/2 (X:Q)\nN\n\n . (14)\nWhen V copies of the test states are employed, the above bound increases by a factor O( \u221a V ). The above result is proved by analyzing the so-called Rademacher complexity of classes involving unconstrained generalized measurements or unconstrained observables \u2013 see Section 5. The approximate inequality (14) shows that generalization with few training examples N is possible when either the input space is small or the quantum states \u03c1(x) do not depend too much on the input x. 3) Unknown data-generation mechanism P, unknown states: We now consider a learner lacking information about both the classical joint distribution P (x, y) and the quantum states \u03c1(x) in the training set. As discussed, in this case, the learner\u2019s performance depends also on the number of copies, S, of the quantum states in the training data set SS . To evaluate the effect of the number S of copies, we can adopt the decomposition (9) of the average loss, i.e., LP(fSS ) = L(fS ,S) + G(fSS ) + ESS , in which the knowledge gap ESS specifically captures the impact of the availability of a finite number of copies. We know from the previous discussion that the HH measurement defined by the empirical average states (12) is optimal, so a learner should in principle try to implement this solution even without knowledge of such states. There are both inductive and transductive strategies to approximate the HH measurement. The simplest inductive strategy is based on quantum state tomography. The learner first uses the S-copy training set to estimate the quantum densities (12), and then it defines a HH measurement based on the reconstructed states. As we discuss in Section 5, this results in a knowledge gap due to the errors in the tomographic reconstruction of such states that grows as ESS = O(d/ \u221a NS), where d is the Hilbert space dimension. If the states in the training set are pure, this can be reduced to ESS = O( \u221a\nd/S). Accordingly, tomographic approaches are not practical for large dimensional systems, since the dimension of the Hilbert space d grows exponentially with the number of qubits. A more favourable scaling in terms of the Hilbert space dimension d can be obtained with a transductive strategy that uses a combination of phase estimation and the state exponentiation algorithms[35] \u2013 see Section 4. This strategy requires a coherent manipulation of the S-copy training data set SS and a new test input \u03c1(x) to produce the binary classification y. Ignoring logarithmic corrections, the knowledge gap scales as ESS = O((NS)\u22121/3), without any dependence on the dimension d \u2013 see Section 5 for a more precise analysis. Therefore, the transductive strategy is the preferred choice when the Hilbert space dimension d is large, such as in many-qubit systems. That said, the scaling of the knowledge gap with NS is worse than for the tomography-based inductive approach. Overall, using the decomposition (9) of the average loss and recalling the inequality (14) for the generalization error, depending on the specific setting given by number of data points, N , number of copies, S, and correlation between input and input quantum state, IS1/2(X :Q), the generalization error or the knowledge gap terms may dominate. Computing Generalization Bounds. As a note regarding the computation of the generalization error bound (14), we observe that, when all the states in S are pure, we can alternatively express the mutual information IS1/2(X :Q) as\nIS1/2(X :Q) = H1/2(\u03c1\u0304S) = H1/2(\u03b7S), (15)\nwhere H\u03b1(\u03c1) = (1\u2212\u03b1)\u22121 log2 Tr[\u03c1\u03b1] is the quantum Re\u0301nyi entropy, \u03c1\u0304S = 1N \u2211N n=1 \u03c1(xn) is the average state from the training set, H\u03b1(p) = (1\u2212 \u03b1)\u22121 log2 \u2211 i p \u03b1 i is the classical Re\u0301nyi entropy, and (\u03b7S)j are the eigenvalues of the kernel matrix[8]\n\u03b7S = spectrum(KS), (KS)nm = 1\nN \u3008\u03c8(xn)|\u03c8(xm)\u3009, (16)\nwhich satisfies KS \u2265 0 and TrKS = 1. Accordingly, vector \u03b7S defines a probability distribution, and H1/2(\u03b7S) is its Re\u0301nyi entropy. The second equality in Eq. (15) follows by defining the state |\u03c8AB\u3009 = N\u22121/2 \u2211\nn |n\u3009A |\u03c8(xn)\u3009B, and noting that the entanglement of the two subsystems A and B is equal, namely H1/2(A) = H1/2(B).\n14\nSince the overlap \u3008\u03c8(xn)|\u03c8(xm)\u3009 between two quantum states can be evaluated efficiently with a shallow quantum circuit, the calculation of the vector \u03b7S is possible even for a large Hilbert spaces. This allows one to estimate the bound (14) based on data."
        },
        {
            "heading": "1.8. Overview of Results with Constrained Operations",
            "text": "In this subsection, we analyze settings in which the space of inference functions available at the learner is constrained. As we will demonstrate, and as is the case also for classical machine learning, controlling the capacity of the space of inference functions is in practice essential in order to guarantee generalization. This can be done by leveraging domain knowledge about the problem under study, such as symmetries or locality properties of the involved quantum states. To understand the connection between model capacity and generalization, let us revisit the bound in (14). As we will detail in the next section, this bound is derived based on uniform-deviation arguments from statistical learning theory, which quantify how different the average loss and the data set loss can be over the entire space of possible inference functions. More precisely, uniform-deviation bounds gauge the worst-case generalization error G(f) over all possible inference functions f \u2208 F in the function class F . If the function class is very large, the uniform deviation can be large even when the generalization error G(fS) of the inferred function fS itself is not. Consequently, uniformdeviation bounds may predict overfitting, and hence poor generalization, even when the model generalize well in practice. As a relevant example, for classical neural networks, uniform-deviation bounds on the generalization error scale with the number of trainable parameters in network. Accordingly, the bounds predict large generalization errors for deep neural networks with billions of parameters, thereby failing to explain its exceptional generalization performance observed in practice. Recent variants of uniform-deviation bounds[40] obtain tighter bounds on generalization error by imposing suitable constraints on the set of inference functions. Indeed, all successful applications of classical machine learning models rely on some understanding of the structure of the data to support the selection of a class of models that matches well the data-generation mechanism. A similar approach can be employed in quantum learning methods. In fact, all the strategies from Table II, with the exception of the Helstrom classifier, introduce some constraints in the set of measurements or in the set of observables. For instance, in quantum neural networks, the circuit ansatz effectively constrains the inference function, quantum kernel methods constrain the underlying observable being evaluated with a regularization term, while quantum reservoir computing and classical shadows use fixed randomized measurements followed by classical postprocessing. As in the classical case, constraints favour generalization by limiting the expressivity of the inference function, namely by selecting a reduced function class, which might result in a larger training error. However, in the quantum case, constraints also limit the learner\u2019s ability to discriminate quantum states, and might also increase the knowledge gap. In this section, we assume that generalization error is the dominant source of error, and investigate the impact of such restrictions on the generalization performance of a quantum model. In order to use the state-dependent bounds from the previous sections, we map the constraints on sets of measurements or observables to a data processing of the quantum inputs, i.e., a completely positive trace preserving quantum map N : Q 7\u2192 QN , mapping the original state space Q into a lower-dimensional space QN . By the data processing inequality, this operation reduces both the mutual information between quantum state and output label, which dictates the minimum average loss (11), and the mutual information between quantum state and classical input, which in turn determines the bound (14) on the generalization error. In particular, we have the inequalities I(Y :QN ) \u2264 I(Y :Q) and I1/2(X :QN ) \u2264 I1/2(X :Q). Therefore, constraining the output space generally causes the minimum average loss to increase, since some important information about the output label may be lost, while the generalization error is expected to decrease. Choosing a model class that is tailored to the problem at hand should hence ensure that only information that is not relevant to the classification task is discarded by the channel that describes it. This situation may be accounted for by imposing the conditions\nI(Y :QN ) = I(Y :Q), I1/2(X :QN ) \u226a I1/2(X :Q). (17)\nIn this way, the capacity of the model to minimize the average loss is not impaired, while reducing the generalization error and hence the data complexity. To this end, prior domain information can be leveraged to define a map N that retains only infomation that is critical for the task of interest. We next provide three examples. Leveraging symmetries: As a first example, suppose that the states are known to be invariant with respect to some symmetry group. The problem can be mapped, without any approximation, into a lower dimensional space\n15\nwhere learning is simpler. For instance, it is possible to define a parametric quantum circuit with the same symmetries as the problem [41], which can be used to classify the states with fewer parameters and less training data. Moreover, if the actual data or measurement strategy breaks the symmetry by introducing some noise, a \u201csymmetric classifier\u201d will directly ignore these differences without first having to learn that these differences are indeed irrelevant for classification. Therefore, as can be expected, exploiting prior information allows learning with less data. We will discuss an explicit example in Section 1.9.3, where we study the problem of classifying entanglement. Leveraging locality: As another example, suppose that the states can be classified using k-local observables, namely with observables acting at most on k qubits. We may exploit this prior knowledge to define the maps\nN\u0304 (\u03c1(x)) = 1 Nk\n\u2211\nsk\nTr s\u0304k [\u03c1(x)], N\u2295(\u03c1(x)) = 1 Nk\n\u2295\nsk\nTr s\u0304k [\u03c1(x)], (18)\nwhere sk are all the possible subsets of k qubits, Nk is the total number of such subsets, and Trs\u0304k is the partial trace over all qubits, except those in sk. Then, since the mutual information I1/2(X :QN ) is at most equal to the logarithm of the Hilbert space dimension, we get\nI1/2(X :QN\u0304 ) \u2264 k, I1/2(X :QN\u2295) \u2264 k log2(Nk). (19)\nIn other words, if the states can be classified using local observables, at most N \u2248 O(2k) samples are required to ensure low generalization error when I(Y :QN\u0304 ) \u2243 I(Y :Q) \u2013 respectively O(Nkk ) samples when I(Y :QN\u2295) \u2243 I(Y :Q). A variant of the projection N\u2295 was employed in Ref. [10] to define a hybrid quantum kernel method with favorable generalization properties. Parametric Quantum Circuits: Complexity can also be limited by adopting a quantum neural network (QNN) with a suitably small number of qubits and quantum gates (see Section 1.4). Details for this scenario can be found in Section 3 and Section 4. Recent works [12, 13, 42] have characterized the generalization error of QNNs. Notably, the work [13] shows that the generalization error for QNNs can be bounded as\nG(fS) / O (\u221a\nNg logNg N\n)\n, (20)\nwhere Ng is the number of trainable gates in the QNN."
        },
        {
            "heading": "1.9. Applications",
            "text": "In this subsection, we review three applications of the concepts reviewed in this section."
        },
        {
            "heading": "1.9.1. Parametric Quantum Circuits",
            "text": "As an explicit example, here we focus on the embedding of classical data xn into a quantum state via a parametric quantum circuit. It was shown [43] that, for many popular choices from the literature, the state produced by the circuit can be expressed as a Fourier-like series\n|\u03c8(x)\u3009 = 1\u221a |\u2126|\n\u2211 \u03c9\u2208\u2126 ei\u03c9x |\u03c6\u03c9\u3009 , (21)\nwhere the different frequencies \u03c9 belong to a set \u2126 with cardinality |\u2126|. In Appendix C we derive an expression similar to (15), but where the probability distribution \u03b7S is replaced by the eigenvalues of the |\u2126| \u00d7 |\u2126| \u201cFourier matrix\u201d FS\n\u03d5S = spectrum(FS), (FS)\u03c9,\u03c9\u2032 = 1 N |\u2126| \u2211\nn\neixn(\u03c9\u2212\u03c9 \u2032). (22)\nSince the entropy cannot be larger than the logarithm of the dimension, we get\nB(S) = 2HS1/2(\u03d5S) \u2264 |\u2126|, IS1/2(X :Q) \u2264 log2(|\u2126|), (23)\n16\nthus recovering a known result from the literature [12, 44]. This shows that a large number of Fourier frequencies in the model entails that more data are required to ensure generalization. Note though that in [12], \u2126 is the set of frequency differences, rather than the set of frequencies. Our result (23) differs from the ones in the literature, as it can be applied to classification, rather than regression problems. Different bounds on |\u2126| from [12] for different circuit structures can be employed, thanks to (23), to classification problems as well. For instance, for strategies based on the repetition of arbitrary Pauli encodings, it was found that for D-dimensional data x and Nge encoding gates, the cardinality of frequencies scales as |\u2126| = O((Nge/D)D)."
        },
        {
            "heading": "1.9.2. Learning to Classify Phases of Matter",
            "text": "Quantum phase transitions describe the abrupt change, in the thermodynamic limit, of the ground state of a quantum many-body system when some external parameters x are continuously varied across a critical region[45]. Common examples of second-order quantum phase transitions can be detected by measuring a local observable, called order parameter. In spin systems, this order parameter is often the magnetization\nM\u03b1 =\n\u2329 \u2211\nn\n\u03c3\u03b1n\n\u232a\n\u03c1\n= Tr ( \u03c3\u03b1N\u0304 [\u03c1] ) , (24)\nwhere \u03b1 = x, y, z and \u03c3\u03b1n are the Pauli operators on the nth spin, and in the second inequality we used the map from (18) to transform the local observables on an n qubit state into a single-qubit observable on the projected average state N (\u03c1), with k = 1. Suppose, on the other hand, that the order parameter is unknown, but that physical wisdom tells us that it is expected to be a combination of k-local observables. By generalizing Eq. (24), we can turn the problem into a quantum learning one with states N (\u03c1(x)), and by the analysis of the previous section we expect that this model can be learnt efficiently, using a number of data which scales at most as 2k. This was also observed in numerical experiments with a simple Ising model[8], where, without any projections, the only errors were close to the phase transition point, where quantum fluctuations may confuse the learner without an explicit projection. Other examples were considered in the literature, e.g. [46\u201348]. For quantum phase transitions with global order parameters, or for topological phase transitions, generalization is less trivial, and whether the information theoretic bounds can provide an explanation is an open question. It was found that, in some cases [49], a learner needs \u201cnon-linear\u201d functions of the density matrix, e.g., observables acting on c copies Tr[A\u03c1\u2297c], but also that k-local reduced density matrices may suffice. We can model this observation by generalizing Eq. (18) as N\u2295(\u03c1(x)\u2297c), where the input \u03c1(x)\u2297c is the space with sufficient information to detect the phases with a non-linear function of the density matrix, and the map N\u2295 performs a projection onto a reduced space that contains a combination of linear and non-linear functions of the reduced density matrices. If such a projection contains enough information about the phases, then generalization is expected with at most O((const)k) data."
        },
        {
            "heading": "1.9.3. Learning to Classify Entanglement",
            "text": "As another instructive example, we focus on the toy problem of classifying two classes of quantum states on a composite Hilbert space AB, namely separable states and maximally entangled states. We generate them as\n|\u03c8sep(x)\u3009 = UA(x) |0\u3009A \u2297 UB(x) |0\u3009B , |\u03c8ent(x)\u3009 = [UA(x)\u2297 UB(x)] |\u03a6\u3009AB , (25)\nwhere |\u03a6\u3009AB = \u2211d n=1 |n\u3009A |n\u3009B / \u221a d is a maximally entangled state, |0\u3009 is a reference state in a d-dimensional Hilbert space, and x models the parameters in the (possibly different) local unitaries UA and UB. We assume that UA and UB form a 2-design, so their average is indistinguishable from a Haar (uniform) average up to the second moments [24]. By construction, linear observables cannot discriminate the two states. Indeed, their average is the same, i.e.,\n\u222b dx |\u03c8sep(x)\u3009\u3008\u03c8sep(x)| = \u222b dx |\u03c8ent(x)\u3009\u3008\u03c8ent(x)| = 1 \u2297 1 d2 , (26)\nnamely they both uniformly \u201cpopulate\u201d the Hilbert space. To make their average distinguishable, we need to focus\n17\non non-linear functions, and the simplest one involves two copies. Therefore, we define the available states as\n\u03c1(x) =\n{\n|\u03c8sep(x)\u3009\u3008\u03c8sep(x)|\u22972 , y = +1, |\u03c8ent(x)\u3009\u3008\u03c8ent(x)|\u22972 , y = \u22121.\n(27)\nWith such two-copy states, we can construct a simple measurement that can unambiguously distinguish between the two classes. In fact, recalling that, per Eq. (25), each copy consists of two subsystems (A and B), we can carry out a SWAP measurement (see Figure 4) on the A subsystems of both copies of our test state. If the state we are testing is |\u03c8sep(x)\u3009\u22972, the measurement result will always be 1, whilst if it is |\u03c8ent(x)\u3009\u22972, the result will only be 1 with probability 1/d. Given that this is a problem that admits a simple analytical solution \u2013 namely the SWAP measurement \u2013 for any Hilbert space dimension, it is an interesting question to address how difficult it is for a machine learning model to learn such classification rule without knowing the structure of the problem. From Eq. (15), we know that generalization error bounds depend on the entropy of the average state, which can be computed explicitly, using Haar integrals, to get 2I1/2(X :Q) = O(d2). Therefore, by (14), for multi-qubit systems where d = 2n grows exponentially with the number of qubits n, the number of samples to learn to classify entanglement grows exponentially with n. In stark contrast, if we have prior information about the structure of the problem, applying our physical understanding of the situation, we may be able to drastically simplify the learning problem. In particular, since entanglement is invariant under local operations and classical communications[50], we can directly average out all local details via the map NA \u25e6 NB(\u03c1) where NA = NB = \u222b dUU\u22972\u03c1U\u22972,\u2020 are local twirling channels. Owing to the Schur-Weyl duality, such channels can be decomposed as projections onto the symmetric and anti-symmetric subspaces. In other words, without losing relevant information, we can transform the problem via the map N (\u03c1) = |0\u3009\u30080|Tr[PS\u03c1]+ |1\u3009 \u30081|Tr[PA\u03c1] where PS/A are the projectors onto the symmetric/antisymmetric subspaces. This map satisfies (17), namely it removes a large number of irrelevant degrees of freedom, mapping a d2-dimensional state to a single-qubit state, without altering the capacity to predict the class Y . Since, after the mapping, we have a single-qubit state regardless of the initial dimension, the mutual information is a constant, I1/2(X :QN ) = O(1), and we recover the expected result that few training data are required to learn this process."
        },
        {
            "heading": "1.10. Organization of the Paper",
            "text": "In the rest of the paper, we will review results on the generalization performance of quantum machine learning in both settings of supervised and unsupervised learning, and for both known and unknown quantum states. The general goal is to understand how the generalization performance is affected by the properties of available data, the number of copies of each datum, the performance loss criterion, and the class of inference operations f under optimization. Results are typically given in the form of scaling laws, describing the decrease of the different errors shown in Figure 2 as a function of parameters such as size of the training set, or the number of copies. To start, Section 2 presents the necessary background on classical statistical learning theory, the branch of statistics that addresses the dependence of generalization on the properties of data, loss measure, and inference operation class. Section 3 describes the baseline quantum information processing task of quantum state discrimination with known quantum states. This problem is introduced as a benchmark, as it corresponds to an idealized situation for quantum supervised learning in which the statistical description P of the test input is available. Section 4 studies quantum state discrimination of unknown quantum states, but known data distribution. In this section, we also discuss transductive learning solutions, which are compared to more conventional inductive learning methods. Section 5 covers quantum supervised learning problems, where both the data distribution and the quantum states are unknown. Here we merge the techniques from Sections 2 and 4 to study the generalization error and optimality gap, due to the unknown data distribution, and excess errors due to the unknown quantum states. Section 6 moves on to the generalization analysis of quantum unsupervised learning for generative modelling, distinguishing applications where quantum generative models either approximate a classical description P (x) or a quantum state \u03c1. Conclusions and outlooks are drawn in Sec 7."
        },
        {
            "heading": "2. Classical Statistical Learning Theory",
            "text": "In this section, we elaborate on generalization theory for classical supervised and unsupervised learning within the general framework presented in the previous section. Most of the material is adapted from [7, 40, 51\u201353]. This material will form the background for the extensions to quantum systems to be studied in the following sections.\n18"
        },
        {
            "heading": "2.1. Supervised Learning",
            "text": "In supervised learning, a data instance is given as a pair (x, y) of input feature x and output label y, which are related according to an unknown input-output mapping. In a deterministic setting, the input-output relationship is assumed to be described by an unknown function y = f(x); while, in a stochastic setting, it is described by an unknown conditional probability distribution f(y|x) of label y given input x. From now on, we use the same symbol f to denote either deterministic or stochastic mappings without making an explicit distinction. The goal of supervised learning is to infer the unknown mapping from input to output based on the observation of a finite training set of data instances. This requires generalizing the observed input-output pairs outside the training set, a process known as induction (see, e.g., [54]). Depending on whether the output label y is discrete or continuous, supervised machine learning problems amount to as classification or regression tasks. In classification, the set of outputs is discrete, e.g., the next word in a text, while in regression the output is continuous, e.g., the next market value of a certain stock. From a statistical perspective, we assume that each data instance (x, y) is drawn from an unknown probability distribution P (x, y), with training and test data instances being independent samples from P (x, y). The first step of the learning process is to fix a function class, known as model class, F of candidate input-output mappings, or models. This class can, for example, consist of neural networks with trainable parameters. Learning then amounts to finding the best function f \u2208 F that can most faithfully predict the output label of any feature input x, with the quality of the prediction evaluated with respect to the underlying, unknown, distribution P (x, y). The predictive performance of a function f \u2208 F on a data instance (x, y) can be measured via a loss function \u2113(f, x, y) that introduces a penalty dependent on the degree to which the predicted output f(x) is different from the real output y. Common losses for regression with deterministic function classes include the square loss \u2113(f, x, y) = (y\u2212f(x))2 and its extension to multivariate outputs via \u21132-norms. For classification, typical loss functions include the probability of misclassification \u2113(f, x, y) = \u2211\ny\u0304(1\u2212 \u03b4y\u0304,y)f(y\u0304|x) = 1\u2212 f(y|x) when the function class is stochastic; and the hinge loss \u2113(f, x, y) = max{0, 1\u2212 yf(x)} with y = \u00b11 for binary classification problems with a deterministic function class. The goal of the supervised learner is to find the candidate function that minimizes the average loss\nL(f) = E (x,y)\u223cP [\u2113(f, x, y)], (28)\nwhich is the average loss with respect to the abstract data distribution P (x, y). We denote as\nf\u2217 = argmin f\u2208F L(f) (29)\nthe optimal function that minimizes L(f). Recall that this optimal solution is a conditional probability distribution for stochastic model classes. Note that, as compared to the general notation introduced in the previous section we drop the dependence of the average loss (28) on the statistical description P , here the distribution P (x, y), in order to simplify the presentation. Finding the optimal solution (29) is impossible in practice, since the average loss (28) cannot be evaluated due to the unknown data distribution P (x, y). In fact, the only information available to the learner about P (x, y) is via a data set S = {(xn, yn)}Nn=1 of N examples sampled i.i.d. according to P (x, y). Hence, the learner replaces the average loss in (29) with an empirical training loss\nL(f,S) = 1 N\nN\u2211\nn=1\n\u2113(f, xn, yn), (30)\nwhich is the empirical average of the losses incurred on examples in the training set S. This results in the following approximation to the optimal function f\u2217:\nfS = argmin f\u2208F\nL(f,S). (31)\nIf the training loss L(fS ,S) corresponding to the learnt function fS is sufficiently small, it indicates that the assumed function class F is sufficiently complex to capture the input-output relationship. However, ensuring small training loss L(fS ,S) on observed training data does not ensure that the learnt function performs well on previously unseen data drawn from distribution P (x, y). The learnt function fS is said to generalize if it also performs well on new, previously unseen data Stest drawn from P (x, y), i.e., more precisely, if the loss L(fS ,Stest), is also small.\n19"
        },
        {
            "heading": "2.2. Generalization Error in Supervised Learning",
            "text": "As discussed in the previous subsection, the ability to generalize is a crucial desired performance criterion for any machine learning algorithm. If the learnt function performs well on the training set, but poorly on test set, we say that the function overfits the training data and fails to generalize. In general, the generalization ability of a learnt function fS is determined by three main factors:\nF.1 The function class F should be sufficiently complex to contain a \u201cgood\u201d approximation of the optimal, unknown input-output mapping.\nF.2 The training set S should be sufficiently comprehensive, so that the empirical training loss L(f,S) provides a \u201cgood\u201d approximation to the (unknown) average loss (28).\nF.3 The learning algorithm that optimizes the empirical training loss L(f,S) should be sufficiently powerful to yield a solution close to the training loss-minimizing model fS .\nGiven a model class F , one is generally interested in finding a model f that ensures a small optimality gap E(f) as defined in (1). For the learnt model fS , which minimizes the training loss, the optimality gap (1) can be decomposed as\nE(fS) = L(fS)\u2212 L(fS ,S) \ufe38 \ufe37\ufe37 \ufe38\n:=G(fS ,S)\n+L(fS ,S) \u2212 LP(f\u2217) \ufe38 \ufe37\ufe37 \ufe38\n:=A(fS ,S)\n, (32)\nwhere the first term, G(fS ,S), is the generalization error of the empirical inference operation, and the second term A(fS ,S) is the excess empirical error. In fact, for any model f \u2208 F , the generalization error G(f,S) can be upper bounded as\nG(f,S) = L(f)\u2212 L(f,S) \u2264 D(F ,S) := sup f\u2208F |L(f)\u2212 L(f,S)|, (33)\nby maximizing over all models in the class F . In (33), we have defined the uniform deviation D(F ,S) of the function class F with respect to data set S. On the other hand, an upper bound on the excess empirical error A(fS ,S) can be derived as\nA(fS ,S) = L(fS ,S)\u2212 L(f\u2217,S) + L(f\u2217,S) \u2212 L(f\u2217) \u2264 D(F ,S) (34)\nwhere we have used the uniform deviation bound, as well as the inequality L(fS ,S) < L(f\u2217,S), which follows since fS minimizes the training loss. Note that, for another model f , which may optimize the training loss only approximately, the last inequality in (34) would not hold, and one should account for the contribution to the optimality gap caused by a non-ideal optimizer. Using (33) and (34) in (32), we have the following upper bound on the optimality gap of the learnt function fS ,\nE(fS) \u2264 2D(F ,S). (35)\nThe relation (35) suggests that the optimality gap can be controlled by ensuring that the uniform deviation D(F ,S) is sufficiently small. In the rest of this section, we describe a statistical complexity measure, the Rademacher complexity, that provides a way to quantify the uniform deviation. The Rademacher complexity RP (F) of a given function class F under the true data distribution P (x, y) is defined as\nRP (F) = ES [R(F ,S)], R(F ,S) = E\u03c3\n[\nsup f\u2208F \u2223 \u2223 \u2223 \u2223 \u2223 1 N N\u2211\nn=1\n\u03c3n\u2113(f, xn, yn) \u2223 \u2223 \u2223 \u2223 \u2223 ] , (36)\nwhere the first expectation is taken over the data S, whose each entry is drawn from distribution P (x, y); and the second expectation is over independent Rademacher variables \u03c3j that take values \u00b11 with equal probability. The quantity R(F ,S) is known as the empirical Rademacher complexity, and is a function solely of the model class F and of the training set S. While the empirical Rademacher complexity R(F ,S) can be evaluated on the basis of the available training data, the Rademacher complexity RP (F) requires averaging over the distribution P (x, y), and, as such, it cannot be computed.\n20\nHowever, in the large training data set limit, i.e., with large N , the two quantities become increasingly close. In fact, with probability higher than 1\u2212 \u03b4 for \u03b4 \u2208 [0, 1], when the loss is in [\u22121, 1] we have the inequality\n|RP (F)\u2212R(F ,S)| \u2264 O (\u221a log(1/\u03b4)\nN\n)\n. (37)\nSimilar bounds hold for more general losses. Furthermore, replacing the Rademacher variables with normally distributed variables \u03c3j \u2208 N (0, 1) yields the Gaussian Rademacher complexity [55], which shares similar properties with RP (F). Although faster rates are known for specific instances[40], the Rademacher complexity for a model class F typically scales as O(B(F)/ \u221a N), where B(F) is some constant that depends on the model class F . Using this result and (37), we can assume the approximate equality R(F ,S) \u2243 RP (F) \u2243 O( \u221a\nB(F)/N), which is increasingly accurate for large values of N . Note that the differences between R(F ,S) and RP (F) result in O(1) corrections to B(F) that are not significant for large N and will be omitted here. See Appendix A for more precise definitions. The Rademacher complexity provides tight upper and lower bounds on the uniform deviation, as summarized in Theorem A.4 in the Appendix. Specifically, using Theorem A.4 and the approximate equality between Rademacher complexity and empirical Rademacher complexity discussed above, we obtain that, with high probability, we have the approximate equalities\nD(F ,S) \u2243 R(F ,S) \u2243 RP (F) \u2243 O (\u221a\nB(F) N\n)\n(38)\nfor sufficiently large N . Using (38) into (33) and (35), we can finally conclude that the generalization error and optimality gap scale as\nG(f,S) \u2264 D(F ,S) \u2243 O (\u221a\nB(F) N\n) , E(fS) \u2264 2D(F ,S) \u2243 O (\u221a\nB(F) N\n)\n. (39)\nThe above inequalities show that the difference between the training and testing errors can be bounded via the same function B(F) of the model class F up to O(1) constant factors. While, thanks to Theorem A.4, the bounds on the uniform deviation are tight, the bounds in (39) may be loose due to the maximization over all functions in the model class that underlies the definition of the uniform deviation (33). For instance, in neural networks it is known that the Rademacher complexity, via function B(F), grows polynomially with the number of network parameters. So, for deep networks with trillions of parameters, one generally has the strong inequality B(F) \u226b N , making the bounds (39) vacuous. Indeed, deep learning is well known to contradict the common statistical wisdom based on Occam\u2019s razor, which stipulates that, all else being equal, simpler models compatible with the data are preferred, since models with trillions of parameters are able to routinely get with little efforts both small training and testing errors. This discussion points to some of the limitations of the capacity-based analysis presented in this work. In particular, this type of investigation leaves out the impact of the training algorithm and of the data distribution [51, 54, 55]. Although the function class F is hugely complex for deep networks, when using gradient descent for training, the set of explored functions is a very small fraction of the entire function space, since only functions in the neighbourhood of the starting point are considered by gradient descent. Therefore, in practice, the size of the overall model class may not matter as much as the quality of solutions obtained in the vicinity of randomized initializations [56]."
        },
        {
            "heading": "2.3. Model Selection via Structural Risk Minimization",
            "text": "The capacity-based analysis outlined in the previous subsection can be leveraged for model selection. This approach is also known as structural risk minimization, and is the subject of this subsection. To elaborate, fix a hierarchy of function classes {Fr} with some notion of ordering such that the inequality r1 \u2264 r2 implies the inclusion Fr1 \u2286 Fr2 . For instance, we may define Fr as a set of neural networks with the norm of parameter vector bounded by r. Such constraints are common in the machine learning literature, e.g., in support vector machines (SVMs) or Lasso regression. For each constrained function class Fr, as discussed in this section, the learning problem is typically formulated as the minimization\nf rS = argmin f\u2208Fr L(f,S). (40)\n21\nThis constrained optimization problem is typically reformulated as the minimization of the regularized loss L(f,S)+ \u00b5rg(f) where \u00b5r > 0 is a Lagrange multiplier and g(f) is a suitable penalty function. As explained in the next paragraph, this reformulation is exact when strong duality holds, e.g., for convex loss functions and constraints [57]. As a notable example, consider a linear regression problem with model defined by a vector of parameters \u03b8 and the squared loss. The family Fr is defined by the constraint g(f) = \u2016\u03b8\u201622 \u2264 r. Accordingly, the training loss can be expressed as L(f,S) = \u2016X\u03b8 \u2212 y\u201622/N , where X is a matrix with rows given by the inputs xj and y is the vector of outputs yj . The regularized training problem is given as\n\u03b8\u03bb = argmin \u03b8\nL(f,S) + \u03bb\u2016\u03b8\u201622. (41)\nBy the Karush\u2013Kuhn\u2013Tucker theorem, a suitable choice of the Lagrange multiplier \u03bb ensures the satisfaction of constraint g(f) = \u2016\u03b8\u201622 \u2264 r [57]. In the limit \u03bb \u2192 0+, the solution of this problem converges to \u03b8\u2217 = X\u266fy, where X\u266f is the pseudo-inverse of X . In structural risk minimization, one optimizes the choice of the parameter r of the function class Fr by considering the following optimization problem [55]\nr\u2217 = argmin r\nL(f rS ,S) + pr, (42)\nwhich aims to find an optimal tradeoff between the empirical risk and a complexity penalty term pr. The final optimal function is then taken as fS = f r\u2217 S . The upper bound on the generalization error in (39) gives a natural choice of the penalty term as pr = B(Fr). For example, if the function class Fr consists of L-layered deep neural networks with r-bounded Frobenius norm of weight matrices in each layer, then the penalty term B(Fr) is known to behave as rL \u221a\nL/N [58]. Finally, in the zero noise limit, namely when the mapping between x and y is deterministic, overparametrized neural networks are complex enough to ensure zero training error, while zero generalization error is possible as long as the complexity r grows sufficiently slowly so that it converges to zero for large number of samples."
        },
        {
            "heading": "2.4. Unsupervised Learning",
            "text": "In this section, we study unsupervised learning problems in which each data instance contains a single vector x, assumed to be drawn from an underlying, unknown, distribution P (x). While the class of unsupervised learning problems encompasses tasks as diverse as clustering and generative modeling, in this paper we focus on the fundamental task of estimating the distribution P (x). Accordingly, the model class F includes a set of candidate probability distributions for vector x. We use f \u2208 F to denote a probability distribution f(x) in the model class. Model classes F may encompass explicit or implicit models f(x). Explicit models provide a function f(x) that can be evaluated for any x. Examples of such models include normalized flows or quantum models with a classical likelihood based on expected values of observables (see, e.g., [4, 54]). In contrast, implicit models do not enable the evaluation of function f(x), but they only support sampling of vectors x from distribution f(x). Examples include generative adversarial networks, diffusion models [59], and single/ multi-shot quantum models [31]. In both cases, as for supervised learning, models are typically functions of a vector of trainable parameters. In this paper, given the focus on quantum models, we will assume implicit models that produce samples x \u223c f(x). We observe that explicit models are obtained in the limit in which one can produce an arbitrarily large number of samples from the model. In fact, in this case, such samples can be used to obtain an arbitrarily accurate estimate of the distribution f(x). To evaluate how far a candidate distribution f(x) is from the true distribution P (x), we need to introduce a measure of divergence between probability distributions. Let D(\u00b7, \u00b7) denote such a divergence measure, which may be selected as, among others, the Jensen-Shannon divergence, the Wasserstein distance, or the maximum mean discrepancy (MMD). For instance, the total variation distance,\nDTV(P, f) = sup A\u2286X\n\u2223 \u2223 \u2223P (A) \u2212 f(A) \u2223 \u2223 \u2223, (43)\nis the maximum absolute difference between the probabilities that the distributions P and f can assign to subsets A of the space X of vectors x. As another example, the class of integral probability metrics (IPMs) contains divergences of the form\nDIPM(P, f) = sup h\u2208H \u2223 \u2223 \u2223 E x\u223cP (x) [h(x)]\u2212 E x\u223cf(x) [h(x)] \u2223 \u2223 \u2223, (44)\n22\nwhere H is an appropriately chosen class of functions. The function h(\u00b7) optimized in (44) is known as the discriminator. This is because the optimal function h(\u00b7) in (44) should output different values for samples x \u223c P (x) generated from the true distribution and for samples x \u223c f(x) produced by the model, hence discriminating between the two distributions. Different classes H determine distinct IPMs. For instance, when the discriminator class H consists of the set of all 1-norm bounded and 1-Lipschitz functions, the IPM in (44) recovers the 1-Wasserstein distance between true and model-generated distributions [60]. When the discriminator class is the set of neural networks, the IPM (44) equals the neural-net distance[52]. Finally, when the set H is the set of functions within a unit ball in the reproducing kernel Hilbert space, the IPM equals the MMD. Having specified a divergence measure, the ultimate goal of unsupervised learning problems aimed at distribution estimation is to find the distribution in the function class F that is closest to the true distribution P (x) in divergence measure D(\u00b7, \u00b7), i.e.,\nf\u2217 = argmin f\u2208F D(P, f). (45)\nThe criterion D(P, f) can be considered as the counterpart of the average loss LP(f) in Section 1 or of the average loss (28) in supervised learning, in that its minimization represents the ultimate aim of the learning problem. Accordingly, we will refer to it as test divergence. Note, however, that, while the average loss (28) depends on the data-generating distribution only through an expectation, the divergence D(P, f) has a more general functional dependence on P (x), as seen, e.g., in the total variational distance (43). As for the average loss in supervised learning, finding the minimizer of the test divergence in (45) is impossible without access to the true distribution P (x). In fact, in a learning problem, one only has access to a finite number, N , of samples generated i.i.d. according to the true distribution P (x), which are included in the training set S = {xn}Nn=1. In addition, in unsupervised learning with implicit models, the distribution f(x) itself is not directly accessible, and the learner can only produce a generated set Sf = {xfm}Mm=1 of M examples sampled i.i.d. from the distribution f(x). Using training and generated sample data sets, one can estimate the divergence measure D(P, f) in different ways.\nA general approach is given by plug-in estimators that first obtain estimates P\u0302 (x) and f\u0302(x) of distributions P (x)\nand f(x), respectively, and then plug these estimates into the divergence metric to obtain the estimate D(P\u0302 , f\u0302). Alternatively, one could directly use the samples S and Sf to evaluate an estimate D\u0302(S,Sf ) of the test divergence D(P, f). For example, an empirical estimate of the IPM (44) can be obtained using the available samples as\nD\u0302IPM(S,Sf ) = sup h\u2208H\n\u2223 \u2223 \u2223 1\nN\nN\u2211\nn=1\nh(xn)\u2212 1\nM\nM\u2211\nm=1\nh(xfm) \u2223 \u2223 \u2223. (46)\nIn the following, we write D\u0302(S,Sf ) to denote either of these two types of estimates, which is referred to as training divergence. The learning problem can be formulated as the minimization of the training divergence\nfS,Sf = argmin f\u2208F\nD\u0302(S,Sf ). (47)\nIn the limiting case in which one can generate an arbitrarily large number of samples from the model, we recover, as mentioned, the setting with explicit model classes. In this case, the learner can directly leverage the distribution f(x), and we write the corresponding training divergence as D\u0302(S, f). The minimizer of the training divergence for explicit models \u2013 obtained equivalently in the limit of large M for implicit models \u2013 is accordingly defined as\nfS = argmin f\u2208F\nD\u0302(S, f). (48)"
        },
        {
            "heading": "2.5. Generalization Error in Unsupervised Learning",
            "text": "As explained in the previous subsection, we can interpret the empirical divergence D\u0302(S,Sf ) as the training loss accrued with model f , and the divergence D(P, f) as the average loss. Accordingly, as for supervised learning, we are interested in analyzing the optimality gap,\nED(fS,Sf ) = D(P, fS,Sf )\u2212D(P, f\u2217), (49)\n23\nof the learnt distribution fS,Sf with respect to the divergence measure D(\u00b7, \u00b7). This is defined as the difference in test divergences of the learnt distribution and of the optimal distribution f\u2217 in (45) from the true data distribution P (x). Following (32), the above optimality gap can be written as the sum of the generalization error of the learnt distribution fS,Sf with respect to divergence D(\u00b7, \u00b7), denoted here as\nGD(fS,Sf ,S,SfS,Sf ) = D(P, fS,Sf )\u2212 D\u0302(S,SfS,Sf ), (50)\nand of the excess empirical error of the learnt distribution fS,Sf , denoted as\nAD(fS,Sf ,S,SfS,Sf ) = D\u0302(S,SfS,Sf )\u2212D(P, f\u2217). (51)\nNote that the generalization error is now a function also of the samples Sf generated by model f . As in the inequalities (33) and (34) for supervised learning, the generalization error (50) and excess empirical error (51) can be upper bounded via the uniform deviation\nDD(F ,S,SF ) = sup f\u2208F |GD(f,S,Sf )| (52)\nof the function class F with respect to data sets S and SF = \u222af\u2208FSf . Overall, this results in the following upper bound on the optimality gap\nED(fS,Sf ) = GD(fS,Sf ,S,SfS,Sf ) +AD(fS,Sf ,S,SfS,Sf ) \u2264 2DD(F ,S,SF ). (53)\nIn general, the analysis of the uniform deviation depends on the choice of divergence measure, as well as on the specific unsupervised learning model considered. As shown in Appendix 1, for any IPM DIPM(\u00b7, \u00b7) in (44), under suitable assumption on functions h \u2208 H, as in Theorem A.4, we can apply the uniform deviation relation (38) to obtain the bound\nEIPM(fS,Sf ) \u2264 2DIPM(F ,S,SF ) \u2243 O (\u221a\nB(H) N\n) +O (\u221a\nB(F \u00d7H) M\n)\n, (54)\nwhere B(\u00b7) is defined in Section 2.2 as the function determining the dependence of the Rademacher complexity on the argument class (see (38)), and F \u00d7H = {(f, h) : h \u2208 H, f \u2208 F} denotes the combined function space of discriminators and models. This bound relates the optimality gap to the statistical complexity of the chosen classH of discriminators, as well as to the class of models F . In the limit of a large M , and in particular for explicit models, the dominant term in (54) is the first one, which depends only on the discriminator class H. This observation provides useful guidelines for the choice of the divergence to be used for training. For instance, as remarked in [52, 53], since the set of all 1-norm bounded and 1-Lipschitz class of functions is larger than the set of parameterized neural networks, the bound motivates the use of the neural-net distance, as opposed to the widely adopted 1-Wasserstein distance."
        },
        {
            "heading": "3. Quantum State Discrimination",
            "text": "Quantum state discrimination (QSD) is the task of deciding which state a certain test quantum system is in, given knowledge of the finite set of possible states. This corresponds to a special case of the quantum learning problem in which the generation mechanism P is fully known, and the output y is a label that determines the identity of the test state. We specifically focus on the case of binary QSD, in which the test state may be equal to one of two known states \u03c1+, identified with label y = +, and \u03c1\u2212, identified with label y = \u2212. Furthermore, it is known that the test state is equally likely to be either \u03c1+ or \u03c1\u2212. Note that in this case, there is no classical input x (see Section 1), and there is no need for training data, since the data-generation mechanism P is available to the learner. QSD underlies several applications of quantum information. For instance, in quantum cryptography and key distribution, one needs to assess whether the received state corresponds to the known state encoding bit 0 or bit 1[61]. As other examples, in quantum illumination [62] and quantum radars [63], a probe light is sent to illuminate an object and, based on the scattered quantum state received by the detector, the task is to decide whether a target was there or not. Extensions beyond the binary case were considered for barcode reading and pattern classification with quantum light [19] or channel position finding [64]. The quantum state discrimination routine may be summarised as follows\n24\nQSD.1 Obtain a classical description of the density matrices, \u03c1+ and \u03c1\u2212, describing the two classes of states, e.g. by using a theoretical model.\nQSD.2 Construct optimal or almost optimal measurement strategies, e.g.,based on Holevo-Helstrom measurements [65, 66] for binary classes or pretty good measurements for multiple classes [67\u201369].\nQSD.3 Apply such measurement strategy to discriminate an unknown state, assuming that it is in either state \u03c1+ or state \u03c1\u2212.\nFundamental works in the theory of state discrimination were performed by Helstrom [66] and Holevo [65], which considered the most general set of measurements. However, there are certain cases, e.g. in multi-qubit systems or in general in many-particle settings, where the most general operator can be highly non-local and difficult to implement. In all of these cases, it makes sense to constrain the available set of measurements and operations [70]. Another reason is that, as we will show, unconstrained measurements can lead to large generalization errors when the physical problem is described by many degrees of freedom. From the discussion in the previous section we know that in all of these cases it is useful to constrain the function class to improve the generalization performances. The fact that the states are known a priori makes QSD different from a learning problem, while forming the conceptual basis, as well as a key benchmark, for many learning algorithms. In this section, we focus on QSD for settings with two possible density matrices \u03c1+ and \u03c1\u2212 in order to set the necessary background for the learning problems studied in the following sections."
        },
        {
            "heading": "3.1. Single-Shot Discrimination with Fixed Measurements",
            "text": "We focus on known states, in the sense that a classical description of the density matrices \u03c1+ and \u03c1\u2212 is available to the discriminator. In this subsection, we fix an arbitrary measurement described by a positive operator-valued measure (POVM), i.e., by a set of positive semi-definite operators \u03a0k satisfying the equality \u2211\nk \u03a0k = 1 . We recall that projection matrices \u03a0k define a specific subclass of POVMs known as projective measurements. Given a fixed POVM, we study the problem of optimizing the binary decision on whether the system is in state \u03c1+ or \u03c1\u2212 on the basis of a single observation, or shot, of the state. In later subsections, we will address the case in which more copies of the state are available, allowing multiple measurements to be made on the system. By Born\u2019s rule, a measurementM = {\u03a0k}maps a density matrix \u03c1 to a random classical outcome k with probability\np\u03c1(k) = Tr[\u03a0k\u03c1]. (55)\nIn principle, as assumed in this subsection, the number of possible output values k is arbitrary, although, as we will see in the next subsection, for QSD with two possible states, it can be taken to be two without loss of generality. Given the random observation k \u223c p\u03c1(k) output by a single-shot measurement, a decision is made via a classical post-processing, i.e. a stochastic distribution f(y|k) that maps the measurement outcome k into the predicted class y \u2208 {+,\u2212}, indicating a decision for density \u03c1+ or \u03c1\u2212 for y = + or y = \u2212, respectively. By following the notation used in the previous sections, we will write f to denote either probabilistic or deterministic mappings from k to y. For a fixed measurement M, the optimal design of the mapping f(y|k) amounts to the problem of distinguishing the two probability distributions p\u03c1\u00b1(k) = p\u00b1(k) = Tr(\u03c1\u00b1\u03a0k) based on measurement output k \u223c p\u00b1(k). The corresponding optimization problem is defined in terms of the minimization of the average loss as\nL(f,M) = 1 2\n\u2211\nk,y\n\u2113(f, k, y)Tr[\u03a0k\u03c1y] = 1\n2\n\u2211\nk,y\n\u2113(f, k, y)py(k), (56)\nwhere we have assumed that the possible states \u03c1+ and \u03c1\u2212 are equally probable. A natural loss function \u2113(f, k, y) is the probability of error, also known as 0-1 loss, \u211301(f, k, y), which equals 0 when y = f(k) and 1 otherwise. For such loss function, the optimal decision, also known as Bayes decision rule, is deterministic, and it sets f(k) = +, if p+(k) \u2265 p\u2212(k) and f(k) = \u2212 otherwise. Accordingly, the Bayes decision rule can be written as\nf\u2217(k) = sign[p+(k)\u2212 p\u2212(k)]. (57)\nA loss is said to be Bayes consistent if the optimal decision function f\u2217 = argminf L(f,M) equals the Bayes decision rule (57). Bayes consistent losses with more desirable properties for numerical optimization include the hinge loss \u2113h(f, x, y) = max{0, 1\u2212 yf(x)}, used in support vector machines, as well as smooth approximation provided by the logistic loss \u2113\u03b3(f, x, y) = \u03b3 \u22121 log(1 + e\u2212\u03b3yf(x)) with margin parameter \u03b3 > 0.\n25"
        },
        {
            "heading": "3.2. Single-Shot Discrimination with Optimized Measurements",
            "text": "In the previous subsection, we have fixed the POVM and optimized over the classical post-processing map f(y|k) with the goal of minimizing an average loss criterion. In this section, we address the problem of optimizing the POVM. We start by observing that, when the POVM is optimized, there is no need for a post-processing map f(y|k), since the latter can be effectively integrated into the POVM. Given any POVM M = {\u03a0k} and any classical stochastic mapping f(y|k), we can define a new POVM {\u03a0+,\u03a0\u2212} that returns the same probabilities p\u03c1(y) = \u2211 k f(y|k)p\u03c1(k) of decisions y \u2208 {+,\u2212} via the matrices \u03a0y = \u2211\nk f(y|k)\u03a0k for y = {+,\u2212}. Therefore, one can implement an optimized POVM {\u03a0+,\u03a0\u2212} and use the output of the measurement as a decision without loss of generality. As a note, one way to implement such binary-valued POVMs is to apply a unitary operation U on the entire system followed by a Pauli measurement on a single qubit, whose binary outcome provides the predicted class. We write this measurement as MU where \u03a0y = U(|ky\u3009\u3008ky| \u2297 1 n\u22121)U \u2020, where k+ = 0 and k\u2212 = 1. For instance, when we consider the single-shot probability of error L01(f,M) as a loss and optimize over all possible measurements, the solution is given by\ninf M L01(M) =\n1 2 \u2212 1 4 \u2016\u03c1+ \u2212 \u03c1\u2212\u20161, argmin M L01(M) = {\u03a0HH\u00b1 }, \u03a0HH\u00b1 = 1 \u00b1 sign(\u03c1+ \u2212 \u03c1\u2212) 2 , (58)\nwhere the optimal measurement MHH = {\u03a0HH\u00b1 } is known as Holevo-Helstrom measurement. As another celebrated example, if one maximizes the mutual information between the true class index and the predicted outcome, the best POVM is not known in closed form, but the maximum mutual information can be upper bounded via the accessible information S(\u03c1++\u03c1\u22122 )\u2212 \u2211 y=\u00b1 S(\u03c1y)/2, where S = \u2212Tr[\u03c1 log2 \u03c1] is the von Neumann entropy."
        },
        {
            "heading": "3.3. Multiple Shot Discrimination via the Majority Rule",
            "text": "In the previous subsections, we have studied the case in which a single observation k is made on a copy of the system in state \u03c1+ or \u03c1\u2212. In this subsection, we study the case in which V copies of the quantum system, all in the same state \u03c1+ or \u03c1\u2212, are available. Accordingly, the system is in either of the many-copy states \u03c1 \u2297V + or \u03c1 \u2297V \u2212 .\nBy the no-cloning theorem [71], with a single copy of the system, it is not possible to carry out multiple independent measurements of the state, since one cannot produce copies of an unknown state. Moreover, the measurement postulates of quantum mechanics stipulate that after a measurement the quantum state generally \u201ccollapses\u201d into a different state, making it impossible to reuse the same system to obtain more measurements of the same state [72]. That said, in spite of the no-cloning theorem, different copies of the state can be made if the recipe to build such state is known, e.g. by lowering the temperature or applying some external control onto a system. When multiple copies V are available, by the discussion in the previous subsection, the optimal measurement in terms of probability of error, or 0-1 loss, is given by the Helstrom measurement. Using Fuchs-van de Graaf inequalities and properties of the fidelity function F (\u03c1+, \u03c1\u2212) = \u2016\u221a\u03c1+\u221a\u03c1\u2212\u20161, the resulting minimum probability of error can be upper bounded as\ninf f,M\nLV01(f,M) = 1 2 \u2212 1 4 \u2016\u03c1\u2297V+ \u2212 \u03c1\u2297V\u2212 \u20161 \u2264\nF (\u03c1+, \u03c1\u2212)V\n2 . (59)\nTherefore, the probability of error with an optimal measurement decreases exponentially with the number of copies, as long as the two states are not identical, i.e., as long as we have F (\u03c1+, \u03c1\u2212) 6= 1. However, the resulting optimal Helstrommeasurement in (58) is in general highly non-local, requiring the application of coherent measurements over all the V copies of the state. Simpler adaptive strategies are known to be optimal only for discriminating pure states [73]. In the rest of this subsection, we explore a simple, suboptimal, measurement strategy based on independent, local, measurements of each copy followed by a majority vote. Accordingly, we consider performing independent measurements, possibly at different times, of the different copies, without having to physically build many copies of the state in parallel. To elaborate, suppose that each local measurement applies some arbitrary binary measurement M = {\u03a0\u00b1}. Based on local measurement M, the majority vote over V copies can be described by a binary POVM MM,Vmaj = {\u03a0V\u00b1} applied on the V copies. Specifically, POVM MM,Vmaj applies the local measurement M onto all copies of the state, obtaining the random, i.i.d., outputs results y1, . . . , yV \u223c p\u03c1(y), with yj \u2208 {\u00b1}, and then outputs the class \u00b1 depending on the majority of outcomes. To avoid the possibility of ending in a draw, we assume that V is odd for simplicity.\n26\nLet us denote as p+ = Tr[\u03a0+\u03c1+] the probability that a local measurement returns the correct decision + when the true state is \u03c1+. The following discussion would equally apply to the case in which the true state is \u03c1\u2212 by swapping the signs. The probability of outputting the correct decision + when the true state is \u03c1+ is given by the binomial distribution\npV\u03c1+(+) = Tr[\u03a0 V +\u03c1 \u2297V + ] =\n\u2211\nv>V/2\n( V\nv\n)\nps+(1\u2212 p+)V \u2212v = 1\u2212 CVV/2(p+), (60)\nwhere we have defined CVk (p) = \u2211 v\u2264k ( V v ) pv(1 \u2212 p)V \u2212v as the cumulative distribution function of the binomial distribution. We now show that, under mild conditions on the probability p+ of correct detection of each local measurement, the majority rule also yields a probability of error that decreases exponentially with the number of copies, V , as for the corresponding probability (59) of the optimal, global, measurement. To this, we observe that, for large V , we have the approximation [74] Ck(p) \u2248 exp(\u2212V DKL(k/V, p)) for k/V < p, where DKL(a, p) = a log(a/p)+ (1\u2212 a) log((1\u2212 a)/(1\u2212 p)) is the binary Kullback-Liebler (KL) divergence. Therefore, as long as the condition\np+ > v + 1\nV =\n1 2 +O(V \u22121) (61)\nholds, where v is the integer such that V = 2v + 1, then the KL divergence term is positive, and the probability of correct detection (60) converges exponentially quickly to 1 as a function of the number of measurement shots V . Note that, as compared to the probability (59) for optimal global measurements, the exponent of the probability of error is generally suboptimal. We conclude that, even without the optimal global measurements (58), provided that the local measurement is able to distinguish the state with probability (61) above chance, i.e., larger than 1/2, perfect discrimination can be obtained in the limit of many measurement shots V ."
        },
        {
            "heading": "3.4. Multiple Shot Discrimination via Expected Value of an Observable",
            "text": "In the previous subsections, we have assumed discrimination models based on POVMs. In this subsection, we consider a conceptually distinct family of discriminators that are based on the expected values of an observable. Instead of optimizing a POVM, such schemes hence optimize over observables. As we will see, for some specific penalties this optimization can be formalized via the representer theorem, creating a link between quantum discriminative models for QSD and kernel-based methods."
        },
        {
            "heading": "3.4.1. Problem Formulation",
            "text": "To elaborate, consider an observable defined by a Hermitian operator A and denote the expected value of observable A over state \u03c1 with \u03c1 \u2208 {\u03c1+, \u03c1\u2212} as \u3008A\u3009\u03c1 = Tr[A\u03c1]. We study the class of decision functions of the form\ny = sign(\u3008A\u3009\u03c1y ). (62)\nIn order to implement and optimize the observable A, one typically relies on a linear decomposition of observable A into operators that are easier to realize. A first approach is to decompose the observable in terms of a POVM {\u03a0k} as A = \u2211\nk ak\u03a0k, where ak are real numbers. This way, one can implement an observable-based predictor in the same way as for the POVM-based predictors studied earlier in this section. To this end, applies the POVM {\u03a0k}, and considers the random variable ak \u223c Tr[\u03a0k\u03c1] as the output of a measurement on the system. By averaging this random variable one obtains the expectation in (62). A natural decomposition of this type is obtained via the spectral decomposition of operator A. However, this becomes impractical for large Hilbert spaces, since the eigenprojectors of an operator A are generally non-local. Alternative decompositions can rely on more convenient bases for the operator space that consist of operators with locality properties. As a notable example, with the basis of multi-qubit Pauli operators P\u03b1, an observable A can be decomposed as A = \u2211\n\u03b1 a\u03b1P\u03b1 with real coefficients a\u03b1. Each operator P\u03b1 is the tensor product of single-qubit Pauli operators. Such single-qubit observables can be efficiently measured, since each single-qubit Pauli operator can be written as a local rotation followed by a local projective POVM in the computational basis {|0\u3009\u30080| , |1\u3009\u30081|}. Therefore, the expected value \u3008A\u3009\u03c1 can be efficiently evaluated as the sum \u3008A\u3009\u03c1 = \u2211\n\u03b1 a\u03b1Tr[P\u03b1\u03c1], where each expectation Tr[P\u03b1\u03c1] can be evaluated separately via local operations.\n27\nWhile it is convenient to assume that decisions are made on the basis of the expectation (62), in practice this decision function cannot be directly evaluated, but only estimated on the basis of measurements over V copies of the state. Accordingly, one replaces the expected value \u3008A\u3009\u03c1y with an empirical average using V independent measurements of the observable A. The corresponding average quadratic error is given by \u2206y/ \u221a V , where \u22062y = Tr[A\n2\u03c1y] \u2212 Tr[A\u03c1y]2 is the single-shot variance. This error should be made sufficiently small to avoid wrong predictions. Informally, one should ensure the condition\n\u2206y\u221a V \u226a |\u3008A\u3009\u03c1y |. (63)\nWhen this is not the case, the model (62) provides an unreliable approximation of the actual empirical average to produce a decision. Furthermore, in this regime, the classifier is known to be vulnerable to adversarial attacks [75, 76], since tiny perturbations in the inputs can alter the prediction of the classifier. More precisely, when the average is estimated via V samples, assuming a zero-error predictor (62), one can bound the residual probability of error caused by the use of V samples using Hoeffding inequality (Appendix A) as\nLV01(A) \u2264 exp(\u2212V/(2\u2016A\u20162\u221e)). (64)\nIn the next subsections we study different ways of optimizing over a set of observables. We start from the simplest case in which the operator structure is fixed in terms of a linear decomposition based on a POVM, and optimization is done only over the linear coefficient of the decomposition. Then, we address the more challenging scenario in which constraints on the operator A do not limit the optimization space to a specific linear decomposition. Finally, we observe that a specific formulation of such constraints enables the application of the representer theorem, which yields a convenient parametrization of the solution to the optimization problem as a combination of the states to be distinguished."
        },
        {
            "heading": "3.4.2. Decision Observables with Fixed Operator Structure",
            "text": "Let us fix a POVM {\u03a0k}, and write the decision observable as the linear combination A = \u2211\nk ak\u03a0k with trainable parameters ak. In this case, the problem amounts to the classical detection of two probability distributions, namely p+(k) = Tr[\u03a0k\u03c1+] and p\u2212(k) = Tr[\u03a0k\u03c1\u2212], based on an average of the observations ak \u223c Tr[\u03a0k\u03c1]. Therefore, it can be addressed via a \u201cquantum data collection\u201d phase followed by classical post-processing as per the decision function (62). We focus here on common losses that take the form \u2113(f, x, y) = \u039b[yf(x)], where \u039b(\u00b7) is a convex function. Examples include the hinge loss with \u039b(z) = max{0, 1\u2212 z}, and the logistic loss with \u039b(z) = log(1+ exp(\u2212z)) (see Table 6.1 in [54]). With this choice, the average loss can be written as\nL(f,MA) = \u2211\nk,y\n\u2113(f, k, y)Tr[\u03a0k\u03c1y] = \u2211\nk,y\n\u039b[yak] Tr[\u03a0k\u03c1y], (65)\nwhich amounts to a classical binary classification problem over the coefficients {ak}."
        },
        {
            "heading": "3.4.3. Norm-Constrained Decision Observables",
            "text": "We now focus on optimizing the entire operator A. In this case, the problem of designing operator A does not reduce to the classical problem of detecting two classical probability distributions, since the structure of the operator determines the distributions of the measurement outputs. Focusing again on losses of the form \u2113(f, x, y) = \u039b[yf(x)], with a convex function \u039b(\u00b7), the design problem amounts to the minimization of the loss\nL(A) = \u2211\ny\n\u039b ( y\u3008A\u3009\u03c1y ) . (66)\nThis optimization is in principle feasible, since the problem is convex as long as the domain of matrix A is a convex set. However, for large systems with many qubits, the size of the optimization variable A becomes unmanageable without imposing some restrictions on the optimization domain.\n28\nBefore addressing this problem, we observe that, given an eigendecomposition A = \u2211\nk ak\u03a0k, for any operator A, the loss L(A) in (66) is no larger than that in (65). In fact, by Jensen\u2019s inequality, we have\nL(A) = \u2211\ny\n\u039b\n( \u2211\nk\nyak Tr[\u03a0k\u03c1y]\n)\n\u2264 L(f,MA). (67)\nThis result will be useful in the next sections. As discussed, in order to address the minimization of function L(A), one needs to impose some constraints on matrix A, while not fixing its structure as done earlier. A typical approach is to assume the form A = UZ1U \u2020, where U is a trainable unitary, e.g., via a parametric quantum circuit, followed by a Pauli-Z measurement on the first qubit. With this choice, however, the minimization problem is no longer convex, making algorithmic solutions and analysis more problematic. Inspired by structural risk minimization (Section 2.3), we will take a different route and map the constraints on the decision observable as a penalty term. As we elaborate next with an example, the penalty should ideally reflect an underlying constraint on the structure of the operator A. As an example, the assumption A = UZ1U\n\u2020 discussed above satisfies the constraint A2 = 1 , and hence this structure can be approximately imposed by adding a penalty term of the form Tr[(A2\u22121 )]. This choice, however, is not unique. For instance, the observable also satisfies the constraint \u2016A\u2016\u221e = 1, since its eigenvalues are \u00b11. Therefore, one could also add a penalty based on the norm \u2016A\u2016\u221e. Generalizing the example, we consider imposing a constraint on the operator A based on the value of a norm \u2016A\u2016\u266f. Accordingly, the optimization domain is defined as A\u266f,p = {A : \u2016A\u2016\u266f \u2264 p}. Introducing a positive convex barrier function g(\u00b7), one can formulate the problem in an unconstrained form, as discussed in Section 2.3, yielding the optimized observable\nA\u2217,\u03bb = argmin A\n[L(A) + \u03bbg(\u2016A\u2016\u266f)]. (68)\nIn the rest of this subsection we discuss some physically motivated operator norms that can be used to construct penalties terms; while the next subsections shows how problem (68) can be addressed for the norm \u2016A\u20162. Classical shadows represent a powerful technique for predicting the expectation values of many observables without doing full tomography[34]. The prediction error with classical shows is quantified by the shadow norm \u2016A\u2016shadow, whose definition depends on the choice of the shadow representation. In particular, for Clifford and local operations it was found that the following bounds hold, respectively,\n\u2016A\u201622 \u2264 \u2016A\u20162shadow,Clifford \u2264 3\u2016A\u201622, \u2016A\u20162shadow,local \u2264 4k\u2016A\u20162\u221e, (69)\nwhere k is the number of qubits on which A acts non-trivially. As a result, a penalty dependent on the norm \u2016A\u20162 can model observables that can be estimated efficiently using classical shadows with Clifford circuits; while a penalty on \u2016A\u2016\u221e can model observables that can be estimated efficiently using local operations. Therefore, for instance, if two states can be distinguished using local observables, a penalty based on norm \u2016A\u2016\u221e may be more appropriate. Another notion of locality is at the heart of the quantum Wasserstein distance of order 1[77]. For a traceless operator X , the W1 norm is defined as \u2016X\u2016W1 = maxA:\u2016A\u2016L\u22641 Tr[XA] where \u2016A\u2016L is the quantum Lipschitz constant of the traceless observable A, which is the dual norm of \u2016 \u00b7 \u2016W1 [77]. The W1 distance provides a quantum version of the Hamming distance, since two quantum states \u03c1\u00b1 that coincide after discarding k qudits satisfy the inequality \u2016\u03c1+ \u2212 \u03c1\u2212\u2016W1 \u2264 2k. We can summarize the properties of these two norms for traceless observables[77] acting on n qudits as\n\u2016A\u20161 \u2264 \u2016A\u2016W1 \u2264 k\u2016A\u20161, \u2016A\u2016L \u2264 max 1\u2264i\u2264n \u2016Ai\u2016\u221e. (70)\nwhere k is the number of qudits on which A act non-trivially, while Ai is the sum of terms in the operator expansion of A that act non-trivially on qudit i. These norms can be used to define tight bounds for parametric quantum circuits [78]. As such, these penalties may represent a natural choice to enforce constraints in the depth of the quantum circuit used to classify the two states."
        },
        {
            "heading": "3.4.4. Decision Observables from the Representer Theorem",
            "text": "In general, obtaining the optimized observable (68) is intractable when the dimension of the Hilbert space is large. However, an application of a classical result from statistical learning, namely the representer theorem [7], can be leveraged to simplify the problem when the average loss function being optimized is suitably regularized.\n29\nSpecifically, assume that we are interested in minimizing the criterion (68) with the choice g(\u2016A\u2016\u266f) = Tr[A2]. For this particular choice, by the representer theorem, the optimal solution of problem (68) can be always expressed as a linear combination of the two possible states, i.e.,\nA = \u03b1+\u03c1+ + \u03b1\u2212\u03c1\u2212, (71)\nwhere the real coefficients \u03b1\u00b1 must be optimized to minimizing the objective function in (68) [37]. As a specific example, consider the hinge loss function. Then using (71) and (66) we can write the average loss and regularizer as\nLhinge(A) = 1\n2 (max{0, 1\u2212 \u03b1+P+ \u2212 \u03b1\u2212F}+max{0, 1 + \u03b1+F + \u03b1\u2212P\u2212}) , (72)\nTr[A2] = \u03b12+P+ + \u03b1 2 \u2212P\u2212 + 2\u03b1+\u03b1\u2212F, (73)\nwhere P\u00b1 = Tr[\u03c12\u00b1] is the purity of each of the two possible states and F = Tr[\u03c1+\u03c1\u2212], which represents the fidelity between two quantum states when at least one of the two states \u03c1+ and \u03c1\u2212 is pure. With this choice, problem (68) has the solution\nA\u2217 = (P\u2212 + F )\u03c1+ \u2212 (P+ + F )\u03c1\u2212\nP+P\u2212 \u2212 F 2 pure = \u03c1+ \u2212 \u03c1\u2212 1\u2212 F , (74)\nwhere the second equality holds when both states are pure, and hence we have the equalities P\u00b1 = 1. Accordingly, in the special case of pure states, for a true, unknown, state \u03c1 \u2208 {\u03c1+, \u03c1\u2212}, the optimal predictor (62) computes the fidelities, or overlaps, Tr[\u03c1\u03c1+] and Tr[\u03c1\u03c1\u2212], and outputs y = +1 or y = \u22121 depending on whether the first or the second is larger than the other. Note that this discriminator was called the \u201cfidelity\u201d classifier in [35]. As mentioned, in practice, the predictor (62) cannot be evaluated exactly based on the availability of V copies of the state. These should be large enough to enforce the condition (63) and make the probability of error (64) sufficiently small. In the case of the \u201cfidelity\u201d classifier with optimal observable (74), when both possible states are pure, we have \u3008A\u2217\u3009\u03c1\u00b1 = \u00b11, and \u2016A\u2016\u221e = 1/(1\u2212 F ) so the conditions (63) and (64) result in\n\u22062 = F\n1\u2212 F \u226a V, L V 01(A) \u2264 exp(\u2212V (1\u2212 F )2/2). (75)\nAccordingly, for larger overlaps F between the two possible states, one needs more shots V to resolve the differences between the two states with sufficiently high probability."
        },
        {
            "heading": "4. Learning To Discriminate Unknown Quantum States",
            "text": "When knowledge of the states to be distinguished is not available, the state discrimination problem is a proper learning problem as described in Section 1. To formalize this problem, assume that, as in the previous section, the system of interest is equally likely to be in one of two possible states \u03c1+ and \u03c1\u2212. Unlike in the previous section, however, the states \u03c1+ and \u03c1\u2212 are now assumed to be unknown. The only information available at the learner about the states \u03c1+ and \u03c1\u2212 is in the form of a training data set composed of S quantum systems in state \u03c1+ and of S quantum systems in state \u03c1\u2212, on which the learner can act with measurements. Accordingly, the training set can be described as the single composite state \u03c1\u2297S+ \u2297 \u03c1\u2297S\u2212 . Note that the positions of the states in the ordering implied by the state description implicitly determines the label of each state as being y = + for the first S systems and y = \u2212 for the last S systems. As per the framework in Section 1, the learner is also given V additional unlabelled test systems which, unbeknownst to the learner, are all in either state \u03c1+ or state \u03c1\u2212. The goal is to determine whether the test systems are in state \u03c1+ or \u03c1\u2212. The test systems are hence collectively in either state \u03c1 \u2297V + or state \u03c1 \u2297V \u2212 .\nOverall, the learner has access to a composite quantum system that is in state\n\u03c1\u2297S+ \u2297 \u03c1\u2297S\u2212 \ufe38 \ufe37\ufe37 \ufe38\ntraining\n\u2297\u03c1\u2297V+ or \u03c1\u2297S+ \u2297 \u03c1\u2297S\u2212 \ufe38 \ufe37\ufe37 \ufe38\ntraining\n\u2297\u03c1\u2297V\u2212 . (76)\nHowever, unlike the discrimination problem of Section 3, the individual states \u03c1+ and \u03c1\u2212 are unknown, and hence it is not possible to design an optimal measurement using the procedures described in the previous section. In fact, as we will discuss in Appendix D, there are even different ways to define optimality in this case.\n30\nThe most general strategy is to apply a joint binary measurement on both training and test systems. The goal of the measurement is to determine if the test systems are more \u201csimilar\u201d to the first S systems in the training data, in which case the detector outputs the label y = + as its decision; or rather if the test systems are more \u201csimilar\u201d to the last S systems in the training data, in which case the detector outputs the label y = \u2212 as its decision. Following the definitions given in Section 1, the general class of joint measurements implements a transductive learning strategy. As seen, in transductive learning, training and test data are jointly processed to produce a decision on the test inputs. In classical machine learning, transductive learning strategies have the disadvantage that, when new test inputs are presented to the learner, training and test inputs would have to be jointly processed anew in order to produce a decision [79]. In contrast, for more conventional inductive learning, as described in Section 1, the learner obtains a general rule f from the training data set, which is then used to make decisions on any new test input. In the case of quantum machine learning, the limitations of transductive learning approaches are compounded by the fact that, once the training data are operated on via a measurement, they are no longer available to be jointly measured with new test input states. Therefore, with quantum transductive learning, training data can be used only once. That said, as also hinted at in Section 1, transductive learning may have advantages in terms of learning performance. In the next subsections we introduce different inductive and transductive state discrimination strategies, and analyze the performance of the inferred predictors with respect to the average loss via the decomposition (8). We start by specializing the average loss decompositions introduced in Section 1 to the problem at hand in the next subsection."
        },
        {
            "heading": "4.1. Generalization Analysis",
            "text": "We now review and specialize the framework introduced in Section 1 to analyze the performance of inductive and transductive learning algorithms. Inductive learning: As discussed in Section 1, the inductive learning strategy adopts a two-step procedure. In the first step, the available S-copy training set is used to extract some classical knowledge, which we denote as So. This can for example be a set of observations generated via measurements on the S-copy training set, or some classical description of the unknown states. We can then define an empirical training loss L(f,So) on this set of observations (see Section 1.6), which can be minimized to obtain the optimized inference function fSo . Since fSo is our finite-S approximation of fS , we also call it fSS . One can then study the average loss LP(fSS ) of the inferred function using (8). For the setting under study here with only two possible states, the abstract dataset loss L(f,S), which uses knowledge of the unknown quantum states, coincides with the average loss LP(f). This in turn results in the respective minimizers being equal, i.e., fS = f\u2217. Using this equality in (8), we get the following equivalent decompositions,\nLP(f S S ) = LP(f\u2217) + E S test = LP(f\u2217) + E S S = LP(f\u2217) + E(fSS ). (77)\nConsequently, for inductive learning schemes, a small optimality gap, or equivalently knowledge gap, results in smaller average loss, thereby ensuring generalization of the classification strategy built using partial information to the general case in which an arbitrary number of observations (or copies) are available. In the next sub-sections, we will reserve analysis via optimality gap E(fSS ) to scenarios when fSS is the result of minimizing an empirical training function and we want to emphasize the connection to the Radamacher complexity. Transductive learning: In contrast, a transductive learning strategy is a one-step implementation that eliminates the need to extract classical knowledge to define a training loss as in the inductive learning strategy. As such, the transductive scheme aims to directly approximate f\u2217 . It does this by applying a single joint measurement on the training and test states that enacts the classifier fSS on the test state. In this setting, as seen in Section 1, the average loss can be decomposed as LP(fSS ) = LP(f\u2217) + E S S . Accordingly, the joint measurement must be chosen such that for any pair of states \u03c1\u00b1, in the asymptotic limit of S \u2192 \u221e, ESS \u2192 0. As in the inductive scheme, a small ESS is an indication that the transductive strategy learned using partial information about unknown states generalizes to the case when arbitrarily large number of copies are available. We now discuss different inductive and transductive learning strategies."
        },
        {
            "heading": "4.2. Tomography-based Quantum State Classification",
            "text": "The first naive approach is based on full state tomography. In this case the state discrimination routine proceeds as in Section 3, with the only difference that the initial step QSD.1 is done empirically by reconstructing the state with state tomography. It is known that [80] full state tomography requires S = O(d2/\u01eb2) copies of a d-dimensional state to obtain an approximate classical description with precision \u01eb in the trace distance. This can be reduced to\n31\nS = O(dr/\u01eb2) if the rank r is known. For mathematical simplicity, we assume that both \u03c1\u00b1 can be reconstructed up to the desired precision \u01eb with the same number of copies S, since the extension to the general case is straighforward.\nLet us call \u03c1empy the approximate reconstructions of the true states \u03c1y and define \u03a0 HH \u00b1 and \u03a0 HH,emp \u00b1 as the Helstrom POVMs constructed using the true and empirical states respectively (per Eq. 58). Finally, let us define \u03b7 = \u2016\u03c1+\u2212\u03c1\u2212\u20161 and \u03b7emp = \u2016\u03c1emp+ \u2212 \u03c1emp\u2212 \u20161. Note that, when carrying out the measurement, we have direct access to \u01eb (which is set by S) and \u03b7emp (which we can calculate from our approximate states), but not to \u03b7 (which depends on the unknown true states). However, using the triangle inequality, we can write the bound |\u03b7 \u2212 \u03b7emp| < 2\u01eb. The average loss for our empirically constructed measurement is (1 \u2212 Tr[\u03a0HH,emp+ (\u03c1+ \u2212 \u03c1\u2212)])/2, so as long as Tr[\u03a0HH,emp+ (\u03c1+ \u2212 \u03c1\u2212)] > 0, it is better than random guessing. This is guaranteed to be the case as long as \u03b7emp > 2\u01eb. Conversely, if \u03b7emp < 2\u01eb, the empirically constructed measurement can be the worst, rather than the best, choice on the true states. Indeed, for a given pair of empirical states, the true states could be the linear combinations \u03c1\u00b1 = (1\u2212 \u01eb/\u03b7emp)\u03c1emp\u00b1 + \u01eb/\u03b7emp\u03c1emp\u2213 , so, for \u03b7emp < \u01eb, we could have \u03c1\u00b1 = \u03c1emp\u2213 . It is also intuitively understandable that if the states are closer together, we need to know them to greater precision in order to discriminate between them. Now let us bound the knowledge gap in terms of \u01eb. Note that \u03a0HH\u00b1 and \u03a0 HH,emp \u00b1 are, respectively, the optimal measurements for the average and empirical losses, so the knowledge gap, ESS , is the only quantity of interest. Using the 0-1 loss (58), the knowledge gap is\nESS = 1\n2 Tr[(\u03a0HH+ \u2212\u03a0HH,emp+ )(\u03c1+ \u2212 \u03c1\u2212)] =\n1 4 \u03b7 \u2212 1 2 Tr[\u03a0HH,emp+ (\u03c1+ \u2212 \u03c1\u2212)]. (78)\nUsing the linearity of the trace, we can rewrite the last term as\nTr[\u03a0HH,emp+ (\u03c1+ \u2212 \u03c1\u2212)] = Tr[\u03a0HH,emp+ (\u03c1emp+ \u2212 \u03c1emp\u2212 )] + Tr[\u03a0HH,emp+ (\u03c1+ \u2212 \u03c1emp+ )] + Tr[\u03a0HH,emp+ (\u03c1emp\u2212 \u2212 \u03c1\u2212)]. (79)\nThe first term is exactly \u03b7emp/2, whilst the last two terms are lower bounded by \u2212\u01eb/2, so the knowledge gap is upper bounded by\nESS \u2264 1\n4 (\u03b7 \u2212 \u03b7emp + 2\u01eb) \u2264 2\u01eb = O ( d\u221a S ) . (80)\nNote that in this approach we have built up a set of measurement outcomes, So, and then minimized the corresponding empirical loss L(f,So). We could therefore, in theory, bound the optimality gap E(fSo) = E(fSS ) using the Rademacher complexity. Whether we do so depends on which approach yields tighter bounds. Suppose V > 1, so we have more than one copy of the test state. If our empirically constructed measurement is better than a random guess for a single copy of the test state, then the error will decay exponentially with V . We know that this is the case if we carry out the (suboptimal) majority vote. The Rademacher complexity bound on the knowledge gap increases with V , but only sublinearly (O( \u221a V )). See Section 5.1.1 for more details.\nA similar analysis can be done using decision observables with loss (66). More precisely, given a \u03bb-Lipschitz functions \u039b and a set of decision observables A we can use the inequality in (35) to write\nESS \u2264 2 sup A\u2208A\n|L(A)\u2212 LS(A)| \u2264 3\u03bb \u2211\ny\nsup A\u2208A\n\u2223 \u2223Tr[A(\u03c1y \u2212 \u03c1empy )] \u2223 \u2223 . (81)\nIf the observables satisfy \u2016A\u2016\u221e \u2264 B, then by Ho\u0308lder\u2019s inequality ESS \u2264 O(\u03bbB\u01eb) = O(\u03bbBd2/S). Therefore, if S \u226b d2, the knowledge gap will go to zero. This comes at the price of performing a large amount of measurements (exponentially many, in case of many-qubits). A different approach consists in using the classical shadows formalism[34], which does not provide a tomographic reconstruction of the density matrices \u03c1\u00b1 but rather some estimators \u03c1empy , which are possibly quite different in trace norm from \u03c1y, but such that Tr[A\u03c1 emp y ] \u2248 Tr[A\u03c1y] for a large number of observables. More precisely, given a set of M traceless observables Ai, using the classical shadow estimator it was found that |Tr[Ai\u03c1empy ] \u2212 Tr[Ai\u03c1y]| \u2264 \u01eb, as long as\nS \u2265 O ( log(M)\n\u01eb2 max i \u2016Ai\u20162shadow\n)\n, (82)\nwhere the \u201cshadow norm\u201d was already introduced in Eq. (69). ForM \u2248 em it is hence possibly to predict exponentially many observables with a number of copies that scale linearly withm. Assuming that the states \u03c1\u00b1 can be distinguished\n32\nwith linear combinations of the observables Ai, we may define the decision class A as the set of observables A = \u2211M\ni=1 \u03b1iAi, then using (81) we get\nESS \u2264 3\u03bb M\u2211\ni=1\n|\u03b1i|\u01eb . 3\u03bb \u221a log(M) \u2211M\ni=1 |\u03b1i|\u221a S max i \u2016Ai\u2016shadow. (83)\nTherefore, although the shadow tomography is able to accurately reconstruct exponentially many observables Ai, because of the sum \u2211M\ni=1 |\u03b1i| it is unclear whether this advantage persists also over families of observables. As such, it is unclear whether the shadow tomography approach can provide an exponential advantage for state discrimination."
        },
        {
            "heading": "4.3. Discrimination with fixed measurements",
            "text": "Another simple inductive strategy is to first get some classical information about \u03c1\u00b1 using a fixed measurement strategy M = {\u03a0k}, \u2013 e.g.,those compatible with the experimental platform \u2013 and then use a purely classical learning approach. Suppose for instance that we have performed S measurements on \u03c1+ and S measurements on \u03c1\u2212 with outcomes kn. We may group these outcomes and define a purely classical training set So = {(kn, yn)} where yn = \u00b11 defines whether kn was obtained by performing the measurement on either \u03c1\u00b1. Suppose that the information contained in So is not enough to perform quantum state tomography to the desired precision, either because M is not tomographically complete or because the number of shots is not enough to reconstruct \u03c1\u00b1 with the desired precision, or both. Since the probabilities p\u00b1(k) are unknown, we cannot explicitly optimize the loss (56). The only thing we can train is the empirical loss L(f,So) of Eq. (30), with N = 2S, which is independent on the choice of measurements. The latter is a purely classical optimization problem and the only quantum part is in getting the experimental data contained in the training set SS . As such, errors can be studied via the optimality gap E(fSS ) (35), see also Figure 2, which can be bounded by the (empirical) Rademacher complexity (36), with a purely classical decision function. Different bounds on the Rademacher complexity for \u201cclassical\u201d function classes F , such as neural networks or support vector machines, are known in the literature [7, 40]. For functions parametrized by real parameters w, these bounds typically scale as\nRN . \u2016w\u2016ES \u2016k\u2016\u221a\nS , (84)\nwhere the choice of the norm depends on the function class. However, from the physical point of view these bounds are not particularly informative, other than saying that POVMs with many outcomes perform more poorly as \u2016k\u2016 gets bigger. When the set of outcomes is continuous, e.g.,when using homodyne or heterodyne measurements, this shows that models with spread outcomes are penalized. However, for discrete outcomes, such dependence cannot be justified. In order to get a better bound we focus on the 0-1 loss (probability of error), with dictionary-type decision functions that map a measurement outcome kn \u2208 {1, . . . ,K} to an output yn = \u00b11. In that setting, using Theorem A.9 from Appendix A we get the empirical Rademacher complexity as\nR01(M,So) \u2264 K\u2211\nk=1\n\u221a Nk 2N\n(85)\nwhere N = 2S = \u2211\nkNk andNk = N + k +N \u2212 k is the number of outcomes k that we get from POVMM = {\u03a0k}, applied\non either \u03c1\u00b1, which follows a multinomial distribution. Using Jensen\u2019s inequality we then obtain an M-dependent bound on the Rademacher complexity\nR01N (M) \u2264 K\u2211\nk=1\n\u221a\nES [Nk]\n2N =\n1\n4\nK\u2211\nk=1\n\u221a\nTr[\u03a0k\u03c1+] + Tr[\u03a0k\u03c1\u2212] S =\n\u221a\n2H1/2(M)\n8S , (86)\nwhere we have introduced the Re\u0301nyi entropy H\u03b1(M) = log2( \u2211 k p \u03b1 k )/(1\u2212\u03b1) and pk = Tr[\u03a0k \u03c1++\u03c1\u22122 ]. From the above inequalities it appears that measurements with the least entropy on the average state \u03c1++\u03c1\u22122 may result in a lower optimality gap. Moreover, since 2H1/2(M) \u2264 K, POVMs with less outcomes are preferable, in line with similar bound that we get from support vector machines and neural networks (84).\n33\nIn terms of information theoretic content, we want H1/2(M) \u2265 1, namely the chosen measurement should contain at least 1 bit of information to distinguish the two classes. The exact equality is obtained by choosing a two-outcome POVM defined by pretty good measurements [67] \u03a0\u00b1 = T\u22121/2\u03c1\u00b1T\u22121/2, where T = \u03c1+ + \u03c1\u2212, which are therefore optimal for minimizing the excess risk. It turns out that pretty good measurements also provide a good approximations of the optimal Helstrom measurement [68, 69], so they seem to be a natural choice for defining measurements that are both optimal to minimize the training error and the optimality gap. However, since \u03c1\u00b1 are unknown, this requires the use of an transductive strategy that acts jointly on the training and test states (76) with a quantum algorithm [81]. In the inductive framework pretty good measurements cannot be applied and can only be seen as the desired goal of the optimization."
        },
        {
            "heading": "4.4. Learning with decision observables",
            "text": "We now focus on learning to classify states according to sign of an expectation value of an observable A (62). When the quantum states are unknown, finding the optimal observable A for a given loss is non-trivial because changing A may also change the probability distribution of the measurement outcomes, making the problem quite different from the classical supervised learning paradigm. Nonetheless, expectation values over quantum states can be measured in different ways, depending on the choice of the resolution A = \u2211\nk ak\u03a0k, where ak are real numbers and \u03a0k form a POVM. For any such resolution, the measurement outcomes ak have associated probabilities Tr[\u03c1\u03a0k]. In the particular case where the above resolution is provided by the spectral decomposition, when we optimize over A, we also optimize over \u03a0k, and the resulting probability distribution is not fixed. This setting is more similar to reinforcement, rather than supervised, learning. There are different ways of simplifying this problem, for instance using a fixed basis of operators constructed from Pauli measurements, as discussed in Section 3.4.1. In this case the problem can be formulated using the classical learning framework on Section 4.3. Let A = \u2211M\nj=1 \u03b1jAj be a decomposition of A, where \u03b1j are real trainable parameters, with \u03b1 \u2208 A, and Aj are fixed observables that can be measured efficiently. By performing MS different measurements, we can build a classical training set So = (a(s)1 , . . . , a (s) M , y (s)) where s = 1, . . . , S and a (s) j denotes the measurement outcome of observable Aj at the sth shot, while y (s) depends on whether we took the measurements on \u03c1\u00b1. By construction\nA(S, y) = 1\nS\nN\u2211\ns=1\n\u03b4y(s),y\nM\u2211\nj=1\n\u03b1ja (s) j , (87)\nprovides an unbiased estimator of the observable, so A(S, y) \u2192 \u3008A\u3009\u03c1y for S \u2192 \u221e. We can then optimize the empirical loss with a suitable regularization to obtain the empirical parameters \u03b1empj , which allows us to construct the optimal empirical decision observable Aemp = \u2211\nj \u03b1 emp j Aj . Here too, the error can be studied via the optimality gap E(fSS ),\nsee Figure 2, which can be bounded by the Rademacher complexity, thanks to (35) and (38). Using Corollary A.11 and a \u03bb-Lipschitz loss we get\nE(fS) . O (\nsup \u03b1\u2208A \u03bbB\u2016\u03b1\u2016p\u221a S\n)\n, (88)\nwhere B \u2265 |a(s)j | and the choice of p depends on the regularization using for training. The error depends on how big can \u2016\u03b1\u2016p can be. We will consider an example in Section 4.6 where \u2016\u03b1\u2016 \u2248 (1 \u2212 F )\u22121 and F is the overlap between \u03c1+ and \u03c1\u2212. So these bounds are useful to quantify how, for a given family of observables, specific properties of the quantum states can affect the number of measurements that are necessary to obtain the desired classification accuracy."
        },
        {
            "heading": "4.5. Learning with Helstrom measurements",
            "text": "We now introduce the first transductive approach, which is based on the implementation of the Helstrom measurement (58) when the states \u03c1\u00b1 are unknown. This approach was developed in References[35, 36] using the state exponentiation (SE) algorithm [82] followed by phase estimation (PE). The state exponentiation algorithm, shown in Figure 3(a), uses a target state \u03c3 and many copies of another arbitrary state \u03c1, and acts on \u03c3 with a unitary U = eit\u03c1 that depends on \u03c1. It is based on the observation that\ne\u2212i\u03c1t\u03c3ei\u03c1t = \u03c3 \u2212 i[\u03c1, \u03c3]t+ ... = Tr 1 [ e\u2212itSWAP (\u03c1\u2297 \u03c3) eitSWAP ] +O(t2), (89)\n34\nwhere SWAP is the swap operator and the Tri is the partial trace over the ith subsystem. This is the first operation shown in Figure 3(a). This routine was generalized in [36] to use S copies of \u03c1+ and S copies of \u03c1\u2212, with S = O(t2/\u03b4) to simulate eiHt, with H = (\u03c1+ \u2212 \u03c1\u2212)/2 up a precision \u03b4 in the trace norm. The number of operations scales as O(S log(d)), where d is the dimension of the Hilbert spaces of \u03c1\u00b1, so it is efficient as the dimension increases. For instance, in multi-qubit systems, it scales linearly with the number of qubits. This simulation method was employed in [35] to obtain sign(H), and hence the Helstrom measurement (58). The starting point is that the exponentiated operator e2\u03c0iH is a unitary, U , with the same eigenvectors asH and eigenvalues e2\u03c0i\u03c6j , where the \u03c6j are the eigenvalues of H (which range between \u22121/2 and 1/2). Since the 2\u03c0\u03c6j are phases, we can equivalently say that the eigenvalues of U are e2\u03c0i\u03c6\n\u2032 j , where \u03c6\u2032j = \u03c6j when \u03c6j > 0 and \u03c6 \u2032 j = 1 + \u03c6j when \u03c6j < 0\n(assuming none of the \u03c6j = 0, with a simple extension if this is not the case). Given a target state \u03c3 (which is either \u03c1\u00b1), we can carry out the phase estimation algorithm (shown in Figure 3(b)) with unitary U = e2\u03c0iH and initial state \u03c3. If the algorithm succeeds, this results in the creation of a bitstring approximation b1 . . . bm of \u03c6 \u2032 j , up to a desired precision m, with probability Tr[\u03a0j\u03c3]. Since \u03c6 \u2032 j > 1/2 iff \u03c6j (the corresponding eigenvalue of H) is negative, the value of the first bit b1, tells us sign(\u03c6j) (specifically, it is 0 iff \u03c6j > 0). The probability of b1 being 0 is therefore \u2211\nj:\u03c6j>0 Tr[\u03a0j\u03c3]. Note that\n\u2211\nj:\u03c6j>0 \u03a0j is precisely the Helstrom operator.\nHowever, the phase estimation algorithm has a chance of failing (i.e. incorrectly approximating \u03c6\u2032j). If we want the algorithm to correctly give the first k digits of the bitstring approximation with probability at least 1\u2212 \u01eb, we require m = k+ log2(2+1/(2\u01eb)). Consequently, to carry out the Helstrom measurement with a probability of failure no more than \u01eb, we must set m \u2265 1 + log2(2 + 1/(2\u01eb)). Calling Z1 the Pauli operator on the first qubit, we can summarise\n\u03c3 \u2297 \u03c1\u2297S+ \u2297 \u03c1\u2297S\u2212 SE+PE\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 \u3008Z1\u3009 =\n\u2211\nj\nsign(\u03c6j)Tr[\u03a0j\u03c3] = Tr[sign(\u03c1+ \u2212 \u03c1\u2212)\u03c3], (90)\nwhere, as already mentioned, \u03c3 is either \u03c1\u00b1. To carry out the approximate Helstrom measurement, we must therefore simulate U , U2,..., U2 m\u22121\n. To understand the error scaling of the total operation, we can assume that each unitary has the same error, \u03b4, so that we need O(20/\u03b4) copies of \u03c1+ and \u03c1\u2212 to simulate the first unitary, O(22/\u03b4) copies for the second unitary, etc., up to O(22(m\u22121)/\u03b4) copies for the last unitary. With O((\n\u2211m\u22121 x=0 2\n2x)/\u03b4) = O(22m/\u03b4) copies of \u03c1+ and \u03c1\u2212, we have an additional error (to be added to the failure probability of the phase estimation algorithm) of at most O(m\u03b4) due to the imperfect simulations of U . The phase estimation algorithm then has a failure probability of at most \u01eb = O(2\u2212m). Therefore, the total failure probability is upper bounded by O(2\u2212m) +O(m\u03b4), and so, setting \u03b4 = O(2\u2212m/m), we can carry out the Helstrom measurement with a probability of failure of at most O(2\u2212m) using S = O(23mm) copies. Finally, we can say that we can enact the Helstrom measurement with a failure probability of at most \u01eb using S = O(\u2212 log(\u01eb)\u01eb\u22123) copies of \u03c1+ and \u03c1\u2212. Recalling that full-state tomography requires O(d2/\u01eb) copies to achieve the same error, we see that the scaling is much worse in terms of the error. However, for the approximate Helstrom measurement scheme, the number of copies does not depend on the dimension of the states at all, so it can be more efficient for high-dimensional states. This\n35\n(a) (b)\nefficiency comes at the price of having to manipulate all copies coherently. This strategy is also transductive, so for V > 1, in order to achieve the same error as tomography, we require approximately V times as many copies (i.e. we can split our training set into V subsets and use one for each test state, then carry out a majority vote, or we could use all of the copies coherently to enact the V -copy Helstrom POVM, but now considering \u03c1\u2297V\u00b1 to be a single copy for the purposes of our circuit)."
        },
        {
            "heading": "4.6. Learning with the representer theorem",
            "text": "We now focus on a different transductive strategy based on the representer theorem (71), which tells us that, for a given loss with a particular penalty, the optimal observable to discriminate two states can be written as a linear combination of the training states \u03c1\u00b1. This strategy is hence transductive as, after training, we still need to use copies of \u03c1\u00b1 to discriminate a new state \u03c3. Using the representer theorem, the discrimination is based on the sign of the following expectation value\n\u3008A\u3009\u03c3 = \u03b1+ Tr[\u03c3\u03c1+] + \u03b1\u2212 Tr[\u03c3\u03c1\u2212]. (91) For known \u03c1\u00b1, the optimal coefficients \u03b1\u00b1 were found explicitly and the resulting optimal observable was analytically constructed in (74). Both the coefficients and the final expectation values can be obtained from the overlap Tr[\u03c1\u03c3] between two states \u03c1 and \u03c3 (possibly equal). Two strategies to measure such overlap are presented in Figure 4, using either the swap test [83] or the swap measurement. In the swap test Figure 4(a), prediction is done from the measurement of a Pauli-Z observable on an ancillary qubit, coupled to \u03c1 and \u03c3, whose outcome at each measurement shot can be either \u00b11. Therefore, we can describe this procedure as a POVM M\u03c1\u00b1, where \u00b1 denotes the measurement outcome. Something similar can be done using the swap measurement Figure 4(b), by defining the resulting POVM M\u03c1\u00b1 depending on the sign of (\u22121) \u2211 i aibi .\nWhen such overlaps are estimated with S measurement shots via either algorithm from Figure 4, the parameters P\u00b1 and F that enter into Eq. (74) have a variance\n\u22062 = \u3008Z2\u3009 \u2212 \u3008Z\u30092\nS = 1\u2212 Tr[\u03c1\u03c3]2 S . (92)\nThe approximate observable AS is constructed by replacing the coefficients from Eq. (74) with the ones estimated with S shots. As such, for any given \u03bb-Lipschitz loss (66) we can bound the knowledge gap as\nESS \u2264 \u03bb \u2211\ny\n|\u3008A\u3009\u03c1y \u2212 \u3008AS\u3009\u03c1y | \u2264 \u03bb \u2223 \u2223 \u2223 \u2223 1 1\u2212 F \u2212 1 1\u2212 FS \u2223 \u2223 \u2223 \u2223 \u2211\ny\n|Tr[\u03c1+\u03c1y]\u2212 Tr[\u03c1\u2212\u03c1y]| \u2264\n\u2264 2\u03bb \u2223 \u2223 \u2223 \u2223 1\u2212 1\u2212 F 1\u2212 FS \u2223 \u2223 \u2223 \u2223 = 2\u03bb |F \u2212 FS | 1\u2212 FS \u2248 2\u03bb\n\u221a ( 1 + F\n1\u2212 F\n) 1\nS , (93)\nwhere for simplicity we have restricted the analysis to pure states, for which P\u00b1 = 1, and in the last expression we assume F \u2212 FS \u2248 \u2206. As expected, more measurements are required to reduce the error when F \u2248 1, namely when \u03c1\u00b1 are less distinguishable.\n36\nMore generally, given (91) and the above expression,\nESS \u2264 \u03bb \u2211\ny\n|\u3008A\u3009\u03c1y \u2212 \u3008AS\u3009\u03c1y | \u2264 2\u03bb\u2016A\u2212AS\u2016\u221e \u2264 2\u03bb \u2211\ny=\u00b1 (\u03b1y \u2212 \u03b1Sy ) \u2243 2\u03bb f(\u2206P+,\u2206P\u2212,\u2206F )\u221a S(P+P\u2212 \u2212 F 2)2 , (94)\nwhere for each estimated quantity X , we have assumed that X\u2212XS \u2248 \u03b4X/ \u221a S. When the states are less distinguishable, F 2 \u2248 P\u2212P+ and a larger S is required. The analytic solution from Eq. (74) is optimal for a particular loss and penalty. In general though analytic solutions are not available and the coefficients \u03b1\u00b1 must be optimized numerically given a few measurement results. In this case the optimization process fits into the framework discussed in Section 4.4. Accordingly, the scaling of the knowledge gap is given by Eq. (88) with B = 1 since the circuits in Figure 4 outputs estimators with outcomes \u00b11. In the examples discussed before, we have shown that the optimal parameters satisfy \u2016\u03b1\u2016 \u2248 (1 \u2212 F )\u22121. Although the behaviour as a function of S shown in Eq. (88) and (93) is the same, the bound in Eq. (93) may be smaller when \u03c1\u00b1 are less distinguishable, namely when 1 \u2212 F = \u01eb. Indeed, in this case (93) is small whenever S \u226b \u01eb\u22121, while (88) is small whenever S \u226b \u01eb\u22122. Therefore, exploiting the analytic solution (74) is expected to provide an advantage to get the same accuracy with fewer copies of the two states."
        },
        {
            "heading": "4.7. Inductive vs. Transductive Strategies",
            "text": "A key difference between inductive and transductive learning processes can be seen in how the error of a process of each type scales with V . Suppose we pick a scheme, calculate the error for V = 1, and then increase V to see how the error changes for that scheme. For an inductive learning process, there is an obvious way of extending the process to multiple copies of the test state. Since we have an intermediate result that tells us which measurement to enact on the test state, we can carry out the same initial measurement on the training set as for the V = 1 case and then repeat the second part of the measurement on as many copies of the test states as we like, with the same probability of error. We are therefore at least able to use the scaling of the majority vote, which is exponential in the number of copies (O(e\u2212V )). In other words, we have learned a rule that we can then apply to any number of unknown states. On the other hand, for a transductive process, the measurement is not divided into two separate parts, so the extension to V > 1 is not necessarily obvious. We do not learn any \u201creusable\u201d information, and since the training set is used up when measuring it, it must be shared amongst the V test states in some way. One option would be to divide our training set into subsets of S/V elements, use each subset to carry out the process for V = 1 separately on each test state, and carry out the majority vote. However, since we are decreasing the number of training states per test state, and the number of training states determines the excess error, the (single-copy) probability of error for each test state will be worse than in the V = 1 case (if S is fixed). As long as the single-copy error probability remains better than that of a random guess, the total error probability for the majority vote will continue to decrease as V increases, but for large enough V (and fixed S), this will eventually no longer be the case. The scaling with V will therefore be worse than for an inductive process \u2013 how much worse depends on how the single-copy error depends on S. As an example, we compare the performance of two approaches, one inductive and one transductive, based on the Helstrom classifier, with the goal of classifying V copies of a test state using S copies of the training states. In the inductive approach, we can use state tomography to achieve a single-copy error of O(d/ \u221a S), per Eq. (80), which is independent of V . In the transductive framework, our S copies allow us to achieve a single-copy error of O((V/S)1/3) (ignoring logarithmic factors). The overall error scaling for each, in terms of V and for fixed S, is\ninductive: O (\nd\u221a S e\u2212V\n) , vs. transductive: O (( V\nS\n)1/3 e\u2212V ) . (95)\nWe note that the scaling of the transductive approach is worse by a polynomial factor, but also that there is a maximum useful value of V in this case. This is because we require S \u226b V for the approximate Helstrom measurement, since we need multiple copies of the states per copy of the test state in order to carry out the state exponentiation/phase estimation algorithms. On the other hand, if S \u226b V , so that the single-copy error is better than a random guess for both approaches, we might instead find it helpful to frame Eq. (95) in terms of the number of training samples, S, required in order to achieve a given single-copy error, \u01eb:\ninductive: O ( d2\n\u01eb2\n)\n, vs. transductive: O ( VM\u01eb\u22123(\u2212 log \u01eb) ) . (96)\n37\nAlthough the scaling with V is worse in the transductive case than in the inductive case (linear vs O(1)), the scaling with dimension can be much more important. Hence, the most suitable approach depends on the dimension of the Hilbert space d and on the number of test states. In multi-qubit systems, where d = 2n and n is the number of qubits, if V is at most polynomial in n and \u01eb is independent on n, the transductive strategy may provide an exponential (in n) advantage compared with tomography based methods. On the other hand, in the \u201cbig data\u201d regime, where V is large, the inductive strategy may be preferable. We can also consider a case in which we must find the identity of multiple different test states, i.e. our test set comprises of multiple unknown states and we must find the identity of all of them. Again, this is not a problem for inductive learning processes, but forces us to use less of our training set for each test state with a transductive approach. The difference between inductive and transductive measurements has also been formulated in terms of a nonsignalling condition between different test instances, in Ref. [84], where the definition of a separation between training and test states is also used."
        },
        {
            "heading": "5. Supervised Learning for Quantum Classification",
            "text": "In the previous two sections, we have first studied the case in which the data-generation mechanism P is known (Section 3), and we have then addressed the new challenges arising from lack of knowledge about the possible states the test input may be in (Section 4). With reference to the general framework introduced in Section 1, in the settings considered so far, there is no classical input x, and hence the joint distribution P (x, y) of classical input x and classical output y plays no role. As a result, the performance of the learner depends solely on the number of copies S and V available during training and testing, respectively (with training being irrelevant for the case of known P). In this section, we investigate a more complex scenario in which data consists of classical input x, quantum embedding \u03c1(x), and classical output y, with the latter taken to be binary as in the previous sections. In this situation, the number N of training data points determines the information that the learner can extract about the joint distribution P (x, y). Therefore, the optimality gap is a function not only of the number of copies S and V , but also of the size of the training set, N . The training set SS = {(xn, yn, \u03c1(xn)\u2297S}n=1,...,N consists of triples containing the classical inputs xn, S copies of the corresponding quantum state \u03c1(xn), and the true class yn = \u00b11. We provide some discussion on how one can generate S copies of state \u03c1(x) in practice at the end of this subsection. For both training and test data, the classical input-output pairs are generated via a generally unknown joint distribution P (x, y). In this setting, the inference operation f introduced in Section 1 is a quantum measurement applied to the test input \u03c1(x) to extract the label y. The optimal inference operation f\u2217 is obtained by assuming knowledge of the joint distribution P (x, y) and of embedding mapping \u03c1(x). The learner optimizes operation f on the basis of training data SS . We will first consider the case in which the abstract training set S is available to the learner, and hence the states \u03c1(x) in the training set are known. In this case, the learner can compute and optimize the training loss. Then, we will tackle the case in which the states \u03c1(x) are unknown, and hence the inference function must be optimized using the available S-copy training set SS . As mentioned, in the case of unknown quantum states \u03c1(x), we assume that it is possible to create S copies of the states \u03c1(x), either by repeating the embedding procedure or by repeating the experiment multiple times. In real experiments, where \u03c1(x) are typically mixed states, one may distinguish the following situations:\n1. The states \u03c1(x) account for a statistical description of the experimental uncertainties. Physically, this means that the timescales on which the environment acts on the system are faster than the measurement times. In other terms, equilibration happens before the measurements are carried out, so each single measurement effectively \u201csees\u201d the same mixed state \u03c1(x). In this regime, by repeating the experiment S times, we effectively act on the product state \u03c1(x)\u2297S .\n2. In contrast, with a \u201cslow\u201d environment, the states \u03c1(x) will always be different, and it is impossible to create perfect copies. Without loss of generality, we may describe the uncertainty about the states before measurement by introducing a random variable e that models all the imperfections that the environment could apply to the states. This yields the mixed state as the statistical mixture\n\u03c1(x) =\n\u222b\nde penv(e|x)\u03c1(x, e), (97)\nwhere \u03c1(x, e) model the joint distribution of the classical input x and of the environment disturbance e. When trying to create copies S, the environment effectively creates \u03c1(x, e1) \u2297 . . . , \u03c1(x, eS), where es \u223c penv(e|x)\n38\nare possibly unknown. Nonetheless, since in the general framework of Section 1 the inputs x are possibly unknown to the learner, we can theoretically describe this limit by introducing a new training set SSenv = {(xn, es, yn, \u03c1(xn, es)}n=1,...,N ;s=1,...,S , with NS different states \u03c1(xn, es) and a larger input variable, modelled by x and e."
        },
        {
            "heading": "5.1. Known Quantum State and Unconstrained Complexity",
            "text": "Let us first study the case in which the states \u03c1(x) are known, and hence the learner has access to the abstract training set S. Therefore, assuming an equal probability for the two classes, the only element that is unknown about the data generation process P is given by the probability distributions p\u00b1(x) of the classical input. Furthermore, we focus here on unconstrained-complexity operations. Adopting the 01 loss, the average loss can be written as\n\u211301(M, x, y) = \u2211\ny\u0304\nTr[\u03a0y\u0304\u03c1(x)] = 1\u2212 Tr[\u03a0y\u03c1(x)]. (98)\nGiven a state \u03c1(x) with true class y, the above loss quantifies the probability that the outcome of M is different from y. Due to the linearity \u211301, the average loss (28) depends on the average states as\nL01(M) = 1\u2212 1\n2\n\u2211 y=\u00b1 Tr[\u03a0y\u03c1\u0304y], \u03c1\u0304y =\n\u222b\ndx py(x) \u03c1(x), (99)\nwhere we used the rule of conditional probabilities P (x, y) = py(x)/2. Accordingly, the optimal classifier f\u2217, see Figure 2, is given by the Helstrom measurement (58) between the two average states \u03c1\u0304\u00b1. However, this optimal inference operation is not accessible by the learner since the probabilities p\u00b1(x) are unknown. Given that the states \u03c1(x) are assumed to be known, the training loss can be evaluated by the learner as (30)\nL01(M,S) = 1\u2212 \u2211\ny=\u00b1\nNy N Tr[\u03a0y\u03c1\u0304 S y ], \u03c1\u0304 S y = 1\nNy\nN\u2211\nn=1\n\u03b4y,yn\u03c1(xn), (100)\nwhere N\u00b1 is the number of samples with yn = \u00b1 and \u03c1\u0304Sy is the ensemble average over all states from the training set S with class y. For simplicity, from now on we will assume an equal number of samples per class, namely N\u00b1/N = 1/2. The optimal data-driven classifier fS , one minimizing the dataset loss (30), is now given by the Helstrom measurement Memp = {\u03a0emp+ ,\u03a0emp\u2212 } over the empirical averages\n\u03a0emp\u00b1 = 1 \u00b1 sign\n( \u03c1\u0304S+ \u2212 \u03c1\u0304S\u2212 ) 2 , L01(Memp,S) = 1 2 \u2212 1 4 \u2225 \u2225\u03c1\u0304S+ \u2212 \u03c1\u0304S\u2212 \u2225 \u2225 1 , (101)\nwhere the second expression provides the analytical single-shot training loss. As long as the two average states are distinguishable, namely \u03c1\u0304S+ 6= \u03c1\u0304S\u2212, arbitrarily small training loss can be obtained by using multiple shots, as we will discuss in a following section. We now study generalization via the Rademacher complexity (36). As we derive in Appendix 2, both the empirical and Rademacher complexities scale as in Eq. (38), namely as R \u2264 \u221a B/N with\nB(M) \u2264 ( Tr \u221a \u222b dx p(x) \u03c1(x)2 )2 , B(M,S) \u2264\n\nTr\n\u221a \u221a \u221a \u221a 1\nN\nN\u2211\nn=1\n\u03c1(xn)2\n\n\n2\n, (102)\nwhere the second expression is the empirical approximation of the Rademacher complexity, which can be explicitly computed for given data. On the other hand, B(M) is purely formal, since the distribution p(x) is unknown. Nonetheless, asymptotically the two quantities differ at most by O(1) factors."
        },
        {
            "heading": "5.1.1. Lowering the Training Error via the Majority Rule",
            "text": "We now extend the result of the previous section by employing V copies of the test state. We repeat a local POVM classifier on each copy, with binary outcomes M = {\u03a0+,\u03a0\u2212}, and then use a majority rule. We set V = 2v+1 as an\n39\nodd integer to avoid ending up in a draw. The training error is given by the average probability of error\nLV01(M,S) = 1\u2212 1\nN\nN\u2211\nn=1\npV (xn, yn) = 1\nN\nN\u2211\nn=1\ncv (Tr[\u03c1(xn)\u03a0yn ]) , (103)\nwhere pV (x, y) = 1\u2212 cv (Tr[\u03c1(x)\u03a0y ]) is the probability that the majority vote over the state \u03c1(x) returns the correct class y, as in Eq. (60), Tr[\u03c1(x)\u03a0y ] is the probability that a single vote provides the correct answer y, and cv(p) = C2v+1v (p) = \u2211v n=0 ( 2v+1 n ) pn(1 \u2212 p)2v+1\u2212n is the probability that a majority vote provides an incorrect answer, given a probability of success p for a single vote. As shown in Section 3.3, as long as p = 1/2 + O(V \u22121), cv(p) decreases exponentially with V , so that the training error goes to zero for V \u2192 \u221e. In order to study the generalization error, in Appendix 3 we show that the Rademacher complexity behaves as in Eq. (38) with\nBV (M) \u2264 \u221a V + 1\n(\nTr\n\u221a \u222b\ndx p(x) \u03c1(x)2\n)2\n, BV (M,S) \u2264 \u221a V + 1\n\nTr\n\u221a \u221a \u221a \u221a 1\nN\nN\u2211\nn=1\n\u03c1(xn)2\n\n\n2\n, (104)\nthus providing a proof of the conjecture from [8], namely that the generalization error only slightly increases with the number of copies used in the majority vote. Since the training error decreases exponentially with V , using more shots is expected to be beneficial, as long as we have enough data to make (104) small."
        },
        {
            "heading": "5.2. Unknown Quantum States and Unconstrained Complexity",
            "text": "In the previous section we consider the error with the optimal unconstrained POVM, which for the 0-1 loss is given by the Helstrom classifier. We now consider the error in implementing such a measurement when the states \u03c1\u0304S\u00b1 in (101) are unknown, and only a finite number of copies of the states \u03c1(x) in the training set are available. We specify the problem as follows: instead of having classical knowledge of the N different states in the abstract training set, S = {\u03c1(xi)} for i ranging from 1 to N , we instead have Si copies of the ith state. The training set can therefore be expressed as\n\u2297N i+1 \u03c1(xi) \u2297Si . We constrain the number of copies by demanding that \u2211 Si = NS, namely that S is\nthe average of Si. In other words, we can only draw NS states in total from N different options. This setting makes sense when it is computationally expensive to make copies of the states in the training set but less so than adding new states to the training set. I.e. there are different costs associated with drawing a new state from the true distribution and making copies of that state, so that it makes sense to distinguish between N and S. This is the case for many of the examples in Section. 1.2. For instance if we are classifying phases of matter, there may be a cost associated with producing copies of a state that we know to be in one phase or the other, but the cost of finding the settings to produce a new state in a known phase might be higher. For quantum classification of quantum sensed data, there will be some cost associated with probing the same sample multiple times, but a different cost associated with producing a new sample, so it again makes sense to make a distinction between N and S. Note that we could have specified the problem in a slightly different way. Instead of simply requiring that a total of NS states are drawn from S, we could have required that we have precisely S copies of each of the N different states in S. However, in many scenarios of interest, there is no reason why we would have this extra constraint and we can simplify the mathematical analysis by only constraining the total number of states in the training set. Let us consider the two possible extremes that S could take. If there is no cost associated with S, so that we can take S to infinity, we once again have classical knowledge of the states, so that the only source of error is the generalization error (with no excess testing error or knowledge gap). In the language of Figure 2, we will always find the minimum of the dataset loss (the red curve) and the only source of error is the difference between the dataset loss and the average loss (the blue curve). As N becomes larger, the two curves get closer together, with the difference between them scaling with N\u22121/2, per Eq. (39). This is essentially a classical learning problem implemented using quantum states. On the other hand, if it is as costly to make the same sample multiple times as it is to produce a new sample, we may have S = 1, so that we have a single copy of each training state. In this particular, special case, we have N samples drawn from the true average states for each class, \u03c1\u0304\u00b1. We can then treat the problem as one of state discrimination between the unknown states \u03c1\u0304+ and \u03c1\u0304\u2212, as per Section 4. We have no generalization error, since the average states \u03c1\u0304+ and \u03c1\u0304\u2212 are precisely what we want to discriminate between, and so the only source of error is the knowledge gap. In the language of Figure 2, the dataset loss is the same as the average loss, but we are not at their minimum value. In this case, we can directly replace S with N in our previous (Section 4) expressions for the\n40\nknowledge gap, which are different for each protocol. As N increases, the two curves remain the same (and identical to each other) but we get closer to the minimum. Now let us return to the intermediate setting, where S is finite, but S \u226b 1. We will have a knowledge gap, coming from our finite knowledge of the states in the training set, and a generalization error, coming from the difference between the training set and the true average states. We define the difference between the empirical average states as\nHS = \u03c1\u0304S+ \u2212 \u03c1\u0304S\u2212 = 2\nN\n\u2211\nn\nyn\u03c1(xn), (105)\nand the difference between the true average states as\nH = \u03c1\u0304+ \u2212 \u03c1\u0304\u2212 = \u222b dxdy P (x, y) y\u03c1(x). (106)\nThen, the knowledge gap quantifies how different the final measurement we perform on the test state is from the empirical Helstrom measurement, Memp = {\u03a0emp+ ,\u03a0emp\u2212 }, where \u03a0emp\u00b1 = (1 \u00b1 sign(HS))/2, as per Eq. (101). The optimality gap E(fS), and the generalization error quantify how different Memp is from the true Helstrom measurement, M = {\u03a0+,\u03a0\u2212}, where \u03a0\u00b1 = (1 \u00b1 sign(H))/2, as per Eq. (58). To understand how the knowledge gap and generalization errors scale with N and S, we must clarify exactly how we generalize the learning strategies from Section 4 to this new setting where we have multiple different states in each class. We draw NS random samples from the set S and apply the protocols from Section 4 to learn to discriminate between the empirical average states \u03c1S+ and \u03c1 S \u2212. The knowledge gap can therefore be found using the expressions from Section 4, but replacing S with NS (since we have NS samples), and the generalisation error comes from the difference between the empirical average states and the true average states, and follows Eq. (39). To show how this works, and the interplay between the two types of errors, let us consider the tomography-based (inductive) approach, from Section 4.2, and the (transductive) approach based on applying the approximate Helstrom measurement via state exponentiation, from Section 4.5. Per Section 4.2, the knowledge gap, \u01eb, for tomography-based quantum state classification scales with the total\nnumber of training states, NS, as \u01eb = O(d/ \u221a NS). The generalization error, \u03b4, scales with the number of different training states, N , as \u03b4 = O( \u221a\nB/N), with B \u2264 d. Thus, increasing S decreases only the knowledge gap, whilst increasing N decreases both the knowledge gap and the generalization error. For large d, the knowledge gap will dominate. However, assuming the states from the training set have rank at most r and the rank of the average states is at most Nr \u226a d, we can write \u01eb \u2264 O( \u221a dNr/ \u221a NS) = O( \u221a\ndr/S), which is independent of N . In the worst case B \u2248 d, assuming r = O(1) we need S as large as N to make \u01eb and \u03b4 of the same order. For the approximate Helstrom measurement, following Kimmel et al.[36] we can apply the state exponentiation algorithm with samples from either \u03c1\u0304Sy or \u03c1\u0304y to approximate either e itHS or eitH to the desired precision, and hence Memp or M. The latter is the S = 1 case, whilst the former is the extension to S \u226b 1 (but still finite). In the former case, the total number of training states required to achieve a knowledge gap of \u01eb is NS = O(\u2212 log(\u01eb)\u01eb\u22123) (per Section 4.5), whilst again the generalization error scales with N as \u03b4 = O( \u221a\nB/N). Again, increasing S decreases the knowledge gap and increasing N decreases both the knowledge gap and the generalization error. If S is kept constant and N is increased, the generalisation error will decrease much more quickly than the knowledge gap. If we want the two errors to be of the same order in N , we require \u01eb = O(\u03b4) = O( \u221a\nB/N). To do this, we must set S = O(SN,B), with SN,B = log(N/B) \u221a\nN/B3. Thus, if we do not want either the knowledge gap or the generalization error to dominate (for large enough N), we must also increase S. As an example, suppose that the model complexity B is known, and that the learner has chosen a dataset with N = O(B\u01eb\u22122) elements to reach a maximum generalization error \u01eb. Then, a knowledge gap of order \u01eb requires S = O(B\u01eb\u22121(\u2212 log \u01eb)). If S is constant or increases slower than O(SN,B), the knowledge gap will become dominant compared to the generalization error. If S increases more quickly than O(SN,B), the generalization error will dominate. We can understand this behaviour intuitively. We know that for S \u2192 \u221e we only have a generalization error and for S = 1 we only have a knowledge gap. Thus, if S is large (compared to N), the generalization error will dominate, since we are close to the S \u2192 \u221e case, whilst for small S, we are close to the S = 1 case, so the knowledge gap dominates."
        },
        {
            "heading": "5.3. Information theoretic understanding of training/testing errors",
            "text": "The errors introduced in Section 5.1 can be understood using information theoretic quantities, as summarized in Section 1. Here we present an extended derivation.\n41\nWe start by defining an abstract space which contains all the possible knowledge of the problem. In our data we have inputs x, outputs y and quantum states \u03c1(x), we may model all of this together by defining the abstract and empirical average states\n\u03c1XY Q = \u2211\ny\n\u222b\ndxP (x, y) |xy\u3009\u3008xy| \u2297 \u03c1(x), \u03c1SXY Q = 1\nN\nN\u2211\nn=1\n|xnyn\u3009\u3008xnyn| \u2297 \u03c1(xn), (107)\nwhere X is the space of inputs (possibly continuous), Y is the space of outputs, and Q is the space of the quantum states \u03c1(x). In \u03c1SXY Q the average over the unknown P (x, y) is replaced with the empirical average over the N data in S. Technically both \u03c1XYQ and \u03c1SXYQ describe a classical-classical-quantum state, as both X and Y are classical. We remark that states in the extended XYQ space should never be computed in applications, this is just a mathematical framework to rephrase some quantities in a information-theoretic language. It was shown [8] that both the training error L01(Memp,S) and the testing error L01(Memp) can be expressed via information theoretic quantities, computed with respect to the states in Eq. (107). In particular, as already discussed in Section 1 the average loss with the optimal Helstrom classifier MHH can be written as\nL01(MHH) = 1\u2212 2\u2212Hmin(Y |Q) \u2264 1\u2212 2\u2212H(Y |Q), L01(Memp,S) \u2264 1\u2212 2\u2212H emp(Y |Q), (108)\nwhere H(Y |Q) is the quantum conditional entropy and Hmin is a similar quantity with a different notion of entropy, see [8, 39] for details, while Hemp(Y |Q) is the quantum conditional entropy, but computed over the empirical average state \u03c1SXYQ. The average loss is zero when the conditional entropy is zero. This happens when Y is completely determined by Q, namely when for a given quantum state there is a direct mapping to find its class. When, given a test state, the information about its class is imperfect, the conditional entropy is greater than zero, and so is the loss. As for the generalization error, as shown in Section 1, we can express the bounds (102) using information theoretic quantities to get the bounds\nG01 = L01(Memp)\u2212 L01(Memp,S) \u2264 O (\u221a\nB(M) N\n)\n= O\n\n\n\u221a\n2I1/2(X:Q)\nN\n\n = O\n\n\n\u221a\n2 Iemp 1/2 (X:Q)\nN\n\n , (109)\nwhere I1/2(X :Q) is the Re\u0301nyi quantum mutual information between X and Q, see also Eq. (13), which quantifies the amount of information that the knowledge of X provides to the knowledge of Q, and vice versa. On the other hand, Iemp1/2 is the same quantity, but computed over \u03c1 S XYQ, as in (102). In the last approximate equality in (109) we replace \u03c1XYQ with \u03c1 S XYQ, since the difference between the Rademacher and empirical Rademacher complexities go to zero for large N \u2013 see Eq. (37). Since \u03c1SXYQ are classical-classical-quantum states, the Re\u0301nyi quantum mutual information satisfies some simple properties, Iemp1/2 (X :Q) \u2264 min{H1/2(Q), log2N}, where the first term is due to the fact that the space X describes classical information represented as N orthogonal vectors. When the mutual information becomes comparable with log2N the bound (109) becomes trivial. Only when the Re\u0301nyi entropy of the Q subsystem is much smaller than log2N , we can expect a small generalization error, even in the worst case. In other words, good generalization is possible when the quantum embedding x 7\u2192 \u03c1(x) effectively discards \u201cirrelevant\u201d information from the input X that is unnecessary to predict the output Y . However, if too much information is discarded, then H(Y |Q) gets larger, and so does the training error."
        },
        {
            "heading": "5.3.1. Learning with imperfect copies",
            "text": "We finally comment on what happens when copies of an unknown quantum state are used for learning. Thanks to the analysis of Section 1.6.2 the S copies of each state (on average) used during training, do not enter into the generalization error. On the other hand, applying a majority rule over more copies of the same test state only slightly increases the generalization error, as shown in (104). In this section we discuss another scenario, introduced in Section 5, namely when it is impossible to create perfect copies of the training states. As shown in that section, we have to replace \u03c1(xn) \u2297S with \u03c1(xn, e1) \u2297 \u00b7 \u00b7 \u00b7 \u2297 \u03c1(xn, eS), where the auxiliary classical inputs es describe the unknown action of an environment. If these copies are processed at different times, we may model the learning process as a single-shot processing the NS states \u03c1(xn, es). At test stage, thanks to (97) and the linearity of the 01 loss, we may simply focus on V copies of the average state \u03c1(x)\u2297V and use the results of the majority vote. Calling E the space of possible unknown actions performed by the\n42\nenvironment, we get a generalization error\nGE01 \u2264 O (\u221a V + 1\nSN 2 Iemp 1/2 (X,E:Q1)\n)\n. (110)\nTherefore, if V = O(S) the dependence on either S and V disappears from the generalization error, making it independent on the number of copies employed either during training or testing. However, the price to pay is modeled by a different mutual information Iemp1/2 (X,E:Q1), which takes into account both the effect of the classical inputs and of the environment. If the environment can significantly alter the states, than this mutual information can be higher, and hence the number of samples N to get a small generalization error will increase. On the other hand, when the perturbations inflicted by the environment are negligible, it is reasonable to expect that Iemp1/2 (X,E:Q1) \u2243 I emp 1/2 (X :Q1). In this regime, the learner can basically ignore the presence of an environment."
        },
        {
            "heading": "5.4. Learning with Observables",
            "text": "We now focus on discriminating quantum states according to the expectation value of an observable A. Given a state \u03c1(x) we will predict its class as in Eq. (62), namely as\nypredicted = sign ( \u3008A\u3009\u03c1(x) ) . (111)\nThe average loss and the training error are constructed from the cost function of Eq. (66) as\nL(A) = \u2211\ny\n\u222b\ndxP (x, y)\u039b ( y\u3008A\u3009\u03c1(x) ) , L(A,S) = 1\nN\nN\u2211\nn=1\n\u039b ( yn\u3008A\u3009\u03c1(xn) ) . (112)\nwhere \u039b is a \u03bb-Lipschitz convex function and A is a set of observables. The optimal empirical observable is the one minimizing L(A,S) under suitable constraints. As discussed in Section 3.4.3, many practical restrictions can be rephrased using norm constraints. Let G\u03b1 the generalization error when the observables are constrained to have \u2016A\u2016\u03b1 \u2264 c\u03b1, for a certain norm and constant c\u03b1. In Appendix. 4 we study different constraints. In particular, we find\nG\u221e \u2264 O\n\n\u03bbc\u221e\n\u221a\n2 Iemp 1/2 (X:Q)\nN\n  , G2 \u2264 O ( \u03bbc2\u221a N ) , G1 \u2264 O ( \u03bbc1 \u221a log d N ) . (113)\nFor observables with maximum eigenvalue equal to 1, we can focus on G\u221e with c\u221e = 1 and the generalization bound that we get by optimizing the loss (112), aside from the constant \u03bb, is equivalent to that obtained by optimizing over POVM, namely Eq. (109). For the hinge loss, where \u03bb = 1, the bounds are exactly equal. For observables with bounded 2-norm, generalization does not explicitly depend on the properties of the quantum state. Something similar is obtained for observables with bounded 1-norm, but with an extra factor due to the logarithm of the dimension d of the quantum states.\nAs an example, we consider a system with n qubits, and we focus on observables made of a linear combination of Pauli measurements,\nA = \u2211\nj\n\u03b1jPj , (114)\nwhere \u03b1j are real trainable coefficients and Pj are Pauli matrices. Since these satisfy Tr[PiPj ] = 2 n\u03b4ij , we get Tr[A2] = 22n\u2016\u03b1\u201622, so c2 \u2265 2n\u2016\u03b1\u20162 is any bound on the norm of the coefficients. Therefore, unless the coefficients \u03b1j are rescaled with an exponentially small quantity, the bound (113) grows exponentially with the number of qubits. A similar problem happens for A1, as \u2016A\u20161 \u2265 \u2016A\u20162. On the other hand, since the eigenvalues of Pj are \u00b11 we get \u2016Pj\u2016\u221e = 1 and c\u221e \u2265 \u2211 j |\u03b1j | = \u2016\u03b1\u20161, which has better scaling as long as A is made of few Pauli observables.\n43"
        },
        {
            "heading": "5.5. Learning with Kernels",
            "text": "As discussed in Section 3.4.4, the optimal observables with a \u21132 penalty can be expressed as a linear combination of the training data\nA = N\u2211\nn=1\n\u03b1n\u03c1(xn). (115)\nTherefore, for computing the expectation value of such quantity with respect to some state \u03c1(x) we need to be able to compute all possible overlaps Tr[\u03c1(x)\u03c1(xn)]. Algorithmically, this can be done using the techniques presented in Section 4.6, e.g. using the swap test of swap measurements from Figure 4. Such overlaps define the kernel\nk(x, x\u2032) = Tr[\u03c1(x)\u03c1(x\u2032)]. (116)\nAccording to Eq. (92), if we want to estimate the kernel with precision \u01eb we need\nS \u2243 1\u2212 k(x, x \u2032)2\n\u01eb2 , (117)\ncopies of \u03c1(x) and \u03c1(x\u2032). Training consists in minimizing the loss (112) with a \u21132 penalty\nL(\u03b1, S) = 1\nN\nN\u2211\nn=1\n\u039b ( yn\u3008A\u3009\u03c1(xn) ) + \u00b5Tr[A2] (118)\n= 1\nN\nN\u2211\nn=1\n\u039b\n(\nyn\nN\u2211\nm=1\n\u03b1mk(xn, xm)\n)\n+ \u00b5 N\u2211\nm,n=1\n\u03b1m\u03b1nk(xn, xm). (119)\nwhich, once the kernel matrix has been estimated, becomes a convex problem in the parameters \u03b1n. Popular machine learning libraries can be used to efficiently solve the above problem numerically, given the kernel matrix. After training, classification of a new state is done as Eq. (111) as\nypredicted = sign\u3008A\u3009\u03c1(x) = sign ( N\u2211\nn=1\n\u03b1nk(xn, x)\n)\n. (120)\nNotice that for prediction the evaluation of new kernels k(xn, x) is necessary, which requires further copies of the training states \u03c1(xn), as discussed in Section 1. This strategy is therefore transductive. Moreover, we have to ensure that the error in the estimation of the kernel, which can be controlled via the number of copies (117), does not significantly alter the classification accuracy. Generalization can be studied thanks to (113). In particular, from Eq. (115) we get Tr[A2] = \u03b1TK\u03b1, where Knm = K(xn, xm) is the kernel matrix. Hence\nG \u2264 O\n\n\u03bb\n\u221a\nsup\u03b1j \u03b1 TK\u03b1\nN\n  \u2243 O ( \u03bbB \u221a\nkmax N\n)\n, (121)\nwhere kmax is the largest eigenvalue of K and B \u2265 \u2016\u03b1\u20162. In numerical simulations, this term can be constrained by using a larger value of \u00b5, which penalises solutions with large Tr[A]2. Finally, we conclude this section by studying what happens when the kernel entries are estimated using the techniques of Section 4.6. Getting analytical results with the loss (119) is complicated, so we focus on a lower bound. Applying Jensen\u2019s inequality we can lower bound the problem as the discrimination of the two average states \u03c1\u0304Sy , which was treated in Section 4.6. Indeed,\nL(A,S) \u2265 1 2\n[ \u039b(\u3008A\u3009\u03c1\u0304S+) + \u039b(\u2212\u3008A\u3009\u03c1\u0304S\u2212) ] + \u00b5Tr[A]2. (122)\nIn this way, we can use Eq. (94) to show that the error using S copies scales as O(S\u22121/2), but with a prefactor that diverges when \u03c1S+ \u2243 \u03c1\u0304S\u2212. Accordingly, if the states are less distinguishable more shots are required.\n44"
        },
        {
            "heading": "5.6. Parameterized Quantum Circuits as Classifiers",
            "text": "So far in this section, we have discussed the problem of classifying a quantum state \u03c1(x), indexed by a classical variable x, into one of the two possible class labels y \u2208 {\u00b11}, via an optimal POVM (Section 5.1) or an optimal observable (Section 5.4). These optimal solutions generally entail the implementation of complex circuits that may not be compatible with noisy intermediate scale quantum (NISQ) computers. In this subsection, we discuss the pragmatic approach of constraining the optimization of the classifying quantum circuit to architectures, or ansa\u0308tze that can be efficiently implemented on NISQ hardware. A parameterized quantum circuit (PQC), also referred to as a quantum neural network (QNN), consists of a sequence of quantum gates that can be efficiently implemented on a given hardware, while possibly allowing for optimization via the tuning of some real-valued parameters. Typical examples include parameterized single-qubit rotations and fixed two-qubit gates such as CNOT (see, e.g., [4]). Accordingly, a PQC implements unitary transformations of the form\nU(\u03b8) =\nNg\u220f\ni=1\nUl(\u03b8l), (123)\nwhere each of the Ng unitary matrices Ul(\u03b8l) is described by a number of fixed gates and by one parameterized gate with real-valued parameter \u03b8l. In practice, quantum gates are subject to quantum noise. Writing as U\u03b8l(\u00b7) the operator defined by the parameterized unitary Ul(\u03b8l) \u2013 i.e., U\u03b8l(\u03c1) = Ul(\u03b8l)\u03c1Ul(\u03b8l)\u2020 for any input density \u03c1 \u2013, the actual operation of each term in (123) is described by a quantum channel N\u03b8l(\u00b7) = N\u0304 \u25e6 U\u03b8l(\u00b7), where N\u0304 (\u00b7) is quantum channel describing gate noise, e.g., depolarizing noise. The notation \u25e6 denotes a composition of channels. Accordingly, the PQC implements the overall quantum channel N\u0304 \u25e6 U\u03b8Ng \u25e6 . . . \u25e6 N\u0304 \u25e6 U\u03b81(\u00b7). Given a classical input x, PQCs operate on a quantum state \u03c1(x) produced using a fixed quantum encoding. In general, one can apply a unitary gate U(x), parameterized by index x, to act on an initial fiducial state to obtain the quantum embedding\n\u03c1(x) = U(x)|0\u3009\u30080|U(x)\u2020. (124)\nIt is also possible to consider strategies in which the input x is \u201creloaded\u201d multiple times by interleaving inputdependent unitaries of the form U(x) and parameterized quantum gates of the form Ul(\u03b8l) as in (123). The PQC produces the quantum state\n\u03c1\u03b8(x) = N\u03b8(\u03c1(x)). (125)\nA classification decision can then be made by measuring the output using a fixed projective measurement. This is typically implemented by applying a standard measurement on one of the qubits. The classification function is then of the form\nf\u03b8(x) = Tr(\u03a0\u03c1\u03b8(x)). (126)\nwhere \u03a0 is a fixed projection matrix. With input (124), and a noiseless PQC (123), this output can be expressed as\nf\u03b8(x) = Tr(\u03a0U(\u03b8)U(x)|0\u3009\u30080|U(x)\u2020U(\u03b8)\u2020). (127)\nThis expression shows that one can think of the optimization of the PQC U(\u03b8) as the design of the encoding circuit U(\u03b8)U(x) for a fixed observable \u03a0, or as the design of the observable U(\u03b8)\u2020\u03a0U(\u03b8) for a fixed encoding circuit U(x), We now discuss the generalization error of the PQC-based classifiers. To this end, we consider the 01 loss \u211301(f, x, y) as in the previous subsections. The generalization error of the learnt PQC-based classifier f\u03b8,S can be studied via the Rademacher complexity of model class F = {f\u03b8(\u00b7) : \u03b8 \u2208 \u0398}, where \u0398 is the domain of the parameters to be learnt. While upper bounds on Rademacher complexity in the form of B(F) can be evaluated directly for simple model classes, for PQC-based classifiers we have to leverage additional tools to evaluate B(F). One such important tool is the Dudley entropy integral bound [85] that bounds Rademacher complexity of F via the covering number of F . In Table III, we summarize some recent results on the generalization error of PQC-based classifiers that leverage covering number based bounds on the Rademacher complexity. Table III shows that the generalization error of PQC-based classifiers depends on (i) the architecture of the PQC, accounted for by the number of trainable gates Ng, as well as the largest number k of qubits operated on by a single parameterized quantum gate in the PQC; (ii) on the strength p of the quantum gate noise; and (iii) on the quantum encoding strategy.\nWhile the bound of Caro et al is tighter than Du et al, both bounds show that PQCs with large number of trainable gates, Ng, incur a larger generalization error when trained on a fixed set of N examples. In particular, the bound of Du et al also demonstrates that the quantum gate noise, which is inherent to currently available NISQ devices, may not be detrimental to generalization and can help prevent over-fitting. Neither of these works account for the impact of the quantum encoding strategy on the generalization error.\nThis aspect is addressed in [12]. Specifically, the authors of [12] consider the quantum encoding in (124), with the unitary U(x) = exp(\u2212ixH), where H is a Hermitian matrix known as the data-encoding Hamiltonian H . With this choice, the function f\u03b8(x) in (127) can be written as the generalized trigonometric polynomial (GTP),\nf\u03b8(x) = \u2211\n\u03c9\u2208\u2126 c\u03c9(\u03b8,\u03a0) exp(i\u03c9x), (128)\nwhere the coefficents c\u03c9 depend on the tunable parameters \u03b8 and observable \u03a0, while \u2126 denotes the set of accessible frequencies determined by the spectrum of the data encoding Hamiltonian H . The key idea in [12] is to consider the function class\nF\u2126 = { f(x) = \u2211\n\u03c9\u2208\u2126 c\u03c9 exp(i\u03c9x) : {c\u03c9}\u03c9\u2208\u2126 ensures that \u2016f\u2016\u221e \u2264M\n}\n, (129)\nthat encompasses the original model class F , i.e, F \u2286 F\u2126. In fact, the model class (129) allows for a larger class of coefficients c\u03c9 that need not depend on tunable PQC parameters \u03b8 and observable \u03a0.\nThis way, an upper bound on Rademacher complexity of the PQC-based classifiers F follows as RP (F) \u2264 RP (F\u2126) via the Rademacher complexity of the GTP-based model class F\u2126. As seen from Table III, the resulting generalization error depends only on the set of accessible frequencies determined by the quantum encoding strategy, and not on the trainable part of the PQC. Furthermore, set of accessible frequencies can be further upper bounded in terms of the number of encoding gates used.\nNote that the Rademacher complexity-based generalization bounds of Caro et al. [13] and Du et al, [42], where the training data scales with the number of trainable parameters Ng, cannot explain the low generalization error of QNNs observed in the over-parameterized [86] regime. In this regime, adopting an encoding-based generalization bound as in [12] can help explain the generalization error. Alternatively, as in classical deep learning, one must move from the capacity-based analysis to a general analysis that accounts for the training algorithm as well as the data distribution. In this regard, reference [14] adopts an algorithmic robustness-based analysis to quantify the generalization error of an ERM-classifier that scales as O( \u221a\n4kNge logNge/N), where Nge denotes the number of data encoding gates each acting on at most k qubits.\n46"
        },
        {
            "heading": "6. Unsupervised Learning for Quantum Generative Modelling",
            "text": "In this section, we turn our attention to the less studied problem of quantum unsupervised learning. As reviewed in Section 2.4, classical unsupervised learning amounts to the general problems of estimating properties of an unknown probability distribution P (x), or of sampling from an unknown probability distribution P (x), by observing a data set drawn from P (x). In particular, generative models implement a model class F of candidate probability distributions f(x) from which sampling can be efficiently carried out on a classical computer. Functions f(x) typically consist of neural networks with classical sources of randomness. In quantum unsupervised learning for generative modelling, quantum generative models implement PQCs, as well as quantum measurements and classical post-processing."
        },
        {
            "heading": "6.1. Learning Tasks",
            "text": "Quantum generative models can be used to approximately sample from an unknown classical distribution P (x), hence addressing the same problem as classical generative models, or to generate a quantum state \u03c1\u0302 that approximates an unknown quantum state \u03c1. For the former case, the learner leverages information available in the form of an training set S of N classical examples sampled i.i.d. from the unknown distribution P (x), while in the latter case the available information is in the form of an S-copy training set SS consisting of S copies of the unknown state \u03c1. Note that the S-copy training set consists of copies of a single (N = 1) unknown quantum state, and that there is no classical input x. Not that there may be also situations in which the generation of the quantum state is conditional on some classical input x [87], but we will not elaborate on such conditional quantum generative models here. Depending on the type of observed input data and on the type of generated target output, we may distinguish the following situations.\n1. Classical input-classical target: In this setting, the goal is the same as for classical generative modelling. Given a training set S = {x1, . . . , xN} of examples sampled i.i.d. from the classical unknown distribution P (x), we wish to optimize an implicit model that can generate samples from a distribution f(x) that is a close approximation of P (x). Unlike the classical case, here the model class consists of PQCs and the output is obtained via quantum measurements [21\u201323].\n2. Classical input-quantum target: In this setting, the goal is to load an unknown data distribution P (x) into a quantum state \u03c1\u0302 by only observing a training set S = {x1, . . . , xN} of examples sampled i.i.d. from P (x) [88]. This setting finds application in quantum state preparation [21].\n3. Quantum input-quantum target: In this setting, the goal is to obtain an approximation \u03c1\u0302 of an unknown quantum state \u03c1. The only information available is in the form of S copies of the unknown quantum state \u03c1, which constitutes the input data set SS = \u03c1\u2297S . This class of problems is also termed quantum state compilation [25], and QGLMs have been employed to approximate unknown pure states [27] as well as mixed states [25].\nIn the following, we will focus on the classical input-classical target and quantum input-quantum target cases."
        },
        {
            "heading": "6.2. Quantum Generative Learning Models",
            "text": "Quantum generative learning models (QGLMs) are PQC-based models used for the purpose of generating synthetic data samples or for approximating an unknown quantum state. At their core, QGLMs consist of a quantum channel N\u03b8(\u00b7), parameterized by tunable classical parameters \u03b8 \u2208 \u0398, that maps an input quantum state \u03c1in to an output quantum state N\u03b8(\u03c1in). We now review quantum circuit Born machines (QCBMs) and variational quantum generators (VQGs) as notable representatives of QGLMs. A QCBM describes a parameterized quantum state\n\u03c1\u0302\u03b8 = N\u03b8(|0\u3009\u30080|) (130)\nobtained via the operation of the channel N\u03b8(\u00b7) on a fiducial input state |0\u3009. For the classical input-classical target case, QCBM leverages the intrinsic randomness of quantum measurements to generate discrete classical data, while for the quantum input-quantum target case, the state \u03c1\u0302\u03b8 in (130) can directly serve as the learned approximation. To elaborate further on the classical input-classical target case, according to Born\u2019s rule, a projective measurement \u03a0i = |i\u3009\u3008i|, for i \u2208 {0, 1, . . . , 2n\u22121}, of the quantum state \u03c1\u0302\u03b8 onto the ith computational basis generates discrete\n47\nsamples i \u2208 {0, 1, . . . , 2n\u22121} with probability f\u03b8(i) = Tr(\u03a0i\u03c1\u03b8). Note that QCBMs are implicit models, since they produce samples i, and not the probabilities f\u03b8(i). While QCBM can generate classical discrete data, VQGs address the classical input-classical output case to generate real-valued data samples [23]. To do this, VQG leverages an external source of randomness, and evaluate expected values of observables, rather than relying on single-shot measurements as QCBMs. A VQG takes as input to the PQC N\u03b8(\u00b7) a quantum state \u03c1in(z) that encodes a classical variable z \u223c Q(z) sampled randomly from a fixed prior distribution Q(z). Accordingly, the VQG prepares the random quantum state \u03c1\u0302\u03b8(z) = N\u03b8(\u03c1in(z)). To generate a d-dimensional real-valued output sample x \u2208 Rd, VQG evaluates the expected value of d observables A1, . . . , Ad as\nx = [\u3008A1\u3009\u03c1\u03b8(z), . . . \u3008Ad\u3009\u03c1\u03b8(z)]. (131) Assuming the possibility to accurately estimate the expectations in (131), the only randomness in x is due to the classical random variable z \u223c Q(z)."
        },
        {
            "heading": "6.3. Excess Risk of Quantum Generative Modelling",
            "text": "In contrast to extensive recent empirical research on the use of quantum models for generative learning, the generalization analysis of quantum generative models has been studied in very few existing works. In this section, we review some known results, and highlight some open problems in this field. To begin with, consider the classical input-classical target setting, where QGLMs are used to generate classical samples from an unknown distribution P (x). In this setting, one can think of QGLM as an implicit model generating samples according to an abstract distribution f(x). Note that, unlike classical unsupervised learning, the distribution f(x) is determined by PQCs as well as quantum measurements. Using a divergence measure D(P, f) to quantify how far the true distribution P (x) is from the generated distribution f(x), we can proceed to define excess risk as in Section 2.4. As discussed before, bounds on the excess risk depends on the choice of divergence measure and the specific QGLM used. When IPM is used as the divergence measure, the upper bound on excess risk in (54) directly applies. For VQGs, recent reference [89] derived bounds on the excess risk with the squared maximum mean discrepancy (MMD) as the divergence measure. Under the assumption of Lipschitz continuous kernel function \u03ba(\u00b7, \u00b7), the resulting upper bound solely depends on the model complexity of VQGs, which is then quantified via the covering number of the considered class of PQCs in a manner similar to the results in Table III. Apart from reference [42], very less is known about the generalization performance of other QGLMs under different divergence measures. More challenging is the problem of quantifying the excess risk of QGLMS used to approximate an unknown quantum state \u03c1. To define excess risk in this setting, one can use appropriate distance measures D(\u03c1, \u03c1\u0302\u03b8) between the unknown quantum state \u03c1 and the approximated quantum state \u03c1\u0302\u03b8. Popular distance measures include trace distance [27], fidelity, quantum-Wasserstein semi-metric [90], or the quantum relative entropy [91]. However, the distance measure D(\u03c1, \u03c1\u0302\u03b8) cannot be evaluated since the true quantum state \u03c1 is unknown. Instead, we have access only to partial information about it in the form of S copies \u03c1\u2297S . Since quantum measurements consume copies, the available partial information is not sufficient to search for the optimal approximation, thereby yielding a sub-optimal approximation. As such, the optimality gap in this setting will be a consequence of the availability of limited number S copies of unknown quantum state as well as the constrained model complexity of the considered PQC ansatz. Characterizing the excess risk in this setting is an open problem. One of the important challenges in generalization analysis of generative models is the lack of a clear definition of what it means for a generative model to generalize. In Section 2.4, we defined the optimality gap in terms of the closeness of the approximated distribution to the true distribution relative to the closeness of best approximate distribution to true distribution. Intuitively, this measures the ability of generative model to efficiently learn the unknown distribution. Recent work [92] advocates an alternate, practical, interpretation of generalization, termed sample-based generalization, that measures the generalization power of a generative model via its ability to efficiently generate data samples. Precisely, a QGLM is said to have good sample-based generalization if it can generate novel, high quality, out-of-training samples. Considering discrete probability distributions, reference [92] gives a formal definition of sample-based generalization and introduces various performance metrics to characterize it."
        },
        {
            "heading": "7. Conclusions and Outlook",
            "text": "We have studied the statistical complexity of quantum learning, namely the number of samples and the number of copies of each sample to reach a desired accuracy. In the most general setting with quantum data, what makes\n48\nquantum learning different from classical learning is the learner\u2019s ignorance about the training states. While classically the learner has access to all the information about the training states (e.g., all the bits in an image), in the quantum case the number of bits that the learner can extract from a single measurement is limited. Moreover, quantum data cannot be copied, so the learner must have access to multiple copies, obtained by repeating the state-preparation mechanism. We have studied different errors depending on the learner\u2019s knowledge: (i) the error when the learner knows both the data distribution and the quantum states, which can be addressed via standard statistical techniques, without learning methods; (ii) the extra generalization error when the learner has full classical knowledge of the training data, but ignores the data distribution \u2013 the typical setting in classical machine learning \u2013 which can be reduced by increasing the amount N of training data; and (iii) the extra knowledge gap when the learner can only extract partial information from the training states, which can be reduced by increasing the average number of copies S of such states. The total error at the test stage, when the learnt model is used to make new predictions, may be decomposed as a sum of these three terms.\nWe have reviewed different learning methods proposed in the literature and studied the three sources of errors using a combination of quantum information theory and statistical learning theory. All these considered, it is an open question to understand which method is \u201cbest\u201d at exploiting the information available at the training stage. For example, classical methods focus on minimizing (i)+(ii), and deep neural networks trained with gradient descent are known to be able to reach remarkably low errors. However, in the quantum setting, e.g., with quantum neural networks, each gradient evaluation requires the consumption of copies of the training data, so it is unlikely that quantum neural networks with many parameters make an efficient use of their copies and are optimal for error (iii).\nIn quantum state classification problems, e.g., in quantum state discrimination and hypothesis testing, optimal or asymptotically optimal strategies have been found to minimize (i)+(iii), making the most efficient use of the available copies. However, identifying the most efficient strategy that minimizes all sources of error, (i)+(ii)+(iii), using the least amount of training resources is still an open question. The answer to this question is likely to depend on the learning scenario, and on whether it is simpler to get new quantum data, that is, to make N large, or to repeat the state preparation procedure to make S large. Moreover, the structure of the quantum data is expected to play a key role in determining the relative importance of the three types of error as a function of resources N and S. For instance, more distinguishable states are expected to require a lower number of copies S, while clustered states in the Hilbert space, which display less variation, are expected to require a lower number of data points N . Therefore, it is important to consider not just the scaling with the numbers N and S, but also to express the pre-factors using quantities that can be simple to interpret. With the ultimate goal of understanding the accuracy of quantum learning as a function of the data and copy resources, we have studied different supervised and unsupervised learning methods and, when technically possible, we obtained a bound on the different error terms. We have done an extensive analysis of the Helstrom classifier, which is optimal for state discrimination, according to the 01 loss, and whose generalization error was studied in [8]. The above generalization analysis has been extended to study the optimality gap of Helstrom classifiers that discriminate quantum states prepared via a pre-trained PQC in [93]. We have also studied how the generalization error changes when the learner is limited to use a finite number of copies of each quantum state, both at training and testing stage. Moreover, we have shown that the same generalization error also describes learning problems involving the optimization of observables with bounded spectrum. However, since the Helstrom classifier uses unconstrained operations, it is unlikely to be optimal for generalization. We have studied how to introduce constraints via information theoretic techniques, and applied this setting to toy problems involving the classification of quantum states or quantum phases of matter.\nFinally, we have also considered kernel methods, within the framework of support vector machines, and some hybrid methods consisting of a quantum data collection part, followed by a purely classical learning process. Moreover, we have reviewed different bounds on parametric quantum circuits, and, in some case, showed how to obtain them using information-theoretic techniques. In this paper, we have deliberately omitted any discussion about quantum advantage for machine learning for two reasons. Firstly, quantum advantage in learning with real-world problems has several caveats [94]. Most results about quantum advantage for machine learning involve ad-hoc datasets, constructed with the specific aim of beating the best classical approach on carefully chosen error metrics. Secondly, it is often complicated to define what the best classical approach is so as to make a fair comparison between quantum and classical learning. Nonetheless, we mention here some future directions to explore the possibility of quantum advantage within the framework presented in this paper. For instance, the generalization error with unconstrained operations depends on the amount of shared information between the space of classical inputs and the space of quantum states. Since quantum states can be used to compress classical data without losing any predictive power [95, 96], it is tempting to conjecture a link between quantum advantage in compressing classical data and quantum advantage in learning, namely in achieving the same accuracy with less data.\n49"
        },
        {
            "heading": "A. Useful mathematical results",
            "text": "We first introduce some general results from probability theory.\nLemma A.1 (Hoeffding\u2019s inequality). Let Xj with j = 1, . . . , n be independent identically distributed random variables with mean \u00b5 = E[Xj ], defined in the interval a \u2264 Xj \u2264 b. For c = b\u2212 a and arbitrary t we get\nP\n\n 1\nn\nn\u2211\nj=1\nXj \u2212 \u00b5 \u2265 t\n\n \u2264 e\u22122nt2/c2 , (A1)\nThe following theorem is useful for mapping inequalities involving a success probability to inequalities involving expectation values.\nTheorem A.2. Suppose that with probability at least 1\u2212 \u03b4, random variable X follows the inequality\nX \u2264 A+ \u221a B + C log(1/\u03b4), (A2)\nfor arbitrary constants (with regard to \u03b4) A and B and positive constant C. The expectation value of X is bounded by\nE[X ] \u2264 A+ \u221a B + 1\n2 eB/Cerfc\n[\u221a\nB\nC\n] \u221a \u03c0C, (A3)\nwhere erfc is the complementary error function.\nProof. The cumulative density function (CDF) of X is an increasing function f(x) that takes values from 0 to 1 such that P (X \u2264 x) = f(x). Let us define\nx = A+ \u221a B + C log(1/\u03b4). (A4)\nThen, we can lower bound the CDF of X as P (X \u2264 x) \u2265 1 \u2212 \u03b4, which is defined for x \u2265 A + \u221a B (or, equivalently, \u03b4 \u2265 0). Rearranging Eq. (A4) to give an expression for 1\u2212 \u03b4 in terms of x, we get\n1\u2212 \u03b4 = 1\u2212 exp [ B \u2212 (A\u2212 x)2\nC\n]\n, (A5)\nagain defined for x \u2265 A+ \u221a B. Therefore, the CDF of x is lower bounded by\nP (X \u2264 x) = f(x) \u2265 g(x) = 1\u2212 exp [ B \u2212 (A\u2212 x)2\nC\n]\n. (A6)\nTo obtain the probability density function (PDF) from the CDF, we can differentiate the CDF, so the (exact) expectation value of X is given by\nE[X ] =\n\u222b \u221e\n\u2212\u221e xf \u2032(x)dx, (A7)\nwhere we note that the lower limit for the true CDF (not the lower bound) may be lower than A+ \u221a B. Evaluating Eq. (A7) for our lower bound on the CDF, we get\n\u222b \u221e\nA+ \u221a B\nxg\u2032(x)dx = A+ \u221a B + 1\n2 eB/Cerfc\n[\u221a\nB\nC\n] \u221a \u03c0C. (A8)\nSince we have a lower bound on the CDF, we have an upper bound on the expectation value. Finally we provide this simple result from Ref. [8].\nLemma A.3. Let Ai be a set of operators and i a random variable with probability distribution pi. Then\nE i\u223cp\n(\u2016Ai\u20161) \u2264 Tr \u221a\nE i\u223cp\n(\nAiA \u2020 i\n)\n, (A9)\nwhere Ei\u223cp f(i) := \u2211 i pif(i).\n50"
        },
        {
            "heading": "1. Bounds on Rademacher Complexities",
            "text": "In this subsection we introduce different bounds on the Rademacher complexity, for both variables and operators.\nTheorem A.4 (Uniform deviation bound[40]). For any loss in [0, 1] and dataset S with N pairs\n1 2 RP (F)\u2212\n\u221a\nlog 2\n2N \u2264 E S [D(F ,S)] \u2264 2RP (F). (A10)\nMoreover, for large N the uniform deviation D(F ,S) is concentrated around its average, namely with arbitrarily high probability 1\u2212 \u03b4\n|E S [D(F ,S)] \u2212D(F ,S)| \u2264\n\u221a\nlog(2/\u03b4)\n2N . (A11)\nEquivalently, using Theorem A.2, we can express Eq. (A11) in terms of the expectation value of the distance from the average value as\nE\n[\n|E S [D(F ,S)] \u2212D(F ,S)|\n] \u2264 \u221a log 2\n2N + erfc\n[\u221a log 2 ] \u221a \u03c0\n2N . (A12)\nTheorem A.5 (Khintchine inequalities [97]). Let xk be real numbers and \u03c3k = \u00b11 be Rademacher variables. Then for all integers p \u2265 1\nAp\u2016x\u20162 \u2264 (\nE \u03c3 \u2223 \u2223 \u2223 \u2223 \u2223 N\u2211\nk=1\n\u03c3kxk \u2223 \u2223 \u2223 \u2223 \u2223 p)1/p \u2264 Bp\u2016x\u20162 (A13)\nwhere \u2016x\u20162 = \u221a\u2211 k x 2 k. Moreover A1 = 1/ \u221a 2 and Ap = 1 for p \u2265 2, while Bp = 1 for p \u2264 2 and Bp = 2\u22121/4 \u221a\n\u03c0p/e for p > 2.\nTheorem A.6 (Operator Khintchine inequalities [98, 99]). Let Xk be Hermitian operators and \u03c3k = \u00b11 be Rademacher variables. Then for each \u03b1 \u2265 2 it holds\n\u2016XR\u2016\u03b1 \u2264 \u03b1 \u221a \u221a \u221a \u221aE\n\u03c3 \u2225 \u2225 \u2225 \u2225 \u2225 \u2211\nk\n\u03c3kXk \u2225 \u2225 \u2225 \u2225 \u2225 \u03b1\n\u03b1\n\u2264 B\u03b1\u2016XR\u2016\u03b1, (A14)\nwhere XR = \u221a\u2211 kX 2 k and B\u03b1 = 2 \u22121/4\u221a\u03c0\u03b1/e is a constant depending on \u03b1 only. On the other hand, for \u03b1 = 1\n1\n2 \u221a e\n\u221a \u2211\nk\n\u2016Xk\u201621 \u2264 E \u03c3 \u2225 \u2225 \u2225 \u2225 \u2225 \u2211\nk\n\u03c3kXk \u2225 \u2225 \u2225 \u2225 \u2225 1 \u2264 \u2016XR\u20161, (A15)\nNote that, since \u2016X\u20161 \u2265 \u2016X\u20162 and \u2016XR\u20161 = Tr[XR] we may simplify the lower bound in the last expression and write\n1\n2 \u221a e \u2016XR\u20162 \u2264 E \u03c3 \u2225 \u2225 \u2225 \u2225 \u2225 \u2211\nk\n\u03c3kXk \u2225 \u2225 \u2225 \u2225 \u2225 1 \u2264 \u2016XR\u20161, (A16)\nTheorem A.7 (Tropp inequality [100]). Let Xk be Hermitian operators of dimension d and \u03c3k = \u00b11 be Rademacher variables. Then\nE \u03c3 \u2225 \u2225 \u2225 \u2225 \u2225 \u2211\nk\n\u03c3kXk \u2225 \u2225 \u2225 \u2225 \u2225 \u221e \u2264 \u221a \u221a \u221a \u221a \u2225 \u2225 \u2225 \u2225 \u2225 \u2211 k X2k \u2225 \u2225 \u2225 \u2225 \u2225 \u221e 2 log d. (A17)\nLemma A.8 (Contraction lemma[7]). Let F be a function class and g a \u03bb-Lipschitz function, then for any sets S\nR(g \u25e6 F ,S) \u2264 \u03bbR(F ,S) (A18)\nwhere g \u25e6 F denotes the set of functions x 7\u2192 g(f(x)).\n51\nTheorem A.9 (credited to Blanchard[24, 101]). Consider a finite set of inputs Ki, with i = 1, . . . ,K, and outputs y = \u00b1. For a training set S = (xn, yn) with n = 1, . . . , N , a set of functions f \u2208 F\u00b1 that assigns the same class f(xn) = \u00b11 to all xn \u2208 Ki, the Rademacher complexity of the 0-1 loss satisfies\n1\n2N\nK\u2211\ni=1\n\u221a\nNi 2 \u2264 R(F\u00b1,S) \u2264 1 2N\nK\u2211\ni=1\n\u221a\nNi, (A19)\nwhere Ni is the number of inputs xn belonging to Ki.\nProof. The 0-1 loss is such that \u211301(f, x, y) = 1\u2212yf(x)\n2 , hence\nR(F\u00b1,S) = E \u03c3\n[\nsup f\u2208F\u00b1\n\u2223 \u2223 \u2223 \u2223 \u2223 1 N N\u2211\nn=1\n\u03c3n 1\u2212 ynf(xn)\n2\n\u2223 \u2223 \u2223 \u2223 \u2223 ] = 1 2N E \u03c3 [ sup f\u2208F\u00b1 \u2223 \u2223 \u2223 \u2223 \u2223 N\u2211\nn=1\n\u03c3nf(xn) \u2223 \u2223 \u2223 \u2223 \u2223 ] = (A20)\n= 1\n2N E \u03c3\n[ K\u2211\ni=1\nsup f\u2208F\u00b1\nf(ki) \u2211\nn:xn\u2208Ki \u03c3n\n]\n= 1\n2N E \u03c3\n[ K\u2211\ni=1\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2211\nn:xn\u2208Ki \u03c3n\n\u2223 \u2223 \u2223 \u2223 \u2223 ] = (A21)\n\u2264 1 2N\nK\u2211\ni=1\n\u221a \u221a \u221a \u221a E \u03c3 ( \u2211\nn:xn\u2208Ki \u03c3n\n)2\n= 1\n2N\nK\u2211\ni=1\n\u221a\nNi, (A22)\nwhich completes the upper bound. The lower bound follows by Khintchine inequalities.\nCorollary A.10.\n1\nN E k\u223cpk\n\u221a Nk \u2264 \u2211\nk: pk\u22641/N pk +\n1\u221a N\n\u2211\nk: pk>1/N\n\u221a pk \u2264 1\u221a N \u2211\nk\n\u221a pk =\n\u221a\n2H1/2(p)\nN . (A23)\nwhere H\u03b1(p) = log2( \u2211 k p \u03b1 k )/(1\u2212 \u03b1) is the \u03b1-Re\u0301nyi entropy.\nProof. see [101].o We now present a new corollary\nCorollary A.11. Consider a finite set of inputs ki, with i = 1, . . . ,K, and outputs y = \u00b1, and a set of functions f \u2208 FB,p that assigns the same outcome f(xn) to all xn \u2208 Ki, with \u2016f\u2016p \u2264 B, and f = (f(k1), . . . , f(kK)). For a training set S = (xn, yn) with n = 1, . . . , N , the Rademacher complexity of any \u03bb-Lipschitz loss satisfies\nR(FB,2,S) \u2264 \u03bbB\u221a N\nR(FB,\u221e,S) \u2264 \u03bbB\nN\nK\u2211\ni=1\n\u221a\nNi. (A24)\nProof. Thanks to the contraction lemma we may write\nR(FB,p,S) \u2264 \u03bb\nN E \u03c3\n[\nsup f\u2208FB,p\n\u2223 \u2223 \u2223 \u2223 \u2223 N\u2211\nn=1\n\u03c3nynf(xn) \u2223 \u2223 \u2223 \u2223 \u2223 ] = \u03bb N E \u03c3 [ K\u2211\ni=1\nsup f\u2208FB,p\nf(ki) \u2211\nn:xn\u2208Ki \u03c3n\n]\n. (A25)\nUsing Cauchy-Schwartz and Jensen inequalities then\nR(FB,2,S) \u2264 \u03bbB\nN E \u03c3\n\u221a \u221a \u221a \u221a K\u2211\ni=1\n( \u2211\nn:xn\u2208Ki \u03c3n\n)2\n\u2264 \u03bbB N\n\u221a \u221a \u221a \u221a K\u2211\ni=1\nE \u03c3\n( \u2211\nn:xn\u2208Ki \u03c3n\n)2\n= \u03bbB\nN\n\u221a \u221a \u221a \u221a K\u2211\ni=1\nNi = \u03bbB\u221a N . (A26)\nOtherwise, using Ho\u0308lder inequality and proceeding as in Thm A.9 we get\nR(FB,\u221e,S) \u2264 B\u03bb\nN E \u03c3\n[ K\u2211\ni=1\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2211\nn:xn\u2208Ki \u03c3n\n\u2223 \u2223 \u2223 \u2223 \u2223 ] \u2264 B\u03bb N K\u2211\ni=1\n\u221a\nNi. (A27)\n52"
        },
        {
            "heading": "B. Derivation of the upper bounds",
            "text": ""
        },
        {
            "heading": "1. Unsupervised Learning under IPM",
            "text": "In this section, we outline the key techniques to derive the upper bound on the optimality gap EIPM(fS,Sf ) in (54). The analysis is inspired by the proof of Theorem 2.1 in [102] which considers the specific Wasserstein IPM. An IPM satisfies the triangle inequality, and hence we can upper bound the absolute value of the generalization error for any model f as\n|GIPM(f,S,Sf )| \u2264 sup h\u2208H \u2223 \u2223 \u2223 E x\u223cP (x) [h(x)] \u2212 1 N N\u2211\nn=1\nh(xn) \u2223 \u2223 \u2223+ sup\nh\u2208H\n\u2223 \u2223 \u2223 E x\u223cf(x) [h(x)]\u2212 1 M M\u2211\nm=1\nh(xfm) \u2223 \u2223 \u2223. (B1)\nThe first term in (B1) quantifies the maximum deviation between the true average, under distribution P (x), and the empirical average over functions h \u2208 H using the training data set, while the second term similarly quantifies the maximum deviation when data is generated according to f(x). Using (B1), the uniform deviation in (52) can be upper bounded as\nDIPM(F ,S,SF ) \u2264 sup h\u2208H \u2223 \u2223 \u2223 E x\u223cP (x) [h(x)] \u2212 1 N N\u2211\nn=1\nh(xn) \u2223 \u2223 \u2223+ sup\nh\u2208H,f\u2208F\n\u2223 \u2223 \u2223 E f(x) [h(x)]\u2212 1 M M\u2211\nm=1\nh(xfm) \u2223 \u2223 \u2223\n= D(H,S) +D(F \u00d7H,SF), (B2) where the first term D(H,S) is the uniform deviation (33) of the discriminator class under data set S (set f = h, F = H and \u2113(f, x, y) = h(x) in (33)); while the second term D(F \u00d7H,SF ) corresponds to the uniform deviation of the combined function space F \u00d7H = {(f, h) : h \u2208 H, f \u2208 F} of discriminator and model under the data set SF . Applying the uniform deviation bound (38) separately on each of the uniform deviations in (B2) yields the scaling of (54)."
        },
        {
            "heading": "2. Supervised Learning with Unconstrained POVM",
            "text": "In this section, we prove the bounds (102). We first note that the optimization over POVM M = {\u03a00,\u03a01} can be cast as an optimization over an observable A such that \u03a0y = (1 + yA)/2, y = \u00b1 and \u2016A\u2016\u221e \u2264 1. Since the constants are averaged out in the calculation of the Rademacher complexity, using the definition (36) we get an exact expression for the empirical Rademacher complexity\nR(M,S) = E \u03c3 sup {\u03a0y}\u2208M\n(\n1\nN\nN\u2211\nn=1\n\u03c3n Tr[\u03a0yn\u03c1(xn)]\n)\n= E \u03c3 sup A:\u2016A\u2016\u221e\u22641\n(\n1\n2N\nN\u2211\nn=1\n\u03c3nyn Tr[A\u03c1(xn)]\n)\n= 1\n2N E \u03c3 \u2225 \u2225 \u2225 \u2225 \u2225 N\u2211\nn=1\n\u03c3n\u03c1(xn) \u2225 \u2225 \u2225 \u2225 \u2225 1 , (B3)\nwhere we used the fact that \u03c3n and \u03c3nyn have the same distribution, as yn = \u00b11, and that, by the Ho\u0308lder inequality, supA:\u2016A\u2016\u221e\u22641 Tr[AB] = \u2016B\u20161. Thanks to the operator Khintchine inequalities (Theorem A.6 in Appendix A) we are now ready to find upper and lower bounds on the empirical Rademacher complexity as\n1\n4 \u221a Ne \u2264 R(M,S) \u2264 1 2N Tr\n\u221a \u221a \u221a \u221a N\u2211\nn=1\n\u03c1(xn)2, (B4)\nwhich proves the second inequality in (102). The upper bound was derived in [8], while from the lower bound we note that the empirical Rademacher complexity decreases at least as O(1/ \u221a N). Finally, the behaviour of the abstract Rademacher complexity in (102) is recovered using properties of the operator square root[8] as\nRN (M) = ES R(M,S) \u2264 1 2N Tr \u221a \u221a \u221a \u221a E S N\u2211\nn=1\n\u03c1(xn)2 = 1\n2 \u221a N Tr\n\u221a \u222b\ndx p(x)\u03c1(x)2 (B5)\nwhere p(x) = \u2211\ny P (x, y) is the marginal distribution.\n53"
        },
        {
            "heading": "3. Rademacher Complexity of the Majority Rule with Unconstrained Local Measurements",
            "text": "In this section, we prove (104) by proceeding as in Eq. (B3) by writing the Rademacher complexity for the loss (103)\nRV (M,S) = E \u03c3 sup {\u03a0y}\u2208M\n(\n1\nN\nN\u2211\nn=1\n\u03c3ncv(Tr[\u03a0yn\u03c1(xn)])\n)\n\u2264 LRV (M,S), (B6)\nwith L \u2265 c\u2032v(p). The inequality is a simple application of the contraction lemma (A.8), where L is the Lipschitz constant of function cv(p). In order to bound such Lipschitz constant, we first note that we may express the cumulative distribution with the regularized incomplete beta function I as cv(p) = I1\u2212p(v + 1, v + 1). Then using the properties of that function\n|c\u2032v\u22121(p)| = [p(1\u2212 p)]v\u22121\nB[v, v] \u2264 1 4v\u22121\n( 2v v ) 2v/v2 \u2264 2 \u221a v \u03c0 , =\u21d2 L = 2 \u221a V + 1 2\u03c0 , (B7)\nwhere B[a, b] is the beta function and we used the fact that p(1\u2212 p) has a maximum for p = 1/2."
        },
        {
            "heading": "4. Supervised Learning with Constrained Observables",
            "text": "We consider the average loss (112), where \u039b is a \u03bb-Lipschitz convex function, and focus on families of observables satisfying some norm constraints\nA\u266f = {A : \u2016A\u2016\u266f \u2264 c\u266f}, (B8)\nwhere \u2016 \u00b7 \u2016\u266f is a suitable norm and c\u266f a fixed constraint. Let \u2016 \u00b7 \u2016\u266d be the dual norm of \u2016 \u00b7 \u2016\u266f, namely the one satisfying \u2016A\u2016\u266d = supB:\u2016B\u2016\u266f\u22641 Tr[AB]. Then using the contraction lemma A.8 we can bound the Rademacher complexity of class A\u266f as\nR(A\u266f,S) \u2264 \u03bb\n2N E \u03c3 sup A\u2208A\u266f\n( N\u2211\nn=1\n\u03c3nyn Tr[A\u03c1(xn)]\n)\n\u2264 \u03bbc\u266f 2N E \u03c3 \u2225 \u2225 \u2225 \u2225 \u2225 N\u2211\nn=1\n\u03c3n\u03c1(xn) \u2225 \u2225 \u2225 \u2225 \u2225 \u266d . (B9)\nFor different norms, in particular for all \u2016 \u00b7 \u2016p norms with dual \u2016 \u00b7 \u2016q, where 1/p+1/q = 1, we can now employ use the operator Khintchine inequalities (cf. Theorem A.6) to find a bound on the Rademacher complexity. In particular, in A\u221e we get the 1-norm, so is equivalent to that obtained for quantum states. As another example, consider observables with bounded 2-norm. In that case, using Jensen\u2019s inequality we find\nG2 \u2264 O\n\n \u03bbc2 N\n\u221a \u221a \u221a \u221a N\u2211\ni=1\nTr[\u03c1(xn)2]\n  \u2264 O ( \u03bbc2\u221a N ) , (B10)\nsince the purity is at most 1. Something similar is obtained for A1 using the Tropp inequality (cf. Theorem A.7), but with an extra factor due to the logarithm of the dimension d of the quantum states.\nG1 \u2264 O\n\n \u03bbc1 N\n\u221a \u221a \u221a \u221a \u2225 \u2225 \u2225 \u2225 \u2225 N\u2211\ni=1\n\u03c1(xn)2 \u2225 \u2225 \u2225 \u2225 \u2225 \u221e log d\n  \u2264 O (\n\u03bbc1\n\u221a\nlog d\nN\n)\n. (B11)"
        },
        {
            "heading": "C. Generalization error with Fourier-like embeddings",
            "text": "Consider the setting of Section 1.9.1, where\n|\u03c8(x)\u3009 = 1\u221a |\u2126|\n\u2211 \u03c9\u2208\u2126 ei\u03c9x |\u03c6\u03c9\u3009 , (C1)\n54\n\u2126 defines a set of allowed \u201cfrequencies\u201d, and |\u2126| is the number of frequencies. In the expression (102) for the bound on the empirical Rademacher complexity, we need to study the average states\n1\nN\nN\u2211\nn=1\n|\u03c8(xn)\u3009\u3008\u03c8(xn)| = 1 N |\u2126| \u2211\nn\n\u2211\n\u03c9,\u03c9\u2032\u2208\u2126 eixn(\u03c9\u2212\u03c9 \u2032) |\u03c6\u03c9\u3009\u3008\u03c6\u03c9\u2032 | . (C2)\nLet F be the |\u2126| \u00d7 |\u2126| Hermitian matrix\nF\u03c9,\u03c9\u2032 = 1 N |\u2126| \u2211\nn\neixn(\u03c9\u2212\u03c9 \u2032). F =\n1 N |\u2126| \u2211\nn\n\u2211\n\u03c9,\u03c9\u2032\u2208\u2126 eixn\u03c9 |\u03c6\u03c9\u3009\u3008\u03c6\u03c9\u2032 | e\u2212ixn\u03c9 \u2032 . (C3)\nDiagonalizing it we get F = S\u2020\u03d5S. Calling |\u03d5\u03bb\u3009 = \u2211 \u03c9\u2208\u2126 S\u03bb,\u03c9 |\u03c6\u03c9\u3009 for \u03bb = 1, . . . , |\u2126| then\n\u03c1\u0304 = 1\nN\nN\u2211\nn=1\n|\u03c8(xn)\u3009\u3008\u03c8(xn)| = |\u2126| \u2211\n\u03bb=1\n\u03d5\u03bb |\u03d5\u03bb\u3009\u3008\u03d5\u03bb| . (C4)\nSince F is a non-negative and Tr[F ] = 1, \u03d5 defines a probability distribution. From Eq. (102) we get\nB(M,S) \u2264 Tr[\u221a\u03c1\u0304]2 = 2H1/2(\u03c1\u0304) \u2264 2H1/2(\u03d5) \u2264 |\u2126|. (C5)\nThe first inequality comes from (102). The second one comes since a mixture of non-orthogonal quantum states has always less entropy than the mixture itself. This can be proven easily by adapting a similar proof for the von Neumann entropy [38], by defining the pure state |\u03d5AB\u3009 = \u2211\n\u03bb\n\u221a \u03d5\u03bb |\u03d5\u03bb\u3009 |\u03bb\u3009, and noting that H1/2(A) = H1/2(B) \u2264 H1/2(B\u2032)\nwhere B\u2032 is state of B after performing a projective measurement on the |\u03bb\u3009 basis, and the inequality follows from the data-processing inequality under unital channels."
        },
        {
            "heading": "D. On Learning to Discriminate: Average-Case Error vs. Worst-Case Error",
            "text": "As discussed in section 4, the problem of state discrimination can be formulated as the determination of whether the training and test systems are in either of the states (76), where the two component states \u03c1+ and \u03c1\u2212 are unknown. As in classical statistics, one can formulate the problem in a Bayesian, or average-case setting, attaching a prior distribution to the unknown states; or in a min-max, or worst-case, setting, in which performance is maximized by assuming the worst-case pair of unknown states \u03c1+ and \u03c1\u2212 (see, e.g., [103]). In a Bayesian formulation, we represent the learner\u2019s prior uncertainty about what the states \u03c1+ and \u03c1\u2212 via a classical probability distribution p(\u03c1+, \u03c1\u2212), which is defined over the space of density matrices \u03c1+ and \u03c1\u2212. For instance, if the learner only knows that both states are pure qubit states, the prior p(\u03c1+, \u03c1\u2212) may be selected as a product uniform distribution over all pairs of states on the surface of the Bloch sphere. Having selected a prior distribution p(\u03c1+, \u03c1\u2212), the task at hand can be expressed as the single-shot discrimination between the average states \u03c3+ and \u03c3\u2212 defined as\n\u03c3\u00b1 = \u222b \u222b\np(\u03c1+, \u03c1\u2212) \u03c1 \u2297S + \u2297 \u03c1\u2297S\u2212 \ufe38 \ufe37\ufe37 \ufe38\ntraining\n\u2297\u03c1\u2297V\u00b1 d\u03c1+d\u03c1\u2212, (D1)\nwhere the integral can be replaced with a sum if the prior is defined over a discrete set of possible state pairs. Therefore, the optimal solution that minimizes the average detection probability is given by the Helstrom measurement between states \u03c3+ and \u03c3\u2212, which is defined by the projection matrices\n\u03a0opt\u00b1 = 1\u00b1 sign(\u03c3+ \u2212 \u03c3\u2212)\n2 =\n1\n2\n( 1\u00b1 \u222b \u222b\np(\u03c1+, \u03c1\u2212)\u03c1 \u2297S + \u2297 \u03c1\u2297S\u2212 \u2297 sign(\u03c1\u2297V+ \u2212 \u03c1\u2297V\u2212 )d\u03c1+d\u03c1\u2212\n)\n. (D2)\nClearly, the optimal measurement is highly dependent on the a priori distribution, p(\u03c1+, \u03c1\u2212). If we were to take an inductive approach, our measurement would need to be expressed in the inductive-learning form [104, 105], via a binary POVM {M, I\u2212M} operating on system (76) with measurement operators of the form\nM =\nk\u2211\ni=1\nAi \u2297Bi, (D3)\n55\nwhere {A1, A2, ...Ak} is a set of measurement operators defining a valid measurement on the training set, whilst each set {Bi, I \u2212 Bi}, for all i = 1, ..., k, defines a valid measurement on the test states. In words, an inductive learning strategy involves carrying out a measurement (or adaptive set of measurements) on the training set in order to extract information about which measurement to carry out on the test states in order to best discriminate between the possible test states \u03c1\u2297V+ and \u03c1 \u2297V \u2212 . There is no guarantee that the optimal measurement from Eq. (D2) can be expressed in this form. For specific a priori distributions, the exact form of the optimal measurement (D2), along with the minimum average error, are known. For example, when V = 1 and the prior is uniform distribution over the surface of the Bloch sphere, it has been shown that the optimum measurement, whose performance was studied in [106], can be achieved with a specific inductive scheme[107]. Similar cases with V > 1[108], mixed states, and qudits[109] have also been studied in this context. In a worst-case formulation, optimality is formulated as the maximization of the minimum detection probability, where the inner minimum is taken over all possible pairs of states {\u03c1+, \u03c1\u2212} within some set of interest. This is the formulation we use in this article."
        },
        {
            "heading": "Acknowledgements",
            "text": "L.B. acknowledges financial support from PNRR MUR project PE0000023-NQSTI. J.P. acknowledges funding from the U.S. Department of Energy, Office of Science, Superconducting Quantum Materials and Systems Center (SQMS) under the Contract No. DE-AC02-07CH11359. The work of O.S. was supported by the European Union\u2019s Horizon Europe project CENTRIC (101096379) and by an Open Fellowship of the EPSRC (EP/W024101/1).\n[1] J. Biamonte, P. Wittek, N. Pancotti, P. Rebentrost, N. Wiebe, and S. Lloyd, Quantum machine learning, Nature 549, 195 (2017). [2] V. Gebhart, R. Santagati, A. A. Gentile, E. M. Gauger, D. Craig, N. Ares, L. Banchi, F. Marquardt, L. Pezze\u0300, and C. Bonato, Learning quantum systems, Nature Reviews Physics 5, 141 (2023). [3] M. Schuld and F. Petruccione, Machine learning with quantum computers (Springer, 2021). [4] O. Simeone et al., An introduction to quantum machine learning for engineers, Foundations and Trends\u00ae in Signal\nProcessing 16, 1 (2022). [5] C. Ciliberto, M. Herbster, A. D. Ialongo, M. Pontil, A. Rocchetto, S. Severini, and L. Wossnig, Quantum machine\nlearning: a classical perspective, Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences 474, 20170551 (2018). [6] V. Dunjko and H. J. Briegel, Machine learning & artif. intell. in the quantum domain: a review of recent progress, Rep. Prog. Phys. 81, 074001 (2018). [7] S. Shalev-Shwartz and S. Ben-David, Understanding machine learning: From theory to algorithms (Cambridge university press, 2014). [8] L. Banchi, J. Pereira, and S. Pirandola, Generalization in quantum machine learning: A quantum information standpoint, PRX Quantum 2, 040321 (2021).\n[9] S. Arunachalam and R. de Wolf, Guest column: A survey of quantum learning theory, ACM Sigact News 48, 41 (2017). [10] H.-Y. Huang, M. Broughton, M. Mohseni, R. Babbush, S. Boixo, H. Neven, and J. R. McClean, Power of data in quantum\nmachine learning, Nature Communications 12, 2631 (2021). [11] H.-Y. Huang, R. Kueng, and J. Preskill, Information-theoretic bounds on quantum advantage in machine learning, Physical\nReview Letters 126, 190505 (2021). [12] M. C. Caro, E. Gil-Fuster, J. J. Meyer, J. Eisert, and R. Sweke, Encoding-dependent generalization bounds for\nparametrized quantum circuits, Quantum 5, 582 (2021). [13] M. C. Caro, H.-Y. Huang, M. Cerezo, K. Sharma, A. Sornborger, L. Cincio, and P. J. Coles, Generalization in quantum\nmachine learning from few training data, Nature communications 13, 4919 (2022). [14] Y. Du, Y. Yang, D. Tao, and M.-H. Hsieh, Demystify problem-dependent power of quantum neural networks on multi-class\nclassification, arXiv preprint arXiv:2301.01597 (2022). [15] A. Abbas, D. Sutter, C. Zoufal, A. Lucchi, A. Figalli, and S. Woerner, The power of quantum neural networks, Nature\nComputational Science 1, 403 (2021). [16] T. Haug, K. Bharti, and M. Kim, Capacity and quantum geometry of parametrized quantum circuits, PRX Quantum 2,\n040309 (2021). [17] C. Gyurik, V. Dunjko, et al., Structural risk minimization for quantum linear classifiers, Quantum 7, 893 (2023). [18] H.-Y. Huang, M. Broughton, J. Cotler, S. Chen, J. Li, M. Mohseni, H. Neven, R. Babbush, R. Kueng, J. Preskill, et al.,\nQuantum advantage in learning from experiments, Science 376, 1182 (2022). [19] L. Banchi, Q. Zhuang, and S. Pirandola, Quantum-enhanced barcode decoding and pattern recognition, Physical Review\nApplied 14, 064026 (2020). [20] G. Carleo, I. Cirac, K. Cranmer, L. Daudet, M. Schuld, N. Tishby, L. Vogt-Maranto, and L. Zdeborova\u0301, Machine learning\nand the physical sciences, Reviews of Modern Physics 91, 045002 (2019).\n56\n[21] M. Benedetti, D. Garcia-Pintos, O. Perdomo, V. Leyton-Ortega, Y. Nam, and A. Perdomo-Ortiz, A generative modeling approach for benchmarking and training shallow quantum circuits, npj Quantum Information 5, 45 (2019). [22] H. Situ, Z. He, Y. Wang, L. Li, and S. Zheng, Information Sciences 538, 193 (2020). [23] J. Romero and A. Aspuru-Guzik, Variational quantum generators: Generative adversarial quantum machine learning for\ncontinuous distributions, Advanced Quantum Technologies 4, 2000003 (2021). [24] C. D. Scott and R. D. Nowak, Learning minimum volume sets, Journal of Machine Learning Research 7, 665 (2006). [25] N. Ezzell, E. M. Ball, A. U. Siddiqui, M. M. Wilde, A. T. Sornborger, P. J. Coles, and Z. Holmes, Quantum mixed state\ncompiling, Quantum Science and Technology 8, 035001 (2023). [26] P. Braccia, L. Banchi, and F. Caruso, Quantum noise sensing by generating fake noise, Physical Review Applied 17,\n024002 (2022). [27] M. Benedetti, E. Grant, L. Wossnig, and S. Severini, Adversarial quantum circuit learning for pure state approximation,\nNew Journal of Physics 21, 043023 (2019). [28] P. Braccia, F. Caruso, and L. Banchi, How to enhance quantum generative adversarial learning of noisy information, New\nJournal of Physics 23, 053024 (2021). [29] L. Maccone, Entropic information-disturbance tradeoff, Europhysics Letters 77, 40002 (2007). [30] V. Vovk, A. Gammerman, and G. Shafer, Algorithmic learning in a random world (Springer, 2023). [31] S. Park and O. Simeone, Quantum conformal prediction for reliable uncertainty quantification in quantum machine\nlearning, arXiv preprint arXiv:2304.03398 (2023). [32] K. Fujii and K. Nakajima, Harnessing disordered-ensemble quantum dynamics for machine learning, Physical Review\nApplied 8, 024030 (2017). [33] P. Mujal, R. Mart\u0301\u0131nez-Pen\u0303a, J. Nokkala, J. Garc\u0301\u0131a-Beni, G. L. Giorgi, M. C. Soriano, and R. Zambrini, Opportunities in\nquantum reservoir computing and extreme learning machines, Advanced Quantum Technologies 4, 2100027 (2021). [34] H.-Y. Huang, R. Kueng, and J. Preskill, Predicting many properties of a quantum system from very few measurements,\nNature Physics 16, 1050 (2020). [35] S. Lloyd, M. Schuld, A. Ijaz, J. Izaac, and N. Killoran, Quantum embeddings for machine learning, arXiv preprint\narXiv:2001.03622 (2020). [36] S. Kimmel, C. Y.-Y. Lin, G. H. Low, M. Ozols, and T. J. Yoder, Hamiltonian simulation with optimal sample complexity,\nnpj Quantum Information 3, 13 (2017). [37] M. Schuld, Supervised quantum machine learning models are kernel methods, arXiv preprint arXiv:2101.11020 (2021). [38] M. A. Nielsen and I. L. Chuang, Quantum Computation and Quantum Information (Cambridge University Press, Cam-\nbridge, 2000). [39] R. Konig, R. Renner, and C. Schaffner, The operational meaning of min-and max-entropy, IEEE Transactions on Infor-\nmation theory 55, 4337 (2009). [40] P. L. Bartlett, A. Montanari, and A. Rakhlin, Deep learning: a statistical viewpoint, Acta numerica 30, 87 (2021). [41] M. Ragone, P. Braccia, Q. T. Nguyen, L. Schatzki, P. J. Coles, F. Sauvage, M. Larocca, and M. Cerezo, Representation\ntheory for geometric quantum machine learning, arXiv preprint arXiv:2210.07980 (2022). [42] Y. Du, Z. Tu, X. Yuan, and D. Tao, Efficient measure for the expressivity of variational quantum algorithms, Physical\nReview Letters 128, 080506 (2022). [43] M. Schuld, R. Sweke, and J. J. Meyer, Effect of data encoding on the expressive power of variational quantum-machine-\nlearning models, Physical Review A 103, 032430 (2021). [44] A. Canatar, E. Peters, C. Pehlevan, S. M. Wild, and R. Shaydulin, Bandwidth enables generalization in quantum kernel\nmodels, arXiv preprint arXiv:2206.06686 (2022). [45] S. Sachdev, Quantum phase transitions, Physics world 12, 33 (1999). [46] X.-Y. Dong, F. Pollmann, X.-F. Zhang, et al., Machine learning of quantum phase transitions, Physical Review B 99,\n121104 (2019). [47] P. Huembeli, A. Dauphin, and P. Wittek, Identifying quantum phase transitions with adversarial neural networks, Physical\nReview B 97, 134109 (2018). [48] B. S. Rem, N. Ka\u0308ming, M. Tarnowski, L. Asteria, N. Fla\u0308schner, C. Becker, K. Sengstock, and C. Weitenberg, Identifying\nquantum phase transitions using artificial neural networks on experimental data, Nature Physics 15, 917 (2019). [49] H.-Y. Huang, R. Kueng, G. Torlai, V. V. Albert, and J. Preskill, Provably efficient machine learning for quantum many-\nbody problems, Science 377, eabk3333 (2022). [50] R. Horodecki, P. Horodecki, M. Horodecki, and K. Horodecki, Quantum entanglement, Reviews of modern physics 81,\n865 (2009). [51] K. Kawaguchi, Y. Bengio, and L. Kaelbling, Mathematical aspects of deep learning (Cambridge University Press, 2022)\nChap. Generalization in Deep Learning, p. 112\u2013148. [52] S. Arora, R. Ge, Y. Liang, T. Ma, and Y. Zhang, Generalization and equilibrium in generative adversarial nets (gans),\nin International Conference on Machine Learning (PMLR, 2017) pp. 224\u2013232. [53] P. Zhang, Q. Liu, D. Zhou, T. Xu, and X. He, On the discrimination-generalization tradeoff in gans, arXiv preprint\narXiv:1711.02771 (2017). [54] O. Simeone, Machine learning for engineers (Cambridge University Press, 2022). [55] P. L. Bartlett and S. Mendelson, Rademacher and gaussian complexities: Risk bounds and structural results, Journal of\nMachine Learning Research 3, 463 (2002). [56] A. Jacot, F. Gabriel, and C. Hongler, Neural tangent kernel: Convergence and generalization in neural networks, in\nAdvances in Neural Information Processing Systems , Vol. 31, edited by S. Bengio, H. Wallach, H. Larochelle, K. Grau-\n57\nman, N. Cesa-Bianchi, and R. Garnett (Curran Associates, Inc., 2018). [57] S. P. Boyd and L. Vandenberghe, Convex Optimization (Cambridge University Press, 2004). [58] N. Golowich, A. Rakhlin, and O. Shamir, Size-independent sample complexity of neural networks, Information and\nInference: A Journal of the IMA 9, 473 (2020). [59] K. P. Murphy, Probabilistic machine learning: an introduction (MIT press, 2022). [60] M. Arjovsky, S. Chintala, and L. Bottou, Wasserstein generative adversarial networks, in International conference on\nmachine learning (PMLR, 2017) pp. 214\u2013223. [61] S. Pirandola, U. L. Andersen, L. Banchi, M. Berta, D. Bunandar, R. Colbeck, D. Englund, T. Gehring, C. Lupo,\nC. Ottaviani, et al., Advances in quantum cryptography, Advances in optics and photonics 12, 1012 (2020). [62] S.-H. Tan, B. I. Erkmen, V. Giovannetti, S. Guha, S. Lloyd, L. Maccone, S. Pirandola, and J. H. Shapiro, Quantum\nillumination with gaussian states, Physical review letters 101, 253601 (2008). [63] L. Maccone and C. Ren, Quantum radar, Physical Review Letters 124, 200503 (2020). [64] Q. Zhuang and S. Pirandola, Ultimate limits for multiple quantum channel discrimination, Physical Review Letters 125,\n080505 (2020). [65] A. S. Holevo, Statistical decision theory for quantum systems, Journal of multivariate analysis 3, 337 (1973). [66] C. W. Helstrom and C. W. Helstrom, Quantum detection and estimation theory, Vol. 3 (Academic press New York, 1976). [67] P. Hausladen and W. K. Wootters, A pretty good measurement for distinguishing quantum states, Journal of Modern\nOptics 41, 2385 (1994). [68] H. Barnum and E. Knill, Reversing quantum dynamics with near-optimal quantum and classical fidelity, Journal of\nMathematical Physics 43, 2097 (2002). [69] A. Montanaro, Pretty simple bounds on quantum state discrimination, arXiv preprint arXiv:1908.08312 (2019). [70] W. Matthews, S. Wehner, and A. Winter, Distinguishability of quantum states under restricted families of measurements\nwith an application to quantum data hiding, Communications in Mathematical Physics 291, 813 (2009). [71] W. K. Wootters and W. H. Zurek, A single quantum cannot be cloned, Nature 299, 802 (1982). [72] A. S. Holevo, Bounds for the quantity of information transmitted by a quantum communication channel, Problemy\nPeredachi Informatsii 9, 3 (1973). [73] A. Ac\u0301\u0131n, E. Bagan, M. Baig, L. Masanes, and R. Munoz-Tapia, Multiple-copy two-state discrimination with individual\nmeasurements, Physical Review A 71, 032338 (2005). [74] R. Ash, Information Theory , Dover books on advanced mathematics (Dover Publications, 1990). [75] W. Ren, W. Li, S. Xu, K. Wang, W. Jiang, F. Jin, X. Zhu, J. Chen, Z. Song, P. Zhang, et al., Experimental quantum\nadversarial learning with programmable superconducting qubits, Nature Computational Science 2, 711 (2022). [76] L. Banchi, Robust quantum classifiers via nisq adversarial learning, Nature Computational Science 2, 699 (2022). [77] G. De Palma, M. Marvian, D. Trevisan, and S. Lloyd, The quantum wasserstein distance of order 1, IEEE Transactions\non Information Theory 67, 6627 (2021). [78] G. De Palma, M. Marvian, C. Rouze\u0301, and D. S. Franc\u0327a, Limitations of variational quantum algorithms: a quantum\noptimal transport approach, PRX Quantum 4, 010309 (2023). [79] P. Derbeko, R. El-Yaniv, and R. Meir, Explicit learning curves for transduction and application to clustering and com-\npression algorithms, Journal of Artificial Intelligence Research 22, 117 (2004). [80] J. Haah, A. W. Harrow, Z. Ji, X. Wu, and N. Yu, Sample-optimal tomography of quantum states, in Proceedings of the\nforty-eighth annual ACM symposium on Theory of Computing (2016) pp. 913\u2013925. [81] A. Gilye\u0301n, S. Lloyd, I. Marvian, Y. Quek, and M. M. Wilde, Quantum algorithm for petz recovery channels and pretty\ngood measurements, Physical Review Letters 128, 220502 (2022). [82] S. Lloyd, M. Mohseni, and P. Rebentrost, Quantum principal component analysis, Nature Physics 10, 631 (2014). [83] A. Barenco, A. Berthiaume, D. Deutsch, A. Ekert, R. Jozsa, and C. Macchiavello, Stabilization of quantum computations\nby symmetrization, SIAM Journal on Computing 26, 1541 (1997). [84] A. Monra\u0300s, G. Sent\u0301\u0131s, and P. Wittek, Inductive Supervised Quantum Learning, Phys. Rev. Lett. 118, 190503 (2017). [85] R. M. Dudley, The sizes of compact subsets of hilbert space and continuity of gaussian processes, Journal of Functional\nAnalysis 1, 290 (1967). [86] A QNN with Ng trainable parameters is said to be overparameterized if the quantum Fischer information (QFI) matrices,\nevaluated simultaneously for all data in the training set, saturates (i.e, has maximal rank) on at least one point in the parameter space [110]. [87] Q.-W. Zeng, H.-Y. Ge, C. Gong, and N.-R. Zhou, Conditional quantum circuit born machine based on a hybrid quantum\u2013 classical framework, Physica A: Statistical Mechanics and its Applications 618, 128693 (2023). [88] C. Zoufal, A. Lucchi, and S. Woerner, Quantum generative adversarial networks for learning and loading random distributions, npj Quantum Information 5, 103 (2019). [89] Y. Du, Z. Tu, B. Wu, X. Yuan, and D. Tao, Theory of quantum generative learning models with maximum mean discrepancy, arXiv preprint arXiv:2205.04730 (2022). [90] S. Chakrabarti, H. Yiming, T. Li, S. Feizi, and X. Wu, Quantum wasserstein generative adversarial networks, in Advances in Neural Information Processing Systems , Vol. 32, edited by H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche\u0301-Buc, E. Fox, and R. Garnett (Curran Associates, Inc., 2019). [91] P.-L. Dallaire-Demers and N. Killoran, Quantum generative adversarial networks, Physical Review A 98, 012324 (2018). [92] K. Gili, M. Mauri, and A. Perdomo-Ortiz, Evaluating generalization in classical and quantum generative models, arXiv\npreprint arXiv:2201.08770 (2022). [93] S. T. Jose and O. Simeone, Transfer learning for quantum classifiers: An information-theoretic generalization analysis, in\n58\n2023 IEEE Information Theory Workshop (ITW) (IEEE, 2023) pp. 532\u2013537. [94] S. Aaronson, Read the fine print, Nature Physics 11, 291 (2015). [95] T. J. Elliott, C. Yang, F. C. Binder, A. J. Garner, J. Thompson, and M. Gu, Extreme dimensionality reduction with\nquantum modeling, Physical Review Letters 125, 260501 (2020). [96] R. Raz, Exponential separation of quantum and classical communication complexity, in Proceedings of the thirty-first\nannual ACM symposium on Theory of computing (1999) pp. 358\u2013367. [97] U. Haagerup, The best constants in the khintchine inequality, Studia Mathematica 70, 231 (1981). [98] F. Lust-Piquard, Ine\u0301galite\u0301s de Khintchine dans Cp (1 < p <\u221e), CR Acad. Sci. Paris 303, 289 (1986). [99] E. Candes and B. Recht, Exact matrix completion via convex optimization, Communications of the ACM 55, 111 (2012). [100] J. A. Tropp et al., An introduction to matrix concentration inequalities, Foundations and Trends\u00ae in Machine Learning 8, 1 (2015). [101] D. Cohen, A. Kontorovich, and G. Wolfer, Learning discrete distributions with infinite support, Advances in Neural Information Processing Systems 33, 3942 (2020). [102] Y. Bai, T. Ma, and A. Risteski, Approximability of discriminators implies diversity in gans, arXiv preprint arXiv:1806.10586 (2018). [103] H. V. Poor, An introduction to signal detection and estimation (Springer Science & Business Media, 1998). [104] G. Sent\u0301\u0131s, M. Gut\u0327a\u0306, and G. Adesso, Quantum learning of coherent states, EPJ Quantum Technol. 2, 1 (2015). [105] M. Fanizza, M. Skotiniotis, J. Calsamiglia, R. Mun\u0303oz-Tapia, and G. Sent\u0301\u0131s, Universal algorithms for quantum data learning, EPL 140, 28001 (2022). [106] A. Hayashi, M. Horibe, and T. Hashimoto, Quantum pure-state identification, Phys. Rev. A 72, 052306 (2005). [107] G. Sent\u0301\u0131s, J. Calsamiglia, R. Mun\u0303oz-Tapia, and E. Bagan, Quantum learning without quantum memory, Sci Rep 2, 708 (2012). [108] B. He and J. A. Bergou, Programmable unknown quantum-state discriminators with multiple copies of program and data: A Jordan-basis approach, Phys. Rev. A 75, 032316 (2007). [109] M. Fanizza, A. Mari, and V. Giovannetti, Optimal Universal Learning Machines for Quantum State Discrimination, IEEE Transactions on Information Theory 65, 5931 (2019). [110] M. Larocca, N. Ju, D. Garc\u0301\u0131a-Mart\u0301\u0131n, P. J. Coles, and M. Cerezo, Theory of overparametrization in quantum neural\nnetworks, Nature Computational Science 3, 542 (2023)."
        }
    ],
    "title": "Statistical Complexity of Quantum Learning",
    "year": 2023
}