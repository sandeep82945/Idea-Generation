{
    "abstractText": "Modeling subrational agents, such as humans or economic households, is inherently challenging due to the difficulty in calibrating reinforcement learning models or collecting data that involves human subjects. Existing work highlights the ability of Large Language Models (LLMs) to address complex reasoning tasks and mimic human communication, while simulation using LLMs as agents shows emergent social behaviors, potentially improving our comprehension of human conduct. In this paper, we propose to investigate the use of LLMs to generate synthetic human demonstrations, which are then used to learn subrational agent policies though Imitation Learning. We make an assumption that LLMs can be used as implicit computational models of humans, and propose a framework to use synthetic demonstrations derived from LLMs to model subrational behaviors that are characteristic of humans (e.g., myopic behavior or preference for risk aversion). We experimentally evaluate the ability of our framework to model sub-rationality through four simple scenarios, including the well-researched ultimatum game and marshmallow experiment. To gain confidence in our framework, we are able to replicate well-established findings from prior human studies associated with the above scenarios. We conclude by discussing the potential benefits, challenges and limitations of our framework. Equal contribution Applied Research Team (ART) IT Department, Bank of Italy, Rome, Italy. This research work was carried out when Andrea Coletta was employed at J.P. Morgan AI Research. The views expressed in this paper are those of the authors and do not necessarily reflect those of the Bank of Italy. J.P. Morgan AI Research, California, USA. J.P. Morgan AI Research, New York, USA. Correspondence to: Andrea Coletta <andrea.coletta@bancaditalia.it>, Kshama Dwarakanath <kshama.dwarakanath@jpmorgan.com>, Penghang Liu <penghang.liu@jpmchase.com>.",
    "authors": [
        {
            "affiliations": [],
            "name": "Andrea Coletta"
        },
        {
            "affiliations": [],
            "name": "Kshama Dwarakanath"
        },
        {
            "affiliations": [],
            "name": "Penghang Liu"
        },
        {
            "affiliations": [],
            "name": "Svitlana Vyetrenko"
        },
        {
            "affiliations": [],
            "name": "Tucker Balch"
        }
    ],
    "id": "SP:6aa34eaa73873a865207dc90aa578921735a7d82",
    "references": [
        {
            "authors": [
                "J. Abeler",
                "D. Nosenzo"
            ],
            "title": "Self-selection into laboratory experiments: pro-social motives versus monetary incentives",
            "venue": "Experimental Economics,",
            "year": 2015
        },
        {
            "authors": [
                "G.V. Aher",
                "R.I. Arriaga",
                "A.T. Kalai"
            ],
            "title": "Using large language models to simulate multiple humans and replicate human subject studies",
            "venue": "In International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "G.A. Akerlof",
                "R.J. Shiller"
            ],
            "title": "Animal spirits: How human psychology drives the economy, and why it matters for global capitalism",
            "venue": "Princeton university press,",
            "year": 2010
        },
        {
            "authors": [
                "Angeletos",
                "G.-M",
                "D. Laibson",
                "A. Repetto",
                "J. Tobacman",
                "S. Weinberg"
            ],
            "title": "The hyperbolic consumption model: Calibration, simulation, and empirical evaluation",
            "venue": "Journal of Economic perspectives,",
            "year": 2001
        },
        {
            "authors": [
                "L.P. Argyle",
                "E.C. Busby",
                "N. Fulda",
                "J.R. Gubler",
                "C. Rytting",
                "D. Wingate"
            ],
            "title": "Out of one, many: Using language models to simulate human samples",
            "venue": "Political Analysis,",
            "year": 2023
        },
        {
            "authors": [
                "N.C. Barberis"
            ],
            "title": "Thirty years of prospect theory in economics: A review and assessment",
            "venue": "Journal of economic perspectives,",
            "year": 2013
        },
        {
            "authors": [
                "S. Benartzi",
                "R.H. Thaler"
            ],
            "title": "Myopic loss aversion and the equity premium puzzle",
            "venue": "The quarterly journal of Economics,",
            "year": 1995
        },
        {
            "authors": [
                "J. Benhabib",
                "A. Bisin",
                "A. Schotter"
            ],
            "title": "Present-bias, quasihyperbolic discounting, and fixed costs",
            "venue": "Games and economic behavior,",
            "year": 2010
        },
        {
            "authors": [
                "R. Bommasani",
                "D.A. Hudson",
                "E. Adeli",
                "R. Altman",
                "S. Arora",
                "S. von Arx",
                "M.S. Bernstein",
                "J. Bohg",
                "A. Bosselut",
                "E Brunskill"
            ],
            "title": "On the opportunities and risks of foundation models",
            "venue": "arXiv preprint arXiv:2108.07258,",
            "year": 2021
        },
        {
            "authors": [
                "Bouchaud",
                "J.-P",
                "J. Bonart",
                "J. Donier",
                "M. Gould"
            ],
            "title": "Trades, quotes and prices: financial markets under the microscope",
            "year": 2018
        },
        {
            "authors": [
                "T. Brown",
                "B. Mann",
                "N. Ryder",
                "M. Subbiah",
                "J.D. Kaplan",
                "P. Dhariwal",
                "A. Neelakantan",
                "P. Shyam",
                "G. Sastry",
                "A Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in neural information processing systems,",
            "year": 1877
        },
        {
            "authors": [
                "S. Bubeck",
                "V. Chandrasekaran",
                "R. Eldan",
                "J. Gehrke",
                "E. Horvitz",
                "E. Kamar",
                "P. Lee",
                "Y.T. Lee",
                "Y. Li",
                "S Lundberg"
            ],
            "title": "Sparks of artificial general intelligence: Early experiments with gpt-4",
            "venue": "arXiv preprint arXiv:2303.12712,",
            "year": 2023
        },
        {
            "authors": [
                "M. Carroll",
                "R. Shah",
                "M.K. Ho",
                "T. Griffiths",
                "S. Seshia",
                "P. Abbeel",
                "A. Dragan"
            ],
            "title": "On the utility of learning about humans for human-ai coordination",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "L. Chan",
                "A. Critch",
                "A. Dragan"
            ],
            "title": "Human irrationality: both bad and good for reward inference",
            "venue": "arXiv preprint arXiv:2111.06956,",
            "year": 2021
        },
        {
            "authors": [
                "Chiang",
                "C.-H",
                "Lee",
                "H.-y"
            ],
            "title": "Can large language models be an alternative to human evaluations",
            "venue": "arXiv preprint arXiv:2305.01937,",
            "year": 2023
        },
        {
            "authors": [
                "A. Coletta",
                "A. Moulin",
                "S. Vyetrenko",
                "T. Balch"
            ],
            "title": "Learning to simulate realistic limit order book markets from data as a world agent",
            "venue": "In Proceedings of the Third ACM International Conference on AI in Finance,",
            "year": 2022
        },
        {
            "authors": [
                "A. Correia",
                "L.A. Alexandre"
            ],
            "title": "A survey of demonstration learning",
            "venue": "arXiv preprint arXiv:2303.11191,",
            "year": 2023
        },
        {
            "authors": [
                "V. Day",
                "D. Mensink",
                "M. O\u2019Sullivan"
            ],
            "title": "Patterns of academic procrastination",
            "venue": "Journal of College Reading and Learning,",
            "year": 2000
        },
        {
            "authors": [
                "S. Desposato"
            ],
            "title": "Ethics and experiments: Problems and solutions for social scientists and policy professionals",
            "year": 2015
        },
        {
            "authors": [
                "J. Dong",
                "K. Dwarakanath",
                "S. Vyetrenko"
            ],
            "title": "Analyzing the impact of tax credits on households in simulated economic systems with learning agents",
            "venue": "arXiv preprint arXiv:2311.17252,",
            "year": 2023
        },
        {
            "authors": [
                "A.D. Dragan",
                "S. Bauman",
                "J. Forlizzi",
                "S.S. Srinivasa"
            ],
            "title": "Effects of robot motion on human-robot collaboration",
            "venue": "In Proceedings of the Tenth Annual ACM/IEEE International Conference on Human-Robot Interaction,",
            "year": 2015
        },
        {
            "authors": [
                "J.D. Farmer",
                "D. Foley"
            ],
            "title": "The economy needs agent-based modelling",
            "venue": "Nature, 460(7256):685\u2013686,",
            "year": 2009
        },
        {
            "authors": [
                "N. Fei",
                "Z. Lu",
                "Y. Gao",
                "G. Yang",
                "Y. Huo",
                "J. Wen",
                "H. Lu",
                "R. Song",
                "X. Gao",
                "T Xiang"
            ],
            "title": "Towards artificial general intelligence via a multimodal foundation model",
            "venue": "Nature Communications,",
            "year": 2022
        },
        {
            "authors": [
                "S. Frederick",
                "G. Loewenstein",
                "T. O\u2019donoghue"
            ],
            "title": "Time discounting and time preference: A critical review",
            "venue": "Journal of economic literature,",
            "year": 2002
        },
        {
            "authors": [
                "E.R. Frederiks",
                "K. Stenner",
                "E.V. Hobman"
            ],
            "title": "Household energy use: Applying behavioural economics to understand consumer decision-making and behaviour",
            "venue": "Renewable and Sustainable Energy Reviews,",
            "year": 2015
        },
        {
            "authors": [
                "C. Gao",
                "X. Lan",
                "Z. Lu",
                "J. Mao",
                "J. Piao",
                "H. Wang",
                "D. Jin",
                "Y. Li"
            ],
            "title": "S3: Social-network simulation system with large language model-empowered agents",
            "venue": "arXiv preprint arXiv:2307.14984,",
            "year": 2023
        },
        {
            "authors": [
                "L. Green",
                "J. Myerson",
                "E. McFadden"
            ],
            "title": "Rate of temporal discounting decreases with amount of reward",
            "venue": "Memory & cognition,",
            "year": 1997
        },
        {
            "authors": [
                "B. Guo",
                "X. Zhang",
                "Z. Wang",
                "M. Jiang",
                "J. Nie",
                "Y. Ding",
                "J. Yue",
                "Y. Wu"
            ],
            "title": "How close is chatgpt to human experts? comparison corpus, evaluation, and detection",
            "venue": "arXiv preprint arXiv:2301.07597,",
            "year": 2023
        },
        {
            "authors": [
                "W. G\u00fcth",
                "R. Schmittberger",
                "B. Schwarze"
            ],
            "title": "An experimental analysis of ultimatum bargaining",
            "venue": "Journal of economic behavior & organization,",
            "year": 1982
        },
        {
            "authors": [
                "T. Hagendorff",
                "S. Fabi",
                "M. Kosinski"
            ],
            "title": "Machine intuition: Uncovering human-like intuitive decision-making in gpt-3.5",
            "venue": "arXiv preprint arXiv:2212.05206,",
            "year": 2022
        },
        {
            "authors": [
                "D. Hendrycks",
                "C. Burns",
                "S. Kadavath",
                "A. Arora",
                "S. Basart",
                "E. Tang",
                "D. Song",
                "J. Steinhardt"
            ],
            "title": "Measuring mathematical problem solving with the math dataset",
            "venue": "arXiv preprint arXiv:2103.03874,",
            "year": 2021
        },
        {
            "authors": [
                "T. Hester",
                "M. Vecerik",
                "O. Pietquin",
                "M. Lanctot",
                "T. Schaul",
                "B. Piot",
                "D. Horgan",
                "J. Quan",
                "A. Sendonaris",
                "I Osband"
            ],
            "title": "Deep q-learning from demonstrations",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "J. Ho",
                "S. Ermon"
            ],
            "title": "Generative adversarial imitation learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "N. Ho",
                "L. Schmid",
                "Yun",
                "S.-Y"
            ],
            "title": "Large language models are reasoning teachers",
            "venue": "arXiv preprint arXiv:2212.10071,",
            "year": 2022
        },
        {
            "authors": [
                "J.J. Horton"
            ],
            "title": "Large language models as simulated economic agents: What can we learn from homo silicus",
            "venue": "Technical report, National Bureau of Economic Research,",
            "year": 2023
        },
        {
            "authors": [
                "D. Houser",
                "K. McCabe"
            ],
            "title": "Neuroeconomics: Chapter 2",
            "venue": "Experimental Economics and Experimental Game Theory. Elsevier Inc. Chapters,",
            "year": 2013
        },
        {
            "authors": [
                "A. Hu",
                "G. Corrado",
                "N. Griffiths",
                "Z. Murez",
                "C. Gurau",
                "H. Yeo",
                "A. Kendall",
                "R. Cipolla",
                "J. Shotton"
            ],
            "title": "Model-based imitation learning for urban driving",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "J. Huang",
                "Chang",
                "K.C.-C"
            ],
            "title": "Towards reasoning in large language models: A survey",
            "venue": "arXiv preprint arXiv:2212.10403,",
            "year": 2022
        },
        {
            "authors": [
                "A. Hussein",
                "M.M. Gaber",
                "E. Elyan",
                "C. Jayne"
            ],
            "title": "Imitation learning: A survey of learning methods",
            "venue": "ACM Computing Surveys (CSUR),",
            "year": 2017
        },
        {
            "authors": [
                "D. Kahneman",
                "A. Tversky"
            ],
            "title": "Prospect theory: An analysis of decision under risk",
            "year": 1979
        },
        {
            "authors": [
                "T. Kojima",
                "S.S. Gu",
                "M. Reid",
                "Y. Matsuo",
                "Y. Iwasawa"
            ],
            "title": "Large language models are zero-shot reasoners",
            "venue": "Advances in neural information processing systems,",
            "year": 2022
        },
        {
            "authors": [
                "A. Korinek"
            ],
            "title": "Language models and cognitive automation for economic research",
            "venue": "Technical report, National Bureau of Economic Research,",
            "year": 2023
        },
        {
            "authors": [
                "D.C. Krawczyk"
            ],
            "title": "Social cognition. In Reasoning: The neuroscience of how we think, pp. 283\u2013311",
            "year": 2018
        },
        {
            "authors": [
                "T.S. Kuhn"
            ],
            "title": "The structure of scientific revolutions",
            "venue": "University of Chicago press,",
            "year": 1964
        },
        {
            "authors": [
                "M. Kwon",
                "S.M. Xie",
                "K. Bullard",
                "D. Sadigh"
            ],
            "title": "Reward design with language models",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "D.I. Laibson"
            ],
            "title": "Hyperbolic discounting and consumption",
            "venue": "PhD thesis, Massachusetts Institute of Technology,",
            "year": 1994
        },
        {
            "authors": [
                "E.J. Langer"
            ],
            "title": "The illusion of control",
            "venue": "Journal of personality and social psychology,",
            "year": 1975
        },
        {
            "authors": [
                "B. LeBaron"
            ],
            "title": "Agent-based computational finance",
            "venue": "Handbook of computational economics,",
            "year": 2006
        },
        {
            "authors": [
                "P. Liu",
                "K. Dwarakanath",
                "S.S. Vyetrenko"
            ],
            "title": "Biased or limited: Modeling sub-rational human investors in financial markets",
            "venue": "arXiv preprint arXiv:2210.08569,",
            "year": 2022
        },
        {
            "authors": [
                "Y. Matsuo",
                "Y. LeCun",
                "M. Sahani",
                "D. Precup",
                "D. Silver",
                "M. Sugiyama",
                "E. Uchibe",
                "J. Morimoto"
            ],
            "title": "Deep learning, reinforcement learning, and world models",
            "venue": "Neural Networks,",
            "year": 2022
        },
        {
            "authors": [
                "B. Min",
                "H. Ross",
                "E. Sulem",
                "A.P.B. Veyseh",
                "T.H. Nguyen",
                "O. Sainz",
                "E. Agirre",
                "I. Heintz",
                "D. Roth"
            ],
            "title": "Recent advances in natural language processing via large pre-trained language models: A survey",
            "venue": "ACM Computing Surveys,",
            "year": 2021
        },
        {
            "authors": [
                "W. Mischel",
                "E.B. Ebbesen"
            ],
            "title": "Attention in delay of gratification",
            "venue": "Journal of personality and social psychology,",
            "year": 1970
        },
        {
            "authors": [
                "A.Y. Ng",
                "S Russell"
            ],
            "title": "Algorithms for inverse reinforcement learning",
            "venue": "In Icml,",
            "year": 2000
        },
        {
            "authors": [
                "T. O\u2019Donoghue",
                "M. Rabin"
            ],
            "title": "Doing it now or later",
            "venue": "American economic review,",
            "year": 1999
        },
        {
            "authors": [
                "J.S. Park",
                "L. Popowski",
                "C. Cai",
                "M.R. Morris",
                "P. Liang",
                "M.S. Bernstein"
            ],
            "title": "Social simulacra: Creating populated prototypes for social computing systems",
            "venue": "In Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology,",
            "year": 2022
        },
        {
            "authors": [
                "J.S. Park",
                "J.C. O\u2019Brien",
                "C.J. Cai",
                "M.R. Morris",
                "P. Liang",
                "M.S. Bernstein"
            ],
            "title": "Generative agents: Interactive simulacra of human behavior",
            "venue": "arXiv preprint arXiv:2304.03442,",
            "year": 2023
        },
        {
            "authors": [
                "R.A. Pollak"
            ],
            "title": "Consistent planning",
            "venue": "The Review of Economic Studies,",
            "year": 1968
        },
        {
            "authors": [
                "S. Reddy",
                "A. Dragan",
                "S. Levine"
            ],
            "title": "Where do you think you\u2019re going?: Inferring beliefs about dynamics from behavior",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "L. Reynolds",
                "K. McDonell"
            ],
            "title": "Prompt programming for large language models: Beyond the few-shot paradigm",
            "venue": "In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "E.D. Rothblum",
                "L.J. Solomon",
                "J. Murakami"
            ],
            "title": "Affective, cognitive, and behavioral differences between high and low procrastinators",
            "venue": "Journal of counseling psychology,",
            "year": 1986
        },
        {
            "authors": [
                "A.G. Sanfey",
                "J.K. Rilling",
                "J.A. Aronson",
                "L.E. Nystrom",
                "J.D. Cohen"
            ],
            "title": "The neural basis of economic decisionmaking in the ultimatum",
            "year": 2003
        },
        {
            "authors": [
                "P. Schramowski",
                "C. Turan",
                "N. Andersen",
                "C.A. Rothkopf",
                "K. Kersting"
            ],
            "title": "Large pre-trained language models contain human-like biases of what is right and wrong to do",
            "venue": "Nature Machine Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "T.J. Sejnowski"
            ],
            "title": "Large language models and the reverse turing test",
            "venue": "Neural computation,",
            "year": 2023
        },
        {
            "authors": [
                "R.S. Shah",
                "V. Marupudi",
                "R. Koenen",
                "K. Bhardwaj",
                "S. Varma"
            ],
            "title": "Numeric magnitude comparison effects in large language models",
            "venue": "arXiv preprint arXiv:2305.10782,",
            "year": 2023
        },
        {
            "authors": [
                "H.A. Simon"
            ],
            "title": "A behavioral model of rational choice",
            "venue": "The quarterly journal of economics,",
            "year": 1955
        },
        {
            "authors": [
                "H.A. Simon"
            ],
            "title": "Models of bounded rationality",
            "venue": "Empirically grounded economic reason,",
            "year": 1997
        },
        {
            "authors": [
                "L.J. Solomon",
                "E.D. Rothblum"
            ],
            "title": "Academic procrastination: Frequency and cognitive-behavioral correlates",
            "venue": "Journal of counseling psychology,",
            "year": 1984
        },
        {
            "authors": [
                "R.S. Sutton",
                "A.G. Barto"
            ],
            "title": "Reinforcement learning: An introduction",
            "venue": "MIT press,",
            "year": 2018
        },
        {
            "authors": [
                "R.H. Thaler"
            ],
            "title": "Anomalies: The ultimatum game",
            "venue": "Journal of economic perspectives,",
            "year": 1988
        },
        {
            "authors": [
                "R.H. Thaler",
                "A. Tversky",
                "D. Kahneman",
                "A. Schwartz"
            ],
            "title": "The effect of myopia and loss aversion on risk taking: An experimental test",
            "venue": "The quarterly journal of economics,",
            "year": 1997
        },
        {
            "authors": [
                "A. Tversky",
                "D. Kahneman"
            ],
            "title": "Advances in prospect theory: Cumulative representation of uncertainty",
            "venue": "Journal of Risk and uncertainty,",
            "year": 1992
        },
        {
            "authors": [
                "J. Wei",
                "Y. Tay",
                "R. Bommasani",
                "C. Raffel",
                "B. Zoph",
                "S. Borgeaud",
                "D. Yogatama",
                "M. Bosma",
                "D. Zhou",
                "D Metzler"
            ],
            "title": "Emergent abilities of large language models",
            "venue": "Transactions on Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "J. Wei",
                "X. Wang",
                "D. Schuurmans",
                "M. Bosma",
                "F. Xia",
                "E. Chi",
                "Q.V. Le",
                "D Zhou"
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "R. Williams",
                "N. Hosseinichimeh",
                "A. Majumdar",
                "N. Ghaffarzadegan"
            ],
            "title": "Epidemic modeling with generative agents",
            "venue": "arXiv preprint arXiv:2307.04986,",
            "year": 2023
        },
        {
            "authors": [
                "C. Ziems",
                "W. Held",
                "O. Shaikh",
                "J. Chen",
                "Z. Zhang",
                "D. Yang"
            ],
            "title": "Can large language models transform computational social science",
            "venue": "arXiv preprint arXiv:2305.03514,",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": "Modeling subrational agents, such as humans or economic households, is inherently challenging due to the difficulty in calibrating reinforcement learning models or collecting data that involves human subjects. Existing work highlights the ability of Large Language Models (LLMs) to address complex reasoning tasks and mimic human communication, while simulation using LLMs as agents shows emergent social behaviors, potentially improving our comprehension of human conduct. In this paper, we propose to investigate the use of LLMs to generate synthetic human demonstrations, which are then used to learn subrational agent policies though Imitation Learning. We make an assumption that LLMs can be used as implicit computational models of humans, and propose a framework to use synthetic demonstrations derived from LLMs to model subrational behaviors that are characteristic of humans (e.g., myopic behavior or preference for risk aversion). We experimentally evaluate the ability of our framework to model sub-rationality through four simple scenarios, including the well-researched ultimatum game and marshmallow experiment. To gain confidence in our framework, we are able to replicate well-established findings from prior human studies associated with the above scenarios. We conclude by discussing the potential benefits, challenges and limitations of our framework.\n*Equal contribution 1Applied Research Team (ART) - IT Department, Bank of Italy, Rome, Italy. This research work was carried out when Andrea Coletta was employed at J.P. Morgan AI Research. The views expressed in this paper are those of the authors and do not necessarily reflect those of the Bank of Italy. 2J.P. Morgan AI Research, California, USA. 3J.P. Morgan AI Research, New York, USA. Correspondence to: Andrea Coletta <andrea.coletta@bancaditalia.it>, Kshama Dwarakanath <kshama.dwarakanath@jpmorgan.com>, Penghang Liu <penghang.liu@jpmchase.com>."
        },
        {
            "heading": "1. Introduction",
            "text": "In the recent years, Large Language Models (LLMs) have been among the most impressive achievements in Artificial intelligence (AI) and Machine Learning (ML), raising questions about whether we are close to an Artificial General Intelligence (AGI) system (Bubeck et al., 2023; Fei et al., 2022). Foundation models like ChatGPT (OpenAI, 2023b) have shown remarkable performance across a wide range of novel and complex tasks, including coding, vision, healthcare, law, education, and psychology (Bubeck et al., 2023). These models perform close to human experts, without needing additional re-training or fine-tuning, apparently mimicking human abilities and reasoning (Guo et al., 2023; Min et al., 2021; Bubeck et al., 2023). In particular, recent work has demonstrated how LLMs can emulate human reasoning in solving complex problems: LLMs can simulate chain of thoughts, generating a reasoning path to decompose complex problems into multiple easier steps to solve (Kojima et al., 2022; Wei et al., 2022b; Ho et al., 2022). We could conclude that LLMs are able to replicate high-level cognitive and reasoning capabilities of humans (Huang & Chang, 2022; Hagendorff et al., 2022), especially with bigger foundation models like GPT-4 (Wei et al., 2022a). If we assume this is true, a natural question arises: \u201cCan we leverage LLMs to simulate aspects of human behavior that deviate from perfect rationality so as to eventually improve our understanding of diverse human conduct?\u201d\nModeling human behavior has seen significant interest in various domains, ranging from robotics for human-robot collaboration (Dragan et al., 2015), to finance and economics for modeling human investors and consumers (Liu et al., 2022; Akerlof & Shiller, 2010). In studies of human-robot interactions, it is commonly embraced that humans must not be modeled as optimal agents (Carroll et al., 2019), and an irrational human when correctly modeled, can communicate more information about their objectives than a perfectly rational human can (Chan et al., 2021). For instance, humans exhibit bounded rationality (Simon, 1997), often making satisfactory but not optimal decisions due to their limited knowledge and processing power. They are also known to be myopic in their decision making as they care more about short-term rewards than long-term rewards. While the discount factor in reinforcement learning (RL) may capture myopicity through exponential discounting, there is ample\nar X\niv :2\n40 2.\n08 75\n5v 1\n[ cs\n.A I]\n1 3\nFe b\n20 24\nevidence to show that other temporal discounting models such as hyperbolic discounting (Ainslie, 1992) and quasihyperbolic discounting (Laibson, 1994) are more realistic in capturing time-inconsistent human behavior (Chabris et al., 2010). The latter models are of particular interest in modeling household consumption and saving behavior in economic studies (Angeletos et al., 2001). Empirical evidence from humans also demonstrates that the rate of discounting reduces with an increase in the amount of reward (Green et al., 1997). When combined with the variability of the rate parameter across humans, this makes the calibration of parameters challenging for different humans, reward amounts, and scenarios. For example, works on estimating parameters for temporal discounting models from human studies exhibit disagreement among each other due to additional experiment-specific considerations (Frederick et al., 2002). The non-exponential discounting models, along with the dependence of discount factor on the reward pose challenges to classical RL algorithms relying on the validity of the Bellman equation. Other models have been proposed to encompass different human biases, including prospect theory, illusion of control, and optimism, which often employ nonlinear regression techniques to calibrate their parameters (Langer, 1975; Tversky & Kahneman, 1992; Sharot et al., 2007). However, such approaches demand substantial effort in collecting samples of human behavior, and it is impractical to obtain a set of parameters capable of representing individuals with different levels of bias and rationality. Also, recruiting human subjects for behavioral experiments comes with concerns surrounding ethics, privacy and general subject disinterest in the absence of monetary incentives (Desposato, 2015; Abeler & Nosenzo, 2015).\nRecent work studies the possibility of using LLMs to simulate and analyze human behaviors (Ziems et al., 2023; Chiang & Lee, 2023; Aher et al., 2023; Sejnowski, 2023; Horton, 2023; Argyle et al., 2023; Korinek, 2023; Williams et al., 2023). G. Aher et al. (Aher et al., 2023) evaluate how LLMs effectively simulate different aspects of human behavior, reproducing findings from some classic economic and psychology experiments. J. J. Horton (Horton, 2023) investigates the computational capabilities of GPT-3 to model economic agents according to classical behavioral economics literature. Other work employs interactive LLM agents to simulate more sophisticated societal contexts, where they are eventually able to reproduce realistic social interactions and emergent behaviors (Park et al., 2023; 2022; Gao et al., 2023). Such remarkable performance in agentbased simulation could be related to the ability of LLMs to emulate different individuals, as studied in (Argyle et al., 2023). Their work shows how LLMs effectively emulate different sociodemographic human groups under appropriate conditioning procedures. Similarly, P. Schramowski et al. (Schramowski et al., 2022) discover that LLMs, because\nof how they are trained and designed, eventually contain human-like biases reflecting our behavior and morality. In fact, LLMs naturally encode a wide range of human behavior seen in the training data (Bommasani et al., 2021; Brown et al., 2020), potentially representing implicit computational models of humans. The ability to display such human qualities is what makes LLMs a disruptive tool, possibly leading to a new paradigm shift: scientists could be able to easily conduct new experiments that previously would have been time-consuming or just impossible, by using LLMs for simulation (Kuhn, 1964; Ziems et al., 2023).\nIn this work, we investigate the ability of LLMs to synthesize a broad range of subrational human behavior in temporal decision making tasks. Our framework learns behavioral policies through Imitation Learning (IL) (Hussein et al., 2017) using synthetic human-like demonstrations generated by LLMs. This addresses the limitations of classical RL techniques in modeling subrational behavior (Ng et al., 2000) by overcoming the need for human demonstration data, which is expensive to gather for different human subgroups. Our approach assumes that LLMs represent implicit computational models of humans. Therefore, we can generate synthetic demonstrations to model subrational human behaviors, including irrational and myopic behaviors. Our primary goal is to initiate a novel research area that uses LLMs to synthesize subrational behavior, which has received limited preliminary attention (Aher et al., 2023; Horton, 2023; Korinek, 2023) albeit with enormous potential. Specifically, we summarize our contributions as follows: \u2022 We propose a novel research direction of using LLMs to calibrate subrational decision making models; \u2022 We propose a framework to leverage the ability of LLMs to generate synthetic demonstrations in order to learn subrational agent policies through IL; \u2022 We evaluate the ability of our framework to produce subrational behavior as observed in established human studies; \u2022 Finally, we discuss the potential benefits, opportunities and pitfalls of utilizing LLMs for human modeling, outlining future research directions in this domain."
        },
        {
            "heading": "2. Literature Overview",
            "text": "Deep learning models have recently found extensive application in simulating complex systems and analyzing agent behaviors (Ha & Schmidhuber, 2018; Matsuo et al., 2022). They have been employed in various domains, including complex financial markets (Coletta et al., 2022; 2023), as well as urban driving scenarios (Hu et al., 2022). The advent of LLMs has created more opportunities for modelling complex systems, especially focused on human behaviors. Recent research efforts like (Park et al., 2023) and (Gao et al., 2023) have utilized LLMs to create sophisticated agent-based simulators. These simulators contain LLM\nbased agents that interact with others through natural language prompts carefully designed by incorporating world perception, reflection and planning within a defined environment. They demonstrate the ability of agents to generate believable behavior as assessed by human evaluators, while giving rise to emergent outcomes such as information diffusion, relationship formation, and agent coordination. J.J. Horton (Horton, 2023) demonstrated the ability of LLMs to mimic human preferences arising from different qualitative personas. For example, given the following scenario \u201cA hardware store has been selling snow shovels for $15. The morning after a large snowstorm, the store raises the price to $20.\u201d, the \"libertarian\" LLM found the increase acceptable while for the \"socialist\" LLM the increase is unfair. Also, recent work (Argyle et al., 2023; Aher et al., 2023) validates the ability of LLMs to model humans from a variety of subgroups. This prompts us to consider whether LLMs can effectively capture subrational human behaviors without the need for extra effort in reward design (Chan et al., 2021), eventually supporting or replacing traditional agent-based models (LeBaron, 2006; Bouchaud et al., 2018). Instead of modelling an entire system using LLM agents, which is expensive and harder to control, we use LLMs to generate a few synthetic demonstrations for subrational behavior. Such demonstrations can be validated and used within an IL framework to learn agent policies encompassing existing human biases and beliefs. Complementary work in (Kwon et al., 2022) employs LLMs to directly generate a proxy reward function based on natural language prompts. While such a proxy reward can align the training of policies with user-defined objectives, it cannot generate human behavior beyond what is reflected in the reward function. For example, (Reddy et al., 2018) argues that subrational human behavior can arise from incorrect beliefs of environment dynamics rather than the reward function. Also, such a proxy reward does not handle non-exponential discounting models, such as hyperbolic or quasi-hyperbolic discounting. By directly updating the policy network to mirror humanlike behavior generated through LLMs, our methodology addresses these limitations, bypassing the dependence on exponential MDPs. On the other hand, we note that our work requires more careful generation of synthetic demonstrations covering the state space of interest. A lacking of which coverage can be handled using approaches combining RL and IL (Correia & Alexandre, 2023). This new paradigm of generating synthetic human demonstrations using LLMs could benefit the broader research community. LLMs can compensate for the scarcity of expert demonstrations, which are extremely useful to learn behaviors from (Hester et al., 2018) and calibrate human subrationality models proposed in psychological and economic studies (Tversky & Kahneman, 1992; Green et al., 1997; Simon, 1997).\nAlgorithm 1 IL with LLM Demonstrations Input: Demonstrations D = {(si, ai)}ni=1, epochs E Initialize: Policy weights \u0398 for all k from 1 to E do\nSample mini-batch s \u2208 D Human-preference h(s) = KDE({a|(s,a) \u2208 D}) Policy preference n(s) = softmax (Q\u0398(s,a)) Compute loss ||h(s)\u2212 n(s)||2 Perform gradient descent update on \u0398\nend for"
        },
        {
            "heading": "3. Learning Subrational Behavior using LLMs",
            "text": "We here discuss our framework to learn subrational policies using LLM demonstrations. We first generate synthetic human demonstration data prompting the LLMs, and then use IL to train agent policies based on such demonstrations. We use Markov Decision Processes (MDPs) as a natural underlying decision model for an agent in temporal decision making tasks. An MDP is a tuple M = (S,A,P,R) comprising: the state space S; the action space A; a transition function P : S \u00d7 A \u2192 \u2206(S), where \u2206(S) denotes probability distributions over state space S; and a reward function R : S \u00d7A \u2192 R. A rational agent follows policy \u03c0 : S \u2192 \u2206(A) that maximizes the expected reward:\nJ(\u03c0) = Eat\u223c\u03c0(st),st+1\u223cP(st,at) [ T\u22121\u2211 t=0 \u03b3tR(st, at) ]\nwhere \u03b3 \u2208 [0, 1] represents the discount factor that is used to prioritize immediate rewards over those got later in time. Three challenges of using direct RL to model human behavior are: 1) the design of a reward function R that accurately captures desired human behavior (Ng et al., 2000; Kwon et al., 2022); 2) the large amount of data and training required before RL policies reach reasonable performance (Hester et al., 2018); and 3) the inability to capture time-inconsistent human preferences when using exponential discounting of rewards e.g., preference reversals (Chabris et al., 2010). We propose to mitigate these challenges by using a small set of synthetic human (or expert) demonstrations that are generated using LLMs. Such demonstration data can overcome the need to design an intricate reward function as we can use IL."
        },
        {
            "heading": "3.1. Generation of LLM Demonstrations",
            "text": "The synthetic demonstration data comprises state action pairs over numerous episodes D = {(si, ai)}ni=1, and is generated by prompting the LLM using either chain-ofthoughts to create a reasoning path (Wei et al., 2022b), or summarizing the state and possible actions using prompt engineering (Reynolds & McDonell, 2021). We report details of data generation in the experimental section where we use\nGPT-4 (OpenAI, 2023a) due to its expressivity. Demonstration data can massively accelerate the RL training process when initialized with a policy learned using supervised learning on the data. Importantly, human agents may have irrational behavior in specific scenarios (i.e., states) which may be difficult to identify and model. LLMs have shown the ability to reproduce such biased behaviors (Aher et al., 2023; Schramowski et al., 2022) providing credible and novel advantages to using such demonstrations to model subrational human agents. Lastly, these demonstrations can be used to fine-tune exponential RL policies to capture timeinconsistent human behavior, which would have otherwise been intractable to model using standard RL."
        },
        {
            "heading": "3.2. Imitation Learning with LLM Demonstrations",
            "text": "Given demonstration data D, we can use IL to learn a policy \u03c0\u03b8 for the agent, formally defined as:\nargmax \u03b8\nE(s,a)\u223cD[log \u03c0\u03b8(a|s)]\nAlgorithm 1 reports the proposed procedure, where we initialize the agent policy network \u03c0\u0398, and in particular the value function Q(s, a). Then for each state s \u2208 S we extract the distribution of agent actions in the synthetic LLM demonstrations, using a non-parametric Kernel Density Estimation. We then update the policy \u03c0\u0398 by minimizing the difference between its estimated action distribution and the one from synthetic demonstrations. While more sophisticathed algorithms can be employed (e.g., GAIL (Ho & Ermon, 2016)) we adopt a simple algorithm as our goal is to study the application of LLMs in generating synthetic human demonstrations, rather than to propose a novel IL algorithm. Note that pure IL like above requires demonstrations encompassing the entire state space. If not, one could start with learning the policies using an RL training phase followed by IL, similarly to (Hester et al., 2018)."
        },
        {
            "heading": "4. Experiments",
            "text": "In this section, we test the ability of our framework to simulate human behaviors in four economic games where humans demonstrate subrational behavior, and show that we can replicate findings from existing literature. We use GPT4 (OpenAI, 2023a) as the current most expressive model, and a 3-layer neural network for the Q value function. We consider the following games: \u2022 The Ultimatum Game (G\u00fcth et al., 1982), a game theory experiment where a player decides how to split an amount of money, and another player can accept or reject the split. \u2022 The Stanford marshmallow experiment (Mischel & Ebbesen, 1970) where a player is offered a choice between one small but immediate reward, or a bigger reward if they waited for a period of time. \u2022 The Double or nothing gamble where a player is offered\na choice to double their money or lose everything, in order to illustrate human decisions under risk as described by prospect theory (Kahneman & Tversky, 1979). \u2022 The Procrastination experiment (O\u2019Donoghue & Rabin, 1999) where a student player chooses which day to write a report and miss a good movie given an academic deadline."
        },
        {
            "heading": "4.1. Ultimatum Game",
            "text": "Scenario. The ultimatum game considers two players splitting a given amount of money T . The first player (the proposer) decides to split the money between himself and the responder. The responder (who knows the total sum) decides to either accept or reject the split. If the responder rejects the proposal, both players receive nothing; if the responder accepts, they both get the agreed amount of money. For example, the proposer decides to split T = $10 by giving $2 to the responder and taking $8 for himself; if the responder accepts they get $8 and $2, respectively. Otherwise they both get nothing.\nSocial Preferences and Limited Rationality. If we assume that agents have stable, well-defined, and rational preferences, we realize that the theory for the simple ultimatum game is rather obvious: a rational responder would accept any non-zero proposal. In fact, even if the proposal is unfair, getting something is better than getting nothing. This represents a Nash equilibrium for the ultimatum game. In reality though, human behavior is much different. The work of Nobel laureate Richard Thaler (Thaler, 1988) focuses on some \"anomalies\" in the ultimatum game, defined as demonstration of limited rationality and social preferences by human participants. They show that a human responder will likely reject unfair proposals, as they prefer to \"punish\" the proposer, even though they will receive nothing. According to their own social preference, people will prefer \"fair\" (i.e., 50:50) splits (Sanfey et al., 2003), or in general they often reject offers less than 30% of the initial sum T (Houser & McCabe, 2013; Krawczyk, 2018).\nExperimental Setting. We consider an initial amount of T = $10 for the game. Therefore, the game has 10 possible actions for the proposer x \u2208 {0, . . . , 10}, where x is the amount to offer and (10 \u2212 x) is the amount they keep for themselves. Accordingly, each offer is a possible state for the responder who has 10 states and 2 actions a \u2208{accept, reject}. In this experimental setup, we focus on modelling the human responder, which we consider the more interesting player in the game. We model the proposer using a Multi-Armed Bandit (Sutton & Barto, 2018); and use an RL policy network for the responder. The two agents are trained together using as reward the amount of money they individually get. We alternately use IL with LLM demonstrations for a human responder or one that cares about a\nfair split to obtain the responder policy.\nSynthetic demonstrations collected using LLMs. We instruct GPT-4 as \"Impersonate Jerry who cares a lot about fairness.\" or \"Impersonate a human called Jerry\". We call the two experiments \"Fair\" and \"Human\", respectively. We then prompt the model as: John receives $T, and he proposes to offer Jerry x and keep T-x for himself. If Jerry accepts they both get the agreed amount, but if Jerry rejects they both receive nothing. Continue the sentence: \u2018Jerry decides to\u2019, where T = 10 and x \u2208 {0, . . . , T}. We then convert each prompt and answer into a state-action (s, a) pair. We generate 10 synthetic observations for each state x.\nResults. Figure 1 shows rewards and distribution of money kept by the proposer. Figure 1.(a) shows classic RL training for both agents, where the proposer learns to split the money in an unfair manner, and the responder accepts any splits. In fact, this is the expected outcome from a fully rational responder. In Figure 1.(b) we use LLM demonstrations to train a \"human\" responder model, which now rejects very low (i.e., less than 20%) splits, and thus the two agents converge to a $8 and $2 split. In Figure 1.(c) we use LLM demonstrations to train a \"fair\" responder model, which now rejects unfair (i.e., unbalanced) splits, and thus the two agents converge around a 50:50 split. Finally, Figure 1.(d) reports the distributions of money the proposer keeps for the three variants.\nComparison to human subject studies. We compare our results with those in economic literature in (Houser &\nMcCabe, 2013) and (Krawczyk, 2018). These works show that a human responder is less likely to accept offers below $3, out of $10. This is inline with our results confirming that synthetic LLM demonstrations could be effectively used to model human interactions, as is also highlighted in recent work (Aher et al., 2023). Importantly, LLMs enable the simulation of a broader range of scenarios without any additional costs (e.g., $100 instead of $10)."
        },
        {
            "heading": "4.2. Stanford Marshmallow Experiment.",
            "text": "Scenario. The Stanford Marshmallow Experiment is a classic psychological study that explores delayed gratification in children. In this experiment, a child is placed in a room with a tempting marshmallow or a similar treat. The researcher presents the child with a choice: they can either eat the marshmallow immediately or wait for a predetermined amount of time (usually 15 minutes) without eating it. If the child can resist the temptation and wait until the time elapses, they are rewarded with an additional marshmallow. This experiment is used to investigate the ability of children to delay their desires and make choices based on long-term benefits rather than immediate gratification.\nSelf-Control and the Stanford Marshmallow Experiment. If we assume that children have strong self-control and can easily delay gratification, the theory behind the Stanford Marshmallow Experiment might seem straightforward: a child with impeccable self-discipline would wait patiently for the delayed reward, in this case, an extra marshmallow. In this ideal scenario, the child would understand that waiting for the second marshmallow is a more desirable outcome than consuming the first one immediately, representing a form of self-control equilibrium in this experiment. In reality, however, the behavior of children in the marshmallow experiment is quite different. The work of Walter Mischel (Mischel & Ebbesen, 1970) uncovered what could be considered \"anomalies\" in human self-control and decision-making. They demonstrated that many children struggle with self-control and are unable to resist the immediate temptation of eating the first marshmallow, even though waiting would lead to a greater reward. In particular, such work revealed that children\u2019s self-control abilities vary, and their choices in the experiment are influenced by factors like age, environment, and individual differences. While incorporating these factors into RL is non-trivial, obtaining human demonstrations for very specific settings can be expensive or impossible, and synthetic demonstrations from LLMs could overcome these challenges.\nExperimental Setting. We focus on modelling a child aligned with the psychological game in this setup. We consider a single agent with 2 actions a \u2208 {accept, wait} for the candy in the current state. We consider one initial\nstate where the agent can choose to accept the candy now or wait, and two subsequent states derived by each choice.\nSynthetic demonstrations collected using LLMs. We instruct GPT-4 as \"You are Janice a Y years old child.\". We prompt the model as: Janice is offered to get one candy now, or to wait for 2 more hours to eventually get two candies. Continue the following sentence \u2019Janice decides to\u2019, where Y \u2208 {2, 5}. Interestingly, we noticed that with \u201815 minutes\u2019 waiting like the original experiment, the model is more inclined to wait (Appendix 15), possibly indicating different human preferences w.r.t. the original paper dated more than 50 years ago. We generate 10 synthetic observations for each age Y .\nResults. In Figure 2.(a) we show a classic RL approach trained until convergence, where the reward is the amount of candies the agent gets. The agent is completely rational, and it learns to always wait to get two candies. Most important, it is very hard to model different background factors for the agent, like age. For example, can we derive an RL policy for a child of 2 years vs a child of 5 years? A common approach could be myopic RL modelling shown in Figure 2.(b) where we modify the discount factor \u03b3 to let the agent being more myopic (i.e,. looking only for immediate rewards). In the setting with \u03b3 = 0.3 the agent chooses to take the candy immediately, with a lower reward. Even if we can use myopicity modeling, it is not clear how to relate the value \u03b3 with the age of the child. Instead, an LLM could be easily prompted to get precise synthetic human observations. Figure 2.(c) shows that if we directly train an IL policy using\nLLM demonstrations, we are able to reproduce behavior of a 2-year old child who takes one candy immediately, or a more rational 5-year old child that waits for more candies.\nComparison to human subject studies. We compare our results with the study of waiting behavior of nursery school children by W. Mischel (Mischel & Ebbesen, 1970) comprising 16 boys and 16 girls, with age ranging from 3 to 8 years. In this experiment, they found that most children do not wait, when both rewards are visible to them; while around 75% of children wait if both rewards having been removed from their sight. Our results show a possible reconstruction of this experiment: a 2-year old child does not wait, while a 5-year old child is more rational and waits; with a mix of both actions for a 3-year old child (details in the appendix). These findings are aligned with the aforementioned study which suggests that the capacity to wait for long-term goals is develop markedly at about ages 3-4. Nonetheless, we also note that the original experiments are from 50 years ago, making current human biases and needs very different (e.g., children may have easier access to treats now)."
        },
        {
            "heading": "4.3. Double or Nothing Gamble",
            "text": "Scenario. We create a simple economic game named \u201cdouble or nothing\" to illustrate human bias in decisionmaking under risk. Assume that two people are gambling, and person A has lost an initial bet of $5 against person B. Before the initial bet is paid, person A (or B) can choose to play a second bet of $5 with a higher probability (> 50%) of losing (or winning). Since the second bet is an unfair game that favors the winner (person B), a rational agent will always deny the second bet as person A or accept the second bet as person B. However, humans in real life are willing to take the risk of the second bet as person A (hoping to offset their loss from the previous bet), or deny the second bet as person B (afraid of losing their current reward).\nProspect theory. The expected utility theory is a fundamental model of human decision making under uncertainty, which says that an agent chooses between risky prospects based on expected utility: U = \u2211n i=1 pi \u00b7xi, where pi is the probability of an outcome with payoff xi. RL aligns with this hypothesis as the value of a state is determined by the expected utility of next states weighted by their transition probabilities. But, a substantial body of evidence has shown that human decisions systematically violate this theory. To model such deviation, Kahneman and Tversky introduced prospect theory (Kahneman & Tversky, 1979) which has been widely used for explaining experimental findings on human risk taking over several decades (Benartzi & Thaler, 1995; Thaler et al., 1997; Barberis, 2013). The model consists of two key elements: (1) a biased value function v(xi) that is concave for gains, convex for losses, and steeper for\nlosses than gains, and (2) a nonlinear probability weighting function w(pi) which overweights small probability and underweights large probability. Humans with prospect bias are risk-averse over gains and risk-seeking over losses.\nExperimental Setting. We create the \"double or nothing\" gamble environment with unfairness factor of the second bet \u03f5 \u2208 {0, 0.1, 0.2, 0.3, 0.4}. The winner of the first bet has probability {0.5 + \u03f5} to win the second bet against the loser. We train both players separately. Given \u03f5 as the state, the agent takes an action a \u2208 {accept, reject} to decide whether or not to add the second bet. A rational RL agent learns to maximize the expected reward from the two bets U = \u2211n i=1 pi \u00b7 xi, whereas the prospect biased RL agent\nhas reward function U = \u2211n\ni=1 w(pi) \u00b7 v(xi) per Eq. 1 and Eq. 2 with parameters estimated from real human data.\nSynthetic demonstrations collected using LLMs. We prompt GPT-4 as follows: \u2022 1) You won an initial $5 bet against Tom. Before you collect your reward, you can choose to add a second bet of $5 with {0.5 + \u03f5} probability to win. If you win the second bet you will double your gain from the initial bet, but if you lose you will gain nothing. Continue the following sentence\n\u2018you decide to\u2019;\n\u2022 2) You lost an initial $5 bet against Tom. Before you pay your debt, you can choose to add a second bet of $5 with {0.5\u2212 \u03f5} probability to win. If you win the second bet you will recover your loss from the initial bet, but if you lose you will double your loss. Continue the following sentence \u2018you decide to\u2019. We collect 10 demonstrations for each \u03f5 \u2208 {0, 0.1, 0.2, 0.3, 0.4} and show the probability of accepting the second bet as the winner/loser in Table 1.\nResults. Table 1 shows the probability of adding the second bet in LLM demonstrations. In general, we observe that the loser (winner) of the first bet is more (less) willing to take the risk of playing an additional bet, unless the unfairness of the second bet is large enough (\u03f5 \u2265 0.3). We compare an IL policy trained over LLM demonstrations with a rational RL policy and the prospect-biased RL policy in Figure 3. Since the second bet always favors the winner, the rational RL agent learns to maximize their reward by accepting it as winner and rejecting as loser. On the other hand, agents trained using prospect theory and LLM demon-\nstrations yield similar (lower) rewards which are subrational e.g., when \u03f5 = 0.1, both are not willing to take the risk as winner, but prefer to take the additional bet as loser.\nComparison to human subject studies. We compare behavior generated by the IL policy using LLM demonstrations to the RL policy using the prospect theory model specified by parameters estimated from real humans in the literature (Tversky & Kahneman, 1992). Both models yield similar behavior and rewards, showing risk-averse behavior in gains and risk-seeking behavior in losses. These results highlight the simplicity of our framework in incorporating subrational and biased behavior into agent policies without the need to design intricate or ad-hoc reward functions."
        },
        {
            "heading": "4.4. Academic Procrastination with Deadlines",
            "text": "Scenario. A student needs to spend one day to write a course report due within the next H days. They receive a credit for the submitted report on the last day. The theatre is playing movies in increasing order of quality over the next H days. Which day does the student choose to write the report and miss the movie? Let ct represent the cost of missing a movie on day t \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , H} so that 0 < c1 < c2 \u00b7 \u00b7 \u00b7 < cH . And, R > 0 is the reward received upon submitting the report on day H . This setup is based on the example on salient costs in (O\u2019Donoghue & Rabin, 1999).\nProcrastination and Quasi-Hyperbolic Discounting. Studies have shown that a large fraction of students procrastinate on their academic deliverables until the deadline gets very close (Rothblum et al., 1986; Day et al., 2000). This is an example for the lack of self-control observed in humans: they choose to watch a movie today with the expectation that their future self would write the report tomorrow. But when tomorrow arrives, the same decision repeats until the deadline gets very close. Sophisticated humans who are aware about their lack of self-control expect their future selves to procrastinate and complete the report earlier than naive humans who are optimistic about their future selves (Pollak, 1968). Such behavior is modeled by quasi-hyperbolic discounting (Laibson, 1994) where the\nvalue function in a state st is given by\nV (st) = E [ R(st) + \u03b2\nH\u2212t\u2211 k=1 \u03b4kR(st+k)\n]\nwhere \u03b4 \u2208 [0, 1] is the long-term discount factor similar to exponential discount factor \u03b3, and \u03b2 \u2208 [0, 1] captures \u2018short-term impatience\u2019 representing the preference over immediate rewards today vs tomorrow (O\u2019Donoghue & Rabin, 1999). With the assumption of exponential discounting (as in standard RL), one arrives at time-consistent preferences. Time-inconsistent preferences are those where the optimal policy from today\u2019s perspective conflicts with the optimal policy from tomorrow\u2019s perspective (Laibson, 1994). While it is unclear how to modify standard RL algorithms (that rely on the Bellman equation) to arrive at this time-inconsistent human behavior modeled by quasi-hyperbolic discounting, we show next that such behavior can be generated through IL using synthetic human demonstrations from LLMs.\nExperimental Setting. Let the deadline be H = 4 with the theatre playing a mediocre movie on day 1, a good movie on day 2, a great movie on day 3 and an excellent movie on day 4. We collect LLM demonstrations for three types of students as categorized by their Grade Point Average (GPA). As a baseline for comparison, we use a policy learned using RL with costs c1 = 1, c2 = 2, c3 = 4, c4 = 7 and final reward R = 14 with exponential discount factor \u03b3 = 1.\nSynthetic demonstrations collected using LLMs. We instruct GPT-4 as \"You impersonate a student with GPA x, who loves watching movies. GPA measures the students commitment towards their academics.\". We then prompt the model as: \"The theatre has a line up of increasingly better movies as days pass. The student has a deadline to submit a course report within the next four days. The student needs to pick one day to write the report, which means they will miss the movie on that day. Continue the sentence \u2018The student writes the report on day \u2019\", where x \u2208 {1, 3, 4.5}. We collect 10 demonstrations for each GPA x as shown in Figure 4.(a), where we see that the probability of writing the report on the first day increases as the GPA increases.\nResults. Figure 4.(b) shows the training rewards for an RL policy using the above cost and reward setting. A histogram of the day of writing the report when using the trained RL policy is plotted in Figure 4.(d). Observe that exponential discounting with \u03b3 = 1 says it\u2019s optimal to write the report on day 1 since it picks t that maximizes R\u2212ct. Thus, the RL policy always corresponds to writing the report on day 1. On the other hand, the LLM demonstrations can be used to train a policy by IL, whose rewards are plotted in Figure 4.(c) with the day of writing the report in Figure 4.(d). Note that when LLM demonstrations are used, they give rise to one\npolicy per student type/GPA. Observe from Figure 4.(c) that the rewards at the end of training are lower for lower GPA values, and improve as GPA increases. Since the cost of writing the report increases with the day of writing it, this says that the learned IL policies give earlier days of writing the report as GPA increases. The same can be observed in Figure 4.(d) where while the RL policy without any LLM demonstrations corresponds to writing the report on the first day, the IL policies with LLM demonstrations correspond to writing it on later days for lower GPAs. Thus, while the RL policy gives time consistent actions, incorporating LLM demonstrations helps us to capture procrastination, while accounting for differences in student attributes such as GPA.\nComparison to human subject studies. We compare our results to a study of procrastinating behavior among 379 undergraduate students in an introductory psychology course (Rothblum et al., 1986). The study examines the extent of prevalence of procrastinating behavior, and the relationship between academic procrastination and academic performance. The authors found a positive correlation between self-reported procrastination and delay in taking selfpaced quizzes, and a negative correlation between procrastination and grade point average. Our findings firstly capture procrastination with the help of LLM demonstrations, and secondly are in line with this study since we observe higher delays in writing the report for students with lower GPAs. However, we note that this relationship between GPA and academic delay requires deeper investigation as remarked in (Solomon & Rothblum, 1984) since the observations are based on correlation and not causation."
        },
        {
            "heading": "5. Discussion and Broader Context",
            "text": "In this work, we investigate the utility of foundation models to synthesize subrational human decision making behavior, that deviates from perfectly rationality (Simon, 1955). Our work is motivated by the need to capture subrational and biased human behavior across disciplines ranging from economics of households (Frederiks et al., 2015) to financial markets (Liu et al., 2022), alongside the difficulty in obtaining historical data for the same to design calibrated subrational human models (Tversky & Kahneman, 1992; Green et al., 1997). We propose a framework to generate synthetic human demonstration data for a variety of human subgroups using LLMs, that is subsequently used with IL to derive behavioral policies for each human. Such demonstration data has potential to be used to calibrate subrational human models while being cost-effective and efficient to generate as opposed to real human studies that cover small subsets of the human population. This data is also useful when trying to capture human behavior that is hard to describe through the use of a reward function (Kwon et al., 2022), and in cases where exponential discounting does not hold (Chabris et al., 2010). Calibrated human models can improve realism of agent-based models of real economic systems, that can be subsequently used for policy applications (Farmer & Foley, 2009; Dong et al., 2023). We ground our framework by comparing our experimental findings in four economic games with real human studies in the literature, where we are able to reproduce similar findings.\nAs we advocate for further research in this field utilizing foundation models to capture subrational behavior in a wider class of problems beyond the games that we modeled in this work, we now discuss challenges and potential limitations to the use of LLMs for the same. As for most applications using LLMs, we share some of the existing limitations such as the requirement for very careful prompt engineering (e.g. we end our prompts saying \u2018continue the sentence\u2019 so as to limit responses by relevance), which may even introduce unintentional user biases into the system. Prompt engineering is particularly important when dealing with complex, temporal scenarios that are common in economics when translating the agent states into natural language. This is also accompanied by their limited ability in handling numbers that can reduce their efficiency in such scenarios (Hendrycks et al., 2021). Also, ever since they rose to popularity, LLMs have seen constant updates accompanied by guardrails that may prevent the reproducibility of their responses. Based on the assumption that LLMs are computational models of humans, memorization of training data by LLMs is not a limitation, as it enables us to produce human-like behavior with more fidelity. That said, LLM responses could be sensitive to protected attributes of agents like age, gender, ethnicity making it challenging to differentiate the effects of biases in training data vs hallucinations. It is also unclear\nif one can trust the demonstrations generated by LLMs in unseen scenarios (e.g. different ages than those seen in the marshmallow experiment). We perform ablation studies by modifying the prompts for the Stanford marshmallow experiment to test for reasoning (or memorization) in the appendix. Further, evaluation of LLM responses for their human-ness or quality can be subjective, requiring human expert knowledge. In our work, we are able to reproduce similar behavior as was observed in real human studies, some of which were conducted over 50 years ago (Mischel & Ebbesen, 1970; Rothblum et al., 1986). It is especially intriguing to think of the evolution of LLMs in the coming years as they are subsequently trained and fine-tuned on more and more LLM generated data!\nTo conclude, our work shows an initial exploration of the use of LLMs for modeling subrational behavior demonstrating great potential with possible broader applications. We believe that LLMs have the capability to emerge as indispensable and cost-effective tools for modeling subrational behaviors, and help decision makers in different scenarios. This as long as their use is accompanied by a careful consideration of the challenges, limitations and potential biases."
        },
        {
            "heading": "Acknowledgements",
            "text": "This paper was prepared for informational purposes in part by the CDAO group of JPMorgan Chase & Co and its affiliates (\u201cJ.P. Morgan\u201d) and is not a product of the Research Department of J.P. Morgan. J.P. Morgan makes no representation and warranty whatsoever and disclaims all liability, for the completeness, accuracy or reliability of the information contained herein. This document is not intended as investment research or investment advice, or a recommendation, offer or solicitation for the purchase or sale of any security, financial instrument, financial product or service, or to be used in any way for evaluating the merits of participating in any transaction, and shall not constitute a solicitation under any jurisdiction or to any person, if such solicitation under such jurisdiction or to such person would be unlawful."
        },
        {
            "heading": "A. Experimental Details",
            "text": "We report additional details about the experimental results in order to improve the reproducibility and usage of our framework. In particular, for each experimental game we report the prompt and the answers from the LLMs, including some analysis on the distribution of results.\nA.1. Implementation\nWe implement our framework using Python3.9. The DQN algorithm is implemented using PyTorch and is composed of 3 full connected layers, where the input is a one-hot encoding of the state and the output represents the probability distribution over the actions space A. We use OpenAI GPT-4 (OpenAI, 2023a) API with model id. \"gpt-4-0613\". Where not otherwise stated, we generate 10 synthetic observations for each state, using the OpenAI python API for Chat Completions based on GPT-4 with temperature equal to 0.5 and a maximum of 5 output tokens. We defined the messages using the system role, to set the behavior of the LLMs, modifying its personality and providing specific instructions about how it should behave, and the user role to specify the environment state in which the LLM should take an action. Notice that, we specifically generate few demonstrations as the considered games are relatively simple (i.e, small state-action space). The few demonstrations are also used to demonstrate the ability of our framework to learn sub-rational policies from few data samples.\nA.2. Ultimatum Game\nHuman behavior. In our first test we evaluate how the model align with human behavior in the ultimatum game of Section 4.1, possibly rejecting too low offers. In such a case we define the API message as follow in Table 2:\nWhere we set the amount of money T = 10 and we vary the state between the 10 different possible splits x \u2208 {0, . . . , T}. We then convert each prompt and answer into a state-action (s, a) pair. We generate 10 synthetic observations for each state x. In Table 3 we report the rate at which the \"human\" LLM accept the proposed amount. All the raw answers are reported in table 12.\nFair behavior. We evaluate how the model align w.r.t. a more fair human behavior, in the ultimatum game of Section 4.1, possibly rejecting unfair splits. In such a case we define the API message as follow in Table 4:\nAlso in this case we set the amount of money T = 10 and we vary the state between the 10 different possible splits x \u2208 {0, . . . , T}. We then convert each prompt and answer into a state-action (s, a) pair. We generate 10 synthetic observations for each state x. In Table 5 we report the rate at which the \"fair\" LLM accept the proposed amount. All the raw answers are reported in table 13.\nSurprisingly, even if the LLMs clearly see a fair split as one around 50:50, the LLMs is also accepting a completely unfair split in its favor (i.e., Jerry takes all the money).\nDiscussion. We note that in our experiment we attempt to use a common name like Jerry to avoid possible biases: the work in (Aher et al., 2023) shows that LLMs can have different results when conditioned on names from different ethnicity or gender. While biases could be bad in machine learning models, in our cases this help to simulate and model different human behaviors. We will show in the next experiment as a age biases could be extremely useful to model a famous physiological game, which is otherwise unclear how to model using classical RL approaches.\nA.3. Stanford Marshmallow Experiment\nFor the Stanford marshmallow experiment we use the prompt define in Section 4.2. We here report more extensive results, simulating child with age Y \u2208 {2, 3, 4, 5}, and\nusing as waiting times both 15 minutes and 2 hours. We evaluate how the model align with child behavior, possibly accepting immediate rewards. In such a case we define the API message as follow in Table 6:\nWe generate 10 synthetic observations for each age Y , limiting the model to generate 5 tokens. In Table 7 we report the probability of waiting, computed as the percentage of observations where the LLM impersonate a waiting child. When we use \"15 minutes\" waiting time as in the original experiment, we notice that the model is more likely to wait, which may indicate a different human preferences w.r.t. original paper dated more than 50 years ago. Additionally, although it seems plausible that the percentage of children waiting increase as the waiting time decreases, we question the extent to which the model is overly rational. A two-year-old child may not possess enough awareness of the concept of time to distinguish and express a preference for 15 minutes over 2 hours. We report all the raw LLM answers in Table 14 and Table 15 for 2 hours and 15minutes waiting times, respectively.\nA.4. Double or Nothing Gamble\nFollowing (Tversky & Kahneman, 1992), we implement the prospect theory model with the following biased value function and probability weighting function:\nBiased Value Function. Empirical studies of human decisions often demonstrate loss aversion, showing that humans feel losses more than two times greater than the equivalent gains. To capture this behavior, (Tversky & Kahneman,\n1992) introduce the value function as\nv(x) = { x\u03b1 if x \u2265 0 \u2212\u03bb(\u2212x)\u03b1 if x < 0 . (1)\nwhere the parameters are estimated from real human data (\u03b1 = 0.88 and \u03bb = 2.25). The value function is concave for gains, convex for losses, and steeper for losses than gains. Thus, humans tend to be risk-averse over gains, preferring $450 with probability 1, over $1000 with probability 0.5; and risk-seeking over losses, preferring to lose -$1000 with probability 0.5 rather than having a certain loss of -$450.\nProbability Weighting Function. Another aspect of the prospect theory is that humans do not perceive probability in a linear fashion. When addressing any uncertainty, humans consider the natural boundaries of probability, impossibility and certainty, as the two reference points. The impact of a change in probability diminishes with its distance to the reference points. Considering a 0.1 increase in the probability of winning a prize, a change from 0 to 0.1 chance to win has more impact than a change from 0.45 to 0.55. To model this distortion in probability seen in humans, (Tversky & Kahneman, 1992) introduce a non-linear weighting function as\nw(p) = p\u03b4\n(p\u03b4 + (1\u2212 p)\u03b4)1/\u03b4 (2)\nwhere p is the probability of a potential outcome, and \u03b4 is estimated to be 0.61, 0.69 for gains, losses respectively.\nIn addition to the prompt in section 4.3, we collect the LLM demonstration from GPT-4 using the API message in table 8.\nDiscussion. While in theory fully rational human should always aim to maximize their expected rewards, we observe that GPT-4 prompted as \"rational human\" sometimes choose\nto play safe and deny the second bet as winner. In fact, over the past decades the human society has developed more concerns on risk, and a low-risk strategy will be recognized as good decision even if the expected utility is mediocre. This interesting results indicates that the LLMs are able to incorporate the changes in perspectives of the society.\nA.5. Academic Procrastination with Deadlines\nIn this experiment, we investigate if GPT-4 exhibits procrastinating behavior observed in students especially in selfpaced courses when they have a deliverable due within the next H days. We try to capture student attributes through their Grade Point Average (GPA), and report the API prompt for this experiment in table 9.\nWe collect 10 demonstrations for each GPA value x \u2208 {1, 3, 4.5} and each academic deadline y \u2208 {4, 10}. The results and analysis for deadline H = 4 are reported in section 4.4 where we observed that the probability of writing the report on the first day increases as the GPA of the student increases. As the deadline is made longer from 4 days to 10 days, we observe that the probability of writing the report on the last day decreases as the GPA of the student increases. The trend for writing the report on the first day with GPA does not hold for the longer deadline. We posit that this is potentially due to the inability of LLMs to reason over larger horizons giving a cluster of observations in the intermediate days with few for the extreme days (with 4.5 GPA potentially being less observed in their training data).\nThe MDP is defined as follows for the RL policy. The state at time t is st = [ t ft ] where t \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , H + 1} and ft \u2208 {0, 1} is a flag that tracks whether the report was written by time step t. The binary action at \u2208 {0, 1} indicates writing (at = 1) or procrastinating (at = 0) on the report at t. The reward is given by\nr(st, at, st+1) =\u2212 ctat \u00b7 1{ft=0}+ R \u00b7 1{t=H,fH+1=1}\u2212 KR \u00b7 1{t=H,fH+1=0}\nfor K > 1. The first term says that the student pays a cost ct at the time t when the report is written the first time. The second term is the final reward given to the student when the report is written before the deadline. The third term is to penalize cases when the student does not write the report within the deadline. When H = 4, the costs and reward are reported in section 4.4 similar to the example in (O\u2019Donoghue & Rabin, 1999). When H = 10, we use ct = t+ ct\u22121\u2200t \u2265 2 where c1 = 1 and R = 2c10.\nDiscussion. Note that we use neutral framing of student attributes other than GPA in our prompt, where we avoid using a name and use neutral pronouns to prevent any biases related to gender, ethnicity, age etc. Exponential discounting with \u03b3 = 1 says it\u2019s optimal to write the report on day 1 since they pick t that maximizes R\u2212 ct. Thus, the student gets to watch all the good movies on day 2 onward. On the other hand, one can show that with quasi-hyperbolic discounting with \u03b4 = 1 and \u03b2 = 0.4, the optimal strategy (for sophisticated humans) is to write the report on day 2. This is because the value function for writing the report on day t is \u03b2R \u2212 ct. Whereas, the value function for not writing the report on day t is \u03b2(R \u2212 c\u03c4 ) for \u03c4 > t where \u03c4 is the earliest day of writing the report after day t. The student chooses to write a report on day t only if \u03b2R\u2212 ct > \u03b2(R\u2212 c\u03c4 ) for \u03c4 described earlier. This discrepancy stems from the higher preference for immediate rewards (also called present-bias (Benhabib et al., 2010))."
        },
        {
            "heading": "B. Testing for Reasoning in LLM Demonstrations",
            "text": "We conduct additional experiments to examine whether the LLM demonstrations come from its reasoning capability or the memorization of historical experiment results. In particular, we modify the prompt in table 6 to see if GPT-4 can extrapolate to new scenarios rather than simply mimicking the results of Stanford marshmallow experiment.\nTable 10 shows the modified prompts of the Stanford marshmallow experiments along with an example of answers from GPT-4. In the default experiments a 5-year old child is willing to wait for more candies. However, we observe that LLM will not wait as a 5-year old if the delayed reward is replaced with bitter melons or the wait time increases to two years. This may indicate that LLM demonstrations are generated through a reasoning process, which are not simply replicated from the known experiment results in the existing literature.\nMeanwhile, we notice that LLMs fail to give reasonable demonstrations for certain scenarios highlighted in Table 11. For example, GPT-4 is willing to wait two hours for an additional bitter melon due to its hallucinations. In addition, GPT-4 sometimes demonstrates poor reasoning on the length of the wait time, and is willing to wait two months for an extra candy. This is possibly related to the well-known issues of LLMs in understanding numerical values (Shah et al., 2023)."
        }
    ],
    "title": "LLM-driven Imitation of Subrational Behavior : Illusion or Reality?",
    "year": 2024
}