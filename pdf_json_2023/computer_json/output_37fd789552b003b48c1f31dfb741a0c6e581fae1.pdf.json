{
    "abstractText": "Natural language processing (NLP) based on deep learning provides a positive performance for generative dialogue system, and the transformer model is a new boost in NLP after the advent of word vectors. In this paper, a Chinese generative dialogue system based on transformer is designed, which only uses a multi-layer transformer decoder to build the system and uses the design of an incomplete mask to realize one-way language generation. That is, questions can perceive context information in both directions, while reply sentences can only output one-way autoregressive. The above system improvements make the one-way generation of dialogue tasks more logical and reasonable, and the performance is better than the traditional dialogue system scheme. In consideration of the long-distance information weakness of absolute position coding, we put forward the improvement of relative position coding in theory, and verify it in subsequent experiments. In the transformer module, the calculation formula of self-attention is modified, and the relative position information is added to replace the absolute position coding of the position embedding layer. The performance of the modified model in BLEU, embedding average, grammatical and semantic coherence is ideal, to enhance long-distance attention.",
    "authors": [
        {
            "affiliations": [],
            "name": "Wenfeng Zheng"
        },
        {
            "affiliations": [],
            "name": "Lirong Yin"
        }
    ],
    "id": "SP:7b59faa113136ad1eba50eb6394c09ee701706c5",
    "references": [
        {
            "authors": [
                "L. Mateju",
                "D. Griol",
                "Z. Callejas",
                "J.M. Molina",
                "A. Sanchis"
            ],
            "title": "An empirical assessment of deep learning approaches to task-oriented dialog management",
            "venue": "Neurocomputing 439(June), 327\u2013339",
            "year": 2021
        },
        {
            "authors": [
                "J. Ni",
                "T. Young",
                "V. Pandelea",
                "F. Xue",
                "E. Cambria"
            ],
            "title": "Recent advances in deep learning based dialogue systems: a systematic survey",
            "venue": "Artif. Intell. Rev. 56(4), 3055\u20133155",
            "year": 2023
        },
        {
            "authors": [
                "I. Lauriola",
                "A. Lavelli",
                "F. Aiolli"
            ],
            "title": "An introduction to deep learning in natural language processing: models, techniques, and tools",
            "venue": "Neurocomputing 470(January), 443\u2013456",
            "year": 2022
        },
        {
            "authors": [
                "X Zhu"
            ],
            "title": "RNN Language Processing Model-Driven Spoken Dialogue System Modeling Method.",
            "venue": "Edited by Xin Ning. Computational Intelligence and Neuroscience",
            "year": 2022
        },
        {
            "authors": [
                "Y. Park",
                "Y. Ko",
                "J. Seo"
            ],
            "title": "BERT-based response selection in dialogue systems using utterance attention mechanisms",
            "venue": "Expert Syst. Appl. 209(December), 118277",
            "year": 2022
        },
        {
            "authors": [
                "T Junaid",
                "D Sumathi",
                "AN Sasikumar",
                "S Suthir",
                "J Manikandan",
                "K Rashmita",
                "PG Kuppusamy",
                "M Janardhana Raju"
            ],
            "title": "A comparative analysis of transformer based models for figurative language classification",
            "venue": "Comput Electr Eng",
            "year": 2022
        },
        {
            "authors": [
                "J. Li",
                "S. Joe Qin"
            ],
            "title": "Applying and dissecting LSTM neural networks and regularized learning for dynamic inferential modeling",
            "venue": "Comput. Chem. Eng. 175(July), 108264",
            "year": 2023
        },
        {
            "authors": [
                "A. Sherstinsky"
            ],
            "title": "Fundamentals of recurrent neural network (RNN) and long short-term memory (LSTM) network",
            "venue": "Physica D 404(March), 132306",
            "year": 2020
        },
        {
            "authors": [
                "P.B. Weerakody",
                "K.W. Wong",
                "G. Wang"
            ],
            "title": "Policy gradient empowered LSTM with dynamic skips for irregular time series data",
            "venue": "Appl. Soft Comput. 142(July), 110314",
            "year": 2023
        },
        {
            "authors": [
                "X. Zhang",
                "J. Shi",
                "M. Yang",
                "X. Huang",
                "A.S. Usmani",
                "G. Chen",
                "Jianmin",
                "Fu.",
                "J. Huang",
                "J. Li"
            ],
            "title": "Real-time pipeline leak detection and localization using an attention-based LSTM approach",
            "venue": "Process. Saf. Environ. Prot. 174(June), 460\u2013472",
            "year": 2023
        },
        {
            "authors": [
                "I Sutskever",
                "O Vinyals"
            ],
            "title": "Sequence to sequence learning with neural networks",
            "venue": "Adv Neural Inform Process Syst",
            "year": 2014
        },
        {
            "authors": [
                "D Bahdanau",
                "K Cho",
                "Y Bengio"
            ],
            "title": "Neural machine translation by jointly learning to align and translate",
            "venue": "Science",
            "year": 2014
        },
        {
            "authors": [
                "J. Li",
                "R. Chen",
                "X. Huang"
            ],
            "title": "A sequence-to-sequence remaining useful life prediction method combining unsupervised LSTM encoding-decoding and temporal convolutional network",
            "venue": "Meas. Sci. Technol. 33(8), 085013",
            "year": 2022
        },
        {
            "authors": [
                "Z. Liang",
                "Junping",
                "Du.",
                "C. Li"
            ],
            "title": "Abstractive social media text summarization using selective reinforced Seq2Seq attention model",
            "venue": "Neurocomputing 410(October), 432\u2013440",
            "year": 2020
        },
        {
            "authors": [
                "D Britz",
                "A Goldie",
                "M-T Luong",
                "L Quoc"
            ],
            "title": "Massive Exploration of Neural Machine Translation Architectures",
            "year": 2017
        },
        {
            "authors": [
                "J Chorowski",
                "D Bahdanau",
                "D Serdyuk",
                "K Cho",
                "Y Bengio"
            ],
            "title": "Attention-Based Models for Speech Recognition",
            "venue": "ArXiv.Org. June",
            "year": 2015
        },
        {
            "authors": [
                "Y. Shen"
            ],
            "title": "Bionic communication network and binary pigeoninspired optimization for multiagent cooperative task allocation",
            "venue": "IEEE Trans. Aerosp. Electron. Syst. 58(5), 3946\u20133961",
            "year": 2022
        },
        {
            "authors": [
                "H. Lv",
                "J. Chen",
                "T. Pan",
                "T. Zhang",
                "Y. Feng",
                "S. Liu"
            ],
            "title": "Attention mechanism in intelligent fault diagnosis of machinery: a review of technique and application",
            "venue": "Measurement 199(August), 111594",
            "year": 2022
        },
        {
            "authors": [
                "Q. Shi",
                "J. Fan",
                "Z. Wang",
                "Z. Zhang"
            ],
            "title": "Multimodal channel-wise attention transformer inspired by multisensory integration mechanisms of the brain",
            "venue": "Pattern Recogn. 130(October), 108837",
            "year": 2022
        },
        {
            "authors": [
                "X. Zhang",
                "Yawen",
                "Wu.",
                "P. Zhou",
                "X. Tang",
                "Jingtong",
                "Hu."
            ],
            "title": "Algorithm-hardware co-design of attention mechanism on FPGA devices",
            "venue": "Acm Trans Embedded Comput Syst 20(5), 71",
            "year": 2021
        },
        {
            "authors": [
                "J. Ni",
                "Z. Huang",
                "Chang",
                "Yu.",
                "D. Lv",
                "C. Wang"
            ],
            "title": "Comparative convolutional dynamic multi-attention recommendation model",
            "venue": "Ieee Trans Neural Netw Learn Syst 33(8), 3510\u20133521",
            "year": 2022
        },
        {
            "authors": [
                "J. Chen",
                "He",
                "Ye."
            ],
            "title": "A novel u-shaped encoder\u2013decoder network with attention mechanism for detection and evaluation of road cracks at pixel level",
            "venue": "Comput-Aid Civ Infrastruct Eng 37(13), 1721\u20131736",
            "year": 2022
        },
        {
            "authors": [
                "S. Du",
                "T. Li",
                "Y. Yang",
                "Horng",
                "S.-J."
            ],
            "title": "Multivariate time series forecasting via attention-based encoder\u2013decoder framework",
            "venue": "Neurocomputing 388(May), 269\u2013279",
            "year": 2020
        },
        {
            "authors": [
                "L. Feng",
                "C. Zhao",
                "Y. Sun"
            ],
            "title": "Dual attention-based encoder\u2013 decoder: a customized sequence-to-sequence learning for soft sensor development",
            "venue": "IEEE Trans Neural Netw Learn Syst 32(8), 3306\u20133317",
            "year": 2021
        },
        {
            "authors": [
                "T Mikolov"
            ],
            "title": "Statistical language models based on neural networks",
            "venue": "PhD thesis, Brno University of Technology",
            "year": 2012
        },
        {
            "authors": [
                "M. Schuster",
                "K. Paliwal"
            ],
            "title": "Bidirectional recurrent neural networks",
            "venue": "IEEE Trans. Signal Process. 45(11), 2673\u20132681",
            "year": 1997
        },
        {
            "authors": [
                "M. Sundermeyer",
                "R. Schluter"
            ],
            "title": "From feedforward to recurrent LSTM neural networks for language modeling",
            "venue": "IEEE/ACM Trans Audio Speech Lang Process 23(3), 517\u2013529",
            "year": 2015
        },
        {
            "authors": [
                "S. Zhu",
                "X. Cheng",
                "Sen",
                "Su."
            ],
            "title": "Knowledge-based question answering by tree-to-sequence learning",
            "venue": "Neurocomputing 372(January), 64\u201372",
            "year": 2020
        },
        {
            "authors": [
                "T Liu",
                "K Wang",
                "L Sha",
                "B Chang",
                "Z Sui"
            ],
            "title": "Table-to-text generation by structure-aware Seq2seq learning. proceedings of the AAAI conference on artificial intelligence 32 https:// doi",
            "venue": "aaai. v32i1",
            "year": 2018
        },
        {
            "authors": [
                "A Vaswani",
                "N Shazeer",
                "N Parmar"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Z. Niu",
                "G. Zhong",
                "Hui",
                "Yu."
            ],
            "title": "A review on the attention mechanism of deep learning",
            "venue": "Neurocomputing 452(September), 48\u201362",
            "year": 2021
        },
        {
            "authors": [
                "Qun",
                "He.",
                "L. Wenjing",
                "C. Zhangli"
            ],
            "title": "B&Anet: combining bidirectional LSTM and self-attention for end-to-end learning of taskoriented dialogue system",
            "venue": "Speech Commun. 125(December), 15\u201323",
            "year": 2020
        },
        {
            "authors": [
                "Beltagy Iz",
                "Matthew EP"
            ],
            "title": "Cohan A (2020) Longformer: The LongDocument Transformer",
            "year": 2020
        },
        {
            "authors": [
                "W. Shan",
                "D. Huang",
                "J. Wang",
                "F. Zou",
                "S. Li"
            ],
            "title": "Self-attention based fine-grained cross-media hybrid network",
            "venue": "Pattern Recogn. International Journal of Computational Intelligence Systems (2023) 16:168 1 3 Page 17 of 17 168 130(October), 108748",
            "year": 2022
        },
        {
            "authors": [
                "P. Dufter",
                "M. Schmitt",
                "H. Sch\u00fctze"
            ],
            "title": "Position information in transformers: an overview",
            "venue": "Comput. Linguist. 48(3), 733\u2013763",
            "year": 2022
        },
        {
            "authors": [
                "W Yida",
                "P Ke",
                "Y Zheng",
                "K Huang",
                "Y Jiang",
                "X Zhu",
                "M Huang"
            ],
            "title": "A large-scale chinese short-text conversation dataset. In: Paper presented at the Natural Language Processing and Chinese Computing, Cham. https:// doi",
            "year": 2020
        },
        {
            "authors": [
                "H.I. Abdalla",
                "A.A. Amer",
                "Amer",
                "Y.A"
            ],
            "title": "Boosting the itembased collaborative filtering model with novel similarity measures",
            "venue": "Int J Comput Intell Syst 16, 123",
            "year": 2023
        },
        {
            "authors": [
                "AA Amer",
                "HI Abdalla",
                "L Nguyen"
            ],
            "title": "Enhancing recommendation systems performance using highly-effective similarity measures",
            "venue": "Knowl-Based Syst",
            "year": 2021
        },
        {
            "authors": [
                "Z. Liu",
                "H. Liu",
                "W. Jia",
                "D. Zhang",
                "J. Tan"
            ],
            "title": "A multi-head neural network with unsymmetrical constraints for remaining useful life prediction",
            "venue": "Adv. Eng. Inform. 50(October), 101396",
            "year": 2021
        },
        {
            "authors": [
                "S. Reza",
                "M.C. Ferreira",
                "J.J.M. Machado",
                "M.R. Jo\u00e3o",
                "S. Tavares"
            ],
            "title": "A multi-head attention-based transformer model for traffic flow forecasting with a comparative analysis to recurrent neural networks",
            "venue": "Expert Syst. Appl. 202(September), 117275",
            "year": 2022
        },
        {
            "authors": [
                "L Zhang",
                "C-C Wang",
                "X Chen"
            ],
            "title": "Predicting Drug-target binding affinity through molecule representation block based on multihead attention and skip connection",
            "venue": "Briefings Bioinform",
            "year": 2022
        },
        {
            "authors": [
                "W. Zheng",
                "L. Yin"
            ],
            "title": "Characterization inference based on jointoptimization of multi-layer semantics and deep fusion matching network",
            "venue": "PeerJ Comput Sci 8(April), e908",
            "year": 2022
        },
        {
            "authors": [
                "W. Zheng",
                "Zhou",
                "Yu.",
                "S. Liu",
                "J. Tian",
                "Yang",
                "Bo.",
                "L. Yin"
            ],
            "title": "A deep fusion matching network semantic reasoning model",
            "venue": "Appl. Sci. 12(7), 3416",
            "year": 2022
        },
        {
            "authors": [
                "E.A. Atta",
                "A.F. Ali",
                "A.A. Elshamy"
            ],
            "title": "A modified weighted chimp optimization algorithm for training feed-forward neural network Edited by Kathiravan Srinivasan",
            "venue": "PLoS ONE 18(3), e0282514",
            "year": 2023
        },
        {
            "authors": [
                "Z. Ma",
                "W. Zheng",
                "X. Chen",
                "L. Yin"
            ],
            "title": "Joint embedding VQA model based on dynamic word vector",
            "venue": "PeerJ Computer Science 7(March), e353",
            "year": 2021
        },
        {
            "authors": [
                "Zong",
                "Yi.",
                "E. Pan"
            ],
            "title": "A SOM-based customer stratification model",
            "venue": "Wirel. Commun. Mob. Comput. 2022(March), e7479110",
            "year": 2022
        },
        {
            "authors": [
                "J Gehring",
                "M Auli",
                "D Grangier",
                "D Yarats",
                "YN. Dauphin"
            ],
            "title": "Convolutional Sequence to Sequence Learning",
            "venue": "Proceedings of the 34th international conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "Liu X",
                "Yu H-F",
                "Dhillon I",
                "Hsieh C-J"
            ],
            "title": "Learning to encode position for transformer with continuous dynamical model",
            "venue": "Proceedings of the 37th international conference on machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "HI Abdalla",
                "AA Amer"
            ],
            "title": "On the integration of similarity measures with machine learning models to enhance text classification performance",
            "venue": "Inform Sci",
            "year": 2022
        },
        {
            "authors": [
                "HI Abdalla",
                "AA Amer",
                "SD Ravana"
            ],
            "title": "BoW-based neural networks vs. cutting-edge models for single-label text classification",
            "venue": "Neural Comput Appl",
            "year": 2023
        },
        {
            "authors": [
                "L Shang",
                "Z Lu",
                "H Li"
            ],
            "title": "Neural responding machine for shorttext conversation",
            "venue": "arXiv. https:// doi. org/",
            "year": 2015
        },
        {
            "authors": [
                "O Vinyals",
                "Q Le"
            ],
            "title": "A neural conversational model",
            "year": 2015
        },
        {
            "authors": [
                "K Papineni",
                "S Roukos",
                "T Ward",
                "W-J Zhu"
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation. In: Proceedings of the 40th annual meeting of the association for computational linguistics, 311\u201318. Philadelphia, Pennsylvania, USA: Association for Computational Linguistics. https:// doi",
            "year": 2002
        },
        {
            "authors": [
                "C Corley",
                "R Mihalcea"
            ],
            "title": "Measuring the Semantic Similarity of Texts. In: Proceedings of the ACL workshop on empirical modeling of semantic equivalence and entailment, 13\u201318",
            "venue": "Ann Arbor,",
            "year": 2005
        },
        {
            "authors": [
                "M Lintean",
                "V Rus"
            ],
            "title": "Measuring semantic similarity in short texts through greedy pairing and word semantics",
            "venue": "Proceedings of the twenty-fifth international FLAIRS conference,",
            "year": 2012
        },
        {
            "authors": [
                "S. Yadav",
                "A. Kaushik"
            ],
            "title": "Do you ever get off track in a conversation? the conversational system\u2019s anatomy and evaluation metrics",
            "venue": "Knowledge 2(1), 55\u201387",
            "year": 2022
        },
        {
            "authors": [
                "J Wieting",
                "M Bansal",
                "K Gimpel",
                "K Livescu"
            ],
            "title": "Towards Universal Paraphrastic Sentence Embeddings",
            "year": 2016
        },
        {
            "authors": [
                "Zhong",
                "S.-H.",
                "P. Liu",
                "Z. Ming",
                "Y. Liu"
            ],
            "title": "How to evaluate singleround dialogues like humans: an information-oriented metric",
            "venue": "IEEE/ACM Trans Audio Speech Lang Process 28, 2211\u20132223",
            "year": 2020
        },
        {
            "authors": [
                "C. Zhang",
                "G. Lee",
                "L.F. D\u2019Haro",
                "H. Li"
            ],
            "title": "D-score: holistic dialogue evaluation without reference",
            "venue": "IEEE/ACM Trans Audio Speech Lang Process 29, 2502\u20132516",
            "year": 2021
        },
        {
            "authors": [
                "O Oluwatobi",
                "E Mueller"
            ],
            "title": "DLGNet: A transformer-based model for dialogue response generation. In: Proceedings of the 2nd workshop on natural language processing for conversational AI",
            "year": 2020
        },
        {
            "authors": [
                "Y Zhang",
                "S Sun",
                "M Galley",
                "Y-C Chen",
                "C Brockett",
                "X Gao",
                "J Gao",
                "J Liu",
                "B Dolan"
            ],
            "title": "Dialogpt: Large-scale generative pretraining for conversational response generation",
            "year": 2019
        },
        {
            "authors": [
                "J Luo",
                "X Zou",
                "M Hou"
            ],
            "title": "A novel character-word fusion chinese named entity recognition model based on attention mechanism",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Vol.:(0123456789)\nKeywords Relative position embedding\u00a0\u00b7 Natural language processing\u00a0\u00b7 Attention mechanism"
        },
        {
            "heading": "1 Introduction",
            "text": "Natural Language Processing (NLP) is a critical area of research that aims to enable machines to emulate human language and engage in seamless conversations with humans. This encompasses the capacity to read, comprehend, and fluently use language, master, and apply knowledge, and engage in logical thinking and inference [1\u20133]. Improved language intelligence through deep learning methods not only enhances a computer's ability to comprehend language\nbut also facilitates emotional expression and logical reasoning. Consequently, there are numerous potential applications for natural language processing solutions based on deep learning.\nIn the field of NLP, the adoption of recurrent neural networks (RNN) [4], attention mechanisms [5], and transformers [6] within end-to-end dialogue systems has significantly elevated the language comprehension and expression capabilities of these systems. In the earlier stages, RNN-based language models (RNNLM) gained prominence, achieving breakthroughs in NLP tasks. However, researchers soon encountered challenges related to long-range dependencies during model training. This arose from the tendency of weight parameters in RNN-based models to approach extremes, resulting in slow convergence and imprecise training outcomes. The introduction of long\u2013short-term memory (LSTM) [7\u201310] addressed this issue. LSTM, a variant of RNN, is better suited for processing lengthy sequences due to its architectural design, which incorporates three gate structures (input gate, output gate, and forgetting gate) for controlling information flow.\nThe concept of sequence-to-sequence (Seq2Seq) [11, 12] models emerged in 2014 as a method to generate\n* Wenfeng Zheng winfirms@uestc.edu.cn\n* Lirong Yin lyin5@lsu.edu\n1 School of\u00a0Automation, University of\u00a0Electronic Science and\u00a0Technology of\u00a0China, Chengdu\u00a0610054, China\n2 College of\u00a0Resource and\u00a0Environment Engineering, Guizhou University, Guiyang\u00a0550025, China\n3 School of\u00a0Geographic Science, Southwest University, Chongqing\u00a0400715, China\n4 Department of\u00a0Geography and\u00a0Anthropology, Louisiana State University, Baton\u00a0Rouge, LA\u00a070803, USA\nInternational Journal of Computational Intelligence Systems (2023) 16:168\n1 3\n168 Page 2 of 17\nsequences based on given input sequences. Initially applied to machine translation, Seq2Seq models addressed the challenge of handling variable-length input and output sequences. Over time, these models have shown promise in other NLP tasks, such as text summarization and dialogue generation. However, during the decoding phase, limitations were identified. The initial approach relied heavily on the last hidden layer state of the encoder, resulting in suboptimal information utilization [13]. In addition, when processing long input sequences, the fixed-length semantic vector struggled to retain critical feature information, leading to reduced accuracy. To overcome these issues, attention mechanisms were introduced [14].\nThe concept of attention mechanisms was initially proposed by Bahdanau et\u00a0al. for machine translation and later improved by Luong et\u00a0al. [15, 16]. Drawing inspiration from human selective attention, attention mechanisms mimic the human process of rapidly scanning and focusing on relevant information while disregarding irrelevant details. In the context of deep learning, attention mechanisms act as a resource allocation mechanism [17\u201319]. They dynamically redistribute the weight of information based on its importance, ensuring that critical information is given higher weight, while less important information is assigned lower weight. This feature extraction and sequential data analysis capability has found applications in various fields, including language modeling and image processing [20, 21].\nAttention mechanism in the decoding process [22], each output not only depends on the fixed-size semantic vector encoded by the encoder, but also depends on the hidden layer state of the previous output unit and the corresponding hidden layer state of the current output unit in the decoding process. Attention is introduced into the Seq2Seq model to solve the problem that the original RNN often loses part of the input sequence information, and the accuracy of the model is improved. In the specific translation task [23, 24], the decoding phase is to translate one word by one word in the time series. When decoding one word, it will not have the same association with all the words in the source sequence. In the decoder phase, the selected reference contributes the most to the semantic vector of the current sequence word, rather than uniformly referring to all the semantic vectors.\nThe introduction of attention mechanisms into Seq2Seq models aimed to address limitations in retaining input sequence information and improve model accuracy. During the decoding phase, rather than uniformly considering all input semantic vectors, attention mechanisms enable the model to selectively focus on the most relevant reference for the current sequence word. Prior to this development, the most effective language models were based on Seq2Seq architecture with LSTM for modeling. However, this approach lacked parallel computing capabilities during\ntraining, limiting the model's ability to meet the computational demands of increasingly larger corpora.\nTo fill the vacancy mentioned above. In this research, we introduced the implementation of a transformer-based generative dialogue system tailored for Chinese text. Theoretical foundations of the basic methods and process design were proposed. We designed a multi-turn generative dialogue system with an end-to-end structure that encodes natural language sentences into the model's vector space and generates sequences as output through the generative dialogue system's decoding process. When modeling and training the system, multi-turn statements were input in segments, and a self-regressive method was used to create a unidirectional generative language model. Generated words were continuously appended to the input until an end token was reached. We introduced a novel method to enhance long-distance attention within the dialogue system, replacing absolute position encoding in the position embedding module with relative position encoding. To test the effectiveness of relative position encoding in mitigating long-range information decay, we conducted experiments using multi-turn dialogue data sets, including the STC label data set and test data set. We compared the results with classical dialogue baseline models. The experimental results indicated that as the sequence length increased, accuracy improved, and the loss value decreased. This aligns with the expected outcomes of introducing relative position encoding, demonstrating that relative position encoding is better suited to handling longtext sequences compared to absolute position encoding. This underscores the effectiveness of our research optimization. In conclusion, the use of relative position encoding mitigates the issue of weak long-distance information, thereby enhancing the dialogue system's understanding of longrange information."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 End\u2011to\u2011end Dialogue Systems",
            "text": "Tomas et\u00a0al. proposed a language model RNNLM based on RNN in 2010[25, 26]. The model uses the vector of hidden states to record the historical information of word sequences. Hidden states can obtain long-range dependencies in the language. In the past, language models can only use the sliding window information of the front and back n words to predict the target words, while the advantage of the cyclic neural network is to fully use the context information to predict the target words. Sundermeyer et\u00a0al. Introduced LSTM into the language model in 2012 and proposed LSTM\u2013RNNLM [27]. The article mentions that LSTM has advantages over feed-forward neural networks, because it can utilize longterm contextual information. However, standard gradient\nInternational Journal of Computational Intelligence Systems (2023) 16:168\n1 3\nPage 3 of 17 168\ndescent algorithms do not perform well in learning longterm dependencies due to the instability of gradient computation. To solve this problem, the article introduces an improved RNN architecture, namely, LSTM. LSTM controls the flow of information by introducing input gates, forget gates, and output gates, thereby avoiding the problems of gradient disappearance and gradient explosion."
        },
        {
            "heading": "2.2 Seq2Seq Eencoder\u2013Decoder Model",
            "text": "Seq2Seq is an encoder\u2013decoder model [28]. The encoder and decoder are two cyclic neural networks, using the abovementioned LSTM or its variant GRU. The recurrent neural network is an autoregressive network structure. The output of the last time in the sequence is the input of the next time. The function of the first recurrent neural network is to embed the input sequence into the fixed-length semantic vector space. The vector represents the characteristics of the input sequence. The network is named encoder. The other task of RNN is to generate the output sequence from the fixed-length vector. The network is named decoder. Seq2Seq model based on recurrent neural network (LSTM or GRU) has achieved good results.\nIn their research, Tianyu Liu et\u00a0al. proposed a novel structure-aware seq2seq model for generating table-to-text descriptions [29]. This model improves generation performance by introducing an attention mechanism and a dual attention mechanism. The model can better utilize the structural information of the table and generate descriptions related to the content of the table. The results show that the model outperforms traditional statistical language models and basic seq2seq models in generative performance."
        },
        {
            "heading": "2.3 Transformer\u2011Based Language Model",
            "text": "In 2017, the language model based on the transformer began to try not to rely on RNN and LSTM modeling [30]. Transformer was proposed by Google in its paper on machine translation tasks, and achieved very good translation results, which consists of a positionwise feed-forward network (FFN) layer and a multi-head attention layer. FFN is used in each position separately, which can guarantee the position information of each symbol in the input sequence during operation. The latter makes the model focus on information from different representation subspaces from different positions [31].\nTransformer uses the self-attention mechanism to model the language model. Compared with RNN, self-attention mechanism not only increases the training parameters, but also realizes the parallelization through the complexity of space and parameters [32], which greatly accelerates the training efficiency of the model. In addition to being more parallelizable, the transformer establishes long-distance\ndependence through the self-attention mechanism. Transformer model is unable to process long sequences due to its self-attention operation, which scales quadratically with the sequence length [33]. Relative position coding originated from Google's paper [34]. Shan et\u00a0al. restricted the scope of self-attention to reduce the hybrid network model\u2019s consumption of memory and calculations and use the relative position encoding to improve robustness of the model. It is generally believed that relative position coding is inspired by absolute position coding [35]. Relative position information coding does not completely model the position information of each input but considers the relative distance between the current position and the position to be noticed when calculating attention, because natural language generally depends more on relative position. Therefore, relative position coding usually has excellent performance, and it is more flexible."
        },
        {
            "heading": "3 Methods",
            "text": "In this section, we will provide a detailed overview of the fundamental methods and process design employed in our study. The entire multi-turn generative dialogue system has been devised as an end-to-end structure. It takes natural language sentences, encodes them into a model vector space, and generates sequences as output through the generative dialogue system's decoding process. Furthermore, we propose the use of relative position encoding for self-attention computations, replacing the absolute position encoding in the dialogue system. This modification enhances long-range attention capabilities."
        },
        {
            "heading": "3.1 Dialogue System Implementation",
            "text": "First, a dialogue model network based on encoder\u2013decoder is proposed, and the autoregressive model is adopted in the implementation process. In the autoregressive model, the statements in the dialogue system are defined as the following equation:\nwhere X is a natural language sentence. xi represents the word vector of the ith word, so the problem turns into encoding these sequences. Suppose the question is X = (a, b, c, d, e, f ) , target output is Y = (P,Q,R, S, T) , the encoder\u2013decoder structure of a basic dialogue system is shown in Fig.\u00a01.\nOn the left of Fig.\u00a01 is the encoder of the dialogue system, which is responsible for encoding the variable length input sequence as long as possible into a fixed-length semantic vector. Theoretically, this fixed-length vector should contain all the useful information of the input sentence. The decoder\n(1)X = (x1, x2,\u2026 , xt)\nInternational Journal of Computational Intelligence Systems (2023) 16:168\n1 3\n168 Page 4 of 17\nof the dialogue system is responsible for decoding the vector just encoded into our desired output sequence. Unlike the encoder, the decoder must be \"unidirectional and recursive\", because the decoding process is recursive. The specific process is as follows:\n1) All output terminals start with the general identifier < CLS > tag and end with the < SEP > tag. These two identifiers are also regarded as one word;\n2) Input < CLS > to the decoder, then obtain the hidden layer vector, mix this vector with the output of the decoder, and then input it to the interpreter. The result of the interpreter should be output as p;\n3) Then input P into the decoder to obtain a new hidden layer vector, mix it with the output of the encoder again, and input it into the interpreter, which should output Q;\n4) Recurse successively until the output of the interpreter is < SEP > .\nIn the decoding process of the decoder, the output language model is shown in the following equation:\nThe so-called one-way language model, in a narrower sense, should be called positive language model. The crucial factor is that we cannot get \"future\" data. For example, in Eq.\u00a0(2), there is no additional input during predicting P; When forecasting Q, you can only enter P; when forecasting R, you can enter P, Q; and so on.\nAs shown in Fig.\u00a0 2, assume that the desired output result is Y = , when\n(2) p(P,Q,R, S, T) = p(P)p(Q|P)p(R|P,Q)p(S|P,Q,R)p(T|P,Q,R, S)\nthe decoder outputs, first the prediction result starts with the < CLS > identifier, input < CLS > into the decoder to get \"\u6c14\", continue to input into the encoder to get y in turn Y = \u3002\nIn the basic architecture, the transformer model is used to implement Seq2Seq. At this time, some key prior knowledge is introduced: considering that the input language and output language are Chinese, the hidden layer of encoder and decoder can share parameters and share the same set of word vectors, which will greatly reduce the number of parameters. The dialogue system is realized through multilayer transformer decoder.\nConsidering that the dialogue system is suitable for multiple rounds of dialogue tasks, there is a context sentence segment with multiple rounds in the dialogue in the input text, which is expressed in English segment. The dialogue system introduces segment-level recurrence mechanism (SLRM), which stores the information of the previous segment every time and splices it with the information of the current segment. Suppose that there are two segments with length L in a sample data text, expressed as s = ( x ,1, x ,2,\u2026 , x ,L ) and s +1 = (x +1,1, x +1,2,\u2026 , x +1,L) . Suppose the hidden layer information stored in s is represented as h \u2208 RL\u00d7D , D represents the dimension of the hidden layer vector, then the calculation method of s +1 is as Eqs.\u00a0(3)\u2013(5):\nSG represents a stop gradient, which means that the parameters of the previous segment remain unchanged. The length of the hidden layer is increased through sequence splicing, and then the whole enters the transformer model for training. The specific process of transformer is shown in Fig.\u00a03.\nTo sum up, the network structure of the dialogue system is mainly composed of the input layer, feature splicing layer, transformer decoder layer and output layer. The specific steps are:\n1) Each data sample is spliced by multiple rounds of dialogue text. The LCCC (large-scale cleaned Chinese conversation) corpus data [36] preprocessing process is used to splice multiple rounds of dialogue into a language sequence for natural language processing, that is, first, take the [CLS] tag as the starting character, extract continuous dialogue sentences and fill in the input sample, insert the [SEP] tag between the sentences of different speakers, and set the maximum length. Note that the sequence length of the input sample is N.\n(3)h\u0303 +1 = [ SG ( h ) \u25e6h +1 ]\n(4)q +1, k +1, v +1 = h\u0303 +1Wq, h\u0303 +1Wk, h\u0303 +1Wv\n(5)h +1 = Transformer(q +1, k +1, v +1)\n1 3\n2) To encode the data samples, first organize all words into a word table. By default, the word is the minimum granularity unit of the word vector. After the word table is established, record the number of word tables as V, and convert each word into a single hot coding vector to obtain N \u00d7 V size matrix as a training sample. The specific operation is to set the value at the index i dimension to 1 and the others to 0. Taking Fig.\u00a02 as an example, it is assumed that the processed sample is Y = , then N = 6, V = 6. The unique heat code in this scenario is described as the following equation:\n3) Learn the word embedding matrix W , and transform the unique hot coding into a word vector suitable for the subsequent transformer model by initializing it into a random word embedding matrix. The word embedding matrix XWE = XW of N \u00d7 D is obtained by embedding the input words of the unique encoding into the network, where D represents the embedding dimension of the word embedding vector, and W is the word embedding matrix, with the size of V \u00d7 D.\n4) Add segment embedding code. Segment embedding code indicates different roles of dialogue, which is represented by SegmentID. The specific vector content is a D-dimensional line vector filled with all 0 or all 1, where 0 or 1, respectively, represents the questioner or respondent, and N D-dimensional row vectors are spliced into a\n(6)\nsegment embedding matrix XSE of D \u00d7 N according to the statement sequence.\n5) To enhance the word vector representations with position coding information, a different approach is required when compared to cyclic neural networks. The transformer model, unlike cyclic neural networks, relies on the self-attention module and does not employ recursive operations. Consequently, it is unable to naturally capture timing information within the input text sequence and lacks inherent positional information for different word vectors in sentences. To address this limitation, the position information for each word needs to be incorporated into the word vectors. This enables the transformer model to distinguish the temporal sequence relationships among words within the current word order. To achieve this, position embedding is introduced, with the dimension of the position embedding set to N \u00d7 D. The method employed is the trigonometric function-based absolute position coding, often referred to as sinusoidal position coding [37, 38]. This technique integrates positional information into the input by performing a linear transformation using both sine and cosine functions, as shown in the following equation:\nEquation\u00a0(7) is the absolute position information coding formula; k represents the position of the word in the sentence. The value range is (0, N). d represents the dimension of the position vector, pk,2i , pk,2i+1 represents the (2i)th , (2i + 1)th components of the position coding vector,\n(7) \u23a7\u23aa\u23a8\u23aa\u23a9 pk,2i = ( k 10000 2i d ) pk,2i+1 = ( k 10000 2i d )\nInternational Journal of Computational Intelligence Systems (2023) 16:168\n1 3\n168 Page 6 of 17\nrespectively, that is, the coding is calculated by sine function and cosine function in even dimension and odd dimension, respectively, to get XPE . Thus, timing information with periodic changes is generated. The position is embedded in the dimension of the length n of the sample sequence. With the increase of the dimension number, the cycle will become slower and slower. Therefore, the model containing position texture information will be generated in even and odd dimensions. From this, the dependence between positions and the timing characteristics of natural language can be learned.\n6) Feature fusion, calculate the matrix after adding location information and segmentation information, as shown in the following equation:\nTo avoid data loss, three information matrices can be spliced together ( XWE XSE XPE ) , due to formula (7), the three information matrices are added directly, as shown in the following equation:\n7) Input XE into language expression layer is stacked by several layers of transformer decoder units, and the specific calculation of each layer module is as Eqs.\u00a0(10)\u2013(12):\nIntroduce the self-attention mechanism to calculate the attention moment matrix Z. First, XE is multiplied by three D \u00d7 D size weight matrices WQ,WK ,WV to obtain the query matrix Q, key matrix K and value matrix V:\nThe specific formula of attention mechanism is as the following equation:\nSuppose a sentence is X = (x1, x2, x3, x4) , that is, there are four-word vectors in the statement. After the operation from Eqs.\u00a0(10) to (12), the respective query vector Q , key vector K and value vector V are obtained, respectively. As shown in Fig.\u00a04:\nWhen calculating the self-attention vector of the first word x1 , it is necessary to calculate the dot product between the key vector of all words and the query vector of the current\n(8) \ufffd XWE XSE XPE \ufffd\u239b\u239c\u239c\u239d WE WS WP \u239e \u239f\u239f\u23a0 = XWEWE + XSEWS + XPEWP\n(9)XE = XWE + XSE + XPE\n(10)Q = XEWQ = [q1, q2,\u2026 , qN]\n(11)K = XEWK = [k1, k2,\u2026 , kN]\n(12)V = XEWV = [v1, v2,\u2026 , vN]\n(13)Attention(Q,K,V) = softmax \ufffd QKT\u221a dk \ufffd V\nword x1 to get the score. Each score is divided by the square root of \u221a dk to get Si, i = 1, 2, 3, 4 . Then, calculate Softmax to normalize the scores of all words to ai, i = 1, 2, 3, 4. This study adds a multi-head attention matrix to the Q , K , and V matrices to improve the attention unit's ability to extract multiple semantics of a word [39\u201343]. The schematic diagram of multi-head attention is shown in Fig.\u00a05. In this example, the implementation process of multi-head attention mechanism is to define the super parameter h = 3 to represent the number of heads, divide D into h parts, and divide Q, K and V into parts ( Qi,Ki,Vi ) , i = 1, 2, 3 through linear mapping, calculate attention for each part. The process is shown in Eqs.\u00a0(14)\u2013(16):\nInternational Journal of Computational Intelligence Systems (2023) 16:168\nPage 7 of 17 168\nW0 is the weight of the linear layer, the h attention features are spliced and linearly projected to obtain the attention feature matrix Z.\nThen connect the residuals, and the specific implementation is to add Z with XE get the attention matrix and get XA = Z + XE . At the same time, layer normalization is performed. The function of standard normalization is to treat the hidden layer in the network as standard normal distribution and speed up the convergence of loss function in the training process. Obtain X\u2032A , as the following equation:\nParameters ui and i , respectively, represent the mean and standard deviation of each element xij , \u03f5, is a minimum constant to prevent numerical calculation problems caused by division by 0, and \u03b1 and \u03b2 are trainable parameters to compensate for information loss caused by normalized.\nTransfer the residual and normalized matrix X\u2032A to the feed-forward layer. The feed-forward module is a multi-layer perceptron (MLP), which has a hidden layer. The hidden layer matrix is obtained by two-layer linear mapping and\n(14)Qi = QW Q i ,Ki = KW K i ,Vi = VW V i , i = 1,\u2026 , h\n(15)headi = Attention ( Qi,Ki,Vi ) , i = 1,\u2026 , h\n(16) Z = MultiHead(Q,K,V) = Concat(head1, head2,\u2026 , headh)W0\n(17) LayerNorm(x) = \u00d7 xij \u2212 ui\u221a 2 i + +\nactivation with the activation function ReLU, as shown in the following equation:\nFor matrix XH is then connected with the residuals and added with X\u2032A to obtain X\ufffdH = X\ufffdA + XH . The X\u2032H matrix is normalized and a new embedded matrix XE is output.\nAfter the multi-layer transformer module, the matrix XTE of D \u00d7 N is output. The processing steps of the multi-layer transformer module are summarized as follows: first, it is processed through the self-attention layer, and then transferred to the neural network layer. After the current transformer module is processed, it then transfers the vector to the next transformer module.\n8) In the output layer, when the last transformer module generates the output, the model multiplies the output vector by the word embedding matrix W. Each row of the word embedding matrix corresponds to the word embedding vector in the model word table, and the attention score corresponding to each word in the word table is obtained by multiplication [44\u201346].\nFinally, Softmax is used to predict word in the output dictionary, and the model uses cross entropy to update the parameters. In this way, the model completes a round and outputs a word. The model then continues to recurse until a complete sequence is generated, the upper limit n of the sequence is reached, or the terminator < SEP > is generated. Finally, the complete basic structure of the system is shown in Fig.\u00a06. Table\u00a01 shows the specific network parameters of the transformer-based generative dialogue system.\n(18)XH = ReLU(Linear(Linear(X\ufffdA)))\nInternational Journal of Computational Intelligence Systems (2023) 16:168\n1 3\n168 Page 8 of 17\nAs mentioned above, the transformer architecture can learn global information. The key lies in the self-attention mechanism. The self-attention mechanism calculates the encoded input sequence with each other to obtain the cosine similarity and form a similarity matrix of size n2 . which represents the length of the input sentence sequence [X, X]. Compared with the spatial complexity O(n) of RNN, the spatial complexity of the self-attention matrix is O ( n2 ) . The complexity of space and parameters realizes parallelism, and the increase of parameters of the self-attention matrix can contain more statement information, rather than the limitation of the result caused by the fixed-length semantic vector.\nEach column of the attention matrix represents input and each row represents output. The matrix represents the correlation between input and output. Suppose you input \"\u4f60\u60f3\u5403\u5565\" and the reply is \" \u767d\u5207\u9e21\", then the above statements are spliced into\nby language sequence preprocessing. When training unidirectional generative language model, input\nuntil < SEP > appears. Considering that current input cannot take advantage of \"future\" information. To generate a unidirectional language sequence, the input and output are staggered by one bit, as shown in Fig.\u00a07a:\nThe white square represents 0. The first line indicates that \"\u4f60\" can only be related to the starting mark < CLS > , the second line indicates that \"\u60f3t\" is related to the starting mark < CLS > and \"\u4f60\", and so on.\nBut the above model will also add the input to the prediction range, which belongs to additional constraints. The only thing that really needs prediction is .Therefore, this study refers to the idea of Mask for UNILM [23]. Design incomplete mask, only mask part, reserve information of\npart. As shown in Fig.\u00a07b: The attention of the input part obtains two-way context information, while the output part is one-way attention. Therefore, in the process of complementing the length of blank remaining sentences and mask in the maximum sentence length, 0 is generally filled in. This process is called padding. However, there will be problems during Softmax. The reason is e0 = 1 is an effective number. In this way, the padded part of Softmax participates in the operation, which is equivalent to allowing the invalid part to participate in the operation, which has an impact on Softmax calculation and results in deviation. Currently, it is necessary to cover up these invalid parts and do not participate in the calculation. The specific method is to add a large negative offset to the invalid part, as shown in Eqs.\u00a0(19)\u2013(21):\n(19)zillegal = zillegal + biasillegal\n(20)biasillegal \u2192 \u2212\u221e\nInternational Journal of Computational Intelligence Systems (2023) 16:168\n1 3\nPage 9 of 17 168\nAfter the calculation of Eqs.\u00a0(19)\u2013(21), the masking part will not be affected after participating in Softmax calculation, and the calculation increment is still 0, to avoid the influence of invalid area on training accuracy."
        },
        {
            "heading": "3.2 Absolute Position Information Coding",
            "text": "The transformer-based attention model is the dot product of the similarity between vectors in the matrix, there is no timing information added by the recursive process of RNN. In most cases, the previous solution is to integrate the location information into the input, which constitutes the general practice of absolute location coding. However, when dealing with multiple rounds of QA dialogue, the dialogue length should be unlimited in theory, but the design of absolute location coding limits the length of the dialogue text, and the above memory effect is not ideal in the long text.\nThe following analyzes several commonly used absolute position codes.\n1) One of the most concise schemes of absolute position coding is to directly train the position coding as a trainable parameter without designing the position coding formula. If the maximum length of the vector is N and the coding dimension is D, then initialize an N \u00d7 D matrix as position vector to update with the training process. The current BERT, GPT and other models use this kind of coding. The earliest Facebook paper in 2017 used this method [47].\nHowever, for this training absolute position coding, its disadvantage is that it has no scalability. If the maximum length formula of pre training is set to 512, it can only process sentences with a length of 512 at most. There is no matching location information for locations longer than 512. The solution is to randomly initialize the position code of more than 512, and then conduct training fine-tuning.\n2) Trigonometric function is another scheme of position coding, also known as sinusoidal position coding [30]. The coding is calculated by sine function and cosine function in even and odd dimensions, respectively, to generate timing information with periodic changes. The position is embedded in the dimension of length N of the sample sequence. With the increase of the dimension number, the cycle will become slower and slower. Therefore, the model containing position texture information is generated in even and odd dimensions, as shown in Fig.\u00a08. Thus, the dependency between positions and the timing characteristics of natural language can be learned.\nIt can be seen from Fig.\u00a08 that the position coding of trigonometric function is characterized by the periodic generation law according to the time sequence. However, with the increase of dimension serial number, the periodic change\n(21)ezillegal \u2192 0\nwill be slower and slower, which leads to the decline of the discrimination of position information for the input of long text in multiple rounds of dialogue.\n3) Theoretically, the reason why the RNN model does not need location coding is that it naturally has the possibility of learning location information. Therefore, assuming that a layer of RNN is added before the input word vector enters the model, and then input into the transformer module, the location information can be obtained theoretically, and the location coding is no longer needed. Similarly, RNN model training can be used to learn absolute position coding. ICML2020's paper [48] continues to develop this idea and proposes to model the position coding by means of differential equation (ODE). This method is called FLOATER. FLOATER belongs to recursive call model, so this differential equation is also called neural differential equation. In terms of basic theory, recursive location coding also has better scalability, and it also has better flexibility than trigonometric location coding. Obviously, recursive location coding sacrifices a certain degree of parallelism and will bring a speed bottleneck in theory.\nIn addition, language model performance improvement techniques other than positional encoding continue to develop. Hassan I. Abdalla et\u00a0al. [49, 50] proposed a scheme to improve text classification neural network using BoW, and improve the performance of text recognition and matching by integrating similarity measures with machine learning models. Internal evaluation of the ensemble model against the baseline model demonstrates\nInternational Journal of Computational Intelligence Systems (2023) 16:168\n1 3\n168 Page 10 of 17\nthat the above method has good optimization performance for neural networks for NLP."
        },
        {
            "heading": "3.3 Design of\u00a0Relative Position Information Coding",
            "text": "In this section, we will solve the problem of location information by introducing the design of relative location coding representation and detailing this process. A new attention calculation method of relative position coding is introduced to replace the absolute position coding of position embedding layer.\nThe disadvantage of absolute position coding is that it will produce remote attenuation. The larger the relative distance, the weaker the correlation between the inputs. The reason why periodic trigonometric functions appear and show attenuation trend is that the integral from high-frequency oscillation asymptotically approaches 0.\nThe assumed model is f (\u2026 , xm,\u2026 , xn,\u2026) , where, xm,xn represent the mth and nth input words, respectively. Now, we discuss the generality, and set f as a scalar function for calculation. This study uses transformer-based attention mechanism, so the function f has the characteristics of total symmetry, that is, for any m and n , there are shown in the following equation:\nFull symmetry is the main reason why transformer cannot recognize the position. Specifically, the function naturally satisfies the identity f (x, y) = f (y, x) , so that it is impossible to distinguish whether the input is (x, y) or (y, x) from the result.\nTherefore, to break this symmetry, it is necessary to add a position coding information. One feasible scheme is to add a position determined coding vector to each position, as shown in the following equation:\nIn general, assuming that all position encoding vectors are not the same, full symmetry does not hold. This means that we can use f\u0303 instead of f to address the input from positional timing information. To simplify the problem, we only consider the position encoding at two positions, m and n , and introduce it as a perturbation term, expanding it to the second-order Taylor term, as shown in the following equation:\nFrom Eq.\u00a0 (24), item 1 is independent of location, items 2 to 5 only depend on a single location, so they\n(22)f ( \u2026 , xm,\u2026 , xn,\u2026 ) = f (\u2026 , xn,\u2026 , xm,\u2026)\n(23) f\u0303 ( \u2026 , xm,\u2026 , xn,\u2026 ) = f (\u2026 , xm + pm,\u2026 , xn + pm,\u2026)\n(24)\n\ufffdf \u2248 f + p\u22a4 m\n\ud835\udf15f\n\ud835\udf15xm + p\u22a4 n\n\ud835\udf15f\n\ud835\udf15xn +\n1 2 p \u22a4 m \ud835\udf152f\n\ud835\udf15x2 m\n+ 1 2 p \u22a4 n \ud835\udf152f\n\ud835\udf15x2 n\n+ p\u22a4 m\n\ud835\udf152f\n\ud835\udf15xmxn pn\nonly depend on absolute location information, and item 6 owned the theinteractionitemofpm, pn at the same time, record it asp\u22a4\nm Ipn , it will be analyzed later, and it is\nexpected to express certain relative position information on this basis.Suppose I is the identity matrix, at this time p\u22a4 m Ip\u22a4 n = p\u22a4 m pn =< pm, pn > is the inner product of two position codes. It is hoped that in this simple example, this item represents the relative position information, that is, there is a function g as the following equation:\nHere, pm, pn is a d-dimensional vector, assuming d = 2, then for a two-dimensional vector, it is derived with the help of the complex number, that is, the vector [x, y] is regarded as the complex number x + yi . According to the algorithm of complex number multiplication, get the following equation:\np\u2217 n is the conjugate complex of pn . Re represents the real part of the complex number. To satisfy Eq.\u00a0(26), it can be assumed that there is a complex number qm\u2212n , as shown in the following equation:\nIn this way, taking the real part on both sides and we can obtain the following equation:\nTo solve this equation, the exponential form of the complex number can be used. Suppose pm = rmei m , p\u2217n = rne\n\u2212i n , qm\u2212n = Rm\u2212ne i\u03a6m\u2212n , then we can get the following equation:\nFor Eq.\u00a0(28), substitute n = m to get rm2 = R0 , rm is a constant, set to 1 for simplicity; for the second equation, let n = 0, then m \u2212 0 = \u03a6m , if 0 = 0, then m = \u03a6m, which is the same as m \u2212 n = m\u2212n . Substitute n = m-1, then m \u2212 m\u22121 = 1 . So that { m } is an arithmetic sequence. Therefore, the solution of position coding in two-dimensional case is shown in the following equation:\nBecause the inner product has the characteristic of linear superposition, the higher dimensional even dimensional position coding can be expressed as a combination of multiple two-dimensional position codes to obtain formula (39).\nEquation\u00a0(31) choose i = 10000 \u22122i\u2215d , this form has a good property: it makes < pm, pn > tends to 0 when |m\u2013n|\n(25)\u27e8pm, pn\u27e9 = g(m \u2212 n)\n(26)\u27e8pm, pn\u27e9 = Re[pmp\u2217n]\n(27)pmp\u2217n = qm\u2212n\n(28)rmrnei( m\u2212 n) = Rm\u2212nei\u03a6m\u2212n\n(29) {\nrmrn = Rm\u2212n m \u2212 n = \u03a6m\u2212n\n(30)pm = eim \u21d4 pm = ( m\nm\n)\nInternational Journal of Computational Intelligence Systems (2023) 16:168\n1 3\nPage 11 of 17 168\ngets larger. The larger the relative distance, the weaker the correlation. The reason is that the high-frequency oscillation integral gradually tends to 0, i.e., Eq.\u00a0(32):\nThe general attention with absolute position coding is as Eqs.\u00a0(33)\u2013(35):\nSoftMax is used to normalize the row dimension j, xi , pi are line vectors, pi indicates the added location information. Preliminary expand qikTj , the expansion is shown in the following equation:\nTo introduce relative position information, the structure of Eq.\u00a0(36) is modified to the following equation:\nThat is, remove the position of the first item and the position information of second item piWK is changed to binary position vector RK\ni,j . Then, expand the following\nequation:\n(31)pm = \u239b \u239c\u239c\u239c\u239c\u239c\u239d eim 0 eim 1 \u2026\ne\nim d\u22152\n\u22121 \u239e \u239f\u239f\u239f\u239f\u239f\u23a0 \u21d4 pm = \u239b \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d cosm 0 sinm 0 cosm 1 sinm 1 \u2026 cosm d\u22152\u22121 sinm d\u22152\u22121 \u239e \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0\n\u27e8pm, pn\u27e9 = Re \ufffd ei(m\u2212n) 0 + ei(m\u2212n) 1 +\u22ef + ei(m\u2212n) d\u22152\u22121 \ufffd\n= d\n2 Re [ d\u22152\u22121\u2211 i=0 ei(m\u2212n)10000 \u22122i\u2215d 1 d\u22152 ]\n(32)\u223c d 2 Re [ \u222b 1\n0\ne i(m\u2212n)10000\u2212t dt\n]\n(33) \u23a7\u23aa\u23a8\u23aa\u23a9 qi = \ufffd xi + pi \ufffd WQ ki = \ufffd xi + pi \ufffd WK vi = \ufffd xi + pi \ufffd WV\n(34)ai,j = softmax ( qik T j )\n(35)Oi = \u2211 j ai,jvi\n(36) qik\nT j = ( xi + pi ) WQW T K ( xi + pi )T = ( xiWQ + piWQ )( WT K xT j +WT K pT j )\n(37)ai,j = softmax(xiWQ(xjWK + RKi,j) T )\nChange piWV to RVi,j , get the following equation:\nAs can be seen from the formula above, the so-called relative position is to change the vector that originally depends on binary coordinates (i, j) to only depend on the relative position distance i \u2212 j , and usually needs to be truncated to adapt to different arbitrary distances. Therefore, the expression of RK\ni,j. is shown in Eqs.\u00a0(40) and (41):\nThrough the above modification, although only a limited number of position coding information are obtained, the relative position of any length can be expressed, pK , pV is the relative position code of trigonometric function formula. The specific definition of relative position code is shown in Eqs.\u00a0(42) and (43):"
        },
        {
            "heading": "4 Experiments and\u00a0Results",
            "text": "The purpose of this experiment is to test the role of relative position coding in long-distance multi-round conversation scenarios, and to verify whether relative coding is better than absolute position coding in slowing down the decline of long-distance information."
        },
        {
            "heading": "4.1 Data Set and\u00a0Environment",
            "text": "The pre-training of all models in this experiment used the LCCC corpus [36] as the training data set. In this study, the Chinese dialogue data set STC (short text conversation) [51] is selected for evaluation experiments, and the Chinese generative dialogue system and some classical dialogue baseline models are compared.\nThe short text dialogue corpus STC published by 2015 Huawei Noah's Ark laboratory is required to predict the reply under a given number of rounds of contextual dialogue\n(38)Oi = \u2211 j ai,jvi = \u2211 j ai,j(xjWV + piWV )\n(39)Oi = \u2211 j ai,j(xjWV + R V i,j )\n(40)RKi,j = pK[clip(i \u2212 j, pmin, pmax)]\n(41)RVi,j = pV [clip(i \u2212 j, pmin, pmax)]\n(42)pij[2k] = ( j \u2212 i\n10000 2k dz\n)\n(43)pij[2k + 1] = ( j \u2212 i\n10000 2k dz\n)\nInternational Journal of Computational Intelligence Systems (2023) 16:168\n168 Page 12 of 17\ncorpus. The data set contains 4.43 million Chinese dialogues. Each post has an average of 20 different replies, and there is a semantic gap between each reply, which is the main difference from the traditional parallel data set.\nThis section mainly uses the division method in STC and selects its test data as the test data set for this study. Compared with the labeled data set, the test data are characterized by sparse roles. As there is no labeled data set, the whole data set is manually labeled, and the tagger will actively guide the topic to talk about the characterization information. Therefore, most of the responses in the data set are characterization-related. However, in a real conversation scenario, it often does not involve a lot of characterization information in every chat. From this point of view, the test data set is closer to the characterization conversation in a real man\u2013machine conversation.\nThe experimental environment used for the experiment are some open-source frameworks, including TensorFlow 2.1.0, keras2.3.1, bert4Keras0.9.8. Specifically, the experimental operation environment is shown in Table\u00a02."
        },
        {
            "heading": "4.2 Experimental Evaluation Criteria",
            "text": "In this study, we first use the accuracy of generated tags (ACC) to evaluate the effectiveness of relative position coding, and then use objective and subjective evaluation methods for the final model.\nIn terms of objective evaluation indicators, we apply several commonly used dialogue evaluation methods, including PPL (perplexity score) [52], BLEU [53], Greedy Matching [54\u201356] and Embedded Average [57]. Some experiments show that the evaluation methods based on word embedding have higher correlations with human [58].\nIn the manual evaluation index [59], Grammatical and Semantic Continuity, Context Relevance and amount of information are used. 200 replies were sampled for each model, and 2 marked students were invited to manually evaluate these replies.\n4.3 Performance of\u00a0Chinese Dialogue System Based on\u00a0Transformer\nIn this experiment, the model features a 12-layer transformer decoder architecture, with individual characters as\nthe smallest unit for word embeddings. It employs 12 multihead attention heads, a vocabulary size of 13,088, character vector dimensions set at 384, and a maximum context length of 256 characters. The batch size used is 16. For training optimization, the dialogue system utilizes the Adam optimizer with a primary learning rate of 2 \u00d7 10\u22125 . The training is conducted over 100 epochs on the LCCC-base data set.\nTo validate the effectiveness of the proposed generative dialogue system in this study, three methods are introduced for comparison with the baseline model: Attn-Seq2Seq, transformer, and GPT. The baseline model is described as follows:\nAttn-Seq2Seq [36]: This model is based on the traditional Seq2Seq architecture with the addition of an attention mechanism. It also employs a multi-turn dialogue approach to concatenate multiple segments, similar to the dialogue system in this chapter. It uses the < CLS > and < SEP > identifiers to segment the segments, and these specific identifiers have corresponding word embeddings. The concatenated history dialogue is encoded and decoded using a bidirectional LSTM as the basic unit for Seq2Seq.\nTransformer [60]: The transformer model serves as the foundational architecture. This model has found broad applications in machine translation and dialogue generation. To ensure fairness in training the transformer model, a 12-layer transformer is used and trained for 100 epochs on the LCCCbase data set.\nGPT-chatbot [61]: This model is based on the GPT2 architecture for generative dialogue. Each training data are \"sequentially concatenated\" following the approach of Microsoft's DialoGPT. The concatenated data are then input into the network for training. The model consists of 12 layers of GPT and is trained for 100 epochs on the LCCC-base data set.\nAll transformer-based models share the same parameter settings for the encoder and decoder structures. They are essentially like the GPT model parameters. The vocabulary size is 13,088, word vector dimensions are 384, the maximum context length is 256, and the batch size is 16.\nBy conducting experiments, the objective evaluation index experimental results as shown in Table\u00a03 are obtained:\nTable\u00a03 displays the results of different language models in terms of perplexity (PPL), BLEU-2, BLEU-4, Dist-1, and Dist-2. A lower perplexity indicates smoother sentence\nInternational Journal of Computational Intelligence Systems (2023) 16:168\n1 3\nPage 13 of 17 168\ngeneration and better relevance to the topic. The best PPL is achieved by the dialogue model in this chapter, which, although only slightly outperforms the GPT2-chatbot, still holds an advantage. This can be attributed to the effectiveness of adding partial masking. In terms of perplexity performance, it surpasses other language models to some extent.\nIn BLEU-2 and BLEU-4 evaluations, this language model performs best in BLEU-4 but falls short of transformer in BLEU-2. This difference can be explained by the fact that BLEU was originally developed as an evaluation metric for translation, and it tends to favor shorter translation results. It may not handle morphologically rich sentences well, making it less friendly for generative dialogue models. Finally, in terms of diversity as measured by Dist-1 and Dist-2, this experimental model outperforms all baseline models.\nIn the human evaluation metrics, 200 responses were sampled for each model, and two annotators were invited to conduct manual evaluations on these responses. The results of the subjective evaluation metrics are presented in Table\u00a04 as follows:\nThe experimental results from the manual subjective evaluations demonstrate that the dialogue system designed in this study outperforms the baseline models across all three metrics. The dialogue system in this chapter is capable of generating high-quality dialogue responses. It not only produces responses with sufficient information but also\nmaintains good fluency in the sentences and relevance to the context. It surpasses other baseline models, confirming the effectiveness of the generative dialogue system developed in this research."
        },
        {
            "heading": "4.4 Relative Position Encoding Performance",
            "text": "Verification\nTo test whether the relative position coding can slow down the weakness of long-distance information, this segment selects the label data set and test data set of multi-round dialogue test data set STC for experiments.\nIn this section, we choose the dialogue system implemented in segment 3.1 as baseline. Then, multiple models are used for comparative experiments. Finally, the two optimization methods of relative position coding in this paper and word-character fusion [62] embedding are applied to the generative dialogue system at the same time, and compared\nwith the baseline model, as shown in Table\u00a05. First, in the model of this section using relative position coding, the results under different conditions are verified by increasing the sequence length and changing the size of batch size. As shown in Table\u00a06\nAt the same time, the attentional decline trend images at different relative distances are expressed. The attentional decline results using different t are shown in Figs.\u00a09 and 10 below.\nexcept t = t is abnormal and intersects with the x-axis, other trends are basically the same. The power function decreases faster in a short distance, while the exponential function decreases faster in a long distance. Therefore, choosing t = 10000\n\u2212t is a compromise. Through the comparative experiments of several models, the objective evaluation index experimental results as shown\nTable 3 Objective evaluation index experimental results\nModel PPL BLEU-2 BLEU-4 Dist-1 Dist-2\nAttn-Seq2Seq 37.23 4.51 0.94 8.5 11.94 Transformer 22.30 6.72 3.14 8.8 12.11 GPT-chatbot 20.52 5.69 2.78 8.1 11.73 Ours 19.83 6.63 3.20 9.2 12.68\nTable 4 Subjective evaluation results\nModel Syntax-semantic coherence Contextual relevance\nInformation amount\nAttn-Seq2Seq 1.13 0.92 1.17 Transformer 1.34 1.15 1.39 GPT-chatbot 1.59 1.20 1.40 Ours 1.64 1.33 1.42"
        },
        {
            "heading": "3 256 32 1.75 64.8%",
            "text": ""
        },
        {
            "heading": "2 256 16 1.78 64.6%",
            "text": ""
        },
        {
            "heading": "1 128 16 1.86 63.4%",
            "text": "1 3\nin Table\u00a07 are obtained. The experimental results of subjective evaluation indicators are shown in Table\u00a08.\nIt can be seen from the experimental results that the final dialogue system using word-character fusion Embedding and relative position coding achieves the best results in all indicators."
        },
        {
            "heading": "5 Discussion",
            "text": "Through an analysis and comparison of the labels generated by the models under varying sequence lengths and batch sizes, some valuable insights can be gleaned. Specifically, it becomes evident that increasing the length of the sequence\nFig. 10 Attentional decline results of different t (long distance trend)\nInternational Journal of Computational Intelligence Systems (2023) 16:168\n1 3\nPage 15 of 17 168\nleads to improved accuracy and smaller loss values. It indicates that relative position encoding has better processing power for long text sequences compared to absolute position encoding, which aligns with the anticipated outcomes of the design involving relative position coding.\nIn terms of objective performance indicators, the model presented in this paper outperforms the baseline and character models in metrics, such as bleu-2, bleu-4, and embedding average. In addition, it outperforms the baseline model in greedy matching and perplexity (ppl), but lags somewhat behind in char word.\nAlthough the model performs admirably in terms of grammatical and semantic coherence during manual evaluation, there is still a definite discrepancy in terms of contextual relevance and the amount of information when compared to the reference response. This emphasizes the significance of future research projects focused on context analysis and maintaining logical coherence in text generation.\nThese findings also demonstrate relative position coding's improved ability to handle lengthy text sequences as compared to absolute position coding. This observation underscores the effectiveness of the optimization approach employed in this study. In summary, this work showcases the proposal to incorporate relative position information into the self-attention formula within the transformer module, thereby enhancing long-distance attention mechanisms."
        },
        {
            "heading": "6 Conclusions",
            "text": "The main focus of this study is to theoretically present a design framework for a generative dialogue system based on the transformer architecture tailored for Chinese text. The use of transformer technology serves as the foundational framework. To address the limitation of unidirectional generation in language sequences and enable bidirectional access to contextual information within input sentences, the application of partial masking is introduced.\nThe study also introduces training and optimization techniques for the dialogue system, including teacher forcing and beam search, along with model pretraining on the LCCC data set. Comparative analyses are conducted against various baseline models, such as Attn-Seq2Seq, transformer, and GPT-chatbot to validate the effectiveness of this dialogue system in generating Chinese generative dialogues.\nSubsequently, the paper addresses the issue of text length limitations associated with common absolute position encoding. Building upon relative position encoding, the paper proposes a novel technique for relative position encoding tailored for Chinese text. In experiments, the transformer-based Chinese text generation dialogue model developed in this paper is used as the baseline model. The test data set in the short text dialogue corpus STC\nreleased by Huawei's Noah's Ark Laboratory was selected as the test data for the research task. Model performance is evaluated using both absolute and relative position encoding. Experimental results demonstrate the feasibility of enhancing the system's ability to mitigate the phenomenon of long-distance information decay by introducing relative position encoding. Modification of the self-attention calculation formula within the transformer module, by incorporating relative position information to replace the absolute position encoding in the embedding layer, results in enhanced long-range attention capabilities.\nIn summary, this study's primary contributions lie in offering a theoretical framework for designing a generative dialogue system for Chinese text, and it introduces a novel approach for relative position encoding to address text length limitations. Experimental findings support the effectiveness of this approach in mitigating long-distance information decay, achieved through adjustments to the self-attention mechanism within the transformer module.\nAcknowledgements The authors wish to thank the anonymous reviewers for their helpful comments.\nAuthor contributions Conceptualization: WZ and LY; methodology: GG, RW and JT; formal analysis and investigation: XL, SL and ZY; writing\u2014original draft preparation: LY, GG, JT and ZY; writing\u2014 review and editing: WZ, LY and JT; software: XL, GG and ZY; data curation: GG and RW; visualization: XL; resources: SL; supervision: WZ; project administration: LY; funding acquisition: WZ. All authors have read and approved the final manuscript.\nFunding Supported by Sichuan Science and Technology Program (2021YFQ0003, 2023YFSY0026, 2023YFH0004).\nData availability The data sets presented during the current study are available from the corresponding author on reasonable request.\nDeclarations\nConflict of interest The authors declare that they have no relevant financial or non-financial interests to disclose.\nEmployment Not applicable.\nEthical approval The Ethic statement is not applicable. This study does not include any animal or human studies.\nInformed consent statement Not applicable.\nInstitutional review board statement Not applicable.\nConsent for publication Not applicable.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in\nInternational Journal of Computational Intelligence Systems (2023) 16:168\n1 3\n168 Page 16 of 17\nthe article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/."
        }
    ],
    "title": "Design of a Modified Transformer Architecture Based on Relative Position Coding",
    "year": 2023
}