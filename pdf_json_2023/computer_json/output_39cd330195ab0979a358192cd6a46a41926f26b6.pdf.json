{
    "abstractText": "In many fields of robotics, knowing the relative position and orientation between two sensors is a mandatory precondition to operate with multiple sensing modalities. In this context, the pair LiDAR-RGB cameras offer complementary features: LiDARs yield sparse high quality range measurements, while RGB cameras provide a dense color measurement of the environment. Existing techniques often rely either on complex calibration targets that are expensive to obtain, or extracted virtual correspondences that can hinder the estimate\u2019s accuracy. In this paper we address the problem of LiDAR-RGB calibration using typical calibration patterns (i.e. A3 chessboard) with minimal human intervention. Our approach exploits the planarity of the target to find correspondences between the sensors measurements, leading to features that are robust to LiDAR noise. Moreover, we estimate a solution by solving a joint non-linear optimization problem. We validated our approach by carrying on quantitative and comparative experiments with other state-of-the-art approaches. Our results show that our simple schema performs on par or better than other approches using complex calibration targets. Finally, we release an open-source C++ implementation at https://github.com/srrg-sapienza/ca2lib",
    "authors": [
        {
            "affiliations": [],
            "name": "Emanuele Giacomini"
        },
        {
            "affiliations": [],
            "name": "Leonardo Brizi"
        },
        {
            "affiliations": [],
            "name": "Luca Di Giammarino"
        },
        {
            "affiliations": [],
            "name": "Omar Salem"
        },
        {
            "affiliations": [],
            "name": "Patrizio Perugini"
        },
        {
            "affiliations": [],
            "name": "Giorgio Grisetti"
        }
    ],
    "id": "SP:3914eb400530c911ff3af7670bc43069bd73eb06",
    "references": [
        {
            "authors": [
                "Jorge Beltr\u00e1n",
                "Carlos Guindel",
                "Arturo de la Escalera",
                "Fernando Garc\u0131\u0301a"
            ],
            "title": "Automatic extrinsic calibration method for lidar and camera sensor setups",
            "venue": "IEEE Trans. on Intelligent Transportation Systems (ITS),",
            "year": 2022
        },
        {
            "authors": [
                "Luca Di Giammarino",
                "Emanuele Giacomini",
                "Leonardo Brizi",
                "Omar Salem",
                "Giorgio Grisetti"
            ],
            "title": "Photometric lidar and rgb-d bundle adjustment",
            "venue": "IEEE Robotics and Automation Letters (RA-L),",
            "year": 2023
        },
        {
            "authors": [
                "Song Fan",
                "Ying Yu",
                "Maolin Xu",
                "Longhai Zhao"
            ],
            "title": "High-precision external parameter calibration method for camera and lidar based on a calibration device",
            "venue": "IEEE Access,",
            "year": 2023
        },
        {
            "authors": [
                "Martin A. Fischler",
                "Robert C. Bolles"
            ],
            "title": "Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography",
            "venue": "Commun. ACM,",
            "year": 1981
        },
        {
            "authors": [
                "S. Garrido-Jurado",
                "R. Mu\u00f1oz-Salinas",
                "F.J. Madrid-Cuevas",
                "M.J"
            ],
            "title": "Mar\u0131\u0301n-Jim\u00e9nez. Automatic generation and detection of highly reliable fiducial markers under occlusion",
            "venue": "Pattern Recognition,",
            "year": 2014
        },
        {
            "authors": [
                "Giorgio Grisetti",
                "Tiziano Guadagnino",
                "Irvin Aloise",
                "Mirco Colosi",
                "Bartolomeo Della Corte",
                "Dominik Schlegel"
            ],
            "title": "Least squares optimization: From theory to practice",
            "year": 2020
        },
        {
            "authors": [
                "Sagi Katz",
                "Ayellet Tal",
                "Ronen Basri"
            ],
            "title": "Direct visibility of point sets",
            "venue": "ACM Trans. Graph.,",
            "year": 2007
        },
        {
            "authors": [
                "Eung-su Kim",
                "Soon-Yong Park"
            ],
            "title": "Extrinsic calibration between camera and lidar sensors by matching multiple 3d planes",
            "venue": "IEEE Sensors Journal,",
            "year": 2020
        },
        {
            "authors": [
                "Nathan Koenig",
                "Andrew Howard"
            ],
            "title": "Design and use paradigms for gazebo, an open-source multi-robot simulator",
            "venue": "In Proc. of the IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS),",
            "year": 2004
        },
        {
            "authors": [
                "Xingxing Li",
                "Feiyang He",
                "Shengyu Li",
                "Yuxuan Zhou",
                "Chunxi Xia",
                "Xuanbin Wang"
            ],
            "title": "Accurate and automatic extrinsic calibration for a monocular camera and heterogenous 3d lidars",
            "venue": "IEEE Sensors Journal,",
            "year": 2022
        },
        {
            "authors": [
                "Xudong Lv",
                "Boya Wang",
                "Ziwen Dou",
                "Dong Ye",
                "Shuo Wang"
            ],
            "title": "Lccnet: Lidar and camera self-calibration using cost volume network",
            "venue": "In Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "Faraz M Mirzaei",
                "Dimitrios G Kottas",
                "Stergios I Roumeliotis"
            ],
            "title": "3d lidar\u2013camera intrinsic and extrinsic calibration: Identifiability and analytical least-squares-based initialization",
            "venue": "Intl. Journal of Robotics Research (IJRR),",
            "year": 2012
        },
        {
            "authors": [
                "Gaurav Pandey",
                "James R McBride",
                "Silvio Savarese",
                "Ryan M Eustice"
            ],
            "title": "Automatic extrinsic calibration of vision and lidar by maximizing mutual information",
            "year": 2015
        },
        {
            "authors": [
                "Yoonsu Park",
                "Seokmin Yun",
                "Chee Sun Won",
                "Kyungeun Cho",
                "Kyhyun Um",
                "Sungdae Sim"
            ],
            "title": "Calibration between color camera and 3d lidar instruments with a polygonal planar board",
            "venue": "IEEE Sensors Journal,",
            "year": 2014
        },
        {
            "authors": [
                "Zoltan Pusztai",
                "Levente Hajder"
            ],
            "title": "Accurate calibration of lidarcamera systems using ordinary boxes",
            "venue": "IEEE International Conference on Computer Vision Workshops (ICCVW),",
            "year": 2017
        },
        {
            "authors": [
                "Ashutosh Singandhupe",
                "Hung Manh La",
                "Quang Phuc Ha"
            ],
            "title": "Single frame lidar-camera calibration using registration of 3d planes",
            "venue": "Sixth IEEE International Conference on Robotic Computing (IRC),",
            "year": 2022
        },
        {
            "authors": [
                "Chao Sun",
                "Zhijie Wei",
                "Wenyi Huang",
                "Qianfei Liu",
                "Bo Wang"
            ],
            "title": "Automatic targetless calibration for lidar and camera based on instance segmentation",
            "venue": "IEEE Robotics and Automation Letters (RA-L),",
            "year": 2022
        },
        {
            "authors": [
                "Tekla T\u00f3th",
                "Zolt\u00e1n Pusztai",
                "Levente Hajder"
            ],
            "title": "Automatic lidarcamera calibration of extrinsic parameters using a spherical target",
            "venue": "In Proc. of the IEEE Intl. Conf. on Robotics & Automation (ICRA),",
            "year": 2020
        },
        {
            "authors": [
                "Jonas Uhrig",
                "Nick Schneider",
                "Lukas Schneider",
                "Uwe Franke",
                "Thomas Brox",
                "Andreas Geiger"
            ],
            "title": "Sparsity invariant cnns, 2017",
            "year": 2017
        },
        {
            "authors": [
                "Alex Wong",
                "Xiaohan Fei",
                "Stephanie Tsuei",
                "Stefano Soatto"
            ],
            "title": "Unsupervised depth completion from visual inertial odometry",
            "venue": "IEEE Robotics and Automation Letters (RA-L),",
            "year": 1899
        },
        {
            "authors": [
                "Byung-Hyun Yoon",
                "Hyeon-Woo Jeong",
                "Kang-Sun Choi"
            ],
            "title": "Targetless multiple camera-lidar extrinsic calibration using object pose estimation",
            "venue": "In Proc. of the IEEE Intl. Conf. on Robotics & Automation (ICRA),",
            "year": 2021
        },
        {
            "authors": [
                "Lipu Zhou",
                "Zimo Li",
                "Michael Kaess"
            ],
            "title": "Automatic extrinsic calibration of a camera and a 3d lidar using line and plane correspondences",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "Existing techniques often rely either on complex calibration targets that are expensive to obtain, or extracted virtual correspondences that can hinder the estimate\u2019s accuracy.\nIn this paper we address the problem of LiDAR-RGB calibration using typical calibration patterns (i.e. A3 chessboard) with minimal human intervention. Our approach exploits the planarity of the target to find correspondences between the sensors measurements, leading to features that are robust to LiDAR noise. Moreover, we estimate a solution by solving a joint non-linear optimization problem.\nWe validated our approach by carrying on quantitative and comparative experiments with other state-of-the-art approaches. Our results show that our simple schema performs on par or better than other approches using complex calibration targets. Finally, we release an open-source C++ implementation at https://github.com/srrg-sapienza/ca2lib\nI. INTRODUCTION\nThe ability to fuse readings from heterogeneous sensors is often beneficial in many robotics and perception applications. In particular, LiDAR and RGB sensors exhibit a strong compatibility: the former being able to capture high precision sparse range readings while the latter measure dense color intensity measurements.\nThese properties makes integration between the two sensors suited for the task of depth estimation. Historically, stereo based solutions leverage the known relative offset between two cameras, along with concepts from epipolar geometry to estimate a depth value for every pixel in a image. Albeit its popularity, due to its optical nature, these approaches suffer in texture-less regions and in areas where the depth exceed a maximum value determined by the baseline of the stereo. While the texture-less problem has been partially solved by the usage of active stereo sensors (i.e. Realsense D435, Kinect), the maximum range still poses\nAll authors are with the Department of Computer, Control, and Management Engineering \u201cAntonio Ruberti\u201d, Sapienza University of Rome, Italy, Email: {giacomini, brizi, digiammarino, salem, grisetti}@diag.uniroma1.it.\nThis work has been supported by PNRR MUR project PE0000013FAIR\n* The authors contributed equally.\na challenge. On the contrary, LiDARs operates using Time of Flight (TOF) principle, which is proven to be robust at detecting range measurements on non-reflective surfaces with accuracy even at high distances.\nThese considerations lead the community to investigate the problem of depth-completion, namely estimating a dense depth image by superimposing an accurate sparse depth measurement along an intensity RGB image. Multiple publicly available datasets like KITTI and VOID [20][21] allowed the community to tackle this problem either by fully leveraging the sparse depth measurement (unguided) or by fusing RGB features (guided). Furthermore, in the field of 3D reconstruction, recent findings show that coupling the two sensors may lead to a more robust and accurate trajectory estimate [2].\nBesides, to accomplish any of these tasks, one would require to know the relative offset between the two sensors.\nThis work aims at solving the task of LiDAR-RGB calibration, namely, estimating the relative offset (extrinsic parameters) between the two sensors, using their raw measurements.\nThe core idea behind this multi-modal calibration is to find spatial correspondences between the sensors measurements. Most approaches rely on one or more calibration patterns to establish common features between the sensors. These\nar X\niv :2\n30 9.\n07 87\n4v 1\n[ cs\n.R O\n] 1\n4 Se\np 20\n23\npatterns are often complex or expensive to produce [1]. The main contribution of this paper is a versatile calibration toolbox that allows to estimate the extrinsic parameters between LiDAR and RGB with minimal user intervention, using a simple calibration checkerboard target. We leverage a joint non-linear formulation of the problem to achieve high accuracy results even with a minimum of three measurements. The requirement for our method is to use a calibration pattern (e.g. Checkerboard, ChAruCO [5]) that must be observed by both sensors during the acquisition. We exploit the planarity of the target to find a common observation used to estimate the extrinsic parameters. Moreover, we release an open-source implementation of our toolbox."
        },
        {
            "heading": "II. RELATED WORK",
            "text": "This paper\u2019s section delves into LiDAR-RGB calibration and explores the two main classes of approaches: targetbased and target-less. As the name suggests, target-based approaches require the user to place artificial markers that both the camera and LiDAR can easily detect. This contrasts with target-less methods that free the use from this task. The core idea of calibration is common in the two classes of approaches: computing common features between heterogeneous measurements and estimating the transformation that minimizes the distance between corresponding features.\nFirst, an overview of target-less approaches is presented: Pandey et al. presents an automatic data-driven approach based upon the maximization of mutual information between the sensor-measured surface intensities [13]. The authors exploits the correlation coefficient for the reflectivity and intensity values of many scan-image pairs using different calibration parameters. However, shadows of objects or colored surfaces that completely absorb infrared lights might result in weaker correlation between scan-image pairs. Yoon et al. proposes a calibration method using regionbased object pose estimation. Objects are segmented in both measurements, then a 3D mesh is generated from the LiDAR measurements, while images are used to reconstruct the scene using Structure from Motion (SfM). The two models are then registered together to acquire an initial guess on the relative pose. The final solution is obtained iteratively by finding correspondences between the reconstructed objects from both measurements [22]. In recent years, the development of learning based methods have also spanned in this field: Lv et al. proposes a real-time self-calibration network that predict the extrinsic parameters by constructing a cost volume between RGB and LiDAR features [11], while Sun et al. first estimates an initial guess by solving an handeye calibration method [18]. Moreover, the guess is finetuned by segmenting the image-cloud pair and by aligning the distances between centroids. The advantage of targetless method is that they can be used without preparing the environment. This comes at a cost of a lower accuracy and robustness when compared to their target-based counterpart.\nTarget-based methods estimate the relative pose using an observed known structure. Given the difference of resolution for the two sensors, it is high unlikely that cor-\nrespondences within the measurements can be established directly. For this reason, point-to-point methods tends to process LiDAR measurements to implicitly obtain virtual points1 easily detectable from an RGB sensor. For instance, Park et al. utilizes a specially designed polygonal planar calibration board with known lengths of adjacent sides [14]. By estimating the 3D corresponding points from the LiDAR, vertices of the board can be determined as the meeting points of two projected sides. The vertices, along with the corresponding points detected from the color image, are used for calibration. Pusztai et al. introduces a methodology that utilizes cubic objects with predetermined side lengths [15], [16]. The corners of the cubes are estimated by initially detecting each side of the box and subsequently determining their intersection points. Furthermore, the corners along with their corresponding RGB image are employed to calibrate the system by solving Iterative Corresponding Point (ICP). Zhouet al. proposes a single-shot calibration method requiring a checkerboard [23]. The target is detected both in the RGB image, and LiDAR measurement, using RANSAC[4] for the latter. Furthermore, the four edges of the checkerboard are estimated and aligned to compute the relative offset between the two sensors. To\u0301th et al. introduces a fully automatic calibration technique that leverages the utilization of spheres, enabling accurate detection in both point clouds and camera images [19]. Upon successful detection, the algorithm aligns the set of sphere centers using SVD. Beltra\u0301n et al. presents a methodology that utilizes a custom calibration target equipped with 4 holes and AruCO markers specifically designed for monocular detection [1]. The methodology employs a set of techniques for each sensor to estimate the center points of the holes. Subsequently, the relative offset between sensors are determined by aligning the set of centers obtained from each sensor, Li et al. adopt a similar approach while using a checkerboard with 4 holes [10]. Fan et al. propose a two-stage calibration method using an auxiliary device with distinctive geometric features [3]. The method extracts lines from images and LiDAR point clouds, providing an initial estimation of the external parameters. Nonlinear optimization is then applied to refine these parameters. In the work of Singandhupe et al., the authors first extract planar information from RGB and LiDAR measurements, then, two grid of points are extracted from the computed planar patches and aligned using a customized ICP algorithm [17].\nAlbeit these approaches provides relatively accurate results with few measurements, care should be taken during the estimation of virtual correspondences, as they can cause significant errors in the estimation step. Moreover these custom targets often requires precise construction or expensive manufacturing.\nAnother group of approaches does not directly solve the calibration problem using point-to-point correspondences, but rather exploit the planarity of the target to reduce the\n1Points that are not explicitly detected, but estimated from the LiDAR measurement.\nfeasible set of solutions using plane-to-plane constraints. Mirzaei et al. addresses the challenge of accurate initial estimates by dividing the problem into two sub-problems and analytically solving each to obtain precise initial estimates [12]. The authors then refine these estimates through iterative minimization. They also discuss the identifiability conditions for accurate parameter estimation. Finally, a method similar to our proposal, Kim et al. combine observed normals to first estimate the relative orientation with SVD and then iteratively estimates an initial guess of the relative translation by minimizing the pairwise planar distances between measurements [8]. Finally, the translation is refined using a non-linear optimization problem using Levenberg-Marquardt (LM). Despite its simplicity, this method decouples the estimation of orientation and translation, thus leading to potential losses in accuracy while also increasing the number of required measurements.\nCompared with the state of the art, we propose: \u2022 a formulation for joint nonlinear optimization that cou-\nples relative rotation and translation using a plane-toplane metric; \u2022 an extensible framework that decouples the optimization from target detection. Currently supports Checkerboard and ChARuCO patterns of typical A3-A4 sizes, easily obtainable from commercial printers; \u2022 the possibility to handle different camera models and distortion; \u2022 an open-source C++ implementation."
        },
        {
            "heading": "III. OUR APPROACH",
            "text": "In this section, we will provide a detailed and comprehensive description of our method. First we describe the preliminaries required to understand our approach, then every component of the pipeline is described, following the procedure from the acquisition of the measurements up to the computation of the relative poses between the two sensors (extrinsic parameter).\nPlane Representation: Let \u03c0 = (n\u0302, d) be a 3D plane, where n\u0302 \u2208 S2 represents the unit vector orthogonal to the plane and d \u2208 R is the shortest distance of the plane respect to the origin. Applying a transform X \u2208 SE(3) to a plane \u03c0 yields new coefficients \u03c0\u2032: as follows:\nX\u03c0 = { n\u0302\u2032 = Rn d\u2032 = d+ (Rn)tt\n(1)\nHere X = \u27e8R; t\u27e9 is represented by a rotation matrix R \u2208 SO(3), and the translation vector t \u2208 R3.\nIf the transformation is modified by a small local perturbation \u2206X = (\u2206R|\u2206t) then we can rewrite:\n(X\u229e\u2206X)\u03c0 =\n{ n\u0303 = \u2206RRn\nd\u0303 = d\u2032 + ntRt\u2206Rt\u2206t (2)\nDeriving the result with respect to \u2206X leads to the following Jacobian:\n\u2202(X\u229e\u2206X)\u03c0 \u2202\u2206X = [ 03\u00d73 \u2212\u230aRn\u230b\u00d7 ntRt 01\u00d73 ] 4\u00d76\n(3)\nThe distance between two planes depends both on the difference between their normals and the signed distance of the planes from the origin. These quantities can be captured by a 4D error vector ep expressing the plane-to-plane error metric:\np(\u03c0k) = \u2212nkdk (4)\nep(\u03c0i, \u03c0j) =\n[ nti(p(\u03c0i)\u2212 p(\u03c0j))\nnj \u2212 ni\n] . (5)\nHere p(\u03c0k) is the point on the plane closest to the origin of the reference system, and it is obtained by taking a point along the normal direction n at distance d.\nPinhole Model (RGB): Let p be a point expressed in camera frame and K be t camera matrix. Assuming any lens distortion effect have been previously corrected, then the projection on the image plane of p is computed as\n\u03c0c(p) = \u03d5(Kp) (6)\nK = fx 0 cx0 fy cy 0 0 1  (7) \u03d5(v) = 1\nvz [ vx vy ] (8)\nwhere \u03d5(v) represents the homogeneous division and \u03c0c(p) the pinhole projection function. For simplicity, we detail only the pinhole camera projection, however the same principle applies for more complex camera models.\nProjection by ID (LiDAR): Let p be a point detected by the LiDAR and expressed in its frame. Its projection is computed as:\n\u03c0l(p) = A\u03c8(p) (9)\nA = [ fx 0 cx 0 1 0 ] (10)\n\u03c8(v) = atan2(vy, vx)ring(v) 1  (11) where fx represent the azimuth resolution of the LiDAR, while cx denotes the offset in pixels. The ring(v) function is either obtained directly from the LiDAR sensor, which augment every measured point with a number that represents the beam that detected it or, assuming the cloud is ordered, by dividing the point index by the horizontal resolution of the sensor. Compared with the classical spherical projection, the projection by ID does not preserve the geometric consistency of the scene. Still, it provides an image with no holes, which is preferred for computer-vision applications.\nFirst, we process the incoming raw LiDAR and RGB measurements to acquire planar information. Assuming the scene to remain static throughout the acquisition of a single joint measurement, the LiDAR cloud is embedded in an image using the projection by ID. Moreover, the system awaits the user interaction to guess the position of the calibration target on the LiDAR image.\nA parametric circular patch around the user\u2019s selection is used to estimate a plane using RANSAC and, concurrently, the calibration target detection is attempted on the RGB image. Once the target is detected, the RGB plane is computed by solving the ICP. If the user is satisfied with both LiDAR and RGB planes, they are stored for processing.\nWhereas a straightforward rank analysis of the Jacobians reveals that just 3 measurments are sufficient to constrain a solution, it is well known from the estimation theory that the accuracy grows with the number of measurements.\nOnce the set of measurements are acquired, we jointly estimate the relative orientation and translation of the LiDAR with respect to the RGB sensor X \u2208 SE(3) by solving the following nonlinear minimization problem:\nX = argmin X\u2208SE(3) \u2211 i\u2208Z \u2225X\u03c0il \u2212 \u03c0ic\ufe38 \ufe37\ufe37 \ufe38 ep \u22252 (12)\nwhere ep represent the plane-to-plane error. During acquisition, it may happen that the user accept one or more wrongly estimated measurements. Due to the quadratic nature of the error terms, these outliers are often over-accounted, resulting in wrong estimations. To account for this factor, as described in [6], we employ an Huber Mestimator \u03c1(\u00b7) that treats differently measurements based on their error. We rewrite Eq. (12) as follows:\nX = argmin X\u2208SE(3) \u2211 i\u2208Z \u03c1(\u2225X\u03c0il \u2212 \u03c0ic\u2225). (13)\nTo resolve Eq. (13) we employ the Gauss-Newton (GN) algorithm implemented in the srrg2 solver[6]."
        },
        {
            "heading": "IV. EXPERIMENTAL EVALUATION",
            "text": "In this section, we describe the experiments we conducted to establish the quality of our calibration toolbox. We perform quantitative experiments in the simulated environment provided by [1] to compare our estimates with groundtruth while we also conduct qualitative and quantitive experiments on real scenarios using our acquisition system. We directly compare our results with [8], as it is the work which is closest to ours. In addition, we compare to [1] that produce accurate results relying on a very complex target (CNC printed)."
        },
        {
            "heading": "A. Synthetic Case",
            "text": "We conducted experiments on Gazebo simulator [9] to evaluate the accuracy and robustness of our approach, injecting different noise figures to the sensor measurements. We also experiment how the number of observations affect the final results. The setup of the scene includes a Velodyne HDL-64 LiDAR, a BlackFly-S RGB sensor and a 6 \u00d7 8 checkerboard target with corner size of 0.2 meters. We randomly generate and acquire 53 valid2 measurements.\nTo quantify the impact of the number of measurements on the accuracy of our approach, we run the calibration procedure with an increasing number of measurements ws = [3 . . . 39] and at three different LiDAR noise levels \u03c3l (0 mm,\n2A valid measurement is one for which both LiDAR and RGB sensor are able to detect the target\n7 mm and 14 mm). For every ws, we sample 40 sets of measurements.\nFrom Tab. I, we observe a steady decrease of error for every noise level, reaching an average of 2.6mm translation error in the intermediate noise case. In case of 3 measurements the high uncertainty is due to the potentially poorly conditioned system when using planes that have similar normals. Nonetheless, we compare our best result with 3 measurements against the best results of the methods presented in [1] and [8]. Tab. II shows the results."
        },
        {
            "heading": "B. Real Case",
            "text": "In this section, we describe the experiments conducted on real measurements. We perform a quantitative test on our acquisition system shown in Fig. 2 that is equipped with a Ouster OS0-128 LiDAR with a resolution of 128 \u00d7 1024, a RealSense T-265 stereo camera and two MantaG145 RGB cameras arranged in a wide horizontal stereo configuration.\nSince no groundtruth information is available, we take advantage of the stereo extrinsics to provide an estimate of the calibration error. The offset between multiple camera is measured using optical calibration procedures which typically reach subpixel precision.\nIn the first experiment, we consider the LiDAR and the Realsense T-265 sensor which provides factory calibrated intrinsic/extrinsic parameters for both cameras. The task of the experiment is to demonstrate the accuracy of the calibrator in real case scenarios and to understand how the number of measurements considered affects the quality of the solution.\nAs for the synthetic case, we first acquire a set of 17 cloud-image LiDAR RGB measurements for both cameras. Moreover we perform 40 calibrations with ws randomly selected measurements with ws \u2208 {3, 15}. Finally, for every ws, we combine the computed extrinsics for both cameras to obtain an estimate stereo transform. Assuming approximately symmetrical errors in the two cameras, Fig. 3 shows the results of this experiment. We were able to obtain at best\nan average error of 7.1 mm in translation and 0.01 rads in orientation.\nThe second experiment is conducted using the wide stereo setup for which we also calibrate the intrinsics and extrinsics of the cameras, providing expected results in a typical scenario. The acquisition procedure is the same as in the first experiment and Fig. 4 shows the experimental result, where we obtain the best solution with 4.6 mm and 0.002 rads in orientation.\nMoreover, Fig. 1 and Fig. 5 show the reprojection onto the right camera respectively of the fisheye and wide baseline RGB sensor. In the latter, the large parallax between the sensors leads to strong occlusions effects, that have been\nmitigated with an hidden point removal algorithm [7]. In summary, our evaluation indicates that our method is capable of generating extrinsic estimates that are comparable or superior to those obtained using other state-of-the-art approaches. It is important to note that careful consideration is required when selecting a minimal number of measurements. However, our experiments clearly demonstrate that the accuracy of these estimates improves as the number of measurements increases."
        },
        {
            "heading": "V. CONCLUSION",
            "text": "In summary, our paper introduces a simple and effective method for accurately estimating extrinsic parameters between LiDARs and RGB sensors. By leveraging the inherent planarity of standard calibration patterns, we establish common observations between these sensors, greatly simplifying the calibration procedure. Our experiments show that planar features mitigate the LiDAR noise, leading to accurate results even with common A3/A4 calibration patterns. Finally, we also release an open source C++ implementation to benefit the community."
        }
    ],
    "title": "CaLib: Simple and Accurate LiDAR-RGB Calibration using Small Common Markers",
    "year": 2023
}