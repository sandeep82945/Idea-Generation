{
    "abstractText": "Simultaneous Speech Translation (SimulST) is a task focused on ensuring high-quality translation of speech in low-latency situations. Despite this, the modality gap (e.g., unknown word boundaries) between audio and text presents a challenge. This gap hinders the effective application of policies from simultaneous text translation (SimulMT) and compromises the performance of offline speech translation. To address this issue, we first leverage the Montreal Forced Aligner (MFA) and utilize audio transcription pairs in pre-training the acoustic encoder, and introduce a token-level crossmodal alignment that allows the wait-k policy from SimulMT to better adapt to SimulST. This token-level boundary alignment simplifies the decision-making process for predicting read/write actions, as if the decoder were directly processing text tokens. Subsequently, to optimize the SimulST task, we propose a robust and random wait-k-tokens strategy. This strategy allows a single model to meet various latency requirements and minimizes error accumulation of boundary alignment during inference. Our experiments on the MuST-C dataset show that our method achieves a better tradeoff between translation quality and latency.",
    "authors": [
        {
            "affiliations": [],
            "name": "Linlin Zhang"
        },
        {
            "affiliations": [],
            "name": "Kai Fan"
        },
        {
            "affiliations": [],
            "name": "Jiajun Bu"
        },
        {
            "affiliations": [],
            "name": "Zhongqiang Huang"
        }
    ],
    "id": "SP:43cb3bb5c4be9287b71b0ec97e7af19a0ea43ca8",
    "references": [
        {
            "authors": [
                "Ashkan Alinejad",
                "Anoop Sarkar."
            ],
            "title": "Effectively pretraining a speech translation decoder with machine translation data",
            "venue": "Proceedings of the 2020",
            "year": 2020
        },
        {
            "authors": [
                "Niehues",
                "Xing Niu",
                "John Ortega",
                "Juan Pino",
                "Elizabeth Salesky",
                "Jiatong Shi",
                "Matthias Sperber",
                "Sebastian St\u00fcker",
                "Katsuhito Sudoh",
                "Marco Turchi",
                "Yogesh Virkar",
                "Alexander Waibel",
                "Changhan Wang",
                "Shinji Watanabe"
            ],
            "title": "Findings of the IWSLT",
            "year": 2022
        },
        {
            "authors": [
                "Naveen Arivazhagan",
                "Colin Cherry",
                "Wolfgang Macherey",
                "Chung-Cheng Chiu",
                "Semih Yavuz",
                "Ruoming Pang",
                "Wei Li",
                "Colin Raffel."
            ],
            "title": "Monotonic infinite lookback attention for simultaneous machine translation",
            "venue": "Proceedings of the 57th Annual",
            "year": 2019
        },
        {
            "authors": [
                "Alexei Baevski",
                "Yuhao Zhou",
                "Abdelrahman Mohamed",
                "Michael Auli"
            ],
            "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations. Advances in neural information processing systems, 33:12449\u201312460",
            "year": 2020
        },
        {
            "authors": [
                "Parnia Bahar",
                "Tobias Bieschke",
                "Hermann Ney."
            ],
            "title": "A comparative study on end-to-end speech to text translation",
            "venue": "2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 792\u2013799. IEEE.",
            "year": 2019
        },
        {
            "authors": [
                "Sameer Bansal",
                "Herman Kamper",
                "Karen Livescu",
                "Adam Lopez",
                "Sharon Goldwater."
            ],
            "title": "Pre-training on high-resource speech recognition improves lowresource speech-to-text translation",
            "venue": "Proceedings of the 2019 Conference of the North American Chap-",
            "year": 2019
        },
        {
            "authors": [
                "Samy Bengio",
                "Oriol Vinyals",
                "Navdeep Jaitly",
                "Noam Shazeer."
            ],
            "title": "Scheduled sampling for sequence prediction with recurrent neural networks",
            "venue": "Advances in neural information processing systems, 28.",
            "year": 2015
        },
        {
            "authors": [
                "Marco Turchi"
            ],
            "title": "Cascade versus direct speech translation: Do the differences still make a difference",
            "venue": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natu-",
            "year": 2021
        },
        {
            "authors": [
                "Alexandre Berard",
                "Laurent Besacier",
                "Ali Can Kocabiyikoglu",
                "Olivier Pietquin."
            ],
            "title": "End-to-end automatic speech translation of audiobooks",
            "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2018, Calgary, AB,",
            "year": 2018
        },
        {
            "authors": [
                "Alexandre Berard",
                "Olivier Pietquin",
                "Christophe Servan",
                "Laurent Besacier."
            ],
            "title": "Listen and translate: A proof of concept for end-to-end speech-to-text translation",
            "venue": "CoRR, abs/1612.01744.",
            "year": 2016
        },
        {
            "authors": [
                "Guandan Chen",
                "Kai Fan",
                "Kaibo Zhang",
                "Boxing Chen",
                "Zhongqiang Huang."
            ],
            "title": "Manifold adversarial augmentation for neural machine translation",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3184\u20133189.",
            "year": 2021
        },
        {
            "authors": [
                "Fahim Dalvi",
                "Nadir Durrani",
                "Hassan Sajjad",
                "Stephan Vogel."
            ],
            "title": "Incremental decoding and training methods for simultaneous translation in neural machine translation",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the As-",
            "year": 2018
        },
        {
            "authors": [
                "Linhao Dong",
                "Bo Xu."
            ],
            "title": "Cif: Continuous integrate-and-fire for end-to-end speech recognition",
            "venue": "ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 6079\u20136083.",
            "year": 2020
        },
        {
            "authors": [
                "Qian Dong",
                "Yaoming Zhu",
                "Mingxuan Wang",
                "Lei Li."
            ],
            "title": "Learning when to translate for streaming speech",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 680\u2013694, Dublin,",
            "year": 2022
        },
        {
            "authors": [
                "Qian Dong",
                "Yaoming Zhu",
                "Mingxuan Wang",
                "Lei Li."
            ],
            "title": "Learning when to translate for streaming speech",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland,",
            "year": 2022
        },
        {
            "authors": [
                "Qianqian Dong",
                "Mingxuan Wang",
                "Hao Zhou",
                "Shuang Xu",
                "Bo Xu",
                "Lei Li."
            ],
            "title": "Consecutive decoding for speech-to-text translation",
            "venue": "Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications",
            "year": 2021
        },
        {
            "authors": [
                "Maha Elbayad",
                "Laurent Besacier",
                "Jakob Verbeek."
            ],
            "title": "Efficient Wait-k Models for Simultaneous Machine Translation",
            "venue": "Proc. Interspeech 2020, pages 1461\u20131465.",
            "year": 2020
        },
        {
            "authors": [
                "Qingkai Fang",
                "Rong Ye",
                "Lei Li",
                "Yang Feng",
                "Mingxuan Wang."
            ],
            "title": "STEMM: Self-learning with speech-text manifold mixup for speech translation",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2022
        },
        {
            "authors": [
                "Qingkai Fang",
                "Rong Ye",
                "Lei Li",
                "Yang Feng",
                "Mingxuan Wang."
            ],
            "title": "STEMM: self-learning with speech-text manifold mixup for speech translation",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2022
        },
        {
            "authors": [
                "Biao Fu",
                "Kai Fan",
                "Minpeng Liao",
                "Zhongqiang Huang",
                "Boxing Chen",
                "Yidong Chen",
                "Xiaodong Shi"
            ],
            "title": "Adapting offline speech translation models for streaming with future-aware distillation and inference",
            "year": 2023
        },
        {
            "authors": [
                "Marco Gaido",
                "Mattia Antonino Di Gangi",
                "Matteo Negri",
                "Marco Turchi."
            ],
            "title": "End-toend speech-translation with knowledge distillation: Fbk@iwslt2020",
            "venue": "Proceedings of the 17th International Conference on Spoken Language Translation,",
            "year": 2020
        },
        {
            "authors": [
                "Mattia Antonino Di Gangi",
                "Roldano Cattoni",
                "Luisa Bentivogli",
                "Matteo Negri",
                "Marco Turchi."
            ],
            "title": "Mustc: a multilingual speech translation corpus",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational",
            "year": 2019
        },
        {
            "authors": [
                "Jiatao Gu",
                "Graham Neubig",
                "Kyunghyun Cho",
                "Victor O.K. Li."
            ],
            "title": "Learning to translate in real-time with neural machine translation",
            "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume",
            "year": 2017
        },
        {
            "authors": [
                "Chi Han",
                "Mingxuan Wang",
                "Heng Ji",
                "Lei Li."
            ],
            "title": "Learning shared semantic space for speech-to-text translation",
            "venue": "Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021, volume ACL/IJCNLP 2021",
            "year": 2021
        },
        {
            "authors": [
                "Sathish Reddy Indurthi",
                "Houjeung Han",
                "Nikhil Kumar Lakumarapu",
                "Beomseok Lee",
                "Insoo Chung",
                "Sangha Kim",
                "Chanwoo Kim."
            ],
            "title": "End-end speech-totext translation with modality agnostic meta-learning",
            "venue": "2020 IEEE International Conference on Acous-",
            "year": 2020
        },
        {
            "authors": [
                "Sathish Reddy Indurthi",
                "Mohd Abbas Zaidi",
                "Beomseok Lee",
                "Nikhil Kumar Lakumarapu",
                "Sangha Kim."
            ],
            "title": "Language model augmented monotonic attention for simultaneous translation",
            "venue": "Proceedings of the 2022 Conference of the North American Chap-",
            "year": 2022
        },
        {
            "authors": [
                "Takatomo Kano",
                "Sakriani Sakti",
                "Satoshi Nakamura."
            ],
            "title": "Structured-based curriculum learning for endto-end english-japanese speech translation",
            "venue": "CoRR, abs/1802.06003.",
            "year": 2018
        },
        {
            "authors": [
                "Taku Kudo."
            ],
            "title": "Subword regularization: Improving neural network translation models with multiple subword candidates",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 66\u201375,",
            "year": 2018
        },
        {
            "authors": [
                "Hang Le",
                "Juan Pino",
                "Changhan Wang",
                "Jiatao Gu",
                "Didier Schwab",
                "Laurent Besacier."
            ],
            "title": "Dual-decoder transformer for joint automatic speech recognition and multilingual speech translation",
            "venue": "COLING 2020 (long paper).",
            "year": 2020
        },
        {
            "authors": [
                "Dan Liu",
                "Mengge Du",
                "Xiaoxi Li",
                "Ya Li",
                "Enhong Chen."
            ],
            "title": "Cross attention augmented transducer networks for simultaneous translation",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 39\u201355, Online",
            "year": 2021
        },
        {
            "authors": [
                "Yuchen Liu",
                "Hao Xiong",
                "Jiajun Zhang",
                "Zhongjun He",
                "Hua Wu",
                "Haifeng Wang",
                "Chengqing Zong."
            ],
            "title": "End-to-end speech translation with knowledge distillation",
            "venue": "Interspeech 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Yuchen Liu",
                "Jiajun Zhang",
                "Hao Xiong",
                "Long Zhou",
                "Zhongjun He",
                "Hua Wu",
                "Haifeng Wang",
                "Chengqing Zong."
            ],
            "title": "Synchronous speech recognition and speech-to-text translation with interactive decoding",
            "venue": "The Thirty-Fourth AAAI Conference on",
            "year": 2020
        },
        {
            "authors": [
                "Zhongjun He",
                "Hairong Liu",
                "Xing Li",
                "Hua Wu",
                "Haifeng Wang."
            ],
            "title": "STACL: Simultaneous translation with implicit anticipation and controllable latency using prefix-to-prefix framework",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Mingbo Ma",
                "Liang Huang",
                "Hao Xiong",
                "Renjie Zheng",
                "Kaibo Liu",
                "Baigong Zheng",
                "Chuanqiang Zhang",
                "Zhongjun He",
                "Hairong Liu",
                "Xing Li",
                "Hua Wu",
                "Haifeng Wang"
            ],
            "title": "STACL: simultaneous translation with implicit anticipation and controllable",
            "year": 2019
        },
        {
            "authors": [
                "Xutai Ma",
                "Juan Miguel Pino",
                "James Cross",
                "Liezl Puzon",
                "Jiatao Gu."
            ],
            "title": "Monotonic multihead attention",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Xutai Ma",
                "Juan Miguel Pino",
                "Philipp Koehn."
            ],
            "title": "Simulmt to simulst: Adapting simultaneous text translation to end-to-end simultaneous speech translation",
            "venue": "Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Com-",
            "year": 2020
        },
        {
            "authors": [
                "Xutai Ma",
                "Yongqiang Wang",
                "Mohammad Javad Dousti",
                "Philipp Koehn",
                "Juan Pino."
            ],
            "title": "Streaming simultaneous speech translation with augmented memory transformer",
            "venue": "ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal",
            "year": 2021
        },
        {
            "authors": [
                "Michael McAuliffe",
                "Michaela Socolof",
                "Sarah Mihuc",
                "Michael Wagner",
                "Morgan Sonderegger."
            ],
            "title": "Montreal forced aligner: Trainable text-speech alignment using kaldi",
            "venue": "Interspeech, volume 2017, pages 498\u2013502.",
            "year": 2017
        },
        {
            "authors": [
                "Ha Nguyen",
                "Yannick Est\u00e8ve",
                "Laurent Besacier."
            ],
            "title": "An empirical study of end-to-end simultaneous speech translation decoding strategies",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2021, Toronto, ON, Canada,",
            "year": 2021
        },
        {
            "authors": [
                "Yusuke Oda",
                "Graham Neubig",
                "Sakriani Sakti",
                "Tomoki Toda",
                "Satoshi Nakamura."
            ],
            "title": "Optimizing segmentation strategies for simultaneous speech translation",
            "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL",
            "year": 2014
        },
        {
            "authors": [
                "Marc\u2019Aurelio Ranzato",
                "Sumit Chopra",
                "Michael Auli",
                "Wojciech Zaremba"
            ],
            "title": "Sequence level training with recurrent neural networks",
            "venue": "In 4th International Conference on Learning Representations,",
            "year": 2016
        },
        {
            "authors": [
                "Yi Ren",
                "Jinglin Liu",
                "Xu Tan",
                "Chen Zhang",
                "Tao Qin",
                "Zhou Zhao",
                "Tie-Yan Liu."
            ],
            "title": "SimulSpeech: End-to-end simultaneous speech to text translation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3787\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Yuchun Shu",
                "Haoneng Luo",
                "Shiliang Zhang",
                "Longbiao Wang",
                "Jianwu Dang."
            ],
            "title": "A cif-based speech segmentation method for streaming e2e asr",
            "venue": "IEEE Signal Processing Letters, 30:344\u2013348.",
            "year": 2023
        },
        {
            "authors": [
                "Yun Tang",
                "Hongyu Gong",
                "Ning Dong",
                "Changhan Wang",
                "Wei-Ning Hsu",
                "Jiatao Gu",
                "Alexei Baevski",
                "Xian Li",
                "Abdelrahman Mohamed",
                "Michael Auli",
                "Juan Pino."
            ],
            "title": "Unified speech-text pre-training for speech translation and recognition",
            "venue": "Proceedings",
            "year": 2022
        },
        {
            "authors": [
                "Laura Cross Vila",
                "Carlos Escolano",
                "Jos\u00e9 AR Fonollosa",
                "Marta R Costa-Jussa."
            ],
            "title": "End-to-end speech translation with the transformer",
            "venue": "IberSPEECH, pages 60\u201363.",
            "year": 2018
        },
        {
            "authors": [
                "Chengyi Wang",
                "Yu Wu",
                "Shujie Liu",
                "Ming Zhou",
                "Zhenglu Yang."
            ],
            "title": "Curriculum pre-training for end-to-end speech translation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10,",
            "year": 2020
        },
        {
            "authors": [
                "Shushu Wang",
                "Jing Wu",
                "Kai Fan",
                "Wei Luo",
                "Jun Xiao",
                "Zhongqiang Huang."
            ],
            "title": "Better simultaneous translation with monotonic knowledge distillation",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2023
        },
        {
            "authors": [
                "Ron J. Weiss",
                "Jan Chorowski",
                "Navdeep Jaitly",
                "Yonghui Wu",
                "Zhifeng Chen."
            ],
            "title": "Sequence-to-sequence models can directly translate foreign speech",
            "venue": "Interspeech 2017, 18th Annual Conference of the International Speech Communication Association, Stock-",
            "year": 2017
        },
        {
            "authors": [
                "Jichen Yang",
                "Kai Fan",
                "Minpeng Liao",
                "Boxing Chen",
                "Zhongqiang Huang"
            ],
            "title": "Bridging the gap between cascade and end-to-end cross-modal translation models: A zero-shot approach",
            "year": 2023
        },
        {
            "authors": [
                "Rong Ye",
                "Mingxuan Wang",
                "Lei Li."
            ],
            "title": "End-toend speech translation via cross-modal progressive training",
            "venue": "Interspeech 2021, 22nd Annual Conference of the International Speech Communication 7825",
            "year": 2021
        },
        {
            "authors": [
                "Rong Ye",
                "Mingxuan Wang",
                "Lei Li."
            ],
            "title": "Crossmodal contrastive learning for speech translation",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2022
        },
        {
            "authors": [
                "Xingshan Zeng",
                "Liangyou Li",
                "Qun Liu."
            ],
            "title": "Realtrans: End-to-end simultaneous speech translation with convolutional weighted-shrinking transformer",
            "venue": "Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August",
            "year": 2021
        },
        {
            "authors": [
                "Linlin Zhang",
                "Kai Fan",
                "Boxing Chen",
                "Luo Si."
            ],
            "title": "A simple concatenation can effectively improve speech translation",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 1793\u2013",
            "year": 2023
        },
        {
            "authors": [
                "Ruiqing Zhang",
                "Zhongjun He",
                "Hua Wu",
                "Haifeng Wang."
            ],
            "title": "Learning adaptive segmentation policy for end-to-end simultaneous translation",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-",
            "year": 2022
        },
        {
            "authors": [
                "Ruiqing Zhang",
                "Chuanqiang Zhang",
                "Zhongjun He",
                "Hua Wu",
                "Haifeng Wang."
            ],
            "title": "Learning adaptive segmentation policy for simultaneous translation",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Shaolei Zhang",
                "Yang Feng."
            ],
            "title": "Informationtransport-based policy for simultaneous translation",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 992\u2013 1013, Abu Dhabi, United Arab Emirates. Association",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 7814\u20137831 December 6-10, 2023 \u00a92023 Association for Computational Linguistics"
        },
        {
            "heading": "1 Introduction",
            "text": "Simultaneous Speech Translation (SimulST) is a task designed to generate real-time translations by incrementally consuming audio frames. A common practice is to cascade a streaming Automatic Speech Recognition (ASR) system and a Simultaneous Text Machine Translation (SimulMT) model (Oda et al., 2014; Dalvi et al., 2018). The latter has been significantly improved earlier, as evidenced by (Gu et al., 2017). Then, the prefix-to-prefix (P2P) framework (such as the wait-k policy) (Ma et al., 2019a; Le et al., 2020) has been developed to\n\u2217 Equal contribution. Work was done during Linlin Zhang\u2019s internship at DAMO Academy, Alibaba Group.\n\u2020 Corresponding author.\nMFA\n# word 1 1 1 1 1 0 1 There was no motorcade back there.\n# subword There was no motor back there.cade 1 1 1 2 1 0 1\nreduce the discrepancy between training and inference. An efficient wait-k method was proposed by (Elbayad et al., 2020) to train multiple paths by randomly sampling k values, instead of training multiple models w.r.t. different k values. The progress in SimulMT has greatly influenced advancements in SimulST. In particular, transformer-based endto-end neural architectures have achieved performance levels comparable to cascade systems in both offline and simultaneous ST tasks (Vila et al., 2018; Bentivogli et al., 2021; Fang et al., 2022a; Ren et al., 2020; Liu et al., 2021; Anastasopoulos et al., 2022).\nTypically, an end-to-end SimulST model requires a high-quality speech translation model that can take as input partial audio and a decision policy for controlling read/write actions. One simple but effective approach is to borrow ideas from SimulMT. However, applying SimulMT strategies directly to SimulST poses a challenge because speech signals are continuous, while text tokens are discrete and have inherent meanings. Therefore, the strategies of SimulMT, which defines input granularity at the token level, cannot be directly applied to SimulST. Integration of a segmentation\n7814\nmodule into SimulST has become a common practice, as it serves as a prerequisite for resembling SimulMT policies. Notable examples include the connectionist temporal classification (CTC)-based approach (Ren et al., 2020; Yang et al., 2023) and the fixed-sized chunk method (Ma et al., 2021). However, CTC was not designed for simultaneous scenarios and the fixed-sized chunk method lacks flexibility in representing semantic meanings. The Continuous Integrate-and-Fire (CIF) method (Dong and Xu, 2020), which is ideally suited for streaming input, has recently attracted more attention in the streaming speech-to-text area (Dong et al., 2022a; Shu et al., 2023; Fu et al., 2023). Our work builds upon the research line of the CIF module.\nWe propose a fine-grained CIF module as the traditional CIF lacks clear and interpretable training supervision, as shown in Figure 1. We use the Montreal Forced Aligner (MFA) (McAuliffe et al., 2017) to acquire frame-word or -phoneme alignments, and use them to pre-train an acoustic encoder with a fine-grained CIF loss. By aligning speech and transcription, we can readily adapt the read and write policy of SimulMT to SimulST.\nAn issue similar to the exposure bias (Ranzato et al., 2016), whereby the prediction error of segmentation may cause a gap between training and inference, is rarely discussed in previous works. We propose a robust-alignment training strategy to minimize this gap. Moreover, previous SimulST works based on Wav2Vec2.0 (Baevski et al., 2020) often overlook its bidirectional attention, which isn\u2019t inherently a streaming fashion. We discuss this by suggesting a random wait-k policy, making our model compatible with a bidirectional encoder. Our robust strategy solely involves frame-transcription alignment and is independent of the read/write policy, thus making it feasible to combine with other adaptive policies. Our main contributions can be summarized as follows.\n(1) We designed a cross-modal pre-training method for the acoustic encoder, based on our fine-grained CIF module. This approach allows for the segmentation and alignment of speech with text representation in both offline and online scenarios.\n(2) We enhanced the robustness and introduced more randomness to the wait-k policy for real-time speech translation. Our robust-alignment training strategy can effectively minimize the gap between training and inference.\n(3) We carried out experiments across multiple language directions on the MuST-C dataset. Our simultaneous translation model achieved a better trade-off between translation quality and various latency requirements with a single model, in both restricted and unrestricted scenarios."
        },
        {
            "heading": "2 Related Works",
            "text": "Speech translation can be roughly classified into offline and simultaneous scenarios, with our discussion primarily focusing on end-to-end models.\nOffline Speech Translation generates each target token based on the full audio representation. Following the advent of the first end-to-end neural network model for ST (Berard et al., 2016), a significant portion of end-to-end ST works emphasized offline scenarios. Weiss et al. (2017); Berard et al. (2018); Bansal et al. (2019); Alinejad and Sarkar (2020); Dong et al. (2021) demonstrated the effectiveness of pre-training in improving performance. Consequently, this has become the standard paradigm for current end-to-end models. Other prevalent research areas include multitask learning (Bahar et al., 2019; Liu et al., 2020; Indurthi et al., 2020; Han et al., 2021; Ye et al., 2021; Zhang et al., 2023), knowledge distillation (Liu et al., 2019; Gaido et al., 2020; Wang et al., 2023), and curriculum learning (Kano et al., 2018; Wang et al., 2020).\nSimultaneous Speech Translation was first realized through a pipelined system featuring streaming ASR and a SimulMT model. ASR played a natural role in segmenting speech frames and aligning with transcriptions, albeit at the cost of increased latency. Recently, end-to-end SimulST has attracted more research attention. Thus, a SimulST model requires a policy that determines whether to wait for more speech frames or generate new target tokens. Previous works often used a fixed size speech chunk (Ma et al., 2020b; Nguyen et al., 2021; Ma et al., 2021; Liu et al., 2021).\nAdaptive size policy has been thoroughly explored in SimulMT (Arivazhagan et al., 2019; Ma et al., 2020a; Zhang et al., 2020). As to SimulST, the works of (Dong et al., 2022a; Zhang et al., 2022) are most relevant to our research. The former first applied CIF to SimulST, and the latter determined the read/write policy by learning to segment audio frames into meaningful units. Our model achieves cross-modal token-level alignment through pre-training a novel fine-grained CIF mod-\nule, thereby facilitating downstream SimulST. Considering the frame length of each segmentation, our model also triggers an adaptive policy."
        },
        {
            "heading": "3 Main Method",
            "text": "End-to-end simultaneous speech translation models typically employ a sequence-to-sequence learning framework built on an encoder-decoder architecture. For a standard ST training corpus, D = {(s,x,y)} represents a triplet dataset that includes audio, transcription, and translation sequences. The SimulST model is generally defined as a probabilistic model:\np(y|s; \u03b8) = \u220f\nj\np(yj |s\u2264g(j),y<j ; \u03b8), (1)\nwhere \u03b8 is the model parameters, and g(j) is a monotonically non-decreasing function that indicates the ending timestamp of the audio required to generate the j-th target token.\nIn our work, the aim of the pre-training stage is to embed audio features into the text representation space (i.e., source embedding space) and to identify the candidate ending timestamp for the function g(j). The fine-tuning stage directly optimizes the SimulST model as per Eq. (1). To achieve these objectives, we initially utilize the transcription-translation pairs in D to pre-train an offline machine translation model (NMT). In practice, additional MT corpus is more readily available, allowing us to pre-train the NMT using even larger data sets. The overall model architecture, comprising both stages, is displayed in Figure 2."
        },
        {
            "heading": "3.1 Semantic Alignment Pre-Training",
            "text": "In this stage, the primary objective is to achieve cross-modal semantic alignment between speech and text. We utilize only the audio-transcription pairs (s,x) to train the parameters of the acoustic encoder, while temporarily discarding the NMT encoder and decoder."
        },
        {
            "heading": "3.1.1 Fine-grained CIF Supervision",
            "text": "Similar to recent SimulST works with adaptive size policy (Dong et al., 2022a; Zhang et al., 2022), we adopt Wav2Vec2.0 (Baevski et al., 2020) as the raw speech feature extractor. For an input audio s, we denote the output features of Wav2Vec2.0 as acoustic tokens a = (a1,a2, ...,aT ).\nWe also utilize the Montreal Forced Aligner (MFA) to achieve frame-word alignment, as shown\nin Figure 1. Because Wav2Vec2.0 involves a 320x subsampling, the alignment between acoustic tokens a and the word-level transcription is also available. We can express the acoustic segment sequence as,\n(a1,a2, ...,aT ) = (ab1:e1 , ...,abW :eW ). (2)\nwhere W is the number of words in x, and b\u03c9, e\u03c9 indicate the beginning and ending indices that align to the \u03c9-th word. Note that x is usually represented as a subword sequence (x1, ..., xL) according to the NMT tokenizer, so L \u0338= W in general. For our running example, b4, e4 are aligned to the word \u201cmotorcade\u201d, whereas \u201cmotorcade\u201d in x is represented as subwords x4 \u201cmotor\u201d and x5 \u201ccade\u201d. Nevertheless, we can easily determine how many subwords each segment corresponds to and denote it as (l1, ..., lW ), satisfying L = \u2211W \u03c9=1 l\u03c9. This quantity will serve as the fine-grained supervision for our proposed CIF.\nFollowing the traditional CIF, we obtain the CIF weights \u03b1\u0302 = (\u03b1\u03021, ..., \u03b1\u0302T ) by inputting the acoustic tokens into a linear layer with sigmoid activation. Contrasting with the conventional loss in CIF, we utilize labels from forced alignment and propose a novel fine-grained CIF loss, defined as follows.\nLcif = W\u2211\n\u03c9=1\n\u2223\u2223\u2223\u2223\u2223\u2223 e\u03c9\u2211\nt=b\u03c9\n\u03b1\u0302t \u2212 l\u03c9 \u2223\u2223\u2223\u2223\u2223\u2223 + \u2223\u2223\u2223\u2223\u2223 T\u2211\nt=1\n\u03b1\u0302t \u2212 L \u2223\u2223\u2223\u2223\u2223 (3)\nThe first term is our proposed fine-grained subwordlevel boundary loss, where the learning signal corresponds to the number of subwords within a given timestamp interval. The second term is the original CIF loss that learns the subword boundary under a weak signal \u2013 the total number of subwords."
        },
        {
            "heading": "3.1.2 Semantic Alignment Supervision",
            "text": "During early training that Eq. (3) does not converge, the \u03b1\u0302 is ill-defined for boundary inference, typically necessitating length normalization.\n\u03b1 = L \u00b7 \u03b1\u0302\u2211 t \u03b1\u0302t\n(4)\nWith this update, \u03b1 fulfills the condition \u2211\nt \u03b1t = L. During inference, normalization is not required, as the value of L is unknown. However, it is theoretically reasonable, as the loss Lcif in Eq. (3) converges to 0, \u2211 t \u03b1\u0302t will also converge to the underlying L."
        },
        {
            "heading": "3 x Transformer Layers",
            "text": "Once we have the normalized weights, we process the acoustic tokens using a threshold-based weighted averaging from left to right, suitable for the simultaneous scenario. An intuitive example featuring \u03b11,2,3 is depicted in the left panel of Figure. 2 (for more details, refer to Appendix A.1). During training, the averaged acoustic tokens should possess the same subword length as the transcription x, denoted as c = (c1, ..., cL).\nNext, we incorporate several Transformer layers for semantic alignment learning. In contrast to the traditional multi-head attention (MHA) of the encoder, our approach is designed as follows.\nhi = MHA(Q = hi\u22121,K = a, V = a). (5)\nwhere h0 is initialized as c. The output length for each layer will always be the same as that of c.\nFor supervisory learning, we propose two semantic objectives through two linear layers, as shown in Figure 2. The first linear layer maps h to the source embedding space of NMT via z = Linearemb(h). The corresponding loss aims to align the acoustic features with subword embeddings. The second linear layer further maps the output z of the acoustic encoder to the discrete vocabulary, employing\nCross-Entropy (CE) loss.\nLemb = \u2225z\u2212 Emb(x)\u22252 , (6)\nLce = \u2211L\nl=1 CE (Linearce(zl), xl) . (7)\nIt is worth noting that the mixup (Chen et al., 2021; Fang et al., 2022b) between z and Emb(x) is randomly applied when calculating the CE loss. Thus, our final pre-training loss is the aggregation of Eq. (3,6,7), i.e.,\nLpt = Lcif + \u03bb \u00b7 Lemb + Lce. (8)\nwhere a tunable hyper-parameter \u03bb is multiplied to balance the scale of mean squared loss Lemb. Note that the embedding matrix is frozen, and the linear layer for CE will be discarded for ST inference.\n3.2 Robust and Random Wait-k-tokens After the previous stage, both the acoustic encoder and the NMT model are well pre-trained. The output of the acoustic encoder is expected to align with the source embedding space in terms of both semantic meaning and sequence length. During the fine-tuning stage, we will use the full ST data, denoted as D, to train the end-to-end SimulST model. 3.2.1 Random Wait-k Our proposal draws inspiration from the efficient wait-k strategy (Elbayad et al., 2020), originally\ndesigned for SimulMT with a unidirectional encoder. However, the raw speech feature extractor, Wav2Vec2.0, is a bidirectional encoder. Prior SimulMT studies (Dong et al., 2022a; Zhang et al., 2022) directly utilized it and overlooked the discrepancy between offline and streaming input. We propose a robust and random training strategy to address this issue.\nIn particular, we first sample a source word-level length \u03c9 from a uniform distribution U(1,W ), and sample a k \u2208 U(K1,K2) as the number of waiting timestamps, where K1 < K2. According to the MFA alignment and word-level length \u03c9, we can readily derive the subword-level source length ls \u2208 [1, L]. The target length then becomes lt = ls\u2212k+1. In our case, K1 could be a negative number as long as lt does not exceed the translation sentence length. Then, we propose the random wait-k-tokens loss.\nLstwk = \u2211\nls\u2208S log p(ylt |z\u2264ls ,y<lt), (9)\nwhere S represents a sampled set of source lengths. To improve efficiency, we use the same k for each example, but resample k across examples in a batch. Note that (i) our approach permits bidirectional attention on the partial input, as Lstwk is essentially optimizing the sampled prefix pairs in the wait-k decoding path; (ii) However, we also encounter the unresolved issue from (Dong et al., 2022a; Zhang et al., 2022): during inference, the encoder hidden states of Wav2Vec2.0 cannot be cached for streaming input. We will discuss potential solutions to this problem as future work in the limitations section.\n3.2.2 Robust Wait-k As previously mentioned in Eq. (4), unless the loss Lcif is strictly equal to 0, the inferred \u03b1\u0302 will be suboptimal, and a discrepancy will exist between L\u0302 = \u2211 \u03b1\u0302 and the actual transcription length. To address this issue, we propose a robust training strategy. Essentially, by emulating the scheduled sampling (Bengio et al., 2015) that can mitigate exposure bias, we use the predicted alignment for the loss Eq. (9) with a probability (e.g., 0.5).\nSpecifically, when sampling the source length in Lstwk, we randomly select the sampling rule from the following two methods.\n1. Sample \u03c9 \u223c U(1,W ), then derive ls; 2. Directly sample ls \u223c U(1, L\u0302).\nIf the former distribution is selected, it implies that the training can utilize the ground-truth segmen-\ntation based on the MFA, otherwise, the sampled length from the latter distribution is already at the subword-level and the training uses the predicted segmentation based on \u03b1\u0302."
        },
        {
            "heading": "3.2.3 Overall Loss",
            "text": "The overall loss of the fine-tuning stage is defined to optimize all model parameters.\nLft = Lstoff + Lstwk + Lmtoff + Lmtwk + Lpt (10) where L\u00b7off is the offline translation loss conditional on full speech or transcription, and Lmtwk is defined via p(ylt |x\u2264ls ,y<lt)."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Experimental Setting",
            "text": ""
        },
        {
            "heading": "4.1.1 Dataset",
            "text": "For a fair comparison with previous works, we conduct our experiments on the widely used MuST-C V1: English\u2192{German, French, Spanish} (En\u2192{De, Fr, Es}) (Gangi et al., 2019). For our multitask ST model with auxiliary MT data, we extract 20M En-Fr and En-Es 15M sentence pairs from the WMT14 and WMT13 data. For EnDe, we use all of the WMT14 dataset. The data statistics are shown in Table 1.\nFollowing the preprocessing recipes of STEMM (Fang et al., 2022b), we use MFA (McAuliffe et al., 2017) to process ASR data to obtain speech-text alignment information. We also filter out the raw audio larger than 450000 frames. The text vocabulary consists of 10,000 subword units learned by SentencePiece (Kudo, 2018), shared between the source and target languages."
        },
        {
            "heading": "4.1.2 Model Configuration and Evaluation",
            "text": "In all experiments, our semantic alignment module comprises 3 transformer layers. The Neural Machine Translation (NMT) model employs 6 transformer encoder and decoder layers. Both have a hidden dimension of 512 and utilize eight attention heads. We pre-train two variants of NMT models: one using the MuST-C data exclusively, and another incorporating an additional MT corpus. During SimulST training, we sample k between 3 and\n10, while for evaluation, k ranges from 2 to 25. We conduct our training using 8 V100 GPUs, with a batch size per device of 3.2M audio frames.\nModel selection is based on the corresponding development set, and we report final results on the tst-COMMON set. We utilize the detokenized case-sensitive BLEU score, calculated using sacreBLEU1. Following the convention in the SimulST, we use the average lagging (AL) evaluation model to measure the latency (Ma et al., 2019b) based on the toolkit SemiEval2."
        },
        {
            "heading": "4.2 Main Results of SimulST",
            "text": "We compared our model with recent simultaneous ST models: SimulSpeech (Ren et al., 2020), RealTranS (Zeng et al., 2021), MoSST (Dong et al., 2022b), MU-ST (Zhang et al., 2022), MMA-SLM (Indurthi et al., 2022), and ITST (Zhang and Feng, 2022). Most of these works explored the SimulST task using the MuST-C dataset alone. MU-ST also conducted experiments with external data, using both additional Automatic Speech Recognition (ASR) and Machine Translation (MT). In our case, we did not use additional ASR data.\nQuality and latency trade-off by single model In Figure 3, we plot the BLEU v.s. AL curve of a single checkpoint of our ST model for both offline and online scenarios across different latency. To achieve these results, we leverage our proposed wait-k-tokens strategy during inference, with token\n1https://github.com/mjpost/sacrebleu 2https://github.com/facebookresearch/SimulEval\nboundaries based on CIF prediction. Regarding En-De, which is the most researched language pair, our method significantly outperforms all previous approaches in terms of translation quality at high latency. In the constrained scenario (represented by dashed lines), our method scores at least 2 BLEU higher than MU-ST and MoSST, even outperforming MU-ST in the unconstrained case. In the unconstrained scenario, our method can further enhance performance. At low latency, it seems the curves of our approach and MU-ST are close. However, if we focus on the snapshot around latency 1000ms, our method surpasses MU-ST by approximately 2 BLEU. For EnFr, fewer works have explored this language pair. Compared to existing baselines, our model in a constrained scenario outperforms them by a significant margin. For instance, around low latency (1000ms), our method achieves a BLEU score exceeding 28. In the case of English to Spanish (En-Es), although our results are generally better, we found the additional MT corpus appears to be less beneficial. The exact numerical results in Figure 3 can be found in Appendix A.7.\nAnalysis of Pre-training Stage In the pretraining stage, the loss Lpt primarily optimizes semantic alignment learning in terms of both sequence length and semantic representations. We will illustrate the performance of these objectives.\nFirst, we conducted a statistical analysis comparing the number of predicted segments by CIF to the actual subword lengths of the corresponding tran-\nscriptions. Figure 4 demonstrates the En-De test set with the ground-truth length (y) and the predicted length (x) by our alignment module used to fit a linear regression. The regression line closely aligns with the diagonal of the coordinate system, i.e., y = x, indicating the high accuracy of our length prediction. Furthermore, few outliers are distant from the regression line, suggesting that our length prediction exhibits low bias and variance. These results collectively highlight the effectiveness of our speech pre-training in predicting sentence length. The analysis of length prediction for En-Fr and En-Es is provided in Appendix A.2.\nSecond, Figure 5 illustrates the L2 distance between the acoustic encoder output and the word embeddings of its corresponding transcription. The elements along the diagonal generally exhibit smaller values, indicating a closer semantic distance for cor-\nresponding acoustic and text representations. If the transcription has two same tokens (such as \u201cdo\u201d in Figure 5), we can still observe semantic similarity for their corresponding acoustic output (z3 and z5 in Figure 5). This phenomenon further verifies the effectiveness of semantic learning. Additional visualizations can be found in Appendix A.3.\nThird, we explicitly visualize the audio-text alignment in Figure 6. In the provided example, we can observe there is a strong correlation between the \u03b1 values in the second row and the amplitude values of the speech in the first row. The segmentation activations in the third row are also highly correlated with the corresponding positions of the subwords. For meaningless blank characters without a corresponding meaningful subword, the predicted \u03b1 is almost 0, and no subword is generated. Additional examples can be found in Appendix A.4."
        },
        {
            "heading": "4.3 Main Results of Offline ST",
            "text": "As the bidirectional architecture of Wav2Vec2.0, our ST model is naturally compatible with offline speech translation. Specifically, we compare our model, previously used for SimulST in Figure 3, with recently published offline ST methods: XSNET, STEMM, ConST, and STPT. We also include two earlier SimulST works (RealTranS and MoSST) that evaluate both offline and online tasks.\nThe overall results are summarized in Table 2. In data-constrained scenarios, our offline ST model\u2019s BLEU evaluation matches the performance of these competitive offline models. In unconstrained scenarios, only the performance of En-Es exhibits a\nsignificantly lower BLEU score. We hypothesize the main reason for this is the considerable variation in the volume of external MT data across different models. Our selected 15M En-Es MT corpus may also have a potential domain mismatch with MuST-C. Generally, these results demonstrate that our single checkpoint can maintain robust offline translation quality while also adapting to real-time translation requirements."
        },
        {
            "heading": "4.4 Ablation Studies",
            "text": "We conducted ablation experiments on the unconstrained En-De dataset. The different settings and critical offline evaluations of our control experiments are listed in Table 3. The corresponding BLEU v.s. AL curves are shown in Figure 7.\nRegarding the offline task, the R2 wait-k-tokens training did not yield any notable difference when comparing model (a) and (b), however, in the SimulST task, the absence of this strategy changed the curve from an arc shape to an almost straight line. When the fine-grained CIF loss was removed (model (c)), the offline performance decreased by about 0.5 BLEU, while the arc curve also uniformly shifted downwards by 0.5 BLEU. Removing the entire semantic alignment pre-training (model (d)) resulted in SimulST performance at low latency that was on par with model (c), but the BLEU scores at high latency significantly decreased. In summary, R2 wait-k primarily enables streaming; the semantic alignment pre-training improves the performance at high latency; and the fine-grained CIF loss is the final touch that further enhances performance at low latency. Another two loss related ablation studies can be found in Appendix A.5. The analysis of how we design the current pre-training loss can refer to Appendix A.6."
        },
        {
            "heading": "4.5 Stable Inference at Low Latency",
            "text": "In practice, during the inference stage, the first few target tokens often have low quality when the streaming input audio is short. Various strategies have been proposed by previous works to address this problem. E.g., RealTranS employs the Wait-K-Stride-N strategy, while MU-ST uses a tailtruncation trick. In our approach, we empirically find that when the first few short speech segments are processed as streaming input, the CIF weights tend to bias towards larger values and require more\ninput frames to stabilize. Conversely, with longer speech inputs, the decoding process may end prematurely. Therefore, we propose a strategy to wait for a few extra segments before initiating the first write operation. After this, the read and write behavior returns to normal. According to our analysis in Figure 8, our \u201cwait-nmore\u201d strategy effectively improves performance at low latency, and we apply the wait-2more strategy in our experiments."
        },
        {
            "heading": "5 Conclusion",
            "text": "We have proposed a two-step training method for simultaneous speech translation. The first step involves performing a token-level cross-modal alignment between the audio and the transcription in terms of both sequence length and semantic rep-\nresentation. The second step entails end-to-end training of the ST model, leveraging a robust and random wait-k tokens policy. Remarkably, our model manages to satisfy the objectives of offline and streaming speech translation within a single checkpoint. Experimental results substantiate that our approach achieves a commendable trade-off between translation quality and latency. Given that our semantic alignment pre-training is independent of the downstream policy in simultaneous translation, we aim to explore opportunities to better accommodate an adaptive read/write policy in future work.\nLimitations\nAs discussed in Section 3, our training strategy can indeed mitigate some issues of the bidirectional Wav2Vec2.0, but it still encounters the same unresolved issue as in (Dong et al., 2022a; Zhang et al., 2022). During inference, the encoder hidden states of Wav2Vec2.0 cannot be cached for streaming input. Thus, it is not an efficient audio feature extractor in a streaming scenario since the recalculation of audio features is necessary whenever new streaming input arrives. A potential research direction could be replacing Wav2Vec2.0 with a streaming acoustic encoder. It\u2019s worth noting that Wav2Vec2.0\u2019s self-supervised learning is pre-trained by masking 49% of all time steps with a mean span length of 300ms. Therefore, finetuning the existing Wav2Vec2.0 with streaming input should be a feasible approach.\nEthics Statement\nOur work complies with the ACL Ethics Policy. Our experiments are based on the open-sourced dataset that is widely used in academia, and there is no violation of this dataset. Our writing is completely based on the authors without plagiarism."
        },
        {
            "heading": "Acknowledgements",
            "text": "We would like to thank all the anonymous reviewers for their insightful and helpful comments. This work was supported by Alibaba Group through Alibaba Research Intern Program."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Threshold-Based Weighted Averaging of CIF\nIn the section, we formulate the threshold-based weighted averaging of CIF into Algorithm 1.\nA.2 Visualization of Length Prediction for CIF\nWe conducted linear regression analysis on the predicted length of CIF and actual transcript token\nAlgorithm 1 Threshold-based Weighted Averaging in Continuous Integrate-and-Fire (CIF)\nInput: The output features of Wav2Vec2.0 as acoustic tokens a = (a1,a2, ...,aT ). The normalized CIF weights \u03b1 = (\u03b11, \u03b12, ..., \u03b1T ) satisfying \u2211T t=1 \u03b1t = L. Threshold \u03b4 = 1 be\ndefault. Output: The averaged acoustic tokens after CIF\nmodule have the same subword length as the transcription x, denoted as c = (c1, ..., cL). 1: function MAIN 2: // Initialize i = 1 and i will end with L,\ninitial accumulated weight \u03b1a0 = 0, initial accumulated state aa0 = 0;\n3: i = 1, \u03b1a0 = 0, a a 0 = 0 4: for t = 1 : T do 5: // calculate accumulated weight; 6: \u03b1at = \u03b1 a t\u22121 + \u03b1t 7: if \u03b1at < \u03b4 then 8: // no boundary is located; 9: aat = a a t\u22121 + \u03b1t \u00d7 at\n10: else 11: // a boundary is located; 12: // \u03b1t is divided into two part, the\nfirst part \u03b1t1 is used to fulfill the integration of current ci;\n13: \u03b1t1 = 1\u2212 \u03b1at\u22121 14: ci = a a t + \u03b1t1 \u00d7 at 15: i++ 16: // The other part \u03b1t2 is used for the\nnext integration; 17: \u03b1at = \u03b1t2 = \u03b1t \u2212 \u03b1t1 18: aat = \u03b1t2 \u00d7 at 19: end if 20: end for 21: 22: end function 23: return c = (c1, ..., cL)\nlength for different language pairs. From Figure 9, it can be seen that our method exhibits high prediction accuracy during both the cross-modal pretraining and ST fine-tuning phases. All models can fit the linear regression equation y = x. The data is primarily concentrated on this line. From the results, our model can maintain a high level of accuracy in predicting length whether or not finetuning is done on the ST. At the same time, there is little variation across different language pairs.\nA.3 Visualization of Cross-modal Alignment\nWe illustrate the L2 distance between the acoustic encoder output and the word embeddings of its corresponding transcription in Figure 10. From Figure 10, it can be observed that regardless of the accuracy of length prediction, all examples exhibit lighter-colored areas along the diagonal, indicating that our method aligns speech and text distances more closely in that region, while distances are greater in other areas.\nAdditionally, subwords such as \"y\" in Figure 10(b), \"to\" in Figure 10(c), \"_the\" in Figure 10(d), and \"_do\" in Figure 10(e) are repeated, with their corresponding speech output being very similar. This suggests that our method not only simply brings speech and text closer in input position order, but truly surpasses input order to align them in semantic space.\nA.4 Speech Segmentation and Alignment\nOur method, after the cross-modal align pretraining, is effective in achieving segmentation and alignment at the token-level. Figure 11 shows some\nrandomly selected examples, including complete speech sentences and some truncated sections of the speech. In general, we can observe an early activation for subwords, i.e., the accumulation sum of \u03b1 tends to reach threshold 1.0 earlier than the boundary detected by MFA. We hypothesize that the boundary from MFA is usually located in the middle of consecutive silence frames, and the corresponding \u03b1 for silence frame is almost 0, thus, the larger \u03b1 values are usually predicted earlier.\nA.5 Additional Ablation on Loss\nThe three pre-training losses (CE loss, embed loss, and fine-grained CIF loss) are by default optimized in the fine-tuning stage, and the relevant ablation study is mainly conducted on the fine-tuning stage in Table 3. In this section, we conduct several new ablation studies of the auxiliary loss to supplement Table 3. In Table 4, if the loss is not marked as\u221a\n, it means the corresponding loss is removed in both the pre-training and fine-tuning stages. Removing both the CE loss and embed loss in both the pre-training and fine-tuning stages results in\na significant drop in performance. Based on the previous ablation experiment results, we can conclude that the CE loss plays a crucial role in the pre-training stage but has minimal impact in the fine-tuning stage. In Table 5, we conduct a new ablation study to explore whether pre-training loss Lpt is still required in the fine-tuning stage. In the bottom row of Table 5, after the pre-training stage, all three losses in Lpt are removed in fine-tuning. As expected, the BLEU on the dev set significantly dropped. Theoretically, the fine-grained CIF loss in is crucial because it provides the ability of audio segmentation for the streaming input. Without CIF loss, we found the length prediction becomes extremely worse, and imposes a big gap between the training and inference. In addition, the convergence becomes disordered. The convergence is usually achieved after 10+ epochs. Surprisingly, when Lpt is removed, the highest BLEU on the dev set is observed in the first epoch, followed by a decline in subsequent epochs. Therefore, our results indicate that Lpt is indeed crucial during the fine-tuning stage. Actually, including the Lpt in fine-tuning stage becomes common in speech translation, e.g.,\noffline model STPT.\nA.6 The Ablation of Alignment Pre-training We demonstrate ablation experiments for crossmodal alignment pre-training. As shown in Table 6, given sufficient training, there is little difference in accuracy and model length between the speech recognition models. However, in order to bring\nthe word vectors of pre-trained MT closer, we ultimately chose the current combination.\nFrom the first 3 metrics (accuracy, WER, and length prediction error), we cannot distinguish the importance of mixup. However, we define a new metric to measure the distance between speech and text representations called Representation dist,\n1\nLz\nLz\u2211\ni=1\nmin j\u2208[1,Le]\n|zi \u2212 ej | (11)\nSince the length of two representations may not be the same during inference, we select the minimum pairwise distance. In this metric, we can observe the mixup training can significantly reduce the representation distance.\nA.7 Numeric Results for Figures These are the numerical results corresponding to our method\u2019s simultaneous ST, shown in Table 7"
        }
    ],
    "title": "Training Simultaneous Speech Translation with Robust and Random Wait-k-Tokens Strategy",
    "year": 2023
}