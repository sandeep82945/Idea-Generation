{
    "abstractText": "We summarize the 8th Competition on Legal Information Extraction and Entailment. In this edition, the competition included five tasks on case law and statute law. The case law component includes an information retrieval Task (Task 1), and the confirmation of an entailment relation between an existing case and an unseen case (Task 2). The statute law component includes an information retrieval Task (Task 3), an entailment/question answering task based on retrieved civil code statutes (Task 4) and an entailment/question answering task without retrieved civil code statutes (Task 5). Participation was open to any group based on any approach. Eight different teams participated in the case law competition tasks, most of them in more than one task. We received results from six teams for Task 1 (16 runs) and 6 teams for Task 2 (17 runs). On the statute law task, there were eight different teams participating, most in more than one task. Six teams submitted a total of 18 runs for Task 3, 6 teams submitted a total of 18 runs for Task 4, and 4 teams submitted a total of 12 runs for Task 5. Here we summarize the approaches, our official evaluation, and analysis on our data and submission results.",
    "authors": [],
    "id": "SP:873ed7a0a6ac2c5883c19cf1d2601ad1aaa09ee6",
    "references": [
        {
            "authors": [
                "S. Althammer",
                "A. Askari",
                "S. Verberne",
                "Hanbury",
                "A. (2021). Dossier@coliee 2021"
            ],
            "title": "Leveraging dense retrieval and summarization-based re-ranking for case law retrieval",
            "venue": "Proceedings of the COLIEE Workshop in ICAIL. (8) AP = \u2211Nret i=0 Precision@i \u2217 Rel(i) Nrel , MAP = \u2211 i = 0qAP Nq (9) Accuracy = NTP NTP + NFP , 132 The Review of Socionetwork Strategies",
            "year": 2022
        },
        {
            "authors": [
                "P. Banerjee",
                "H. Han"
            ],
            "title": "Language modeling approaches to information",
            "venue": "retrieval. JCSE,",
            "year": 2009
        },
        {
            "authors": [
                "T.B. Brown",
                "B. Mann",
                "N. Ryder",
                "M. Subbiah",
                "J. Kaplan",
                "P. Dhariwal",
                "A. Neelakantan",
                "P. Shyam",
                "G. Sastry",
                "A. Askell",
                "S. Agarwal",
                "A. Herbert-Voss",
                "G. Krueger",
                "T. Henighan",
                "R. Child",
                "A. Ramesh",
                "D.M. Ziegler",
                "J. Wu",
                "C. Winter",
                "C. Hesse",
                "M. Chen",
                "E. Sigler",
                "M. Litwin",
                "S. Gray",
                "B. Chess",
                "J. Clark",
                "C. Berner",
                "S. McCandlish",
                "A. Radford",
                "I. Sutskever",
                "D. Amodei"
            ],
            "title": "Language models are few-shot learners",
            "year": 2020
        },
        {
            "authors": [
                "K. Clark",
                "M.T. Luong",
                "Q.V. Le",
                "C.D. Manning"
            ],
            "title": "Electra: Pre-training text encoders as discriminators rather than generators",
            "venue": "CoRR. http:// arxiv. org/ abs/",
            "year": 2020
        },
        {
            "authors": [
                "J. Devlin",
                "M. Chang",
                "K. Lee",
                "K. Toutanova"
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "CoRR. http:// arxiv. org/ abs/",
            "year": 2018
        },
        {
            "authors": [
                "M. Fujita",
                "N. Kiyota",
                "Y. Kano"
            ],
            "title": "Predicate\u2019s argument resolver and entity abstraction for legal question answering: Kis teams at coliee",
            "venue": "Proceedings of the COLIEE Workshop in ICAIL",
            "year": 2021
        },
        {
            "authors": [
                "P. He",
                "X. Liu",
                "J. Gao",
                "W. Chen"
            ],
            "title": "Deberta: Decoding-enhanced BERT with disentangled attention",
            "venue": "CoRR. https:// arxiv. org/ abs/",
            "year": 2020
        },
        {
            "authors": [
                "V. Karpukhin",
                "B. Oguz",
                "S. Min",
                "L. Wu",
                "S. Edunov",
                "D. Chen",
                "W. Yih"
            ],
            "title": "Dense passage retrieval for open-domain question answering",
            "venue": "CoRR. https:// arxiv. org/ abs/",
            "year": 2020
        },
        {
            "authors": [
                "M.Y. Kim",
                "J. Rabelo",
                "R. Goebel"
            ],
            "title": "Bm25 and transformer-based legal information extraction and entailment",
            "venue": "Proceedings of the COLIEE Workshop in ICAIL",
            "year": 2021
        },
        {
            "authors": [
                "M.J. Kusner",
                "Y. Sun",
                "N.I. Kolkin",
                "K.Q. Weinberger"
            ],
            "title": "From word embeddings to document distances",
            "venue": "Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37,",
            "year": 2015
        },
        {
            "authors": [
                "J. Li",
                "X. Zhao",
                "J. Liu",
                "J. Wen",
                "M. Yang"
            ],
            "title": "Siat@coliee-2021: Combining statistics recall and semantic ranking for legal case retrieval and entailment",
            "venue": "Proceedings of the COLIEE Workshop in ICAIL",
            "year": 2021
        },
        {
            "authors": [
                "Y. Liu",
                "M. Ott",
                "N. Goyal",
                "J. Du",
                "M. Joshi",
                "D. Chen",
                "O. Levy",
                "M. Lewis",
                "L. Zettlemoyer",
                "V. Stoyanov"
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "year": 2019
        },
        {
            "authors": [
                "Y. Ma",
                "Y. Shao",
                "B. Liu",
                "Y. Liu",
                "M. Zhang",
                "S. Ma"
            ],
            "title": "Retrieving legal cases from a large-scale candidate corpus",
            "venue": "Proceedings of the 18th International conference on Artificial Intelligence and Law (ICAIL.)",
            "year": 2021
        },
        {
            "authors": [
                "H.T. Nguyen",
                "P.M. Nguyen",
                "T.H.Y. Vuong",
                "Q.M. Bui",
                "C.M. Nguyen",
                "B.T. Dang",
                "V. Tran",
                "M.L. Nguyen",
                "K. Satoh"
            ],
            "title": "Jnlp team: Deep learning approaches for legal processing tasks in coliee 2021",
            "venue": "Proceedings of the COLIEE Workshop in ICAIL",
            "year": 2021
        },
        {
            "authors": [
                "H.T. Nguyen",
                "V. Tran",
                "N.L. Minh",
                "M.P. Nguyen",
                "T.H.Y. Vuong",
                "M.Q. Bui",
                "M.C. Nguyen",
                "B. Dang",
                "K. Satoh"
            ],
            "title": "Paralaw nets - cross-lingual sentence-level pretraining for legal text processing",
            "venue": "Proceedings of the COLIEE Workshop in ICAIL",
            "year": 2021
        },
        {
            "authors": [
                "J. Rabelo",
                "M.Y. Kim",
                "R. Goebel",
                "M. Yoshioka",
                "Y. Kano",
                "K. Satoh"
            ],
            "title": "COLIEE 2020: Methods for legal document retrieval and entailment, pp",
            "venue": "https:// doi",
            "year": 2021
        },
        {
            "authors": [
                "C. Raffel",
                "N. Shazeer",
                "A. Roberts",
                "K. Lee",
                "S. Narang",
                "M. Matena",
                "Y. Zhou",
                "W. Li",
                "P.J. Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "CoRR. http:// arxiv. org/ abs/",
            "year": 2019
        },
        {
            "authors": [
                "N. Reimers",
                "I. Gurevych"
            ],
            "title": "Sentence-bert: Sentence embeddings using siamese bertnetworks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),",
            "year": 2019
        },
        {
            "authors": [
                "G.M. Rosa",
                "R.C. Rodrigues",
                "R. de Alencar Lotufo",
                "R. Nogueira"
            ],
            "title": "To tune or not to tune? zero-shot models for legal case entailment",
            "venue": "Proceedings of the 18th International conference on Artificial Intelligence and Law (ICAIL)",
            "year": 2021
        },
        {
            "authors": [
                "G.M. Rosa",
                "R.C. Rodrigues",
                "R. Lotufo",
                "R. Nogueira"
            ],
            "title": "Yes, bm25 is a strong baseline for legal case retrieval",
            "venue": "Proceedings of the 18th International conference on Artificial Intelligence and Law (ICAIL)",
            "year": 2021
        },
        {
            "authors": [
                "F. Schilder",
                "D. Chinnappa",
                "K. Madan",
                "J. Harmouche",
                "A. Vold",
                "H. Bretz",
                "J. Hudzina"
            ],
            "title": "A pentapus grapples with legal reasoning",
            "venue": "Proceedings of the COLIEE Workshop in ICAIL",
            "year": 2021
        },
        {
            "authors": [
                "H.L. Shao",
                "Y.C. Chen",
                "S.C. Huang"
            ],
            "title": "BERT-based ensemble model for the statute law retrieval and legal information entailment. In: The Proceedings of the 14th International 133 1 3 The Review of Socionetwork Strategies",
            "venue": "Workshop on Juris-Informatics",
            "year": 2020
        },
        {
            "authors": [
                "T. Strohman",
                "D. Metzler",
                "H. Turtle",
                "W.B. Croft"
            ],
            "title": "Indri: A language model-based search engine for complex queries",
            "venue": "Proceedings of the International Conference on Intelligent Analysis,",
            "year": 2005
        },
        {
            "authors": [
                "H. Trivedi",
                "H. Kwon",
                "T. Khot",
                "A. Sabharwal",
                "N. Balasubramanian"
            ],
            "title": "Repurposing entailment for multi-hop question answering tasks",
            "year": 2019
        },
        {
            "authors": [
                "S. Wehnert",
                "V. Sudhi",
                "S. Dureja",
                "L. Kutty",
                "S. Shahania",
                "E.W.D. Luca"
            ],
            "title": "Legal norm retrieval with variations of the bert model combined with tf-idf vectorization",
            "venue": "Proceedings of the 18th International conference on Artificial Intelligence and Law (ICAIL)",
            "year": 2021
        },
        {
            "authors": [
                "M. Yoshioka",
                "Y. Aoki",
                "Y. Suzuki"
            ],
            "title": "Bert-based ensemble methods with data augmentation for legal textual entailment in coliee statute law task",
            "venue": "Proceedings of the COLIEE Workshop in ICAIL",
            "year": 2021
        },
        {
            "authors": [
                "M. Yoshioka",
                "Y. Suzuki",
                "Y. Aoki"
            ],
            "title": "Bert-based ensemble methods for information retrieval and legal textual entailment in coliee statute law task. In: Proceedings of the COLIEE Workshop in ICAIL. Publisher\u2019s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations",
            "venue": "Authors and Affiliations",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Vol.:(0123456789)\n1\nKeywords COLIEE2021\u00a0\u00b7 Legal information retrieval\u00a0\u00b7 Legal information entailment"
        },
        {
            "heading": "1 Introduction",
            "text": "The objective of the Competition on Legal Information Extraction/Entailment (COLIEE) is to build a research community and establish the state of the art for information retrieval and entailment using legal texts. It is usually co-located with JURISIN, the Juris-Informatics workshop series, which was created to promote community discussion on both fundamental and practical issues on legal information\n* Juliano Rabelo rabelo@ualberta.ca; jrabelo@gmail.com\nExtended author information available on the last page of the article\n112 The Review of Socionetwork Strategies (2022) 16:111\u2013133\n1 3\nprocessing, with the intention to embrace various disciplines, including law, social sciences, information processing, logic and philosophy, including the existing conventional \u201cAI and law\u201d area. In alternate years, COLIEE is organized as a workshop with the International Conference on AI and Law (ICAIL), which was the case in 2017, 2019, and again in 2021. Until 2017, COLIEE consisted of two tasks: information retrieval (IR) and entailment using Japanese Statute Law (civil law). Since COLIEE 2018, IR and entailment tasks using Canadian case law were introduced, and the 2021 edition included a fifth task (entailment in statute law text without relying on previously retrieved data).\nTask 1 is a legal case retrieval task, and it involves reading a query case and extracting supporting cases from the provided case law corpus, hypothesized to be relevant to the query case. Task 2 is the legal case entailment Task, which involves the identification of a paragraph or paragraphs from existing cases, which are hypothesized to entail a given fragment of a new case. For the information retrieval task (Task 3), based on the discussion about the analysis of previous COLIEE IR Tasks, we modify the evaluation measure of the final results and ask participants to submit ranked relevant articles relevant to the difficulty of the questions. For the entailment task (Task 4), we performed categorized analyses to expose different issues of the problems and characteristics of the submissions, in addition to the evaluation accuracy as in previous COLIEE tasks. Task 5 is similar to Task 4, but competitors can not rely on previously retrieved statute data.\nThe rest of the paper is organized as follows: Sections\u00a02, 3, 4, 5, describe each task, presenting their definitions, datasets, list of approaches submitted by the participants, and results attained. Section\u00a06 presents some final remarks."
        },
        {
            "heading": "2 Task 1\u2014Case Law Retrieval",
            "text": ""
        },
        {
            "heading": "2.1 Task Definition",
            "text": "The Case Law Retrieval Task consists in finding which cases should be \u201cnoticed\u201d1 with respect to a given query case. More formally, given a set of cases C, a set of query cases Q, a set of the true noticed cases N, and a set of false noticed cases F, such that C = {Q \u222a N \u222a F} , the Task is to find the set of answers A = {A1 \u222a A2... \u222a An} , such that n = |Q| and each Ai \u2282 N contains all the true noticed cases and only the true noticed cases with respect to the query case qi \u2208 Q."
        },
        {
            "heading": "2.2 Dataset",
            "text": "The dataset is comprised of 4415 case law files. A labelled training set of 650 cases is provided, together with a total of 3311 true noticed cases. At first glance, the task\n1 \u201cNotice\u201d is a legal technical term that denotes a legal case description that is considered to be relevant to a query case.\n113\n1 3\nThe Review of Socionetwork Strategies (2022) 16:111\u2013133\nmay seem simple, as one could think competitors need to identify the 3311 cases among the 4415 total cases. However, the task actually requires competitors to identify the noticed cases for each given query case. On average, there are approximately five noticed cases per query case in the provided training dataset, which should be identified among the 4415 cases. To prevent merely using citations of past cases, citations are suppressed from the case contents and replaced by a \u201cFRAGMENT_ SUPPRESSED\u201d tag indicating that fragment was removed.\nA test set is given with 250 query cases and a total of 900 true noticed cases, which means there are on average 3.6 noticed cases per query case in the test dataset. In future editions, we intend to ensure that the training and test datasets have similar distributions. Initially, the golden labels for that test set is not provided to competitors."
        },
        {
            "heading": "2.3 Approaches",
            "text": "We received 15 submissions from 7 different teams for Task 1, but only 5 teams submitted papers describing their approaches. Their methods are briefly described below. Please refer to the corresponding papers for further details.\n\u2013 Li et\u00a0 al. [11] (team name: siat) propose a pipeline method based on statistical features and semantic understanding models, which enhances the retrieval method with both recall and semantic ranking. siat\u2019s best submission had an f1score of 0.030. \u2013 Schilder et\u00a0 al. [21] (team name: TR) applies a two-phase approach for Task 1: first, they generate a candidate set which tentatively contains all true noticed cases but eliminates some of the false candidates (i.e., this step is optimized for recall). The second step is a binary classifier which receives as input the pair (query case, candidate case) and predicts whether they represent a true noticed relationship. \u2013 Rosa et at. [20] (team name: NM) presents a vanilla application of BM25 to the case law retrieval problem. They do that by first indexing all base and candidate cases contained in the dataset. Before indexing, each document is split into segments of texts using a context window of 10 sentences with overlapping strides of five sentences (which are called \u2019candidate case segments\u2019). BM25 is then used to retrieve candidate case segments for each base case segment. The relevance score for a (base case, candidate case) pair is the maximum score among all their base case segment and candidate case segment pairs. The candidates are then ranked according to threshold-based heuristics. The NM team submitted only one run, which was ranked second place among all submissions with an f1score of 0.0937. \u2013 Ma et\u00a0al. [13] (team name: TLIR) was the top ranked team for Task 1. They apply two methods: the first is a traditional language model for IR (LMIR) [2], which consists of an application of LMIR on a pre-processed version of the dataset. The TLIR team did not use the full case contents, but cleverly made use of the tags inserted in the text to indicate a fragment has been suppressed in order\n114 The Review of Socionetwork Strategies (2022) 16:111\u2013133\n1 3\nto heuristically identify the potentially most relevant text fragments. The fact this approach ranked first place among all Task 1 competitors indicates traditional IR methods can achieve good results in the case law retrieval task. The second approach is a transformer based method, which factors a document into paragraphs and then computes measures on interactions between paragraphs using BERT. Compared with other neural models, BERT-PLI can take long text representations as an input without truncating them at some threshold. Yet, the results attained with this approach in COLIEE 2021 were not as good as the simpler IRbased approach, ranking at third and fifth places among all submission with an f1-score of 0.0456 and 0.0330. \u2013 Althammer et\u00a0 al. [1] (team name: DSSIR) combine retrieval methods with neural re-ranking methods using contextualized language models like BERT. Since the cases are typically long documents exceeding BERT\u2019s maximum input length, the authors adopt a two phase approach. The first phase combines lexical and dense retrieval methods on the paragraph-level of the cases. They then rerank the candidates by summarizing the cases and then apply a fine-tuned BERT re-ranker on said summaries. Their best ranking submission attained fourth place overall, with an f1-score of 0.0411."
        },
        {
            "heading": "2.4 Results and\u00a0Discussion",
            "text": "Table\u00a01 shows the results of all submissions received for Task 1 in COLIEE 2021. A total of 15 submissions from 7 different teams have been received. It can be seen the f1-scores were, in general, much lower than in previous editions, reflecting the fact the task is now more challenging than its previous formulation. The best performing team in Task 1 in the 2020 edition, for example, achieved an f1-score of 0.6774. For\n115\n1 3\nThe Review of Socionetwork Strategies (2022) 16:111\u2013133\nmore information on the previous task formulation and approaches, please see the COLIEE 2020 summary [16].\nMost of the participating teams applied traditional IR techniques such as BM25, transformer based methods such as BERT, or a combination of both. The best performing team was TLIR, with an f1-score of 0.1917, with an approach that combined traditional IR methods with simple heuristics to identify the most relevant fragments in a case law. Also worth mentioning is the NM team, whose approach was a vanilla application of BM25 and achieved the second place overall.\nFor future editions of COLIEE, we intend to make the distributions of the training and test datasets more similar with respect to average and standard deviation of number of noticed cases. Besides that, we will fix a few minor issues which were found in the dataset, such as two different files with the exact same contents (i.e., the same case represented as two separate cases). This is a problem with the original dataset from where the competition\u2019s data is drawn, and knowing that dataset presents those issues we will improve our collection methods to correct them. Fortunately, those issues were rare and did not have an impact on the final results.\nA known issue with the dataset is that tags inserted to indicate suppression of fragments provide an artificial clue as to where there is potentially highly relevant contents. That aspect was exploited by the winning team in COLIEE 2021. Whereas that is not a problem with that team\u2019s approach, we would like our datasets to represent as accurately as possible real-world problems, so options to improve such datasets will be explored in future editions."
        },
        {
            "heading": "3 Task 2\u2014Case Law Entailment",
            "text": ""
        },
        {
            "heading": "3.1 Task Definition",
            "text": "Task 2 is a legal case entailment task and it involves the identification of a paragraph from existing cases that can be claimed to entail the decision of a new case. Given a decision Q of a new case and a relevant case R, the challenge is to identify a specific paragraph in R that entails the decision Q. The organizers have confirmed that the answer paragraph cannot be identified merely by information retrieval techniques using some examples. Because the case R is a relevant case to Q, many paragraphs in R could be relevant to Q, regardless of confirming entailment. This task requires one to identify a paragraph which entails the decision of Q, so required is a specific entailment method that compares the meaning of each paragraph in R and the decision in Q. The data are drawn from an existing collection of predominantly Federal Court of Canada case law documents. The evaluation measure will be precision, recall and F-measure.\nFor COLIEE 2021, the Task 2 training and testing sets contain 426 and 100 base cases respectively. Table\u00a02 shows the dataset information for Task 2.\nTraining data is provided in the form of triples, each consisting of a query, a noticed case, and a paragraph number of the noticed case by which the decision of the query is entailed. Here, \u201cnoticed case\u201d means the relevant case of the query. An example is shown in Table\u00a03.\n116 The Review of Socionetwork Strategies (2022) 16:111\u2013133\n1 3"
        },
        {
            "heading": "3.2 Approaches",
            "text": "Seven teams participated in Task 2, and a total of 17 results were submitted (average 2.43 results per team). Each team was allowed to submit a maximum of three results. Table\u00a04 shows the approaches that teams used in Task 2. Althammer et\u00a0al. [1] (team name:DSSIR) used either BM25 or DPR [8] model to produce the first two results, which were trained on the entailing paragraph pairs in order to rank each paragraph in the noticed case, given the query paragraph. They also combined the ranking of BM25 and DPR as their third result.\nSchilder et\u00a0 al. [21] (team name: TR) used hand-crafted similarity features and applied a classical random forest classifier. Using n-gram vectors, universal sentence\n117\n1 3\nThe Review of Socionetwork Strategies (2022) 16:111\u2013133\nencoder vectors, and averaged word embedding vectors, they computed the similarity between each paragraph in the noticed case and the decision fragment in the query. After selecting the most similar k paragraphs, they trained a random forest classifier.\nKim et\u00a0 al. [9] (team name: UA) used BERT pre-trained on a large (general purpose) dataset by fine-tuning on the provided training dataset. If the tokenization step produced more than the 512 token limit, they apply another transformer-based model to generate a summary of the input text, and then process the pair again. Since the input text often includes text in French, they apply a simple language detection model based on naive Bayesian filter to remove those fragments. There are usually very few actual entailing paragraphs in a case (by far, most of the cases only have one entailing paragraph). So in the post-processing step they establish limits for the maximum number of outputs allowed per case. At the same time, they observe a minimum score in an attempt to reduce the number of the false positives.\nLi et\u00a0 al. [11] (team name: siat) proposed a pre-training Task on BERT (BERTbase-uncased) with dynamic N-gram masking, to get a special BERT model with legal knowledge (BERTLegal). They utilized n-gram masking to generate masked inputs for what they call \u201cmasked language model\u201d targets. The length of each n-gram mask is randomly selected amongst 1, 2, and 3. They also did data augmentation and used a Fast Gradient method.\nNguyen et\u00a0al. [14] (team name: JNLP) used the supporting model and lexical model for two submissions, and in the last submission, they used a neighbouring structures fingerprint (NSFP) model.\n[19] (team name: NM) used monoT5-zero-shot, monoT5 and DeBERTa [7]. They also evaluated an ensemble of their monoT5 and DeBERTa models. The model monoT5-zero-shot is a sequence-to-sequence adaptation of the T5 [17] model.\nWe were not able to identify the approach of the team MAN01 as there was no corresponding paper submission."
        },
        {
            "heading": "3.3 Evaluation Measure",
            "text": "Task 2 uses micro-average precision, recall and F1-measure as evaluation metrics, which are formulated as follows:\nwhere NTP denotes the number of true positive prediction for all queries, NTP + NFP is the total positive prediction number for all queries, and NTP + NFN is the ground truth positive case number.\n(1)Precision = NTP\nNTP + NFP ,\n(2)Recall = NTP\nNTP + NFN ,\n(3)F1 = 2 \u2217 Precision \u2217 Recall\nPrecision + Recall ,\n118 The Review of Socionetwork Strategies (2022) 16:111\u2013133\n1 3"
        },
        {
            "heading": "3.4 Results and\u00a0Discussion",
            "text": "Table\u00a0 5 shows the Task 2 results. NM team\u2019s three submissions are all ranked no. 1\u20133. In particular, their Ensemble of DeBERTa and monoT5 showed the best performance with the F1 score of 0.6912. As shown in Table\u00a06, the systems of the the winning team (NM) show balanced performance between precision and recall. This task is to find the paragraph(s) that entails the decision of the query, and in most cases, only one paragraph is the correct answer. So, systems are likely to show better precision than recall. An interesting observation in Table\u00a06 is that the system monoT5 showed better recall than precision.\nMost of the systems combined the traditional BM25 information retrieval algorithm and BERT Transformer language model. They showed that the traditional BM25 system is still useful in legal information retrieval and entailment. To solve the issue of the dataset imbalance, some teams tried data augmentation. In addition, some approaches tried to extract semantic relationships between paragraphs using BERT. Finally, there was an approach to use LEGAL-BERT, a BERT system optimized for the legal domain, but the performance was not promising.\n1 3\nParticipants have stated that the extreme class-imbalance nature of the problem and the limited data size make it challenging to train an efficient and generalizable classification model. Because of the limited data size, the winning team (NM) adopted zero-shot models, and they showed that zero-shot models can have at least equivalent performance to models that have been fine-tuned on a legal case entailment task. They also confirmed a counter-intuitive result: that models with little or no adaption to the target task can be more robust to changes in the data distribution than models that have been carefully fine-tuned to the task at hand."
        },
        {
            "heading": "4 Task 3\u2014Statute Law Information Retrieval",
            "text": ""
        },
        {
            "heading": "4.1 Task Definition",
            "text": "Task 3 requires the retrieval of an appropriate subset ( S1 , S2,..., Sn ) of Japanese Civil Code Articles from the Civil Code texts dataset, used for answering a Japanese legal bar exam question Q.\nAn appropriate subset means the identification of a subset of statutes for which an entailment system can judge whether the statement Q is true Entails(S1, S2, ..., Sn,Q) or not Entails(S1, S2, ..., Sn,\u00acQ)."
        },
        {
            "heading": "4.2 Dataset",
            "text": "For Task 3, questions related to Japanese civil law were selected from the Japanese bar exam. Since there were some updates of Japanese Civil Code on April 2020, we revised the text database to reflect this revision for Civil Code, and its translation into English. However, since the English translated version is not provided for a portion of this code, we exclude those untranslated parts from the civil code text and their related questions. As a result, the number of civil code articles used in the dataset is 768, or about half of previous COLIEE competitions. Training data (the questions and relevant article pairs) were constructed by using previous COLIEE data (806 questions). In this data, questions related to revised articles are reexamined and those for excluded articles are removed from the training data. For the test data, new questions were selected from the 2020 bar exam (81 questions).\nThe number of questions classified by the number of relevant articles is listed in Table\u00a07.\nNumber of relevant article(s) 1 2 4 Total\nNumber of questions 65 14 2 81\n120 The Review of Socionetwork Strategies (2022) 16:111\u2013133\n1 3"
        },
        {
            "heading": "4.3 Approaches",
            "text": "The following six teams submitted their results (18 runs in total). We describe approaches for each team as follows, using a header format of the form Team Name (number of submitted runs). All teams had experience in submitting results in previous competition. Because the best performance system [22] of COLIEE 2020 uses BERT [5], most of the teams (HUKB, JNLP OvGU, and TR) use BERT and ensemble results with an ordinary IR system (HUKB and OvGU). One characteristic feature proposed in this year\u2019s task is extension of training data for BERT-based IR system training. OvGU proposed a method to extend the contents of original article using text data related to the article (metadata, text from the website). JNLP proposed a method to select a corresponding part of the article for the query using a sliding window mechanism. HUKB proposed a method to add detailed information from the referred articles. Other common techniques used in the system were well known IR engine mechanisms such as BM25, TF-IDF, Indri [23], and Word Movers\u2019 Distance (WMD) [10].\n\u2013 HUKB (three runs) [27] uses a BERT-based IR system and Indri for the IR module, and compares the result of each system output to create final results. They construct a new article database with the following two types: one expands the detailed information using the referred article, and the other uses text splitting for describing one judicial decision. They submitted three runs with almost similar settings and the best run is HUKB-3. \u2013 JNLP (three runs) [14] uses a BERT-based IR models that combines multiple BERT models for generating results. They also construct training data of relevant articles by selecting the most relevant part of the article using a sliding window. The best run is JNLP.CrossLMultiLThreshlod that uses an ensemble of three different systems outputs by selecting the highest result among them. \u2013 LLNTU (three runs) has not submitted a paper describing their methods. \u2013 OvGU (three runs) [25] uses a variety of BERT models with different data\nenrichment techniques. The best run is OvGU_run1 that uses sentence-BERT embedding [18] with TF-IDF by enriching the articles in the training data by using metadata, text from the web data related to the article and relevant queries from training data. \u2013 TR (three runs) [21] submits three runs and the best run is TR_HB uses Word Mover\u2019s Distance (WMD) approach to calculate the similarity between query and articles. \u2013 UA (three runs) [9] uses ordinary IR modules for generating results. The best run is BM25.UA that uses BM25 as an IR module."
        },
        {
            "heading": "4.4 Results and\u00a0Discussion",
            "text": "Table\u00a08 shows the evaluation results of submitted runs. The official evaluation measures used in this task were macro average (average of evaluation measure values for\n121\n1 3\nThe Review of Socionetwork Strategies (2022) 16:111\u2013133\neach query over all queries) of the F2 measure, precision, and recall (See Appendix 1 for the definition of those measures).\nWe also calculate the mean average precision (MAP) and recall at k (Rk : recall is calculated by using the top k ranked documents as returned documents) by using the long ranking list (100 articles). Table\u00a08 shows the results of the evaluation of submitted results2.\nThis year, OvGU is the best run among all runs. JNLP achieves almost similar score and have higher MAP. This year, ordinary IR model BM25 achieves good performance for finding one relevant article for the question. From this results, we confirm the effectiveness of using deep learning technology such as BERT for this task.\nFigures\u00a0 1, 2, and 3 show the average of evaluation measure for all submission runs. As we can see from Fig.\u00a01, there are many easy questions for which almost all system can retrieve the relevant article. The easiest question is R02-10-E \u201cAn underground space or airspace may be established as the subject matter of superficies for ownership of structures, through the specification of upper and lower extents.\u201d whose relevant article (Article 269-2) has the same sentence in the text.\nHowever, there are five queries for which none of the system can retrieve the relevant articles. All questions (R02-9-E, R02-15-I, 02-15-U, 02-15-E, and R02-23-E) are based on the use case of the article that requires semantic matching and handling anonymized symbols such as \u201cA\u201d and \u201cB\u201d for referring person or other entities. For example, question of R02-9-E is \u201cB obtained A\u2019s bicycle by fraud. In this case, A may demand the return of the bicycle against B by filing an action for recovery of possession.\u201d A related article is \u201cArticle 192 A person that commences the possession of movables peacefully and openly by a transactional act acquires the rights that are exercised with respect to the movables immediately if the person possesses it in good faith and without negligence.\u201d3 It is necessary to recognize following semantic relationship (\u201cbicycle\u201d as \u201cmovables\u201d and \u201cA\u201d and \u201cB\u201d as persons, and conflict between \u201cby fraud\u201d and \u201cpeacefully\u201d). This semantic interpretation of the\n2 Due to errors in the evaluation data, this result is different from the one used in the workshop paper. However, the order of teams is same as that in the workshop.\n1 3\n1 3\nstatue statements is an instance of the greater challenge of identifying relationships between abstract statutes and specific texts."
        },
        {
            "heading": "4.5 Discussion",
            "text": "Since the statute law retrieval task is one of the oldest tasks of COLIEE, it is appropriate to discuss which kind of issues have been addressed over the development process. As we can see, there are three different types of questions for which we can describe the challenges.\nOne of the characteristics of difficult questions of this year are those that uses anonymized symbols as pronouns or placeholders, such as \u201cA\u201d and \u201cB\u201d for referring person or other entities. In the test case of COLIEE 2021, 35 questions contain such anonymized symbol and 27 (out of 35) questions have one related article.\nTable\u00a09 represents the number of query with one relevant article for the F2 measure (average) classified by one with anonymized symbol or other. Table\u00a010 represents the number of query with multiple relevant article for the F2 measure classified by one with anonymized symbol.\nFrom Table\u00a09, we confirm that most of the retrieval questions without anonymized symbol can be identified by most of the submitted systems (there is no question whose F2 measure (average) is lower than 0.6). However, it is still difficult for the system to retrieve relevant articles for the question with anonymized symbols (16 out of 27 questions has F2 measure (average) lower than 0.6).\n1 3\nThis result reflects the different characteristics of the question with anonymized symbol or not. In most of the cases, questions with anonymized symbols represent question about use cases of the articles, therefore they require the handling of semantic relationships that we discussed in Sect.\u00a0 4.4. On the contrary, most of the questions without anonymized symbols do not require the handling of such semantic relationships. In addition, since deep learning based NLP such as BERT can handle the context information, it is helpful to select appropriate relevant articles from the ones that use a similar vocabulary. However, the similarity of terms in the legal domain may not be same as ones in the usual texts. For example, \u201cjewelry,\u201d \u201ccar\u201d and \u201cpaintings\u201d are similar terms in the context of valuable movables in the legal domain, but those terms are not similar context in the ordinary texts. Usage of legal-BERT [25] is one of the possible solution for this problem, but their performance is not good as the best run. It is necessary to investigate appropriate model of the transformer (including BERT and other variations) for this task.\nFor the questions with multiple relevant articles, we still have difficulties to retrieve all relevant articles (Table\u00a010). This is because most of the systems tried to deal this problem as simple rank-based retrieval problems. For example, the best performance system OvGU [21] and the second best team JNLP [14] also use a thresholding approach to select relevant articles. These selection processes can be\nF2 Anonymize Other\n0\u20130.1 6 0 0.1\u20130.2 0 0 0.2\u20130.3 2 0 0.3\u20130.4 2 0 0.4\u20130.5 2 0 0.5\u20130.6 4 0 0.6\u20130.7 6 7 0.7\u20130.8 2 15 0.8\u20130.9 3 13 0.9\u20131.0 0 3\nTable 10 Number of questions classified by F2 score and query type (multiple relevant articles)\nF2 Anonymize Other\n0\u20130.1 3 0 0.1\u20130.2 1 1 0.2\u20130.3 1 2 0.3\u20130.4 1 1 0.4\u20130.5 1 2 0.5\u20130.6 1 0 0.7\u20130.8 0 2\n125\n1 3\nThe Review of Socionetwork Strategies (2022) 16:111\u2013133\ninterpreted as one for deciding of number of relevant documents using rank-based retrieval results.\nHowever, it is better to consider the relationships among statute law articles using article reference information from the legal perspective. HUKB [27] tried to identify the relationships among articles based on the reference information with rank-based retrieval approach. However, their performance is not currently as good as expected.\nBased on the discussion, we can confirm that success can use conventional IR methods for retrieving simple questions whose topic are not use cases and have one relevant article. However, we still have difficulty to handle questions about use cases and ones with multiple relevant articles.\nFor possible future directions, it is necessary to propose a framework to encourage participants to tackle these problems."
        },
        {
            "heading": "5 Tasks 4 and\u00a05\u2014Statute Law Entailment and\u00a0Question Answering",
            "text": ""
        },
        {
            "heading": "5.1 Task Definition",
            "text": "Task 4 is a task to determine textual entailment relationships between a given problem sentence and relevant article sentences. Competitor systems should answer \u201cyes\u201d or \u201cno\u201d regarding the given problem sentences and given article sentences. Until COLIEE 2016, the competition had only pure entailment tasks, where t1 (relevant article sentences) and t2 (problem sentence) were given. Due to the limited number of available problems, COLIEE 2017, 2018 did not retain this style of task. In the Task 4 of COLIEE 2019 and 2020, we returned to the pure textual entailment task to attract more participants, which produced more focused analyses. In COLIEE 2021, we revived the question answering task as Task 5, and retained the textual entailment task as Task 4; Task 5 requires a system to answer \u201cyes\u201d or \u201cno\u201d given a problem sentence(s) only. Participants can use any external data, however this assumes that they do not use the test dataset."
        },
        {
            "heading": "5.2 Dataset",
            "text": "Our training dataset and test dataset are the same as for Task 3. Questions related to Japanese civil law were selected from the Japanese bar exam. The organizers provided a data set used for previous campaigns as training data (806 questions) and new questions selected from the 2020 bar exam as test data (81 questions). The Task 5 dataset is the same as Task 4. We performed Task 5 before Task 4 in order not to reveal the gold standard article labels which are included in the Task 4 dataset."
        },
        {
            "heading": "5.3 Approaches",
            "text": "All teams submitted three runs for each of Tasks 4 and 5, except that the OvGU and HUKB teams participated Task 4 only.\n126 The Review of Socionetwork Strategies (2022) 16:111\u2013133\n1 3\n\u2013 HUKB (three runs) [26] used an ensemble architecture of BERT methods with data augmentation. They prepared an ensemble of 10 models. Their data augmentation extracts judicial decision sentences, then makes positive/negative data from articles. \u2013 JNLP (three runs) [15] uses bert-base-Japanese-whole-word-masking with tfidf based data augmentation. Their models are trained with different numbers of pretrained/fine-tuned epochs (JNLP.Enss5a and JNLP.Enss5b), and an ensemble of these two models (JNLP.EnssBest). For Task 4, their proposed methods use their proposed Next Foreign Sentence Prediction (JNLP. NFSP) which trains to determine if semantic of two sentences in different languages belong to two consecutive sentences in a document, and Neighbor Multilingual Sentence Prediction (JNLP. NMSP) which adds pairs of same-language sentences in two languages to the bilingual pairs of NFSP, together with the original multilingual BERT (JNLP. BERT_Multilingual) for Task 5. \u2013 KIS (three runs) [6] extended their previous work using a classic NLP approach, to be explainable, based on predicate-argument structure analysis, original legal dictionary, negation detection, and ensemble of modules with different thresholds and combinations of these features. \u2013 OvGU (three runs) [25] employed an ensemble of graph neural networks where each node represents either a query or an article, sentences embedded by a pretrained paraphrase-distilroberta-base-v1 (OvGU_run1), and LEGAL-BERT based on legalbert- base-uncased with different training phases (OvGU_run2 and OvGU_run3). \u2013 TR (three runs) [21] uses existing models: TR-Ensemble using T5 [17]-based ensemble, TR-MTE using Multee [24], and TR_ Electra using Electra [4] for Task 4; (TRDistill-Roberta) using distilled version of RoBERTa [12], TRGPT3Davinci using the largest model of GPT-3 [3] and TRGPT3Ada using the smaller one for Task 5. \u2013 UA (three runs) [9] uses BERT (UA_dl), with semantic information (using the Kadokawa thesaurus concept number) (UA_parser)."
        },
        {
            "heading": "5.4 Results and\u00a0Discussion",
            "text": "Tables\u00a011 and 13 show evaluation results of Tasks 4 and 5, respectively. Tables\u00a012 and 14 show our categorization results of Tasks 4 and 5, respectively. Because an entailment task is essentially a complex composition of different subtasks, we manually categorized our test data into linguistic categories, depending on what sort of technical issues require resolution. As this is a composite task, overlap is allowed between categories. Our categorization is based on the original Japanese version of the legal bar exam. The BL column in Table\u00a012 shows correct answer ratios for each category when answering the majority answer \u201cNo\u201d to all problems. Interestingly, all runs are under the baseline in the Negation category, which is expected to answer easier than other categories. This comparison supports the discussion that the task is complex and composite one, the result is not simply regarded as it is better when the overall score is better.\n1 3\nThe test dataset characteristics seems not to be coherent throughout these years of the COLIEE series. For example, we observe more problems which require handling of anonymized symbol such as \u201cA\u201d and \u201cB\u201d for referring persons (discussed in the Task 3 part as well) than previous years. Such problems should be still very difficult for any NLP method to solve, except similar possible patterns could be sufficiently covered by some external training dataset. The Anaphora rows of Tables\u00a012 The best team in Task 4 would have solved \u201ceasier\u201d problems well, while remaining \u201cdifficult\u201d linguistic issues remain for future work."
        },
        {
            "heading": "6 Conclusion",
            "text": "We have summarized the systems and their performance as submitted to the COLIEE 2021 competition. For Task 1, TLIR was the best performing team with an F1 score of 0.1917, whose approach applied a combination of LMIR and a BERTbased method. In Task 2, the winning team ensembled DeBERTa and monoT5 and achieved an F1 score of 0.6912. For Task 3, the top ranked team (OvGU) employed sentence-BERT embeddings and augmented the training data with metadata, web\nsid submission id, L Dataset Language (J: Japanese, E: English), Correct: number of correct answers (81 problems in total). JNLP. Enss5Ca and JNLP.Enss5Cb stand for JNLP.Enss5C15050 and JNLP.Enss5C15050SilverE2E10, respectively\nTeam L Correct Accuracy N/A BaseLine N/A Yes 43/all 81 0.5309\nHUKB HUKB-2 J 57 0.7037 HUKB HUKB-1 J 55 0.6790 HUKB HUKB-3 J 55 0.6790 UA UA_parser E 54 0.6667 JNLP JNLP.Enss5Ca J 51 0.6296 JNLP JNLP.Enss5Cb J 51 0.6296 JNLP JNLP.EnssBest J 51 0.6296 OVGU OVGU_run3 E 48 0.5926 TR TR-Ensemble J 48 0.5926 TR TR-MTE J 48 0.5926 OVGU OVGU_run2 E 45 0.5556 KIS KIS1 J 44 0.5432 KIS KIS3 J 44 0.5432 UA UA_1st E 44 0.5432 KIS KIS2 E 43 0.5309 UA UA_dl E 43 0.5309 TR TR_Electra J 41 0.5062 OVGU OVGU_run1 E 36 0.4444\n128 The Review of Socionetwork Strategies (2022) 16:111\u2013133\n1 3\nTa bl\ne 12\nTa\nsk 4\n\u2019s L\nin gu\nist ic\nc at\neg or\ny st\nat ist\nic s o\nf p ro\nbl em\ns, an\nd co\nrr ec\nt a ns\nw er\ns o f s\nub m\nitt ed\nru ns\nfo r e\nac h\nca te\ngo ry\nin n\num be\nrs o\nf c ou\nnt s a\nnd p\ner ce\nnt ag\nes\nTy pe\nc ol\num n\nsh ow\ns t he\nc at\neg or\ny na\nm es\n, # c\nol um\nn sh\now s t\nhe n\num be\nr o f p\nro bl\nem s f\nor e\nac h\nca te\ngo ry\n, a lp\nha be\ntic al\nh ea\nde r n\nam es\nin o\nth er\nc ol\num ns\nc or\nre sp\non d\nto fo\nrm al\nru n\nna m\nes a\ns fo\nllo w\ns, sh\now in\ng co\nrr ec\nt a ns\nw er\nra tio\np er\nce nt\nag e\nfo r e\nac h\nru n.\nA : H\nU K\nB -1\n, B : H\nU K\nB -2\n, C : H\nU K\nB -3\n, D : J\nN LP\n.E ns\ns5 C\n15 05\n0, E\n: E ns\ns5 C\n15 05\n0S ilv\ner E2\nE1 0,\nF :\nJN LP\n.E ns\nsB es\nt, G\n: K IS\n1, H\n: K IS\n2, I:\nK IS\n3, J\n: O V\nG U\n_r un\n1, K\n: O V\nG U\n_r un\n2, L\n: O V\nG U\n_r un\n3, M\n: T R-\nEn se\nm bl\ne, N\n: T R-\nM TE\n, O : T\nR _E\nle ct\nra , P\n: U A\n_1 st,\nQ : U\nA _d\nl, R : U A _p ar se r, B L: B as el in e (A ns w er in g N o to A ll) Ty pe # A B C D E F G H I J K L M N O P Q R B L C on di tio n 65 60 63 60 58 58 58 45 45 45 45 52 54 52 52 49 51 45 58 54 Pe rs . r el tn sh p. 44 61 66 64 55 57 55 45 43 45 41 52 57 59 55 52 43 50 61 55 A na ph or a 37 68 68 65 65 57 59 62 57 59 41 57 54 51 51 49 54 46 65 57 Pe rs . r ol e 34 68 71 65 53 59 56 44 41 44 44 47 53 56 56 53 44 50 65 59 Pr ed . a rg um en t 32 75 81 78 69 69 66 44 41 44 50 50 59 47 59 34 54 47 69 69 N eg at io n 28 64 64 64 61 64 57 50 46 50 54 43 54 68 57 57 43 50 68 79 Ve rb p ar ap hr s. 23 61 70 65 61 61 51 48 48 48 35 57 70 70 74 48 65 57 83 74 Le ga l f ac t 22 68 73 64 55 55 55 55 55 55 41 55 64 55 50 45 36 41 59 52 D ep en de nc y 22 73 73 73 50 64 59 55 50 50 41 55 59 45 59 32 50 36 59 55 M or ph em e 21 81 81 81 76 71 76 81 81 81 48 67 62 71 71 57 71 76 81 24 Ite m iz ed 11 55 55 55 55 45 55 55 55 55 73 64 64 55 45 55 73 55 55 64 A rti cl e se ar ch 9 67 67 67 44 56 44 67 56 56 44 56 67 44 56 44 56 33 44 67 C as e ro le 2 0 0 0 50 0 0 50 0 0 50 50 50 50 0 50 0 50 0 50 Pa ra ph ra se 2 50 50 50 10 0 50 10 0 50 50 50 50 50 0 10 0 10 0 0 50 10 0 50 50\n1 3\nsid submission id, L Dataset Language (J: Japanese, E: English), Correct: number of correct answers (81 problems in total). JNLP. Task5.BERT_Multilingual is abbreviated as JNLP.Task5.BERT\nTeam sid L Correct Accuracy N/A BaseLine N/A No 43/All 81 0.5309\nJNLP JNLP.NFSP J 49 0.6049 UA UA_parser E 46 0.5679 JNLP JNLP.NMSP J 45 0.5556 UA UA_dl E 45 0.5556 TR TRDistillRoberta J 44 0.5432 KIS KIS_2 J 41 0.5062 KIS KIS_3 J 41 0.5062 UA UA_elmo E 40 0.4938 JNLP JNLP.Task5.BERT J 38 0.4691 KIS KIS_1 J 35 0.4321 TR TRGPT3Ada J 35 0.4321 TR TRGPT3Davinci J 35 0.4321\nType column shows the category names, # column shows the number of problems for each category, alphabetical header names in other columns correspond to formal run names as follows, showing correct answer ratio percentage for each run. a: HUKB-1, b: HUKB-2, c: HUKB-3, d: JNLP.NFSP, e: JNLP.NMSP, f: JNLP.Task5.BERT_Multilingual, g: KIS_1, h: KIS_2, i: KIS_3, j: TRDistillRoberta, k: TRGPTAda, l: TRGPT3Davinci, m: UA_dl, n: UA_elmo, o: UA_parser\n130 The Review of Socionetwork Strategies (2022) 16:111\u2013133\n1 3\ndata related to the articles and relevant queries from the training data, to achieve an F2 score of 0.73. HUKB was the Task 4 winner, with an Accuracy of 0.7037. They applied an ensemble of BERT models and data augmentation. In Task 5, JNLP was the best performing team and applied a variety of BERT-based models, achieving an Accuracy of 0.6049.\nIn this edition, we introduced a new task on statute law question answering (Task 5) and a new formulation for the case law retrieval task (Task 1). We intend to further improve the datasets quality in future editions of COLIEE so the tasks more accurately represent real-world problems.\nEvaluation Measure\nIn the COLIEE tasks, the following measures are used for evaluation.\n\u2013 Precision is a measure to analyze accuracy of the returned results using following formula, where NTP,NFP denote the number of true positive and false positive prediction respectively. NTP + NFP equals to the number of all positive cases in the result.\n\u2013 Recall is a measure to analyze the comprehensiveness of the returned results using following formula, where NTP,NFP denote the number of true positive and false positive prediction respectively. NTP + NTN equals to the number of all true cases in the evaluation set.\n\u2013 F1 is a measure that consider accuracy and comprehensiveness using harmonic mean of Precision and Recall.\n\u2013 F2 is a variation of F1 that puts more emphasis on Recall.\n\u2013 Precision@Rank, Recall@Rank are measures used for evaluating ranked list by selecting top Rank results as returned results for calculating Precision and Recall respectively. \u2013 Mean Average Precision (MAP) is a measure to evaluate the quality of ranked retrieval results for document retrieval using following formula , where Rel(i),Nrel,Nret,Nq denote functions to check whether ith results is relevant or\n(4)Precision = NTP\nNTP + NFP ,\n(5)Recall = NTP\nNTP + NTN ,\n(6)F1 = 2 \u2217 Precision \u2217 Recall\nPrecision + Recall ,\n(7)F2 = 5 \u2217 Precision \u2217 Recall\n4 \u2217 Precision + Recall ,\n131\n1 3\nThe Review of Socionetwork Strategies (2022) 16:111\u2013133\nnot, the number of relevant document for the query, returned documents and one of queries respectively.\n\u2013 Accuracy is calculated the accuracy of the returned results for the task that system returns one answer for each case. This is equivalent to Precision and Recall, because number of all positive cases from the system and the number of all true cases in the evaluation set are same.\nWe also use micro-average and macro-average for aggregating the evaluation measures with multiple cases. Micro average is calculated measure without considering the cases, but macro average is calculated as average of original evaluation measures for each query and calculate average of all cases.\nAcknowledgements This competition would not be possible without the significant support of Colin Lachance from vLex and Compass Law, and the guidance of Jimoh Ovbiagele of Ross Intelligence and Young-Yik Rhim of Intellicon. Our work to create and run the COLIEE competition is also supported by our institutions: the National Institute of Informatics (NII), Shizuoka University and Hokkaido University in Japan, and the University of Alberta and the Alberta Machine Intelligence Institute in Canada.\nDeclarations\nConflict of Interest On behalf of all authors, the corresponding author states that there is no conflict of interest.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article\u2019s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article\u2019s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/."
        }
    ],
    "title": "Overview and Discussion of the Competition on Legal Information Extraction/Entailment (COLIEE) 2021",
    "year": 2022
}