{
    "abstractText": "The Prophet Inequality and Pandora\u2019s Box problems are fundamental stochastic problem with applications in Mechanism Design, Online Algorithms, Stochastic Optimization, Optimal Stopping, and Operations Research. A usual assumption in these works is that the probability distributions of the n underlying random variables are given as input to the algorithm. Since in practice these distributions need to be learned under limited feedback, we initiate the study of such stochastic problems in the Multi-Armed Bandits model. In the Multi-Armed Bandits model we interact with n unknown distributions over T rounds: in round t we play a policy x and only receive the value of x as feedback. The goal is to minimize the regret, which is the difference over T rounds in the total value of the optimal algorithm that knows the distributions vs. the total value of our algorithm that learns the distributions from the limited feedback. Our main results give near-optimal \u00d5 ( poly(n) \u221a T ) total regret algorithms for both Prophet Inequality and Pandora\u2019s Box. Our proofs proceed by maintaining confidence intervals on the unknown indices of the optimal policy. The exploration-exploitation tradeoff prevents us from directly refining these confidence intervals, so the main technique is to design a regret upper bound function that is learnable while playing low-regret Bandit policies. (gatmiry@mit.edu) Electrical Engineering and Computer Science, Massachusetts Institute of Technology (thomas.kesselheim@uni-bonn.de) Institute of Computer Science and Lamarr Institute for Machine Learning and Artificial Intelligence, University of Bonn (ssingla@gatech.edu) School of Computer Science, Georgia Tech. Supported in part by NSF award CCF-2327010. (ywang3782@gatech.edu) School of Computer Science, Georgia Tech. Supported in part by NSF award CCF2327010.",
    "authors": [
        {
            "affiliations": [],
            "name": "Khashayar Gatmiry"
        },
        {
            "affiliations": [],
            "name": "Thomas Kesselheim"
        },
        {
            "affiliations": [],
            "name": "Sahil Singla"
        },
        {
            "affiliations": [],
            "name": "Yifan Wang"
        }
    ],
    "id": "SP:a4b42e8a9ae84000ac5d833bd619ade363f04a41",
    "references": [
        {
            "authors": [
                "Peter Auer",
                "Nicolo Cesa-Bianchi",
                "Paul Fischer"
            ],
            "title": "Finite-time analysis of the multiarmed bandit problem",
            "venue": "Machine learning,",
            "year": 2002
        },
        {
            "authors": [
                "Alexia Atsidakou",
                "Constantine Caramanis",
                "Evangelia Gergatsouli",
                "Orestis Papadigenopoulos",
                "Christos Tzamos"
            ],
            "title": "Contextual pandora\u2019s box",
            "venue": "arXiv preprint arXiv:2205.13114,",
            "year": 2022
        },
        {
            "authors": [
                "Sanjeev Arora",
                "Elad Hazan",
                "Satyen Kale"
            ],
            "title": "The multiplicative weights update method: a meta-algorithm and applications",
            "venue": "Theory of computing,",
            "year": 2012
        },
        {
            "authors": [
                "Pablo Daniel Azar",
                "Robert Kleinberg",
                "S. Matthew Weinberg"
            ],
            "title": "Prophet inequalities with limited information",
            "venue": "In Proceedings of the Twenty-Fifth Annual ACM-SIAM Symposium on Discrete Algorithms,",
            "year": 2014
        },
        {
            "authors": [
                "Nima Anari",
                "Rad Niazadeh",
                "Amin Saberi",
                "Ali Shameli"
            ],
            "title": "Nearly optimal pricing algorithms for production constrained and laminar bayesian selection",
            "venue": "In Proceedings of the 2019 ACM Conference on Economics and Computation,",
            "year": 2019
        },
        {
            "authors": [
                "S\u00e9bastien Bubeck",
                "Nicol\u00f2 Cesa-Bianchi"
            ],
            "title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems",
            "venue": "Foundations and Trends in Machine Learning,",
            "year": 2012
        },
        {
            "authors": [
                "Mark Braverman",
                "Mahsa Derakhshan",
                "Antonio Molina Lovett"
            ],
            "title": "Max-weight online stochastic matching: Improved approximations against the online benchmark",
            "venue": "In ACM Conference on Economics and Computation,",
            "year": 2022
        },
        {
            "authors": [
                "Richard Bellman"
            ],
            "title": "Dynamic programming",
            "year": 1957
        },
        {
            "authors": [
                "Nicolo Cesa-Bianchi",
                "G\u00e1bor Lugosi"
            ],
            "title": "Prediction, learning, and games",
            "venue": "Cambridge university press,",
            "year": 2006
        },
        {
            "authors": [
                "Jos\u00e9 R. Correa",
                "Paul D\u00fctting",
                "Felix A. Fischer",
                "Kevin Schewior"
            ],
            "title": "Prophet inequalities for I.I.D. random variables from an unknown distribution",
            "venue": "In Proceedings of the 2019 ACM Conference on Economics and Computation, EC,",
            "year": 2019
        },
        {
            "authors": [
                "Shuchi Chawla",
                "Jason D. Hartline",
                "David L. Malec",
                "Balasubramanian Sivan"
            ],
            "title": "Multiparameter mechanism design and sequential posted pricing",
            "venue": "In Proceedings of the 42nd ACM Symposium on Theory of Computing,",
            "year": 2010
        },
        {
            "authors": [
                "Tomer Ezra",
                "Michal Feldman",
                "Nick Gravin",
                "Zhihao Gavin Tang"
            ],
            "title": "Online stochastic max-weight matching: Prophet inequality for vertex and edge arrival models",
            "venue": "In The 21st ACM Conference on Economics and Computation, EC,",
            "year": 2020
        },
        {
            "authors": [
                "Hossein Esfandiari",
                "Mohammad Taghi Hajiaghayi",
                "Brendan Lucier",
                "Michael Mitzenmacher"
            ],
            "title": "Online pandora\u2019s boxes and bandits",
            "venue": "In The Thirty-Third AAAI Conference on Artificial Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Michal Feldman",
                "Nick Gravin",
                "Brendan Lucier"
            ],
            "title": "Combinatorial auctions via posted prices",
            "venue": "In Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms,",
            "year": 2015
        },
        {
            "authors": [
                "Hu Fu",
                "Tao Lin"
            ],
            "title": "Learning utilities and equilibria in non-truthful auctions",
            "venue": "In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "Moran Feldman",
                "Ola Svensson",
                "Rico Zenklusen"
            ],
            "title": "Online contention resolution schemes",
            "venue": "In Proceedings of the Twenty-Seventh Annual ACM-SIAM Symposium on Discrete Algorithms,",
            "year": 2016
        },
        {
            "authors": [
                "Hu Fu",
                "Zhihao Gavin Tang",
                "Hongxun Wu",
                "Jinzhao Wu",
                "Qianfan Zhang"
            ],
            "title": "Random order vertex arrival contention resolution schemes for matching, with applications",
            "venue": "In 48th International Colloquium on Automata, Languages, and Programming,",
            "year": 2021
        },
        {
            "authors": [
                "Chenghao Guo",
                "Zhiyi Huang",
                "Zhihao Gavin Tang",
                "Xinzhi Zhang"
            ],
            "title": "Generalizing complex hypotheses on product distributions: Auctions, prophet inequalities, and pandora\u2019s problem",
            "venue": "In Conference on Learning Theory, COLT,",
            "year": 2021
        },
        {
            "authors": [
                "Anupam Gupta",
                "Haotian Jiang",
                "Ziv Scully",
                "Sahil Singla"
            ],
            "title": "The markovian price of information",
            "venue": "In Proceedings of Integer Programming and Combinatorial Optimization, IPCO,",
            "year": 2019
        },
        {
            "authors": [
                "Buddhima Gamlath",
                "Sagar Kale",
                "Ola Svensson"
            ],
            "title": "Beating greedy for stochastic bipartite matching",
            "venue": "In Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms,",
            "year": 2019
        },
        {
            "authors": [
                "Evangelia Gergatsouli",
                "Christos Tzamos"
            ],
            "title": "Online learning for min sum set cover and pandora\u2019s box",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Jason D Hartline"
            ],
            "title": "Mechanism design and approximation",
            "venue": "Book draft.,",
            "year": 2022
        },
        {
            "authors": [
                "Elad Hazan"
            ],
            "title": "Introduction to online convex optimization",
            "venue": "Foundations and Trends\u00ae in Optimization,",
            "year": 2016
        },
        {
            "authors": [
                "Mohammad Taghi Hajiaghayi",
                "Robert D. Kleinberg",
                "Tuomas Sandholm"
            ],
            "title": "Automated online mechanism design and prophet inequalities",
            "venue": "In Proceedings of the TwentySecond AAAI Conference on Artificial Intelligence,",
            "year": 2007
        },
        {
            "authors": [
                "Ulrich Krengel",
                "Louis Sucheston"
            ],
            "title": "Semiamarts and finite values",
            "venue": "Bull. Am. Math. Soc,",
            "year": 1977
        },
        {
            "authors": [
                "Ulrich Krengel",
                "Louis Sucheston"
            ],
            "title": "On semiamarts, amarts, and processes with finite value",
            "venue": "Advances in Prob,",
            "year": 1978
        },
        {
            "authors": [
                "Robert Kleinberg",
                "S. Matthew Weinberg"
            ],
            "title": "Matroid prophet inequalities",
            "venue": "In Proceedings of the 44th Symposium on Theory of Computing Conference,",
            "year": 2012
        },
        {
            "authors": [
                "Robert D. Kleinberg",
                "Bo Waggoner",
                "E. Glen Weyl"
            ],
            "title": "Descending price optimally coordinates search",
            "venue": "In Proceedings of the ACM Conference on Economics and Computation, EC,",
            "year": 2016
        },
        {
            "authors": [
                "Allen Liu",
                "Renato Paes Leme",
                "Martin P\u00e1l",
                "Jon Schneider",
                "Balasubramanian Sivan"
            ],
            "title": "Variable decomposition for prophet inequalities and optimal ordering",
            "venue": "In The 22nd ACM Conference on Economics and Computation, EC,",
            "year": 2021
        },
        {
            "authors": [
                "Renato Paes Leme",
                "Balasubramanian Sivan",
                "Yifeng Teng",
                "Pratik Worah"
            ],
            "title": "Pricing query complexity of revenue maximization",
            "venue": "In Proceedings of the 2023 ACM-SIAM Symposium on Discrete Algorithms,",
            "year": 2023
        },
        {
            "authors": [
                "Brendan Lucier"
            ],
            "title": "An economic view of prophet inequalities",
            "venue": "SIGecom Exch.,",
            "year": 2017
        },
        {
            "authors": [
                "Christos H. Papadimitriou",
                "Tristan Pollner",
                "Amin Saberi",
                "David Wajc"
            ],
            "title": "Online stochastic max-weight bipartite matching: Beyond prophet inequalities",
            "venue": "In The 22nd ACM Conference on Economics and Computation,",
            "year": 2021
        },
        {
            "authors": [
                "Tim Roughgarden"
            ],
            "title": "Twenty lectures on algorithmic game theory",
            "year": 2016
        },
        {
            "authors": [
                "Aviad Rubinstein",
                "Sahil Singla"
            ],
            "title": "Combinatorial prophet inequalities",
            "venue": "In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA,",
            "year": 2017
        },
        {
            "authors": [
                "Aviad Rubinstein"
            ],
            "title": "Beyond matroids: secretary problem and prophet inequality with general constraints",
            "venue": "In Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing,",
            "year": 2016
        },
        {
            "authors": [
                "Aviad Rubinstein",
                "Jack Z. Wang",
                "S. Matthew Weinberg"
            ],
            "title": "Optimal single-choice prophet inequalities from samples",
            "venue": "In 11th Innovations in Theoretical Computer Science Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Ester Samuel-Cahn"
            ],
            "title": "Comparison of threshold stop rules and maximum for independent nonnegative random variables",
            "venue": "Annals of Probability,",
            "year": 1984
        },
        {
            "authors": [
                "Sahil Singla"
            ],
            "title": "Combinatorial optimization under uncertainty: Probing and stopping-time algorithms",
            "venue": "Unpublished doctoral dissertation,",
            "year": 2018
        },
        {
            "authors": [
                "Sahil Singla"
            ],
            "title": "The price of information in combinatorial optimization",
            "venue": "In Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms",
            "year": 2018
        },
        {
            "authors": [
                "Aleksandrs Slivkins"
            ],
            "title": "Introduction to multi-armed bandits",
            "venue": "Foundations and Trends in Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Danny Segev",
                "Sahil Singla"
            ],
            "title": "Efficient approximation schemes for stochastic probing and prophet problems",
            "venue": "In The 22nd ACM Conference on Economics and Computation,",
            "year": 2021
        },
        {
            "authors": [
                "Martin L. Weitzman"
            ],
            "title": "Optimal search for the best alternative",
            "venue": "Econometrica: Journal of the Econometric Society,",
            "year": 1979
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n21 1.\n08 58\n6v 2\n[ cs\n.D S]\n7 D\nIn the Multi-Armed Bandits model we interact with n unknown distributions over T rounds: in round t we play a policy x(t) and only receive the value of x(t) as feedback. The goal is to minimize the regret, which is the difference over T rounds in the total value of the optimal algorithm that knows the distributions vs. the total value of our algorithm that learns the distributions from the limited feedback. Our main results give near-optimal O\u0303 ( poly(n) \u221a T ) total regret algorithms for both Prophet Inequality and Pandora\u2019s Box.\nOur proofs proceed by maintaining confidence intervals on the unknown indices of the optimal policy. The exploration-exploitation tradeoff prevents us from directly refining these confidence intervals, so the main technique is to design a regret upper bound function that is learnable while playing low-regret Bandit policies.\n\u2217(gatmiry@mit.edu) Electrical Engineering and Computer Science, Massachusetts Institute of Technology \u2020(thomas.kesselheim@uni-bonn.de) Institute of Computer Science and Lamarr Institute for Machine Learning and Artificial Intelligence, University of Bonn \u2021(ssingla@gatech.edu) School of Computer Science, Georgia Tech. Supported in part by NSF award CCF-2327010. \u00a7(ywang3782@gatech.edu) School of Computer Science, Georgia Tech. Supported in part by NSF award CCF2327010.\nContents"
        },
        {
            "heading": "1 Introduction 1",
            "text": "1.1 Prophet Inequality under Bandit Feedback . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n1.2 Pandora\u2019s Box under Bandit Feedback . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.3 High-Level Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.4 Further Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6"
        },
        {
            "heading": "2 Prophet Inequality and Pandora\u2019s Box for n = 2 6",
            "text": "2.1 Prophet Inequality via an Interval-Shrinking Algorithm . . . . . . . . . . . . . . . . . . . . . 7 2.2 Doubling Framework for Low-Regret Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . 10 2.3 Extending to Pandora\u2019s Box with a Fixed Order . . . . . . . . . . . . . . . . . . . . . . . . . 10"
        },
        {
            "heading": "3 Prophet Inequality for General n 13",
            "text": "3.1 Interval-Shrinking Algorithm for General n . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 3.2 Initialization and Putting Everything Together . . . . . . . . . . . . . . . . . . . . . . . . . . 18"
        },
        {
            "heading": "4 Pandora\u2019s Box for General n 20",
            "text": "4.1 High-Level Approach via Valid Policies. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n4.2 Step 1: Interval-Shrinking to Bound Moving Difference . . . . . . . . . . . . . . . . . . . . . . 22 4.3 Step 2: Updating Order Constraints to Bound Swapping Difference . . . . . . . . . . . . . . . 25 4.4 Converting our Policy to the Optimal Policy in Polynomial Steps . . . . . . . . . . . . . . . . 29 4.5 Putting Everything Together . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 4.6 Making the Algorithm Efficient . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n5 Lower Bounds 34\n5.1 \u2126( \u221a T ) Lower Bound for Stochastic Input . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n5.2 \u2126(T ) Lower Bound for Adversarial Input . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35"
        },
        {
            "heading": "A Basic Probabilistic Inequalities 39",
            "text": ""
        },
        {
            "heading": "B Missing Proofs from Section 2 40",
            "text": "B.1 Proof of Lemma 2.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 B.2 Missing Details of Pandora\u2019s Box Algorithm for n = 2 . . . . . . . . . . . . . . . . . . . . . . 40"
        },
        {
            "heading": "C Missing Proofs from Section 4 44",
            "text": "C.1 Proof of Lemma 4.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 C.2 Proof of Claim 4.6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 C.3 Proof of Claim 4.7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 C.4 Proof of Lemma 4.8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46"
        },
        {
            "heading": "1 Introduction",
            "text": "The field of Stochastic Optimization deals with optimization problems under uncertain inputs, and has had tremendous success since [Bel57]. A standard model is that the inputs are random variables that are drawn from known probability distributions. The goal is to design a policy (an adaptive algorithm) to optimize the expected objective function. Examples of such problems include Prophet Inequality [HKS07, CHMS10, KW12, Rub16], Pandora\u2019s Box [KWW16, Sin18b, GKS19], and Auction Design [Har22, Rou16]. Most prior works assume that the underlying distributions are known to the algorithm and the challenge is in computing an (approximately) optimal policy. However, in practical applications, the distributions are typically unknown and must be learned concurrently with decision-making.\nA foundational framework that examines stochastic problems with unknown distributions is the stochastic online learning model; see books [CBL06, BC12, Haz16]. Here, the learner interacts with the environment for T days. On each day t \u2208 [T ], the learner plays a certain policy a(t) \u2208 A, where A represents the set of all policies (actions/algorithms). The environment draws a sample X(t) \u223c D, where D indicates the environment\u2019s unknown underlying distribution, and then the learner receives a reward a(t)(X(t)) along with some \u201cfeedback\u201d. For a maximization problem, the goal of the online learning model is to approach the optimal policy with reward Opt := maxa\u2208A EX\u223cD[a(X)] while minimizing in expectation the total regret:\nT \u00b7Opt\u2212\u2211t\u2208[T ] a(t)(X(t)).\nThe best regret bound that can be achieved for an online learning problem highly depends on the feedback given to the algorithm. In the full-feedback model, the learner observes the complete sample X(t) as daily feedback. Since accessing the entire sample X(t) is often not feasible in many real-world applications, several partial feedback models have been considered. The most limiting of them is the bandit feedback model where the only feedback available is the reward a(t)(X(t)); see books [Sli19, LS20].\nInterestingly, in many online learning scenarios, limiting feedback does not excessively impair the regret bound. For instance, consider the classic Learning from Experts problem where the goal is to identify the optimal action. In this case, for a small action set, both full feedback and bandit feedback result in an optimal regret bound of \u0398( \u221a T ). This motivates us to address the following question for general online stochastic optimization problems:\nWhat is the minimum amount of feedback necessary to learn a stochastic optimization problem while maintaining a near-optimal regret bound in T as the full feedback model?\nIn addition to being an intellectually intriguing question, there are several other motivations for designing low regret algorithms that operate with limited feedback.\n\u2022 In numerous real-world scenarios, accessing the complete sample X(t) as feedback is infeasible. Furthermore, in order to safeguard data privacy to the greatest extent possible, it is advantageous to utilize minimal information in real-world online learning tasks. \u2022 An online learning algorithm that operates with less feedback is concurrently applicable to all partial feedback models that incorporate the required feedback. We can therefore obtain nearoptimal online learning algorithms that function uniformly across different feedback models.\nSpecifically, in this paper, we address the above question in the context of the fundamental Prophet Inequality and Pandora\u2019s Box problems, which have wide-ranging applications in areas such as Mechanism Design, Online Algorithms, Microeconomics, Operations Research, and Optimal Stop-\nping. Our main results imply near-optimal O\u0303 ( poly(n) \u221a T ) regret algorithms for both these problems under most limited bandit feedback, where O\u0303(\u00b7) hides logarithmic factors."
        },
        {
            "heading": "1.1 Prophet Inequality under Bandit Feedback",
            "text": "In the classical Optimal Stopping problem of Prophet Inequality [KS77, KS78, SC84], we are given distributions D1, . . . ,Dn of n independent random variables. The outcomes Xi \u223c Di for i \u2208 [n] are revealed one-by-one and we have to immediately select/discard Xi with the goal of maximizing the selected random variable in expectation. They have become popular in Algorithmic Game Theory in the last 15 years since they imply posted pricing mechanisms that are \u201csimple\u201d (and hence more practical) and approximately optimal; see related work in Section 1.4.\nThe optimal policy for Prophet Inequality is given by a simple (reverse) dynamic program: always select Xn on reaching it and select Xi for i < n if its value is more than the expected value of this optimal policy on Xi+1, . . . ,Xn. Thus, the optimal policy with expected value Opt can be thought of as a fixed-threshold policy where we select Xi iff Xi > \u03c4i for \u03c4i being the expected value of this policy after i. How to design this optimal policy for unknown distributions? (See Remark 1.2 on the \u201chindsight optimum\u201d benchmark.)\nAs a motivating example, consider a scenario where you want to sell a perishable item (e.g., cheese) in the market each day for the entire year. For simplicity, assume that there are 8 buyers, one arriving in each hour between 9 am to 5 pm. Your goal is to set price thresholds for each hour to maximize the total value. If the buyer value distributions are known, this can be modeled as a Prophet Inequality problem with n = 8 distributions. However, for unknown value distributions this becomes a repeated game with a fixed arrival order where on each day you play some price thresholds and obtain a value along with feedback. Next, we formally describe this repeated game.\nOnline Learning Prophet Inequality. In this problem the distributions D1, . . . ,Dn of Prophet Inequality are unknown to the algorithm in the beginning. We make the standard normalization assumption that each Di is supported on [0, 1]. Without this normalization, a non-trivial additive regret is not achievable. Now we play a T rounds repeated game1: in round t \u2208 [T ] we play a policy, which is a set of n thresholds (\u03c4\n(t) 1 , . . . , \u03c4 (t) n ), and receives as reward its value on freshly\ndrawn independent random variables X (t) 1 \u223c D1, . . . ,X (t) n \u223c Dn, i.e., the reward is X(t)Alg(t) where Alg(t) \u2208 [n] is the smallest index i with X(t)i > \u03c4 (t) i . The goal is to minimize the total regret :\nT \u00b7Opt\u2212E [\u2211T\nt=1 X (t) Alg(t)\n] .\nSince per-round reward is bounded by 1, the goal is to get o(T ) regret. Moreover, standard examples show that every algorithm incurs \u2126( \u221a T ) regret; see Section 5.\nAn important question is what amount of feedback the algorithm receives after a round. One might consider a full-feedback setting, where after each round t the algorithm gets to know the entire sample X (t) 1 , . . . ,X (t) n as feedback, which could be used to update beliefs regarding the distributions\nD1, . . . ,Dn. Here it is easy to design an O\u0303 ( poly(n) \u221a T ) regret algorithm. This is because after discretization, we may assume that the there are only T candidate thresholds for each Xi, so there are only T n candidate policies. Now the classical multiplicative weights algorithm [AHK12] implies that the regret is O (\u221a T log(#policies) ) = O\u0303 ( poly(n) \u221a T ) . Although this na\u0308\u0131ve algorithm is not polytime, a recent work of [GHTZ21] on O(n/\u01eb2) sample complexity for prophet inequality can\n1We will always assume T \u2265 n since otherwise getting an O(poly(n)) regret algorithm is trivial.\nbe interpreted as giving a polytime O\u0303(poly(n) \u221a T ) regret algorithm under full-feedback2. These results, however, do not extend to bandit feedback, where the algorithm does not see the entire sample.\nBandit Feedback. In many applications, it is unreasonable to assume that the algorithm gets the entire sample X (t) 1 , . . . ,X (t) n . For instance, in the above scenario of selling a perishable item, we may only see the winning bid (e.g., if you don\u2019t run the shop and delegate someone else to sell the item at the given price thresholds). There are several reasonable partial feedback models, namely:\n(a) We see X (t) 1 , . . . ,X (t) Alg(t) but not X (t) Alg(t)+1, . . . ,X (t) n , meaning that we do not observe the\nsequence after it has been stopped. (b) We see the index Alg(t) and the value XAlg(t) that we select but no other Xi. (c) We only see the value of XAlg(t) that we select and not even the index Alg(t).\nWhat is the least amount of feedback needed to obtain O\u0303(poly(n) \u221a T ) regret?\nOur first main result is that even with the most restrictive feedback (c), it is possible to obtain O\u0303(poly(n) \u221a T ) regret. Thus, the same bounds also hold under (a) and (b). Note that these bounds are almost optimal because standard examples show that even with full feedback every algorithm incurs \u2126( \u221a T ) regret (see Section 5). Theorem 1.1. There is a polytime algorithm with O(n3 \u221a T log T ) regret for the Bandit Prophet Inequality problem where we only receive the selected value as the feedback.\n(We remark that it is possible to improve the n3 factor in this result but we do not optimize it to keep the presentation cleaner.)\nTheorem 1.1 may come as a surprise since there are several stochastic problems that admit O(poly(n)/\u01eb2) sample complexity but do not admit O\u0303 ( poly(n) \u221a T ) regret bandit algorithms. Indeed, a close variant of prophet inequality is sequential posted pricing. Here, the reward is defined as the revenue, i.e., it is the threshold itself if a random variable crosses it rather than the value of the random variable (welfare) as in prophet inequality. It is easy to show that sequential posted pricing has O(1/\u01eb2) sample complexity [GHTZ21], but even for n = 1 every bandit algorithm incurs \u2126(T 2/3) regret [LSTW23].\nOne might wonder whether O\u0303 ( poly(n) \u221a T ) regret in Theorem 1.1 holds even for adversarial\nonline learning, i.e., where X (t) 1 , . . . ,X (t) n are chosen by an adversary in each round t and we compete against the optimal fixed-threshold policy in hindsight. In Section 5 we prove that this is impossible since every online learning algorithm incurs \u2126(T ) regret for adversarial inputs, even under full-feedback.\nRemark 1.2 (Hindsight Optimum). There is a lot of work on Prophet Inequality (with Samples) where the benchmark is the expected hindsight optimum E [maxXi]; see Section 1.4. However, we will be interested in the more realistic benchmark of the optimal policy, or in other words the optimal solution to the underlying MDP, which is standard in stochastic optimization. Firstly, comparing to the hindsight optimum does not make sense for most stochastic problems, including Pandora\u2019s Box, since it cannot be achieved even approximately. Secondly, optimal policy gives us a much more fine-grained picture than comparing to the offline optimum. For instance, it is known that a single sample suffices to get the optimal 2-competitive guarantee compared to the offline optimum [RWW20]. This might give the impression that there is nothing to be learned about the\n2Their results are in the PAC model for \u201cstrongly monotone\u201d stochastic problems. They immediately imply O\u0303( \u221a nT ) regret under full-feedback using the standard doubling-trick.\ndistributions for Prophet Inequality and sublinear regrets are impossible. However, this is incorrect as Theorem 1.1 obtains sublinear regret bounds w.r.t. the optimal policy."
        },
        {
            "heading": "1.2 Pandora\u2019s Box under Bandit Feedback",
            "text": "The Pandora\u2019s Box problem was introduced by Weitzman, motivated by Economic search applications [Wei79]. For example, how should a large organization decide between competing research technologies to produce some commodity. In the classical setting, we are given distributions D1, . . . ,Dn of n independent random variables. The outcome Xi \u223c Di for i \u2208 [n] can be obtained by the algorithm by paying a known inspection cost ci. The goal is to find a policy to adaptively inspect a subset S \u2286 [n] of the random variables to maximize utility : E [ maxi\u2208S Xi \u2212 \u2211 i\u2208S ci ] . Note that unlike the Prophet Inequality, we may now inspect the random variables in any order by paying a cost and we don\u2019t have to immediately accept/reject Xi.\nEven though Pandora\u2019s Box has an exponential state space, [Wei79] showed a simple optimal policy where we inspect in a fixed order (using \u201cindices\u201d) along with a stopping rule. We study this problem in the Online Learning model where the distributions Di supported on [0, 1] are unknownbut-fixed. Without loss of generality, we will assume that the deterministic costs ci \u2208 [0, 1] are known to the algorithm3.\nFormally, in Online Learning for Pandora\u2019s Box we play a T rounds repeated game where in round t \u2208 [T ] we play a policy a(t), which is an order of inspection along with a stopping rule. As reward, we receive our utility (value minus total inspection cost) on freshly drawn independent random variables X (t) 1 \u223c D1, . . . ,X (t) n \u223c Dn. The goal is to minimize the total regret, which is the difference over T rounds in the expected utility of the optimal algorithm that knows the underlying distributions and the total utility of our algorithm.\nIn the full-feedback setting the algorithm receives the entire sample X (t) 1 , . . . ,X (t) n as feedback\nin each round. Here, it is again easy to design an O\u0303 ( poly(n) \u221a T ) regret polytime algorithm relying on the results in [GHTZ21, FL20]. But these results do not extend to partial feedback.\nThere are again multiple ways of defining partial feedback. E.g., we could see the values of all Xi for i \u2208 S, meaning that we get to see the values of the inspected random variables. Indeed, our results again apply to the most restrictive form of partial feedback: We only see the total utility of a policy and not even the indices of inspected random variables or any of their values. Theorem 1.3. There is a polytime algorithm with O(n5.5 \u221a T log T ) regret for the Bandit Pandora\u2019s Box problem where we only receive utility (selected value minus total cost) as feedback.\nAgain, standard examples show that every algorithm incurs \u2126( \u221a nT ) regret even will full feedback; see Section 5. Furthermore, we will prove in Section 5 that Theorem 1.3 cannot hold for adversarial online learning where X (t) 1 , . . . ,X (t) n are chosen by an adversary: every online learning algorithm incurs \u2126(T ) regret for adversarial inputs, even under full-feedback."
        },
        {
            "heading": "1.3 High-Level Techniques",
            "text": "Let\u2019s consider the general Prophet Inequality problem or the subproblem of Pandora\u2019s Box where the optimal order is given. In both cases, a policy is described by n thresholds \u03c41, . . . , \u03c4n \u2208 [0, 1],\n3If the costs ci are unknown but fixed then the problem trivially reduces to the case of known costs. This is because we could simply open each box once without keeping the prize inside and receive as feedback the cost ci.\ndefining when to stop inspecting. It would be tempting to apply standard multi-armed bandit algorithms to maximize the expected reward over [0, 1]n. However, such approaches are bound to fail because the expected reward is not even continuous4, let alone convex or Lipschitz. Discretizing the action space and applying a bandit algorithm only leads to \u2126(T 2/3) regret. Another reasonable approach is to try to learn the distributions Di. However, recall that we only get feedback regarding the overall reward of a policy and do not see which Xi is selected. It is possible to obtain samples from each Xi by considering policies that ignore all other boxes; however, such algorithms that use separate exploration and exploitation also have \u2126(T 2/3) regret.\nOur algorithms combine exploration and exploitation. We maintain confidence intervals [\u2113i, ui] for i \u2208 [n] satisfying w.h.p. that the optimal thresholds \u03c4\u2217i \u2208 [\u2113i, ui]. The crucial difference from UCB-style algorithms [ACBF02] is that we don\u2019t get unbiased samples with low regret, so we cannot maintain or play upper confidences. Instead, we need a \u201crefinement\u201d procedure to shrink the intervals while ensuring that the regret during the refinement is bounded.\nMore precisely, our algorithm works in O(log T ) phases. In each phase, we start with confidence intervals [\u2113i, ui] that satisfy: (i) \u03c4 \u2217 i \u2208 [\u2113i, ui] and (ii) playing any thresholds within the confidence intervals incur at most some \u01eb regret. During the phase, we refine the confidence interval to [\u2113\u2032i, u \u2032 i] while only playing thresholds within our original confidence intervals, so that we don\u2019t incur much regret. We will show that the new confidence intervals satisfy that \u03c4\u2217i \u2208 [\u2113\u2032i, u\u2032i] and that playing any thresholds within [\u2113\u20321, u \u2032 1], . . . , [\u2113 \u2032 n, u \u2032 n] incur at most \u01eb 2 regret. Thus, the regret bound goes down by a constant factor in each phase.\nBounding Function to Refine for n = 2. To illustrate the idea behind a refinement phase, let\u2019s discuss the case of n = 2; see Section 2 for more technical details. In this case, there is only one confidence interval [\u2113, u] that we have to refine. Our idea is to define a \u201cbounding function\u201d \u03b4(\u00b7) such that the expected regret in a single round when using threshold \u03c4 \u2208 [\u2113, u] is bounded by |\u03b4(\u03c4)|. Ideally, we would like to choose the optimal threshold \u03c4\u2217 for which \u03b4(\u03c4\u2217) = 0. However, this requires the knowledge of \u03b4, which we don\u2019t have since the distributions are unknown. Instead, we compute an estimate \u03b4\u0302 of \u03b4 and construct the new confidence interval [\u2113\u2032, u\u2032] to include all \u03c4 for which |\u03b4\u0302(\u03c4)| is small. The main technical difficulty is to obtain \u03b4\u0302 while only playing low-regret policies. We achieve this by choosing \u03b4 such that \u03b4\u0302 can be obtained by using only the estimates F\u0302i of the CDF and the empirical average rewards when choosing the boundaries of the confidence interval as thresholds. Note that we do not make any statements about the width of the confidence interval; we only ensure that the regret is bounded when choosing any threshold inside the confidence interval.\nProphet Inequality for General n. In the case of general n, each refinement phase updates the confidence intervals from the last random variable Xn to the first one X1. To refine confidence interval [\u2113i, ui], we use our algorithm for the n = 2 case as a subroutine, i.e., we play \u2113i and ui sufficiently many times keeping the other thresholds fixed. However, there are several challenges in this approach. The first important one is that the probability of reaching Xi will change depending on which thresholds are applied before it. We deal with this issue by always using thresholds from our confidence intervals that maximize the probability of reaching Xi. Another important challenge while refining [\u2113i, ui] is that the current choice of thresholds for Xi+1, . . . ,Xn is not optimal, so we maybe learning a threshold different from \u03c4\u2217i . We handle this issue by choosing the other thresholds in a way that they only improve from phase to phase. We then leave some space in the confidence intervals to accommodate for the improvements in later phases.\n4For example, consider the Prophet Inequality instance in which X1 is a distribution that returns 1 4 w.p. 1 2 and\n3 4 otherwise, while X2 is a distribution that always returns 1 2 . The reward of this example is a piece-wise constant function: When \u03c4 < 1 4 or \u03c4 \u2265 3 4 , the expected reward is 1 2 . When 1 4 \u2264 \u03c4 < 3 4 , the expected reward is 5 8 .\nPandora\u2019s Box for General n. We still maintain confidence and refine them using ideas similar to Prophet Inequality for general n. The main additional challenge arising in Pandora\u2019s box is that the inspection ordering is not fixed. The optimal order is given by ordering the random variables by decreasing thresholds. However, there might be multiple orders consistent with our confidence intervals. Therefore, we keep a set S of constraints corresponding to a directed acyclic graph on the variables, where an edge from Xi to Xj means that Xi comes before Xj in the optimal order. We update this set by consider pairwise swaps. Then, during refinement of confidence interval [\u2113i, ui], we choose an inspection order satisfying these constraints while (approximately) maximizing a difference of products objective."
        },
        {
            "heading": "1.4 Further Related Work",
            "text": "There is a long line of work on both Prophet Inequality (PI) and Pandora\u2019s Box (PB), so we only discuss the most relevant papers. For more references, see [Luc17, Sin18a]. Both PI and PB are classical single-item selection problems, but were popularized in TCS in [HKS07] and [KWW16], respectively, due to their applications in mechanism design. Extensions of these problems to combinatorial settings have been studied in [CHMS10, KW12, FGL15, FSZ16, Rub16, RS17, EFGT20] and in [KWW16, Sin18b, GKS19, GJSS19, FTW+21], respectively. Although the optimal policy for PI with known distributions is a simple dynamic program, designing optimal policies for free-order or in combinatorial PI settings is challenging. Some recent works designing approximately-optimal policies are [ANSS19, PPSW21, SS21, LLP+21, BDL22].\nStarting with Azar, Kleinberg, and Weinberg [AKW14], there is a lot of work on PI-withSamples where the distributions are unknown but the algorithm has sample access to it [CDFS19, RWW20, GHTZ21, CDF+22]. These works, however, compete against the benchmark of expected hindsight optimum, so lose at least a multiplicative factor of 1/2 due the classical single-item PI and do not admit sublinear regret algorithms.\nThe field of Online Learning under both full- and bandit-feedback is well-established; see books [CBL06, BC12, Haz16, Sli19, LS20]. Most of the initial works focused on obtaining sublinear regret for single-stage problems (e.g., choosing the reward maximizing arm). The last decade has seen progress on learning multi-stage policies for tabular MDPs under bandit feedback; see [LS20, Chapter 38]. However, these algorithms have a regret that is polynomial in the state space, so they do not apply to PI and PB that have large MDPs.\nFinally, there is some recent work at the intersection of Online Learning and Prophet Inequality/Pandora\u2019s Box [EHLM19, ACG+22, GT22]. These models are significantly different from ours, so do not apply to our problems. The closest one is [GT22], where the authors consider Pandora\u2019s Box under partial feedback (akin to model (a)), but for adversarial inputs (i.e., no underlying distributions). They obtain O(1)-competitive algorithms and leave open whether sublinear regrets are possible [Ger22]. Our lower bounds in Section 5.2 resolve this question by showing that sublinear regrets are impossible for adversarial inputs (even under full feedback), and one has to lose a multiplicative factor in the approximation."
        },
        {
            "heading": "2 Prophet Inequality and Pandora\u2019s Box for n = 2",
            "text": "In this section, we give O( \u221a T log T ) regret algorithms for both Bandit Prophet Inequality and Bandit Pandora\u2019s Box problems with n = 2 distributions. We discuss this special case of Theorem 1.1 before since it\u2019s already non-trivial and showcases one of our main ideas of designing a\nregret bounding function that is learnable while playing low-regret Bandit policies.\nOur algorithms run in O(log T ) phases, where the number of rounds doubles each phase. Starting with an initial confidence interval containing the optimal threshold \u03c4\u2217, the goal of each phase is to refine this interval such that the one-round regret drops by a constant factor for the next phase. In Section 2.1 we discuss each phase\u2019s algorithm for Prophet Inequality with n = 2. In Section 2.2 we give a generic doubling framework that combines all phases to prove total regret bounds. Finally, in Section 2.3 we extend these ideas to Pandora\u2019s Box with n = 2."
        },
        {
            "heading": "2.1 Prophet Inequality via an Interval-Shrinking Algorithm",
            "text": "We first introduce the setting of the Bandit Prophet Inequality Problem with two distributions. Let D1,D2 denote the two unknown distributions over [0, 1] with cdfs F1, F2 and densities f1, f2. Consider a T rounds game where in each round t we play a threshold \u03c4 (t) \u2208 [0, 1] and receive as feedback the following reward:\n\u2022 Independently draw X (t) 1 from D1. If X (t) 1 \u2265 \u03c4 (t), return X (t) 1 as the reward. \u2022 Otherwise, independently draw X (t) 2 from D2 and return it as the reward.\nThe only feedback we receive is the reward, and not even which random variable gets selected.\nIf the distributions are known then the optimal policy is to play \u03c4\u2217 := E [X2] in each round. For \u03c4 \u2208 [0, 1], let R(\u03c4) be the expected reward of playing one round with threshold \u03c4 , i.e.,\nR(\u03c4) := F1(\u03c4) \u00b7E [X2] + \u222b 1\n\u03c4 x \u00b7 f1(x)dx = 1 + F1(\u03c4)(E [X2]\u2212 \u03c4)\u2212\n\u222b 1\n\u03c4 F1(x)dx, (1)\nwhere the second equality uses integration by parts. The total regret is T \u00b7 R(\u03c4\u2217)\u2212\u2211Tt=1 R(\u03c4 (t)). Initialization. For the initialization, we get \u0398( \u221a T log T ) samples from both D1 and D2 by playing\n\u03c4 = 0 and \u03c4 = 1, respectively. This incurs \u0398( \u221a T log T ) regret since each round incurs at most 1 regret. The following simple lemma uses the samples to obtain initial distribution estimates.\nLemma 2.1. After getting C \u00b7 sqrtT log T samples from D1 and D2, with probability 1 \u2212 T\u221210 we can:\n\u2022 Calculate F\u03021(x) such that |F\u03021(x)\u2212 F1(x)| \u2264 T\u2212 1 4 for all x \u2208 [0, 1] simultaneously. \u2022 Calculate \u2113 and u such that u\u2212 \u2113 \u2264 T\u2212 14 and E [X2] \u2208 [\u2113, u].\nProof. The first statement follows the DKW inequality (Theorem A.3). After taking N = C \u00b7\u221a T log T samples, the probability that \u2203x s.t. |F\u03021(x)\u2212F1(x)| > \u03b5 = T\u2212 1 4 is at most 2 exp(\u22122N\u03b52) = 2T\u22122C < T\u22122C+1. So, the first statement holds with probability at least 1\u2212 T\u221211 when C > 10. The second statement follows the Hoeffding\u2019s Inequality (Theorem A.1). After taking N = C \u00b7 \u221a T log T samples, let \u00b5 be the average reward. Let \u2113 = \u00b5 \u2212 \u03b5 and u = \u00b5 + \u03b5 for \u03b5 = 12T\u2212 1 4 . Then, u \u2212 \u2113 \u2264 T\u2212 14 by definition. Since the reward of each sample in inside [0, 1], by Hoeffding\u2019s Inequality the probability that |\u00b5\u2212E [X2] | > \u03b5 is bounded by 2 exp(\u22122N\u03b52) = 2T\u22122C < T\u22122C+1. So, the second statement holds with probability at least 1 \u2212 T\u221211 when C > 10. Taking a union bound for two statements gives the desired lemma.\nNext we discuss our core algorithm.\nInterval-Shrinking Algorithm. Starting with an initial confidence interval containing \u03c4\u2217 = E [X2], our Interval-Shrinking algorithm (Algorithm 1) runs for \u0398( log T \u01eb2 ) rounds and outputs a\nrefined confidence interval. In the following lemma, we will show that this refined interval still contains \u03c4\u2217 and that the regret of playing any \u03c4 inside this refined interval is bounded by O(\u01eb).\nAlgorithm 1: Interval-Shrinking Algorithm for Prophet Inequality\nInput: Interval [\u2113, u], approximate cdf F\u03021(x), and accuracy \u01eb. 1 Run C \u00b7 log T\u01eb2 rounds with \u03c4 = \u2113. Let R\u0302\u2113 be the average reward. 2 Run C \u00b7 log T\n\u01eb2 rounds with \u03c4 = u. Let R\u0302u be the average reward.\n3 For \u03c4 \u2208 [\u2113, u], define \u2206\u0302(\u03c4) := F\u03021(u)(\u03c4 \u2212 u)\u2212 F\u03021(\u2113)(\u03c4 \u2212 \u2113) + \u222b u \u2113 F\u03021(x)dx. 4 For \u03c4 \u2208 [\u2113, u], define \u03b4\u0302(\u03c4) := \u2206\u0302(\u03c4)\u2212 (R\u0302u \u2212 R\u0302\u2113). 5 Let \u2113\u2032 := min{\u03c4 \u2208 [\u2113, u] s.t. \u03b4\u0302(\u03c4) \u2265 \u22125\u01eb} and let u\u2032 := max{\u03c4 \u2208 [\u2113, u] s.t. \u03b4\u0302(\u03c4) \u2264 5\u01eb}. Output: [\u2113\u2032, u\u2032]\nLemma 2.2. Suppose we are given:\n\u2022 Initial interval [\u2113, u] of length u\u2212 \u2113 \u2264 T\u2212 14 and satisfying \u03c4\u2217 \u2208 [\u2113, u]. \u2022 Distribution estimate F\u03021(x) satisfying |F1(x)\u2212 F\u03021(x)| \u2264 T\u2212 1 4 for all x \u2208 [0, 1] simultaneously. Then, for \u01eb > T\u2212 1 2 Algorithm 1 runs thresholds inside [\u2113, u] for at most 1000 \u00b7 log T\u01eb2 rounds, and outputs a sub-interval [\u2113\u2032, u\u2032] \u2286 [\u2113, u] satisfying with probability 1\u2212 T\u221210 the following statements: 1. \u03c4\u2217 \u2208 [\u2113\u2032, u\u2032]. 2. For every \u03c4 \u2208 [\u2113\u2032, u\u2032] the expected one-round regret of playing \u03c4 is at most 10\u01eb.\nProof Overview of Lemma 2.2. The main idea is to define a bounding function\n\u03b4(\u03c4) := (F1(u)\u2212 F1(\u2113)) \u00b7 (\u03c4 \u2212 \u03c4\u2217).\nAs we show in Claim 2.3 below, this function satisfies R(\u03c4\u2217) \u2212 R(\u03c4) \u2264 |\u03b4(\u03c4)| for all \u03c4 \u2208 [\u2113, u], i.e., |\u03b4(\u03c4)| is an upper bound on the one-round regret when choosing \u03c4 instead of \u03c4\u2217. So, ideally, we would like to choose \u03c4 that minimizes |\u03b4(\u03c4)|. However, we do not know \u03b4(\u03c4). Therefore, we derive an estimate \u03b4\u0302(\u03c4) for all \u03c4 \u2208 [\u2113, u] and discard \u03c4 for which |\u03b4\u0302(\u03c4)| is too large because these cannot be the minimizers.\nIn order to estimate \u03b4(\u03c4), we rewrite it in a different way as sum of terms that can be estimated well. First, consider the difference in expected rewards when choosing thresholds u and \u2113, i.e., R(u)\u2212R(\u2113) = (F1(u)\u2212F1(\u2113))\u03c4\u2217\u2212 \u222b u\n\u2113 xf1(x)dx = F1(u) \u00b7 (\u03c4\u2217\u2212u)\u2212F1(\u2113) \u00b7 (\u03c4\u2217\u2212 \u2113)+\n\u222b u\n\u2113 F1(x)dx,\nwhere we used integration by parts. Adding this with \u03b4(\u03c4) gives \u03b4(\u03c4) + (R(u)\u2212R(\u2113)) equals\nF1(u)(\u03c4 \u2212 u)\u2212 F1(\u2113)(\u03c4 \u2212 \u2113) + \u222b u\n\u2113 F1(x)dx =: \u2206(\u03c4), (2)\nwhich gives an alternate way of expressing \u03b4(\u03c4) = \u2206(\u03c4) \u2212 (R(u) \u2212 R(\u2113)). (Another way of understanding the definition of \u2206(\u03c4) is that it represents the difference of playing thresholds u and \u2113, assuming that E [X2] = \u03c4 .) So, we define the estimate\n\u03b4\u0302(\u03c4) := \u2206\u0302(\u03c4)\u2212 (R\u0302u \u2212 R\u0302\u2113),\nwhere \u2206\u0302 uses the estimate F\u03021 instead of F1 in (2) and to estimate R\u0302u and R\u0302\u2113 we use empirical averages obtained in the current phase. The advantage is that besides the coarse knowledge of\nF\u03021 we assumed to be given, we only need to choose thresholds from within our current confidence interval to obtain \u03b4\u0302. Claim 2.4 will show that \u03b4\u0302(\u03c4) estimates \u03b4(\u03c4) within an additive error of O(\u01eb).\nCompleting the Proof of Lemma 2.2. Now we complete the missing details. We first prove that |\u03b4(\u03c4)| gives an upper bound on one-round regret with threshold \u03c4 .\nClaim 2.3. If \u03c4, \u03c4\u2217 \u2208 [\u2113, u], then R(\u03c4\u2217)\u2212R(\u03c4) \u2264 |\u03b4(\u03c4)|.\nProof. Consider R(\u03c4\u2217) \u2212 R(\u03c4). The two settings are different only when X1 is between \u03c4\u2217 and \u03c4 , and the difference of the reward is bounded by |\u03c4\u2217 \u2212 \u03c4 |. Therefore, R(\u03c4\u2217) \u2212 R(\u03c4) \u2264 |\u03c4\u2217 \u2212 \u03c4 | \u00b7 |F1(\u03c4\u2217)\u2212F1(\u03c4)| \u2264 |\u03c4\u2217\u2212 \u03c4 | \u00b7 |F1(u)\u2212F1(\u2113)| = |\u03b4(\u03c4)|, where the second inequality uses \u03c4\u2217, \u03c4 \u2208 [\u2113, u] implies |F1(\u03c4)\u2212 F1(\u03c4\u2217)| \u2264 |F1(u)\u2212 F1(\u2113)|.\nNext, we prove that \u03b4\u0302(\u03c4) is a good estimate of \u03b4(\u03c4).\nClaim 2.4. In Algorithm 1, if the conditions in Lemma 2.2 hold then with probability 1\u2212T\u221210 we have |\u03b4\u0302(\u03c4)\u2212 \u03b4(\u03c4)| \u2264 5 \u00b7 \u01eb for all \u03c4 \u2208 [\u2113, u] simultaneously.\nProof. Recall that \u03b4(\u03c4) = \u2206(\u03c4)\u2212 ( R(u)\u2212R(\u2113) ) . We first bound the error |\u2206\u0302(\u03c4)\u2212\u2206(\u03c4)|. Notice,\n|\u2206\u0302(\u03c4)\u2212\u2206(\u03c4)| \u2264 |F1(u)\u2212 F\u03021(u)| \u00b7 |\u03c4 \u2212 u|+ |F1(\u2113)\u2212 F\u03021(\u2113)| \u00b7 |\u03c4 \u2212 \u2113|+ \u222b u\n\u2113 |F1(x)\u2212 F\u03021(x)|dx.\nThe main observation is that all three terms on the right-hand-side can be bounded by T\u2212 1 2 since |F1(x)\u2212 F\u03021(x)| \u2264 T\u2212 1 4 and u\u2212 \u2113 \u2264 T\u2212 14 . Hence, |\u2206\u0302(\u03c4)\u2212\u2206(\u03c4)| \u2264 3T\u2212 12 \u2264 3\u01eb.\nNext, we bound the errors for |R\u0302\u2113 \u2212 R(\u2113)| and for |R\u0302u \u2212 R(u)|. For |R\u0302\u2113 \u2212 R(\u2113)|, notice that R\u0302\u2113 is an estimate of R(\u2113) with N = C \u00b7 log T\u01eb2 samples. Since the reward of each sample is in [0, 1], by Hoeffding\u2019s Inequality (Theorem A.1) the probability that |R\u0302\u2113 \u2212 R(\u2113)| > \u01eb is bounded by 2 exp(\u22122N\u01eb2) = 2T\u22122C . Then, |R\u0302\u2113 \u2212 R(\u2113)| \u2264 \u01eb holds with probability at least 1 \u2212 T\u221211 when C > 10. The error bound for |R\u0302u \u2212 R(u)| is identical. Taking a union bound for two error for |R\u0302\u2113\u2212R(\u2113)| and for |R\u0302u\u2212R(u)|, and then summing them with the error for |\u2206\u0302(\u03c4)\u2212\u2206(\u03c4)| completes the proof.\nNow, we are ready to prove Lemma 2.2.\nProof of Lemma 2.2. We will assume that |\u03b4\u0302(\u03c4)\u2212\u03b4(\u03c4)| \u2264 5\u01eb, which is true with probability 1\u2212T\u221210 by Claim 2.4.\nObserve that \u03b4\u0302(\u03c4) is a monotone increasing function because \u03b4\u0302\u2032(\u03c4) = \u2206\u0302\u2032(\u03c4) = F\u03021(u)\u2212 F\u03021(\u2113) \u2265 0. Therefore, according to the definition of \u2113\u2032 and u\u2032, we have [\u2113\u2032, u\u2032] = {\u03c4 \u2208 [\u2113, u] : |\u03b4\u0302(\u03c4)| \u2264 5\u01eb}. Now, we can use this property to prove the two statements of this lemma separately.\nFor Statement 1, notice that \u03b4(\u03c4\u2217) = 0. Claim 2.4 gives |\u03b4\u0302(\u03c4\u2217)| \u2264 5\u01eb. Then, since \u03c4\u2217 \u2208 [\u2113, u] and |\u03b4\u0302(\u03c4\u2217)| \u2264 5\u01eb, we must have \u03c4\u2217 \u2208 [\u2113\u2032, u\u2032] as [\u2113\u2032, u\u2032] = {\u03c4 \u2208 [\u2113, u] : |\u03b4\u0302(\u03c4)| \u2264 5\u01eb}.\nNext, we prove Statement 2. By Claim 2.3, it suffices to bound |\u03b4(\u03c4)| for all \u03c4 \u2208 [\u2113\u2032, u\u2032]. By Claim 2.4, we have w.h.p. for all \u03c4 \u2208 [\u2113\u2032, u\u2032] that |\u03b4(\u03c4)| \u2264 |\u03b4\u0302(\u03c4)|+5\u01eb \u2264 10\u01eb, where the last inequality uses the definition of \u2113\u2032 and u\u2032."
        },
        {
            "heading": "2.2 Doubling Framework for Low-Regret Algorithms",
            "text": "In this section we show how to run Algorithm 1 for multiple phases with a doubling trick to get O( \u221a T log T ) regret. Instead of directly proving the regret bound for Prophet Inequality with n = 2, we first give a general doubling framework that will later be useful for Prophet Inequality and Pandora\u2019s Box problems with n random variables:\nLemma 2.5. Consider an online learning problem with size n. Assume the one-round regret for every possible action is bounded by 1. Suppose there exists an action set-updating algorithm Alg satisfying: Given accuracy \u01eb and action set A, algorithm Alg runs \u0398(n\n\u03b1 log T \u01eb2\n) rounds in A and outputs A\u2032 \u2286 A satisfying the following with probability 1\u2212 T\u221210:\n\u2022 The optimal action in A belongs to A\u2032. \u2022 For a \u2208 A\u2032, the one-round regret of playing a is bounded by \u01eb.\nThen, with probability 1\u2212 T\u22129 the regret of Algorithm 2 is O(n\u03b1/2 \u221a T log T ).\nAlgorithm 2: General Doubling Algorithm\nInput: Time horizon T , problem size n, action space A, algorithm Alg, and parameter \u03b1. 1 Let i = 1, \u01eb1 = 1, A1 = A 2 while \u01ebi > n\u03b1/2 log T\u221a\nT do\n3 Call Alg with input \u01ebi and Ai, and get output Ai+1 4 \u01ebi+1 \u2190 \u01ebi2 5 i\u2190 i+ 1 6 Run a \u2208 Ai for the remaining rounds.\nThe proof of the lemma uses simple counting; see Appendix B.\nBased on Lemma 2.5, we can immediately give the Bandit Prophet Inequality regret bound.\nTheorem 2.6. There exists an algorithm that achieves O( \u221a T \u00b7log T ) regret with probability 1\u2212T\u22129 for Bandit Prophet Inequality problem with two distributions. Proof. The initialization runs O( \u221a T log T ) rounds, so the regret is O( \u221a T log T ). For the following interval shrinking procedure, Algorithm 1 matches the algorithm Alg described in Lemma 2.5 with \u03b1 = 0. Therefore, applying Lemma 2.5 completes the proof."
        },
        {
            "heading": "2.3 Extending to Pandora\u2019s Box with a Fixed Order",
            "text": "In order to extend the approach to Pandora\u2019s Box, in this section we consider a simplified problem with a fixed box order. There are two boxes taking values in [0, 1] from unknown distributions D1,D2 with cdfs F1, F2 and densities f1, f2. The boxes have known costs c1, c2 \u2208 [0, 1]. We assume that we always pay c1 to observe X1 (i.e., E [X1] > c1), and then decide whether to observe X2 by paying c2. Indeed, it might be better to open the second box before the first box or not to open any box. We make these simplifying assumptions in this section to make the presentation cleaner. Generally, determining an approximately optimal order will be one of the main technical challenges that we will need to handle for general n in Section 4.\nFormally, consider a T rounds game where in each round t we play a threshold \u03c4 (t) \u2208 [0, 1] and receive as feedback the following utility:\n\u2022 Independently draw X (t) 1 from D1. If X (t) 1 \u2265 \u03c4 (t), we stop and receive X (t) 1 \u2212 c1 as the utility. \u2022 Otherwise, we pay c2 to see X (t) 2 drawn independently from D2, and receive max{X1,X2} \u2212\n(c1 + c2) as utility.\nThe only feedback we receive is the utility, and not even which random variable gets selected.\nTo see the optimal policy, define a gain function g(v) := E [max{0,X2 \u2212 v} \u2212 c2] to represent the expected additional utility from opening X2 assuming we already have X1 = v, i.e.,\ng(v) = \u2212 c2 + \u222b 1\nv (x\u2212 v)f2(x)dx = \u2212 c2 + (1\u2212 v)\u2212\n\u222b 1\nv F2(x)dx. (3)\nThe optimal threshold (Weitzman\u2019s reservation value) \u03c4\u2217 is now the solution to g(\u03c4\u2217) = 0, i.e., E [max{X2 \u2212 \u03c4\u2217, 0}] = c2. Since our algorithm does not know F2(x) but only an approximate distribution F\u03022(x), we get an estimate g\u0302(v) of g(v) by replacing F2(x) with F\u03022(x) in (3).\nFor \u03c4 \u2208 [0, 1], let reward function R(\u03c4) denote the expected reward of playing \u03c4 . With the definition of gain function g(v) and linearity of expectation, we can write\nR(\u03c4) := \u2212 c1 +E [X1] + \u222b \u03c4\n0 f1(x)g(x)dx.\nThe total regret of our algorithm is now defined as T \u00b7R(\u03c4\u2217)\u2212\u2211Tt=1 R(\u03c4 (t)).\nInterval-Shrinking Algorithm. Starting with an initial confidence interval [\u2113, u] containing \u03c4\u2217, we again design an Interval-Shrinking algorithm (Algorithm 3) that runs for \u0398( log T\u01eb2 ) rounds and outputs a refined confidence interval [\u2113\u2032, u\u2032]. We will show that this refined interval still contains \u03c4\u2217 and that the regret of playing any \u03c4 inside this refined interval is bounded by O(\u01eb). Now we give the algorithm and the theorem.\nAlgorithm 3: Interval-Shrinking Algorithm for Pandora\u2019s Box\nInput: Interval [\u2113, u], length m, and CDF estimates F\u03021(x), F\u03022(x). 1 Run C \u00b7 log T\n\u01eb2 rounds with \u03c4 = \u2113. Let R\u0302\u2113 be the average reward.\n2 Run C \u00b7 log T\u01eb2 rounds with \u03c4 = u. Let R\u0302u be the average reward. 3 For \u03c4 \u2208 [\u2113, u], define \u2206\u0302(\u03c4) := (g\u0302(u)\u2212 g\u0302(\u03c4))F\u03021(u)\u2212 (g\u0302(\u2113)\u2212 g\u0302(\u03c4))F\u03021(\u2113)\u2212 \u222b u \u2113 g\u0302 \u2032(x)F\u0302 (x)dx. 4 For \u03c4 \u2208 [\u2113, u], define \u03b4\u0302(\u03c4) := \u2206\u0302(\u03c4)\u2212 (R\u0302u \u2212 R\u0302\u2113). 5 Let \u2113\u2032 = min{\u03c4 \u2208 [\u2113, u] s.t. \u03b4\u0302(\u03c4) \u2265 \u22124\u01eb} and let u\u2032 = max{\u03c4 \u2208 [\u2113, u] s.t. \u03b4\u0302(\u03c4) \u2264 4\u01eb}. Output: [\u2113\u2032, u\u2032]\nLemma 2.7. Suppose we are given:\n\u2022 Initial interval [\u2113, u] satisfying \u03c4\u2217 \u2208 [\u2113, u], gain function |g(\u03c4)| \u2264 T\u2212 14 , and bounding function |\u03b4(\u03c4)| \u2264 16\u01eb where \u03b4 is defined in (4). \u2022 CDF estimate F\u03021(x) which is constructed via 1000 \u00b7 log T\u01eb new i.i.d. samples of X1. \u2022 CDF estimate F\u03022(x) which is constructed via 1000 \u00b7 log T\u01eb new i.i.d. samples of X2. Then, for \u01eb > T\u2212 1 2 , Algorithm 3 runs thresholds inside [\u2113, u] for no more than 10000 \u00b7 \u01eb\u22122 log T rounds and outputs with probability 1\u2212 T\u221210 a sub-interval [\u2113\u2032, u\u2032] \u2286 [\u2113, u] satisfying: 1. \u03c4\u2217 \u2208 [\u2113\u2032, u\u2032]. 2. Simultaneously for every \u03c4 \u2208 [\u2113\u2032, u\u2032], we have |\u03b4(\u03c4)| \u2264 8\u01eb. 3. Simultaneously for every \u03c4 \u2208 [\u2113\u2032, u\u2032], the expected one-round regret of playing \u03c4 is at most 8\u01eb.\nTo understand the main idea of the proof, let\u2019s compare the expected reward of choosing the optimal threshold \u03c4\u2217 and an arbitrary threshold \u03c4 \u2208 [\u2113, u]. The difference is given by\nR(\u03c4\u2217)\u2212R(\u03c4) = \u222b \u03c4\u2217\n0 f1(x)g(x)dx \u2212\n\u222b \u03c4\n0 f1(x)g(x)dx =\n\u222b \u03c4\u2217\n\u03c4 f1(x)g(x)dx.\nNote that g is non-increasing since g\u2032(x) = F2(x) \u2212 1 \u2264 0. So, using \u03c4\u2217, \u03c4 \u2208 [\u2113, u] imply |F1(\u03c4\u2217)\u2212 F1(\u03c4)| \u2264 |F1(\u2113) \u2212 F1(u)|, we get R(\u03c4\u2217) \u2212 R(\u03c4) \u2264 |(F1(\u2113) \u2212 F1(u)) \u00b7 g(\u03c4)|. This motivates defining bounding function\n\u03b4(\u03c4) := (F1(u)\u2212 F1(\u2113)) \u00b7 ( g(\u03c4\u2217)\u2212 g(\u03c4)) = \u2212 (F1(u)\u2212 F1(\u2113)) \u00b7 g(\u03c4), (4)\nand we get the following upper bound on the one-round regret when choosing \u03c4 instead of \u03c4\u2217.\nClaim 2.8. If \u03c4, \u03c4\u2217 \u2208 [\u2113, u] then R(\u03c4\u2217)\u2212R(\u03c4) \u2264 |\u03b4(\u03c4)|.\nIn order to define an estimate \u03b4\u0302(\u03c4) that can be computed using the available information, again consider the rewards when playing thresholds u and \u2113. The difference is given by\nR(u)\u2212R(\u2113) = \u222b u\n\u2113 f1(x)g(x)dx = F1(u)g(u) \u2212 F1(\u2113)g(\u2113) \u2212\n\u222b u\n\u2113 F1(x)g\n\u2032(x)dx\n= F1(u)g(u) \u2212 F1(\u2113)g(\u2113) \u2212 \u222b u\n\u2113 F1(x) \u00b7 (F2(x)\u2212 1)dx.\nAdding this equation with the definition of \u03b4(\u03c4) gives \u03b4(\u03c4) +R(u)\u2212R(\u2113) equals\nF1(u) \u00b7 ( g(u) \u2212 g(\u03c4) ) \u2212 F1(\u2113) \u00b7 ( g(\u2113) \u2212 g(\u03c4) ) \u2212 \u222b u\n\u2113 F1(x) \u00b7 (F2(x)\u2212 1)dx =: \u2206(\u03c4), (5)\nwhich gives us an alternate way to express \u03b4(\u03c4) = \u2206(\u03c4)\u2212 (R(u)\u2212R(\u2113)). So, we define the estimate\n\u03b4\u0302(\u03c4) := \u2206\u0302(\u03c4)\u2212 (R\u0302u \u2212 R\u0302\u2113),\nwhere \u2206\u0302 uses the estimates F\u03021 and g\u0302 instead of F1 and g in (5), and to estimate (R\u0302u \u2212 R\u0302\u2113) we use empirical averages obtained in the current phase. We have the following claim on the accuracy of \u03b4\u0302 in Appendix B.2, which is similar to Claim 2.4.\nClaim 2.9. In Algorithm 3, if the conditions in Lemma 2.7 hold, then with probability 1 \u2212 T\u221210 |\u03b4\u0302(\u03c4)\u2212 \u03b4(\u03c4)| \u2264 4\u01eb simultaneously for all \u03c4 \u2208 [\u2113, u].\nThe proof of Claim 2.9 is different from Claim 2.4: After the initialization, it\u2019s not possible to give an initial confidence interval of length at most T\u2212 1 4 . So, we cannot prove an O(T\u2212 1 2 ) accuracy for \u2206(\u03c4). Instead, we use the fact that Var\u2206(\u03c4) \u2264 O(\u01eb) to give an O(\u01eb) accuracy bound using Bernstein inequality (Theorem A.2) for a single \u03c4 . To extend the bound to the whole interval, we discretize and apply a union bound. To avoid the dependency from the previous phases when discretizing, in each phase we use new samples to construct F\u03021 and F\u03022. This is the reason that we introduce sample sets in Algorithm 3.\nNow the proof of Lemma 2.7 is similar to the proof of Lemma 2.2 via Claims 2.8 and 2.9.\nFinally, we state the main theorem for Pandora\u2019s Box problem with two boxes in a fixed order.\nTheorem 2.10. For Bandit Pandora\u2019s Box learning problem with two boxes in a fixed order, there exists an algorithm that achieves O( \u221a T log T ) total regret.\nThe proof of Theorem 2.10 is similar to Theorem 2.6: We first show that \u0398( \u221a T log T ) initial samples are sufficient to meet the conditions in Lemma 2.7. Combining this with Lemma 2.5 proves the theorem. See Appendix B.2 for details."
        },
        {
            "heading": "3 Prophet Inequality for General n",
            "text": "In the Bandit Prophet Inequality problem, there are n unknown independent distributions D1, . . . , Dn taking values in [0, 1] with cdfs F1, . . . , Fn and densities f1, . . . , fn. Consider a T rounds game where round t we play thresholds \u03c4 (t) = (\u03c4\n(t) 1 , \u03c4 (t) 2 , . . . , \u03c4 (t) n\u22121, \u03c4 (t) n = 0) and receive the following\nreward: For i \u2208 [n], independently draw X(t)i from Di. Let j = min{i \u2208 [n] : X (t) i \u2265 \u03c4 (t) i }. X (t) j is returned as the reward. The only feedback is the reward, and we do not see the index j of the selected random variable. Since we have \u03c4 (t) n = 0, the algorithm will always select a value. In the following, we omit \u03c4 (t) n and only use \u03c4 (t) := (\u03c4 (t) 1 , \u03c4 (t) 2 , . . . , \u03c4 (t) n\u22121) to represent a threshold setting.\nLet Opti represent the optimal expected reward if only running on distributionsDi,Di+1, . . . ,Dn. Then, the optimal i-th threshold setting is exactly Opti+1. We can calculate {Opti+1} as follows:\n\u2022 Let Optn = E [Xn] \u2022 For i = n \u2212 1 \u2192 1: Let Opti = R(1, 1, . . . , 1,Opti+1,Opti+2, . . . ,Optn), where the function R(\u03c4 ) represents the expected one-round reward under thresholds \u03c4 = (\u03c41, . . . , \u03c4n\u22121).\nThe total regret is defined T \u00b7Opt1 \u2212 \u2211T t=1 R(\u03c4 (t)).\nHigh-Level Approach. Following the doubling framework from Algorithm 2, we only need to design an initialization algorithm and a constraint-updating algorithm. For the initialization, we get O(poly(n) \u221a T log T ) i.i.d. samples for each Xi by playing thresholds (1, 1, . . . , \u03c4i\u22121 = 1, \u03c4i =\n0, 0, . . . , 0). Besides, we run O(poly(n) \u221a T log T ) samples to get the initial confidence intervals with small length. For the constraint-updating algorithm, we reuse the idea from the n = 2 case where we shrink confidence intervals by testing Xi with thresholds \u2113i or ui. However, there are two major new challenges while testing Xi.\nThe first challenge while testing Xi is that we may stop early, and not get sufficiently many samples for Xi. Although the probability of reaching Xi could be very small, this also means that we will not reach Xi frequently. To avoid this problem, for j < i, we use the upper confidence bounds as thresholds since they maximize the probability of reaching Xi. In particular, it is at least as high as in the optimal policy. Therefore, we will be able to show that the probability term cancels in calculation, so the total loss from Xi can still be bounded.\nThe second challenge is that when we are testing Xi, we need to also set thresholds \u03c4j for j > i. The problem is that the optimal choice for \u03c4i depends on \u03c4j for j > i. To cope this this problem, in our algorithm we use the lower confidence bounds as thresholds for j > i. Formally, let Algi denote the expected reward if only running on distributions Di, . . . ,Dn with lower confidence bounds as the thresholds, i.e.,\nAlgi := R(1, . . . , 1, \u03c4i = \u2113i, \u03c4i+1 = \u2113i+1, . . . , \u03c4n\u22121 = \u2113n\u22121).\nNow, under our threshold setting, we can only hope to learn Algi+1, while the optimal threshold is Opti+1. So, our key idea is to first get a new confidence interval for Algi+1. Then, since we have Algi+1 \u2264 Opti+1, the lower bound for Algi+1 is also a lower bound for Opti+1. For the upper bound,\nwe first bound the difference between Opti+1 and Algi+1, and adding this difference to the upper bound for Algi+1 gives the upper bound for Opti+1."
        },
        {
            "heading": "3.1 Interval-Shrinking Algorithm for General n",
            "text": "In this section, we give the interval shrinking algorithm, and provide the regret analysis to show that we can get a new group of confidence intervals that achieves O(\u01eb) regret after O\u0303(poly(n)\n\u01eb2 ) rounds.\nWe first give the algorithm and the corresponding lemma.\nAlgorithm 4: Interval shrinking Algorithm for general n\nInput: Intervals [\u21131, u1], . . . , [\u2113n\u22121, un\u22121], CDF estimates F\u03021(x), . . . , F\u0302n(x), and \u01eb. 1 For i \u2208 [n\u2212 1], define P\u0302i := \u220f j\u2208[i\u22121] F\u0302j(uj) 2 for i = n\u2212 1\u2192 1 do 3 Run C \u00b7 log T\n\u01eb2 rounds with thresholds (u1, . . . , ui\u22121, \u2113i, \u2113\u2032i+1, . . . \u2113 \u2032 n\u22121) and C \u00b7 log T\u01eb2 rounds\nwith (u1, . . . , ui\u22121, ui, \u2113\u2032i+1, . . . \u2113 \u2032 n\u22121). Let D\u0302i be the difference of the average rewards.\n4 For \u03c4 \u2208 [\u2113i, ui], define \u2206\u0302i(\u03c4) := P\u0302i(F\u0302i(ui)(\u03c4 \u2212 ui)\u2212 F\u0302i(\u2113i)(\u03c4 \u2212 \u2113i) + \u222b ui \u2113i F\u0302i(x)dx). 5 For \u03c4 \u2208 [\u2113i, ui], define \u03b4\u0302i(\u03c4) := \u2206\u0302i(\u03c4)\u2212 D\u0302i. 6 Let \u2113\u2032i = min { \u03c4 \u2208 [\u2113i, ui] s.t. \u03b4\u0302i(\u03c4) \u2265 \u2212\u01eb } .\n7 Let u\u2032i = max { \u03c4 \u2208 [\u2113i, ui] s.t. \u03b4\u0302i(\u03c4) \u2264 (2n\u2212 2i\u2212 1)\u01eb } .\nOutput: [\u2113\u20322, u \u2032 2], . . . , [\u2113 \u2032 n, u \u2032 n]\nLemma 3.1. Suppose we are given:\n\u2022 Distribution estimates F\u0302i(x) for i \u2208 [n\u2212 1] satisfying | \u220f i\u2208S F\u0302i(x)\u2212 \u220f\ni\u2208S Fi(x)| \u2264 T\u22121/4 for all x \u2208 [0, 1] and S \u2286 [n]. \u2022 Initial intervals [\u2113i, ui] for i \u2208 [n \u2212 1] of length ui \u2212 \u2113i \u2264 T\u22121/4 that satisfy Opti+1 \u2208 [\u2113i, ui] and Algi+1 \u2208 [\u2113i, ui].\nThen, for \u01eb > 12T\u2212 1 2 Algorithm 4 runs no more than 1000 \u00b7 n log T\n\u01eb2 rounds such that in each round\nthe threshold \u03c4 satisfies \u03c4i \u2208 [\u2113i, ui] for all i \u2208 [n \u2212 1]. Moreover, with probability 1 \u2212 T\u221210 the following statements hold:\n(i) Opti+1 \u2208 [\u2113\u2032i, u\u2032i] for all i \u2208 [n\u2212 1]. (ii) Let Alg\u2032i := R(1, . . . 1, \u2113 \u2032 i, . . . , \u2113 \u2032 n\u22121) for i \u2208 [n \u2212 1]. Then Alg\u2032i+1 \u2208 [\u2113\u2032i, u\u2032i].\n(iii) For every threshold setting \u03c4 = (\u03c41, . . . , \u03c4n\u22121) where \u03c4i \u2208 [\u2113\u2032i, u\u2032i], the expected one-round regret of playing \u03c4 is at most 2n2\u01eb.\nWe first introduce some notation to prove Lemma 3.1. First, we define a single-dimensional function Ri(\u03c4) to generalize reward function R(\u03c4) from the n = 2 case in Section 2.1. Ideally, Ri(\u03c4) should represent the reward of playing \u03c4i = \u03c4 , but thresholds \u03c4j for j > i also affect its expected reward. So, to match the setting in Algorithm 4, we set thresholds \u03c4i+1, . . . , \u03c4n\u22121 to be the updated lower bounds, i.e., define\nRi(\u03c4) := R(1, . . . , 1, \u03c4i = \u03c4, \u2113 \u2032 i+1, . . . , \u2113 \u2032 n\u22121).\nNext, we introduce Pi, representing the maximum probability of observing Xi when we have confidence intervals {[\u2113i, ui]}, i.e.,\nPi := \u220fi\u22121 j=1 Fj(uj).\nReplacing Fj with F\u0302j in this equation defines estimate P\u0302i.\nNotice that Pi also equals the probability of reaching Xi when we play thresholds \u03c4j = uj for all j < i in Algorithm 4. So, the loss of playing a sub-optimal threshold \u03c4i will be Pi\u00b7(Ri(Alg\u2032i+1)\u2212Ri(\u03c4)) because Pi is the probability of reaching Xi and Alg \u2032 i+1 is the optimal threshold when \u03c4j = \u2113 \u2032 j for all j > i. We define the generalized bounding function:\n\u03b4i(\u03c4) := Pi \u00b7 ( Fi(ui)\u2212 Fi(\u2113i) ) \u00b7 (\u03c4 \u2212 Alg\u2032i+1).\nWe will show in Claim 3.2 below that |\u03b4i(\u03c4)| upper bounds Pi \u00b7(Ri(Alg\u2032i+1)\u2212Ri(\u03c4)) for all \u03c4 \u2208 [\u2113i, ui]. Since we don\u2019t know \u03b4i(\u03c4), we will estimate it by writing in a different way.\nConsider the difference in expected rewards between \u03c4i = ui and \u03c4i = \u2113i when the other thresholds are set to \u03c4j = uj for j < i and \u03c4j = \u2113 \u2032 j for j > i. The difference between these two settings only comes from \u03c4i, so the expected difference is\nPi \u00b7 (Ri(ui)\u2212Ri(\u2113i)) = Pi \u00b7 ( (Fi(ui)\u2212 Fi(\u2113i))Alg\u2032i+1 \u2212 \u222b ui\n\u2113i\nxfi(x)dx\n)\n= Pi \u00b7 ( Fi(ui)(Alg \u2032 i+1 \u2212 ui)\u2212 Fi(\u2113i)(Alg\u2032i+1 \u2212 \u2113i) + \u222b ui\n\u2113i\nFi(x)dx\n) .\nAdding this with \u03b4i(\u03c4) implies \u03b4i(\u03c4) + Pi \u00b7 (Ri(ui)\u2212Ri(\u2113i)) equals\nPi \u00b7 ( Fi(ui)(\u03c4 \u2212 ui)\u2212 Fi(\u2113i)(\u03c4 \u2212 \u2113i) + \u222b ui\n\u2113i\nFi(x)dx ) =: \u2206i(\u03c4), (6)\nwhich gives another way of writing \u03b4i(\u03c4) = \u2206i(\u03c4)\u2212Pi\u00b7(Ri(ui)\u2212Ri(\u2113i)). Since D\u0302i from Algorithm 4 is the difference between average rewards of taking samples with \u03c4i = ui and \u03c4i = \u2113i, it is an unbiased estimator of Pi \u00b7 (Ri(ui)\u2212Ri(\u2113i)). So, we define estimate\n\u03b4\u0302i(\u03c4) := \u2206\u0302i(\u03c4)\u2212 D\u0302i,\nwhere \u2206\u0302i(\u03c4) is obtained by replacing Fi with F\u0302i and Pi with P\u0302i in (6).\nSimilar to Claim 2.3 and Claim 2.4, we introduce the following claims for Algorithm 4.\nClaim 3.2. For i \u2208 [n\u22121], if Alg\u2032i+1 \u2208 [\u2113i, ui] and \u03c4 \u2208 [\u2113i, ui], then Pi \u00b7 ( Ri(Alg \u2032 i+1)\u2212Ri(\u03c4) ) \u2264 |\u03b4i(\u03c4)|.\nProof. We only need to prove that Ri(Alg \u2032 i+1) \u2212 Ri(\u03c4) \u2264 |\u03b4i(\u03c4)|Pi = ( Fi(ui) \u2212 Fi(\u2113i) ) \u00b7 (\u03c4 \u2212 Alg\u2032i+1). Now the proof is identical to Claim 2.3 by replacing function R(\u00b7) with Ri(\u00b7).\nClaim 3.3. In Algorithm 4, if the conditions in Lemma 3.1 hold, then with probability 1 \u2212 T\u221210, we have |\u03b4\u0302i(\u03c4)\u2212 \u03b4i(\u03c4)| \u2264 \u01eb simultaneously for all \u03c4 \u2208 [\u2113i, ui].\nProof. There are two terms in \u03b4i(\u03c4) = \u2206i(\u03c4)\u2212Pi \u00b7 (Ri(u)\u2212Ri(\u2113)). We prove that the error of each term is bounded by \u01eb2 with high probability, which will complete the proof by a union bound.\nWe first bound |\u2206\u0302i(\u03c4) \u2212 \u2206i(\u03c4)|. There are three terms in \u2206i(\u03c4)Pi = Fi(ui)(\u03c4 \u2212 ui) \u2212 Fi(\u2113i)(\u03c4 \u2212 \u2113i) + \u222b ui \u2113i\nFi(x)dx. Since the conditions in Lemma 3.1 guarantee that |F\u0302i(x) \u2212 Fi(x)| \u2264 T\u22121/4 and ui \u2212 \u2113i \u2264 T\u22121/4, the error in each term is at most 1\u221aT and the total error \u2223\u2223\u2223\u2206i(\u03c4)Pi \u2212 \u2206\u0302i(\u03c4)\nP\u0302i\n\u2223\u2223\u2223 \u2264 3\u221a T .\nFor Pi, the preconditions in Lemma 3.1 guarantee that |P\u0302i \u2212 P\u0302i| \u2264 T\u22121/4. Moreover, observe that ui \u2212 \u2113i \u2264 T\u22121/4 implies that |\u2206i(\u03c4)|Pi \u2264 3T \u22121/4. So,\n\u2223\u2223\u2223\u2206\u0302i(\u03c4)\u2212\u2206i(\u03c4) \u2223\u2223\u2223 \u2264 \u2223\u2223\u2223\u2223\u2223P\u0302i ( \u2206\u0302i(\u03c4) P\u0302i \u2212 \u2206i(\u03c4) Pi )\u2223\u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223(P\u0302i \u2212 Pi) \u2206i(\u03c4) Pi \u2223\u2223\u2223\u2223 \u2264 6\u221a T \u2264 \u01eb 2 ,\nwhere the last inequality uses \u01eb > 12T\u2212 1 2 .\nFor Pi \u00b7 (Ri(ui) \u2212 Ri(\u2113i)), note that D\u0302i is an unbiased estimator of Pi \u00b7 (Ri(ui) \u2212 Ri(\u2113i)) with N = C \u00b7 log T\n\u01eb2 samples. So, by Hoefdding\u2019s Inequality,\nPr [\u2223\u2223D\u0302i \u2212 Pi \u00b7 (Ri(ui)\u2212Ri(\u2113i)) \u2223\u2223 > \u01eb 2 ] \u2264 2 exp(\u22128N\u01eb2) = 2T\u22128C .\nThus, |D\u0302i \u2212 Pi \u00b7 (Ri(ui)\u2212Ri(\u2113i))| \u2264 \u01eb2 holds with probability 1\u2212 T\u221210 when C > 10.\nBesides Claim 3.2 and Claim 3.3, we also need some other properties of Algorithm 4 to prove Lemma 3.1. The next claim shows that the expected reward of playing lower confidence bounds increases phase to phase.\nClaim 3.4. Assume the conditions in Lemma 3.1 and the bound in Claim 3.3 hold. Then, for i \u2208 [n], we have Alg\u2032i \u2265 Algi.\nProof. We prove by induction for i going from n to 1. The base case i = n holds because Alg\u2032n = Algn = Optn = E [Xn] by definition.\nFor the induction step, assume that Alg\u2032i+1 \u2265 Algi+1 by induction hypothesis. Observe that\nR(1, . . . , 1, \u2113i, \u2113i+1, . . . , \u2113n\u22121) = E [Xi \u00b7 1Xi>\u2113i ] +Pr [Xi \u2264 \u2113i]R(1, . . . , 1, 1, \u2113i+1, . . . , \u2113n\u22121) \u2264 E [Xi \u00b7 1Xi>\u2113i ] +Pr [Xi \u2264 \u2113i]R(1, . . . , 1, 1, \u2113\u2032i+1, . . . , \u2113\u2032n\u22121) = R(1, . . . , \u2113i, \u2113 \u2032 i+1, . . . , \u2113 \u2032 n\u22121), (7)\nwhere the inequality uses induction hypothesis as R(1, . . . , 1, 1, \u2113i+1, . . . , \u2113n\u22121) = Algi+1 \u2264 Alg\u2032i+1 = R(1, . . . , 1, 1, \u2113\u2032i+1, . . . , \u2113 \u2032 n\u22121).\nNext, we have Ri(\u2113i) \u2264 Ri(\u2113\u2032i), i.e.,\nR(1, . . . , 1, \u2113i, \u2113 \u2032 i+1, . . . , \u2113 \u2032 n\u22121) \u2264 R(1, . . . , 1, \u2113\u2032i, \u2113\u2032i+1, . . . , \u2113\u2032n\u22121). (8)\nTo prove this, we first observe that if \u2113i = \u2113 \u2032 i, then the inequality is an equality. Otherwise, there must be \u03b4\u0302i(\u2113 \u2032 i) = \u2212\u01eb. Next, combining the definition of \u03b4i(\u03c4) and Claim 3.3, we have \u03b4\u0302i(Alg\u2032i+1) \u2265 \u03b4i(Alg \u2032 i+1) \u2212 |\u03b4i(Alg\u2032i+1) \u2212 \u03b4\u0302i(Alg\u2032i+1)| \u2265 \u2212\u01eb. Since \u03b4\u0302\u2032i(\u03c4) = Pi \u00b7 (F\u0302i(ui) \u2212 F\u0302i(\u2113i)) \u2265 0 means \u03b4\u0302i(\u03c4) is increasing, there must be Alg\u2032i+1 \u2265 \u2113\u2032i. Now consider function Ri(\u03c4). Recall that Ri(\u03c4) = R(1, ...1, \u03c4i = \u03c4, \u2113 \u2032 i+1, ..., \u2113 \u2032 n\u22121). Therefore,\nRi(\u03c4) = Pr [Xi \u2264 \u03c4 ] \u00b7 Alg\u2032i+1 +E [Xi \u00b7 1Xi>\u03c4 ] = Fi(\u03c4) \u00b7 Alg\u2032i+1 + \u222b 1\n\u03c4 fi(x)x dx,\nwhich means R\u2032i(\u03c4) = fi(\u03c4)(Alg \u2032 i+1 \u2212 \u03c4), showing that Ri(\u03c4) is a unimodular function and reaches its maximum when \u03c4 = Alg\u2032i+1. Hence, (8) holds because \u2113i+1 \u2264 \u2113\u2032i+1 \u2264 Alg\u2032i+1. Combining (7) and (8) proves the claim.\nNext, we prove that Alg\u2032i+1 \u2208 [\u2113i, ui], which is crucial for us to use Claim 3.2.\nClaim 3.5. Assume that the preconditions in Lemma 3.1 and the bound in Claim 3.3 hold, then Alg\u2032i+1 \u2208 [\u2113i, ui] for all i \u2208 [n\u2212 1].\nProof. Claim 3.4 shows that Algi+1 \u2264 Alg\u2032i+1. On the other hand, Alg\u2032i+1 \u2264 Opti+1 holds because Opti+1 is the maximum achievable reward. Then, Claim 3.5 holds because Opti+1,Algi+1 \u2208 [\u2113i, ui] by the preconditions in Lemma 3.1.\nFinally, we show that Alg\u2032i cannot be much smaller than Opti.\nClaim 3.6. Assume that the preconditions in Lemma 3.1 and the bound in Claim 3.3 hold, then Opti \u2212 Alg\u2032i \u2264 2(n\u2212i)\u01ebPi for all i \u2208 [n\u2212 1].\nProof. We prove by induction for i going from n to 1. The base case i = n holds because Optn = Alg\u2032n = E [Xn].\nFor the induction step, we assume that Opti+1\u2212Alg\u2032i+1 \u2264 2(n\u2212i\u22121)\u01ebPi+1 and would like to show that Opti \u2212 Alg\u2032i \u2264 2(n\u2212i)\u01ebPi . We first have R(1, . . . , 1,Opti+1,Opti+2, . . . ,Optn) = E [ Xi \u00b7 1Xi>Opti+1 ] +Pr [ Xi \u2264 Opti+1 ] Opti+1\n\u2264 E [ Xi \u00b7 1Xi>Opti+1 ] +Pr [ Xi \u2264 Opti+1 ] (Alg\u2032i+1 + 2(n\u2212i\u22121)\u01eb Pi+1 ) \u2264 E [ Xi \u00b7 1Xi>Opti+1 ] +Pr [ Xi \u2264 Opti+1 ] Alg\u2032i+1 + 2(n\u2212i\u22121)\u01eb Pi = R(1, . . . , 1,Opti+1, \u2113 \u2032 i+1, . . . , \u2113 \u2032 n\u22121) + 2(n\u2212i\u22121)\u01eb Pi , (9)\nwhere we use the induction hypothesis in the second line, and the fact that Pr [ Xi \u2264 Opti+1 ] \u2264 Pr [Xi \u2264 ui] = Pi+1Pi in the third line. Next, since Alg\u2032i+1 is the optimal threshold, we have\nR(1, . . . , 1,Opti+1, \u2113 \u2032 i+2, . . . , \u2113 \u2032 n\u22121) \u2264 R(1, . . . , 1,Alg\u2032i+1, \u2113\u2032i+2, . . . , \u2113\u2032n\u22121). (10)\nFinally, |\u03b4(\u2113\u2032i)| \u2264 |\u03b4\u0302(\u2113\u2032i)|+ |\u03b4\u0302(\u2113\u2032i)\u2212 \u03b4(\u2113\u2032i)| \u2264 \u01eb+ \u01eb = 2\u01eb, where the bound of |\u03b4\u0302(\u2113\u2032i)\u2212 \u03b4(\u2113\u2032i)| is from Claim 3.3, and the bound of |\u03b4\u0302(\u2113\u2032i)| is from Algorithm 4. Combining this with Claim 3.2, we have Ri(Alg \u2032 i+1)\u2212Ri(\u2113\u2032i) \u2264 |\u03b4i(\u2113\u2032i)| Pi \u2264 2\u01ebPi , which is exactly\nR(1, . . . , 1,Alg\u2032i+1, \u2113 \u2032 i+1, . . . , \u2113 \u2032 n\u22121) \u2264 R(1, . . . , 1, \u2113\u2032i, \u2113\u2032i+1, . . . , \u2113\u2032n\u22121) +\n2\u01eb Pi .\nSumming this with (9) and (10) completes the induction step.\nFinally, we can prove Lemma 3.1.\nProof of Lemma 3.1. In this proof, we assume Claim 3.3 always holds. Then the whole proof should success with probability 1\u2212 T\u221210.\nWe prove the three statements separately:\nStatement (i). For the upper bound, Claim 3.6 shows that Opti \u2212 Alg\u2032i \u2264 2(n\u2212i)\u01ebPi . Therefore, \u03b4i(Opti+1) \u2264 Pi \u00b7 Fi(ui) \u00b7 2(n\u2212i\u22121)\u01ebPi+1 = 2(n \u2212 i \u2212 1)\u01eb. Combining this with Claim 3.3, we have \u03b4\u0302i(Opti+1) \u2264 2(n \u2212 i \u2212 1)\u01eb + \u01eb = (2n \u2212 2i \u2212 1)\u01eb. Then Opti+1 \u2264 u\u2032i, because Opti+1 \u2208 [\u2113i, ui], u\u2032i = max{\u03c4 : \u03c4 \u2208 [\u2113i, ui] \u2227 \u03b4\u0302i(\u03c4) \u2264 (2n\u2212 2i\u2212 1)\u01eb} and the monotonicity of \u03b4\u0302i function.\nFor the lower bound, at least we have Opti+1 \u2265 Alg\u2032i+1. Therefore, \u03b4i(Opti+1) \u2265 0, so \u03b4\u0302i(Opti+1) \u2265 \u2212\u01eb. Then Opti+1 \u2265 \u2113\u2032i, because Opti+1 \u2208 [\u2113i, ui], \u2113\u2032i = max{\u03c4 : \u03c4 \u2208 [\u2113i, ui]\u2227 \u03b4\u0302i(\u03c4) \u2265 \u2212\u01eb} and the monotonicity of \u03b4\u0302i function. Combining the two bounds proves Statement (i). Statement (ii). The proof idea is the same as Statement (i). Notice that \u03b4i(Alg \u2032 i+1) = 0. Then, according to Claim 3.3, |\u03b4\u0302i(Alg\u2032i+1)| \u2264 \u01eb. So Statement (ii) hold because Alg\u2032i+1 \u2208 [\u2113i, ui], which is from Claim 3.5, and [\u2113\u2032i, u \u2032 i] \u2287 {\u03c4 \u2208 [\u2113i, ui] : |\u03b4\u0302i(\u03c4)| \u2264 \u01eb}. Statement (iii). We prove the following stronger statement by induction on i: If \u03c4j \u2208 [\u2113\u2032j, u\u2032j ] for all j \u2208 {i, . . . , n}, then\nAlg\u2032i \u2212 R(1, . . . 1, \u03c4i, . . . , \u03c4n\u22121) \u2264 (n\u2212i+1) 2\u01eb\nPi .\nWhen the statement above holds, taking i = 1 gives R(\u03c41, . . . , \u03c4n\u22121) \u2265 Alg\u20321 \u2212 n2\u01eb. Furthermore, Claim 3.6 shows that Alg\u20321 \u2265 Opt1 \u2212 2(n \u2212 1)\u01eb. Combining these two inequalities proves Statement (iii).\nIt remains to prove the induction statement. The base case i = n holds trivially.\nFor the induction step, we will assume that the statement holds for i+ 1 and we have to show it also holds for i. By induction hypothesis,\nR(1, . . . , 1, \u03c4i, . . . , \u03c4n\u22121) = E [Xi \u00b7 1Xi\u2265\u03c4i ] +Pr [Xi < \u03c4i]R(1, . . . , 1, \u03c4i+1, . . . , \u03c4n\u22121) \u2265 E [Xi \u00b7 1Xi\u2265\u03c4i ] +Pr [Xi < \u03c4i] ( R(1, . . . , 1, \u2113\u2032i+1, . . . , \u2113 \u2032 n\u22121)\u2212 (n\u2212i) 2\u01eb Pi+1 )\n\u2265 E [Xi \u00b7 1Xi\u2265\u03c4i ] +Pr [Xi < \u03c4i]R(1, . . . , 1, \u2113\u2032i+1, . . . , \u2113\u2032n\u22121)\u2212 (n\u2212i) 2\u01eb\nPi\n= R(1, . . . , 1, \u03c4i, \u2113 \u2032 i+1, . . . , \u2113 \u2032 n\u22121)\u2212 (n\u2212i) 2\u01eb Pi .\nFurthermore, |\u03b4i(\u03c4i)| \u2264 |\u03b4\u0302i(\u03c4i)| + \u01eb by Claim 3.3 and |\u03b4\u0302i(\u03c4)| \u2264 (2n \u2212 2i \u2212 1)\u01eb by the definitions of \u2113\u2032i and u \u2032 i, which means |\u03b4i(\u03c4)| \u2264 2(n\u2212 i)\u01eb. So, Claim 3.2 implies\nR(1, . . . , 1,Alg\u2032i+1, \u2113 \u2032 i+1, . . . , \u2113 \u2032 n\u22121)\u2212R(1, . . . , 1, \u03c4i, \u2113\u2032i+1, . . . , \u2113\u2032n\u22121) \u2264 2(n\u2212 i)\u01eb Pi .\nFinally, using R(1, . . . , 1,Alg\u2032i+1, \u2113 \u2032 i+1, . . . , \u2113 \u2032 n\u22121) \u2265 Alg\u2032i, we get\nR(1, \u00b7 \u00b7 \u00b7 , 1, \u03c4i, . . . , \u03c4n\u22121) \u2265 Alg\u2032i \u2212 2(n \u2212 i)\u01eb Pi \u2212 (n\u2212 i) 2\u01eb Pi \u2265 Alg\u2032i \u2212 (n\u2212 i+ 1)2\u01eb Pi ."
        },
        {
            "heading": "3.2 Initialization and Putting Everything Together",
            "text": "Now, we can give the initialization algorithm. The main goal of the initialization is to satisfy the conditions listed in Lemma 3.1. Starting from the second call of Algorithm 4, the confidence interval length constraint and the distribution estimates constraints hold from the initialization, and the constraints Opti+1,Algi+1 \u2208 [\u2113i, ui] are guaranteed by Statements (i) and (ii) in Lemma 3.1. Then, we can apply Lemma 2.5 to bound the total regret.\nWe first give the initialization algorithm:\nAlgorithm 5: Initialization\nInput: Time horizon T , problem size n. 1 for i = 1\u2192 n do 2 Run 1000n2 \u221a T log T free samples for Xi to estimate F\u0302i(x). 3 for i = n\u2212 1\u2192 1 do 4 Run 1000n2 \u221a T log T samples under the threshold setting\n(1, . . . , 1, \u03c4i+1 = \u2113 \u2032 i+1, . . . , \u03c4n\u22121 = \u2113 \u2032 n\u22121). Let \u00b5i be the average reward.\n5 Let \u2113i = \u00b5i \u2212 T \u22121/4 10n , ui = \u00b5i + (2n\u2212 2i\u2212 1) \u00b7 T \u22121/4 10n\nOutput: [\u21131, u1], . . . , [\u2113n\u22121, un\u22121].\nLemma 3.7. Algorithm 5 runs O(n3 \u221a T log T ) rounds. The output satisfies with probability 1\u2212T\u221210 all constraints listed in Lemma 3.1.\nProof. For the accuracy bound of F\u0302i(x), we first show that |F\u0302i(x)\u2212Fi(x)| \u2264 T \u22121/4\n2n with probability\n1 \u2212 T\u221211 after running N = C \u00b7 n2 \u221a T log T samples with C = 1000. With DKW inequality (Theorem A.3), we have\nPr [ |F\u0302i(x)\u2212 Fi(x)| > \u03b5 = T\u2212 1 4\n2n\n] \u2264 2 exp(\u22122N\u03b52) = 2T\u2212C/4.\nSo the bound holds with probability 1\u2212T\u221212 when C = 1000. By the union bound, with probability 1 \u2212 T\u221211, we have |F\u0302i(x) \u2212 Fi(x)| \u2264 T \u2212 1 4\n2n holds for every i \u2208 [n]. Then, for the accuracy of\u220f i\u2208S Fi(x), we have ( (1 \u2212 T\u2212 1 4 2n ) n \u2212 1 ) \u2264 \u220fi\u2208S F\u0302i(x) \u2212 \u220f i\u2208S Fi(x) \u2264 ( (1 + T \u2212 1 4 2n ) n \u2212 1 ) . For the lower bound, we have (1 \u2212 T\u2212 1 4\n2n ) n \u2212 1 \u2265 1 \u2212 T\u2212\n1 4\n2 \u2212 1 > \u2212T\u2212 1 4 . For the upper bound, we have\n(1 + T \u2212\n1 4\n2n ) n \u2212 1 \u2264 exp(T\u2212\n1 4\n2n \u00b7 n)\u2212 1 \u2264 1 + 2 \u00b7 T \u2212\n1 4\n2 \u2212 1 = T\u2212 1 4 . Combining two bounds finishes the\nproof.\nFor the confidence interval, the constraints ui \u2212 \u2113i \u2264 T\u2212 1 4 hold by definition. Then, it only remains to show Opti+1 \u2208 [\u2113i, ui] and Algi+1 \u2208 [\u2113i, ui]. We start from proving Algi+1 \u2208 [\u2113i, ui]. Notice that \u00b5i is an estimate of Algi+1 with N =\nC \u00b7 n2 \u221a T log T samples with C = 1000. With Hoeffding\u2019s Inequality (Theorem A.1), we have\nPr [ |\u00b5i \u2212 Algi+1| > \u03b5 = T\u22121/4\n10n\n] < 2 exp(\u22122N\u03b52) = 2T\u2212C/50.\nNotice that \u2113i = \u00b5i\u2212 T \u22121/4 10n and ui = \u00b5i + T\u22121/4 10n . Then, by the union bound for all i \u2208 [n] , we have Algi+1 \u2208 [\u2113i, ui] holds for all i with probability 1\u2212 T\u221211 when C \u2265 1000.\nFor Opti+1, we prove the statement by doing induction with the assumption that |Algi+1\u2212\u00b5i| \u2264 T\u22121/4\n10n for all i. The base case is i = n, the statement simply holds because Algn = Optn. Next, we consider i, with the condition that Optj+1 \u2208 [\u2113j , uj ] for all j > i. For the lower bound, since we know that Algi+1 \u2265 \u2113i, there must be Opti+1 \u2265 \u2113i, because Opti+1 \u2265 Algi+1. For the upper bound, we first bound the difference between Algi+1 and Opti+1. Consider the setting (1, . . . 1, \u03c4i+1 = \u2113i+1, . . . , \u03c4n\u22121 = \u2113n\u22121) and (1, . . . , 1, \u03c4i+1 = Opti+2, . . . , \u03c4n\u22121 = Optn). The first\nsetting incurs an extra loss only when its behavior is different from the second setting. Assume the two settings behave differently when meeting a threshold \u03c4j. Notice that this extra loss is bounded by |\u2113j \u2212 Optj+1|. Since Optj+1 \u2208 [\u2113j , uj ] for all j > i, this difference is upper bounded by maxj>i uj \u2212 \u2113j = ui+1 \u2212 \u2113i+1 = (2n \u2212 2i\u2212 2) \u00b7 T \u22121/4\n10n . Therefore,\nOpti+1 \u2264 Algi+1 + (2n\u2212 2i\u2212 2) \u00b7 T\u22121/4\n10n \u2264 \u00b5i + (2n \u2212 2i\u2212 1) \u00b7\nT\u22121/4\n10n = ui.\nCombining the lower bound and the upper bound proves Opti+1 \u2208 [\u2113i, ui]. Finally, taking union bounds for all events that hold with probability 1\u2212 T\u221211 finishes the proof.\nNow we are ready to prove the main theorem.\nTheorem 1.1. There is a polytime algorithm with O(n3 \u221a T log T ) regret for the Bandit Prophet Inequality problem where we only receive the selected value as the feedback. Proof. For the initialization, Algorithm 5 runs O(n3 \u221a T log T ) rounds, so the total regret from the\ninitialization is O(n3 \u221a T log T ).\nFor the main algorithm, we run Algorithm 2 with Algorithm 4 being the required sub-routine Alg. This is feasible because the requirements in Lemma 3.1 are guaranteed by the initialization and Lemma 3.1 itself. Besides, Lemma 3.1 implies that Algorithm 4 upper-bound the one-round regret by \u01eb after O(n 5 log T \u01eb2 ) samples. Applying Lemma 2.5 with \u03b1 = 5, we have the O(n2.5 \u221a T log T ) regret bound. Combining two parts finishes the proof."
        },
        {
            "heading": "4 Pandora\u2019s Box for General n",
            "text": "In the Bandit Pandora\u2019s Box problem, there are n unknown independent distributions D1, . . . , Dn representing the values of the n boxes. The distributions have cdfs F1, . . . , Fn and densities f1, . . . , fn. Moreover, each box/distribution Di has a known inspection cost ci. Although in the original problem in introduction we assumed that the values and costs have support [0, 1], in this section we will scale down the costs and values by a factor of 2n, so that they have support [0, 12n ]. This scaling helps to bound the utility in each round between [\u22120.5, 0.5]. To obtain bounds for the original unscaled problem, we will multiply our bounds with this factor 2n in the final analysis.\nConsider a T rounds game where in each round we play some permutation \u03c0 representing the order of inspection and n thresholds (\u03c4\u03c0(1), . . . , \u03c4\u03c0(n)). Our algorithm receives the following utility as feedback: For i \u2208 [n], draw X\u03c0(i) \u223c D\u03c0(i). Let j be the minimum index that satisfies max{X\u03c0(1), . . . ,X\u03c0(j \u2212 1)} \u2265 \u03c4\u03c0(j). If such j does not exist, j is set to be n+1 (all boxes opened). The utility we receive in this round is max{X\u03c0(1), . . . ,X\u03c0(j \u2212 1)} \u2212 \u2211 k<j c\u03c0(k).\nNote that the only feedback is the utility, and we do not see any value or even the index j where we stop.\nIn the case of known distributions, the optimal one-round policy for this problem was designed by Weitzman [Wei79]: For every distribution Di, solve the equation E [max{Xi \u2212 \u03c3i, 0}] = ci; now play permutation \u03c0 by sorting in decreasing order of \u03c3i and set threshold \u03c4 \u2217 i = \u03c3i. Let OPT be the optimal expected reward according to this optimal policy. Let ALGt be the expected reward of our policy in the t-th round. Then, we want to design an algorithm with total regret T \u00b7OPT \u2212\u2211t\u2208T ALGt at most O\u0303(poly(n) \u221a T ).\nBefore introducing the algorithm, we define the gain function for this general case:\ngi(v) := \u2212 ci + \u222b 1\nv (x\u2212 v)fi(x)dx = \u2212 ci + (1\u2212 v)\u2212\n\u222b 1\nv Fi(x)dx. (11)\nSimilar to the n = 2 case, this gain function is the expected additional utility we get on opening Xi when we already have value v in hand. Note that the optimal threshold \u03c4 \u2217 i satisfies gi(\u03c4 \u2217 i ) = 0."
        },
        {
            "heading": "4.1 High-Level Approach via Valid Policies.",
            "text": "We first briefly introduce the initialization algorithm. The following lemma shows what we achieve in the initialization (proved in Appendix C.1). Lemma 4.1. The initialization algorithm runs 1000 \u00b7 \u221a T log T samples for each distribution to output interval [\u2113i, ui], such that with probability 1\u2212 T\u221210 the following hold simultaneously for all i \u2208 [n]:\n\u2022 \u2113i \u2264 \u03c3i \u2264 ui. \u2022 |gi(x)| \u2264 T\u2212 1 4 simultaneously for all x \u2208 [\u2113i, ui].\nAfter initialization, the main part is the action set-updating algorithm. Similar to the algorithm for n = 2, we hope to use estimates F\u0302i(x) to gradually shrink the intervals [\u2113i, ui]. However, one major challenge is that we don\u2019t have a fixed order. If n is a constant, we can just simply try all possible permutations and use a multi-armed bandit style algorithm to find the optimal permutation. But the number of permutations is exponential in n, so this approach is impossible when n is a general parameter. To get a polynomial regret algorithm, we can only test poly(n) number of different orders.\nAnother challenge is that the idea for n = 2 can bound the regret when we play a sub-optimal threshold, but it tells nothing about playing a sub-optimal order. We don\u2019t have a direct way to bound the regret when playing an incorrect order.\nBoth difficulties imply that only keeping the confidence intervals as the constraint for the actions is not enough. Therefore, we also introduce a set of order constraints:\nDefinition 4.2 (Valid Constraint Group). Given a set of confidence intervals I = {[\u21131, u1], [\u21132, u2], ..., [\u2113n, un]} and a set S of order constraints, satisfying:\n\u2022 ui \u2212 \u2113i \u2264 T\u2212 1 4 . \u2022 \u03c3i \u2208 [\u2113i, ui] \u2022 Every constraint in S can be defined as (i, j) that means \u03c3i > \u03c3j. \u2022 The constraints in S are closed, i.e., if (i, j), (j, k) \u2208 S, there must be (i, k) \u2208 S. \u2022 If (i, j) \u2208 S, we must have ui \u2265 uj and \u2113i \u2265 \u2113j.\nFor (I, S) satisfying the conditions above, we call it a valid constraint group.\nThe intuition of the extra order constraints is: When we are shrinking the intervals, if it is evident that \u03c3i > \u03c3j , we will requireDi to be in front of Dj in the following rounds. Correspondingly, we give the following definition for a \u201cvalid\u201d policy. During the algorithm, we will only run valid policies, according to the current constraint group we have.\nDefinition 4.3 (Valid Policy). Let (\u03c4\u03c0(1), \u03c4\u03c0(2), ..., \u03c4\u03c0(n)) be a policy to play in one round, where \u03c0 is the distribution permutation for this policy, and the threshold in front of box \u03c0(i) is \u03c4\u03c0(i). For simplicity, we use \u03c0 to represent a policy.\nFor a policy \u03c0, we say it is valid for a constraint group (I, S) if the following conditions hold: \u2022 For i \u2208 [n], \u03c4\u03c0(i) \u2208 [\u2113\u03c0(i), u\u03c0(i)]. \u2022 If (i, j) \u2208 S, then Di must be in front of Dj , i.e., \u03c0\u22121(i) < \u03c0\u22121(j). \u2022 For i < j, \u03c4\u03c0(i) \u2265 \u03c4\u03c0(j).\nNotice that for a valid constraint group, we have \u03c3i \u2208 [\u2113i, ui] for all i \u2208 [n], and \u03c3i > \u03c3j for all (i, j) \u2208 S. Then, the optimal policy is valid. Therefore, we can always find a valid policy from the constraint group.\nNow, we are ready to give the main idea of the constraint-updating algorithm. In each phase, we first update the confidence intervals and then update the order constraints as follows:\n\u2022 Step 1: For each i \u2208 [n], we run O\u0303(poly(n) \u01eb2\n) samples to update the confidence interval to [\u2113\u2032i, u \u2032 i], such that for every threshold pair \u03c4i, \u03c4 \u2032 i \u2208 [\u2113\u2032i, u\u2032i], the moving difference is small, i.e., if we move \u03c4i to \u03c4 \u2032 i and keep the validity, the difference of the expected reward is bounded by\nO(poly(n) \u00b7 \u01eb). \u2022 Step 2: For each distribution pair (i, j) without a constraint, we run O\u0303(poly(n)\n\u01eb2 ) samples to\ntest the order between them, such that we can either clarify which one is bigger between \u03c3i and \u03c3j, or we can claim that the swapping difference (the difference before and after swapping Di and Dj) is bounded by O(poly(n) \u00b7 \u01eb).\nFinally, we argue that for every valid policy, we can convert it into the optimal policy by using poly(n) number of moves and swaps. This is sufficient for us to give O(poly(n) \u00b7 \u01eb) regret bound.\nIn the following analysis, we use separate sub-sections to introduce each part. Section 4.2 provides the Interval-Shrinking algorithm to bound the moving difference. Section 4.3 introduces the way to add a new order constraint to bound the swapping difference. Section 4.4 shows how to convert a valid policy to the optimal policy using a poly(n) number of moves and swaps. Finally, Section 4.5 combines the results of three sub-sections to complete the analysis."
        },
        {
            "heading": "4.2 Step 1: Interval-Shrinking to Bound Moving Difference",
            "text": "The goal of this sub-section is: Given i \u2208 [n] and an original constraint group (I, S), we want to update the confidence interval [\u2113i, ui], to make sure that moving \u03c4i inside the new confidence interval incurs a small difference. The key idea of the Interval-Shrinking algorithm is similar to the case when n = 2: For each i \u2208 [n], we want to play two different values for \u03c4i, and see the difference of the expected reward. However, playing \u03c4i = \u2113i and \u03c4i = ui might be impossible. The reason is: We hope to keep a decreasing threshold setting. There may not be a policy that allow \u03c4i to be set to ui and \u2113i without changing other thresholds. If we need different permutations to test \u03c4i = ui and \u03c4i = \u2113i, this makes the analysis involved. Therefore, we should find a policy that fixes the order and other thresholds, then test \u03c4i under this fixed policy while keeping a decreasing thresholds.\nWhen we set \u03c4i to be different values, the two policies will be different only when the maximum reward before \u03c4i falls between the two thresholds. Therefore, to see the largest difference, we hope the probability of this event is maximized. This intuition allows us to give the following definition:\nDefinition 4.4 (MoveBound Policy). Given (I, S) and i \u2208 [n], a MoveBound policy is a valid partial policy \u03c0 parameterized by \u2113 and u5, such that F\u03c0,i(u)\u2212 F\u03c0,i(\u2113) is maximized.\n5Here, we say \u03c0 is a partial policy because it\u2019s not completely fixed. We fix the permutation of the distributions and the value of all other thresholds, but the value of \u03c4i is flexible.\nIn the definition, F\u03c0,i(x) is the probability that the algorithm reaches distribution Xi with maximum value v < x in hand, i.e.,\nF\u03c0,i(x) := \u220f\nj<\u03c0\u22121(i)\nF\u03c0(j)(x).\nFurthermore, u and \u2113 represents two possible value of \u03c4i to keep a valid \u03c0, i.e., \u03c0 is valid when both \u03c4i = u and \u03c4i = \u2113.\nA key fact of MoveBound policy is that for every different distribution, we might find a different MoveBound policy. This is different from the Prophet Inequality problem: In the Pandora\u2019s Problem, we don\u2019t keep a fixed order. Every order that satisfies the constraints (I, S) is possible to be tested.\nNow, the key idea of the Interval-Shrinking algorithm is clear: For each i, find the MoveBound policy and run samples with \u03c4i = u and \u03c4i = \u2113. Then, use a method similar to Algorithm 3 to calculate the new interval. The following algorithm describes the details of this idea:\nAlgorithm 6: Interval-Shrinking Algorithm\nInput: (I, S), \u01eb, i, F\u03021(x), ..., F\u0302n(x) 1 Get an approximate MoveBound policy \u03c0\u0302 and \u2113, u using Lemma 4.8.\n2 Calculates F\u0302\u03c0\u0302,i(x). 3 For \u03c4 \u2208 [\u2113i, ui], let\n\u2206\u0302i(\u03c4) := F\u0302\u03c0\u0302,i(u) \u222b u \u03c4 (F\u0302i(x)\u2212 1)dx+ F\u0302\u03c0\u0302,i(\u2113) \u222b \u03c4 \u2113 (F\u0302i(x)\u2212 1)dx\u2212 \u222b u \u2113 F\u0302\u03c0\u0302,i(x)(F\u0302i(x)\u2212 1)dx.\n4 Run C \u00b7 \u01eb\u22122 log T samples with \u03c4i = u. Let the average reward be R\u0302u. 5 Run C \u00b7 \u01eb\u22122 log T samples with \u03c4i = \u2113. Let the average reward be R\u0302\u2113. 6 Define \u03b4\u0302i(\u03c4) := \u2206\u0302i(\u03c4)\u2212 (R\u0302u \u2212 R\u0302\u2113). 7 Let u\u2032i = max\u03c4\u2208[\u2113i,ui] |\u03b4\u0302i(\u03c4)| < \u01eb and let \u2113\u2032i = min\u03c4\u2208[\u2113i,ui] |\u03b4\u0302i(\u03c4)| < \u01eb. Output: [\u2113\u2032i, u \u2032 i]\nThen, the following lemma shows the bound when modifying a threshold:\nLemma 4.5 (Moving Difference Bound). Suppose we are given (I, S), \u01eb > 16T\u2212 1 2 , CDF estimates F\u03021(x), \u00b7 \u00b7 \u00b7 , F\u0302n(x), and i \u2208 [n], satisfying the following conditions for all j \u2208 [n]: \u2022 |gj(\u03c4)| \u2264 T\u2212 1 4 for all \u03c4 \u2208 [\u2113j, uj ].\n\u2022 (I, S) is valid. \u2022 For any valid partial policy \u03c0\u2032 of (I, S), we fix the order and the other thresholds except \u03c4j . Assume \u03c0\n\u2032 is valid when both \u03c4j = \u2113\u2032 and \u03c4j = \u2113\u2032. Define \u03b4\u03c0\u2032,u\u2032,\u2113\u2032,j(\u03c4) = (F\u03c0\u2032,j(\u2113\u2032) \u2212 F\u03c0\u2032,j(u\n\u2032))gi(\u03c4). Then |\u03b4\u03c0\u2032,u\u2032,\u2113\u2032,j(\u03c4)| \u2264 6\u01eb. \u2022 CDF estimate F\u0302j(x) is constructed via 10\n5 \u00b7 n2 log T\u01eb fresh i.i.d. samples of Xj . Then, Algorithm 6 runs O( log T\u01eb2 ) samples and calculates a new interval [\u2113 \u2032 i, u \u2032 i], such that the following properties hold with probability 1\u2212 T\u221211: (i) \u03c3i \u2208 [\u2113\u2032i, u\u2032i] (ii) Let I \u2032i = (I \\ {[\u2113i, ui]}) \u222a {[\u2113\u2032i, u\u2032i]}. For any valid partial policy \u03c0\u2032 of (I \u2032i, S), we fix the\norder and the other thresholds. Assume \u03c0\u2032 is valid when both \u03c4i = u\u2032 and \u03c4i = \u2113\u2032. Define \u03b4\u03c0\u2032,u\u2032,\u2113\u2032,i(\u03c4) = (F\u03c0\u2032,i(\u2113 \u2032)\u2212 F\u03c0\u2032,i(u\u2032))gi(\u03c4). Then |\u03b4\u03c0\u2032,u\u2032,\u2113\u2032,i(\u03c4)| \u2264 3\u01eb.\n(iii) For any valid policy of (I \u2032i, S), if we fix the order and the other thresholds, but modify \u03c4i to \u03c4 \u2032i , satisfying that the new policy is still valid, the difference of the expected reward between these two policies is less than 3\u01eb.\nBefore starting the proof, we first give an accuracy bound of the distribution estimates, which is proved in Appendix C.2.\nClaim 4.6. Assume the preconditions in Lemma 4.5 hold. Then with probability 1\u2212T\u221212, we have |\u220fi\u2208S F\u0302i(x)\u2212 \u220f i\u2208S Fi(x)| \u2264 \u221a \u01eb simultaneously hold for all x \u2208 [0, 1] and S \u2286 [n].\nProof of Lemma 4.5. Fix the MoveBound policy \u03c0. Assume we want to move \u03c4i from \u03c4i = u to \u03c4i = \u2113, such that the policies are both valid when \u03c4i = \u2113 and \u03c4i = u. Since we only care about the absolute value of the difference between two expected rewards, we may assume u > \u2113.\nIf moving \u03c4i from u to \u2113, the performance of the two policies will only be different if the previous maximum reward falls between \u2113 and u: It will reject the previous maximum if \u03c4i = u, but accept it when \u03c4i = \u2113. Besides, since \u2113 is greater than the next threshold in \u03c0, when the previous maximum is inside [\u2113, u], the algorithm must stop before the next threshold, which means the difference only comes from \u03c4i and Xi.\nRecall that F\u03c0\u0302,i(x) = \u220f j<\u03c0\u0302\u22121(i) Fj(x), i.e., F\u03c0\u0302(i)(x) is the probability that Algorithm 4 reaches \u03c4i with v \u2264 x in hand. Let f\u03c0\u0302,i(x) = F \u2032\u03c0\u0302,i(x). Then, the difference of the expected reward is\u222b u \u2113 f\u03c0\u0302,i(x)gi(x)dx = F\u03c0\u0302,i(u)gi(u)\u2212F\u03c0\u0302,i(\u2113)gi(\u2113)\u2212 \u222b u \u2113 F\u03c0\u0302,i(x)g \u2032 i(x)dx. To upper-bound this difference, define generalized bounding function\n\u03b4i(\u03c4) := \u2212 (F\u03c0\u0302,i(u)\u2212 F\u03c0\u0302,i(\u2113)) \u00b7 gi(\u03c4). (12)\nThen, to learn \u03b4i(\u03c4), we define\n\u2206i(\u03c4) := F\u03c0\u0302,i(u)(gi(u)\u2212 gi(\u03c4))\u2212 F\u03c0\u0302,i(\u2113)(gi(\u2113)\u2212 gi(\u03c4))\u2212 \u222b u\n\u2113 F\u03c0\u0302,i(x)g\n\u2032 i(x)dx\n= F\u03c0\u0302,i(u)\n\u222b u\n\u03c4 (Fi(x)\u2212 1)dx+ F\u03c0\u0302,i(\u2113)\n\u222b \u03c4\n\u2113 (Fi(x)\u2212 1)dx\u2212\n\u222b u\n\u2113 F\u03c0\u0302,i(x)(Fi(x)\u2212 1)dx.\nObserve that \u03b4i(\u03c4) = \u2206i(\u03c4)\u2212 (Ru\u2212R\u2113), where Ru and R\u2113 correspond to the expected reward in \u03c0\u0302 with \u03c4i = u and \u03c4i = \u2113 respectively. Then, by replacing Fi(x) with F\u0302i(x), we can get \u2206\u0302i(\u03c4), which is an estimate of \u2206i(\u03c4). For Ru and R\u2113, we can learn the estimates R\u0302u and R\u0302\u2113 via running samples. Combining these estimates results in \u03b4\u0302i(\u03c4). Then, the following claim shows that \u03b4\u0302i(\u03c4) estimates \u03b4i(\u03c4) accurately (proved in Appendix C.3). Claim 4.7. In Algorithm 6, if the conditions in Lemma 4.5 holds, then with probability 1 \u2212 T\u221212 we have |\u03b4\u0302i(\u03c4)\u2212 \u03b4i(\u03c4)| \u2264 \u01eb simultaneously for all \u03c4 \u2208 [\u2113i, ui].\nNow we prove the statements in Lemma 4.5. In the following proofs, we assume |\u03b4i(\u03c4)\u2212\u03b4\u0302i(\u03c4)| \u2264 \u01eb holds simultaneously for all \u03c4 \u2208 [\u2113i, ui]. Statement (i). Look at Algorithm 6: It finds \u03c0\u0302, \u2113, u, gets \u03b4\u0302i(\u03c4), then calculates [\u2113 \u2032 i, u \u2032 i] = {\u03c4 \u2208 [\u2113i, ui] : |\u03b4\u0302i(\u03c4) < \u01eb|}. Since \u03b4i(\u03c4\u2217i ) = 0, there must be |\u03b4\u0302i(\u03c4\u2217i )| \u2264 \u01eb. Therefore, \u03c4\u2217i \u2208 [\u2113\u2032i, u\u2032i]. Statement (ii). Notice that [\u2113\u2032i, u \u2032 i] = {\u03c4 \u2208 [\u2113i, ui] : |\u03b4\u0302i(\u03c4) \u2264 \u01eb|}. Therefore, for all \u03c4 \u2208 [\u2113\u2032i, u\u2032i], |\u03b4i(\u03c4)| \u2264 |\u03b4\u0302i(\u03c4)| + |\u03b4i(\u03c4) \u2212 \u03b4\u0302i(\u03c4)| \u2264 2\u01eb. We first assume that \u03c0\u0302 is an accurate MoveBound policy. Then, from the definition, we have F\u03c0\u0302,i(u)\u2212F\u03c0\u0302,i(\u2113) \u2265 F\u03c0\u2032,i(u\u2032)\u2212F\u03c0\u2032,i(\u2113\u2032) for all valid partial policy \u03c0\u2032 parameterized by u\u2032, \u2113\u2032. Therefore, |\u03b4\u03c0\u2032,u\u2032,\u2113\u2032,i(\u03c4)| \u2264 |\u03b4i(\u03c4)| \u2264 2\u01eb.\nStatement (iii). We again assume that \u03c0\u0302 is an accurate MoveBound policy. Recall that we just proved |\u03b4i(\u03c4)| \u2264 2\u01eb. Combining this with (12), we have |gi(\u03c4)| \u2264 2\u01eb(F\u03c0\u0302,i(u)\u2212F\u03c0\u0302,i(\u2113)) .\nNow, consider the policy \u03c0\u2032. Assume we first have \u03c4i = u\u2032 and we want to move it to \u03c4i = \u2113\u2032, satisfying \u2113\u2032, u\u2032 \u2208 [\u2113\u2032i, u\u2032i] and \u03c0\u2032 is valid when both \u03c4i = \u2113\u2032 and \u03c4i = u\u2032. Then, the difference of the expected reward is \u2223\u2223\u2223 \u222b u\u2032 \u2113\u2032 f\u03c0\u2032,i(x)gi(x)dx\n\u2223\u2223\u2223, and we have the following bound: \u2223\u2223\u2223\u2223\u2223 \u222b u\u2032 \u2113\u2032 f\u03c0\u2032,i(x)gi(x)dx \u2223\u2223\u2223\u2223\u2223 \u2264 |F\u03c0\u2032,i(u \u2032)\u2212 F\u03c0\u2032,i(\u2113\u2032)| max v\u2208[\u2113\u2032,u\u2032] |gi(v)| \u2264 2\u01eb, (13)\nwhere in the last inequality we use the fact that F\u03c0\u0302,i(u)\u2212 F\u03c0\u0302,i(\u2113) \u2265 |F\u03c0\u2032,i(u\u2032)\u2212 F\u03c0\u2032,i(\u2113\u2032)| when \u03c0 is a MoveBound policy, and |gi(v)| \u2264 2\u01eb(F\u03c0,i(u)\u2212F\u03c0,i(\u2113)) for all v \u2208 [\u2113 \u2032 i, u \u2032 i]. This gives an upper bound on the difference of the expected reward when we want to move \u03c4i.\nThe remaining part is to show how to get a MoveBound policy. However, since we only have CDF estimates F\u0302i(x) instead of an accurate Fi(x), there is no hope to get an accurate MoveBound policy. The following Lemma then shows that we can calculate an approximate MoveBound policy:\nLemma 4.8. There exists an algorithm with time complexity O(n \u00b72n) that calculates a MoveBound policy with an extra 4 \u221a \u01eb additive error.\nWe leave the details of the algorithm and the proof to Appendix C.4. Finally, we show that this 4 \u221a \u01eb error doesn\u2019t hurt too much for both Statement (ii) and (iii).\nDefine qi := max\n\u03c0 F\u03c0,i(u)\u2212 F\u03c0,i(\u2113), and q\u0302i := F\u03c0\u0302,i(u)\u2212 F\u03c0\u0302,i(\u2113),\nwhere \u03c0\u0302 is the approximate MoveBound policy we get via Lemma 4.8. Then, we have qi \u2264 q\u0302i+4 \u221a \u01eb. For Statement (ii), we have\n|\u03b4\u03c0\u2032,u\u2032,\u2113\u2032,i(\u03c4)| \u2264 qi max v\u2208[\u2113\u2032i,u\u2032i]\n|gi(v)| \u2264 (q\u0302i + 4 \u221a \u01eb) \u00b7 max\nv\u2208[\u2113\u2032i,u\u2032i] |gi(v)|\n\u2264 q\u0302i \u00b7 maxv\u2208[\u2113\u2032i,u\u2032i] |\u03b4i(v)| q\u0302i + 4 \u221a \u01eb \u00b7 T\u22121/4 \u2264 2\u01eb+ 4\u221a\u01eb \u00b7 T\u2212 14 < 3\u01eb.\nHere, the second line follows the definition of \u03b4i(v) and the precondition in Lemma 4.5. The third line holds because the condition |\u03b4i(v)| \u2264 2\u01eb does not require \u03c0\u0302 to be accurate, and the last inequality holds when \u01eb > 16T\u2212 1 2 .\nFor Statement (iii), following (13), we can bound the moving difference to\n\u2223\u2223\u2223\u2223\u2223 \u222b u\u2032 \u2113\u2032 f\u03c0\u2032,i(x)gi(x) \u2223\u2223\u2223\u2223\u2223 \u2264 qi maxv\u2208[\u2113\u2032i,u\u2032i] |gi(v)|.\nTherefore, the same 3\u01eb bound holds."
        },
        {
            "heading": "4.3 Step 2: Updating Order Constraints to Bound Swapping Difference",
            "text": "In this section, our goal is to verify \u03c3i and \u03c3j which one is larger, or claiming that reversing the order of Xi and Xj doesn\u2019t hurt too much. We first provide the following lemma, which shows the\ndifference of the expected reward when we swap two distributions with a same threshold:\nLemma 4.9. For a policy \u03c0, such that Xi and Xj are consecutive with \u03c4i = \u03c4j = \u03c4 , let \u2206\u03c0,i,j(\u03c4) be the change of the expected reward after swapping Xi and Xj , then\n\u2206\u03c0,i,j(\u03c4) = F\u03c0,i(\u03c4)(gi(\u03c4)(1\u2212 Fj(\u03c4)) \u2212 gj(\u03c4)(1 \u2212 Fi(\u03c4))).\nProof. Assume we have value v in hand before arrivingXi andXj . To pass the threshold, there must be v \u2264 \u03c4 . IfXi is in the front, the expected gain of openingXi is gi(v). After that, if Xi < \u03c4 , we can play Xj as well. The expected gain is Fi(v)gj(v)+ \u222b \u03c4 v fi(x)gj(x)dx = Fi(\u03c4)gj(\u03c4)\u2212 \u222b \u03c4 u Fi(x)g \u2032 j(x)dx.\nTherefore, the total expected gain fromXj andXi is gi(v)+Fi(\u03c4)gj(\u03c4)\u2212 \u222b \u03c4 u Fi(x)g \u2032 j(x)dx. Similarly,\nif Xj is in the front, the total expected gain from Xi and Xj is gj(v)+Fj(\u03c4)gi(\u03c4)\u2212 \u222b \u03c4 u Fj(x)g \u2032 i(x)dx.\nNotice that the order of Xi and Xj doesn\u2019t affect the expected gain from the distributions behind Xi and Xj. Therefore, the difference of the gain from Xi and Xj is exactly the difference of the expected reward:\n( gi(v) + Fi(\u03c4)gj(\u03c4)\u2212 \u222b \u03c4\nu Fi(x)g\n\u2032 j(x)dx ) \u2212 ( gj(v) + Fj(\u03c4)gi(\u03c4)\u2212 \u222b \u03c4\nu Fj(x)g\n\u2032 i(x)dx\n)\n= gi(v) + Fi(\u03c4)gj(\u03c4)\u2212 gj(v) \u2212 Fj(\u03c4)gi(\u03c4) + \u222b \u03c4\nu (Fj(x)(Fi(x)\u2212 1)\u2212 Fi(x)(Fj(x)\u2212 1)) dx\n= ( gi(v) + u\u2212 \u03c4 + \u222b \u03c4\nu Fi(x)dx\n) \u2212 ( gj(v) + u\u2212 \u03c4 + \u222b \u03c4\nu Fj(x)dx\n) + Fi(\u03c4)gj(\u03c4)\u2212 Fj(\u03c4)gi(\u03c4)\n= gi(\u03c4)(1 \u2212 Fj(\u03c4))\u2212 gj(\u03c4)(1 \u2212 Fi(\u03c4)).\nSince the probability that v arrives with v < \u03c4 is exactly F\u03c0,i(\u03c4), the expected difference is \u2206\u03c0,i,j(\u03c4) = F\u03c0,i(\u03c4)(gi(\u03c4)(1 \u2212 Fj(\u03c4))\u2212 gj(\u03c4)(1 \u2212 Fi(\u03c4))).\nLemma 4.9 shows the following properties:\n1. Assume \u03c3i > \u03c3j . When \u03c4 \u2208 [\u03c3j, \u03c3i], \u2206\u03c0,i,j(\u03c4) < 0, i.e., letting Xi be in the front is better. This implies: If we know the sign of \u2206\u03c0,i,j(\u03c4), and we are sure that \u03c4 is between \u03c3i and \u03c3j , then we can determine that \u03c3i and \u03c3j which one is greater. 2. Fix i, j, \u03c4 , |\u2206\u03c0,i,j(\u03c4)| is maximized when F\u03c0,i(\u03c4) is maximized. According to Property 2, we hope to test Xi and Xj with a policy \u03c0 that maximizes F\u03c0,i(\u03c4). If the difference is bounded when F\u03c0,i(\u03c4) is maximized, the swapping difference is bounded in all policies. Inspired by this, we give the definition of the SwapTest policy:\nDefinition 4.10 (SwapTest Policy). Given (I, S) and i, j \u2208 [n] with i 6= j and (i, j), (j, i) /\u2208 S. Assume we have [\u2113\u2032i, u \u2032 i], [\u2113 \u2032 j , u \u2032 j ] \u2208 I. A SwapTest policy is a pair of valid policies (\u03c0, \u03c0\u2032), such that\n\u2022 \u03c4i = \u03c4j = max{\u2113\u2032i, \u2113\u2032j}. \u2022 Xi and Xj are adjacent in both \u03c0 and \u03c0\n\u2032, but under different orders, and this is the only difference between \u03c0 and \u03c0\u2032. W.l.o.g, assume Xi is in the front in \u03c0, while Xj is in the front in \u03c0\u2032, i.e., \u03c0\u22121(i) = \u03c0\u22121(j) \u2212 1, and \u03c0\u2032\u22121(j) = \u03c0\u2032\u22121(i)\u2212 1.\n\u2022 The SwapTest policy maximizes F\u03c0,i(\u03c4) when the first two conditions are satisfied.\nThen, the algorithm for testing Xi and Xj is clear: We find the SwapTest policy for Xi and Xj , run some samples for two policies and see the difference. If the difference is too large, we can verify \u03c3i and \u03c3j which one is larger. Otherwise, we can bound the swapping difference. Algorithm 7 gives the details of this idea.\nAlgorithm 7: SwapTest Algorithm\nInput: Distribution indices i and j 1 Run Algorithm 8 to get SwapTest policy (\u03c0, \u03c0\u2032) 2 Run C \u00b7 log T n2\u01eb2 samples with policy \u03c0. Let R\u0302i,j be the average reward. 3 Run C \u00b7 log Tn2\u01eb2 samples with policy \u03c0\u2032. Let R\u0302j,i be the average reward. 4 if |R\u0302i,j \u2212 R\u0302j,i| > 40n\u01eb then 5 Add constraint (i, j) into S\u2032 if R\u0302i,j > R\u0302j,i, otherwise add constraint (j, i) into S\u2032. 6 Update S\u2032 according to the transitivity. Update I \u2032 according to the new order\nconstraints, i.e., when adding a constraint (a, b), let u\u2032b \u2190 min{u\u2032a, u\u2032b} and \u2113\u2032a \u2190 max{\u2113\u2032a, \u2113\u2032b}.\nOutput: Updated constraint group (I \u2032, S\u2032)\nAlgorithm 8: Finding SwapTest Policy\nInput: Input: (I \u2032, S\u2032), m, i, j 1 Let \u03c4 = \u03c4i = \u03c4j = max{\u2113\u2032i, \u2113\u2032j} 2 Let T = {k|(k, i) \u2208 S\u2032 \u2228 (k, j) \u2208 S\u2032 \u2228 \u2113\u2032k > \u03c4}. 3 For k \u2208 T , let \u03c4k = u\u2032k 4 For k \u2208 [n] \\ ({i, j} \u222a T ), let \u03c4k = \u2113\u2032k 5 Let \u03c0 and \u03c0\u2032 be two policies that sort the distributions in a decreasing threshold order, and\nbreak ties according to S\u2032. The only difference is: Xi is in front of Xj in \u03c0, but Xj is in front of Xi in \u03c0\n\u2032. Output: \u03c0 and \u03c0\u2032\nBefore analysing the algorithm, we point out two facts of Algorithm 7:\n\u2022 Algorithm 7 relies on Algorithm 6, i.e., we need to first run Algorithm 6 to get n new confidence intervals, then run Algorithm 7 to update order constraints. This is critical to the regret analysis. \u2022 In the SwapTest algorithm, we only test the swapping difference with \u03c4i = \u03c4j = max{\u2113\u2032i, \u2113\u2032j}, and give the difference bound only with this threshold. This is sufficient for our regret analysis.\nLemma 4.11 (Swapping Difference Bound). Given (I \u2032, S), \u01eb, and i, j \u2208 [n] with i 6= j and (i, j), (j, i) /\u2208 S, where I \u2032 is generated by Algorithm 6. Assume the preconditions in Lemma 4.1 hold. Algorithm 7 runs O( log T\nn2\u01eb2 ) samples and achieves one of the following:\n\u2022 Clarify \u03c3i and \u03c3j which one is bigger with probability 1 \u2212 T\u221212, and give a new constraint (i, j) or (j, i). \u2022 Make the following claim with probability 1 \u2212 T\u221212: For every two valid policies of (I \u2032, S), satisfying:\n\u2013 \u03c4i = \u03c4j = max{\u2113\u2032i, \u2113\u2032j}. \u2013 Xi and Xj are consecutive in both policies but in a different order. This is the only\ndifference between two policies. The difference of the expected reward between these two policies is no more than 60n\u01eb.\nProof. We first prove the theorem assuming Algorithm 8 returns an accurate SwapTest policy (\u03c0, \u03c0\u2032). According to the definition of SwapTest policy, \u03c0 and \u03c0\u2032 maximizes the probability of reaching Xi and Xj when \u03c4i = \u03c4j = max{\u2113\u2032i, \u2113\u2032j}. According to Property 2, for any valid policy, such that Xi\nand Xj are consecutive with \u03c4i = \u03c4j = max{\u2113\u2032i, \u2113\u2032j}, the swapping difference is no more than the difference between \u03c0 and \u03c0\u2032. Therefore, if we are evident that the difference between \u03c0 and \u03c0\u2032 is no more than 60n\u01eb, we can claim that this upper bounds the swapping difference between Xi and Xj for any other policy. The proof idea is the following: We run multiple samples to estimate Ri,j and Rj,i, where Ri,j is the expected reward of \u03c0 and Rj,i is the expected reward of \u03c0\n\u2032. Next, we show that |Ri,j\u2212R\u0302i,j| \u2264 10n\u01eb and |Rj,i\u2212R\u0302j,i| \u2264 10n\u01eb with probability 1\u2212T\u221212. Then, |Ri,j\u2212Rj,i| \u2264 60n\u01eb when |R\u0302i,j \u2212 R\u0302j,i| \u2264 40n\u01eb.\nNow, we bound |Ri,j \u2212 R\u0302i,j | with Hoeffding\u2019s Inequality (Theorem A.1). R\u0302i,j is an estimate of Ri,j by running N = C \u00b7 log Tn2\u01eb2 samples, and the per-round reward is bounded by [\u22120.5, 0.5]. Then, Pr [ |Ri,j \u2212 R\u0302i,j| > 10n\u01eb ] < 2 exp(\u22122N \u00b7 100n2\u01eb2/4) = 2T\u221250C . Hence, |Ri,j \u2212 R\u0302i,j| \u2264 10n\u01eb with probability 1 \u2212 T\u221213 when C > 10. Bounding |Rj,i \u2212 R\u0302j,i| is identical, and by the union bound, |Ri,j \u2212 R\u0302i,j| \u2264 10n\u01eb and |Rj,i \u2212 R\u0302j,i| \u2264 10n\u01eb simultaneously hold with probability 1\u2212 T\u221212.\nThe concentration proof above also shows that when |R\u0302i,j \u2212 R\u0302j,i| > 40n\u01eb, we can claim that w.h.p. |Ri,j \u2212 Rj,i| > 20n\u01eb. Next, we show that this is evident to clarify which of \u03c3i and \u03c3j is greater. We first introduce a special case to give the intuition: Consider the case that all other confidence intervals are disjoint with [\u2113\u2032i, u \u2032 i] or [\u2113 \u2032 j , u \u2032 j ]. W.l.o.g., assume \u03c0 (Xi in the front) is better than \u03c0\u2032 (Xj in the front). If \u03c4 = max{\u2113\u2032i, \u2113\u2032j} is between \u03c3i and \u03c3j, we can immediately claim that \u03c3i > \u03c3j according to Property 1. If \u03c4 doesn\u2019t fall between \u03c3i and \u03c3j, there must be \u03c4 < min{\u03c3i, \u03c3j}. Then, we adjust \u03c0 and \u03c0\u2032 by increasing \u03c4i and \u03c4j to min{\u03c3i, \u03c3j}. According to Lemma 4.5, these operations do not change the expected reward too much: Since we move two thresholds in each policy, the expected reward of \u03c0 can decrease by at most 6\u01eb, and the expected reward of \u03c0\u2032 can increase by at most 6\u01eb. Therefore, if the original \u03c0 is at least 20\u01eb better than \u03c0\u2032, we can still claim that \u03c3i > \u03c3j .\nHowever, this moving process can be invalid in the general case: min{\u03c3i, \u03c3j} might be greater than some thresholds in front of Xi and Xj . To fix this issue, consider the following process:\n\u2022 Step 1: Increase \u03c4i and \u03c4j until reaching \u03c4k, where Xk is the distribution just in front of Xi and Xj . \u2022 Step 2: Swap Xi and Xj with Xk. \u2022 Repeat Step 1 and 2 until \u03c4i = \u03c4j = min{\u03c3i, \u03c3j}. Let \u2206\u03c0,\u03c0\u2032 be the difference between expected values of \u03c0 and \u03c0 \u2032. We monitor the change of \u2206\u03c0,\u03c0\u2032 during these operations. Step 1 can decrease \u2206\u03c0,\u03c0\u2032 by at most 12\u01eb < 20\u01eb. Step 2 can increase the absolute value of \u2206\u03c0,\u03c0\u2032. Since there can be at most n Step 1 and 2, if initially \u2206\u03c0,\u03c0\u2032 > 20n\u01eb, this is sufficient to guarantee that \u2206\u03c0,\u03c0\u2032 > 0 at the end of the process. Then, we are evident to claim \u03c3i > \u03c3j .\nIt remains to show that Algorithm 8 returns a SwapTest policy. Besides, this policy should also guarantee that when we are swapping Xi and Xj with Xk, the policy after doing a swap is still valid. Therefore, we introduce the following lemma:\nLemma 4.12. Algorithm 8 calculates a SwapTest policy. Besides, it has the following property: Let \u03c4 = max{\u2113\u2032i, \u2113\u2032j} and \u03c4 \u2032 = min{\u03c3i, \u03c3j}. If \u03c4 \u2032 > \u03c4 , then for all k \u2208 [n] \\ {i, j}, if \u03c4k \u2208 [\u03c4, \u03c4 \u2032], there must be (k, i) /\u2208 S and (k, j) /\u2208 S.\nProof. The first two conditions in Definition 4.10 directly follows Algorithm 8. For the objective condition, observe that no distribution in the set T can be moved behind Xi and Xj . Therefore, the policy calculated by Algorithm 8 minimizes F\u03c0,i(\u03c4), which means the third condition holds.\nFor the additional property, assume there exists k satisfying \u03c4k = u \u2032 k, \u03c4k < min{\u03c3i, \u03c3j}. Notice\nthat if (k, i) \u2208 S\u2032, there must be u\u2032k \u2265 u\u2032i \u2265 min{\u03c3i, \u03c3j}, which is in contrast to the condition \u03c4k = u \u2032 k < min{\u03c3i, \u03c3j}. Therefore, (k, i) /\u2208 S\u2032. Similarly, (k, j) /\u2208 S\u2032. Therefore, the additional property in Lemma 4.12 holds.\nFinally, applying Lemma 4.12 immediately proves Lemma 4.11."
        },
        {
            "heading": "4.4 Converting our Policy to the Optimal Policy in Polynomial Steps",
            "text": "In this section, we show that using poly(n) number of moves and swaps can convert any valid policy into the optimal policy. Since Lemma 4.5 and Lemma 4.11 already show that the difference of each move and swap is bounded by O(poly(n)\u01eb), combining these results, we can argue that the per-round loss of a valid policy is bounded by O(poly(n)\u01eb). Formally, we give the following lemma:\nLemma 4.13. Given a valid constraint group (I, S). For a valid policy of (I, S), we use a \u201cmove\u201d to represent the action that modifies a single threshold, and guarantees that the policy after modifying the threshold is still valid. Besides, we use a \u201cswap\u201d to represent the action that swaps two consecutive distributions with the same threshold. This threshold should be equal to the maximum of the two lower confidence bounds, and the policy after swapping the distributions should still be valid.\nFor any valid policy of (I, S), it can be converted into the optimal policy using 2n2 moves and 2n2 swaps.\nProof. Let \u03c0 be the policy that \u03c4i = \u2113i for all i \u2208 [n], and the distributions are sorted in a decreasing order of \u03c4 . Since for every constraint (i, j) \u2208 S, we have \u2113i \u2265 \u2113j, \u03c0 must be a valid policy.\nWe can prove Lemma 4.13 by showing the following statement: Starting from the policy \u03c0, we can move it to any valid policy \u03c0\u2032 using n2 moves and n2 swaps:\n\u2022 Step 1: Let i = argmaxi \u03c4 \u2032 i , where \u03c4 \u2032 i is the threshold of Xi in policy \u03c0 \u2032. \u2022 Step 2: If Xi is not the first distribution in \u03c0, move \u03c4i to \u03c4\u03c0\u03c0\u22121(i)\u22121 , then swap Xi and X\u03c0\u03c0\u22121(i)\u22121 . \u2022 Step 3: Do Step 2 until Xi is moved to the first place. Then move \u03c4i to \u03c4 \u2032 i . \u2022 Step 4: Ignore Xi in both \u03c0 and \u03c0 \u2032, repeat Step 1, 2 and 3 until every distribution is settled.\nEach distribution only involves in n swaps and n moves, so the total number of moves and swaps are both bounded by n2. Then, we need to show the validity of every operation. For each move, we increase \u03c4i to let it be closer to \u03c4 \u2032 i . Since \u03c4 \u2032 i \u2208 [\u2113i, ui], every move is valid. For each swap, the threshold in the front must reach its lower confidence bound. Besides, every swap happens only when there is no constraint between two distributions, so every swap is valid.\nFinally, notice that every operation is bidirected. It means that starting from any valid policy \u03c0\u2032, we can convert it to the policy \u03c0, and then convert it to the optimal policy using 2n2 moves and swaps, which finishes the proof."
        },
        {
            "heading": "4.5 Putting Everything Together",
            "text": "In this section, we show how to combine Algorithm 6 and Algorithm 7 to generate a new valid constraint group (I \u2032, S\u2032), then proves that this leads to an O\u0303(poly(n) \u221a T ) regret algorithm. We first give the one-phase algorithm:\nWe can directly give the following lemma according to the three lemmas above:\nAlgorithm 9: Constraint Updating Algorithm for Pandora\u2019s Box\nInput: I = {[\u21131, u1], ..., [\u2113n, un]}, S = {(i, j)}, F\u03021(x), . . . , F\u0302n(x), m 1 //STEP 1: Calculate new confidence interval for each distribution 2 for i \u2208 [n] do 3 For j \u2208 [n], construct F\u0302j(x) using 105 \u00b7 n 2 log T \u01eb new i.i.d. samples of Xj 4 Run Algorithm 6 with new CDF estimates to get \u2113\u2032i and u \u2032 i.\n5 //Adjust the confidence intervals to meet constraints in S. 6 for (i, j) \u2208 S do 7 Let \u2113\u2032i = max{\u2113\u2032i, \u2113\u2032j} and u\u2032j = min{u\u2032j , u\u2032i}. 8 Let I \u2032 = {[\u2113\u2032i, u\u2032i]} and S\u2032 = S 9 //Add new constraints for disjoint confidence intervals\n10 for (i, j) /\u2208 S\u2032 do 11 if \u2113\u2032i > u \u2032 j then Add (i, j) into S \u2032 ;\n12 13 //STEP 2: Calculate new constraints for each distribution pair 14 Let Q = {(i, j)|(i, j) /\u2208 S\u2032 \u2227 (j, i) /\u2208 S\u2032} 15 while Q 6= \u2205 do 16 Choose (i, j) \u2208 Q and remove (i, j) from Q 17 Run Algorithm 7 with input (i, j) and update I \u2032 and S\u2032 18 //New constraints may fail some previous tests. Should add them back 19 For every k such that \u2113\u2032k changes in Algorithm 7, if \u2203k\u2032 such that (k, k\u2032), (k\u2032, k) /\u2208 S\u2032,\nadd (k, k\u2032) into Q.\nOutput: (I \u2032, S\u2032)\nLemma 4.14 (Main Lemma). Given (I, S) and \u01eb > 16T\u2212 1 2 . Assume the pre-conditions in Lemma 4.5 hold, i.e.,\n\u2022 |gj(\u03c4)| \u2264 T\u2212 1 4 for all \u03c4 \u2208 [\u2113j, uj ]. \u2022 (I, S) is valid. \u2022 For any valid partial policy \u03c0\u2032 of (I, S), we fix the order and the other thresholds except \u03c4j . Assume \u03c0\n\u2032 is valid when both \u03c4j = \u2113\u2032 and \u03c4j = \u2113\u2032. Define \u03b4\u03c0\u2032,u\u2032,\u2113\u2032,j(\u03c4) = (F\u03c0\u2032,j(\u2113\u2032) \u2212 F\u03c0\u2032,j(u\n\u2032))gi(\u03c4). Then |\u03b4\u03c0\u2032,u\u2032,\u2113\u2032,j(\u03c4)| \u2264 6\u01eb. \u2022 CDF estimate F\u0302j(x) is constructed via 10\n5 \u00b7 n2 log T\u01eb fresh i.i.d. samples of Xj . Then, Algorithm 9 runs O(n log T\n\u01eb2 ) rounds, such that the policy in each round is valid for (I, S)\n(except Line 3), and output a new constraint group (I \u2032, S\u2032), satisfying the following statements with probability 1\u2212 T\u221210:\n\u2022 (I \u2032, S\u2032) is valid. \u2022 For all j \u2208 [n], for any valid partial policy \u03c0\u2032 of (I \u2032, S\u2032), we fix the order and the other thresholds except \u03c4j. Assume \u03c0\n\u2032 is valid when both \u03c4j = \u2113\u2032 and \u03c4j = \u2113\u2032. Define \u03b4\u03c0\u2032,u\u2032,\u2113\u2032,i(\u03c4) = (F\u03c0\u2032,j(\u2113\n\u2032)\u2212 F\u03c0\u2032,j(u\u2032))gi(\u03c4). Then |\u03b4\u03c0\u2032,u\u2032,\u2113\u2032,i(\u03c4)| \u2264 3\u01eb. \u2022 For a valid policy of (I \u2032, S\u2032), the per-round regret is no more than 126n3\u01eb.\nProof. In this proof, we assume Lemma 4.5 and Lemma 4.11 holds. We use Lemma 4.5 for no more than n times and Lemma 4.11 for no more than n2 times. By the union bound6, our proof fails\n6We assume T > 10n, otherwise an O(n) regret algorithm is trivial.\nwith probability at most n \u00b7 T\u221211 + n2 \u00b7 T\u221212 \u2264 T\u221210. For the validity of (I \u2032, S\u2032), the statement \u03c3i \u2208 [\u2113\u2032i, u\u2032i] follows Lemma 4.5, and the statement \u03c3i > \u03c3j for all (i, j) \u2208 S\u2032 follows Lemma 4.11. All other statements hold by definition. Therefore, (I \u2032, S\u2032) is valid.\nFor the bound of |\u03b4\u03c0\u2032,u\u2032,\u2113\u2032,i(\u03c4)|, it\u2019s guaranteed directly by Lemma 4.5. Notice that Lemma 4.5 even provides a stronger bound for the constraint group (I \u2032i, S). Since all possible choices of \u03c0 \u2032, \u2113\u2032, u\u2032 must be valid for (I \u2032i, S) when it\u2019s valid for (I \u2032, S\u2032), this doesn\u2019t hurt the statement.\nFor the per-round regret bound, Lemma 4.5 says that the difference of a move is bounded by 3\u01eb. Lemma 4.11 says that the difference of a swap is bounded by 60n\u01eb. Then, according to Lemma 4.13, we can convert any valid policy to the optimal policy using 2n2 moves and swaps. Therefore, the per-round regret is bounded by 126n3\u01eb.\nNext, we argue that Algorithm 9 runs no more than O(n log T \u01eb2 ) rounds. Note that Algorithm 6\nis called n times, and Algorithm 6 uses O( log T \u01eb2 ) rounds in one call. So the number of rounds is O(n log T \u01eb2\n). For Algorithm 7, we might test a distribution pair (Xi,Xj) for multiple times. The reason is the following: When using Lemma 4.13, we need to make sure that the value of the final max{\u2113\u2032i, \u2113\u2032j} is the one that we test. Therefore, if the value of \u2113\u2032i changes, we need to re-test some distribution pairs (i, j). We can argue that the total number of tests is bounded: When doing an extra test for (i, j), at least one of \u2113\u2032i or \u2113 \u2032 j must change. This can happen only when a new constraint related to i or j is added into S\u2032. There are only 2n constraints related to i and j, so we can test (i, j) for at most 4n times. Therefore, the total number of calls of Algorithm 7 is no more than 4n3, and Algorithm 7 uses O( log T\nn2\u01eb2 ) samples in one call, so the number of samples is bounded\nby O(n log T \u01eb2 ). Combining the two results finishes the proof.\nNow, we are ready to show the total regret bound.\nTheorem 4.15. There exists an O(n4.5 \u221a T log T ) regret algorithm for Pandora\u2019s Box problem.\nProof. We run Algorithm 2 and then use Lemma 2.5 to bound the main part of the total regret. To run Algorithm 2, we require the pre-conditions listed in Lemma 4.14 hold. We discuss them separately:\n\u2022 |gj(\u03c4)| \u2264 T\u22121/4: This is guaranteed by Lemma 4.1. \u2022 (I, S) is valid: For the first phase, the condition \u03c4\u2217i \u2208 [\u2113i, ui] is guaranteed by Lemma 4.1, and we don\u2019t have any initial order constraints between distributions (except those distributions with disjoint confidence intervals). Therefore, (I, S) is valid for the first phase. Starting from the second phase, this is guaranteed by Lemma 4.14. \u2022 |\u03b4\u03c0\u2032,u\u2032,\u2113\u2032,i(\u03c4)| \u2264 6\u01eb: For the first phase, this is true because |\u03b4\u03c0\u2032,u\u2032,\u2113\u2032,i(\u03c4)| \u2264 |g(\u03c4)| \u2264 T\u2212 1 4 , and\ninitially we have \u01eb = O(1). Starting from the second phase, this is from Lemma 4.14 regarding the previous phase. Notice that parameter \u01eb in the new phase is exactly \u01eb2 in the previous phase. Therefore, there is an extra 2 factor in the condition.\n\u2022 New CDF estimates: This is guaranteed by Algorithm 9. Lemma 4.14 implies that after O(n 7 log T \u01eb2 ) rounds, the one-round regret in the new constraint\ngroup is bounded by \u01eb. Applying Lemma 2.5 with \u03b1 = 7, we have the O(n3.5 \u221a T log T ) regret bound.\nBesides, there are some extra rounds not covered by Lemma 2.5, including the initialization and the CDF estimates construction (Line 3 in Algorithm 9). For the initialization, Lemma 4.1 runs O(n \u221a T log T ) samples, so the regret is O(n \u221a T log T ). For the CDF estimates construction, let k\nbe the number of phases in the doubling algorithm. Then, the total number of samples is\nk\u2211\ni=1\nn \u00b7 O(n 2 log T\n\u01ebi ) = O(\n\u221a T )\nCombining three parts of regret, the total regret is O(n3.5 \u221a T log T ).\nFinally, recall that until now we are working on a scaled Pandora\u2019s Box problem: We scale down the values and the costs by a factor of 2n. Therefore, for the original problem, the final regret bound is O(n4.5 \u221a T log T )."
        },
        {
            "heading": "4.6 Making the Algorithm Efficient",
            "text": "Currently, the running time of the whole algorithm is exponential in n as just Lemma 4.8 introduces an algorithm with O(n2n) running time. If we want a polynomial time algorithm, we may need an approximation. The following lemma shows a new regret bound with approximation:\nLemma 4.16. Assume for every i, we can \u03b3-approximate max\u03c0,u,\u2113 F\u0302\u03c0,i(u) \u2212 F\u0302\u03c0,i(\u2113), then there exists an O(max{\u03b3n4.5, \u03b32n} \u221a T log T ) regret algorithm.\nProof. In this proof, we first discuss the problem for the scaled Pandora\u2019s Box problem, and add the scaled 2n factor back at last.\nWe first see how the \u03b3 approximation changes Lemma 4.5. Recall that qi = max\u03c0 F\u03c0,i(u) \u2212 F\u03c0,i(\u2113). We further define q\u0303i = max\u03c0 F\u0302\u03c0,i(u) \u2212 F\u0302\u03c0,i(\u2113) and q\u0304i = F\u03c0\u0302,i(u) \u2212 F\u03c0\u0302,i(\u2113), where \u03c0\u0302 is the chosen policy that \u03b3-approximates max\u03c0,u,\u2113 F\u0302\u03c0,i(u) \u2212 F\u0302\u03c0,i(\u2113). According to Claim 4.6, we have q\u0303i \u2265 qi \u2212 2 \u221a \u01eb and q\u0304i \u2265 q\u0303i\u03b3 \u2212 2 \u221a \u01eb. So qi \u2264 \u03b3q\u0304i + (2\u03b3 + 2) \u221a \u01eb. According to (13), Statement (ii) and (iii) are both bounded by\nqi max v\u2208[\u2113\u2032i,u\u2032i]\n|gi(v)| \u2264 (\u03b3q\u0304i + (2\u03b3 + 2) \u221a \u01eb) max\nv\u2208[\u2113\u2032i,u\u2032i] |gi(v)|\n\u2264 \u03b3q\u0304i \u00b7 2\u01eb\nq\u0304i + (2\u03b3 + 2)\n\u221a \u01eb \u00b7 T\u2212 14 \u2264 3\u03b3\u01eb.\nFor Statement (ii), this changes the bound of |\u03b4\u03c0\u0302,u\u2032,\u2113\u2032,i(\u03c4)| to O(\u03b3\u01eb). In our proof, we use this bound when proving Claim 4.7: The bound of |\u03b4\u03c0\u0302,u\u2032,\u2113\u2032,i(\u03c4)| provides a bound for the variance of the \u2206i(\u03c4) function, and then we use Bernstein Inequality to show |\u2206\u0302i(\u03c4) \u2212\u2206i(\u03c4)| \u2264 O(\u01eb). When the bound changes to O(\u03b3\u01eb), to get an O(\u01eb) approximation of \u2206i(\u03c4), the number of samples for constructing CDF estimates should be multiplied by \u03b32, leading to an O(\u03b32 \u221a T log T ) regret bound.\nFor Statement (iii), notice that we need to use this moving difference to bound the swapping difference. The main idea of the original proof is: Assume we want to test Xi and Xj . After O(n) moves, we can adjust \u03c4i and \u03c4j to min{\u03c3i, \u03c3j}, then bound the swapping difference by O(n) \u00b7O(\u01eb). Since there is an extra \u03b3 factor in the new moving difference bound, the new swapping difference should be O(\u03b3n\u01eb).\nNext, Lemma 4.13 shows that we need 2n2 move operations and swap operations to convert a policy to the optimal one, so the new regret bound after O(n log T\n\u01eb2 ) samples is O(\u03b3n3\u01eb). Then,\nthe parameter \u03b1 in Lemma 2.5 changes to \u03b32n7, so the total regret from the doubling algorithm is O(\u03b3n3.5 \u221a T log T ).\nFinally, after combining these two new regret bounds and adding the scaled 2n factor back to the regret bound, we get the O(max{\u03b3n4.5, \u03b32n2} \u221a T log T ) final regret bound.\nLemma 4.16 shows that: If we can get a poly(n) approximation for the MoveBound policy in polynomial time, we can still get an O(poly(n) \u221a T ) regret algorithm. To achieve this goal, we introduce the following sub-routine:\nDefinition 4.17 (sub-routine). Let Problem A be the following: Given n and real numbers a1, ..., an, b1, ..., bn, satisfying 0 \u2264 ai \u2264 bi \u2264 1 for all i \u2208 [n]. The objective of Problem A is to calculate\nmax B\u2208[n]\n\u220f i\u2208B bi \u2212 \u220f i\u2208B ai.\nunder a set of constraints {(i, j)}, where a constraint (i, j) means that if we have i \u2208 B, there must be j \u2208 B.\nLemma 4.18. If there exists an algorithm that calculates an \u03b3-approximation for Problem A, then there exists an algorithm that \u03b3-approximates max\u03c0 F\u0302\u03c0,i(u) \u2212 F\u0302\u03c0,i(\u2113). If the running time of the algorithm for approximating Problem A is polynomial, then the algorithm for approximating F\u0302\u03c0,i(u)\u2212 F\u0302\u03c0,i(\u2113) is also polynomial.\nProof. Consider calculating a MoveBound policy for Xi. Assume that we know the value of \u2113 and u. Then, we only need to pick a subset B \u2286 [n] \\ {i} to maximize \u220fj\u2208B bj \u2212 \u220f j\u2208N aj , where bj = F\u0302j(u), and aj = F\u0302j(\u2113).\nHowever, not all subsets B are valid. Firstly, for j \u2208 B, there must be \u03c4j \u2265 u, which means uj \u2265 u is required. Similarly, we should also guarantee that \u2113j \u2264 \u2113 for all j \u2208 [n] \\ B. Besides, if there is an order constraint (j, k), then k \u2208 B implies j \u2208 B, which can be represented as a constraint in Problem A. If all constraints are satisfied, policy \u03c4j = uj for j \u2208 B and \u03c4j = \u2113j for j /\u2208 B \u222a {i} is a feasible policy. Therefore, finding the optimal policy with fixed \u2113 and u is captured by Problem A. So, an \u03b3-approximation algorithm for Problem A also \u03b3-approximates F\u0302\u03c0,i(u)\u2212 F\u0302\u03c0,i(\u2113).\nNotice that when maximizing F\u0302\u03c0,i(u)\u2212F\u0302\u03c0,i(\u2113), we want to push the thresholds to the boundaries to give \u03c4i enough space. Therefore, the value of \u2113 and u must be equal to some \u2113j or uj, which means that there are only O(n2) candidates. Therefore, if the algorithm that \u03b3- approximates Problem A runs in polynomial time, the running time of the algorithm for approximating F\u0302\u03c0,i(u) \u2212 F\u0302\u03c0,i(\u2113) is also polynomial.\nIt remains to give a poly(n)-approximation algorithm for Problem A, with O(poly(n)) running time. The following theorem shows that this is possible:\nLemma 4.19. Given an instance of Problem A. Let Bj be the subset with the smallest size that contains j. Let qj = \u220f i\u2208Bj bi \u2212 \u220f i\u2208Bj ai Then, maxj qj is an n-approximation of problem A.\nProof. Construct a graph G = (V,E), such that V = [n], and E is the set of all constraints, i.e., a constraint (u, v) is represented as a directed edge (u, v) \u2208 E. Then, Bj is the set of all vertices which is reachable from j.\nNotice that when G contains a connected component, we can shrink the component into one single vertex, because picking any single vertex in the connected component means picking the\nwhole component. Therefore, we only need to prove the theorem when G is a directed acyclic graph (DAG).\nRe-index the vertices in G, to make sure that for every edge (u, v) \u2208 E, there must be u > v. Besides, make sure that B\u2217 = {1, ..., k} is exactly the optimal set of Problem A. Then,\n\u220f\ni\u2208[k] bi \u2212\n\u220f i\u2208[k] ai = \u2211 j\u2208[k]\n  j\u22121\u220f\ni=1\nbi \u00b7 (bj \u2212 aj) \u00b7 k\u220f\ni=j+1\nai\n \n\u2264 \u2211\nj\u2208[k]\n  \u220f\ni\u2208Bj\\{j} bi \u00b7 (bj \u2212 aj) \u00b7 1\n \n\u2264 \u2211\nj\u2208[k]\n  \u220f\ni\u2208Bj bi \u2212\n\u220f i\u2208Tj ai\n  \u2264\n\u2211 j\u2208[n] qj,\nwhere the second-last inequality uses ai \u2264 bi. Therefore, maxj qj is an n-approximation of \u220f\ni\u2208B\u2217 bi\u2212\u220f i\u2208B\u2217 ai.\nFinally, combining Lemma 4.16, Lemma 4.18, and Lemma 4.19 gives the following main theorem: Theorem 1.3. There is a polytime algorithm with O(n5.5 \u221a T log T ) regret for the Bandit Pandora\u2019s Box problem where we only receive utility (selected value minus total cost) as feedback."
        },
        {
            "heading": "5 Lower Bounds",
            "text": "In this section we prove lower bounds for Online Learning Prophet Inequality and Online Learning Pandora\u2019s Box. Our lower bounds will hold even against full-feedback.\n5.1 \u2126( \u221a T ) Lower Bound for Stochastic Input We show an \u2126( \u221a T ) regret lower bound for Bandit Prophet Inequality and an \u2126( \u221a nT ) lower bound\nfor Pandora\u2019s Box problem, which implies that the \u221a T factor in our regret bounds is tight. We first give the lower bound for Prophet Inequality.\nTheorem 5.1. For Bandit Prophet Inequality there exists an instance with n = 2 such that all online algorithms incur \u2126( \u221a T ) regret.\nProof. Let D1 be a distribution that always gives 12 . Let D2 be a Bernoulli distribution. The probability of X2 = 1 might be 1 2 + 1\u221a T or 12 \u2212 1\u221aT . Both settings appear w.p. 1 2 . The online\nalgorithm doesn\u2019t know which is the real setting. If it chooses not to open X2, it will lose \u221a T w.p. 1 2 . Otherwise, because of the variance, the algorithm needs \u2126(T ) samples from X2 to learn the real\nsetting, and loses 12 \u00b7 \u221a T for each round it runs. In both cases, the online algorithm should lose\n\u2126( \u221a T ), which finishes the proof.\nFor the Pandora\u2019s Box problem, [GHTZ21] already shows a lower bound for the sample complexity of Pandora\u2019s Box problem, which directly implies a lower bound for the online learning setting.\nTheorem 5.2 ([GHTZ21]). For any instance of Pandora\u2019s problem in which the rewards are bounded in [0, 1], running \u2126( n\n\u01eb2 ) samples is necessary to get an \u01eb-additive algorithm.\nCorollary 5.3. For Pandora\u2019s Box problem, all online algorithms incur \u2126( \u221a nT ) regret. Proof. Assume there exists an online algorithm that achieves o( \u221a nT ) regret. This implies that after T rounds, we can achieve o( n\u221a T ) per-round regret, which is in contradiction with Theorem 5.2.\nWe remark that [GHTZ21] claims that \u2126( n \u01eb2 ) samples are necessary to get an \u01eb-additive algorithm for Prophet Inequality but without giving a proof. However, this claim seems incorrect since in an ongoing work we show an O\u0303( \u221a T ) regret algorithm for Prophet Inequality with full-feedback."
        },
        {
            "heading": "5.2 \u2126(T ) Lower Bound for Adversarial Input",
            "text": "In this paper, we study Bandit Prophet Inequality and Bandit Pandora\u2019s Box problems under the stochastic assumption that input is drawn from unknown-but-fixed distributions. A natural extension would be: can we obtain o(T ) regret for adversarial inputs where the the input distribution may change in each time step? The following theorems shows that sub-linear regret is impossible even for oblivious adversarial inputs with n = 2 under full-feedback.\nTheorem 5.4. For Bandit Prophet Inequality with oblivious adversarial inputs, there exists an instance with n = 2 such that the optimal fixed-threshold strategy has total value 34T but no online algorithm (even under full-feedback) can obtain total value more than 12T .\nProof. We first introduce a notation used in this proof. Let s be a 01-string. Define Bin(s) to be the binary decimal corresponding to s. For example, Bin(1) = (0.1)2 = 1 2 , Bin(0011) = (0.0011)2 = 3 16 .\nNow, we introduce the main idea of the counter example: At the beginning, the adversary will choose a T -bits code s = s1s2...sT uniformly at random (i.e., si is set to be 0 or 1 w.p. 1 2 independently). The value of X1 is 1 2 plus a small bias that contains the information of the code. The value of X2 is either 1 or 0, which is decided by the code. Formally, in the i-th round:\n\u2022 X1 = 1 2 + \u01eb \u00b7 vi, where \u01eb is an arbitrarily small constant that doesn\u2019t effect the reward, and vi\nis a value between Bin(s1s2...si\u22121 +0+ 1T\u2212i) and Bin(s1s2...si\u22121 + 1+ 0T\u2212i). The notation 0k represents a length-k string with all 0s, and 1k represents a length-k string with all 1s.\n\u2022 X2 = 1 if si = 0, otherwise X2 = 1. For an online algorithm, it only knows that the next si can be 0 or 1 w.p. 1 2 . Therefore, no matter it switches to the next box or not, it can only get 12 in expectation. So the maximum total reward it can achieve is 12T .\nHowever, if we know the code, playing \u03c4 = 12 + \u01eb \u00b7Bin(s) gets 34T : X2 = 1 when X1 < \u03c4 , while X2 = 0 when X1 \u2265 \u03c4 . Therefore, playing \u03c4 allows us to pick every 1, but stays in X1 = 12 when X2 = 0. Since we generate the code uniformly at random, X2 is 1 w.p. 1 2 . Therefore, the expected reward is T \u00b7 (12 \u00b7 1 + 12 \u00b7 12 ) = 34T .\nNext, we use a similar proof idea to prove lower bound for Pandora\u2019s Box. This resolves an open question of [Ger22, GT22] on whether sublinear regrets are possible for Online Learning of Pandora\u2019s Box with adversarial inputs.\nTheorem 5.5. For Bandit Pandora\u2019s Box with oblivious adversarial inputs, there exists an instance with n = 2 such that the optimal fixed-threshold strategy has total utility 14T but no online algorithm (even under full-feedback) can obtain total utility more than 0.\nProof. At the beginning, the adversary will choose a T -bits code s = s1s2...sT uniformly at random (si is set to be 0 or 1 w.p. 1 2 independently). The cost c1 is 0, and the value of X1 is 0 plus a small bias that contains the information of the code. The cost c2 is 1 2 , and the value of X2 is either 1 or 0, which is decided by the code. Formally, in the i-th round:\n\u2022 X1 = 0+ \u01eb \u00b7 vi, where \u01eb is an arbitrarily small constant that doesn\u2019t effect the reward, and vi is a value between Bin(s1s2...si\u22121 +0+ 1T\u2212i) and Bin(s1s2...si\u22121 + 1+ 0T\u2212i). The notation 0k represents a length-k string with all 0s, and 1k represents a length-k string with all 1s. \u2022 X2 = 1 if si = 0, otherwise X2 = 1.\nThe cost of X1 is 0, so we can always first open X1. Then, for an online algorithm, it doesn\u2019t know whether X2 is 1 or 0. No matter it opens X2 or not, the expected reward will only be 0.\nHowever, when we know the code, playing \u03c4 = \u01eb \u00b7 Bin(s) gets 14T , because it will open X2 whenever X2 = 1, and skip it when X2 is 0. Since we generate the code uniformly at random, X2 is 1 w.p. 12 . Therefore, the expected reward is T \u00b7 (12 \u00b7 (1\u2212 12)) = 14T ."
        },
        {
            "heading": "A Basic Probabilistic Inequalities",
            "text": "Theorem A.1 (Hoeffding\u2019s Inequality). Let X1, . . . ,XN be independent random variables such that ai \u2264 Xi \u2264 bi. Let SN = \u2211 i\u2208[N ]Xi. Then for all t > 0, we have Pr [|SN \u2212E [Sn] | \u2265 t] \u2264 2 exp ( \u2212 2t2\u2211\ni\u2208[n](bi\u2212ai)2 ) . This implies, that if Xi are i.i.d. samples of random variable X, and\na = ai, b = bi for all i \u2208 [N ], let X\u0302 := 1N \u2211 i\u2208[N ]Xi, then for every \u03b5 > 0,\nPr [ |X\u0302 \u2212E [X] | \u2265 \u03b5 ] \u2264 2 exp ( \u2212 2N\u03b5 2 (b\u2212 a)2 ) .\nTheorem A.2 (Bernstein Inequality). Given mean zero random variables {Xi}Ni=1 with P(|Xi| \u2264 c) = 1 and VarXi \u2264 \u03c32i . If X\u0304N denotes their average and \u03c32 = 1N \u2211n i=1 \u03c3 2 i , then\nP(|X\u0304N | \u2265 \u03b5) \u2264 2 exp ( \u2212 N\u03b5 2\n2\u03c32 + 2c\u03b5/3\n) .\nTheorem A.3 (DKW Inequality). Given a natural number N , let X1, . . . ,Xn be i.i.d. samples with cumulative distribution function F (\u00b7). Let F\u0302 (\u00b7) be the associated empirical distribution function F\u0302 (x) := 1N \u2211 i\u2208[N ] 1Xi\u2264x. Then, for every \u03b5 > 0, we have\nPr [ sup x |F\u0302 (x)\u2212 F (x)| > \u03b5 ] \u2264 2 exp(\u22122N\u03b52)."
        },
        {
            "heading": "B Missing Proofs from Section 2",
            "text": "B.1 Proof of Lemma 2.5\nProof. There are two different sources of regret. We bound them separately.\nLoss 1 from the while loop: The main idea of the proof is to use the regret bound from the previous phase to bound the total regret in the next phase. Specifically, assume \u01eb0 = O(1) be the maximum possible one-round regret, and assume there are k phases in the while loop. Then the total regret can be bounded by\nk\u2211\ni=1\nO\n( n\u03b1 log T\n\u01eb2i\n) \u00b7 \u01ebi\u22121 = k\u2211\ni=1\nO\n( n\u03b1 log T\n\u01ebi\n) = O(n\u03b1/2 \u221a T ). (14)\nTherefore, the total regret from the while loop is bounded by O(n\u03b1/2 \u221a T ).\nLoss 2 after the while loop: After the while loop, the one-round regret is bounded by \u01ebk = n\u03b1/2 log T\u221a\nT ,\nso the total regret can be bounded by O(n \u03b1/2 log T\u221a\nT ) \u00b7 T = O(n\u03b1/2\n\u221a T log T ).\nFinally, combining the two sources of regret proves the theorem.\nBesides, we should also verify that Algorithm 2 succeeds with probability 1\u2212T\u22129, and it runs no more than O(T ) rounds. For the success probability, Algorithm 2 runs k = O(log T ) < T rounds, and the subroutine Alg succeeds with probability 1 \u2212 T\u221210. By the union bound, Algorithm 2 succeeds with probability 1 \u2212 T\u22129. As for the number of rounds, in the while loop, Algorithm 2 runs\nk\u2211\ni=1\nn\u03b1 log T\n\u01eb2i \u2264 4n\n\u03b1 log T\n\u01eb2k = O(T )\nnumber of rounds. Therefore, Algorithm 2 is a valid algorithm with respect to time horizon T .\nB.2 Missing Details of Pandora\u2019s Box Algorithm for n = 2\nB.2.1 Proof of Lemma 2.7\nTo prove Lemma 2.7, we need two claims. The first claim says that when we have a good guess \u03c4 with a small \u03b4(\u03c4), the loss of playing \u03c4 is bounded:\nClaim 2.8. If \u03c4, \u03c4\u2217 \u2208 [\u2113, u] then R(\u03c4\u2217)\u2212R(\u03c4) \u2264 |\u03b4(\u03c4)|.\nProof. We first upper-bound R(\u03c4\u2217)\u2212R(\u03c4). The two settings are different only when X1 is between \u03c4 and \u03c4\u2217: Playing \u03c4 will loss an extra |g(X1)|. Since g(x) is monotone, we can bound |g(X1)| by |g(\u03c4)|. Therefore, the extra loss of playing \u03c4 is no more than |F1(\u03c4\u2217)\u2212 F1(\u03c4)||g(\u03c4)|.\nOn the other hand,\n|\u03b4(\u03c4)| = (F1(u)\u2212 F1(\u2113)) \u2223\u2223\u2223\u2223\u2223 \u222b \u03c4\u2217 \u03c4 g\u2032(x)dx \u2223\u2223\u2223\u2223\u2223 = (F1(u)\u2212 F1(\u2113)) \u00b7 |g(\u03c4)|.\nWhen \u03c4, \u03c4\u2217 \u2208 [\u2113, u], F1(u)\u2212 F1(\u2113) \u2265 |F1(\u03c4\u2217)\u2212 F1(\u03c4)|. Therefore, |\u03b4(\u03c4)| \u2265 R(\u03c4\u2217)\u2212R(\u03c4).\nThe second claim shows that we can get a good estimate for function \u03b4(\u03c4):\nClaim 2.9. In Algorithm 3, if the conditions in Lemma 2.7 hold, then with probability 1 \u2212 T\u221210 |\u03b4\u0302(\u03c4)\u2212 \u03b4(\u03c4)| \u2264 4\u01eb simultaneously for all \u03c4 \u2208 [\u2113, u].\nProof. Recall that \u03b4(\u03c4) = \u2206(\u03c4) \u2212 (R(u)\u2212R(\u2113)). We will give the bound for |\u2206(\u03c4\u2217) \u2212 \u2206(\u03c4)|, |R(u)\u2212 R\u0302(u)| and |R(\u2113)\u2212 R\u0302(\u2113)| separately.\nFor |\u2206(\u03c4\u2217)\u2212\u2206(\u03c4)|, we first bound the magnitude of \u2206(\u03c4):\n\u2206(\u03c4) =\n\u222b u\n\u03c4 (F1(u)\u2212 F1(x))(F2(x)\u2212 1)dx\u2212\n\u222b \u03c4\n\u2113 (F1(x)\u2212 F1(\u2113))(F2(x)\u2212 1)dx,\nwhich implies\n|\u2206(\u03c4)| \u2264 (F1(u)\u2212 F1(\u03c4)) \u222b u\n\u03c4 (1\u2212 F2(x))dx + (F1(\u03c4)\u2212 F1(\u2113))\n\u222b \u03c4\n\u2113 (1\u2212 F2(x))dx (15)\n= (F1(u)\u2212 F1(\u03c4))(g(\u03c4) \u2212 g(u)) + (F1(\u03c4)\u2212 F1(\u2113))(g(\u2113) \u2212 g(\u03c4)) (16) \u2264 (F1(u)\u2212 F1(\u2113))(g(\u2113) \u2212 g(u)) (17) \u2264 |\u03b4(u)| + |\u03b4(\u2113)| \u2264 32\u01eb, (18)\nwhere the last equality follows from the bound |\u03b4(\u03c4)| \u2264 16\u01eb for all \u03c4 \u2208 [\u2113, u] in Lemma 2.7. Now notice that the estimate \u2206\u0302(\u03c4) we have based on our initial estimates F\u03021 and F\u03022 is unbi-\nased i.e. E [ \u2206\u0302(\u03c4) ] = \u2206(\u03c4) \u2264 32\u01eb. This simply follows from exchanging interval integration and\nexpectation combined with the independence of X1 and X2:\nE\n[\u222b u\n\u03c4 (F\u03021(u)\u2212 F\u03021(x))(F\u03022(x)\u2212 1)dx\n] = \u222b u \u03c4 (E [ F\u03021(u) ] \u2212E [ F\u03021(x) ] )(E [ F\u03022(x) ] \u2212 1)dx\n=\n\u222b u\n\u03c4 (F1(u)\u2212 F1(x))(F2(x)\u2212 1)dx. (19)\nNow let us define \u2206\u0302(\u03c4) per sample i for each initial sample. We run N = C \u00b7 log T\u01eb samples for C = 1000. Then for i \u2208 [N ], we define\n\u2206\u0302(k)(\u03c4) =\n\u222b u\n\u03c4 (F\u0302\n(k) 1 (u)\u2212 F\u0302 (k) 1 (x))(F\u0302 (k) 2 (x)\u2212 1)dx \u2212\n\u222b \u03c4\n\u2113 (F\u0302\n(k) 1 (x)\u2212 F\u0302 (k) 1 (\u2113))(F\u0302 (k) 2 (x)\u2212 1)dx,\nwhere F\u0302 (k) 1 (.) and F\u0302 (k) 2 (.) are simple threshold functions at the ith initial sample, which are estimates for the densities F1 and F2 respectively. Note that\n\u2206\u0302(\u03c4) = 1\nN\n\u2211\nk\u2208[N ] \u2206\u0302(k)(\u03c4).\nNow again similar to (19) we have\nE [ \u2206(k)(\u03c4) ] = E [\u2206(\u03c4)] \u2264 32\u01eb. (20)\nMoreover, note that the random variable \u2206\u0302(i)(\u03c4) is bounded by one since\n|\u2206\u0302(k)(\u03c4)| \u2264 \u222b u\n\u03c4\n\u2223\u2223\u2223(F\u0302 (k)1 (u)\u2212 F\u0302 (k) 1 (x))(F\u0302 (k) 2 (x)\u2212 1) \u2223\u2223\u2223dx\u2212 \u222b \u03c4\n\u2113\n\u2223\u2223\u2223(F\u0302 (k)1 (x) + F\u0302 (k) 1 (\u2113))(F\u0302 (k) 2 (x)\u2212 1) \u2223\u2223\u2223dx\n\u2264 \u222b u\n\u03c4 1dx+\n\u222b \u03c4\n\u2113 1dx = u\u2212 \u2113 \u2264 1. (21)\nCombining Equations (20) and (21), we have the variance bound:\nVar[\u2206\u0302(\u03c4)] \u2264 E [ \u2206\u0302(\u03c4)2 ] \u2264 E [ \u2206\u0302(\u03c4) ] \u2264 32\u01eb. (22)\nNow, combining (21) and (22), we can apply Bernstein inequality for the random variables \u2206\u0302(i)(\u03c4). We have:\nPr [ |\u2206\u0302(\u03c4)\u2212\u2206(\u03c4)| \u2265 \u01eb ] \u2264 2 exp(\u2212 N\u01eb 2\n2Var[\u2206\u0302(\u03c4)] + 23\u01eb ) = 2T\u2212 3C 194 . (23)\nTherefore, |\u2206\u0302(\u03c4)\u2212\u2206(\u03c4)| < \u01eb holds with probability 1\u2212 T\u221212 when C = 1000. Notice that we only prove the bound for a single \u03c4 . To strengthen this concentration bound to hold simultaneously for all \u03c4 and [\u2113, u], we take a union over appropriate cover sets. In particular, consider C as a discretization of the interval [\u2113, u] with accuracy 1/T . To be able to exploit the high probability argument for the elements inside the cover for the ones outside, we need to show that \u2206 is Lipschitz with respect to \u03c4 , u and \u2113.\nFor \u2206 function, we have |\u2206(\u03c4)\u2212\u2206(\u03c4 \u2032)| \u2264 2|\u03c4 \u2212 \u03c4 \u2032| since \u2223\u2223\u2223\u2206(\u03c4)\u2212\u2206(\u03c4 \u2032) \u2223\u2223\u2223 = \u2223\u2223\u2223 \u222b \u03c4 \u2032\n\u03c4 (F1(u)\u2212 F1(x))(F2(x)\u2212 1)dx\n\u2223\u2223\u2223+ \u2223\u2223\u2223 \u222b \u03c4 \u2032\n\u03c4 (F1(u)\u2212 F1(x))(F2(x)\u2212 1)dx\n\u2223\u2223\u2223\n\u2264 2|\u03c4 \u2212 \u03c4 \u2032|.\nIt is easy to see that the same Lipschitz bound also holds for \u2206\u0302.\nNow for an arbitrary \u03c4 \u2032 \u2208 [\u2113, u], if we consider the closest \u03c4 to it in C, we have |\u03c4 \u2032 \u2212 \u03c4 | \u2264 1T . Then, using the Lipschitz constant of \u2206 and \u2206\u0302:\n\u2223\u2223\u2223\u2206(\u03c4)\u2212\u2206(\u03c4 \u2032) \u2223\u2223\u2223 \u2264 2\nT and\n\u2223\u2223\u2223\u2206\u0302(\u03c4)\u2212 \u2206\u0302(\u03c4 \u2032) \u2223\u2223\u2223 \u2264 2\nT . (24)\nNow we apply a union bound over the events |\u2206\u0302(\u03c4) \u2212 \u2206(\u03c4)| < \u01eb for all \u03c4 \u2208 C. Since running over all possibilities of |C| \u2264 T , after taking a union bound we know that all of these events happen simultaneously with probability at least 1\u2212 T\u221211. We then have for \u03c4 \u2032 and its closest element \u03c4 in C:\n|\u2206(\u03c4)\u2212\u2206(\u03c4 \u2032)|+ |\u2206\u0302(\u03c4)\u2212 \u2206\u0302(\u03c4 \u2032)| \u2264 4|\u03c4 \u2212 \u03c4 \u2032| \u2264 4 T . (25)\nWe simply upper-bound 4T by \u01eb. This must be true because \u01eb \u2265 T\u2212 1 2 = \u03c9( 1T ). Then, combining the bound in (25) with (23) implies |\u2206\u0302(\u03c4 \u2032) \u2212 \u2206(\u03c4 \u2032)| \u2264 2\u01eb holds with probability 1 \u2212 T\u221211 for all \u03c4 \u2208 [\u2113, u].\nNext, we bound |R\u0302\u2113\u2212R(\u2113)| and |R\u0302u\u2212R(u)|. For |R\u0302\u2113\u2212R(\u2113)|, Notice that R\u0302\u2113 is an estimate of R(\u2113)\nwith N = C \u00b7 log T \u01eb2 samples, and the reward of each sample falls in [\u22121, 1]. By Hoeffding\u2019s Inequality (Theorem A.1), the probability that |R\u0302\u2113\u2212R(\u2113)| > \u01eb is bounded by 2 exp(\u22122N\u01eb2/4) = 2T\u2212C/2. So, with probability 1 \u2212 T\u221211 |R\u0302\u2113 \u2212 R(\u2113)| \u2264 \u01eb when C > 100. The bound for |R\u0302u \u2212R(u)| is identical. Finally, combining three parts with union bound finishes the proof.\nFinally, we have the tools to prove Lemma 2.7:\nProof of Lemma 2.7. We will assume that |\u03b4\u0302(\u03c4)\u2212\u03b4(\u03c4)| \u2264 4\u01eb, which is true with probability 1\u2212T\u221210 by Claim 2.9.\nObserve that \u03b4\u0302(\u03c4) is a monotone increasing function, because \u03b4\u0302\u2032(\u03c4) = \u2206\u0302\u2032(\u03c4) = (F\u03021(u)\u2212F\u03021(\u2113))(1\u2212 F\u03022(\u03c4) \u2265 0. Therefore, according to the definition of \u2113\u2032 and u\u2032, we have [\u2113\u2032, u\u2032] = {\u03c4 \u2208 [\u2113, u] : |\u03b4\u0302(\u03c4)| \u2264 4\u01eb}. Now, we can use this property to prove two statements separately:\nFor the statement that \u03c4\u2217 \u2208 [\u2113\u2032, u\u2032], notice that \u03b4(\u03c4\u2217) = 0. According to Claim 2.9, |\u03b4\u0302(\u03c4\u2217)| \u2264 4\u01eb. Then, since \u03c4\u2217 \u2208 [\u2113, u] and |\u03b4\u0302(\u03c4\u2217)| \u2264 4\u01eb, there must be \u03c4\u2217 \u2208 [\u2113\u2032, u\u2032], because [\u2113\u2032, u\u2032] = {\u03c4 \u2208 [\u2113, u] : |\u03b4\u0302(\u03c4)| \u2264 4\u01eb}.\nNext, we prove that |\u03b4(\u03c4)| \u2264 8\u01eb for all \u03c4 \u2208 [\u2113, u]. This is true because [\u2113\u2032, u\u2032] = {\u03c4 \u2208 [\u2113, u] : |\u03b4\u0302(\u03c4)| \u2264 4\u01eb}, and we have |\u03b4\u0302(\u03c4) \u2212 \u03b4(\u03c4)| \u2264 4\u01eb from Claim 2.9. Therefore, |\u03b4(\u03c4)| \u2264 |\u03b4\u0302(\u03c4)| + |\u03b4\u0302(\u03c4) \u2212 \u03b4(\u03c4)| \u2264 8\u01eb for all \u03c4 \u2208 [\u2113\u2032, u\u2032].\nFinally, the bound R(\u03c4\u2217)\u2212R(\u03c4) \u2264 8\u01eb directly follows Claim 2.8 and that |\u03b4(\u03c4)| \u2264 8\u01eb.\nB.2.2 Proof of Theorem 2.10\nTo prove Theorem 2.10, we need to first give an initialization algorithm such that its output should satisfy the conditions listed in Lemma 2.7. Formally, we have the following lemma: Lemma B.1. After running no more than 1000 \u221a T log T samples from D1 and D2, with probability 1\u2212 T\u221210 we can output an initial interval [\u2113, u] that satisfies |g(\u03c4)| \u2264 T\u22121/4 and \u03c4\u2217 \u2208 [\u2113, u]. Proof. We first run 1000 \u221a T log T extra samples for X2 and calculate an estimate F\u03022(x). We can show that |F\u03022(x) \u2212 F2(x)| \u2264 12T\u2212 1 4 with probability 1 \u2212 T\u221210: After running N = C \u00b7 \u221a T log T\nsamples, the DKW inequality (Theorem A.3) shows that Pr [ |F\u03022(x)\u2212 F2(x)| > \u03b5 = 12T\u2212 1 4 ] \u2264 2 exp(\u22122N\u03b52) = 2T\u2212C/2. Then, with probability 1 \u2212 T\u221210, we have |F\u03022(x) \u2212 F2(x)| \u2264 12T\u2212 1 4 simultaneously holds for all x \u2208 [0, 1] when C > 100. In the following proof, we assume this accuracy bound always holds.\nNext, we calculate g\u0302(\u03c4) by replacing F2(x) with F\u03022(x) in (3). When |F\u03022(x) \u2212 F2(x)| \u2264 12T\u2212 1 4\nholds simultaneously for all x \u2208 [0, 1], we have |g\u0302(\u03c4) \u2212 g(\u03c4)| \u2264 \u222b 1 \u03c4 |F\u03022(x) \u2212 F2(x)| \u2264 12T\u2212 1 4 . Then, we let [\u2113, u] := {\u03c4 : |g\u0302(\u03c4)| \u2264 12T\u2212 1 4 }. Since g\u0302\u2032(\u03c4) = F\u03022(\u03c4) \u2212 1 \u2264 0, function g\u0302(\u03c4) is a nonincreasing. So, the set {\u03c4 : |g\u0302(\u03c4)| \u2264 12T\u2212 1 4} must form an interval. Besides, notice that g(\u03c4\u2217) = 0, which means |g\u0302(\u03c4\u2217)| \u2264 12T\u2212 1 4 , so we must have \u03c4\u2217 \u2208 [\u2113, u]. Furthermore, for every \u03c4 \u2208 [\u2113, u], |g(\u03c4)| \u2264 |g\u0302(\u03c4)|+ |g\u0302(\u03c4)\u2212 g(\u03c4)| \u2264 T\u2212 14 , which finishes the proof.\nNow, we are ready to prove Theorem 2.10:\nProof of Theorem 2.10. For the core part of the algorithm, we run Algorithm 2 and then use Lemma 2.5 to bound the regret. To run Algorithm 2, we let the constraints to mean that the\nthreshold played in each round is inside the interval [\u2113, u] given by Algorithm 3. Besides, we require the conditions listed in Lemma 2.7 hold (with high probability). We discuss them separately:\n\u2022 |g(\u03c4)| \u2264 T\u2212 14 for all \u03c4 \u2208 [\u2113, u]: This is guaranteed by Lemma B.1. \u2022 \u03c4\u2217 \u2208 [\u2113, u]: For the first phase, this is guaranteed by Lemma B.1. Starting from the second phase, this is from Lemma 2.7 of the previous phase. \u2022 |\u03b4(\u03c4)| \u2264 16\u01eb: For the first phase, this is true because \u01eb1 = 1. Starting from the second phase, this is from Lemma 2.7 of the previous phase. Notice that the statement in Lemma 2.7 is a little bit different: It guarantees that |\u03b4(\u03c4)| \u2264 8\u01eb with respect to the [\u2113, u] and \u01eb from the previous phase. When switching to the new phase, notice that F2(u\n\u2032)\u2212F2(\u2113\u2032) \u2264 F2(u)\u2212F2(\u2113), which means |\u03b4(\u03c4)| drops when switching to the new phases. Besides, the parameter \u01ebnew in the new phase is exactly 12\u01ebold. Combining these two differences shows that |\u03b4(\u03c4)| \u2264 16\u01eb holds in the new phase.\nTherefore, Algorithm 3 satisfies algorithm Alg in Lemma 2.5. Applying Lemma 2.5 gives the O( \u221a T log T ) regret bound.\nBesides, we also run samples for initialization and constructing CDF estimates for Algorithm 3. These are not coverd by Lemma 2.5. For the initialization, Lemma B.1 states that \u0398( \u221a T log T )\nrounds are sufficient. So the regret from the initialization is O( \u221a T log T ). For constructing F\u03021(x) and F\u03022(x), assume we run k phases, then the total number of samples is\nk\u2211\ni=1\n\u0398( log T\n\u01ebi ) = O(\n\u221a T log T ).\nCombining three parts finishes the proof."
        },
        {
            "heading": "C Missing Proofs from Section 4",
            "text": "C.1 Proof of Lemma 4.1 Proof. We first prove the lemma for a single i. For [\u2113i, ui], we run C \u00b7 \u221a T log T extra samples for Xi with C = 1000, and calculate an estimate F\u0302i(x). We can show that |F\u0302i(x) \u2212 Fi(x)| \u2264 12T\u2212 1 4 with\nprobability 1\u2212T\u221211: After running N = C \u00b7 \u221a T log T samples, the DKW inequality (Theorem A.3) shows that Pr [ |F\u0302i(x)\u2212 Fi(x)| > \u03b5 = 12T\u2212 1 4 ] \u2264 2 exp(\u22122N\u03b52) = 2T\u2212C/2. Then with probability 1\u2212T\u221211, we have |F\u0302i(x)\u2212Fi(x)| \u2264 12T\u2212 1 4 holds for every x \u2208 [0, 1] when C > 100. In the following proof, we assume this accuracy bound always holds. By the union bound over all i \u2208 [n], the whole proof succeeds with probability 1\u2212 T\u221210.\nNext, we calculate g\u0302i(\u03c4) by replacing Fi(x) with F\u0302i(x) in (11). When |F\u0302i(x) \u2212 Fi(x)| \u2264 T\u2212 1 4\nholds for all x \u2208 [0, 1], we have |g\u0302i(\u03c4)\u2212 gi(\u03c4)| \u2264 \u222b 1 \u03c4 |F\u0302i(x)\u2212 Fi(x)| \u2264 12T\u2212 1 4 . Then, we let [\u2113i, ui] := {\u03c4 : |g\u0302i(\u03c4)| \u2264 12T\u2212 1 4 }. Since g\u0302\u2032(\u03c4) = F\u0302i(\u03c4) \u2212 1 \u2264 0, which means g\u0302i(\u03c4) is a decreasing function, then the set {\u03c4 : |g\u0302i(\u03c4)| \u2264 12T\u2212 1 4 } must form an interval. Besides, notice that gi(\u03c4\u2217) = 0, which means |g\u0302i(\u03c4\u2217)| \u2264 12T\u2212 1 4 , so there must be \u03c4\u2217i = \u03c3i \u2208 [\u2113i, ui]. Furthermore, for every \u03c4 \u2208 [\u2113i, ui], |gi(\u03c4)| \u2264 |g\u0302i(\u03c4)|+ |g\u0302i(\u03c4)\u2212 gi(\u03c4)| \u2264 T\u2212 1 4 .\nFinally, combining the statements for all n intervals finishes the proof.\nC.2 Proof of Claim 4.6 Proof. We first show that |F\u0302i(x)\u2212Fi(x)| \u2264 \u221a \u01eb 2n with probability 1\u2212T\u221213 withN = C\u00b7 n2 log T \u01eb samples,\nwhere C is set to be 1000. Using DKW inequality (Theorem A.3), we havePr [ |F\u0302i(x)\u2212 Fi(x)| > \u221a \u01eb\n2n\n] \u2264\n2 exp(\u22122N \u01eb 4n2 ) = 2T\u2212C/4. So the bound holds with probability 1\u2212 T\u221213 when C = 1000. By the union bound, with probability 1\u2212T\u221212 we have |F\u0302i(x)\u2212Fi(x)| \u2264 \u221a \u01eb\n2n holds for every i \u2208 [n]. Then, for the accuracy of \u220f i\u2208S Fi(x), we have ( (1\u2212 \u221a \u01eb 2n ) n\u22121 ) \u2264\u220fi\u2208S F\u0302i(x)\u2212 \u220f i\u2208S Fi(x) \u2264 ( (1+ \u221a \u01eb 2n ) n\u22121 ) .\nFor the lower bound, we have (1 \u2212 \u221a \u01eb\n2n ) n \u2212 1 \u2265 1 \u2212 \u221a \u01eb 2 \u2212 1 > \u2212 \u221a \u01eb. For the upper bound, we have\n(1+ \u221a \u01eb\n2n ) n\u22121 \u2264 exp(\n\u221a \u01eb 2n \u00b7n)\u22121 \u2264 1+2 \u00b7 \u221a \u01eb 2 \u22121 = \u221a \u01eb. Combining two bounds finishes the proof.\nC.3 Proof of Claim 4.7\nProof. Since \u03b4i(\u03c4) = \u2206i(\u03c4)\u2212(Ru\u2212R\u2113), there are three parts in \u03b4i(\u03c4). We show that the accuracy of each part is bounded by \u01eb3 with probability 1\u2212T 13, then taking a union bound over three accuracy bounds gives Claim 4.7.\nFirst, similar to the derivation in Equation (18) we bound the magnitude of the \u2206i function:\n|\u2206i(\u03c4)| \u2264 (F\u03c0,i(u)\u2212 F\u03c0,i(\u03c4)) \u222b u\n\u03c4 (1\u2212 Fi(x))dx+ (F\u03c0,i(\u03c4)\u2212 F\u03c0,i(\u2113))\n\u222b \u03c4\n\u2113 (1\u2212 Fi(x))dx\n= (F\u03c0,i(u)\u2212 F\u03c0,i(\u03c4))(gi(\u03c4)\u2212 gi(u)) + (F\u03c0,i(\u03c4)\u2212 F\u03c0,i(\u2113))(gi(\u2113)\u2212 gi(\u03c4)) \u2264 (F\u03c0,i(u)\u2212 F\u03c0,i(\u2113))(gi(\u2113)\u2212 gi(u)) \u2264 |\u03b4\u03c0,u,\u2113,i(\u2113)|+ |\u03b4\u03c0,u,\u2113,i(u)| \u2264 12\u01eb, (26)\nwhere we use the bound |\u03b4\u03c0\u2032,u\u2032,\u2113\u2032,i(\u03c4)| \u2264 6\u01eb in Lemma 4.5. Next, we hope to propose an estimator \u2206\u0302\n(k) i (\u03c4) for the \u2206i function which uses N = C \u00b7 log T\u01eb\nsamples for C = 105. For k \u2208 [N ], define\n\u2206\u0302 (k) i (\u03c4) =\n\u222b u\n\u03c4 (F\u0302\n(k) \u03c0,i (u)\u2212 F\u0302 (k) \u03c0,i (x))(F\u0302 (k) i (x)\u2212 1)dx \u2212\n\u222b \u03c4\n\u2113 (F\u0302\n(k) \u03c0,i (x)\u2212 F\u0302 (k) \u03c0,i (\u2113))(F\u0302 (k) i (x)\u2212 1)dx,\nwhere F\u0302 (k) \u03c0,i (.) and F\u0302 (k) i (.) are simple threshold functions at the ith initial sample, which are estimates for the densities F\u03c0,i and Fi, respectively. This definition implies \u2206\u0302i(\u03c4) = 1 N \u2211 k\u2208[N ] \u2206\u0302 (k) i (\u03c4), and Equation (26) implies\nE [ \u2206\u0302\n(k) i (\u03c4) ] = \u2206i(\u03c4) \u2264 12\u01eb. (27)\nNow it is easy to see that F\u0302 (k) \u03c0,i (x)\u2212 F\u0302 (k) \u03c0,i (y) is a Bernoulli random variable which are one if and only if the maximum value obtained from X\u03c0(1), . . . ,X\u03c0(\u03c0\u22121(i)\u22121) is in [\u2113i, ui]. In particular, this implies that \u2206\u0302 (k) i (\u03c4) is bounded by 1 since\n|\u2206\u0302(k)i (\u03c4)| \u2264 \u222b u\n\u03c4\n\u2223\u2223\u2223(F\u0302 (k)\u03c0,i (u)\u2212 F\u0302 (k) \u03c0,i (x))(F\u0302 (k) i (x)\u2212 1) \u2223\u2223\u2223dx\u2212 \u222b \u03c4\n\u2113\n\u2223\u2223\u2223(F\u0302 (k)\u03c0,i (x)\u2212 F\u0302 (k) \u03c0,i (\u2113))(F\u0302 (k) i (x)\u2212 1) \u2223\u2223\u2223dx\n\u2264 \u222b u\n\u03c4 1dx+\n\u222b \u03c4\n\u2113 1dx = u\u2212 \u2113 \u2264 1. (28)\nCombining Equations (27) and (28), we have the variance bound:\nVar[\u2206\u0302i(\u03c4)] \u2264 E [ \u2206\u0302i(\u03c4) 2 ] \u2264 E [ \u2206\u0302i(\u03c4) ] \u2264 12\u01eb. (29)\nHence, using Bernstein inequality, we have\nPr [ |\u2206\u0302i(\u03c4)\u2212\u2206i(\u03c4)| \u2265 \u01eb\n12\n] \u2264 2 exp(\u2212 N\u01eb 2/144\n2Var[\u2206\u0302i(\u03c4)] + 2 3 \u01eb 12\n) = 2T\u2212 C 3464 . (30)\nTherefore, |\u2206\u0302i(\u03c4)\u2212\u2206i(\u03c4)| \u2264 \u01eb12 holds with probability 1\u2212 T\u221214 when C = 105. The bound above is only for a single \u03c4 . To give the bound for a whole interval, we discretize [\u2113i, ui] uniformly into a discrete set C and make sure that each pair of adjacent \u03c4, \u03c4 \u2032 \u2208 C follows |\u03c4 \u2212 \u03c4 \u2032| \u2264 1T . Then, there must be |C| \u2264 T and the union bound implies |\u2206\u0302i(\u03c4)\u2212\u2206i(\u03c4)| \u2264 \u01eb12 holds with probability 1\u2212 T\u221213 for all \u03c4 \u2208 C.\nNext, we bound the Lipschitz constant of \u2206i (and similarly \u2206\u0302i):\n\u2223\u2223\u2223\u2206i(\u03c4)\u2212\u2206i(\u03c4 \u2032) \u2223\u2223\u2223 = \u2223\u2223\u2223 \u222b \u03c4 \u2032\n\u03c4 (F\u03c0,i(u)\u2212 F\u03c0,i(x))(Fi(x)\u2212 1)dx\n\u2223\u2223\u2223+ \u2223\u2223\u2223 \u222b \u03c4 \u2032\n\u03c4 (F\u03c0,i(u)\u2212 F\u03c0,i(x))(Fi(x)\u2212 1)dx\n\u2223\u2223\u2223\n\u2264 2|\u03c4 \u2212 \u03c4 \u2032|.\nFinally, for every \u03c4 \u2208 [\u2113i, ui], let \u03c4 \u2032 be the closest value in C. Then, we have: \u2223\u2223\u2223\u2206\u0302i(\u03c4)\u2212\u2206i(\u03c4) \u2223\u2223\u2223 \u2264 \u2223\u2223\u2223\u2206i(\u03c4)\u2212\u2206i(\u03c4 \u2032) \u2223\u2223\u2223+ \u2223\u2223\u2223\u2206\u0302i(\u03c4 \u2032)\u2212\u2206i(\u03c4 \u2032) \u2223\u2223\u2223+ \u2223\u2223\u2223\u2206\u0302i(\u03c4 \u2032)\u2212 \u2206\u0302i(\u03c4) \u2223\u2223\u2223 \u2264 \u01eb12 + 4T \u2264 \u01eb6 ,\nwhere the last inequality is true because \u01eb > T\u2212 1 2 = \u03c9( 1T ). Therefore, with probability 1 \u2212 T\u221213,\nwe have \u2223\u2223\u2223\u2206\u0302i(\u03c4)\u2212\u2206i(\u03c4) \u2223\u2223\u2223 \u2264 \u01eb6 for all \u03c4 \u2208 [\u2113i, ui] simultaneously.\nNext, we use Hoeffding\u2019s Inequality (Theorem A.1) to bound the accuracy of |R\u2113 \u2212 R\u0302\u2113|. In each round, the reward falls in [-0.5,0.5]. Then, after running N = C \u00b7 \u01eb\u22122 log T samples, we have Pr [ |R\u2113 \u2212 R\u0302\u2113| > \u01eb6 ] \u2264 2 exp(\u22122N\u01eb2/36) = 2T\u2212C/18. Therefore, |R\u2113 \u2212 R\u0302\u2113| \u2264 \u01eb6 with probability 1\u2212T\u221213 when C > 1000. Besides, the proof for |Ru\u2212 R\u0302u| is identical. Combining three parts with union bound finishes the proof.\nC.4 Proof of Lemma 4.8\nIn this section, we show that Algorithm 10 finds an approximately clever threshold setting. We first introduce the following lemma: Lemma C.1. Algorithm 10 calculates a clever threshold setting, up to an 4 \u221a \u01eb additive error. The running time of Algorithm 10 is O(n \u00b7 2n).\nProof. The goal of a clever threshold setting is to maximize F\u03c0,i(u) \u2212 F\u03c0,i(\u2113). Fix i. When the set P , which represents the distributions in front of Xi is determined, the function F\u03c0,i(x) is fixed. Therefore, to maximize F\u03c0,i(u) \u2212 F\u03c0,i(\u2113), we should maximize u and minimize \u2113. This can be achieved by maximizing the thresholds in P and minimizing the thresholds in [n] \\ (P \u222a{i}), which is exactly lines 8 and 9 in Algorithm 6. Then, after enumerating all valid subsets P , we can find a setting that maximizes F\u03c0,i(u)\u2212 F\u03c0,i(\u2113).\nAlgorithm 10: Finding Approximately Clever Threshold\nInput: (I, S), m, i, F\u03021(x), ..., F\u0302n(x) 1 for P \u2286 [n] do 2 if \u2203k : (k, i) \u2208 S \u2227 k /\u2208 P or \u2203k : (i, k) \u2208 S \u2227 k \u2208 P or \u2203k, j : (k, j) \u2208 S \u2227 k /\u2208 P \u2227 j \u2208 P then Skip this P ; 3 For k \u2208 P , let \u03c4k = uk 4 For k \u2208 [n] \\ (T \u222a {i}), let \u03c4k = \u2113k 5 Let uT = min{ui, \u03c4k:k\u2208T}, \u2113T = max{\u2113i, \u03c4k:k/\u2208(T\u222a{i})} 6 Set partial setting \u03c0T be: Let \u03c4i \u2208 [\u2113T , uT ]. \u03c0T sorts the thresholds in a decreasing order. Break the ties according to the constraints in S. 7 Let F\u0302\u03c0T ,i(x) = \u220f k\u2208T F\u0302k(x). 8 Calculate qT := F\u0302\u03c0T ,i(uT )\u2212 F\u0302\u03c0T ,i(\u2113T ) 9 Let T \u2217 = argmax qT .\nOutput: \u03c0T \u2217 , \u2113T \u2217 , uT \u2217 , F\u0302\u03c0T\u2217 ,i.\nThere is one missing detail: we only know the value of F\u0302i(x). From Claim 4.6, we know F\u0302i(x) is an estimate of Fi(x) with accuracy \u221a \u01eb. Therefore, max\u03c0 F\u0302\u03c0,i(u) \u2212 F\u0302\u03c0,i(\u2113) is at most 2 \u221a \u01eb different from max\u03c0 F\u03c0,i(u) \u2212 F\u03c0,i(\u2113). After getting \u03c0\u2032 = argmax\u03c0 F\u0302\u03c0,i(u) \u2212 F\u0302\u03c0,i(\u2113), the real value of F\u03c0\u2032,i(u)\u2212 F\u03c0\u2032,i(\u2113) is at most 2 \u221a \u01eb different from F\u0302\u03c0\u2032,i(u)\u2212 F\u0302\u03c0\u2032,i(\u2113). Combining two errors proves\nthe 4 \u221a \u01eb error bound.\nFor the running time of Algorithm 10, we need to enumerate a subset S, then calculate the corresponding F\u03c0,i(x) function. So the running time is O(n \u00b7 2n)."
        }
    ],
    "title": "Bandit Algorithms for Prophet Inequality and Pandora\u2019s Box",
    "year": 2023
}