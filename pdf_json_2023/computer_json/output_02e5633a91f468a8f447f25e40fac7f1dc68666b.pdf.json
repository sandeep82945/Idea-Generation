{
    "abstractText": "Adversarial attacks expose vulnerabilities of deep learning models by introducing minor perturbations to the input, which lead to substantial alterations in the output. Our research focuses on the impact of such adversarial attacks on sequence-to-sequence (seq2seq) models, specifically machine translation models. We introduce algorithms that incorporate basic text perturbation heuristics and more advanced strategies, such as the gradient-based attack, which utilizes a differentiable approximation of the inherently non-differentiable translation metric. Through our investigation, we provide evidence that machine translation models display robustness displayed robustness against best performed known adversarial attacks, as the degree of perturbation in the output is directly proportional to the perturbation in the input. However, among underdogs, our attacks outperform alternatives, providing the best relative performance. Another strong candidate is an attack based on mixing of individual characters.",
    "authors": [
        {
            "affiliations": [],
            "name": "Pavel Burnyshev"
        },
        {
            "affiliations": [],
            "name": "Alexey Zaytsev"
        }
    ],
    "id": "SP:189b53e3fa033b33fa9845012c2316077bc423ae",
    "references": [
        {
            "authors": [
                "Y. Belinkov",
                "Y. Bisk"
            ],
            "title": "Synthetic and natural noise both break neural machine translation",
            "year": 2017
        },
        {
            "authors": [
                "A. Chakraborty",
                "M. Alam",
                "V. Dey",
                "A. Chattopadhyay",
                "D. Mukhopadhyay"
            ],
            "title": "Adversarial attacks and defences: A survey",
            "year": 2018
        },
        {
            "authors": [
                "J. Chen",
                "D. Tam",
                "C. Raffel",
                "M. Bansal",
                "D. Yang"
            ],
            "title": "An empirical survey of data augmentation for limited data learning in nlp",
            "year": 2021
        },
        {
            "authors": [
                "Y. Cheng",
                "L. Jiang",
                "W. Macherey"
            ],
            "title": "Robust neural machine translation with doubly adversarial inputs",
            "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 4324\u20134333",
            "year": 2019
        },
        {
            "authors": [
                "J. Ebrahimi",
                "D. Lowd",
                "D. Dou"
            ],
            "title": "On adversarial examples for character-level neural machine translation",
            "year": 2018
        },
        {
            "authors": [
                "J. Ebrahimi",
                "A. Rao",
                "D. Lowd",
                "D. Dou"
            ],
            "title": "Hotflip: White-box adversarial examples for text classification",
            "year": 2017
        },
        {
            "authors": [
                "J. Ebrahimi",
                "A. Rao",
                "D. Lowd",
                "D. Dou"
            ],
            "title": "Hotflip: White-box adversarial examples for text classification",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). pp. 31\u201336",
            "year": 2018
        },
        {
            "authors": [
                "I. Fursov",
                "A. Zaytsev",
                "P. Burnyshev",
                "E. Dmitrieva",
                "N. Klyuchnikov",
                "A. Kravchenko",
                "E. Artemova",
                "E. Komleva",
                "E. Burnaev"
            ],
            "title": "A differentiable language model adversarial attack on text classifiers",
            "venue": "IEEE Access 10, 17966\u201317976",
            "year": 2022
        },
        {
            "authors": [
                "I. Fursov",
                "A. Zaytsev",
                "N. Kluchnikov",
                "A. Kravchenko",
                "E. Burnaev"
            ],
            "title": "Gradientbased adversarial attacks on categorical sequence models via traversing an embedded world",
            "venue": "Analysis of Images, Social Networks and Texts: 9th International Conference, AIST 2020, Skolkovo, Moscow, Russia, October 15\u201316, 2020, Revised Selected Papers 9. pp. 356\u2013368. Springer",
            "year": 2021
        },
        {
            "authors": [
                "L. Gomez",
                "M. Rusinol",
                "D. Karatzas"
            ],
            "title": "Lsde: Levenshtein space deep embedding for query-by-string word spotting",
            "venue": "pp. 499\u2013504",
            "year": 2017
        },
        {
            "authors": [
                "M. Junczys-Dowmunt",
                "R. Grundkiewicz",
                "T. Dwojak",
                "H. Hoang",
                "K. Heafield",
                "T. Neckermann",
                "F. Seide",
                "U. Germann",
                "A.F. Aji",
                "N. Bogoychev",
                "A.F.T. Martins",
                "A. Birch"
            ],
            "title": "Marian: Fast neural machine translation in C++",
            "venue": "Proceedings of ACL 2018, System Demonstrations. pp. 116\u2013121. Association for Computational Linguistics, Melbourne, Australia",
            "year": 2018
        },
        {
            "authors": [
                "P. Michel",
                "X. Li",
                "G. Neubig",
                "J. Pino"
            ],
            "title": "On evaluation of adversarial perturbations for sequence-to-sequence models",
            "venue": "Proceedings of the 2019 Conference of the North",
            "year": 2019
        },
        {
            "authors": [
                "M. Popovi\u0107"
            ],
            "title": "chrF: character n-gram F-score for automatic MT evaluation",
            "venue": "Proceedings of the Tenth Workshop on Statistical Machine Translation. pp. 392\u2013 395. Association for Computational Linguistics, Lisbon, Portugal",
            "year": 2015
        },
        {
            "authors": [
                "N. Reimers",
                "I. Gurevych"
            ],
            "title": "Sentence-bert: Sentence embeddings using siamese bertnetworks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics",
            "year": 2019
        },
        {
            "authors": [
                "S. Sadrizadeh",
                "A.D. Aghdam",
                "L. Dolamic",
                "P. Frossard"
            ],
            "title": "Targeted adversarial attacks against neural machine translation",
            "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). pp. 1\u20135. IEEE",
            "year": 2023
        },
        {
            "authors": [
                "S. Samanta",
                "S. Mehta"
            ],
            "title": "Towards crafting text adversarial samples",
            "year": 2017
        },
        {
            "authors": [
                "Y. Tang",
                "C. Tran",
                "X. Li",
                "P. Chen",
                "N. Goyal",
                "V. Chaudhary",
                "J. Gu",
                "A. Fan"
            ],
            "title": "Multilingual translation with extensible multilingual pretraining and finetuning",
            "venue": "CoRR abs/2008.00401",
            "year": 2008
        },
        {
            "authors": [
                "Vaibhav",
                "S. Singh",
                "C. Stewart",
                "G. Neubig"
            ],
            "title": "Improving robustness of machine translation with synthetic noise",
            "venue": "Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL). Minneapolis, USA",
            "year": 2019
        },
        {
            "authors": [
                "A. Vaswani",
                "N. Shazeer",
                "N. Parmar",
                "J. Uszkoreit",
                "L. Jones",
                "A.N. Gomez",
                "L. Kaiser",
                "I. Polosukhin"
            ],
            "title": "Attention is all you need",
            "year": 2017
        },
        {
            "authors": [
                "W. Wang",
                "B. Tang",
                "R. Wang",
                "L. Wang",
                "A. Ye"
            ],
            "title": "A survey on adversarial attacks and defenses in text",
            "venue": "CoRR abs/1902.07285",
            "year": 2019
        },
        {
            "authors": [
                "H. Xu",
                "Y. Ma",
                "H. Liu",
                "D. Deb",
                "H. Liu",
                "J. Tang",
                "A.K. Jain"
            ],
            "title": "Adversarial attacks and defenses in images, graphs and text: A review",
            "venue": "International Journal of Automation and Computing 17, 151\u2013178",
            "year": 2020
        },
        {
            "authors": [
                "T. Zhang",
                "V. Kishore",
                "F. Wu",
                "K.Q. Weinberger",
                "Y. Artzi"
            ],
            "title": "Bertscore: Evaluating text generation with bert",
            "year": 2019
        },
        {
            "authors": [
                "X. Zhang",
                "J. Zhang",
                "Z. Chen",
                "K. He"
            ],
            "title": "Crafting adversarial examples for neural machine translation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). pp. 1967\u20131977",
            "year": 2021
        },
        {
            "authors": [
                "Z. Zhao",
                "D. Dua",
                "S. Singh"
            ],
            "title": "Generating natural adversarial examples",
            "year": 2017
        },
        {
            "authors": [
                "Y. Zhou",
                "J.Y. Jiang",
                "K.W. Chang",
                "W. Wang"
            ],
            "title": "Learning to discriminate perturbations for blocking adversarial attacks in text classification",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 4904\u20134913. Association for Computational Linguistics, Hong Kong, China",
            "year": 2019
        },
        {
            "authors": [
                "V. Zhukov",
                "E. Golikov",
                "M. Kretov"
            ],
            "title": "Differentiable lower bound for expected bleu score",
            "year": 2017
        },
        {
            "authors": [
                "W. Zou",
                "S. Huang",
                "J. Xie",
                "X. Dai",
                "J. Chen"
            ],
            "title": "A reinforced generation of adversarial examples for neural machine translation",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "Keywords: Adversarial attack \u00b7 Robustness \u00b7 Neural machine translation"
        },
        {
            "heading": "1 Introduction",
            "text": "Modern neural machine translation models demonstrate high-quality generated translations, and they are widely used in real-world applications as part of automatic translation systems. For this reason, the robustness and reliability of such models become crucial factors.\nAdversarial attacks, as detailed [3,21], encompass a broad range of techniques aimed at exposing and probing the vulnerabilities of these models. These attacks introduce slight perturbations to the input data, which can, in turn, lead to significant misinterpretations or errors in the output. The aim is to understand the model\u2019s weak points and stability under these \"attacks\".\nThe core concept of an adversarial attack is not conditioned on the data nature: an attacker tries to significantly change the model output by modifying the input object. Nevertheless, constructing adversaries for NLP models is complicated due to the discrete structure of the text data [1,16,24]. As we can not straightly use derivatives of the loss function, we compute differentiable approximations of metrics [26] and derivatives of the adversarial loss with respect\nar X\niv :2\n30 9.\n06 52\n7v 1\n[ cs\n.C L\n] 1\n0 Se\np 20\nto non-discrete token embeddings. We can use this idea to generate adversarial examples from the embedding space [9] The work [8] goes further in this way by proposing the use of a generative model to make the adversarial attack work.\nHowever, one can spot a common point in significant part of all these articles: they mostly pay attention to models with an output that consists of a single number. Nowadays, many use cases for natural language processing models focus on sequence2sequence problems, where both input and output for a model are sequences. One particular example of such a problem is a classic machine translation. The input, in this case, is a sequence in one language, and the output is a sequence in another language. Our research can help not only investigate vulnerabilities of these models to adversarial perturbations but also provide new insights on the possibility of detecting anomalies and estimating uncertainty for these models.\nOur main contributions to adversarial attacks on machine translation models are:\n\u2013 We propose new techniques to construct adversaries on the machine translation task. The first algorithm replaces input tokens based on the gradient of the target function with respect to the model\u2019s embeddings. Another approach exploits approximations of non-differentiable metrics.\n\u2013 We conduct a fair comparison of different attacks based on a diverse set of metrics for a machine translation problem.\n\u2013 Our experiments demonstrate that modern machine translation models are only slightly vulnerable to adversarial inputs. They do degrade for carefully created adversarial examples via a range of techniques, while the effect is less evident compared to drastic performance drops for computer vision and NLP classification models [12].\n\u2013 The biggest vulnerability comes from attacks that work at character levels suggesting that in this case the adversarial examples fall out of the domain of data used for training."
        },
        {
            "heading": "2 Related work",
            "text": "Various types of adversarial attacks on machine translation models have detected their sensitivity to disrupted inputs [2, 20]. The first family of attack strategies finds the most loss-increasing perturbations of the source sentence using a gradient in the embedding space. The HotFlip attack [7] vectorizes simple char-level operations such as replacement, deletion, and insertion and uses directional derivatives to select the change of input sample. Targeted attack [15] uses gradient projections in the latent space to make perturbations. It preserves the similarity between initial and adversarial translations by inserting a target keyword into adversarial output. AdvGen algorithm [4] works on word level and craft adversarial examples based on the similarity between the loss gradient and distance between initial word and adversarial candidates.\nThe second group of attacks exploits differentiable estimations of standard NLP metrics to control text perturbations. Authors of [26] propose such approximation to BLEU, and authors of [10] use a deep learning model to estimate Levenstein distance between sentences. Dependence on metrics allows selecting perturbations in discrete space more naturally.\nThe third type of attack can successfully fool a machine translation model by imitating typos or letter emission. Authors of [1] add synthetic noise to attacked sentences which includes replacement of letters and varying their order. In addition to swapped characters, distorted inputs can contain emojis and profanity [18].\nSeveral approaches can produce high-quality adversarial examples but require more complicated training and generation processes. GAN-based framework [24] operates on a sentence level, and its training process is adapted for the discrete data structure. Authors of [27] propose a reinforcement learning paradigm to generate meaning-preserving examples.\nThere are certain methods to evaluate adversarial attacks on NLP data. Attack Success Rate measures the proportion of successful attacks, which reduces twice the BLEU score of adversarial translation compared to initial translation [5]. Authors of [12] propose an evaluation framework for attacks on seq2seq models that focuses on the semantic equivalence of the pre- and post-perturbation input.\nIn this study, we provide a comparison of principal attack types: gradientbased, synthetic, and metric approximation. Our modifications to existing methods allow both saving the semantic and grammar correctness of adversaries and altering the attacked translation."
        },
        {
            "heading": "3 Methods",
            "text": ""
        },
        {
            "heading": "3.1 General description of a Machine Translation Model",
            "text": "The backbone of the majority of modern research and production Translation models is a Transformer model [19]. It consists of Encoder and Decoder parts, each of which includes sequential application of a multi-head attention mechanism that forces latent representation of tokens to interact with each other. The encoder of the model maps the input sentence X = {x1, x2 . . . xn} into latent representation Z = {z1, z2 . . . zk}. Decoder likewise translates it into output embedding representation. The decoder output goes into the classification head, which chooses the next token yj , the process repeats until the model generates a special end token. Choice of the next output token y<j depends on input text X, hidden representations z and already generated text y<j :\np\u03b8(Y |X) = m\u220f j=1 p\u03b8(yj |y<j , X, Z),\nwhere \u03b8 are model parameters and Y = {y1, . . . , yk\u2032}. The loss function can be defined as J(\u03b8,X, Y ) = 1n \u2211n i=1 \u2212 logP (yi|X, \u03b8)."
        },
        {
            "heading": "3.2 Gradient Machine Translation attack",
            "text": "The proposed gradient attack algorithm has a white-box full access to the model\u2019s parameters \u03b8, adversarial loss Ladv we want to minimize, and an input sequence of tokens X that corresponds to a text. We suppose that for a set of tokens, we have a dictionary of embeddings V. The model works on the token level, and the number of tokens in the alphabet is |V|.\nThe core idea of the attack is inspired by Hotflip [7] attack: we iteratively replace input tokens according to the adversarial loss, calculated with respect to the model\u2019s input embeddings e.The new token\u2019s embedding would minimize the first-order Taylor approximation of adversarial loss:\nargmin e\u2032i\u2208V\n[e\u2032i \u2212 ei] \u22a4 \u2207eiLadv.\n\u2207eiLadv means computing gradient with respect to token at position i. The subtracted part of the expression does not depend on substitute embeddings, so the optimization problem reduces to\nargmin e\u2032i\u2208V\n[e\u2032i] \u22a4 \u2207eiLadv.\nTo select the replacement token, we try all possible indexes i and compare their respective difference in loss.\nThe overall approach is illustrated in Figure 1. It\u2019s essential to preserve the semantics and grammar of the initial text. Otherwise, the attack discriminators [25] would always detect the attack. So, following [6], we use several constraints in our experiments. They aim to save the initial meaning of the sentence and prevent an attacker from turning a sentence into a meaningless string of characters that doesn\u2019t resemble the initial meaning of a sentence:\n1. The cosine distance between new and replaced embeddings must not be smaller than the threshold. 2. Attacker can replace each token position only once. 3. Attacker can separate all tokens in the vocabulary into two parts. The first\npart of the subset of tokens always stays at the beginning of the word. Another part stays at the second and next positions. We discourage the algorithm from replacing tokens from one part with tokens from another. 4. We disallow replacement of tokens denoting punctuation, first and last tokens of the sentence, and stop words."
        },
        {
            "heading": "3.3 BLEUER attack",
            "text": "The gradient attack described before does not guarantee any estimations on the primary translation metrics change: BLEU, METEOR, etc. Instead of optimizing adversarial loss, which does not directly depend on text metric, we can incorporate dependence from the differentiable approximation of the metric. An illustration of the approach for BLEU score is presented in Figure 2.\nBefore applying an attack, an adversary needs to train extra layers to predict BLEU. We translated a subset from the text corpus using the initial translation model and computed initial BLEU scores for pairs of sequences. Those scores are used as targets during the training of additional layers on the top of the encoder part of the model. During the training, we minimize MSE loss between the predicted value of the BLEU score and the original one:\nJ = MSE(f(z),BLEU(Yorig, Ytrans)),\nwhere z is the encoder output, f means applying additional layers, Yorig is expected translation from data corpus, Ytrans is the model\u2019s translation. The attack algorithm executes the following steps:\n1. Get encoder outputs z;\n2. Calculate prediction of BLEU score f(z) using differentiable layers and loss value J ;\n3. Calculate gradients with respect to loss and update encoder outputs z, so that approximate BLEU score decreases:\nz := z + \u03b5 \u00b7 \u2207eiMSE(f(z), 1)).\nThe updated encoder output is served at the entrance to the decoder part of the model, which generates adversarial translation."
        },
        {
            "heading": "3.4 MBART attack",
            "text": "The proposed gradient attack and BLEUER attack approaches can be combined in the attack, which we call the MBART attack. Depending on gradients, obtained after predicting BLEU score on encoder outputs, we iteratively replace input tokens. After several iterations we get adversarial input Xadv and use the model to get adversarial translation Yadv."
        },
        {
            "heading": "3.5 Synthetic attacks",
            "text": "We propose an extremely naive synthetic attack as an alternative method to attack machine translation tasks. Synthetic attack [1] initially simulates errors or mistakes that can happen during daily usage of translation systems: keyboard typos, accidental omission/addition, or swapping chars. Additionally, we tried some uncommon sentence perturbations, such as randomly swapping a subset of words or randomly swapping a subset of chars in one word. As the main hyperparameter of such an attack, we used a portion of perturbed words or chars in the sentence."
        },
        {
            "heading": "4 Experiments",
            "text": "We perform the experiments with Marian and MBART Transformer models. For them, the comparison is between the three approaches described above: gradient, BLEUER, and synthetic attacks, since each of them represents a principal type of attack method. We also conduct a comparison of existing and novel attack algorithms. We pay special attention to balancing the trade-off between preserving the original sentence and altering the attacked translation. The attack should not be easily recognized by adversarial detectors, so the text should save logical and semantic literacy and grammar structure. We use a wide range of automatic linguistic metrics to evaluate attack approaches from this point of view. The code of our experiments will be available on a public online repository in the case of acceptance."
        },
        {
            "heading": "4.1 Metrics",
            "text": "Machine translation attacks aim to decrease the quality of translation metrics. We used 6 metrics in our experiments. BLEU is the most famous metric for evaluating the similarity of two sentences, and it is highly correlated with the human concept of text similarity. chrF metric is calculated as an F-score between character n-grams [13]. METEOR is another n-gram metric. It is calculated as an F-score for unigrams. WER metric considers a number of basic text operations: adding, deleting, and swapping characters for transforming one text into another. Paraphrase similarity metric is built upon pre-trained Sentence-Transformer [14], model, which matches texts into 768-dimensional vectors. Cosine distance between such vectors correlates greatly with a human opinion of text similarity. BertScore [22] leverages vectors, obtained from pre-trained models. Bert Score has been found to correspond with human judgment at the sentence level meaning. It calculates each token\u2019s precision, recall, and F1 measures in assessed sentences."
        },
        {
            "heading": "4.2 Baselines",
            "text": "In addition to the proposed methods, we evaluated naive approaches and stateof-the-art approaches.\nIn particular, we consider our variant of Gradient attack itself and a modification of it Gradient attack and ML constraint. For the later attack, we utilize constraints on how much we can change the initial sentence. These constraints are described above in the methods section and aim at keeping the meaning and structure of the attacked sentence similar to the initial one. The attack uses Marian [11] Transformers, pre-trained on English-Russian corpora.\nWe consider two types of attacks that consider an approximation of the target metric during an attack: BLEUER and MBART attacks. For these attacks, we train an additional head that takes encoder outputs as an input. These heads are predicting BLEU or BertScore correspondingly. As these heads are differentiable, we incorporate these scores into the loss function to maximize the difference between the initial translation Y and the attacked sentence translation Yattacked and minimize the difference between the initial sentence X and its adversarial perturbation Xattacked. For training of BLEUER and MBART, we use the validation data of wmt-14 dataset.\nTo make sure that all main types of attacks are considered, we evaluate methods from the literature. Prefix attack inserts tokens at the beginning as we try to select tokens that serve as a prompt. SWLS is the attack from the article [23]. This attack tries to leverage a bidirectional translation model and looks for perturbations that maximize the difference between adversarial sequence Xattacked and its back-translation.\nThe last two methods consider attacks at separate character levels. Char swap tries to randomly swap characters to make the attack stronger. Char + grad swap is a version of our gradient attack at the character level.\nAttack type\nSentence type Sentence\nGradient Orig. sentence Cars get many more miles to the gallon. Attacked sentence Cars get many more miles to the ormoneon. Orig. translation \u0410\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u0438 \u043f\u0440\u043e\u0435\u0437\u0436\u0430\u044e\u0442 \u0431\u043e\u043b\u044c\u0448\u0435 \u043c\u0438\u043b\u044c \u043d\u0430 \u043e\u0434\u0438\u043d"
        },
        {
            "heading": "4.3 Attack examples",
            "text": "At first, we visually examined the results of the conducted attacks by comparing examples of adversarially perturbed sentences. While sometimes the results are imperfect, in general, we see the desired effect. Examples of such sentences are provided in Table 1."
        },
        {
            "heading": "4.4 Experiment Setup",
            "text": "For gradient attack 3.2 and \"BLEUER\" attack 3.3 we used Marian [11] Transformers, pre-trained on English-Russian text corpora. For MBART 3.4 attack we used MBart-50 [17]). For \"BLEUER\" we additionally trained layers for approximating actual BLEU metric. For training we used validation data of wmt-14 dataset."
        },
        {
            "heading": "4.5 Main results",
            "text": "There is an important factor to be considered while evaluating machine translation adversarial attacks: perturbations should preserve the lexicon and grammar structure of the initial sentence. Authors of [12] proposed a new definition, meaning-preserving perturbations, which underline the importance of the correct\nassessment of an attack. We decided to take care of the balance of perturbing initial sentences and translations. Computing two similarities between the source sentence X and its perturbed sentence Xattacked and the similarity between the initial translation Y and translation of the attacked sentence Yattacked is key to holding such a balance. Suppose the violation of the initial sentence approximately coincides with the violation of the translation. In that case, we cannot talk about the attack\u2019s success: the model honestly works out on a distorted sentence. An ideal attack would slightly change the initial similarity metric but significantly decrease the similarity between translations.\nWe vary the ability to introduce modifications into the initial sentence by modifying hyperparameters for all attacks. For each attack setting, they form a Pareto frontier, which can help us analyze the attack\u2019s impact. Numbers near the dots indicate hyperparameters of the attack. For the gradient attack and BLEUER attack, we provided a threshold for minimum cosine distance between vectors of original and substitute tokens for each dot. For synthetic attacks, we provided a maximum number of basic transformations for each dot.\nPareto frontiers for the full set of considered attacks are presented in Figure 3. In general, the considered attacks could not show high values of attack success rates, supporting the evidence that modern translation models are robust due to the architectural features of models, computational expenses on training, and the colossal size of datasets. The top-performing attack is based on a swap at the level of characters. Both modifications show a significant improvement over others jointly providing a desired Pareto frontier."
        },
        {
            "heading": "4.6 Performance with respect to different metrics",
            "text": "We provide Pareto frontiers for 6 automatic text metrics for 3 types of attacks: gradient attack, BLEUER, and a synthetic attack. Experimental results are presented in Figure 4. Hitting as low and to the right as possible is the most successful attack, showing min distance between original and adversarial sentences and maximum distance between original and adversarial translations. It is rather evident from the graphics that most dots correspond to the same distortion of the initial and translation sequences. We can not ignore that the dots of the most straightforward method, synthetic attack on average, lie lower than the dots of more complicated approaches. That fact is especially noticeable for chrF metric due to a char-level of that attack. Simple char operations break the structure of tokens, heavily damaging deep models for machine translation, which usually work on the token level. The numerical summary is given in Table 2. It also supports the evidence that Synthetic attack provides superior metrics compared to an embedding-based approach that leverages the gradients of a model."
        },
        {
            "heading": "5 Acknowledgements",
            "text": "The research was supported by the Russian Science Foundation grant 20-71- 10135."
        },
        {
            "heading": "6 Conclusion",
            "text": "Adversarial attacks face limitations in the NLP domain. Especially for the machine translation task, both creating adversarial sequences and evaluating attacks become non-trivial. Most of the existing approaches have a high attack success rate, but they still suffer from lacking semantics and losing lexicon and grammar correctness. In our investigations, we focus on how we can make attacks more meaningful and valuable in analyzing the translation model\u2019s vulnerabilities. We tried to control translation metrics directly by using differentiable approximations.\nThe primary outcome of MT experiments is that we still did not find a method that guaranteed that translation would be changed stronger than a\nsource sentence. We compared a range of metrics between initial and corrupted sentences and between initial and attacked translations. We made many additional rules and constraints which forced the attack algorithm not to collapse the initial sentence and save its semantic meaning totally, but they did not significantly change the situation."
        }
    ],
    "title": "Machine Translation Models Stand Strong in the Face of Adversarial Attacks",
    "year": 2023
}