{
    "abstractText": "We combine replica exchange (parallel tempering) with normalizing flows, a class of deep generative models. These two sampling strategies complement each other, resulting in an efficient strategy for sampling molecular systems characterized by rare events, which we call learned replica exchange (LREX). In LREX, a normalizing flow is trained to map the configurations of the fastest-mixing replica into configurations belonging to the target distribution, allowing direct exchanges between the two without the need to simulate intermediate replicas. This can significantly reduce the computational cost compared to standard replica exchange. The proposed method also offers several advantages with respect to Boltzmann generators that directly use normalizing flows to sample the target distribution. We apply LREX to some prototypical molecular dynamics systems, highlighting the improvements over previous methods. Graphical TOC Entry RE X Thigh T1 T2 T3 T4 Tlow Potential Energy LR EX Thigh Mapped Thigh Tlow",
    "authors": [
        {
            "affiliations": [],
            "name": "Michele Invernizzi"
        },
        {
            "affiliations": [],
            "name": "Andreas Kr\u00e4mer"
        },
        {
            "affiliations": [],
            "name": "Cecilia Clementi"
        },
        {
            "affiliations": [],
            "name": "Frank No\u00e9"
        }
    ],
    "id": "SP:489c6be7dd8f51e14bbac1b7962fcc75978bce6a",
    "references": [],
    "sections": [
        {
            "text": "We combine replica exchange (parallel tempering) with normalizing flows, a class of deep generative models. These two sampling strategies complement each other, resulting in an efficient strategy for sampling molecular systems characterized by rare events, which we call learned replica exchange (LREX). In LREX, a normalizing flow is trained to map the configurations of the fastest-mixing replica into configurations belonging to the target distribution, allowing direct exchanges between the two without the need to simulate intermediate replicas. This can significantly reduce the computational cost compared to standard replica exchange. The proposed method also offers several advantages with respect to Boltzmann generators that directly use normalizing flows to sample the target distribution. We apply LREX to some prototypical molecular dynamics systems, highlighting the improvements over previous methods."
        },
        {
            "heading": "Graphical TOC Entry",
            "text": "RE X\nThigh T1 T2 T3 T4 Tlow\nPotential Energy\nLR EX\nThigh Mapped Thigh Tlow\nSimulation Time"
        },
        {
            "heading": "Keywords",
            "text": "parallel tempering, Boltzmann generators, enhanced sampling, machine learning, neural networks, molecular dynamics\n1\nar X\niv :2\n21 0.\n14 10\n4v 2\n[ ph\nys ic\ns. co\nm p-\nMolecular simulations are becoming more and more important for studying complex phenomena in physics, chemistry, and biology. One of the long-lasting challenges for molecular simulations is to efficiently sample the equilibrium Boltzmann distribution, which is often characterized by multiple metastable states. In this letter, we propose a novel sampling method that combines the well-established replica exchange method1,2 with a recent machine-learning technique known as normalizing flows.3\u20135 To this end, we first summarize the two original methods and discuss their strengths and weaknesses. We then introduce flow-based exchange moves and show with a few prototypical examples how they help outperform classical replica exchange.\nReplica exchange (REX), also known as parallel tempering, is a popular enhanced sampling method.1,2 It uses a ladder of overlapping distributions to connect the target distribution one wishes to sample with an easier-to-sample probability distribution, for example at higher temperature, that we call here prior distribution. We keep a general notation and indicate with q(x) = e\u2212uq(x)/Zq the prior distribution, and with p(x) = e\u2212up(x)/Zp the target, where up and uq are the respective reduced energies, Zq = \u222b e\u2212uq(x)dx and Zp = \u222b e\u2212up(x)dx are partition functions, and x is the system configuration. A set of M + 1 replicas of the system are chosen to form a ladder of Boltzmann distributions pi(x) \u221d e\u2212ui(x) from p0(x) = p(x) to pM(x) = q(x), such that each pi(x) overlaps with its neighbors in configuration space. In the typical case of temperature expansion, one has ui(x) = (kBTi)\n\u22121U(x), where U(x) is the potential energy, kB the Boltzmann constant, and the temperatures Ti interpolate between the temperatures of the target and prior distributions, T0 = Tlow and TM = Thigh, respectively. Other kinds of expansions are also possible, such as solute tempering6 or alchemical transformations,7 generally known as Hamiltonian replica exchange. Each replica is sampled with local moves, such as Markov chain Monte Carlo or molecular dynamics (MD), and at regular time intervals an exchange is proposed between the configurations of different replicas,\nxi xj, and accepted with probability\n\u03b1REX = min\n{ 1, pj(xi)\npi(xi)\npi(xj) pj(xj)\n}\n= min { 1, e\u2206uij(xi)\u2212\u2206uij(xj) } , (1)\nwhere xi and xj are configurations sampled from the corresponding distribution pi or pj, and \u2206uij(x) = ui(x) \u2212 uj(x) is the difference in reduced energy between replica i and replica j. The best choice of intermediate pi is not trivial even in the well-studied case of temperature REX, and several different approaches have been proposed to optimize it.2,7 The total number of replicas M required to allow exchanges increases both with the number of degrees of freedom of the system N , M \u221d \u221a N , and with the distance (e.g. in temperature) between prior and target.8\nOne of the main limitations of REX is that a large number of replicas might be necessary to connect the target and the prior distributions, making the method computationally too expensive. There are variants of REX that do not require a fixed number of parallel simulations, such as simulated tempering,9 or expanded ensemble methods,10,11 but they share the same\u221a N scaling of sampling efficiency as REX (see Fig. S2 of the Supporting Information). Several approaches have been proposed to mitigate this scaling, such as using nonequilibrium switches,12,13 but with limited practical success. Two popular strategies to reduce the total number of replicas are to apply the tempering only to part of the system,6 or to broaden the distributions with metadynamics.14 We will show how to combine REX with normalizing flows to avoid altogether the need to simulate intermediate replicas.\nNormalizing flows (NF) are a class of invertible deep neural networks that can be used to generate samples according to a given target distribution, and are at the core of the recently proposed Boltzmann generators.15 A normalizing flow f is an invertible function that maps a configuration x drawn from a prior distribution, q(x), into a new configuration x\u2032 = f(x) that samples the output distribution of the flow, q\u2032(x), also called mapped distribution. Exploit-\n2\ning the invertibility of f , one can compute the output probability density as:\nq\u2032(x\u2032) = q(x) | det Jf (x)|\u22121 , (2)\nwhere det Jf (x) is the determinant of the Jacobian of f and | det Jf (x)|\u22121 = | det Jf\u22121(x\u2032)|. The aim of NF is to approximate the ideal map which transforms the prior into the target, thus q\u2032(x) = p(x). However, even without a perfect map, one can compute the importance weights wf (x) needed to reweight from q\u2032 to p the ensemble average of any observable O(x), \u3008O(x)\u3009p = \u3008O(f(x))wf (x)\u3009q/\u3008wf (x)\u3009q, as discussed in Ref. 15. We choose to define the weights as\nwf (x) = e uq(x)\u2212up(f(x))+log | det Jf (x)| , (3)\nso that wf (x) \u221d p(x)/q\u2032(x), where the precise proportionality constant is irrelevant for the purpose of reweighting. In Boltzmann generators, one typically chooses as prior q a normal or uniform distribution, so that independent and identically distributed samples can be used to estimate the ensemble averages \u3008\u00b7\u3009q. The weights are also useful to assess how effective the f mapping is in increasing the overlap between q and p, for example through the Kish effective sample size,16\nneff = [ \u2211n i wf (xi)] 2\n\u2211n i [wf (xi)] 2 . (4)\nIn NF, the mapping f is implemented by an invertible deep neural network that by construction has an easy-to-calculate det Jf . It can be trained to learn the target probability distribution p(x) by minimizing the Kullback-Leibler divergence:\nDKL(q \u2032\u2016p) =\u2212 \u222b q\u2032(x\u2032) log p(x\u2032)\nq\u2032(x\u2032) dx\u2032\n=\u2212 \u222b q(x) logwf (x) dx\u2212\u2206Fpq ,\n(5)\nwhere the second line has been obtained via the change of variables x\u2032 = f(x), and \u2206Fpq = \u2212 log Zq Zp is the free energy difference between\nq and p that is unknown but independent of f . The loss function then simplifies to Lf = \u2212\u3008logwf (x)\u3009q, which in Ref. 15 is referred to as energy-based training, as opposed to maximumlikelihood training that instead requires sampling the target distribution p(x) and minimizing DKL(p\u2016q\u2032). The loss function is an upper bound to the free energy difference Lf \u2265 \u2206Fpq, with the identity holding only in case of the ideal map.17\nThe main limitations of NF are that they might not be expressive enough to represent the complex map from the prior to the target, or that they can be hard to train in practice. In particular, energy-based training is characterized by mode-seeking behavior, and struggles to converge reliably when the target distribution is multimodal.18\nIn this letter, we propose to use normalizing flows in a replica exchange setting to map the fast-mixing prior distribution to the target Boltzmann distribution, giving rise to the phase space overlap necessary for direct exchange, without the need to sample intermediate distributions. The idea of using a map to improve the overlap between distributions was first proposed by Jarzynski under the name of targeted free energy perturbation,19 and has recently been combined with NF in the learned free energy perturbation (LFEP) method.17,20\u201322 By analogy, we will refer to our method as learned replica exchange (LREX). Contrary to LFEP, in LREX we combine the NF mapping with local moves, which can greatly improve the sampling, as shown by several recent works.18,23\u201326 Our approach can also be seen as a type of Boltzmann generator,15 with two main differences: (i) the prior is a nontrivial distribution sampled via MD, and (ii) the target is directly sampled in a replica exchange setting, rather than only reconstructed via reweighting.\nWe now present the proposed LREX method in detail. To perform LREX, we first run a relatively short MD simulation to gather samples from the prior distribution q(x), and use them to train the NF. Following Ref. 17, the NF is initialized to be the identity, thus x\u2032 = x and q\u2032(x) = q(x). Ideally, the prior distribution should be easy to sample while still ex-\n3\nhibiting the main features of the target, such as all the relevant metastable basins. This choice of prior allows us to avoid most of the problems related to energy-based training of normalizing flows,15,27 and significantly reduces computational cost. Training in the LREX setup typically requires only a few epochs and does not suffer from mode collapse, nor from numerical instabilities linked to extremely high energies that can arise from atom clashes and other nonphysical configurations in the prior. While the NF is training, it is possible to estimate its efficiency in increasing the phase space overlap between prior and target by computing neff, Eq. (4), over the training set and/or a validation set of n prior samples. The sampling efficiency neff/n also provides an estimate of the frequency with which exchanges will be accepted in the final LREX run. Once the NF is trained, the prior and the target systems are simulated in parallel with MD and, at fixed time intervals, an exchange between mapped configurations is attempted according to the following probability:\n\u03b1LREX = min\n{ 1, p(x\u2032q)\nq\u2032(x\u2032q)\nq\u2032(xp) p(xp)\n}\n= min {1, wf (xq)wf\u22121(xp)} (6)\nwhere xp and xq are the current configurations of the prior and target replica, respectively, and wf\u22121 are the weights of the inverse mapping, defined as in Eq. (3). If the exchange is accepted, the MD simulation of the prior continues from the configuration x\u2032q = f(xq), while the target continues from f\u22121(xp). New velocities are randomly assigned from the respective equilibrium distributions.\nTo demonstrate the advantages of the LREX approach over standard REX and Boltzmann generators, we present three examples. The first system considered is a particle moving with Langevin dynamics in a N -dimensional doublewell potential, shown in Fig. 1a. The first two dimensions, with coordinates x1 and x2, feel the 2D potential introduced in Ref. 28, while all the other N\u22122 are subject to a harmonic potential (details in the SI). Target and prior distribution are obtained from the same system, but at dif-\nferent temperatures. The target distribution is at reduced temperature Tlow = 1, where transitions between the two basins are extremely rare, while the prior has Thigh = 5, which instead can be sampled efficiently also with a short MD run.\nFigure 1c shows a visual comparison of standard REX (top row) and the proposed LREX method (bottom row). In replica exchange, the number of intermediate temperatures grows with the system size, independently of the fact that the added degrees of freedom are trivial Gaussian noise. Moreover, as the number of replicas increases, the mixing time required to equilibrate the simulation also increases, further limiting the efficiency of the method.29 In LREX, instead, only the highest and lowest temperatures need to be sampled, because the NF easily learns a transformation that makes the high-temperature configuration space overlap with the low-temperature one. The NF architecture and training procedure are kept identical for all sizes, and are described in detail in the supporting information (SI). Although the size of the neural network increases with N , the slowdown in training time and efficiency is minor, thus the overall scaling of LREX with system size is more favorable than the one of standard REX, for all the systems we studied.30 Figure 1b presents the sampling efficiency of REX and LREX as a function of system size N . Other expanded ensemble methods would have similar scaling as REX (see Fig. S2 in SI). The sampling efficiency is estimated by considering the importance weights of all the replicas and is an upper bound estimate strictly valid only in the infinite simulation limit (see SI). In a finite REX simulation, the efficiency can be further reduced by long time correlations due to a slow mixing time or small acceptance rate (see Fig. S1 in SI). We also do not include the training cost for LREX, which would shift the curve down by a fixed small amount, depending on the total simulation length.\nNext, we consider alanine dipeptide in vacuum. This molecule has two long-lived metastable basins that can be identified by its \u03c6 torsion angle. The target is to sample the system at Tlow = 300 K, and as prior we consider a very high temperature, Thigh = 1000 K, at\n4\n5\n6\nwhich there are no sampling issues. With standard REX, one would need about four parallel replicas of the system to ensure proper overlap and good acceptance rate for the exchanges (see SI). Alanine dipeptide is composed of 22 atoms, thus its configuration space has 66 dimensions, x \u2208 R66. Following Refs. 15,31 we remove global rotations and translations by modeling the NF in internal coordinates (bonds, angles, and torsion angles). This also makes it easy to constrain hydrogen atoms at fixed bond length, and leaves us with a total of 48 degrees of freedom. To train the NF, we perform a short 20 ns MD simulation, from which we take 20\u2032000 configurations at regular intervals of 500 MD steps, which are used as prior samples for the training. Thanks to our choice of prior distributions, we can use a simple flow architecture with only a few coupling layers.32 Three epochs are enough to reach a good sampling efficiency neff/n > 1%, and the entire training takes only a few minutes on a single consumergrade CPU. Overall, the training is orders of magnitude faster than for previous Boltzmann generators of alanine dipeptide.27,31,33 The reason for this speedup is twofold. Firstly, our prior is closer to the target, so the flow needs to learn a simpler transformation. Secondly, the fact that LREX combines NF with local sampling of the target distribution allows one to settle for a smaller sampling efficiency, without compromising on the accuracy at which observables can be computed.25\nTo get a sense of the effect of the trained NF map, we report in Fig. 2c the 1D marginal distribution of some of the internal coordinates and in Fig. 2d histograms over the 2D \u03c6 and \u03c8 plane. Intuitively, the NF map pushes hightemperature configurations towards more lowtemperature-like configurations, with smaller fluctuations from the local minima. The NF map is far from perfect, but, as indicated by Fig. 2b, it makes the two distributions overlap, so that reweighting can be used to reconstruct the target. This is shown in Fig. 2a, where the free energy surface (FES) along \u03c6 is reported. Interestingly, the NF does not map configurations from one metastable state to the other; it mostly makes local adjustments. This ensures\nthat no mode collapse occurs during the NF energy-based training, which is one of the main current limitations of energy-based training for NF.12 In the case of alanine dipeptide, the NF we obtain is good enough that there is no need to perform the full LREX procedure; it is possible to accurately compute the torsion angle FES by direct reweighting, similarly as in Rizzi et al. 17. However, by running the prior and target in parallel and swapping configurations according to the LREX procedure, the sampling efficiency further increases, matching the one of standard REX for this system. In the SI we report the case of a target distribution at a much lower temperature of Tlow = 100 K, where LREX is significantly more efficient than REX.\nAs a last example we consider the alanine tetrapeptide molecule. It has three pairs of \u03c6-\u03c8 torsion angles, for a total of 8 metastable basins that can be identified by the signs of the three \u03c6 angles. The system has 42 atoms, roughly double that of alanine dipeptide, for a total of 98 internal coordinates. This time we choose a less trivial prior and target, to show that LREX can be used not only across temperatures, but also when the potential energy between prior and target differs. As the target we take the system in implicit solvent at Tlow = 300 K, while for the prior we use the system in vacuum at Thigh = 1000 K and also add a restraint potential Ur(\u03c8) acting on the three \u03c8 angles,\nUr(\u03c8) = 3\u2211\ni=1\nk sin2(\u03c8i/2) , (7)\nwith k = 100 kJ/mol. The chosen prior distribution not only has no phase space overlap with the target, as in the previous two examples, but it also has a smaller number of local minima in some metastable basins, as can be seen in Fig. 3a. We use the same NF architecture as for alanine dipeptide (see SI), and the same amount of training data, obtained from just 20 ns of MD from the prior distribution. Also in this case the training requires only a few epochs and is computationally extremely cheap. Compared to the alanine dipeptide case, we obtain a lower sampling efficiency, neff/n \u2248 0.1%, which indicates a poorer phase space overlap.\n7\nAs shown in Fig. 3c, the resulting NF fails to sample the correct target distribution when directly used via reweighting, as in Boltzmann generators and LFEP. Using it in a LREX setup is a better strategy, and allows for an accurate reconstruction of the FES, Fig. 3d. To perform LREX, we concurrently run 100 ns of MD for both the prior and the target replica, attempting an exchange every 1 ps, according to Eq. (6). The acceptance rate obtained is around 2% and about 15% of the accepted exchanges are useful exchanges, meaning that they make the target MD replica jump from one metastable basin into a different one. Although it is possible to train a larger NF and use more MD data to better learn the target distribution, this would be computationally much more expensive to the extent of being less efficient than a simpler REX with many replicas. Thanks to the LREX setup, we can make good use of a less than perfect NF, significantly improving efficiency and accuracy over the standard reweighting strategy of Boltzmann generators.\nIn conclusion, this letter presents the learned replica exchange method that combines replica exchange and normalizing flow, resulting in improved performance over the two methods used separately. The LREX method has three phases: (i) run a short simulation to gather samples from the prior distribution, (ii) use these samples for energy-based training of the NF, (iii) run parallel simulations of the prior and target, attempting an exchange at regular intervals with acceptance given by Eq. (6). An important aspect of the method is that it is possible to estimate the efficiency of LREX before performing the final production run, simply by computing neff over the training data. If the system is particularly challenging and neff remains very low despite increasing the training or tuning the hyperparameters of the NF, it is possible to expand LREX by dividing the problem into smaller parts and training the NF to map to an intermediate system. This provides a straightforward way to improve the accuracy of the sampling by gradually increasing the number of replicas, in analogy with REX. It should be noted that in the worst case scenario, when the NF is completely wrong, LREX is equiva-\n8\nlent to plain MD. Compared to standard REX, using LREX allows for a drastic reduction of the number of replicas that have to be simulated in parallel, which can lead to a significant reduction in computation cost, especially when machine learning potentials are employed. This could allow the use of prior distributions that would otherwise be impractical for REX, such as coarsegrained versions of the target system.35 Unfortunately, a simple scaling law with system size is not available for NF, so new experiments are needed to assess feasibility on larger systems.30 Compared to other setups that use NF as Boltzmann generators, LREX can be significantly more efficient and handle larger systems, due to the similarity between prior and target. It can also be more accurate due to the combination with local MD sampling. The main limitations of LREX are due to the current limitations of normalizing flows. For example, NF have not yet been successfully applied to systems with explicit solvent, except in cases where only a subset of the system has been considered.36,37 However, NFs have been significantly improved in recent years, and any future improvements to the architecture or training of normalizing flows, making them more efficient or expressive, can also be readily exploited in LREX. For this reason, we expect LREX to become a useful tool for molecular simulations, especially as normalizing flows capabilities are developing rapidly, allowing larger and more diverse systems to be handled.\nData availability All the code and data needed to reproduce the simulations are available at https://github.com/invemichele/ learned-replica-exchange. Molecular dynamics simulations were performed with the OpenMM software.38 For the reference simulations the on-the-fly probability enhanced sampling11,34,39 has been used, as implemented in PLUMED.40,41 The normalizing flows have been implemented using the bgflow library (https://github.com/noegroup/bgflow).\nAcknowledgement The authors thank Jonas Ko\u0308hler for useful discussions. M.I. ac-\nknowledges support from the Swiss National Science Foundation through an Early Postdoc.Mobility fellowship and the Humboldt Foundation for a Postdoctoral Research Fellowship. F.N. and C.C. acknowledge funding from Deutsche Forschungsgemeinschaft (DFG) via CRC 1114 Project B08. F.N. and A.K. acknowledge funding from European Research Council (ERC) via CoG 772230 \u201cScaleCell\u201d.\nSupporting Information Avail-\nable\nComputational details and extra figures for the double well potential, alanine dipeptide, and alanine tetrapeptide."
        },
        {
            "heading": "Data availability",
            "text": "All the code and input files to reproduce the simulations presented in the paper are available at https://github.com/invemichele/ learned-replica-exchange, together with the data of the main results."
        },
        {
            "heading": "Double-well potential",
            "text": "We consider an N -dimensional double-well potential defined as\nUdw(x) = Uwq(x1, x2) + 15\n2\nN\u2211\ni=3\nx2i , (S1)\nwhere\nUwq(x1, x2) = 2(y 4 1 + y 4 2 \u2212 2y21 \u2212 4y22+\n+2y1y2 + 0.8y1 + 0.1y2 + 9.28) (S2)\nis the modified Wolfe-Quapp potential introduced in Ref. 1, and y[1,2] = y[1,2](x1, x2) are rotated coordinates,\ny1(x1, x2) = x1 cos(\u03b8)\u2212 x2 sin(\u03b8) y2(x1, x2) = x1 sin(\u03b8) + x2 cos(\u03b8)\n(S3)\nwith \u03b8 = 0.15\u03c0. The target distribution is p(x) \u221d e\u2212Udw(x)/Tlow with Tlow = 1 in reduced units, while the prior one is q(x) \u221d e\u2212Udw(x)/Thigh with Thigh = 5. We perform Langevin molecular dynamics (MD) with OpenMM2 for several values of N , from N = 2 to N = 214 = 16\u2032384. The replica exchange simulations are performed with geometrically spaced temperatures. The total number of replicasM is chosen so that on average the acceptance rate is between 15% and 20%. Exchanges are attempted every 100 MD steps, and we run each replica for a total of 1\u2032000\u2032000 steps. As expected, a significant increase of the mixing time is observed as N and M increase. This can be seen e.g. from the trajectory of the lower temperature replica, Fig. S1a, where the number of useful exchanges between different metastable basins is significantly smaller for higher N . The normalizing flow (NF) used is an affine flow with two coupling layers obtained by splitting in half the system\u2019s dimensions. The neural networks used have 4 layers of size [N/2, 128, 128, N/2], for each of the studied system sizes N . The NF is initialized to the identity by setting all parameters to zero. For the training we collect 64\u2032000 samples from the prior, one each 100 MD steps. The size of training data has not been optimized, and a smaller\nS1\ntime\n2\n0\n2\n4\nx\n(a) REX, T = Tlow N=16, M=4, =27%\n0 2000 4000 6000 8000 10000 time\n2\n0\n2\n4\nx\nN=4096, M=40, =19%\ntime\n2\n0\n2\n4\nx\n(b) LREX, T = Tlow N=16, M=2, =20%\n0 2000 4000 6000 8000 10000 time\n2\n0\n2\n4\nx\nN=4096, M=2, =14%\nFigure S1: The x coordinate trajectories of the low temperature replica of the double-well system obtained via standard REX (a) and LREX (b). For each of the trajectories we report the system size N , the total number of replicas M , and the average acceptance rate \u03b1\u0304.\none would likely work just as well. Training is performed with the Adam optimizer for one epoch with batch size 128 and the learning rate is progressively reduced with a scheduler from 10\u22122 to 10\u22125. We found the learning rate to be an important hyperparameter. We hypothesise that since the NF is initialized to the identity, a large learning rate is needed to move it out of this low entropy state, while in order to precisely converge to the correct high-dimensional target, a very small learning rate is essential to avoid noise that can kill the sampling efficiency. Once the NF is trained, LREX can be run by simulating only the high and low temperature and directly exchanging configurations via the NF. The observed average exchange rate is well approximated by the Kish effective sampling size measured on the training data using the NF weights. It is around 20% for all N values, except for the two largest tested systems, N = 212 and N = 214, for which it lowers to about 17% and 12% respectively, as reported in Fig.1b of the main text. Contrary to REX, in LREX we do not observe mixing issues, since the exchanges are only between two replicas and the acceptance rate is more than large enough for any size N , Fig. S1b.\n4 16 64 256 1024 4096 16384 N\n1%\n5%\n10%\n50%\n100%\nsa m\npl in\ng ef\nfic ie\nnc y\n%\nREX OPES LREX\nFigure S2: Sampling efficiency of the doublewell system as a function of the sizeN for different methods. Data for REX and LREX are the same shown in Fig. 1b of the main text. OPES samples the same ensemble as REX, but with a single replica, thanks to an adaptive bias potential.3 Despite being more efficient than REX, OPES follows the same scaling law.\nFigure S2 reproduces Fig. 1b of the main text,\nS2\nwith the addition of the sampling efficiency obtained with the on-the-fly enhanced sampling method (OPES).3 OPES can be used to sample an ensemble equivalent to the one sampled by REX, but in a single MD run, by adding an adaptive bias potential. As can be seen from Fig. S2, OPES is more efficient than REX, but presents the same efficiency scaling as a function of system size, \u03b7 \u221d \u221a N . The number of samples needed to converge OPES adaptive potential increases with system size, but the resulting slow down is less dramatic than the one due to slow mixing in REX. The sampling efficiency \u03b7 shown in Fig. S2 and Fig. 1b of the main text is calculated as follows:\n\u03b7 = 1\nMn\nM\u2211\ni\nn (i) eff , (S4)\nwhereM is the total number of replicas, n is the total number of samples and n\n(i) eff is the effective\nsampling size of replica i, calculated from the importance weights according to Eq. (4) of the main text. For the LREX case these are the weights given by the NF, Eq. (3) main text, while for REX they are calculated as\nwi(x) = e (T\u22121i \u2212T \u22121 low)k \u22121 B U(x) , (S5)\nwhere Ti is the temperature of replica i. Thus the neff of the low temperature replica is always equal to n, as expected, and in REX only the nearest replica usually has a neff > 1. From a practical point of view, unless M is very small, one has that \u03b7REX \u2248 1M . Instead for LREX the contribution to the sampling efficiency coming from the high temperature replica remains higher than 2.5% even for N = 214, thus \u03b7LREX > 50% for each N , as shown in Fig. S2. Finally, in the case of OPES where only one replica of the system is simulated, the weights to calculate neff are computed according to the value of the bias potential, as explained in Ref. 3. As a final remark, we notice that for this simple double-well system it is also possible to analytically define a simple mapping f \u2217 from high temperature to low temperature that allows for direct exchanges. This mapping does\nnot change the first two nontrivial coordinates and properly rescales the remaining ones:\nf \u2217(xi) = { xi if i \u2208 {1, 2} xi \u221a\nTlow Thigh\nif i \u2208 {3, N} (S6)\nFor N = 2 the f \u2217 map is the identity, and its efficiency is neff/n = 17%, lower than what the trained NF was able to achieve, that is around 22%. Contrary to the NF, f \u2217 exactly maps the added degrees of freedom and as a consequence its efficiency does not depend on N , meaning that it remains constant at 17% no matter how much we increase the system size. The catch it that such an analytical map is extremely difficult to guess for any system but the most trivial ones. Already for alanine dipeptide it would be quite difficult to find one, while the NF can easily be trained to learn it."
        },
        {
            "heading": "Alanine dipeptide",
            "text": "The alanine dipeptide molecular dynamics simulations where performed with OpenMM2 using a Langevin integrator with standard parameters (see GitHub repository). To obtain a reference we run a multithermal simulation using the on-the-fly probability enhanced sampling method (OPES).3 OPES provides a simple way to sample all the relevant temperatures in a single MD simulation, by adding an adaptive bias. The ensemble sampled at convergence is equivalent to the one sampled by REX, and the sampling efficiency has the same scaling, albeit being typically higher. The normalizing flow is implemented using the bgflow library (https://github.com/ noegroup/bgflow), and acts on the internal coordinates of bonds, angles and torsions. It consists of two spline coupling layers4 that act separately on bonds, angles and torsions. Circular splines are used for torsion angles, to handle the periodicity. For bonds and angles, the domain of the spline flows are defined using the high temperature training data, with the assumption that for the target low temperature it will be the same or smaller. The NF is initialized to the identity by setting all 438\u2032842\nS3\n0 1 2 3 Epochs\n5\n0\n5\n10\n15\n20\nLo ss\n[k BT\n]\ntraining testing\nF\nFigure S3: Typical training of the alanine dipeptide normalizing flow. The loss function is an upper bound to the free energy difference \u2206F between the two temperatures, Thigh = 1000 K and Tlow = 300 K. The testing data is taken from another MD run at Thigh.\nparameters to zero. Training by energy is done with the Adam optimizer and a fixed learning rate of 10\u22123 and a small batch size of 32. We already find good results after 3 epochs over the 20\u2032000 samples from the prior (20 ns), see Fig. S3. Similar results are obtained also when using half the training points, a 10 ns MD run. We clip contributions to the energy that are too high, to make training more stable. Once the flow is trained, we run LREX for 100 ns, simulating both prior and target while trying an exchange every 1 ps (500 MD steps). The average acceptance rate is approximately 23% and among the accepted exchanges approximately 4% move the low temperature replica from one metastable state to the other one. With the same setup we can train the NF using as target Tlow = 100 K instead of 300 K. The resulting sampling efficiency is smaller, neff/n \u2248 0.1%, but still high enough to be useful. In this case the flow is not good enough to reconstruct the FES along the \u03d5 angle from reweighting alone, so we must use LREX to have an accurate estimate. Given the extremely low temperature of the target, the free energy difference between the two metastable states is very large, \u2248 10kBT . As a consequence, we need a much longer simulation to obtain a relevant amount of samples also from the least populated metastable state. We report the results in Fig. S4."
        },
        {
            "heading": "Alanine tetrapeptide",
            "text": "The reference simulations for alanine tetrapeptide were obtained with OPES5 by enhancing the sampling of the three \u03d5 angles. The very same NF setup used for alanine dipeptide is also used for alanine tetrapeptide, with no fine tuning. The resulting flow is slightly bigger, with 792\u2032022 parameters. The sampling efficiency neff/n of the trained NF is approximately 0.8%, and the acceptance rate of the LREX exchanges over the 100 ns MD run is approximately 2.4%. Of the accepted exchanges about 12.6% make the target replica move to a different metastable state. Figure S5 shows the two dimensional FES estimates as Fig. 3 from the main text, but for all Ramachandran angles, while Fig. S6 reports the one dimensional FES for the same data."
        }
    ],
    "title": "Skipping the Replica Exchange Ladder with Normalizing Flows",
    "year": 2022
}