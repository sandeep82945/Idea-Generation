{
    "abstractText": "In the noisy intermediate-scale quantum era, an important goal is the conception of implementable algorithms that exploit the rich dynamics of quantum systems and the high dimensionality of the underlying Hilbert spaces to perform tasks while prescinding from noise-proof physical systems. An emerging class of quantum learning machines is that based on the paradigm of quantum kernels. Here, we study how dissipation and decoherence affect their performance. We address this issue by investigating the expressivity and the generalization capacity of these models within the framework of kernel theory. We introduce and study the effective kernel rank, a figure of merit that quantifies the number of independent features a noisy quantum kernel is able to extract from input data. Moreover, we derive an upper bound on the generalization error of the model that involves the average purity of the encoded states. Thereby we show that decoherence and dissipation can be seen as an implicit regularization for the quantum kernel machines. As an illustrative example, we report exact finite-size simulations of machines based on chains of driven-dissipative quantum spins to perform a classification task, where the input data are encoded into the driving fields and the quantum physical system is fixed. We determine how the performance of noisy kernel machines scales with the number of nodes (chain sites) as a function of decoherence and examine the effect of imperfect measurements.",
    "authors": [
        {
            "affiliations": [],
            "name": "Valentin Heyraud"
        },
        {
            "affiliations": [],
            "name": "Zejian Li"
        },
        {
            "affiliations": [],
            "name": "Zakari Denis"
        },
        {
            "affiliations": [],
            "name": "Alexandre Le Boit\u00e9"
        },
        {
            "affiliations": [],
            "name": "Cristiano Ciuti"
        }
    ],
    "id": "SP:33baeb56ae08bcdcaf837f9d203169a05842fed5",
    "references": [
        {
            "authors": [
                "Yann LeCun",
                "Yoshua Bengio",
                "Geoffrey Hinton"
            ],
            "title": "Deep learning",
            "venue": "Nature 521, 436\u2013444 (2015).",
            "year": 2015
        },
        {
            "authors": [
                "Ian Goodfellow",
                "Yoshua Bengio",
                "Aaron Courville"
            ],
            "title": "Deep Learning, edited by Francis Bach, Adaptive Computation and Machine Learning Series",
            "year": 2016
        },
        {
            "authors": [
                "Emma Strubell",
                "Ananya Ganesh",
                "Andrew McCallum"
            ],
            "title": "Energy and policy considerations for deep learning in NLP",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (Association for Computational Linguistics, 2019) pp. 3645\u20133650.",
            "year": 2019
        },
        {
            "authors": [
                "Gouhei Tanaka",
                "Toshiyuki Yamane",
                "Jean Benoit H\u00e9roux",
                "Ryosho Nakane",
                "Naoki Kanazawa",
                "Seiji Takeda",
                "Hidetoshi Numata",
                "Daiju Nakano",
                "Akira Hirose"
            ],
            "title": "Recent advances in physical reservoir computing: A review",
            "venue": "Neural Networks 115, 100\u2013123 (2019).",
            "year": 2019
        },
        {
            "authors": [
                "Guang-Bin Huang",
                "Qin-Yu Zhu",
                "Chee-Kheong Siew"
            ],
            "title": "Extreme learning machine: Theory and applications",
            "venue": "Neurocomputing Neural Networks, 70, 489\u2013501 (2006).",
            "year": 2006
        },
        {
            "authors": [
                "Andrzej Opala",
                "Sanjib Ghosh",
                "Timothy C.H. Liew",
                "Micha\u0142 Matuszewski"
            ],
            "title": "Neuromorphic Computing in Ginzburg-Landau Polariton-Lattice Systems",
            "venue": "Physical Review Applied 11, 064029 (2019).",
            "year": 2019
        },
        {
            "authors": [
                "Zakari Denis",
                "Ivan Favero",
                "Cristiano Ciuti"
            ],
            "title": "Photonic Kernel Machine Learning for Ultrafast Spectral Analysis",
            "venue": "Physical Review Applied 17, 034077 (2022). 17",
            "year": 2022
        },
        {
            "authors": [
                "Dario Ballarini",
                "Antonio Gianfrate",
                "Riccardo Panico",
                "Andrzej Opala",
                "Sanjib Ghosh",
                "Lorenzo Dominici",
                "Vincenzo Ardizzone",
                "Milena De Giorgi",
                "Giovanni Lerario",
                "Giuseppe Gigli",
                "Timothy C.H. Liew",
                "Michal Matuszewski",
                "Daniele Sanvitto"
            ],
            "title": "Polaritonic Neuromorphic Computing Outperforms Linear Classifiers",
            "venue": "Nano Letters 20, 3506\u2013 3512 (2020).",
            "year": 2020
        },
        {
            "authors": [
                "Davide Pierangeli",
                "Giulia Marcucci",
                "Claudio Conti"
            ],
            "title": "Photonic extreme learning machine by free-space optical propagation",
            "venue": "Photon. Res. 9, 1446\u20131454 (2021).",
            "year": 2021
        },
        {
            "authors": [
                "Thomas Hofmann",
                "Bernhard Sch\u00f6lkopf",
                "Alexander J. Smola"
            ],
            "title": "Kernel methods in machine learning",
            "venue": "The Annals of Statistics 36, 1171\u20131220 (2008).",
            "year": 2008
        },
        {
            "authors": [
                "Arthur Jacot",
                "Franck Gabriel",
                "Clement Hongler"
            ],
            "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
            "venue": "Advances in Neural Information Processing Systems, Vol. 31 (Curran Associates, Inc., 2018).",
            "year": 2018
        },
        {
            "authors": [
                "Jacob Biamonte",
                "Peter Wittek",
                "Nicola Pancotti",
                "Patrick Rebentrost",
                "Nathan Wiebe",
                "Seth Lloyd"
            ],
            "title": "Quantum machine learning",
            "venue": "Nature 549, 195\u2013202 (2017).",
            "year": 2017
        },
        {
            "authors": [
                "Vedran Dunjko",
                "Hans J. Briegel"
            ],
            "title": "Machine learning & artificial intelligence in the quantum domain: A review of recent progress",
            "venue": "Reports on Progress in Physics 81, 074001 (2018).",
            "year": 2018
        },
        {
            "authors": [
                "Maria Schuld",
                "Ilya Sinayskiy",
                "Francesco Petruccione"
            ],
            "title": "The quest for a Quantum Neural Network",
            "venue": "Quantum Information Processing 13, 2567\u20132586 (2014).",
            "year": 2014
        },
        {
            "authors": [
                "Danijela Markovi\u0107",
                "Julie Grollier"
            ],
            "title": "Quantum neuromorphic computing",
            "venue": "Applied Physics Letters 117, 150501 (2020).",
            "year": 2020
        },
        {
            "authors": [
                "Marcello Benedetti",
                "Erika Lloyd",
                "Stefan Sack",
                "Mattia Fiorentini"
            ],
            "title": "Parameterized quantum circuits as machine learning models",
            "venue": "Quantum Science and Technology 4, 043001 (2019).",
            "year": 2019
        },
        {
            "authors": [
                "Edward Farhi",
                "Hartmut Neven"
            ],
            "title": "Classification with Quantum Neural Networks on Near Term Processors",
            "venue": "(2018), arXiv:1802.06002.",
            "year": 2018
        },
        {
            "authors": [
                "Daniel K. Park",
                "Carsten Blank",
                "Francesco Petruccione"
            ],
            "title": "The theory of the quantum kernel-based binary classifier",
            "venue": "Physics Letters A 384, 126422 (2020).",
            "year": 2020
        },
        {
            "authors": [
                "Maria Schuld",
                "Nathan Killoran"
            ],
            "title": "Quantum Machine Learning in Feature Hilbert Spaces",
            "venue": "Physical Review Letters 122, 040504 (2019).",
            "year": 2019
        },
        {
            "authors": [
                "Seth Lloyd",
                "Maria Schuld",
                "Aroosa Ijaz",
                "Josh Izaac",
                "Nathan Killoran"
            ],
            "title": "Quantum embeddings for machine learning",
            "venue": "(2020), 10.48550/arXiv.2001.03622.",
            "year": 2020
        },
        {
            "authors": [
                "Thomas Hubregtsen",
                "David Wierichs",
                "Elies Gil-Fuster",
                "Peter-Jan H.S. Derks",
                "Paul K. Faehrmann",
                "Johannes Jakob Meyer"
            ],
            "title": "Training Quantum Embedding Kernels on Near-Term Quantum Computers",
            "venue": "(2021), arXiv:2105.02276.",
            "year": 2021
        },
        {
            "authors": [
                "Maria Schuld"
            ],
            "title": "Supervised quantum machine learning models are kernel methods",
            "venue": "(2021), arXiv:2101.11020.",
            "year": 2021
        },
        {
            "authors": [
                "Takeru Kusumoto",
                "Kosuke Mitarai",
                "Keisuke Fujii",
                "Masahiro Kitagawa",
                "Makoto Negoro"
            ],
            "title": "Experimental quantum kernel trick with nuclear spins in a solid",
            "venue": "npj Quantum Information 7, 1\u20137 (2021).",
            "year": 2021
        },
        {
            "authors": [
                "Yunchao Liu",
                "Srinivasan Arunachalam",
                "Kristan Temme"
            ],
            "title": "A rigorous and robust quantum speed-up in supervised machine learning",
            "venue": "Nature Physics 17, 1013\u2013 1017 (2021).",
            "year": 2021
        },
        {
            "authors": [
                "Yusen Wu",
                "Bujiao Wu",
                "Jingbo Wang",
                "Xiao Yuan"
            ],
            "title": "Provable Advantage in Quantum Phase Learning via Quantum Kernel Alphatron",
            "venue": "(2021), arXiv:2111.07553.",
            "year": 2021
        },
        {
            "authors": [
                "Jonas M. K\u00fcbler",
                "Simon Buchholz",
                "Bernhard Sch\u00f6lkopf"
            ],
            "title": "The Inductive Bias of Quantum Kernels",
            "venue": "(2021), 10.48550/arXiv.2106.03747.",
            "year": 2021
        },
        {
            "authors": [
                "Ruslan Shaydulin",
                "Stefan M. Wild"
            ],
            "title": "Importance of Kernel Bandwidth in Quantum Machine Learning",
            "venue": "(2021), arXiv:2111.05451.",
            "year": 2021
        },
        {
            "authors": [
                "Xinbiao Wang",
                "Yuxuan Du",
                "Yong Luo",
                "Dacheng Tao"
            ],
            "title": "Towards understanding the power of quantum kernels in the NISQ era",
            "venue": "Quantum 5, 531 (2021).",
            "year": 2021
        },
        {
            "authors": [
                "Karol Bartkiewicz",
                "Clemens Gneiting",
                "Anton\u00edn \u010cernoch",
                "Kate\u0159ina Jir\u00e1kov\u00e1",
                "Karel Lemr",
                "Franco Nori"
            ],
            "title": "Experimental kernel-based quantum machine learning in finite feature space",
            "venue": "Scientific Reports 10, 12356 (2020).",
            "year": 2020
        },
        {
            "authors": [
                "John Preskill"
            ],
            "title": "Quantum Computing in the NISQ era and beyond",
            "venue": "Quantum 2, 79 (2018).",
            "year": 2018
        },
        {
            "authors": [
                "Keisuke Fujii",
                "Kohei Nakajima"
            ],
            "title": "Harnessing Disordered-Ensemble Quantum Dynamics for Machine Learning",
            "venue": "Physical Review Applied 8, 024030 (2017).",
            "year": 2017
        },
        {
            "authors": [
                "Huawen Xu",
                "Tanjung Krisnanda",
                "Wouter Verstraelen",
                "Timothy C.H. Liew",
                "Sanjib Ghosh"
            ],
            "title": "Superpolynomial quantum enhancement in polaritonic neuromorphic computing",
            "venue": "Physical Review B 103, 195302 (2021).",
            "year": 1953
        },
        {
            "authors": [
                "J.A.K. Suykens",
                "J. Vandewalle"
            ],
            "title": "Least Squares Support Vector Machine Classifiers",
            "venue": "Neural Processing Letters 9, 293\u2013300 (1999).",
            "year": 1999
        },
        {
            "authors": [
                "Trevor Hastie",
                "Robert Tibshirani",
                "Jerome Friedman"
            ],
            "title": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction (Springer",
            "venue": "Science & Business Media,",
            "year": 2013
        },
        {
            "authors": [
                "Sofiene Jerbi",
                "Lukas J. Fiderer",
                "Hendrik Poulsen Nautrup",
                "Jonas M. K\u00fcbler",
                "Hans J. Briegel",
                "Vedran Dunjko"
            ],
            "title": "Quantum machine learning beyond kernel methods",
            "venue": "(2021), arXiv:2110.13162.",
            "year": 2021
        },
        {
            "authors": [
                "Vern I. Paulsen",
                "Mrinal Raghupathi"
            ],
            "title": "An Introduction to the Theory of Reproducing Kernel Hilbert Spaces, Cambridge Studies in Advanced Mathematics",
            "year": 2016
        },
        {
            "authors": [
                "Maria Schuld",
                "Ryan Sweke",
                "Johannes Jakob Meyer"
            ],
            "title": "Effect of data encoding on the expressive power of variational quantum-machine-learning models",
            "venue": "Physical Review A 103, 032430 (2021).",
            "year": 2021
        },
        {
            "authors": [
                "Blake Bordelon",
                "Abdulkadir Canatar",
                "Cengiz Pehlevan"
            ],
            "title": "Spectrum Dependent Learning Curves in Kernel Regression and Wide Neural Networks",
            "venue": "(2021), arXiv:2002.02561.",
            "year": 2021
        },
        {
            "authors": [
                "Abdulkadir Canatar",
                "Blake Bordelon",
                "Cengiz Pehlevan"
            ],
            "title": "Spectral bias and task-model alignment explain generalization in kernel regression and infinitely wide neural networks",
            "venue": "Nature Communications 12, 2914 (2021).",
            "year": 2021
        },
        {
            "authors": [
                "Amira Abbas",
                "David Sutter",
                "Christa Zoufal",
                "Aurelien Lucchi",
                "Alessio Figalli",
                "Stefan Woerner"
            ],
            "title": "The power of quantum neural networks",
            "venue": "Nature Computational Science 1, 403\u2013409 (2021).",
            "year": 2021
        },
        {
            "authors": [
                "Trevor Hastie",
                "Ji Zhu"
            ],
            "title": "Comment: [support vector machines with applications",
            "venue": "Statistical Science 21, 352\u2013 357 (2006).",
            "year": 2006
        },
        {
            "authors": [
                "Hsin-Yuan Huang",
                "Michael Broughton",
                "Masoud Mohseni",
                "Ryan Babbush",
                "Sergio Boixo",
                "Hartmut Neven",
                "Jarrod R. McClean"
            ],
            "title": "Power of data in quantum machine learning",
            "venue": "Nature Communications 12, 2631 (2021).",
            "year": 2021
        },
        {
            "authors": [
                "Nello Cristianini",
                "Jaz Kandola",
                "Andre Elisseeff",
                "John Shawe-Taylor"
            ],
            "title": "On Kernel Target Alignment",
            "venue": "Innovations in Machine Learning: Theory and Applications, Studies in Fuzziness and Soft Computing, edited by Dawn E. Holmes and Lakhmi C. Jain (Springer, Berlin, Heidelberg, 2006) pp. 205\u2013256.",
            "year": 2006
        },
        {
            "authors": [
                "Leonardo Banchi",
                "Jason Pereira",
                "Stefano Pirandola"
            ],
            "title": "Generalization in Quantum Machine Learning: A Quantum Information Standpoint",
            "venue": "PRX Quantum 2, 040321 (2021).",
            "year": 2021
        },
        {
            "authors": [
                "Artur Czerwinski"
            ],
            "title": "Quantum state tomography with informationally complete POVMs generated in the time domain",
            "venue": "Quantum Information Processing 20, 105 (2021).",
            "year": 2021
        },
        {
            "authors": [
                "Antonio Di Lorenzo"
            ],
            "title": "Sequential Measurement of Conjugate Variables as an Alternative Quantum State Tomography",
            "venue": "Physical Review Letters 110, 010404 (2013).",
            "year": 2013
        },
        {
            "authors": [
                "Christopher Williams",
                "Matthias Seeger"
            ],
            "title": "Using the nystr\u00f6m method to speed up kernel machines",
            "venue": "Advances in Neural Information Processing Systems 13 (MIT Press, 2001) pp. 682\u2013688.",
            "year": 2001
        }
    ],
    "sections": [
        {
            "text": "Noisy Quantum Kernel Machines\nValentin Heyraud,1 Zejian Li,1 Zakari Denis,1 Alexandre Le Boit\u00e9,1 and Cristiano Ciuti1 1Universit\u00e9 Paris Cit\u00e9, CNRS, Laboratoire Mat\u00e9riaux et Ph\u00e9nom\u00e8nes Quantiques (MPQ), F-75013 Paris, France\n(Dated: April 27, 2022)\nIn the noisy intermediate-scale quantum era, an important goal is the conception of implementable algorithms that exploit the rich dynamics of quantum systems and the high dimensionality of the underlying Hilbert spaces to perform tasks while prescinding from noise-proof physical systems. An emerging class of quantum learning machines is that based on the paradigm of quantum kernels. Here, we study how dissipation and decoherence affect their performance. We address this issue by investigating the expressivity and the generalization capacity of these models within the framework of kernel theory. We introduce and study the effective kernel rank, a figure of merit that quantifies the number of independent features a noisy quantum kernel is able to extract from input data. Moreover, we derive an upper bound on the generalization error of the model that involves the average purity of the encoded states. Thereby we show that decoherence and dissipation can be seen as an implicit regularization for the quantum kernel machines. As an illustrative example, we report exact finite-size simulations of machines based on chains of driven-dissipative quantum spins to perform a classification task, where the input data are encoded into the driving fields and the quantum physical system is fixed. We determine how the performance of noisy kernel machines scales with the number of nodes (chain sites) as a function of decoherence and examine the effect of imperfect measurements.\nI. INTRODUCTION\nIn recent years, machine learning has blossomed in a wide variety of fields and delivered a large number of applications driven by the achievements of the everdeveloping field of artificial neural networks, particularly those presenting deep architectures [1, 2]. Neural networks build predictions upon processing the input data of interest through a series of parametrized nonlinear transformations, whose (typically numerous) parameters are determined by training. This optimization procedure most often consists in minimizing a task-dependent loss function that quantifies the error of the parametrized model over a training dataset. This is most often implemented via software executed on standard computers. As a result, the growing demand for computational resources and energy for training such deep architectures on ever-increasing amounts of data makes its long-term sustainability uncertain [3]. In this context, devolving computationally demanding tasks to machine-learning devices with suitable physical systems acting as hardware is emerging as a relevant alternative. However, while the neural-network sequential architecture is well suited for software implementations on standard computers, the great number of parameters to be tuned during training remains in practice an obstacle to physical implementations. A simpler alternative approach is provided by the category of \u201cshallow models\u201d, such as reservoircomputing [4] or extreme learning machines [5], which have led to physical proposals [6, 7] and experimental realizations [8, 9]. In such machines, the input data are encoded in the dynamics of a physical system and the associated predictions are obtained by considering a linear combination of measured observables, weighted by a set of trainable parameters to be optimized by training. Importantly, this is done while keeping the parame-\nters of the physical system fixed, hence requiring hardly any degree of control over the system. Kernel machines, whose trial functions can be represented in terms of positive semi-definite and symmetric kernel functions [10], belong to this category. More generally, kernel theory has proved to be a very useful tool to understand a wide range of machine-learning algorithms. Recently, a close connection between kernel machines and deep neural networks in the infinite width limit has been established [11], further extending the relevance of these methods.\nIn parallel to the advent of quantum information, the last decade has also witnessed a growing interest in the emerging field of quantum machine learning [12, 13], a research domain that explores the potential advantages of quantum systems for machine-learning applications. Due to the success of deep neural-network algorithms, a large amount of work in this field has been devoted to finding quantum analogs to neural-network models [14], and more generally to find brain-inspired algorithms to be implemented on quantum devices [15]. Parametrized quantum circuits used as trainable ans\u00e4tze [16] appeared as natural candidates for such a generalization. These models, often called quantum neural networks [17], are among the most studied quantum machine-learning models and significant progress has been achieved in the comprehension of their properties. Analogously to classical systems, quantum \u201cshallow\" machines have also been put forward, such as those based on quantum reservoir-computing, extreme learning and quantum kernels [18\u201329], where the physical system (the network) is fixed and the optimization concerns only a linear map acting on measured outcomes.\nMost often, quantum machine-learning investigations have been focusing on isolated quantum systems with unitary dynamics. At present, however, we are in the socalled noisy intermediate-scale quantum era [30]: most\nar X\niv :2\n20 4.\n12 19\n2v 1\n[ qu\nan t-\nph ]\n2 6\nA pr\n2 02\n2\n2 \u03c1\u03020 M(x) \u03c1\u0302(x) = M(x)[\u03c1\u03020] \u03d5(x) = ( 1,Tr[\u03c1\u0302(x)O\u03021], \u00b7 \u00b7 \u00b7 ,Tr[\u03c1\u0302(x)O\u0302P ] )T\nx \u03d5(x) \u03d5\nfeature map fw : x 7\u2192 wT\u03d5(x)\nw\nlinear weights\nDynamical map Initial state\nEvolved density matrix\nMeasured feature vector\nInput space X Feature space F Trial function to train\nFIG. 1. Scheme of a noisy quantum kernel machine. An element x of the input space X is encoded into a density matrix \u03c1\u0302(x) obtained by evolving in time a fixed initial state described by the density matrix \u03c1\u03020 [see Fig. 2 for a specific example of the encoding process described by the evolution map M(x)]. The measured features are represented by a vector of observables \u03c6(x) (with an added 1 corresponding to the unity operator as first element to create an offset term) that belongs to the feature space F . The trial function is obtained by applying a linear transformation to the feature vector (depending nonlinearly on x) with a vector of weights w that is optimized via the training procedure described in the main text.\nquantum devices within practical reach are subject to a significant degree of dissipation and/or decoherence. An important problem is therefore to understand the impact of realistic noise on quantum machine-learning settings. The literature on the subject is yet in its very infancy. For time-dependent tasks, one study on quantum reservoir computing suggested that dissipation increases the processing capacity and the non-linearity of the embedding, at the price of a reduced memory capacity of the system [31]. An advantageous scaling of the performance of a quantum reservoir-computing scheme, as compared to its classical counterpart, was recently reported [32]. However, a systematic study of the dissipation and decoherence on quantum machine-learning models is missing. In particular, to the best of our knowledge, no investigation has explored its role on the important class of quantum kernel machines.\nIn this article, we investigate the use of open quantum systems as noisy quantum kernel machines. Within the formalism of kernel theory, we show how the expressive power and generalization capacity of the corresponding nonlinear feature maps are controlled by both the dissipation and decoherence affecting the system as well as the level of experimental uncertainty on the physical measurements. We introduce and study the effective kernel rank to quantify the effective number of independent features a noisy quantum kernel is able to extract from the input data. Moreover, we derive an upper bound on the generalization error of the model that involves the average purity of the encoded states. As an illustrative example, we simulate noisy quantum kernel machines implemented via driven-dissipative chains of spins. We provide a comprehensive study of the performance of noisy quantum kernel machines, showing how they scale with\nthe number of network nodes (chain sites) and the degree of dissipation and decoherence.\nThe paper is organized as follows. In section II, we describe the general scheme for encoding the input data into the quantum system dynamics and decoding the output through measurements. In section III, we analyze the noisy quantum kernel machine within the kerneltheory framework. In particular we study the link between the kernel spectrum and important properties of machine-learning models, such as the expressive power and the generalization capacity. We introduce and study the effective kernel rank. Within a statistical-learning approach, we provide an upper-bound on the generalization error for noisy quantum kernels. In section IV, we describe a class of noisy quantum kernel machines based on driven-dissipative chains of spins. We report a comprehensive study of the dependence of the performance metrics on the system size and noise for this class of models in section V. Finally, conclusions and perspectives are drawn in section VI. The most technical details are reported in Appendices A, B and C."
        },
        {
            "heading": "II. GENERAL SCHEME",
            "text": "The objective of supervised learning is to approximate a causal relation between elements x of an input set X and some target quantities y \u2208 Y, based upon a set of known training examples S = {(xi, yi) | i = 1, . . . , Ntrain}. The input features are considered as independent realizations of a random variable following a probability distribution p(x) on X . In the following, we denote Ep [f(x)] the expectation value of a quantity f(x) over the distribution p [33]. We also define the corre-\n3 sponding centered quantity as\n\u03b4f(x) = f(x)\u2212 Ep [f ] . (1)\nUpon assuming the inputs and target quantities are related according to an unknown ground-truth function yi = y(xi), we aim to approximate it using a trial function f parametrized by w, to be optimized using the training set S. The specific form of f depends on the considered model architecture. In this paper, we describe noisy quantum kernel machines exploiting the dynamics of open quantum systems to generate such a trial function. This scheme is summarized pictorially in Fig. 1."
        },
        {
            "heading": "A. Encoding on the quantum system",
            "text": "Let us consider a system initially prepared in a state \u03c1\u03020. For each element of the input space, represented by a vector x \u2208 X , a procedure can be defined to encode it into the non-unitary dynamics of a generic open quantum system. As will be shown in Sec. IV, this can be achieved, for instance, by encoding the input vector in a proper modulation of the driving fields acting on the system.\nWe consider the dynamics of the open quantum system to be described by a Lindblad master equation [34] of the form:\n\u2202\u03c1\u0302 \u2202t = \u2212 i ~ [H\u0302, \u03c1\u0302] +\nN\u2211\nj=1\n\u03b3jD(A\u0302j)[\u03c1\u0302], (2)\nwhere \u03b3j is the relaxation rate at site i and the dissipator D(A\u0302) denotes the superoperator\nD(A\u0302) [\u03c1\u0302] = A\u0302\u2020\u03c1\u0302A\u0302\u2212 1 2 {A\u0302\u2020A\u0302, \u03c1\u0302}. (3)\nNote that the Lindblad operator A\u0302j depends on the considered system-bath interaction. The master equation describes the evolution from an initial density matrix into a final density matrix:\n\u03c1\u0302(x, t) =M(x, t)[\u03c1\u03020], (4)\nwhere the completely-positive trace-preserving map M(x, t) is the propagator of the Lindblad master equation capturing the non-unitary evolution of \u03c1\u03020. It depends on x via the encoding procedure: If the input is encoded in driving fields, as we will consider later, the Hamiltonian, and consequently the density matrix at any time, bears a dependence on the input. In principle, one could also encode the input into a modulation of the loss rates, although we will not treat this case here. In what follows, when considering a fixed final time tf for the time-evolution, we denoteM(x) =M(x, tf ) and \u03c1\u0302(x) = \u03c1\u0302(x, tf ) to simplify the notation."
        },
        {
            "heading": "B. Decoding through measurements",
            "text": "At time tf , after the encoding procedure, we extract the processed information by performing a set of measurements of the system. Given the density matrix \u03c1\u0302(x) and a set of system observables O = {O\u0302j | j = 1, . . . , P}, information about the response of the open quantum system to the input x is contained in the following vector\n\u03c6(x) \u2261 ( 1, \u3008O\u03021\u3009x, . . . , \u3008O\u0302P \u3009x )T , (5)\nwhere\n\u3008O\u0302j\u3009x = Tr[O\u0302j \u03c1\u0302(x)]. (6)\nThe vector \u03c6(x) belongs to the feature space F \u2286 RP and depends on the input x, generally in a nonlinear fashion. Note that the constant component 1, which ensures that the trial function can fit a biased target function, can be seen as the measurement of the identity observable, since the density matrix \u03c1\u0302(x) always has unit trace.\nFinally, the trial function f of the noisy quantum kernel machine, which depends on the vector of variational parameters w, is given by the affine transformation:\nf : x 7\u2192 wT\u03c6(x), (7)\nwhere the vector w \u2261 (b, w1, . . . , wP )T \u2208 RP+1 contains the parameters of the linear transformation and b represents the bias term. An alternative approach to the construction of the feature vector, based on timemultiplexing measurements, will be presented in Sec. IV."
        },
        {
            "heading": "C. Training procedure",
            "text": "A trial function characterized by its weights w can be optimized using a regularized least-squares loss function over a training set (xi, yi) \u2208 S consisting of Ntrain inputs xi \u2208 X and labels yi \u2208 Y, namely:\nL (w | S) := 1 2Ntrain\nNtrain\u2211\ni=1\n( yi \u2212wT\u03c6(xi) )2 + \u03bb\n2 \u2016w\u201622 .\n(8) The second term in Eq. (8) is a regularization penalty that helps to prevent overfitting. The corresponding regularization parameter \u03bb controls the strength of the overfitting penalty. Adding such a regularization bias is on average equivalent to adding a centered Gaussian noise of variance \u03bb to the measurement features before the optimization [2].\nSuch classifiers are known as least-square supportvector classifiers [35]. Although most classification problems are commonly treated with other loss functions [36], using the least-squares loss function allows us to perform the optimization analytically. Indeed, upon introducing the (P + 1) \u00d7 Ntrain matrix \u03a6, whose columns are the\n4 quantum feature vectors \u03c6(xi) associated to the training input xi, and y, the column vector of size Ntrain containing the corresponding labels, the optimal weights are given by [37]\nw\u2217 = ( \u03a6\u03a6T +Ntrain\u03bb1 )\u22121 \u03a6y . (9)"
        },
        {
            "heading": "III. QUANTUM KERNEL AND DECOHERENCE",
            "text": "The generic encoding-decoding scheme encompasses a large class of quantum machine-learning models. Here, we describe a decoding based on a linear combination of measurements, but other decoding methods were proposed in the literature. In particular, it was recently shown that quantum neural networks can be mapped to models with an encoding/decoding structure [38], where the decoding is achieved by optimizing a single parametrized measurement.\nModels described by the previous scheme can be analyzed in the framework of kernel theory, which provides useful tools to understand properties such as expressivity, trainability and capacity to generalize to a test sample of unseen data. In this section we first concisely introduce the kernel framework. We then specialize our discussion to noisy quantum kernels, and show how we can link the role of dissipation and decoherence to the kernel\u2019s main figures of merit.\nWe aim at determining the largest class of functions that can be approximated by our trial function f . This class depends on the type of decoding used, that is on the specific set of measurements that are performed on the quantum system. When measuring a set O of observables, this function space reads\nH(O) = {f : x 7\u2192 Tr[\u03c1\u0302(x)A\u0302] | A\u0302 \u2208 Span(O)} . (10)\nIn this case, the feature vector \u03c6(x) gives rise to a positive semi-definite and symmetric function which we call the feature kernel:\nkO(x,x \u2032) = \u03c6(x)T\u03c6(x\u2032). (11)\nThis kernel function, together with the probability distribution p of inputs x \u2208 X [39], uniquely determines a specific set of real-valued functions, the so-called reproducing kernel Hilbert space (RKHS):\nSpan{f : x 7\u2192 kO(x,x\u2032) | x\u2032 \u2208 X}. (12)\nThe RKHS associated to kO can be shown to be exactly the space of hypothesis functions H(O) [40]. Hence, the study of the kernel function allows one to investigate the structure of H(O). In particular, it follows that one can use the eigendecomposition of the kernel function as a basis of the class of functions that can be represented by our model. This useful property motivates the adoption of a kernel standpoint in what follows."
        },
        {
            "heading": "A. Quantum kernel",
            "text": "In order to discuss the expressive power of our model, we introduce the largest class of transformations Hfull that can be achieved for a given encoding strategy [41]:\nHfull = {f : x 7\u2192 Tr[\u03c1\u0302(x)A\u0302] | A\u0302 = A\u0302\u2020} . (13)\nThe class of transformation yielded by a set of measurements O is necessarily included in this maximal class H(O) \u2286 Hfull; the equality holds whenever O is a complete set of observables. In the following, we will use the term \u201cfull tomography\" to refer to this ideal implementation. It turns out that Hfull is the RKHS of a particular kernel, the quantum kernel, that solely depends on the feature map \u03c1\u0302(x) [42] [22]:\nk(x,x\u2032) = Tr [\u03c1\u0302(x)\u03c1\u0302(x\u2032)] . (14)\nThis kernel arises naturally from the Hilbertian structure of the space of quantum states. As it represents the maximal achievable class of transformation an encoding can give, the quantum kernel provides insight on the expressive power of our model. Note that this kernel can be identified with the previous feature kernel kO provided that the measurements O form an orthonormal basis B = {Bj}j of the space of observables, i.e. k = kB with Tr[B\u0302iB\u0302j ] = \u03b4ij and we impose B\u03020 \u221d 1\u0302 by convention.\nIn what follows, it will be useful to work with a \u201ccentered\" version of the quantum kernel. Centering the kernel is equivalent to working with hypothesis functions that have zero mean value on the input set. As we will show, this is convenient for interpreting some of the key quantities we will introduce in terms of probabilistic quantities. In Appendix B, we show that, at least for balanced data, the use of the L2 loss function allows us to work with a centered version of the quantum kernel without lack of generality. The centered kernel is given by\nkc(x,x \u2032) = Tr [\u03b4\u03c1\u0302(x)\u03b4\u03c1\u0302(x\u2032)] (15)\nand the corresponding RKHS is\nHk,c = Span{f : x 7\u2192 kc(x,x\u2032) | x\u2032 \u2208 X}. (16)\nThe constant feature we introduced in Eq. (5) becomes irrelevant when using centered quantities, so we drop it and define\n\u03b4\u03c6(x) \u2261 ( \u03b4\u3008O\u03021\u3009x, . . . , \u03b4\u3008O\u0302P \u3009x )T . (17)\nWe can also correspondingly drop the weight term b, so that the weight vectors can be redefined as w = (w1, . . . , wP ) T \u2208 RP . The space Hk,c can be rewritten as\nHk,c = {f : x 7\u2192 wT \u03b4\u03c6(x), w \u2208 RP }, (18)\n5 where the centered quantum kernel reads\nkc(x,x \u2032) = \u03b4\u03c6(x)T \u03b4\u03c6(x\u2032) (19)\nwith the choice of O = B. The quantum feature matrix \u03a6 is then replaced by a P \u00d7 Ntrain matrix \u03b4\u03a6, whose columns are the centered feature vectors \u03b4\u03c6(xi)."
        },
        {
            "heading": "B. Kernel eigen-decomposition",
            "text": "Under general assumptions, the centered quantum kernel admits a decomposition into an orthonormal family of eigenfunctions [40]:\nkc(x,x \u2032) =\n\u2211\ni\n\u03bbi\u03b4\u03c8i(x)\u03b4\u03c8i(x \u2032) ,\nEp [\u03b4\u03c8i\u03b4\u03c8j ] = \u03b4ij , (20)\nwhere {\u03bbi}i are positive eigenvalues sorted in a decreasing order, namely \u03bbi+1 \u2264 \u03bbi, \u2200i. When necessary, we can complete this orthonormal family into a basis with eigenfunctions associated to zero eigenvalues. In the case of the uncentered quantum kernel, the kernel eigenfunctions correspond to an orthonormal basis of system observables [26]. When the kernel is centered, the basis of kernel eigenfunctions corresponds to an orthonormal basis {E\u0302i}i of the space of zero-trace observables, which we call eigenobservables. Such operators satisfy the following properties:\nTr [ E\u0302iE\u0302j ] = \u03b4ij ,\nTr [ E\u0302i ] = 0 .\n(21)\nThe eigenfunctions are given by\n\u03b4\u03c8i(x) = 1\u221a \u03bbi\nTr [ \u03b4\u03c1\u0302(x)E\u0302i ]\n= 1\u221a \u03bbi \u03b4\u3008E\u0302i\u3009x .\n(22)\nThe corresponding eigenvalues are then given by the variances of the eigenobservable measurements over the input set, namely:\n\u03bbi = Ep [ \u03b4\u3008E\u0302i\u30092x ] = Varp [ \u3008E\u0302i\u3009x ] . (23)\nOne can see this eigen-decomposition of the kernel as a principal-component analysis in the space of quantum features, as it yields an orthogonal basis of measurement functions ordered by their variances on the input set. We stress that these are variances of the observables expectation values over the quantum states representing the different inputs, and thus are very different from the quantum variance of the corresponding observable for a specific state.\nThe previous decomposition of the kernel is very useful for grasping the learning mechanism and the model\nexpressivity. Upon working with centered features, the loss function introduced in Eq. (8) becomes\nLc (w | S) = 1\n2Ntrain\nNtrain\u2211\ni=1\n( yi \u2212wT \u03b4\u03c6(xi) )2 + \u03bb\n2 \u2016w\u201622 .\n(24) Following [36], we can decompose the trial function f(x) = wT \u03b4\u03c6(x) in the basis of the kernel eigenfunctions, namely as f(x) = \u2211 j \u03b2j\u03b4\u03c8j(x). Exploiting such decomposition, the loss function becomes\nLc (\u03b2 | S) = 1\n2Ntrain\nNtrain\u2211\ni=1\n[ yi \u2212 \u2211\nj\n\u03b2j\u03b4\u03c8j(xi) ]2\n+ \u03bb\n2\n\u2211\nj\n\u03b22j \u03bbj .\n(25)\nNote that in the regularization term the components of the trial function on the eigenbasis are weighted by the corresponding kernel eigenvalues. The lower the variance of an eigenobservable, the more the corresponding eigenfunction is penalized. Hence the regularization parameter \u03bb acts as a smooth cutoff on the basis of the kernel eigenfunctions, which are then used to approximate the target function.\nThe spectrum of the kernel characterizes the generalization capacity and the expressivity of our model. It also finds applications in understanding many other machine learning scenarios. For instance, in the context of classical neural networks it has links with learning curves [43, 44]. Moreover, the kernel (or the neural tangent kernel in the context of classical neural networks) shares its spectrum with the Fisher information matrix, of particular relevance for quantum neural networks [45]."
        },
        {
            "heading": "C. Role of decoherence on expressivity and generalization error",
            "text": "The exponential growth of the Hilbert space dimension with the number of qubits in a network and the complex dynamics of quantum systems have created hope for a quantum advantage in the field of quantum machine learning. However, it is known that having a very highdimensional feature space does not necessarily guarantee high machine-learning performances [36, 46]. Indeed, recent investigations within the quantum kernel framework somehow mitigated the hope for a general quantum advantage [26, 28, 47]. Yet, a clear quantum advantage has been demonstrated for some specific tasks [24, 25], again by exploiting the quantum-kernel formalism. In order for a quantum-kernel-based model to perform well on a given task, the set of transformations achieved must be well \u201caligned\" with the target function y(x). This notion of alignment is mathematically encapsulated in the kernel-target-alignment measure [48] which reads, for the\n6 centered quantum kernel,\nA(kc, y) = Ep [y(x)kc(x,x\u2032)y(x\u2032)]\nEp [kc(x,x\u2032)2]1/2 Ep [y(x)2]\n=\n\u2211 i \u03bbiEp [\u03b4\u03c8i(x)y(x)] 2\n( \u2211 i \u03bb 2 i ) 1/2Ep [y(x)2] .\n(26)\nAlthough the kernel-target alignment measures how well a kernel and the associated embedding fits a specific function, in this article we introduce another figure of merit that does not depend on a specific task, namely the \u201ceffective kernel rank\" Reff(k), which quantifies the effective number of independent transformations that a given kernel can yield. Such a quantity is defined as:\n\u221a Reff(kc) = \u2211\nj\nA(kc, gj) , (27)\nwhere {gj}j is any orthonormal basis of functions on the input space. As shown in Appendix A 1, for the centered quantum kernel, the effective kernel rank can be also expressed in terms of variances of the quantum expectation values of the measured observables:\n\u221a Reff(kc) =\n\u2211P i=1 Varp [ \u3008O\u0302i\u3009x ]\n(\u2211P i,j=1 Covp [ \u3008O\u0302i\u3009x, \u3008O\u0302j\u3009x ]2) 12 . (28)\nNote that the denominator acts as a normalization and can be seen as a measure of the redundancy of the embedding when expressed in terms of O\u0302i. In section V, we will investigate in a rather general class of physical models how the kernel effective rank scales with the system size and with noise.\nIn Appendix A 1, we also provide the proof showing that the kernel effective rank can be expressed in terms of the kernel spectrum:\n\u221a Reff(k) = \u2211 i \u03bbi\u221a\u2211 i \u03bb 2 i . (29)\nThis expression is reminiscent of the reciprocal of the inverse participation ratio. The kernel effective rank provides information about the size of its support. Moreover, we have the following inequality:\nReff(k) \u2264 |{\u03bbi 6= 0}| . (30)\nThis is saturated when all the non-zero eigenvalues are all equal. The numerator in the expression for the squareroot of the effective kernel rank is the kernel trace, which can be rewritten as:\n\u2211\ni\n\u03bbi = Ep [ Tr [ \u03c1\u0302(x)2 ]] \u2212 Tr [ Ep [\u03c1\u0302(x)]2 ] . (31)\nIn this expression, we recognize the difference between the average purity of the embedded density matrices over\nthe input space and the purity of the average embedding matrix. The first term is of great relevance to our study, as it crucially depends on the dissipation and decoherence affecting the noisy quantum system: indeed, a low purity is the consequence of the openness of the quantum system. The second term instead measures the diversity of the embedding map; its importance is discussed in [26].\nWe emphasize that the kernel trace also appears to be relevant when investigating the ability of the model to perform well on unseen data, hence on its generalization properties. To measure the performance of a model on a binary classification task we use the accuracy A. Given a prediction function f , the accuracy is given by the fraction of samples for which f assigns the right label and it can be defined as the expectation of a 0-1 loss function:\nA(f) = E [ 1y(x)f(x)\u22650 ] . (32)\nSince during the training we only have access to the data set S and not to the true distribution p, expectations values can only be approximated using the empirical distribution p\u0302 on S. The corresponding empirical expectations are given by Ep\u0302 [f(x)] = 1Ntrain \u2211Ntrain i=1 f(xi). From this, we can define the empirical accuracy A on the training set S and the true accuracy A\u2217. Correspondingly, we can introduce the risk R\u2217, also called error or inaccuracy, as R\u2217 = 1\u2212A\u2217 (its empirical counterpart is defined analogously). It is convenient to introduce slightly modified versions of the risk and inaccuracy that depend on a margin-parameter \u03b7 > 0. We introduce the \u03b7-margin loss as:\n\u03a6\u03b7(y) =    1 if y \u2264 0 1\u2212 y\u03b7 if 0 \u2264 y \u2264 \u03b7 0 if \u03b7 \u2264 y . (33)\nCorrespondingly, we can introduce the empirical \u03b7margin risk as:\nR\u03b7(f) = Ep\u0302 [\u03a6\u03b7(y(x)f(x))] . (34) The \u03b7-margin-risk and the risk satisfy the following inequality:\nR(f) \u2264 R\u03b7(f) \u2264 Ep\u0302 [ 1y(x)f(x)\u2264\u03b7 ] . (35)\nThe ability of the model to generalize well on unseen data is then quantified by the generalization error:\nE = R\u2217 \u2212R . (36) For kernel methods with kernel k, the generalization error admits an upper-bound involving the Ntrain \u00d7Ntrain empirical kernel matrix K whose entries are defined as Kij = k(xi,xj). This bound depends on the specific task under consideration and on the exact space of trial functions used (details on the bound used and its derivation can be found in [49] and in Appendix A 2).\nTo derive the upper-bound, we fix a class of trial functions of the form f : x 7\u2192 wT \u03b4\u03c6(x) where \u03b4\u03c6(x) corresponds to measurements of an orthonormal basis of observable: \u03b4\u03c6i(x) = \u03b4\u3008B\u0302i\u3009x. We further constrain this\n7 class by choosing a parameter \u039b \u2265 0, and require that the trial function\u2019s parameters w satisfy \u2016w\u20162\u039b \u2264 1. By exploiting Eq. (31), we get that, for such functions, the following inequality holds with probability at least 1\u2212 \u03b4 on the training set S:\nR\u2217(f)\u2212R\u03b7(f) \u2264 2\n\u03b7\n  Ep\u0302 [ Tr [ \u03c1\u03022 ]] \u2212 Tr [ Ep\u0302 [\u03c1\u0302]2 ]\nNtrain\u039b\n  1 2\n+ 3\n\u221a log( 2\u03b4 )\n2Ntrain .\n(37) Other generalization bounds can be established, in particular the authors of [50] found another bound using a quantum information theory standpoint, and their conclusions are in agreement with our results. Let us make a few important comments on the meaning of this inequality. The inequality has a probabilistic character controlled by \u03b4 > 0. If we set this parameter to 0+, the bound is always satisfied although it becomes trivial. The same goes with the margin parameter \u03b7: as \u03b7 \u2192 0+ the margin-error R\u03b7(f) tends to the training error R(f), but again the right-hand side of the inequality diverges. The parameter \u039b is another sort of regularization parameter, as the parameter \u03bb: if \u039b \u2192 0+, the norm of the weight vector \u2016w\u2016 can be arbitrarily large and overfitting is not limited. Correspondingly, the right-hand side diverges and the bound becomes trivial. The most important crucial physical quantity involved in the upper bound is the kernel trace given in Eq. (31). Such a quantity accounts for the model expressivity. This duality between expressivity and generalization is crucial in machine learning [36]. What is relevant to our study is that this expressivity measure involves the mean purity of the embedded states and hence is affected by dissipation and decoherence acting on the noisy quantum kernel machine. The appearance of the regularization parameter \u039b in this upper bound is also relevant as it allows us to establish a link with experimental constraints, such as imperfect measurements. In fact, as we will see in Section V, adding a Gaussian error of standard deviation \u03c3 to the observable measurements is equivalent to working with an infinitely precise measurement apparatus while replacing the regularization parameter \u03bb with \u03bb+ \u03c32 [2]."
        },
        {
            "heading": "IV. NOISY QUANTUM KERNEL MACHINES WITH DRIVEN-DISSIPATIVE SPIN CHAINS",
            "text": "As an illustrative example, we here numerically simulate noisy quantum kernel machines based on 1D chains of spins subject to both driving and decoherence.\nThe simulation of such an open quantum system for a large number of inputs, various choices of the number of sites and distinct disorder realizations is a computationally daunting task [51]. Indeed, this requires to exactly integrate a large set of corresponding Lindblad master\nRaw image\nNp \u00d7Np\ndownsampled image\nN \u2032p \u00d7N \u2032p  \nx1 ... xi ... xM  \nrandom-projection features\n\u00d7W\n(N \u20322p \u00d7M)\ndriving pulses \u03be(t)\nti\nxi\nFj(t) = \u03b7j\u03be(t)\n\u03c1\u0302(x) encoded state\nFIG. 2. Schematic representation of the encoding procedure for the MNIST classification task. The input grayscale image, of original size Np\u00d7Np, with Np = 28, is first downsampled to a size of N \u2032p\u00d7N \u2032p (or N \u20322p \u00d71 when viewed as a column vector), with N \u2032p = 8, and linearly transformed by a fixed N \u20322p \u00d7M random projection filter W to yield the vectors x\u2032 containing M = 10 random-projection features. Those features are normalized by 3 times the standard deviation over the set of all features for all images in the training set, and we denote x the normalized vectors representing the images. The vector x is then encoded into a sequence of driving pulses \u03be(t), where the amplitude of the ith pulse (at time ti) is proportional to the input\u2019s ith component xi. Finally, the pulses are used to drive a spin chain (initially prepared in the state \u03c1\u03020), where the driving amplitude at site j is Fj(t) = \u03b7j\u03be(t) with \u03b7j a random site-dependent scale factor. We define the state of the spin chain immediately after the driving sequence to be the encoded state, represented by its density matrix \u03c1\u0302(x).\nM(x)\n\u03c1\u03020 \u03c1\u0302(x)G G G\nRX(\u03b71x1)\nRX(\u03b7ix1)\nRX(\u03b7Nx1)\nRX(\u03b71x2)\nRX(\u03b7ix2)\nRX(\u03b7Nx2)\nRX(\u03b71xM )\nRX(\u03b7ixM )\nRX(\u03b7NxM )\n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7\n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7\n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7\n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7\n\u00b7 \u00b7 \u00b7\n\u00b7 \u00b7 \u00b7\n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7\n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7\n\u00b7 \u00b7 \u00b7\n\u00b7 \u00b7 \u00b7\n\u00b7 \u00b7 \u00b7\nFIG. 3. Equivalent circuit of the encoding procedure for the MNIST classification task. If the driving pulses are sharp enough, the encoding process of Fig. 2 can be equivalently seen as a quantum circuit, where the i-th driving pulse on site j is effectively a single-qubit X-rotation gate RX(\u03b7jxi), and the pulses at different times are separated by the gate G generated by the free dynamics of the spin system in the absence of the drive. Note that the entire process between \u03c1\u03020 and \u03c1\u0302(x) serves as the dynamical mapM(x) shown in Fig. 1.\nequations of the form of Eq. (2). Hence, we have considered a simplified classification task involving only a subset of the MNIST dataset, namely classifying images of handwritten digits corresponding to the digits 3, 6 and 8, which share common shapes. A schematic description of the task and of the feature encoding through driving of the considered physical system is presented in Fig. 2.\nThe original MNIST dataset consists of 28 \u00d7 28-pixel\n8 0.0 0.1 0.2 0.3 0.4 0.5 N (\u03c1\u0302 ) (a) \u03b3/J = 0.01 \u03b3/J = 0.1 \u03b3/J = 1.0\n0 2 4 6 8 10\nJt\n0\n1\n2\n3\nS (\u03c1\u0302\n)\n(b)\nFIG. 4. Time dynamics of the average entanglement negativity N (\u03c1\u0302) (a) and the average von Neumann entropy S(\u03c1\u0302)(b) in presence of pure dephasing with different values of the corresponding rates \u03b3. At the initial time t = 0 the system is in the pure state \u03c1\u03020 defined in the text. Note that the driving sequence finishes at the time Jt ' 5 indicated by the vertical dotted lines on the figures. The time is expressed in units of 1/J where J is the average value of the spin coupling. We define N (\u03c1\u0302) as the average over all the sites of the negativities associated to the system partitions having the form {{site i}, {site j | j 6= i}}. This quantity is averaged over 20 inputs x \u2208 X , 5 disordered configurations of spin couplings and for a chain of N = 5 spins. The filled areas correspond to a one standard deviation confidence interval.\nimages. Encoding such high-dimensional features in the state of a quantum system is not an easy task. Therefore, we first linearly down-sample the raw images from 28 \u00d7 28 to 8 \u00d7 8 pixels, thereby reducing the dimension of the input features. The down-sampled images, viewed as vectors, are then multiplied by a random 82 \u00d7 10 matrix W, whose entries are uniformly drawn over the interval [\u22121, 1], yielding vectors x\u2032 = (x\u20321, . . . , x\u2032M )T of M = 10 random-projection features. These are finally normalized by 3 times the standard deviation of the set {x\u2032i | i = 1, . . . ,M, x \u2208 S}. At the end of this procedure, every image in the dataset is represented by a vector x of size M = 10, which will be used as inputs in the following. These are computed only once and reused throughout this article, except in section VB.\nThis encoding is designed so as to fix the amount of information fed to the system, independently from its number of sites. It allows us to perform a fair comparison of models associated to quantum systems of increasing sizes. In particular, this ensures that any observed increase of the performance with the system size is solely due to an intrinsic enhancement of the model expressive power. In Section VB, we lift the above-defined \u201cinformation bot-\ntleneck\u201d and use a different encoding, where the number of encoded features M scales with the system size N . Therein, we show that this results in competitive performances, as compared to classical reservoir-computing settings involving hundreds to thousands of degrees of freedom [6, 8].\nIn what follows, we denote X \u2286 RM the input space consisting of the random-projection features representing the images to classify, and Y = {3, 6, 8} the set of corresponding labels. Our dataset consists of 17,000 images, which we split into a training set of Ntrain = 15,000 images and a testing set of Ntest = 2000 images. As before, the training set is denoted as S = {(xi, yi) \u2208 X \u00d7Y | i = 1, . . . , Ntrain}.\nThe system in which we encode the previous features is a driven-dissipative one-dimensional chain ofN spins-1/2 described by the following Heisenberg XYZ Hamiltonian:\nH\u0302(t;x) = ~ 2\nN\u2211\ni=1\n( Fi(t;x)\u03c3\u0302 i x + \u2206i\u03c3\u0302 i z )\n\u2212~ 2\n\u2211 \u3008i,j\u3009 (Jxij \u03c3\u0302 i x\u03c3\u0302 j x + J y ij \u03c3\u0302 i y\u03c3\u0302 j y + J z ij \u03c3\u0302 i z\u03c3\u0302 j z),\n(38)\nwith Fi(t;x) an input-dependent driving field, \u2206i an onsite frequency detuning, and Jkij the symmetric coupling rate between nearest neighbors. Here, indices \u3008i, j\u3009 run over all pairs of nearest neighbors. Parameters Jkji and \u2206i are uniformly drawn at random in the interval [0, 2J ]. 1/J will be used as unit of time in the numerical plots. We prepare the system in an initial state with all spins down \u03c1\u03020 = \u2297N i=1 |0\u3009\u30080|.\nThe encoding of the input x corresponding to a given image into the system state is performed by driving the system with a series of M = 10 sharp Gaussian pulses, whose amplitudes are proportional to the input vector elements, as illustrated in Fig. 2. We first define a generic driving \u03be(t;x) from the feature x:\n\u03be(t;x) =\nN\u2211\nk=1\nxk\u221a 2\u03c0\u03c3 exp\n( \u2212 (t\u2212 tk) 2\n2\u03c32\n) ,\ntk = (k \u2212 1)\u2206t+ 10\u03c3, \u2200k = 1, . . . ,M , (39)\nwhere the time interval between two successive pulses is \u2206t = 1/(2J) and the width of each pulse is \u03c3 = 1/(50J). Then the driving on site i is taken to be proportional to this generic driving:\nFi(t;x) = \u03b7i\u03be(t;x), (40)\nwhere the \u03b7i are random factors uniformly distributed in the interval [\u2212\u03c0, \u03c0]. Under these driving conditions, the coherent part of the system dynamics can be thought of as that of an equivalent quantum circuit alternating between a set of local X-rotation gates, of the form RXi (\u03b7ixk), and a deep block generating entanglement among qubits [52], as illustrated in Fig. 3. The scaling factors \u03b7i prevent the spins from rotating all together.\n9 10\u221214 10\u221212 10\u221210 10\u22128 10\u22126 10\u22124 10\u22122 100\n\u03bb\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7 0.8 E rr o rs (a) R(training set) R\u2217(testing set)\n10\u221214 10\u221212 10\u221210 10\u22128 10\u22126 10\u22124 10\u22122 100\n\u03bb\n(b)\n2 3 4 5 6 7 8\nN\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nR \u2217 m\nin\n(c) \u03b3/J = 1.0\n\u03b3/J = 0.4\n\u03b3/J = 0.16\n\u03b3/J = 0.01\n2 3 4 5 6 7 8\nN\n(d)\nFIG. 5. (a) Training error R (dashed lines) and testing error R\u2217 (solid lines and markers) as a function of the regularization parameter \u03bb for a chain with N = 7 spins in the presence of pure dephasing for different values of the corresponding rate \u03b3 (different markers in the legend) in units of the average spin coupling J . Measurements are assumed to be ideal . (b) Same as (a) with an extra random Gaussian noise of width \u03c3 = 10\u22123 added to the observable expectation values to account for imperfect measurements. For each value of \u03bb the corresponding errors are averaged over 15 disordered configurations, and the error bars are bootstrap estimates of the standard deviation for the estimated mean values. We use 10 bootstrap sets, each consisting of 15 samples randomly drawn with replacement from the original set of 15 disorder realizations. (c) Minimal testing error as a function of the number of spins N for different values of the dephasing rate \u03b3. For each disorder configuration, the regularization parameter \u03bb is chosen to minimize the testing error and the resulting minimum is averaged over the disorder. The error bars are derived using the same bootstrap procedure. Number of disorder configurations: 50 for N = 2 to N = 5 spins, 25 for N = 6, 15 for N = 7 and 5 for N = 8. (d) Same as (c) with an extra random Gaussian noise of width \u03c3 = 10\u22123 added to the observable expectation values to account for imperfect measurements.\nThis procedure, where a random-projection feature is fed to the system every \u2206t, is in close analogy with the repeated-encoding prescription in variational quantum circuits, which is known to improve the expressivity of a model [41].\nShortly after the last pulse of the driving ends, at time \u03c4 = 30\u03c3 + M\u2206t, we get the final encoded state represented by the density matrix \u03c1\u0302(x). This encoding procedure acts as a non-linear map from the input space of images to the high-dimensional space of N -spin mixed quantum states.\nConcerning the non-unitary dynamics due to the openness of the quantum kernel machine, we will consider spin dephasing as the source of decoherence. Within the Lindblad master equation formalism [Eq. (2)], this process is described by the jump operators A\u0302j = \u03c3\u0302jz, and we consider a uniform dephasing rate for each site\n\u03b3i = \u03b3, \u2200i \u2208 {1, \u00b7 \u00b7 \u00b7 , N}. Note that while the considered illustrative task involves three classes, it can be reduced to a set of binary classification problems by changing the labels y \u2208 {3, 6, 8} into vector labels of the form (y1, y2, y3)T with yj \u2208 {\u22121, 1}3. For example an outcome (\u22120.3,\u22120.2, 0.9) would correspond to the digit 8. This \u201cOne-vs-Rest\" approach is equivalent to training three binary classifiers, one for each class, and takes the highest output among the three classifiers as a prediction. However, for the sake of simplicity, we will use binary classification notations in the following, and consider that the labels belong to {\u22121, 1}.\nRegarding the measurements of the system observables, we will consider two measurement protocols:\n(i) A full tomography of the output density matrix. In\n10\nthis case we consider that the measurements are made without delay after the end of the encoding, and the extracted features are exactly the components of the generalized Bloch vector \u03c6(x) by considering a complete set of observables.\n(ii) A time-multiplexing measurement protocol, where the output is obtained by sequential measurements at different times of a set of local observables."
        },
        {
            "heading": "A. Full tomography",
            "text": "Any Hermitian operator of the considered spin system can be decomposed on the orthogonal (for the HilbertSchmidt inner product) basis of Pauli strings. For a system of N spins, we write this basis {O\u0302i | i = 0, . . . , P}, with P = 4N \u2212 1. The corresponding observables are such that\nO\u0302i =\nN\u2297\nk=1\n\u03c3\u0302kik , ik \u2208 {0, 1, 2, 3};\nTr [ O\u0302\u2020i O\u0302j ] = 2N\u03b4ij , \u2200i, j ,\n(41)\nwith O\u03020 = 1\u0302, and thus any observable A\u0302 is decomposed in this basis through the expansion:\nA\u0302 = 1\n2N\n( Tr [ A\u0302 ] 1\u0302 + P\u2211\ni=1\nTr [ O\u0302iA\u0302 ] O\u0302i ) . (42)\nThe density matrix associated to the input x can also be decomposed into this basis:\n\u03c1\u0302(x) = 1\n2N\n( 1\u0302+ P\u2211\ni=1\n\u3008O\u0302i\u3009xO\u0302i ) , (43)\nand hence any density matrix is uniquely characterized by its associated generalized Bloch vector. For the fulltomography decoding we take these Bloch vectors as the quantum features, which is equivalent to rescaling the quantum kernel function [Eq. (14)] by a constant factor of 2N .\nThe encoding method we use leads to embedded states that exhibit entanglement. Fig. 4a shows that the average entanglement negativity quickly increases during the encoding, and then eventually decays at a rate depending on \u03b3. In parallel, as we see from Fig. 4b, there is a finite von Neumann entropy of the system due to mixed character of the state. In Section V, we will show how these processes affect the performances of noisy quantum kernel machines."
        },
        {
            "heading": "B. Time multiplexing measurements",
            "text": "A simplified and experimentally less demanding decoding is obtained by measuring all the single-site observables (i.e., the three local Pauli spin operators) at different times after the end of the encoding. In the following, we will denote Nrep the number of repetitions of these measurements. Hence, for a system of N spins, a total number 3N\u00d7Nrep of measurements have been performed after Nrep repetitions. We use measurements of the onsite observables for each spin, which correspond to the components of the Bloch vectors of the reduced density matrices on each site. We consider corresponding observables in the Heisenberg picture. The new feature vector \u03c6\u0303(x) in the time-multiplexing protocol have entries of the form \u3008Bi(t+k\u03b4tm)\u3009x with 1 \u2264 i \u2264 3N, 1 \u2264 k \u2264 Nrep, where \u03b4tm is the time interval between two consecutive measurements. Similar methods were used in previous\n11\nworks to perform an approximate tomography of the system state [53, 54]. Note that the time-multiplexing procedure can only decrease the model expressive power when compared to the full tomography, as information leaks into the system\u2019s environment as the system evolves between successive measurement times (see Appendix C)."
        },
        {
            "heading": "V. NUMERICAL RESULTS",
            "text": "In this section, we discuss the numerical results on the noisy quantum kernel machines obtained by considering the model spin Hamiltonian, dephasing channels, input encoding via driving, decoding protocol through measurement and the classification task detailed in the previous section."
        },
        {
            "heading": "A. Performances, noise and system size",
            "text": "The main goal is to determine how the performance of the noisy quantum kernel machine scales with the amount of noise and the number of chain sites, i.e. network nodes. To provide a fair comparison, it is necessary to ensure that the same amount of information is fed into the system for all the system sizes. This is achieved by keeping fixed the number M of projections and resolution of the images. As it will be shown in Section VB, the performance can be greatly enhanced when this information bottleneck is lifted and the amount of encoded information is varied.\nThe first point to address is the trainability and generalization properties. In Fig. 5, we show the dependence of the training and testing errors on the generalization parameter \u03bb. The curves in (a) are obtained assuming a full\ntomography and ideal measurements. Panel (b) instead presents the same results, but with imperfect measurements (see caption for more details). In panel (a), the training error (dashed lines) drops to zero as \u03bb \u2192 0+; this is a manifestation of overfitting and indicates that, thanks to the high dimensionality of the quantum feature space, the system is able to completely fit the training data. Instead, the testing error (solid lines and markers) has a minimum value for some optimal value of \u03bb, which depends on the dephasing rate \u03b3 (different markers denote different rates). For large enough values of \u03bb the testing and training error curves eventually overlap. For increasing \u03b3 the minimum shifts to vanishing values of \u03bb. A remarkable result is that the minimal testing error is very little affected by the dephasing rate. As shown in Fig. 5b, the situation changes in the presence of imperfect measurements. Indeed, the minimum of the testing error is obtained for a finite value of \u03bb even for large values of \u03b3. Importantly, the minimum error increases with increasing dephasing noise.\nIn panels (c) and (d) of Fig. 5, we report the dependence of the minimal testing error as a function of the number of spins N for increasing values of the dephasing rate. Again, panel (c) corresponds to ideal measurements, while curves in panel (d) are obtained under imperfect measurements. Panel (c) shows that the testing error diminishes as a function of the number of spins and increases with dephasing rate. Note that also for very small dephasing rate the minimal testing error appears to saturate at large system sizes. This is hardly surprising as the input images have been preprocessed and considerably down-sampled. This deliberate choice aims at making the task harder in order to gauge the expressivity of the machine without overloading the input information. As shown in panel (d) of Fig. 5, by con-\n12\nsidering imperfect measurements the role of dephasing is dramatically amplified.\nAs we have described in the analytical discussion in Section III, the quantum kernel spectrum allows us to assess the capacity of our model independently from the specific task one wants to achieve. Fig. 6a shows the dependence of the quantum kernel\u2019s effective rank Reff(Kc) on the system size and noise strength. For vanishing values of the dephasing rate \u03b3, we see that this figure of merit first increases exponentially with the number of spins before saturating. For increasing \u03b3, the effective quantum kernel rank decreases approaching one in the limit of very large \u03b3.\nThe same behavior is observed in the empirical spectrum in Fig. 6b as the noise rate is varied. For increasing values of \u03b3, we observe a faster decrease of the empirical kernel eigenvalues as a function of the eigenvalue number. For comparison, we have indicated with markers the largest eigenvalue below the optimal generalization parameter. This gives a rough estimate of the number of kernel eigenfunctions required to correctly approximate the target function. Note that in the context of imperfect measurements the generalization parameter is bounded from below, and hence some of the kernel eigenfunctions becomes out-of-reach. This shows a clear link between the kernel eigenvalues and the expressivity of the machine.\nThe results discussed relied on full tomography. As we have explained in Section IV, it is possible to design a simplified and less expensive measurement protocol based on a time-multiplexing procedure where a set of local spin observables are measured at Nrep different times. The results obtained with such an approach are summarized in Fig. 7. As appears from Fig. 7a, by increasing the number of repetitions Nrep the error diminishes. For small enough values of dephasing \u03b3, the error converges to the value in the ideal case of full-tomography. For increasing \u03b3, however, the saturating value departs from the ideal one given by full tomography, showing that the timemultiplexing expressivity deteriorates more than that of the full tomography for larger noise. This trend is further elucidated in Fig. 7b, where the difference between the time-multiplexing error and the full-tomography error is reported as a function of the total number of measured observables. By increasing the number of spins and hence the dimension of the Hilbert space for a given dephasing rate, the required number of repetitions increases."
        },
        {
            "heading": "B. Optimizing the encoding",
            "text": "In this section we investigate an alternative encoding scheme for which the amount of information fed to the system scales with the system size. The embedding studied in the previous sections involved a set of M = Npulse random-projection features derived from the down-sampled images. Here we derive a number M = N \u00d7 Npulse of such features and split them in N\nsequences of Npulse features, which we use to drive the N sites. In particular, the driving sequences sent to different sites are unique, while for the previous encoding those sequences were proportional to each other. The new encoding procedure is presented in Fig. 8 in the form of its equivalent circuit. In Fig. 9 we report the evolution of the performances given by this new encoding as a function of the number of driving pulses. As the number of pulses rises the corresponding number of encoded features M = N \u00d7Npulse increases and so does the amount of encoded information. The corresponding maximal testing accuracy reaches an optimum of 94.5% for Npulse = 3. For large number of pulses Npulse the performances drop. This effect is due to the fact that\n13\nfor such parameters the transformations yielded by the encoding are poorly adapted to the task at hand, i.e. the kernel and the target function become less \u201caligned\". Note that the performance is very sensitive to both the encoding method and the physical system parameters. While controlling the physical parameters might be hard, it appears that a careful design of the encoding procedure can significantly boost the performances. This makes the research of tailored encoding procedures a promising avenue of research."
        },
        {
            "heading": "VI. CONCLUSION",
            "text": "In this work, we have presented a quantum machinelearning model based on the quantum-kernel paradigm. Within the formalism of kernel theory, we have characterized the expressivity and generalization capacity of this model. We have linked the relevant figure of merits to the spectrum of the associated centered quantum kernel. In particular, we presented an upper bound on the generalization error involving the average purity of quantum states representing the data to classify. This upperbound shows that dissipation and decoherence act as a regularization for the quantum kernel machines. By considering an illustrating example of a driven-dissipative spin chain as the noisy quantum kernel machine, we have shown how the expressivity and generalization capacity are controlled by both the dephasing rate and by experimental uncertainties on the measurements. Moreover, we have shown how the performances of the noisy quantum kernel machines are modified when the full-tomography measurement protocol is replaced by a time-multiplexing procedure requiring only local observables, and how the openness of the system mitigates the efficiency of this protocol. We observed a qualitative improvement in the processing performance of our model when going from a scenario where the system is fed a constant amount of information to one where the inputs are encoded at a finite information rate that scales extensively with the system size. How to design tailored encoding strategies able to harness the full power of quantum kernel machines remains an open question. In particular, investigating encoding schemes that would allow to inject information at a rate scaling exponentially in the system size seems promising. The concepts presented here and the unavoidable role of the decoherence in any realistic physical system are relevant for a wide range of quantum machinelearning models, ranging from quantum extreme-learning machines to quantum neural networks."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "We would like to thank Hugo Tschirhart for help at the early stages of this project. This work was supported by the FET FLAGSHIP Project PhoQuS (grant agreement ID: 820392), by Region \u00cele-de-France in the framework of\nDIM SIRTEQ, and the French agency ANR through the grants NOMOS (ANR-18-CE24-0026) and TRIANGLE (ANR-20-CE47-0011). We also acknowledge access to the high performance computation center TGCC of the French national computational facility GENCI under the project 2021-A0100512462."
        },
        {
            "heading": "Appendix A: Expressivity and generalization for noisy quantum kernel",
            "text": ""
        },
        {
            "heading": "1. Expressivity and kernel effective rank",
            "text": "To measure the ability of a kernel k to learn a function y(x), we have introduced in Eq. (26) the kernel target alignment A(k, y). We then defined the kernel effective rank Reff by considering a set of orthonormal basis functions {gi}, that gives the following equalities: \u221a Reff(k) = \u2211\nj\nA(k, gj)\n= 1\n( \u2211 i \u03bb 2 i ) 1/2\n\u2211\nj\n\u2211\ni\n\u03bbiEp [\u03c8i(x)gj(x)]2\n= 1\n( \u2211 i \u03bb 2 i ) 1/2\n\u2211\ni\n\u03bbiEp [ \u03c8i(x) 2 ]\n=\n\u2211 i \u03bbi\n( \u2211 i \u03bb 2 i ) 1/2 .\n(A1)\nNote that the final expression concerns only the spectrum of the kernel and is independent of the choice of the basis functions {gi}. From the Cauchy-Schwarz inequality, we have\nReff(k) \u2264 |{\u03bbi 6= 0}| , (A2)\nwhere the equality is attained if and only if all non-zero eigenvalues of the kernel are equal. Therefore, it provides information about the flatness of the spectrum of the kernel.\nGiven a training sample of size Ntrain, the kernel spectrum can be empirically computed using the Ntrain \u00d7 Ntrain kernel matrix K associated to the kernel k, whose entries are Kij = k(xi,xj). The eigenvalues \u03bbi of the kernel k can then be approximated by those of the matrix K/Ntrain [55]. For the centered quantum kernel kc with the associated kernel matrix Kc, we can compute the effective rank empirically as:\n\u221a Reff(Kc) =\nTr [Kc]\u221a Tr [K2c ] = \u2211 i \u03bb\u0302i\u221a\u2211 i \u03bb\u0302 2 i , (A3)\nwhere the \u03bb\u0302i are the empirical eigenvalues [56]. The numerator can be expressed using the empirical kernel eigenobservables. In order to keep light notations we use the same notations as in the main text, i.e. the empirical kernel eigenobservables are denoted\n14\nE\u0302i. Whether this notation refers to the exact or the empirical observable should be clear from the context. We have for the numerator:\n\u2211\ni\n\u03bb\u0302i = Ep\u0302\n[\u2211\ni\n\u03b4\u3008E\u0302i\u30092x\n]\n= Ep\u0302\n[\u2211\ni\nTr [ \u03b4\u03c1\u0302(x)E\u0302i ]2 ] .\n(A4)\nSince Tr[\u03b4\u03c1\u0302(x)] = 0, \u03b4\u03c1\u0302(x) can be decomposed onto the eigenobservable basis {E\u0302i} through the expression:\n\u03b4\u03c1\u0302(x) = \u2211\ni\nTr [ \u03b4\u03c1\u0302(x)E\u0302i ] E\u0302i . (A5)\nConsequently, the squared Hilbert-Schmidt norm reads:\nTr [ \u03b4\u03c1\u0302(x)2 ] = \u2211\ni\nTr [ \u03b4\u03c1\u0302(x)E\u0302i ]2 . (A6)\nEq. (A4) therefore becomes:\n\u2211\ni\n\u03bb\u0302i = Ep\u0302 [ Tr [ \u03b4\u03c1\u0302(x)2 ]]\n= Tr [ Ep\u0302 [ (\u03c1\u0302(x)\u2212 Ep\u0302 [\u03c1\u0302(x)])2 ]] = Ep\u0302 [ Tr [ \u03c1\u0302(x)2 ]] \u2212 Tr [ Ep\u0302 [\u03c1\u0302(x)]2 ] ,\n(A7)\ngiving Eq. (31) in the main text (as the same relation holds between the true eigenvalues \u03bbi and the distribution p). This quantity can also be written in terms of the measured observables (note that O = B for a quantum kernel):\n\u2211\ni\n\u03bb\u0302i = Tr [Kc]\nNtrain\n= 1\nNtrain\n\u2211\ni\nkc(xi,xi)\n= 1\nNtrain\n\u2211\ni\n\u2211\nk\n\u03b4\u03c6k(xi)\u03b4\u03c6k(xi)\n= \u2211\nk\n( 1\nNtrain\n\u2211\ni\n\u03b4\u03c6k(xi)\u03b4\u03c6k(xi)\n)\n= \u2211\nk\nEp\u0302 [ \u03b4\u03c6k(x) 2 ]\n= \u2211\nk\nEp\u0302 [ \u03b4\u3008O\u0302k\u30092x ]\n= \u2211\nk\nVarp\u0302 [ \u3008O\u0302k\u3009x ] .\n(A8)\nSimilarly, in the denominator of Eq. (A3), we get:\n\u2211\ni\n\u03bb\u03022i = Tr [ K2c ]\nN2train\n= 1\nN2train\n\u2211\ni,j\nkc (xi,xj) 2\n= 1\nN2train\n\u2211\ni,j\n(\u2211\nk\n\u03b4\u03c6k(xi)\u03b4\u03c6k(xj)\n)2\n= \u2211\nk,l\n( 1\nNtrain\n\u2211\ni\n\u03b4\u03c6k(xi)\u03b4\u03c6l(xi)\n)2\n= \u2211\nk,l\nEp\u0302 [ \u03b4\u3008O\u0302k\u3009x\u03b4\u3008O\u0302l\u3009x ]2\n= \u2211\nk,l\nCovp\u0302 [ \u3008O\u0302k\u3009x, \u3008O\u0302l\u3009x ]2 .\n(A9)\nFinally, we get the general expression:\n\u221a Reff(Kc) =\n\u2211P i=1 Varp\u0302 [ \u3008O\u0302i\u3009x ]\n(\u2211P i,j=1 Covp\u0302 [ \u3008O\u0302i\u3009x, \u3008O\u0302j\u3009x ]2) 12 .\n(A10) Note that this relation also holds for the true (nonempirical) effective rank Reff(kc) provided that the variances and the covariances are taken with respect to the true probability distribution p instead of the empirical one p\u0302."
        },
        {
            "heading": "2. Generalization and Rademacher complexity",
            "text": "Here we give the detailed derivation of Eq. (37) using methods of statistical learning theory applied to the specific case of a noisy centered quantum kernel [49].\nIn the standard setup of statistical learning, the inputs x \u2208 X are considered as a random variable following a probability distribution p(x). We define the target function y : X 7\u2192 Y that assigns to each input its right label. We will consider the case of a binary classification, for which Y = {\u22121, 1}. In practice the true distribution p of the inputs is unknown and during the training we only have access to a finite training dataset S = {(xi, yi) | i = 1, . . . , Ntrain}. The elements of the dataset are considered as realizations of a set of independent and identically distributed random variables following p. The empirical distribution associated to this training set is given by:\np\u0302(x) = 1\nNtrain\nNtrain\u2211\ni=1\n\u03b4(x\u2212 xi) . (A11)\nWe rely on this empirical distribution to evaluate expec-\n15\ntations of any function f(x), namely:\nEp [f(x)] = \u222b\nX f(x)p(x)dx . (A12)\nThe expectation value is approximated by its empirical counterpart:\nEp\u0302 [f(x)] = \u222b\nX f(x)p\u0302(x)dx =\n1\nNtrain\nNtrain\u2211\ni=1\nf(xi) (A13)\nA common question in statistical learning is to know how a model trained on a given set of data will perform on any other set of unseen data. For a binary classification task with balanced data one can use the accuracy as measure of the model performance. Given a trial function f(x) = wT\u03c6(x) that has been optimized using the training set S, we define the corresponding prediction function as f\u0303(x) = sign[f(x)]. An input x is correctly classified if f\u0303(x) = y(x). The true accuracy A\u2217(f) is defined as the probability that any input in X is correctly classified by f\u0303 :\nA\u2217(f) = Ep [ 1y(x)=f\u0303(x) ] = Ep [ 1y(x)f(x)\u22650 ]\n= 1\u2212 Ep [ 1y(x)f(x)\u22640 ] = 1\u2212R\u2217(f) ,\n(A14)\nwhere we define the risk (also called error or inaccuracy) as R\u2217(f) = 1 \u2212 A\u2217(f). The corresponding empirical quantities A(f) and R(f) are defined in an analogous way using the empirical distribution p\u0302 instead of p. The ability to perform well on new data is measured by the generalization error:\nE(f) = R\u2217(f)\u2212R(f) . (A15)\nStatistical learning theory provides probabilistic upperbounds on the generalization error depending on the type of task at hand and on the specific model used to tackle it. In order to find such an upper bound for a binary classification tasks, it is convenient to consider a relaxed version of the risk, the \u03b7-margin-risk R\u03b7(f) defined in the main text. The upper bound on the generalization properties involves the empirical Rademacher complexity of a class of trial functions H with respect to the training sample S. It is defined as:\nRS (H) = E\u03c3 [\nsup f\u2208H\n1\nNtrain\nNtrain\u2211\ni=1\n\u03c3if(xi)\n] (A16)\nwhere \u03c3 is a vector of Rademacher variables that are discrete, independent and identically distributed following a uniform law over {\u22121, 1}. The Rademacher complexity measures the ability of a hypothesis class H to fit noise, and as such it is a measure of the expressivity of H. We now give a upper-bound on the generalization error (Theorem 5.8 in [49]):\nTheorem A.1. Let H be a set of trial functions and \u03b7 > 0. Then \u2200\u03b4 > 0, with probability at least 1 \u2212 \u03b4, we have \u2200f \u2208 H:\nR\u2217(f) \u2264 R\u03b7(f) + 2\n\u03b7 RS(H) + 3\n\u221a log( 2\u03b4 )\n2Ntrain\nThis upper-bound can be specialized to the case of kernel methods where the hypothesis class is the RKHS of a kernel k. In this the Rademacher complexity is upper bounded by a quantity that depends only on the trace of the empirical kernel matrix K (Theorem 6.12 in [49]):\nTheorem A.2. Let H by the RKHS associated to a given kernel k. For \u039b \u2265 0 consider the set of hypothesis functions H\u039b = {f : x 7\u2192 wT \u03b4\u03c6(x), \u2016w\u20162\u039b \u2264 1} \u2286 H. Then we have:\nRS (H\u039b) \u2264 1\nNtrain\n\u221a Tr [K]\n\u039b .\nInjecting this result in the previous upper bound, we get the desired result. In particular, using the centered noisy quantum kernel kc and Eq. (A7), we get Eq. (37)."
        },
        {
            "heading": "Appendix B: Intercept and kernel centering",
            "text": "The initial optimization problem is to find a weight vector w = (b, w1, . . . , wP )T that minimizes the regularized loss function of Eq. (8). We slightly change our notation and drop the first constant term of the embedding map \u03c6(x) to explicitly seperate the bias b from the weight w, so that the loss can be rewritten:\nL (w, b | S) = 1 2Ntrain\nNtrain\u2211\ni=1\n( yi \u2212wT\u03c6(xi)\u2212 b )2\n+ \u03bb\n2 \u2016w\u201622 .\n(B1)\nThe optimal intercept b is found by imposing \u2202L\u2202b = 0. The solution reads:\nb\u2217 = 1\nNtrain\nNtrain\u2211\ni=1\nyi \u2212wT ( 1\nNtrain\nNtrain\u2211\ni=1\n\u03c6(xi)\n) . (B2)\nWe see that the optimal intercept consists of two terms: one that has the effect of centering the labels, while the other centers the features. Assuming the dataset we use are balanced, we have \u2211Ntrain i=1 yi ' 0 Plugging back the optimal intercept into the previous regularized loss function, we get a new effective loss:\nL\u2217(w | S) = 1 2Ntrain\nNtrain\u2211\ni=1\n( yi \u2212wT (\u03c6(xi)\u2212 Ep\u0302 [\u03c6]) )2\n+ \u03bb\n2 \u2016w\u201622 .\n(B3)\n16\nIf the data are not balanced one can simply replace the labels yi by their centered counterpart y\u2032i = yi \u2212\n1 Ntrain \u2211Ntrain i=1 yi such that \u2211Ntrain i=1 y \u2032 i = 0. Note that this might lead to issues when using the accuracy metric with unbalanced labels. This issue can be fixed, e.g. by changing the metric used for a balanced one. Thus, working with the quantum kernel without regularizing the intercept term is equivalent to working with the centered kernel and centered labels."
        },
        {
            "heading": "Appendix C: Time-multiplexing and model expressivity",
            "text": "The maximal class of trial functions Hfull (see section III) associated to a given embedding is obtained by performing a complete tomography of the embedded quantum states \u03c1\u0302(x) right after the end of the encoding procedure. The system evolution after time \u03c4 according to a Lindblad master equation with a constant Hamiltonian and disspator for a given duration \u03b4tm can be expressed into a set of Kraus operators {W\u0302i} [34] satisfying:\n\u2211\ni\nW\u0302 \u2020i W\u0302i = 1\u0302 . (C1)\nThe evolved density matrices \u03c1\u0302(x; \u03b4tm) are given by:\n\u03c1\u0302(x; \u03b4tm) = \u2211\ni\nW\u0302i\u03c1\u0302(x)W\u0302 \u2020 i . (C2)\nIn the Heisenberg picture, the observables evolve in time following an adjoint master equation [34]. Hence, we can see the non-unitary evolution of the open quantum system as a simple change in the set of observables that are measured on the state \u03c1\u0302(x). Suppose that we want to measure observables from the orthonormal basis introduced in section II B after the previous evolution. We define the (P + 1) \u00d7 (P + 1) matrix \u039e whose elements are:\n\u039ekl = Tr\n[\u2211\ni\nW\u0302 \u2020i O\u0302kW\u0302iO\u0302l\n] . (C3)\nThe measurement at time \u03c4 + \u03b4tm of the observable O\u0302l can now be expressed using the decomposition in Eq. (43) and the elements of \u039e as:\nTr [ \u03c1\u0302(x; \u03b4tm)O\u0302l ] = 1\n2N\n( \u039e0l + \u2211\nk\nTr [ \u03c1\u0302(x)O\u0302k ] \u039ekl ) .\n(C4) Thus the embedding map \u03c6(x) is transformed by the non-unitary evolution during \u03b4tm and becomes:\n\u03c6(x; \u03b4tm) = \u039e\u03c6(x) . (C5)\nAssuming we only make measurements on a subset of the basis {O\u0302i}, then we can write for the feature vector:\n\u03c6(x; \u03b4tm) = D\u039e\u03c6(x) , (C6) where D is a diagonal (P + 1) \u00d7 (P + 1) matrix whose diagonal entries i are 1 if O\u0302i is measured and 0 otherwise. When we repeat the measurements at different times, we can stack the previous vectors at each time steps. For Nrep repetitions, we denote \u039b the Nrep(P + 1)\u00d7 (P + 1) matrix of the form:\n\u039b =   D\u039e D\u039e2\n... D\u039eNrep\n  . (C7)\nThe final vector reads:\n\u03c6\u0303(x) = \u039b\u03c6(x) . (C8)\nHence, by performing repeated measurements in-between non-unitary evolutions amount to performing a restricted number of measurements on the encoded states \u03c1\u0302(x) at time \u03c4 . This implies that the time-multiplexing decoding lowers the model expressivity. The difference between the models obtained from the full tomography and the timemultiplexing decoding is encapsulated in the matrix \u039b.\n[1] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton, \u201cDeep learning,\u201d Nature 521, 436\u2013444 (2015). [2] Ian Goodfellow, Yoshua Bengio, and Aaron Courville, Deep Learning, edited by Francis Bach, Adaptive Computation and Machine Learning Series (MIT Press, Cambridge, MA, USA, 2016). [3] Emma Strubell, Ananya Ganesh, and Andrew McCallum, \u201cEnergy and policy considerations for deep learning in NLP,\u201d in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (Association for Computational Linguistics, 2019) pp. 3645\u20133650. [4] Gouhei Tanaka, Toshiyuki Yamane, Jean Benoit H\u00e9roux, Ryosho Nakane, Naoki Kanazawa, Seiji Takeda,\nHidetoshi Numata, Daiju Nakano, and Akira Hirose, \u201cRecent advances in physical reservoir computing: A review,\u201d Neural Networks 115, 100\u2013123 (2019). [5] Guang-Bin Huang, Qin-Yu Zhu, and Chee-Kheong Siew, \u201cExtreme learning machine: Theory and applications,\u201d Neurocomputing Neural Networks, 70, 489\u2013501 (2006). [6] Andrzej Opala, Sanjib Ghosh, Timothy C.H. Liew, and Micha\u0142 Matuszewski, \u201cNeuromorphic Computing in Ginzburg-Landau Polariton-Lattice Systems,\u201d Physical Review Applied 11, 064029 (2019). [7] Zakari Denis, Ivan Favero, and Cristiano Ciuti, \u201cPhotonic Kernel Machine Learning for Ultrafast Spectral Analysis,\u201d Physical Review Applied 17, 034077 (2022).\n17\n[8] Dario Ballarini, Antonio Gianfrate, Riccardo Panico, Andrzej Opala, Sanjib Ghosh, Lorenzo Dominici, Vincenzo Ardizzone, Milena De Giorgi, Giovanni Lerario, Giuseppe Gigli, Timothy C. H. Liew, Michal Matuszewski, and Daniele Sanvitto, \u201cPolaritonic Neuromorphic Computing Outperforms Linear Classifiers,\u201d Nano Letters 20, 3506\u2013 3512 (2020). [9] Davide Pierangeli, Giulia Marcucci, and Claudio Conti, \u201cPhotonic extreme learning machine by free-space optical propagation,\u201d Photon. Res. 9, 1446\u20131454 (2021). [10] Thomas Hofmann, Bernhard Sch\u00f6lkopf, and Alexander J. Smola, \u201cKernel methods in machine learning,\u201d The Annals of Statistics 36, 1171\u20131220 (2008). [11] Arthur Jacot, Franck Gabriel, and Clement Hongler, \u201cNeural Tangent Kernel: Convergence and Generalization in Neural Networks,\u201d in Advances in Neural Information Processing Systems, Vol. 31 (Curran Associates, Inc., 2018). [12] Jacob Biamonte, Peter Wittek, Nicola Pancotti, Patrick Rebentrost, Nathan Wiebe, and Seth Lloyd, \u201cQuantum machine learning,\u201d Nature 549, 195\u2013202 (2017). [13] Vedran Dunjko and Hans J. Briegel, \u201cMachine learning & artificial intelligence in the quantum domain: A review of recent progress,\u201d Reports on Progress in Physics 81, 074001 (2018). [14] Maria Schuld, Ilya Sinayskiy, and Francesco Petruccione, \u201cThe quest for a Quantum Neural Network,\u201d Quantum Information Processing 13, 2567\u20132586 (2014). [15] Danijela Markovi\u0107 and Julie Grollier, \u201cQuantum neuromorphic computing,\u201d Applied Physics Letters 117, 150501 (2020). [16] Marcello Benedetti, Erika Lloyd, Stefan Sack, and Mattia Fiorentini, \u201cParameterized quantum circuits as machine learning models,\u201d Quantum Science and Technology 4, 043001 (2019). [17] Edward Farhi and Hartmut Neven, \u201cClassification with Quantum Neural Networks on Near Term Processors,\u201d (2018), arXiv:1802.06002. [18] Daniel K. Park, Carsten Blank, and Francesco Petruccione, \u201cThe theory of the quantum kernel-based binary classifier,\u201d Physics Letters A 384, 126422 (2020). [19] Maria Schuld and Nathan Killoran, \u201cQuantum Machine Learning in Feature Hilbert Spaces,\u201d Physical Review Letters 122, 040504 (2019). [20] Seth Lloyd, Maria Schuld, Aroosa Ijaz, Josh Izaac, and Nathan Killoran, \u201cQuantum embeddings for machine learning,\u201d (2020), 10.48550/arXiv.2001.03622. [21] Thomas Hubregtsen, David Wierichs, Elies Gil-Fuster, Peter-Jan H. S. Derks, Paul K. Faehrmann, and Johannes Jakob Meyer, \u201cTraining Quantum Embedding Kernels on Near-Term Quantum Computers,\u201d (2021), arXiv:2105.02276. [22] Maria Schuld, \u201cSupervised quantum machine learning models are kernel methods,\u201d (2021), arXiv:2101.11020. [23] Takeru Kusumoto, Kosuke Mitarai, Keisuke Fujii, Masahiro Kitagawa, and Makoto Negoro, \u201cExperimental quantum kernel trick with nuclear spins in a solid,\u201d npj Quantum Information 7, 1\u20137 (2021). [24] Yunchao Liu, Srinivasan Arunachalam, and Kristan Temme, \u201cA rigorous and robust quantum speed-up in supervised machine learning,\u201d Nature Physics 17, 1013\u2013 1017 (2021). [25] Yusen Wu, Bujiao Wu, Jingbo Wang, and Xiao Yuan, \u201cProvable Advantage in Quantum Phase Learning via\nQuantum Kernel Alphatron,\u201d (2021), arXiv:2111.07553. [26] Jonas M. K\u00fcbler, Simon Buchholz, and Bernhard\nSch\u00f6lkopf, \u201cThe Inductive Bias of Quantum Kernels,\u201d (2021), 10.48550/arXiv.2106.03747.\n[27] Ruslan Shaydulin and Stefan M. Wild, \u201cImportance of Kernel Bandwidth in Quantum Machine Learning,\u201d (2021), arXiv:2111.05451. [28] Xinbiao Wang, Yuxuan Du, Yong Luo, and Dacheng Tao, \u201cTowards understanding the power of quantum kernels in the NISQ era,\u201d Quantum 5, 531 (2021). [29] Karol Bartkiewicz, Clemens Gneiting, Anton\u00edn \u010cernoch, Kate\u0159ina Jir\u00e1kov\u00e1, Karel Lemr, and Franco Nori, \u201cExperimental kernel-based quantum machine learning in finite feature space,\u201d Scientific Reports 10, 12356 (2020). [30] John Preskill, \u201cQuantum Computing in the NISQ era and beyond,\u201d Quantum 2, 79 (2018). [31] Keisuke Fujii and Kohei Nakajima, \u201cHarnessing Disordered-Ensemble Quantum Dynamics for Machine Learning,\u201d Physical Review Applied 8, 024030 (2017). [32] Huawen Xu, Tanjung Krisnanda, Wouter Verstraelen, Timothy C. H. Liew, and Sanjib Ghosh, \u201cSuperpolynomial quantum enhancement in polaritonic neuromorphic computing,\u201d Physical Review B 103, 195302 (2021). [33] In the case of multivariate functions the variables against which the expectation is taken is indicated in subscript. [34] Heinz-Peter Breuer and Francesco Petruccione, The Theory of Open Quantum Systems (Oxford University Press, Oxford, 2007). [35] J.A.K. Suykens and J. Vandewalle, \u201cLeast Squares Support Vector Machine Classifiers,\u201d Neural Processing Letters 9, 293\u2013300 (1999). [36] Trevor Hastie, Robert Tibshirani, and Jerome Friedman, The Elements of Statistical Learning: Data Mining, Inference, and Prediction (Springer Science & Business Media, 2013). [37] Note that in certain scenario, one may prefer to exclude the first component of ~w, which is a constant intercept term, in the regularization. Then it suffices to replace the (1, 1) entry of 1 by 0 in Eq. (9) to obtain the optimal weights. This is equivalent to using a centered kernel, as discussed in Appendix B. [38] Sofiene Jerbi, Lukas J. Fiderer, Hendrik Poulsen Nautrup, Jonas M. K\u00fcbler, Hans J. Briegel, and Vedran Dunjko, \u201cQuantum machine learning beyond kernel methods,\u201d (2021), arXiv:2110.13162. [39] The probability measure p on the input space X is important here as it determines the scalar product on the space of real-valued functions on X through \u3008f, g\u3009 = Ex\u223cp [f(x)g(x)]. The reproducing property, crucial to link the RKHS and its kernel, relies on such a well-defined scalar product. [40] Vern I. Paulsen and Mrinal Raghupathi, An Introduction to the Theory of Reproducing Kernel Hilbert Spaces, Cambridge Studies in Advanced Mathematics (Cambridge University Press, Cambridge, 2016). [41] Maria Schuld, Ryan Sweke, and Johannes Jakob Meyer, \u201cEffect of data encoding on the expressive power of variational quantum-machine-learning models,\u201d Physical Review A 103, 032430 (2021). [42] For a closed system, this quantum kernel can be directly evaluated through measurement [18] and the trial function can be expressed in terms of the quantum kernel and optimized in an equivalent way.\n18\n[43] Blake Bordelon, Abdulkadir Canatar, and Cengiz Pehlevan, \u201cSpectrum Dependent Learning Curves in Kernel Regression and Wide Neural Networks,\u201d (2021), arXiv:2002.02561. [44] Abdulkadir Canatar, Blake Bordelon, and Cengiz Pehlevan, \u201cSpectral bias and task-model alignment explain generalization in kernel regression and infinitely wide neural networks,\u201d Nature Communications 12, 2914 (2021). [45] Amira Abbas, David Sutter, Christa Zoufal, Aurelien Lucchi, Alessio Figalli, and Stefan Woerner, \u201cThe power of quantum neural networks,\u201d Nature Computational Science 1, 403\u2013409 (2021). [46] Trevor Hastie and Ji Zhu, \u201cComment: [support vector machines with applications],\u201d Statistical Science 21, 352\u2013 357 (2006). [47] Hsin-Yuan Huang, Michael Broughton, Masoud Mohseni, Ryan Babbush, Sergio Boixo, Hartmut Neven, and Jarrod R. McClean, \u201cPower of data in quantum machine learning,\u201d Nature Communications 12, 2631 (2021). [48] Nello Cristianini, Jaz Kandola, Andre Elisseeff, and John Shawe-Taylor, \u201cOn Kernel Target Alignment,\u201d in Innovations in Machine Learning: Theory and Applications, Studies in Fuzziness and Soft Computing, edited by Dawn E. Holmes and Lakhmi C. Jain (Springer, Berlin, Heidelberg, 2006) pp. 205\u2013256. [49] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar, Foundations of machine learning (MIT press, 2018). [50] Leonardo Banchi, Jason Pereira, and Stefano Pirandola, \u201cGeneralization in Quantum Machine Learning: A Quan-\ntum Information Standpoint,\u201d PRX Quantum 2, 040321 (2021). [51] In total, the results presented in this work required approximately 800,000 scalar hours (\u223c 90 years) of computation and 5 terabytes of storage on the acknowledged French National High Performance Computing facility (GENCI). During the simulations, we used approximately up to 30, 000 cores simultaneously. [52] This can be explicitly expressed as a Ddeep circuit via Trotterization as G =[\u220fN\ni=1 R Z i\n( \u2212 2\u2206i\u2206t\nD )\u220f \u3008i,j\u3009 \u220f K\u2208{X,Y,Z}R KK ij ( 2JKij \u2206t D ) ]D +\nO(J20 \u2206t 2/D2).\n[53] Artur Czerwinski, \u201cQuantum state tomography with informationally complete POVMs generated in the time domain,\u201d Quantum Information Processing 20, 105 (2021). [54] Antonio Di Lorenzo, \u201cSequential Measurement of Conjugate Variables as an Alternative Quantum State Tomography,\u201d Physical Review Letters 110, 010404 (2013). [55] Christopher Williams and Matthias Seeger, \u201cUsing the nystr\u00f6m method to speed up kernel machines,\u201d in Advances in Neural Information Processing Systems 13 (MIT Press, 2001) pp. 682\u2013688. [56] The hat symbol is used for estimators such as the empirical eigenvalues, as customary in statistical theory. The hat must not confused with the one used for the quantum operators."
        }
    ],
    "title": "Noisy Quantum Kernel Machines",
    "year": 2022
}