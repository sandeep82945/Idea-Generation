{
    "abstractText": "COPYRIGHT \u00a9 2023 Zhang, Hu, Xu and Zhao. This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms. TYPE Original Research PUBLISHED 05 June 2023 DOI 10.3389/fpls.2023.1174556",
    "authors": [
        {
            "affiliations": [],
            "name": "Mehedi Masud"
        },
        {
            "affiliations": [],
            "name": "Weifu Li"
        },
        {
            "affiliations": [],
            "name": "Chong Zhang"
        },
        {
            "affiliations": [],
            "name": "Zhuhua Hu"
        },
        {
            "affiliations": [],
            "name": "Lewei Xu"
        },
        {
            "affiliations": [],
            "name": "Yaochi Zhao"
        }
    ],
    "id": "SP:21c80aec70fb8fcbe0ec0f8f1b9ec52174012b0b",
    "references": [
        {
            "authors": [
                "D. Al Bashish",
                "M. Braik",
                "S. Bani-Ahmad"
            ],
            "title": "A framework for detection and classification of plant leaf and stem diseases,\u201d in 2010 international conference on signal and image processing (IEEE)",
            "venue": "113\u2013118.",
            "year": 2010
        },
        {
            "authors": [
                "J. Amara",
                "B. Bouaziz",
                "A. Algergawy"
            ],
            "title": "A deep learning-based approach for banana leaf diseases classification,\" in Datenbanksysteme f\u00fcr Business, Technologie und Web (BTW 2017), 17",
            "venue": "Fachtagung des GI-Fachbereichs, Datenbanken und Informationssysteme (DBIS). Stuttgart, Germany: Workshopband, 6\u201310.",
            "year": 2017
        },
        {
            "authors": [
                "A. Bochkovskiy",
                "Wang",
                "C.-Y",
                "Liao",
                "H.-Y. M"
            ],
            "title": "Yolov4: optimal speed and accuracy of object detection (arXiv preprint arXiv:2004.10934)",
            "year": 2020
        },
        {
            "authors": [
                "R. Girshick"
            ],
            "title": "Fast r-cnn,\u201d in Proceedings of the IEEE international conference on computer vision",
            "venue": "1440\u20131448.",
            "year": 2015
        },
        {
            "authors": [
                "R. Girshick",
                "J. Donahue",
                "T. Darrell",
                "J. Malik"
            ],
            "title": "Rich feature hierarchies for accurate object detection and semantic segmentation,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition",
            "venue": "580\u2013587.",
            "year": 2014
        },
        {
            "authors": [
                "K. He",
                "G. Gkioxari",
                "P. Doll\u00e1r",
                "R. Girshick"
            ],
            "title": "Mask r-cnn,\u201d in Proceedings of the IEEE international conference on computer vision",
            "venue": "2961\u20132969.",
            "year": 2017
        },
        {
            "authors": [
                "C. Li",
                "L. Li",
                "H. Jiang",
                "K. Weng",
                "Y. Geng",
                "L Li"
            ],
            "title": "Yolov6: a single-stage object detection framework for industrial applications (arXiv preprint arXiv:2209.02976)",
            "year": 2022
        },
        {
            "authors": [
                "Lin",
                "T.-Y.",
                "P. Doll\u00e1r",
                "R. Girshick",
                "K. He",
                "B. Hariharan",
                "S. Belongie"
            ],
            "title": "Feature pyramid networks for object detection,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition",
            "venue": "2117\u20132125.",
            "year": 2017
        },
        {
            "authors": [
                "Lin",
                "T.-Y.",
                "P. Goyal",
                "R. Girshick",
                "K. He",
                "P. Doll\u00e1r"
            ],
            "title": "Focal loss for dense object detection,\u201d in Proceedings of the IEEE international conference on computer vision",
            "venue": "2980\u20132988.",
            "year": 2017
        },
        {
            "authors": [
                "W. Liu",
                "D. Anguelov",
                "D. Erhan",
                "C. Szegedy",
                "S. Reed",
                "Fu",
                "C.-Y"
            ],
            "title": "Ssd: single shot multibox detector,",
            "venue": "European Conference on Computer Vision (Amsterdam,",
            "year": 2016
        },
        {
            "authors": [
                "B. Liu",
                "Z. Hu",
                "Y. Zhao",
                "Y. Bai",
                "Y. andWang"
            ],
            "title": "Recognition of pyralidae insects using intelligent monitoring autonomous robot vehicle in natural farm scene (arXiv preprint arXiv:1903.10827)",
            "year": 2019
        },
        {
            "authors": [
                "F. Liu",
                "Z. Shen",
                "J. Zhang",
                "H. Yang"
            ],
            "title": "Automatic insect identification based on color characters",
            "venue": "Chin. Bull. Entomol. 45, 150\u2013153. doi: 10.1016/S1005-9040(08)60003-3",
            "year": 2008
        },
        {
            "authors": [
                "S.P. Mohanty",
                "D.P. Hughes",
                "M. Salath\u00e9"
            ],
            "title": "Using deep learning for image-based plant disease detection",
            "venue": "Front. Plant Sci. 7, 1419. doi: 10.3389/ fpls.2016.01419",
            "year": 2016
        },
        {
            "authors": [
                "L.G. Nachtigall",
                "R.M. Araujo",
                "G.R. Nachtigall"
            ],
            "title": "Classification of apple tree disorders using convolutional neural networks,\u201d in 2016 IEEE 28th International Conference on Tools with Artificial Intelligence (ICTAI) (IEEE)",
            "venue": "472\u2013476.",
            "year": 2016
        },
        {
            "authors": [
                "J. Redmon",
                "S. Divvala",
                "R. Girshick",
                "A. Farhadi"
            ],
            "title": "You only look once: unified, real-time object detection,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition",
            "venue": "779\u2013788.",
            "year": 2016
        },
        {
            "authors": [
                "J. Redmon",
                "A. Farhadi"
            ],
            "title": "Yolo9000: better, faster, stronger,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition",
            "venue": "7263\u20137271.",
            "year": 2017
        },
        {
            "authors": [
                "J. Redmon",
                "A. Farhadi"
            ],
            "title": "Yolov3: an incremental improvement (arXiv preprint arXiv:1804.02767)",
            "year": 2018
        },
        {
            "authors": [
                "S. Ren",
                "K. He",
                "R. Girshick",
                "J. Sun"
            ],
            "title": "Faster r-cnn: towards real-time object detection with region proposal networks",
            "venue": "Adv. Neural Inf. Process. Syst. 28, 1137\u20131149. doi: 10.1109/TPAMI.2016.2577031",
            "year": 2015
        },
        {
            "authors": [
                "M. Sammany",
                "T. Medhat"
            ],
            "title": "Dimensionality reduction using rough set approach for two neural networks-based applications,\u201d in Rough Sets and Intelligent Systems Paradigms",
            "venue": "(Warsaw, Poland: Springer), 639\u2013647.",
            "year": 2007
        },
        {
            "authors": [
                "Y. Sasaki",
                "T. Okamoto",
                "K. Imou",
                "T. Torii"
            ],
            "title": "Automatic diagnosis of plant disease-spectral reflectance of healthy and diseased leaves",
            "venue": "IFAC Proc. Volumes",
            "year": 1998
        },
        {
            "authors": [
                "S. Sladojevic",
                "M. Arsenovic",
                "A. Anderla",
                "D. Culibrk",
                "D. Stefanovic"
            ],
            "title": "Deep neural networks based recognition of plant diseases by leaf image classification",
            "venue": "Comput. Intell. Neurosci. 2016, 1\u201311. doi: 10.1155/2016/3289801",
            "year": 2016
        },
        {
            "authors": [
                "Z. Tian",
                "C. Shen",
                "H. Chen",
                "T. He"
            ],
            "title": "Fcos: fully convolutional one-stage object detection,\u201d in Proceedings of the IEEE/CVF international conference on computer vision",
            "venue": "9627\u20139636.",
            "year": 2019
        },
        {
            "authors": [
                "T. V\u0131zh\u00e1ny\u00f3",
                "J. Felf\u00f6ldi"
            ],
            "title": "Enhancing colour differences in images of diseased mushrooms",
            "venue": "Comput. Electron. Agric. 26, 187\u2013198. doi: 10.1016/S0168-1699(00)00071-5",
            "year": 2000
        },
        {
            "authors": [
                "Wang",
                "C.-Y",
                "A. Bochkovskiy",
                "Liao",
                "H.-Y. M"
            ],
            "title": "Yolov7: trainable bag-of-freebies sets new state-of-the-art for real-time object detectors (arXiv preprint arXiv:2207.02696)",
            "year": 2022
        },
        {
            "authors": [
                "J. Wang",
                "Y. Zhao",
                "Y. Wang",
                "W. Chen",
                "H. Li",
                "Y Han"
            ],
            "title": "Marked watershed algorithm combined with morphological preprocessing based segmentation of adherent spores,",
            "venue": "in International Conferences on Communications, Signal Processing, and Systems. (Springer),",
            "year": 2020
        },
        {
            "authors": [
                "X. Xie",
                "P. Zhou",
                "H. Li",
                "Z. Lin",
                "S. Yan"
            ],
            "title": "Adan: adaptive nesterov momentum algorithm for faster optimizing deep models (arXiv preprint arXiv:2208.06677)",
            "year": 2022
        },
        {
            "authors": [
                "L. Xu",
                "H. Mao",
                "L. Pingping"
            ],
            "title": "Study on color feature extraction of color images of vegetation-deficient leaves",
            "venue": "J. Agric. Eng. 4, 150\u2013154. doi: 10.3321/j.issn:10026819.2002.04.037",
            "year": 2002
        },
        {
            "authors": [
                "Y. Zhao",
                "Z. Hu"
            ],
            "title": "Segmentation of fruit with diseases in natural scenes based on logarithmic similarity constraint otsu",
            "venue": "Trans. Chin. Soc. Agric. Machinery 46, 9\u201315. doi: 10.6041/j.issn.1000-1298.2015.11.002",
            "year": 2015
        },
        {
            "authors": [
                "Y. Zhao",
                "Z. Hu",
                "Y. Bai",
                "F. Cao"
            ],
            "title": "An accurate segmentation approach for disease and pest based on drlse guided by texture difference",
            "venue": "Trans. Chin. Soc. Agric. Machinery 46, 14\u201319. doi: 10.6041/j.issn.1000-1298.2015.02.003",
            "year": 2015
        },
        {
            "authors": [
                "Y. Zhao",
                "S. Liu",
                "Z. Hu",
                "Y. Bai",
                "C. Shen",
                "X. Shi"
            ],
            "title": "Separate degree based otsu and signed similarity driven level set for segmenting and counting anthrax spores",
            "venue": "Comput. Electron. Agric . 169, 105230. doi : 10.1016/ j.compag.2020.105230",
            "year": 2020
        },
        {
            "authors": [
                "Y. Zhao",
                "Y. Wang",
                "J. Wang",
                "Z. Hu",
                "F. Lin",
                "M. Xu"
            ],
            "title": "Gmm and drlse based detection and segmentation of pests: a case study,\u201d in Proceedings of the 2019 4th International Conference on Multimedia Systems and Signal Processing",
            "venue": "62\u201366.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Zhao",
                "K. Wang",
                "B. Zhongying",
                "S. Li",
                "R. Xie",
                "S. Gao"
            ],
            "title": "Application of bayesian methods in image recognition of maize leaf diseases",
            "venue": "Comput. Eng. Appl. 5, 193\u2013195. doi: 10.3321/j.issn:1002-8331.2007.05.058",
            "year": 2007
        },
        {
            "authors": [
                "Zhu",
                "L.-Q.",
                "Z. Zhang"
            ],
            "title": "Using cart and llc for image recognition of lepidoptera",
            "venue": "Pan-Pacific Entomol. 89, 176\u2013186. doi: 10.3956/2013-08.1",
            "year": 2013
        },
        {
            "authors": [
                "L. Zhu",
                "Z Zhang"
            ],
            "title": "Automatic insect classification based on local mean colour feature and supported vector machines",
            "venue": "Oriental Insects",
            "year": 2012
        },
        {
            "authors": [
                "L. Zhu",
                "Z. Zhang",
                "P Zhang"
            ],
            "title": "Image identification of insects based on color histogram and dual tree complex wavelet transform (dtcwt)",
            "venue": "Acta Entomol. Sin",
            "year": 2010
        },
        {
            "authors": [
                "L. Zhu",
                "D. Zhang",
                "Z Zhang"
            ],
            "title": "Feature description of lepidopteran insect wing images based on wld and hoc and its application in species recognition",
            "venue": "Acta Entomol. Sin",
            "year": 2015
        },
        {
            "authors": [
                "L. Zhu",
                "D. Zhang",
                "Z Zhang"
            ],
            "title": "Recognition of lepidopteran species based on color name and opponentsift features",
            "venue": "Acta Entomol. Sin",
            "year": 2015
        }
    ],
    "sections": [
        {
            "text": "Frontiers in Plant Science"
        },
        {
            "heading": "OPEN ACCESS",
            "text": ""
        },
        {
            "heading": "EDITED BY",
            "text": "Mehedi Masud, Taif University, Saudi Arabia"
        },
        {
            "heading": "REVIEWED BY",
            "text": "Weifu Li, Huazhong Agricultural University, China Jun Wang, Tianjin University, China Tonglai Liu, Zhongkai University of Agriculture and Engineering, China\n*CORRESPONDENCE Zhuhua Hu\neagler_hu@hainanu.edu.cn\nRECEIVED 26 February 2023 ACCEPTED 02 May 2023 PUBLISHED 05 June 2023"
        },
        {
            "heading": "CITATION",
            "text": "Zhang C, Hu Z, Xu L and Zhao Y (2023) A YOLOv7 incorporating the Adan optimizer based corn pests identification method. Front. Plant Sci. 14:1174556. doi: 10.3389/fpls.2023.1174556"
        },
        {
            "heading": "COPYRIGHT",
            "text": "\u00a9 2023 Zhang, Hu, Xu and Zhao. This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.\nTYPE Original Research PUBLISHED 05 June 2023 DOI 10.3389/fpls.2023.1174556\nA YOLOv7 incorporating the Adan optimizer based corn pests identification method\nChong Zhang1, Zhuhua Hu1*, Lewei Xu1 and Yaochi Zhao2\n1School of Information and Communication Engineering, State Key Laboratory of Marine Resource Utilization in South China Sea, Hainan University, Haikou, China, 2School of Cyberspace Security, State Key Laboratory of Marine Resource Utilization in South China Sea, Hainan University, Haikou, China\nMajor pests of corn insects include corn borer, armyworm, bollworm, aphid, and corn leaf mites. Timely and accurate detection of these pests is crucial for effective pests control and scientific decision making. However, existing methods for identification based on traditional machine learning and neural networks are limited by high model training costs and low recognition accuracy. To address these problems, we proposed a YOLOv7 maize pests identification method incorporating the Adan optimizer. First, we selected three major corn pests, corn borer, armyworm and bollworm as research objects. Then, we collected and constructed a corn pests dataset by using data augmentation to address the problem of scarce corn pests data. Second, we chose the YOLOv7 network as the detection model, and we proposed to replace the original optimizer of YOLOv7 with the Adan optimizer for its high computational cost. The Adan optimizer can efficiently sense the surrounding gradient information in advance, allowing the model to escape sharp local minima. Thus, the robustness and accuracy of the model can be improved while significantly reducing the computing power. Finally, we did ablation experiments and compared the experiments with traditional methods and other common object detection networks. Theoretical analysis and experimental result show that the model incorporating with Adan optimizer only requires 1/2-2/3 of the computing power of the original network to obtain performance beyond that of the original network. The mAP@[.5:.95] (mean Average Precision) of the improved network reaches 96.69% and the precision reaches 99.95%. Meanwhile, the mAP@[.5:.95] was improved by 2.79%-11.83% compared to the original YOLOv7 and 41.98%- 60.61% compared to other common object detection models. In complex natural scenes, our proposed method is not only time-efficient and has higher recognition accuracy, reaching the level of SOTA.\nKEYWORDS\nYOLOv7, smart agriculture, object detection, deep learning, pests identification\nfrontiersin.org01"
        },
        {
            "heading": "1 Introduction",
            "text": "In the past decade, due to the excellent performance of machine learning and deep learning techniques on other tasks, scholars have applied them to crop pests and disease identification and have made good, progress in pests and disease identification. Scholars have applied them to crop pests and disease identification and made good progress. In 2010, Al-Bashish et al. (Al Bashish et al., 2010a). Introduced proposed the use of K-means clustering with HSI color space co-occurrence to extract color and texture features of plants, ultimately classifying five different plant diseases with a simple neural network. Since then, research works based on various machine learning methods to identify plant diseases and pests have emerged. In 2016, Sladojevic et al. (Al Bashish et al., 2010a). developed a new method for identifying 13 different plant diseases using deep convolutional neural networks, achieving a final accuracy of 96.3%. The authors created a comprehensive database and methodology for modeling, which is essential for future research in this field. Scholars have gradually realized the great potential of deep learning techniques, and research on pests and disease identification based on various deep learning methods has proliferated. For instance, Amara et al. (2017) identified and classified banana leaf diseases in the natural environment by using LeNet network, Nachtigall et al. (Bochkovskiy et al., 2020). used CNN to recognize diseases, nutrient deficiencies and herbicide damage in apple leaf images. Inspired by these previous works, our team conducted research on corn borer and anthracnose spore identification using different machine learning methods, all of which yielded promising result. However, these traditional machine learning and deep learning methods above still have their own limitations, such as high model training cost and poor robustness, which sharply increase the cost of academic research or industrial implementation. Therefore, it is important to find a pests identification method with low training cost, accurate identification and good robustness."
        },
        {
            "heading": "1.1 Related work and motivation",
            "text": "With the development of digital image processing and machine learning techniques, intelligent detection and identification of crop diseases and pests have become increasingly prevalent. In plant disease identification, Sasaki et al. (Girshick, 2015). utilized spectral reflectance differences to distinguish healthy and diseased areas on cucumber leaves, while V\u0131z\u0301ha\u0301nyo\u0301 et al. (Girshick et al., 2014). used color point differences to identify diseased mushrooms. In China, Guili Xu et al. (He et al., 2017). achieved over 70% accuracy in identifying tomato leaves based on histogram-based color feature extraction. Yuxia Zhao et al. (Li et al., 2022). used a Bayesian classifier to successfully identify five diseases, including maize rust. Our team has proposed several algorithms, such as the marker watershed algorithm (Lin et al., 2017a) and the Otsu separation and symbolic similarity-driven level set algorithm (Lin et al., 2017b), for accurate statistics of anthracnose spore distribution density on farms for better control. Additionally, our team proposed an accurate segmentation method for diseased fruits based on log\nFrontiers in Plant Science 02\nsimilarity-constrained Otsu and distance rule level set activity profile evolution (Liu et al., 2019), which can achieve good segmentation of diseased fruits.\nIn the field of plant pests identification, various methods have been proposed to improve the accuracy and efficiency of the identification process. However, most of these methods have limitations that need to be addressed. For instance, In terms of plant pests identification, Prof. Zorui Shen of China Agricultural University (Liu et al., 2008) firstly used mathematical morphology to solve the problem and achieved good result, but the variation of the selection of structural elements in mathematical morphology will affect the identification result, then it will cause the robustness of the identification algorithm is not strong. For insects\u2019 color characteristics, Dr. Zhu used color histogram and double-tree complex wavelet transform (Liu et al., 2016) and support vector machine (Mohanty et al., 2016) to further improve the recognition rate, but this method requires reliable data sets for training, so a large number of images need to be acquired and the cost is high. In addition, Dr. Zhu also proposed the color histogram combined with Weber descriptors for insect recognition of Lepidoptera (Nachtigall et al., 2016), CART-based combined with LLC (Redmon et al., 2016), and color-based combined with OpponentSIFT features (Redmon and Farhadi, 2017). However, these methods require manual extraction of features and are not applicable to borer moth family pests. To address these limitations, we propose an automatic pests monitoring robotic vehicle with a Pyralidae recognition scheme based on histogram and multi-template image reverse mapping method (Redmon and Farhadi, 2018). This new approach enables the automatic capture of pests images and achieves a recognition accuracy of up to 94.3% in the natural farm planting scenario. We also propose a pests image segmentation method based on GMM and DRLSE (Ren et al., 2015), which can automatically identify positive and negative samples of specific pests from a large number of scene images with recognition accuracy of up to 95%. Additionally, our proposed hybrid Gaussian model-based texture disparity representation and texture disparity-guided DRLSE model (Sammany and Medhat, 2007) can also achieve accurate segmentation of crop pests and diseases.\nWhile the traditional machine learning methods have contributed to the field of crop pests and disease identification, they have certain limitations that prevent them from achieving the desired result. The advancements in deep learning technology have paved the way for researchers to apply deep learning algorithms to pests recognition, resulting in significant progress in this field. Deep learning algorithms can automatically extract image features, making good use of this information to achieve high accuracy in pests and disease identification. Several studies have used deep learning techniques to identify and classify pests, achieving higher robustness, generalization, and accuracy. For example, Sammany et al. (Sasaki et al., 1998). utilized genetic algorithms to improve neural networks, reducing the dimensionality of feature vectors and improving pests recognition efficiency. Similarly, Al Bashish et al. (Sladojevic et al., 2016). used the K-means clustering algorithm to classify images into clusters, extracted feature values of color and texture for each cluster, and inputted them into neural networks for\nfrontiersin.org\nclassification. Mohanty et al. (Tian et al., 2019). used the GoogleNet convolutional neural network structure to build a pests identification model with satisfactory result. Compared to traditional machine learning methods, deep neural network-based pests recognition methods have better accuracy, making them an important research direction in pests recognition. As deep learning technology continues to advance, we can expect more breakthroughs in crop pests and disease identification, which will undoubtedly benefit the agriculture industry.\nDeep learning models have shown promising result in identifying and detecting pests. However, there are still limitations that need to be addressed. In recent years, various sophisticated training methods have been developed to improve the generalization and robustness of deep models. Nevertheless, the cost of training these models has increased significantly due to the higher computing power required. This increase in training cost has a considerable impact on the research and industrial implementations. One common approach to reduce the training time is to increase the batch size and assist parallel training. However, a larger batch size often leads to a decrease in performance. The YOLOv7 method (V\u0131zha\u0301nyo\u0301 and Felf\u00f6ldi, 2000), which is the current SOTA in object detection, also faces the same challenge. In this context, a new YOLOv7 corn pests identification method is proposed in this paper, which incorporates the Adan optimizer. This new method uses Adan (Wang et al., 2022), a novel optimizer that can sense the surrounding gradient information and efficiently escape from sharp local minimal areas. By replacing the original optimizer of YOLOv7 with Adan, the model can achieve faster and better training without compromising its accuracy. The proposed YOLOv7 method can identify major corn pests in complex natural environments quickly and accurately, reducing the cost of practical application of model. With fewer parameter updates, the deep model can achieve faster and more accurate identification, making it suitable for various applications. In summary, the YOLOv7 corn pests identification method incorporating the Adan optimizer presented in this paper can significantly reduce the training time and cost while maintaining the accuracy of the model. It is expected to contribute to the efficient and accurate identification of pests in agricultural production."
        },
        {
            "heading": "1.2 Contributions",
            "text": "Fron\n1. To address the lack of maize pests data, we used data\naugmentation techniques to construct a maize pests image dataset, which effectively improved the training of the model. 2. We replaced the original optimizer of YOLOv7 with a new\noptimizer, Adan, which combines a rewritten Nesterov momentum algorithm with an adaptive optimization algorithm and introduces decoupled weight decay, allowing the model to increase its speed without degrading its accuracy, thus enabling faster and better training of the model and reducing the cost of implementing the model.\ntiers in Plant Science 03\n3. From the theoretical analysis and experimental result, it can\nbe seen that the YOLOv7 network incorporating the Adan optimizer can effectively alleviate the negative impact caused by the increase of batch size, and solve the problem that the training speed and training accuracy cannot be achieved at the same time."
        },
        {
            "heading": "1.3 Paper organization",
            "text": "The rest of this paper is organized as follows. The second part Section 2 mainly introduces the related network model; In the third part Section 3, experimental scheme, process and results are introduced in detail; The fourth part Section 4 discusses the experimental results; The fifth part Section 5 summarizes the full text and puts forward the existing deficiencies and the direction that can be improved."
        },
        {
            "heading": "2 Materials and methods",
            "text": "This section first introduces the basic concepts of object detection network. Then it describes the YOLOv7 network and Adan optimizer used in this project, and finally introduces the proposed improved network."
        },
        {
            "heading": "2.1 Object detection network",
            "text": "Object detection is one of the core problems in the field of computer vision. It needs to find out all the objects of interest in an image, and determine their classes and locations. Object detection is always the most challenging problem in the field of computer vision because of the different appearances, shapes and poses of various objects, as well as the interference of illumination, occlusion and other factors during imaging. A diagram of the object detection task is shown in Figure 1.\nThe current popular algorithms can be divided into two categories, one is the two-stage algorithm based on Region Proposal, which find out some candidate regions primarily, and then adjust the regions for classification, such as the series of RCNN (Regions with CNN features) algorithm (Xu et al., 2002; Zhao and Hu, 2015; Wang et al., 2020; Xie et al., 2022). The other category is one-stage algorithm, such as SSD (Zhao et al., 2015) (Single Shot Multibox Detector), the series of YOLO (You Only Look Once) algorithm (V\u0131zha\u0301nyo\u0301 and Felf\u00f6ldi, 2000; Zhao et al., 2007; Zhu et al., 2015a; Zhu et al., 2015b; Zhao et al., 2019; Zhao et al., 2020), RetinaNet (Zhu et al., 2010), FCOS (Zhu et al., 2012) (Fully Convolutional One-Stage Object Detection) and other such side-to-side networks. They only use a convolutional neural network to directly predict classes and locations of different objects. Comparing the two categories of object detection algorithms, the former is more accurate but slower, while the latter is faster but less accurate. In this paper, some representative\nfrontiersin.org\nnetworks in the above two categories are selected for comparative experiments."
        },
        {
            "heading": "2.2 YOLOv7",
            "text": "YOLOv7 is a new network framework based on the series of YOLO algorithm, which mainly designs a better performance detect ion model through the fol lowing four aspects : backbone design with new ELAN module, composite model scaling, deep supervision label assignment strategy and model re-parameterization.\nThe first improvement is the design of new network structure. YOLOv7 proposes such a view: the shortest and longest gradient paths can be controlled to achieve more effective learning and convergence of deep networks. Based on this idea, YOLOv7 designs the E-ELAN network structure as shown in Figure 2 on the basis of ELAN. In common ELAN module, the whole network reaches a stable state regardless of the gradient path length and the number of computing modules. However, if more ELAN modules are stacked indefinitely, this stable state may be destroyed and the parameter utilization may be reduced. Based on the above shortcomings, YOLOv7 proposes the E-ELAN module. E-ELAN module adopts the structure of expand, shuffle and merge cardinality, and it can guide different computing blocks to learn more diversified characteristics compared to common ELAN module, thus improving the learning ability of the network without destroying the original gradient path.\nThe second improvement is composite model scaling. The main purpose of model scaling is to adjust certain properties of the model and generate models of different sizes to meet the needs of different inference speeds. If the E-ELAN method described above is applied directly to a cascaded model, the action of directly scaling up the depth of the model will result in a change in the scale of the input and output channels. As a result, the model\u2019s usage of hardware may decrease. Therefore, for the cascaded model, a composite model method must be proposed. The method must consider that the width of the transition layer should also be changed by the same amount when the depth of the computing module is scaled. Based on these ideas, YOLOv7 proposes a network architecture as shown in Figure 3. The network only needs to scale the depth in the\nFrontiers in Plant Science 04\ncomputation block when performing the model scaling, and the rest of the transport block will use the corresponding width scaling. The composite scaling method can preserve the properties of the model at the initial design and maintain the optimal structure.\nThe third improvement is deep supervision label assignment strategy. Deep supervision is a common technique in deep network training, it adds auxiliary head for loss calculation in the middle of the network to assist training. In order to differentiate auxiliary head for different functions, the final output head is called the Lead Head and the auxiliary training head is called the Aux Head. The core idea of deep supervision is to take shallow network weight and auxiliary loss as guidance, combine the output result with Ground\nA B\nFIGURE 1 Schematic diagram of object detection: (A) Original map. (B) Object detection map.\nfrontiersin.org\nTrue (GT), and use some calculation and optimization methods to generate reliable soft labels. For example, YOLO uses the bounding box regression and GT and the IOU of the prediction box as soft labels. The current common method of assigning soft labels to Aux Head and Lead Head is shown in the Figure 4A, which separates Aux Head and Lead Head, and uses their respective prediction result and GT to perform label assignment. In contrast, YOLOv7 network uses the Lead Head prediction result as a guide to generate coarse-to-fine hierarchical labels for Aux Heads and other Lead Heads learning. The two proposed deep supervision label assignment strategies are shown in Figures 4B, C. The reason for this is that the Lead Head has strong learning ability, and the generated soft labels should better represent the distribution and correlation between the source and target data. By allowing the shallow Aux Heads to directly learn the information that Lead Heads has already learned, the Lead Heads will be better able to focus on learning residual information that has not yet been learned.\nThe last improvement is model re-parameterization. Reparameterization is a technique used to improve a model after training, which increases the training time but improves the inference result. Although model re-parameterization has achieved excellent performance on VGG, when applied directly to architectures such as ResNet and DenseNet, it instead causes a significant decrease in accuracy. For these reasons, YOLOv7 uses the constant connection-free RepConvN to redesign the architecture of the reparameterized convolution by replacing the\nFrontiers in Plant Science 05\n3\u00d73 convolutional layers of the E-ELAN computational block with constant connection-free RepConv layers."
        },
        {
            "heading": "2.3 Adan optimizer",
            "text": "The most direct way to speed up the convergence of the optimizer is to import momentums. The deep model optimizers proposed in recent years all follow the same momentum paradigm used in Adam - the reball method. However, with the advent of ViT, researchers found that Adam was not able to train ViT. And AdamW, an improved version of Adam, gradually became the preferred choice for training ViT and even ConvNext. However, AdamW does not change the momentum paradigm in Adam, which tends to cause the performance of AdamW-trained networks to drop dramatically when the batch size increases to a certain threshold.\nIn the field of traditional convex optimization, there is an momentum algorithm equal to the heavy ball method, the Nesterov momentum algorithm. As shown in Equation 1.\nAGD : gk = \u2207f (qk \u2212 h(1 \u2212 b1)mk\u22121) + xk,mk\n= (1 \u2212 b1)mk\u22121 + gk, qk+1 = qk \u2212 hmk (1)\nThe Nesterov momentum algorithm has a faster theoretical convergence rate than the heavy ball method for smooth and generally convex problems, and can theoretically withstand larger\nA B C\nFIGURE 4 Deep supervision label assignment strategies: (A) Common strategy. (B, C) Two proposed strategies of YOLOv7. bold values means the better results.\nFIGURE 3 Composite model scaling for YOLOv7. bold values means the better results.\nfrontiersin.org\nbatch size. Different from the heavy ball method, Nesterov algorithm does not calculate the gradient at the current point, but uses the momentum to find an extrapolation point, and then carries on the momentum accumulation after calculating the gradient at the point. Although Nesterov momentum algorithm has some advantages, it is rarely applied and explored in depth optimizers. One of the main reasons is that Nesterov algorithm needs to calculate gradient at extrapolated points, which requires multiple overloading of model parameters during updating at current points and requires artificial back-propagation (BP) at extrapolated points. These inconveniences greatly limit the application of Nesterov momentum algorithm in depth model optimizer.\nIn order to give full play to the advantages of the Nesterov momentum algorithm, Adan researchers obtained the final Adan optimizer by combining the rewritten Nesterov momentum with the adaptive optimization algorithm and introducing decoupled weight attenuation. In order to solve the problem of multiple model parameter overloads in the Nesterov momentum algorithm, the researchers first rewrote the Nesterov momentum algorithm as shown in Equation 2.\nReformulated AGD : gk = Ez\u223cD\u00bd\u2207f (qk, z ) + xk mk = (1 \u2212 b1)mk\u22121 + \u00bdgk + (1 \u2212 b1)(gk \u2212 gk\u22121) qk+1 = qk \u2212 hmk 8>< >:\n(2)\nCombining the rewritten Nesterov momentum algorithm with the adaptive class optimizer - replacing the update of m_k from the cumulative form to the moving average form and using the secondorder moment to deflate the learning rate - has resulted in a basic version of Adan\u2019s algorithm. As shown in Equation 3.\nVanilla Adan :\nmk = (1 \u2212 b1)mk\u22121 + b1\u00bdgk + (1 \u2212 b1)(gk \u2212 gk\u22121) nk = (1 \u2212 b3)nk\u22121 + b3\u00bdgk + (1 \u2212 b1)(gk \u2212 gk\u22121) 2\nnk = h= ffiffiffiffiffiffiffiffiffiffiffiffi nk + \u03f5 p\nqk+1 = qk \u2212 hk \u2218mk\n8>>>>< >>>>:\n(3)\nAlthough it can be seen that the update of m_k combines the gradient with the gradient\u2019s difference, in real-world applications it is frequently necessary to treat the two physically distinct meaningful things separately. For this reason, the researchers developed the gradient difference momentum v_k, as shown in Equation 4.\nmk = (1 \u2212 b1)mk\u22121 + b1gk, vk = (1 \u2212 b2)vk\u22121 + b2(gk \u2212 gk\u22121) (4)\nHere different momentum/average coefficients are set for the momentum of the gradient and its difference. The gradient difference term can slow down the optimizer update when adjacent gradients are not consistent and, conversely, speed up the update when the gradients are in the same direction.\nBased on the idea of L2 regular decoupling, Adan introduces a weight attenuation strategy, each iteration of Adan can be regarded as minimizing some first-order approximation of the optimization objective F, as shown in Equation 5.\nFrontiers in Plant Science 06\nqk+1 = qk \u2212 hk \u2218 mk = argmin\u00a0 q (F(qk) + \u2329 mk, b \u2212 qk \u232a+ 1 2h jjq \u2212 qk j2 ffiffiffinkp ),\nwhere j xj jj2 ffiffiffinkp : = \u2329 x, ffiffiffiffiffiffiffiffiffiffiffiffi nk + \u03f5 p \u2218 x \u232a, mk : = mk + (1 \u2212 b2)vk\n(5)\nBecause L2 weight regularization in F is too simple and smooth, it is unnecessary to make a first-order approximation. Therefore, only the first-order approximation of training loss can be performed and L2 weight regularization can be ignored. Then the last iteration of Adan will become as shown in Equation 6.\nqk+1 = qk \u2212 hk \u2218 mk = argmin q F(qk) + mk, q \u2212 lqk + 1 2h jjq \u2212 qk j2 ffiffiffinkp (6)\nThe final Adan optimization algorithm can be obtained by combining the above two improvements Equation 4 and Equation 6 into the base version of Adan.\n2.4 The proposed identification method\nSince the network architecture is not changed, we still use the\noriginal network structure of YOLOv7, as shown in Figure 5.\nAfter replacing the optimizer inside YOLOv7 with Adan, the loss function module will calculate the loss of this forward inference according to the difference between the output of model and the real label. Subsequently, the model will take the derivative of loss to obtain the gradient of each learnable parameter. Then the Adan optimizer can obtain the gradient and update parameters through the optimization strategy described above, such as m_k, v_k, n_k, etc. The model keeps the loss decreasing by updating these parameters after each inference, thus gradually reducing the difference between the output of model and the real label, and finally achieving the convergence. The whole model training process is shown in Figure 6, and the pseudocode is shown in algorithm 1.\nInput: An image [H\u00d7W\u00d73]. Output: Detection image. Preprocessing: The input RGB image aligned to an RGB image of size 640\u00d7640. Training for every image in training set do\nStage 1: The processed images are input into\nthe backbone module for feature extraction, while the backbone module will output three feature maps in different scales. And these feature maps will be input into the head module together for prediction.\nStage 2: In the head module, three types of feature maps will be fused and input into\nRepVGG block and detect block to predict objects.\nStage 3: The loss function module will calculate the loss of this inference\nfrontiersin.org\nen\nFron\naccording to the difference between the output of model and the real label. Subsequently, the model will take the derivative of loss to get the gradient and pass it to the optimizer module. Stage 4: Adan Optimizer will initialize the following parameters: initialization \u03b80,\nstep size \u03b7, average parameter (\u03b21,\u03b22,\u03b23,\u03f5[0,1] 3), stable parameter, weight decays \u03f5>0 and restart condition \u03bb k>0, and then start the optimizing strategy.\nfor k<K do\ncompute the stochastic gradient estimator gk at \u03b8k;\nmk = (1 \u2212 b1) mk \u2212 1 + b1gk=*set m0 = g0*=\nvk = (1 \u2212 b2) vk \u2212 1 + b2(gk \u2212 gk \u2212 1)=*set v1 = g1 \u2212 g0*=;\nnk = (1 \u2212 b3) nk \u2212 1 + b3\u00bdgk + (1 \u2212 b2)(gk \u2212 gk \u2212 1) 2 nk = h=( ffiffiffiffiffiffiffiffiffiffiffiffi nk + \u03f5 p )\nqk + 1 = (1 + lkh) \u2212 1\u00bdqk \u2212 hk\u22121 \u2218 (mk + (1 \u2212 b2)vk) if restart condition holds then\nget stochastic gradient estimator g0 at \u03b8k +1; m0 = g0, v0 = 0, n0 = (g0) 2, update \u03b81 by Line 7, k = 1;\ntiers in Plant Science 07\nend\nend d\nALGORITHM 1 Description of the algorithm of YOLOv7 incorporating the Adan optimizer"
        },
        {
            "heading": "3 Experiments and result",
            "text": ""
        },
        {
            "heading": "3.1 Experimental scheme",
            "text": "The experimental scheme proposed is shown as Figure 7. We first pre-processed the original dataset, mainly including data recovery, data filtering and data filling. In order to solve the problem of scarce data, we used data augmentation and transfer learning to ensure that the network can fully learn the features. The two technologies will be introduced in detail in the following sections. And then, the augmented dataset was divided into training set, testing set and validation set. The training set and validation set was input into the original YOLOv7 network, the improved YOLOv7 network and other comparative networks respectively. If the performance of the model does not meet expectations, we will adjust the network\u2019s hyperparameters and retrain it. After that, the testing set was input into trained models to test the performance of different models. Finally, we compared and analyzed the experimental result.\nFIGURE 6 Flow chart of model training.\nFIGURE 5 Network structure diagram.\nfrontiersin.org"
        },
        {
            "heading": "3.2 Evaluation metrics",
            "text": "For binary classification problem, A is called \u201cpositive\u201d and B is called \u201cnegative,\u201d and the classifier correctly predicts \u201cTrue\u201d and incorrectly predicts \u201cFalse\u201d. According to these four basic combinations, the four basic elements of the confusion matrix are TP (True Positive), FN (False Negative), TN ((True Negative), and FP (False Positive), as shown in Table 1.\nIn object detection experiments, IoU, Precision, Recall, AP and mAP are commonly used as evaluation indexes. Among them, IoU represents the intersection ratio between the predicted result and the true label for each category, as shown in Eq.7. Precision refers to the proportion of data whose value is true indeed when the classifier predicts it to be true, while Recall refers to the percentage of the classifier predicts to be correct for all data that is true, respectively, the formulas of the two is Eq.8 and Eq.9. However, all three indexes have their limitations, therefore AP/mAP is often used to evaluate the performance of object detection task.\nIoU = TPTP+FP+FN (7)\nPrecision = TPTP+FP (8)\nRecall = TPTP+FN (9)\nIf we take different confidence levels, we can get different Precision and Recall, and if we get the confidence level dense enough, we will obtain the Precision-Recall curve(PR curve), as shown in Figure 8. While AP refers to the area under the curve, and\nFrontiers in Plant Science 08\nmAP is the average of the AP values for all classes. In particular, the mAP@[.5:.95] refers to the mAP at different IoU thresholds (from 0.5 to 0.95, in steps of 0.05)."
        },
        {
            "heading": "3.3 Dataset acquisition",
            "text": "Due to the scarcity of public corn pests dataset, we collected some images of three major corn pests: corn borer, bollworm, and armyworm on the web as our original dataset, including 31 images of corn borer, 36 images of bollworm, 31 images of armyworm and 31 negative images. Prior to beginning the experiment, we used data augmentation techniques to the technique expands a total of 129 images to 5160 images as our final dataset. During training, we use a ratio of 8:1:1 to split the dataset into a training set, a validation set and a testing set. And the training set has 4128 images, the validation set has 516 images and the testing set has 516 images.\nFIGURE 7 Flow chart of experimental scheme.\nTABLE 1 Confusion matrix.\nTruth Prediction\nT F\nP TP FN\nN FP TN\nfrontiersin.org"
        },
        {
            "heading": "3.4 Data augmentation",
            "text": "As deep learning requires a large amount of data for training, we used data augmentation on the original dataset, such as random rotation transform, blur transformation, flip transform, addition of Gaussian noise and so on. The random rotation and flip transformation models are able to simulate the different locations of insect presence, while the blur transformation and Gaussian noise could better simulate the various environment that may occur in reality. Figure 9 shows the images which performing data augmentation.\n1 https://github.com/xpwu95/IP102"
        },
        {
            "heading": "3.5 Transfer learning",
            "text": "Transfer learning is a popular method in the field of computer vision, because it can build accurate models in less time. By using transfer learning, model do not start training from scratch, but start with the patterns of solving problems that learned from previous problems. In the field of computer vision, transfer learning is usually represented by the use of pre-trained models. Pre-trained models are models that trained on large baseline datasets. For example, in object detection tasks, backbone neural network is first used for feature extraction. The backbone used here is generally a neural network such as VGG, ResNet, etc. Therefore, when training an object detection model, the parameters of the backbone can be initialized by using the pre-trained weights of these neural networks so that more effective features can be extracted at the beginning.\nFrontiers in Plant Science 09\nIn this paper, we selected the IP102 public dataset as a pretrained dataset1. The IP102 dataset is a large-scale dataset for pests identification, which contains more than 75,000 images of 102 pests classes. These images exhibit a natural long-tailed distribution. In addition, about 19,000 of these images have added bounding boxes for object detection. We select these images with object detection frames, and feed them into individual networks for training to obtain pre-trained weights. The pre-trained weights will be transferred to our own dataset for use, and it can make the final model more robust and convincing in the pests identification task."
        },
        {
            "heading": "3.6 Experimental environment and parameter settings",
            "text": "The experimental environment configuration of this paper is as follows: OS is Linux, GPUs are two Tesla V100 with 80G memories, training environment is python 3.7, Pytorch 1.11.0. while Labelme is used to annotate the data. In training, to ensure comparability across experiments and appropriateness of training, each training epoch consists of 100 rounds and the img_size is 320\u00d7320. In order to verify the good performance of our proposed algorithm under large batch size, we set the batch size to 512. While for training of YOLOv7, the weight_decay is 0.002 and learning rate is 0.001.\nfrontiersin.org"
        },
        {
            "heading": "3.7 Experiment result",
            "text": "Figure 10 shows the prediction performance of the YOLOv7 network incorporating with the Adan optimizer when facing different species of maize pests.\nIn order to verify the effectiveness of the algorithm proposed in this paper, we compared the improved network with the original network which using Adam, AdamW and SGD. We also tested several other object detection networks: SSD (Zhao et al., 2015), RetinaNet (Zhu et al., 2010), FCOS (Zhu et al., 2012), Faster RCNN (Xu et al., 2002) and FPN (Zhu and Zhang, 2013). Finally, we put these networks together and compared them with the result of our previous works, and the performance evaluation indexes are [mAP@.5:.95] and precision which are described above. The result is shown in Table 2.\nWe also compared the differences between the YOLOv7 network loaded with Adan and other networks when face with the same image. And the prediction result are shown in Figure 11."
        },
        {
            "heading": "4 Discussion",
            "text": "The experimental result in Table 2 shows that the YOLOv7 network incorporating the Adan outperforms traditional ML algorithms and other comparative networks in the comparison of both mAP@[.5:.95] and precision. Meanwhile, from Figure 10 we can see that the improved network has a good performance on different types of maize pests. What\u2019s more, further comparison of three different networks in Figure 11 shows that the YOLOv7 network incorporating Adan can still perform well in more complex natural environment with no errors. SSD network and the YOLOv7 network incorporating the Adam both have errors in prediction of the same images. The YOLOv7 network with the Adam misidentified the background as insects in two images, while SSD network misidentified insects as the background in both images. The final comparison of performance indexes and prediction result verifies that Adan optimizer can effectively improve the model performance and help the YOLOv7 network\nFIGURE 10 Test result: (A) bollworm. (B) armyworm. (C) corn borer.\nreduce the possibility of false recognition and missed recognition, thus making the network more efficient and error-free in pests recognition task. For further confirmation, we collected data of the map[.5:.95] and precision of YOLOv7 networks which using different optimizers in the experiment when the epoch changed, as shown in Tables 3, 4. Based on these data, we plotted the performance trends of four optimizers, as shown in Figure 12.\nFrom Figure 12 we can see that the YOLOv7 incorporating with Adan optimizer converges faster than YOLOv7 loaded with other optimizers in both mAP@[.5:.95] and precision, and the result are consistent with our theoretical analysis. In process of calculating momentums, Adan uses the modified Nesterov momentum algorithm, while Adam with AdamW use the traditional reballing algorithm. The modified Nesterov momentum algorithm helps Adan to sense the surrounding gradient information in advance, which helps model to escape from the sharp local minimal regions efficiently, thus speeding up the convergence of Adan. The comparison of map[.5:.95] and precision shows that Adan can obtain greater performance by using only 1/2-2/3 of the computation of other optimizers. What\u2019s more, the mAP@[.5:.95] increases by 2.79%-11.83% compared to original optimizers. The experimental result also confirm that Adan only needs less than 2/3\nFrontiers in Plant Science 11\nof computation of the original network to obtain the performance beyond it, which is proposed in the original paper of Adan."
        },
        {
            "heading": "5 Conclusions",
            "text": "In this paper, a new deep learning algorithm based on YOLOv7 network and Adan optimizer is proposed, and a feasible maize pests identification scheme is proposed as well, which is successfully applied to the identification task of maize pests. The mAP@[.5:.95] of the improved network reaches 96.69% and the precision reaches 99.95% in this task, which breaks the bottleneck of the original networks. And it also confirms the feasibility and effectiveness of applying deep convolutional neural networks to the task of crop pests and disease identification, and it has positive significance for crop pests and disease prevention and control. We can quickly identify common corn pests and take appropriate measures by using this model, and scientifically carryout pests control methods to reduce possible economic losses and promote agricultural modernization.\nHowever, the environment is more complex in real life. There are many other insects with similar characteristics, while the difficulty of detection in complex environment will be greatly increased due to the\nfrontiersin.org\nBA\nFIGURE 12 Comparison of precision changes with epoches for different optimizers. (A) map@[.5:.95] (B) precision.\nlimitations of scarce data. Meanwhile, some corn pests will appear in the form of eggs in real life, while these eggs are tiny and their characteristics are difficult to distinguish, making identification more difficult. What\u2019s worse, pests data are scarce and difficult to collect, and the cost of manual labeling is very high. Therefore, how to obtain sufficient data and enough computing power is the key of future pests controlling technology researches."
        },
        {
            "heading": "Data availability statement",
            "text": "The original contributions presented in the study are included in the article/supplementary material. Further inquiries can be directed to the corresponding author."
        },
        {
            "heading": "Author contributions",
            "text": "Conceptualization, ZH, YZ and CZ; methodology, CZ and LW; software, CZ and LW; validation, ZH, CZ, and LW; formal analysis, ZH, LW, and CZ; investigation, ZH and YZ; resources, ZH; data curation, CZ and LW; writing\u2014original draft preparation, CZ and LW; writing\u2014review and editing, ZH; visualization, CZ; supervision, ZH; project administration, ZH and YZ; funding acquisition, ZH and YZ. All authors contributed to the article and approved the submitted version.\nFrontiers in Plant Science 13"
        },
        {
            "heading": "Funding",
            "text": "This work was supported in part by the Key Research and Development Project of Hainan Province (Grant No.ZDYF2022GXJS348, Grant No.ZDYF2022SHFZ039) and the National Natural Science Foundation of China (Grant No.62161010, 61963012). The authors would like to thank the referees for their constructive suggestions."
        },
        {
            "heading": "Conflict of interest",
            "text": "The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest."
        },
        {
            "heading": "Publisher\u2019s note",
            "text": "All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.\nThe bold values means the better results.\nfrontiersin.org"
        }
    ],
    "title": "A YOLOv7 incorporating the Adan optimizer based corn pests identification method",
    "year": 2023
}