{
    "abstractText": "We consider the problem of estimating the mean of a sequence of random elements f(X1, \u03b8) , . . . , f(Xn, \u03b8) where f is a fixed scalar function, S = (X1, . . . , Xn) are independent random variables, and \u03b8 is a possibly S-dependent parameter. An example of such a problem would be to estimate the generalization error of a neural network trained on n examples where f is a loss function. Classically, this problem is approached through concentration inequalities holding uniformly over compact parameter sets of functions f , for example as in Rademacher or VC type analysis. However, in many problems, such inequalities often yield numerically vacuous estimates. Recently, the PAC-Bayes framework has been proposed as a better alternative for this class of problems for its ability to often give numerically non-vacuous bounds. In this paper, we show that we can do even better: we show how to refine the proof strategy of the PAC-Bayes bounds and achieve even tighter guarantees. Our approach is based on the coin-betting framework that derives the numerically tightest known time-uniform concentration inequalities from the regret guarantees of online gambling algorithms. In particular, we derive the first PAC-Bayes concentration inequality based on the coin-betting approach that holds simultaneously for all sample sizes. We demonstrate its tightness showing that by relaxing it we obtain a number of previous results in a closed form including Bernoulli-KL and empirical Bernstein inequalities. Finally, we propose an efficient algorithm to numerically calculate confidence sequences from our bound, which often generates nonvacuous confidence bounds even with one sample, unlike the state-of-the-art PAC-Bayes bounds.",
    "authors": [
        {
            "affiliations": [],
            "name": "Kyoungseok Jang"
        },
        {
            "affiliations": [],
            "name": "Kwang-Sung Jun"
        },
        {
            "affiliations": [],
            "name": "Ilja Kuzborskij"
        }
    ],
    "id": "SP:f1556f362ca959e1e982021c95e098df78029107",
    "references": [
        {
            "authors": [
                "P. Alquier"
            ],
            "title": "User-friendly introduction to PAC-Bayes bounds",
            "venue": "arXiv preprint arXiv:2110.11216,",
            "year": 2021
        },
        {
            "authors": [
                "P. Alquier",
                "J. Ridgway",
                "N. Chopin"
            ],
            "title": "On the properties of variational approximations of Gibbs posteriors",
            "venue": "Journal of Machine Learning Research,",
            "year": 2016
        },
        {
            "authors": [
                "A. Ambroladze",
                "E. Parrado-Hern\u00e1ndez",
                "J. Shawe-Taylor"
            ],
            "title": "Tighter PAC-Bayes bounds",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2006
        },
        {
            "authors": [
                "J.-Y. Audibert",
                "R. Munos",
                "Cs. Szepesv\u00e1ri"
            ],
            "title": "Tuning bandit algorithms in stochastic environments",
            "venue": "In Algorithmic Learning Theory (ALT),",
            "year": 2007
        },
        {
            "authors": [
                "P.L. Bartlett",
                "S. Mendelson"
            ],
            "title": "Rademacher and gaussian complexities: Risk bounds and structural results",
            "venue": "Journal of Machine Learning Research,",
            "year": 2002
        },
        {
            "authors": [
                "S. Boucheron",
                "G. Lugosi",
                "P. Massart"
            ],
            "title": "Concentration inequalities: A nonasymptotic theory of independence",
            "year": 2013
        },
        {
            "authors": [
                "O. Catoni"
            ],
            "title": "PAC-Bayesian Supervised Classification: The Thermodynamics of Statistical Learning",
            "venue": "IMS Lecture Notes-Monograph Series,",
            "year": 2007
        },
        {
            "authors": [
                "N. Cesa-Bianchi",
                "G. Lugosi"
            ],
            "title": "Prediction, learning, and games",
            "year": 2006
        },
        {
            "authors": [
                "T.M. Cover"
            ],
            "title": "Universal portfolios",
            "venue": "Mathematical Finance, pages",
            "year": 1991
        },
        {
            "authors": [
                "M.D. Donsker",
                "S.S. Varadhan"
            ],
            "title": "Asymptotic evaluation of certain Markov process expectations for large time",
            "venue": "Communications on Pure and Applied Mathematics,",
            "year": 1975
        },
        {
            "authors": [
                "P. Dupuis",
                "R.S.R.S. Ellis"
            ],
            "title": "A Weak Convergence Approach to the Theory of Large Deviations",
            "year": 1997
        },
        {
            "authors": [
                "G.K. Dziugaite",
                "D. Roy"
            ],
            "title": "Entropy-SGD optimizes the prior of a PAC-Bayes bound: Generalization properties of Entropy-SGD and data-dependent priors",
            "venue": "In International Conference on Machine Learing (ICML),",
            "year": 2018
        },
        {
            "authors": [
                "G.K. Dziugaite",
                "D.M. Roy"
            ],
            "title": "Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data",
            "venue": "In Uncertainty in Artificial Intelligence (UAI),",
            "year": 2017
        },
        {
            "authors": [
                "X. Fan",
                "I. Grama",
                "Q. Liu"
            ],
            "title": "Exponential inequalities for martingales with applications",
            "venue": "Electronic Journal of Probability,",
            "year": 2015
        },
        {
            "authors": [
                "P.D. Gr\u00fcnwald",
                "N.A. Mehta"
            ],
            "title": "A tight excess risk bound via a unified PAC-BayesianRademacher-Shtarkov-MDL complexity",
            "venue": "In Algorithmic Learning Theory (ALT),",
            "year": 2019
        },
        {
            "authors": [
                "M. Haddouche",
                "B. Guedj"
            ],
            "title": "PAC-Bayes with unbounded losses through supermartingales",
            "venue": "arXiv preprint arXiv:2210.00928,",
            "year": 2022
        },
        {
            "authors": [
                "K.-S. Jun",
                "F. Orabona"
            ],
            "title": "Parameter-free online convex optimization with sub-exponential noise",
            "venue": "In Proc. of the Conference on Learning Theory (COLT),",
            "year": 2019
        },
        {
            "authors": [
                "S.M. Kakade",
                "K. Sridharan",
                "A. Tewari"
            ],
            "title": "On the complexity of linear prediction: Risk bounds, margin bounds, and regularization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2008
        },
        {
            "authors": [
                "O. Kallenberg"
            ],
            "title": "Random Measures, Theory and Applications. Springer, 2017",
            "year": 2017
        },
        {
            "authors": [
                "J.L. Kelly",
                "jr"
            ],
            "title": "A new interpretation of information rate",
            "venue": "IRE Transactions on Information Theory,",
            "year": 1956
        },
        {
            "authors": [
                "I. Kuzborskij",
                "N. Cesa-Bianchi",
                "C. Szepesv\u00e1ri"
            ],
            "title": "Distribution-Dependent Analysis of GibbsERM Principle",
            "venue": "In Conference on Computational Learning Theory (COLT),",
            "year": 2019
        },
        {
            "authors": [
                "J. Langford",
                "R. Caruana"
            ],
            "title": "Not) bounding the true error",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2001
        },
        {
            "authors": [
                "B. London",
                "T. Sandler"
            ],
            "title": "Bayesian counterfactual risk minimization",
            "venue": "In International Conference on Machine Learing (ICML),",
            "year": 2019
        },
        {
            "authors": [
                "A. Maurer"
            ],
            "title": "A bound on the deviation probability for sums of non-negative random variables",
            "venue": "J. Inequalities in Pure and Applied Mathematics,",
            "year": 2003
        },
        {
            "authors": [
                "A. Maurer"
            ],
            "title": "A note on the PAC Bayesian theorem",
            "venue": "arXiv preprint arXiv:0411099,",
            "year": 2004
        },
        {
            "authors": [
                "A. Maurer",
                "M. Pontil"
            ],
            "title": "Empirical bernstein bounds and sample variance penalization",
            "venue": "In Conference on Computational Learning Theory (COLT),",
            "year": 2009
        },
        {
            "authors": [
                "D.A. McAllester"
            ],
            "title": "Some PAC-Bayesian theorems",
            "venue": "In Proceedings of the eleventh annual conference on Computational learning theory,",
            "year": 1998
        },
        {
            "authors": [
                "Z. Mhammedi",
                "P. Gr\u00fcnwald",
                "B. Guedj"
            ],
            "title": "PAC-Bayes un-expected Bernstein inequality",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "F. Orabona"
            ],
            "title": "A modern introduction to online learning",
            "venue": "arXiv preprint arXiv:1912.13213,",
            "year": 2019
        },
        {
            "authors": [
                "F. Orabona",
                "K.-S. Jun"
            ],
            "title": "Tight concentrations and confidence sequences from the regret of universal portfolio",
            "venue": "arXiv preprint arXiv:2110.14099,",
            "year": 2021
        },
        {
            "authors": [
                "F. Orabona",
                "D. P\u00e1l"
            ],
            "title": "Coin betting and parameter-free online learning",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2016
        },
        {
            "authors": [
                "M. P\u00e9rez-Ortiz",
                "O. Rivasplata",
                "J. Shawe-Taylor",
                "C. Szepesv\u00e1ri"
            ],
            "title": "Tighter risk certificates for neural networks",
            "venue": "Journal of Machine Learning Research,",
            "year": 2021
        },
        {
            "authors": [
                "M. Raginsky",
                "A. Rakhlin",
                "M. Telgarsky"
            ],
            "title": "Non-convex learning via stochastic gradient langevin dynamics: a nonasymptotic analysis",
            "venue": "In Conference on Computational Learning Theory (COLT),",
            "year": 2017
        },
        {
            "authors": [
                "A. Rakhlin",
                "K. Sridharan"
            ],
            "title": "On equivalence of martingale tail bounds and deterministic regret inequalities",
            "venue": "In Proc. of the Conference On Learning Theory (COLT),",
            "year": 2017
        },
        {
            "authors": [
                "R.E. Schapire"
            ],
            "title": "The strength of weak learnability",
            "venue": "Machine Learning,",
            "year": 1990
        },
        {
            "authors": [
                "M. Seeger"
            ],
            "title": "PAC-Bayesian generalisation error bounds for Gaussian process classification",
            "venue": "Journal of Machine Learning Research,",
            "year": 2002
        },
        {
            "authors": [
                "Y. Seldin",
                "F. Laviolette",
                "N. Cesa-Bianchi",
                "J. Shawe-Taylor",
                "P. Auer"
            ],
            "title": "PAC-Bayesian inequalities for martingales",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 2012
        },
        {
            "authors": [
                "G. Shafer",
                "V. Vovk"
            ],
            "title": "Probability and finance: it\u2019s only a game",
            "year": 2001
        },
        {
            "authors": [
                "S. Shalev-Shwartz",
                "O. Shamir",
                "N. Srebro",
                "K. Sridharan"
            ],
            "title": "Learnability, stability and uniform convergence",
            "venue": "Journal of Machine Learning Research,",
            "year": 2010
        },
        {
            "authors": [
                "I.O. Tolstikhin",
                "Y. Seldin"
            ],
            "title": "PAC-Bayes-empirical-Bernstein inequality",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2013
        },
        {
            "authors": [
                "J. Ville"
            ],
            "title": "\u00c9tude critique de la notion de collectif",
            "venue": "Gauthier-Villars, Paris,",
            "year": 1939
        },
        {
            "authors": [
                "M.J. Wainwright"
            ],
            "title": "High-dimensional statistics: A non-asymptotic viewpoint, volume 48",
            "year": 2019
        },
        {
            "authors": [
                "Y.-S. Wu",
                "Y. Seldin"
            ],
            "title": "Split-kl and PAC-Bayes-split-kl inequalities for ternary random variables",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "C. Zhang",
                "S. Bengio",
                "M. Hardt",
                "B. Recht",
                "O. Vinyals"
            ],
            "title": "Understanding deep learning requires rethinking generalization",
            "venue": "In 5th International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "W. Zhou",
                "V. Veitch",
                "M. Austern",
                "R.P. Adams",
                "P. Orbanz"
            ],
            "title": "Non-vacuous generalization bounds at the ImageNet scale: a PAC-Bayesian compression approach",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2019
        },
        {
            "authors": [
                "JANG JUN KUZBORSKIJ ORABONA A"
            ],
            "title": "Proof of Proposition 3 The proof is based on the following proposition: Proposition 11 ((Orabona and Jun",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Suppose that S = (X1, . . . , Xn) \u2208 X are random elements distributed identically and independently from each other, on a probability space (X ,\u03a3(X ), PX). For illustration, assume that X = R: A classical problem in probability and statistics is to quantify how quickly an average (X1 + \u00b7 \u00b7 \u00b7+Xn)/n converges to the mean E[X1], and over the decades this problem was successfully attacked under various assumptions on the probability space through concentration inequalities (Boucheron et al., 2013). The key assumption which enables these concentration inequalities to exhibit fast convergence to the mean is independence. However, in many learning-theoretic problems we are interested in the concentration of random elements which themselves depend on the sample S, and therefore are not independent. In this paper, we formalize the above by assuming\n* Authors are ordered alphabetically.\n\u00a9 K. Jang, K.-S. Jun, I. Kuzborskij & F. Orabona.\nar X\niv :2\n30 2.\n05 82\n9v 1\n[ cs\n.L G\n] 1\n2 Fe\nthat we are given a fixed measurable function f : \u0398 \u00d7 X \u2192 [0, 1], where \u0398 is a parameter space, and so now we are interested in the concentration of (f(\u03b8,X1)+\u00b7 \u00b7 \u00b7+f(\u03b8,Xn))/n around its mean, where \u03b8 is potentially S-dependent. For example, f(\u03b8,Xi) could be the loss incurred by a learning algorithm on the i-th example, where the parameters \u03b8 are generated based on the sample S. In the context of this example, the mean E[f(\u03b8,X1)] is called the statistical risk.\nTo this end, the classical approach to alleviating the dependence nuance is to derive uniform concentration inequalities that hold simultaneously for all parameters in a compact set \u0398. For example, consider the following concentration inequality that holds with probability at least 1\u2212 \u03b4,1\nsup \u03b8\u2208\u0398 \u2223\u2223\u2223\u2223\u2223\u2223 1n n\u2211 i=1 f(\u03b8,Xi)\u2212 E[f(\u03b8,X1)] \u2223\u2223\u2223\u2223\u2223\u2223 . \u221a capacity(\u0398) + ln 1\u03b4 n .\nHere the capacity term, such as VC dimension, metric entropy, or Rademacher complexity (Wainwright, 2019), scales with the \u201csize\u201d of the set \u0398. Mentioned notions accurately capture the capacity in many learning problems, such as with linear parameterizations (Bartlett and Mendelson, 2002; Kakade et al., 2008). However, in some other problems, e.g., in learning with overparameterized neural networks, the pessimistic nature of uniform bounds makes them vacuous (Zhang et al., 2017).\nOn the other hand, in recent years, there has been a strong interest in the alternative to the uniform bounds, based on the PAC-Bayes analysis (McAllester, 1998), which, remarkably, on some instances demonstrates non-vacuous bounds for the generalization ability of deep learning algorithms (Dziugaite and Roy, 2017; Pe\u0301rez-Ortiz et al., 2021; Zhou et al., 2019). In the PAC-Bayes analysis instead of taking sup\u03b8\u2208\u0398 as in the uniform approach, we assume that the parameters \u03b8 are now random and follow a data-dependent, so-called, posterior distribution Pn. In this paper, we are interested in estimating an expected mean \u222b \u00b5\u03b8 dPn(\u03b8) with \u00b5\u03b8 := E[f(X1, \u03b8) | \u03b8], uniformly over all data-dependent posteriors Pn. This setting covers the one considered in the PAC-Bayes literature (Alquier, 2021) where usually the function f represents the loss of a predictor parameterized by \u03b8 from a parameter space \u0398. In this view, we can think of \u00b5\u03b8 as the risk of a predictor parameterized by \u03b8, while \u222b \u00b5\u03b8 dPn(\u03b8) is the risk of a randomized predictor that uses a random \u03b8 drawn from the distribution Pn. The second important component of the PAC-Bayes model is a prior distribution P0 which does not depend on S and captures our prior belief about the inductive bias in the problem instance. A basic PAC-Bayes concentration inequality (McAllester, 1998) then takes the form of\u2223\u2223\u2223\u2223\u2223 \u222b 1 n n\u2211 i=1 f(\u03b8,Xi) dPn \u2212 \u222b \u00b5\u03b8 dPn(\u03b8) \u2223\u2223\u2223\u2223\u2223 . \u221a KL(Pn\u2016P0) + ln 1\u03b4 n with probability at least 1\u2212 \u03b4, where instead of the capacity term we have a Kullback-Liebler (KL) divergence between the posterior and the prior. Numerically speaking, PAC-Bayes bounds tend to give much tighter bounds than their uniform counterparts, largely because the KL term is typically smaller than the capacity term (such as VC dimension), for an appropriate (user\u2019s) choice of Pn, P0.\nOur contributions: PAC-Bayes meets coin-betting In this paper, we show that we can obtain even tighter PAC-Bayes bounds using recent advances in the theory of concentration inequalities through gambling algorithms (Orabona and Jun, 2021). In particular, we show that it is possible to obtain a new coin-betting based PAC-Bayes bound that directly implies a number of previous results. Moreover, numerically evaluating this new upper bound, we show that it is numerically tighter than all previous approaches. In Section 5 we present our main result, a concentration inequality of\n1. The notation . hides universal constants and logarithmic factors.\nthe following form, which holds with probability at least 1 \u2212 \u03b4, simultaneously for all n \u2208 N, all data-dependent posteriors Pn, and all data-free priors P0:\u222b\nmax \u03bb\u2208[\u2212 1\n1\u2212\u00b5\u03b8 , 1 \u00b5\u03b8 ] n\u2211 i=1 ln ( 1 + \u03bb(f(\u03b8,Xi)\u2212 \u00b5\u03b8) ) dPn(\u03b8) \u2264 KL(Pn\u2016P0) + ln c \u221a n \u03b4 . (1)\nMoreover, the confidence interval for \u222b \u00b5\u03b8 dPn(\u03b8) is obtained by solving the optimization problems[\nmin (\u00b5\u03b8)\u03b8\u2208\u0398\u2208M\n\u222b \u00b5\u03b8 dPn(\u03b8), max\n(\u00b5\u03b8)\u03b8\u2208\u0398\u2208M\n\u222b \u00b5\u03b8 dPn(\u03b8) ] , (2)\nwhere M is a class of mean functions (\u00b5\u03b8)\u03b8\u2208\u0398 that satisfy the constraint given by Eq. (1), which is the first of its kind in the PAC-Bayes literature. We formalize these optimization problems in Section 5.1 and show that the constraint is convex, making them efficiently solvable in some cases. In Section 6 we also experimentally validate our approach.\nIn addition, we show that Eq. (1) is tighter than some well-known PAC-Bayes inequalities, such as McAllester\u2019s inequality (McAllester, 1998), Maurer\u2019s inequality for Bernoulli KL divergence (Maurer, 2004), and PAC-Bayes empirical Bernstein\u2019s inequality (Tolstikhin and Seldin, 2013). This is done by relaxing inequality in Eq. (1) by simple lower bounds of the logarithmic term. We show that even relaxing Eq. (1) leads to a tighter bound than Maurer\u2019s one that is known to be very tight numerically.\nThe observation above implies that from our result we can derive all these versions, obtain bounds, and then take an intersection of all of them without having to split \u03b4. This is in stark contrast to empirical Bernstein\u2019s bounds (Tolstikhin and Seldin, 2013) that are often numerically looser than KL bounds, while being orderwise tighter than KL bounds like Maurer\u2019s one. Attempting to take an intersection with KL bounds requires splitting \u03b4, which undesirably inflates the bound. This is not the case for our method \u2013 our result can be seen as \u201cthe right\u201d type of concentration inequality that is superior to the rest up to constant factors inside an additive logarithmic term. Finally, Eq. (1) (and all its corollaries) holds simultaneously for all sample sizes, delivering time-uniform PAC-Bayes confidence sequences.\nOrganization of the paper After a discussion of related work (Section 2) and notations (Section 3), in Section 4 we briefly present the idea behind concentration through coin-betting. In Section 5 we present our main results, discuss some implications, and include the proof of a new concentration inequality in Section 5.2. In Section 5.1, we discuss how to compute our concentration inequality numerically (without any relaxations). Finally, in Section 6 we present numerical simulations comparing our inequality to a number of baselines from the PAC-Bayes literature."
        },
        {
            "heading": "2. Related Work",
            "text": "Concentration from coin-betting The coin-betting formalism considered here (see Section 4) goes back to Ville (1939) and Kelly betting system (Kelly, 1956) and has an intimate connection to the Universal Portfolio theory (Cover, 1991). Building on the ideas of Ville (1939), Shafer and Vovk (2001) introduced a general framework aiming at giving a foundation to the theory of probability rooted in gambling strategies. However, their framework is very general and it does not suggest specific methods to construct the betting strategies. The first paper to introduce the idea of using the regret of online betting algorithms to produce new concentration inequalities was in Jun and\nOrabona (2019), which in turn builds on Rakhlin and Sridharan (2017) that showed the equivalence between the regret guarantees of generic online linear algorithms and martingale tail bounds.\nPAC-Bayes Since the introduction of PAC-Bayes bounds by McAllester (1998), there has been significant growth and development in both theory and applications; see Alquier (2021) for a comprehensive survey. Early papers focused on tightening the bound of McAllester (1998), which can be seen as a PAC-Bayes version of Hoeffding\u2019s inequality. In particular, Langford and Caruana (2001); Seeger (2002); Maurer (2004) focused on the setting of a binary classification where the goal is to bound KL divergence between Bernoulli distributions. Such bounds are tighter than the mere difference of the risk and empirical risk due to Pinsker\u2019s inequality, and the numerically tightest known inequality within this group is Maurer\u2019s inequality (Maurer, 2004) (see for instance experiments of Mhammedi et al. (2019)). In this paper, we recover the result of Maurer (2004) by relaxing Eq. (1).\nTowards data-dependent bounds, Tolstikhin and Seldin (2013) adapted an empirical Bernstein\u2019s inequality (Audibert et al., 2007; Maurer and Pontil, 2009) to the PAC-Bayes setting, making generalization bounds variance dependent. Once again, we recover the PAC-Bayes empirical Bernstein\u2019s inequality by relaxing our main result of Eq. (1) without any plug-in arguments, through a relatively straightforward proof. Several works went further in making bounds data-dependent by manipulating the KL term. Ambroladze et al. (2006) explored the idea of splitting the sample and deriving the prior from a held-out sample while obtaining the posterior from the remaining part. This technique proved very fruitful in making PAC-Bayes bounds much tighter. Indeed, recent non-vacuous generalization bounds for deep neural networks are largely attributed to this technique (Dziugaite and Roy, 2018; Pe\u0301rez-Ortiz et al., 2021). Clearly, the results developed in this paper can be readily applied together with the splitting technique. The splitting technique was also investigated beyond the KL term. In particular, Mhammedi et al. (2019); Wu and Seldin (2022) developed intricate bounds akin to empirical Bernstein\u2019s inequalities where the splitting is done with respect to the sample variance (two variance terms) in addition to the KL term. These are among the numerically tightest known PAC-Bayes bounds. However, due to their highly problem-dependent nature, it is challenging to compare these bounds theoretically.\nThe proof of our main result relies on showing that the exponential moment of the optimal log-wealth with respect to \u03b8 is a martingale. Several papers in PAC-Bayes literature have shown results exploiting (super-)martingale concentration, which allowed them to relax the independence assumption in the data sequence (Seldin et al., 2012) or to replace unboundedness of f() by weaker assumptions (Kuzborskij and Szepesva\u0301ri, 2019; Haddouche and Guedj, 2022). To this end, Haddouche and Guedj (2022) exploited Ville\u2019s inequality (as in our proof), which allowed them to show a bound that holds uniformly over n \u2208 N.\nFinally, it is known that solving the classical PAC-Bayes bound of McAllester (1998) for the posterior results in a Gibbs posterior Pn(\u03b8) \u221d e\u2212 1 n \u2211 i f(\u03b8,Xi) dP0(\u03b8). A large body of literature has looked at learning-theoretic properties of Gibbs predictors (Catoni, 2007; Alquier et al., 2016; Raginsky et al., 2017; Kuzborskij et al., 2019; Gru\u0308nwald and Mehta, 2019). The concentration inequality we develop here (Eq. (1)) is of a very different shape compared to (McAllester, 1998), though it can be easily relaxed to obtain it. As such, the Gibbs predictor might be a suboptimal solution to Eq. (1), and it is an interesting open problem to characterize such a solution."
        },
        {
            "heading": "3. Definitions",
            "text": "We denote by (x)+ = max{x, 0}. If P and Q are probability measures over \u0398 such that P Q, the KL divergence between P and Q is defined as KL(P,Q) := \u222b P (dx) ln dPdQ(x). With a slight abuse of notation, we also write KL(p\u2016q) where p = dP/ d\u03bb and q = dQ/d\u03bb are densities of P and Q with respect to some common \u03c3-finite measure \u03bb. If a set X is uniquely equipped with a \u03c3-algebra, the underlying \u03c3-algebra will be denoted by \u03a3(X ). We formalize a \u201cdata-dependent distribution\u201d through the notion of a probability kernel (see, e.g., Kallenberg, 2017) which is defined as a map K : X n \u00d7 \u03a3(\u0398) \u2192 [0, 1] such that for each B \u2208 \u03a3(\u0398) the function s 7\u2192 K(s,B) is measurable and for each s \u2208 X n the function B 7\u2192 K(s,B) is a probability measure over \u0398. We write K(X n,\u0398) to denote the set of all probability kernels from X n to distributions over \u0398. In that light, when P \u2208 K(X n,\u0398) is evaluated on S \u2208 X n we use the shorthand notation Pn = P (S)."
        },
        {
            "heading": "4. Warm-up: From Betting To Concentrations",
            "text": "In this section, we briefly explain how to obtain new concentration inequalities from betting algorithms, following Orabona and Pa\u0301l (2016); Rakhlin and Sridharan (2017); Jun and Orabona (2019).\nLet ct \u2208 [\u22121, 1] be a sequence of \u201ccontinuous coin\u201d outcomes chosen arbitrarily. In each round, the bettor bets |xt| money on the outcome SGN(xt). Then, ct is revealed and the bettor wins/loses xtct money. Define the initial wealth Wealth0 := 1 and the wealth at the end of round t as\nWealtht := Wealtht\u22121 +ctxt = 1 + t\u2211 s=1 csxi .\nWe also assume that the algorithm guarantees Wealtht \u2265 0, hence we must have xt \u2208 [\u2212Wealtht\u22121, Wealtht\u22121]. Given that no assumptions are made on how ct is generated, this is essentially an online game (Cesa-Bianchi and Lugosi, 2006; Orabona, 2019). So, our aim is to achieve an amount of money close to the one of a fixed comparator. In particular, let Wealtht(\u03bb) be the wealth obtained by a bettor that bets \u03bbWealtht\u22121(\u03bb) in round t with initial wealth equal to Wealth0(\u03bb) := 1 and\nWealtht(\u03bb) := Wealtht\u22121(\u03bb) + ct\u03bbWealtht\u22121(\u03bb) = t\u220f s=1 (1 + cs\u03bb) .\nWe can now formally define the regret of the betting algorithm as\nRegretT := max\u03bb\u2208[\u22121,1] WealthT (\u03bb)\nWealthT .\nIt is well-known that it is possible to design optimal online betting algorithms where the regret is polynomial in T (Cesa-Bianchi and Lugosi, 2006, Chapters 9 and 10).\nClosed form concentration, following Rakhlin and Sridharan (2017) Here, we summarize the basic idea of Rakhlin and Sridharan (2017) used to obtain concentration inequalities from online learning algorithms, specializing it to online betting algorithms as in Jun and Orabona (2019).\nConsiderXt to be a sequence of i.i.d. random variables supported on [0, 1] such that E[Xt] = \u00b5. Set ct = Xt\u2212\u00b5 \u2208 [\u22121, 1], so that regardless of the online betting algorithm we have E[Wealtht] = 1. Also, assume that RegretT \u2264 R(T ), where R : N \u2192 R+. Let\u2019s now lower bound WealthT (\u03bb)\nto obtain a familiar quantity. Using the inequality 1 + x \u2265 exp(x\u2212 x2) for x \u2265 \u22120.68, we obtain\nmax \u03bb\u2208[\u22121,1] WealthT (\u03bb) \u2265 max \u03bb\u2208[\u22121/2,1/2] WealthT (\u03bb) = max \u03bb\u2208[\u22121/2,1/2] T\u220f t=1 (1 + \u03bb(Xt \u2212 \u00b5))\n\u2265 max \u03bb\u2208[\u22121/2,1/2] exp \u03bb T\u2211 t=1 (Xt \u2212 \u00b5\u2212 \u03bb(Xt \u2212 \u00b5)2)  \u2265 max \u03bb\u2208[\u22121/2,1/2] exp \u03bb T\u2211 t=1 (Xt \u2212 \u00b5\u2212 \u03bb)  . Putting it all together and using Markov\u2019s inequality, for any \u03bb \u2208 [\u22121/2, 1/2] we get\nP \u03bb T\u2211 t=1 (Xt \u2212 \u00b5\u2212 \u03bb) \u2265 ln R(T ) \u03b4  \u2264 P { WealthT (\u03bb) \u2265 R(T ) \u03b4 } \u2264 P { WealthT \u2265 1 \u03b4 } \u2264 \u03b4.\nChoosing \u03bb with the proper sign and of the order of 1\u221a T , we get roughly Hoeffding inequality when the Regret is O(1), which is possible for fixed T and for this specific lower bound to the optimal wealth. Even better concentrations can be obtained carrying around the (Xt \u2212 \u00b5)2 terms, resulting in an empirical Bernstein-style bound.\nIt is important to stress that we do not need to run the betting algorithm to obtain the concentration. Instead, we only need the existence of a betting algorithm and its associated regret guarantee.\nTighter concentration inequalities From the above reasoning, it should be clear that we can obtain a tighter bound by giving up the closed-form expression by avoiding to lower bound the wealth:\nP  max\u03bb\u2208[\u22121,1] T\u2211 t=1 ln(1 + \u03bb(Xt \u2212 \u00b5)) \u2265 ln R(T ) \u03b4  = P { max \u03bb\u2208[\u22121,1] WealthT (\u03bb) \u2265 R(T ) \u03b4 } \u2264 \u03b4 .\nIn this case, we can numerically invert this inequality and obtain a tighter concentration. Now, we depart from Rakhlin and Sridharan (2017) and, instead of using Markov\u2019s inequality, we follow Jun and Orabona (2019) using Ville\u2019s inequality (Theorem 9). We can do it because, by the assumptions on the betting algorithm, the wealth is a non-negative martingale. The use of Ville\u2019s inequality gives the uniformity over time for free and gives us a high-probability timeuniform concentration inequality. Namely, with probability at least 1\u2212 \u03b4, we have\nmax t max \u03bb\u2208[\u22121,1] t\u2211 s=1 ln(1 + \u03bb(Xs \u2212 \u00b5)) \u2264 ln R(t) \u03b4 . (3)\nNote that to obtain upper and lower bounds for \u00b5 it is enough to find the set of values of \u00b5 that satisfies Eq. (3). This can be done efficiently because the argument of the max can be proved to be a quasi-convex one-dimensional function in \u00b5 (Orabona and Jun, 2021). The concentration inequality above can be seen as a tight and implicit version of the empirical Bernstein\u2019s inequality for bounded random variables, just like how the KL-divergence concentration inequality is an implicit and tight version of the Bernstein\u2019s inequality for Bernoulli random variables."
        },
        {
            "heading": "5. Main Results",
            "text": "The concentration inequality of Eq. (3) holds for i.i.d. random variables S = (X1, . . . , Xn). However, in many learning-theoretic applications, we are interested in providing confidence intervals for the mean of some data-dependent function (such as the generalization error). To this end, in this sec-\ntion, we explore a scenario where X1, . . . , Xn are replaced by a sequence f(\u03b8,X1), . . . , f(\u03b8,Xn) such that f is a fixed scalar function and \u03b8 is a data-dependent parameter. Clearly, elements of such a sequence are not independent, since dependence is introduced through parameter \u03b8. Following the PAC-Bayes viewpoint (McAllester, 1998; Alquier, 2021), \u03b8 is now random and distributed according to some user-chosen data-dependent distribution Pn called posterior. In addition, unlike in the traditional PAC-Bayes literature, our ours hold uniformly not only in Pn, but also in the sample size n. Thus, we construct a high-probability PAC-Bayes confidence sequence.\nThe next theorem, proved in Section 5.2, is the main result of our paper, which generalizes the concentration analysis of Section 4 to the PAC-Bayes setting.\nTheorem 1 Let S = (X1, . . . , Xn) be a tuple of i.i.d. random variables taking values in some measurable space X . Let Pn be a data-dependent distribution over some measurable space \u0398 and let P0 be any probability measure over \u0398 independent from sample S. Let f : \u0398 \u00d7 X \u2192 [0, 1] be any fixed measurable function, let its mean be denoted by \u00b5\u03b8 = E[f(\u03b8,X1)], and introduce\n\u03c8?n(\u03b8, \u00b5\u03b8) := max \u03bb\u2208[\u2212 1\n1\u2212\u00b5\u03b8 , 1 \u00b5\u03b8 ] n\u2211 i=1 ln ( 1 + \u03bb(f(\u03b8,Xi)\u2212 \u00b5\u03b8) ) , (\u03b8 \u2208 \u0398, \u00b5\u03b8 \u2208 [0, 1]) .\nThen, for all P0, with probability at least 1\u2212 \u03b4 for any \u03b4 \u2208 (0, 1], we have2\nP { \u2203n \u2208 N , \u2203Pn : \u222b \u03c8?n(\u03b8, \u00b5\u03b8) dPn(\u03b8)\u2212 KL(Pn\u2016P0)\u2212 ln \u221a \u03c0 \u0393(n+ 1)\n\u0393(n+ 12) \u2265 ln 1 \u03b4\n} \u2264 \u03b4 . (4)\nNow we discuss some of the implications of Eq. (4) and compare it to existing PAC-Bayes results. The important feature of Eq. (4) is that it holds simultaneously for all posterior distributions, so we can freely choose the one that depends on the data. At the same time, Eq. (4) is similar in shape to the concentration inequality of Eq. (3). In particular, \u03c8?n is an optimal log-wealth discussed in Section 4, while \u0393(n + 1)/\u0393(n + 1/2) \u223c \u221a n is the regret bound (R(n)) of a certain betting algorithm. Observe that unlike Eq. (3), the left-hand side of the inequality is now integrated over \u03b8 \u223c Pn, and the term KL(Pn\u2016P0) appears on the right-hand side. In particular, the term KL(Pn\u2016P0) captures the capacity of the class of posterior distributions with respect to the prior P0, and it is a standard component in PAC-Bayes analyses.\nObtaining known PAC-Bayes inequalities by relaxing Eq. (4) By relaxing Eq. (4), we demonstrate that Theorem 1 gives a tighter concentration inequality compared to some inequalities in PACBayes literature (proofs are deferred to Section A). Importantly, our results extend these bounds as our relaxations hold uniformly over n \u2208 N, whereas previous results hold for a fixed n. Abbreviate\n\u00b5\u0302\u03b8 := 1\nn n\u2211 i=1 f(\u03b8,Xi) and Cn := KL(Pn\u2016P0) + ln \u221a \u03c0 \u0393(n+ 1) \u0393(n+ 12) .\nAs a basic sanity-check, we first recover a classical result of McAllester (1998) through the elementary inequality ln(1 + x) \u2265 x\u2212 x2 for x \u2265 \u22120.68 (similarly as in Section 4), proof in Section A.1.\nProposition 2 (McAllester\u2019s inequality) Set \u03b4 \u2208 (0, 1]. Under conditions of Theorem 1, for all priors P0, with probability at least 1\u2212\u03b4 over the sample S, for all n \u2208 N and for all data-dependent\n2. Here \u2203n \u2208 N , \u2203Pn is a shorthand notation for \u2203n \u2208 N ,\u2203P \u2208 K(Xn,\u0398) where K() is a set of probability kernels as defined in Section 3.\ndistributions Pn simultaneously we have\u2223\u2223\u2223\u2223\u222b \u00b5\u03b8 dPn(\u03b8)\u2212 \u222b \u00b5\u0302\u03b8 dPn(\u03b8)\u2223\u2223\u2223\u2223 \u2264 2 \u221a Cn + ln 1\u03b4\nn .\nNote that, up to constants, the above matches the result of McAllester (1998), and extends it \u2014 now the bound holds simultaneously for all n \u2208 N.\nNow we turn our attention to a type of PAC-Bayes inequality, where we the bound is given on a KL divergence between Bernoulli distributions. Such bounds are useful in a setting of a binary classification, where the parameter of a Bernoulli distribution models a conditional probability of a positive class label. In particular, relaxing Eq. (4) gets a well-known inequality of Maurer (2004):\nProposition 3 (Maurer\u2019s inequality) For p, q \u2208 [0, 1] let kl(p, q) := p ln(p/q)\u2212 (1\u2212 p) ln((1\u2212 p)/(1\u2212q)), i.e., the KL divergence between Bernoulli distributions with parameters p and q respectively. Set \u03b4 \u2208 (0, 1]. Under the conditions of Theorem 1, for all priors P0, with probability at least 1\u2212 \u03b4 over the sample S, for all n \u2208 N and for all data-dependent distributions Pn simultaneously,\nkl (\u222b\n\u00b5\u0302\u03b8 dPn(\u03b8), \u222b \u00b5\u03b8 dPn(\u03b8) ) \u2264 Cn + ln 1\u03b4\nn .\nThe above inequality matches Maurer\u2019s bound up to a constant inside a logarithmic factor. Furthermore, the proof of Proposition 3 in Section A.2 reveals that even relaxing Theorem 1 to have\u222b kl(\u00b5\u0302, \u00b5\u03b8) dPn(\u03b8) in place of \u03c8?n(\u03b8, \u00b5\u03b8) on the LHS results in a bound that is tighter than Maurer\u2019s inequality. We confirm this numerically in Section 6. We now consider a more sophisticated, sample variance-dependent concentration inequality, which exhibits a faster rate of order 1/n whenever the sample variance is sufficiently small. In the non-PAC Bayes form, such a empirical Bernstein\u2019s inequality was shown by Audibert et al. (2007); Maurer and Pontil (2009), whereas the PAC-Bayes version was first presented by Tolstikhin and Seldin (2013). The following result recovers their result up to constants through a much simpler proof by relaxing Theorem 1:\nProposition 4 (PAC-Bayes empirical Bernstein\u2019s inequality) Set \u03b4 \u2208 (0, 1]. Introduce\nV\u0302 (Pn) := 1\nn \u222b n\u2211 i=1 (f(\u03b8,Xi)\u2212 \u00b5\u0302\u03b8)2 dPn(\u03b8) , Cn,\u03b4 := Cn + ln 1 \u03b4 .\nUnder the conditions of Theorem 1, for all priors P0, with probability at least 1\u2212 \u03b4 over the sample S, for all n \u2208 N and for all data-dependent distributions Pn simultaneously we have\u2223\u2223\u2223\u2223\u222b \u00b5\u03b8 dPn(\u03b8)\u2212 \u222b \u00b5\u0302\u03b8 dPn(\u03b8)\u2223\u2223\u2223\u2223 \u2264 \u221a 2 Cn,\u03b4 V\u0302 (Pn)(\u221a n\u2212 2\u221a\nn Cn,\u03b4 ) + + 2 Cn,\u03b4( n\u2212 2 Cn,\u03b4 ) + .\nNote that the inequality is fully empirical and non-vacuous as long as Cn,\u03b4 \u2264 n/2 \u2014 similar (empirically verifiable) requirement is also present in (Tolstikhin and Seldin, 2013, Theorem 4). Clearly, the fact that we relaxed Theorem 1 to get Proposition 4 implies that our inequality is tighter."
        },
        {
            "heading": "5.1. How to compute confidence intervals from Eq. (4) numerically",
            "text": "So far we discussed analytically computable relaxations of our inequality. Now we turn our attention to numerical computation of Eq. (4) which does not require any relaxation. Given a concrete\nposterior and prior pair (Pn, P0), we propose to obtain confidence bounds for the mean \u222b \u00b5\u03b8 dPn(\u03b8) by solving the following optimization problem:\nProposition 5 Set \u03b4 \u2208 (0, 1]. Consider the optimization problem\nMU = max {\u00b5\u03b8:\u03b8\u2208supp(Pn)}\n\u222b \u00b5\u03b8 dPn(\u03b8) subject to\n\u222b \u03c8?n(\u03b8, \u00b5\u03b8) dPn(\u03b8) \u2264 Cn + ln 1\n\u03b4 , (5)\nwhere Cn + ln 1\u03b4 is the right hand side of Eq. (4). Moreover, let ML be obtained by replacing max with min. Then, under the conditions of Theorem 1 and with probability at least 1\u2212 \u03b4, we have\nML \u2264 \u222b \u00b5\u03b8 dPn(\u03b8) \u2264MU .\nIn other words, the optimization in Eq. (5) is carried out over the class of means of a given distribution, and the solution gives us a valid confidence interval since Theorem 1 holds for any datadependent posterior, and so it must hold for some posteriors with means within the class. Moreover, surprisingly enough, the optimization problem is convex since \u03c8?n(\u03b8, \u00b5\u03b8) appearing in the constraint is convex in \u00b5\u03b8, thanks to the following lemma proven in Section A.4.\nLemma 6 (Convexity of the constraint) Let c \u2208 [0, 1]. Define f(x) = max\u2212 1 1\u2212x\u2264\u03bb\u2264 1 x ln(1 + \u03bb(c\u2212 x)). Then, f(x) is convex for any x \u2208 [0, 1].\nIn Section 6 we present synthetic experiments validating the numerical tightness of the confidence intervals obtained by solving the problem in Proposition 5."
        },
        {
            "heading": "5.1.1. MONTE CARLO APPROXIMATION OF THE INTEGRAL",
            "text": "The confidence intervals of Proposition 5 can be obtained efficiently as long as we can efficiently compute or estimate integrals over parameters. When \u0398 is finite we can clearly replace integrals by summations. On the other hand, for continuous (or prohibitively large finite) \u0398 we can employ a Monte Carlo approximation of the integral. In particular, we can use the procedure in Algorithm 1.\nThe following proposition (proved in Section A.6) states its correctness.\nProposition 7 Set \u03b4 \u2208 (0, 1]. Under the assumptions of Theorem 1, let K = dln(1/\u03b4)e. Then, with probability at least 1\u2212 3\u03b4, the outputs ML and MU of Algorithm 1 satisfy\nML \u2264 \u222b \u00b5\u03b8 dPn(\u03b8) \u2264MU .\nAlgorithm 1 works by carefully controlling the Monte Carlo approximation through the deviation of the sample average over parameters from the integral. In fact, while this is straightforward for bounded random variables, \u03c8?n considered here is not bounded. One may attempt to make it bounded by clipping \u03c8\u2217n(\u03b8i, \u00b5\u03b8i) or reducing the range of \u03bb in the max operator in the definition of \u03c8\u2217n, but these both lead to nonconvex constraints in Eq. (6). Alternatively, since Eq. (5) suggests that we need to lower bound an integral, we could right away get a \u201clow-probability\u201d bound arising from Markov\u2019s inequality: 1\u03b4 \u222b \u03c8?n(\u03b8, \u00b5\u03b8) dPn(\u03b8) \u2265 1m \u2211 i\u2208Bk \u03c8 ? n(\u03b8i, \u00b5i) . Since this is unsatisfactory, here we resort to the \u201cboosting-the-confidence\u201d method (Schapire, 1990; Shalev-Shwartz et al., 2010) which allows to convert polynomial concentration bounds into exponential ones at the expense of sample partitioning and running the algorithm multiple times, as described in Proposition 7. Note that in our case this just translates into extra computation (running Monte Carlo\nAlgorithm 1 Monte Carlo Approximation 1: Input: Failure probability \u03b4, sample size parameters K,m \u2208 N, posterior Pn, prior P0 2: Sample K tuples independently (\u03b8i)i\u2208Bk \u223c Pmn for k \u2208 [K] where Bk = {(k \u2212 1)m+ 1, . . . , km}\n3: Solve the following optimization problem for every k \u2208 [K]:\n\u03bd\u0304U (k) := max {\u03bd\u03b8i}i\u2208Bk\n1\nm \u2211 i\u2208Bk \u03bd\u03b8i s.t. 1 em \u2211 i\u2208Bk \u03c8?n(\u03b8i, \u03bd\u03b8i) \u2264 Cn + ln 1 \u03b4 (6)\n4: Repeat the above while replacing max with min. Call the resulting optimal objective function as \u03bd\u0304L(k), \u2200k \u2208 [K] 5: Let k\u0302U = arg maxk\u2208[K] \u03bd\u0304U (k) and k\u0302L = arg mink\u2208[K] \u03bd\u0304L(k) 6: Compute\nMU =max\n{ \u00b5 : kl ( \u03bd\u0304U (k\u0302U ), \u00b5 ) \u2264\nln K2\u03b4 m\n} , ML=min { \u00b5 : kl ( \u03bd\u0304L(k\u0302L), \u00b5 ) \u2264\nln K2\u03b4 m\n} (7)\n7: Output: ML and MU\napproximation on K independent parameter tuples), because we can always sample more parameter observations from Pn. Proposition 7 is then justified through the use of the following inequality shown in Section A.5:\nProposition 8 Under conditions of Proposition 7, with probability at least 1\u2212 e\u2212K ,\u222b \u03c8?n(\u03b8, \u00b5\u03b8) dPn(\u03b8) \u2265 min\nk\u2208[K]\n1\nem \u2211 i\u2208Bk \u03c8?n(\u03b8i, \u00b5\u03b8i) . (8)\nHow largem needs to be? One question not discussed so far is the choice of Monte Carlo sample size m. Technically, Proposition 8 holds for any m \u2208 N, but we can expect that choosing small m will result in overly loose constraints in Eq. (8) and so the final confidence intervals will be wide. To gauge a good choice of m we consider a lower tail Bernstein\u2019s inequality (Maurer, 2003), which lower bounds the left-hand side of the constraint Eq. (8):\n1\nm m\u2211 i=1 \u03c8n(\u03b8i, \u00b5\u03b8i) >\n\u222b \u03c8?n(\u03b8, \u00b5\u03b8) dPn(\u03b8)\u2212 \u221a\n2 ln 1\u03b4 m\n\u222b \u03c8?n(\u03b8, \u00b5\u03b8) 2 dPn(\u03b8)  + . (9)\nThus, having a raw second moment of \u03c8?n of order o(m) guarantees asymptotic convergence of the sample average to the integral in the constraint (8). Having a finite raw second moment suggests that the constraint is tight and a reasonable choice, is, for instance, m = n2. On the other hand, for \u201chard\u201d problems (e.g., heavy-tailed) such moment is infinite and Monte Carlo estimation is infeasible."
        },
        {
            "heading": "5.2. Proof of Theorem 1",
            "text": "Let \u2206i(\u03b8) := f(\u03b8,Xi) \u2212 E[f(\u03b8,X1)] and notably E[\u2206i(\u03b8)] = 0 for any \u03b8 \u2208 \u0398. Consider an algorithm betting a signed fraction of its wealth equal to Bi(\u03b8) at step i and observing the outcome \u2206i(\u03b8). Note thatBi(\u03b8) is \u03a3(X1, . . . , Xi\u22121, \u03b8)-measurable. Let the following be the cumulative loss\n(log-wealth) of the algorithm and the optimal cumulative loss, respectively \u03c8n(\u03b8) := n\u2211 i=1 ln(1 +Bi(\u03b8)\u2206i(\u03b8)) , \u03c8 ? n(\u03b8) := max \u03bb\u2208 [ \u2212 1 1\u2212\u00b5 , 1 \u00b5 ] n\u2211 i=1 ln ( 1 + \u03bb\u2206i(\u03b8) ) .\nWe are interested in showing an upper bound on \u222b \u03c8?n(\u03b8) dPn(\u03b8) which holds for all data-dependent distribution Pn simultaneously, and with high probability over the data. To this end, Orabona and Jun (2021) show that there exists a betting algorithm that guarantee that for any \u03b8 \u2208 \u0398,3\n\u03c8?n(\u03b8)\u2212 \u03c8n(\u03b8) \u2264 ln \u221a \u03c0\u0393(n+ 1)\n\u0393(n+ 12) . (10)\nSo it remains to give a bound on \u03c8n(Pn). We will need the following concentration inequality.\nTheorem 9 (Ville\u2019s inequality (Ville, 1939, p. 84)) Let \u22061, . . . ,\u2206n be a sequence of non-negative random variables such that E[\u2206i | \u22061, . . . ,\u2206i\u22121] = 0. Let Mt > 0 be \u03a3(\u22061, . . . ,\u2206t\u22121)measurable such that M0 = 1, and moreover let E[Mt | \u22061, . . . ,\u2206t\u22121] \u2264 Mt\u22121. Then, for any \u03b4 \u2208 (0, 1], P { maxtMt \u2265 1\u03b4 } \u2264 \u03b4.\nThe proof will also require the following well-known change-of-measure inequality:\nLemma 10 (Donsker and Varadhan (1975); Dupuis and R. S. Ellis (1997)) Let p and q be probability measures on \u0398 such that p q. Then, for any measurable function f : \u0398\u2192 R, we have\u222b\nf(\u03b8) dp(\u03b8) \u2264 KL(p\u2016q) + ln \u222b ef(\u03b8) dq(\u03b8) .\nApplying the above with p = Pn, q = P0, f = \u03c8n, and taking maxn\u2208N, we obtain\nmax n\u2208N\n\u222b \u03c8n(\u03b8) dPn(\u03b8)\u2212 KL(Pn\u2016P0) \u2264 ln max\nn\u2208N \u222b n\u220f i=1 ( 1 +Bi(\u03b8)\u2206i(\u03b8) ) dP0(\u03b8)\ufe38 \ufe37\ufe37 \ufe38\nMn\n, (11)\nwhere we exchanged ln and maxn\u2208N. Now, the plan is to apply Theorem 9 to Mn, which requires to show that Mn is a martingale. Using the notation Ei[\u00b7] = E[\u00b7 |\u22061(\u03b8), . . . ,\u2206i(\u03b8), \u03b8], we have\nEn\u22121[Mn] = En\u22121 \u222b n\u220f\ni=1\n( 1 +Bi(\u03b8)\u2206i(\u03b8) ) dP0(\u03b8) (a) = \u222b En\u22121 n\u220f i=1 ( 1 +Bi(\u03b8)\u2206i(\u03b8) ) dP0(\u03b8)\n= \u222b En\u22121 [( 1 +Bn(\u03b8)\u2206n(\u03b8) )] n\u22121\u220f i=1 ( 1 +Bi(\u03b8)\u2206i(\u03b8) ) dP0(\u03b8)\n= \u222b n\u22121\u220f i=1 ( 1 +Bi(\u03b8)\u2206i(\u03b8) ) dP0(\u03b8) = Mn\u22121,\nwhere (a) comes using the fact that P0 is independent from the sample S and by Fubini\u2019s theorem. Thus, applying Theorem 9 to Eq. (11), we obtain\nP { max n\u2208N sup P\u2208K(Xn,\u0398) \u222b \u03c8n(\u03b8) dPn(\u03b8)\u2212 KL(Pn\u2016P0) \u2264 ln 1 \u03b4 } \u2265 1\u2212 \u03b4 (\u03b4 \u2208 (0, 1]) .\nFinally, using Eq. (10) gives the statement and completes the proof.\n3. Data-dependent bounds on the regret were also shown by Orabona and Jun (2021)."
        },
        {
            "heading": "6. Experiments",
            "text": "In this section, we validate the numerical tightness of Theorem 1. Additional experiments are in Section B. We perform experiments on simple synthetic scenarios where the parameter space is finite, and we fix the posterior and prior distributions. We evaluated all the bounds on a sample size range n \u2208 {2c : c = 1, . . . , 15}, and we averaged the bound over 20 repetitions for each sample size. In particular, we compare Proposition 7 to several PAC-Bayes baselines such as McAllester (1998), London and Sandler (2019), Maurer (2004) and Tolstikhin and Seldin (2013), and one additional algorithm KL-ver under several synthetic environments.\nKL-ver, the KL version of our algorithm, uses n \u00b7 KL(\u00b5\u0302\u03b8, \u00b5\u03b8) for the optimization problem in Eq. (5), instead of \u03c8\u2217n(\u03b8, \u00b5\u03b8). Theoretically, the log-wealth function \u03c8 ? n(\u03b8, \u00b5\u03b8) is always greater than n \u00b7 KL(\u00b5\u0302\u03b8, \u00b5\u03b8) by Proposition 3. On the other hand, Maurer\u2019s bound is looser than KL-ver, which is shown in the proof of Proposition 3 in Appendix A.2. Hence, KL-ver is an ablation study on our novel optimization problem \u2013 KL-ver is looser than our proposed method of Eq. (5) but tighter than Maurer\u2019s bound that is known to be very tight numerically.\nThe first experiment, reported in Figure 1, represents the case whereXi \u223c Bernoulli(1/2) i.i.d., P0 = Bernoulli(0.8), Pn = Bernoulli(0.9), and f(x, \u03b8) = x\u03b8. The second experiment, in Figure 2, represents the case whereXi \u223c N(0, 1) i.i.d., P0 = (Bin(6, 0.7)\u22123)/4, Pn = (Bin(6, 0.8)\u22123)/4, and f(x, \u03b8) = (erf(x\u03b8) + 1)/2. Here, erf is the Gaussian error function and Bin(n, p) is a binomial distribution, with n \u2208 N being the number of samples, and p \u2208 [0, 1] the probability of success.\nSince \u0398 is finite, we can explicitly calculate the means without resorting to the Monte Carlo simulation. Hence, the optimization problem of Proposition 5 reduces to (and similarly for the lower bound by replacing max with min)\nmax {\u00b5\u03b8:\u03b8\u2208supp(Pn)} \u2211 \u03b8\u2208\u0398 \u00b5\u03b8 \u00b7 Pn(\u03b8) subject to \u2211 \u03b8\u2208\u0398 \u03c8?n(\u03b8, \u00b5\u03b8) \u00b7 Pn(\u03b8) \u2264 Cn + ln 1 \u03b4 . (12)\nThis is a convex optimization problem as we showed before, so we can use any off-the-shelf solver.4\nBoth figures show that our confidence intervals are consistently tighter than the ones of the baselines. Moreover, our guarantee and the one of KL-ver hold uniformly over time, while it holds for a fixed number of samples for the baselines. Furthermore, for the Bernoulli case, KL-ver is the same as our bound and still better than Maurer\u2019s, and in the Binomial case, it is worse than\n4. We use the fmincon function in Matlab.\nours and very close to Maurer\u2019s bound. This confirms our theoretical finding that our approach is \u201ctwo-inequalities away\u201d from Maurer\u2019s bound. For the case of continuous \u0398, check Appendix B."
        },
        {
            "heading": "7. Conclusions, limitations, and future work",
            "text": "We have presented a new PAC-Bayes bound based on a concentration technique derived from the coin-betting formalism. Our new upper bound implies some previous results from PAC-Bayes literature, and at the same time, we have shown that it is tighter in numerical simulations.\nOne limitation of our result is that it lacks a closed-form minimizer of the upper bound, such as the Gibbs measure in the standard PAC-Bayes analysis. While this is not surprising, it introduces a trade-off between computational complexity and tightness of the bound that was absent in previous approaches. In the future, we aim at precisely characterizing this trade-off, possibly delineating its Pareto frontier. Another interesting venue is to investigate the numerical minimization of our upper bound over data-dependent distributions for risk minimization problems."
        },
        {
            "heading": "Acknowledgements",
            "text": "Francesco Orabona is supported by the National Science Foundation under the grants no. 2022446 \u201cFoundations of Data Science Institute\u201d and no. 2046096 \u201cCAREER: Parameter-free Optimization Algorithms for Machine Learning\u201d."
        },
        {
            "heading": "Appendix A. Remaining proofs",
            "text": ""
        },
        {
            "heading": "A.1. Proof of Proposition 2",
            "text": "Consider the right hand side of Eq. (3) without maxn\u2208N. Then, we have the following inequalities:\u222b max\n\u03bb\u2208[\u2212 1 1\u2212\u00b5\u03b8 , 1 \u00b5\u03b8 ] n\u2211 i=1 ln ( 1 + \u03bb(f(\u03b8,Xi)\u2212 \u00b5\u03b8) ) dPn(\u03b8)\n(a) \u2265 \u222b\nmax \u03bb\u2208[\u22121,1] n\u2211 i=1 ln ( 1 + \u03bb(f(\u03b8,Xi)\u2212 \u00b5\u03b8) ) dPn(\u03b8)\n\u2265 \u222b\nmax \u03bb\u2208[\u22121,1]\n{ \u03bb n\u2211 i=1 (f(\u03b8,Xi)\u2212 \u00b5\u03b8)\u2212 \u03bb2 n\u2211 i=1 (f(\u03b8,Xi)\u2212 \u00b5\u03b8)2 } dPn(\u03b8)\n\u2265 \u222b\nmax \u03bb\u2208[\u22121,1]\n{ \u03bb n\u2211 i=1 (f(\u03b8,Xi)\u2212 \u00b5\u03b8)\u2212 \u03bb2n }\n= 1\n4n \u222b ( n\u2211 i=1 (f(\u03b8,Xi)\u2212 \u00b5\u03b8) )2 dPn(\u03b8) (Maximizing in \u03bb; note that optimal \u03bb \u2208 [\u22121, 1])\n\u2265 1 4n (\u222b n\u2211 i=1 (f(\u03b8,Xi)\u2212 \u00b5\u03b8) dPn(\u03b8) )2 , (Jensen\u2019s inequality)\nwhere step (a) comes since [\u22121, 1] \u2282 [\u2212 11\u2212\u00b5\u03b8 , 1 \u00b5\u03b8 ] since \u00b5\u03b8 \u2208 [0, 1] almost surely. Now, applying Theorem 1 gives\nP  14n( \u222b n\u2211\ni=1\n(f(\u03b8,Xi)\u2212 \u00b5\u03b8) dPn(\u03b8) )2 \u2212 Cn \u2265 ln 1\n\u03b4  \u2264 \u03b4 and the statement follows."
        },
        {
            "heading": "A.2. Proof of Proposition 3",
            "text": "The proof is based on the following proposition:\nProposition 11 ((Orabona and Jun, 2021, Proposition 1)) Let X1, . . . , Xn \u2208 [0, 1], let \u00b5\u0302 := (X1 + \u00b7 \u00b7 \u00b7+Xn)/n, and moreover let \u00b5 := E[X1] \u2208 [0, 1]. Then,\nmax \u03bb\u2208[\u2212 1\n1\u2212\u00b5 , 1 \u00b5 ] n\u2211 i=1 ln ( 1 + \u03bb(Xi \u2212 \u00b5) ) \u2265 n kl(\u00b5\u0302, \u00b5) .\nMoreover, if X1, . . . , Xn \u2208 {0, 1}, we achieve equality in the above.\nThen, Proposition 11 combined with Theorem 1 gives that with probability at least 1 \u2212 \u03b4, simultaneously for all n \u2208 N and all Pn, we have\nln 1\u03b4 + Cn n\n\u2265 \u222b\nkl(\u00b5\u0302\u03b8, \u00b5\u03b8) dPn(\u03b8)\n= \u222b \u2211 x\u2208{0,1} ln ( Bern(x | \u00b5\u0302\u03b8) Bern(x | \u00b5\u03b8) ) Bern(x | \u00b5\u0302\u03b8) dPn(\u03b8)\n(a) \u2265 \u2211\nx\u2208{0,1}\nln (\u222b Bern(x | \u00b5\u0302\u03b8) dPn(\u03b8)\u222b Bern(x | \u00b5\u03b8) dPn(\u03b8) ) \u222b Bern(x | \u00b5\u0302\u03b8) dPn(\u03b8)\n= kl (\u222b\n\u00b5\u0302\u03b8 dPn(\u03b8), \u222b \u00b5\u03b8 dPn(\u03b8) ) ,\nwhere (a) comes by exchanging summation and integration and applying the log-sum inequality."
        },
        {
            "heading": "A.3. Proof of Proposition 4",
            "text": "The proof largely follows that of Orabona and Jun (2021, Theorem 6). Set \u03b8 = \u00b5\u03b8 \u2212 \u00b5\u0302\u03b8, and so \u03b8 + \u00b5\u0302\u03b8 \u2208 [0, 1]. Then, we have\n\u03c8?(\u03b8, \u00b5\u03b8) \u2265 max \u03bb\u2208[\u22121,1] n\u2211 i=1 ln ( 1 + \u03bb(f(\u03b8,Xi)\u2212 (\u00b5\u0302\u03b8 + \u03b8)) ) ,\nand applying Jensen\u2019s inequality\u222b \u03c8?(\u03b8, \u00b5\u03b8) dPn \u2265 max\n\u03bb\u2208[\u22121,1] \u222b n\u2211 i=1 ln ( 1 + \u03bb(f(\u03b8,Xi)\u2212 (\u00b5\u0302\u03b8 + \u03b8)) ) dPn . (13)\nNow, we further relax the above by taking a lower bound. In particular, (Fan et al., 2015, Eq. 4.12) shows that for any |x| \u2264 1 and |\u03bb| \u2264 1,\nln(1 + \u03bbx) \u2265 \u03bbx+ ( ln(1\u2212 |\u03bb|) + |\u03bb| ) x2 . (14)\nThe above is combined with the following lemma: Lemma 12 ((Orabona and Jun, 2021, Lemma 5)) Let f(\u03bb) = a\u03bb+b ( ln(1\u2212|\u03bb|)+|\u03bb| )\nfor some a \u2208 R, b \u2265 0. Then, max\u03bb\u2208[\u22121,1] f(\u03bb) \u2265 a 2 (4/3)|a|+2b .\nThus,\u222b \u03c8?(\u03b8, \u00b5\u03b8) dPn (a) \u2265 \u03bb \u222b n\u2211\ni=1\n( f(\u03b8,Xi)\u2212 (\u00b5\u0302\u03b8 + \u03b8) ) dPn\n+ ( ln(1\u2212 |\u03bb|) + |\u03bb| ) \u222b n\u2211\ni=1\n( f(\u03b8,Xi)\u2212 (\u00b5\u0302\u03b8 + \u03b8) )2 dPn\n= \u2212n\u03bb \u222b \u03b8 dPn\n+ ( ln(1\u2212 |\u03bb|) + |\u03bb| )\u222b n\u2211\ni=1\n(f(\u03b8,Xi)\u2212 \u00b5\u0302\u03b8)2 dPn + n \u222b 2\u03b8 dPn  (b)\n\u2265 \u2212n\u03bb \u222b \u03b8 dPn\n+ ( ln(1\u2212 |\u03bb|) + |\u03bb| )\u222b n\u2211\ni=1\n(f(\u03b8,Xi)\u2212 \u00b5\u0302\u03b8)2 dPn + n (\u222b \u03b8 dPn )2 (c)\n\u2265 n2 (\u222b \u03b8 dPn )2 (4/3)n\n\u2223\u2223\u222b \u03b8 dPn\u2223\u2223+ 2 \u222b \u2211ni=1(f(\u03b8,Xi)\u2212 \u00b5\u0302\u03b8)2 dPn + 2n (\u222b \u03b8 dPn)2 , where step (a) comes by application of Eqs. (13) and (14), step (b) comes by Jensen\u2019s inequality, and step (c) comes by application of Lemma 12. Now, the above combined with Theorem 1 gives\nn2 (\u222b\n\u03b8 dPn\n)2 \u2264 n ( Cn + ln 1\n\u03b4\n) ( 4\n3\n\u2223\u2223\u2223\u2223\u222b \u03b8 dPn\u2223\u2223\u2223\u2223+ 2V\u0302 (Pn) + 2(\u222b \u03b8 dPn)2 ) .\nFinally, solving the above for \u222b \u03b8 dPn, using subadditivity of square root, and relaxing some numerical constants we get\n\u2223\u2223\u2223\u2223\u222b \u03b8 dPn\u2223\u2223\u2223\u2223 \u2264 \u221a 2 ( Cn + ln 1\u03b4 ) V\u0302 (Pn)(\n\u221a n\u2212 2\u221a\nn ( Cn + ln 1\u03b4 )) +\n+ 2 ( Cn + ln 1\u03b4 ) ( n\u2212 2 ( Cn + ln 1\u03b4 )) + ."
        },
        {
            "heading": "A.4. Proof of Lemma 6",
            "text": "We can rewrite f as\nf(x) = max 0\u2264b\u22641 ln 1 + (c\u2212 x)(\u2212 1 1\u2212 x + ( 1 1\u2212 x + 1 x ) b ) . Now, consider the argument of the max. We claim that it is convex in x for any b \u2208 [0, 1]. In fact, the second derivative is\n1\nx2 +\n1 (1\u2212 x)2 \u2212 (b+ c\u2212 1) 2 (\u2212x(b+ c) + bc+ x)2 = 1 x2 +\n1 (1\u2212 x)2 \u2212 1\n(x+ bc1\u2212b\u2212c) 2\n(15)\nWe claim that for b, c \u2208 [0, 1], we have g(b, c) := bc1\u2212b\u2212c \u2208 (\u2212\u221e,\u22121] \u222a [0,\u221e). To see this, fix b and consider c \u2264 1 \u2212 b. In this regime, g(b, 0) = 0 and g(b, c) is increasing to infinity as c goes from 0 to 1\u2212 b. In the other regime of c > 1\u2212 b, we have g(b, 1) = \u22121 and g(b, c) is decreasing as c decreases from 1 to 1\u2212 b. This proves the claim.\nTherefore, to lower bound (15) we need to lower bound (x+z)2 where z \u2208 (\u2212\u221e,\u22121]\u222a [0,\u221e). Since (x + z)2 is minimized at \u2212x but the range of z never includes \u2212x (except for the boundary case), we have that minz\u2208(\u2212\u221e,\u22121]\u222a[0,\u221e)(x+ z)2 = min{(x\u2212 1)2, x2}. Therefore,\n\u2212 (b+ c\u2212 1) 2\n(\u2212x(b+ c) + bc+ x)2 \u2265 \u2212max\n{ 1\n(x\u2212 1)2 ,\n1\nx2\n} \u2265 \u2212 1\n(x\u2212 1)2 \u2212 1 x2 .\nHence, f is a maximum of convex functions, that concludes the proof."
        },
        {
            "heading": "A.5. Proof of Proposition 8",
            "text": "It is clear that the optimal log-wealth \u03c8?n(\u03b8, \u00b5\u03b8) is non-negative over all arguments, because the maximization range includes 0. Then, for a fixed block Bk, by the non-negativity of \u03c8?n, Markov\u2019s inequality gives\nP e \u222b \u03c8?n(\u03b8, \u00b5\u03b8) dPn(\u03b8) \u2264 1 m \u2211 i\u2208Bk \u03c8?n(\u03b8i, \u00b5\u03b8i)  \u2264 1/e . Since all blocks are independent,\nP  \u22c2 k\u2208[K] { e \u222b \u03c8?n(\u03b8, \u00b5\u03b8) dPn(\u03b8) \u2264 1 m \u2211 i\u2208Bk \u03c8?n(\u03b8i, \u00b5\u03b8i) } \u2264 e\u2212K\nand so there exists at least one Bk such that with probability at least 1\u2212 e\u2212K ,\u222b \u03c8?n(\u03b8, \u00b5\u03b8) dPn(\u03b8) \u2265 1\nem \u2211 i\u2208Bk \u03c8?n(\u03b8i, \u00b5\u03b8i) .\nThus, with probability at least 1\u2212 e\u2212K ,\u222b \u03c8?n(\u03b8, \u00b5\u03b8) dPn(\u03b8) \u2265 min\nk\u2208[K]\n1\nem \u2211 i\u2208Bk \u03c8?n(\u03b8i, \u00b5\u03b8i) .\nNote that we can change e to any other constant C > 1. Then, with probability 1\u2212 C\u2212K ,\u222b \u03c8?n(\u03b8, \u00b5\u03b8) dPn(\u03b8) \u2265 min\nk\u2208[K]\n1\nCm \u2211 i\u2208Bk \u03c8?n(\u03b8i, \u00b5\u03b8i) ."
        },
        {
            "heading": "A.6. Proof of Proposition 7",
            "text": "Recall that (\u03b8i)i\u2208Bk \u223c Pmn for any k \u2208 [K] and \u00b5\u03b8i = E[f(\u03b8i, X1)],\u2200i \u2208 [Km]. Our goal is to find an upper and lower bound for \u00b5\u2217 := \u222b \u00b5\u03b8 dPn(\u03b8). Given Pn(\u03b8), Theorem 1 gives us\nP { \u2203n \u2208 N, \u2203Pn : \u222b \u03c8?n(\u03b8, \u00b5\u03b8) dPn(\u03b8)\u2212 Cn \u2264 ln 1\n\u03b4\n} \u2265 1\u2212 \u03b4 . (16)\nMoreover, using the union bound and the standard KL-divergence concentration inequality for [0, 1]- bounded random variable, we have\nP maxk\u2208[K] kl  1 m \u2211 i\u2208Bk \u00b5\u03b8i , \u00b5 \u2217  \u2264 ln K 2\u03b4  \u2265 1\u2212 \u03b4 . (17) Furthermore, Proposition 8 states that\nP mink\u2208[K] 1m \u2211 i\u2208Bk \u03c8?n(\u03b8i, \u00b5\u03b8i) \u2264 \u222b \u03c8?n(\u03b8, \u00b5\u03b8) dPn(\u03b8) \u2223\u2223\u2223\u2223\u2223\u2223 X1, . . . , Xn  \u2265 1\u2212 e\u2212K . (18)\nBy the union bound, and assuming setting K = dln(1/\u03b4)e one can see that with probability at least 1 \u2212 3\u03b4, the concentration events in the three inequalities are all true. Assume that these events are true. It suffices to show \u00b5\u2217 \u2264MU since the proof of \u00b5\u2217 \u2265ML is symmetric.\nDenote by \u03bdU\u03b8i the solutions of the optimization problem in (6). Let k\u2217 = arg mink\u2208[K] 1m \u2211 i\u2208Bk \u03c8 ? n(\u03b8i, \u00b5\u03b8i). By the events in (16) and (18), we have that\n(\u00b5\u03b8i)i\u2208Bk\u2217 is a feasible solution of (6) with k = k \u2217. This implies that\n\u00b5\u0304(k\u2217) := 1\nm \u2211 i\u2208Bk\u2217 \u00b5\u03b8i \u2264 1 m \u2211 i\u2208Bk\u2217 \u03bdU\u03b8i \u2264 1 m \u2211 i\u2208Bk\u0302U \u03bdU\u03b8i = \u03bd\u0304U (k\u0302U ) =: \u03bd \u2217. (19)\nConsider the following two cases:\n\u2022 Case 1: \u03bd\u2217 \u2265 \u00b5\u2217. We just need to verify that \u03bd\u2217 \u2264MU . This holds by the definition of MU .\n\u2022 Case 2: \u03bd\u2217 < \u00b5\u2217. We have\nkl(\u03bd\u2217, \u00b5\u2217) \u2264 kl(\u00b5\u0304(k\u2217), \u00b5\u2217) \u2264 ln K 2\u03b4 (7) = kl(\u03bd\u2217,MU ) .\nwhere the first inequality is due to \u00b5\u0304(k\u2217) \u2264 \u03bd\u2217 < \u00b5\u2217. Applying the monotonicity of kl in the second argument to kl(\u03bd\u2217, \u00b5\u2217) \u2264 kl(\u03bd\u2217,MU ), we have \u00b5\u2217 \u2264MU .\nThis concludes the proof."
        },
        {
            "heading": "Appendix B. Monte Carlo experiment",
            "text": "In this section, we investigate numerically the Monte Carlo approximation discussed in Section 5.1.1. In particular, we want to validate the claim that the confidence intervals calculated with Algorithm 1 are better than the ones of Maurer (2003) when enough Monte Carlo sample are used.\nHere is the list of parameters in our experiment:\n\u2022 The number of samples: n = 32 (Figure 3), n \u2208 {2c : c \u2208 {1, . . . , 4}} (Figure 4).\n\u2022 The number of groups: K = 4, the corresponding multiplier: C = 2.1147 (check the end of Appendix A.5).\n\u2022 The number of Monte Carlo samples in each group: m \u2208 {2c : c = 1, . . . , 10} (Figure 3), m = 256 (Figure 4).\n\u2022 Parameter space \u0398 = R.\n\u2022 Prior distribution P0 is N(0, 1).\n\u2022 Posterior distribution Pn is N(0, 0.25).\n\u2022 Samples: X1, . . . , Xn drawn from N(0, 1), i.i.d.\n\u2022 f(x, \u03b8) = (erf(x\u03b8) + 1)/2.\n\u2022 Failure probability \u03b4 = 0.05.\nFor the fair use of parameters, we ensure that the total number of MC samples used in Maurer\u2019s bound matches that of ours. That is, when ours usemMonte Carlo samples for each group k \u2208 [K], Maurer\u2019s bound use M = m \u00b7 K Monte Carlo samples. We describe how we compute Maurer\u2019s bound numerically in Algorithm 2.\nAlgorithm 2 Monte Carlo Approximation for Maurer\u2019s Bound 1: Input: Failure probability \u03b4, sample size parameters M \u2208 N, posterior Pn, prior P0 2: Sample (\u03b8i)i\u2208[M ] \u223c PMn 3: \u00b5\u0302M = 1 M \u2211M j=1 \u00b5\u0302\u03b8i = 1 Mn \u2211M j=1 \u2211n i=1 f(Xi, \u03b8i)\n4: Let w = \u221a ln( 2 \u03b4 )\n2M\n5: k\u0302U = \u00b5\u0302M + w and k\u0302L = \u00b5\u0302M \u2212 w 6: Compute\nMU =max\n{ \u00b5 : kl ( k\u0302U , \u00b5 ) \u2264 Cn + ln 2\u03b4\nn\n} , ML=min { \u00b5 : kl ( k\u0302L, \u00b5 ) \u2264 Cn + ln 2\u03b4\nn\n} (20)\n7: Output: ML and MU\nThe value w in Algorithm 2 is the confidence width of the Monte Carlo error between \u00b5\u0302m and\u222b \u00b5\u0302\u03b8 dPn(\u03b8) based on the following Proposition 13.\nProposition 13 (Monte Carlo error bound) Let \u03b4 \u2208 (0, 1]. Given the samples X1, . . . , Xn, with probability at least 1\u2212 \u03b4 over the Monte Carlo samples (\u03b8i)i\u2208[M ], we have\n1\nM M\u2211 i=1 \u00b5\u0302\u03b8i \u2212 \u222b \u00b5\u0302\u03b8 dPn(\u03b8) \u2264 \u221a ln(1\u03b4 ) 2M .\nProof When the set of samples {Xi}ni=1 is given, we can consider {\u00b5\u0302\u03b8i}Mi=1 as i.i.d. samples from a distribution supported on [0, 1]. Therefore, by Hoeffding\u2019s inequality,\nP  1M M\u2211 i=1 \u00b5\u0302\u03b8i \u2212 \u222b \u00b5\u0302\u03b8 dPn(\u03b8) \u2265 \u221a ln(1\u03b4 ) 2M  \u2264 \u03b4 .\nWhen the parameter space \u0398 is discrete, we can precisely compute \u222b \u00b5\u0302\u03b8 dPn(\u03b8), and from this value we can use Maurer\u2019s bound to obtain the confidence bound. However, in our current setting we only have [k\u0302L, k\u0302U ] that is a confidence interval for \u222b \u00b5\u0302\u03b8 dPn(\u03b8). In this case, the confidence\nbound of actual \u222b \u00b5\u03b8 dPn(\u03b8) is the union of all possible confidence intervals, or formally,\n\u222a\u00b5\u2032\u2208[k\u0302L,k\u0302U ]\n{ \u00b5 : kl(\u00b5\u2032, \u00b5) \u2264\nCn + ln 1\u03b4 n\n} .\nThanks to the property of the kl, we only need to check two endpoints k\u0302L and k\u0302U to get the final Maurer\u2019s bound (ML,MU ).\nFigure 3 shows that our Algorithm 1 outperforms the Maurer\u2019s bound with sufficient amount of Monte Carlo samples. Even with the increasing number of data samples (Figure 4), our algorithm steadily outperforms the Maurer\u2019s bound."
        }
    ],
    "title": "Tighter PAC-Bayes Bounds Through Coin-Betting",
    "year": 2023
}