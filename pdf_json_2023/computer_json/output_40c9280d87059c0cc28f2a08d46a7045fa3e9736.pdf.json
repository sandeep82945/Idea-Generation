{
    "abstractText": "Chain-of-thought (CoT) prompting combined with large language models (LLMs) have achieved encouraging results on complex reasoning tasks. Text-to-SQL is a critical semantic parsing task that converts natural language questions into SQL statements, involving a complex reasoning process. However, there is little work about using CoT prompting to activate LLM\u2019s reasoning capabilities on Text-to-SQL tasks. In this work, we propose a new paradigm for prompting Text-toSQL tasks, called Divide-and-Prompt, which first divides the task into subtasks, and then approach each subtask through CoT. We present 3 prompting-based methods to enhance the Text-to-SQL ability of LLMs. Experiments show that these prompts guide LLMs to generate Text-to-SQL with higher execution accuracy.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xiping Liu"
        },
        {
            "affiliations": [],
            "name": "Zhao Tan"
        }
    ],
    "id": "SP:af9cd28f0f82444f726619878cb58523ae49c306",
    "references": [
        {
            "authors": [
                "Tom B. Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "year": 2020
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Paul Christiano",
                "Jan Leike",
                "Tom B. Brown",
                "Miljan Martic",
                "Shane Legg",
                "Dario Amodei"
            ],
            "title": "Deep reinforcement learning from human preferences",
            "year": 2023
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "year": 2019
        },
        {
            "authors": [
                "Yujian Gan",
                "Xinyun Chen",
                "Qiuping Huang",
                "Matthew Purver"
            ],
            "title": "Measuring and improving compositional generalization in text-to-sql via component alignment",
            "year": 2022
        },
        {
            "authors": [
                "Yujian Gan",
                "Xinyun Chen",
                "Qiuping Huang",
                "Matthew Purver",
                "John R. Woodward",
                "Jinxia Xie",
                "Pengsheng Huang."
            ],
            "title": "Towards robustness of text-toSQL models against synonym substitution",
            "venue": "Proceedings of the 59th Annual Meeting of the Associa-",
            "year": 2021
        },
        {
            "authors": [
                "Yujian Gan",
                "Xinyun Chen",
                "Matthew Purver"
            ],
            "title": "Exploring underexplored limitations of cross-domain text-to-sql generalization",
            "year": 2021
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang Shane Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa"
            ],
            "title": "Large language models are zero-shot reasoners",
            "year": 2023
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Ves Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "Bart: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2019
        },
        {
            "authors": [
                "Haoyang Li",
                "Jing Zhang",
                "Cuiping Li",
                "Hong Chen"
            ],
            "title": "2023a. Resdsql: Decoupling schema linking and skeleton parsing for text-to-sql",
            "year": 2023
        },
        {
            "authors": [
                "Jinyang Li",
                "Binyuan Hui",
                "Reynold Cheng",
                "Bowen Qin",
                "Chenhao Ma",
                "Nan Huo",
                "Fei Huang",
                "Wenyu Du",
                "Luo Si",
                "Yongbin Li"
            ],
            "title": "2023b. Graphix-t5: Mixing pre-trained transformers with graph-aware layers for text-to-sql parsing",
            "year": 2023
        },
        {
            "authors": [
                "Aiwei Liu",
                "Xuming Hu",
                "Lijie Wen",
                "Philip S. Yu"
            ],
            "title": "A comprehensive evaluation of chatgpt\u2019s zeroshot text-to-sql capability",
            "year": 2023
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "year": 2020
        },
        {
            "authors": [
                "Nitarshan Rajkumar",
                "Raymond Li",
                "Dzmitry Bahdanau"
            ],
            "title": "Evaluating the text-to-sql capabilities of large language models",
            "year": 2022
        },
        {
            "authors": [
                "Torsten Scholak",
                "Nathan Schucher",
                "Dzmitry Bahdanau"
            ],
            "title": "Picard: Parsing incrementally for constrained auto-regressive decoding from language models",
            "year": 2021
        },
        {
            "authors": [
                "Peng Shi",
                "Patrick Ng",
                "Zhiguo Wang",
                "Henghui Zhu",
                "Alexander Hanbo Li",
                "Jun Wang",
                "Cicero Nogueira dos Santos",
                "Bing Xiang"
            ],
            "title": "Learning contextual representations for semantic parsing with generation-augmented pre-training",
            "year": 2020
        },
        {
            "authors": [
                "Romal Thoppilan",
                "Daniel De Freitas",
                "Jamie Hall",
                "Noam Shazeer",
                "Apoorv Kulshreshtha",
                "Heng-Tze Cheng",
                "Alicia Jin",
                "Taylor Bos",
                "Leslie Baker",
                "Yu Du",
                "YaGuang Li",
                "Hongrae Lee",
                "Huaixiu Steven Zheng"
            ],
            "title": "Lamda: Language models for dialog",
            "year": 2022
        },
        {
            "authors": [
                "Bailin Wang",
                "Richard Shin",
                "Xiaodong Liu",
                "Oleksandr Polozov",
                "Matthew Richardson"
            ],
            "title": "Rat-sql: Relation-aware schema encoding and linking for text-to-sql parsers",
            "year": 2021
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Brian Ichter",
                "Fei Xia",
                "Ed Chi",
                "Quoc Le",
                "Denny Zhou"
            ],
            "title": "2023a. Chain-of-thought prompting elicits reasoning in large language models",
            "year": 2023
        },
        {
            "authors": [
                "Jerry Wei",
                "Jason Wei",
                "Yi Tay",
                "Dustin Tran",
                "Albert Webson",
                "Yifeng Lu",
                "Xinyun Chen",
                "Hanxiao Liu",
                "Da Huang",
                "Denny Zhou",
                "Tengyu Ma"
            ],
            "title": "2023b. Larger language models do in-context learning differently",
            "year": 2023
        },
        {
            "authors": [
                "Tao Yu",
                "Rui Zhang",
                "Kai Yang",
                "Michihiro Yasunaga",
                "Dongxu Wang",
                "Zifan Li",
                "James Ma",
                "Irene Li",
                "Qingning Yao",
                "Shanelle Roman",
                "Zilin Zhang",
                "Dragomir Radev"
            ],
            "title": "Spider: A largescale human-labeled dataset for complex and cross",
            "year": 2019
        },
        {
            "authors": [
                "Zhuosheng Zhang",
                "Aston Zhang",
                "Mu Li",
                "Alex Smola"
            ],
            "title": "Automatic chain of thought prompting in large language models",
            "year": 2022
        },
        {
            "authors": [
                "Ruiqi Zhong",
                "Tao Yu",
                "Dan Klein"
            ],
            "title": "Semantic evaluation for text-to-sql with distilled test suites",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "With the increasing size of large language models (LLMs), they excel at various natural language processing tasks and have become an essential element of natural language processing. Models such as BERT(Devlin et al., 2019), BART(Lewis et al., 2019), and T5(Raffel et al., 2020) require finetuning with a small amount of relevant data. However, their fine-tuning becomes very costly as the models\u2019 size grows. Models such as GPT-3(Brown et al., 2020), LaMDA(Thoppilan et al., 2022), and PaLM(Chowdhery et al., 2022) require prompt design to generate target outputs. ChatGpt1has a powerful performance in (in-context) few-shot and zeroshot learning by employing Reinforcement Learning for Human Feedback (RLHF)(Christiano et al., 2023). However, scaling up the model size has yet to prove sufficient for achieving high performance on challenging tasks such as Text-to-SQL(Liu et al., 2023; Rajkumar et al., 2022).\n1https://chat.openai.com/\nTranslating text to SQL is a challenging process involving extensive and complex reasoning. As show in Figure 1.\nIt is promising to consider Text-to-SQL as a reasoning task. We notice that CoT prompting(Wei et al., 2023a; Kojima et al., 2023) induces LLMs to produce a series of intermediate steps before the final answer to a question; CoT prompting elicits challenging tasks such as arithmetic, commonsense, and symbolic reasoning in LLMs. However, as shown in table 1, normal CoT prompting does not perform well on Text-to-SQL. This is probably because the task involves much reasoning steps, concerning the understanding of the query intentions as well as the database schema.\nInspired by the characteristics of Text-to-SQL, we propose a new paradigm for prompting Text-toSQL tasks, called Divide-and-Prompt (DnP). The basic idea is to divide the task into subtasks, and then tackle each subtask through CoT. We design 3 DnP promptings for Text-to-SQL and evaluate them on LLMs. Based on the experimental results, the following findings were obtained:\n\u2022 DnP promptings is very effective for Text-toSQL. Compared with the standard zero-shot prompt, our proposed prompts improve execution accuracy by 4.3% .\nar X\niv :2\n30 4.\n11 55\n6v 1\n[ cs\n.C L\n] 2\n3 A\npr 2\n02 3\n\u2022 DnP promptings is specially useful for difficult Text-to-SQL. On hard-level Text-to-SQL tasks, our prompts improved the execution accuracy by up to 10.8%; for extra-level tasks, it also showed a 3% improvement.\n\u2022 Normal CoT prompting does not perform well on SQL generation. Due to the strict structure and syntax of SQL, normal CoT prompting hard to inducing the helpful reasoning chains for Text-to-SQL in LLMs."
        },
        {
            "heading": "2 Method",
            "text": "In this work, we propose a new paradigm for prompts of Text-to-SQL, called Divide-and-prompt (DnP). The basic idea is to instruct the model to divide complex tasks into subtasks, and then solve each subtasks. There are different ways of dividing a Text-to-SQL task, therefore, there are many possible DnP methods. We designed 3 DnP methods, as shown in Figure 2, which show effectiveness on Text-to-SQL task. Note that we used only natural language to construct the prompts. This is because Text-to-SQL technology is expected to be used by non-expert users; making prompts easier to understand for users is essential.\nClause by Clause DnP (CC-DnP). In this method, the model is induced to generates an SQL query clause by clause, e.g. first generate the SELECT clause, then FROM clause, \u00b7 \u00b7 \u00b7, as shown in Figure 4. We found that the order in which clauses are generated is an essential factor that impacts the results, and we discuss this in 3.3.\nSchema Linking DnP (SL-DnP). (Li et al., 2023a) has confirmed that schema linking, i.e. identifying relevant schema elements (tables, columns, etc.), is valuable for the model to generate SQL correctly. We proposed SL-DnP method, by which the model first learn to identify the relevant schema elements relevant to the question, and then generate the SQL. What\u2019s more, we found that different ways of schema linking have impacts on the performance. More discussions can be found in 3.3.\nGenerate and Refine DnP(GR-DnP). Recently, there have been some attempts(Madaan et al., 2023) to use LLMs to modify the raw output that may have some mistakes. In this work, we propose the GR-DnP method, which generate an SQL query in two stages. In the first stage, the model generates an\ninitial SQL; In the second stage, the model checks and refines the SQL as needed. Figure 2 shows an example."
        },
        {
            "heading": "3 Experiment",
            "text": ""
        },
        {
            "heading": "3.1 Experiment Setup",
            "text": "Model. Our experiment focuses on the models accessible via the OpenAI API: GPT-3.5-Turbo. We also conducted experiments on other GPT3.5 models(text-davinci-002, text-davinci-003, i.e.). However, their Text-to-SQL capability is far inferior to GPT-3.5-Turbo. The model parameters we set for temperature and top_p are 0.3 and 1, respectively.\nDatasets. We conduct experiments on the standard Spider dataset(Yu et al., 2019), a large-scale crossdomain Text-to-SQL benchmark containing 8659 training samples across 146 databases and 1034 evaluation samples across 20 databases. The data richness of the Spider dataset is sufficient to verify the validity of the methods designed in this paper.\nEvaluation Metrics. The most popular evaluation metrics for Text-to-SQL are Exact Match(EM) and Execution Accuracy(EX). EM evaluates whether the generated queries exactly match the golden answers, while EX evaluates the correctness of generated answers based on the execution results. In this work, we do not adopt EM, because we think that SQL is flexible in syntax, thus EX makes much sense. What is more, GPT-3.5 was not finetuned on the Spider dataset, it tends to generate SQL queries with a lot of variety different from the golden answers. We also adopt valid SQL(VA) and test-suite accuracy (TS)(Zhong et al., 2020) as evaluation metrics. All results are the average of three experiment results.\nBaselines. We primarily utilized the following baselines: (1) PICARD (Scholak et al., 2021) is a method for constraining auto-regressive decoders of language models through incremental parsing. (2) Graphix-T5 (Li et al., 2023b) propose a mixed model with the standard pre-trained transformer model augmented by some speciallydesigned graph-aware layers. (3) RESDSQL (Li et al., 2023a) proposes a ranking-enhanced encoding and skeleton-aware decoding framework to decouple the schema linking and the skeleton parsing.\n(4) (Rajkumar et al., 2022) perform an empirical evaluation of Text-to-SQL capabilities of the Codex language model. (5) (Liu et al., 2023) presents the comprehensive analysis of ChatGPT\u2019s Text-to-SQL ability."
        },
        {
            "heading": "3.2 Main Experiment",
            "text": "Overall Performance We use natural language to construct prompts. In the zero-shot scenario, the EX performance of GPT-3.5 is 70.8%. We construct normal CoT, CC-DnP, SL-DnP, and GR-DnP promoting in the few-shot learning scenario, and the demonstrations in the few-shot are the clustering results of question in the Spider training set.\nWe construct demonstrations for CoT promopting by requiring the model to think step by step. Manually construct demonstrations for CC-DnP, SL-DnP, and GR-DnP promopting.\nThe experiment results show that our methods improve execution accuracy by 4.3% . Despite there being a gap (9%) in EX compared to the current SOTA model(Li et al., 2023a), it is remarkable that GPT-3.5 achieved 75.1% EX with GR-DnP prompting considering that it was not finetuned on the Spider.\nResults on Complex Queries. We compare the more precise performance results of our prompts\nin four separate SQL difficulty levels separated by Spider officially. As (Wei et al., 2023a) pointed out, CoT promptings are particularly helpful for complex reasoning tasks.\nIt is evident in Table 2. For difficult Text-to-SQL tasks, our prompts stimulate the potential of the model. For hard-level Text-to-SQL tasks, GR-DnP prompting improved the EX by 10.8% compared to standard prompting, which is encouraging.\nResults of zero-shot learning. We compared the performance of prompts in zero-shot and few-shot learning scenarios. We have designed corresponding zero-shot prompts for our methods; the results are shown in Figure 3.\nFor Text to SQL, designing a effective zero-shot prompt is very challenging. In the zero-shot scenario, the performance of all prompts was greatly reduced; in LR-DnP, the performance declined by 13.1%.\nBy observing the model\u2019s output, in the zeroshot scenario, the model does not follow the reasoning steps provided by the prompt. CC-DnP, LR-DnP, and normal CoT prompting have no significant difference for performers in the zero-shot scenario."
        },
        {
            "heading": "3.3 Ablation Study",
            "text": "Ablation study of CC-DnP. The order of clause generation is essential, and we have experimented with three different orders of prompts, as shown in Figure 4.\nDifferent orders represent different reasoning paths. For example, the SELECT clause first represents thinking in SQL generating standard or-\nder. However, this is counterintuitive because when writing an SQL, we must first consider which tables the SQL involves rather than which columns.\nDue to SQL queries can be expressed in various ways, it is challenging to define an optimal order. This paper considers 3 orders and concludes that the SELECT clause last is the most suitable order.\nAblation study of SL-DnP. Finding relevant schema elements before generating SQL is an intuitive idea that is almost impossible. We want to guide the model via SL-DnP prompting to identify relevant tables or columns, which ultimately helps generate the target SQL.\nFor SL-DnP prompting, it is necessary to consider whether it is necessary to identify each related table and column. Because identifying tables and columns are inherently challenging for the LLMs. Based on this, we propose 3 SL-DnP are shown in Figure 4.\nThe experiment results show that identifying which schema elements significantly impact the results. The first prompt in Figure 4 induces the model to identify each relevant table and column name, resulting in the worst performance. However, less precise prompts have better performance, such as the third prompt in the Figure 4 only guiding the model to find the relevant table and all the columns in this table, achieving the best performance.\nAblation study of GR-DnP. For the GR-DnP, model generate SQL in stage-1, and in stage-2, checke and modify SQL as needed. We compared the performance of two stages in zero-shot and fewshot learning scenarios, respectively, as shown in figure5."
        },
        {
            "heading": "4 Related Work",
            "text": "This section reviews two lines of research that form the basis of this work: CoT prompting and Text-toSQL task."
        },
        {
            "heading": "4.1 Chain of Thought Prompting",
            "text": "These are two primary paradigms for CoT prompting. One is called zero-shot-CoT(Kojima et al., 2023), adding a single prompt like \u201cLet\u2019s think step by step\u201d before the answer to inducing the reasoning chains in LLMs. The other paradigm is fewshot prompting with manual or model-generated reasoning demonstrations(Wei et al., 2023a). Each demonstration has a question and a reasoning chain. A reasoning chain comprises a series of intermediate reasoning steps and an expected answer. CoT prompting enhances perform of LLMs in challenging reasoning tasks."
        },
        {
            "heading": "4.2 Text-to-SQL",
            "text": "Text-to-SQL is a task that translates natural language questions posed by non-expert users into SQL statements. Spider dataset(Yu et al., 2019) collects many natural language questions and their corresponding SQL and covers many complex language structures and operations. Based on Spider, many other challenging datasets have been proposed(Spider-SYN(Gan et al., 2021a), SpiderDK(Gan et al., 2021b), Spider-CG(Gan et al., 2022)).\nFrom a modeling viewpoint, two distinct approaches are often utilized in Text-to-SQL. One is employing Graph Neural Networks to utilize the structural information of text and schema(e.g., RATSQL(Wang et al., 2021), Graphix-T5(Li et al., 2023b)). Another is the use of pre-trained models (e.g., T5(Raffel et al., 2020), GAP(Shi et al., 2020)); LLMs have been applied in Text-to-SQL, and PICARD(Scholak et al., 2021) utilizes the T53B model but still requires the training data for fine-tuning. (Rajkumar et al., 2022)investigated the Text-to-SQL capabilities of the GPT3 model. (Liu et al., 2023)evaluate the comprehensive Textto-SQL capabilities of ChatGPT.\nHowever, prior works employed the direct prompt, which only partially exploits the LLMs\u2019 capabilities. To the best of our knowledge, there\nis currently no work to explore CoT prompting of Text-to-SQL, and we are the first to connect Text-to-SQL with reasoning task employing CoT prompting to enhance LLMs\u2019 ability to generate Text-to-SQL."
        },
        {
            "heading": "5 Discussion",
            "text": "Why few-shot learning? Experiment results show that our prompts have impressive performance only in the few-shot learning scenario. We observe the model\u2019s output in the zero-shot scenario, and observe that the model does not strictly follow the reasoning steps required in the prompts. Zero-shot prompting cannot guide the model to reason as required.\nDemonstrations in few-shot learning. The demonstrations in the few-shot have a significant effect on the results(Wei et al., 2023b); they should be the representative. We cluster the question in\nSpider training set into 5 clusters(Zhang et al., 2022). The clustering result is 5 questions that start with \u201cwhat, find, how, which, and show.\u201d However, we expect the demonstrations to represent various query methods instead of questioning methods. We will find more suitable methods to generate demonstrations for Text-to-SQL in the future."
        },
        {
            "heading": "6 Conclusion",
            "text": "This paper considers Text-to-SQL as a reasoning task and proposes 3 prompting-based methods to enhance LLMs\u2019 ability to generate Text-to-SQL. SQL statements have strict syntax and structure, and normal CoT prompting cannot induce LLMs to generate Text-to-SQL well. We have designed CC-DnP, SL-DnP, and GR-DnP prompting for Textto-SQL based on Text-to-SQL characteristics to induce LLMs to make helpful reasoning chains. Compared to the standard prompting, our proposed prompts improve execution accuracy by 4.3%, especially for hard-level Text-to-SQL tasks, improving execution accuracy by 10.8%."
        }
    ],
    "title": "Divide and Prompt: Chain of Thought Prompting for Text-to-SQL",
    "year": 2023
}