{
    "abstractText": "Smart contracts are self-executing programs on the blockchain that are critical to a range of industries, including finance, supply chain management, and healthcare. However, comprehending smart contracts can be challenging due to a lack of effective comments in most user-defined code. To address this challenge, we propose a novel retrieval-enhanced approach CCGRA that leverages retrieval knowledge to generate high-quality comments for Solidity language code. Our approach carefully eliminates duplicated data and template data in the widely-used smart contract dataset to ensure a high-quality corpus. Extensive experiments and comprehensive analysis demonstrate the effectiveness applicability of our approach after being compared with eight state-of-the-art baselines. Finally, we conduct a human study and find the comment quality generated by our approach is better than baselines in terms of similarity, naturalness, and informativeness.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zhenhua Zhang"
        },
        {
            "affiliations": [],
            "name": "Shizhan Chen"
        },
        {
            "affiliations": [],
            "name": "Guodong Fan"
        },
        {
            "affiliations": [],
            "name": "Guang Yang"
        },
        {
            "affiliations": [],
            "name": "Zhiyong Feng"
        }
    ],
    "id": "SP:65e593ac12a6680a58341eb423f574a5a5ca5229",
    "references": [
        {
            "authors": [
                "P. Heged\u0171s"
            ],
            "title": "Towards analyzing the complexity landscape of solidity based ethereum smart contracts",
            "venue": "Proceedings of the 1st International Workshop on Emerging Trends in Software Engineering for Blockchain, 2018, pp. 35\u201339.",
            "year": 2018
        },
        {
            "authors": [
                "L. Shi",
                "F. Mu",
                "X. Chen",
                "S. Wang",
                "J. Wang",
                "Y. Yang",
                "G. Li",
                "X. Xia",
                "Q. Wang"
            ],
            "title": "Are we building on the rock? on the importance of data preprocessing for code summarization",
            "venue": "Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, 2022, pp. 107\u2013119.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Zhuang",
                "Z. Liu",
                "P. Qian",
                "Q. Liu",
                "X. Wang",
                "Q. He"
            ],
            "title": "Smart contract vulnerability detection using graph neural network.",
            "venue": "in IJCAI,",
            "year": 2020
        },
        {
            "authors": [
                "S. Haiduc",
                "J. Aponte",
                "A. Marcus"
            ],
            "title": "Supporting program comprehension with source code summarization",
            "venue": "Proceedings of 2010 ACM/IEEE 32nd international conference on software engineering, vol. 2. IEEE, 2010, pp. 223\u2013226.",
            "year": 2010
        },
        {
            "authors": [
                "S. Haiduc",
                "J. Aponte",
                "L. Moreno",
                "A. Marcus"
            ],
            "title": "On the use of automated text summarization techniques for summarizing source code",
            "venue": "Proceedings of 2010 17th Working Conference on Reverse Engineering. IEEE, 2010, pp. 35\u201344.",
            "year": 2010
        },
        {
            "authors": [
                "P. Rodeghero",
                "C. Liu",
                "P.W. McBurney",
                "C. McMillan"
            ],
            "title": "An eyetracking study of java programmers and application to source code summarization",
            "venue": "IEEE Transactions on Software Engineering, vol. 41, no. 11, pp. 1038\u20131054, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "X. Hu",
                "G. Li",
                "X. Xia",
                "D. Lo",
                "Z. Jin"
            ],
            "title": "Deep code comment generation",
            "venue": "Proceedings of 2018 IEEE/ACM 26th International Conference on Program Comprehension (ICPC). IEEE, 2018, pp. 200\u2013 20 010.",
            "year": 2018
        },
        {
            "authors": [
                "Hu",
                "Xing",
                "Li",
                "Ge",
                "Xia",
                "Xin",
                "Lo",
                "David",
                "Jin",
                "Zhi"
            ],
            "title": "Deep code comment generation with hybrid lexical and syntactical information",
            "venue": "Empirical Software Engineering, vol. 25, no. 3, pp. 2179\u2013 2217, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "W. Ye",
                "R. Xie",
                "J. Zhang",
                "T. Hu",
                "X. Wang",
                "S. Zhang"
            ],
            "title": "Leveraging code generation to improve code retrieval and summarization via dual learning",
            "venue": "Proceedings of The Web Conference 2020, 2020, pp. 2309\u2013 2319.",
            "year": 2020
        },
        {
            "authors": [
                "G. Yang",
                "X. Chen",
                "Y. Zhou",
                "C. Yu"
            ],
            "title": "Dualsc: Automatic generation and summarization of shellcode via transformer and dual learning",
            "venue": "2022 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER). IEEE, 2022, pp. 361\u2013372.",
            "year": 2022
        },
        {
            "authors": [
                "W. Ahmad",
                "S. Chakraborty",
                "B. Ray",
                "K.-W. Chang"
            ],
            "title": "A transformerbased approach for source code summarization",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 2020, pp. 4998\u20135007.",
            "year": 2020
        },
        {
            "authors": [
                "Z. Feng",
                "D. Guo",
                "D. Tang",
                "N. Duan",
                "X. Feng",
                "M. Gong",
                "L. Shou",
                "B. Qin",
                "T. Liu",
                "D. Jiang"
            ],
            "title": "Codebert: A pre-trained model for programming and natural languages",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, 2020, pp. 1536\u20131547.",
            "year": 2020
        },
        {
            "authors": [
                "W. Ahmad",
                "S. Chakraborty",
                "B. Ray",
                "K.-W. Chang"
            ],
            "title": "Unified pretraining for program understanding and generation",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2021, pp. 2655\u20132668.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Wang",
                "W. Wang",
                "S. Joty",
                "S.C. Hoi"
            ],
            "title": "Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 2021, pp. 8696\u20138708.",
            "year": 2021
        },
        {
            "authors": [
                "J. Zhang",
                "X. Wang",
                "H. Zhang",
                "H. Sun",
                "X. Liu"
            ],
            "title": "Retrieval-based neural source code summarization",
            "venue": "Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering, 2020, pp. 1385\u20131397.",
            "year": 2020
        },
        {
            "authors": [
                "B. Wei",
                "Y. Li",
                "G. Li",
                "X. Xia",
                "Z. Jin"
            ],
            "title": "Retrieve and refine: exemplar-based neural comment generation",
            "venue": "Proceedings of 2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 2020, pp. 349\u2013360.",
            "year": 2020
        },
        {
            "authors": [
                "P. Tsankov",
                "A. Dan",
                "D. Drachsler-Cohen",
                "A. Gervais",
                "F. Buenzli",
                "M. Vechev"
            ],
            "title": "Securify: Practical security analysis of smart contracts",
            "venue": "Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security, 2018, pp. 67\u201382.",
            "year": 2018
        },
        {
            "authors": [
                "Y. Zhang",
                "S. Ma",
                "J. Li",
                "K. Li",
                "S. Nepal",
                "D. Gu"
            ],
            "title": "Smartshield: Automatic smart contract protection made easy",
            "venue": "2020 IEEE 27th International Conference on Software Analysis, Evolution and Reengineering (SANER). IEEE, 2020, pp. 23\u201334.",
            "year": 2020
        },
        {
            "authors": [
                "C. Shi",
                "Y. Xiang",
                "J. Yu",
                "K. Sood",
                "L. Gao"
            ],
            "title": "Machine translationbased fine-grained comments generation for solidity smart contracts",
            "venue": "Information and Software Technology, vol. 153, p. 107065, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "X. Hu",
                "Z. Gao",
                "X. Xia",
                "D. Lo",
                "X. Yang"
            ],
            "title": "Automating user notice generation for smart contract functions",
            "venue": "2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 2021, pp. 5\u201317.",
            "year": 2021
        },
        {
            "authors": [
                "G. Yang",
                "K. Liu",
                "X. Chen",
                "Y. Zhou",
                "C. Yu",
                "H. Lin"
            ],
            "title": "Ccgir: Information retrieval-based code comment generation method for smart contracts",
            "venue": "Knowledge-Based Systems, vol. 237, p. 107858, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "L. Ouyang",
                "J. Wu",
                "X. Jiang",
                "D. Almeida",
                "C.L. Wainwright",
                "P. Mishkin",
                "C. Zhang",
                "S. Agarwal",
                "K. Slama",
                "A. Ray"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "arXiv preprint arXiv:2203.02155, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "K. Papineni",
                "S. Roukos",
                "T. Ward",
                "W.-J. Zhu"
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th annual meeting of the Association for Computational Linguistics, 2002, pp. 311\u2013318.",
            "year": 2002
        },
        {
            "authors": [
                "S. Banerjee",
                "A. Lavie"
            ],
            "title": "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments",
            "venue": "Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, 2005, pp. 65\u201372.",
            "year": 2005
        },
        {
            "authors": [
                "C.-Y. Lin"
            ],
            "title": "Rouge: A package for automatic evaluation of summaries",
            "venue": "Text summarization branches out, 2004, pp. 74\u201381.",
            "year": 2004
        },
        {
            "authors": [
                "G. Yang",
                "Y. Zhou",
                "X. Chen",
                "C. Yu"
            ],
            "title": "Fine-grained pseudo-code generation method via code feature extraction and transformer",
            "venue": "2021 28th Asia-Pacific Software Engineering Conference (APSEC). IEEE, 2021, pp. 213\u2013222.",
            "year": 2021
        },
        {
            "authors": [
                "D. Gros",
                "H. Sezhiyan",
                "P. Devanbu",
                "Z. Yu"
            ],
            "title": "Code to comment \u201ctranslation\u201d: Data, metrics, baselining & evaluation",
            "venue": "Proceedings of 2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 2020, pp. 746\u2013757.",
            "year": 2020
        },
        {
            "authors": [
                "S. Robertson",
                "H. Zaragoza"
            ],
            "title": "The probabilistic relevance framework: BM25 and beyond",
            "venue": "Now Publishers Inc,",
            "year": 2009
        },
        {
            "authors": [
                "Z. Liu",
                "X. Xia",
                "A.E. Hassan",
                "D. Lo",
                "Z. Xing",
                "X. Wang"
            ],
            "title": "Neuralmachine-translation-based commit message generation: how far are we?",
            "venue": "Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering,",
            "year": 2018
        },
        {
            "authors": [
                "C. Yu",
                "G. Yang",
                "X. Chen",
                "K. Liu",
                "Y. Zhou"
            ],
            "title": "Bashexplainer: Retrieval-augmented bash code comment generation based on finetuned codebert",
            "venue": "2022 IEEE International Conference on Software Maintenance and Evolution (ICSME). IEEE, 2022, pp. 82\u201393.",
            "year": 2022
        },
        {
            "authors": [
                "S. Stapleton",
                "Y. Gambhir",
                "A. LeClair",
                "Z. Eberhart",
                "W. Weimer",
                "K. Leach",
                "Y. Huang"
            ],
            "title": "A human study of comprehension and code summarization",
            "venue": "Proceedings of the 28th International Conference on Program Comprehension, 2020, pp. 2\u201313.",
            "year": 2020
        },
        {
            "authors": [
                "G. Yang",
                "X. Chen",
                "J. Cao",
                "S. Xu",
                "Z. Cui",
                "C. Yu",
                "K. Liu"
            ],
            "title": "Comformer: Code comment generation via transformer and fusion method-based hybrid code representation",
            "venue": "2021 8th International Conference on Dependable Systems and Their Applications (DSA). IEEE, 2021, pp. 30\u201341.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014Code Comment Generation, Smart Contract, CodeT5, Information Retrieval\nI. INTRODUCTION\nSmart contracts are self-executing programs that reside on the blockchain. They comprise of template code and userdefined code. Template code provide basic functionalities such as asset transfer, voting, or escrow, while user-defined code tailors the smart contract to specific needs, including business logic and rules. However, user-defined code is often not commented, making it challenging for traders to read, especially in high-stakes transactions [1].\nAdding precise comments to user-defined code is critical for contract code comprehensibility and trust-building. However, the quality of the dataset used for training comment generation models can significantly impact their ability to accurately capture the nuances of user-defined code [2]. Our investigation highlights that the Solidity datasets [3], which widely used for smart contract code comment generation tasks, contains a significant amount of template code data that can skew\nDOI reference number: 10.18293/SEKE2023-090 \u2217Corresponding Author\nthe model\u2019s learning towards the template code. This, in turn, compromises the model\u2019s ability to generate high-quality comments for user-defined code. To address this challenge, we propose a solution that involves removing duplicate template code comments, which can lead to more effective and accurate comment generation for user-defined code.\nIn this paper, we propose a novel retrival-enhanced approach CCGRA (Code Comment Generation with Retrival-enhanced Approach). Our approach leverages the advantages of pretrained language models and retrieval techniques to generate reliable and high-quality comments for smart contract. To achieve this objective, we utilize the widely-used smart contract dataset [3], carefully eliminating duplicated data and template data to ensure a high-quality training set. In the end, the dataset we used contains 29,720 \u2329code, comment\u232apairs, and the experimental results indicate that it outperforms eight state-of-the-art baselines. Our approach showcases the potential of combining pre-trained language models and retrieval techniques to improve the quality of generated comment in the smart contract.\nThe main contributions can be summarized as follows:\n\u2022 We address the challenges associated with current dataset used for smart contract code comment generation by eliminating duplicated data and template data. \u2022 We conduct a comprehensive empirical study and a human evaluation to evaluate the performance of various baselines and demonstrate that our proposed approach, CCGRA, achieves state-of-the-art results. \u2022 We share our corpus and scripts on our project homepage 1 to promote the replication of our research.\n1https://github.com/ZZHbible/CCGRA"
        },
        {
            "heading": "II. RELATED WORK",
            "text": "A. Code Comment Generation\n1) Information retrieval-based approaches: In the early study phase, the researchers aimed to retrieve similar code from the software repository to improve the quality of code comments. Haiduc et al. [4], [5] considered two information retrieval models VSM and LSI. Rodeghero et al. [6] utilized the eye-tracking technique to identify the statements and keywords focused on by the developers.\n2) Deep learning-based approaches: Recently, most of the previous studies followed deep learning-based approaches and achieved promising results. For example, Hu et al. [7] proposed the approach DeepCom by analyzing abstract syntax trees (ASTs). Later, Hu et al. [8] further proposed the improved approach Hybrid-DeepCom. Ye et al. [9] and Yang et al. [10] exploited the probabilistic correlation between the code summarization and code generation task via dual learning.\nAhmad et al. [11] used the Transformer model to generate code comments. The Transformer model is a kind of Seq2Seq model based on multi-head self-attention, which could effectively capture long-range dependencies. Then the first pretrained model CodeBert on the source code was proposed by Feng et al. [12]. Later, Ahmad et al. [13] proposed another pre-training model PLBART for the source code. Recently, Wang et al. [14] proposed the pre-training model CodeT5.\n3) Retrieval-enhanced approaches: Except for the approaches based on information retrieval or deep learning, recent studies also proposed approaches, which could fuse information retrieval and deep learning. Zhang et al. [15] were the first to take advantage of both information retrieval and deep learning approaches and proposed the approach Rencos. Wei et al. [16] used the comment of the similar code snippet as exemplars and proposed the approach Re2Com.\nB. Code Intelligence in Smart Contract\n1) Vulnerability detection and repair: Code intelligence in smart contracts is its ability to detect and prevent security vulnerabilities. Code intelligence can analyze the code and identify potential security flaws, such as re-entrancy attacks, and recommend appropriate measures to mitigate them. Tsankov et al. [17] release static analysis tools named Securify used to analyze code vulnerabilities in smart contracts. Zhang et al. [18] proposed a system for automatically repairing code.\n2) Smart contract code comment generation: The comment generation of smart contracts can effectively help traders understand the contract content. Shi et al. [19] propose an automated translation approach based on AST, and leveraged reinforcement learning to train a syntax synthesizer to generate comprehensible comments. Hu et al. [20] exploited the Transformer and Pointer mechanism to learn the representation of source code and generates natural language descriptions.\nIn this study, our main focus is on combining the retrieval augmentation method with a pre-training model, leveraging the existing code library and the pre-training model generalization ability for generating comments for smart contract code."
        },
        {
            "heading": "III. APPROACH",
            "text": "In this section, we introduce the framework of CCGRA, which is illustrated in Fig. 1. Overall, CCGRA consists of two modules: (a) Retrieval Module. This module retrieves the most similar candidate code from the corpus to the given code. The comment associated with the retrieved code is then used as the basis for generating comment on the given code. (b) Generation Module. This module combines the retrieved comment with the given code, adds a prompt, and learns to generate the comment using the prior knowledge of a pretrained language model.\nA. Retrieval Module\nSuppose we index the corpus into a list of key-value pairs, i.e. Z = {(xi, yi)}, where x means the code and y means the comment. Then, given the input code x, the retrieval module \u0398 matches it with all code and returns the most similar code together with its comment:\n\u0398(x | Z) = {(x\u2032i, y\u2032i)} (1)\nIn this work, we build the retrieval engine based on the CCGIR [21]. Therefore, for the given input code x, we define the input code sequence {xi}Mi=1, where M is the length of the code sequence. Then we encode the sequence via CodeBert, extract the hidden states to get the semantic vector X \u2208 RD, which D denotes the hidden dimension of CodeBert. Then we further perform a linear transformation of X via BERTwhitening get X\u0303 \u2208 Rd, which can reduce its dimension from D to d. Thus for the target code snippet x and the code snippet xi in the corpus, we can get the semantic vectors X\u0303 and X\u0303i respectively. Then we can calculate their dot product score as their semantic similarity to select the top \u2212 k most similar code snippets as the candidates from the corpus Z .\nTo better combine syntactic and lexical knowledge of the source code, we separately utilize syntactic-level similarity and lexical-level similarity to find the most similar code x\u2032.\nSince the computational cost of calculating the similarities based on tree matching algorithms is high, we generate its corresponding AST sequence for each code snippet and then calculate the syntactical-level similarity via the edit distance. Since keywords occur more frequently than other tokens in the source code, these duplicated keywords may have a negative impact. We calculate the lexical-level similarity based on the set structure and Jaccard score.\nFinally, we use mixed score to retrieve the most similar code. For the code snippets x1 and x2, mixed score can be calculated as follows.\nmixed score (x1, x2) = \u03bb\u00d7 lexical similarity (x1, x2) + (1\u2212 \u03bb)\u00d7 syntactic similarity (x1, x2)\n(2)\nwhere \u03bb is a hyper-parameter for knowledge fusion, which can control the ratio of the lexical-level similarity and the syntactical-level similarity.\nB. Generation Module For the given input code x, we use the retrieval engine to find the the most similar code x\u2032 together with its comment y\u2032. As retrieval from a large corpus is computationally costly, we propose to retrieve from the labeled training data. In other words, we directly adopt the training data T = {(x1, y1) , . . . , (xN , yN )} as the indexed corpus Z , where xi is the input code and yi is the ground-truth comment. Note that during training, as the input code x is already indexed, we filter it from the retrieval results to avoid data leakage.\nInspired by Instruct-GPT [22], constructing instructions for downstream tasks can stimulate the potential of pre-trained models. For our task, we design the template function f by appending task-specific instructions as follows.\nf(x, y\u2032) = \u201csummarize Solidity : y\u2032 \u2295 x\u201d (3)\nHere we use f(x, y\u2032) to denote the input of the pre-trained model, and we use CodeT5 [14] as the backbone model for our task. The encoder inputs f(x, y\u2032) and outputs the hidden representation h = Enc(f(x, y\u2032)). Then the decoder iterates on the previously generated token y < j via selfattention, and then predicts the probability of the next text token P\u03a6(yj |y<j , x) = Dec(y<j , h). We train our model \u03a6 by minimizing the negative log-likelihood of the target text tokens y for a given input f(x, y\u2032). The formula can be defined as follows\nL\u03a6 = \u2212 |y|\u2211 j=1 logP\u03a6 (yj | y<j , f(x, y\u2032)) (4)"
        },
        {
            "heading": "IV. EXPERIMENTAL SETUP",
            "text": "In our empirical study, we aim to answer the following three research questions (RQs). RQ1: How effective is our CCGRA compared to the baseline models in terms of automatic performance measures? RQ2: How do various retrieval methods affect the retrievalaugmented pre-training model? RQ3: How effective is our CCGRA at generating higherquality comments in terms of human evaluation?\nA. Experimental Subjects\nIn our empirical study, we employed a carefully selected corpus of smart contracts sourced from Etherscan.io and provided by Zhuang et al. [3] and Yang et al. [21]. To ensure a high-quality dataset, we carefully eliminated duplicate data and template data. Our final dataset consisted of 29,720 pairs of data, which we split into training, validation, and testing sets in an 8:1:1 ratio. We also computed the average number of tokens in both code and comments, providing detailed statistics in Table I.\nB. Performance Evaluation Measures\nIn our experimental study, we use three performance evaluation measures (i.e., BLEU [23], METEOR [24], and ROUGE -L [25]) from the source code summarization domain to automatically evaluate the quality of the generated comments. Moreover, these performance measures have also been widely used in previous studies for source code summarization [26], which can alleviate the construct threats of our empirical study.\nTo avoid the result difference due to different performance measure implementation versions [27], we utilize the nlg-eval package2, which can ensure the implementation correctness of these performance measures and guarantee a fair comparison.\nC. Baselines\nTo show the competitiveness of our proposed approach CCGRA, we evaluate our proposed approach against eight state-of-the-art source code summarization baselines. Specifically, we classify these baselines into three groups. The first group is information retrieval approaches, including BM25 [28], NNGen [29], and CCGIR [21]. The second group is deep learning approaches, including CodeBert [12], UniXcoder [13], and CodeT5 [14]. The last group is hybrid approaches, including Rencos [15] and BashExplainer [30].\nD. Experimental Settings\nIn our empirical study, we use the packages Faiss3 and Transformers4 to implement our proposed approach CCGRA. The hyper-parameters and their values in our empirical study are summarized in Table II.\nAll the experiments run on a computer with an Intel(R) Xeon(R) Silver 4210 CPU and a GeForce RTX3090 GPU with 24 GB memory. The running OS platform is Ubuntu operation system.\n2https://github.com/Maluuba/nlg-eval 3https://github.com/facebookresearch/faiss 4https://github.com/huggingface/transformers"
        },
        {
            "heading": "V. EXPERIMENTAL RESULTS",
            "text": "A. RQ1: How effective is our CCGRA compared to the baseline models in terms of automatic performance measures?\nIn this RQ, we want to investigate how effective our approach is and how much performance improvement our approach can achieve over the baselines.\nThe results are shown in Table III where the best results are in bold fonts. We can find that we proposed CCGRA achieve the best results among the eight baselines. By comparing the average growth rate of four metrics with the baseline models, our proposed CCGRA outperforms them by a substantial margin of 26.58%, 22.17%, 18.58%, 23.88%, 25.49%, 6.21%, 30.12%, and 9.57% for BM25, NNGen, CCGIR, CodeBert, UniXcoder, Rencos, and BashExplainer, respectively. And the results indicate that retrieval-based methods outperform the CodeBert and UniXcoder models in terms of BLEU scores, suggesting that the comments generated by retrieval-based methods are effective in maintaining language consistency. However, in terms of ROUGE-L scores, pre-trained models perform better than retrieval-based methods, indicating that the comments generated by pre-trained models match the reference comments more closely in terms of semantic similarity. Moreover, it can be observed that the transformer with an encoder-decoder structure has better performance in generating Solidity comment (which does not appear in the pre-training data) compared to models with a single encoder structure.\nTo further demonstrate the effectiveness of CCGRA, we conduct qualitative analysis and two examples of generated comments are listed in Fig. 2. We denote comments written by humans in bold black font, and highlight comparisons\nin light blue and red. From Fig. 2(a), we find that through comparing the light blue and red comments that CCGIR and BashExplainer generated \u201cactivities\u201d and \u201ccontributors\u201d respectively, which is not correct. Meanwhile, CodeT5 generated \u201cpartner account\u201d which is not as accurate as human-written comments. Only CCGRA is more accurate in identifying the objects in generating comments compared to the approaches of retrieval and pre-trained models. From Fig. 2(b), we find that when generating long text comments, the comments generated by retrieval-based approaches are not accurate enough, and those generated by pre-trained models are relatively short and lacked comprehensiveness. However, CCGRA can effectively overcome the shortcomings of these approaches and generate comments that are most in line with human-written comments. CCGRA combines the advantages of retrieval-based approaches by retrieving similar comment and constructing prompts to reduce semantic bias in generating comments on unseen datasets during pretraining, achieving optimal results.\nSummary for RQ1: CCGRA can achieve better performance than eight state-of-the-art baselines in automatic evaluation.\nB. RQ2: How do various retrieval methods affect the retrievalaugmented pre-training model?\nTo show the relative importance of retrieval-based approach in CCGRA, we perform a series of ablation studies over the key modules.\nTable IV shows the performance comparison of several retrieval-based approaches combined with pre-trained models. We observe that an effective retrieval approach can promote the model to learn the representation of the code, thereby guiding the generation of appropriate comments by the model. Meanwhile, the results show that BM25, NNGen, and CCGIR can improve UniXcoder and CodeT5 by 10.08%, 12.08%, 23.67%, and 2.57%, 1.17%, 8.40%, respectively, in terms of the BLEU-4 score. Furthermore, our results show that an improved retrieval ability is positively correlated with the model\u2019s learning ability. We find that the performance of the retrieval module directly impacted the model\u2019s overall performance, which can highlight the positive impact of retrieval augment.\nSummary for RQ2: The incorporation of a retrieval module into the overall model has a significant impact on its performance. Specifically, the performance of the retrieval module is positively correlated with the overall model\u2019s effectiveness.\nC. RQ3: How effective is our CCGRA at generating higherquality comments in terms of human evaluation?\nAlthough automatic performance metrics can evaluate the gap between the generated comments and reference comments written by humans, these performance measures may not truly reflect the semantic similarity between different comments [31]. To verify the effectiveness of our proposed approach CCGRA, we further conducted a human study. In our human study, we only compare CCGRA with CodeT5, which can achieve the best performance in the all baselines. We follow the methodology used by Wei et al. [16] and Yang et al. [32] to conduct the human evaluation from three aspects:\n\u2022 Similarity evaluates the semantic similarity between the generated comments and the reference comments. \u2022 Naturalness evaluates the fluency of the generated comments. \u2022 Informativeness evaluates the amount of content transferred from the code to the generated comments.\nWe invite five master students, who have 1\u223c3 years of smart contract experience and have good English reading ability. Due to the high cost of manually analyzing all these samples in the testing set, we use a commonly-used sampling method [33] to select the minimum random samples. The number of selected samples can be determined via the following formula:\nMIN = n0\n1 + n0\u22121size (5)\nwhere n0 is related to the confidence level and the error margin n0 ( = Z 2\u00d70.25 e2 ) . Here Z is the confidence level score and e is the error margin. size is the size of the testing set. In this RQ, we select MIN examples with the error margin e = 0.05 at 95% confidence level. Specifically, we randomly selected 340 samples from the corpus.\nFor each code snippet, we generate a questionnaire for each participant. Each participant is asked to score each comment\nin terms of similarity, naturalness, and informativeness aspects for two comments generated by CCGRA and the baseline CodeT5 respectively. All these scores are integers, ranging from 0 to 4 (the higher the better). During the comment quality evaluation process, the participants can search the Internet for relevant information and unfamiliar concepts. To guarantee a fair comparison, the participants do not know which comment is generated by which approach, and the order of questionnaires is different for different participants. To guarantee the comment evaluation quality, we need each participant to review only 50 code snippets in half a day to avoid fatigue.\nWe compute the average score of the participants\u2019 feedback and the results are shown in Table V. In this table, we can find that CCGRA can outperform the approach CodeT5 by 0.70, 0.29, and 0.17 respectively in terms of similarity, naturalness, and informativeness. Therefore, our human study can further verify the competitiveness of CCGRA.\nSummary for RQ3: Our human study shows that CCGRA can generate higher quality comments in terms of similarity, naturalness, and informativeness."
        },
        {
            "heading": "VI. THREATS TO VALIDITY",
            "text": "Internal threats. The internal threat is the potential defects in the implementation of our proposed approach and baselines. To alleviate this threat, we first check the code carefully and re-implement baselines according to the original studies. External threats. The external threat is the choice of corpora. To alleviate this threat, we select the popular corpora, which have been widely used in previous studies on smart contract code summarization [21]. Construct threats. This threat relates to the suitability of our selected performance measures. To alleviate this threat, we consider the widely used performance measures and also conduct a human study to verify the effectiveness of our proposed approach."
        },
        {
            "heading": "VII. CONCLUSION",
            "text": "In this study, we propose a novel retrieval-enhanced approach CCGRA for generating high-quality comments for user-defined code in smart contracts. By leveraging retrieval techniques and pre-trained language models, CCGRA is able to produce reliable and informative comments that improve the comprehensibility and trust-building of smart contract code. We have demonstrated the effectiveness of our approach through extensive experiments and comprehensive analysis. In\naddition, a human study was conducted to show that the quality of comments generated by CCGRA outperforms baselines in terms of similarity, naturalness, and informativeness.\nIn the future, we aim to further improve the performance of CCGRA by exploring advanced code representation methods. Additionally, we plan to expand our dataset by mining more high-quality data of smart contracts, which will facilitate the practical application of our research in various industries, such as finance and healthcare."
        },
        {
            "heading": "ACKNOWLEDGMENT",
            "text": "This work is supported by the National Natural Science Key Foundation of China grant No.62032016 and No.61832014."
        }
    ],
    "title": "CCGRA: Smart Contract Code Comment Generation with Retrieval-enhanced Approach",
    "year": 2023
}