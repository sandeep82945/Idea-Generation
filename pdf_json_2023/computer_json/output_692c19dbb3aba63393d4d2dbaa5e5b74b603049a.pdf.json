{
    "abstractText": "We present a conceptually simple, efficient, and general framework for localization problems in DETR-like models. We add plugins to well-trained models instead of inefficiently designing new models and training them from scratch. The method, called RefineBox, refines the outputs of DETR-like detectors by lightweight refinement networks. RefineBox is easy to implement and train as it only leverages the features and predicted boxes from the welltrained detection models. Our method is also efficient as we freeze the trained detectors during training. In addition, we can easily generalize RefineBox to various trained detection models without any modification. We conduct experiments on COCO and LVIS 1.0. Experimental results indicate the effectiveness of our RefineBox for DETR and its representative variants (Figure 1). For example, the performance gains for DETR, Conditinal-DETR, DAB-DETR, and DN-DETR are 2.4 AP, 2.5 AP, 1.9 AP, and 1.6 AP, respectively. We hope our work will bring the attention of the detection community to the localization bottleneck of current DETR-like models and highlight the potential of the RefineBox framework. Code and models will be publicly available at: https://github.com/YiqunChen1999/RefineBox.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yiqun Chen"
        },
        {
            "affiliations": [],
            "name": "Qiang Chen"
        },
        {
            "affiliations": [],
            "name": "Peize Sun"
        },
        {
            "affiliations": [],
            "name": "Shoufa Chen"
        },
        {
            "affiliations": [],
            "name": "Jingdong Wang"
        },
        {
            "affiliations": [],
            "name": "Jian Cheng"
        }
    ],
    "id": "SP:b64c1c451f8315b6a6ffdbf2d500162efb99e407",
    "references": [
        {
            "authors": [
                "Daniel Bolya",
                "Sean Foley",
                "James Hays",
                "Judy Hoffman"
            ],
            "title": "Tide: A general toolbox for identifying object detection errors",
            "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Zhaowei Cai",
                "Nuno Vasconcelos"
            ],
            "title": "Cascade r-cnn: Delving into high quality object detection",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Nicolas Carion",
                "Francisco Massa",
                "Gabriel Synnaeve",
                "Nicolas Usunier",
                "Alexander Kirillov",
                "Sergey Zagoruyko"
            ],
            "title": "End-toend object detection with transformers",
            "venue": "In European conference on computer vision,",
            "year": 2020
        },
        {
            "authors": [
                "Qiang Chen",
                "Xiaokang Chen",
                "Gang Zeng",
                "Jingdong Wang"
            ],
            "title": "Group detr: Fast training convergence with decoupled one-to-many label assignment",
            "venue": "arXiv preprint arXiv:2207.13085,",
            "year": 2022
        },
        {
            "authors": [
                "Qiang Chen",
                "Jian Wang",
                "Chuchu Han",
                "Shan Zhang",
                "Zexian Li",
                "Xiaokang Chen",
                "Jiahui Chen",
                "Xiaodi Wang",
                "Shuming Han",
                "Gang Zhang"
            ],
            "title": "Group detr v2: Strong object detector with encoder-decoder pretraining",
            "venue": "arXiv preprint arXiv:2211.03594,",
            "year": 2022
        },
        {
            "authors": [
                "Bowen Cheng",
                "Yunchao Wei",
                "Rogerio Feris",
                "Jinjun Xiong",
                "Wen-mei Hwu",
                "Thomas Huang",
                "Humphrey Shi"
            ],
            "title": "Decoupled classification refinement: Hard false positive suppression for object detection",
            "venue": "arXiv preprint arXiv:1810.04002,",
            "year": 2018
        },
        {
            "authors": [
                "Bowen Cheng",
                "Yunchao Wei",
                "Honghui Shi",
                "Rogerio Feris",
                "Jinjun Xiong",
                "Thomas Huang"
            ],
            "title": "Revisiting rcnn: On awakening the classification power of faster rcnn",
            "venue": "In Proceedings of the European conference on computer vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Spyros Gidaris",
                "Nikos Komodakis"
            ],
            "title": "Object detection via a multi-region and semantic segmentation-aware cnn model",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2015
        },
        {
            "authors": [
                "Ross Girshick"
            ],
            "title": "Fast r-cnn",
            "venue": "Proceedings of the IEEE international conference on computer vision,",
            "year": 2015
        },
        {
            "authors": [
                "Ross Girshick",
                "Jeff Donahue",
                "Trevor Darrell",
                "Jitendra Malik"
            ],
            "title": "Rich feature hierarchies for accurate object detection and semantic segmentation",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2014
        },
        {
            "authors": [
                "Agrim Gupta",
                "Piotr Dollar",
                "Ross Girshick"
            ],
            "title": "Lvis: A dataset for large vocabulary instance segmentation",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Kaiming He",
                "Georgia Gkioxari",
                "Piotr Doll\u00e1r",
                "Ross Girshick"
            ],
            "title": "Mask r-cnn",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Borui Jiang",
                "Ruixuan Luo",
                "Jiayuan Mao",
                "Tete Xiao",
                "Yuning Jiang"
            ],
            "title": "Acquisition of localization confidence for accurate object detection",
            "venue": "In Proceedings of the European conference on computer vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Harold W Kuhn"
            ],
            "title": "The hungarian method for the assignment problem",
            "venue": "Naval research logistics quarterly,",
            "year": 1955
        },
        {
            "authors": [
                "Weicheng Kuo",
                "Yin Cui",
                "Xiuye Gu",
                "AJ Piergiovanni",
                "Anelia Angelova"
            ],
            "title": "F-vlm: Open-vocabulary object detection upon frozen vision and language models",
            "venue": "arXiv preprint arXiv:2209.15639,",
            "year": 2022
        },
        {
            "authors": [
                "Feng Li",
                "Hao Zhang",
                "Shilong Liu",
                "Jian Guo",
                "Lionel M Ni",
                "Lei Zhang"
            ],
            "title": "Dn-detr: Accelerate detr training by introducing query denoising",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Yanghao Li",
                "Yuntao Chen",
                "Naiyan Wang",
                "Zhaoxiang Zhang"
            ],
            "title": "Scale-aware trident networks for object detection",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2019
        },
        {
            "authors": [
                "Feng Lin",
                "Wenze Hu",
                "Yaowei Wang",
                "Yonghong Tian",
                "Guangming Lu",
                "Fanglin Chen",
                "Yong Xu",
                "Xiaoyu Wang"
            ],
            "title": "Million-scale object detection with large vision model",
            "venue": "arXiv preprint arXiv:2212.09408,",
            "year": 2022
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Piotr Doll\u00e1r",
                "Ross Girshick",
                "Kaiming He",
                "Bharath Hariharan",
                "Serge Belongie"
            ],
            "title": "Feature pyramid networks for object detection",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C Lawrence Zitnick"
            ],
            "title": "Microsoft coco: Common objects in context",
            "venue": "In European conference on computer vision,",
            "year": 2014
        },
        {
            "authors": [
                "Yutong Lin",
                "Ze Liu",
                "Zheng Zhang",
                "Han Hu",
                "Nanning Zheng",
                "Stephen Lin",
                "Yue Cao"
            ],
            "title": "Could giant pretrained image models extract universal representations",
            "venue": "arXiv preprint arXiv:2211.02043,",
            "year": 2043
        },
        {
            "authors": [
                "Shilong Liu",
                "Feng Li",
                "Hao Zhang",
                "Xiao Yang",
                "Xianbiao Qi",
                "Hang Su",
                "Jun Zhu",
                "Lei Zhang"
            ],
            "title": "Dab-detr: Dynamic anchor boxes are better queries for detr",
            "venue": "arXiv preprint arXiv:2201.12329,",
            "year": 2022
        },
        {
            "authors": [
                "Wei Liu",
                "Shengcai Liao",
                "Weidong Hu",
                "Xuezhi Liang",
                "Xiao Chen"
            ],
            "title": "Learning efficient single-stage pedestrian detectors by asymptotic localization fitting",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Ze Liu",
                "Yutong Lin",
                "Yue Cao",
                "Han Hu",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Baining Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Xin Lu",
                "Buyu Li",
                "Yuxin Yue",
                "Quanquan Li",
                "Junjie Yan"
            ],
            "title": "Grid r-cnn",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Depu Meng",
                "Xiaokang Chen",
                "Zejia Fan",
                "Gang Zeng",
                "Houqiang Li",
                "Yuhui Yuan",
                "Lei Sun",
                "Jingdong Wang"
            ],
            "title": "Conditional detr for fast training convergence",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Dimity Miller",
                "Georgia Goode",
                "Callum Bennie",
                "Peyman Moghadam",
                "Raja Jurdak"
            ],
            "title": "Why object detectors fail: Investigating the influence of the dataset",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),",
            "year": 2022
        },
        {
            "authors": [
                "Dimity Miller",
                "Peyman Moghadam",
                "Mark Cox",
                "Matt Wildie",
                "Raja Jurdak"
            ],
            "title": "What\u2019s in the black box? the false negative mechanisms inside object detectors",
            "venue": "IEEE Robotics and Automation Letters,",
            "year": 2022
        },
        {
            "authors": [
                "Jiangmiao Pang",
                "Kai Chen",
                "Jianping Shi",
                "Huajun Feng",
                "Wanli Ouyang",
                "Dahua Lin"
            ],
            "title": "Libra r-cnn: Towards balanced learning for object detection",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga"
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Quazi Marufur Rahman",
                "Niko S\u00fcnderhauf",
                "Feras Dayoub"
            ],
            "title": "Did you miss the sign? a false negative alarm system for traffic sign detectors",
            "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),",
            "year": 2019
        },
        {
            "authors": [
                "Cyrus Anderson",
                "Ram Vasudevan",
                "Matthew Johnson-Roberson"
            ],
            "title": "Failing to learn: Autonomously identifying perception failures for self-driving cars",
            "venue": "IEEE Robotics and Automation Letters,",
            "year": 2018
        },
        {
            "authors": [
                "Shaoqing Ren",
                "Kaiming He",
                "Ross Girshick",
                "Jian Sun"
            ],
            "title": "Faster r-cnn: Towards real-time object detection with region proposal networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2015
        },
        {
            "authors": [
                "Hamid Rezatofighi",
                "Nathan Tsoi",
                "JunYoung Gwak",
                "Amir Sadeghian",
                "Ian Reid",
                "Silvio Savarese"
            ],
            "title": "Generalized intersection over union: A metric and a loss for bounding box regression",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Cristina Vasconcelos",
                "Vighnesh Birodkar",
                "Vincent Dumoulin"
            ],
            "title": "Proper reuse of image classification features improves object detection",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Xiongwei Wu",
                "Doyen Sahoo",
                "Daoxin Zhang",
                "Jianke Zhu",
                "Steven CH Hoi"
            ],
            "title": "Single-shot bidirectional pyramid networks for high-quality object detection",
            "year": 2020
        },
        {
            "authors": [
                "Qinghua Yang",
                "Hui Chen",
                "Zhe Chen",
                "Junzhe Su"
            ],
            "title": "Introspective false negative prediction for black-box object detectors in autonomous driving",
            "year": 2021
        },
        {
            "authors": [
                "Hongkai Zhang",
                "Hong Chang",
                "Bingpeng Ma",
                "Naiyan Wang",
                "Xilin Chen"
            ],
            "title": "Dynamic r-cnn: Towards high quality object detection via dynamic training",
            "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Hao Zhang",
                "Feng Li",
                "Shilong Liu",
                "Lei Zhang",
                "Hang Su",
                "Jun Zhu",
                "Lionel Ni",
                "Harry Shum"
            ],
            "title": "Dino: Detr with improved denoising anchor boxes for end-to-end object detection",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Xingyi Zhou",
                "Vladlen Koltun",
                "Philipp Kr\u00e4henb\u00fchl"
            ],
            "title": "Probabilistic two-stage detection",
            "venue": "arXiv preprint arXiv:2103.07461,",
            "year": 2021
        },
        {
            "authors": [
                "Xizhou Zhu",
                "Weijie Su",
                "Lewei Lu",
                "Bin Li",
                "Xiaogang Wang",
                "Jifeng Dai"
            ],
            "title": "Deformable detr: Deformable transformers for end-to-end object detection",
            "venue": "arXiv preprint arXiv:2010.04159,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Transformer [40] has become an alternative to convolutional neural networks in object detection. DETR [3] introduces the Transformer into object detection and forms a new detection fashion. Recently, some DETR-like models became state-of-the-art on the COCO benchmark [23], e.g., Co-DETR [48], Group-DETR-V2 [5], DETA [32], DINO [45], etc. Despite their success, the detection community might not be clear about the bottleneck of the DETR-like models and the potential development direction.\nThe DETR-like detectors produce a set of predictions and assign each ground truth a prediction result during training. The positive samples, i.e., that match with ground truth,\ncontribute significantly to the model training. We are curious about how to improve the prediction results of the DETR-like models by correcting the positive samples during training. We start by exploring the performance upper bound of DETR-like models with perfect localization or classification ability in a simple way. Specifically, we calculate the performance gains brought by eliminating classification and localization errors of the positive samples. We observe that the gain of improving localization ability is substantial while improving classification capability is much lower, i.e., about 25 AP vs. 3-5 AP (Figure 3, discussed later in Section 3). This phenomenon suggests that the localization capability is the bottleneck restricting the performance of current DETR-like models rather than the classification ability. Based on the above finding, we focus on improving the localization ability of DETR-like models.\nWe present a novel framework RefineBox to solve the localization problem in this paper. Our method, called Re-\nar X\niv :2\n30 7.\n11 82\n8v 1\n[ cs\n.C V\n] 2\n1 Ju\nl 2 02\nfineBox, adopts the two-stage detection framework by applying a lightweight box refinement network to the welltrained object detectors, as illustrated in Figure 2. The object detectors are responsible for producing the detection results, including the classification logits and the bounding boxes. The extracted features and predicted bounding boxes from the trained detector serve as inputs to the refinement network, which is responsible for enhancing the boxes to mitigate localization errors.\nRefineBox explores a new fashion for improving the localization quality of the trained detectors, which is different from previous works on two-stage detection that mainly design and train new detectors. Our refinement network acts like a plug-and-play plugin and brings clear benefits. It is easy to implement and train as we only need to fit the newly added parameters by freezing the trained detectors. Furthermore, it is highly efficient and cost-effective, facilitating speedy experimentation and the implementation of enhancements to large models with limited hardware. Moreover, RefineBox can be generalized to various detection models without any modification.\nWe validate the effectiveness of our method on DETR [3] and its representative variants, including ConditionalDETR-R50 [29], DAB-DETR-R50 [25], and DN-DETRR50 [19]. Without bells and whistles, our RefineBox brings non-trivial improvements to them, as illustrated in Figure 1. We also show that our RefineBox can easily combine with techniques that aim to speed up model convergence, e.g., Group-DETR [4]. Our example gives a gain of 2.7 AP on Group-Conditional-DETR-R50 on the COCO dataset [23]."
        },
        {
            "heading": "2. Related Works",
            "text": "Two-Stage Detection. The two-stage detection architecture [28, 44, 20, 33] has become predominant several years ago due to the success of the models in the R-CNN series [11, 10, 37]. A typical two-stage detection model splits the detection pipeline into two stages: (1) produces a set of region proposals that might contain an object; (2) adjust the proposed bounding boxes and predict the categories. Our RefineBox takes the idea of two-stage detection but makes some changes: RefineBox builds on top of well-trained detection models and freezes them. Error Analysis for Object Detection. The community\nmeasures the detection performance by AP (Averaged Precision) and mAP (mean Average Precision). To further understand why we consider a detection a failure, Hoiem et al. [15] introduced a categorization for false positive detections. The COCO [23] analysis tool further computes the numbers of several error types progressively. However, the analysis results rely on the computing order of the error types [1]. TIDE [1] then solves this issue by avoiding computing errors progressively. DCR [7, 6] recognizes that the classification capability is responsible for the false positives of the Faster RCNN [37]. It combines the Faster RCNN and a classification refinement network to mitigate this issue. Some works [35, 36, 43, 30] focus on false negatives and solve this issue by introducing some prior. Miller et al. [31] split the detection pipeline into several stages to locate the stage when a model fails to detect an object.\nFreezing Detector\u2019s Parameters. Recently, there are some works [39, 24, 21, 18] on exploiting the frozen components on detection. Vasconcelos et al. [39] pay attention to freezing the backbone. They observed that with the powerful detection components, detection models trained with frozen backbones perform better than that with a non-frozen strategy. Lin et al. [24] further answer how to better apply this frozen setting. An open-vocabulary detection method, FVLM [18], freezes the vision and language models and only fits the detection head during training. Recently, LargeUniDet [21] freezes the backbone to handle the millionscale multi-domain universal object detection problem. Although RefineBox shares the similar idea of freezing trained models, it has several differences: (1) They aim to reuse the image classification features properly. In contrast, we explore a new fashion for improving localization quality. (2) They freeze the classification model while RefineBox freezes the whole detector."
        },
        {
            "heading": "3. Motivation",
            "text": "The recent DETR-like models produce a set of predictions and adopt the one-to-one assignment strategy to find positive samples for training. We are eager to know how to improve the performance of DETR-like models by reducing the errors of positive samples during training. As the detection task consists of two sub-tasks: classification and localization, we try to answer this question by investigating the classification and localization errors of the positive samples.\nWe start by finding predictions of an object detector that match the ground truth via Hungarian Matching [17]. To calculate the performance with perfect localization, we replace the predicted bounding boxes with ground truth to eliminate the localization errors. Similarly, replacing predicted classification results with ground truth labels enables us to simulate models with perfect classification ability and calculate their ideal performance.\nWe visualize the performance of various DETR-like models [3, 29, 25, 19] before and after replacing the predictions with the ground truth. We have two observations from Figure 3. First, eliminating the localization errors leads to impressive improvements in the detectors (green bars vs. gray bars). We observe that the gains of most models are about 25 AP, e.g., 24.8 AP for DETR-R50 [3], 25.5 AP for DAB-DETR-50 [25], and 25.2 AP for DN-DETR-R50 [19], etc. This indicates that there is significant potential to improve localization performance.\nSecond, eliminating classification errors leads to far smaller gains (blue bars vs. gray bars), e.g., 3.7 AP for DETR-R50, 5.3 AP for DAB-DETR-R50, and 5.6 AP for DN-DETR-R50. This suggests that models with perfect classification ability can only provide limited performance gains for current models. The remaining errors are due to localization. Based on the above observations, we suppose\nlocalization is the bottleneck for current DETR-like models: The localization errors limit the possible benefits of improving the classification ability. Even models with ideal classification performance can only bring limited gains.\nThe above findings motivate our work to tackle the challenges of poor localization ability. We propose RefineBox, a novel framework for refining the predicted bounding boxes of existing detection models. We present a lightweight example in the following, which only introduces around 0.5 M parameters but produces non-trivial improvements, introduced next."
        },
        {
            "heading": "4. Methodology",
            "text": "In this section, we first give more details of the RefineBox framework and the example in Section 4.1. Then we present discussions on the relation to other methods in Section 4.2."
        },
        {
            "heading": "4.1. Method Details",
            "text": "The proposed RefineBox (Figure 2) incorporates two components: (1) an object detector, and (2) a refinement network for refining the predicted bounding boxes. In our experiments, the refinement network (Figure 4) mainly performs two steps. Firstly, by leveraging the feature pyramid network (FPN) [22], the refinement network extracts the feature pyramid from the backbone of the trained detection model. Secondly, it efficiently leverages the multi-scale features via a sequence of Refiner modules to refine the boxes predicted by the detector. During training, we freeze the parameters of the trained object detector and only update the weights of the FPN and Refiner modules. Extracting the Multi-scale Features. The feature pyramid of the detector backbone is utilized as the input to the FPN, which subsequently reduces the channels to a specific numerical value denoted as C and is regarded as the model di-\nmension. Specifically, for the ResNet-style [14] backbone, the inputs of FPN are res2, res3, res4, and res5. As for the Swin-style [27], p0, p1, p2, and p3 serve as the inputs to the FPN. Supplementary material provides ablation studies on the model dimension. Refining the Boxes. Once we obtain the multi-scale features, we apply a series of Refiner modules to refine the predicted boxes, as depicted in Figure 4. The Refiner module takes the boxes and the feature pyramid as inputs. Similar to Deformable DETR [47], we correct the boxes by predicting the deltas to the ground truth. Each Refiner module comprises an ROI Align layer [13], a Residual Module [14], and a multi-layer perceptron (MLP). Our Residual Module consists of several Bottleneck Blocks [14]. Each block stacks a 1\u00d7 1 conv layer, a 3\u00d7 3 conv layer, and a 1\u00d7 1 conv layer. The bottleneck channels correspond to the input channels of the 3 \u00d7 3 conv layer. We find that sharing the weights of the Refiner modules does not harm the performance (see Section 5.4), so their parameters are shared unless specified. Ablation on the number of Refiner modules, the bottleneck channels, and the number of Bottleneck Blocks are available in the supplementary material. Loss Functions. Our RefineBox framework only refines the predicted bounding boxes and remains the classification results unchanged. Therefore, we only consider the regression loss in this framework. Specifically, we apply the GIoU loss [38] and the L1 loss to the outputs of each Refiner module. Following the common practice [3], the weights of GIoU loss and L1 loss remains unchanged. We sum the regression losses over all Refiner modules. Training and Inference. During training, we select the predictions that match the ground truth and refine the corresponding bounding boxes. Other predictions that are not assigned ground truth are ignored. For inference, we select top K predictions according to the classification scores for refinement, as it\u2019s impossible to access the ground truth. The value of K is determined as the highest number of objects in an image in the dataset, e.g., 100 in COCO [23]. Implementation Details. Unless specified, we set the output channels C of FPN [22] as 64 and the number of Bottleneck Blocks as 3. The output size of ROI Align layer is (7, 7). The default value of channels of 3\u00d7 3 conv layers in Bottleneck blocks is 64. We refine the top 100 predictions during inference. The number of Refiner modules is 3."
        },
        {
            "heading": "4.2. Discussion: Relation to Other Formulation.",
            "text": "Relation to Two-Stage/Cascade Detection. In principle our RefineBox is a two-stage detection framework [11, 10, 37]. In the first stage, the detector produces initial predictions. In this stage, the detection model serves as a region proposal network. In the second stage, a refinement network takes the proposals as input and outputs the refined results. Though our RefineBox takes the two-stage detec-\ntion pipeline, there are some differences from the typical two-stage detectors. In the first stage, the detector produces detection results instead of rejecting proposals that are unlikely to have an object. In addition, we build RefineBox on top of well-trained detectors and freeze them, and only train the newly added plug-and-play refinement network.\nCascade detection [2] is an extension of the two-stage detection. Several works have adopted a similar idea, e.g., IoUNet [16], Multi-Region CNN [9], ALFNet [26], and BPN [41]. The cascade detection architecture consists of a sequence of detection heads and is trained with increasing IoU thresholds. Thanks to the simplicity of our RefineBox, we can easily generalize it to a cascade refinement framework by stacking several refinement networks. It is worth noting that we still only need to fit the refinement network. Relation to Frozen Feature Extractors. There are several works for freezing feature extractors [39, 18], e.g., image and text feature extractors. One representative work is [39]. The method concentrates on freezing the backbone of object detectors. Although our approach also employs a similar idea of freezing model components, the motivation behind our method is quite different. Their methods aim to transfer the well-trained image/text feature extractor to object detection and utilize features produced by pre-trained models. In contrast, we aim to improve the localization ability of trained detection models efficiently."
        },
        {
            "heading": "5. Experimental Results",
            "text": "In this section, we present the experimental results to validate the effectiveness of our RefineBox."
        },
        {
            "heading": "5.1. Settings",
            "text": "We conduct our experiments based on PyTorch [34], Detectron2 [42] and detrex [8]. Unless specified, we train our model for 12 epochs (90k iterations). We consider DETR [3] and its several popular variants as our baselines. We follow the training settings of baselines in detrex implementation to set the optimization algorithm and other hyperparameters. Datasets. We adopt the following two challenging benchmarks: (1) COCO [23]: The challenging COCO dataset contains about 118k images for training (train2017 split) and 5k images for validation (val2017). We mainly conduct experiments on this dataset. (2) LVIS [12]: LVIS contains about 100k images for training and about 19.8k images for validation."
        },
        {
            "heading": "5.2. Main Results",
            "text": "We conduct experiments on the challenging COCO and LVIS 1.0 datasets to demonstrate the effectiveness of our RefineBox."
        },
        {
            "heading": "5.2.1 Results on COCO",
            "text": "Improvements Over Various Baselines. We present the results on the COCO dataset in Table 1, our RefineBox significantly improves DETR [3] and its variants, e.g., Conditional-DETR [29], DAB-DETR [25], and DNDETR [19]. Specifically, our method brings a 2.4 AP gain to DETR-R50 (from 42.0 AP to 44.4 AP). Besides, the RefineBox gives 2.5 AP, 1.9 AP, and 1.6 AP improvements to Conditional-DETR-R50, DAB-DETR-R50, and DN-DETR-R50, respectively. Remarkably, our approach only adjusts the bounding boxes and keeps the classification results unchanged. This phenomenon indicates our RefineBox is a promising framework.\nWe also note that our RefineBox considerably enhances recall rates. For example, the gains on AR100 for DETRR50 and Conditional-DETR-R50 are 3.7 and 4.1, respectively. The improvements on ARs are more impressive: 6.6 for DETR-R50 and 7.5 for Conditional-DETR-R50. RefineBox and More Powerful Backbones. To further demonstrate robustness and effectiveness, we explore the influence of a detector with a more powerful backbone, e.g., ResNet-101 [14] and Swin-Tiny [27]. Results presented in Table 1 show that our RefineBox still works. With the ResNet-101 as the backbone of the object detector, our RefineBox gives 2.0 AP, 2.1 AP, and 1.4 AP improvements on\nthe DETR, Conditional-DETR, and DAB-DETR, respectively. Besides, we also observe 1.9 AP gains on DABDETR-Swin-Tiny."
        },
        {
            "heading": "5.2.2 Results on LVIS 1.0",
            "text": "We also conduct experiments on the LVIS 1.0 dataset [12] to evaluate our method. Firstly, we train DAB-DETR-R50 from scratch on LVIS without performing any modification. All hyperparameters except the ones related to the number of categories are the same as those on the COCO dataset. We then freeze the DAB-DETR-R50 and apply our RefineBox. We also apply the federated loss [46] to the baseline to improve the detection results.\nWe have similar findings on LVIS as COCO from Table 2: (1) our RefineBox improves the baseline by producing more accurate bounding boxes. For example, the AP increased from 26.0 to 28.8 AP (+ 2.8 AP); (2) the gains mainly come from the AP75 and APs; (3) the improvements on the recall are significant. These phenomena are consistent with that on the COCO dataset and imply our method improves the localization quality.\nInterestingly, our RefineBox gives limited gains on rare classes (+ 0.8 AP) compared to the frequent categories (+ 3.9 AP). For better understanding, we eliminate the classification errors (refer to Section 3 for more details) and present\nthe ideal performance in Table 3. We observe that: (1) Our RefineBox increases the upper bound of APr, APc, and APf by 4.0 AP, 4.9 AP, and 5.0 AP, respectively, which are comparable. (2) The upper bound of rare classes is much higher than the actual APr, no matter if we apply the RefineBox. These phenomena imply that the classification capability of the rare classes is one of the bottlenecks on LVIS: our method improves the localization quality, but detection fails due to the wrong classification results of DAB-DETR-R50."
        },
        {
            "heading": "5.3. Going Deeper",
            "text": "We delve deeper into exploring an extension of our model and the rationale behind the effectiveness of our approach. We first present that RefineBox improves the ideal performance (i.e., error-free classification). We then demonstrate that our RefineBox can work with techniques designed to speed up model convergence. We also discuss several possible factors of the success of our model: (1) Whether the gains originate from the additional parameters or FLOPs; (2) Whether the gains come from the extra training time. Additionally, we consider: (1) Would an additional classification branch lead to further improvement? (2) would joint training with the detector aid in enhancing the model\u2019s performance? Finally, we hypothesize that the efficient utilization of multi-scale features may contribute to the success of the refinement network. RefineBox Increases the Ideal Performance. Following how we locate the bottleneck of detection models (Section 3), we replace the labels with ground truth to investigate the ideal performance (the case without classification errors). Figure 5 suggests that our RefineBox effectively reduces the localization errors and improves the ideal performance of detection models. The results suggest that en-\nPerformance for Detectors with Perfect Classification\nhancing classification ability would likely result in further gains. We leave this exploration as future work. Improvements on Group DETR Our method is orthogonal to the recently proposed Group DETR [4] and can further boost the latter\u2019s performance. Specifically, we train Conditional-DETR-R50 with 11 groups for 12 epochs and then apply our RefineBox. As illustrated in Table 4a, our RefineBox improves the AP, AP75, APs, and APm of the baseline by 2.7 AP, 4.0 AP, 4.7 AP, and 2.5 AP, respectively. Do the gains come from the additional parameters or FLOPs? We compare our RefineBox with DAB-DETRR50 [25] with 12 decoder layers (DAB-DETR-R50+) in Table 4b. The DAB-DETR-R50+ is designed to have comparable GFLOPs with our refinement network. We train all models for 12 epochs. The model with only six decoder layers gives 35.9 AP while adding another six decoder layers slightly degrades the performance, i.e., AP decreases to 35.1. In contrast, applying our RefineBox to DAB-DETRR50 increases the score from 35.9 AP to 39.2 AP (+3.3 AP). In summary, the key reason for the performance gains is neither the extra parameters nor the additional FLOPs.\nBesides the performance advantage, our RefineBox also shows the efficiency of the parameters. Compared to our method\u2019s 0.5 M, DAB-DETR-R50+ introduces an additional 11 M parameters, 22 times of ours. Please note that all layers in DAB-DETR and RefineBox only update 12 epochs in all experiments, so the comparison is fair. Also, freezing DAB-DETR and adding six decoder layers is an example of our RefineBox framework. Do the gains come from the extra training time? We\npresent the results of object detectors without our RefineBox but trained longer in Table 4c. We observe that: (1) With 108 training epochs, Conditional-DETR-R50 reports 2.0 AP improvements over the baseline (50 epochs). (2) Our RefineBox under 12 epochs training leads to a gain of 2.5 AP, surpassing the performance of the Conditional-DETRR50 with 108 training epochs. Given these observations, we conclude that extra training time is not the primary reason for the performance gain. Would an additional classification branch help? Our RefineBox only refines the predicted boxes. We are curious if an additional classification branch improves performance. The construction of the classification branch is similar to the localization branch. Specifically, we add the Refiner modules after the FPN, in parallel with the localization branch. We don\u2019t share the parameters of the localization branch with the classification branch (see supplementary material for more details).\nTable 4d shows that adding an additional classification branch does not help the final performance. We suspect that: (1) The capacity of the classification branch is much smaller. A prior work aiming to reduce the confidence score of false positives, i.e., DCR [7], improves the Faster RCNN [37] by adopting a ResNet-152 [14] as the refinement network. In contrast, our design is lightweight. A more powerful classifier may further increase the performance. (2) It might be difficult for current DETR-like models to improve the overall performance by increasing the classification ability, as discussed in Section 3. Would training the detector together help? Our design of freezing the object detection models during training is efficient. We also explore whether training the detector concurrently would improve performance. Results in Table 4e suggest that the performance slightly degrades, i.e., from 45.2 AP to 44.9 AP. We suspect the reason is the inconsistency between the well-trained model and the randomly initialized refinement network. For our RefineBox framework,\nthe refinement network only needs to fit the output features of the frozen detector. Therefore, it cannot influence the feature distribution of the detection model. In contrast, training the detector together makes the two affect each other. The detector may shift the distribution of the features under the effect of the refinement network. The refinement network should fit a new distribution once the shift occurs. The training instability may cause the optimization problem. Given the above findings, we suppose that freezing detectors in our RefineBox is effective and efficient.\nEffective Utilization of Multi-scale Features. We observe that our example mainly increases the localization quality of APs and AP75 (see Table 1 and Table 2). We attribute this finding to the efficient utilization of the multi-scale features. The proposed model utilizes larger-size features, which might be difficult for the original detection models partially because the self-attention complexity is quadratic. We hypothesize leveraging multi-scale features is a core factor in the success of the refinement network. It might be one of the reasons why the refinement network in our experiments gives relatively low gains to Deformable DETR [47]: Deformable DETR applies a relatively more complex Deformable Attention mechanism to leverage the feature pyramid. Further improvements can be expected with more sophisticated refinement network designs.\nSummary. From the above discussion, we conclude that: (1) The extra training time, parameters, and FLOPs are not the main reasons for the success of our method. (2) Training together with an additional classification branch or welltrained detectors is inefficient and brings no benefits.\nModel Params GFLOPs DAB-DETR-R50 43.7 M 89.2 + RefineBox 44.2 M 95.7 DAB-DETR-R101 62.6 M 155.6 + RefineBox 63.1 M 162.1 DAB-DETR-SwinT 47.4 M 99.4 + RefineBox 47.8 M 104.9\n(a) Our method is lightweight: RefineBox only introduces 0.5 M parameters and 6.5 GFLOPs for DAB-DETR-R50/101.\nModel Epochs AP AP75 APs Cond-DETR-R50 50 41.0 43.4 20.6 + RefineBox +12 43.5 46.8 24.4 + RefineBox +24 43.9 47.1 24.8 + RefineBox +36 44.0 47.3 25.0 + RefineBox +50 44.1 48.6 26.7\n(b) Additional training epochs give limited gains. We suspect the model capacity is the bottleneck (the refinement network brings only 0.5 M parameters).\nModel K AP AP75 APs FLOPs DAB-DETR - 43.3 45.9 23.4 89.2 + RefineBox 300 45.2 48.6 26.6 99.8 + RefineBox 100 45.2 48.6 26.6 95.7 + RefineBox 30 45.0 48.3 26.3 94.3 + RefineBox 10 44.5 47.6 25.4 93.9\n(c) Model performance increases as K gets larger until K exceeds the maximum number of objects that can appear in an image.\nRange 0 < x \u2264 10 10 < x \u2264 30 30 < x \u2264 100 #Images 3754 1124 74 Percentage 75.08% 22.48% 1.48%\n(d) We present the distribution of the number of objects (x) in an image. Some images have no annotated objects, so the sum of percentages is not 100%.\nBaseline Share Weights? AP AP75 APs Params DAB-DETR-R50 \u2717 45.2 48.5 26.7 44.5 M DAB-DETR-R50 \u2713 45.2 48.6 26.6 44.2 M\n(e) Sharing weights among the Refiner modules introduces extra parameters and does not degrade the performance. For parameter efficiency, we share the parameters by default.\nTable 6. We present ablation studies on the COCO val2017 split. We also show the distribution of the number of objects in an image."
        },
        {
            "heading": "5.4. Ablation Studies",
            "text": "To investigate the behavior of the proposed RefineBox, we conducted several ablation studies on COCO. More results are available in the supplementary material. Training Cost. During training, we freeze the well-trained object detectors and update only the parameters belonging to the refinement network. The frozen detection models require no time and memory to calculate and save the gradients. As shown in Table 5, it only requires 58% of the training time of DAB-DETR-R50 if we generate features and prediction results from the detector online. These properties benefit researchers whose computing resources are limited. Inference Cost. Our refinement network is efficient as it introduces only 0.5 M parameters and 6.5 GFLOPs 1 (Table 6a). The extra parameters and FLOPs account for 1.1% and 7.3% of DAB-DETR, respectively. For ResNet-101 and Swin-Tiny, the numbers will be further decreased: for ResNet-101, the percentage of extra parameters and FLOPs reduced to 0.8% and 4.2%. Training Epochs. We investigate the training epochs of our RefineBox. We choose the commonly used settings: 12, 24, 36, and 50 epochs. Results listed in Table 6b indicate a 12 epochs training schedule is sufficient to produce impressive performance. It is expected as we build our refinement network on top of a well-trained object detector.\nWe also notice that longer training time gives limited gains compared with the 12 epochs training setting. For example, compared with the basic setting (12 training epochs), training for 24 and 50 epochs gives an additional 0.4 AP and 0.6 AP improvements, respectively. We suspect that this phenomenon should be attributed to the low capacity of the refinement network: the total number of parameters of the\n1For Swin-Tiny, the additional parameter and FLOPs are 0.4 M and 5.5 G because the channels of the output feature pyramid differ from the ResNet.\nrefinement network is 0.5 M, which is much less than the well-trained object detector. Refining Top K Boxes. We refine the top 100 predictions based on the classification logits. We also investigate the influence of K and present results in Table 6c. We observe no gains when increasing the K to 300. This phenomenon can be explained by the maximum number of objects that may exist in an image of the COCO dataset is 100.\nWe then try to reduce the value of K from 100 to 30. Interestingly, the loss of AP is small, i.e., from 45.2 AP to 45.0 AP. Refining only the top 30 predictions gives a 1.7 AP improvement on DAB-DETR-R50. This is because only 1.48% of images contain more than 30 objects, as illustrated in Table 6d. The FLOPs are also lowered by 21.5%, resulting in 5.1 extra GFLOPs over DAB-DETR-R50.\nOur RefineBox suffers a 0.7 AP decrease if we set the K as 10 but still outperforms DAB-DETR-R50 by 1.2 AP. We suspect that the top 10 prediction results contain false positives. In addition, the number of objects appearing in an image may exceed 10, though most images have no more than 10 objects, as shown in Table 6d. Although smaller K leads to lower FLOPs, we still set K to the maximum number of objects that may appear in an image, which is 100 for COCO. Sharing Weights. We conduct experiments on sharing weights between the Refiner modules. Results in Table 6e show that sharing parameters does not influence performance. As a result, we choose to share weights for parameter efficiency."
        },
        {
            "heading": "6. Conclusion",
            "text": "We analyze various DETR-like models to seek methods to improve object detection performance by reducing the training errors of positive samples. We find that the localization ability is a bottleneck for current DETR-like detec-\ntors. Based on this observation, we propose a simple, flexible, and general framework called RefineBox to improve the localization capability of DETR-like models. We demonstrate the effectiveness and efficiency of our framework. In the future, we will concentrate on studying more effective methods to guide the refinement network to reduce localization errors. We plan to explore reinforcement learning with rewards as feedback for object detection, especially reinforcement learning from human feedback."
        },
        {
            "heading": "B. RefineBox with Classification Branch",
            "text": "In Section 5.3 of our main paper, we delve deeper into whether an additional classification refinement branch is beneficial. As shown in Figure 7, the classification re-\nfinement branch is structured similarly to the localization refinement branch, with separate parameters and Refiner modules. Additional information about the Refiner module can be found in the main paper\u2019s Figure 4."
        },
        {
            "heading": "C. More Ablation Studies",
            "text": "Number of Refiner Modules. Table 8a showcases the ablation studies on the number of Refiner modules used in the RefineBox. The results indicate that our RefineBox performs well across various Refiner module counts, including 1, 3, and 6. As we observe the performance gains tend to saturate, we choose 3 as the value of the number of Refiner modules. Large Batch Size. Our proposed RefineBox offers the advantage of enabling training with larger batch sizes without the need for memory optimization, which can be especially beneficial for researchers with limited resources. For instance, when employing the detector as an online feature extractor and region proposal network, our RefineBox allows training DAB-DETR-R50 + RefineBox with a total batch size of 160 on 4 GPUs, each equipped with 24GB of memory.\nWe conducted experiments on larger batch sizes with DETR, as presented in Table 8b. The results show that using a batch size of 64 yields slightly better performance than a\nbatch size of 16. For both DETR-R50 and DETR-R101, increasing the batch size from 16 to 64 leads to a 0.3 increase in AP. Bottleneck Channels. As mentioned in the paper, we denote the input channels of the 3\u00d73 conv layer in the Bottleneck Block as the bottleneck channels. We perform ablation studies on the bottleneck channels to strike a balance between performance and inference cost. From the results in Table 8c, we observe that increasing the bottleneck channels results in improved AP performance. However, compared to the default value of 64, increasing the bottleneck channels to 128 only provides a modest gain of 0.2 AP while introducing 5.3 GFLOPs, which does not seem like an optimal trade-off. Therefore, we choose 64 as the default bottleneck channel size. The Number of Bottleneck Blocks. Table 8d presents the ablation study on the number of bottleneck blocks in our RefineBox. The results show that the performance of RefineBox with different numbers of bottleneck blocks is comparable, indicating that adding more blocks may not significantly contribute to the learning of features related to localization. These findings suggest that there is room for further exploration in designing the bounding box refinement module of our RefineBox framework. We leave the investigation of alternative architectures for future work. Model Dimension. We ablate the model dimension in Table 8e. As we have introduced in the paper, the model dimension is the input and output channels of modules in the refinement module, except for FPN. This value is the key to restricting the GFLOPs and the number of parameters of the refinement module. Setting the dimension as 32 leads to the lightest version of RefineBox, but the AP is also the worst. The converse approach is to increase the value of the model\ndimension, which leads to only marginal performance gains but introduces a significant number of FLOPs. Considering the trade-off between performance and computational cost, we chose 64 as the default model dimension."
        },
        {
            "heading": "D. More Discussion on Multi-scale Features",
            "text": "We have hypothesized that the refinement network in our experiments improves the DETR-like models by efficiently and effectively leveraging the multi-scale backbone features, and more sophisticated designs may bring more gains. Here we present more discussion on the utilization of the multi-scale features.\nIt\u2019s interesting to see that the gains from the refinement network are higher on single-scale models compared to those that utilize multi-scale features. For example, as shown in Table 9, the refinement network in our experiments only gives 0.6 AP improvement for Deformable DETR [47]. We notice that each Transformer layer in the Deformable DETR utilizes the multi-scale backbone features with the help of the sophisticated Multi-scale Deformable Attention mechanism. It combines the best of the sparse spatial sampling of deformable convolution and\nthe relation modeling capability of Transformers and enables feature aggregation across the inter-scale and intrascale features. We refer to Deformable DETR [47] for more details of the Deformable Attention mechanism.\nIn contrast, the refinement network in our experiments is quite simple, which only adopts a sequence of residual blocks to refine the boxes. We suspect the model capability of the refinement network may not be much better than the Multi-scale Deformable Attention mechanism. Given the simplicity of our refinement network, it\u2019s not a surprise that our model gives limited improvements on such multi-scale models.\nPlease note that the goal of our RefineBox is to explore the potential of refining the boxes of well-trained DETRlike models instead of exploring a better multi-scale features aggregation mechanism. We hope that our work can inspire future research in this area and lead to more powerful refinement networks and object detection models."
        }
    ],
    "title": "Enhancing Your Trained DETRs with Box Refinement",
    "year": 2023
}