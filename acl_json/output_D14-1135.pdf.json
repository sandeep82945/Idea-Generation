{
    "abstractText": "In this paper, we demonstrate that significant performance gains can be achieved in CCG semantic parsing by introducing a linguistically motivated grammar induction scheme. We present a new morpho-syntactic factored lexicon that models systematic variations in morphology, syntax, and semantics across word classes. The grammar uses domain-independent facts about the English language to restrict the number of incorrect parses that must be considered, thereby enabling effective learning from less data. Experiments in benchmark domains match previous models with one quarter of the data and provide new state-of-the-art results with all available data, including up to 45% relative test-error reduction.",
    "authors": [
        {
            "affiliations": [],
            "name": "Adrienne Wang"
        }
    ],
    "id": "SP:a075ad29a0cf9cf6ef9e1c4c457d42f42e53a284",
    "references": [
        {
            "authors": [
                "J. Andreas",
                "A. Vlachos",
                "S. Clark"
            ],
            "title": "Semantic parsing as machine translation",
            "year": 2013
        },
        {
            "authors": [
                "Y. Artzi",
                "L. Zettlemoyer"
            ],
            "title": "Bootstrapping semantic parsers from conversations",
            "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing.",
            "year": 2011
        },
        {
            "authors": [
                "Y. Artzi",
                "L. Zettlemoyer"
            ],
            "title": "UW SPF: The University of Washington Semantic Parsing Framework",
            "year": 2013
        },
        {
            "authors": [
                "Y. Artzi",
                "L. Zettlemoyer"
            ],
            "title": "Weakly supervised learning of semantic parsers for mapping instructions to actions",
            "venue": "Transactions of the Association for Computational Linguistics, 1(1):49\u201362.",
            "year": 2013
        },
        {
            "authors": [
                "J. Berant",
                "A. Chou",
                "R. Frostig",
                "P. Liang"
            ],
            "title": "Semantic parsing on freebase from question-answer pairs",
            "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing.",
            "year": 2013
        },
        {
            "authors": [
                "J. Bos"
            ],
            "title": "Wide-coverage semantic analysis with boxer",
            "venue": "Proceedings of the Conference on Semantics in Text Processing.",
            "year": 2008
        },
        {
            "authors": [
                "Q. Cai",
                "A. Yates"
            ],
            "title": "Large-scale semantic parsing via schema matching and lexicon extension",
            "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.",
            "year": 2013
        },
        {
            "authors": [
                "Q. Cai",
                "A. Yates"
            ],
            "title": "Semantic parsing freebase: Towards open-domain semantic parsing",
            "venue": "Proceedings of the Joint Conference on Lexical and Computational Semantics.",
            "year": 2013
        },
        {
            "authors": [
                "B. Carpenter"
            ],
            "title": "Type-Logical Semantics",
            "venue": "The MITPress.",
            "year": 1997
        },
        {
            "authors": [
                "D. Chen",
                "R. Mooney"
            ],
            "title": "Learning to interpret natural language navigation instructions from observations",
            "venue": "Proceedings of the National Conference on Artificial Intelligence.",
            "year": 2011
        },
        {
            "authors": [
                "J. Clarke",
                "D. Goldwasser",
                "M. Chang",
                "D. Roth"
            ],
            "title": "Driving semantic parsing from the world\u2019s response",
            "venue": "Proceedings of the Conference on Computational Natural Language Learning.",
            "year": 2010
        },
        {
            "authors": [
                "A. Rudnicky",
                "E. Shriberg"
            ],
            "title": "Expanding the scope of the atis task: The atis",
            "year": 1994
        },
        {
            "authors": [
                "D. Davidson"
            ],
            "title": "The logical form of action sentences",
            "venue": "Essays on actions and events, pages 105\u2013148.",
            "year": 1967
        },
        {
            "authors": [
                "J. de Bruin",
                "R. Scha"
            ],
            "title": "The interpretation of relational nouns",
            "venue": "In Proceedings of the Conference of the Association of Computational Linguistics,",
            "year": 1988
        },
        {
            "authors": [
                "D. Goldwasser",
                "D. Roth"
            ],
            "title": "Learning from natural instructions",
            "venue": "Proceedings of the International Joint Conference on Artificial Intelligence.",
            "year": 2011
        },
        {
            "authors": [
                "L. Heck",
                "D. Hakkani-T\u00fcr",
                "G. Tur"
            ],
            "title": "Leveraging knowledge graphs for web-scale unsupervised semantic parsing",
            "venue": "Proc. of the INTERSPEECH.",
            "year": 2013
        },
        {
            "authors": [
                "J.R. Hobbs",
                "M. Stickel",
                "P. Martin",
                "D. Edwards"
            ],
            "title": "Interpretation as abduction",
            "venue": "Proceedings of the Association for Computational Linguistics.",
            "year": 1988
        },
        {
            "authors": [
                "M. Honnibal",
                "J.K. Kummerfeld",
                "J.R. Curran"
            ],
            "title": "Morphological analysis can improve a ccg parser for english",
            "venue": "Proceedings of the International Conference on Computational Linguistics.",
            "year": 2010
        },
        {
            "authors": [
                "B.K. Jones",
                "M. Johnson",
                "S. Goldwater"
            ],
            "title": "Semantic parsing with bayesian tree transducers",
            "venue": "Proceedings of Association of Computational Linguistics.",
            "year": 2012
        },
        {
            "authors": [
                "R. Kate",
                "R. Mooney"
            ],
            "title": "Using stringkernels for learning semantic parsers",
            "venue": "Proceedings of the Conference of the Association for Computational Linguistics.",
            "year": 2006
        },
        {
            "authors": [
                "J. Krishnamurthy",
                "T. Kollar"
            ],
            "title": "Jointly learning to parse and perceive: Connecting natural language to the physical world",
            "venue": "Transactions of the Association for Computational Linguistics, 1(2).",
            "year": 2013
        },
        {
            "authors": [
                "J. Krishnamurthy",
                "T. Mitchell"
            ],
            "title": "Weakly supervised training of semantic parsers",
            "venue": "Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natu-",
            "year": 2012
        },
        {
            "authors": [
                "L. Zettlemoyer"
            ],
            "title": "Scaling semantic parsers with on-the-fly ontology matching",
            "year": 2013
        },
        {
            "authors": [
                "T. Kwiatkowski",
                "S. Goldwater",
                "L. Zettlemoyer",
                "M. Steedman"
            ],
            "title": "A probabilistic model of syntactic and semantic acquisition from child-directed utterances and their meanings",
            "venue": "Proceedings of the Conference of",
            "year": 2012
        },
        {
            "authors": [
                "T. Kwiatkowski",
                "L. Zettlemoyer",
                "S. Goldwater",
                "M. Steedman"
            ],
            "title": "Inducing probabilistic CCG grammars from logical form with higher-order unification",
            "venue": "Proceedings of the Conference on Empirical",
            "year": 2010
        },
        {
            "authors": [
                "T. Kwiatkowski",
                "L. Zettlemoyer",
                "S. Goldwater",
                "M. Steedman"
            ],
            "title": "Lexical generalization in CCG grammar induction for semantic parsing",
            "venue": "Proceedings of the Conference on Empirical Methods in Natu-",
            "year": 2011
        },
        {
            "authors": [
                "M. Lewis",
                "M. Steedman"
            ],
            "title": "Combined distributional and logical semantics",
            "venue": "Transactions of the Association for Computational Linguistics, 1:179\u2013192.",
            "year": 2013
        },
        {
            "authors": [
                "P. Liang",
                "M. Jordan",
                "D. Klein"
            ],
            "title": "Learning dependency-based compositional semantics",
            "venue": "Proceedings of the Conference of the Association for Computational Linguistics.",
            "year": 2011
        },
        {
            "authors": [
                "C. Matuszek",
                "N. FitzGerald",
                "L. Zettlemoyer",
                "L. Bo",
                "D. Fox"
            ],
            "title": "A joint model of language and perception for grounded attribute learning",
            "venue": "Proceedings of the International Conference on Machine Learning.",
            "year": 2012
        },
        {
            "authors": [
                "S. Miller",
                "D. Stallard",
                "R. Bobrow",
                "R. Schwartz"
            ],
            "title": "A fully statistical approach to natural language interfaces",
            "venue": "Proceedings Association for Computational Linguistics.",
            "year": 1996
        },
        {
            "authors": [
                "S. Muresan"
            ],
            "title": "Learning for deep language understanding",
            "venue": "Proceedings of the International Joint Conference on Artificial Intelligence.",
            "year": 2011
        },
        {
            "authors": [
                "B.H. Partee",
                "V. Borschev"
            ],
            "title": "Integrating lexical and formal sematics: Genitives, relational nouns, and type-shifting",
            "venue": "Proceedings of the Second Tbilisi Symposium on Language, Logic, and Computation.",
            "year": 1998
        },
        {
            "authors": [
                "H. Poon"
            ],
            "title": "Grounded unsupervised semantic parsing",
            "venue": "Association for Computational Linguistics (ACL).",
            "year": 2013
        },
        {
            "authors": [
                "J. Pustejovsky"
            ],
            "title": "The generative lexicon",
            "venue": "volume 17.",
            "year": 1991
        },
        {
            "authors": [
                "M. Steedman"
            ],
            "title": "Surface Structure and Interpretation",
            "venue": "The MIT Press.",
            "year": 1996
        },
        {
            "authors": [
                "M. Steedman"
            ],
            "title": "The Syntactic Process",
            "venue": "The MIT Press.",
            "year": 2000
        },
        {
            "authors": [
                "M. Steedman"
            ],
            "title": "Taking Scope",
            "venue": "The MIT Press.",
            "year": 2011
        },
        {
            "authors": [
                "G. Tur",
                "A. Deoras",
                "D. Hakkani-Tur"
            ],
            "title": "Semantic parsing using word confusion networks with conditional random fields",
            "venue": "Proc. of the INTERSPEECH.",
            "year": 2013
        },
        {
            "authors": [
                "Y. Wong",
                "R. Mooney"
            ],
            "title": "Learning synchronous grammars for semantic parsing with lambda calculus",
            "venue": "Proceedings of the Conference of the Association for Computational Linguistics.",
            "year": 2007
        },
        {
            "authors": [
                "X. Yao",
                "B. Van Durme"
            ],
            "title": "Information extraction over structured data: Question answering with freebase",
            "venue": "Association for Computational Linguistics (ACL).",
            "year": 2014
        },
        {
            "authors": [
                "J. Zelle",
                "R. Mooney"
            ],
            "title": "Learning to parse database queries using inductive logic programming",
            "venue": "Proceedings of the National Conference on Artificial Intelligence.",
            "year": 1996
        },
        {
            "authors": [
                "L. Zettlemoyer",
                "M. Collins"
            ],
            "title": "Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars",
            "venue": "Proceedings of the Conference on Uncertainty in Artificial Intelli-",
            "year": 2005
        },
        {
            "authors": [
                "L. Zettlemoyer",
                "M. Collins"
            ],
            "title": "Online learning of relaxed CCG grammars for parsing to logical form",
            "venue": "Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Com-",
            "year": 2007
        },
        {
            "authors": [
                "L. Zettlemoyer",
                "M. Collins"
            ],
            "title": "Learning context-dependent mappings from sentences to logical form",
            "venue": "Proceedings of the Joint Conference of the Association for Computational Linguistics and Interna-",
            "year": 2009
        }
    ],
    "sections": [
        {
            "text": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1284\u20131295, October 25-29, 2014, Doha, Qatar. c\u00a92014 Association for Computational Linguistics"
        },
        {
            "heading": "1 Introduction",
            "text": "Semantic parsers map sentences to formal representations of their meaning (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011). One common approach is to induce a probabilistic CCG grammar, which defines the meanings of individual words and phrases and how to best combine them to analyze complete sentences. There has been recent work developing learning algorithms for CCG semantic parsers (Kwiatkowski et al., 2010; Artzi and Zettlemoyer, 2011) and using them for applications ranging from question answering (Cai and Yates, 2013b; Kwiatkowski et al., 2013) to robot control (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013).\nOne key learning challenge for this style of learning is to induce the CCG lexicon,\nwhich lists possible meanings for each phrase and defines a set of possible parses for each sentence. Previous approaches have either hand-engineered a small set of lexical templates (Zettlemoyer and Collins, 2005, 2007) or automatically learned such templates (Kwiatkowski et al., 2010, 2011). These methods are designed to learn grammars that overgenerate; they produce spurious parses that can complicate parameter estimation.\nIn this paper, we demonstrate that significant gains can instead be achieved by using a more constrained, linguistically motivated grammar induction scheme. The grammar is restricted by introducing detailed syntactic modeling of a wider range of constructions than considered in previous work, for example introducing explicit treatments of relational nouns, Davidsonian events, and verb tense. We also introduce a new lexical generalization model that abstracts over systematic morphological, syntactic, and semantic alternations within word classes. This includes, for example, the facts that verbs in relative clauses and nominal constructions (e.g., \u201cflights departing NYC\u201d and \u201cdeparting flights\u201d) should be infinitival while verbs in phrases (e.g., \u201cWhat flights depart at noon?\u201d) should have tense. These grammar modeling techniques use universal, domain-independent facts about the English language to restrict word usage to appropriate syntactic contexts, and as such are potentially applicable to any semantic parsing application.\nMore specifically, we introduce a new morpho-syntactic, factored CCG lexicon that imposes our grammar restrictions during learning. Each lexical entry has (1) a word stem, automatically constructed from Wiktionary, with part-of-speech and morphological attributes, (2) a lexeme that is learned\n1284\nand pairs the stem with semantic content that is invariant to syntactic usage, and (3) a lexical template that specifies the remaining syntactic and semantic content. The full set of templates is defined in terms of a small set of base templates and template transformations that model morphological variants such as passivization and nominalization of verbs. This approach allows us to efficiently encode a general grammar for semantic parsing while also eliminating large classes of incorrect analyses considered by previous work.\nWe perform experiments in two benchmark semantic parsing datasets: GeoQuery (Zelle and Mooney, 1996) and ATIS (Dahl et al., 1994). In both cases, our approach achieves state-of-the-art performance, including a nearly 45% relative error reduction on the ATIS test set. We also show that the gains increase with less data, including matching previous model\u2019s performance with less than 25% of the training data. Such gains are particularly practical for semantic parsers; they can greatly reduce the amount of data that is needed for each new application domain."
        },
        {
            "heading": "2 Related Work",
            "text": "Grammar induction methods for CCG semantic parsers have either used hand-engineered lexical templates, e.g. (Zettlemoyer and Collins, 2005, 2007; Artzi and Zettlemoyer, 2011), or algorithms to learn such templates directly from data, e.g. (Kwiatkowski et al., 2010, 2011). Here, we extend the first approach, and show that better lexical generalization provides significant performance gains.\nAlthough CCG is a common choice for semantic parsers, many other formalisms have been studied, including DCS trees (Liang et al., 2011), integer linear programs (Clarke et al., 2010), and synchronous grammars (Wong and Mooney, 2007; Jones et al., 2012; Andreas et al., 2013). All of these approaches build complete meaning representations for individual sentences, but the data we use has also been studied in related work on cross-sentence reasoning (Miller et al., 1996; Zettlemoyer and Collins, 2009) and modeling semantic interpretation as a tagging problem (Tur et al., 2013; Heck et al., 2013). Although we focus on full analysis with CCG,\nthe general idea of using linguistic constraints to improve learning is broadly applicable.\nSemantic parsers are also commonly learned from a variety of different types of supervision, including logical forms (Kate and Mooney, 2006; Wong and Mooney, 2007; Muresan, 2011; Kwiatkowski et al., 2012), questionanswer pairs (Clarke et al., 2010; Liang et al., 2011), conversational logs (Artzi and Zettlemoyer, 2011), distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013b), sentences paired with system behavior (Goldwasser and Roth, 2011; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013b), and even from database constraints with no explicit semantic supervision (Poon, 2013). We learn from logical forms, but CCG learning algorithms have been developed for each case above, making our techniques applicable.\nThere has been significant related work that influenced the design of our morpho-syntactic grammars. This includes linguistics studies of relational nouns (Partee and Borschev, 1998; de Bruin and Scha, 1988), Davidsonian events (Davidson, 1967), parsing as abduction (Hobbs et al., 1988), and other more general theories for lexicons (Pustejovsky, 1991) and CCG (Steedman, 2011). It also includes work on using morphology in CCG syntactic parsing (Honnibal et al., 2010) and more broad-coverage semantics in CCG (Bos, 2008; Lewis and Steedman, 2013). However, our work is unique in studying the use of related ideas for semantic parsing.\nFinally, there has also been recent progress on semantic parsing against large, open domain databases such as Freebase (Cai and Yates, 2013a; Kwiatkowski et al., 2013; Berant et al., 2013). Unfortuantely, existing Freebase datasets are not a good fit to test our approach because the sentences they include have relatively simple structure and can be interepreted accurately using only factoid lookups with no database joins (Yao and Van Durme, 2014). Our work focuses on learning more syntactically rich models that support compositional reasoning."
        },
        {
            "heading": "3 Background",
            "text": "Lambda Calculus We represent the meanings of sentences, words and phrases with\nlambda calculus logical expressions. We use a version of the typed lambda calculus (Carpenter, 1997), in which the basic types include entities, events, truth values and numbers. Function types are assigned to lambda expressions. The expression \u03bbx.flight(x) with type \u3008e, t\u3009 takes an entity and returns a truth value, and represents a set of flights.\nCombinatory Categorial Grammar CCG (Steedman, 1996, 2000) is a formalism that tightly couples syntax and semantics, and can be used to model a wide range of linguistic phenomena. A traditional CCG grammar includes a lexicon \u039b with lexical entries like the following:\nflights ` N :\u03bbx.flight(x) from ` PP/NP :\u03bby.\u03bbx.from(x, y) cities ` N :\u03bbx.city(x)\nwhere a lexical item w `X : h has words w, syntactic category X, and logical expression h.\nCCG uses a small set of combinatory rules to jointly build syntactic parses and semantic representations. Two common combinatory rules are forward (>) and backward (<) application:\nX/Y : f Y : g \u21d2 X : f(g) (>) Y : g X\\Y : f \u21d2 X : f(g) (<)\nCCG also includes combinatory rules of forward (> B) and backward (< B) composition:\nX/Y : f Y/Z : g \u21d2 X/Z : \u03bbx.f(g(x)) (> B) Y \\Z : g X\\Y : f \u21d2 X\\Z : \u03bbx.f(g(x)) (< B)\nThese rules apply to build syntactic and semantic derivations concurrently.\nIn this paper, we also implement type raising rules for compact representation of PP (prepositional phrase) and AP (adverbial phrase).\nPP : g \u21d2 N\\N : \u03bbf\u03bbx.f(x) \u2227 g(x) (T) AP : g \u21d2 S\\S : \u03bbf\u03bbe.f(e) \u2227 g(e) (T) AP : g \u21d2 S/S : \u03bbf\u03bbe.f(e) \u2227 g(e) (T) Figure 1 shows an example CCG parse (Steedman, 1996, 2000) where the lexical entries are listed across the top and the output lambda-calculus meaning representation is at the bottom. This meaning is a function (denoted by \u03bbx...) that defines a set of flights with certain properties and includes a generalized Skolem constant (Steedman, 2011) (Ay...) that performs existential quantification. Following recent work (Artzi and Zettlemoyer, 2013b), we use meaning representations that model a variety of linguistic constructions, for example including Skolem constants for plurals and Davidson quantifiers for events, which we will introduce briefly throughout this paper as they appear.\nWeighted CCGs A weighted CCG grammar is defined as G = (\u039b,\u0398), where \u039b is a CCG lexicon and \u0398 \u2208 Rd is a d-dimensional parameter vector, which will be used to rank the parses allowed under \u039b.\nFor a sentence x, G produces a set of candi-\ndate parse trees Y = Y(x;G). Given a feature vector \u03a6 \u2208 Rd, each parse tree y for sentence x is scored by S(y; \u0398) = \u03b8 \u00b7\u03c6(x, y). The output logical form z\u0302 is then defined to be at the root of the highest-scoring parse y\u0302:\ny\u0302 = arg max y\u2208Y(x;G) S(y; \u0398) (1)\nWe use existing CKY-style parsing algorithms for this computation, implemented with UW SPF (Artzi and Zettlemoyer, 2013a). Section 7 describes the set of features we use in the learned models.\nLearning with GENLEX We will also make use of an existing learning algorithm (Zettlemoyer and Collins, 2007) (ZC07). We first briefly review the ZC07 algorithm, and describe our modifications in Section 7.\nGiven a set of training examples D = {(xi, zi) : i = 1...n}, xi being the ith sentence and zi being its annotated logical form, the algorithm learns a set of parameters \u0398 for the grammar, while also inducing the lexicon \u039b.\nThe ZC07 learning algorithm uses a function GENLEX(x, z) to define a set of lexical entries that could be used to parse the sentence x to construct the logical form z. For each training example (x, z), GENLEX(x, z) maps all substrings x to a set of potential lexical entries, generated by exhaustively pairing the logical constants in z using a set of hand-engineered templates. The example is then parsed with this much bigger lexicon and lexical entries from the highest scoring parses are added to \u039b. The parameters \u0398 used to score parses are updated using a perceptron learning algorithm."
        },
        {
            "heading": "4 Morpho-Syntactic Lexicon",
            "text": "This section defines our morpho-syntactic lexical formalism. Table 1 shows examples of how lexemes, templates, and morphological transformations are used to build lexical entries for example verbs. In this section, we formally define each of these components and show how they are used to specify the space of possible lexical entries that can be built for each input word. In the following two sections, we will provide more discussion of the complete sets of templates (Section 5) and transformations (Section 6).\nWe build on the factored CCG lexicon introduced by Kwiatkowski et al. (2011) but (a) further generalize lexemes to represent word stems, (b) constrain the use of templates with widely available syntactic information, and (c) efficiently model common morphological variations between related words.\nThe first step, given an input word w, is to do morphological and part-of-speech analysis with the morpho-syntactic function F . F maps a word to a set of possible morphosyntactic representations, each containing a triple (s, p,m) of word stem s, part-of-speech p and morphological category m. For example, F maps the word flies to two possible representations:\nF (flies) = {(fly,Noun, (plural)), (fly,Verb, (third, singular, simple, present))}\nfor the plural noun and present-tense verb senses of the word. F is defined based on the stems, part-of-speech types, and morphological attributes marked for each definition in Wiktionary. 1 The full sets of possible part-ofspeech and morphological types required for our domains are shown in Table 2 and Table 3.\nEach morpho-syntactic analysis a \u2208 F (w) is then paired with lexemes based on stem match. A lexeme (s,~c) pairs a word stem s with a list of logical constants ~c = [c1 . . . ck]. Table 1 shows the words \u2018depart\u2019, \u2018departing\u2019, \u2018departure\u2019, which are all assigned the lexeme (depart, [depart]). In general, there can\n1www.wiktionary.com\nbe many different lexemes for each stem, that vary in the selection of which logical constants are included.\nGiven analysis (s, p,m) and lexeme (s,~c), we can use a lexical template to construct a lexical entry. Each template has the form:\n\u03bb(\u03be,~v).[\u03be `X : h~v]\nwhere \u03be and ~v are variables that abstract over the words and logical constants that will be used to define a lexical entry with syntax X and templated logical form h~v.\nTo instantiate a template, \u03be is filled with the original word w and the constants in ~c replace the variables ~v. For example, the template \u03bb(\u03be,~v).[\u03be ` S\\NP : \u03bbx\u03bbe.v1(e, x)] could be used with the word \u2018departing\u2019 and the lexeme (depart, [depart]) to produce the lexical entry departing ` S\\NP : \u03bbx\u03bbe.depart(e, x). When clear from context, we will omit the function signature \u03bbp(\u03be,~v). for all templates, as seen in Table 1.\nIn general, there can be many applicable templates, which we organize as follows. Each final template is defined by applying a morphological transformation to one of a small set of possible base templates. The pairing is found based on the morphological analysis (s, p,m), where each base template is associated with part-of-speech p and each transformation is indexed by the morphology m. A transformation fm is a function:\nfm(\u03bbp(\u03be,~v).[\u03be `X : h~v]) = \u03bbp(\u03be,~v).[\u03be `X \u2032 : h\u2032~v]\nthat takes the base template as input and produces a new template to model the inflected form specified by m.\nFor example, both base templates in Table 1 are for verbs. The template \u03be ` S\\NP : \u03bbx\u03bbe.v1(e, x) can be translated into three other templates based on the transformations I, fpres, and fnom, depending on the analysis of the original words. These transformations generalize across word type; they can be used for the transitive verb \u2018use\u2019 as well as the intransitive \u2018depart.\u2019 Each resulting template, potentially including the original input if the identity transformation I is available, can then be used to make an output lexical entry, as we described above."
        },
        {
            "heading": "5 Lexical Templates",
            "text": "The templates in our lexicon, as introduced in Section 4, model the syntactic and semantic aspects of lexical entries that are shared within each word class. Previous approaches have also used hand-engineered lexical templates, as described in Section 2, but we differ by (1) using more templates allowing for more fine grained analysis and (2) using word class information to restrict template use, for example ensuring that words which cannot be verbs are never paired with templates designed for verbs. This section describes the templates used during learning, first presenting those designed to model grammatical sentences and then a small second set designed for more elliptical spoken utterances.\nBase Forms Table 4 lists the primary template set, where each row shows an example with a sentence illustrating its use. Templates are also grouped by the word classes, including adjectives, adverbs, prepositions, and several types of nouns and verbs. While there is not enough space to discuss each row, it is worth\nconsidering nouns as an illustrative example. We model nouns as denoting a set of entities that satisfy a given property. Regular nouns are represented using unary predicates. Relational nouns syntactically function as regular nouns but semantically describe sets of entities that have some relationship with a complement (Partee and Borschev, 1998). For example, the relational noun fare describes a binary relationship between flights and their price information, as we see in this parse:\nfares of flights\nN/PP PP/NP N \u03bbx\u03bby.fare(x, y) \u03bbx.x \u03bbx.flight(x)\n>T NP Ax.flight(x) >\nPP Ax.flight(x)\n> N\n\u03bbx.fare(Ay.flight(y), x) This analysis differs from previous ap-\nproaches (Zettlemoyer and Collins, 2007), where relational nouns were treated as regular nouns and prepositions introduced the binary relationship. The relational noun model reduces lexical ambiguity for the prepositions, which are otherwise highly polysemous.\nAdjectives are nominal modifiers that take a noun or a noun phrase as an argument and add properties through conjunction. Prepositions take nominal objects and function as adjectival modifiers for nouns or adverbial modifiers for verbs. Verbs can be subcategorized by their grammatical structures into transitive (Vtrans), intransitive (Vintrans), impersonal (Vimperson), auxiliary (Vaux) and copula (Vcopula). Adverbs are verb modifiers defining aspects like time, rate and duration. The adoption of event semantics allows adverbial modifiers to be represented by predicates and\nlinked by the shared events. Determiners precede nouns or noun phrases and distinguish a reference of the noun. Following the generalized Skolem terms, we model determiners, including indefinite and definite articles, as a \u3008\u3008e, t\u3009, e\u3009 function that selects a unique individual from a \u3008e, t\u3009-typed function defining a singleton set.\nMissing Words The templates presented so far model grammatically correct input. However, in dialogue domains such as ATIS, speakers often omit words. For example, speakers can drop the preposition \u201cfrom\u201d in \u201cflights from Newark to Cleveland\u201d to create the elliptical utterance \u201cflights Newark to Cleveland\u201d. We address this issue with the templates telliptical illustrated in Table 5. Each of these adds a binary relation P to a lexeme with a single entity typed constant. For our example, the word \u201cNewark\u201d could be assigned the lexical item Newark `PP : \u03bbx.from(x, newark) by selecting the first template and P = from.\nAnother common problem is the use of metonymy. In the utterance \u201cWhat airlines depart from New York?\u201d, the word \u201cairlines\u201d is used to reference flight services operated by a specific airline. This is problematic because the word \u201cdepart\u201d needs to modify an event of type flight. We solve this with the tmetonymy templates in Table 5. These introduce a binary predicate P that would, in the case of our example, map airlines on to the flights that they operate.\nThe templates in Table 5 handle the major cases of missing words seen in our data and are more efficient than the approach taken by (Zettlemoyer and Collins, 2007) who introduced complex type shifting rules and relaxed the grammar to allow every word order."
        },
        {
            "heading": "6 Morphological Transformations",
            "text": "Finally, the morpho-syntactic lexicon introduces morphological transformations, which are functions from base lexical templates to lexical templates that model the syntactic and semantic variation as the word is inflected. These transformations allow us to compactly model, for example, the facts that argument order is reversed when moving from active to passive forms of the same verb, and that the subject can be omitted. To the best of our\nknowledge, we are the first to study such transformations for semantic parsing.\nTable 6 shows the transformations. Each row groups a set of transformations by linguistic category, including singular vs. plural number, active vs. passive voice, and so on, and also includes example sentences where the output templates could be used. Again, for space, we do not detail the motivation for every class, but it is worth looking at some of the alternations for verbs and nouns as our prototypical example.\nSome verbs can act as noun modifiers. For example, the present participle \u201cusing\u201d modifies \u201cflights\u201d in \u201cflights using twa\u201d. To model this variation, we use the transformation fpresent part, a mapping that changes the root of the verb signature S\\NP to PP :\nfpresent part : \u03be `S\\NP/T :\u03bbx1..xn\u03bbe.v(e, xn..x1) \u2192 \u03be `PP/T : \u03bbx1..xn\u03bbe.v(e, xn..x1)\nwhere T = [ ,NP,NP/NP ] instantiates this rule for verbs that take different sets of arguments, effectively allowing any verb that is in its finite or -ing form to behave syntactically like a prepositional phrase.\nIntransitive present participles can also act as prenominal adjectival modifiers as in \u201cthe departing flight\u201d. We add a second mapping that maps the intransitive category S\\NP to the noun modifier N/N .\nfpresent part : \u03be `S\\NP :\u03bbx\u03bbe.v(e, x) \u2192 \u03be `N/N : \u03bbf\u03bbx\u03bbe.f(x) \u2227 v(e, x)\nFinally, verbal nouns have meanings derived from actions typically described by verbs but syntactically function as nouns. For example, landing in the phrase \u201clanding from jfk\u201d is the gerundive use of the verb land. We add the following mapping to fpresent part and fnominal:\n\u03be `S\\NP/T :\u03bbx1..xn\u03bbe.v(e, xn..x1) \u2192 \u03be `N/T : \u03bbx1..xn\u03bbe.v(e, xn..x1)\nwith T from above. This allows for reuse of the same meaning across quite different syntactic constructs, including for example \u201cflights that depart from Boston\u201d and \u201cdeparture from Boston.\u201d\nNouns can be inflected by number to denote singular and plural forms or by adding an apostrophe to mark a possessive case. The transformation function fsingular is an identity transformation. Plurals may have different interpretations: one is the generic \u3008e, t\u3009 set representation, which requires no transformation on the base, or plurals can occur without overt determiners (bare plurals), but semantically imply quantification. We create a plural to singular type shifting rule which implements the \u3008\u3008e, t\u3009, e\u3009 skolem function to select a unique individual from the set. The possessive transformation fpossess transfers the base template to a noun modifier, and adds a binary predicate P that encodes the relation.\nThere are also a number of instances of the identity transformation function I, which does not change the base template. Because the semantics we are constructing was designed to answer questions against a static database, it does not need to represent certain phenomena to return the correct answer. This includes more advanced variants of person, tense, aspect, and potentially many others. Ideally, these morphological attributes should add semantic modifiers to the base meaning, for example, tense can constrain the time at which\nan event occurs. However, none of our domains support such reasoning, so we assign the identity transformation, and leave the exploration of these issues to future work."
        },
        {
            "heading": "7 Learning",
            "text": "One advantage of our morpho-syntactic, factored lexicon is that it can be easily learned with small modifications to existing algorithms (Zettlemoyer and Collins, 2007). We only need to modify the GENLEX procedure that defines the space of possible lexical entries. For each training example (x, z), GENLEX(x, z, F ) first maps each substring in the sentence x into the morphological representation (s, p, c) using F introduced in Section 4. A candidate lexeme set L\u2032 is then generated by exhaustively pairing the word stems with all subsets of the logical constants from z. Lexical templates are applied to the lexemes in L\u2032 to generate candidate lexical entries for x. Finally, the lexemes that participate in the top scoring correct parse of x are added to the permanent lexicon.\nInitialization Following standard practice, we compile an initial lexicon \u039b0, which consists of a list of domain independent lexical\nitems for function words, such as interrogative words and conjunctions. These lexical items are mostly semantically vacuous and serve particular syntactic functions that are not generalizable to other word classes. We also initialize the lexemes with a list of NP entities complied from the database, e.g., (Boston, [bos]).\nFeatures We use two types of features in the model for discriminating parses. Four lexical features are fired on each lexical item: \u03c6(s,~c) for the lexeme, \u03c6tp for the base template, \u03c6tm for the morphologically modified template, and \u03c6l for the complete lexical item. We also compute the standard logical expression features (Zettlemoyer and Collins, 2007) on the root semantics to track the pairwise predicate-argument relations and the cooccuring predicate-predicate relations in conjunctions and disjunctions."
        },
        {
            "heading": "8 Experimental Setup",
            "text": "Data and Metrics We evaluate performance on two benchmark semantic parsing datasets, Geo880 and ATIS. We use the standard data splits, including 600/280 train/test for Geo880 and 4460/480/450 train/develop/test for ATIS. To support the new representations in Section 5, we systematically convert annotations with existential quantifiers, temporal events and relational nouns to new logical forms with equivalent meanings. All systems are evaluated with exact match accuracy, the percentage of fully correct logical forms.\nInitialization We assign positive initial weights to the indicator features for entries in the initial lexicon, as defined in Section 7, to encourage their use. The elliptical template and metonymy template features are initialized with negative weights to initially discourage word skipping.\nComparison Systems We compare performance with all recent CCG grammar induction algorithms that work with our datasets. This includes methods that used a limited set of hand-engineered templates for inducing the lexicon, ZC05 (Zettlemoyer and Collins, 2005) and ZC07 (Zettlemoyer and Collins, 2007), and those that learned grammar structure by automatically splitting the labeled log-\nical forms, UBL (Kwiatkowski et al., 2010) and FUBL (Kwiatkowski et al., 2011). We also compare the state-of-the-art for Geo880 (DCS (Liang et al., 2011) and DCS+ which includes an engineered seed lexicon) and ATIS (which is ZC07). Finally, we include results for GUSP (Poon, 2013), a recent unsupervised approach for ATIS.\nSystem Variants We report results for a complete approach (Full), and variants which use different aspects of the morpho-syntactic lexicon. The TEMP-ONLY variant learned with the templates from Section 5 but, like ZC07, does not use any word class information to restrict their use. The TEMP-POS removes morphology from the lexemes, but includes the word class information from Wiktionary. Finally, we also include DCS+, which initialize a set of words with POS tag JJ, NN, and NNS."
        },
        {
            "heading": "9 Results",
            "text": "Full Models Tables 7 and 8 report the main learning results. Our approach achieves state-of-the-art accuracies on both datasets, demonstrating that our new grammar induction scheme provides a type of linguistically motivated regularization; restricting the algorithm to consider a much smaller hypothesis space allows to learn better models.\nOn Geo880 the full method edges out the best systems by 2% absolute on the test set, as compared to other systems with no domainspecific lexical initialization. Although DCS requires less supervision, it also uses external signals including a POS tagger.\nWe see similarly strong results for ATIS, outperforming FUBL on the ATIS development set by 6.8%, and improving the accuracy on the test set by 7.9% over the previous best system ZC07. Unlike FUBL, which excels at the development set but trails ZC07\u2019s templated grammar by almost 2 points on the test set, our approach demonstrates consistent improvements on both. Additionally, although the unsupervised model (GUSP) rivals previous approaches, we are able to show that more careful use of supervision open a much wider performance gap.\nLearning Curve with Ablations Figure 2 presents a learning curve for the ATIS domain, demonstrating that the learning improvements become even more dramatic for smaller training set sizes. Our model outperforms FUBL by wide margins, matching its final accuracy with only 22% of the total training examples. Our full model also consistently beats the variants with fewer word class restrictions, although by smaller margins. Again, these results further highlight the benefit of importing external syntactic resources and enforcing linguistically motivated constraints during learning.\nLearned Lexicon The learned lexicon is also more compact. Table 9 summarizes statistics on unique lexical entries required to parse the ATIS development set. The\nmorpho-syntactic model uses 80.3% of the lexical entries and 63.7% of the lexemes that FUBL needs, while increase performance by nearly 7 points. Upon inspection, our model achieves better lexical decomposition by learning shorter lexical units, for example, the adoption of Davidsonian events allows us to learn unambiguous adverbial modifiers, and the formal modeling of nominalized nouns and relational nouns treats prepositions as syntactic modifiers, instead of being encoded in the semantics. Such restrictions generalize to a much wider variety of syntactic contexts."
        },
        {
            "heading": "10 Summary and Future Work",
            "text": "We demonstrated that significant performance gains can be achieved in CCG semantic parsing by introducing a more constrained, linguistically motivated grammar induction scheme. We introduced a morpho-syntactic factored lexicon that uses domain-independent facts about the English language to restrict the number of incorrect parses that must be considered and demonstrated empirically that it enables effective learning of complete parsers, achieving state-of-the-art performance.\nBecause our methods are domain independent they should also benefit other semantic parsing applications and other learning algorithms that use different types of supervision, as we hope to verify in future work. We would also like to study how to generalize these gains to languages other than English, by inducing more of the syntactic structure."
        },
        {
            "heading": "Acknowledgements",
            "text": "The research was supported by the NSF (IIS1115966, IIS-1252835) and the Intel Center for Pervasive Computing at the Univeristy of Washington. The authors thank Robert Gens, Xiao Ling, Xu Miao, Mark Yatskar and the UW NLP group for helpful discussions, and the anonymous reviewers for helpful comments."
        }
    ],
    "title": "Morpho-syntactic Lexical Generalization for CCG Semantic Parsing",
    "year": 2014
}