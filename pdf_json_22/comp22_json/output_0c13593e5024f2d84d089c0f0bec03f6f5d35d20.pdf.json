{
    "abstractText": "State-of-the-art maximum entropy models for texture synthesis are built from statistics relying on image representations defined by convolutional neural networks (CNN). Such representations capture rich structures in texture images, outperforming wavelet-based representations in this regard. However, conversely to neural networks, wavelets offer meaningful representations, as they are known to detect structures at multiple scales (e.g. edges) in images. In this work, we propose a family of statistics built upon non-linear wavelet based representations, that can be viewed as a particular instance of a one-layer CNN, using a generalized rectifier non-linearity. These statistics significantly improve the visual quality of previous classical wavelet-based models, and allow one to produce syntheses of similar quality to state-of-the-art models, on both gray-scale and color textures. We further provide insights on memorization effects in these models.",
    "authors": [
        {
            "affiliations": [],
            "name": "Antoine Brochard"
        },
        {
            "affiliations": [],
            "name": "Sixin Zhang"
        }
    ],
    "id": "SP:f93fb973182b95a72cbfece784ea9304f9bc090a",
    "references": [
        {
            "authors": [
                "E Allys",
                "T Marchand",
                "J-F Cardoso",
                "F Villaescusa-Navarro",
                "S Ho",
                "S Mallat"
            ],
            "title": "New interpretable statistics for large-scale structure analysis and generation",
            "venue": "Physical Review D,",
            "year": 2020
        },
        {
            "authors": [
                "Guillaume Berger",
                "Roland Memisevic"
            ],
            "title": "Incorporating long-range consistency in cnn-based texture generation",
            "venue": "In International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Joan Bruna",
                "Stephane Mallat"
            ],
            "title": "Multiscale Sparse Microcanonical Models",
            "venue": "Mathematical Statistics and Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "IEEE conference on computer vision and pattern recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Arturo Deza",
                "Aditya Jonnalagadda",
                "Miguel P. Eckstein"
            ],
            "title": "Towards metamerism via foveated style transfer",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Jeremy Freeman",
                "Eero P Simoncelli"
            ],
            "title": "Metamers of the ventral stream",
            "venue": "Nature neuroscience,",
            "year": 2011
        },
        {
            "authors": [
                "Leon Gatys",
                "Alexander S Ecker",
                "Matthias Bethge"
            ],
            "title": "Texture synthesis using convolutional neural networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2015
        },
        {
            "authors": [
                "Leon A Gatys",
                "Alexander S Ecker",
                "Matthias Bethge"
            ],
            "title": "Image style transfer using convolutional neural networks",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "P. Goupillaud",
                "A. Grossmann",
                "J. Morlet"
            ],
            "title": "Cycle-octave and related transforms in seismic signal analysis",
            "venue": "Geoexploration, 23(1):85\u2013102,",
            "year": 1984
        },
        {
            "authors": [
                "Jieqian He",
                "Matthew Hirn"
            ],
            "title": "Texture synthesis via projection onto multiscale, multilayer statistics",
            "venue": "arXiv preprint arXiv:2105.10825,",
            "year": 2021
        },
        {
            "authors": [
                "David J Heeger",
                "James R Bergen"
            ],
            "title": "Pyramid-based texture analysis/synthesis",
            "venue": "In Proceedings of the 22nd annual conference on Computer graphics and interactive techniques,",
            "year": 1995
        },
        {
            "authors": [
                "Edwin T Jaynes"
            ],
            "title": "Information theory and statistical mechanics",
            "venue": "Physical review,",
            "year": 1957
        },
        {
            "authors": [
                "Bela Julesz"
            ],
            "title": "Visual pattern discrimination",
            "venue": "IRE transactions on Information Theory,",
            "year": 1962
        },
        {
            "authors": [
                "P Laube",
                "M Grunwald",
                "MO Franz",
                "G Umlauf"
            ],
            "title": "Image inpainting for high-resolution textures using cnn texture synthesis",
            "venue": "In Proceedings of the Conference on Computer Graphics & Visual Computing,",
            "year": 2018
        },
        {
            "authors": [
                "Roberto Leonarduzzi",
                "Gaspar Rochette",
                "Jean-Phillipe Bouchaud",
                "St\u00e9phane Mallat"
            ],
            "title": "Maximumentropy scattering models for financial time series",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2019
        },
        {
            "authors": [
                "Chuan Li",
                "Michael Wand"
            ],
            "title": "Combining markov random fields and convolutional neural networks for image synthesis",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Gang Liu",
                "Yann Gousseau",
                "Gui-Song Xia"
            ],
            "title": "Texture synthesis through convolutional neural networks and spectrum constraints",
            "venue": "In 2016 23rd International Conference on Pattern Recognition (ICPR),",
            "year": 2016
        },
        {
            "authors": [
                "St\u00e9phane Mallat"
            ],
            "title": "A Wavelet Tour of Signal Processing: The Sparse Way, 3rd Edition",
            "year": 2001
        },
        {
            "authors": [
                "St\u00e9phane Mallat",
                "Sixin Zhang",
                "Gaspar Rochette"
            ],
            "title": "Phase harmonic correlations and convolutional neural networks",
            "venue": "Information and Inference: A Journal of the IMA,",
            "year": 2020
        },
        {
            "authors": [
                "Jorge Nocedal"
            ],
            "title": "Updating quasi-newton matrices with limited storage",
            "venue": "Mathematics of computation,",
            "year": 1980
        },
        {
            "authors": [
                "Javier Portilla",
                "Eero P Simoncelli"
            ],
            "title": "A parametric texture model based on joint statistics of complex wavelet coefficients",
            "venue": "International journal of computer vision,",
            "year": 2000
        },
        {
            "authors": [
                "Lara Raad",
                "Axel Davy",
                "Agn\u00e8s Desolneux",
                "Jean-Michel Morel"
            ],
            "title": "A survey of exemplar-based texture synthesis",
            "venue": "Annals of Mathematical Sciences and Applications,",
            "year": 2018
        },
        {
            "authors": [
                "Kai Schneider",
                "J\u00f6rg Ziuber",
                "Marie Farge",
                "Alexandre Azzalini"
            ],
            "title": "Coherent vortex extraction and simulation of 2D isotropic turbulence",
            "venue": "Journal of Turbulence,",
            "year": 2006
        },
        {
            "authors": [
                "Omry Sendik",
                "Daniel Cohen-Or"
            ],
            "title": "Deep correlations for texture synthesis",
            "venue": "ACM Transactions on Graphics (ToG),",
            "year": 2017
        },
        {
            "authors": [
                "E.P. Simoncelli",
                "W.T. Freeman"
            ],
            "title": "The steerable pyramid: a flexible architecture for multi-scale derivative computation",
            "venue": "In Proceedings., International Conference on Image Processing,",
            "year": 1995
        },
        {
            "authors": [
                "Karen Simonyan",
                "Andrew Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "arXiv preprint arXiv:1409.1556,",
            "year": 2014
        },
        {
            "authors": [
                "M. Unser",
                "N. Chenouard"
            ],
            "title": "A unifying parametric framework for 2d steerable wavelet transforms",
            "venue": "SIAM Journal on Imaging Sciences,",
            "year": 2013
        },
        {
            "authors": [
                "Ivan Ustyuzhaninov",
                "Wieland Brendel",
                "Leon A. Gatys",
                "Matthias Bethge"
            ],
            "title": "What does it take to generate natural textures",
            "venue": "In International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Jonathan Vacher",
                "Thibaud Briand"
            ],
            "title": "The portilla-simoncelli texture model: towards understanding the early visual cortex",
            "venue": "Image Processing On Line,",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan Vacher",
                "Aida Davila",
                "Adam Kohn",
                "Ruben Coen-Cagli"
            ],
            "title": "Texture interpolation for probing visual perception",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Thomas SA Wallis",
                "Christina M Funke",
                "Alexander S Ecker",
                "Leon A Gatys",
                "Felix A Wichmann",
                "Matthias Bethge"
            ],
            "title": "Image content is more important than bouma\u2019s law for scene metamers",
            "venue": "ELife,",
            "year": 2019
        },
        {
            "authors": [
                "Jianwen Xie",
                "Yang Lu",
                "Song-Chun Zhu",
                "Yingnian Wu"
            ],
            "title": "A theory of generative convnet",
            "venue": "In International Conference on Machine Learning,",
            "year": 2016
        },
        {
            "authors": [
                "Jianwen Xie",
                "Yang Lu",
                "Ruiqi Gao",
                "Song-Chun Zhu",
                "Ying Nian Wu"
            ],
            "title": "Cooperative training of descriptor and generator networks",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "2022 Ning Yu",
                "Connelly Barnes",
                "Eli Shechtman",
                "Sohrab Amirghodsi",
                "Michal Lukac"
            ],
            "title": "Texture mixer: A network for controllable synthesis and interpolation of texture",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Yang Zhou",
                "Zhen Zhu",
                "Xiang Bai",
                "Dani Lischinski",
                "Daniel Cohen-Or",
                "Hui Huang"
            ],
            "title": "Nonstationary texture synthesis by adversarial expansion",
            "venue": "Applied and Computational Harmonic Analysis,",
            "year": 2018
        },
        {
            "authors": [
                "In Ustyuzhaninov"
            ],
            "title": "2017), the authors proposed to use the synthesis loss of the VGG model to evaluate the quality of syntheses from any model. The goal is to define a quantitative, and more objective evaluation method than mere visual inspection. Since the VGG model produces syntheses almost indistinguishable from real textures, it is natural to consider its loss to asses the quality of a synthesis. We computed this loss for the first two examples of Figure 3 (radishes and cherries)",
            "year": 2017
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Textures ares spatially homogeneous images, consisting of similar patterns forming a coherent ensemble. In texture modeling, one of the standard approaches to synthesize textures relies on defining a maximum entropy model (Jaynes, 1957) using a single observed image (Raad et al., 2018). It consists of computing a set of prescribed statistics from the observed texture image, and then generating synthetic textures producing the same statistics as the observation. If the statistics correctly describe the structures present in the observation, then any new image with the same statistics should appear similar to the observation. A major challenge of such methods resides in finding a suitable set of statistics, that can generate both high-quality and diverse synthetic samples. This problem is fundamental as it is at the heart of many texture related problems. For example, in patch re-arrangement methods for texture modeling, these statistics are used to compute high-level similarities of image patches (Li & Wand, 2016; Raad et al., 2018). Such models are also used for visual perception (Freeman & Simoncelli, 2011; Wallis et al., 2019; Vacher et al., 2020), style transfer (Gatys et al., 2016; Deza et al., 2019) and image inpainting (Laube et al., 2018).\nA key question along this line of research is to find what it takes to generate natural textures. This problem was originally posed in Julesz (1962), in which the author looks for a statistical characterization of textures. In the classical work of Portilla & Simoncelli (2000) (noted PS in this work), the authors presented a model whose statistics are built on the wavelet transform of an input texture image. These statistics were carefully chosen, by showing that each of them captured a specific aspect of the structure of the image. This model produces satisfying results for a wide range of textures, but fails to reproduce complex geometric structures present in some natural texture images. Figure 1 presents a typical example composed of radishes, and synthetic images from three state-of-the-art models developed over the last few decades. To address this problem, the work of Gatys et al. (2015) proposes to use statistics built on the correlations between the feature maps of a deep CNN, pre-trained on the ImageNet classification problem (Deng et al., 2009; Simonyan & Zisserman, 2014). While this model produces visually appealing images, these statistics are hard to\nar X\niv :2\n20 3.\n07 90\n2v 1\n[ cs\n.C V\n] 1\n4 M\nar 2\ninterpret. The work of Ustyuzhaninov et al. (2017) made a significant simplification of such statistics, by using the feature maps of a one-layer rectifier CNN with random filters (without learning). A crucial aspect of this simplification relies on using multi-scale filters, which are naturally connected to the wavelet transform. In this paper, we propose a wavelet-based model, more interpretable than CNN-based models (with learned or random filters), to synthesize textures with complex geometric structures. It allows to bridge the gap between the classical work of Portilla & Simoncelli (2000), and state-of-the-art models.\nThis model is built on the recent development of the phase harmonics for image representations and non-Gaussian stationary process modeling (Mallat et al., 2020; Zhang & Mallat, 2021). The phase harmonics are non-linear transformations that adjust the phase of a complex number. In Portilla & Simoncelli (2000); Zhang & Mallat (2021), the authors illustrate that the phase dependencies between wavelet coefficients across scales contain important information about the geometric structures in textures and turbulent flows, and that they can be captured by applying the phase harmonics to complex wavelet coefficients. Remarkably, Mallat et al. (2020) show that the phase harmonics admit a dual representation, closely related to the rectifier non-linearity in CNNs. Our main contributions are:\n\u2022 We develop a family of texture models based on the wavelet transform and a generalized rectifier non-linearity, that significantly improves the visual quality of the classical waveletbased model of Portilla & Simoncelli (2000) on a wide range of textures. It relies on introducing spatial shift statistics across scales to capture geometric structures in textures.\n\u2022 By changing the number of statistics in our models, we show explicitly the trade-off on the quality and diversity of the synthesis. When there are too many statistics, our model tends to memorize image patches. We further investigate such memorization effects on non-stationary images and find that it sometimes relies on what statistics are chosen, rather than on how many.\n\u2022 Through the modeling of geometric structures in gray-scale textures, our model indicates the possibility of reducing significantly the number of statistics in the works of Gatys et al. (2015) and Ustyuzhaninov et al. (2017), to achieve a similar visual quality.\nThe rest of the paper is organized as follows: Section 2 reviews the framework of microcanonical maximum-entropy models, build upon a general family of covariance statistics. We then present our model for both gray-scale and color textures in Section 3. Section 4 shows synthesis results of our model, compared with state-of-the-art models. Finally, in Section 5, we discuss possible improvements of our model2.\nNotations Throughout the paper,N denotes a positive integer. A gray-scale image x is an element of RN\u00d7N , i.e. x = x(u), u \u2208 \u2126N , with \u2126N := {0, \u00b7 \u00b7 \u00b7 , N\u22121}2. A color image x = {xc}c=1,2,3 is an element of R3\u00d7N\u00d7N , or equivalently, each xc \u2208 RN\u00d7N . We shall denote x\u0304 the observed texture (observation), which is assumed to be a realisation a random vector X . For any complex number z \u2208 C, z\u2217 is the complex conjugate of z, Real(z) its real part, |z| its modulus, and \u03d5(z) its phase.\n1As the model from Ustyuzhaninov et al. (2017) uses on random filters, we shall use the abbreviation RF. 2All calculations can be reproduced by a Python software available at https://github.com/\nabrochar/wavelet-texture-synthesis."
        },
        {
            "heading": "2 MICROCANONICAL COVARIANCE MODELS",
            "text": "We briefly review the standard framework of micro-canonical maximum-entropy models for textures. To reliably estimate the statistics in these models, we assume that a texture is a realization of a stationary and ergodic process X (restricted to \u2126N ). We then review a special family of statistics that are used in the state-of-the-art texture models (mentioned in Figure 1), based on covariance statistics of an image representation."
        },
        {
            "heading": "2.1 FRAMEWORK",
            "text": "Given a observation texture x\u0304, we aim at generating new texture images, similar but different from x\u0304. To that end, a classical method is to define a set of statistics Cx\u0304, computed on the observation, and try to sample from the microcanonical set\n{x : \u2016Cx\u2212 Cx\u0304\u2016 \u2264 },\nwhere \u2016 \u00b7 \u2016 denotes the L2 norm. Under the stationary and ergodic assumption of X , one can construct Cx as a statistical estimator of E(CX), from a complex-valued representation Rx.3 The set of covariance statistics Cx of a model can then be constructed by computing an averaging over the spatial variable u, i.e.\nCx(\u03b3, \u03b3\u2032, \u03c4) := 1 |\u2126N | \u2211 u\u2208\u2126N Rx(\u03b3, u)Rx(\u03b3\u2032, u\u2212 \u03c4)\u2217, (1)\nfor (\u03b3, \u03b3\u2032, \u03c4) \u2208 \u03a5 \u2286 \u0393 \u00d7 \u0393 \u00d7 \u2126N . The statistics Cx(\u03b3, \u03b3\u2032, \u03c4) can be interpreted as estimating the covariance (resp. correlations) betweenRX(\u03b3, u) andRX(\u03b3\u2032, u\u2212\u03c4) for zero-meanRX (resp. nonzero meanRX). The ergodicity assumption ensures that whenN is large enough, the approximation Cx\u0304 ' E(CX) over \u03a5 should hold with high probability. Under these conditions, it makes sense to sample the microcanonical set in order to generate new texture samples.\nThis framework encompasses a wide range of state-of-the-art texture models4. In particular, the PS model takes inspiration from the human early visual system to define a multi-scale representation based on the wavelet transform of the image (Heeger & Bergen, 1995). We next review a family of covariance model which generalizes the statistics in the PS model. We write CM the statistics for a specific model M that uses the representation RM."
        },
        {
            "heading": "2.2 WAVELET PHASE HARMONIC COVARIANCE MODELS",
            "text": "We review a family of microcanonical covariance models defined by a representation built upon the wavelet transform and phase harmonics. It defines a class of covariance statistics that capture dependencies between wavelet coefficients across scales."
        },
        {
            "heading": "2.2.1 WAVELET TRANSFORM",
            "text": "The wavelet transform is a powerful tool in image processing to analyze signal structures, by defining a sparse representation (Mallat, 2001). For texture modeling, we consider oriented wavelets to model geometric structures in images at multiple scales. They include the Morlet wavelets and steerable wavelets, proposed in Goupillaud et al. (1984); Simoncelli & Freeman (1995); Unser & Chenouard (2013). In particular, the Simoncelli steerable wavelets have been used to model a diverse variety of textures in Portilla & Simoncelli (2000).\nOriented wavelets are defined by the dilation and rotation of a complex function \u03c8 : R2 7\u2192 C on a plane. Let r\u03b8 denote the rotation by angle \u03b8 in R2. They are derived from \u03c8 with dilations by factors 2j , for j \u2208 {0, 1, \u00b7 \u00b7 \u00b7 , J \u2212 1}, and rotations r\u03b8 over angles \u03b8 = `\u03c0/L for 0 \u2264 ` < L, where L is the number of angles in [0, \u03c0). The wavelet at scale j and angle \u03b8 is defined by\n\u03c8j,\u03b8(u) = 2 \u22122j\u03c8(2\u2212jr\u03b8u), u \u2208 R2.\n3The complex-valued representation Rx(\u03b3, u) \u2208 C is a function of (\u03b3, u) in an index set \u0393\u00d7 \u2126N . 4e.g. Portilla & Simoncelli (2000); Gatys et al. (2015); Ustyuzhaninov et al. (2017); Zhang & Mallat (2021)\nScales equal or larger than J are carried by a low-pass filter \u03c6J .\nThe wavelet transform of an image x \u2208 RN\u00d7N is a family of functions obtained by the convolution of x with discrete wavelets.5 Let \u039b := {0, \u00b7 \u00b7 \u00b7 , J \u2212 1} \u00d7 \u03c0L{0, \u00b7 \u00b7 \u00b7 , L \u2212 1} be an index set. The wavelet coefficients are\nx ? \u03c8j,\u03b8(u) = \u2211 v\u2208\u2126N x(u\u2212 v)\u03c8j,\u03b8(v), u \u2208 \u2126N , (j, \u03b8) \u2208 \u039b. (2)\nThe low-pass coefficients x ? \u03c6J are defined similarly."
        },
        {
            "heading": "2.2.2 WAVELET PHASE HARMONICS AND THE PS MODEL",
            "text": "To model natural textures, it has been shown (Portilla & Simoncelli, 2000; Zhang & Mallat, 2021) that it is crucial to capture statistical dependencies between wavelet coefficients across scales. This can be achieved by using a wavelet phase harmonic representation, which is defined by the composition of a linear wavelet transform of x, and a non-linear phase harmonic transform.\nIn Mallat et al. (2020), the authors introduce the phase harmonics to adjust the phase of a complex number z \u2208 C. More precisely, the phase harmonics {[z]k}k\u2208Z of a complex number z \u2208 C are defined by multiplying its phase \u03d5(z) of z by integers k, while keeping the modulus constant, i.e. \u2200 k \u2208 Z, [z]k := |z|eik\u03d5(z). The wavelet phase harmonic representation (WPH) is then defined by\nRWPHx(\u03b3, u) = [x ? \u03c8j,\u03b8(u)] k \u2212 \u00b5\u03b3 , \u03b3 = (j, \u03b8, k) \u2208 \u0393 = \u039b\u00d7 Z, (3)\nwhere \u00b5\u03b3 is defined as the spatial average of [x\u0304 ? \u03c8j,\u03b8]k.\nIt is shown in Zhang & Mallat (2021) that the PS model can be regarded as a low-order wavelet phase harmonics covariance model, which considers only a restricted number of pairs (k, k\u2032) (see Appendix A for more details). In the next section, we shall use a dual representation of the phase harmonic operator to define a covariance model to capture high-order phase harmonics."
        },
        {
            "heading": "3 GENERALIZED RECTIFIER WAVELET COVARIANCE MODEL",
            "text": "In the previous section, we presented a class of models, built from the wavelet phase harmonic representation. A dual representation of the phase harmonic operator [\u00b7]k can be defined via a generalized rectified linear unit, that we review in Section 3.1. We then discuss in Section 3.2 how to define an appropriate index set of \u0393 for gray-scale textures. Section 3.3 extends the model to color textures."
        },
        {
            "heading": "3.1 FROM PHASE HARMONICS TO THE GENERALIZED RECTIFIER",
            "text": "The generalized rectified linear unit of a complex number z, with a phase shifted by \u03b1 \u2208 [0, 2\u03c0], is defined by\n\u03c1\u03b1(z) = \u03c1(Real(ei\u03b1z)), (4)\nwhere \u03c1 is a rectified linear unit, i.e. for any t \u2208 R, \u03c1(t) := max(0, t). In Mallat et al. (2020), it is shown that applying a Fourier transform on \u03c1\u03b1(z) along the variable \u03b1 results in the phase harmonics of z (up to some normalization constant). This suggests an alternative model, defined by coefficients of the form\nRALPHAx(\u03b3, u) = \u03c1\u03b1(x ? \u03c8j,\u03b8(u))\u2212 \u00b5\u03b3 , \u03b3 = (j, \u03b8, \u03b1), (5) for \u03b3 \u2208 \u0393 = \u039b\u00d7 [0, 2\u03c0], and \u00b5\u03b3 is defined as the spatial average of \u03c1\u03b1(x\u0304 ? \u03c8j,\u03b8(u)) over u \u2208 \u2126N .\nRelation with high-order phase harmonics Based on the duality between the phase harmonics k \u2208 Z and the phase shift variable \u03b1 \u2208 [0, 2\u03c0], we now present the relation between CALPHA and the high-order phase harmonics in CWPH, first proved in Mallat et al. (2020).\nProposition 1 There exists a complex-valued sequence {ck}k\u2208Z such that for all (j, \u03b8, \u03b1) \u2208 \u0393, (j\u2032, \u03b8\u2032, \u03b1\u2032) \u2208 \u0393, and all \u03c4 \u2208 \u2126N ,\nCALPHAx((j, \u03b8, \u03b1), (j\u2032, \u03b8\u2032, \u03b1\u2032), \u03c4) = \u2211\n(k,k\u2032)\u2208Z2 ckc \u2217 k\u2032C WPHx((j, \u03b8, k), (j\u2032, \u03b8\u2032, k\u2032), \u03c4)ei(k\u03b1\u2212k \u2032\u03b1\u2032).\n5The continuous wavelets are discretized with periodic boundary conditions on the spatial grid \u2126N .\nThe proof is given in Appendix B. We remark that the sequence {ck}k\u2208Z is uniquely determined by the rectifier non-linearity \u03c1, and they are non-zero if k is even (Mallat et al., 2020). This result shows that for a suitable choice of (\u03b1, \u03b1\u2032), the covariance statistics CALPHAx can implicitly capture CWPHx with a wide range of k and k\u2032.\nRelation with second order statistics Using a simple decomposition of wavelet coefficients into their positive, negative, real and imaginary parts, we can further show that the covariance statistics CALPHAx capture the classical second order statistics of wavelet coefficients, also used in the PS model (with phase harmonic coefficients k = k\u2032 = 1).\nProposition 2 Let I = {0, \u03c02 , \u03c0, 3\u03c0 2 }. There exists a finite complex-valued sequence {w\u03b1,\u03b1\u2032}(\u03b1,\u03b1\u2032)\u2208I2 such that for all (j, \u03b8) \u2208 \u039b, and all \u03c4 \u2208 \u2126N ,\u2211 (\u03b1,\u03b1\u2032)\u2208I2 w\u03b1,\u03b1\u2032C ALPHAx((j, \u03b8, \u03b1), (j\u2032, \u03b8\u2032, \u03b1\u2032), \u03c4) = \u2211 u\u2208\u2126N ( x ? \u03c8j,\u03b8(u) )( x ? \u03c8j\u2032,\u03b8\u2032(u\u2212 \u03c4) )\u2217 . (6)\nThe proof is given in Appendix C. This shows that using only four \u03b1 uniformly chosen between [0, 2\u03c0] is sufficient to capture second order statistics. Because the wavelet transform is an invertible linear operator (on its range space), computing the r.h.s of eq. (6) for all (j, \u03b8, \u03c4), as well as the low-pass coefficients carried out by \u03a6J , is equivalent to computing the correlation matrix of x.\nRelation with the RF model Setting aside the subtraction by the spatial mean \u00b5\u03b3 , the RF model can be viewed as a particular case of models defined by eq. (5). Indeed, the statistics of the RF model take the form of eq. (1), with\nRRFx(f, u) = \u03c1(x ? \u03c8f (u)),\nwhere {\u03c8f} being a family of multi-scale random filters. By writing \u03c1\u03b1(x ? \u03c8j,\u03b8(u \u2212 \u03c4)) = \u03c1(x ? Real(\u03c8\u03c4j,\u03b8e\ni\u03b1)(u)), with \u03c8\u03c4j,\u03b8 denoting the translation of \u03c8j,\u03b8 by \u03c4 , we see that the models are similar, the difference being that our models use wavelet-based filters instead of random ones."
        },
        {
            "heading": "3.2 DEFINING AN APPROPRIATE \u03a5",
            "text": "The choice of the covariance set \u03a5 is of central importance in the definition of the model. Intuitively, a too small set of indices will induce a model that could miss important structural information about the texture that we want to synthesize. Conversely, if \u03a5 contains too many indices, the syntheses can have good visual quality, but the statistics of the model may have a large variance, leading to the memorization of some patterns of the observation. There is a trade-off between these two aspects: one must capture enough information to get syntheses of good visual quality, but not much, so as not to reproduce parts of the original image. To illustrate this point, we shall study the model ALPHA defined with three different sets \u03a5 : A smaller model ALPHAS with a limited amount of elements in \u03a5, an intermediate model ALPHAI, and a larger model ALPHAL.\nTo precisely define these models, let us note J := {0, \u00b7 \u00b7 \u00b7 , J\u22121}, \u0398 := \u03c0L{0, \u00b7 \u00b7 \u00b7 , L\u22121}, andAA = 2\u03c0 A {0, \u00b7 \u00b7 \u00b7 , A\u22121}. Let us also define the set T := {0}\u222a{2\nj(cos(\u03b8), sin(\u03b8))}0\u2264j<J, \u03b8\u2208 \u03c0L{0,\u00b7\u00b7\u00b7 ,2L\u22121}, from which the spatial shift shall be selected. Table 1 summarizes the conditions that all parameters have to satisfy to be contained in these sets. Additionally, these models include large scale information through the covariance of a low-pass filter, i.e. the spatial average of x ?\u03c6J(\u00b7)x ?\u03c6J(\u00b7 \u2212 \u03c4), for \u03c4 \u2208 T. To count the size of \u03a5 without redundancies, Appendix A.3 provides an upper bound on the non-redundant statistics in our models. This upper bound is used to count the number of statistics in our models. To keep this number from being too large, instead of taking all shifts in a square box, such as in Portilla & Simoncelli (2000), we choose to select only shifts of dyadic moduli, and with the same orientations as the wavelets.\nALPHAS vs. ALPHAI The small model ALPHAS is inspired from the PS model, as it only takes into account of the interactions between nearby scales (i.e. |j\u2032 \u2212 j| \u2264 1), and the spatial shift correlations are only considered for (j, \u03b8) = (j\u2032, \u03b8\u2032). There are two notable differences in the statistics included in the ALPHAS and ALPHAI models. The first one is the range of scales being correlated. It has been shown in Zhang & Mallat (2021) that constraining correlation between a wider\nrange of scales induces a better model for non-Gaussian stationary processes, and a better estimation of cosmological parameters from observed data (Allys et al., 2020). The second difference, which has a significant impact on the number of statistics (it increases the model size by a factor \u223c10), is the number of spatial shifts in the correlations. In the ALPHAI model, spatially shifted correlations are computed for all pairs of coefficient (\u03b3, \u03b3\u2032). For both stationary textures and non-stationary images in gray-scale, shape and contours of salient structures and objects are better reproduced with ALPHAI, as illustrated in Figure 2. More examples are given in Appendix D.\nALPHAI vs. ALPHAL As we observe in Figure 2, the ALPHAI model, containing 4 times less coefficients than the ALPHAL, suffers less from memorization effects, while still capturing most of the geometric information in the images. This small loss of information can be partially explained by the frequency transposition property of the phase harmonics operator (Mallat et al., 2020), for compactly supported wavelets in the frequency domain, as detailed in Appendix E. In order to avoid this memorization effect, we shall, in the rest of the paper, consider only the intermediate model."
        },
        {
            "heading": "3.3 MODELLING COLOR INTERACTIONS",
            "text": "In order to generate color textures, the covariance model ALPHAI defined in Section 3.2 could be directly applied to each R, G and B color channel independently. However, it would not take into account the color coherence in the structures of the observation.\nTo capture color interactions in the observation image, we shall impose the covariance between the coefficients of eq. (5) for all indices in \u03a5 and all color channels. More precisely, let x = {xc}c=1,2,3 be a color image, with the parameter c representing the color channel. The ALPHAC color model is defined by correlations between coefficients of the form:\nRALPHACx(\u03b3, u) = \u03c1\u03b1(x c ? \u03c8j,\u03b8)(u)\u2212 \u00b5\u03b3 , \u03b3 = (j, \u03b8, \u03b1, c). (7)\nThe set of indices is defined as \u03a5ALPHAC := {(\u03b3, \u03b3\u2032, \u03c4) : ((j, \u03b8, \u03b1), (j\u2032, \u03b8\u2032, \u03b1\u2032), \u03c4) \u2208 \u03a5ALPHAI , (c, c\u2032) \u2208 {1, 2, 3}2}.\nReduced ALPHAC The model ALPHAC has a large size as it computes correlations between all coefficients for all color channels. This size can be significantly reduced by computing spatially shifted coefficients only for the same color channels (to capture their geometries). This reduced model contains three times less coefficients (\u223c113k), without significant reduction of the visual quality of syntheses, as detailed in Appendix F."
        },
        {
            "heading": "4 NUMERICAL RESULTS",
            "text": "In this section, we compare our intermediate model to the state-of-art models (PS, RF and VGG) on both gray-scale and color textures. We first specify the experimental setup. We then present the synthesis results of various examples, and discuss their quality through visual inspection. A quantitative evaluation of the quality of the syntheses, based on the synthesis loss of the VGG model, and proposed in Ustyuzhaninov et al. (2017), is discussed in Appendix G."
        },
        {
            "heading": "4.1 EXPERIMENTAL SETUP",
            "text": "For our experiments, we choose gray-scale and color textures with various visual structures.6 In the gray-scale examples, we also include a stationary turbulent field (vorticity), which is simulated from Navier-Stokes equations in two dimensions (Schneider et al., 2006). These examples all contain complex geometric structures that are hard to model by the classical PS model.\nThe texture images presented in this work have a size of N = 256, giving a number of pixels of \u223c65k. For all the ALPHA models, we use Morlet wavelets with a maximal scale J = 5 and number of orientations L = 4. This choice differs from the PS model, which uses Simoncelli steerable wavelets. In Appendix H, we discuss the impact of these wavelets on our model. To draw the samples, we follow gradient-based sampling algorithms, suitable in high-dimensions (Bruna & Mallat, 2018), to minimize the objective function \u2016Cx\u2212Cx\u0304\u20162, starting from an initial sample from a normal distribution. Similarly to Gatys et al. (2015); Ustyuzhaninov et al. (2017), we use the LBFGS algorithm (Nocedal, 1980) for the optimization of the objective. As in the VGG model (Gatys et al., 2015), we further apply a histogram matching procedure as post-processing after optimization. The details of the PS, RF and VGG models, as well as detailed specifications of our models, are given in Appendix A. More synthesis examples can be found in Appendix I."
        },
        {
            "heading": "4.2 RESULTS",
            "text": "In Figure 3, we present examples of syntheses from the ALPHAI (or ALPHAC), PS, RF and VGG models, for both gray-scale and color textures, as well as for non-stationary images. We observe that\n6The source for the presented textures are given in Appendix A.\nour model ALPHAI produces texture syntheses of similar visual quality to the RF and VGG models. It also significantly outperforms the PS model in terms of the visual quality, without introducing visible memorization effects. As the model PS uses the statistics closer to ALPHAS compared to ALPHAI, the performance of PS is somehow expected.\nNote that for the tiles example (the fifth row) in Figure 3, the VGG model produces less convincing textures, because the long-range correlations present in the image (aligned tiles) are not reproduced. To remedy this issue, it has been proposed in Berger & Memisevic (2017) to add spatial shifts to the correlations of the network feature maps. These shift statistics are similar to the parameter \u03c4 in our model. We also observe that, in the case of the sixth row example (flowers), all models fail to reproduce complex structures at object-level. Possible improvements of such models is further discussed in Section 5.\nFor non-stationary images, we find that certain image patches can be more or less memorized by the RF, VGG and ALPHAI models, as illustrated in the seventh row example. Understanding such memorization effect of non-stationary images is a subtle topic, as we find that in some binary images (x\u0304(u) \u2208 {0, 1}), only the PS model can reproduce the observation, even though it has a much smaller number of statistics (the last row example). We find that this is related to the spatial correlation statistics in PS (non-zero \u03c4 ). By removing this constraint, x\u0304 is no longer always reproduced.7 The non-stationary nature also appear in some logo near the boundary of some textures (e.g. bottom left in the observation of the first and fourth rows). Although this logo is reproduced by RF, VGG, ALPHAI and ALPHAC, it is a very local phenomenon, as we do not find visible copies of the textures when there is no logo, and it is likely related to the way one addresses the boundary effect (see more in Appendix A.5)."
        },
        {
            "heading": "5 DISCUSSION",
            "text": "In this work, we presented a new wavelet-based model for natural textures. This model incorporates a wide range of statistics, by computing covariances between rectified wavelet coefficients, at different scales, phases and positions. We showed that this model is able to capture and reproduce complex geometric structures, present in natural textures or physical processes, producing syntheses of similar quality to state-of-the-art models that use CNN-based representations.\nBeing defined with a wavelet family instead of multi-scale random filters, the proposed model uses less statistics than the RF model. For the gray-scale textures, our model has about 15 times less statistics, as it focuses on capturing the geometric structures present in images. Although our color model has a slightly larger number of statistics than VGG, the reduced color model, presented in Section 3.3 is three times smaller than ALPHAC, while achieving similar visual quality. It shows the potential to further reduce the size of the color model. In the PS model, a PCA on the color channels is performed (Vacher & Briand, 2021). The same idea could also be applied to our model.\nFurthermore, there are examples where all the models may all fail to produce some geometric structures at object-level, as illustrated in Figure 3 (sixth row). In this situation, we still need to find more informative statistics. One may for example consider to incorporate a second layer of wavelet transform as in the wavelet scattering transform (Leonarduzzi et al., 2019; He & Hirn, 2021). Another line of research is to introduce other kinds of losses (such as to encourage image smoothness) in order to improve the VGG model (Liu et al., 2016; Sendik & Cohen-Or, 2017). These losses are complementary, and could thus also be added to our models. Integrating these models with learning-based approaches is another promising direction (Zhou et al., 2018; Zhu et al., 1997; Xie et al., 2016; 2018).\nFinding a minimal set of statistics to define a texture model remains important because a large number of statistics can result in a high variance of the estimators, and the associated model may suffer from memorization effects. This is a problem because the aim of the model is to approximate the underlying distribution of the observation, and therefore produce diverse textures. In this regard, the mere visual evaluation of the synthetic textures can fail to take this aspect of the model into account. Defining a quantitative evaluation of quality and diversity, coherent with visual perception, remains an open problem (Ustyuzhaninov et al., 2017; Yu et al., 2019).\n7Set the parameter \u2206 = 0 in the PS model. See Appendix A for more details. This simple example suggests that sometimes it is very important to choose the right statistics to capture specific geometric structures.\nAcknowledgments We thank all the reviewers for their insightful feedback. This work was partially supported by a grant from the PRAIRIE 3IA Institute of the French ANR-19-P3IA- 0001 pro-\ngram. Sixin Zhang acknowledges the support provided by 3IA Artificial and Natural Intelligence Toulouse Institute, French \u201dInvesting for the Future - PIA3\u201d program under the Grant agreement ANR-19-PI3A-0004, and by Toulouse INP ETI and AAP unique CNRS 2022 under the Project LRGMD.\nReproducibility Statement In order to reproduce the experiments presented in this work, the main parameters to define the model can be found in Section 4.1. More details are given in Appendix A. Additionally, the code is made publicly available.\nEthics Statement The authors acknowledge that no potential conflicts of interest, discrimination, bias, fairness concerns or research integrity issues have been raised during the completion of this work."
        },
        {
            "heading": "A MODEL AND ALGORITHMIC SPECIFICATION",
            "text": "We provide additional information needed to reproduce the numerical results of the models (for both gray-scale and color textures) considered in this paper. First, we detail the models of PS, RF, VGG. We then give algorithmic parameters to obtain the synthesis images. For natural textures, we also propose a strategy to synthesize non-periodic images in our models. An image is non-periodic if a periodic extension of the image to the domain outside \u2126N create discontinuities at the contour of \u2126N .\nSources of textures Our natural texture examples were obtained from the following three sources: CNS NYU8, Textures.com9, Describable Textures Dataset model10 and the Github page of Berger & Memisevic (2017) 11.\nA.1 MODEL PARAMETERS\nWe specify the model parameters to synthesize both gray-scale and color textures. We also discuss how to extend the RF and the VGG model, originally designed for color textures, to model gray-scale textures.\n\u2022 PS: For both gray-scale and color models, we set the number of scales J = 5, and the number of orientations L = 8 for the Simoncelli steerable wavelets. The spatial shift \u03c4 = (\u03c41, \u03c42) \u2208 \u2126N is chosen to be in the range of max (\u03c41, \u03c42) \u2264 \u2206 = 4. Note that this is different to the model parameters reported for the default PS model (which is J = 4, L = 4,\u2206 = 3) as we find that it results in a larger set of statistics and better visual quality. The synthesis results of this model can be reproduced by a Matlab software.12\n\u2022 RF: For gray-scale textures, we consider J \u00d7 L random convolutional filters (\u03c8f )1\u2264f\u2264JL. Let f = (j, `), the index j representing the scale of each filter, whose size is (Wj ,Wj) for 1 \u2264 j \u2264 J . For \u0393 = {f = (j, `)|j \u2264 J, ` \u2264 L}, the representation is\nRRFx(\u03b3, u) = \u03c1(x ? \u03c8j,`(u)), \u03b3 = (j, `) \u2208 \u0393.\nFor a color image x = {xc}1\u2264c\u22643, we use 3 \u00d7 J \u00d7 L random convolutional filters. The representation of x is\nRRFx(\u03b3, u) = \u03c1 ( 3\u2211 c=1 xc ? \u03c8c,j,`(u) ) , \u03b3 = (j, `) \u2208 \u0393.\nThe correlations CRFx are defined for all pairs (\u03b3, \u03b3\u2032) \u2208 \u0393\u00d7 \u0393. In both the gray and color cases, it results in a correlation matrix CRF with J2L2 statistics.\n8http://www.cns.nyu.edu/\u02dclcv/texture/ 9https://textures.com/\n10https://www.robots.ox.ac.uk/\u02dcvgg/data/dtd/index.html 11https://github.com/guillaumebrg/texture_generation 12https://www.cns.nyu.edu/\u02dclcv/texture/\nFollowing the default setting of the RF model, we set J = 8 and L = 128 for filters whose sizes are W1 = 3,W2 = 5,W3 = 7,W4 = 11,W5 = 15,W6 = 23,W7 = 37,W8 = 55. Each filter \u03c8j,` or \u03c8c,j,` is generated randomly according to GlorotUniform in the software Lasagne. 13\n\u2022 VGG. For a color image x = {xc}, the VGG model computes a correlation matrix CVGGx between the features maps within different layers of a pre-trained CNN network. To adapt this model to gray-scale textures, we shall add one input layer which converts a gray-scale image y into a color image, by setting xc = y for each color channel c. This allows one to use the same CVGGx to compute the gradient of the VGG loss with respect to y, and therefore to synthesise a gray-scale texture. For both gray-scale and color textures, we use only five layers \u2019conv1 1\u2019, \u2019pool1\u2019, \u2019pool2\u2019, \u2019pool3\u2019, \u2019pool4\u2019, as proposed in the original work.\n\u2022 ALPHA. See the main text.\nA.2 ALGORITHMIC PARAMETERS\nWe specify the optimization parameters used to synthesize both gray-scale and color textures.\n\u2022 PS: It utilizes iterative projections onto constraint sets to generate textures. We set the number of iterations to 200.\n\u2022 RF: It uses the L-BFGS procedure14 with a memory size 20, and with a maximal number of iterations 2000. The initialization for each pixel value of a gray-image is Uniform between [\u22121, 1]. For the color image case, each RGB channel is initialized independently with a Uniform distribution between [\u22121, 1]. To address non-zero mean textures (i.e. E(X(u)) 6= 0), the empirical mean of x\u0304 is subtracted from the input x to compute the representation. It is added back to the output of the optimization to produce a synthesis.\n\u2022 VGG: It uses the L-BFGS procedure15 with a memory size of 20, and with a maximal number of iterations of 2000.For both gray-scale and color images, bounds constraints are used for the optimization. The initialization of each pixel value is the standard normal distribution (zero mean, unit variance). To address non-zero mean textures (i.e. E(X(u)) 6= 0), the VGG mean is subtracted from the input x of the representation16. It is added back to the output after the optimization to produce a synthesis (with an additional histogram matching post-processing).\n\u2022 ALPHA: For all the models in ALPHA, we use the L-BFGS optimization algorithm with restarts. Starting from the standard normal distribution, with mean and standard deviation estimated from the observation, we use the L-BFGS procedure implemented in Pytorch. It runs for 500 iterations and then it is restarted with an initialization obtained from the previous L-BFGS result. This is repeated 10 times to obtain the synthesis (with an additional histogram matching post-processing).\nA.3 NUMBER OF STATISTICS IN THE ALPHA MODELS\nIn this section, we detail the number of statistics in the different ALPHA models. We begin by giving the formula for each model (note that we do not include the low-pass statistics, which numbers are negligible). To (partially) avoid redundancy in the coefficients, for all models, we compute only the correlations for indices in \u03a5 such that j2 \u2265 j1. This gives us the following formulas for the number of statistics:\n\u2022 #(ALPHAS) = (2J \u2212 1)|\u03984|2|A4|+ J |\u03984||A4||T| \u2022 #(ALPHAI) = 12J(J + 1)|\u03984| 2|A4||T|\n13https://lasagne.readthedocs.io/ 14scipy.optimize.fmin_l_bfgs_b in Python 15scipy.optimize.minimize in Python 16For the color image whose pixel value is between zero and one, the mean of BGR is 0.40760392,\n0.45795686, 0.48501961. For the gray-scale, we simply take the average of the BGR mean.\n\u2022 #(ALPHAL) = 12J(J + 1)|\u03984| 2|A4|2|T| \u2022 #(ALPHAC) = 92J(J + 1)|\u03984| 2|A4||T|\nNote that, for all ALPHA models, we also compute first order statistics \u00b5\u03b3 , i.e. the spatial averages of RALPHAx\u0304(\u03b3, u). There are J |\u03984||A4| statistics of this sort in every model, which is negligible with respect to the total number of second order statistics.\nNote also that there are still some redundancies in these statistics, as for j = j\u2032, all correlations for \u03b8, \u03b8\u2032, \u03b1, \u03b1\u2032, \u03c4 are counted twice. The number of such statistics is17, for all model, superior to the number of first order statistics, which shows that our formula is in fact an upper bound for the exact number of statistics.\nA.4 NUMBER OF STATISTICS IN THE PS MODELS\nThe PS model can be interpreted as a particular case of the WPH covariance model as it contains the following three key categories of statistics:\n\u2022 Raw coefficient correlations (k = k\u2032 = 1): they capture 2nd order statistics of a stationary process X , i.e. the correlations between X(u) and X(u\u2032) for (u, u\u2032) \u2208 \u21262N . \u2022 Coefficient magnitude statistics (k = k\u2032 = 0): they capture information of X beyond the 2nd order statistics. Very often, nearby scales and angles are considered in the model, such as j\u2032 \u2248 j and \u03b8\u2032 \u2248 \u03b8. \u2022 Cross-scale phase statistics (k = 1, k\u2032 = 2): they capture local phase alignments of the wavelet coefficients at nearby scales j\u2032 = j + 1, which are complementary to the magnitude correlations.\nThe major issue to count the number of statistics of this model is to avoid double counting the statistics which are the same. This is mostly due to the symmetries of the covariance matrices. We next detail how obtain the number of statistics for both the gray-scale and color model, by following the work of Portilla & Simoncelli (2000) and Vacher & Briand (2021).\nAs before, we assume the number of wavelet scales is J , the number of wavelet orientations is L, and the spatial shift range is within a square of size (2\u2206+1)\u00d7(2\u2206+1). Let us denoteNa = 2\u2206+1. We next describe in detail the number of statistics of each category counted in our paper,\nA.4.1 PS IN GRAY-SCALE\n\u2022Marginal statistics of x: 6. They include mean, variance, skewness, etc. \u2022Marginal statistics of wavelet coefficients: 2(J + 1) + 1. \u2022 Auto-correlation of wavelet coefficients (raw coefficient correlations): (J + 1)(N2a + 1)/2. \u2022 Auto-correlation of magnitude of wavelet coefficients (coefficient magnitude statistics): JL(N2a + 1)/2 + JL(L\u2212 1)/2 + (J \u2212 1)L2. \u2022Mean of magnitude of wavelet coefficients: JL+ 2. This is not counted in the paper of Portilla & Simoncelli (2000), but it used in the Matlab software.\n\u2022 Cross-correlation of phase of wavelet coefficients (cross-scale phase statistics): 2(J\u22121)L2 +JL2. The extra JL2 coefficients are cross-correlation of real sub-band cousin coefficients, which are not counted in the paper, but used in the Matlab software.\nTo compare with the model size of the original work of Portilla & Simoncelli (2000), one can check that the sum of these number is 792 when J = 4, L = 4,\u2206 = 3 (Na = 7). If we do not count the coefficients which are only counted in the Matlab software (JL + 2 + JL2 = 82), it results in 710 as reported in the original paper.\nA.4.2 PS IN COLOR\n\u2022Marginal statistics of x and PCA transform of x: 6\u00d7 3 + 3\u00d7 4 = 30. 17The number of such moments is of the order of J |\u0398|2|A4| for the small model, J |\u0398|2|A4|2 for the intermediate and large models, and 9J |\u0398|2|A4|2 for the color model.\n\u2022Marginal statistics of wavelet coefficients: 6(J + 1) + 9. \u2022 Auto-correlation of wavelet coefficients (raw coefficient correlations): 3(J + 2)(N2a + 1)/2. This is called Central autoCorr of the PCA bands in the Matalb software (which includes lowband).\n\u2022 Auto-correlation of magnitude of wavelet coefficients (coefficient magnitude statistics): 3JL(N2a + 1)/2 + J(3L)(3L\u2212 1)/2 + (J \u2212 1)(3L)2. \u2022Mean of magnitude of wavelet coefficients: 3(JL+ 2). \u2022 Cross-correlation of phase of wavelet coefficients (cross-scale phase statistics): J(3L)2 + (J \u2212 1)(3L)(6L).\nA.5 NON PERIODIC BOUNDARIES IN NATURAL IMAGES\nThe convolution operation in the wavelet transform (equation 2) is performed using the Fast Fourier Transform. Additionally, recall from Section 2.1 that spatial shifts are defined with periodic boundary conditions. This implies periodicity of the input image x. However, natural texture images are not periodic, so one needs to adapt the computation of coefficients to take into account possible border effects. To that end, instead of averaging over all u \u2208 \u2126N as in eq. (1), each correlation coefficient is averaged over a sub-window inside \u2126N , which size depends on the scales of the coefficients being correlated. More precisely, let \u03b3 = (j, \u03b8, \u03b1) and \u03b3\u2032 = (j\u2032, \u03b8\u2032, \u03b1\u2032). Note jm := max(j, j\u2032). We define \u2126jm := {u = (u1, u2) \u2208 \u2126N : 2jm \u2264 ui < N \u2212 2jm , i = 1, 2}. Then, for non periodic images, we compute\nCALPHAx(\u03b3, \u03b3\u2032, \u03c4) = 1 |\u2126jm | \u2211 u\u2208\u2126N 1\u2126jm (u)1\u2126jm (u\u2212 \u03c4)R ALPHAx(\u03b3, u)RALPHAx(\u03b3\u2032, u\u2212 \u03c4),\n(8)\nwhere the spatial shifts are defined periodically. Note that the spatial averages \u00b5\u03b3 and \u00b5\u03b3\u2032 are also performed on \u2126jm ."
        },
        {
            "heading": "B PROOF OF PROPOSITION 1",
            "text": "Using the fact that\n\u03c1(Real(zei\u03b1)) = \u03c1(Real(|z|ei(\u03d5(z)+\u03b1))) = |z|\u03c1(cos(\u03b1+ \u03d5(z)),\nand computing the Fourier coefficients of the 2\u03c0-periodic function \u03c1\u03b1(z) in the variable \u03b1, we obtain\nF(\u03c1\u03b1(z))(k) := 1\n2\u03c0 \u222b [0,2\u03c0] \u03c1\u03b1(z)e \u2212ik\u03b1d\u03b1\n= |z| 1 2\u03c0 \u222b [0,2\u03c0] \u03c1(cos(\u03b1+ \u03d5(z))e\u2212ik\u03b1d\u03b1\n= |z|eik\u03d5(z)ck = [z]kck,\nwhere ck is the Fourier transform of h(.) := \u03c1(cos(.)) at the frequency k. The function \u03b1 7\u2192 \u03c1\u03b1(z) being periodic in \u03b1, we have its decomposition in Fourier series\n\u03c1\u03b1(z) = \u2211 k\u2208Z F(\u03c1\u03b1(z))(k)eik\u03b1\n= \u2211 k\u2208Z ck[z] keik\u03b1.\nWe can then write, for any z, z\u2032 \u2208 C, and \u03b1, \u03b1\u2032 \u2208 [0, 2\u03c0], \u03c1\u03b1(z)\u03c1\u03b1\u2032(z \u2032)\u2217 = \u2211\nk,k\u2032\u2208Z2 ckc \u2217 k\u2032 [z] k[z\u2032]\u2212k \u2032 ei(k\u03b1\u2212k \u2032\u03b1\u2032).\nReplacing z and z\u2032 by any two wavelet coefficients x ?\u03c8j,\u03b8(u) and x ?\u03c8j\u2032,\u03b8\u2032(u\u2212 \u03c4), we thus obtain the relation in Proposition 1."
        },
        {
            "heading": "C PROOF OF PROPOSITION 2",
            "text": "Let z \u2208 C, and recall from eq. (4), that \u03c1\u03b1(z) = \u03c1(Real(zei\u03b1)). Note that we have the following relation\nz = \u03c10(z)\u2212 \u03c1\u03c0(z)\u2212 i(\u03c1\u03c02 (z)\u2212 \u03c1 3\u03c02 (z)). (9)\nWe can then write zz\u2032\u2217 = ( \u03c10(z)\u2212 \u03c1\u03c0(z)\u2212 i(\u03c1\u03c02 (z)\u2212 \u03c1 3\u03c02 (z)) )( \u03c10(z \u2032)\u2212 \u03c1\u03c0(z\u2032)\u2212 i(\u03c1\u03c02 (z \u2032)\u2212 \u03c1 3\u03c0 2 (z\u2032)) ) =\n\u2211 \u03b1,\u03b1\u2032\u2208I2 w\u2032\u03b1,\u03b1\u2032\u03c1\u03b1(z)\u03c1\u03b1\u2032(z \u2032),\nwith I = {0, \u03c02 , \u03c0, 3\u03c0 2 }. Replacing z with x ? \u03c8j,\u03b8(u), z \u2032 with x ? \u03c8j\u2032,\u03b8\u2032(u\u2212 \u03c4),, and injecting this relation in eq. (1) gives us the desired result, with w\u03b1,\u03b1\u2032 = \u2126Nw\u2032\u03b1,\u03b1\u2032 .\nD SUPPLEMENTARY RESULTS FOR THE GRAY-SCALE ALPHA MODELS\nHere, we present further visual comparison between the different ALPHA models for gray-scale images, to illustrate the trade-off between quality and diversity.\nE RELATION BETWEEN ALPHAL AND ALPHAI\nHere, we informally explain why, under some conditions on the wavelet family, setting \u03b1\u2032 \u2208 {0} in the ALPHA models should not lose too much (but still some) information captured by the statistics.\nFirst, remark that the simple linear relation( \u2211 \u03b1\u2208A4 \u03c1\u03b1(z)e ik\u03b1 ) \u03c10(z) = \u2211 \u03b1\u2208A4 \u03c1\u03b1(z)\u03c10(z)e ik\u03b1 (9)\ntells us that computing all correlations for \u03b1 \u2208 A4 and \u03b1\u2032 = 0 gives us at least the information contained in the r.h.s. of eq. (9).\nFurthermore, if \u03b1 \u2208 A4 = {0, \u00b7 \u00b7 \u00b7 , 3\u03c04 }, we make the following approximation\u2211 \u03b1\u2208A4 \u03c1\u03b1(z)e ik\u03b1 ' \u222b [0,2\u03c0] \u03c1\u03b1(z)e \u2212ik\u03b1d\u03b1 = 2\u03c0ck[z] k.\nRecall also from the proof of Proposition 2, thatF(\u03c1\u03b1(z))(k) = [z]kck, where the Fourier transform is taken along the variable \u03b1. Therefore,\n\u03c10(z) = \u2211 k\u2208Z ck[z] k.\nTherefore, ( \u2211 \u03b1\u2208A4 \u03c1\u03b1(z)e ik\u03b1 ) \u03c10(z) ' ( \u222b [0,2\u03c0] \u03c1\u03b1(z)e \u2212ik\u03b1d\u03b1 ) \u03c10(z)\n= 2\u03c0ck[z] k ( \u2211 k\u2032\u2208Z ck\u2032 [z \u2032]k \u2032)\n= 2\u03c0ck \u2211 k\u2032\u2208Z ck\u2032 [z] k[z\u2032]k \u2032 .\nThen, replacing z and z\u2032 with wavelet coefficients, we get\n\u2211 \u03b1\u2208AA e\u2212ik\u03b1CALPHAx((j, \u03b8, \u03b1), (j\u2032, \u03b8\u2032, 0), \u03c4) ' 2\u03c0ck \u2211 k\u2032\u2208Z ck\u2032C WPHx((j, \u03b8, k), (j\u2032, \u03b8\u2032, k\u2032), \u03c4).\nUsing Plancherel\u2019s theorem, we can write that\nCWPHx((j, \u03b8, k), (j\u2032, \u03b8\u2032, k\u2032), \u03c4) = 1 |\u2126N | \u2211\n\u03c9\u2208 2\u03c0N \u2126N\nF([x ? \u03c8\u03bb]k)(\u03c9)F([x ? t\u03c4\u03c8\u03bb]\u2212k \u2032 )(\u03c9),\nwhere the Fourier transform of an image x is defined by F(x)(\u03c9) := \u2211 u\u2208\u2126N x(u)e\n\u2212i\u03c9u, and t\u03c4 denotes the translation by \u03c4 , i.e. t\u03c4f(\u00b7) = f(\u00b7 \u2212 \u03c4). Now, suppose that the wavelets \u03c8\u03b1 have disjoint compact frequency support, in balls B\u03bb(2\u2212jC \u2032), where \u03bb = 2\u2212jr\u2212\u03b8\u03be0, and \u03be0 is the central frequency of the mother wavelet \u03c8 (cf. ?). Suppose also\nthat frequency transposition property of the phase harmonics operator (cf. Mallat et al. (2020)) is such that [x ? \u03c8\u03bb]k has (approximately) frequency support in Bk\u03bb(k2\u2212jC \u2032). Then, for all \u03bb, \u03bb\u2032, and all k \u2208 Z, there exists only one k\u2217 such that the frequency supports of [x ? \u03c8\u03bb]k and [x ? \u03c8\u03bb\u2032 ]\u2212k \u2217 are not disjoint, i.e. such that CWPHx((j, \u03b8, k), (j\u2032, \u03b8\u2032, k\u2032), \u03c4) 6= 0. This tells us that\u2211 \u03b1\u2208AA e\u2212ik\u03b1CALPHAx((j, \u03b8, \u03b1), (j\u2032, \u03b8\u2032, 0), \u03c4) ' ckck\u2217CWPHx((j, \u03b8, k), (j\u2032, \u03b8\u2032, k\u2217), \u03c4).\nThus, computing all correlations for \u03b1 \u2208 A4, and \u03b1\u2032 = 0 gives us (approximately) all the information contained in WPH coefficients for any pair k, k\u2032.\nThis result lies on several approximations, and strong assumptions about the wavelets, which are not fully met in practice. For this reason, setting \u03b1\u2032 = 0 instead of \u03b1\u2032 \u2208 A4 effectively reduced the amount of information captured by the statistics, and therefore increases the diversity of the model. However, as we observe in Section 3.2, there is not too much information lost, and the resulting model still captures most of the important geometric structures in texture images."
        },
        {
            "heading": "F REDUCED COLOR MODEL",
            "text": "One can reduce the number of statistics in the color model by selection the spatial shift parameter \u03c4 to be non-zero only for correlations between the same color channels. More precisely, it is defined by the following index set: \u03a5 := {(\u03b3, \u03b3\u2032, \u03c4) : ((j, \u03b8, \u03b1), (j\u2032, \u03b8\u2032, \u03b1\u2032), \u03c4) \u2208 \u03a5ALPHAI , c = c\u2032 \u2208 {1, 2, 3}} \u222a {(\u03b3, \u03b3\u2032, 0) : ((j, \u03b8, \u03b1), (j\u2032, \u03b8\u2032, \u03b1\u2032), 0) \u2208 \u03a5ALPHAI , (c, c\u2032) \u2208 {1, 2, 3}2}. This gives a model of size \u223c 113k, with little degradation of the visual quality, as shown in Figure 5.\nG VGG SCORE\nIn Ustyuzhaninov et al. (2017), the authors proposed to use the synthesis loss of the VGG model to evaluate the quality of syntheses from any model. The goal is to define a quantitative, and more objective evaluation method than mere visual inspection. Since the VGG model produces syntheses almost indistinguishable from real textures, it is natural to consider its loss to asses the quality of a synthesis. We computed this loss for the first two examples of Figure 3 (radishes and cherries), and the frist two examples of Figure 9 (gravel and Turbulence flow). Note however that this loss is not exactly the same as the one used in Ustyuzhaninov et al. (2017), as the layers selected to compute the loss are different. In this work, we chose to use the layers suggested in Gatys et al. (2015), (i.e. \u2019conv1 1\u2019, \u2019pool1\u2019, \u2019pool2\u2019, \u2019pool3\u2019, and \u2019pool4\u2019 of the VGG-19 network (Simonyan & Zisserman, 2014)), and compute the relative VGG loss18.\nWe notice that this score is not always consistent with visual inspection, as there are texture examples and models for which the syntheses do not look much like the observation image, yet produce a small VGG loss (see e.g. the first and last rows of Figure 3, the RF model syntheses have the smallest loss). It should also be noted that the VGG loss reported on the VGG syntheses is not the synthesis loss after optimization, as a histogram matching (HM) procedure is performed as post-processing after optimization. We observed that the VGG loss of the syntheses from the VGG model after HM was considerably higher than the one for syntheses before it, while being visually very similar as illustrated in Figure 6. These observations suggest that the VGG score suffers from instabilities after reaching a certain level (that is, if the VGG loss is small enough, small perturbations of the values of the image pixels might have a strong impact on the loss).\nH INFLUENCE OF THE CHOICE OF THE WAVELET TRANSFORM\nH.1 INFLUENCE OF THE WAVELET FAMILY\nIn Section 3.2, we illustrated the importance of the set of indices \u03a5 that define the wavelet coefficients being correlated. Another important role is played by the choice of the wavelets used in\n18Using the code from https://github.com/ivust/random-texture-synthesis/blob/ master/vgg_loss.py (function style loss relative).\nequation 2. As illustrated in Figure 7, this choice can have a visible impact on the quality of the textures. We observe that, while on the first example, the coherence of the structures appear similar for the three wavelet families, the second example shows that the wavelets used in Portilla & Simoncelli (2000) are less efficient in reproducing the contours of the objects (pebbles). While in our experiments, we chose to use the classical Morlet wavelets, an optimal choice for the wavelet family remains an open problem.\nH.2 INFLUENCE OF SCALE PARAMETER\nRecall from Section 2.2.1, the wavelet transform of an image x is defined by\n{x ? \u03c8j,\u03b8, x ? \u03c6J}0\u2264j<J, \u03b8\u2208 \u03c0L{0,\u00b7\u00b7\u00b7 ,L\u22121}.\nThe maximal scale parameter J also plays an important role in the definition of the wavelet transform. It determines the scales of the structures being captured by the transform. If this parameter is too small, large structures in the observation image might not be captured and reproduced in the model syntheses. Conversely, if J is too large, then the large scale statistics may have a high variance, inducing a memorization effect in the syntheses. Figure 8 illustrates this point on two examples from Section 4.2. By setting J = 4 (i.e. the maximal range of structures captured by the wavelets is of size 24 = 16), we observe on the first example that the larger structures (bubbles) are not well reproduced. When J is set to 6, the observation is almost identically reproduced by the synthesis. Similarly on the second examples, several parts of the synthesis appear very similar to ones in the observation. We found that a suitable trade-off consists in setting J = 5 for images of size N = 256.\nI SUPPLEMENTARY RESULTS OF ALPHAI/ALPHAC VS. PS, RF, VGG\nIn Figure 9 we present additional syntheses on various examples, from the PS, ALPHAC, RF and VGG models. These examples can be viewed as random (Turbulence flow, tree bark, porous stone), structured (gravel, paisley pattern, tree leaf, school text), or inhomogeneous (crafted pattern of third row, but also the porous stone).\nWe see that ALPHAI again significantly improves the visual quality of the PS model on gray-scale textures such as the gravel and turbulence flow. The visual quality of RF and VGG seems also worse on Turbulence flow compared to ALPHAI . In some examples such as tree leaf, we find the synthesis of all the models are similar. On the inhomogeneous porous stone, non of the models give satisfying visual results.\nSimilarly, in Figure 10 are presented supplementary syntheses form the color models, for structured images (radishes, bubbles, flowers), quasi-periodic images (scales, honeycomb, bricks), and nonstationary images (feathers). As previously observed, for highly structured quasi-periodic images such as the bricks example, the VGG model fails to capture long-range correlation, which can be solved using the method of Berger & Memisevic (2017). Syntheses of non-stationary images exhibit memorization effects, as previously observed."
        }
    ],
    "title": "GENERALIZED RECTIFIER WAVELET COVARIANCE MODELS FOR TEXTURE SYNTHESIS",
    "year": 2022
}