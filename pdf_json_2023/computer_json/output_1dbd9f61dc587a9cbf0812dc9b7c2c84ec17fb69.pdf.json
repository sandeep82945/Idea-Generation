{
    "abstractText": "Deep models are dominating the artificial intelligence (AI) industry since the ImageNet challenge in 2012. The size of deep models is increasing ever since, which brings new challenges to this field with applications in cell phones, personal computers, autonomous cars, and wireless base stations. Here we list a set of problems, ranging from training, inference, generalization bound, and optimization with some formalism to communicate these challenges with mathematicians, statisticians, and theoretical computer scientists. This is a subjective view of the research questions in deep learning that benefits the tech industry in long run.",
    "authors": [
        {
            "affiliations": [],
            "name": "Vahid Partovi Nia"
        },
        {
            "affiliations": [],
            "name": "Guojun Zhang"
        },
        {
            "affiliations": [],
            "name": "Ivan Kobyzev"
        },
        {
            "affiliations": [],
            "name": "Michael R. Metel"
        },
        {
            "affiliations": [],
            "name": "Xinlin Li"
        },
        {
            "affiliations": [],
            "name": "Ke Sun"
        },
        {
            "affiliations": [],
            "name": "Sobhan Hemati"
        },
        {
            "affiliations": [],
            "name": "Masoud Asgharian"
        },
        {
            "affiliations": [],
            "name": "Linglong Kong"
        },
        {
            "affiliations": [],
            "name": "Wulong Liu"
        },
        {
            "affiliations": [],
            "name": "Boxing Chen"
        }
    ],
    "id": "SP:5e362138af3b11d5531833e6327ff1193187b60d",
    "references": [
        {
            "authors": [
                "D. Acuna",
                "G. Zhang",
                "M.T. Law",
                "S. Fidler"
            ],
            "title": "f-domain adversarial learning: Theory",
            "year": 2021
        },
        {
            "authors": [
                "N. de Freitas"
            ],
            "title": "Learning to learn by gradient descent by gradient descent",
            "year": 2016
        },
        {
            "authors": [
                "R. Banner",
                "I. Hubara",
                "E. Hoffer",
                "D. Soudry"
            ],
            "title": "Neural Information Processing Systems",
            "year": 2018
        },
        {
            "authors": [
                "M. Belkin",
                "D. Hsu",
                "S. Ma",
                "S. Mandal"
            ],
            "title": "Reconciling modern machine-learning",
            "year": 2019
        },
        {
            "authors": [
                "E. Bengio",
                "M. Jain",
                "M. Korablyov",
                "D. Precup",
                "Y. Bengio"
            ],
            "title": "Advances in neural information processing systems",
            "year": 2021
        },
        {
            "authors": [
                "T. Birdal",
                "A. Lou",
                "L.J. Guibas",
                "U. Simsekli"
            ],
            "title": "Intrinsic dimension, persistent homology and generalization in neural networks",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "L. Blier",
                "Y. Ollivier"
            ],
            "title": "The description length of deep learning models",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2018
        },
        {
            "authors": [
                "A. Blumer",
                "A. Ehrenfeucht",
                "D. Haussler",
                "M.K. Warmuth"
            ],
            "title": "Learnability and the vapnik-chervonenkis dimension",
            "venue": "Journal of the ACM (JACM)",
            "year": 1989
        },
        {
            "authors": [
                "M. Cacciola",
                "A. Frangioni",
                "M. Asgharian",
                "A. Ghaffari",
                "V.P. Nia"
            ],
            "title": "On the convergence of stochastic gradient descent in low-precision number formats",
            "year": 2023
        },
        {
            "authors": [
                "X. Chen",
                "X. Hu",
                "H. Zhou",
                "N. Xu"
            ],
            "title": "FxpNet: Training a deep convolutional neural network in fixed-point representation",
            "venue": "In IJCNN,",
            "year": 2017
        },
        {
            "authors": [
                "R. Tachet des Combes",
                "H. Zhao",
                "Wang",
                "Y.-X",
                "G.J. Gordon"
            ],
            "title": "Domain adaptation with conditional distribution matching and generalized label shift",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "M. Courbariaux",
                "Y. Bengio",
                "David",
                "J.-P"
            ],
            "title": "BinaryConnect: Training Deep Neural Networks with binary weights during propagations",
            "venue": "NeurIPS",
            "year": 2015
        },
        {
            "authors": [
                "M. Danilova",
                "A. Kulakova",
                "B. Polyak"
            ],
            "title": "Non-monotone behavior of the heavy ball method",
            "year": 2020
        },
        {
            "authors": [
                "D. Das",
                "N. Mellempudi",
                "D. Mudigere",
                "D. Kalamkar",
                "S. Avancha",
                "K. Banerjee",
                "S. Sridharan",
                "K. Vaidyanathan",
                "B. Kaul",
                "E. Georganas",
                "A. Heinecke",
                "P. Dubey",
                "J. Corbal",
                "N. Shustrov",
                "R. Dubtsov",
                "E. Fomenko",
                "V. Pirogov"
            ],
            "title": "Mixed Precision Training of Convolutional Neural Networks using Integer Operations",
            "venue": "In ICLR",
            "year": 2018
        },
        {
            "authors": [
                "S. Dhar",
                "J. Guo",
                "J. Liu",
                "S. Tripathi",
                "U. Kurup",
                "M. Shah"
            ],
            "title": "A survey of on-device machine learning: An algorithms and learning theory perspective",
            "venue": "ACM Transactions on Internet of Things",
            "year": 2021
        },
        {
            "authors": [
                "B. Dherin",
                "M. Munn",
                "M. Rosca",
                "D.G. Barrett"
            ],
            "title": "Why neural networks find simple solutions: the many regularizers of geometric complexity",
            "year": 2022
        },
        {
            "authors": [
                "L. Dinh",
                "R. Pascanu",
                "S. Bengio",
                "Y. Bengio"
            ],
            "title": "Sharp minima can generalize for deep nets",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "B. Efron"
            ],
            "title": "The estimation of prediction error: covariance penalties and cross-validation",
            "venue": "Journal of the American Statistical Association",
            "year": 2004
        },
        {
            "authors": [
                "J. Friedman",
                "T. Hastie",
                "R. Tibshirani"
            ],
            "title": "Additive logistic regression: a statistical view of boosting (with discussion and a rejoinder by the authors)",
            "venue": "The annals of statistics",
            "year": 2000
        },
        {
            "authors": [
                "Y. Ganin",
                "E. Ustinova",
                "H. Ajakan",
                "P. Germain",
                "H. Larochelle",
                "F. Laviolette",
                "M. Marchand",
                "V. Lempitsky"
            ],
            "title": "2016) Domain-adversarial training of neural networks. The journal of machine learning research",
            "year": 2030
        },
        {
            "authors": [
                "A. Ghaffari",
                "M.S. Tahaei",
                "M. Tayaranian",
                "M. Asgharian",
                "V.P. Nia"
            ],
            "title": "Is integer arithmetic enough for deep learning training",
            "year": 2022
        },
        {
            "authors": [
                "A. Goyal",
                "Y. Bengio"
            ],
            "title": "Inductive biases for deep learning of higher-level cognition",
            "venue": "Proceedings of the Royal Society A",
            "year": 2022
        },
        {
            "authors": [
                "P. Goyal",
                "P. Doll\u00e1r",
                "R.B. Girshick",
                "P. Noordhuis",
                "L. Wesolowski",
                "A. Kyrola",
                "A. Tulloch",
                "Y. Jia",
                "K. He"
            ],
            "title": "Accurate, large minibatch SGD: training imagenet in 1 hour",
            "venue": "CoRR abs/1706.02677",
            "year": 2017
        },
        {
            "authors": [
                "E. Grant",
                "Y. Wu"
            ],
            "title": "Predicting generalization with degrees of freedom in neural networks",
            "venue": "ICML",
            "year": 2022
        },
        {
            "authors": [
                "M. Haghifam",
                "G.K. Dziugaite",
                "S. Moran",
                "D. Roy"
            ],
            "title": "Towards a unified informationtheoretic framework for generalization",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "T. Hastie",
                "A. Montanari",
                "S. Rosset",
                "R.J. Tibshirani"
            ],
            "title": "Surprises in highdimensional ridgeless least squares interpolation",
            "venue": "The Annals of Statistics",
            "year": 2022
        },
        {
            "authors": [
                "I. Hubara",
                "M. Courbariaux",
                "D. Soudry",
                "R. El-Yaniv",
                "Y. Bengio"
            ],
            "title": "Quantized neural networks: Training neural networks with low precision weights and activations",
            "venue": "The Journal of Machine Learning Research",
            "year": 2017
        },
        {
            "authors": [
                "M. Imaizumi",
                "K. Fukumizu"
            ],
            "title": "Deep neural networks learn non-smooth functions effectively",
            "venue": "In The 22nd international conference on artificial intelligence and statistics,",
            "year": 2019
        },
        {
            "authors": [
                "M. Imaizumi",
                "K. Fukumizu"
            ],
            "title": "Advantage of deep neural networks for estimating functions with singularity on hypersurfaces",
            "venue": "Journal of Machine Learning Research",
            "year": 2022
        },
        {
            "authors": [
                "B. Jacob",
                "S. Kligys",
                "B. Chen",
                "M. Zhu",
                "M. Tang",
                "A. Howard",
                "H. Adam",
                "D. Kalenichenko"
            ],
            "title": "Quantization and Training of Neural Networks for Efficient Integer-ArithmeticOnly Inference",
            "year": 2018
        },
        {
            "authors": [
                "C. Ji"
            ],
            "title": "Generalization error and the expected network complexity",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 1993
        },
        {
            "authors": [
                "N.S. Keskar",
                "D. Mudigere",
                "J. Nocedal",
                "M. Smelyanskiy",
                "P.T.P. Tang"
            ],
            "title": "On large-batch training for deep learning: Generalization gap and sharp minima",
            "year": 2016
        },
        {
            "authors": [
                "N.S. Keskar",
                "R. Socher"
            ],
            "title": "Improving generalization performance by switching from adam to sgd",
            "venue": "ArXiv abs/1712.07628",
            "year": 2017
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "In 3rd International Conference on Learning Representations,",
            "year": 2015
        },
        {
            "authors": [
                "V. Koltchinskii"
            ],
            "title": "Rademacher penalties and structural risk minimization",
            "venue": "IEEE Transactions on Information Theory",
            "year": 2001
        },
        {
            "authors": [
                "Y. LeCun",
                "L. Bottou",
                "Y. Bengio",
                "P. Haffner"
            ],
            "title": "Gradient-based learning applied to document recognition",
            "venue": "Proceedings of the IEEE",
            "year": 1998
        },
        {
            "authors": [
                "E. Levina",
                "P. Bickel"
            ],
            "title": "Maximum likelihood estimation of intrinsic dimension. Advances in neural information processing systems 17",
            "year": 2004
        },
        {
            "authors": [
                "Z. Li",
                "E. Wallace",
                "S. Shen",
                "K. Lin",
                "K. Keutzer",
                "D. Klein",
                "J. Gonzalez"
            ],
            "title": "Train big, then compress: Rethinking model size for efficient training and inference of transformers",
            "venue": "In International Conference on machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "N. Littlestone"
            ],
            "title": "Relating data compression and learnability",
            "year": 1986
        },
        {
            "authors": [
                "S. Lotfi",
                "M.A. Finzi",
                "S. Kapoor",
                "A. Potapczynski",
                "M. Goldblum",
                "A.G. Wilson"
            ],
            "title": "Pac-bayes compression bounds so tight that they can explain generalization",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2022
        },
        {
            "authors": [
                "D.J. MacKay",
                "Z. Ghahramani"
            ],
            "title": "Comments on\u2019maximum likelihood estimation of intrinsic dimension\u2019by",
            "venue": "e. levina and p. bickel (2005)",
            "year": 2005
        },
        {
            "authors": [
                "D.A. McAllester"
            ],
            "title": "Some pac-bayesian theorems",
            "venue": "In Proceedings of the eleventh annual conference on Computational learning theory,",
            "year": 1998
        },
        {
            "authors": [
                "M.R. Metel"
            ],
            "title": "Variants of sgd for lipschitz continuous loss functions in low-precision environments",
            "year": 2022
        },
        {
            "authors": [
                "L. Metz",
                "J. Harrison",
                "C.D. Freeman",
                "A. Merchant",
                "L. Beyer",
                "J. Bradbury",
                "N. Agrawal",
                "B. Poole",
                "I. Mordatch",
                "A. Roberts",
                "J. Sohl-Dickstein"
            ],
            "title": "Velo: Training versatile learned optimizers by scaling up",
            "venue": "CoRR abs/2211.09760",
            "year": 2022
        },
        {
            "authors": [
                "R. Nakada",
                "M. Imaizumi"
            ],
            "title": "Adaptive approximation and generalization of deep neural network with intrinsic dimensionality",
            "venue": "The Journal of Machine Learning Research",
            "year": 2020
        },
        {
            "authors": [
                "P. Nakkiran",
                "G. Kaplun",
                "Y. Bansal",
                "T. Yang",
                "B. Barak",
                "I. Sutskever"
            ],
            "title": "Deep double descent: Where bigger models and more data hurt",
            "venue": "Journal of Statistical Mechanics: Theory and Experiment",
            "year": 2021
        },
        {
            "authors": [
                "Y.E. Nesterov"
            ],
            "title": "A Method for Solving a Convex Programming Problem with Convergence Rate O(1/k2)",
            "venue": "Soviet Mathematics Doklady",
            "year": 1983
        },
        {
            "authors": [
                "B. Neyshabur",
                "Z. Li",
                "S. Bhojanapalli",
                "Y. LeCun",
                "N. Srebro"
            ],
            "title": "The role of overparametrization in generalization of neural networks",
            "venue": "In International Conference on Learning Representations",
            "year": 2018
        },
        {
            "authors": [
                "B.T. Polyak"
            ],
            "title": "Some methods of speeding up the convergence of iteration methods",
            "venue": "USSR Computational Mathematics and Mathematical Physics",
            "year": 1964
        },
        {
            "authors": [
                "B.T. Polyak"
            ],
            "title": "Introduction to optimization",
            "venue": "Optimization Software Inc., Publications Division,",
            "year": 1987
        },
        {
            "authors": [
                "P. Pope",
                "C. Zhu",
                "A. Abdelkader",
                "M. Goldblum",
                "T. Goldstein"
            ],
            "title": "The intrinsic dimension of images and its impact on learning",
            "year": 2021
        },
        {
            "authors": [
                "M. Rastegari",
                "V. Ordonez",
                "J. Redmon",
                "A. Farhadi"
            ],
            "title": "2016) XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks",
            "venue": "In European Conference on Computer Vision,",
            "year": 2016
        },
        {
            "authors": [
                "S.J. Reddi",
                "S. Kale",
                "S. Kumar"
            ],
            "title": "On the convergence of adam and beyond",
            "venue": "In International Conference on Learning Representations",
            "year": 2018
        },
        {
            "authors": [
                "A. Reuther",
                "P. Michaleas",
                "M. Jones",
                "V. Gadepally",
                "S. Samsi",
                "J. Kepner"
            ],
            "title": "Survey and benchmarking of machine learning accelerators",
            "year": 2019
        },
        {
            "authors": [
                "U. Shaham",
                "A. Cloninger",
                "R.R. Coifman"
            ],
            "title": "Provable approximation properties for deep neural networks",
            "venue": "Applied and Computational Harmonic Analysis",
            "year": 2018
        },
        {
            "authors": [
                "S. Shalev-Shwartz",
                "S. Ben-David"
            ],
            "title": "Understanding machine learning: From theory to algorithms",
            "year": 2014
        },
        {
            "authors": [
                "J. Steinhardt",
                "G. Valiant",
                "S. Wager"
            ],
            "title": "2016) Memory, communication, and statistical queries",
            "venue": "In Conference on Learning Theory,",
            "year": 2016
        },
        {
            "authors": [
                "L.G. Valiant"
            ],
            "title": "A theory of the learnable",
            "venue": "Communications of the ACM 27(11),",
            "year": 1984
        },
        {
            "authors": [
                "V.N. Vapnik",
                "A.Y. Chervonenkis"
            ],
            "title": "On the uniform convergence of relative frequencies of events to their probabilities",
            "venue": "In Measures of complexity,",
            "year": 1971
        },
        {
            "authors": [
                "A. Vaswani",
                "N. Shazeer",
                "N. Parmar",
                "J. Uszkoreit",
                "L. Jones",
                "A.N. Gomez",
                "L. Kaiser",
                "I. Polosukhin"
            ],
            "title": "Attention is all you need. Advances in neural information processing systems 30",
            "year": 2017
        },
        {
            "authors": [
                "M. Wang",
                "S. Rasoulinezhad",
                "P.H. Leong",
                "So",
                "H.K.-H"
            ],
            "title": "NITI: Training Integer Neural Networks Using Integer-Only Arithmetic",
            "venue": "IEEE Transactions on Parallel and Distributed Systems",
            "year": 2022
        },
        {
            "authors": [
                "A.C. Wilson",
                "R. Roelofs",
                "M. Stern",
                "N. Srebro",
                "B. Recht"
            ],
            "title": "The marginal value of adaptive gradient methods in machine learning",
            "venue": "In Proceedings of the 31st International Conference on Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "H. Wu",
                "P. Judd",
                "X. Zhang",
                "M. Isaev",
                "P. Micikevicius"
            ],
            "title": "Integer quantization for deep learning inference: Principles and empirical evaluation",
            "year": 2020
        },
        {
            "authors": [
                "S. Wu",
                "G. Li",
                "F. Chen",
                "L. Shi"
            ],
            "title": "Training and Inference with Integers in Deep Neural Networks",
            "venue": "In ICLR",
            "year": 2018
        },
        {
            "authors": [
                "A.J. Wyner",
                "M. Olson",
                "J. Bleich",
                "D. Mease"
            ],
            "title": "Explaining the success of adaboost and random forests as interpolating classifiers",
            "venue": "The Journal of Machine Learning Research",
            "year": 2017
        },
        {
            "authors": [
                "K. Xia",
                "Lee",
                "K.-Z",
                "Y. Bengio",
                "E. Bareinboim"
            ],
            "title": "The causal-neural connection: Expressiveness, learnability, and inference",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "L. Xia",
                "S. Massei",
                "M. Hochstenbach",
                "B. Koren"
            ],
            "title": "On the influence of roundoff errors on the convergence of the gradient descent method with low-precision floating-point computation",
            "year": 2022
        },
        {
            "authors": [
                "J. Ye"
            ],
            "title": "On measuring and correcting the effects of data mining and model selection",
            "venue": "Journal of the American Statistical Association",
            "year": 1998
        },
        {
            "authors": [
                "Y. You",
                "I. Gitman",
                "B. Ginsburg"
            ],
            "title": "Large batch training of convolutional networks. arXiv: Computer Vision and Pattern Recognition",
            "year": 2017
        },
        {
            "authors": [
                "Y. You",
                "J. Li",
                "S.J. Reddi",
                "J. Hseu",
                "S. Kumar",
                "S. Bhojanapalli",
                "X. Song",
                "J. Demmel",
                "K. Keutzer",
                "C. Hsieh"
            ],
            "title": "Large batch optimization for deep learning: Training BERT in 76 minutes",
            "venue": "In 8th International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "J. Zhang",
                "S.P. Karimireddy",
                "A. Veit",
                "S. Kim",
                "S.J. Reddi",
                "S. Kumar",
                "S. Sra"
            ],
            "title": "Why are adaptive methods good for attention models? In Advances in Neural Information Processing Systems",
            "venue": "Annual Conference on Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "R. Zhang",
                "A.G. Wilson",
                "C. De Sa"
            ],
            "title": "Low-precision stochastic gradient langevin dynamics",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Y. Zhang",
                "T. Liu",
                "M. Long",
                "M. Jordan"
            ],
            "title": "Bridging theory and algorithm for domain adaptation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "H. Zhao",
                "R.T. Des Combes",
                "K. Zhang",
                "G. Gordon"
            ],
            "title": "On learning invariant representations for domain adaptation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "S. Zhou",
                "Y. Wu",
                "Z. Ni",
                "X. Zhou",
                "H. Wen",
                "Y. Zou"
            ],
            "title": "2016) DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients",
            "year": 2016
        },
        {
            "authors": [
                "C. Zhu",
                "S. Han",
                "H. Mao",
                "W.J. Dally"
            ],
            "title": "Trained Ternary Quantization",
            "venue": "In ICLR",
            "year": 2017
        },
        {
            "authors": [
                "S. Zhu",
                "B. An",
                "F. Huang"
            ],
            "title": "Understanding the generalization benefit of model invariance from a data perspective",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Mathematical Challenges in Deep Learning\nVahid Partovi Nia[1], Guojun Zhang[1], Ivan Kobyzev[1],\nMichael R. Metel[1], Xinlin Li[1], Ke Sun [3], Sobhan Hemati [1],\nMasoud Asgharian[2], Linglong Kong[3], Wulong Liu[1], Boxing Chen[1]\u2217\nOpen Letter\nSummary\nDeep models are dominating the artificial intelligence (AI) industry since the ImageNet challenge in 2012. The size of deep models is increasing ever since, which brings new challenges to this field with applications in cell phones, personal computers, autonomous cars, and wireless base stations. Here we list a set of problems, ranging from training, inference, generalization bound, and optimization with some formalism to communicate these challenges with mathematicians, statisticians, and theoretical computer scientists. This is a subjective view of the research questions in deep learning that benefits the tech industry in long run.\nKeywords: Learnable class; Low bit computation; Floating-point arithmetic; Degrees of freedom; Regularization; VC dimension; computational complexity; stochastic gradient descent.\n\u2217[1] Noah\u2019s Ark Lab, [2] McGill University, [3] University of Alberta. This document reflects a subjective viewpoint of the Noah\u2019s Ark Montreal Research Centre about some important mathematical challenges in deep learning. The corresponding author is boxing.chen@huawei.com 1\nar X\niv :2\n30 3.\n15 46\n4v 1\n[ cs\n.L G\n] 2\n4 M\nar 2\n02 3\n2 Mathematical Challenges in Deep Learning Montreal Research Centre\nContents"
        },
        {
            "heading": "1 Introduction 3",
            "text": ""
        },
        {
            "heading": "2 Background 3",
            "text": ""
        },
        {
            "heading": "3 Inference 6",
            "text": "3.1 Learnability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n3.2 Lowbit Large Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8"
        },
        {
            "heading": "4 Training 9",
            "text": "4.1 Lowbit SGD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n4.2 Effective Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n4.3 Data Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13"
        },
        {
            "heading": "5 Ambient and Intrinsic Dimension 15",
            "text": ""
        },
        {
            "heading": "6 Optimizer 16",
            "text": ""
        },
        {
            "heading": "7 Generalization 18",
            "text": "7.1 In Domain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n7.2 Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n7.3 Out of Domain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n8 Challenge 23\n\u00a9Huawei Technologies Mathematical Challenges in Deep Learning 3"
        },
        {
            "heading": "1 Introduction",
            "text": "Deep learning-based technology is finding its way to consumer products faster than expected. Conversational agents such as ChatGPT, deep learning-based perception modules in autonomous driving, automatic speech recognition in voice assistants implemented in our cell phones, context-aware translation engines on the web, are all concrete examples. The deep learning community has been obsessed with increasing the accuracy of the model to beat human precision. This started with the ImageNet classification challenge, and growing towards other applications ever since. This obsession with accuracy has led to large models with too many parameters that consequently face two major challenges: i) models are too large that no one can train them anymore, except big enterprises ii) even if the trained model is available, their deployment still relies on big enterprises, due to their large deployment resource requirement.\nThis trend will lead to the monopoly of artificial intelligence (AI) innovation to a handful of big enterprises, marginalizing small enterprises, universities, and the public from contributing to this growing field. This trend not only slows down AI innovation but it may affect AI to serve humanity in long run. We believe a fundamental rethinking of the current research directions is required to address the aforementioned two major issues. There has been efforts to gather important questions of the field such as Dhar et al. (2021). New directions has been proposed by fundamental re-thinking about deep models, see for instance Bengio (2017); Bengio et al. (2021); Xia et al. (2021). We take, however, another perspective in this document and aim to encourage researchers to attack questions that revolve around solving i) and ii) in particular."
        },
        {
            "heading": "2 Background",
            "text": "As models get larger, more memory and computational resources are required to learn (training step) and deploy (inference step) them in practice. We specifically target deep learning models that are emerging fast and transforming the tech industry. We begin by setting the required mathematical notation in Table 1.\nBasic learning theory deals with the predictor function f \u2208 F , where F is called the hypothesis class and f is the machine learning model such as multi-layer perceptron, perhaps indexed by some continuous parameters w, say fw. One may augment w with a set of discrete parameters such as the number of layers and the number of units per layer, to generalize weight\nestimation towards neural architecture search. It makes sense to consider the cardinality of the class to be finite in practice |F| < \u221e because any finite-precision function fw (e.g. in 32 bit single precision) provides many but finite set of choices for w. The parameter w is typically trained using optimization methods such as the stochastic gradient descent (SGD). Suppose (x, y) \u223c D denote observed data generated from distribution D, where x \u2208 Rd is the input feature and y the output label. The goal is to find the function f such that f (x) approximates y well, i.e. to learn function f from the training data (x, y). In other words we aim at finding the best model from the hypothesis class F according to the expected loss E(L{f\u0302 (x), y} = R(f\u0302 ) where the expectation is taken over the generating distribution D, so R(\u00b7) is the true risk. In practice the empirical risk 1 n \u2211n i=1 Lw(xi , yi) is evaluated and minimized. SGD is commonly used to optimize the empirical risk for deep models. Table 2 lists the training and inference complexity for a few well-known machine learning models.\nThere are special cases of SGD update that are commonly used in practice. Suppose the positive real number \u03b7k is the learning rate at iteration k . The common SGD updates the weights according to\nwk+1 = wk \u2212 \u03b7kgk , (1)\nwhere gk is the gradient \u2202L \u2202wk . The SGD with momentum updates the weights according to\nwk+1 = wk \u2212 \u03b7kmk , (2)\nwhere mk = \u03b21mk\u22121 + (1\u2212\u03b21)gk is the momentum. These updates are often implemented in 32 bit float, but AI industry is pushing these computations in lower bits; e.g. Google\u2019e brain float that uses 16 bits, or the Grace Hopper NVIDIA chip that uses 8-bit float.\n6 Mathematical Challenges in Deep Learning Montreal Research Centre\nEmpirical risk minimization averages the loss over the data samples (xi , yi) \u2208 S, i = 1, . . . , n instead D, which introduces approximation error and estimation error as explained in the following. One may re-write that R(f )\u2212R\u2217 differently, where R\u2217 is the true minimum over all possible functions. Note that the optimum function may probably fall out of the hypothesis class F . This inductive bias of constraining f \u2208 F calls for the following approximation and estimation error decomposition,\nR(f )\u2212 R\u2217 = {\nmin f \u2208F\nR(f )\u2212 R\u2217 } + { R(f )\u2212min\nf \u2208F R(f )\n} ,\nwhere the first term is the approximation error, and the second term is the estimation error. The above decomposition facilitates better understanding of finding a model f whose risk is reasonably close to R\u2217 in and out of F ."
        },
        {
            "heading": "3 Inference",
            "text": "The main challenge of large models is to train and deploy them while the resource is constrained according to C(f\u0302 ) due to power, memory, and latency consumption. The common solution is to embed the computations in lower bits. Figure 1 summarizes the state-of-the-art low bit solutions for deep models, see Reuther et al. (2019) for a survey.\nMore formally, we want to estimate the function f\u0302 : X \u2192 Y that minimizes R(\u00b7) while satisfying C(f\u0302 ). The most important constraints are typically, i) memory, ii) latency, iii) energy. Most of the literature focuses on memory because it is difficult to model the latency and power constraints as they are hardware dependent. In many scenarios, latency constraints can be translated into memory constraints for a given hardware.\nMemory constraints appear at inference to fit the model into registers. For instance, the deep model f\u0302w is indexed with weights w and the weight value w has a certain range like \u00b13.4 \u00d7 1038 if w is 32-bit float, and w \u2208 {0, . . . ,\u00b1215} if the model is 16 bit integer. The range and the resolution of computation define the memory capacity.\n\u00a9Huawei Technologies Mathematical Challenges in Deep Learning 7"
        },
        {
            "heading": "3.1 Learnability",
            "text": "Learnability of a class is perhaps one of the most crucial properties required to ensure appropriateness of the chosen loss and class. In a learning problem the true risk is minimized, i.e. f\u0302 = arg minf \u2208F R(f ), and the optimum risk within the class is R(f\u0302 ) \u2265 R\u2217. In practice, however, the empirical risk is minimized, i.e. f\u0302n = arg minf \u2208F 1 n \u2211n i=1 L{f (xi , yi)}. A class is learnable if the risk of f\u0302n approaches to f\u0302 ,\nlim n\u2192\u221e\nPr{R(f\u0302n)\u2212 R(f\u0302 ) > } = 0.\n8 Mathematical Challenges in Deep Learning Montreal Research Centre\nThis convergence must be uniform on the probability distribution D so that a class becomes a learnable class. If a uniformly convergent sequence of f\u0302n does not exist, the class is not learnable. If such a sequence exists, the rate of convergence of R(f\u0302n) \u2192 R(f\u0302 ) defines how hard it is to learn from data. For instance, decision trees are hard to learn because this convergence rate is slow. Suppose the constrained class is C = {f | C(f ) < c}. Before deploying the model at inference in low bits, one may need to make sure the low-bit version is learnable. In other words, the constraint C(\u00b7) does not restrict the learnability of the class F . In more precise terms F \u2032 = F \u2229 C is still a learnable class. If a class is learnable, the quantized low-bit float or fixed-point projection of the class, Q(F), that reflects F \u2032 may or may not remain learnable."
        },
        {
            "heading": "3.2 Lowbit Large Models",
            "text": "In large deep models such as transformers (Vaswani et al., 2017), the predictive function fw even after training w, requires massive deployment resources. Suppose a model is already trained with weights w\u0302. A common deployment strategy is to look for a low bit projection Q(fw\u0302).The first step is to quantize the weights Q(w\u0302), and the second step is to implement the internal computations of fw\u0302 in low bits. There are three strategies to look for a lower-bit projection: i) a data-free method in which only the model is used, ii) only a small calibration set of data is used iii) the whole training data is used. Methods i) and ii) are referred to as post-training quantization while iii) is called quantize-aware training. With the advent of large models i) and ii) attracts more attention. The quantize-aware training is recently dismissed because the training data is often unavailable, and also the resources required for retraining a smaller model are very costly. Quantizing weights only, can be re-written simply as\nQ(w\u0302) = arg min w\u2208Q(Rd )\n\u2016w \u2212 w\u0302\u2016, (3)\nwhere the optimization is performed on the discrete set Q(Rd). A common method is to choose a good Q(w\u0302) directly, for instance, a step function that transforms a continous w\u0302 to a discrete Q(w\u0302), which is known as the quantization function. However, quantizing weights directly using the quantization function, does not assure a good approximation of w\u0302, because a deep model is composed of several layers and the approximation error of each layer affects the\n\u00a9Huawei Technologies Mathematical Challenges in Deep Learning 9\ncomputation of the next layer. Ignoring the inter-layer computations by focusing on \u2016w\u0302\u2212w\u2016 may lead to a large function approximation error \u2016fw\u0302 \u2212 fw\u2016. Perhaps a better strategy is to choose the weights so that the output of the function is properly approximated, i.e.\nQ(w\u0302) = arg min w\u2208Q(Rd )\n\u2016fw \u2212 fw\u0302\u2016, (4)\nbut the tedious computation of f makes this optimization infeasible. A greedy approach is used to match each layer instead. A deep model is composed of several layers starting with the input features X0, which is a matrix of dimension ndim(x), built by concatenating the input features xi . Each layer l includes a weight matrix Wl . The collection of such matrices forms the total weight w, such that Xl = \u03c3(Xl\u22121Wl\u22121) where \u03c3(\u00b7) is the nonlinear activation function. In each layer the following optimization is performed\nQ(W\u0302l) = arg min Wl\n\u2016\u03c3(XlWl)\u2212 \u03c3(XlW\u0302l)\u2016. (5)\nQuantizing the weights of layer l \u2212 1 will affect quantizion of the next layer l , and of course, calibration data are required to feed X0. A more precise quantization can be performed by optimizing equation 4 directly, or by priortizing a block of leading layers in the approximation error in equation 5."
        },
        {
            "heading": "4 Training",
            "text": "Suppose the hypothesis class F is learnable. The challenge is to devise a computationally efficient algorithm A(\u00b7) : S \u2192 F that uses the training data (xi , yi) \u2208 S to pave the way towards finding a good candidate function f\u0302 \u2208 F . This is equivalent to finding fw, i.e. estimating w\u0302. In deep learning the number of parameters d = dim(w) is overwhelming. A common remedy for large resource requirements is to lower the number of bits, from the standard 32 bit single-precision float towards 16 bit half-precision, or even lower (Hubara et al., 2017). The loss of model accuracy is the main obstacle in lowering the number of bits. Training on 8 bits Ghaffari et al. (2022), and inference on 8 bits Wu et al. (2020) would not hurt the accuracy compared to single precision in practice. However, the limits of lowering the bit width with no accuracy loss is still being evaluated empirically, and require more theoretical study (Cacciola et al., 2023; Metel, 2022; Zhang et al., 2022).\n10 Mathematical Challenges in Deep Learning Montreal Research Centre\nThe training constraint is two-fold (Steinhardt et al., 2016), i) the constraint on the hypothesis class F , ii) the constraint on the approximating algorithm A(\u00b7) : S \u2192 F towards estimating f\u0302 \u2208 F . Suppose A(S) leads to a proper f\u0302 , given F \u2229 C is learnable. The main challenge is to find a training algorithm A : S \u2192 F that minimizes R(\u00b7) while it satisfies C{A(S)} to deliver Q(f\u0302 ). In the sequel we only focus on the common training algorithms for deep models, i.e. we assume A(\u00b7) to be a low-bit SGD."
        },
        {
            "heading": "4.1 Lowbit SGD",
            "text": "Neural network training has been performed in single-precision (32-bit) floating-point. The ever-increasing size of deep learning models motivated the use of lower precision data types, such as low-bit floating, fixed, or dynamic fixed-point number representations during model training and for the final model representation. Besides decreasing memory requirements, model training and inference time can be reduced, as well as hardware and electricity costs. This makes the development and the use of deep models accessible to more people.\nA large body of research uses different number formats for different types of data to save resources while at the same time maintaining the model accuracy achieved using single precision, see Table 3. Model weights are quantized during or after training. Often only the most time-consuming operations, such as matrix multiplication are performed in a lowbit format. Given the difficulty in training neural networks, certain non-linear operations or weights are typically left in full precision.\nAn existing gap between optimization theory and neural network training is amplified by the use of low-bit number formats. Almost all optimization theory is developed in Euclidean space, with its convergence results relying on concepts such as continuity, limits of sequences, gradients, etc., whereas neural network training is performed numerically in finite precision environments. Unlike single-precision floating-point, the gap between theory and computation cannot be ignored in general given non-trivial rounding errors in low bits.\nA step of SGD can be modelled as\nwk+1 = wk \u2212 \u03b7k(gk + e1k) + e2k , (6)\n\u00a9Huawei Technologies Mathematical Challenges in Deep Learning 11\nwhere wk are the trainable parameters of the neural network in iteration k , \u03b7k is the stepsize, gk is a stochastic gradient, e1k is the rounding error from approximately computing gk , see Figure 2, and e2k is the rounding error from computing all of the arithmetic operations in equation 6. The error e1k is most problematic given that all arithmetic operations in computing the forward and back propagation contribute to it, increasing its upper bound as the model size increases. In addition, unbiased error E(e1k) = 0 does not hold in general, even when using stochastic rounding. The convergence of gradient descent with computational error in the gradient is a long-studied problem, see for example (Polyak, 1987, Chapter 4) and (Bertsekas, 1999, Chapter 1.2). Recently Xia et al. (2022); Cacciola et al. (2023); Metel (2022), studied the convergence of gradient descent in low-precision environments. Taking all computations to a sufficiently low precision will destroy the error assumptions in these works, implying the inability of equation 6 to converge in general.\nWe wonder if stochastic gradient descent is still a viable training algorithm for general low-precision neural network training. Especially if the low-precision number format no longer\nsufficiently approximates Euclidean space. Perhaps a more appropriate algorithm should be used, acknowledging that the optimization is being performed in a finite space. The error e1k could be decreased by using a finite difference approach to approximately compute the gradient. An alternative is to abandon approximate gradient methods for purely heuristic search methods used for black-box optimization adopted for low bit training structures."
        },
        {
            "heading": "4.2 Effective Parameters",
            "text": "Deep learning models include many parameters that overloads their computation. This complicates the training, because all such large models require proper and mostly complicated regularization schemes. Redesigning a smaller model trained with a lower amount of regularization can not only simplifies training, but also lead to lower resource inference. This requires rethinking the regularization concept, and calls for a new optimization algorithm that relates the large and highly regularized models to smaller and less regularized models. The concept of effective parameters allows us to have an idea about a new model that can approximate the original model with good accuracy but smaller number of parameters. This is closely related to the complexity of the true underlying model. Figure 3 illustrates how the number of parameters decreases as more regularization is exercised in training.\nYe (1998) formalizes the linear model fit with `2 regularization\nw\u0302\u03bb = arg min w\u2208Rd\n\u2016y \u2212 Xw\u2016+ \u03bb\u2016w\u2016\nd\u03bb := dim(w\u0302\u03bb) \u221d cov(y, y\u0302\u03bb),\nwhere y\u0302\u03bb = Xw\u03bb, and d\u03bb decreases as \u03bb increases (Efron, 2004). This concept is closely related to compression bound (Blier and Ollivier, 2018), geometric complexity (Dherin et al., 2022), and generalization error (Ji, 1993; Grant and Wu, 2022).\n\u00a9Huawei Technologies Mathematical Challenges in Deep Learning 13\nWe wonder how to extend this concept to deep models to have an idea about their effective dimensions. Even knowing the effective parameters may not help to construct the smaller model. A proper algorithm to find a more compact model given the effective number of parameters is still an open research question."
        },
        {
            "heading": "4.3 Data Dimension",
            "text": "Data used in deep learning such as image pixels, language words, or speech intensity has a low-dimensional structure despite the high-dimensional representation. This property is the reason for the remarkable success of deep models. The common intuition is that each layer folds the dimension through a nonlinear activation before and passes the folded information to the next layer. The data dimension is model-free and only relies on data only. Levina and Bickel (2004) suggests to count the neighbouring points to estimate the data dimension and Pope et al. (2021) shows the impact of data dimension on learning.\nGiven a set of sample points in Rn, it is common to assume that the data lies on or near a low-dimensional manifold, see Figure 4. The common approach is to use a Poisson process to model the number of points found by random sampling within a given radius\n14 Mathematical Challenges in Deep Learning Montreal Research Centre\naround each sample point. By relating the rate of this process to the surface area of the sphere, the likelihood equations yield an estimate of the inverse intrinsic dimension at a given point d\u0302\u22121(xi). Therefore ultimate estimation d\u0302 \u22121 is averaging d\u0302\u22121(x) over the n data points to provide an estimation of the data dimension (MacKay and Ghahramani, 2005)\nd\u0302 \u2248\n{ 1\nn n\u2211 i=1 d\u0302\u22121(xi)\n}\u22121 . (7)\nThis means each data point carries a weight about the true data dimension. Intuitively weighting samples leads to weighting their respective fitted models (Friedman et al., 2000). In other words models are smoother version of data. Therefore, an alternative data dimension estimation can be deployed through the concept of effective parameters explained earlier. While models vary in parameter size, their effective dimensions remain close to the true data dimension. One may call for an algorithm that estimates data dimension during training by connecting effective parameters with batch data dimension to lower the parameters of the model and compress while training, simultaneously. An ideal model uses the training parameters effectively and matches the data dimension with the model dimension. An ideal descent direction takes the gradient in two direction i) weight direction ii) model size direction. In training step, optimal weights are found given the dimension, and in compression optimal dimension is found given the weights.\n\u00a9Huawei Technologies Mathematical Challenges in Deep Learning 15"
        },
        {
            "heading": "5 Ambient and Intrinsic Dimension",
            "text": "It is widely believed that deep neural networks work well when data are essentially on a lowdimensional manifold embedded in a high-dimensional ambient space. This view is particularly pervasive for natural image data Pope et al. (2021) and Ansuini et al. (2019). Shaham et al. (2018) formalize this view by proving, under some conditions, that the universal approximation depends strongly on the intrinsic dimension of the data while the dependence on the dimension of ambient space is comparatively rather weak. The knowledge about the intrinsic dimension is therefore imperative in order to decide how well neural networks work and how they should be designed. The more recent work by Imaizumi and Fukumizu (2019) and Imaizumi and Fukumizu (2022) formally establish the advantage of DNN in estimating non-smooth functions. Nakada and Imaizumi (2020) further show that the optimal minimax rate is achievable using DNN and the rate essentially depends on the intrinsic dimension measured using Minkowski\u2019s fractal dimension. More recent studies by Birdal et al. (2021) sheds further light on computational aspects of intrinsic dimension and connection to generalization of DNN. It is, however, remain to understand how intrinsic dimension is related to the depth and width of DNN. To be more concrete given the intrinsic dimension of data, what is the minimal depth and width to achieve a pre-specified level of accuracy in training and latency in inference.\nWhile different deterministic approaches for measuring intrinsic dimension using variants of Hausdorff (topological) dimension, including Minkowski\u2019s, persistent homology based measures of dimension or other methods aim at measuring the dimension of the whole data cloud, one may take a statistical perspective and try to measure the dimension of a manifold that can cover the great majority, say over 90 or 95 percent, of the data cloud. In view of the concentration phenomena in large dimension, it is plausible to expect such approach leads to a much smaller intrinsic dimension. To establish what that have been already studied by the aforementioned authors using such statistical approaches in measuring the intrinsic dimension seems a fruitful direction in studying advantages of deep neural networks. Further to such studies, one may try to answer the question posed in the previous paragraph about the connection between data dimension and the hyper-parameters of deep neural networks.\n16 Mathematical Challenges in Deep Learning Montreal Research Centre\nA thorough study on effective methods of dimension estimation and universal approximation of deep neural networks can hopefully lead to an explicit, though approximate, formula connecting the intrinsic data dimension to the architecture of deep neural networks. Such studies can provide guidelines for at least part of neural network architectures."
        },
        {
            "heading": "6 Optimizer",
            "text": "Different modifications of stochastic gradient descent (SGD) have been successfully used for the optimization (training) of neural networks. The method constitutes the iterative updates of model weights ideally reaching a lower value of loss function at each step.\nA variety of learning rate schedulers (dependency of the learning rate \u03b7k on k , the iteration step) are used in practice. The simplest one is the constant learning rate (all \u03b7k are the same). Usually practitioners apply some kind of decay on the learning rate during training (\u03b7k is a monotone decreasing function of k). Goyal et al. (2017) showed the importance of learning rate warm-up for some settings: starting with a very small \u03b7, then increase it during the training and then anneal back.\nThere are also implicit ways to modify the learning rate, so called adaptive methods.\n\u2022 RMSProp:\nwk+1 = wk \u2212 \u03b7 \u221a\nvk gk , where vk = \u03b22vk\u22121 + (1\u2212 \u03b22)g2k is the second momentum. (8)\n\u2022 Adam (Kingma and Ba, 2015):\nwk+1 = wk \u2212 \u03b7\u221a v\u0302k m\u0302k , where m\u0302k = mk 1\u2212 \u03b2k1 , v\u0302k =\nvk\n1\u2212 \u03b2k2 are unbiased estimators of the first and second momentums.\n(9)\n\u2022 LARS (You et al., 2017):\nw(l),k+1 = w(l),k \u2212 \u03b7 \u2016w(l),k\u2016 \u2016g(l),k\u2016 g(l),k , where (l) corresponds to the l-th layer parameters.\n(10)\nIt has been formally proven that adding momentum gives an acceleration in convergence for stochastic gradient methods (Polyak, 1964; Nesterov, 1983; Danilova et al., 2020). Adaptive\n\u00a9Huawei Technologies Mathematical Challenges in Deep Learning 17\noptimizers are not guaranteed to converge to the optimal solutions even in the convex case (Reddi et al., 2018), but in practice, it has been demonstrated to be fast and reliable (Kingma and Ba, 2015). SGD with momentum could outperform adaptive optimizers in vision tasks (Keskar and Socher, 2017), however, adaptive methods become especially important for attention models like transformers (Zhang et al., 2020). Finally, adaptive optimizers of LARS type help stabilizing the training with large batch sizes and hence increasing the training speed (You et al., 2020).\nA general optimizer update rule can be written as\nwk+1 = h(w1:k ,g1:k , k,F) (11)\nThere has been an effort (Andrychowicz et al., 2016; Metz et al., 2022) to learn the update function h from equation 11 in a meta-learning setting for different task and architectures. Although being promising, this approach still doesn\u2019t scale well to large network and requires expensive training.\nThe main problem with tracing first and second-order momentum is the memory. One needs roughly 3\u00d7 more memory for the gradient update, which becomes especially problematic with training large neural networks. Furthermore, SGD provides the solution with better generalization than its more advanced counterparts like ADAM (Wilson et al., 2017). However, utilization of momentum stabilizes the training and in some cases accelerates it. Momentum is currently applied in most optimizers for large networks. Is utilizing momentum really necessary for training and is there a way to achieve stability and acceleration without them?\nIdeally we want to modify history dependency in the update equation, while keeping the training stable and efficient. It would be highly beneficial for practitioners to find the effective version of SGD applicable to many deep model, in particular, large transformers. We wonder if there is a way to design network-specific optimizers rather than using default methods with cumbersome hyperparameter fine-tuning. In other words, we wonder how to utilize the inductive bias fw \u2208 F (Goyal and Bengio, 2022) to design an effective (and efficient!) update step with some theoretical guarantees on convergence.\n18 Mathematical Challenges in Deep Learning Montreal Research Centre"
        },
        {
            "heading": "7 Generalization",
            "text": "One of the most profound and broadest math challenges in deep learning is the generalization problem. For example, in autonomous driving, the training environments (e.g. daytime in a park) often differ from the test environments (e.g. night in an urban area); the training text in a sentiment analysis system differs from the real text to classify. The generalization problem deals with obtaining a machine learning model with good performances on our training datasets, that formally guarantee it also performs well on new datasets. There are two types of assumptions for this problem i) In-domain (ID) generalization, i.e. samples from the training set and test set are both drawn from the same underlying distribution and ii) out-of-domain (OOD) generalization in which the training set and the test set are drawn from different underlying distributions. For classification, a sample (x, y) \u223c D is composed of input x (e.g. an image) and label output y .\nIn this section, we summarize existing mathematical formulations of both in-domain and out-of-domain generalizations. We denote D as an underlying distribution (domain) and S as a finite set of n samples from D. We use F as a shorthand of the hypothesis class and f as a hypothesis (model). We focus on the classification task throughout, but generalization bounds for other tasks (such as regression) are also possible (Mohri et al., 2018). In classification, a sample (x, y) \u223c D is composed of input x (e.g. an image) and output y (a label)."
        },
        {
            "heading": "7.1 In Domain",
            "text": "To evaluate the performances, we need to define the evaluation metric. In classification, the default choice is classification error, i.e., the percentage of wrong predictions. Given a model h, the error on domain D and on the sample set S are computed as the following:\nD(f ) = ED[1{f (x) 6= y}], S(f ) = ES[1 {f (x) 6= y}], (12)\nwhere 1(\u00b7) is an indicator function. The goal of the in-domain generalization is to provide the following bound\nD(f ) \u2264 S(f ) + g(n,F), (13)\n\u00a9Huawei Technologies Mathematical Challenges in Deep Learning 19\nand the function g(n,F) represents the generalization gap between test and training errors. Ideally, we want g(n,F)\u2192 0 as n \u2192\u221e. The dependence on F is usually characterized by the model capacity, i.e., how expressive our model class is. In many scenarios, the number of samples we can collect is limited (e.g. in healthcare). On the other hand, modern models contain millions or even billions of parameters (e.g. Transformers, Vaswani et al. 2017). Therefore, the exact form of g(n,F) will guide us towards i) finding how many samples are necessary and this is important since labelling is costly in practice; ii) finding the right model architecture. Even though Transformers could contain billions of parameters, in many cases, they do not suffer from overfitting. This requires a better understanding of the function g(n,F), which could help us design better model architectures or even conduct model compression.\nUnfortunately, existing bounds of type equation 13 are often vacuous for neural network\nmodels.\nFor binary classification, the earliest model capacity measure is called Vapnik\u2013Chervonekis (VC) dimension (Vapnik and Chervonenkis, 1971; Valiant, 1984; Blumer et al., 1989). With VC dimension, we can obtain the uniform convergence bound (e.g. Shalev-Shwartz and BenDavid, 2014, Corollary 3.9 and Theorem 3.17):\nPr  D(h) \u2264 S(h) + \u221a 2 log \u2211dvc i=0 ( n i ) n + \u221a log(1/\u03b4) 2n  \u2265 1\u2212 \u03b4, (14) where dvc is the VC dimension. This theorem tells us given the training error, the VC dimension, and the number of i.i.d. samples, we can provide an upper bound for the test error. For ReLU networks, a nearly tight VC dimension bound has been given in (Bartlett et al., 2019).\nFor example, suppose our dataset is MNIST (LeCun et al., 1998), and there are m = 50, 000 samples. Using Theorem 7 from Bartlett et al. (2019), one can obtain that the VC dimension is around 4.4 \u00d7 106 for a two-hidden layer MLP where each hidden layer has 256 neurons. Plugging it back into equation 14 we obtain:\nD(f ) \u2264 S(f ) + 1.185. (15)\nSince both S(f ) and D(f ) are between 0 and 1, this bound does not provide us with a vacuous guarantee. This problem is even worse for large models with billions of parameters.\n20 Mathematical Challenges in Deep Learning Montreal Research Centre\nSuppose we have n i.i.d. samples from a distribution D and a machine learning model f from class F that can achieve good performance on the training set. Can we provide theoretical guarantees for the test performance of f on D that could guide model selection and data collection?"
        },
        {
            "heading": "7.2 Complexity",
            "text": "An alternative model capacity measure is the Rademacher complexity (Koltchinskii, 2001). Similar to equation 14, the Rademacher complexity bound (e.g. Mohri et al., 2018, Theorem 3.5) can be written as\nPr { D(f ) \u2264 S(f ) + Rn(F) + \u221a log(1/\u03b4)\n2n\n} \u2265 1\u2212 \u03b4. (16)\nThe term Rn(F) is called the Rademacher complexity. Intuitively, it is the capability of the function class F to fit random fair coins. Deriving tight Rademacher complexity is also a hot research topic in recent years. For example, Neyshabur et al. (2018) proposes a Rademacher complexity bound for two-layer ReLU networks that can partially explain the effect of overparametrization. For deep neural networks, a tight generalization bound is yet to be found.\nOther than the VC dimension and Rademacher complexity, there are other capacity measures that could potential explain generalization in deep learning, such as covering number (e.g. Shalev-Shwartz and Ben-David, 2014; Zhu et al., 2021), PAC-Bayes bounds (McAllester, 1998; Lotfi et al., 2022), compression schemes (Littlestone, 1986; Ashtiani et al., 2018), and information theoretical bounds (Haghifam et al., 2021). These generalization bounds have been applied to deep learning to partially explain the role of data augmentation, model size, model compression, etc.\nThe non-vacuous generalization bound can provide theoretical support for us to understand an important generalization phenomenon: double descent (Belkin et al., 2019; Nakkiran et al., 2021): as illustrated in Figure 5, as the model size increases, the performance of machine learning models first improves, then gets worse, and then improves again.\nThe modern double descent regime requires a new learning theory beyond the classical statistical one, e.g., VC dimension and Rademacher complexity mentioned above. We expect\n\u00a9Huawei Technologies Mathematical Challenges in Deep Learning 21\nthat a better approximated g(n,F) could also help to explain the double descent phenomenon. This phenomenon is fairly universal that happens in CNNs, ResNets, transformers and even linear models (Hastie et al., 2022) as well as decision trees (Wyner et al., 2017), and occurs in a wide variety of different tasks, including image classification and language translation. Therefore, solving the aforementioned mathematical challenge to provide the non-vacuous generalization guarantee is potentially beneficial for model and data size selection in real-world scenarios. In addition, this generalization phenomenon is also closely linked with optimization techniques we apply, such as SGD (Keskar et al., 2016; Dinh et al., 2017). Involving the optimization analysis will be more mathematically challenging."
        },
        {
            "heading": "7.3 Out of Domain",
            "text": "Compared to in-domain generalization, a more challenging task is out-of-domain generalization. We have the distribution shift problem in this generalization, meaning that the training and test sets are drawn from different distributions. Let us assume that the data generating distribution is partitioned into D = S \u222aT training set is from a source domain S and the test set is from a target domain T . We also assume that each domain can be properly estimated to disentangle the out of domain generalization from the in-domain one.\n22 Mathematical Challenges in Deep Learning Montreal Research Centre\nIf the target domain is not related to the source domain, then there is no hope that we can learn a model that performs well on the target. Therefore, there has to be some connection between the two domains. There are two popular types of domain shift under research:\n\u2022 Covariate Shift: the input distributions are different, i.e., pS(x) 6= pT (x) for some x,\nbut the conditional distributions are the same, i.e., pS(y |x) = pT (y |x);\n\u2022 Lable Shift: the label distributions are different, but the input distributions for each\nclass are the same, i.e., pS(y) 6= pT (y) for some y but pS(x|y) = pT (x|y) for all (x, y).\nFirst, we discuss the covariate shift case. The first generalization bound under the assumption of covariate shift is from Ben-David et al. (2006). It gives an upper bound for the target (test) error based on the source (training) error:\nT (f ) \u2264 S(f ) + \u2206F {pS(x), pT (x)}+ \u03bb\u2217, for any f \u2208 F . (17)\nThe second term \u2206F measures the difference between the two input distributions {pS(x), pT (x)}, and \u03bb\u2217 = arg minf \u2208F { S(f ) + T (f )} denotes the optimal joint error of the source and target domains. The term \u2206F {pS(x), pT (x)}+ \u03bb\u2217 is the generalization gap, as it is an upper bound of the gap between source and target domains. The generalization gap is small if i) the input distributions of S and T are close to each other; ii) the optimal joint error is small.\nTo achieve a small generalization gap, people use deep neural networks to embed the input distributions (Ganin et al., 2016; Zhang et al., 2019; Acuna et al., 2021). These embeddings are called deep features, and this method is called feature matching. Suppose g is the aforementioned neural network, feature matching requires pS{g(x)} = pT {g(x)} for any x \u2208 X . Under the covariance shift assumption pS{y |g(x)} = pT {y |g(x)}, and thus the out of domain generalization vanishes while using g(x) instead of x and for instance the Bayesian optimal classifier on both domains coincide.\nSecond, we discuss the label shift scenario. Switching the roles of x and y in covariate shift, we obtain the label shift assumption. Under this assumption, Zhao et al. (2019) argues that the optimal joint error in equation 17 can be lower bounded. Therefore, separate generalization bound under label shift is needed. Tachet des Combes et al. (2020) proposes a generalization bound based on the label shift \u2206{pS(y), pT (y)} and the domain shift of the conditional\n\u00a9Huawei Technologies Mathematical Challenges in Deep Learning 23\ndistribution pD(y\u0302 |y), where y\u0302 is the prediction. In order to minimize such generalization bound, Tachet des Combes et al. (2020) aims to enforce the embedding-type approach and look for invariance of the class-conditional distributions pD{g(x)|y}. However, this will induce computational inefficiency when there are many classes.\nIn many cases, the aforementioned generalization bounds are difficult to verify in practice and similar to in-domain generalization, such bounds are often vacuous. We wonder if one can train a machine learning model on one or more source domains so that this model provably performs well on new target domains. We call to define all such domains formally with easily verifiable criteria."
        },
        {
            "heading": "8 Challenge",
            "text": "In this section we aim at re-stating the challenges we explained in the text more concisely.\nLowbit model: Given F is learnable, we wonder if a lower bit projection P\u2126(F) where P is the pro-\njection function and \u2126 \u2282 R is the space of the lower bit fixedpoint or floating point representation of f \u2208 F .\nConstraint: Given F is learnable, we wonder a constrained version F \u2229C is also learnable. The con-\nstraints may reflect memory (hardware independent), or latency (hardware dependent).\nQuantization: Quantizing large language models allows to run on the lower resource cloud and edge.\nWe wonder how one can optimize equation 4 more effectively. For instance instead of optimizing \u2016\u03c3(XlWl)\u2212\u03c3(XlW\u0302l)\u2016 over layers l as in equation 5 we may look for a model subgraph G and alternate between optimizing minW|G and minG|W.\nLowbit SGD: As stated in equation 6 weight update in each iteration k involves two kinds of errors,\nthe error in computing gradient e1k , and the error in computing the update e2k . We wonder the gradient gk needs to be redefined using a computationally more meaningful way such as gk( ) \u2248 L(wk)\u2212L(wk+ ) for a computationally meaningful .\nFusion: We wonder if the train large then compress (Li et al., 2020) can be regarded as an\nadaptive method to fuse these two steps. In other words, the number of model parameters need to be updated during training to combine training and compression into a\n24 Mathematical Challenges in Deep Learning Montreal Research Centre\nsingle framework. Start with a large w0 and in each SGD update wk not only updates the weight values but also updates dim(wk), for instance dim(wk) = 1(wk\u22121 > ).\nMeta Size: Dropout provides training many sparse models. On the other hand each data carries\na weight about its true dimension through equation 7 which is difficult to compute. Averaging over the sparse models from dropout instead of data can estimate the required dimension during training. Suppose each iteration consist of a dropout with dk activated\nneurons dk = 1(wk 6= 0), and d\u0302 \u2248 1K \u2211 k g(dk).\nMeta SGD: We propose to explore a meta SGD where the weights and model complexity are updated\nsimultaneously until matching the model dimension with the data dimension to combine training and compression in a single framework. In each iteration i) update weights wk = g1(wk\u22121) ii) update model dimension dim(wk) = g2(wk\u22121), iii) estimate data dimension d\u0302k = g3(fwk ), take a gradient step to on the dimension space to bring them closer as the function of the other two dimensions g4(d\u0302k , dim{wk)}.\nMeta update: We wonder how to find an optimal update strategy in equation 11 which is explicit\nenough to be implemented for a wide class of models, and at the same time general enough to be used for a large class of models. As a special case wk+1 = h(w1:k ,g1:k , k,F) can be refined to choosing the proper scheduling. Different depth of ResNets and different depth of Transformers are scheduled differently, so inherently \u03b7k is \u03b7k(f ).\nComplexity: Define the complexity measure g(n,F) and assumptions on the hypothesis class F\nbeyond the classical theory to obtain tight bounds for in-domain and out-of-domain generalization bounds in equation 14, and equation 16 respectively that ideally satisfies Pr{ D(f ) \u2264 S(f ) + g(n,F)} \u2265 1\u2212 \u03b4 while satisfying g(n,F) \u2264 1 and g(n,F)\u2192 0.\nTransfer: We wonder if transferring learning from the source distribution pS to the target pT\ndistribution needs to be re-formalized so that they cover pre-training (source) and finetuning (target) while conditions are i) formally meaningful ii) practically verifiable iii) exhibit tight bounds for out-of-domain generalization in equation 16.\n\u00a9Huawei Technologies Mathematical Challenges in Deep Learning 25"
        }
    ],
    "title": "Mathematical Challenges in Deep Learning",
    "year": 2023
}