{
    "abstractText": "Diseases such as diabetic retinopathy and agerelated macular degeneration pose a significant risk to vision, highlighting the importance of precise segmentation of retinal vessels for the tracking and diagnosis of progression. However, existing vessel segmentation methods that heavily rely on encoderdecoder structures struggle to capture contextual information about retinal vessel configurations, leading to challenges in reconciling semantic disparities between encoder and decoder features. To address this, we propose a novel feature enhancement segmentation network (FES-Net) that achieves accurate pixel-wise segmentation without requiring additional image enhancement steps. FES-Net directly processes the input image and utilizes four prompt convolutional blocks (PCBs) during downsampling, complemented by a shallow upsampling approach to generate a binary mask for each class. We evaluate the performance of FES-Net on four publicly available state-of-theart datasets: DRIVE, STARE, CHASE, and HRF. The evaluation results clearly demonstrate the superior performance of FES-Net compared to other competitive approaches documented in the existing literature.",
    "authors": [
        {
            "affiliations": [],
            "name": "Tariq M. Khan"
        },
        {
            "affiliations": [],
            "name": "Imran Razzak"
        },
        {
            "affiliations": [],
            "name": "Erik Meijering"
        }
    ],
    "id": "SP:091afb7a8f5ff1adae574f849c3f962bf908e269",
    "references": [
        {
            "authors": [
                "M.M. Fraz",
                "P. Remagnino",
                "A. Hoppe",
                "B. Uyyanonvara",
                "A.R. Rudnicka",
                "C.G. Owen",
                "S.A. Barman"
            ],
            "title": "An ensemble classification-based approach applied to retinal blood vessel segmentation",
            "venue": "IEEE Transactions on Biomedical Engineering, vol. 59, no. 9, pp. 2538\u20132548, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "R. Imtiaz",
                "T.M. Khan",
                "S.S. Naqvi",
                "M. Arsalan",
                "S.J. Nawaz"
            ],
            "title": "Screening of glaucoma disease from retinal vessel images using semantic segmentation",
            "venue": "Computers & Electrical Engineering, vol. 91, p. 107036, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "R. Crosby-Nwaobi",
                "L.Z. Heng",
                "S. Sivaprasad"
            ],
            "title": "Retinal vascular calibre, geometry and progression of diabetic retinopathy in type 2 diabetes mellitus",
            "venue": "Ophthalmologica, vol. 228, no. 2, pp. 84\u201392, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "M.S. Habib",
                "B. Al-Diri",
                "A. Hunter",
                "D.H. Steel"
            ],
            "title": "The association between retinal vascular geometry changes and diabetic retinopathy and their role in prediction of progression\u2013an exploratory study",
            "venue": "BMC Ophthalmology, vol. 14, no. 1, pp. 1\u201311, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "S.S. Naqvi",
                "N. Fatima",
                "T.M. Khan",
                "Z.U. Rehman",
                "M.A. Khan"
            ],
            "title": "Automatic optic disk detection and segmentation by variational active contour estimation in retinal fundus images",
            "venue": "Signal, Image and Video Processing, vol. 13, pp. 1191\u20131198, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "K. Naveed",
                "F. Abdullah",
                "H.A. Madni",
                "M.A. Khan",
                "T.M. Khan",
                "S.S. Naqvi"
            ],
            "title": "Towards automated eye diagnosis: An improved retinal vessel segmentation framework using ensemble block matching 3d filter",
            "venue": "Diagnostics, vol. 11, no. 1, p. 114, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S. Iqbal",
                "T.M. Khan",
                "K. Naveed",
                "S.S. Naqvi",
                "S.J. Nawaz"
            ],
            "title": "Recent trends and advances in fundus image analysis: A review",
            "venue": "Computers in Biology and Medicine, vol. 151, p. 106277, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Zhao",
                "J. Xie",
                "P. Su",
                "Y. Zheng",
                "Y. Liu",
                "J. Cheng",
                "J. Liu"
            ],
            "title": "Retinal artery and vein classification via dominant sets clusteringbased vascular topology estimation",
            "venue": "Medical Image Computing and Computer Assisted Intervention (MICCAI), 2018, pp. 56\u201364.",
            "year": 2018
        },
        {
            "authors": [
                "T.M. Khan",
                "S.S. Naqvi",
                "A. Robles-Kelly",
                "I. Razzak"
            ],
            "title": "Retinal vessel segmentation via a multi-resolution contextual network and adversarial learning",
            "venue": "Neural Networks, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "S. Iqbal",
                "K. Naveed",
                "S.S. Naqvi",
                "A. Naveed",
                "T.M. Khan"
            ],
            "title": "Robust retinal blood vessel segmentation using a patch-based statistical adaptive multi-scale line detector",
            "venue": "Digital Signal Processing, p. 104075, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "S. Iqbal",
                "T.M. Khan",
                "M. Alhussein",
                "S.S. Naqvi",
                "M. Usman",
                "K. Aurangzeb"
            ],
            "title": "LDMRes-Net: Enabling real-time disease monitoring through efficient image segmentation",
            "venue": "arXiv:2306.06145, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "T.A. Soomro",
                "T.M. Khan",
                "M.A.U. Khan",
                "J. Gao",
                "M. Paul",
                "L. Zheng"
            ],
            "title": "Impact of ICA-based image enhancement technique on retinal blood vessels segmentation",
            "venue": "IEEE Access, vol. 6, pp. 3524\u20133538, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "A. Khawaja",
                "T.M. Khan",
                "K. Naveed",
                "S.S. Naqvi",
                "N.U. Rehman",
                "S. Junaid Nawaz"
            ],
            "title": "An improved retinal vessel segmentation framework using Frangi filter coupled with the probabilistic patch based denoiser",
            "venue": "IEEE Access, vol. 7, pp. 164 344\u2013164 361, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "K.B. Khan",
                "A.A. Khaliq",
                "A. Jalil",
                "M. Shahid"
            ],
            "title": "A robust technique based on VLM and Frangi filter for retinal vessel extraction and denoising",
            "venue": "PLoS One, vol. 13, no. 2, p. e0192203, 2018.",
            "year": 1922
        },
        {
            "authors": [
                "T.A. Soomro",
                "A.J. Afifi",
                "J. Gao",
                "O. Hellwich",
                "L. Zheng",
                "M. Paul"
            ],
            "title": "Strided fully convolutional neural network for boosting the sensitivity of retinal blood vessels segmentation",
            "venue": "Expert Systems with Applications, vol. 134, pp. 36\u201352, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "T.M. Khan",
                "M. Mehmood",
                "S.S. Naqvi",
                "M.F.U. Butt"
            ],
            "title": "A region growing and local adaptive thresholding-based optic disc detection",
            "venue": "Plos one, vol. 15, no. 1, p. e0227566, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "T.M. Khan",
                "A. Robles-Kelly",
                "S.S. Naqvi"
            ],
            "title": "T-Net: A resourceconstrained tiny convolutional neural network for medical image segmentation",
            "venue": "IEEE/CVF Winter Conference on Application of Computer Vision (WACV), 2022, pp. 644\u2013653.",
            "year": 2022
        },
        {
            "authors": [
                "O. Ronneberger",
                "P. Fischer",
                "T. Brox"
            ],
            "title": "U-Net: Convolutional networks for biomedical image segmentation",
            "venue": "Medical Image Computing and Computer-Assisted Intervention (MICCAI), 2015, pp. 234\u2013241.",
            "year": 2015
        },
        {
            "authors": [
                "Z. Gu",
                "J. Cheng",
                "H. Fu",
                "K. Zhou",
                "H. Hao",
                "Y. Zhao",
                "T. Zhang",
                "S. Gao",
                "J. Liu"
            ],
            "title": "CE-Net: Context encoder network for 2D medical image segmentation",
            "venue": "IEEE Transactions on Medical Imaging, vol. 38, pp. 2281\u20132292, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Z. Yan",
                "X. Yang",
                "K.-T. Cheng"
            ],
            "title": "Joint segment-level and pixelwise losses for deep learning based retinal vessel segmentation",
            "venue": "IEEE Transactions on Biomedical Engineering, vol. 65, no. 9, pp. 1912\u20131923, 2018.",
            "year": 1912
        },
        {
            "authors": [
                "B. Wang",
                "S. Qiu",
                "H. He"
            ],
            "title": "Dual encoding U-Net for retinal vesselsegmentation",
            "venue": "Medical Image Computing and Computer Assisted Intervention (MICCAI), 2019, pp. 84\u201392.",
            "year": 2019
        },
        {
            "authors": [
                "H. Fu",
                "Y. Xu",
                "S. Lin",
                "D.W. Kee Wong",
                "J. Liu"
            ],
            "title": "DeepVessel: Retinal vessel segmentation via deep learning and conditional random field",
            "venue": "Medical Image Computing and Computer-Assisted Intervention (MICCAI), 2016, pp. 132\u2013139.",
            "year": 2016
        },
        {
            "authors": [
                "K.C.L. Wong",
                "M. Moradi"
            ],
            "title": "SegNAS3D: Network architecture search with derivative-free global optimization for 3D image segmentation",
            "venue": "Medical Image Computing and Computer-Assisted Intervention (MICCAI), 2019, pp. 393\u2013401.",
            "year": 2019
        },
        {
            "authors": [
                "G. Lin",
                "A. Milan",
                "C. Shen",
                "I. Reid"
            ],
            "title": "RefineNet: Multi-path refinement networks for high-resolution semantic segmentation",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 1925\u20131934.",
            "year": 2017
        },
        {
            "authors": [
                "V. Nekrasov",
                "C. Shen",
                "I. Reid"
            ],
            "title": "Light-weight RefineNet for real-time semantic segmentation",
            "venue": "arXiv:1810.03272, 2018.",
            "year": 1810
        },
        {
            "authors": [
                "H. Zhao",
                "X. Qi",
                "X. Shen",
                "J. Shi",
                "J. Jia"
            ],
            "title": "ICNet for real-time semantic segmentation on high-resolution images",
            "venue": "European Conference on Computer Vision (ECCV), 2018, pp. 405\u2013420.",
            "year": 2018
        },
        {
            "authors": [
                "C. Yu",
                "C. Gao",
                "J. Wang",
                "G. Yu",
                "C. Shen",
                "N. Sang"
            ],
            "title": "BiSeNet V2: Bilateral network with guided aggregation for real-time semantic segmentation",
            "venue": "International Journal of Computer Vision, vol. 129, no. 11, pp. 3051\u20133068, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J. Yuan",
                "W. Zhou",
                "T. Luo"
            ],
            "title": "DMFNet: Deep multi-modal fusion network for RGB-D indoor scene segmentation",
            "venue": "IEEE Access, vol. 7, pp. 169 350\u2013169 358, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "F. Chollet"
            ],
            "title": "Xception: Deep learning with depthwise separable convolutions",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 1251\u20131258.",
            "year": 2017
        },
        {
            "authors": [
                "A.G. Howard",
                "M. Zhu",
                "B. Chen",
                "D. Kalenichenko",
                "W. Wang",
                "T. Weyand",
                "M. Andreetto",
                "H. Adam"
            ],
            "title": "MobileNets: Efficient convolutional neural networks for mobile vision applications",
            "venue": "arXiv:1704.04861, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "Z. Zhang",
                "C. Wu",
                "S. Coleman",
                "D. Kerr"
            ],
            "title": "DENSE-INception U-net for medical image segmentation",
            "venue": "Computer Methods and Programs in Biomedicine, vol. 192, p. 105395, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "X. Zhang",
                "X. Zhou",
                "M. Lin",
                "J. Sun"
            ],
            "title": "ShuffleNet: An extremely efficient convolutional neural network for mobile devices",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 6848\u20136856.",
            "year": 2018
        },
        {
            "authors": [
                "N. Ma",
                "X. Zhang",
                "H.-T. Zheng",
                "J. Sun"
            ],
            "title": "ShuffleNet V2: Practical guidelines for efficient CNN architecture design",
            "venue": "European Conference on Computer Vision (ECCV), 2018, pp. 116\u2013131.",
            "year": 2018
        },
        {
            "authors": [
                "S. Iqbal",
                "S.S. Naqvi",
                "H.A. Khan",
                "A. Saadat",
                "T.M. Khan"
            ],
            "title": "G-Net Light: A lightweight modified Google Net for retinal vessel segmentation",
            "venue": "Photonics, vol. 9, no. 12, p. 923, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "F. Isensee",
                "P.F. Jaeger",
                "S.A. Kohl",
                "J. Petersen",
                "K.H. Maier-Hein"
            ],
            "title": "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation",
            "venue": "Nature Methods, vol. 18, no. 2, pp. 203\u2013211, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "T. Lei",
                "W. Zhou",
                "Y. Zhang",
                "R. Wang",
                "H. Meng",
                "A.K. Nandi"
            ],
            "title": "Lightweight V-Net for liver segmentation",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020, pp. 1379\u20131383.",
            "year": 2020
        },
        {
            "authors": [
                "T. Tarasiewicz",
                "M. Kawulok",
                "J. Nalepa"
            ],
            "title": "Lightweight U-Nets for brain tumor segmentation",
            "venue": "International MICCAI Brain Lesion Workshop, 2020, pp. 3\u201314.",
            "year": 2020
        },
        {
            "authors": [
                "C. Li",
                "Y. Fan",
                "X. Cai"
            ],
            "title": "PyConvU-Net: A lightweight and multiscale network for biomedical image segmentation",
            "venue": "BMC Bioinformatics, vol. 22, no. 1, pp. 1\u201311, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M. Arsalan",
                "T.M. Khan",
                "S.S. Naqvi",
                "M. Nawaz",
                "I. Razzak"
            ],
            "title": "Prompt deep light-weight vessel segmentation network (PLVS-Net)",
            "venue": "IEEE/ACM Transactions on Computational Biology and Bioinformatics, vol. 20, no. 2, pp. 1363\u20131371, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "T.M. Khan",
                "M. Arsalan",
                "A. Robles-Kelly",
                "E. Meijering"
            ],
            "title": "MKIS-Net: A light-weight multi-kernel network for medical image segmentation",
            "venue": "arXiv:2210.08168, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "V. Badrinarayanan",
                "A. Kendall",
                "R. Cipolla"
            ],
            "title": "SegNet: A deep convolutional encoder-decoder architecture for image segmentation",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 39, no. 12, pp. 2481\u20132495, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "L.-C. Chen",
                "Y. Zhu",
                "G. Papandreou",
                "F. Schroff",
                "H. Adam"
            ],
            "title": "Encoderdecoder with atrous separable convolution for semantic image segmentation",
            "venue": "Lecture Notes in Computer Science, p. 833\u2013851, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "S. Xie",
                "Z. Tu"
            ],
            "title": "Holistically-nested edge detection",
            "venue": "IEEE International Conference on Computer Vision (ICCV), 2015, pp. 1395\u20131403.",
            "year": 2015
        },
        {
            "authors": [
                "K.-K. Maninis",
                "J. Pont-Tuset",
                "P. Arbel\u00e1ez",
                "L.V. Gool"
            ],
            "title": "Deep retinal image understanding",
            "venue": "Medical Image Computing and Computer- Assisted Intervention (MICCAI), 2016, pp. 140\u2013148.",
            "year": 2016
        },
        {
            "authors": [
                "J.I. Orlando",
                "E. Prokofyeva",
                "M.B. Blaschko"
            ],
            "title": "A discriminatively trained fully connected conditional random field model for blood vessel segmentation in fundus images",
            "venue": "IEEE Transactions on Biomedical Engineering, vol. 64, no. 1, pp. 16\u201327, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "O. Oktay",
                "J. Schlemper",
                "L.L. Folgoc",
                "M. Lee",
                "M. Heinrich",
                "K. Misawa",
                "K. Mori",
                "S. McDonagh",
                "N.Y. Hammerla",
                "B. Kainz",
                "B. Glocker",
                "D. Rueckert"
            ],
            "title": "Attention U-Net: Learning where to look for the pancreas",
            "venue": "arXiv:1804.03999, 2018.",
            "year": 1804
        },
        {
            "authors": [
                "X. Li",
                "H. Chen",
                "X. Qi",
                "Q. Dou",
                "C.-W. Fu",
                "P.-A. Heng"
            ],
            "title": "H- DenseUNet: Hybrid densely connected UNet for liver and tumor segmentation from CT volumes",
            "venue": "IEEE Transactions on Medical Imaging, vol. 37, no. 12, pp. 2663\u20132674, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "Z. Yan",
                "X. Yang",
                "K.-T. Cheng"
            ],
            "title": "A three-stage deep learning model for accurate retinal vessel segmentation",
            "venue": "IEEE Journal of Biomedical and Health Informatics, vol. 23, no. 4, pp. 1427\u20131436, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "S. Guo",
                "K. Wang",
                "H. Kang",
                "Y. Zhang",
                "Y. Gao",
                "T. Li"
            ],
            "title": "BTS-DSN: Deeply supervised neural network with short connections for retinal vessel segmentation",
            "venue": "International Journal of Medical Informatics, vol. 126, pp. 105\u2013113, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Q. Jin",
                "Z. Meng",
                "T.D. Pham",
                "Q. Chen",
                "L. Wei",
                "R. Su"
            ],
            "title": "DUNet: A deformable network for retinal vessel segmentation",
            "venue": "Knowledge-Based Systems, vol. 178, pp. 149\u2013162, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "T. Laibacher",
                "T. Weyde",
                "S. Jalali"
            ],
            "title": "M2U-Net: Effective and efficient retinal vessel segmentation for resource-constrained environments",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, 2019, pp. 1\u201310.",
            "year": 2019
        },
        {
            "authors": [
                "T. Xiang",
                "C. Zhang",
                "D. Liu",
                "Y. Song",
                "H. Huang",
                "W. Cai"
            ],
            "title": "BiO- Net: Learning recurrent bi-directional connections for encoder-decoder architecture",
            "venue": "Medical Image Computing and Computer-Assisted Intervention (MICCAI), 2020, pp. 74\u201384.",
            "year": 2020
        },
        {
            "authors": [
                "S. Feng",
                "Z. Zhuo",
                "D. Pan",
                "Q. Tian"
            ],
            "title": "CcNet: A cross-connected convolutional network for segmenting retinal vessels using multi-scale features",
            "venue": "Neurocomputing, vol. 392, pp. 268\u2013276, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "K. Wang",
                "X. Zhang",
                "S. Huang",
                "Q. Wang",
                "F. Chen"
            ],
            "title": "CTF-Net: Retinal vessel segmentation via deep coarse-to-fine supervision network",
            "venue": "IEEE International Symposium on Biomedical Imaging (ISBI), 2020, pp. 1237\u20131241.",
            "year": 2020
        },
        {
            "authors": [
                "B. Wang",
                "S. Wang",
                "S. Qiu",
                "W. Wei",
                "H. Wang",
                "H. He"
            ],
            "title": "CSU-Net: A context spatial U-Net for accurate blood vessel segmentation in fundus images",
            "venue": "IEEE Journal of Biomedical and Health Informatics, vol. 25, no. 4, pp. 1128\u20131138, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "X. Wei",
                "K. Yang",
                "D. Bzdok",
                "Y. Li"
            ],
            "title": "Orientation and context entangled network for retinal vessel segmentation",
            "venue": "arXiv:2207.11396, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Liu",
                "J. Shen",
                "L. Yang",
                "H. Yu",
                "G. Bian"
            ],
            "title": "Wave-Net: A lightweight deep network for retinal vessel segmentation from fundus images",
            "venue": "Computers in Biology and Medicine, vol. 152, p. 106341, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "S. Guo"
            ],
            "title": "LightEyes: A lightweight fundus segmentation network for mobile edge computing",
            "venue": "Sensors, vol. 22, no. 9, p. 3112, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "L. Mou",
                "Y. Zhao",
                "L. Chen",
                "J. Cheng",
                "Z. Gu",
                "H. Hao",
                "H. Qi",
                "Y. Zheng",
                "A. Frangi",
                "J. Liu"
            ],
            "title": "CS-Net: Channel and spatial attention network for curvilinear structure segmentation",
            "venue": "Medical Image Computing and Computer-Assisted Intervention (MICCAI), 2019, pp. 721\u2013730.",
            "year": 2019
        },
        {
            "authors": [
                "C. Guo",
                "M. Szemenyei",
                "Y. Yi",
                "W. Wang",
                "B. Chen",
                "C. Fan"
            ],
            "title": "SA-UNet: Spatial attention U-Net for retinal vessel segmentation",
            "venue": "International Conference on Pattern Recognition (ICPR), 2021, pp. 1236\u20131242.",
            "year": 2021
        },
        {
            "authors": [
                "H. Wu",
                "W. Wang",
                "J. Zhong",
                "B. Lei",
                "Z. Wen",
                "J. Qin"
            ],
            "title": "SCS-Net: A scale and context sensitive network for retinal vessel segmentation",
            "venue": "Medical Image Analysis, vol. 70, p. 102025, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Q. Zhang",
                "G. Yang",
                "G. Zhang"
            ],
            "title": "Collaborative network for superresolution and semantic segmentation of remote sensing images",
            "venue": "IEEE Transactions on Geoscience and Remote Sensing, vol. 60, pp. 1\u201312, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Hu",
                "Z. Qiu",
                "D. Zeng",
                "L. Jiang",
                "C. Lin",
                "J. Liu"
            ],
            "title": "SuperVessel: Segmenting high-resolution vessel from low-resolution retinal image",
            "venue": "arXiv:2207.13882, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J. Staal",
                "M.D. Abr\u00e0moff",
                "M. Niemeijer",
                "M.A. Viergever",
                "B. Van Ginneken"
            ],
            "title": "Ridge-based vessel segmentation in color images of the retina",
            "venue": "IEEE Transactions Medical Imaging, vol. 23, no. 4, pp. 501\u2013509, 2004.",
            "year": 2004
        },
        {
            "authors": [
                "A. Hoover",
                "V. Kouznetsova",
                "M. Goldbaum"
            ],
            "title": "Locating blood vessels in retinal images by piecewise threshold probing of a matched filter response",
            "venue": "IEEE Transactions Medical Imaging, vol. 19, no. 3, pp. 203\u2013 210, 2000.",
            "year": 2000
        },
        {
            "authors": [
                "J. Odstrcilik",
                "R. Kolar",
                "A. Budai",
                "J. Hornegger",
                "J. Jan",
                "J. Gazarek",
                "T. Kubena",
                "P. Cernosek",
                "O. Svoboda",
                "E. Angelopoulou"
            ],
            "title": "Retinal vessel segmentation by improved matched filtering: evaluation on a new high-resolution fundus image database",
            "venue": "IET Image Processing, vol. 7, no. 4, pp. 373\u2013383, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "R. Azad",
                "M. Asadi-Aghbolaghi",
                "M. Fathy",
                "S. Escalera"
            ],
            "title": "Bidirectional ConvLSTM U-net with densley connected convolutions",
            "venue": "IEEE International Conference on Computer Vision Workshops, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "N. Ibtehaz",
                "M.S. Rahman"
            ],
            "title": "MultiResUNet: Rethinking the U-Net architecture for multimodal biomedical image segmentation",
            "venue": "Neural Networks, vol. 121, pp. 74\u201387, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "T.M. Khan",
                "M. Alhussein",
                "K. Aurangzeb",
                "M. Arsalan",
                "S.S. Naqvi",
                "S.J. Nawaz"
            ],
            "title": "Residual connection-based encoder decoder network (RCED-Net) for retinal vessel segmentation",
            "venue": "IEEE Access, vol. 8, pp. 131 257\u2013131 272, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Z. Zhou",
                "M.M. Rahman Siddiquee",
                "N. Tajbakhsh",
                "J. Liang"
            ],
            "title": "UNet++: A nested U-Net architecture for medical image segmentation",
            "venue": "Deep Learning in Medical Image Analysis & Multimodal Learning for Clinical Decision Support, 2018, pp. 3\u201311.",
            "year": 2018
        },
        {
            "authors": [
                "A. Howard",
                "M. Sandler",
                "B. Chen",
                "W. Wang",
                "L.-C. Chen",
                "M. Tan",
                "G. Chu",
                "V. Vasudevan",
                "Y. Zhu",
                "R. Pang",
                "H. Adam",
                "Q. Le"
            ],
            "title": "Searching for MobileNetV3",
            "venue": "IEEE/CVF International Conference on Computer Vision (ICCV), 2019, pp. 1314\u20131324.",
            "year": 2019
        },
        {
            "authors": [
                "E. Romera",
                "J.M. \u00c1lvarez",
                "L.M. Bergasa",
                "R. Arroyo"
            ],
            "title": "ERFNet: Efficient residual factorized convnet for real-time semantic segmentation",
            "venue": "IEEE Transactions on Intelligent Transportation Systems, vol. 19, no. 1, pp. 263\u2013272, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "M. Arsalan",
                "M. Oqais",
                "tahir Mahmood",
                "S.W. Cho",
                "K.R. Park"
            ],
            "title": "Aiding the diagnosis of diabetic and hypertensive retinopathy using artificial intelligence-based semantic segmentation",
            "venue": "Journal of Clinical Medicine, vol. 8, no. 9, 2019.",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014Medical Image Segmentation, Lightweight Deep Networks, Retinal Blood Vessel Segmentation, Diabetic Retinopathy, Convolutional Neural Networks.\nI. INTRODUCTION\nCOMPUTER-aided wide-scale screening presents a feasi-ble approach to the detection of diseases in people with diabetes mellitus, as it has the potential to increase scarce healthcare resources and support healthcare professionals [1], [2]. Previous research [3]\u2013[5] has underscored the importance of the size and structure of the retinal vessels in the diagnosis and prediction of the prognosis of diabetic retinopathy. The noticeable alterations in vessel dimensions act as robust markers of the severity of the disease and could be used to forecast the potential future progression of the condition, as supported by the conclusions of these studies [3].\nIn computer-aided diagnosis (CAD) systems, segmentation of retinal blood vessels is crucial and time-consuming [6]. This is due to the fact that many retinal abnormalities, such as hemorrhages and microaneurysms, typical manifestations of diabetic retinopathy, are often observed close to these vessels [7]. Furthermore, efficient segmentation of retinal vessels can allow estimation of the topology of vessel maps [8], [9].\nTariq M. Khan, Imran Razzak and Erik Meijering are with the School of Computer Science & Engineering, UNSW, Sydney, Australia (e-mail: {tariq.khan, imran.razzak, erik.meijering}@unsw.edu.au)\nShahzaib Iqbal is with the Department of Electrical and Computer Engineering, COMSATS University Islamabad (CUI), Islamabad, Pakistan\nMuhammad Arsalan is with the Department of Computer Science, Qatar University, Doha, Qatar\nSegmenting retinal blood vessels is a challenging task in retinal image analysis due to the presence of numerous obstacles. These include low contrast, uneven intensity, and varying thickness between primary vessels and capillaries present in images of the retinal fundus. Additionally, the presence of exudates and lesions further complicates the segmentation process. To address these challenges, numerous studies have employed supervised or unsupervised algorithms and computer vision techniques, in order to achieve accurate and automatic segmentation [10]\u2013[14]. The latest research points out that deep learning architectures have better performance compared to other techniques [15]\u2013[17].\nRetinal vessel segmentation has been aided by a number of approaches based on deep learning. U-Net is proposed for medical image segmentation, but it identifies false boundaries in retinal images along with blood vessels [18]. Gu et al. put forth a context encoder network that captures higherorder information while maintaining spatial data [19] for the segmentation of blood vessels. Yan et al. enhanced U-Net\u2019s performance by introducing a joint loss function [20].\nWang et al. introduced DEU-Net, which uses a fusion module function to merge a spatial path with a large kernel, thus preserving spatial data while capturing semantic details [21]. Fu et al. presented DeepVessel, which uses a CNN featuring a multiscale and multilevel layer to construct a dense hierarchical representation [22]. Additionally, they incorporated a conditional random field to model extended interactions among pixels [22]. Notwithstanding the effectiveness of these approaches, they overlook the computational commutation needed to customize the network for use with manageable resources on embedded systems.\nThe proposed FES-Net aims to address some of the difficulties encountered in retinal vessel segmentation by improving the original image and bypassing the need for conventional image processing methods. The network employs four convolutional blocks on the downsampling end and a relatively simpler upsampling end to generate an output binary mask for each category. To keep the overall computational cost low, only a limited number of transposed convolutions are used in the upsampling procedure. The final stage assigns a label to each vessel pixel using the pixel classification layer."
        },
        {
            "heading": "II. RELATED WORK",
            "text": "In recent years, lightweight segmentation networks have gained attention in image segmentation challenges. SegNAS3D [23] introduced a network search to optimize the 3D\nar X\niv :2\n30 9.\n03 53\n5v 1\n[ ee\nss .I\nV ]\n7 S\nep 2\n02 3\nsegmentation structure and significantly reduce the complexity of the model. RefineNet [24] was used as the lightweight backbone network by Nekrasov et al. [25] to improve the performance of the model. IC-Net [26] is a real-time semantic segmentation network that uses an image cascade with branch training to accelerate model convergence. BiSeNet [27] is a lightweight model based on a dual-branch topology, employing multiple paths to refine spatial information. DMFNet [28] partitions channels into numerous groups and employs a weighted three-dimensional expanded convolution. This approach ultimately results in a decrease in the parameter count while simultaneously enhancing the accuracy of inference. Xception [29] and MobileNet [30] use deep separable convolution to improve the inference speed. The Dense-Inception U-Net [31] uses a compact encoder with a lightweight Inception backbone and a dense module to capture high-level semantic information. This architecture is designed for medical image\nsegmentation tasks. ShuffleNet [32], [33] applies group convolution and channel\nshuffling techniques to reduce computational expenses compared to more complex models. Iqbal et al. [34] explored the development of lightweight segmentation networks specifically for medical images. However, creating networks with low model complexity, high inference speed, and excellent performance still poses a challenge in most medical image segmentation tasks. nnU-Net [35] improves network adaptability by preprocessing data and postprocessing segmentation results, but model parameters increase with this approach. A lightweight V-Net [36] uses convolution inverted residual bottleneck block (IRB block). To ensure segmentation accuracy and use fewer parameters, depth-wise convolution and point-wise convolution are employed. However, it does not speed up the inference process. Furthermore, Tarasiewicz et al. [37] developed Lightweight U-Nets that can accurately delineate brain tumors from multimodal magnetic resonance imaging and trained several skinny networks in all image planes. PyConvU-Net [38] increases segmentation accuracy while using fewer parameters by replacing all traditional convolutional layers of U-Net with pyramidal convolution. However, its inference speed is low. G-Net Light [34], PLVSNet [39], and MKIS-Net [40] are effective CNN architectures to segment retinal blood vessels while being lightweight."
        },
        {
            "heading": "III. PROPOSED METHOD",
            "text": "The conventional semantic segmentation networks are deep\nand have many trainable parameters, such as SegNet [41], U-Net [18], and DeepLab [42]. These networks are based on encoder-decoder designs, where the decoder mirrors the encoder. This means that the network has double number of layers. Our proposed FES-Net, on the other hand, utilizes shallow upsampling that helps to reduce the depth of the network, resulting in parameter reduction. This network is based on four\nprompt convolutional blocks (PCBs), which are a combination of general convolutions and separable convolutions to reduce the cost of the network (Figs. 1 and 2).\nEach PCB in FES-Net comprises a 3\u00d73 general convolution, a separable convolution of 3\u00d73, and one 1\u00d71 general convolution (Fig. 2). These convolutions are directly connected to a strided (dilated) convolution and employ batch normalization and rectified linear units (ReLU) for the combination of their outputs. The depth-wise concatenation layer is used to merge the outputs of these three convolutions, and FESNet contains four such PCBs. The main feature of FES-Net is the enhancement of spatial features achieved through the feature enhancement block (FEB). FEB consists of a shallow convolutional block that preserves low-level features, does not significantly downsample them, and provides them at the end of the network. It uses four convolutions with a maximum depth of 16 channels only, which minimizes the network\u2019s parameters. FEB enhances spatial information on the initial layers of the network and provides rich features to the final stage of the network.\nIn the FES-Net architecture with FEB (Fig. 1), the upsampling block receives the feature FD from the downsampling side, and from the shallow upsampling, it outputs the feature FUS , which is common in semantic segmentation networks. Continuous downsampling and multiple convolutional layers result in feature deterioration and FUS cannot provide a better true positive rate. Therefore, FEB takes the characteristic Fi from the initial layers of the network and generates the characteristic FE after the shallow convolutional operation, which contains low-level characteristics. The two characteristics FUS and FE are combined to produce the characteristic SC :\nSC = FUS \u00a9FE (1)\nwhere the symbol \u00a9 denotes depth-wise concatenation of the two features, namely the feature on the upsampling side and the feature on FEB. The resulting feature SC contains rich edge information, resulting in higher sensitivity. Finally, at the end of the network, FES-Net uses the pixel classification layer to assign a predicted label to each pixel of the image."
        },
        {
            "heading": "IV. RESULTS AND DISCUSSION",
            "text": "The efficacy of the proposed FES-Net for segmenting retinal blood vessels was evaluated on publicly available datasets and compared with previously published state-of-the-art methods."
        },
        {
            "heading": "A. Datasets",
            "text": "DRIVE: The DRIVE [64] dataset has 40 color retinal images divided into 20 training images and 20 testing images, with a resolution of 584 \u00d7 565 pixels. It includes patients of different ages diagnosed with diabetes, with seven images representing the early stages of mild diabetic retinopathy. Manual pixel annotations are available for vessel and background segmentation in both training and testing images.\nSTARE: The STARE [65] dataset consists of 20 color fundus images with 35\u25e6 field of view (FOV) and a resolution of 700 \u00d7 605 pixels. The ground truth is established using\ntwo manual segmentation options, and 10 images are used for training, while the remaining 10 are used for testing.\nCHASE: The CHASE [1] dataset consists of 28 color images of 14 British schoolchildren. Each image has a resolution of 999 \u00d7 996 pixels and is centered on the optical disc. Two manual segmentation maps are provided for ground truth. The dataset does not have a specific separation of training and testing sets. For our experiments, we used the initial 20 images for training and the final 8 images for testing.\nHRF: The HRF dataset [66] includes 45 images, consisting of 15 images each from healthy individuals, patients with glaucoma, and patients with diabetic retinopathy. The images have a resolution of 3504\u00d7 2336 pixels and a viewing angle of 60\u25e6. Ground truth segmentation was performed by a team\nof specialists for each image."
        },
        {
            "heading": "B. Experimental Setup",
            "text": "For network training, the ADAM optimizer was used with a starting learning rate of 0.00002 and a decay rate of 0.90. Before starting the training, we resized the images for each dataset to a standard width of 640 pixels and normalized each image based on the z-score statistic. To enrich the dataset, we applied contrast enhancement, brightness adjustment, and random image flipping and rotation (ranging between 1\u2013360\u25e6). These techniques were employed to synthetically expand the number of training images."
        },
        {
            "heading": "C. Evaluation Criteria",
            "text": "Vessel segmentation maps exhibit a distinctive characteristic compared to other types of segmentation maps, as they adopt a binary representation, where each pixel is assigned exclusively to either the vessel or the background class. Accomplished ophthalmologists with specialized expertise manually annotated the \u201cground truth\u201d labels in publicly accessible datasets. These annotations serve as a reference standard against which the performance of generated segmentation results can be assessed. The process involves the classification of individual image pixels into either the vascular or nonvascular category.\nFor each output image, there are four possible outcomes that can occur: true positive (TP ), which represents pixels correctly identified as vessels; true negative (TN ), which represents pixels correctly identified as non-vessels; false positive (FP ), which represents pixels mistakenly identified as vessels; and false negative (FN ), which represents pixels mistakenly identified as non-vessels. To allow comparisons between different methods, five widely used parameters are used in the field: sensitivity (Se), specificity (Sp), accuracy (Acc), area under the curve (AUC), and the F1 score. These measures assess different aspects of the performance of a method and their mathematical formulations are as follows:\nSe = TP\nTP + FN , (2)\nSp = TN\nTN + FP , (3)\nAcc = TP + TN\nTP + TN + FP + FN , (4)\nAUC = 1\u2212 1 2\n( FP\nFP + TN + FN FN + TP\n) , (5)\nF1 = 2\u00d7 TP\n(2\u00d7 TP ) + FP + FN . (6)"
        },
        {
            "heading": "D. Comparison and Experiments",
            "text": "Here we present the results obtained with our proposed network, as well as a range of alternate approaches, on the DRIVE, STARE, CHASE, and HRF datasets. We commence by providing a prior qualitative and quantitative evaluation of a comprehensive spectrum of unsupervised and supervised techniques commonly employed in the field. This evaluation aims to establish a baseline performance and to compare the effectiveness of various existing methods. Subsequently, we perform an exhaustive comparison between U-Net [18] and SegNet [41], which are widely recognized deep learning architectures for semantic segmentation tasks. The objective of this section is to present a self-contained comparison, while also showcasing the performance of our proposed network against these well-established benchmarks.\nThe quantitative results of our proposed FES-Net and other methods (Tables I\u2013IV) provide evidence that the proposed FES-Net consistently outperforms other techniques in terms of Acc. Moreover, the results demonstrate that our proposed network obtains competitive performance in terms of Se, Sp and AUC, particularly for the CHASE dataset, and consistently ranks among the top-performing models. In particular, there is no apparent discernible pattern among alternative methods in terms of optimal Se, Sp and AUC, suggesting that the proposed FES-Net exhibits unique strengths in accurately segmenting retinal vessels across different datasets.\nIn addition to the quantitative analysis, we present a qualitative comparison of the results obtained by FES-Net and other methods on the DRIVE, STARE, and CHASE datasets. The results obtained on the DRIVE dataset (Fig. 3) demonstrate that FES-Net significantly reduces false positives in small\nvessels compared to current methods. For example, U-Net variants struggle to accurately delineate vessel boundaries, resulting in a higher number of false positives, while SegNet [41] tends to generate false tiny vessels in most images, and BCD-Unet [67] appears to overlook crucial information about vessel structures, leading to suboptimal segmentation performance. In contrast, FES-Net effectively captures this information while minimizing the generation of false vessel information, resulting in more accurate segmentations.\nAlternative methods tend to produce more false positives when applied to the STARE dataset (Fig. 4), particularly around the retinal boundaries, optic nerves, and small vessels. This may be attributed to the challenges posed by the complex retinal structures and image artifacts present in this dataset. The proposed FES-Net method proves to be more robust against these artifacts, preserving the fine details of the vessel structures while maintaining a low rate of false positives. These findings indicate that FES-Net is capable of capturing the subtle characteristics of retinal vessels in challenging scenarios, showcasing its efficacy in this domain.\nSimilar observations are made when applying FES-Net to the CHASE dataset (Fig. 5). Despite the presence of vari-\nous challenges, such as image quality variations and vessel abnormalities, FES-Net consistently achieves accurate vessel segmentation. The proposed method effectively suppresses false positives while preserving the true vessel structures, even in regions with low contrast or overlapping vessel patterns. These results further establish the robustness and efficacy of FES-Net in a variety of datasets and image conditions.\nIn general, the comprehensive evaluation demonstrates the superiority of our proposed FES-Net method in terms of accuracy and robustness compared to the alternative methods examined. Quantitative analysis reveals consistent high performance, while visual comparisons highlight the ability of FES-Net to accurately capture retinal vessel structures with minimal false positives. These results provide solid evidence for the efficacy of our proposed method for retinal vessel segmentation tasks and its potential to help diagnose and monitor retinal diseases."
        },
        {
            "heading": "E. Discussion",
            "text": "There are notable architectural distinctions between our proposed network and the alternative models discussed above, which merit attention. First, it is important to emphasize that\nthe FES-Net consistently outperforms the alternatives, with a substantially reduced number of trainable parameters. Our network is designed to achieve a balance between performance and cost-effectiveness. Specifically, it consists of 1 million (M) trainable parameters, while for example, MultiResUNet [68] has about 7M and VessNet [69] about 9M parameters (Table V).\nIn recent research, modifications to the U-Net and SegNet architectures have been introduced, leading to outstanding performance in the segmentation of retinal vessels [67], [70]. However, it is crucial to emphasize that these methods often involve a substantially larger number of trainable parameters, typically an order of magnitude larger, in comparison to our proposed network. This suggests the potential of our approach to achieving comparable results with fewer parameters, which is promising from a computational perspective.\nFurthermore, our experiments were conducted on publicly available standard datasets and performance metrics which are used widely. U-Net [18] and SegNet [41] are established benchmark methods in the field, and our proposed method consistently demonstrates strong competitiveness against the above-mentioned benchmarks, as evidenced by the results obtained by the FES-Net.\nIn general, our proposed network successfully balances performance and computational efficiency, delivering exceptional results with notably fewer trainable parameters compared to existing methods. This serves as compelling evidence for the effectiveness and potential of our approach for retinal vessel segmentation tasks."
        },
        {
            "heading": "V. CONCLUSIONS",
            "text": "In this study, a novel feature enhancement network specifically designed for the segmentation of retinal blood vessels is proposed, taking into account the computational demands\nto deploy the network on resource-limited devices such as smartphones. Significantly, our segmentation network achieves a remarkable decrease in the overall parameter count when compared to cutting-edge segmentation networks documented in the literature. It consists of approximately 1 million parameters only. We have presented extensive experiments and comparative analyses of different alternative methods using four publicly accessible datasets. The results provide ample evidence of the robustness, competitiveness, and effectiveness of our proposed network."
        }
    ],
    "title": "Feature Enhancer Segmentation Network (FES-Net) for Vessel Segmentation",
    "year": 2023
}