{
    "abstractText": "Multifocus image fusion is an effective way to overcome the limitation of optical lenses. Many existing methods obtain fused results by generating decision maps. However, such methods often assume that the focused areas of the two source images are complementary, making it impossible to achieve simultaneous fusion of multiple images. Additionally, the existing methods ignore the impact of hard pixels on fusion performance, limiting the visual quality improvement of fusion image. To address these issues, a combining generation and recombination model, termed as GRFusion, is proposed. In GRFusion, focus property detection of each source image can be implemented independently, enabling simultaneous fusion of multiple source images and avoiding information loss caused by alternating fusion. This makes GRFusion free from the number of inputs. To distinguish the hard pixels from the source images, we achieve the determination of hard pixels by considering the inconsistency among the detection results of focus areas in source images. Furthermore, a multi-directional gradient embedding method for generating full focus images is proposed. Subsequently, a hard-pixel-guided recombination mechanism for constructing fused result is devised, effectively integrating the complementary advantages of feature reconstruction-based method and focused pixel recombination-based method. Extensive experimental results demonstrate the effectiveness and the superiority of the proposed method. The source code will be released on https: //github.com/xxx/xxx.",
    "authors": [
        {
            "affiliations": [],
            "name": "Huafeng Li"
        },
        {
            "affiliations": [],
            "name": "Dan Wang"
        },
        {
            "affiliations": [],
            "name": "Yuxin Huang"
        },
        {
            "affiliations": [],
            "name": "Yafei Zhang"
        },
        {
            "affiliations": [],
            "name": "Zhengtao Yu"
        }
    ],
    "id": "SP:3547830b18134e91ad0cf3d1c42540d984eea059",
    "references": [
        {
            "authors": [
                "H. Li",
                "B. Manjunath",
                "S.K. Mitra"
            ],
            "title": "Multisensor image fusion using the wavelet transform",
            "venue": "Graphical Models and Image Processing, vol. 57, no. 3, pp. 235\u2013245, 1995.",
            "year": 1995
        },
        {
            "authors": [
                "X. Li",
                "F. Zhou",
                "H. Tan",
                "Y. Chen",
                "W. Zuo"
            ],
            "title": "Multi-focus image fusion based on nonsubsampled contourlet transform and residual removal",
            "venue": "Signal Processing, vol. 184, p. 108062, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "H. Ma",
                "Q. Liao",
                "J. Zhang",
                "S. Liu"
            ],
            "title": "An \u03b1-matte boundary defocus model-based cascaded network for multi-focus image fusion",
            "venue": "IEEE Transactions on Image Processing, vol. 29, pp. 8668\u20138679, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "O. Bouzos",
                "I. Andreadis",
                "N. Mitianoudis"
            ],
            "title": "A convolutional neural network-based conditional random field model for structured multi-focus image fusion robust to noise",
            "venue": "IEEE Transactions on Image Processing, vol. 32, pp. 2915\u20132930, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "Y. Liu",
                "L. Wang",
                "H. Li",
                "X. Chen"
            ],
            "title": "Multi-focus image fusion with deep residual learning and focus property detection",
            "venue": "Information Fusion, vol. 86, pp. 1\u201316, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "B. Yu",
                "B. Jia",
                "L. Ding",
                "Z. Cai",
                "Q. Wu",
                "R. Law",
                "J. Huang",
                "L. Song",
                "S. Fu"
            ],
            "title": "Hybrid dual-tree complex wavelet transform and support vector machine for digital multi-focus image fusion",
            "venue": "Neurocomputing, vol. 182, pp. 1\u20139, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "A.D. Cunha",
                "J. Zhou",
                "M. Do"
            ],
            "title": "The nonsubsampled contourlet transform: Theory, design, and applications",
            "venue": "IEEE Transactions on Image Processing, vol. 15, no. 10, pp. 3089\u20133101, 2006.",
            "year": 2006
        },
        {
            "authors": [
                "Z. Liu",
                "Y. Chai",
                "H. Yin",
                "J. Zhou",
                "Z. Zhu"
            ],
            "title": "A novel multi-focus image fusion approach based on image decomposition",
            "venue": "Information Fusion, vol. 35, pp. 102\u2013116, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "H. Li",
                "X. Liu",
                "Z. Yu",
                "Y. Zhang"
            ],
            "title": "Performance improvementschemeofmultifocusimagefusionderived bydifferenceimages",
            "venue": "Signal Processing, vol. 128, pp. 474\u2013493, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "H. Li",
                "X. He",
                "D. Tao",
                "Y. Tang",
                "R. Wang"
            ],
            "title": "Joint medical image fusion, denoising and enhancement via discriminative low-rank sparse dictionaries learning",
            "venue": "Pattern Recognition, vol. 79, pp. 130\u2013146, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "B. Yang",
                "S. Li"
            ],
            "title": "Multifocus image fusion and restoration with sparse representation",
            "venue": "IEEE transactions on Instrumentation and Measurement, vol. 59, no. 4, pp. 884\u2013892, 2009. JOURNAL OF LATEX CLASS FILES 13",
            "year": 2009
        },
        {
            "authors": [
                "X. Ma",
                "S. Hu",
                "S. Liu",
                "J. Fang",
                "S. Xu"
            ],
            "title": "Multi-focus image fusion based on joint sparse representation and optimum theory",
            "venue": "Signal Processing: Image Communication, vol. 78, pp. 125\u2013134, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Liu",
                "X. Chen",
                "R.K. Ward",
                "Z.J. Wang"
            ],
            "title": "Image fusion with convolutional sparse representation",
            "venue": "IEEE Signal Processing Letters, vol. 23, no. 12, pp. 1882\u20131886, 2016.",
            "year": 1882
        },
        {
            "authors": [
                "B. Yang",
                "S. Li"
            ],
            "title": "Multifocus image fusion and restoration with sparse representation",
            "venue": "IEEE Transactions on Instrumentation and Measurement, vol. 59, no. 4, pp. 884\u2013892, 2009.",
            "year": 2009
        },
        {
            "authors": [
                "Y. Zhang",
                "M. Yang",
                "N. Li",
                "Z. Yu"
            ],
            "title": "Analysis-synthesis dictionary pair learning and patch saliency measure for image fusion",
            "venue": "Signal Processing, vol. 167, p. 107327, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "H. Li",
                "Y. Wang",
                "Z. Yang",
                "R. Wang",
                "X. Li",
                "D. Tao"
            ],
            "title": "Discriminative dictionary learning-based multiple component decomposition for detailpreserving noisy image fusion",
            "venue": "IEEE Transactions on Instrumentation and Measurement, vol. 69, no. 4, pp. 1082\u20131102, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "H. Li",
                "X. He",
                "Z. Yu",
                "J. Luo"
            ],
            "title": "Noise-robust image fusion with lowrank sparse decomposition guided by external patch prior",
            "venue": "Information Sciences, vol. 523, pp. 14\u201337, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "W. Zhao",
                "D. Wang",
                "H. Lu"
            ],
            "title": "Multi-focus image fusion with a natural enhancement via a joint multi-level deeply supervised convolutional neural network",
            "venue": "IEEE Transactions on Circuits and Systems for Video Technology, vol. 29, no. 4, pp. 1102\u20131115, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "Y. Zang",
                "D. Zhou",
                "C. Wang",
                "R. Nie",
                "Y. Guo"
            ],
            "title": "Ufa-fuse: A novel deep supervised and hybrid model for multifocus image fusion",
            "venue": "IEEE Transactions on Instrumentation and Measurement, vol. 70, pp. 1\u201317, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "H. Jung",
                "Y. Kim",
                "H. Jang",
                "N. Ha",
                "K. Sohn"
            ],
            "title": "Unsupervised deep image fusion with structure tensor representations",
            "venue": "IEEE Transactions on Image Processing, vol. 29, pp. 3845\u20133858, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "S. Xu",
                "L. Ji",
                "Z. Wang",
                "P. Li",
                "K. Sun",
                "C. Zhang",
                "J. Zhang"
            ],
            "title": "Towards reducing severe defocus spread effects for multi-focus image fusion via an optimization based strategy",
            "venue": "IEEE Transactions on Computional Imaging, vol. 6, pp. 1561\u20131569, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "X. Hu",
                "J. Jiang",
                "X. Liu",
                "J. Ma"
            ],
            "title": "Zmff: Zero-shot multi-focus image fusion",
            "venue": "Information Fusion, vol. 92, pp. 127\u2013138, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "Z. Wang",
                "X. Li",
                "H. Duan",
                "X. Zhang"
            ],
            "title": "A self-supervised residual feature learning model for multifocus image fusion",
            "venue": "IEEE Transactions on Image Processing, vol. 31, pp. 4527\u20134542, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "X. Nie",
                "B.H.X. Gao"
            ],
            "title": "Mlnet: A multi-domain lightweight network for multi-focus image fusion",
            "venue": "IEEE Transactions on Multimedia, 2023, doi:10.1109/TMM.2022.3194991.",
            "year": 2023
        },
        {
            "authors": [
                "H.T. Mustafa",
                "M. Zareapoor",
                "J. Yang"
            ],
            "title": "Mldnet: Multi-level dense network for multi-focus image fusion",
            "venue": "Signal Processing: Image Communication, vol. 85, p. 115864, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Liu",
                "X. Chen",
                "H. Peng",
                "Z. Wang"
            ],
            "title": "Multi-focus image fusion with a deep convolutional neural network",
            "venue": "Information Fusion, vol. 36, pp. 191\u2013207, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "X. Guo",
                "R. Nie",
                "J. Cao",
                "D. Zhou",
                "W. Qian"
            ],
            "title": "Fully convolutional network-based multifocus image fusion",
            "venue": "Neural Computation, vol. 30, no. 7, pp. 1775\u20131800, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "B. Ma",
                "Y. Zhu",
                "X. Yin",
                "X. Ban",
                "H. Huang",
                "M. Mukeshimana"
            ],
            "title": "Sesffuse: An unsupervised deep model for multi-focus image fusion",
            "venue": "Neural Computing and Applications, vol. 33, pp. 5793\u20135804, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "B. Xiao",
                "B. Xu",
                "X. Bi",
                "W. Li"
            ],
            "title": "Global-feature encoding u-net (geu-net) for multi-focus image fusion",
            "venue": "IEEE Transactions on Image Processing, vol. 30, pp. 163\u2013175, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "J. Ma",
                "Z. Le",
                "X. Tian",
                "J. Jiang"
            ],
            "title": "Smfuse: Multi-focus image fusion via self-supervised mask-optimization",
            "venue": "IEEE Transactions on Computational Imaging, vol. 7, pp. 309\u2013320, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Liu",
                "L. Wang",
                "J. Cheng",
                "X. Chen"
            ],
            "title": "Multiscale feature interactive network for multifocus image fusion",
            "venue": "IEEE Transactions on Instrumentation and Measurement, vol. 70, pp. 1\u201316, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "H. Li",
                "H. Qiu",
                "Z. Yu",
                "B. Li"
            ],
            "title": "Multifocus image fusion via fixed window technique of multiscale images and non-local means filtering",
            "venue": "Signal Processing, vol. 138, pp. 71\u201385, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "S. Woo",
                "J. Park",
                "J.-Y. Lee",
                "I.S. Kweon"
            ],
            "title": "Cbam: Convolutional block attention module",
            "venue": "Proceedings of the European Conference on Computer Vision (ECCV), 2018, pp. 3\u201319.",
            "year": 2018
        },
        {
            "authors": [
                "H. Tang",
                "C. Yuan",
                "Z. Li",
                "J. Tang"
            ],
            "title": "Learning attention-guided pyramidal features for few-shot fine-grained recognition",
            "venue": "Pattern Recognition, vol. 130, p. 108792, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J. Li",
                "X. Guo",
                "G. Lu",
                "B. Zhang",
                "Y. Xu",
                "F. Wu",
                "D. Zhang"
            ],
            "title": "Drpl: Deep regression pair learning for multi-focus image fusion",
            "venue": "IEEE Transactions on Image Processing, vol. 29, pp. 4816\u20134831, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "O. Russakovsky",
                "J. Deng",
                "H. Su",
                "J. Krause",
                "S. Satheesh",
                "S. Ma",
                "Z. Huang",
                "A. Karpathy",
                "A. Khosla",
                "M. Bernstein"
            ],
            "title": "Imagenet large scale visual recognition challenge",
            "venue": "International Journal of Computer Vision, vol. 115, pp. 211\u2013252, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "M. Nejati",
                "S. Samavi",
                "S. Shirani"
            ],
            "title": "Multi-focus image fusion using dictionary-based sparse representation",
            "venue": "Information Fusion, vol. 25, pp. 72\u201384, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "H. Zhang",
                "Z. Le",
                "Z. Shao",
                "H. Xu",
                "J. Ma"
            ],
            "title": "Mff-gan: An unsupervised generative adversarial network with adaptive and gradient joint constraints for multi-focus image fusion",
            "venue": "Information Fusion, vol. 66, pp. 40\u201353, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S. Xu",
                "X. Wei",
                "C. Zhang",
                "J. Liu",
                "J. Zhang"
            ],
            "title": "Mffw: A new dataset for multi-focus image fusion",
            "venue": "arXiv preprint arXiv:2002.04780, 2020.",
            "year": 2002
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "C. Cheng",
                "T. Xu",
                "X.-J. Wu"
            ],
            "title": "Mufusion: A general unsupervised image fusion network based on memory unit",
            "venue": "Information Fusion, vol. 92, pp. 80\u201392, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "J. Ma",
                "L. Tang",
                "F. Fan",
                "J. Huang",
                "X. Mei",
                "Y. Ma"
            ],
            "title": "Swinfusion: Cross-domain long-range learning for general image fusion via swin transformer",
            "venue": "IEEE/CAA Journal of Automatica Sinica, vol. 9, no. 7, pp. 1200\u20131217, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "B. Ma",
                "X. Yin",
                "D. Wu",
                "H. Shen",
                "X. Ban",
                "Y. Wang"
            ],
            "title": "End-to-end learning for simultaneously generating decision map and multi-focus image fusion result",
            "venue": "Neurocomputing, vol. 470, pp. 204\u2013216, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "M. Hossny",
                "S. Nahavandi",
                "D. Creighton"
            ],
            "title": "Comments on\u2018information measure for performance of image fusion",
            "venue": "Electronics letters, vol. 44, no. 18, pp. 1066\u20131067, 2008.",
            "year": 2008
        },
        {
            "authors": [
                "C.S. Xydeas",
                "V. Petrovic"
            ],
            "title": "Objective image fusion performance measure",
            "venue": "Electronics Letters, vol. 36, no. 4, pp. 308\u2013309, 2000.",
            "year": 2000
        },
        {
            "authors": [
                "Y. Chen",
                "R.S. Blum"
            ],
            "title": "A new automated quality assessment algorithm for image fusion",
            "venue": "Image and Vision Computing, vol. 27, no. 10, pp. 1421\u20131432, 2009.",
            "year": 2009
        },
        {
            "authors": [
                "Q. Wang",
                "Y. Shen",
                "J. Jin"
            ],
            "title": "Performance evaluation of image fusion techniques",
            "venue": "Image fusion: algorithms and applications, vol. 19, pp. 469\u2013492, 2008.",
            "year": 2008
        },
        {
            "authors": [
                "Z. Wang",
                "A.C. Bovik",
                "H.R. Sheikh",
                "E.P. Simoncelli"
            ],
            "title": "Image quality assessment: from error visibility to structural similarity",
            "venue": "IEEE transactions on image processing, vol. 13, no. 4, pp. 600\u2013612, 2004.",
            "year": 2004
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014Multifocus image fusion; Image generation; Free number of inputs; Hard pixel detection.\nI. INTRODUCTION Multifocus image fusion aims to integrate the information of several images captured from the same scene with different focus settings into a single image where all regions are in focus. This technique has received wide attention from researchers as it could overcome the depth of field limitation of optical lenses and produce an image with all objects in focus. Roughly, the existing fusion methods can be classified into two main categories: feature reconstruction-based methods and focused pixel recombination-based methods.\nFeature reconstruction-based methods extract features from the source images as the initial step, followed by fusing these features using specific fusion strategies. Finally, the fused features are used to generate the fusion result. Typically, feature extraction methods include multiscale geometric\nThis work was supported in part by the National Natural Science Foundation of China (61966021, 62276120, 62161015), and the Yunnan Fundamental Research Projects (202301AV070004).\nH. Li, D. Wang, X. Huang, Yafei Zhang and Z. Yu are with the Faculty of Information Engineering and Automation, Kunming University of Science and Technology, Kunming 650500, China.(E-mail:lhfchina99@kust.edu.cn (H. Li); wd97@kust.edu.cn (D. Wang))\nManuscript received xxxx;\nanalysis methods(such as wavelet transform [1], multiscale contourlet transform [2]) and deep neural networks [3], [4], etc. Fused images obtained by feature reconstruction-based methods often exhibit high visual quality and can effectively avoid block artifacts. However, since the fusion result is reconstructed or generated based on fused features, there is still a slight gap between the fusion image and the ideal result. Focused pixel recombination-based methods typically involve detection and recombination of focused pixels. Although these methods usually achieve better objective evaluation quality, they are prone to introducing artifacts due to ambiguity of pixel focus property, resulting in the degradation of the visual quality of the fusion result.\nTo integrate the advantages of these two types of methods, Liu et al. [5] proposed to combine deep residual learning and focus property detection to achieve the fusion of multifocus images. This method assumes that the focused regions between the two source images are complementary. Based on this assumption, the detection results of focused pixels in one source image can be obtained immediately after the focus property of another source image is determined. It\u2019s a commonly used method to obtain focus property detection of two source images. However, this strategy has the following limitations. Firstly, there are a large number of hard pixels in the source images, which exhibits significant ambiguity in their focus properties. These pixels typically come from the smooth regions or boundaries of focused regions. It\u2019s difficult to determine whether the pixels are in focus or not, so there will be a large number of misclassifications in detection result of focused pixels. Subsequently, the quality of the fused image\nar X\niv :2\n30 9.\n04 65\n7v 1\n[ cs\n.C V\n] 9\nS ep\n2 02\n3\nwill be compromised if the above detection result is directly used to determine the focus property of another source image. Secondly, these methods can only fuse two images at a time. When faced with the fusion of multiple images, they typically adopt an alternating fusion strategy as shown in Fig.1 (a), i.e., fuse two source images firstly, and then fuse the fused result of the previous stage with that of the remaining source images, until all source images are fused. Although the strategy is feasible, it may lead to information loss due to multiple feature extraction and fusion, which is detrimental to the final fused image.\nTo address the aforementioned issues, we propose a multifocus image fusion method named GRFusion that is open to the number of fused images. Specifically, the focus property of each source image is determined independently with the assistance of remaining source images. Therefore, the detection result of one source image will no longer affect that of other images. Furthermore, owing to the independent focus property detection for each source image, GRFusion is endowed with the ability to process the fusion of arbitrary number of images in an open manner, reducing the risk of information loss caused by alternating fusion. Additionally, to integrate the complementary advantages of feature reconstruction-based and pixel recombination-based methods, a hard-pixel-guided fusion result construction mechanism is devised. It can effectively reduce the negative impact caused by hard pixels on the fusion result. Technically, this method can determine the origin of the corresponding pixel in the fused image based on whether the focus property of pixel is determined or not. Specifically, for the pixel whose focus property is determined, the corresponding pixel in fused result is derived from the corresponding source image. Otherwise, the pixel in fused image is derived from the result generated by a feature reconstruction-based method with multi-directional gradient embedding proposed in this paper. In summary, this paper has three main contributions as follows.\n\u2022 A hard pixels detection method is designed to meet the requirement of parallel fusion of multiple images, where the hard pixels are identified by finding outliers from the detection results of the focus property of the source image. Owing to the independent pixel focus property detection, the developed method is free from the number of fused source images. By integrating the advantages of feature reconstructionbased method and pixel recombination-based method into a single fusion framework, a novel multifocus image fusion approach termed GRFusion is devised. Unlike existing methods that only use this combined fusion strategy in the transition region between focused and defocused regions. GRFusion applies this strategy to all hard pixels, thus avoiding the negative impact on fused result caused by the detection error of pixel focus property. \u2022 Experimental results on both visual quality and objective evaluation demonstrate the effectiveness and superiority of our method in comparison to 9 well-known multifocus image fusion methods.\nThe rest of the paper is structured as follows: Section II\nbriefly reviews the related work; Section III describes the details of the proposed method; Section IV gives the analysis of the experimental results; And Section V summarizes the paper and draws some conclusions."
        },
        {
            "heading": "II. RELATED WORK",
            "text": ""
        },
        {
            "heading": "A. Feature Reconstruction-Based Multifocus Image Fusion",
            "text": "Feature reconstruction-based methods usually fuse the extracted multifocus image features and then generate the fused images based on the fused features. In such methods, the representation of image features is a crucial factor that influences the fusion image. Currently, the commonly used methods for image feature representation include multi-scale decomposition (MSD) [1], [6]\u2013[9], sparse representation (SR) [10]\u2013[17], and deep learning (DL) [4], [18]\u2013[24]. MSD-based methods often employ multiscale image analysis method such as Wavelet Transform [1], [6], Nonsubsampled Contourlet Transform [2], [7], and Curvelet Transform [8] to represent the the source images. However, these methods employ fixed basis to represent the input image, so their sparse representation ability is weak. SR-based methods have been widely used in multifocus image fusion since they can learn a set of basis from the training samples to sparsely represent the image. Specifically, Yang et al. [14] proposed a SR-based method for restoration and fusion of noisy multifocus images. Zhang et al. [15] developed an analysis-synthesis dictionary pair learning method for fusion of multi-source images, including multifocus images. Li et al. [16] devised an edge-preserving method for fusion of multi-source noisy images. This method can perform fusion and denoising of multifocus noisy images in one framework.\nIn DL-based methods, the feature reconstruction-based approach generally includes three steps: feature extraction, feature fusion, and fusion image reconstruction. Since the fusion result is reconstructed from fused features, there may be some errors between the reconstructed and the ideal results. This demonstrates that the result obtained by the method based on feature reconstruction is not ideal. To alleviate this issue, Zhao et al. [18] proposed a CNN-based multi-level supervised network for multifocus image fusion, which utilized multi-level supervision to refine the extracted features. Zang et al. [19] introduced spatial and channel attention to enhance the salient information of the source images, effectively avoiding the loss of details. To address the challenge raised by the absence of ground truth, Jung et al. [20] proposed an unsupervised CNN framework for fusion of multi-source images, where an unsupervised loss function was deigned for unsupervised training.\nIn an unsupervised learning mode, to prevent the loss of source image details, Mustafa et al. [25] added dense connections into feature extraction network at different levels, improving the visual quality of the fused images. Hu et al. [22] proposed a mask prior network and a deep image prior network to achieve multifocus image fusion with zero-shot learning, thereby solving the problem caused by inconsistency between synthetic images and real images. Although the feature reconstruction-based method can generate images that\nlook natural, there is still a certain gap between the generated and the ideal results due to the information loss during the feature extraction and image reconstruction."
        },
        {
            "heading": "B. Pixel Recombination-Based Multifocus Image Fusion",
            "text": "Pixel recombination-based methods usually combine the focused pixels of the source image to construct the fused image. Therefore, how to accurately detect the pixels in the focus area is critical to them. To this end, Liu et al. [26] proposed a twin network and trained with a large number of samples generated by gaussian filtering to achieve the detection of focused pixels. Guo et al. [27] employed a fully connected network to explore the relationship between pixels in the entire image, enabling the determination of pixel focus property. Ma et al. [28] introduced an encoder-decoder feature extraction network consisting of SEDense Blocks to extract the features from source images, and then the focused pixel is distinguished by measuring spatial frequency of the features. Xiao et al. [29] proposed a U-Net with global-feature encoding to determine the focus property of pixel. Ma et al. [30] proposed SMFuse for focused pixel detection where the repeated blur map and the guided filter are both used to generate the binary mask. Considering the complementarity of features at different receptive fields, Liu et al. [31] proposed multiscale feature interactive network to improve detection performance of the focused pixel.\nThe aforementioned methods usually construct fused image using the detected focused pixels. However, it is extremely challenging to determine the focus property of hard pixels located at the smooth regions or the boundaries of focused regions. If the fused image is obtained like above, severe artifacts will be introduced into the fused result. To solve this problem, Li et al. [32] proposed an effective multifocus image\nfusion method based on non-local means filtering, in which the detection result of focused pixel in the transition regions between focused and defocused areas is smoothed by the nonlocal means filtering, and then the smoothed result is used to guide the fusion process. Similarly, Liu et al. [5] combined feature reconstruction with focused pixel recombination, and then developed an effective method for fusion of multifocus images. Although above methods are effective, they ignore the negative impact of hard pixels on the fusion quality. Additionally, the most of existing methods often assume that the number of input images is two and the focus property is complementary. As a result, they cannot fuse the multiple images in a parallel mode. Alternating fusion strategy is a commonly used method to solve this problem, but it tends to lose the information of the source images. In contrast, the developed GRFusion can fuse multiple images simultaneously and can solve the problems caused by hard pixels effectively."
        },
        {
            "heading": "III. PROPOSED METHOD",
            "text": ""
        },
        {
            "heading": "A. Overview",
            "text": "As shown in Fig. 2, the proposed method mainly consists of three components: hard pixel detection (HPD), full-focus image generation with multi-directional feature embedding (FFIG-MFE), and fusion result construction guided by hard pixels (FRC-G-HP). HPD is mainly used to determine which pixels are from the focus ambiguity area. This allows us to take specific solution to address the fusion of hard pixels. FFIGMFE is used to generate an all-focus fusion image based on the fused features of the source images. FRC-G-HP combines the all-focus fusion result and the source images to generate the final fused result under the guidance of detection results of focused pixels. In the following section, we will elaborate on these components carefully."
        },
        {
            "heading": "B. Hard Pixel Detection",
            "text": "1) Focus Modulation: As shown in Fig.2, the HPD is mainly composed of two sub-modules: focus modulation (FM) and multi-scale feature aggregation (MSFA). Meanwhile, convolutional layers are embedded between FM and MSFA to further extract features. FM is mainly used to adjust the discriminability of focused features and improve the detection accuracy of pixel focus property. The specific process is shown in Fig.3. It consists of source image feature extractor (SIFE) and modulation parameter generator (MPG). Let inputi and inputi\u2032 be the inputs of FM, where inputi denotes the i-th source image, and inputi\u2032 denotes the sum of remaining source images. inputi\u2032 is mainly used to provide assistance for the focus detection of inputi. SIFE is used to extract features from inputi and inputi\u2032 , and it consists of convolutional operation, batch normalization and ReLU activation function. MPG is used to generate the modulation parameters to highlight the effect of important features. Technically, we first obtain the edge details of two inputs by:\nedgel = inputl\u2212 inputl \u229bG(l = i, i\u2032), (1)\nwhere G denotes the Gaussian blur kernel and \u229b denotes the convolutional operation. In order to highlight the focus property of each pixel in the source images and prevent the data in edgel from being dispersed, a normalization process is performed on edgel :\nBl = edgel\u2212min(edgel)\nmax(edgel)\u2212min(edgel) (l=i,i\u2032). (2)\nTo make full use of the focus information in edgel , we reverse the normalization result to obtain:\nRl = 1\u2212Bl , (3)\nwhere 1 denotes an all-one matrix with the same size as Rl . Since Ri and Bi\u2032 share the same focus property, the concatenated result can highlight the effect of the feature in focus region. For this reason, we send the concatenated features [Bi,Ri\u2032 ] to two sub-networks which both consist of 3\u00d73 convolution and sigmoid activation function to generate scale factor \u03b1i and transform parameter \u03b2i, respectively:\n\u03b1i = sigmoid(Conv3\u00d73([Bi,Ri\u2032 ])) \u03b2i = sigmoid(Conv3\u00d73([Bi,Ri\u2032 ])) , (4)\nwhere [\u00b7, \u00b7] denotes concatenation operation along the channel dimension, Conv3\u00d73 denotes 3 \u00d7 3 convolutional layer. It should be noted that the parameters in the two convolutional layer in Eq.(4) are not shared.\nTo prevent the over-modulation of \u03b1i and \u03b2i from causing irreparable effects on the features, the parameters \u03b1i and \u03b2i are only performed on the first half of the feature channels of inputl(l = i, i\u2032). In detail, inputl(l = i, i\u2032) is sent into a feature extraction network consisting of 3\u00d73 convolutional layer, BN and ReLU activation function, and then the results obtained after channel split can be expressed as:\n( \u2190\u2212 Fi, \u2212\u2192 Fi) = split(ReLU(BN(Conv(inputi,3\u00d73)))\u00d72), (5)\nwhere \u2190\u2212 Fi and \u2212\u2192 Fi denote the first half features and the second half features of inputl(l = i, i\u2032) along the channel, respectively. The result after \u03b1i and \u03b2i modulation is:\nFmodi = [\u03b1i\u2299 \u2190\u2212 Fi +\u03b2i, \u2212\u2192 Fi]. (6)\nFmodi denotes the result after focus modulation. To use F mod i and Fmodi\u2032 jointly, in this paper, F mod i and F mod i\u2032 are concatenated and then fed into MSFA to further extract the features of focused pixels.\n2) Multi-Scale Feature Aggregation: After FM, the features in the focused region are highlighted but still not enough to make a correct determination for the focus property of pixels. To this end, MSFA is designed as shown in Fig.4. In this module, the saliency of the features in the focused region is improved by interactive enhancement of features extracted under different receptive fields. Methodologically, MSFA mainly consists of a multiscale convolution, BN, spatial attention (SA) [33], channel attention(CA) [33], 1\u00d71 convolution, and multiscale cross-attention. In detail, we send [Fmodi ,F mod i\u2032 ] into MSFA, the outputs of BN can be formulated:\nF 3\u00d73i = BN(Conv3\u00d73([F mod i ,F mod i\u2032 ]) F 7\u00d77i = BN(Conv7\u00d77([F mod i ,F mod i\u2032 ]) . (7)\nTo highlight the effect of important information in F 3\u00d73i and F 7\u00d77i in hard pixel detection, F 3\u00d73 i and F 7\u00d77 i are sent into SA and CA and then integrated by 1\u00d71 convolution:\nF\u0304 3\u00d73i =Conv1\u00d71[CA(F 3\u00d73 i ),SA(F 3\u00d73 i )] F\u0304 7\u00d77i =Conv1\u00d71[CA(F 7\u00d77 i ),SA(F 7\u00d77 i )] . (8)\nFeatures under different perspective fields not only have\nconsistent focus property, but also have certain complementary effects in focus property detection. If the features of the same image at different scales can be used to assist each other and enhance the focus information, the saliency of the focus area features will be effectively highlighted [34], which is beneficial for determining the pixel focus property. To this end, a feature saliency cross-enhancement mechanism is introduced into MSFA. Specifically, F\u0303 3\u00d73i and F\u0303 7\u00d77 i are obtained after linearly mapping of F\u0304 3\u00d73i and F\u0304 7\u00d77 i :\nF\u0303 3\u00d73Q,i =W 3 QF\u0304 3\u00d73 i , F\u0303 3\u00d73 K,i =W 3 KF\u0304 3\u00d73 i , F\u0303 3\u00d73 V,i =W 3 V F\u0304 3\u00d73 i F\u0303 7\u00d77Q,i =W 7 QF\u0304 7\u00d77 i , F\u0303 7\u00d77 K,i =W 7 KF\u0304 7\u00d77 i , F\u0303 7\u00d77 V,i =W 7 V F\u0304 7\u00d77 i ,\n(9) where W kl\u2032 (l\n\u2032 = Q,K,V ;k = 3,7) is a linear transformation matrix. The dimension of F\u0303 kl\u2032,i \u2208 R\nC\u00d7H\u00d7W (l\u2032 = Q,K;k = 3,7) is changed to C\u00d7(HW ) after reshaping operation. To enhance the features of focused region, the feature saliency crossenhancement mechanism is introduced:\nF\u0302 3\u00d73i = softmax( F\u0303 3\u00d73Q,i \u00d7(F\u0303 7\u00d77 K,i )\nT\n\u221a d\n)\u00d7 F\u0303 3\u00d73V,i + F\u0303 3\u00d73 i\nF\u0302 7\u00d77i = softmax( F\u0303 7\u00d77Q,i \u00d7(F\u0303 3\u00d73 K,i )\nT\n\u221a d\n)\u00d7 F\u0303 7\u00d77V,i + F\u0303 7\u00d77 i\n, (10)\nwhere T denotes the transpose operation. In F\u0302 3\u00d73i and F\u0302 7\u00d77 i , the larger value, the corresponding focus property is more significant. Therefore, we can use Eq.(11) to synthesize the significant information:\nF\u0302i(\u00b7, \u00b7)= { F\u0302 3\u00d73i (\u00b7, \u00b7), i f : abs(F\u0302 3\u00d73 i (\u00b7, \u00b7))\u2265 abs(F\u0302 7\u00d77 i (\u00b7, \u00b7))\nF\u0302 7\u00d77i (\u00b7, \u00b7), i f : abs(F\u0302 3\u00d73 i (\u00b7, \u00b7))< abs(F\u0302 7\u00d77 i (\u00b7, \u00b7))\n,\n(11) where abs(\u00b7) denotes absolute value operation.\n3) Hard Pixel Detection: For focused pixel detection, the features output by FM are sent to an encoder and a decoder to obtain the focus property detection results of each source image, where the encoder consists of five MSFAs and convolutional layers and the decoder consists only five convolutional layers. Thanks to the independent focus detection of each source image, it enables the proposed method to fuse multiple source images in parallel. In this process, the method in this paper needs to separately fuse pixels with ambiguous focus property.\nLet Mi be focus property detection result of the source image Ii, and Mi is a binary mask comprised of 0 and 1, where 1 indicates that the corresponding pixel in Ii is focused, and 0 indicates that the corresponding pixel in Ii is defocused. Since each source image has an independent detection result, the hard pixels can be identified by finding inconsistencies between detection results. For N source images, M1(x,y)+ M2(x,y)+ \u00b7 \u00b7 \u00b7+MN(x,y) = 1 means that the focus property of the pixel at (x,y) can be determined, otherwise the pixel at (x,y) is identified regarded as hard pixel. The process can be formulated as:\nMh(x,y) = {\n0, i f : M1(x,y)+M2(x,y)+ \u00b7 \u00b7 \u00b7+MN(x,y) = 1 1/2,otherwise ,\n(12) where Mh(x,y) = 0 indicates that the focus property of pixel at location (x,y) is determined, Mh(x,y) = 1/2 indicates that the pixel at location (x,y) is hard pixel."
        },
        {
            "heading": "C. Full-focus Image Generation With Multi-directional Feature Embedding",
            "text": "Due to the ambiguity of the focus properties of hard pixels, it\u2019s not optimal to construct the final fused result by selecting the focused pixels from source images. To reduce the negative effect of hard pixels on fusion result, we devise a fullfocus image generation method with multi-directional feature embedding (FFIG-MFE) to construct the fusion result of hard pixels. FFIG-MFE consists of multi-directional edge extraction (MDEE), weight generator and edge feature embedding (EFE). As shown in Fig.2, multi-directional edge extraction in FFIGMFE can be achieved by the designed multi-directional edge extraction kernels presented in Fig.5. Specifically, the size of each kernel is 3\u00d73 with the value of center position being 1. In FFIG-MFE, there are eight kernels corresponding to eight different directions are used to extract the edges from the source images. The multi-directional edges are denoted as:\nFi,kn = abs(Ii \u229bKn) (n = 1,2,3...8), (13)\nwhere Fi,kn denotes the edge features of the source image Ii in the n-th direction.\nTo fully utilize the features at eight direction to generate a full-focus image, a weight generation mechanism is designed to dynamically produce the weights for integrating multidirection features. As shown in Fig.6, the weight generator consists of 1\u00d71 convolutional layer and a residual branch, which first employs, 1\u00d71 convolutional operation to reduce the number of channels to 1/4 of the original number, and then utilizes a 3\u00d73 convolutional operation and a 3\u00d73 convolutional operation with dilation rate of d(d = 1,3,5,7) to extract features under different respective field. The outputs of 1\u00d71 convolutional layer and residual block are concatenated, and then weights \u03c9ni (x,y) can be achieved after 1\u00d71 convolution\nand softmax. With \u03c9ni (x,y), the integrated multi-directional edge features can be represented as:\nFedge,i(x,y) = 8\n\u2211 n=1 \u03c9ni (x,y)\u2299Fi,kn(x,y). (14)\nSince Fedge,i contains the focus property of source image Ii, we use it to highlight the effect of focused features in EFE for generating a full-focus image. As shown in Fig.7, we extract the features from Fedge,i using 1\u00d71 convolution to make the dimension consistent with Fi. Let fedge,i(x,y) be the feature vector composed of elements from different channels of Fedge,i at location (x,y). The result is represented as Fi after Ii is processed by convolutional operation, BN and ReLU activate function. We measure the similarity between fedge,i(x,y) and all feature vectors fi(x\u00b1 \u03b4 ,y\u00b1 \u03b4 ) by inner product, where fi(x\u00b1\u03b4 ,y\u00b1\u03b4 ) denotes the feature vector within [x\u00b1\u03b4 ,y\u00b1\u03b4 ]. Then, the weight is obtained after softmax activated:\nDi(x\u2032,y\u2032) = so f tmax(fedge,i(x,y)f T i (x \u2032,y\u2032)), (15)\nwhere (x\u2032,y\u2032) \u2208 [x\u00b1\u03b4 ,y\u00b1\u03b4 ]. The weight is used to scale all feature vectors fi(x\u00b1\u03b4 ,y\u00b1\u03b4 ):\nf\u0302i(x,y) = y+\u03b4\n\u2211 y\u2032=y\u2212\u03b4\nx+\u03b4\n\u2211 x\u2032=x\u2212\u03b4 Di(x\u2032,y\u2032)fi(x \u2032,y\u2032). (16)\nWhen (x,y) in f\u0302i(x,y) traverses all pixels of the entire image, the features for generating a full-focus image are generated. Let\nF\u0302 (x,y) = max{f\u03021(x,y)+f1(x,y), \u00b7 \u00b7 \u00b7 , f\u0302N(x,y)+fN(x,y)}. (17) Since Mh can represent the difficulty of focused pixel detection, we concatenate it with F\u0302 and then feed it to the decoder to generate full-focus image Fg:\nFg =Ed([2Mh, F\u0302 ]), (18)\nwhere Ed denotes a full-focus image decoder consisting of a residual block and a convolutional layer."
        },
        {
            "heading": "D. Fusion Result Construction Guided by Hard Pixels",
            "text": "With Mh and full-focus image Fg, a hard pixels guided fusion result construction is proposed. If the pixel does not belong to the hard pixels, the fused result is constructed directly by selecting the corresponding pixel from source images. Otherwise, the corresponding pixel of Fg is selected.\nTo this end, the Eq.(19) is developed to update the decision map for fusion result construction:\nM\u0303i(x,y) = {\n1, i f :Mi(x,y)+Mh(x,y) = 1 0,otherwise , (19)\nwhere M\u0303i(x,y) = 1 indicates that the pixel located at (x,y) is the focused pixel in the source image Ii. The final fused result guided by Mh can be expressed as:\nF = M\u03031\u2299I1 +M\u03032\u2299I2 + \u00b7 \u00b7 \u00b7+M\u0303N\u2299IN +2Mh\u2299Fg. (20)"
        },
        {
            "heading": "E. Loss Function",
            "text": "The loss functions of the method in this paper are divided into two parts: the hard pixels detection loss and the full-focus image generation loss:\nHard pixels detection loss: In HPD, cross entropy loss formulated in Eq.(21) is used to enable the network to realize the detection of pixel focus property:\n\u2113ce =\u2212 N\n\u2211 i=1\nH,W\n\u2211 x=1,y=1 (gi(x,y) log(pi(x,y))+\n(1\u2212gi(x,y) log(1\u2212 pi(x,y)) , (21)\nwhere pi(x,y) denotes the focused probability at position (x,y) for the ith source image, and gi(x,y) is its corresponding label.\nFull-focus image generation loss: To obtain a clear fullfocus image, besides the global reconstruction loss, the local reconstruction loss is also used in the hard pixel region:\n\u2113rec = \u2225\u2225Fg\u2212G f\u2225\u22251 +\u03bb\u2225\u2225Mh\u2299Fg\u2212Mh\u2299G f\u2225\u22251, (22)\nwhere G f denotes the label of generated full-focus image, \u03bb is a hyperparameter that balances the effects of global and local loss."
        },
        {
            "heading": "IV. EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "A. Datasets",
            "text": "In training of GRFusion, we construct training set by artificial synthesis. Similar to DRPL [35], 200 all-in-focus images selected from ImageNet Large Scale Visual Recognition Challenge 2012 (ILSVRC2012) [36] are used to synthesize the training samples. To achieve the data augmentation, each raw image is randomly cropped to 9 sub-images whose sizes are 256\u00d7256. Let a 256\u00d7256 sub-image be I , a pair of multifocus images is synthesized by:\nI1 =M \u2299I+(1\u2212M)\u2299 G\u0303(I) I2 =M \u2299 G\u0303(I)+(1\u2212M)\u2299I , (23)\nwhere M denotes a binary mask randomly generated by the \u201cfindContours\u201d function in OpenCV, and G\u0303(\u00b7) is Gaussian filter with five different blurred versions. In the testing, the datasets used in our experiments are Lytro [37], MFI-WHU [38] and MFFW [39]. The details of the testing datasets are shown in Table I and the source images used to show the fused results are displayed in Fig.8:\nB. Implementation Details\nThe training process includes two stages: the training of HPD and the training of FFIG-MF. FFIG-MF is trained after the training of HPD. In training of HPD, the total epoch is set to 600 with batch size of 24. In training of FFIG-MF, the total epoch is also set to 600 and the batch size is set to 4. The loss functions in Eq.(21) and Eq.(22) are minimized with the Adam Optimizer [40]. The learning rate is initialized to 0.0001 and the exponential decay rate is set to 0.9. The proposed method is implemented on Pytorch framework and the experiments are conducted on a desktop with a single GeForce RTX 3090."
        },
        {
            "heading": "C. Comparison Methods and Evaluation Metrics",
            "text": "To demonstrate the effectiveness of our GRFusion, both qualitative and quantitative quality assessment are performed. In this process, we compare GRFusion with 9 state-of-the-art methods. These methods can be divided into two categories, i.e., feature reconstruction-based methods including MFFGAN [38], ZMFF [22], MUFusion [41] and SwinFusion [42], pixel recombination-based methods including DRPL [35], GACN [43], SESF [28], SMFuse [30] and MSFIN [31]. To quantitatively evaluate the fusion results, five commonly used metrics including mutual information(QMI [44]), edge based similarity metric(QAB/F [45]), chen-blum metric(QCB [46]), nonlinear correlation information entropy(QNCIE [47]) and structural similarity (QSSIM [48]) are used in our experiments. In these metrics, QMI evaluates how much information of the source images is included in the fusion result. QAB/F measures the degree of edges of source images transferred into the fused result. QCB assesses the amount of information transferred from source images into fused result. QNCIE evaluates the\nfused image from preservation of detail, structure, color and brightness. QSSIM measures the structural similarity between the fusion result and the source images. The larger the values of these evaluation indicators, the better the quality of the fusion results."
        },
        {
            "heading": "D. Fusion of Two Source Images",
            "text": "The first experiment is performed on fusion of two source images. In this process, the image pairs from \u201cLytro\u201d and \u201cMFI-WHU\u201d are used as the test samples, and the fusion results of the source image pairs shown in Fig.8 are displayed in Fig.9 and Fig.10. As observed, GRFusion performs observable advantages when compared to the other 9 methods. First of all, GRFusion is deigned by combining feature reconstructionbased method and focused pixel recombination-based method. Therefore, it can maximize the retention of focused information in the source image, avoiding the loss of detailed information and the introduction of artifacts. Meanwhile, thanks to FRC-G-HP, the degradation of fusion results caused by focus detection errors can be effectively alleviated. Consequently, the developed method can produce the results with better visual quality as shown in Fig.9 and Fig.10.\nSpecifically, from the boxed area we can see that the residual information in the results generated by the proposed method is less than that of the compared methods. This indicates that GRFusion can transfer more focused information of the source images into the fusion results. Moreover, we can also find that MFF-GAN, MUFusion and SwinFusion methods suffer from chromatic aberrations, and ZMFF tends to loss the information in focused regions. The results obtained by the focused pixel recombination-based methods, i.e., DRPL, GACN, SESF, SMFuse and MSFIN are prone to blurring at the boundary of the focus region. This can be seen from the residual information in the boxed areas. To evaluate the fused results of each method more objectively, we further list quantitative comparison results in Table II. As we can see, the results generated by the proposed method are optimal in most matrices. It further demonstrates the effectiveness of the proposed method."
        },
        {
            "heading": "E. Fusion of Multiple Source Images",
            "text": "Unlike most of the existing methods, GRFusion is able to achieve the parallel fusion of multiple images and shows superior fusion performance. To validate this argument, the sequence sample set in Lytro and MFFW with more than two multifocus source images are tested. Specifically, there are 3, 4 and 6 source images in \u2018Lytro-triple series-03\u201d, \u201cMFFW3-03\u201d, \u201cMFFW3-06\u201d and \u201cMFFW3-02\u201d. The fusion results generated by different methods are shown in Fig. 11. From the boxed areas shown in the first and the second columns, it can be seen that GRFusion can not only achieve fusion processing of three source images, but also more effectively preserve the focus information, resulting in a fusion image with higher visual quality. From the remaining results illustrated in Fig. 11, we can draw similar conclusions. This is because the method proposed in this paper can perform parallel fusion processing\non multiple source images, avoiding the introduction of multiple feature extraction used in alternating fusion. Therefore, the proposed method can effectively alleviate the information loss caused by insufficient feature extraction. To objectively evaluate the quality of fusion results, the evaluation results of five metrics on the fusion images shown in Fig. 11 are displayed in Fig. 12. The objective evaluation results further prove that the proposed method also has advantages when fusing more than two source images.\nTABLE II QUANTITATIVE COMPARISON OF FUSION RESULTS ON \u201cLYTRO\u201d AND \u201cMFI-WHU\u201d DATASETS. THE RED FONT REPRESENTS THE OPTIMAL RESULT, WHILE THE BLUE FONT REPRESENTS THE SUBOPTIMAL RESULT.\nMethods Lytro dataset MFI-WHU datasetQMI QAB/F QCB QNCIE QSSIM QMI QAB/F QCB QNCIE QSSIM MFF-GAN [38] 0.80995 0.64471 0.61022 0.82540 1.65349 0.76162 0.62241 0.60932 0.82352 1.64444\nZMFF [22] 0.90869 0.67569 0.69665 0.83001 1.72113 0.71858 0.61924 0.66719 0.82227 1.63110 MUFusion [41] 0.73928 0.61593 0.59090 0.82231 1.60150 0.65023 0.56568 0.56337 0.81991 1.56340 SwinFusion [42] 0.81836 0.66624 0.61071 0.82555 1.66042 0.76030 0.64815 0.63719 0.82306 1.64592\nDRPL [35] 1.09872 0.73257 0.76223 0.84132 1.72350 1.07076 0.71212 0.78625 0.83999 1.71368 GACN [43] 1.12565 0.73649 0.77273 0.84336 1.72347 1.09467 0.70672 0.79610 0.84212 1.70880 SESF [28] 1.17177 0.74152 0.79114 0.84681 1.72534 1.20181 0.72097 0.80893 0.84908 1.71498\nSMFuse [30] 1.16521 0.74007 0.77957 0.84628 1.72435 1.17048 0.72933 0.81081 0.84724 1.71463 MSFIN [31] 1.18912 0.74739 0.79569 0.84747 1.72547 1.19948 0.72820 0.82107 0.84911 1.71533 GRFusion 1.18998 0.74742 0.79497 0.84798 1.72495 1.20633 0.73382 0.82473 0.85014 1.71536"
        },
        {
            "heading": "F. Ablation Study",
            "text": "1) Ablation Study on Network Structure: To verify the effectiveness of combining the feature reconstruction and focused pixel recombination, we compare GRFusion with three fusion modes, where mode1 generates the fusion image by I1\u2299M1 + I2\u2299 (1\u2212M1), mode2 merges the source image by I1\u2299 (1\u2212M2)+I2\u2299M2 and mode 3 produces the fusion result by the full-focus image generation method proposed in this paper. In this process, Mi(i = 1,2) is the binary map of focused pixel detection result of source image Ii(i = 1,2). As boxed regions shown in Fig.13, the defocused information will be introduced when binary map is estimated incorrectly, and fused results will be not so ideal when only the fullfocus image generation method is used. In contrast, GRFusion can integrate the complementary advantages of mode1, mode2 and mode3. Consequently, the fused results with better performance can be achieved. The above results are also objectively evaluated in Table III.\n2) Ablation Study on Focus Detection: To validate the effectiveness of FM and MSFA in focus property detection, we remove them from HPD respectively. The ablation results are shown in Fig.14, where \u201cBaseline\u201d means that the baseline constructed by removing FM and MSFA from HPD, \u201cBaseline+FM\\MPG\u201d means that the modulation parameter generator is removed from FM, \u201cBaseline+FM\\Rev\u201d means\nFig. 14. Ablation study on focus detection. (a) Source images (b) Baseline (c) Baseline+FM\\MPG (d) Baseline+FM\\Rev (e) Baseline+FM (f) Baseline+MSFA (g) Baseline+FM+MSFA.\nthat the reverse operator in Eq.(3) is moved from FM, \u201cBaseline+FM\u201d means that FM is added into the Baseline, \u201cBaseline+MSFA\u201d means that MSFA is added into the Baseline and \u201cBaseline+FM+MSFA\u201d means that both FM and MSFA are added into the Baseline. From these results we can see the effects of different components in HPD, and find that all the components have played a positive role in detecting focused\npixels. 3) Ablation Analysis on FFIG-MFE: To verify the effectiveness of different components in FFIG-MFE, ablation experiments are performed for MDEE, EFE, weight generator (WG) and hard pixel information implantation, respectively. In this process, the remaining components of FFIG-MFE after removing MDEE, EFE, WG and hard pixels implantation are used as the baseline. In FFIG-MFE, MDEE is used to extract the edge features from different directions of the source images and then integrate them via WG. In ablation study of MDEE, the Laplace operator is employed to replace MDEE. To investigate the effectiveness of WG and EFE, WG is replaced by addition operation and EFE is replaced with common concatenation operation. In FFIG-MFE, hard pixel information implantation is implemented by Eq.(18). In ablation study of hard pixel information implantation, we directly remove Mh from Eq.(18). As seen in Fig.15 and Table IV, the performance is optimal when all components are included, and the residual information in the boxed region is also minimal in Fig.15(g). This demonstrates the effectiveness of each component in FFIG-MFE."
        },
        {
            "heading": "G. Analysis and Selection of \u03bb",
            "text": "In our method, there is one critical hyperparameter, i.e., \u03bb , in loss function. To search an appropriate value for \u03bb , we investigate the impact of \u03bb when it changes within [0,1]. As shown in Fig.17, when \u03bb = 0 and \u03bb = 0.01, the fusion result suffers from slight chromatic, and when \u03bb = 0.1, \u03bb = 0.5, \u03bb = 1, there is residual information presenting in the boxed areas. This indicates that not all information of the focused regions has been transferred into the fusion result. When \u03bb = 0.05, the fusion result shows higher quality since there is less residual information remaining in the boxed region. From the visual quality of the fusion results, we can conclude that \u03bb = 0.05 is the best choice. The quantitative evaluation of fusion results when \u03bb takes different values is shown in Fig.16, which further demonstrates superiority of \u03bb = 0.05 over other settings."
        },
        {
            "heading": "V. CONCLUSION",
            "text": "In this paper, a novel multifocus image fusion method named GRFusion is proposed which combines feature reconstruction with focus pixel recombination. By performing focus property detection of each input source image individually, GRFusion is able to fuse multiple source images simultaneously and avoid information loss caused by alternating fusing strategy effectively. Meanwhile, the determination of hard pixels is realized based on the inconsistency of all the detection results. To reduce the difficulty of fusion of hard pixels, a fullfocus image generation method with multi-directional gradient embedding is proposed. With the generated full-focus image, a hard pixel-guided fusion result construction mechanism is designed, which effectively integrates the respective advantages of the feature reconstruction-based method and the focused pixel recombination-based method. Experiment results and ablation studies demonstrate the effectiveness of the proposed method and each component."
        }
    ],
    "title": "Generation and Recombination for Multifocus Image Fusion with Free Number of Inputs",
    "year": 2023
}