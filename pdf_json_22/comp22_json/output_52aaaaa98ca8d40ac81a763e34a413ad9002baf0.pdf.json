{
    "abstractText": "Real-world datasets exhibit imbalances of varying types and degrees. Several techniques based on re-weighting and margin adjustment of loss are often used to enhance the performance of neural networks, particularly on minority classes. In this work, we analyze the class-imbalanced learning problem by examining the loss landscape of neural networks trained with re-weighting and margin-based techniques. Specifically, we examine the spectral density of Hessian of class-wise loss, through which we observe that the network weights converge to a saddle point in the loss landscapes of minority classes. Following this observation, we also find that optimization methods designed to escape from saddle points can be effectively used to improve generalization on minority classes. We further theoretically and empirically demonstrate that Sharpness-Aware Minimization (SAM), a recent technique that encourages convergence to a flat minima, can be effectively used to escape saddle points for minority classes. Using SAM results in a 6.2% increase in accuracy on the minority classes over the state-of-the-art Vector Scaling Loss, leading to an overall average increase of 4% across imbalanced datasets. The code is available at https://github.com/val-iisc/Saddle-LongTail.",
    "authors": [
        {
            "affiliations": [],
            "name": "Harsh Rangwani"
        },
        {
            "affiliations": [],
            "name": "Sumukh K Aithal"
        },
        {
            "affiliations": [],
            "name": "Mayank Mishra"
        }
    ],
    "id": "SP:72546093129afaaba40b3b221e05827e4914adcc",
    "references": [
        {
            "authors": [
                "Momin Abbas",
                "Quan Xiao",
                "Lisha Chen",
                "Pin-Yu Chen",
                "Tianyi Chen"
            ],
            "title": "Sharp-maml: Sharpnessaware model-agnostic meta learning",
            "venue": "arXiv preprint arXiv:2206.03996,",
            "year": 2022
        },
        {
            "authors": [
                "Guillaume Alain",
                "Nicolas Le Roux",
                "Pierre-Antoine Manzagol"
            ],
            "title": "Negative eigenvalues of the hessian in deep neural networks",
            "venue": "arXiv preprint arXiv:1902.02366,",
            "year": 1902
        },
        {
            "authors": [
                "Maksym Andriushchenko",
                "Nicolas Flammarion"
            ],
            "title": "Understanding sharpness-aware minimization, 2022",
            "venue": "URL https://openreview.net/forum?id=qXa0nhTRZGV",
            "year": 2022
        },
        {
            "authors": [
                "Dara Bahri",
                "Hossein Mobahi",
                "Yi Tay"
            ],
            "title": "Sharpness-aware minimization improves language model generalization",
            "venue": "arXiv preprint arXiv:2110.08529,",
            "year": 2021
        },
        {
            "authors": [
                "Lukas Biewald"
            ],
            "title": "Experiment tracking with weights and biases, 2020",
            "venue": "URL https://www. wandb.com/. Software available from wandb.com",
            "year": 2020
        },
        {
            "authors": [
                "Devansh Bisla",
                "Jing Wang",
                "Anna Choromanska"
            ],
            "title": "Low-pass filtering sgd for recovering flat optima in the deep learning optimization landscape",
            "venue": "arXiv preprint arXiv:2201.08025,",
            "year": 2022
        },
        {
            "authors": [
                "Mateusz Buda",
                "Atsuto Maki",
                "Maciej A Mazurowski"
            ],
            "title": "A systematic study of the class imbalance problem in convolutional neural networks",
            "venue": "Neural Networks,",
            "year": 2018
        },
        {
            "authors": [
                "Jonathon Byrd",
                "Zachary Lipton"
            ],
            "title": "What is the effect of importance weighting in deep learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Kaidi Cao",
                "Colin Wei",
                "Adrien Gaidon",
                "Nikos Arechiga",
                "Tengyu Ma"
            ],
            "title": "Learning imbalanced datasets with label-distribution-aware margin loss",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Pratik Chaudhari",
                "Anna Choromanska",
                "Stefano Soatto",
                "Yann LeCun",
                "Carlo Baldassi",
                "Christian Borgs",
                "Jennifer Chayes",
                "Levent Sagun",
                "Riccardo Zecchina"
            ],
            "title": "Entropy-sgd: Biasing gradient descent into wide valleys",
            "venue": "Journal of Statistical Mechanics: Theory and Experiment,",
            "year": 2019
        },
        {
            "authors": [
                "Nitesh V Chawla",
                "Kevin W Bowyer",
                "Lawrence O Hall",
                "W Philip Kegelmeyer"
            ],
            "title": "Smote: synthetic minority over-sampling technique",
            "venue": "Journal of artificial intelligence research,",
            "year": 2002
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton"
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "arXiv preprint arXiv:2002.05709,",
            "year": 2020
        },
        {
            "authors": [
                "Andrew Cotter",
                "Maya Gupta",
                "Heinrich Jiang",
                "Nathan Srebro",
                "Karthik Sridharan",
                "Serena Wang",
                "Blake Woodworth",
                "Seungil You"
            ],
            "title": "Training well-generalizing classifiers for fairness metrics and other data-dependent constraints",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Jiequan Cui",
                "Zhisheng Zhong",
                "Shu Liu",
                "Bei Yu",
                "Jiaya Jia"
            ],
            "title": "Parametric contrastive learning",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Yin Cui",
                "Menglin Jia",
                "Tsung-Yi Lin",
                "Yang Song",
                "Serge Belongie"
            ],
            "title": "Class-balanced loss based on effective number of samples",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Hadi Daneshmand",
                "Jonas Kohler",
                "Aurelien Lucchi",
                "Thomas Hofmann"
            ],
            "title": "Escaping saddles with stochastic gradients",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Yann N Dauphin",
                "Razvan Pascanu",
                "Caglar Gulcehre",
                "Kyunghyun Cho",
                "Surya Ganguli",
                "Yoshua Bengio"
            ],
            "title": "Identifying and attacking the saddle point problem in high-dimensional nonconvex optimization",
            "venue": "Advances in neural information processing systems,",
            "year": 2014
        },
        {
            "authors": [
                "Pierre Foret",
                "Ariel Kleiner",
                "Hossein Mobahi",
                "Behnam Neyshabur"
            ],
            "title": "Sharpness-aware minimization for efficiently improving generalization",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Rong Ge",
                "Furong Huang",
                "Chi Jin",
                "Yang Yuan"
            ],
            "title": "Escaping from saddle points\u2014online stochastic gradient for tensor decomposition",
            "venue": "In Conference on learning theory,",
            "year": 2015
        },
        {
            "authors": [
                "Behrooz Ghorbani",
                "Shankar Krishnan",
                "Ying Xiao"
            ],
            "title": "An investigation into neural net optimization via hessian eigenvalue density",
            "venue": "Proceedings of the 36th International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Justin Gilmer",
                "Behrooz Ghorbani",
                "Ankush Garg",
                "Sneha Kudugunta",
                "Behnam Neyshabur",
                "David Cardoze",
                "George Dahl",
                "Zachary Nado",
                "Orhan Firat"
            ],
            "title": "A loss curvature perspective on training instability in deep learning",
            "venue": "arXiv preprint arXiv:2110.04369,",
            "year": 2021
        },
        {
            "authors": [
                "Haibo He",
                "Edwardo A. Garcia"
            ],
            "title": "Learning from imbalanced data",
            "venue": "IEEE Transactions on Knowledge and Data Engineering,",
            "year": 2009
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2016
        },
        {
            "authors": [
                "Yin-Yin He",
                "Jianxin Wu",
                "Xiu-Shen Wei"
            ],
            "title": "Distilling virtual examples for long-tailed recognition",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Chi Jin",
                "Rong Ge",
                "Praneeth Netrapalli",
                "Sham M Kakade",
                "Michael I Jordan"
            ],
            "title": "How to escape saddle points efficiently",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Chi Jin",
                "Praneeth Netrapalli",
                "Rong Ge",
                "Sham M. Kakade",
                "Michael I. Jordan"
            ],
            "title": "Stochastic gradient descent escapes saddle points efficiently",
            "venue": "ArXiv, abs/1902.04811,",
            "year": 2019
        },
        {
            "authors": [
                "Bingyi Kang",
                "Saining Xie",
                "Marcus Rohrbach",
                "Zhicheng Yan",
                "Albert Gordo",
                "Jiashi Feng",
                "Yannis Kalantidis"
            ],
            "title": "Decoupling representation and classifier for long-tailed recognition",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Nitish Shirish Keskar",
                "Dheevatsa Mudigere",
                "Jorge Nocedal",
                "Mikhail Smelyanskiy",
                "Ping Tak Peter Tang"
            ],
            "title": "On large-batch training for deep learning: Generalization gap and sharp minima",
            "venue": "arXiv preprint arXiv:1609.04836,",
            "year": 2016
        },
        {
            "authors": [
                "Prannay Khosla",
                "Piotr Teterwak",
                "Chen Wang",
                "Aaron Sarna",
                "Yonglong Tian",
                "Phillip Isola",
                "Aaron Maschinot",
                "Ce Liu",
                "Dilip Krishnan"
            ],
            "title": "Supervised contrastive learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Ganesh Ramachandra Kini",
                "Orestis Paraskevas",
                "Samet Oymak",
                "Christos Thrampoulidis"
            ],
            "title": "Label-imbalanced and group-sensitive classification under overparameterization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Ranjay Krishna",
                "Yuke Zhu",
                "Oliver Groth",
                "Justin Johnson",
                "Kenji Hata",
                "Joshua Kravitz",
                "Stephanie Chen",
                "Yannis Kalantidis",
                "Li-Jia Li",
                "David A Shamma"
            ],
            "title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
            "venue": "International journal of computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "year": 2009
        },
        {
            "authors": [
                "Hao Li",
                "Zheng Xu",
                "Gavin Taylor",
                "Christoph Studer",
                "Tom Goldstein"
            ],
            "title": "Visualizing the loss landscape of neural nets",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Tianhao Li",
                "Limin Wang",
                "Gangshan Wu"
            ],
            "title": "Self supervision to distillation for long-tailed visual recognition",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Xinyan Li",
                "Qilong Gu",
                "Yingxue Zhou",
                "Tiancong Chen",
                "Arindam Banerjee"
            ],
            "title": "Hessian based analysis of sgd for deep nets: Dynamics and generalization",
            "venue": "In Proceedings of the 2020 SIAM International Conference on Data Mining,",
            "year": 2020
        },
        {
            "authors": [
                "Yong Liu",
                "Siqi Mai",
                "Xiangning Chen",
                "Cho-Jui Hsieh",
                "Yang You"
            ],
            "title": "Towards efficient and scalable sharpness-aware minimization",
            "venue": "arXiv preprint arXiv:2203.02714,",
            "year": 2022
        },
        {
            "authors": [
                "Ziwei Liu",
                "Zhongqi Miao",
                "Xiaohang Zhan",
                "Jiayun Wang",
                "Boqing Gong",
                "Stella X Yu"
            ],
            "title": "Largescale long-tailed recognition in an open world",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Aditya Krishna Menon",
                "Sadeep Jayasumana",
                "Ankit Singh Rawat",
                "Himanshu Jain",
                "Andreas Veit",
                "Sanjiv Kumar"
            ],
            "title": "Long-tail learning via logit adjustment",
            "venue": "arXiv preprint arXiv:2007.07314,",
            "year": 2020
        },
        {
            "authors": [
                "DM Merkulov",
                "Ivan V Oseledets"
            ],
            "title": "Empirical study of extreme overfitting points of neural networks",
            "venue": "Journal of Communications Technology and Electronics,",
            "year": 2019
        },
        {
            "authors": [
                "Seulki Park",
                "Jongin Lim",
                "Younghan Jeon",
                "Jin Young Choi"
            ],
            "title": "Influence-balanced loss for imbalanced visual classification",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga",
                "Alban Desmaison",
                "Andreas Kopf",
                "Edward Yang",
                "Zachary DeVito",
                "Martin Raison",
                "Alykhan Tejani",
                "Sasank Chilamkurthy",
                "Benoit Steiner",
                "Lu Fang",
                "Junjie Bai",
                "Soumith Chintala"
            ],
            "title": "Pytorch: An imperative style, highperformance deep learning library",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2019
        },
        {
            "authors": [
                "Harsh Rangwani",
                "Sumukh K Aithal",
                "Mayank Mishra",
                "Arihant Jain",
                "R. Venkatesh Babu"
            ],
            "title": "A closer look at smoothness in domain adversarial training",
            "venue": "In Proceedings of the 39th International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Olga Russakovsky",
                "Jia Deng",
                "Hao Su",
                "Jonathan Krause",
                "Sanjeev Satheesh",
                "Sean Ma",
                "Zhiheng Huang",
                "Andrej Karpathy",
                "Aditya Khosla",
                "Michael Bernstein"
            ],
            "title": "Imagenet large scale visual recognition challenge",
            "venue": "International journal of computer vision,",
            "year": 2015
        },
        {
            "authors": [
                "Levent Sagun",
                "Utku Evci",
                "V Ugur Guney",
                "Yann Dauphin",
                "Leon Bottou"
            ],
            "title": "Empirical analysis of the hessian of over-parametrized neural networks",
            "venue": "arXiv preprint arXiv:1706.04454,",
            "year": 2017
        },
        {
            "authors": [
                "Dvir Samuel",
                "Gal Chechik"
            ],
            "title": "Distributional robustness loss for long-tail learning",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Jingru Tan",
                "Changbao Wang",
                "Buyu Li",
                "Quanquan Li",
                "Wanli Ouyang",
                "Changqing Yin",
                "Junjie Yan"
            ],
            "title": "Equalization loss for long-tailed object recognition",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Bart Thomee",
                "David A Shamma",
                "Gerald Friedland",
                "Benjamin Elizalde",
                "Karl Ni",
                "Douglas Poland",
                "Damian Borth",
                "Li-Jia Li"
            ],
            "title": "Yfcc100m: The new data in multimedia research",
            "venue": "Communications of the ACM,",
            "year": 2016
        },
        {
            "authors": [
                "Grant Van Horn",
                "Pietro Perona"
            ],
            "title": "The devil is in the tails: Fine-grained classification in the wild",
            "venue": "arXiv preprint arXiv:1709.01450,",
            "year": 2017
        },
        {
            "authors": [
                "Grant Van Horn",
                "Oisin Mac Aodha",
                "Yang Song",
                "Yin Cui",
                "Chen Sun",
                "Alex Shepard",
                "Hartwig Adam",
                "Pietro Perona",
                "Serge Belongie"
            ],
            "title": "The inaturalist species classification and detection dataset",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Xudong Wang",
                "Long Lian",
                "Zhongqi Miao",
                "Ziwei Liu",
                "Stella Yu"
            ],
            "title": "Long-tailed recognition by routing diverse distribution-aware experts",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Yuzhe Yang",
                "Zhi Xu"
            ],
            "title": "Rethinking the value of labels for improving class-imbalanced learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Yuzhe Yang",
                "Zhi Xu"
            ],
            "title": "Rethinking the value of labels for improving class-imbalanced learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Zhewei Yao",
                "Amir Gholami",
                "Kurt Keutzer",
                "Michael W Mahoney"
            ],
            "title": "Pyhessian: Neural networks through the lens of the hessian",
            "venue": "IEEE international conference on big data (Big data),",
            "year": 2020
        },
        {
            "authors": [
                "Han-Jia Ye",
                "Hong-You Chen",
                "De-Chuan Zhan",
                "Wei-Lun Chao"
            ],
            "title": "Identifying and compensating for feature deviation in imbalanced deep learning",
            "venue": "arXiv preprint arXiv:2001.01385,",
            "year": 2020
        },
        {
            "authors": [
                "Chuanhai Zhang"
            ],
            "title": "Medical image classification under class imbalance",
            "venue": "PhD thesis, Iowa State University,",
            "year": 2019
        },
        {
            "authors": [
                "Songyang Zhang",
                "Zeming Li",
                "Shipeng Yan",
                "Xuming He",
                "Jian Sun"
            ],
            "title": "Distribution alignment: A unified framework for long-tail visual recognition",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Yifan Zhang",
                "Bryan Hooi",
                "Lanqing Hong",
                "Jiashi Feng"
            ],
            "title": "Test-agnostic long-tailed recognition by test-time aggregating diverse experts with self-supervision",
            "venue": "arXiv preprint arXiv:2107.09249,",
            "year": 2021
        },
        {
            "authors": [
                "Zhisheng Zhong",
                "Jiequan Cui",
                "Shu Liu",
                "Jiaya Jia"
            ],
            "title": "Improving calibration for long-tailed recognition",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Jianggang Zhu",
                "Zheng Wang",
                "Jingjing Chen",
                "Yi-Ping Phoebe Chen",
                "Yu-Gang Jiang"
            ],
            "title": "Balanced contrastive learning for long-tailed visual recognition",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Juntang Zhuang",
                "Boqing Gong",
                "Liangzhe Yuan",
                "Yin Cui",
                "Hartwig Adam",
                "Nicha C Dvornek",
                "sekhar tatikonda",
                "James s Duncan",
                "Ting Liu"
            ],
            "title": "Surrogate gap minimization improves sharpness-aware training",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "In recent years, there has been a lot of progress in visual recognition thanks to the availability of wellcurated datasets [34, 45], which are artificially balanced in terms of the frequency of samples across classes. However, modern real-world datasets are often imbalanced (i.e. long-tailed etc.) [33, 49, 50] and suffer from various kinds of distribution shifts. Overparameterized models like deep neural networks usually overfit classes with a high frequency of samples ignoring the minority (tail) ones [8, 50]. In such scenarios, when evaluated for metrics that focus on performance on minority data, these models perform poorly. These metrics are an essential and practical criterion for evaluating models in various domains like fairness [14], medical imaging [57] etc.\nMany approaches designed for improving the generalization performance of models trained on imbalanced data, are based on the re-weighting of loss [16]. The relative weights for samples of each class are determined, such that the expected loss closely approximates the testing criterion objective [10]. In recent years, re-weighting techniques such as Deferred Re-Weighting (DRW) [10], and Vector Scaling (VS) Loss [32] have been introduced, which improve over the classical reweighting method of weighting the loss of each class sample proportionally to the inverse of class frequency. However, even these improved re-weighting techniques lead to overfitting on the samples of tail classes. Also, it has been shown that use of re-weighted loss for training deep networks converges to final solutions similar to the un-weighted loss case, rendering it to be ineffective [9].\n\u2217Equal Contribution\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\nar X\niv :2\n21 2.\n13 82\n7v 1\n[ cs\n.L G\n] 2\n8 D\nec 2\nThis work looks at the loss landscape in weight space around final converged solutions for networks trained with re-weighted loss. We find that the generic Hessian-based analysis of the average loss used in prior works [21, 19], does not uncover any interesting insights about the sub optimal generalization on tail classes (Sec. 3). As the frequency of samples is different for each class due to imbalance, we analyze the Hessian of the loss for each class. This proposed way of analysis finds that re-weighting cannot prevent convergence to saddle points in the region of high negative curvature for tail classes, which eventually leads to poor generalization [18]. Whereas for head classes, the solutions converge to a minima with almost no significant presence of negative curvature, similar to networks trained on balanced data. This problem of converging to saddle points has not received much traction in recent times, as the negative eigenvalues disappear when trained on balanced datasets, indicating convergence to local minima [11, 21]. However, surprisingly our analysis shows that convergence to saddle points is still a practical problem for neural networks when they are trained on imbalanced (long-tailed) data (Fig. 1).\nA pletheora of optimization methods in literature have been designed to be able to escape saddle points efficiently [20, 27, 28], some of which involve adding a component of isotropic noise to gradients. However, these methods have not been able to improve the performance of deep networks in practice, as the implicit noise of SGD in itself mitigates the issue of saddle points when trained on balanced data [17, 28]. However in the case of imbalanced datasets, we find that the component of SGD along negative curvature (i.e., implicit noise) is insufficient to escape saddle points for minority classes. Thus, learning on imbalanced data can be serve as a practical benchmark for optimization algorithms that can escape saddle points.\nWe further demonstrate that Sharpness-Aware Minimization (SAM) [19] a recent optimization technique, with re-weighting can effectively enhance the gradient component along the negative curvature, allowing effective escape from saddle points which leads to improved generalization performance. We find that SAM can significantly improve the performance across various reweighting and margin enhancing methods designed for long-tailed and class-imbalanced learning. The significant improvements are also observed on large-scale datasets of ImageNet-LT and iNaturalist 2018, demonstrating our resutls\u2019 applicability at scale. We summarize our contributions below:\n\u2022 We propose class-wise Hessian analysis of loss which reveals convergence to saddle points in the loss landscape for minority classes. We find that even loss re-weighting solutions converge to saddle point, leading to sub-optimal generalization on the minority classes.\n\u2022 We theoretically demostrate that SAM with re-weighting and high regularization factor significantly enhances the component of stochastic gradient along the direction of negative curvature , that results in effective escape from saddle points.\n\u2022 We find that SAM can successfully enhance the performance of even state-of-the-art techniques for learning on imbalanced datasets which have a re-weighting component (e.g. VS Loss and LDAM) across various datasets and degrees of imbalance."
        },
        {
            "heading": "2 Related Work & Background",
            "text": "In this work, we use g(x) to denote the output of a model,\u2207g(x) to denote the gradients with respect to parameters, x and y denote the data and labels, respectively. We review the re-weighting methods used for training on imbalanced data with distribution shifts, followed by optimization techniques related to our work."
        },
        {
            "heading": "2.1 Long-Tailed Learning",
            "text": "Re-sampling [8] and Re-weighting [23] are the most commonly used methods to train on classimbalanced datasets. Oversampling the minority classes [12] and undersampling the majority classes [8] are two approaches to re-sampling. Oversampling leads to overfitting on the tail classes, and undersampling discards a large amount of data, which inevitably results in poor generalization. Kang et al. [29] proposed to decouple representation learning and classifier training to improve performance with the same. Mixup Shifted Label-Aware Smoothing model (MiSLAS) [60] aims to improve the calibration of models trained on long-tailed datasets by mixup and label-aware smoothing and thereby improve performance. RIDE [52] and TADE [59] are ensemble-based methods that achieve state-of-the-art on the long-tailed visual recognition. Samuel and Chechik [47] introduces a new loss, DRO-LT, based on distributionally robust optimization for learning balanced feature representations. We explore the problem of training class-imbalanced datasets through the lens of optimization and loss landscape. We will now describe some representative recent effective methods in detail, which we will use as baselines. Additional discussion on long-tailed learning methods is present in App. H.\nLDAM [10]: LDAM introduces optimal margins for each class based on reducing the error through a generalization bound. It results in the following loss function where \u2206j is the margin for each class:\nLLDAM(y; g(x)) = \u2212 log eg(x)y\u2212\u2206y eg(x)y\u2212\u2206y + \u2211 j 6=y e g(x)j where \u2206j = C n 1/4 j for j \u2208 {1, . . . , k} (1)\nThe core idea of LDAM is to regularize the classes with low frequency (low i.e. nj) more, in comparison to the head classes with high frequency.\nDRW [10]: Deferred Re- Weighting refers to training the model with average loss till certain epochs (K), then introducing weight wj proportional to 1/nj to loss term specific to each class j at a later stage. This way of re-weighting has been shown to be effective for improving generalization performance when combined with various losses such as Cross Entropy (CE), LDAM etc. We will be using CE+DRW method as a representative re-weighting method for our analysis. We define CE+DRW loss below for completeness:\nLCE(y; g(x)) = \u2212wy log(eg(x)y/ k\u2211 j=1 eg(x)y ) where wj = 1 1 + (nj \u2212 1)1epoch\u2265K (2)\nVS[32]: Vector Scaling loss is a recently proposed loss function which unifies the idea of multiplicative shift (CDT shift [56]), additive shift (i.e Logit Adjustment [40]) and loss re-weighting. The final loss has the following form:\nLVS(y; g(x)) = \u2212wy log(e\u03b3yg(x)y+\u2206y/ k\u2211 j=1 e\u03b3jg(x)j+\u2206j ) (3)\nHere the \u03b3j and \u2206j are the multiplicative and additive logit hyperparameters, respectively.\n2Figures for the minima and saddle point are from [4] and used for illustration purposes only."
        },
        {
            "heading": "2.2 Loss Landscape",
            "text": "Saddle Points: Saddle points are regions in loss landscape that usually depict a plateau region with some negative curvature. In the non-convex setting, it has been shown that there is an existence of an exponential number of saddle points in loss landscape and convergence to these points demonstrate poor generalization [18]. There has been a lot of effort in developing methods for effectively escaping saddle points which involve the addition of noise (e.g., Perturbed Gradient Descent (PGD) [20, 27, 28]). However, these algorithms have not received much attention in the deep learning community as it has been shown that the implicit noise in SGD can escape saddles easily and converge to local minima [17]. Also, it has been empirically shown that negative eigenvalues from the Hessian spectrum disappear after a few steps of training, indicating escape from saddle points when neural networks are trained on balanced datasets [2, 11, 46]. However, contrary to this, we demonstrate that convergence to saddle points is prevalent in minority class loss landscapes and is a practical problem that can serve as a practical benchmark for the development of algorithms that escape saddle points.\nFlat Minima based Optimization methods: Empirically, it has been shown that converging to a flat minima in loss landscape for a deep network leads to improved generalization in comparison to sharp minima [26, 30]. Recent works have tried to exploit this connection between the geometry of the loss landscape and generalization to achieve lower generalization error. Sharpness-Aware Minimization (SAM) [19] is one such algorithm that aims to simultaneously minimize the loss value and sharpness of the loss surface. SAM has shown impressive generalization abilities across various tasks including Natural Language processing [5], meta-learning [1] and domain adaptation [44]. Low-Pass Filtering SGD (LPF-SGD) [7] is another recently proposed optimization algorithm that aims to recover flat minima from the optimization landscape. LPF-SGD convolves the loss function with a Gaussian Kernel with variance proportional to the norm of the parameters of each filter in the network. In this work, we aim to explore the effectiveness of such algorithms for the task of escaping saddle points, which is a new direction for these algorithms."
        },
        {
            "heading": "3 Convergence to Saddle Points in Tail Class Loss Landscape",
            "text": "This section analyzes the dynamics of the loss landscape of neural networks trained on imbalanced datasets. We use the Cross Entropy (CE) loss L\u0302CE to denote the average cross entropy loss for each class. For fine-grained analysis, we focus on average loss on each class L\u0302CE(y). We visualize the loss landscape of the head and tail classes through the computation of Hessian Eigenvalue Density [21]. The Hessian of the train loss for each class H = \u22072wL\u0302CE(y) contains important properties about\nthe curvature of the classwise loss landscape. The Hessian Eigenvalue Density provides all suitable information regarding the eigenvalues of H . In this work, we focus on \u03bbmax(max eigenvalue) and \u03bbmin(min eigenvalue), which depict the extent of positive and negative curvature present. We use the Lanczos algorithm as introduced in Ghorbani et al. [21] to compute the Hessian Eigenvalue Density (spectral density) tractably. We further calculate the validation accuracy of a particular class y and its eigen spectral density for analysis. We provide more details for these experiments in the App. D.\nDoes the proposed class-wise analysis of loss landscape offer any additional insights? In prior works [21, 22, 37], the Hessian of the average loss is used to characterize the nature of the converged point in the loss landscape. However, we find that when particularly trained on imbalanced datasets like CIFAR-10 LT, the eigen spectral density of the Hessian of average loss (Fig. 2A) does not differ from that of head class loss (Fig. 2B), indicating convergence to a local minima. However, explicitly analyzing the Hessian for the tail class loss (Fig. 2C) gives the correct indication of the presence of negative eigenvalues (i.e., curvature), which is in contrast to average loss. Hence, our proposed class-wise analysis of Hessian is essential for characterizing the nature of the converged solution when the training data is imbalanced.\nWhat happens when you train a neural network with CE-DRW method on CIFAR-10 LT? Fig. 2 shows the spectral density on samples from the head class (Class 0 with 5000 samples) and tail class (Class 8 with 83 samples) at the checkpoint with the best validation accuracy. The spectral density of the head class contains few negative eigenvalues. Most of the eigenvalues are centered around zero, as also observed when training on a balanced dataset [21]. On the other hand, for the tail class, there exists a large number of both negative and positive eigenvalues, indicating convergence to a saddle point. We find that at this point, the L\u0302CE(y) is low along with the norm of gradient, which indicates a stationary saddle point. We also observe that the spectral density of the tail class contains many outlier eigenvalues, and \u03bbmax is much larger compared to the head class indicating sharp curvature. These evidences show that the tail class solution converges to a saddle point instead of a local minimum. Merkulov and Oseledets [41] indicated the existence of stationary points with low error but poor generalization in the loss landscape of neural networks. Also, the existence of saddle points being associated with poor generalization has been observed for small networks [18]. However, in this work, we show that convergence to saddle points can specifically occur in the loss landscape of tail classes even for the popular ResNet [24] family of networks, which is an important and novel observation to the best of our knowledge.\nDynamics of training on Long-Tailed Datasets: We analyze the |\u03bbmin/\u03bbmax| for the head, mid and tail classes at various epochs (10, 50, 160, 170, 190, 200) across training to understand the dynamics of optimization with CE+DRW on long-tailed data (Fig. 3A). |\u03bbmin/\u03bbmax| is a measure of non-convexity of the loss landscape [35], where a high value of |\u03bbmin/\u03bbmax| conveys non-convexity indicating convergence to points with significant negative curvature. The network converges to non-convex regions with negative curvature for tail classes, showing convergence to the saddle point. Also, we find that for the certain tail (Class 7, 8) and mid classes (Class 5), the network starts converging towards regions with negative curvature after applying loss re-weighting (DRW at 160th epoch). This indicates that DRW leads to convergence to a saddle point rather than preventing it."
        },
        {
            "heading": "4 Escaping Saddle Points for Improved Generalization",
            "text": "In this section, we analyze the Sharpness-Aware Minimization technique for escaping from saddle points in tail class loss landscape. In existing works [3, 38, 62], the effectiveness of SAM in escaping saddle points has not been explored to the best of our knowledge.\nSharpness-Aware Minimization (SAM): Sharpness-Aware Minimization is a recent technique which aims to flatten the loss landscape by first finding a sharp maximal point in the neighborhood of current weight w. It then minimizes the loss at the sharp point (w + ).\nmin w max || ||\u2264\u03c1 f(w + ) (4)\nHere, f is any objective (eg. CE or LDAM loss function) and \u03c1 is the hyperparameter that controls the extent of neighborhood. A high value of \u03c1 leads to convergence to much flat loss landscape. The inner optimization in above objective is first approximated using a first order solution:\n\u0302(w) \u2248 arg max || ||\u2264\u03c1 f(w) + T\u2207f(w) = \u03c1\u2207f(w)/||\u2207f(w)||2 (5)\nAfter finding \u0302(w), the network weights are updated using the gradient \u2207f(w)|w+\u0302(w). In recent work [3], it has been shown that the normalization of the norm of the gradient for \u0302(w) calculation above leads to oscillation which implies non-convergence theoretically. Also, it has been empirically shown that the unnormalized version of the gradient with adjusted \u03c1 performs better than the normalized version. Hence, we use the approximation i.e. \u0302(w) = \u03c1\u2207f(w) for our theoretical results. As we will be using the stochastic version of the gradient, we use z as the stochasticity parameter and denote the gradient as\u2207fz(w). With this, we now define the gradient with respect to w that is associated with SAM:\n\u2207fSAMz (w) = \u2207fz(w + \u0302(w)) = \u2207fz(w + \u03c1\u2207fz(w)) (6)\nAs we are using the same batch for obtaining the gradient to calculate the \u0302(w) and loss, we can use the same z as the argument. We now analyze the component of the SAM gradient in the direction of negative curvature, which is required for escaping saddle points [17]."
        },
        {
            "heading": "4.1 Analysis of SAM for Escaping Saddle Points",
            "text": "Our analysis is based on the Correlated Negative Curvature (CNC) assumption [17] that states that stochastic gradients have components along the direction of negative curvature, which helps them escape from saddle points. This assumption has been shown to be theoretically valid for the problem of learning half-spaces and also has been empirically verified for a large number of neural networks of different sizes [17]. We now formally state the assumption below: ASSUMPTION 1 (Correlated Negative Curvature [17]). Let vw be the minimum eigenvector corresponding to the minimum eigenvalue of the Hessian matrix \u22072f(w). The stochastic gradient \u2207fz(w) satisfies the CNC assumption if the second moment of the projection along the direction vw is uniformly bounded away from zero, i.e.\n\u2203 \u03b3 \u2265 0 s.t. \u2200w : E[< vw,\u2207fz(w) >2] \u2265 \u03b3 (7)\nIt has also been emphasized that the value of \u03b3 is shown to correlate with the magnitude of \u03bb2min. This shows that with a high negative eigenvalue, there is a large component of gradient along the negative curvature along vw. This allows the SGD algorithms to escape the saddle points. However, we find that in the case of class imbalanced learning (Fig. 1) even stochastic gradients may have an insufficient component in the direction of negative curvature to escape the saddle points. We now show that SAM technique, which aims to reach a flat minima, further amplifies the gradient component along negative curvature and can be effectively used to escape the saddle point. We now formally state our theorem based on the CNC assumption below: THEOREM 2. Let vw be the minimum eigenvector corresponding to the minimum eigenvalue \u03bbmin of the Hessian matrix\u22072f(w). The\u2207f SAMz (w) satisfies that it\u2019s second moment of projection in vw is atleast (1 + \u03c1\u03bbmin)2 times the original (component of\u2207fz(w)):\n\u2203 \u03b3 \u2265 0 s.t. \u2200w : E[< vw,\u2207f SAMz (w) >2] \u2265 (1 + \u03c1\u03bbmin)2\u03b3 (8)\nREMARK. The above theorem adds the factor (1 + \u03c1\u03bbmin)2 to increase the component in direction of negative curvature (\u03b3) when \u03bbmin \u2264 \u22122\u03c1 . Due to this increase, the model will be able to escape from directions with high negative curvature, leading to an increased \u03bbmin. Also, as the factor \u22122\u03c1 is inversely proportional to \u03c1, the high value of \u03c1 aids in effectively increasing the minimum negative eigenvalue. To empirically verify this, we evaluate the Hessian spectrum for the CIFAR-10 LT dataset using CE-DRW method for different values of \u03c1 (Fig. 3C). We find that, as expected from the theorem, in practice, the high values of \u03c1 lead to less negative values of \u03bbmin. This indicates escaping the saddle points effectively, hence avoiding convergence to regions having negative curvature in loss landscape. The proof of the above theorem and additional details is provided in Appendix B.\nWe also want to convey that theoretically, techniques like Perturbed Gradient Descent (PGD), and LPF-SGD (Low-Pass Filter SGD), which add Gaussian noise into gradient to escape saddle points can also be used for mitigating negative curvature. Also it has been found that SGD [17] can also escape the saddle points and converges to solutions with a flat loss landscape. Also, theoretically according to Theorem 2 in Daneshmand et al. [17] the SGD algorithm convergence to a second-order stationary point depends on the \u03b3 as O(\u03b3\u22124) under some assumptions on f . As we find that as SAM with high \u03c1 enhances the component of SGD in direction of negative curvature (\u03b3) by (1+\u03c1\u03bbmin)2, it is reasonable to expect that SAM is able to escape saddle points effectively and converge to solutions with significant less negative curvature quickly implying better generalization. We provide empirical evidence for this in Fig. 3B and Sec. 5.2.\nWhat happens when you train a neural network with SAM + DRW? With SAM (high \u03c1), the large negative eigenvalues present in the loss landscape of the tail class get suppressed (Fig. 2F). In the spectral density for the tail class, it can be seen that \u03bbmin is much closer to zero for SAM compared to its counterpart with SGD. This aligns with the hypothesis that SAM escapes regions of negative curvature, leading to improved accuracy on the tail classes. However, the spectral density of the head class does not change significantly compared to that of Empirical Risk Minimization (ERM), although the \u03bbmax is much lower for SAM, indicating a flatter minima for the head class.\nWe also analyze the |\u03bbmin/\u03bbmax| across multiple steps of training with SAM (Fig. 3B), where |\u03bbmin/\u03bbmax| is a measure of non-convexity of the loss surface. We observe that SAM does not allow the tail classes to reach a region of high non-convexity. The values of |\u03bbmin/\u03bbmax| is much lower for SAM compared to SGD (Fig. 3A) throughout training, indicating minimal negative eigenvalues (i.e. more convexity) in the loss landscape, especially for the tail and medium classes. This clearly shows that SAM avoids regions of substantial negative curvature in the search of flat minima. Further, we note that once the re-weighting begins, SAM is able to avoid convergence to a saddle point (nonconvexity decreases), which is contrary to what we observe with CE+DRW (with SGD). Theorem 2 states that SAM consists of larger component in the direction of negative curvature which allows to reach a solution with minimal negative curvature. Empirically, Fig. 3B also supports the Theorem 2 as we observe that SAM reaches a minima (high convexity) for all the classes."
        },
        {
            "heading": "5 Experiments",
            "text": ""
        },
        {
            "heading": "5.1 Class-Imbalanced Learning",
            "text": "Datasets: We report our results on four long-tailed datasets: CIFAR-10 LT [10], CIFAR-100 LT [10], ImageNet-LT [39], and iNaturalist 2018 [51]. a) CIFAR-10 LT and CIFAR-100 LT: The original CIFAR-10 and CIFAR-100 datasets consist of 50,000 training images and 10,000 validation images, spread across 10 and 100 classes, respectively. We use two imbalance versions, i.e., long-tail imbalance and step imbalance, as followed in Cao et al. [10]. The imbalance factor, \u03b2 = NmaxNmin , denotes the ratio between the number of samples in the most frequent (Nmax) and least frequent class (Nmin). For both the imbalanced versions, we analyze the results with \u03b2 = 100. b) ImageNet-LT and iNaturalist 2018: We use the ImageNet-LT version as proposed by [39], which is an class-imbalanced version of the large-scale ImageNet dataset [45]. It consists of 115.8K images from 1000 classes, with 1280 images in the most frequent class and 5 images in the least. iNaturalist 2018 [51] is a real-world long-tailed dataset that contains 437.5K images from 8,142 categories. In the case of long-tail imbalance, we segregate the classes of all the datasets into Head (Many), Mid (Medium), and Tail (Few) subcategories, as defined in [60]. For step imbalance experiments on CIFAR datasets, we split the classes into Head (Frequent) and Tail (Minority), as done in [10].\nExperimental Details: We follow the hyperparameters and setup as in Cao et al. [10] for CIFAR-10 LT and CIFAR-100 LT datasets. We train a ResNet-32 architecture as the backbone and SGD with a momentum of 0.9 as the base optimizer for 200 epochs. A multi-step learning rate schedule is used, which drops the learning rate by 0.01 and 0.0001 at the 160th and 180th epoch, respectively. For training with SAM, we set a constant \u03c1 value of either 0.5 or 0.8 for most methods. For ImageNet-LT and iNaturalist 2018 datasets, we use the ResNet-50 backbone similar to [60]. An initial learning rate of 0.1 and 0.2 is set for iNaturalist 2018 and ImageNet-LT, respectively, followed by a cosine learning rate schedule. We initialize the \u03c1 value with 0.05 and utilize a step schedule to increase the \u03c1 value during the course of training for SAM experiments. We run every experiment on long-tailed CIFAR datasets with three seeds and report the mean and standard deviation. Additional implementation details are provided in the App. C. Algorithm for DRW+SAM is defined in App. G.\nBaselines: a) Cross-Entropy (CE): CE minimizes the average loss across all samples, and thus, the performance of tail classes is much lower than that of head classes. b) CE + Deferred Re-Weighting (DRW) [10]: The re-weighting of CE loss inversely by class frequency is done in the later stage of training. c) LDAM + DRW [10]: Label-Distribution-Aware Margin (LDAM) proposes a margin-based loss that encourages larger margins for less-frequent classes. d) Vector Scaling (VS) Loss [32]: VS loss incorporates both additive and multiplicative logit adjustments to modify inter-class margins.\nResults: Table 1 summarizes our results on CIFAR-10 LT and CIFAR-100 LT with \u03b2 of 100. It can be observed that SAM with re-weighting significantly improves the accuracy on mid and tail classes while preserving the accuracy on head classes. SAM improves upon the overall performance\nof CE+DRW by 5.1% on CIFAR-10 LT and 3.6% on CIFAR-100 LT datasets, with the tail class accuracy increasing by 11.7% and 7.7% respectively. These results empirically show that escaping saddle points with SAM leads to a notable increase in overall accuracy primarily due to the major gain in the accuracy on the tail classes. The addition of SAM to recently proposed long-tail learning methods like LDAM and VS loss leads to a significant increase in performance, which indicates that the role of SAM is orthogonal to the margin-based methods. On the other hand, SAM without re-weighting (CE+SAM) improves accuracy on the head and mid classes rather than the tail class. This can be attributed to the fact that standard ERM minimizes the average loss across all the samples without re-weighting such that the weightage of tail class samples in the overall loss is minimal. This shows that naive application of SAM is ineffective in improving tail class performance, in comparison to proposed combination of re-weighting methods with SAM. We also show improved results with various imbalance factors (\u03b2) in App. F.\nWe also show results with step imbalance (\u03b2 = 100) on CIFAR-10 and CIFAR-100 datasets (Table 2). With step imbalance on CIFAR-10, the first five classes have 5000 samples each, and the remaining classes have 50 samples each. The addition of SAM improves the overall performance of CE+DRW on CIFAR-10 by 7.1%, with the tail class accuracy increasing by 14.6%. We observe that on most tail classes, the density of negative eigenvalues in the spectral density is much lower with SAM. This indicates that despite multiple classes with few samples, SAM with DRW can avoid the saddle points. SAM systematically improves performance with LDAM and VS loss leading to state-of-the-art performance on both CIFAR-10 and CIFAR-100 in the step imbalance setting.\nDo these observations scale to large-scale datasets? We report the results on ImageNet-LT dataset in Table 3. We also compare with recent long-tail learning methods: cRT [29], MisLAS [60], DisAlign [58] and DRO-LT [47]. The observations on CIFAR-10 LT and CIFAR-100 LT hold good even on ImageNet-LT. For example, the accuracy on tail classes increases by 6.5% with the introduction of SAM on CE + DRW, which is similar to the gain observed in CIFAR-100 LT with CE + DRW. We observe that LDAM+DRW+SAM surpasses the performance of two-stage training methods including MisLAS, cRT, LWS, and DisAlign. Compared to these two-stage methods, our method is a single stage method and outperforms these two-stage methods. These observations point out that the problem of saddle points also exists in large datasets and convey that SAM is easily generalizable to large-scale imbalanced datasets without making any significant changes. On iNaturalist 2018 [51] too, the accuracy on tail classes gets boosted by more than 3% with SAM (Table 3).\nComparison with SOTA: VS loss [32] is a recently proposed margin-based method that achieves state-of-the-art performance on class-imbalanced datasets with single-stage training without strong augmentations [60], ensembles [59] or self-supervision [54]. SAM significantly improves upon the performance of VS on both CIFAR-10 LT and CIFAR-100 LT. For the practitioners, we suggest using high \u03c1 SAM with re-weighting or margin based methods for effective learning on long-tailed data. We also integrate SAM with more recent IB-Loss [42] and Parametric Contrastive Learning (PaCo) [15] methods and report the results in App. E. We find that SAM is also effectively able to improve performance of these recent methods."
        },
        {
            "heading": "5.2 Ablation Studies",
            "text": "A note on \u03c1 value: We observe that as we increase the smoothness parameter (\u03c1) in SAM, the accuracy on the tail classes increases significantly (Fig. 4). The accuracy on tail classes increases from 63% for \u03c1 = 0.05 to 73% for \u03c1 = 0.8 on CIFAR-10 LT with CE+DRW. This can be ascribed to the correlation between \u03bbmin and \u03c1 as discussed in Sec. 4.1. As the \u03c1 increases, the negative curvature in the tail classes disappears because SAM aims to find a flat minima with a large neighborhood with a low loss value. A very large \u03c1 (0.8) leads to a drop in the head accuracy because it restricts the solution space of the head class, resulting in a drop in the overall accuracy. This also emphasizes that a high \u03c1 is necessary for escaping saddle points and achieving the best results.\nOther methods to escape saddle points: In Table 4, we show that other methods developed to escape saddle points, such as PGD, can be used for improving generalization on tail classes. LPFSGD, an algorithm promoting convergence to flat landscape, inherently adds Gaussian noise to the network parameters and could be considered similar to PGD. We can see that the addition of PGD and LPF-SGD to CE+DRW leads to a substantial gain in the performance of tail classes on CIFAR-10 LT and CIFAR-100 LT. It can also be observed that CE+DRW+SAM outperforms both PGD and LPF-SGD by 2% on average. This further highlights that various methods in literature developed to escape saddle points efficiently can be directly used to improve the performance of minority classes when training on class-imbalanced datasets."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this work, we show that training on imbalanced datasets can lead to convergence to points with sufficiently large negative curvature in the loss landscape for minority classes. We find that this is quite common when neural networks are trained with loss functions that are re-weighted or modified to enhance the focus on minority classes. Due to the occurrence of saddle points, we observe that the network suffers from poor generalization on minority classes. We propose to use SharpnessAware Minimization (SAM) with a high regularization factor \u03c1 as an effective method to escape regions of negative curvature and enhance the generalization performance. We theoretically and empirically demonstrate that SAM with high \u03c1 is able to escape saddle points faster than SGD and converge to better solutions, which is a novel observation to the best of our knowledge. We show that combining SAM with state-of-the-art techniques for learning with imbalanced data leads to significant gains in performance on minority classes. We hope that our work leads to further research in studying the effect of negative curvature in generalization as we show they are a practical issue for class-imbalanced learning using deep neural networks.\nAcknowledgements: This work was supported in part by SERB-STAR Project (Project:STR/2020/000128), Govt. of India. Harsh Rangwani is supported by Prime Minister\u2019s Research Fellowship (PMRF). We are thankful for their support.\nAppendix"
        },
        {
            "heading": "A Limitations of Our Work",
            "text": "We would like to highlight that our theoretical results are based on Daneshmand et al. [17] which verified CNC condition for small scale neural networks, verifying the CNC condition for large networks and exactly characterizing the saddle point solutions obtained by SAM for minority classes, are good directions for future work.\nAlso empirically, we propose to use Sharpness-Aware Minimization with high \u03c1 for tail classes to escape from saddle points. Although the general guideline is to use a higher \u03c1 value like 0.5 or 0.8 to achieve the best result, we do find that \u03c1 as a hyperparameter still requires tuning to obtain the best results. We believe making SAM hyper-parameter free is an interesting direction to pursue in the future."
        },
        {
            "heading": "B Proof of Theorem",
            "text": "In this section, we re-state Theorem 2 and provide it\u2019s proof. The theorem analyzes the variance of stochastic gradient for SAM along the direction of negative curvature and shows that SAM amplifies the variance by a factor, which signals that it has a stronger component in direction of negative curvature under certain conditions. Hence, SAM can be used for effectively escaping saddle points in the loss landscape. This is based on Correlated Negative Curvature (CNC) Assumption for stochastic gradients (Assumption 1). The vw,\u2207f(w) \u2208 Rp\u00d71 whereas the Hessian denoted by H(f(w)) (also\u22072f(w)) \u2208 Rp\u00d7p where p is the number of parameters in the model. THEOREM 3. Let vw be the minimum eigenvector corresponding to the minimum eigenvalue \u03bbmin of the Hessian matrix\u22072f(w). The\u2207f SAMz (w) satisfies that it\u2019s second moment of projection in vw is atleast (1 + \u03c1\u03bbmin)2 times the original (component of\u2207fz(w)):\n\u2203 \u03b3 \u2265 0 s.t. \u2200w : E[< vw,\u2207f SAMz (w) >2] \u2265 (1 + \u03c1\u03bbmin)2\u03b3 (9)\nProof. Using the first-order approximation of a vector valued function through Taylor series: f(w + ) = f(w) + J(\u2207f(w)) (10)\nhere J is the jacobian operator. After considering \u03c1 to be small we have the following approximation for the SAM gradient:\n\u2207fSAM(w) = \u2207f(w + \u03c1\u2207f(w)) (11) = \u2207f(w) + \u03c1H(f(w))\u2207f(w) (12)\nHere, we have used the following property that J(\u2207f(w)) is the Hessian matrix H(f(w)) (also written as \u22072f(w)). Also, as we now want to work with stochastic gradients, we replace gradient \u2207f(w) with it\u2019s stochastic version \u2207fz(w) and introduce an expectation expression. Now, we analyze the second-moment of the SAM gradient along the direction of most negative curvature vw:\nE[< vw,\u2207fSAMz (w) >2] = E[< vw,\u2207fz(w) + \u03c1H(f(w))\u2207fz(w)) >2] = E[(< vw,\u2207fz(w) > +\u03c1 < vw, H(f(w))\u2207fz(w) >)2] = E[(< vw,\u2207fz(w) > +\u03c1vwTH(f(w))\u2207fz(w))2]\nHere, we use the matrix notation for dot product < x, y >= xT y. Using the property of the eigen vector: vTwH(f(w)) = \u03bbminv T w, we substitute the value below:\nE[< vw,\u2207fSAMz (w) >2] = E[(< vw,\u2207fz(w) > +\u03c1\u03bbminvTw\u2207fz(w) >)2] = E[(< vw,\u2207fz(w) > +\u03c1\u03bbmin < vw,\u2207fz(w) >)2] = E[((1 + \u03c1\u03bbmin) < vw,\u2207fz(w) >)2] = (1 + \u03c1\u03bbmin)\n2E[< vw,\u2207fz(w) >2] \u2265 (1 + \u03c1\u03bbmin)2\u03b3\nThe last step follows from the CNC Assumption 1. This completes the proof."
        },
        {
            "heading": "C Experimental Details",
            "text": "Imbalanced CIFAR-10 and CIFAR-100: For the long-tailed imbalance (CIFAR-10 LT and CIFAR100 LT), the sample size across classes decays exponentially with \u03b2 = 100. CIFAR-10 LT holds 5000 samples in the most frequent class and 50 in the least, whereas CIFAR-100 LT decays from 500 samples in the most frequent class to 5 in the least. The classes are divided into three subcategories: Head (Many), Mid (Medium), and Tail (Few). For CIFAR-10 LT, the first 3 classes (> 1500 images each) fall into the head classes, following 4 classes (> 250 images each) into the mid classes, and the final 3 classes (< 250 images each) into the tail classes. Whereas for CIFAR-100 LT, head classes consist of the initial 36 classes, mid classes contain the following 35 classes, and the tail classes consist of the remaining 29 classes.\nIn the step imbalance setting, both CIFAR-10 and CIFAR-100 are split into two classes, i.e., Head (Frequent) and Tail (Minority), with \u03b2 = 100. The first 5 (Head) classes of CIFAR-10 contain 5000 samples each, along with 50 samples each in the remaining 5 (Tail) classes. On the other hand, the top first 50 (Head) classes of CIFAR-100 contain 500 samples each, and the remaining 50 (Tail) classes consist of 5 samples each.\nAll the experiments on imbalanced CIFAR-10 and CIFAR-100 are run with ResNet-32 backbone and SGD with momentum 0.9 as the base optimizer. All the methods train on imbalanced CIFAR-10 and CIFAR-100 with a batch size of 128 for 200 epochs, except for VS Loss, which runs for 300 epochs. We follow the learning rate schedule mentioned in Cao et al. [10]. In the initial 5 epochs, we linearly increase the learning rate to reach 0.1. Following that, a multi-step learning rate schedule decays the learning rate by scaling it with 0.001 and 0.0001 at 160th and 180th epoch, respectively. For LDAM runs on imbalanced CIFAR, the value of C is tuned so that \u2206j is normalised to set maximum margin of 0.5 (refer to Equation. 1 in main text). In the case of VS Loss, we use \u03b3 as 0.05 and \u03c4 as 0.75 for imbalanced CIFAR-10 and CIFAR-100 datasets (refer to Equation. 3 in main text).\nImageNet-LT and iNaturalist 2018: The classes in ImageNet-LT and iNaturalist 2018 datasets are also divided into three subcategories, i.e., Head (Many), Mid (Medium), and Tail (Few). For ImageNet-LT, the head classes consist of the first 390 classes, mid classes contain the subsequent 445 classes, and the tail classes hold the remaining 165 classes. Whereas for iNaturalist 2018, first 842 classes fall into the head classes, subsequent 3701 classes into the mid classes, and the remaining 3599 into the tail classes.\nFor ImageNet-LT and iNaturalist 2018, all the models are trained for 90 epochs with a batch size of 256. We use ResNet-50 architecture as the backbone and SGD with momentum 0.9 as the base optimizer. A cosine learning rate schedule is deployed with an initial learning rate of 0.1 and 0.2 for iNaturalist 2018 and ImageNet-LT, respectively. For LDAM runs on ImageNet-LT and iNaturalist 2018, the value of C is tuned so that \u2206j is normalised to set maximum margin of 0.3 (refer to Equation. 1 in main text).\nOptimum \u03c1 value: Table 5 compiles the \u03c1 value used by SAM across various methods on imbalanced CIFAR-10 and CIFAR-100 datasets. The \u03c1 value in these runs is kept constant throughout the duration of training. We adopt a common step \u03c1 schedule for the SAM runs on both ImageNet-LT and iNaturalist 2018. We initialise the \u03c1 with 0.05 for the initial 5 epochs and change it to 0.1 till the 60th epoch. Following that, we increase the \u03c1 value to 0.5 for the final 30 epochs.\nHow to select \u03c1 ? \u03c1 is an hyperparameter in the SAM algorithm and it is important to choose the right value of \u03c1 for best performance on long-tailed learning. We observe that default value of \u03c1 (0.05) as suggested in Foret et al. [19] does not lead to significant gain in accuracy (Refer Fig. 4 in main paper), as it is not able to escape the region of negative curvature. On long-tail CIFAR-10 and CIFAR-100 setting with re-weighting (DRW), a large value of \u03c1 (0.5 or 0.8) seems to work best instead, as in this work our objective to escape saddle points instead of improving generalization. This can be intuitively understood as large regularization (\u03c1) is required for highly imbalanced datasets to escape saddle points as suggested by Theorem 2. In Table 5, we have reported the \u03c1 value used in every experiment. For the large scale datasets like ImageNet-LT and iNaturalist 18, we found that progressively increasing the \u03c1 value gives the best results. This is based on the idea that, as the training progresses, more flatter regions can be recovered from the loss landscape [7]. In our experiments on ImageNet-LT, we use a large \u03c1 of 0.5 in the last 30 epochs of training and we observe that the tail accuracy significantly increases at this stage of training. For using the proposed method on a new imbalanced dataset, we suggest starting with \u03c1 = 0.05 and increasing \u03c1 till the overall accuracy starts to decrease.\nLPF-SGD and PGD: We use the official implementation of LPF-SGD [7] 3 to report the results on CIFAR-10 LT and CIFAR-100 LT. For LPF-SGD, we use Monte Carlo iterations (M ) = 8 and a constant filter radius (\u03b3) of 0.001 (as defined in Algorithm 4.1 in Bisla et al. [7]). We implement the stochastic PGD method [27, 28] on our own since there is no official PyTorch implementation available. We sample the perturbation (noise) from a Gaussian distribution with zero mean and (\u03c3) standard deviation. We use a \u03c3 of 0.0001 for CIFAR-10 and CIFAR-100 LT experiments.\nHessian Experiments: For calculating the Eigen Spectral Density, we use the PyHessian library [55]. PyHessian uses Lanczos algorithm for fast and efficient computation of the complete Hessian eigenvalue density. The Hessian is calculated on the average loss of the training samples as done in [21, 55]. \u03bbmin and \u03bbmax are extracted from the complete Hessian eigenvalue density. It has been shown that the estimated spectral density calculated with the Lanczos algorithm can be used as an approximate to the exact spectral density [21]. Several works [19, 21, 22, 55] have used the same method to calculate spectral density and analyze the loss landscape of neural networks.\nAll of our implementations are based on PyTorch [43]. For experiments pertaining to imbalanced CIFAR, we use NVIDIA GeForce RTX 2080 Ti, whereas for the large scale ImageNet-LT and iNaturalist 2018, we use NVIDIA A100 GPUs. We log all our experiments with Wandb [6]."
        },
        {
            "heading": "D Additional Eigen Spectral Density Plots",
            "text": "We find that the spectral density of a class is representative of the other classes in same category (Head, Mid or Tail), hence for brevity we only display the eigen spectrum of one class per category for analysis.\nCE: The spectral density on the standard CE loss (without re-weighting) can be seen in Fig. 5. We notice that the density and magnitude of negative eigenvalues is much larger for the tail class (Class\n3https://github.com/devansh20la/LPF-SGD\n7 and Class 9 in Fig. 5B and 5C) compared to the head classes (Fig. 5A). On the other hand, the spectral density of the head class (Class 0) is very different from that of the tail class, with \u03bbmin of the head class very close to 0 indicating convergence to minima.\nIt must be noted that without re-weighting, the loss on the tail class samples is high because CE minimizes the average loss. Hence, the solution may not converge for tail class loss. However, in CE+DRW after re-weighting, we observe that the loss on tail class samples is very low, which indicates convergence to a stationary point. Thus, in CE+DRW, we can evidently conclude that the presence of large negative curvature indicates convergence to a saddle point. In summary, we find that just using CE converges to a point with significant negative curvature in tail class loss landscape. Further, though DRW is able to decrease the loss on tail classes, it still does converge to a point with significant negative curvature. This indicates that it converges to a saddle point instead of a minima. Hence, both CE and CE+DRW do not converge to local minima in tail class loss landscape.\nCE+DRW: We show additional class wise Eigen Spectral Density plots with CE+DRW and CE+DRW with SAM in Fig. 6. We analyze the spectral density plots on Head (Class 2), Mid (Class 5) and Tail (Class 7). It can be seen that the magnitude of \u03bbmax and \u03bbmin is much lower with SAM in all the classes (Fig. 6 D, E, F). This indicates that SAM reaches a flatter local minima with no significant presence of negative eigenvalues, escaping saddle points.\nLDAM: We also show Spectral density plots of Class 0 (Fig. 7 A, C) and Class 9 (Fig. 7 B, D) with LDAM+DRW method (SGD and SAM) in Fig. 7. The existence of negative eigenvalues in the tail class spectral density (Fig. 7B) indicates that even for LDAM loss (a regularized margin based loss), the solutions do converge to a saddle point. This also indicates that observations with CE+DRW hold good for long-tailed learning methods like LDAM which use margins instead of re-weighting directly. Hence, this gives evidence of the reason why SAM can be combined easily with LDAM, VS Loss etc. to effectively improve performance.\nThe spectral density of the tail class of LDAM with SAM (Fig. 7D) contains fewer negative eigenvalues compared to SGD (Fig. 7B). This indicates convergence to local minima and clearly explains why SAM improves the performance of LDAM by 12.7%."
        },
        {
            "heading": "E Additional Results",
            "text": "For further establishing the generality of our method, we choose two recent orthogonal method Influence-Balanced Loss [42] (IB-Loss) and Parametric Contrastive Learning (PaCo) [15] and apply proposed high \u03c1 SAM over them. We use the open-source implementations of IB-Loss 4 and PaCo 5 to reproduce the results and add our proposed method (high \u03c1 SAM) to that setup to obtain the results reported in the table below. We show results on CIFAR-100 LT with an imbalance factor (\u03b2) of 100 and 200. We observe that SAM with high \u03c1 significantly improves overall performance along with the performance on tail classes with both IB-Loss and PaCo method (Table 9). Despite PaCo baseline achieving close to state-of-the-art performance, the addition of high \u03c1 SAM is able to further improve the accuracy. This indicates the generality and applicability of proposed method across various long-tailed learning algorithms.\nWe show additional results on the large scale ImageNet-LT (Table 6) and iNaturalist 2018 (Table 3) dataset with LDAM-DRW. We also compare with recent long-tail learning methods: cRT [29], MisLAS [60], DisAlign [58] and DRO-LT [47]. On ImageNet-LT, LDAM+DRW with SAM leads to a 3.2% gain in overall accuracy with 6.5% increase in tail class accuracy. It can be seen that LDAM+DRW+SAM outperforms most other methods, including MisLAS which uses mixup. Also, it is important to note that MisLAS is trained for 180 epochs unlike LDAM+DRW which is trained only for 90 epochs. We observe that LDAM+DRW+SAM surpasses the performance of two-stage training methods including MisLAS, cRT, LWS, and DisAlign. Compared to these two-stage methods, our method is a single stage method and outperforms these two-stage methods. We want to add that\n4https://github.com/pseulki/IB-Loss 5https://github.com/dvlab-research/Parametric-Contrastive-Learning\nwe were not able to reproduce the numbers reported in DRO-LT* [47] when we were trying to incorporate SAM with DRO-LT.\nWith LDAM+DRW, the addition of SAM results in an increase in Head, Mid and Tail categories on iNaturalist 2018 (Table 3). Specifically, LDAM+DRW+SAM outperforms all other methods in the tail class accuracy.\nThis further emphasizes that our analysis is applicable to large scale imbalanced datasets like ImageNet-LT and iNaturalist 2018. We also want to highlight that our analysis shows that high \u03c1 SAM with re-weighting can be used as a strong baseline in long tailed visual recognition problem. We also find that SAM is highly compatible with different loss-based methods (like LDAM, VS) for tackling imbalance and can be used to achieve significantly better performance."
        },
        {
            "heading": "F Additional Results with Varying Imbalance Factor",
            "text": "We show the results with different imbalance factors (\u03b2 = 10, 50, 100 and 200) on CIFAR-10 LT (Table 7) and CIFAR-100 LT (Table 8) datasets with two methods. It can be seen that the observations in Table 1 are applicable with different degrees of imbalance. SAM with re-weighting improves upon the performance of CE and LDAM losses in all the experiments with varied imbalance factor. We observe an average increase of 3.9% and 3.2% on CIFAR-10 LT and CIFAR-100 LT datasets, respectively. This gain in performance is primarily due to the improvement in the tail accuracy, which increases by 8.6% on CIFAR-10 LT and 3.9% on CIFAR-100 LT.\nAs the dataset becomes more imbalanced (\u03b2 increases), the gain in accuracy with SAM on the tail classes improves significantly. For instance, on CIFAR-10 LT with \u03b2 = 10 (Table 7), CE+DRW+SAM improves upon CE+DRW by 1.2% with a 3.9% increase in tail class accuracy. However, with a more imbalanced dataset (i.e. CIFAR-10 LT \u03b2 = 200), SAM leads to a 6.7% boost in overall accuracy with a massive 15.6% increase in the tail class performance."
        },
        {
            "heading": "G Algorithm",
            "text": "We describe our method in detail in Algorithm 1. On the large scale ImageNet-LT and iNaturalist-18 dataset, we use \u03c1drw > \u03c1. For CIFAR-10 LT and CIFAR-100 LT, we find that \u03c1 = \u03c1drw works well."
        },
        {
            "heading": "H Related Work: Long-tailed Learning",
            "text": "In this section, we discuss some recent approaches in long-tailed learning. Equalization loss is proposed in Tan et al. [48] based on the proposition that the gradients of negative samples overpower\nthe gradient of positive samples for minority classes. Influence-Balanced Loss [42] is a samplelevel re-weighting method that reweights each sample by the inverse of the norm of the gradient of each sample. The gradient of each sample estimates the influence of that sample in determining the decision boundary. Distill the Virtual Examples (DiVE) [25] addresses the problem of classimbalanced learning from the lens of knowledge distillation. It is shown that the teacher models\u2019 predictions (virtual examples) can be distilled into the student model by making use of cross-category interactions. This leads to an improvement in the accuracy of the minority class samples.\nSelf-Supervised Learning methods have been shown to learn generalizable representations [13] which are useful for a wide variety of downstream tasks. Self-Supervised pre-training (SSP) has been shown to improve the performance of class-imbalanced learning [53]. Parametric Contrastive Learning (PaCo) [15] introduces parametric class-wise learnable centers into the Supervised Contrastive Learning [31] framework to improve the performance on imbalanced datasets. PaCo achieves close to state-of-the-art performance on most of the long-tailed learning benchmarks. Self Supervised to Distillation (SSD) [36] is a multi-stage training framework for long-tailed recognition with a total of four stages of training. The first two stages involve self-supervised training followed by the generation of soft labels. The final two stages include joint training with distillation and classifier fine-tuning. Balanced Contrastive Learning (BCL) [61] adapts the Supervised Contrastive framework [31] by proposing a Balanced Contrastive loss which ensures that the feature space is balanced when training with an imbalanced dataset."
        },
        {
            "heading": "I Code and License Details",
            "text": "Our codebase is derived from the official implementation of LDAM-DRW[10]6, VS-Loss [32]7 and SAM[19]8 which have been released under the MIT license. We have included the code and the\n6https://github.com/kaidic/LDAM-DRW 7https://github.com/orparask/VS-Loss 8https://github.com/davda54/sam\nAlgorithm 1 DRW + SAM Require: Network g with parameters w; Training set S; Batch size b; Learning rate \u03b7 > 0; Neighbor-\nhood size \u03c1 > 0, Neighborhood size for re-weighted loss \u03c1drw >= \u03c1; Total Number of Iterations E; Deferred Reweighting Threshold T ; Number of samples in class y: ny; Loss Function L (Cross-Entropy, LDAM).\n1: for i = 1 to E do 2: Sample a mini-batch B \u2282 S with size b. 3: if E < T then 4: Compute Loss L \u2190 1b \u2211 (x,y)\u2208B L(y; gw(x)) 5: Compute \u2190 \u03c1 \u2217 \u2207wL/||\u2207wL|| . Compute Sharp-Maximal Point 6: Compute Loss at w + ;L \u2190 1b \u2211 (x,y)\u2208B L(y; gw+ (x)) 7: Calculate gradient d: d\u2190 \u2207wL 8: else . Deferred Re-Weighting (DRW) 9: Compute re-weighted Loss LRW \u2190 1b \u2211 (x,y)\u2208B n \u22121 y \u00b7 L(y; gw(x))\n10: Compute \u2190 \u03c1drw \u2217 \u2207wLRW/||\u2207wLRW|| 11: Compute re-weighted Loss at w + ;LRW \u2190 1b \u2211 (x,y)\u2208B n \u22121 y \u00b7 L(y; gw+ (x)) 12: Calculate gradient d: d\u2190 \u2207wLRW 13: Update weights wi+1 \u2190 wi \u2212 \u03b7d\npretrained weights of the CE+DRW model trained of CIFAR-10 LT in the supplementary material. The code to reproduce the experiments is available at https://github.com/val-iisc/Saddle-LongTail."
        }
    ],
    "title": "Escaping Saddle Points for Effective Generalization on Class-Imbalanced Data",
    "year": 2022
}