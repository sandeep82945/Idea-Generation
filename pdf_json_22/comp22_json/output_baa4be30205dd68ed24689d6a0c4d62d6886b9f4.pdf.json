{
    "abstractText": "Current research on Deep Reinforcement Learning (DRL) for automated on-ramp merging neglects vehicle powertrain and dynamics. This work considers automated onramp merging for a power-split Plug-In Hybrid Electric Vehicle (PHEV), the 2015 Toyota Prius Plug-In, using DRL. The onramp merging control and the PHEV energy management are co-optimized such that the DRL policy directly outputs the power split between the engine and the electric motor. The testing results show that DRL can be successfully used for cooptimization, leading to collision-free on-ramp merging. When compared with sequential approaches wherein the upper-level on-ramp merging control and the lower-level PHEV energy management are performed independently and in sequence, we found that co-optimization results in economic but jerky on-ramp merging while sequential approaches may result in collisions due to neglecting powertrain power limit constraints in designing the upper-level on-ramp merging controller.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yuan Lin"
        }
    ],
    "id": "SP:3130bf54af1218f2f89ea951fd4ed52dfb7c2993",
    "references": [
        {
            "authors": [
                "A.A.J. Rios-Torres"
            ],
            "title": "Malikopoulos, \u201cA survey on the coordination of connected and automated vehicles at intersections and merging at highway on-ramps,",
            "venue": "IEEE Transactions on Intelligent Transportation Systems,",
            "year": 2016
        },
        {
            "authors": [
                "M. Treiber",
                "A. Hennecke"
            ],
            "title": "and D",
            "venue": "Helbing, \u201cCongested traffic states in empirical observations and microscopic simulations,\u201d Physical Review E, vol. 62, no. 2, p. 1805",
            "year": 2000
        },
        {
            "authors": [
                "A.A.J. Rios-Torres"
            ],
            "title": "Malikopoulos, \u201cAutomated and cooperative vehicle merging at highway on-ramps,",
            "venue": "IEEE Transactions on Intelligent Transportation Systems,",
            "year": 2017
        },
        {
            "authors": [
                "A.T. Tran",
                "M. Kawaguchi",
                "H. Okuda"
            ],
            "title": "and T",
            "venue": "Suzuki, \u201cA model predictive control-based lane merging strategy for autonomous vehicles,\u201d in 2019 IEEE Intelligent Vehicles Symposium (IV). IEEE",
            "year": 2019
        },
        {
            "authors": [
                "T. Nishi",
                "P. Doshi"
            ],
            "title": "and D",
            "venue": "Prokhorov, \u201cMerging in congested freeway traffic using multipolicy decision making and passive actor-critic learning,\u201d IEEE Transactions on Intelligent Vehicles, vol. 4, no. 2, pp. 287\u2013 297",
            "year": 2019
        },
        {
            "authors": [
                "R.S. Sutton",
                "A.G. Barto"
            ],
            "title": "Reinforcement learning: An introduction",
            "venue": "MIT press",
            "year": 2018
        },
        {
            "authors": [
                "D. Silver",
                "A. Huang",
                "C.J. Maddison",
                "A. Guez",
                "L. Sifre"
            ],
            "title": "G",
            "venue": "Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot et al., \u201cMastering the game of go with deep neural networks and tree search,\u201d Nature, vol. 529, no. 7587, pp. 484\u2013489",
            "year": 2016
        },
        {
            "authors": [
                "Y. Lin",
                "J. McPhee"
            ],
            "title": "and N",
            "venue": "L. Azad, \u201cComparison of deep reinforcement learning and model predictive control for adaptive cruise control,\u201d IEEE Transactions on Intelligent Vehicles, vol. 6, no. 2, pp. 221\u2013231",
            "year": 2021
        },
        {
            "authors": [
                "Y. Hu",
                "A. Nakhaei",
                "M. Tomizuka"
            ],
            "title": "and K",
            "venue": "Fujimura, \u201cInteraction-aware decision making with adaptive strategies under merging scenarios,\u201d in 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE",
            "year": 2019
        },
        {
            "authors": [
                "M. Bouton",
                "A. Nakhaei",
                "K. Fujimura"
            ],
            "title": "and M",
            "venue": "J. Kochenderfer, \u201cCooperation-aware reinforcement learning for merging in dense traffic,\u201d in 2019 IEEE Intelligent Transportation Systems Conference (ITSC). IEEE",
            "year": 2019
        },
        {
            "authors": [
                "Y. Lin",
                "J. McPhee"
            ],
            "title": "and N",
            "venue": "L. Azad, \u201cAnti-jerk on-ramp merging using deep reinforcement learning,\u201d in 2020 IEEE Intelligent Vehicles Symposium (IV). IEEE",
            "year": 2020
        },
        {
            "authors": [
                "J. Liu",
                "H. Peng"
            ],
            "title": "Modeling and control of a power-split hybrid vehicle,",
            "venue": "IEEE Transactions on Control Systems Technology,",
            "year": 2008
        },
        {
            "authors": [
                "Y. Gao",
                "L. Chen"
            ],
            "title": "and M",
            "venue": "Ehsani, \u201cInvestigation of the effectiveness of regenerative braking for EV and HEV,\u201d in Future Transportation Technology Conference & Exposition. SAE International",
            "year": 1999
        },
        {
            "authors": [
                "Y. Shao",
                "Z. Sun"
            ],
            "title": "Optimal speed control for a connected and autonomous electric vehicle considering battery aging and regenerative braking limits,",
            "venue": "Dynamic Systems and Control Conference,",
            "year": 2019
        },
        {
            "authors": [
                "A. Taghavipour",
                "M. Vajedi",
                "N.L. Azad"
            ],
            "title": "Intelligent control of connected plug-in hybrid electric vehicles",
            "venue": "Springer",
            "year": 2019
        },
        {
            "authors": [
                "J. Jeong",
                "N. Kim",
                "K. Stutenberg"
            ],
            "title": "and A",
            "venue": "Rousseau, \u201cAnalysis and model validation of the Toyota Prius Prime,\u201d in WCX SAE World Congress Experience. SAE International",
            "year": 2019
        },
        {
            "authors": [
                "A. Taghavipour",
                "N.L. Azad"
            ],
            "title": "and J",
            "venue": "McPhee, \u201cReal-time predictive control strategy for a plug-in hybrid electric powertrain,\u201d Mechatronics, vol. 29, pp. 13\u201327",
            "year": 2015
        },
        {
            "authors": [
                "A. Taghavipour",
                "M. Vajedi",
                "N.L. Azad"
            ],
            "title": "and J",
            "venue": "McPhee, \u201cA comparative analysis of route-based energy management systems for PHEVs,\u201d Asian Journal of Control, vol. 18, no. 1, pp. 29\u201339",
            "year": 2016
        },
        {
            "authors": [
                "C.-C. Lin",
                "H. Peng"
            ],
            "title": "J",
            "venue": "W. Grizzle, and J.-M. Kang, \u201cPower management strategy for a parallel hybrid electric truck,\u201d IEEE Transactions on Control Systems Technology, vol. 11, no. 6, pp. 839\u2013849",
            "year": 2003
        },
        {
            "authors": [
                "H. Banvait",
                "S. Anwar"
            ],
            "title": "and Y",
            "venue": "Chen, \u201cA rule-based energy management strategy for plug-in hybrid electric vehicle (PHEV),\u201d in 2009 American Control Conference. IEEE",
            "year": 2009
        },
        {
            "authors": [
                "N. Kim",
                "S. Cha"
            ],
            "title": "and H",
            "venue": "Peng, \u201cOptimal control of hybrid electric vehicles based on Pontryagin\u2019s minimum principle,\u201d IEEE Transactions on Control Systems Technology, vol. 19, no. 5, pp. 1279\u20131287",
            "year": 2010
        },
        {
            "authors": [
                "C. Musardo",
                "G. Rizzoni",
                "Y. Guezennec"
            ],
            "title": "and B",
            "venue": "Staccia, \u201cA-ECMS: An adaptive algorithm for hybrid electric vehicle energy management,\u201d European Journal of Control, vol. 11, no. 4-5, pp. 509\u2013524",
            "year": 2005
        },
        {
            "authors": [
                "H. Borhan",
                "A. Vahidi",
                "A.M. Phillips",
                "M.L. Kuang",
                "I.V. Kolmanovsky"
            ],
            "title": "and S",
            "venue": "Di Cairano, \u201cMPC-based energy management of a powersplit hybrid electric vehicle,\u201d IEEE Transactions on Control Systems Technology, vol. 20, no. 3, pp. 593\u2013603",
            "year": 2011
        },
        {
            "authors": [
                "L. Serrao",
                "S. Onori"
            ],
            "title": "and G",
            "venue": "Rizzoni, \u201cA comparative analysis of energy management strategies for hybrid electric vehicles,\u201d Journal of Dynamic Systems, Measurement, and Control, vol. 133, no. 3",
            "year": 2011
        },
        {
            "authors": [
                "X. Lin",
                "Y. Wang",
                "P. Bogdan",
                "N. Chang"
            ],
            "title": "and M",
            "venue": "Pedram, \u201cReinforcement learning based power management for hybrid electric vehicles,\u201d in 2014 IEEE/ACM International Conference on Computer-Aided Design (ICCAD). IEEE",
            "year": 2014
        },
        {
            "authors": [
                "C. Liu",
                "Y.L. Murphey"
            ],
            "title": "Power management for plug-in hybrid electric vehicles using reinforcement learning with trip information,",
            "venue": "IEEE Transportation Electrification Conference and Expo (ITEC). IEEE,",
            "year": 2014
        },
        {
            "authors": [
                "Y. Li",
                "H. He",
                "J. Peng"
            ],
            "title": "and H",
            "venue": "Wang, \u201cDeep reinforcement learningbased energy management for a series hybrid electric vehicle enabled by history cumulative trip information,\u201d IEEE Transactions on Vehicular Technology, vol. 68, no. 8, pp. 7416\u20137430",
            "year": 2019
        },
        {
            "authors": [
                "B. Xu",
                "X. Hu",
                "X. Tang",
                "X. Lin",
                "H. Li",
                "D. Rathod"
            ],
            "title": "and Z",
            "venue": "Filipi, \u201cEnsemble reinforcement learning-based supervisory control of hybrid electric vehicle for fuel economy improvement,\u201d IEEE Transactions on Transportation Electrification, vol. 6, no. 2, pp. 717\u2013727",
            "year": 2020
        },
        {
            "authors": [
                "X. Qi",
                "Y. Luo",
                "G. Wu",
                "K. Boriboonsomsin"
            ],
            "title": "and M",
            "venue": "Barth, \u201cDeep reinforcement learning enabled self-learning control for energy efficient driving,\u201d Transportation Research Part C: Emerging Technologies, vol. 99, pp. 67\u201381",
            "year": 2019
        },
        {
            "authors": [
                "T. Liu",
                "X. Hu",
                "W. Hu"
            ],
            "title": "and Y",
            "venue": "Zou, \u201cA heuristic planning reinforcement learning-based energy management for power-split plug-in hybrid electric vehicles,\u201d IEEE Transactions on Industrial Informatics, vol. 15, no. 12, pp. 6436\u20136445",
            "year": 2019
        },
        {
            "authors": [
                "T. Liu",
                "X. Hu",
                "S.E. Li"
            ],
            "title": "and D",
            "venue": "Cao, \u201cReinforcement learning optimized look-ahead energy management of a parallel hybrid electric vehicle,\u201d IEEE/ASME Transactions on Mechatronics, vol. 22, no. 4, pp. 1497\u2013 1507",
            "year": 2017
        },
        {
            "authors": [
                "Z. Zhu",
                "Y. Liu"
            ],
            "title": "and M",
            "venue": "Canova, \u201cEnergy management of hybrid electric vehicles via deep Q-networks,\u201d in 2020 American Control Conference. IEEE",
            "year": 2020
        },
        {
            "authors": [
                "B. Chen",
                "S.A. Evangelou"
            ],
            "title": "and R",
            "venue": "Lot, \u201cSeries hybrid electric vehicle simultaneous energy management and driving speed optimization,\u201d IEEE/ASME Transactions on Mechatronics, vol. 24, no. 6, pp. 2756\u2013 2767",
            "year": 2019
        },
        {
            "authors": [
                "D. Chen",
                "Y. Kim",
                "M. Huang"
            ],
            "title": "and A",
            "venue": "Stefanopoulou, \u201cAn iterative and hierarchical approach to co-optimizing the velocity profile and powersplit of plug-in hybrid electric vehicles,\u201d in 2020 American Control Conference (ACC). IEEE",
            "year": 2020
        },
        {
            "authors": [
                "L. Li",
                "X. Wang"
            ],
            "title": "and J",
            "venue": "Song, \u201cFuel consumption optimization for smart hybrid electric vehicle during a car-following process,\u201d Mechanical Systems and Signal Processing, vol. 87, pp. 17\u201329",
            "year": 2017
        },
        {
            "authors": [
                "F. Xu",
                "T. Shen"
            ],
            "title": "Decentralized optimal merging control with optimization of energy consumption for connected hybrid electric vehicles,",
            "venue": "IEEE Transactions on Intelligent Transportation Systems,",
            "year": 2021
        },
        {
            "authors": [
                "M. Vajedi",
                "N.L. Azad"
            ],
            "title": "Ecological adaptive cruise controller for plug-in hybrid electric vehicles using nonlinear model predictive control,",
            "venue": "IEEE Transactions on Intelligent Transportation Systems,",
            "year": 2015
        },
        {
            "authors": [
                "A. Taghavipour",
                "R. Masoudi",
                "N.L. Azad"
            ],
            "title": "and J",
            "venue": "McPhee, \u201cHighfidelity modeling of a power-split plug-in hybrid electric powertrain for control performance evaluation,\u201d in International Design Engineering Technical Conferences and Computers and Information in Engineering Conference, vol. 55843. American Society of Mechanical Engineers",
            "year": 2013
        },
        {
            "authors": [
                "S. Buggaveeti",
                "M. Batra",
                "J. McPhee"
            ],
            "title": "and N",
            "venue": "Azad, \u201cLongitudinal vehicle dynamics modeling and parameter estimation for plug-in hybrid electric vehicle,\u201d SAE International Journal of Vehicle Dynamics, Stability, and NVH, vol. 1, no. 2017-01-1574, pp. 289\u2013297",
            "year": 2017
        },
        {
            "authors": [
                "T. Fersch",
                "R. Weigel"
            ],
            "title": "and A",
            "venue": "Koelpin, \u201cChallenges in miniaturized automotive long-range lidar system design,\u201d in Three-Dimensional Imaging, Visualization, and Display 2017, vol. 10219. International Society for Optics and Photonics",
            "year": 2017
        },
        {
            "authors": [
                "J. Hecht"
            ],
            "title": "Lidar for self-driving cars,",
            "venue": "Optics and Photonics News, vol. 29,",
            "year": 2018
        },
        {
            "authors": [
                "A. Aashto"
            ],
            "title": "Policy on geometric design of highways and streets,",
            "venue": "American Association of State Highway and Transportation Officials, Washington, DC,",
            "year": 2001
        },
        {
            "authors": [
                "M. Batra",
                "J. McPhee"
            ],
            "title": "and N",
            "venue": "L. Azad, \u201cReal-time model predictive control of connected electric vehicles,\u201d Vehicle System Dynamics, vol. 57, no. 11, pp. 1720\u20131743",
            "year": 2019
        },
        {
            "authors": [
                "T. Haarnoja",
                "A. Zhou",
                "P. Abbeel"
            ],
            "title": "and S",
            "venue": "Levine, \u201cSoft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor,\u201d in International Conference on Machine Learning",
            "year": 2018
        },
        {
            "authors": [
                "S. Ioffe",
                "C. Szegedy"
            ],
            "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift,",
            "venue": "in International conference on machine learning. PMLR,",
            "year": 2015
        },
        {
            "authors": [
                "H. Van Hasselt",
                "A. Guez"
            ],
            "title": "and D",
            "venue": "Silver, \u201cDeep reinforcement learning with double Q-learning,\u201d in Proceedings of the AAAI conference on artificial intelligence, vol. 30, no. 1",
            "year": 2016
        },
        {
            "authors": [
                "T.P. Lillicrap",
                "J.J. Hunt",
                "A. Pritzel",
                "N. Heess",
                "T. Erez",
                "Y. Tassa",
                "D. Silver"
            ],
            "title": "and D",
            "venue": "Wierstra, \u201cContinuous control with deep reinforcement learning,\u201d arXiv preprint arXiv:1509.02971",
            "year": 2015
        },
        {
            "authors": [
                "J. Schulman",
                "F. Wolski",
                "P. Dhariwal",
                "A. Radford"
            ],
            "title": "and O",
            "venue": "Klimov, \u201cProximal policy optimization algorithms,\u201d arXiv preprint arXiv:1707.06347",
            "year": 2017
        },
        {
            "authors": [
                "Y. Lin",
                "A. Eskandarian"
            ],
            "title": "Experimental evaluation of cooperative adaptive cruise control with autonomous mobile robots,",
            "venue": "IEEE Conference on Control Technology and Applications (CCTA). IEEE,",
            "year": 2017
        },
        {
            "authors": [
                "S. Fujimoto",
                "H. Hoof"
            ],
            "title": "and D",
            "venue": "Meger, \u201cAddressing function approximation error in actor-critic methods,\u201d in International Conference on Machine Learning",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n20 3.\n03 11\n3v 1\n[ ee\nss .S\nY ]\n7 M\nar 2\n02 2\nIndex Terms\u2014Autonomous Driving; On-Ramp Merging; Deep Reinforcement Learning; Plug-In Hybrid Electric Vehicles.\nI. INTRODUCTION\nAutomated on-ramp merging is an on-going research topic that has seen different methods proposed [1]. The early approaches include heuristics-based methods such as IntelligentDriver-Model (IDM) [2] and optimization-based methods such as Model Predictive Control (MPC) [1]. The solution framework can be centralized wherein merging is controlled by a roadside unit to some extent [3], or decentralized wherein the merging vehicle merges on ramp without the control by a roadside unit [4].\nRecently, reinforcement learning (RL), particularly DRL, has been used to solve the automated on-ramp merging problem [5]. In RL, an agent learns to maximize cumulative discounted reward for optimal decision-making and control in an environment [6]. DRL that utilizes deep (multi-layer) neural nets as the policy has seen breakthroughs as its trained policy surpassed human champions in playing complex board games [7]. It has been demonstrated that DRL offers near-optimal control performance and high modeling-error tolerance while requiring much less computation time than MPC [8].\nCurrently DRL-based on-ramp merging is mostly decentralized. A popular approach is integrated decision and control\nYuan Lin is with the Shien-Ming Wu School of Intelligent Engineering at the South China University of Technology, Guangzhou, China 511442. yuanlin@scut.edu.cn\nJohn McPhee and Nasser L. Azad are with the Systems Design Engineering Department at the University of Waterloo, Ontario, Canada N2L 3G1. mcphee@uwaterloo.ca; nlashgarianazad@uwaterloo.ca\nsuch that DRL outputs acceleration whose values reflect the decision to merge behind or ahead a vehicle on the main road [9], [10]. It is shown that DRL could lead to collision-free on-ramp merging [11]. However, current DRL-based on-ramp merging only considers the merging vehicle as a point-mass kinematic model, neglecting vehicle powertrain and dynamics. As far as our work is concerned, current literature has not considered DRL-based on-ramp merging for a PHEV.\nIn recent years, hybrid electric vehicles (HEVs) are becoming popular among consumers as they offer better fuel economy compared to conventional internal combustion engine vehicles [12]. HEVs have two power sources: the engine and the battery. HEVs can save fuel via regenerative braking and engine stops in stop-and-go traffic [13], [14]. Compared to HEVs, PHEVs have a larger battery that can be plugged into an external power grid [15], [16]. Thus, PHEVs could provide an initial electric drive, similar to battery electric vehicles. HEVs and PHEVs offer a transition towards the pure electric vehicle era.\nThe energy management for HEVs or PHEVs is a challenging problem that has spurred abundant research [17], [18]. The goal of energy management is to minimize energy consumption by deciding the power split between the engine and the motor. There are three main approaches to energy management: rule-based, optimization-based, and RL-based. Rulebased methods utilize the battery state of charge (SOC) and the power demand to define the power split rules [19], [20]. For HEVs, a typical rule-based method is Charge Sustaining (CS) wherein the battery SOC is sustained within a certain range; for PHEVs, an initial Charge Depleting (CD) is added before CS to deplete the battery SOC first. Optimization-based energy management utilizes trip information and vehicle powertrain and dynamics modeling to solve for the optimal power split [21], [22], [23], [24].\nRecently, RL has emerged as an alternative to classical optimization for HEV or PHEV energy management [25], [26]. With vehicle powertrain and dynamics modeling, RLbased energy management utilizes pre-defined drive cycles to train optimal power-split policies [27], [28], [29], [30]. Studies show that RL could provide near-optimal energy management compared to benchmark dynamic programming solutions [31], [32]. However, there is currently no literature addressing RLbased HEV or PHEV energy management during on-ramp merging.\nTo handle on-ramp merging for a PHEV, both cooptimization and sequential approaches can be used. In co-\n2 optimization, the upper-level vehicle behavior such as onramp merging and the lower-level PHEV energy management are simultaneously optimized. In sequential approaches, the upper-level vehicle behavior and the lower-level PHEV energy management are performed independently and in sequence. Co-optimization is shown to result in better fuel economy but higher jerk for cruise and car-following behaviors [33], [34], [35]. There is recent work on co-optimization for centralized on-ramp merging [36]. However, current work on co-optimization has only considered classical optimization to solve for the optimal solutions. To the best of our knowledge, there is no published work on HEV or PHEV co-optimization using DRL.\nThis work focuses on PHEV co-optimization using DRL for on-ramp merging. The co-optimization dictates that the upperlevel on-ramp merging and the lower-level PHEV energy management are simultaneously optimized via a single DRL framework such that the DRL policy directly outputs the power split. For sequential approaches, the upper-level DRL-based merging controller outputs the power or acceleration demand, which is then given to the lower-level rule-based PHEV energy management to decide the power split, see Fig. 1.\nThe contributions of this work include: (1) Co-optimization of the upper-level on-ramp merging and\nthe lower-level PHEV energy management using RL;\n(2) Comparison of co-optimization and sequential ap-\nproaches for PHEV on-ramp merging;\n(3) RL-based on-ramp merging with vehicle powertrain and\ndynamics considered.\nThis work allows us to evaluate if PHEV co-optimization using DRL is workable. It also enables us to observe the PHEV powertrain response during DRL-based on-ramp merging.\nThe rest of the paper is organized as follows: Section II presents the Control-Oriented Model (COM) of the PHEV, the 2015 Toyota Prius Plug-In. Section III illustrates the onramp merging environment in a driving simulator. Section IV explains the DRL framework for co-optimization. Section V presents the sequential approaches. Section VI shows the DRL training and testing results. Section VII draws conclusions and suggests future work."
        },
        {
            "heading": "II. PHEV CONTROL-ORIENTED MODEL",
            "text": "The PHEV COM is a simplified model of the PHEV that includes the powertrain configuration, battery, and vehicle dynamics modeling. The PHEV COM is established for studying\nthe PHEV powertrain response and energy consumption during on-ramp merging."
        },
        {
            "heading": "A. PHEV powertrain",
            "text": "Fig. 2. Powertrain configuration of the 2015 Toyota Prius Plug-In.\nFig. 2 shows the powertrain configuration of the 2015 Toyota Prius Plug-In. The PHEV has an engine, a battery, two planetary gears, and two motor-generators: MG1 and MG2. MG1 is mostly used as a generator to aid the engine to work in its optimal operation while MG2 is used as both a motor to propel the vehicle and a generator for regenerative braking. As the actual powertrain control algorithms are unknown, we base the COM on the powers of components: the engine power Peng, motor-generator combined power Pmg, friction brake power Pf bk, and power demand before transmission loss Pd . These powers are related by the following equation.\nPd = Peng +Pmg +Pf bk (1)\nThe battery power Pb is considered to be related to the\nmotor-generator combined power Pmg as follows.\nPb = Pmg/\u03b7m +Paux,Pmg \u2265 0\nPb = Pmg\u03b7g +Paux,Pmg < 0 (2)\nwhere \u03b7m is the motor efficiency when the combined effect of motor-generator is propelling the vehicle, \u03b7g is the generator efficiency when the combined effect of motor-generator is storing energy, and Paux is the auxiliary power needed for vehicle accessories which don\u2019t include air conditioning.\nThe engine fuel rate m\u0307 f is considered to have a linear relationship with the engine power Peng, assuming that the engine always runs in its optimal operation.\nm\u0307 f = a1Peng + a2,Peng > 0\nm\u0307 f = 0,Peng = 0 (3)\nwhere a1 and a2 are coefficients."
        },
        {
            "heading": "B. Battery model",
            "text": "The battery SOC is defined as \u02d9SOC = \u2212Ib/Qmax where Ib is the battery current and Qmax is the battery capacity. The battery is modeled as a simple circuit system with the battery power computed as Pb = IbVoc\u2212 I 2 b Rb where Voc is the battery\n3 open circuit voltage and Rb is the battery internal resistance. Thus, the battery SOC in terms of the battery power Pb is\n\u02d9SOC =\u2212 Voc \u2212\n\u221a\nV 2oc \u2212 4RbPb 2RbQmax\n(4)\nwhere Voc and Rb are considered functions of the SOC [12]. Quadratic functions are used to estimate their relationships.\nVoc = b1SOC 2 + b2SOC+ b3\nRb = c1SOC 2 + c2SOC+ c3\n(5)\nwhere b1, b2, b3, c1, c2 and c3 are coefficients. Eqns. 4 and 5 determine the battery SOC dynamics once Pb is known.\nC. Vehicle dynamics\nThe power demand before transmission loss Pd achieves the vehicle acceleration by overcoming resistant forces, which include the aerodynamic drag, the rolling resistance, and the gravity due to road grades.\nma = Pd\u03b7\nk t\nv \u2212 [0.5Cd\u03c1Av\n2 +(\u00b5 + \u00b52v)mgcos(\u03b8 )+mgsin(\u03b8 )]\nv\u0307 = a\nk = 1,Pd \u2265 0\nk =\u22121,Pd < 0 (6)\nwhere \u03b7t \u2208(0,1) represents the transmission efficiency and the superscript k is either 1 or -1 for non-negative or negative power demand, respectively. When Pd \u2265 0, \u03b7 k t = \u03b7t , which means that the non-negative power demand propels the vehicle with transmission loss. When Pd < 0, \u03b7 k t = \u03b7 \u22121 t , which means that the negative power demand results from vehicle braking with transmission loss. In Eqn. 6, m is the vehicle mass, a is the acceleration, v is the velocity, Cd is the aerodynamic drag coefficient, A is the vehicle frontal area, \u00b5 and \u00b52 are rolling coefficients, g is the gravity constant, and \u03b8 is the road grade in radians. In this work, we only consider flat roads with \u03b8 = 0."
        },
        {
            "heading": "D. Energy cost",
            "text": "The energy cost of a PHEV is the combination of both the fuel and electricity monetary costs [37]. The instantaneous energy cost during a time step \u2206t is computed as\nc = (k f m\u0307 f + ke Pb\n\u03b7b\u03b7chr )\u2206t (7)\nwhere k f is the unit price of fuel in US dollars (USD), m\u0307 f is the fuel rate, ke is the unit price of electricity power in USD, \u03b7b is the battery efficiency, \u03b7chr is the external charger efficiency. The unit prices k f = 0.93 USD/kg and ke = 0.13 USD/kWh are the US national averages in 2019.\nThe constraints of the vehicle variables are\n0 \u2264 Peng \u2264 Peng,max\nPbrk,min \u2264 Pbrk \u2264 0\nPg,min \u2264 Pmg \u2264 Pm,max\nPb,min \u2264 Pb \u2264 Pb,max\n(8)\nwhere Peng,max is the maximum engine power, Pbrk,min is the minimum friction brake power, Pg,min is the minimum generator power, Pm,max is the maximum motor power, and Pb,min and Pb,max are the minimum and maximum battery powers, respectively. The motor-generator power is constrained by not only its own limits but also the battery power limits. All the COM parameter values are determined according to a highfidelity vehicle model that has been validated against a 2015 Toyota Prius Plug-In in our previous work [38], [39]."
        },
        {
            "heading": "III. ON-RAMP MERGING ENVIRONMENT",
            "text": "The merging environment is created in the Simulation of Urban Mobility (SUMO) driving simulator for DRL training and testing, see Fig. 3. We consider a single-lane main road and a single-lane on-ramp. The main road has a speed limit of vlimit = 29.06m/s (65mph). There are intelligent vehicles on the main road that perform car following and collision avoidance based on the IDM. Each of the intelligent vehicles has a desired speed of \u03b1 vlimit where \u03b1 is a constant. \u03b1 is based on a Gaussian distribution with mean 1 and standard deviation 0.1 with values clipped within [0.85, 1.15]. Thus, the desired speeds of the intelligent vehicles are within [24.70,33.42]m/s (55-75mph). The intelligent vehicles are generated at the bottom of the main road with a probability of 0.5 per second. The different desired speeds, the probabilistic traffic generation, and the IDM parameter variances result in significantly different traffic densities at the merging junction at different time instants. The normal acceleration of the intelligent vehicles lies within [amin,amax] = [-4.5, 2.6]m/s 2. However, during emergency braking when the merging vehicle is too close, the acceleration can further decrease to -9m/s2.\n4 In this work, the merging vehicle is only controlled in a\ncontrol zone defined as 100m behind the merging point and 100m ahead of the merging point. The merging point is defined as the intersection between the main road and the on-ramp. We assume that there are no other vehicles in front of the merging vehicle on the ramp. The merging vehicle has a sensing radius of 200m which could be enabled by advanced perception sensors [40], [41]. The merging vehicle has no wireless communication and there is no centralized control or coordination.\nThe vehicles on the main road are designed to have the right of way and only interact with the merging vehicle when the merging vehicle enters the junction between the main road and the on-ramp. The junction is created by the SUMO driving simulator and covers the road blocks around 15m away from the merging point. When the merging vehicle enters the junction on the ramp, its projection on the main road is used by the main-road vehicles to perform car following based on the IDM. Its projection on the main road is defined to have the same distance to the merging point as it does on the ramp.\nFor every merging episode, the merging vehicle is generated 100m behind the merging point on the ramp (at the control zone bottom) with an initial speed randomly distributed in [22.35, 26.82]m/s (50-60mph). This is based on the recommendation by the US Department of Transportation that the merging vehicle reaches at least 50mph to merge onto a highway of 65mph speed limit [42]. The initial acceleration of the merging vehicle is zero. The initial battery SOC of the merging vehicle is randomly distributed in [0.3,0.9]. A merging episode terminates when the merging vehicle stops, collides with another vehicle, or reaches the destination which is 100m ahead of the merging point."
        },
        {
            "heading": "IV. DRL FRAMEWORK FOR CO-OPTIMIZATION",
            "text": "To provide the training dataset for DRL, the DRL environment state, the action for the merging vehicle, and the reward function are defined. In this section, we introduce the DRL framework for co-optimization."
        },
        {
            "heading": "A. Environment state",
            "text": "The DRL environment state S consists of the states of 5 vehicles: the merging vehicle or its projection, along with 2 preceding and 2 following vehicles. The projection of the merging vehicle has the same distance to the merging point as the merging vehicle, see Fig. 3. For the 2 preceding and 2 following vehicles, the states are their distances to the merging point and velocities. For the merging vehicle, the state includes its distance to the merging point d, velocity v, and acceleration a. The inclusion of the merging vehicle\u2019s velocity enables the calculation of its acceleration when the power demand is given, see Eqn. 6. The inclusion of acceleration enables the calculation of jerk since there is a DRL reward that penalizes high jerk, see the following Reward subsection. The inclusion of the battery SOC enables finding its relationship with the battery power, see Eqn. 4. Thus, the DRL environment state S is\nS = [dp2,vp2,dp1,vp1,d,v,a,SOC,d f 1,v f 1,d f 2,v f 2] (9)\nwhere p2 denotes the second preceding vehicle, p1 denotes the first preceding vehicle, f 1 denotes the first following vehicle, and f 2 denotes the second following vehicle.\nWhen there are fewer than 2 preceding or 2 following vehicles in the circular sensing range of the merging vehicle, virtual vehicles are introduced to deliberately construct the five-vehicle environment state S. The introduction of virtual vehicles is needed since DRL requires a fixed-size environment state. The virtual vehicles are positioned at the intersections between the merging vehicle\u2019s circular sensing space and the main lane, with velocities as the speed limit of the main lane and zero acceleration."
        },
        {
            "heading": "B. Action",
            "text": "For co-optimization, DRL directly outputs the power split. Therefore, the DRL action A includes the engine power Peng, and the combined power Pcb of the motor-generator and friction brake powers. That is,\nA = [Peng,Pcb]\nPcb = Pmg +Pf bk (10)\nWhen the combined power Pcb is positive, we set Pf bk = 0; then Pmg = Pcb, meaning that the motor propels the vehicle. When the combined power Pcb is negative, the vehicle brakes. During braking, the generator is utilized first to store the braking energy into the battery; the friction brake is only used when the braking power exceeds the generator power limit. With this rule for the combined power Pcb, we guarantee that regenerative braking is prioritized over friction brake for energy-saving. In contrast, if the motor-generator Pmg and friction brake Pf bk powers are outputted separately by DRL, it adds to the challenge for DRL to find the policy that perfectly prioritizes regenerative braking."
        },
        {
            "heading": "C. Reward",
            "text": "Several rewards are designed at each time step for the\ndesired merging performance:\n(1) The merging vehicle is encouraged to merge midway between two vehicles with the velocity of the first preceding vehicle vp1. We do not require the merging vehicle\u2019s velocity to be dependent on that of the first following vehicle because otherwise the braking of the first following vehicle leads to the braking of the merging vehicle. We define the following penalizing reward after the merging vehicle merges onto the main road:\nrm =\u2212wm(\u03bb + |vp1 \u2212 v|\n\u2206vmax ) (11)\nwhere wm is the weight, \u2206vmax = 5m/s is the maximum allowed velocity difference between the first preceding and the merging vehicles, and \u03bb is the midway ratio defined as follows:\n\u03bb = |ddp1 \u2212 dd f 1|\nddp1 + dd f 1 (12)\nwhere ddp1 is the inter-vehicular distance between the first preceding and the merging vehicles, and dd f 1 is the intervehicular distance between the merging and the first following\n5\nvehicles. When ddp1 = dd f 1, \u03bb = 0 and the merging vehicle merges exactly midway. When ddp1 = 0 or dd f 1 = 0, \u03bb = 1 and the merging vehicle has zero distance to either the first preceding or the first following vehicle.\n(2) Merging may result in congestion on the main road. Thus, we define a penalizing reward to reduce the braking of the first following vehicle.\nrb =\u2212wb |a f 1|\nmax(|amin|,amax) ,a f 1 < 0 (13)\nwhere wb is the weight and a f 1 is the acceleration of the first following vehicle.\n(3) For passenger comfort, we define a penalizing reward\nto reduce the jerk of the merging vehicle j.\nr j =\u2212w j | j|\u2212 j0 jmax \u2212 j0 , | j|> j0\nj = a\u0307\njmax = amax \u2212 amin\n\u2206t\n(14)\nwhere w j is the weight, \u2206t=0.1s is the time step, and j0=3m/s 3 is the maximum allowed absolute jerk value within which the passenger feels comfortable [43].\n(4) For economic merging, we define a penalizing reward to reduce the instantaneous energy cost of the merging vehicle c.\nrc =\u2212wc c\ncmax (15)\nwhere wc is the weight and cmax is the maximum energy cost at each time step. cmax is computed based on the instantaneous energy cost equation Eqn. 7 when the engine and battery powers are set at maximum.\n(5) A penalizing reward of rstop =-1 is given when the merging vehicle stops and the episode terminates.\n(6) A penalizing reward of rcollision =-1 is given when the merging vehicle collides with any vehicles and the episode terminates. Collision is defined when the inter-vehicular distance is smaller than 2.5m.\n(7) A reward of rsuccess =1 is given when the merging vehicle reaches the destination which is 100m ahead of the merging point and the episode terminates.\nNote that there is no weighting for the terminating rewards (5) to (7) above. The tuning of the weights for rewards (1) to (4) is based on trial and error. The weight tuning priority order is safety+traffic free flow, passenger comfort, and energy saving. Reward (1) demands merging midway, which contributes to safety. Reward (2) penalizes the braking of the first flowing vehicle for traffic free flow, which also contributes to safety between the first following and the merging vehicles. Thus, rewards (1) and (2) are tuned together for safety+traffic free flow. Particularly, we first set the weights for jerk and energy cost to zeros and tune the weights for rewards (1) and (2) for zero collisions and zero stops. Then we increase the weight for jerk to maximum while maintaining zero collisions and zero stops. Last, we increase the weight for energy cost to maximum while maintaining zero collisions, zero stops, and average jerk less than or equal to 1m/s3. Table I shows the reward weights."
        },
        {
            "heading": "V. SEQUENTIAL APPROACHES",
            "text": "In order to evaluate co-optimization, two sequential approaches are considered for comparison purposes. In the first sequential approach (sequential approach I), the upper-level DRL-based merging controller outputs the power demand, which is then given to the lower-level PHEV blended CD energy management to decide the power split. In the second sequential approach (sequential approach II), the upperlevel DRL-based merging controller outputs the acceleration demand, which is then given to the lower-level PHEV blended CD energy management to decide the power split.\nSince most published work utilizes acceleration demand as the output of the upper-level controller, sequential approach II may be better received in the current literature. Moreover, sequential approach II does not require powertrain information to design the upper-level merging controller, which makes it more general than sequential approach I and co-optimization.\nFor sequential approach I, since the upper-level DRL-based merging controller outputs the power demand, the DRL action is the power demand.\nA1 = Pd (16)\nThe rewards and their weights are the same as those for co-optimization except for the reward to penalize energy cost, which is defined as\nrc1 =\u2212wc Pd\nmax(|Pg,min +Pbrk,min|,Pm,max +Peng,max) (17)\nThe weight wc is the same as that for co-optimization.\nFor sequential approach II, since the upper-level DRL-based merging controller outputs the acceleration demand, the DRL action is the acceleration demand.\nA2 = ad\namin \u2264 ad \u2264 amax (18)\nThe rewards and their weights are the same as those for co-optimization except for the reward to penalize energy cost, which is defined as\nrc2 =\u2212wc ad\nmax(|amin|,amax) (19)\nThe weight wc is the same as that for co-optimization.\nFor both sequential approaches, the upper-level DRL training does not involve PHEV energy management. Thus, compared to co-optimization, the DRL environment state for both sequential approaches does not include the battery SOC.\nS12 = [dp2,vp2,dp1,vp1,d,v,a,d f 1,v f 1,d f 2,v f 2] (20)\nFor both sequential approaches, the lower-level PHEV energy management method is the blended CD mode. In blended\n6 CD, if the power demand can be supplied by the battery only, the engine is off and the vehicle is in electric drive mode. Otherwise, the engine is on and supplies all the power demand; if the engine cannot supply all the power demand, the battery makes up for the power insufficiency.\nThe blended CD mode prioritizes electric drive over engine power, which results in low energy costs as the cost of electricity is lower than that of fuel. We do not consider the CS mode since CS sometimes prioritizes engine power when the power demand is above a threshold, which results in higher energy costs. The blended CD mode allows us to evaluate if co-optimization could offer comparably low energy costs.\nIn our simulation, merging usually takes less than 10 seconds, so the resulting SOC change is very small. There is no concern of constraining the SOC above a threshold during the short-period merging for the sequential and co-optimization approaches."
        },
        {
            "heading": "VI. DRL TRAINING AND TESTING",
            "text": ""
        },
        {
            "heading": "A. The DRL algorithm",
            "text": "RL can be formulated as a Markov Decision Process. That is, at time step t, an RL agent observes the environment state st , performs an action at based on its policy, and subsequently receives a reward rt as the environment moves to the next state st+1. The goal of RL is to learn a policy that maximizes the cumulative discounted reward \u2211t=Tt=0 \u03b3 trt where T is the total time step and \u03b3 is the discount factor. The DRL algorithm used here is Soft Actor-Critic (SAC) [44]. SAC is an entropy-regulated actor-critic algorithm wherein the RL agent is given an additional reward related to the entropy of the policy. The actor is the policy \u03c0\u03c6 with the deep neural net parameters \u03c6 and the critic is the Q-value Q\u03b8 with the deep neural net parameters \u03b8 . Q-value is defined as the cumulative discounted reward from the current time step t to the total time step T : Q\u03b8 (st ,at) = \u2211 \u03c4=T \u03c4=t \u03b3\n\u03c4\u2212tr\u03c4 where \u03c4 denotes a time step between t and T .\nThe critic network \u03b8 is trained according to Bellman\u2019s optimality principle by minimizing the root-mean-squared loss Lt = {rt + \u03b3[Q\u03b8 (st+1,\u03c0\u03c6 (st+1))\u2212\u03b1log\u03c0\u03c6 (st+1)]\u2212Q\u03b8 (st ,at)} 2 where \u03b1 is the weight for the entropy reward that is the log value of the policy. The policy network \u03c6 is trained by maximizing [Q\u03b8 (st ,\u03c0\u03c6 (st))\u2212 \u03b1log\u03c0\u03c6 (st )]. Techniques including target networks, mini-batch gradient descent/ascent, and batch normalization [45] are used to improve training stability and convergence. Additionally, Double Q-Learning is also used to reduce Q-value over-estimation [46].\nThe entropy regularization in SAC results in stochasticity and exploration, which leads to better optimum seeking and training stability. Application of SAC to complex tasks shows that SAC is better than some of the well-known DRL algorithms such as Deep Deterministic Policy Gradient (DDPG) [47] and Proximal Policy Optimization [48]. For more details, readers are encouraged to read the original SAC paper [44]."
        },
        {
            "heading": "B. DRL training",
            "text": "For the co-optimization and two sequential approaches, the merging environment setting is the same. The DRL training is\nbased on episodes. Each episode starts as the merging vehicle is generated 100m behind the merging point on the ramp, and terminates at 100m ahead of the merging point if merging is successful, or in the control zone when there are stops or collisions. The total training time is 1 million time steps for each approach, which result in roughly 12500 to 13000 episodes. Before each episode starts, there is a 10-second buffer for traffic initialization on the main road.\nFor SAC training for each approach, both the actor and critic networks have 2 hidden layers with 64 neurons for each hidden layer. On a desktop computer with a 16-core (32-thread) AMD processor and a GeForce RTX 2080Ti GPU, the training took around 6 hours.\nThe DRL training was not robust since training the same approach could lead to noticeably different testing results. Thus, we trained each approach twice and chose the trained policy that resulted in zero collisions, zero stops, and better values of the evaluation metrics defined in the following DRL testing section. Fig. 4 shows the undiscounted episodic reward during training for the co-optimization approach. The undiscounted episodic rewards for the two sequential approaches look similar and are not plotted here. From Fig. 4, we see that the undiscounted episodic reward converges during training."
        },
        {
            "heading": "C. DRL testing",
            "text": "Testing is performed on the same merging environment used for training. In other words, the trained policies are not tested in unseen environments. However, each episode has a unique initial condition, which means that each episode is not the same. For each approach, the testing time is also 1 million time steps, which took around 4 hours on the same computer used for training.\nIn the following, we first present two testing episodes to showcase the merging dynamics and the PHEV powertrain response; then we summarize the testing results to compare the co-optimization and the two sequential approaches. The two testing episodes presented include: (1) A testing episode for co-optimization. In this episode, the merging vehicle maintains its relative positioning order between the first preceding and first following vehicles during the entire merging process, i.e.,\n7 merges ahead; (2) A testing episode for sequential approach II. In this episode, the merging vehicle merges behind the first following vehicle. In addition, this episode presents the issue of power limit saturation in which the acceleration demand is not achievable due to the PHEV engine and battery power limits.\n1) A testing episode for co-optimization: The testing episode for co-optimization is shown in Fig. 5. From the plot of distance to the merging point d, the merging vehicle merges ahead the first following vehicle. Additionally, the merging vehicle is closer to the first preceding vehicle than the first following vehicle. Even though one of the rewards dictates that the merging vehicle merges in the middle between two vehicles, the other reward that penalizes the braking of the first following vehicle dictates that the merging vehicle merges away from the first following vehicle. The combined effect results in the merging vehicle being closer to the first preceding vehicle in this episode.\nFrom the plot of velocity v, the merging vehicle decelerates first, and then accelerates to reach the speed of the first preceding vehicle as dictated by one of the rewards.\nThe plot of acceleration a and jerk j shows that the merging vehicle has mild acceleration and jerk, indicating comfortable merging. The first following vehicle has very mild braking, indicating neglectable influence on the traffic flow. The first following vehicle\u2019s response to the merging vehicle is based on the IDM whose parameters are adjustable for different driving styles. Hence the braking of the first following vehicle is subjective evaluation.\nThe plot of the powertrain powers shows negative power demand during initial deceleration and positive power demand during acceleration. Friction brake power remains zero during the entire episode. During the initial deceleration, regenerative braking stores energy into the battery and hence negative battery power. The engine power is not zero during braking, possibly because SAC does not lead to zero action values. The non-zero engine power during braking results in more fuel consumption, indicating that the SAC policy is not optimal with regards to energy saving. During acceleration, the engine and battery co-power the vehicle.\nThe battery SOC increases during initial regenerative braking and decreases during acceleration. The instantaneous energy cost c decreases during initial regenerative braking due to energy storage and increases as both the engine and battery power the vehicle.\n2) A testing episode for sequential approach II: Fig. 6 shows the testing episode for sequential approach II wherein the upper-level DRL merging policy outputs the acceleration demand. From the plot of distance to the merging point d, the merging vehicle merges behind. The merging vehicle\u2019s initial distance to the first following vehicle is relatively small, which may contribute to the decision of merging behind through learning.\nFrom the plot of velocity v, the merging vehicle decelerates initially for the merging behind. Then it accelerates to match the speed of the first preceding vehicle as dictated by one of the rewards.\nThe merging vehicle\u2019s jerk j has large absolute values initially for a short period of time, which results in passenger discomfort in this episode. Note that the issue of shortperiod large jerk exists for all three (co-optimization and two sequential) approaches.\nFrom the plot of the powertrain powers, the vehicle brakes hard initially and regenerative braking is not able to provide all the braking power. Thus, friction braking is utilized as a supplement. There is power limit saturation of the power demand from 3.2s to 5s during acceleration, evidenced by both the engine and the battery reaching their power limits.\nThe battery SOC increases during initial regenerative braking and then decreases when the battery is utilized to power the vehicle. Correspondingly, the instantaneous energy cost decreases initially and increases when the battery and the engine co-power the vehicle.\n3) Testing results summary: We define the following evaluation metrics to summarize the results of the co-optimization and the two sequential approaches.\n(1) Power limit saturation rate. Power limit saturation rate is the number of episodes wherein the power demand is not achievable due to the powertrain power limits, divided by the total number of testing episodes. For sequential approach II, the acceleration demand can be transformed into power demand via the vehicle dynamics equation Eqn. 6.\n(2) Collision rate. Collision rate is the number of episodes wherein the merging vehicle collides with another vehicle, divided by the total number of testing episodes.\n(3) Average episodic energy cost in USD. Average episodic energy cost is the average of episodic energy costs over the total number of testing episodes. The energy cost is computed based on the instantaneous monetary energy cost definition in Eqn. 7. The average episodic fuel and electricity costs are also computed separately to evaluate the power splits.\n(4) Average jerk. Average jerk is defined as the average of the mean jerk of each episode over the total number of testing episodes.\n(5) Merge-behind rate. Merge-behind rate is the number of episodes wherein the merging vehicle merges behind the first following vehicle, divided by the total number of testing episodes. Merging behind shows the decision-making of the merging vehicle.\nA summary of the testing results is shown in Table II. Note\nthat all the policies result in no stops.\nThe power limit saturation rate is zero for co-optimization since co-optimization outputs the power split whose limits are inherently considered in the DRL framework. For sequential approach I wherein the upper-level merging policy outputs the power demand, the power limit saturation rate is also zero since the power limits are also inherently considered. For sequential approach II wherein the upper-level merging policy outputs the acceleration demand, the power demand may exceed the power limits as shown by the 25% power limit saturation rate in Table II. When power limit saturation happens, the acceleration demand is not achieved, resulting in reduced actual acceleration magnitudes.\nThe collision rates are zero for co-optimization and sequential approach I. For sequential approach II, collisions happen\n8 0 1 2 3 4 5 6 7 -300 -200 -100 0 100 200 300 0 1 2 3 4 5 6 7 21 22 23 24 25 26 27 28 29 0 1 2 3 4 5 6 7 -14 -12 -10 -8 -6 -4 -2 0 2 4 6 8\n0 1 2 3 4 5 6 7 -100\n-50\n0\n50\n100\n0 1 2 3 4 5 6 7\n0.788\n0.79\n0.792\n0.794\n0.796\n0.798\n0 1 2 3 4 5 6 7\n0\n5\n10\n15\n10-3\n9 at times due to power limit saturation and subsequent reduced merging performance. Note that when we test the DRL policy in another simulation where there are no power limits, the collision rate is zero for sequential approach II.\nSequential approach I has a higher energy cost than those of co-optimization and sequential approach II. This indicates that co-optimization finds economic power splits. However, the average episodic electricity cost for co-optimization is negative, suggesting that the trained co-optimization policy makes use of regenerative braking to harness energy.\nThe average jerk is less than or equal to 1m/s3 for all cases, suggesting comfortable merging on average. However, the average jerk for co-optimization is higher than those for the two sequential approaches.\nThe merge-behind rates are only between 6.9% to 8.7% for all approaches. For majority of the episodes, the merging vehicle merges ahead the first following vehicle. Note that, in very limited episodes (< 0.01%), the merging vehicle merges ahead of the first preceding vehicle. This may in turn suggest the consideration of 2 vehicles ahead and behind in the DRL environment state for decision-making."
        },
        {
            "heading": "VII. CONCLUSIONS",
            "text": "This work considers DRL-based on-ramp merging for a PHEV for the first time. We also consider DRL-based cooptimization for the first time and compare it with sequential approaches. In co-optimization, the on-ramp merging and the PHEV energy management are simultaneously optimized in a single DRL framework, directly outputting the PHEV power split for on-ramp merging. In sequential approaches, the upper-level DRL on-ramp merging policy outputs either the power or acceleration demand, which is utilized by the lower-level blended CD PHEV energy management for power split decisions.\nThe testing results show that the sequential approach II wherein the upper-level merging policy outputs the acceleration demand could lead to collisions if the acceleration demand cannot achieved due to PHEV power limit constraints. For cooptimization and the sequential approach I wherein the upperlevel merging policy outputs the power demand, the power limits are inherently considered and the DRL-trained policies result in collision-free merging. This stresses the importance of considering power limit constraints in designing upper-level automated driving controllers.\nCo-optimization offers comparable fuel economy compared to sequential approaches with the blended CD energy management. However, in co-optimization, the merging vehicle overwhelmingly performs regenerative braking for energy recuperation, suggesting that the merging behavior of cooptimization is different than that of the sequential approaches wherein the battery energy is depleted. We comment that since merging is a short-period event, energy saving for a single vehicle is secondary compared to safety.\nThe average jerk for co-optimization is higher than those for the sequential approaches. This is consistent with findings in the literature that co-optimization results in higher jerk. As suggested by the literature, the higher jerk could be a tradeoff with energy saving [35].\nThe merging vehicle merges ahead in the majority of the episodes for the co-optimization and two sequential approaches, suggesting possibly aggressive merging. We believe that the decision to merge ahead or behind results from the designed rewards and the initial conditions.\nAlthough we do not test the neural net solution on production vehicles, our previous work has suggested the realtime applicability of the same neural net size on automotive Electronic Control Units (ECUs). For the neural net size (2 hidden layers with 64 neurons each) considered, the execution time is an order of magnitude smaller than that of MPC with a linear model on a desktop computer [8]. And MPC has been shown to be deployable in real time on automotive ECUs [17], [37], [43]. Thus, our neural net solution for on-ramp merging could satisfy the computational requirement for realtime deployment on production vehicles.\nFuture work includes testing the trained policies on real or scale vehicles [49]. We would also consider other DRL algorithms such as Twin-Delayed DDPG [50] for training."
        }
    ],
    "title": "Co-Optimization of On-Ramp Merging and Plug-In Hybrid Electric Vehicle Power Split Using Deep Reinforcement Learning",
    "year": 2022
}