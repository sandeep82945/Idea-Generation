{
    "abstractText": "In various learning-based image restoration tasks, such as image denoising and image super-resolution, the degradation representations were widely used to model the degradation process and handle complicated degradation patterns. However, they are less explored in learning-based image deblurring as blur kernel estimation cannot perform well in real-world challenging cases. We argue that it is particularly necessary for image deblurring to model degradation representations since blurry patterns typically show much larger variations than noisy patterns or high-frequency textures. In this paper, we propose a framework to learn spatially adaptive degradation representations of blurry images. A novel joint image reblurring and deblurring learning process is presented to improve the expressiveness of degradation representations. To make learned degradation representations effective in reblurring and deblurring, we propose a Multi-Scale Degradation Injection Network (MSDI-Net) to integrate them into the neural networks. With the integration, MSDI-Net can handle various and complicated blurry patterns adaptively. Experiments on the GoPro and RealBlur datasets demonstrate that our proposed deblurring framework with the learned degradation representations outperforms state-of-the-art methods with appealing improvements. The code is released at https://github.com/ dasongli1/Learning_degradation.",
    "authors": [
        {
            "affiliations": [],
            "name": "Dasong Li"
        },
        {
            "affiliations": [],
            "name": "Yi Zhang"
        },
        {
            "affiliations": [],
            "name": "Ka Chun Cheung"
        },
        {
            "affiliations": [],
            "name": "Xiaogang Wang"
        },
        {
            "affiliations": [],
            "name": "Hongwei Qin"
        },
        {
            "affiliations": [],
            "name": "Hongsheng Li"
        }
    ],
    "id": "SP:29b576e598a743a91b57e77d3bde58969d8cb4bb",
    "references": [
        {
            "authors": [
                "T. Chan",
                "C.K. Wong"
            ],
            "title": "Total variation blind deconvolution",
            "venue": "IEEE Transactions on Image Processing 7(3), 370\u2013375",
            "year": 1998
        },
        {
            "authors": [
                "H. Chen",
                "J. Gu",
                "O. Gallo",
                "M.Y. Liu",
                "A. Veeraraghavan",
                "J. Kautz"
            ],
            "title": "Reblur2deblur: Deblurring videos via self-supervised learning",
            "venue": "2018 IEEE International Conference on Computational Photography (ICCP). pp. 1\u20139",
            "year": 2018
        },
        {
            "authors": [
                "L. Chen",
                "X. Lu",
                "J. Zhang",
                "X. Chu",
                "C. Chen"
            ],
            "title": "Hinet: Half instance normalization network for image restoration",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops. pp. 182\u2013192",
            "year": 2021
        },
        {
            "authors": [
                "S.J. Cho",
                "S.W. Ji",
                "J.P. Hong",
                "S.W. Jung",
                "S.J. Ko"
            ],
            "title": "Rethinking coarse-to-fine approach in single image deblurring",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 4641\u20134650",
            "year": 2021
        },
        {
            "authors": [
                "S. Cho",
                "Y. Matsushita",
                "S. Lee"
            ],
            "title": "Removing non-uniform motion blur from images",
            "venue": "2007 IEEE 11th International Conference on Computer Vision. pp. 1\u20138",
            "year": 2007
        },
        {
            "authors": [
                "H. Gao",
                "X. Tao",
                "X. Shen",
                "J. Jia"
            ],
            "title": "Dynamic scene deblurring with parameter selective sharing and nested skip connections",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 3848\u20133856",
            "year": 2019
        },
        {
            "authors": [
                "I. Goodfellow",
                "J. Pouget-Abadie",
                "M. Mirza",
                "B. Xu",
                "D. Warde-Farley",
                "S. Ozair",
                "A. Courville",
                "Y. Bengio"
            ],
            "title": "Generative adversarial nets",
            "venue": "Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N., Weinberger, K.Q. (eds.) Advances in Neural Information Processing Systems. vol. 27. Curran Associates, Inc.",
            "year": 2014
        },
        {
            "authors": [
                "S. Guo",
                "Z. Yan",
                "K. Zhang",
                "W. Zuo",
                "L. Zhang"
            ],
            "title": "Toward convolutional blind denoising of real photographs",
            "venue": "2019 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2019
        },
        {
            "authors": [
                "J. Johnson",
                "A. Alahi",
                "L. Fei-Fei"
            ],
            "title": "Perceptual losses for real-time style transfer and super-resolution",
            "venue": "European Conference on Computer Vision",
            "year": 2016
        },
        {
            "authors": [
                "D.P. Kingma",
                "M. Welling"
            ],
            "title": "Auto-Encoding Variational Bayes",
            "venue": "2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 1416, 2014, Conference Track Proceedings",
            "year": 2014
        },
        {
            "authors": [
                "D. Krishnan",
                "R. Fergus"
            ],
            "title": "Fast image deconvolution using hyper-laplacian priors",
            "venue": "Bengio, Y., Schuurmans, D., Lafferty, J., Williams, C., Culotta, A. (eds.) Advances in Neural Information Processing Systems. vol. 22. Curran Associates, Inc.",
            "year": 2009
        },
        {
            "authors": [
                "O. Kupyn",
                "V. Budzan",
                "M. Mykhailych",
                "D. Mishkin",
                "J. Matas"
            ],
            "title": "Deblurgan: Blind motion deblurring using conditional adversarial networks",
            "venue": "CVPR. pp. 8183\u2013 8192. Computer Vision Foundation / IEEE Computer Society",
            "year": 2018
        },
        {
            "authors": [
                "O. Kupyn",
                "T. Martyniuk",
                "J. Wu",
                "Z. Wang"
            ],
            "title": "Deblurgan-v2: Deblurring (ordersof-magnitude) faster and better",
            "venue": "ICCV. pp. 8877\u20138886. IEEE",
            "year": 2019
        },
        {
            "authors": [
                "A. Levin",
                "Y. Weiss",
                "F. Durand",
                "W.T. Freeman"
            ],
            "title": "Understanding and evaluating blind deconvolution algorithms",
            "venue": "2009 IEEE Conference on Computer Vision and Pattern Recognition. pp. 1964\u20131971",
            "year": 2009
        },
        {
            "authors": [
                "D. Li",
                "Y. Zhang",
                "K.L. Law",
                "X. Wang",
                "H. Qin",
                "H. Li"
            ],
            "title": "Efficient burst raw denoising with variance stabilization and multi-frequency denoising network",
            "venue": "International Journal of Computer Vision 130(8), 2060\u20132080",
            "year": 2022
        },
        {
            "authors": [
                "J. Liang",
                "G. Sun",
                "K. Zhang",
                "L. Van Gool",
                "R. Timofte"
            ],
            "title": "Mutual affine network for spatially variant kernel estimation in blind image super-resolution",
            "venue": "IEEE International Conference on Computer Vision",
            "year": 2021
        },
        {
            "authors": [
                "J.H. Lim",
                "J.C. Ye"
            ],
            "title": "Geometric gan (2017)",
            "year": 2017
        },
        {
            "authors": [
                "G. Liu",
                "S. Chang",
                "Y. Ma"
            ],
            "title": "Blind image deblurring using spectral properties of convolution operators",
            "venue": "IEEE Transactions on Image Processing 23(12), 5047\u20135056",
            "year": 2014
        },
        {
            "authors": [
                "I. Loshchilov",
                "F. Hutter"
            ],
            "title": "SGDR: stochastic gradient descent with warm restarts",
            "venue": "5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings",
            "year": 2017
        },
        {
            "authors": [
                "R. Mechrez",
                "I. Talmi",
                "L. Zelnik-Manor"
            ],
            "title": "The contextual loss for image transformation with non-aligned data",
            "venue": "arXiv preprint arXiv:1803.02077",
            "year": 2018
        },
        {
            "authors": [
                "B. Mildenhall",
                "J.T. Barron",
                "J. Chen",
                "D. Sharlet",
                "R. Ng",
                "R. Carroll"
            ],
            "title": "Burst denoising with kernel prediction networks",
            "venue": "CVPR",
            "year": 2018
        },
        {
            "authors": [
                "T. Miyato",
                "T. Kataoka",
                "M. Koyama",
                "Y. Yoshida"
            ],
            "title": "Spectral normalization for generative adversarial networks",
            "venue": "International Conference on Learning Representations",
            "year": 2018
        },
        {
            "authors": [
                "J.G. Nagy",
                "D.P. O\u2019Leary"
            ],
            "title": "Restoring images degraded by spatially variant blur",
            "venue": "SIAM Journal on Scientific Computing 19(4), 1063\u20131082",
            "year": 1998
        },
        {
            "authors": [
                "S. Nah",
                "T.H. Kim",
                "K.M. Lee"
            ],
            "title": "Deep multi-scale convolutional neural network for dynamic scene deblurring",
            "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2017
        },
        {
            "authors": [
                "A. Odena",
                "V. Dumoulin",
                "C. Olah"
            ],
            "title": "Deconvolution and checkerboard artifacts",
            "venue": "Distill",
            "year": 2016
        },
        {
            "authors": [
                "J. Pan",
                "D. Sun",
                "H. Pfister",
                "M.H. Yang"
            ],
            "title": "Blind image deblurring using dark channel prior",
            "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 1628\u20131636",
            "year": 2016
        },
        {
            "authors": [
                "D. Park",
                "D.U. Kang",
                "J. Kim",
                "S.Y. Chun"
            ],
            "title": "Multi-temporal recurrent neural networks for progressive non-uniform single image deblurring with incremental temporal training",
            "venue": "ECCV (6). Lecture Notes in Computer Science, vol. 12351, pp. 327\u2013343. Springer",
            "year": 2020
        },
        {
            "authors": [
                "T. Park",
                "M.Y. Liu",
                "T.C. Wang",
                "J.Y. Zhu"
            ],
            "title": "Semantic image synthesis with spatially-adaptive normalization",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "year": 2019
        },
        {
            "authors": [
                "D. Ren",
                "K. Zhang",
                "Q. Wang",
                "Q. Hu",
                "W. Zuo"
            ],
            "title": "Neural blind deconvolution using deep priors",
            "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 3338\u20133347. IEEE Computer Society, Los Alamitos, CA, USA",
            "year": 2020
        },
        {
            "authors": [
                "J. Rim",
                "H. Lee",
                "J. Won",
                "S. Cho"
            ],
            "title": "Real-world blur dataset for learning and benchmarking deblurring algorithms",
            "venue": "ECCV (25). Lecture Notes in Computer Science, vol. 12370, pp. 184\u2013201. Springer",
            "year": 2020
        },
        {
            "authors": [
                "O. Ronneberger",
                "P. Fischer",
                "T. Brox"
            ],
            "title": "U-net: Convolutional networks for biomedical image segmentation",
            "venue": "Navab, N., Hornegger, J., Wells, W.M., Frangi, A.F. (eds.) Medical Image Computing and Computer-Assisted Intervention \u2013 MICCAI 2015",
            "year": 2015
        },
        {
            "authors": [
                "C.J. Schuler",
                "M. Hirsch",
                "S. Harmeling",
                "B. Sch\u00f6lkopf"
            ],
            "title": "Learning to deblur",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 38, 1439\u20131451",
            "year": 2016
        },
        {
            "authors": [
                "Q. Shan",
                "W. Xiong",
                "J. Jia"
            ],
            "title": "Rotational motion deblurring of a rigid object from a single image",
            "venue": "2007 IEEE 11th International Conference on Computer Vision. pp. 1\u20138",
            "year": 2007
        },
        {
            "authors": [
                "H. Son",
                "J. Lee",
                "J. Lee",
                "S. Cho",
                "S. Lee"
            ],
            "title": "Recurrent video deblurring with blurinvariant motion estimation and pixel volumes",
            "venue": "ACM Transactions on Graphics (TOG) 40(5)",
            "year": 2021
        },
        {
            "authors": [
                "Starck",
                "J.L",
                "F. Murtagh",
                "A. Bijaoui"
            ],
            "title": "Image processing and data analysis",
            "venue": "Cambridge University Press, Cambridge",
            "year": 1998
        },
        {
            "authors": [
                "M. Suin",
                "K. Purohit",
                "A.N. Rajagopalan"
            ],
            "title": "Spatially-attentive patch-hierarchical network for adaptive motion deblurring",
            "venue": "CVPR. pp. 3603\u20133612. Computer Vision Foundation / IEEE",
            "year": 2020
        },
        {
            "authors": [
                "J. Sun",
                "W. Cao",
                "Z. Xu",
                "J. Ponce"
            ],
            "title": "Learning a convolutional neural network for non-uniform motion blur removal",
            "venue": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 769\u2013777",
            "year": 2015
        },
        {
            "authors": [
                "X. Tao",
                "H. Gao",
                "X. Shen",
                "J. Wang",
                "J. Jia"
            ],
            "title": "Scale-recurrent network for deep image deblurring",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2018
        },
        {
            "authors": [
                "P. Tran",
                "A. Tran",
                "Q. Phung",
                "M. Hoai"
            ],
            "title": "Explore image deblurring via encoded blur kernel space",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2021
        },
        {
            "authors": [
                "D. Ulyanov",
                "A. Vedaldi",
                "V. Lempitsky"
            ],
            "title": "Deep image prior",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2018
        },
        {
            "authors": [
                "L. Wang",
                "Y. Wang",
                "X. Dong",
                "Q. Xu",
                "J. Yang",
                "W. An",
                "Y. Guo"
            ],
            "title": "Unsupervised degradation representation learning for blind super-resolution",
            "venue": "CVPR",
            "year": 2021
        },
        {
            "authors": [
                "T.C. Wang",
                "M.Y. Liu",
                "J.Y. Zhu",
                "A. Tao",
                "J. Kautz",
                "B. Catanzaro"
            ],
            "title": "Highresolution image synthesis and semantic manipulation with conditional gans",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "year": 2018
        },
        {
            "authors": [
                "Y. Wang",
                "H. Huang",
                "Q. Xu",
                "J. Liu",
                "Y. Liu",
                "J. Wang"
            ],
            "title": "Practical deep raw image denoising on mobile devices",
            "venue": "European Conference on Computer Vision (ECCV). pp. 1\u201316",
            "year": 2020
        },
        {
            "authors": [
                "O. Whyte",
                "J. Sivic",
                "A. Zisserman",
                "J. Ponce"
            ],
            "title": "Non-uniform deblurring for shaken images",
            "venue": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. pp. 491\u2013498",
            "year": 2010
        },
        {
            "authors": [
                "Xintao Wang",
                "C.D. Ke Yu",
                "C.C. Loy"
            ],
            "title": "Recovering realistic texture in image superresolution by deep spatial feature transform",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2018
        },
        {
            "authors": [
                "L. Xu",
                "S. Zheng",
                "J. Jia"
            ],
            "title": "Unnatural l0 sparse representation for natural image deblurring",
            "venue": "2013 IEEE Conference on Computer Vision and Pattern Recognition. pp. 1107\u20131114",
            "year": 2013
        },
        {
            "authors": [
                "S.W. Zamir",
                "A. Arora",
                "S. Khan",
                "M. Hayat",
                "F.S. Khan",
                "M.H. Yang",
                "L. Shao"
            ],
            "title": "Multi-stage progressive image restoration",
            "venue": "CVPR",
            "year": 2021
        },
        {
            "authors": [
                "H. Zhang",
                "I. Goodfellow",
                "D. Metaxas",
                "A. Odena"
            ],
            "title": "Self-attention generative adversarial networks",
            "venue": "Chaudhuri, K., Salakhutdinov, R. (eds.) Proceedings of the 36th International Conference on Machine Learning. Proceedings of Machine Learning Research, vol. 97, pp. 7354\u20137363. PMLR",
            "year": 2019
        },
        {
            "authors": [
                "H. Zhang",
                "Y. Dai",
                "H. Li",
                "P. Koniusz"
            ],
            "title": "Deep stacked hierarchical multi-patch network for image deblurring",
            "venue": "CVPR. pp. 5978\u20135986. Computer Vision Foundation / IEEE",
            "year": 2019
        },
        {
            "authors": [
                "J. Zhang",
                "J. Pan",
                "J. Ren",
                "Y. Song",
                "L. Bao",
                "R.W. Lau",
                "M.H. Yang"
            ],
            "title": "Dynamic scene deblurring using spatially variant recurrent neural networks",
            "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 2521\u2013 2529",
            "year": 2018
        },
        {
            "authors": [
                "K. Zhang",
                "W. Zuo",
                "L. Zhang"
            ],
            "title": "Ffdnet: Toward a fast and flexible solution for CNN based image denoising",
            "venue": "IEEE Transactions on Image Processing",
            "year": 2018
        },
        {
            "authors": [
                "K. Zhang",
                "W. Luo",
                "Y. Zhong",
                "L. Ma",
                "B. Stenger",
                "W. Liu",
                "H. Li"
            ],
            "title": "Deblurring by realistic blurring",
            "venue": "CVPR. pp. 2734\u20132743. Computer Vision Foundation / IEEE",
            "year": 2020
        },
        {
            "authors": [
                "S. Zhou",
                "J. Zhang",
                "J. Pan",
                "H. Xie",
                "W. Zuo",
                "J. Ren"
            ],
            "title": "Spatio-temporal filter adaptive network for video deblurring",
            "venue": "Proceedings of the IEEE International Conference on Computer Vision",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "Keywords: Image Deblurring, Degradation Representations"
        },
        {
            "heading": "1 Introduction",
            "text": "Image restoration is required to handle various and complicated degradation patterns produced in different degradation processes. The degradation representations act as a crucial component to model the degradation processes and handle complicated degradation patterns, such as different noise levels in image denoising [51,8,21] and different combinations of Gaussian blurs and motion blurs in blind super-resolution [41]. However, the degradation representations are less exploited in learning-based deblurring methods and have not been well integrated into state-of-the-art deblurring networks.\n\u2020Corresponding authors\nar X\niv :2\n20 8.\n05 24\n4v 1\n[ cs\n.C V\n] 1\n0 A\nug 2\nThe general blurring process can be formulated as\ny = F (x, k) + \u03b7, (1)\nwhere x and y are sharp image and blurry image respectively. F (x, k) is usually modeled as a blurring operator with kernel k. \u03b7 represents the Gaussian noise.\nA popular paradigm for image deblurring is based on the Maximum A Posterior (MAP) estimate framework,\n(k, x) = argmaxP(y|x, k)P(x)P(k), (2)\nwhere P(x) and P(k) model the priors of the clean images and the blur kernels. Many handcrafted priors for modeling P(x) and P(k) have been proposed [26,18,11,1]. But most of them are insufficient in characterizing the clean images and blur kernels accurately. Furthermore, the operator F is generally modeled as a convolution operation in conventional MAP frameworks, which does not hold in practice and causes unpleasing artifacts on real-world challenging cases.\nBased on the limitations of kernel-based blurring modeling, a series of kernelfree approaches [6,38,3,47,4] are proposed to directly learn the mapping from blurry images to corresponding sharp images. While those methods outperform previous deblurring methods significantly, their performances are still limited in complicated blurry patterns, due to the lack of explicit modeling of the degradation process. This is because, unlike denoising methods, where the noise level might be similar across different images, blurring of different images generally have totally different patterns and cannot be well handled by fixed-weight networks without considering the degradation process. To combine the modeling of degradation and learning-based deblurring, recent works [39,29] propose to learn explicit degradation representations by using Deep Image Prior (DIP) [40] to reparameterize the kernel k and the sharp image x. This inevitably involves the time-consuming iterative inverse optimization and hyperparameter tuning of DIP to adapt to the deblurring process. Moreover, degradation representations have not been taken as a common component in SOTA deblurring methods [3,47].\nIn this paper, we propose to learn explicit degradation representations with a novel joint sharp-to-blurry image reblurring and blurry-to-sharp image deblurring learning framework. Specifically, the degradation representations are learned in the process of sharp-to-blurry image reblurring. The process takes as input a blurry image and learns the degradation representations as a multi-channel spatial latent map to encode the spatially varying blur patterns in replacement of the conventional convolutional blur kernels or the DIP prior. A reblurring generator then takes as input the latent degradation map and the original sharp image and reblurs the sharp image back to its corresponding blurry image.\nTo effectively integrate the learned degradation representations into the reblurring process, we introduce a multi-scale degradation injection network (MSDINet) for achieving conditional image reblurring. The network adopts a U-Net like architecture [31]. The sharp image is fed into the encoder of the U-Net but the degradation is input into the encoder-decoder via the skip-connections to modulate the shortcut encoder feature maps. Specifically, the latent degradation\nmap is gradually upsampled via nearest-neighbor interpolation and a convolution layer to multiple resolutions and are then used to predict spatially varying weighting and bias parameters of the shortcut feature maps at each corresponding resolution. In this way, the learning of the latent degradation representation is supervised by the original blurry image for sharp-to-blurry image reblurring.\nTo make the learned degradation representations contributing to image deblurring, another blurry-to-sharp image generator network is also introduced, which shares the same MSDI-Net architecture but does not share weights with the reblurring generator. The learned degradation representations are processed similarly to deblur a blurry input image. Specifically, its encoder takes the blurry image as input, while the latent degradation representations are used to modulate the blurry image\u2019s encoder-decoder shortcut feature maps at multiple resolutions. With the help of learned degradation map, the deblurring generator can handle complicated spatially varying blurry patterns, which are adaptively learned from the data to optimize both reblurring and deblurring tasks.\nThe main contributions of this work are two-fold: 1) We propose a novel joint framework for learning both sharp-to-blurry image reblurring and blurryto-sharp image deblurring to adaptively encode spatially varying degradations and model the image blurring process, which in turn, benefits the image deblurring performance. 2) The proposed joint reblurring and deblurring framework outperforms state-of-the-art image deblurring methods on the widely used GoPro [24] and RealBlur [30] datasets."
        },
        {
            "heading": "2 Related Work",
            "text": "In this section, we briefly talk about the related works of image restoration with degradation representations and different image deblurring methods. Image restoration with degradation representations. Image restoration tasks are usually required to handle different and complicated degradations on real-world applications. The degradation representations have been exploited and taken as one crucial component in several image restoration tasks, such as image denoising and image super-resolution. In image denoising, [51,21,8] take the noise variance as one network input to adaptively handle various noise strengths. Several practical denoising methods stabilize the noise variance caused by various ISO [43] and the property of Poisson-Gaussian distribution [35,15]. Similarly, image blind super-resolution is required to handle various degradations (different Gaussian blurs, motion blurs, and noises) on real-world applications. [41] proposes an unsupervised learning scheme for learning degradation representations based on the assumption that the degradation is the same in an image but can vary for different images. However, this assumption does not hold in image deblurring. Optimization-based deblurring. A popular approach for image deblurring is based on the Maximum A Posterior (MAP). Most MAP-based methods focus on finding good priors for sharp images and blur kernels. Many priors are designed to model clean images and blur kernels. They include total variation\n(TV) [1], hyper-Laplacian prior [11], l0-norm gradient prior [46] and sparse image priors [14]. They all assume the blur kernel is linear and uniform and can be represented as a convolution kernel. However, this assumption does not hold for real-world blurring with the non-uniform kernels. Some non-uniform deblurring methods [44,23,33,5] are proposed based on the assumption that the blur is locally uniform. They are not practical even with high computational costs. Learning-based deblurring. Many deep deblurring models have been proposed over the past few years. Earlier attempts [32,37] utilize deep convolution neural networks to facilitate blur kernel estimation. However, there are several limitations [24] in estimating kernels. 1) Simple kernel convolution is not practical on real-world challenging cases. 2) The incorrect kernel estimations, caused by noise and large motions, may cause unpleasing artifacts. 3) Estimating spatially varying kernels requires a huge amount of computation. To avoid the above limitations, a series of kernel-free methods [38,24,12,13,49,6,36,27,52,47,4,3] are proposed with much better performances. Nah et al. [24] propose a multi-scale network for image deblurring. Similarly, Tao et al. [38] propose a scale-recurrent structure for image deblurring. Adversarial training is also introduced in image deblurring [12,13]. Chen et al. [2] introduces a reblur2deblur framework for video deblurring. Zhang et al. [52] proposes a reblurring network to synthesize additional blurry training images. The reblurring network and deblurring network are separate in both two methods. In this work, we combine reblurring network and deblurring network in learning the degradation representations. Recently, multistage approaches [49,36,47,3] achieve impressive performance against previous methods. While those deblurring networks outperform the traditional deblurring methods significantly, their performances are still limited due to the lack of explicit degradation modeling. Learning Blurring Degradation Representations.While the explicit degradation representations have shown convincing improvements on many low-level vision tasks, it is rarely explored in learning-based deblurring methods. SelfDeblur [29] introduces the Deep Image Prior (DIP) [40] to model the clean images and the kernels separately. But, it assumes the blur kernels are linear and uniform, which is not practical in complicated real-world scenes. Tran et al. [39] address the limitation by introducing the explicit representation for the blur kernels and the blur operators and reparameterizing the degradation and the sharp image by using DIP. This inevitably involves alternative optimization, which is time-consuming. The learned representations cannot improve the performance of existing deblurring networks directly and their application is limited to time-consuming DIP-based optimization methods."
        },
        {
            "heading": "3 Methodology",
            "text": "In this section, we first introduce a joint learning framework for both sharp-toblurry image reblurring and blurry-to-sharp image deblurring to encode latent spatially varying degradation representations from blurry images. A blur-aware loss is introduced to enhance the image deblurring performance. To more effec-\n&#&$\n&# &$\ntively integrate the latent degradation representations for reblurring and deblurring, a multi-scale degradation injection network is proposed for both tasks."
        },
        {
            "heading": "3.1 Learning Degradation Representations from Joint Reblurring and Deblurring",
            "text": "Most existing deblurring methods [32,26,18,11,1] take the blur kernels as the degradation representations and model the blurring process as a convolution on the input image. However, the simple kernel convolution is not practical on realworld challenging cases and it is usually difficult to estimate the blur kernels in large motions and spatially varying blurring cases. We propose to encode latent degradation representations from blurry images via the joint learning of image reblurring and deblurring. As shown in Fig. 1, we introduce an encoder E to encode the blurry image y into the degradation representations E(y), which is modeled as a multi-channel latent map encoding 2D spatially varying blurring degradation in a latent space. Then an image reblurring generator Gr and an image deblurring generator Gd are introduced to generate the reblurred image y\u0301 and the deblurred image x\u0301, respectively. The degradation representations do not only help the reblurring generator Gr model the degradation process, but also help the deblurring network handle complicated spatially varying degradation patterns. The joint training of reblurring and deblurring strengthens the expressiveness of learned degradation representations. Sharp-to-blurry image reblurring. Different from modeling the blurring process as a convolution, our generator Gr models the degradation process via learning to generate the blurry image y\u0301, given the sharp image x and the corresponding degradation representations E(y). In addition, instead of generating the whole blurry image from scratch, the reblurring generator Gr learns to predict the residual between the sharp image x and the blurry image y. The learning\nof the blurring degradation process in our framework is therefore formulated as generating a blurry image y\u0301 from its clean image x as\ny\u0301 = x+Gr(x,E(y)), (3)\nwhere y\u0301 is the reblurred image conditioned on the sharp image x and degradation representation E(y). With the residual learning, the encoder E is encouraged to neglect the contents of the blurry image and to focus on disentangling the content-independent degradation representation E(y) from the blurry image y.\nNote that most existing encoder-decoder networks, such as VAE [10], can also learn implicit image representations via image reconstruction. They aim at encoding the whole-image contents into the latent representations. However, our framework aims at encoding only the degradation information by predicting the residuals between the sharp and blurry images. Such a task actually has a lower difficulty level than reconstructing all contents of an input image and therefore leads to encoding better blurring degradation representations.\nWe first tried the mainstream L1 distance as the loss function. But the L1 loss function merely measures the pixel-wise distance and cannot properly describe the similarity of the blurry patterns of two images. Then the reblurring generator Gr cannot well generate the blurry images with L1 loss, which harms the learning of degradation representation. Therefore, we further resort to perceptual loss [9] and adversarial training [7] to distinguish different degradation patterns in training. Adversarial loss is applied on the output of the generator Gr to distinguish real and fake blurry images so that the decoder can well model the blurring process and improve the expressiveness of degradation representation. We take the hinge loss [17,48,22,28] as the adversarial loss to help the reblurring generator Gr model the degradation (blurring) process and improve the expressiveness of learned degradation representations. We train the reblurring generator Gr to generate the blurry images with the multi-scale discriminator D used in [42]. The training objective for image reblurring is formulated as\nLG = \u2212Ex\u223cpdataD(x, y\u0301) + \u03bb1Lperceptual(y, y\u0301), LD = \u2212E(x,y)\u223cpdata [min(0,\u22121 +D(x, y))]\u2212 Ex\u223cpdata [min(0,\u22121\u2212D(x, y\u0301))],\n(4) where \u03bb1 balances the L1 loss and the discriminator loss for image reblurring, and the discriminator D is a conditional discriminator conditioning on the sharp image x. Conditioning on the sharp image x, the conditional discriminator D can focus on whether the reblurred image y\u0301 has the same image contents with the corresponding sharp image x. The reblurring helps extract content-independent degradation information (shown in Fig), which is different with the contentdependent conditional networks [45,50,53]. Blurry-to-sharp image deblurring. To make the learned degradation representations contributing to image deblurring, we also model the image deblurring process with the image reblurring process jointly. The image deblurring generator Gd follows a similar design to that of Gr. The deblurring generator Gd is only required to learn the deblurring residuals between the blurry image y and\nits corresponding sharp image x, and the learned degradation representations E(y). The blurry-to-sharp image deblurring is modeled as\nx\u0301 = y +Gd(y,E(y)). (5)\nThanks to the 2D learnable degradation representations, the image deblurring generator Gd is aware of the spatially varying blurry patterns and thus can adaptively handle various and complicated degradation patterns. The learning of image deblurring makes the learned degradation representations adapted to the deblurring task. The loss function for image deblurring is formulated as\nL(x, x\u0301) = \u03bb2L1(x, x\u0301). (6)\nDiscussion of the learned degradation representation. The learned degradation representation has two main advantages against conventional kernel modeling: 1) Our degradation representations can learn non-uniform spatially varying degradations effectively. Figs. 1 and 6 show that the encoder can distinguishe different degradation representations. 2) Interpolating on the latent space of representations can generate blurry images with controllable blurry levels (as shown in Fig. 5). The representations are also content-independent (as shown in Fig. 6), which is different from previous conditional networks [45,50,53]. Built on this latent space, the representations show better interpretability and expressiveness."
        },
        {
            "heading": "3.2 Image Deblurring with Learned Degradation Representations",
            "text": "After obtaining the pre-trained encoder E, we freeze the pre-trained encoder and re-train the deblurring generator Gd to illustrate that our improvement is not from the complicated framework but the learned degradation representations. To further demonstrate the generality of learned degradation representations, we also train the deblurring generator Gd on the RealBlur dataset [30] with the encoder trained on the GoPro dataset [24].\nFollowing HINet [3], we select the Peak Signal-to-Noise Ratio (PSNR) loss as the main supervision. We also utilize a blur-aware loss function as extra supervision. The well-trained encoder E should be quite sensitive to capture various and even subtle blur patterns. We, therefore, define the blur-aware loss as the distance between degradation encoder features of the ground-truth sharp image x and the estimated sharp image x\u0301. The deviation between the encoder feature maps \u2225E(x) \u2212 E(x\u0301)\u22251 of images x and x\u0301 can give more weights on the remaining blurry regions of the network output x\u0301. Similar to perceptual loss [9], the L1 distances between the encoder feature maps can be calculated at multiple scales. The blur-aware loss Lblur is therefore formulated as\nLblur(x\u0301, x) = N\u2211 i=1 1 |E(i)| [||E(i)(x\u0301)\u2212 E(i)(x)||1], (7)\nwhere E(i) denotes the i-th layer of the encoder E and |E(i)| denotes the number of pixels in feature map E(i)(x). Using the blur-aware loss makes the deblurring\ngenerator Gd pay more attention to the remaining blurry regions of the network output x\u0301. Then the deblurring objective is formulated as\nL(x, x\u0301) = PSNR(x, x\u0301) + \u03bb2Lblur(x, x\u0301). (8)"
        },
        {
            "heading": "3.3 Multi-scale Degradation Injection Network",
            "text": "To effectively integrate the degradation E(y) to predict the reblurring and deblurring residuals, we propose a multi-scale degradation injection network (MSDI-Net) for both reblurring and deblurring. Our reblurring generator Gr and deblurring generator Gd consists of two MSDI-Nets, which are stacked as HINet [3] does. for simplicity, the overview of a MSDI-Net is shown in Fig. 2.\nThe MSDI-Net consists of an encoder, a decoder, concatenation-based skipconnections, and a multi-scale degradation injection module. The first three modules of the MSDI-Net are widely adopted in U-Net like architectures [31]. The multi-scale degradation injection module modulates the feature of each skipconnection spatially, based on the learned degradation representation. The spatially variant modulation are explored in image synthesis [28] and image superresolution [16]. We adopt it as the key to connecting learnable degradation representations to reblurring and deblurring. Let fi \u2208 RCi\u00d7Hi\u00d7Wi denote the features extracted at scale i = 1, . . . , 5 in the encoder. The extracted feature fi is passed to the decoder at the skip connection of scale i. Our MSDI-Net integrates\nthe degradation representations into concatenation-based skip-connections of scale i. At scale i, we obtain the degradation map Mi \u2208 RCi\u00d7Hi\u00d7Wi by a convolution-based upsampling block on the original degradation map Mi\u22121 \u2208 RCi\u22121\u00d7Hi\u22121\u00d7Wi\u22121 (Hi = 2 \u00d7 Hi\u22121,Wi = 2 \u00d7 Wi\u22121), which is implemented by a nearest-neighbor interpolation and a convolution layer to avoid checkerboard artifacts [25]. Then we utilize a spatially adaptive modulation (SAM) module to modulate the skip-connection feature fi at scale i. The spatially adaptive modulation modulates the feature map channels in a spatially varying manner with both predicted scaling and additions. At the skip connection of scale i, we use several convolution (3 \u00d7 3) layers on the degradation map Mi to predict the modulating parameters \u03b3i \u2208 RCi\u00d7Hi\u00d7Wi and \u03b2i \u2208 RCi\u00d7Hi\u00d7Wi respectively, which modulate the feature map fi as\nFi = \u03b3i \u2299 fi + \u03b2i, (9)\nwhere Fi is the modulated skip-connection features. Since the modulation parameters are predicted from the degradation representations, the learnable modulation of the feature channels makes the deblurring network aware of spatially varying degradations. The degradation-aware feature Fi is then concatenated with the decoder feature at scale i and the last decoder layer predicts the image residuals for both image reblurring and deblurring.\nInjecting the degradation representations enables the networks to handle various and complicated degradation patterns adaptively. Injection at multiple scales improves the expressiveness of degradation representations by strengthening the connections between the degradation representations and two generators."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Dataset and implementation details",
            "text": "We train and evaluate our method on the GoPro [24] and RealBlur datasets [30]. The GoPro dataset consists of 2,103 pairs of blurry and sharp images for training and 1,111 pairs for testing. The RealBlur dataset consists of 3,758 pairs for training and 980 pairs for testing. We first train the framework of reblurring and deblurring on GoPro dataset [24]. We apply horizontal flipping and rotation as data augmentation and crop image patch of size 256\u00d7 256 from the dataset for training. \u03bb1 is set as 30 and \u03bb2 is set as 10. The networks of whole framework are trained with a batch size of 32 for 200k iterations. Then we freeze the weights of well-trained encoder E and train the deblurring generator Gd on the GoPro dataset [24] and the RealBlur dataset [30] respectively. \u03bb3 is set as 1. The deblurring generator Gd is trained with a batch size of 64 for 400k iterations. We use the Adam optimizer and the learning rate is set as 3\u00d710\u22124 at the beginning and decreased to 1\u00d7 10\u22127 following the cosine annealing strategy [19]."
        },
        {
            "heading": "4.2 Performance comparison",
            "text": "We compare our method with state-of-the-art deblurring methods [3,47,4] on the GoPro test dataset [24]. The quantitative results are reported in Table 1.\nFor testing, we slice the whole image into several 256 \u00d7 256 patches and test all patches to report the results of HINet [3], MPRNet-patch256 [47] and our method. Our method achieves 0.45 dB improvement in terms of PSNR over the previous best-performing method HINet [3]. To evaluate effectiveness and generality of learned degradation representations, we also evaluate our method on the RealBlur dataset. As listed in Table 3, our method achieves the best performance in terms of PSNR and SSIM. Since HINet [3] does not release the model for RealBlur dataset [30], we train the HINet model on RealBlur dataset based on their released training code. Note that we apply the degradation representations trained on GoPro dataset [24] directly on RealBlur dataset [30]. Our method outperforms the previous SOTA HINet [3] by 0.23 dB PSNR, which demonstrate the generality of learnable degradation representations.\nIn Table 2, we provide detailed comparisons between our method and MPRNet [47]. We divide the whole gopro dataset into blurriest 10% and sharpest 10% as [34] does. It is observed that the main improvement of our method is the improvement of 0.34dB PSNR in the blurriest 10%, which demonstrates the advantage of proposed degradation learning. What\u2019s more, our method\u2019s computational cost is less than 50% of MPRNet\u2019s [47].\nFig. 3 and Fig. 4 show example deblurred results from the GoPro [24] and RealBlur [30] test sets by the evaluated approaches. Our method produces sharper\nimages and recovers more details in the regions of texts and moving objects, compared with other methods."
        },
        {
            "heading": "4.3 Interpolation and decoupleness of degradation representations",
            "text": "To demonstrate the effectiveness of learned degradation representations, we study the interpolation and decoupleness of learned degradation representations. Given a blurry image y and its corresponding sharp image x, we can obtain two degradation representations E(y) and E(x). Then we obtain several intermediate degradation representations by interpolating from E(y) to E(x). The corresponding output of the decoder changes smoothly from sharp to blurry images. The blur interpolation in Fig. 5 shows that our degradation representations are built on the latent space and accurately aware of different degradations. We empirically validate the decoupleness of blur and image contents on GoPro dataset. We divide the GoPro test dataset into 555 pairs of images. For each pair of sharp images {A,B}, the corresponding blurry images are {blur(A),blur(B)} and the degradation representations are obtained as {degA,degB}. The sharp image A can be reblurred according to degB to obtain ReBlur(A,degB). We use the aver-\nage contextual similarity CX [20] to measure pairwise image similarity. We have CX(blur(A), A) = 2.72, CX(ReBlur(A,degB), A) = 2.65, CX(blur(A), B) = 5.43, and CX(ReBlur(A,degB), B) = 5.39, averaging over all the pairs. The similarities empirically show that ReBlur(A,degB) has similar contents with A (the former two equations) but doesn\u2019t have similar contents with B (the latter two equations). The visual examples on degradation decoupleness are provided in Figure 6. The decoupleness of learned degradation representations shows our difference with the content-dependent conditional networks [45,50,53] and the capacity of being a general operator to replace conventional blurring process."
        },
        {
            "heading": "4.4 Ablation study",
            "text": "We evaluate the effectiveness of learning degradation representations and the multi-scale degradation injection network by revising one of the components of\nour model at a time. Table 4 lists the performances of different settings on the GoPro test set [24]. We first remove the degradation encoder for learning the degradation representations and make Gd only takes as input the blurry image (denoted as \u201cOurs w/o degradation\u201d). The performance suffers a large drop of 0.47dB PSNR. Then we remove the generator of image reblurring and reserve the encoder to provide additional encoding from the blurry images (denoted as \u201cOur w/o reblurring\u201d), this operation causes a drop of 0.19 dB PSNR, demonstrating that reblurring indeed contributes to the learning of better degradation representations. Then we remove the blur-aware loss function (Eq. (7)). The training objective for deblurring becomes only the PSNR loss (denoted as \u201cOurs w/o blur loss\u201d). The performance drops slightly by about 0.07 PSNR. We further experiment with different ways of integrating degradation representations into the generators. When we remove the injection at multiple scales, the degradation representation is integrated just at the lowest resolution skip connection (denoted as \u201cOur injection w/o multi-scale\u201d). The performance suffers a significant drop of 0.48 PSNR, which means that injecting the degradation representation into a single scale would affect the deblurring performance significantly. Then we replace the modulation and test on concatenating latent degradation map with\nskip-connection feature maps (denoted as \u201cOur w/ concat injection\u201d). For fair comparisons of computational cost, we add two-layer residual blocks after the feature concatenation. The operation causes a drop of 0.23 PSNR. We also remove the integration totally and concatenate the upsampling degradation maps with the blurry images at the network entrance with the blurry image (denoted as \u201cOurs input w/ concat\u201d), which is the mainstream design in image denoising [51,21,8]. Its performance drops by about 0.3 PSNR. All the ablation studies demonstrate the effectiveness of our proposed learnable degradation representations and MSDI-Net for image deblurring."
        },
        {
            "heading": "5 Conclusions",
            "text": "In this paper, we propose a framework for learning degradation representation with image reblurring and image deblurring. We first utilize an encoder to learn the degradation representations explicitly. Then we propose a multi-scale degradation injection network to effectively integrate the degradation representations for reblurring and deblurring. With the degradation representations, our networks can be aware of and handle spatially varying degradation patterns adaptively. The experimental results demonstrate that our method outperforms other methods with a clear margin."
        }
    ],
    "title": "Learning Degradation Representations for Image Deblurring",
    "year": 2022
}