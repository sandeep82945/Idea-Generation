{
    "abstractText": "Sememes are defined as the minimum semantic units of human languages. As important knowledge sources, sememe-based linguistic knowledge bases have been widely used in many NLP tasks. However, most languages still do not have sememe-based linguistic knowledge bases. Thus we present a task of cross-lingual lexical sememe prediction, aiming to automatically predict sememes for words in other languages. We propose a novel framework to model correlations between sememes and multi-lingual words in low-dimensional semantic space for sememe prediction. Experimental results on real-world datasets show that our proposed model achieves consistent and significant improvements as compared to baseline methods in cross-lingual sememe prediction. The codes and data of this paper are available at https: //github.com/thunlp/CL-SP.",
    "authors": [
        {
            "affiliations": [],
            "name": "Fanchao Qi"
        },
        {
            "affiliations": [],
            "name": "Yankai Lin"
        },
        {
            "affiliations": [],
            "name": "Maosong Sun"
        },
        {
            "affiliations": [],
            "name": "Hao Zhu"
        },
        {
            "affiliations": [],
            "name": "Ruobing Xie"
        },
        {
            "affiliations": [],
            "name": "Zhiyuan Liu"
        }
    ],
    "id": "SP:8431fe6ce03800fafd5acf5149cb688bad066547",
    "references": [
        {
            "authors": [
                "Waleed Ammar",
                "George Mulcaire",
                "Yulia Tsvetkov",
                "Guillaume Lample",
                "Chris Dyer",
                "Noah A Smith."
            ],
            "title": "Massively multilingual word embeddings",
            "venue": "arXiv preprint arXiv:1602.01925.",
            "year": 2016
        },
        {
            "authors": [
                "Sarath Chandar AP",
                "Stanislas Lauly",
                "Hugo Larochelle",
                "Mitesh Khapra",
                "Balaraman Ravindran",
                "Vikas C Raykar",
                "Amrita Saha."
            ],
            "title": "An autoencoder approach to learning bilingual word representations",
            "venue": "Proceedings of NIPS.",
            "year": 2014
        },
        {
            "authors": [
                "Mikel Artetxe",
                "Gorka Labaka",
                "Eneko Agirre."
            ],
            "title": "Learning bilingual word embeddings with (almost) no bilingual data",
            "venue": "Proceedings of ACL.",
            "year": 2017
        },
        {
            "authors": [
                "Leonard Bloomfield."
            ],
            "title": "A set of postulates for the science of language",
            "venue": "Language, 2(3):153\u2013164.",
            "year": 1926
        },
        {
            "authors": [
                "Danushka Bollegala",
                "Mohammed Alsuhaibani",
                "Takanori Maehara",
                "Ken-ichi Kawarabayashi."
            ],
            "title": "Joint word representation learning using a corpus and a semantic lexicon",
            "venue": "Proceedings of AAAI.",
            "year": 2016
        },
        {
            "authors": [
                "Alexis Conneau",
                "Guillaume Lample",
                "Marc\u2019Aurelio Ranzato",
                "Ludovic Denoyer",
                "Herv\u00e9 J\u00e9gou"
            ],
            "title": "Word translation without parallel data",
            "venue": "arXiv preprint arXiv:1710.04087",
            "year": 2017
        },
        {
            "authors": [
                "Jocelyn Coulmance",
                "Jean-Marc Marty",
                "Guillaume Wenzek",
                "Amine Benhalloum."
            ],
            "title": "Transgram, fast cross-lingual word-embeddings",
            "venue": "Proceedings of EMNLP.",
            "year": 2015
        },
        {
            "authors": [
                "Lei Dang",
                "Lei Zhang."
            ],
            "title": "Method of discriminant for chinese sentence sentiment orientation based on hownet",
            "venue": "Application Research of Computers, 4:43.",
            "year": 2010
        },
        {
            "authors": [
                "Georgiana Dinu",
                "Angeliki Lazaridou",
                "Marco Baroni."
            ],
            "title": "Improving zero-shot learning by mitigating the hubness problem",
            "venue": "arXiv preprint arXiv:1412.6568.",
            "year": 2014
        },
        {
            "authors": [
                "Zhendong Dong",
                "Qiang Dong."
            ],
            "title": "Hownet-a hybrid language and knowledge resource",
            "venue": "Proceedings of NLP-KE.",
            "year": 2003
        },
        {
            "authors": [
                "Long Duong",
                "Hiroshi Kanayama",
                "Tengfei Ma",
                "Steven Bird",
                "Trevor Cohn."
            ],
            "title": "Learning crosslingual word embeddings without bilingual corpora",
            "venue": "Proceedings of EMNLP.",
            "year": 2016
        },
        {
            "authors": [
                "Manaal Faruqui",
                "Jesse Dodge",
                "Sujay Kumar Jauhar",
                "Chris Dyer",
                "Eduard Hovy",
                "Noah A Smith."
            ],
            "title": "Retrofitting word vectors to semantic lexicons",
            "venue": "Proceedings of NAACL-HLT.",
            "year": 2015
        },
        {
            "authors": [
                "Manaal Faruqui",
                "Chris Dyer."
            ],
            "title": "Improving vector space word representations using multilingual correlation",
            "venue": "Proceedings of the EACL.",
            "year": 2014
        },
        {
            "authors": [
                "Lev Finkelstein",
                "Evgeniy Gabrilovich",
                "Yossi Matias",
                "Ehud Rivlin",
                "Zach Solan",
                "Gadi Wolfman",
                "Eytan Ruppin."
            ],
            "title": "Placing search in context: The concept revisited",
            "venue": "ACM Transactions on Information Systems, 20(1):116\u2013131.",
            "year": 2002
        },
        {
            "authors": [
                "Xianghua Fu",
                "Guo Liu",
                "Yanyan Guo",
                "Zhiqiang Wang."
            ],
            "title": "Multi-aspect sentiment analysis for chinese online social reviews based on topic modeling and hownet lexicon",
            "venue": "Knowledge-Based Systems, 37:186\u2013195.",
            "year": 2013
        },
        {
            "authors": [
                "Stephan Gouws",
                "Yoshua Bengio",
                "Greg Corrado."
            ],
            "title": "Bilbowa: fast bilingual distributed representations without word alignments",
            "venue": "Proceedings of ICML.",
            "year": 2015
        },
        {
            "authors": [
                "Yihong Gu",
                "Jun Yan",
                "Hao Zhu",
                "Zhiyuan Liu",
                "Ruobing Xie",
                "Maosong Sun",
                "Fen Lin",
                "Leyu Lin."
            ],
            "title": "Language modeling with sparse product of sememe experts",
            "venue": "Proceedings of EMNLP.",
            "year": 2018
        },
        {
            "authors": [
                "Karl Moritz Hermann",
                "Phil Blunsom."
            ],
            "title": "Multilingual distributed representations without word alignment",
            "venue": "Proceedings of ICLR.",
            "year": 2014
        },
        {
            "authors": [
                "Felix Hill",
                "Roi Reichart",
                "Anna Korhonen."
            ],
            "title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation",
            "venue": "Computational Linguistics, 41(4):665\u2013695.",
            "year": 2015
        },
        {
            "authors": [
                "Huiming Jin",
                "Hao Zhu",
                "Zhiyuan Liu",
                "Ruobing Xie",
                "Maosong Sun",
                "Fen Lin",
                "Leyu Lin."
            ],
            "title": "Incorporating chinese characters of words for lexical sememe prediction",
            "venue": "Proceedings of ACL.",
            "year": 2018
        },
        {
            "authors": [
                "Peng Jin",
                "Yunfang Wu."
            ],
            "title": "SemEval-2012 Task 4: Evaluating chinese word similarity",
            "venue": "Proceddings of *SEM.",
            "year": 2012
        },
        {
            "authors": [
                "Tom\u00e1\u0161 Ko\u010disk\u1ef3",
                "Karl Moritz Hermann",
                "Phil Blunsom."
            ],
            "title": "Learning bilingual word representations by marginalizing alignments",
            "venue": "Proceedings of ACL.",
            "year": 2014
        },
        {
            "authors": [
                "Angeliki Lazaridou",
                "Georgiana Dinu",
                "Marco Baroni."
            ],
            "title": "Hubness and pollution: Delving into cross-space mapping for zero-shot learning",
            "venue": "Proceedings of ACL-IJCNLP.",
            "year": 2015
        },
        {
            "authors": [
                "Zhongguo Li",
                "Maosong Sun."
            ],
            "title": "Punctuation as implicit annotations for chinese word segmentation",
            "venue": "Computational Linguistics, 35(4):505\u2013512.",
            "year": 2009
        },
        {
            "authors": [
                "Quan Liu",
                "Hui Jiang",
                "Si Wei",
                "Zhen-Hua Ling",
                "Yu Hu."
            ],
            "title": "Learning semantic word embeddings based on ordinal knowledge constraints",
            "venue": "Proceedings of ACL-IJCNLP.",
            "year": 2015
        },
        {
            "authors": [
                "Qun Liu",
                "Sujian Li."
            ],
            "title": "Word similarity computing based on hownet",
            "venue": "International Journal of Computational Linguistics&Chinese Language Processing, 7(2):59\u201376.",
            "year": 2002
        },
        {
            "authors": [
                "Ang Lu",
                "Weiran Wang",
                "Mohit Bansal",
                "Kevin Gimpel",
                "Karen Livescu."
            ],
            "title": "Deep multilingual correlation for improved word embeddings",
            "venue": "Proceedings of NAANL-HLT.",
            "year": 2015
        },
        {
            "authors": [
                "Thang Luong",
                "Hieu Pham",
                "Christopher DManning."
            ],
            "title": "Bilingual word representations with monolingual quality in mind",
            "venue": "Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing.",
            "year": 2015
        },
        {
            "authors": [
                "Laurens van der Maaten",
                "Geoffrey E Hinton."
            ],
            "title": "Visualizing data using t-sne",
            "venue": "Journal of Machine Learning Research, 9:2579\u20132605.",
            "year": 2008
        },
        {
            "authors": [
                "Tomas Mikolov",
                "Kai Chen",
                "Greg Corrado",
                "Jeffrey Dean."
            ],
            "title": "Efficient estimation of word representations in vector space",
            "venue": "Proceedings of ICLR.",
            "year": 2013
        },
        {
            "authors": [
                "TomasMikolov",
                "Quoc V Le",
                "Ilya Sutskever."
            ],
            "title": "Exploiting similarities among languages for machine translation",
            "venue": "arXiv preprint arXiv:1309.4168.",
            "year": 2013
        },
        {
            "authors": [
                "George AMiller."
            ],
            "title": "Wordnet: a lexical database for english",
            "venue": "Communications of the ACM, 38(11):39\u2013",
            "year": 1995
        },
        {
            "authors": [
                "Nikola Mrk\u0161ic",
                "Diarmuid OS\u00e9aghdha",
                "Blaise Thomson",
                "Milica Ga\u0161ic",
                "Lina Rojas-Barahona",
                "Pei-Hao Su",
                "David Vandyke",
                "Tsung-Hsien Wen",
                "Steve Young"
            ],
            "title": "Counter-fitting word vectors to linguistic constraints",
            "venue": "Proceedings of NAACL-HLT",
            "year": 2016
        },
        {
            "authors": [
                "Yilin Niu",
                "Ruobing Xie",
                "Zhiyuan Liu",
                "Maosong Sun."
            ],
            "title": "Improved word representation learning with sememes",
            "venue": "Proceedings of ACL.",
            "year": 2017
        },
        {
            "authors": [
                "Jeffrey Pennington",
                "Richard Socher",
                "Christopher Manning."
            ],
            "title": "Glove: Global vectors for word representation",
            "venue": "Proceedings of EMNLP.",
            "year": 2014
        },
        {
            "authors": [
                "Sebastian Ruder."
            ],
            "title": "A survey of cross-lingual embedding models",
            "venue": "arXiv preprint arXiv:1706.04902.",
            "year": 2017
        },
        {
            "authors": [
                "Tianze Shi",
                "Zhiyuan Liu",
                "Yang Liu",
                "Maosong Sun."
            ],
            "title": "Learning cross-lingual word embeddings via matrix co-factorization",
            "venue": "Proceedings of ACLIJCNLP.",
            "year": 2015
        },
        {
            "authors": [
                "Jingguang Sun",
                "Dongfeng Cai",
                "Dexin Lv",
                "Yanju Dong."
            ],
            "title": "Hownet based chinese question automatic classification",
            "venue": "Journal of Chinese Information Processing, 21(1):90\u201395.",
            "year": 2007
        },
        {
            "authors": [
                "Shyam Upadhyay",
                "Manaal Faruqui",
                "Chris Dyer",
                "Dan Roth."
            ],
            "title": "Cross-lingual models of word embeddings: An empirical comparison",
            "venue": "Proceedings of ACL.",
            "year": 2016
        },
        {
            "authors": [
                "Ivan Vuli\u0107",
                "Anna Korhonen."
            ],
            "title": "On the role of seed lexicons in learning bilingual word embeddings",
            "venue": "Proceedings of ACL.",
            "year": 2016
        },
        {
            "authors": [
                "Ivan Vuli\u0107",
                "Marie-Francine Moens."
            ],
            "title": "Bilingual word embeddings from non-parallel documentaligned data applied to bilingual lexicon induction",
            "venue": "Proceedings of ACL-IJCNLP.",
            "year": 2015
        },
        {
            "authors": [
                "Michael Wick",
                "Pallika Kanani",
                "Adam Craig Pocock."
            ],
            "title": "Minimally-constrained multilingual embeddings via artificial code-switching",
            "venue": "Proceedings of AAAI.",
            "year": 2016
        },
        {
            "authors": [
                "Ruobing Xie",
                "Xingchi Yuan",
                "Zhiyuan Liu",
                "Maosong Sun."
            ],
            "title": "Lexical sememe prediction via word embeddings and matrix factorization",
            "venue": "Proceedings of AAAI.",
            "year": 2017
        },
        {
            "authors": [
                "Xiangkai Zeng",
                "Cheng Yang",
                "Cunchao Tu",
                "Zhiyuan Liu",
                "Maosong Sun."
            ],
            "title": "Chinese liwc lexicon expansion via hierarchical classification of word embeddings with sememe attention",
            "venue": "Proceedings of AAAI.",
            "year": 2018
        },
        {
            "authors": [
                "Meng Zhang",
                "Haoruo Peng",
                "Yang Liu",
                "Huan-Bo Luan",
                "Maosong Sun."
            ],
            "title": "Bilingual lexicon induction from non-parallel data with minimal supervision",
            "venue": "Proceedings of AAAI.",
            "year": 2017
        },
        {
            "authors": [
                "Yuntao Zhang",
                "Ling Gong",
                "Yongcheng Wang."
            ],
            "title": "Chinese word sense disambiguation using hownet",
            "venue": "Proceedings of International Conference on Natural Computation.",
            "year": 2005
        },
        {
            "authors": [
                "Will Y Zou",
                "Richard Socher",
                "Daniel Cer",
                "Christopher D Manning."
            ],
            "title": "Bilingual word embeddings for phrase-based machine translation",
            "venue": "Proceedings of EMNLP.",
            "year": 2013
        }
    ],
    "sections": [
        {
            "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 358\u2013368 Brussels, Belgium, October 31 - November 4, 2018. c\u00a92018 Association for Computational Linguistics\n358"
        },
        {
            "heading": "1 Introduction",
            "text": "Words are regarded as the smallest meaningful unit of speech or writing that can stand by themselves in human languages, but not the smallest indivisible semantic unit of meaning. That is, the meaning of a word can be represented as a set of semantic components. For example, \u201cMan = human + male + adult\u201d and \u201cBoy = human +male + child\u201d. In linguistics, the minimum semantic unit of meaning is named sememe (Bloomfield, 1926). Some people believe that semantic meanings of concepts such as words can be composed of a limited closed set of sememes. And sememes can help us comprehend human languages better. Unfortunately, the lexical sememes of words are not explicit in most human languages. Hence, people construct sememe-based linguistic knowledge\n\u2217 Indicates equal contribution \u2020 Corresponding author\nbases (KBs) via manually annotating every words with a pre-defined closed set of sememes. HowNet (Dong and Dong, 2003) is one of the most wellknown sememe-based linguistic KBs. Different fromWordNet (Miller, 1995) which focuses on the relations between senses, it annotates each word with one or more relevant sememes. As illustrated in Fig. 1, the word apple has two senses including apple (fruit) and apple (brand) in HowNet. The sense apple (fruit) has one sememe fruit, and the sense apple (brand) has five sememes including computer, PatternValue, able, bring and SpecificBrand. There exist about 2, 000 sememes and over 100 thousand labeled Chinese and English words in HowNet. HowNet has been widely used in various NLP applications such as word similarity computation (Liu and Li, 2002), word sense disambiguation (Zhang et al., 2005), question classification (Sun et al., 2007) and sentiment classification (Dang and Zhang, 2010). However, most languages do not have such sememe-based linguistic KBs, which prevents us understanding and utilizing human languages to a greater extent. Therefore, it is important to build sememe-based linguistic KBs for various languages. Manual construction for sememebased linguistic KBs requires efforts of many linguistic experts, which is time-consuming and\nlabor-intensive. For example, the construction of HowNet has cost lots of Chinese linguistic experts more than 10 years. To address the issue of the high labor cost of manual annotation, we propose a new task, crosslingual lexical sememe prediction (CLSP) which aims to automatically predict lexical sememes for words in other languages. CLSP aims to assist in the annotation of linguistic experts. There are two critical challenges for CLSP: (1) There is not a consistent one-to-one match between words in different languages. For example, English word \u201cbeautiful\u201d can refer to Chinese words of either \u201c\u7f8e\u4e3d\u201d or \u201c\u6f02\u4eae\u201d. Hence, we cannot simply translate HowNet into another language. And how to recognize the semantic meaning of a word in other languages becomes a critical problem. (2) Since there is a gap between the semantic meanings of words and sememes, we need to build semantic representations for words and sememes to capture the semantic relatedness between them. To tackle these challenges, in this paper, we propose a novel model for CLSP, which aims to transfer sememe-based linguistic KBs from source language to target language. Ourmodel contains three modules including (1) monolingual word embedding learning which is intended for learning semantic representations of words for source and target languages respectively; (2) cross-lingual word embedding alignment which aims to bridge the gap between the semantic representations of words in two languages; (3) sememe-based word embedding learning whose objective is to incorporate sememe information into word representations. For simplicity, we do not consider the hierarchy information in HowNet in this paper. In experiments, we take Chinese as source language and English as target language to show the effectiveness of our model. Experimental results show that our proposed model could effectively predict lexical sememes for words with different frequencies in other languages. Our model also has consistent improvements on two auxiliary experiments including bilingual lexicon induction and monolingual word similarity computation by jointly learning the representations of sememes, words in source and target languages."
        },
        {
            "heading": "2 Related Work",
            "text": "Since HowNet was published (Dong and Dong, 2003), it has attracted wide attention of re-\nsearchers. Most of related works focus on applying HowNet to specific NLP tasks (Liu and Li, 2002; Zhang et al., 2005; Sun et al., 2007; Dang and Zhang, 2010; Fu et al., 2013; Niu et al., 2017; Zeng et al., 2018; Gu et al., 2018). To the best of our knowledge, only Xie et al. (2017) and Jin et al. (2018) conduct studies of augmenting HowNet by recommending sememes for new words. However, both of the two works are aimed to recommend sememes for monolingual words and not applicable to cross-lingual circumstance. Accordingly, our work is the first effort to automatically perform cross-lingual sememe prediction to enrich sememe-based linguistic KBs. Our novel model adopts the method of word representation learning (WRL). Recent years have witnessed great advances in WRL. Models like Skip-gram, CBOW (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014) are immensely popular and achieve remarkable performance in many NLP tasks. However, most WRL methods learn distributional information of words from large corpora while the valuable information contained in semantic lexicons are disregarded. Therefore, some works try to inject semantic information of KBs intoWRL (Faruqui et al., 2015; Liu et al., 2015; Mrk\u0161ic et al., 2016; Bollegala et al., 2016). Nevertheless, these works are all applied to word-based KBs such as WordNet, few works pay attention to how to incorporate the knowledge from sememe-based linguistic KBs. There also have been plenty of studies working on cross-lingual WRL (Upadhyay et al., 2016; Ruder, 2017). Most of them require parallel corpora (Zou et al., 2013; AP et al., 2014; Hermann and Blunsom, 2014; Ko\u010disk\u1ef3 et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015). Some of them adopt unsupervised or weakly supervised methods (Mikolov et al., 2013b; Vuli\u0107 and Moens, 2015; Conneau et al., 2017; Artetxe et al., 2017). There are also some works using a seed lexicon as the cross-lingual signal (Dinu et al., 2014; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Shi et al., 2015; Lu et al., 2015; Gouws et al., 2015; Wick et al., 2016; Ammar et al., 2016; Duong et al., 2016; Vuli\u0107 and Korhonen, 2016). In terms of our cross-lingual sememe prediction task, parallel data-based bilingual WRL methods are unsuitable because most language pairs have no large parallel corpora. Besides, unsupervised\nmethods are not appropriate either as they are generally hard to learn high-quality bilingual word embeddings. Therefore, we choose the seed lexicon method in our model, and further introduce matching mechanism that is inspired by Zhang et al. (2017) to enhance its performance."
        },
        {
            "heading": "3 Methodology",
            "text": "In this section, we introduce our novel model for CLSP. Here we define the language with sememe annotations as source language and the language without sememe annotations as target language. The main idea of our model is to learn word embeddings of source and target languages jointly in a unified semantic space, and then predict sememes for words in target language according to the words with similar semantic meanings in source language. Ourmethod consists of three parts: monolingual word representation learning, cross-lingual word embedding alignment and sememe-based word representation learning. Hence, we define the objective function of our method corresponding to the three parts:\nL = Lmono + Lcross + Lsememe. (1)\nHere, the monolingual term Lmono is designed for learning monolingual word embeddings from nonparallel corpora for source and target languages respectively. The cross-lingual term Lcross aims to align cross-lingual word embeddings in a unified semantic space. And Lsememe can draw sememe information into word representation learning and conduce to better word embeddings for sememe prediction. In the following subsections, we introduce the three parts in detail."
        },
        {
            "heading": "3.1 Monolingual Word Representation",
            "text": "Monolingual word representation is responsible for explaining regularities in monolingual corpora of source and target languages. Since the two corpora are non-parallel,Lmono comprises twomonolingual sub-models that are independent of each other:\nLmono = LSmono + LTmono, (2)\nwhere the superscripts S and T denote source and target languages respectively. As a common practice, we choose the well established Skip-gram model to obtain monolingual\nword embeddings. Skip-gram model is aimed at maximizing the predictive probability of context words conditioned on the centered word. Formally, taking the source side for example, given a trainingword sequence {wS1 , \u00b7 \u00b7 \u00b7 , wSn}, Skip-gram model intends to minimize: LSmono = \u2212 n\u2212K\u2211\nc=K+1 \u2211 \u2212K\u2264k\u2264K,k \u0338=0 logP (wSc+k|wSc ),\n(3) where K is the size of the sliding window. P (wSc+k|wSc ) stands for the predictive probability of one of the context words conditioned on the centered word wSc , formalized by the following softmax function:\nP (wSc+k|wSc ) = exp(wSc+k \u00b7 wSc )\u2211\nwSs \u2208V S exp(w S s \u00b7 wSc )\n, (4)\nin which V s indicates the word vocabulary of source language. LTmono can be formulated similarly."
        },
        {
            "heading": "3.2 Cross-lingual Word Embedding Alignment",
            "text": "Cross-lingual word embedding alignment aims to build a unified semantic space for the words in source and target languages. Inspired by Zhang et al. (2017), we align the cross-lingual word embeddings with signals of a seed lexicon and selfmatching. Formally, Lcross is composed of two terms including alignment by seed lexiconLseed and alignment by matching Lmatch:\nLcross = \u03bbsLseed + \u03bbmLmatch, (5)\nwhere \u03bbs and \u03bbm are hyperparameters for controlling relative weightings of the two terms."
        },
        {
            "heading": "Alignment by Seed Lexicon",
            "text": "The seed lexicon term Lseed encourages word embeddings of translation pairs in a seed lexiconD to be close, which can be achieved via a L2 regularizer:\nLseed = \u2211\n\u27e8wSs ,wTt \u27e9\u2208D\n\u2225wSs \u2212 wTt \u22252, (6)\nin which wSs and wTt indicate the words in source and target languages in the seed lexicon respectively."
        },
        {
            "heading": "Alignment by Matching Mechanism",
            "text": "As for the matching process, it is founded on an assumption that each target word should be matched to a single source word or a special empty word, and vice versa. The goal of the matching process is to find the matched source (target) word for each target (source) word and maximize the matching probabilities for all the matched word pairs. The loss of this part can be formulated as:\nLmatch = LT2Smatch + LS2Tmatch, (7)\nwhere LT2Smatch is the term for target-to-source matching and LS2Tmatch is the term for source-totarget matching. Next, we give a detailed explanation of target-to-source matching, and the source-totarget matching is defined in the same way. We first introduce a latent variable mt \u2208 {0, 1, \u00b7 \u00b7 \u00b7 , |V S |} (t = 1, 2, \u00b7 \u00b7 \u00b7 , |V T |) for each target word wTt , where |V S | and |V T | indicate the vocabulary size of source and target languages respectively. Here, mt specifies the index of the source word that wTt matches with, and mt = 0 signifies the empty word is matched. Then we havem = {m1,m2, \u00b7 \u00b7 \u00b7 ,m|V T |}, and can formalize the target-to-source matching term:\nLT2Smatch = \u2212 logP (CT |CS) = \u2212 log \u2211 m P (CT ,m|CS), (8)\nwhere CT and CS denote the target and source corpus respectively. Here, we simply assume that the matching processes of target words are independent of each other. Therefore, we have:\nP (CT ,m|CS) = \u220f\nwT\u2208CT P (wT ,m|CS)\n= |V T |\u220f t=1 P (wTt |wSmt) c(wTt ),\n(9)\nwhere wSmt is the source word that w T t matches with, and c(wTt ) is the number of times wTt occurs in the target corpus."
        },
        {
            "heading": "3.3 Sememe-based Word Representation",
            "text": "Sememe-based word representation is intended for improving word embeddings for sememe prediction by introducing the information of sememebased linguistic KBs of source language. In this section, we present two methods of sememe-based word representation."
        },
        {
            "heading": "Word Relation-based Approach",
            "text": "A simple and intuitive method is to let words with similar sememe annotations tend to have similar word embeddings, which we name word relationbased approach. To beginwith, we construct a synonym list from sememe-based linguistic KBs of source language, where we regard words sharing a certain number of sememes as synonyms. Next, we force synonyms to have closer word embeddings. Formally, we let wSi be original word embedding of wSi and w\u0302Si be its adjusted word embedding. And let Syn(wSi ) denote the synonym set of word wSi . Then the loss function is:\nLsememe = \u2211\nwSi \u2208V S\n[ \u03b1i\u2225wSi \u2212 w\u0302Si \u22252+\n\u2211 wSj \u2208Syn(wSi ) \u03b2ij\u2225w\u0302Si \u2212 w\u0302Sj \u22252 ] ,\n(10)\nwhere \u03b1 and \u03b2 control the relative strengths of the two terms. It should be noted that the idea of forcing similar words to have close word embeddings is similar to the state-of-theart retrofitting approach (Faruqui et al., 2015). However, retrofitting approach cannot be applied here because sememe-based linguistic KBs such as HowNet cannot directly provide its needed synonym list."
        },
        {
            "heading": "Sememe Embedding-based Approach",
            "text": "Simple and effective as the word relation-based approach is, it cannot make full use of the information of sememe-based linguistic KBs because it disregards the complicated relations between sememes and words as well as relations between different sememes. To address this limitation, we propose sememe embedding-based approach, which learns both sememe and word embeddings jointly. In this approach, we represent sememes with distributed vectors as well and place them into the same semantic space as words. Similar to SPSE (Xie et al., 2017), which learns sememe embeddings by decomposing word-sememe matrix and sememe-sememe matrix, our method utilizes sememe embeddings as regularizers to learn better word embeddings. Different from SPSE, we do not use pre-trained word embeddings. Instead, we learn word embeddings and sememe embeddings simultaneously.\nMore specifically, from HowNet we can extract a source-side word-sememe matrixMS with MSsj = 1 indicating word wSs is annotated with sememe xj , otherwise MSsj = 0. Hence by factorizing MS , we can define the loss function as: Lsememe = \u2211\nwSs \u2208V S ,xj\u2208X\n(wSs \u00b7xj+bs+b\u2032j\u2212MSsj)2,\n(11) where bs and b\u2032j are the biases of wSs and xj , and X denotes sememe set. In this approach, we obtain word and sememe embeddings in a unified semantic space. The sememe embeddings bear all the information about the relationships between words and sememes, and they inject the information into word embeddings. Therefore, the word embeddings are expected to be more suitable for sememe prediction."
        },
        {
            "heading": "3.4 Training and Prediction",
            "text": ""
        },
        {
            "heading": "Training",
            "text": "When training monolingual word embeddings, we use negative sampling following Mikolov et al. (2013a). In the optimization of sememe part, we adopt the iterative updating method following Faruqui et al. (2015) for word relation-based approach and stochastic gradient descent (SGD) for sememe embedding-based approach. As for the optimization of the seed lexicon term of crosslingual part, we also apply SGD. Nevertheless, due to the existence of the latent variable, optimization of the matching process in cross-lingual part poses a challenge. We settle on Viterbi EM algorithm to address the problem. Next, we still take the target-to-source side as an example and give a detailed description of the training process using Viterbi EM algorithm. Viterbi EM algorithm alternates between a Viterbi E step and a subsequent M step. The Viterbi E step aims to find the most probable matched word pairs given the current parameters. Considering the independence, we can seek the match for each word individually:\nm\u0302t = argmax s\u2208{0,1,\u00b7\u00b7\u00b7 ,|V S |} P (wTt |wSs ). (12)\nAs for the parametrization of the matching probability, there are various choices. For computational simplicity, we select cosine similarity:\nP (wTt |wSs ) = { \u03f5 if s = 0, cos(wTt ,wSs ) otherwise, (13)\nwhere \u03f5 is a hyperparameter indicating the probability of matching the empty word. Therefore, the Viterbi E step computes matching by:\nm\u0303t = argmax s\u2208{1,\u00b7\u00b7\u00b7 ,|V S |} cos(wTt ,wSs ), (14)\nm\u0302t = { m\u0303t if cos(wTt ,wSm\u0303t) > \u03f5, 0 otherwise.\n(15)\nFrom this, we can see that \u03f5 serves as a threshold to keep out unreliable matched pairs. The Viterbi M step performs maximization as if the latent variable has been observed in the Viterbi E step. Thus, we can treat the matched pairs as correct translations, and use a L2 regularizer as well. Consequently, the M step computes:\n(w\u0302S , w\u0302T ) = argmax wS ,wT M(wS ,wT ), (16)\nwhereM(wS ,wT ) is defined as: M(wS ,wT ) = \u2212 |V T |\u2211 t=1 I[m\u0303t \u0338= 0] c(wTt ) |CT | \u2225wTt \u2212wSm\u0303t\u2225 2.\n(17)"
        },
        {
            "heading": "Prediction",
            "text": "Since we assume that words with similar sememe annotations are similar and similar words should have similar sememes, which resembles collaborative filtering in personalized recommendation, we can recommend sememes for target words according to their most similar source words. Formally, we define the score function P (xj |wTt ) of sememes xj given a target word wTt as:\nP (xj |wTt ) = \u2211\nwSs \u2208V S cos(wSs ,wTt )\u00b7MSsj \u00b7crs , (18)\nwhere rs is the descending rank of word similarity cos(wSs ,wTt ) for the source word wSs , and c \u2208 (0, 1) is a hyperparameter. Thus, crs is a declined confidence factor which can eliminate the noise from irrelevant sourcewords and concentrate on the most similar source words when predicting sememes for target words."
        },
        {
            "heading": "4 Experiments",
            "text": "In this section, we first introduce the dataset used in the experiments and then describe the experimental settings of both baseline method and our\nmodel. Next, we present the experimental results of different methods on the task of cross-lingual lexical sememe prediction. And then we conduct detailed analysis and exhaustive case studies. Following this, we investigate the effect of word frequency on cross-lingual sememe prediction results. Finally, we perform further quantitative analysis through two sub-tasks including bilingual lexicon induction and word similarity computation."
        },
        {
            "heading": "4.1 Dataset",
            "text": "We use sememe annotations in HowNet for sememe prediction. HowNet annotates sememes for 118, 346 Chinese words and 104, 025 English words. The number of sememes in total is 1, 983. Since some sememes only appear few times in HowNet, which are expected to be unimportant, we filter out those low-frequency sememes. Specifically, the frequency threshold is 5, and the final number of distinct sememes used in our experiments is 1, 400. In our experiments, Chinese is source language and English is target language. To learn Chinese and English monolingual word embeddings, we extract about 2.0G text from Sogou-T1 and Wikipedia2 respectively. And we use THULAC3 (Li and Sun, 2009) for Chineseword segmentation. As for seed lexicon, we build it in a similar way to Zhang et al. (2017). First, we employ Google Translation API4 to translate the source side (Chinese) vocabulary. Then the translations in the target language (English) are queried again in the reverse direction to translate back to the source language (Chinese). And we only keep the translation pairs whose back translated words match with the original source words. In the task of bilingual lexicon induction, we opt for Chinese-English Translation Lexicon Version 3.05 to be the gold standard. In the task of word similarity computation, we choose WordSim-240 and WordSim-297 (Jin and Wu, 2012) datasets for Chinese, and WordSim-353 (Finkelstein et al., 2002) and SimLex-999 (Hill et al., 2015) datasets for English to evaluate the performance of our\n1Sogou-T is a corpus of web pages provided by a Chinese commercial search engine. https://www.sogou.com/ labs/resource/t.php\n2https://dumps.wikimedia.org/ 3http://thulac.thunlp.org/ 4https://cloud.google.com/translate/ 5https://catalog.ldc.upenn.edu/\nLDC2002L27\nmodel. These datasets contain word pairs as well as human-assigned similarity scores. The word vectors are evaluated by ranking the word pairs according to their cosine similarities, and measuring Spearman\u2019s rank correlation coefficient with the human ratings."
        },
        {
            "heading": "4.2 Experimental Settings",
            "text": "We empirically set the dimension of word and sememe embeddings to 200. And the embeddings are all randomly initialized. In monolingual word embedding learning, we follow the optimal parameter settings in Mikolov et al. (2013a). We set the window sizeK to 5, down-sampling rate for highfrequency words to 10\u22125, learning rate to 0.025 and the number of negative samples to 5. In crosslingual word embedding alignment, the seed lexicon term weight \u03bbs is 0.01, and the matching term weight \u03bbm is 1, 000. In sememe-based word representation, the number of shared sememes for synonyms in the word relation-based approach is 2. In the training of matching process, we set \u03f5 to 0.5 empirically. When predicting sememes for words in target language, we only consider 100most similar source words for each target word and the attenuation parameter c is 0.8. The testing set for cross-lingual lexical sememe prediction contains 2, 000 randomly selected English words from the vocabulary."
        },
        {
            "heading": "4.3 Cross-lingual Lexical Sememe Prediction",
            "text": "We evaluate our model by recommending sememes for English words. In HowNet, many words have multiple sememes, so that sememe prediction can be regarded as a multi-label classification task. We use mean average precision (MAP) and F1 score to evaluate the sememe prediction results. We compare our model that incorporates sememe information with word relation-based approach (named CLSP-WR) and our model which jointly trains word and sememe embeddings (named CLSP-SE) with a baseline method BiLex (Zhang et al., 2017), a bilingual WRL model without incorporation of sememe information. For BiLex, we use its trained bilingual word embeddings to predict sememes for the words in target language with our sememe prediction approach. Table 1 exhibits the evaluation results of crosslingual lexical sememe prediction with different\nseed lexicon sizes in {1000, 2000, 4000, 60006}. From the table, we can clearly see that: (1) Our two models perform much better compared with BiLex in all the seed lexicon size settings. It indicates that incorporating sememe information into word embeddings can effectively improve the performance of predicting sememes for target words. The reason is that both of our models makewords with similar sememe annotations have similar embeddings, and as a result, we can recommend better sememes for target words according to its related source words. (2) CLSP-SE model achieves better results than CLSP-WRmodel. The reason is that by representing sememes in a latent semantic space, CLSPSE model can further capture the relatedness between sememes as well as the relatedness between words and sememes, which is helpful for modeling the representations of those words with similar sememes."
        },
        {
            "heading": "4.4 Case Study",
            "text": "In case study, we conduct qualitative analysis to explain the effectiveness of our models with detailed cases. We show two examples of crosslingual word sememe prediction, in which we predict sememes for handcuffs and canoeist. Fig. 2 shows the embeddings of five closest Chinese and English words to handcuffs and canoeist, and the vector of each word is projected down to two dimensions using t-SNE (Maaten and Hinton, 2008).\n6The largest seed lexicon size is 6000 because that is the maximum number of translation word pairs that we can obtain from the bilingual corpora.\nTable 2 lists top-5 sememes we predict for the two words and the sememes annotated for each word in HowNet are in boldface. In the table, we also exhibit the annotated sememes of the five closest Chinese words.\nIn the first example, our model finds the best translated word for handcuffs in Chinese \u2f3f \u94d0 \u201chandcuffs\u201d, whose sememe annotations are exactly the same as those of handcuffs. In addition, the second closest Chinese word \u9563 \u94d0 \u201cshackles\u201d is a synonym for \u2f3f\u94d0 \u201chandcuffs\u201d and also has the same sememe annotations. Therefore, our model predicts all the correct sememes successfully. From the prediction results of this example, we notice that our model can accurately predict general sememes like \u7528\u5177 \u201ctool\u201d and \u2f08 \u201chuman\u201d, which are supposed to be difficult to predict.\nIn the second example, accurate Chinese translated counterpart for canoeist does not exist, but our model still hits all the three annotated sememes in the top-5 predicted sememes. By observing the most similar Chinese words, we can find that although these words do not have the same meaning as canoeist, they are related to canoeist in different aspects. For example, \u77ed\u8dd1 \u201csprint\u201d and canoeist are both in the sports domain so that they share the sememes \u953b\u70bc \u201cexercise\u201d and \u4f53\u80b2 \u201csport\u201d. \u540d\u5c06 \u201csports star\u201d has the meaning of sports star and it can provide the sememe \u2f08 \u201chuman\u201d in sememe prediction. Furthermore, it is noteworthy that our model predicts \u8239 \u201cship\u201d due to the nearest Chinese words \u72ec\u2f4a\u2f88 \u201ccanoe\u201d and \u76ae\u8247 \u201ckayak\u201d, whereas \u8239 \u201cship\u201d is not annotated for canoeist in HowNet. It is obvious that \u8239 \u201cship\u201d is an appropriate sememe for canoeist. Since HowNet is manually annotated by experts, misannotated words always exist inevitably, which in some cases underestimates our models."
        },
        {
            "heading": "4.5 Effect of Word Frequency",
            "text": "To explore how frequencies of target words affect cross-lingual sememe prediction results, we split the testing set into four subsets according to word frequency and then calculate the sememe predictionMAP and F1 score for each subset. The results are shown in Table 3.\nFrom the table we can see that: (1) The more frequently a target word appears in the corpus, the better its predicted sememes are. It is because high-frequency words normally have better word embeddings, which are crucial to sememe prediction. (2) Our models evidently perform better than BiLex in different word frequencies, especially in low frequency. It indicates that by considering external information of HowNet, our models are more robust and can competently handle sparse\nscenarios."
        },
        {
            "heading": "4.6 Further Quantitative Analysis",
            "text": "In this section, we conduct two typical auxiliary experiments to further analyze the superiority of our models quantitatively."
        },
        {
            "heading": "Bilingual Lexicon Induction",
            "text": "Our models learn bilingual word embeddings in one unified semantic space. Here we use translation top-1 and top-5 average precision (P@1 and P@5) to evaluate bilingual lexicon induction performance of our models and BiLex. The seed lexicon size also varies in {1000, 2000, 4000, 6000}.\nThe results are shown in Table 4. From this table, we observe that our models, especially CLSPSEmodel, enhance the performance of word translation compared to BiLex no matter how large the seed lexicon is. It indicates that our models can bind bilingual word embeddings better."
        },
        {
            "heading": "Word Similarity Computation",
            "text": "We also evaluate the task of monolingual word similarity computation on WordSim-240 (WS240) and WordSim-297 (WS-297) datasets for Chinese, and WordSim-353 (WS-353) and SimLex-999 (SL-999) datasets for English.\nTable 5 shows the results of monolingual word similarity computation on four datasets. From the table, we find that: (1) Our models perform better than BiLex on both Chinese word similarity datasets. It signifies incorporating sememe information helps learn better monolingual embeddings; (2) CLSP-WR model does not enhance English word similarity results but CLSPSE model does. It is because CLSP-WR model only post-processes Chineseword embeddings and keeps English word embeddings unchanged while CLSP-SE model undertakes bilingual alignment and sememe information incorporation together, which makes English word embeddings improve with Chinese word embeddings."
        },
        {
            "heading": "5 Conclusion and Future Work",
            "text": "In this paper, we introduce a new task of crosslingual sememe prediction. This task is very important because the construction of sememe-based linguistic knowledge bases in various languages is beneficial to better understanding these languages. We propose a simple and effective model for this task, including monolingual word representation learning, cross-lingual word representation alignment and sememe-based word representation learning. Experimental results on real-world datasets show that our model achieves consistent and significant improvements compared to baseline method in cross-lingual sememe prediction. In the future, we will explore the following research directions: (1) In this paper, for simplification, we ignore the rich hierarchy information in HowNet and also ignore the fact that a word may have multiple senses. We will extend our\nmodels to consider the structure information of sememe and multiple senses of words; (2) In fact, our framework for cross-lingual lexical sememe prediction can be transferred to other cross-lingual tasks. We will explore the effectiveness of our model in these tasks such as cross-lingual information retrieval."
        },
        {
            "heading": "Acknowledgments",
            "text": "This research is funded by the National 973 project (No. 2014CB340501). It is also partially supported by the NExT++ project, the National Research Foundation, Prime Minister\u2019s Office, Singapore under its IRC@Singapore Funding Initiative. Hao Zhu is supported by Tsinghua University Initiative Scientific Research Program. We also thank the anonymous reviewers for their valuable comments and suggestions."
        }
    ],
    "title": "Cross-lingual Lexical Sememe Prediction",
    "year": 2018
}