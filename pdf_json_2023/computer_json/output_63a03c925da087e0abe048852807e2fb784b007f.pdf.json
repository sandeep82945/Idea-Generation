{
    "abstractText": "18 Emerging viral infections, especially the global pandemic COVID-19, have had 19 catastrophic impacts on public health worldwide. The culprit of this pandemic, 20 SARS-CoV-2, continues to evolve, giving rise to numerous sublineages with dis21 tinct characteristics. The traditional post-hoc wet-lab approach is lagging behind, 22 and it cannot quickly predict the evolutionary trends of the virus while consuming 23 high costs. Capturing the evolutionary drivers of virus and predicting poten24 tial high-risk mutations has become an urgent and critical problem to address. 25 To tackle this challenge, we introduce ProtFound-V, an evolution-inspired deep26 learning framework designed to explore the mutational trajectory of virus. Take 27 SARS-CoV-2 as an example, ProtFound-V accurately identifies the evolutionary 28 advantage of Omicron and proposes evolutionary trends consistent with wet29 lab experiments through in silico deep mutational scanning. This showcases the 30 potential of deep learning predictions to replace traditional wet-lab experimental 31 measurements. With the evolution-guided large language model, ProtFound-V 32 1 . CC-BY-NC-ND 4.0 International license perpetuity. It is made available under a preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in The copyright holder for this this version posted November 27, 2023. ; https://doi.org/10.1101/2023.11.27.568815 doi: bioRxiv preprint",
    "authors": [
        {
            "affiliations": [],
            "name": "Zhiwei Nie"
        },
        {
            "affiliations": [],
            "name": "Xudong Liu"
        },
        {
            "affiliations": [],
            "name": "Jie Chen"
        },
        {
            "affiliations": [],
            "name": "Zhennan Wang"
        },
        {
            "affiliations": [],
            "name": "Yutian Liu"
        },
        {
            "affiliations": [],
            "name": "Haorui Si"
        },
        {
            "affiliations": [],
            "name": "Tianyi Dong"
        },
        {
            "affiliations": [],
            "name": "Fan Xu"
        },
        {
            "affiliations": [],
            "name": "Guoli Song"
        },
        {
            "affiliations": [],
            "name": "Yu Wang"
        },
        {
            "affiliations": [],
            "name": "Peng Zhou"
        },
        {
            "affiliations": [],
            "name": "Wen Gao"
        },
        {
            "affiliations": [],
            "name": "Yonghong Tian"
        }
    ],
    "id": "SP:6813d5c1c1f8d0378fdc7d4285505e5b53faf873",
    "references": [
        {
            "authors": [
                "T. Kuiken",
                "R. Fouchier",
                "G. Rimmelzwaan",
                "A. Osterhaus"
            ],
            "title": "Emerging viral 666 infections in a rapidly changing world",
            "venue": "Current opinion in biotechnology 14,",
            "year": 2003
        },
        {
            "authors": [
                "G.G. Luo",
                "Gao",
                "S.-J"
            ],
            "title": "Global health concerns stirred by emerging viral 669 infections",
            "venue": "Journal of medical virology 92,",
            "year": 2020
        },
        {
            "authors": [
                "L Prokunina-Olsson"
            ],
            "title": "Covid-19 and emerging viral infections: The case for 671 interferon lambda",
            "venue": "Journal of Experimental Medicine",
            "year": 2020
        },
        {
            "authors": [
                "R Lu"
            ],
            "title": "Genomic characterisation and epidemiology of 2019 novel coro- 673 navirus: implications for virus origins and receptor binding. The lancet",
            "year": 2020
        },
        {
            "authors": [
                "C.B. Jackson",
                "M. Farzan",
                "B. Chen",
                "H. Choe"
            ],
            "title": "Mechanisms of sars-cov-2 entry 676 into cells",
            "venue": "Nature reviews Molecular cell biology 23,",
            "year": 2022
        },
        {
            "authors": [
                "J Shang"
            ],
            "title": "Structural basis of receptor recognition by sars-cov-2",
            "venue": "Nature",
            "year": 2020
        },
        {
            "authors": [
                "Walls",
                "A. C"
            ],
            "title": "Structure, function, and antigenicity of the sars-cov-2 spike 680 glycoprotein",
            "venue": "Cell 181,",
            "year": 2020
        },
        {
            "authors": [
                "K Tao"
            ],
            "title": "The biological and clinical significance of emerging sars-cov-2 682 variants",
            "venue": "Nature Reviews Genetics 22,",
            "year": 2021
        },
        {
            "authors": [
                "R.L. Graham",
                "R.S. Baric"
            ],
            "title": "Recombination, reservoirs, and the modular spike: 684 mechanisms of coronavirus cross-species transmission",
            "venue": "Journal of virology 84,",
            "year": 2010
        },
        {
            "authors": [
                "A Vaswani"
            ],
            "title": "Attention is all you need. Advances in neural information 687 processing systems",
            "year": 2017
        },
        {
            "authors": [
                "A Elnaggar"
            ],
            "title": "Prottrans: Toward understanding the language of life through 691 self-supervised learning",
            "venue": "IEEE transactions on pattern analysis and machine",
            "year": 2021
        },
        {
            "authors": [
                "S. Wang",
                "Y. Guo",
                "Y. Wang",
                "H. Sun",
                "J. Huang"
            ],
            "title": "Smiles-bert: large scale 694 unsupervised pre-training for molecular property prediction",
            "year": 2019
        },
        {
            "authors": [
                "Z Wu"
            ],
            "title": "Knowledge-based bert: a method to extract molecular features like 696 computational chemists",
            "venue": "Briefings in Bioinformatics 23,",
            "year": 2022
        },
        {
            "authors": [
                "V. Bagal",
                "R. Aggarwal",
                "P. Vinod",
                "U.D. Priyakumar"
            ],
            "title": "Molgpt: molecular 698 generation using a transformer-decoder model",
            "venue": "Journal of Chemical Information 699 and Modeling",
            "year": 2021
        },
        {
            "authors": [
                "J Chen"
            ],
            "title": "Running ahead of evolution\u2014ai-based simulation for predict- 701 ing future high-risk sars-cov-2 variants",
            "venue": "The International Journal of High 702 Performance Computing Applications",
            "year": 2023
        },
        {
            "authors": [
                "J Gu"
            ],
            "title": "Recent advances in convolutional neural networks",
            "venue": "Pattern 704 recognition 77,",
            "year": 2018
        },
        {
            "authors": [
                "C. Huang",
                "W. Talbott",
                "N. Jaitly",
                "J.M. Susskind"
            ],
            "title": "Efficient representation 706 learning via adaptive context pooling",
            "venue": "(PMLR,",
            "year": 2022
        },
        {
            "authors": [
                "Q. Dong",
                "S. Gong",
                "X. Zhu"
            ],
            "title": "Class rectification hard mining for imbalanced deep",
            "year": 2017
        },
        {
            "authors": [
                "H Sheng"
            ],
            "title": "Mining hard samples globally and efficiently for person 710 reidentification",
            "venue": "IEEE Internet of Things Journal",
            "year": 2020
        },
        {
            "authors": [
                "Lin",
                "T.-Y",
                "P. Goyal",
                "R. Girshick",
                "K. He",
                "P. Doll\u00e1r"
            ],
            "title": "Focal loss for dense object",
            "year": 2017
        },
        {
            "authors": [
                "Y. Zhang",
                "Q. Yang"
            ],
            "title": "An overview of multi-task learning",
            "venue": "National Science",
            "year": 2018
        },
        {
            "authors": [
                "Y. Zhang",
                "Q. Yang"
            ],
            "title": "A survey on multi-task learning",
            "venue": "IEEE Transactions on 716 Knowledge and Data Engineering",
            "year": 2021
        },
        {
            "authors": [
                "G Nelson"
            ],
            "title": "Molecular dynamic simulation reveals e484k mutation enhances 718 spike rbd-ace2 affinity and the combination of e484k, k417n and n501y muta- 719 tions (501y. v2 variant) induces conformational change greater than n501y mutant 720 alone, potentially resulting in an escape mutant",
            "venue": "BioRxiv",
            "year": 2021
        },
        {
            "authors": [
                "B. Luan",
                "H. Wang",
                "T. Huynh"
            ],
            "title": "Enhanced binding of the n501y-mutated sars- 726 cov-2 spike protein to the human ace2 receptor: insights from molecular dynamics 727 simulations",
            "venue": "FEBS letters 595,",
            "year": 2021
        },
        {
            "authors": [
                "Y Yang"
            ],
            "title": "Key residues of the receptor binding domain in the spike protein 729 of sars-cov-2 mediating the interactions with ace2: a molecular dynamics study",
            "venue": "Nanoscale",
            "year": 2021
        },
        {
            "authors": [
                "C Chen"
            ],
            "title": "Computational prediction of the effect of amino acid changes on 732 the binding affinity between sars-cov-2 spike rbd and human ace2",
            "venue": "Proceedings of 733 the National Academy of Sciences 118,",
            "year": 2021
        },
        {
            "authors": [
                "Starr",
                "T. N"
            ],
            "title": "Deep mutational scanning of sars-cov-2 receptor binding domain 735 reveals constraints on folding and ace2 binding. cell",
            "year": 2020
        },
        {
            "authors": [
                "Starr",
                "T. N"
            ],
            "title": "Deep mutational scans for ace2 binding, rbd expression, 737 and antibody escape in the sars-cov-2 omicron ba. 1 and ba. 2 receptor-binding 738 domains",
            "venue": "PLoS pathogens 18,",
            "year": 2022
        },
        {
            "authors": [
                "R. Zhang",
                "S. Ghosh",
                "R. Pal"
            ],
            "title": "Predicting binding affinities of emerging vari- 740 ants of sars-cov-2 using spike protein sequencing data: observations, caveats and 741 recommendations",
            "venue": "Briefings in Bioinformatics 23,",
            "year": 2022
        },
        {
            "authors": [
                "M Heinzinger"
            ],
            "title": "Modeling aspects of the language of life through transfer- 747 learning protein sequences",
            "venue": "BMC bioinformatics 20,",
            "year": 2019
        },
        {
            "authors": [
                "A Elnaggar"
            ],
            "title": "Prottrans: towards cracking the language of life\u2019s code through 749 self-supervised deep learning and high performance computing",
            "venue": "arXiv preprint",
            "year": 2007
        },
        {
            "authors": [
                "A Rives"
            ],
            "title": "Biological structure and function emerge from scaling unsu- 752 pervised learning to 250 million protein sequences",
            "venue": "Proceedings of the National 753 Academy of Sciences 118,",
            "year": 2021
        },
        {
            "authors": [
                "J Meier"
            ],
            "title": "Language models enable zero-shot prediction of the effects of muta- 755 tions on protein function",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "A Moulana"
            ],
            "title": "Compensatory epistasis maintains ace2 affinity in sars-cov-2 760 omicron ba",
            "venue": "Nature Communications",
            "year": 2022
        },
        {
            "authors": [
                "P Han"
            ],
            "title": "Receptor binding and complex structures of human ace2 to spike 762 rbd from omicron and delta sars-cov-2",
            "venue": "Cell 185,",
            "year": 2022
        },
        {
            "authors": [
                "C Yue"
            ],
            "title": "Ace2 binding and antibody evasion in enhanced transmissibility of 764 xbb",
            "venue": "The Lancet Infectious Diseases",
            "year": 2023
        },
        {
            "authors": [
                "Y Cao"
            ],
            "title": "Imprinted sars-cov-2 humoral immunity induces convergent omicron 766 rbd evolution",
            "venue": "Nature 614,",
            "year": 2023
        },
        {
            "authors": [
                "A Yisimayi"
            ],
            "title": "Repeated omicron infection alleviates sars-cov-2 immune 768 imprinting",
            "venue": "bioRxiv 2023\u201305",
            "year": 2023
        },
        {
            "authors": [
                "M. Steinegger",
                "J. S\u00f6ding"
            ],
            "title": "Clustering huge protein sequence sets in linear time",
            "venue": "Nature communications 9,",
            "year": 2018
        },
        {
            "authors": [
                "A.J. Greaney",
                "T.N. Starr",
                "J.D. Bloom"
            ],
            "title": "An antibody-escape estimator for 772 mutations to the sars-cov-2 receptor-binding domain. Virus evolution",
            "year": 2022
        },
        {
            "authors": [
                "N. Ferruz",
                "B. H\u00f6cker"
            ],
            "title": "Controllable protein design with language models",
            "venue": "Nature 775 Machine Intelligence",
            "year": 2022
        },
        {
            "authors": [
                "Suzek",
                "B. E"
            ],
            "title": "Uniref clusters: a comprehensive and scalable alternative for 777 improving sequence similarity searches",
            "venue": "Bioinformatics 31,",
            "year": 2015
        },
        {
            "authors": [
                "J. Devlin",
                "Chang",
                "M.-W",
                "K. Lee",
                "Toutanova",
                "K. Bert"
            ],
            "title": "Pre-training of 779 deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint 780",
            "year": 2018
        },
        {
            "authors": [
                "Y You"
            ],
            "title": "Large batch optimization for deep learning: Training bert",
            "venue": "minutes. arXiv preprint arXiv:1904.00962",
            "year": 2019
        },
        {
            "authors": [
                "T. Litfin",
                "Y. Yang",
                "Y. Zhou"
            ],
            "title": "Spot-peptide: template-based prediction 784 of peptide-binding proteins and peptide-binding sites. Journal of chemical 785 information and modeling",
            "year": 2019
        },
        {
            "authors": [
                "Y Lei"
            ],
            "title": "A deep-learning framework for multi-level peptide\u2013protein 787 interaction prediction",
            "venue": "Nature communications 12,",
            "year": 2021
        },
        {
            "authors": [
                "H. \u00d6zt\u00fcrk",
                "A. \u00d6zg\u00fcr",
                "E. Ozkirimli"
            ],
            "title": "Deepdta: deep drug\u2013target binding affinity 789 prediction",
            "venue": "Bioinformatics 34,",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "Evolution-guided large language model is a1\npredictor of virus mutation trends2\nZhiwei Nie1,2\u2020, Xudong Liu1,2\u2020, Jie Chen1,2*, Zhennan Wang2,3 Yutian Liu3,2, Haorui Si4,5,6, Tianyi Dong4,5,6, Fan Xu2,4 Guoli Song2, Yu Wang2, Peng Zhou6, Wen Gao2,3,5 Yonghong Tian1,3,2*6\n1School of Electronic and Computer Engineering, Peking University,7 Shenzhen, China.8\n2Peng Cheng Laboratory, Shenzhen, China.9 3School of Computer Science, Peking University, China.10 4CAS Key Laboratory of Special Pathogens, Wuhan Institute of11 Virology, Chinese Academy of Sciences, Wuhan, China.12 5University of Chinese Academy of Sciences, Beijing, China.13 6Guangzhou Laboratory, Guangzhou, China.14\n*Corresponding author(s). E-mail(s): jiechen2019@pku.edu.cn;15 yhtian@pku.edu.cn;16 \u2020These authors contributed equally to this work.17\nAbstract18\nEmerging viral infections, especially the global pandemic COVID-19, have had19 catastrophic impacts on public health worldwide. The culprit of this pandemic,20 SARS-CoV-2, continues to evolve, giving rise to numerous sublineages with dis-21 tinct characteristics. The traditional post-hoc wet-lab approach is lagging behind,22 and it cannot quickly predict the evolutionary trends of the virus while consuming23 high costs. Capturing the evolutionary drivers of virus and predicting poten-24 tial high-risk mutations has become an urgent and critical problem to address.25 To tackle this challenge, we introduce ProtFound-V, an evolution-inspired deep-26 learning framework designed to explore the mutational trajectory of virus. Take27 SARS-CoV-2 as an example, ProtFound-V accurately identifies the evolutionary28 advantage of Omicron and proposes evolutionary trends consistent with wet-29 lab experiments through in silico deep mutational scanning. This showcases the30 potential of deep learning predictions to replace traditional wet-lab experimental31 measurements. With the evolution-guided large language model, ProtFound-V32\npresents a new state-of-the-art performance in key property predictions. Despite33 the challenge posed by epistasis to model generalization, ProtFound-V remains34 robust when extrapolating to lineages with different genetic backgrounds. Overall,35 this work paves the way for rapid responses to emerging viral infections, allowing36 for a plug-and-play approach to understanding and predicting virus evolution.37\n1 Main38\nSince 2000, numerous emerging viral infections [1, 2], especially COVID-19 [3], have39 had a profound global impact. Severe acute respiratory syndrome coronavirus 240 (SARS-CoV-2), the causative agent of COVID-19, is a single-stranded RNA virus41 belonging to the coronaviridae family [4]. It enters cells by targeting the human42 angiotensin-converting enzyme 2 (ACE2) as the host receptor [5\u20137]. To combat the43 ongoing pandemic, various vaccines and drugs have been designed to mitigate the44 damage caused by SARS-CoV-2. However, a concerning aspect is that SARS-CoV-245 continues to evolve, with mutations accumulating and recombination processes con-46 tributing to increased diversity [8, 9]. The sub-lineages of Omicron, in particular,47 remain prevalent and continue to cause breakthrough infections.48 The increasing frequency of emerging viral infections demands rapid human49 responses. Understanding the evolution mechanism and predicting the direction of50 viral evolution is a potent approach to tackle these infections at their root. Artificial51 intelligence technology, particularly large language models (LLMs) based on Trans-52 former [10], has shown promising applications in various life science fields, including53 proteins [11, 12] and molecules [13\u201315]. Inspired by this progress, our aim is to cre-54 ate a dedicated large language model tailored specifically for studying virus evolution.55 However, direct transfer of LLMs to investigate viral evolutionary mechanisms faces56 significant challenges due to the unique traits of virus evolution. Despite viruses hav-57 ing a high mutation rate during their evolution, only a small number of offspring58 acquire mutations that are beneficial to themselves. Consequently, virus variants typ-59 ically exhibit few-site mutations, and beneficial mutations are far fewer than harmful60 mutations in the final mutational outcomes.61\nIn our previous work [16], we proposed a pipeline for SARS-CoV-2 mutation sim-62 ulation that approximates a lineage through high-throughput variant generation and63 screening. In this work, we go one step further and develop an evolution-inspired64 framework, ProtFound-V (Protein Foundation Model for Virus), for viral property65 prediction (Fig.1). With the modeling challenges caused by evolutionary traits greatly66 alleviated, the prediction of evolutionary trends consistent with wet-lab experiments is67 achieved. Specifically, ProtFound-V addresses two challenges posed by the evolution-68 ary nature of viruses. Firstly, we introduce a local-global feature coupling mechanism69 that captures amino acid interactions at different scales, enabling the detection of the70 subtle effects of few-site mutations. Secondly, we design a multi-task focal loss func-71 tion to identify rare beneficial mutations. By combining evolution-inspired modeling72 with large language models, ProtFound-V effectively learns the evolutionary rules of73 viruses, enabling the prediction of key properties and evolutionary trends. We demon-74 strate the efficacy of ProtFound-V using SARS-CoV-2 as an example, analyzing its75 evolutionary advantages and predicting evolutionary trends based on three key proper-76 ties: ACE2 binding, expression, and antibody escape. ProtFound-V provides not only77 insights into the leap-forward variation of SARS-CoV-2, from its wild type to Omicron,78 but also accurately predicts its evolution trends. As a powerful tool to study virus79 evolution, ProtFound-V can facilitate rapid responses to emerging viral infections by80 aiding in vaccine and drug design.81\n2 Results82\n2.1 Evolution-inspired architecture of ProtFound-V83\nThe evolution-inspired architecture of ProtFound-V consists of four modules as shown84 in Fig.2a, including protein language model pretraining, feature extracting, local-85 global feature coupling, and multi-task learning. Specifically, the first module is the86 pretraining of protein language model, in which two-step training strategy is adopted87 to promote convergence and improve efficiency of the training process (Fig.2b). After88 the self-supervised learning on approximately 144 million raw protein sequences, pre-89 dicted amino acid probabilities of each position of protein sequences are mapped. The90 second module is feature extracting of SARS-CoV-2 receptor-binding domain (RBD)91 or antibody sequences. For ACE2 binding and RBD expression prediction, only the92 mutated RBDs are taken as input. However, for antibody escape prediction, both93 mutated RBDs and antibodies are input because there are multiple antibodies. The94 embeddings of each amino acid in the mutated RBD or antibody are extracted in this95 module, and they are pooled through length to obtain fixed-size protein-level embed-96 ding in the next module, called local-global feature coupling. The few-site mutations97 lead to an extremely high degree of similarity between virus variants, which makes98 it difficult to learn the effects of few-site mutations. While a mutation affects nearby99 amino acids, it may also interact with distant amino acids. Therefore, we design the100 local-global feature coupling module to fuse the effects of mutations on amino acid101 interactions at different scales, thereby amplifying the subtle effects of few-site muta-102 tions. Convolutional Neural Network (CNN) [17] and ContextPool-attention [18] are103 used to mine the interactions of nearby amino acids (local feature) and distant amino104\nacids (global feature), respectively. The fused protein-level embeddings are fed into the105 fourth module, multi-task learning. This module aims to solve the hard sample min-106 ing problem caused by the rarity of beneficial mutations, which is a difficult problem107 often encountered in the field of deep learning [19, 20]. Focal Loss for object detec-108 tion [21] achieves better results in classification under class imbalance, but a similar109 loss function is lacking in regression. To fill this gap, the regression focal loss function110 is proposed, which puts more focus on difficult-to-learn samples. Furthermore, con-111 sidering the high correlation between classification and regression tasks of a certain112 property, we introduce multi-task learning [22, 23] to utilize useful information between113 these two related tasks to improve prediction performance. In summary, ProtFound-V114 solves the virus-evolution-induced problem of capturing the subtle effects of few-site115\nmutations and rare beneficial mutations by leveraging large protein language models,116 which supports the exploration of virus evolution mechanism.117\n2.2 Local-global feature coupling reveals the subtle effects of118 few-site mutations119\nAs the target of virus entry into human cells, the binding of SARS-CoV-2 RBD120 to human ACE2 is crucial, which determines the course of infection. To quantify the121 ACE2 binding strength, most of the existing methods [24\u201328] need to preform time-122 consuming molecular dynamics simulations to obtain trajectories between the mutated123\nRBDs and ACE2, and then evaluate the binding strength. In contrast, ProtFound-V124 can rapidly assess and predict the ACE2 binding strength of variants in an efficient125 manner. Here, due to the importance of ACE2 binding for infection, we take ACE2126 binding prediction as an example to demonstrate the advantage of ProtFound-V in127 mining evolutionary trajectories.128 Although viruses usually have a high mutation rate during evolution, only a few129 offspring acquire beneficial mutations. This leads to a very similarity between the130 mutated RBD sequences, that is, there are only a few residue differences. Therefore,131 the initial embeddings extracted by ProtFound-V are too resembling to be distin-132 guished, which makes it difficult for the neural network to learn the subtle effects of133 few-site mutations. The proposed local-global feature coupling is capable of exploit134 the interactions between amino acids at different scales (Fig.2c). CNN extracts the135 local dependencies, i.e., contextual features between residues of mutated RBDs. The136 ContextPool-attention, an improved version of standard self-attention[10] for efficient137 representation learning, is adopted to capture long-range dependencies of amino acids.138 It models long-range interactions with adapting attention granularity for each token139 instead of assuming a fixed granularity to construct the query and key vectors for indi-140 vidual tokens as in self-attention mechanism, thus better models complex dependencies141 at higher levels. Followed by the feature coupling, the local and global information can142 be fused to represent subtle difference among mutated RBDs. The max-pooling oper-143 ations are adopted on the amino-acid-dimension to get the protein-level embedding of144 the whole RBD sequence, which is subsequently used for multi-task learning.145 To demonstrate the impact of local-global feature coupling mechanism, we per-146 form the dimensional reduction visualization on the embeddings of mutated RBDs147 using Principal Component Analysis (PCA). Previous work [29] used deep muta-148 tional scanning (DMS) to systematically observe the effects of single-site mutations149 to the SARS-CoV-2 RBD on ACE2 binding and expression. Therefore, we obtain the150 sequences of mutated RBDs with single-site mutations based on this wet-lab data for151 dimensional reduction. It can be seen in Fig.3 that positive and negative samples, i.e.,152 binding-affinity-increased variants and binding-affinity-decreased variants, are clearly153 distinguished whether in the form of three-category or two-category after fusion of154 global and local dependencies. The results demonstrate that the subtle effects of single-155 site mutations are accurately identified by mining and fusing interactions between156 nearby and distant amino acids. We can conclude that local-global feature coupling157 reveals subtle effects of few-site mutations through fusion of interactions at different158 scales.159\n2.3 Multi-task focal loss captures rare beneficial mutations160\nAs an RNA virus, SARS-CoV-2 has a high mutation rate, but most of the mutations161 are harmful, and only a small number of mutations can give the virus a stronger162 selection advantage. Therefore, this can be translated into a severe class imbalance163 problem for deep learning. The effects of mutations on key properties of viruses can164 be defined as two types of related tasks for prediction, i.e. classification (increase or165 not)and regression (the specific value). Leveraging useful information between the two166 related tasks to improve model generalization, the multi-task learning architecture167\nis designed to improve prediction performance. Inspired by Focal Loss [21] designed168 for classification, we propose a hard sample mining loss function for regression. The169 classification-oriented focal loss and regression-oriented focal loss are weighted summed170 as a whole, called multi-task focal loss, to improve the performance of hard sample171 mining.172\nTo demonstrate the improvement of our multi-task focal loss in hard sample min-173 ing, we use ACE2 binding prediction task for ablation experiments and case studies.174 Here, deep mutational scanning dataset [29] is adopted for ACE2 binding predic-175 tion, which has the obvious imbalance between positive and negative samples, i.e.,176 the binding-affinity-increased mutations and binding-affinity-decreased mutations are177 usually differ by nearly an order of magnitude. For binding affinity prediction between178 mutated RBDs and human ACE2, multilayer perceptron (MLP) is adopted here to pre-179 dict the association constant (Ka) of variants relative to wild type RBD, i.e. Ka ratio,180 to judge whether the mutation has increased the binding strength. Ka ratio values181 greater than one represent increased binding affinity and values less than one repre-182 sent decreased binding affinity. Therefore, a classification task (increase or not) and a183 regression task (the Ka ratio) are combined here to drive the multi-task learning.184\nFor classification tasks and regression tasks, the most common loss functions are185 Mean Squared Error (MSE) Loss and Binary Cross Entropy (BCE) Loss. In order to186 explore the respective roles of Classification Focal Loss (ClsFocalLoss) and Regres-187 sion Focal Loss (RegFocalLoss), we construct three types of multi-task loss, including188 BCE&MSE, BCE&RegFocalLoss, and MSE&ClsFocalLoss. As shown in Fig.4a and189 Table S1, we compare our multi-task focal loss with the three multi-task losses on var-190 ious indicators. Due to the lack of ability to mine hard samples, BCE&MSE performs191 very poorly on the three key indicators, Accuracy, F1-Score, and Recall. After replacing192 MSE with RegFocalLoss, all six metrics for classification and regression tasks improve193 slightly. If we replace BCE with ClsFocalLoss, all six metrics improve significantly,194 especially on Accuracy, F1-Score, and Recall, i.e. the three key indicators. Our multi-195 task focal loss has the best performance on the three key metrics, achieving 91.11% for196 Accuracy, 91.58% for F1-Score, and 96.30% for Recall. The ablation experiment proves197 that the synergistic use of ClsFocalLoss and RegFocalLoss can effectively improve the198 performance of ProtFound-V in mining hard samples, i.e. accurately capturing rare199 beneficial mutations.200 As shown in Fig.4b, referring to the mutation sites of the recently popular sublin-201 eage XBB.1.5, we select 15 mutations with increased binding strength at corresponding202 sites in the deep mutational scanning dataset to test the ability of our multi-task203 focal loss to identify high-risk mutations. Multi-task focal loss achieves excellent pre-204 diction performance, correctly predicting 12 out of 15 high-risk mutations, i.e. 80%205 accuracy. In contrast, the prediction performance of BCE&MSE lags far behind, only206 correctly predicting 2 out of 15 high-risk mutations, i.e. about 13% accuracy. This case207 study demonstrates that our multi-task focal loss efficiently captures rare beneficial208 mutations, i.e. high-risk mutations.209\n2.4 ProtFound-V showcases a new state-of-the-art and robust210 performance211\n2.4.1 Binding affinity prediction on two benchmark datasets212\nWith the evolution-inspired architecture, ProtFound-V is competitive in high-risk213 mutation prediction. Here, ProtFound-V is tested on a single-site mutation benchmark214 dataset and a multi-site mutation benchmark dataset to demonstrate its advantages215 over other models. The deep mutational scanning dataset of wild type [29] is adopted216 as the single-site mutation benchmark dataset. The deep mutational scanning dataset217 of wild type [29], four VOCs (Alpha, Beta, Delta, Omicron), and one VOI (Eta)218 [30] are integrated to form the multi-site mutation benchmark dataset. The models219 involved are NN MM-GBSA [28], BiLSTM [31], CNN [32], ProtFound-V with one-hot220 as encoder, and ProtFound-V with other six protein language models as encoders.221 Specifically, the compared pretrained protein language models here are SeqVec [33],222 ProtTrans T5 [34], ESM-1b [35], ESM-1v [36], ESM-2 with 650 million and 15 billion223 parameters [11].224 As shown in Table S2 and Fig.5a, ProtFound-V achieves state-of-the-art perfor-225 mance on single-site mutation benchmark dataset and excels in key indicators for226 classification and regression. ProtFound-V with the protein language model as the227\nencoder is overall superior to other methods, which means that the protein language228 models have strong abilities to extract protein sequence information. Compared with229 other protein language models, our protein language model achieves the best perfor-230 mance even when the model parameters are much smaller than them, which proves231 that our model pretrained from scratch has ideal feature extraction ability. Over-232 all, ProtFound-V using our protein language model as an encoder has significantly233 improved compared to the baseline method (NN MM-GBSA), in which the Accuracy234 is boosted by more than 10% and the correlation exceeds baseline method by 8%.235 Previous works [37, 38] have implicated that epistasis drives ongoing major evo-236 lution of SARS-CoV-2 where the effect of a mutation is determined by its genetic237 background. New variants are constantly emerging, creating an ever-changing genetic238 background. Compared with ancestral Wuhan-Hu-1 background, the same mutations239\nof subsequent variants may cause different effects. Therefore, it is necessary to incor-240 porate more variants under different genetic backgrounds for model training, which241 helps ProtFound-V to update the effects of multi-site mutations. As shown in Table S3242 and Fig.5b, ProtFound-V outperforms other pretrained protein language models on243 multi-site mutation benchmark dataset in key indicators, including AUC, Accuracy,244 Precision, and the proposed metric Ordinal Pair Proportion (OPP ). The reman-245 ufactured multi-site benchmark dataset breaks the limit of ProtFound-V to more246 variants, bringing in the constantly updated effects of mutations under different genetic247 backgrounds.248\n2.4.2 Transfer ProtFound-V to different genetic backgrounds249\nAs the genetic background continues to change, the effects of mutations are con-250 stantly updating. Whether the model can generalize to predict high-risk mutations251 in unseen genetic backgrounds is crucial. Here, we adopt the deep mutational scan-252 ning data of four VOCs (Alpha, Beta, Delta, Omicron) and one VOI (Eta) [30] to253 test the robustness of ProtFound-V under different backgrounds. As shown in Fig.5c,254 ProtFound-V trained with single-site mutation dataset achieves robust prediction per-255 formance under different backgrounds, including Alpha, Beta, Eta, Delta, Omicron256 BA.1, and Omicron BA.2. Although only trained on a single-site mutation dataset,257 ProtFound-V is robust in predicting multi-site mutations. Due to possible random258 errors in wet-lab experiments, i.e. batch effect, the absolute values of the measured259 properties are not stable. As a result of this, the relative ranking among different260 variants has more reliable reference value. To this end, we propose a new evaluation261 metric, Ordinal Pair Proportion (OPP ), to represent the correctly predicted pro-262 portion of any pair of mutated RBDs. In the lineages before Omicron, the OPPs are263 maintained above 78%, and that of Eta even reaches above 85%. OPPs of the two264 Omicron sublineages (Omicron BA.1, Omicron BA.2) are smaller than those of pre-265 vious four lineages (Alpha, Beta, Eta, Delta), which is caused by the larger genetic266 background differences between Omicron and wild type. Even so, the OPP of Omicron267 BA.1 exceeds 71%, and that of Omicron BA.2 exceeds 66%. In conclusion, ProtFound-268 V remains robust in generalizing to new lineages in unseen genetic backgrounds, a269 benefit of evolution-inspired design.270\n2.5 ProtFound-V perceives evolutionary trends271\n2.5.1 ProtFound-V identifies the evolutionary advantage of Omicron272\nSince the outbreak of COVID-19, five VOCs have emerged, of which Omicron has273 developed many sublineages. Hence, the evolutionary advantages of Omicron are worth274 exploring, which can provide an important reference for analyzing future evolutionary275 trends. ACE2 binding is a crucial step for virus infection of host cells, so we started276 with it. ProtFound-V is applied to predict the binding affinity between human ACE2277 and five VOCs, including Alpha, Beta, Gamma, Delta, and Omicron. As shown in278 Table S4, the predicted Ka ratios (greater than 1 means higher binding affinity and279 vice versa) of Omicron are close to 1, which means that its binding strengths to human280 ACE2 is similar to SARS-CoV-2 wild type RBD. We can deduce that the evolutionary281\nadvantage of Omicron may lie not in binding affinity, but in immune escape and trans-282 missibility. In fact, Han et al [39] measured and analyzed Omicron from the aspects283 of binding affinity, cell infection efficiency, and structural interaction, and came to284 a consistent conclusion: Omicron has similar binding strength with wild type RBD,285 which may be due to compensation in terms of immune escape and transmissibility.286 However, even if the enhancement of immune escape or transmissibility is the future287 direction of Omicron\u2019s mutations, we should be clear that maintaining qualified affin-288 ity is the basis for the survival of variants. ProtFound-V recognizes the evolutionary289 advantage of Omicron, which is a powerful tool for understanding the great leap from290 wild type to Omicron.291\n2.5.2 ProtFound-V predicts evolutionary trends consistent with292 wet-lab experiments293\nThrough comprehensive validation of ProtFound-V on the binding affinity prediction,294 we demonstrate its ability to reveal few-site mutation effects and capture rare ben-295 eficial mutations, which is expected to be a powerful tool to predict the evolution296 trends of viruses. Evolutionary trends can facilitate the identification of potential high-297 risk mutations and prompt the prospective deployment of drug design. Therefore, as298 shown in Fig.6a, we construct a pipeline for predicting evolutionary trends based on299 ProtFound-V, which comprehensively evaluates single-site mutations through in sil-300 ico deep mutational scanning. Specifically, referring to the practice of wet-lab deep301 mutational scanning, we sequentially mutate a amino acid of a single site in the RBD302 region to another 19 amino acids, and complete the mutational scanning in the entire303 region site by site. Subsequently, the binding affinity with ACE2, expression and anti-304 body escape induced by each single-site mutation are predicted by ProtFound-V, and305 we comprehensively consider these properties to determine high-risk mutation sites.306 The proposed pipeline is capable of exploring evolutionary differences in trans-307 missibility between different lineages. More recently, XBB.1.5 is a lineage of popular308 dominance 1. It has a significant growth advantage over XBB.1, although there is only309 one-site difference between them. To explore the mutational effects of this site, we per-310 form a mutation scanning on site 486 and predict three key properties (ACE2 binding,311 expression, and antibody escape) for each mutation (Fig.6b). Compared with F486 in312 wild type, S486 in XBB.1 and P486 in XBB.1.5 have improved in all the three key prop-313 erties. It is worth noting that P486 has a significant increase in ACE2-binding affinity314 and a certain degree of expression increase compared with S486, which is consistent315 with the results of previous deep mutational scanning studies [29, 30] and wet-lab316 measurements [40]. XBB.1.5 achieves a surge in binding affinity through F486P muta-317 tion while maintaining the qualified antibody escape, which likely contributes to its318 higher transmissibility, thus resulting in a significant growth advantage.319 The proposed pipeline is competitive for wet-lab experiments in predicting evolu-320 tionary trends. Previous work [41, 42] estimated the mutational preference of Omicron321 BA.5 and XBB.1.5 in RBD region based on deep mutational scanning, which sys-322 tematically observed evolutionary trends through a statistical method for wet-lab323 measurements. Using this statistical method for wet-lab deep mutational scanning, we324\n1https://outbreak.info/\nincorporate two types of weights, ACE2 binding and RBD expression, on the basis325 of the antibody escape score to produce Fig.6c and Fig.6e. Since these results are326 statistics on the results of wet-lab experiments [41, 42], they can be considered as327 credible ground truth. However, virus mutations occur all the time, making expen-328 sive wet-lab measurements unable to keep up with the emergence of new variants. To329 address this issue, we systematically evaluate single-site mutations by in silico deep330 mutational scanning, i.e., predict three key properties for each mutation. The three331 key properties are then sequentially used to screen for high-risk mutations. As shown332 in Fig.6d and Fig.6f, the predicted evolutionary trends of BA.5 and XBB.1.5 have333 a high degree of coincidence with those based on a statistical method, i.e. ground334 truth. Consistency of predictions with ground truth demonstrates the effectiveness335 of ProtFound-V in predicting evolutionary trends by leveraging protein large lan-336 guage models. Sites marked in yellow (representing sites where BA.5 and XBB.1.5337 have been mutated) need continuous attention because they still have a certain degree338 of threat. The emerging new sites indicate the potential evolutionary trends in the339 future, which is the possible mutation direction of SARS-CoV-2. Previous work [41]340 clustered antibodies based on deep mutational scanning experiments and proposed 30341 corresponding escape hotspots, 19 of which are included in the predicted evolution-342 ary trends of BA.5 and XBB.1.5. This indicates that the evolution of SARS-CoV-2343 in the future tends to acquire stronger antibody escape capabilities in order to obtain344 stronger transmissibility.345\n3 Discussion346\nViruses are ubiquitous and constantly evolving, demanding constant vigilance against347 sudden outbreaks of emerging viral infections. Traditional wet-lab experimental meth-348 ods or conventional computing techniques fall short in meeting the throughput and349 speed requirements, making it difficult for humans to keep up with virus evolution.350 On the other hand, the development of artificial intelligence technology has led to the351 widespread use of large language models across various fields. Applications like Chat-352 GPT have sparked curiosity about the untapped potential of these language models.353 Consequently, it is natural for us to consider constructing a customized large language354 model for the field of viruses to explore their evolutionary mechanisms. In recent years,355 significant progress has been made in modeling biological sequences using natural lan-356 guage processing techniques, particularly in the domain of proteins. This advancement357 prompts us to pretrain a large-scale language model on extensive protein sequences358 and employ it for predicting evolutionary trends of viruses. However, directly applying359 large language models to the virus domain proves impractical due to the unique tra-360 jectories of viruses. Generally, viruses tend to have only a small number of beneficial361 mutations in their evolutionary outcomes, leading to minimal changes in the variants362 and far fewer beneficial mutations compared to harmful ones. To address this chal-363 lenge, we propose the evolution-inspired framework called ProtFound-V, leveraging364 customized protein language models pretrained from scratch. In this work, we conduct365 an in-depth exploration of SARS-CoV-2, a representative pathogen of emerging viral366 infections.367\nSpecifically, our approach incorporates a local-global feature coupling mechanism368 to address the challenge posed by indistinguishable sequences of highly similar vari-369 ants. This mechanism effectively fuses amino-acid-level interactions at different scales,370 enabling us to discern the subtle effects caused by a small number of mutations. As371 demonstrated in the dimension reduction visualization, we are able to clearly identify372 the subtle effects of few-site mutations. Additionally, we introduce a multi-task focal373 loss function that tackles the issue of extreme imbalance between beneficial and harm-374 ful mutations. This novel loss function alleviates the negative impact of rare samples,375 which are often difficult to learn in a multi-task setting. Through ablation experiments376 and case studies, we validate the accuracy of our high-risk mutation predictions, show-377 casing the sensitivity of our multi-task focal loss in identifying high-risk mutations. In378 summary, our evolution-inspired designs, including the local-global feature coupling379 and multi-task focal loss, empower ProtFound-V to effectively learn the evolutionary380 routes of viruses and explore their underlying mechanisms.381 As an example, ProtFound-V proposes a new state-of-the-art performance on the382 single-site mutation benchmark dataset for ACE2 binding prediction. Notably, even383 though our protein language model has a significantly smaller parameter count com-384 pared to other models in the comparison, ProtFound-V still exhibits a competitive385 edge in key performance indicators. This outcome underscores the success of our pre-386 training strategy. To expand the predictive capabilities of ProtFound-V to multi-site387 mutations, we integrate data from multiple lineages, enabling the incorporation of388 updated mutational effects influenced by different genetic backgrounds. Results on389 the multi-site mutation benchmark dataset demonstrate the superior performance of390 ProtFound-V in discriminating mutations. Moreover, when we transfer ProtFound-V,391 trained solely on the single-site mutation benchmark dataset, to other lineages with392 multi-site mutations under distinct genetic backgrounds, the model remains robust.393 However, it is essential to note that the substantial genetic background differences394 observed in Omicron have a slight impact on prediction performance.395 The first value of ProtFound-V lies in its ability to discriminate the evolutionary396 advantages among different viral lineages. In our ACE2 binding predictions for the397 five well-known VOCs (Alpha, Beta, Gamma, Delta, and Omicron), we have observed398 consistent results with wet-lab measurements from previous studies. Notably, our find-399 ings indicate that the troublesome Omicron variant exhibits a binding affinity similar400 to that of the wild type. This leads us to conclude that the evolutionary advantage of401 Omicron may not be in binding strength but rather in its potential for immune escape402 and increased transmissibility. Nevertheless, it is important to emphasize that main-403 taining qualified binding strength remains a fundamental requirement for SARS-CoV-2404 variants, even if the binding affinity compensates for other abilities.405 The second and most important value of ProtFound-V lies in its ability to pre-406 dict evolutionary trends. By constructing an evolutionary trend prediction process407 with ProtFound-V as the core, we can conduct in silico mutational scanning on differ-408 ent lineages. This pipeline performs effectively in identifying evolutionary differences409 between various lineages and has revealed an important reason for the dominance of410 the current popular strain XBB.1.5. Most importantly, ProtFound-V demonstrates411 competitive performance with wet-lab experiments in predicting evolutionary trends,412\nhighlighting the potential of using deep learning predictions to replace costly wet-lab413 experiments. By simulate the evolutionary trajectory of viruses using large language414 models and evolution-inspired design, ProtFound-V can facilitate efficient responses415 to emerging viral infections in a way that predicts mutation trends.416 Through ProtFound-V, we have presented a paradigm that can be utilized to417 explore evolutionary mechanisms by leveraging large language models. This approach418 is also transferable and applicable for processing other biological sequences. Notably,419 the multi-task focal loss function we proposed for regression and classification420 addresses the challenges of multi-task hard sample mining, effectively dealing with421 issues such as limited data and class imbalance commonly encountered in the life sci-422 ences field. As a result, the potential applications of ProtFound-V extend beyond its423 current use, and it can be applied to optimize antibodies, enzymes, and more.424 ProtFound-V, despite its notable achievements, still exhibits limitations that stem425 from three aspects: training data, research scale, and in vitro selection pressure.426 Regarding the training data, the dataset used in this study is derived from deep427 mutational scanning experiments, which may be limited in terms of variety and quan-428 tity for effectively training deep learning models. As data availability increases in the429 future, the prediction performance of ProtFound-V is expected to further improve. In430 terms of research scale, this study focuses on the key region RBD in the Spike protein431 of SARS-CoV-2 as an illustrative example to showcase ProtFound-V\u2019s effectiveness432 in deciphering the laws of virus evolution. Expanding its scope to encompass larger433 regions could enhance its understanding capabilities to a higher level. Furthermore,434 while this study involves predicting key properties of virus variants at the molecular435 level, the spread of viruses in the real world is also influenced by various in vitro selec-436 tion pressures. Integrating an in vitro selection pressure module in the future would437 enable ProtFound-V to extend its prediction ability to the spread scale of emerging438 viruses. Addressing these limitations in future research would undoubtedly enhance439 ProtFound-V\u2019s effectiveness and applicability, further contributing to our understand-440 ing of virus evolution and aiding in the prediction and response to emerging viral441 infections.442\n4 Methods443\n4.1 Dataset444\nThe UniRef90 dataset [43]is adopted for the pretraining of language model. It contains445 approximately 144 million raw protein sequences.446 For ACE2 binding prediction, the deep mutational scanning (DMS) datasets of447 previous work [29, 30] reported the difference in log10Kd, i.e. apparent dissociation448 constant, relative to wild type or a specific lineage. Specifically, the original data form449 is that one mutation corresponds to one \u2206 log10 Kd. First, based on the mutations, we450 perform residue substitutions at corresponding positions on wild type RBD sequence451 to obtain the mutated RBD sequences. Second, we convert \u2206 log10 Kd to Ka ratio as452\nKa ratio = 10 \u2206 log10 Kd (1)\nAfter the above two-step processing, we can obtain the mutated RBD sequences453 and corresponding labels representing the binding strength. Regarding the single-site454 mutation benchmark dataset, there are 3648 samples, including 3361 negative samples455 with label less than 1, and 287 positive samples with label greater than 1. Regarding456 the multi-site mutation benchmark dataset, we integrate seven groups of DMS dataset457 [30] including wild type, Alpha, Beta, Delta, Eta, Omicron BA.1 and Omicron BA.2.458 Finally, there are 26,369 samples, including 23,629 negative samples with label less459 than 1, and 2,740 positive samples with label greater than 1.460 The in silico deep mutational scanning consists of three steps, namely ACE2461 binding prediction, expression prediction and antibody escape prediction. For ACE2462 binding prediction and expression prediction, we use the DMS dataset of previous463 work [30]. We only use the samples of Omicron BA.1 and Omicron BA.2 due to the464 significant mutation differences between the lineages before and after Omicron. We465 normalize with the log10 Kd and the expression value of Omicron BA.2 as a reference466 to get the final labels. Samples with labels that are greater than 1 are considered as467 positive samples, while those with labels that are less than 1 are considered as neg-468 ative samples. Finally, there are 7,193 negative samples and 628 positive samples in469 the ACE2 binding dataset, while there are 7,182 negative samples and 639 positive470 samples in the expression dataset.471 For antibody escape prediction, we use the DMS dataset of wild type with different472 antibodies from previous work [41]. The dataset is the DMS results of 3051 antibodies473 with wild type, including antibody names, mutations on wild type, and the corre-474 sponding escape scores. Here we use the escape scores as labels directly, and set 0.4475 as the classification threshold, i.e. samples that are greater than 0.4 are considered as476 positive samples, others are negative samples. According to previous work [44], 0.4 is477 a high escape score among all the scores, so we assume that 0.4 is a proper standard478 to distinguish a strong escape. Finally, there are 57,740 positive samples and 284,032479 negative samples in the dataset.480\n4.2 Protein language model pretraining481\nAs the universal basic unit of life, protein can be regarded as the language of life.482 There are three levels of similarity between natural language and protein sequences,483 i.e., amino acids corresponding to letters, words corresponding to conserved protein484 fragments, and sentences corresponding to structures carrying different functions [45].485 UniRef90 [43], a database with approximately 144 million protein sequences clustered486 on the base of UniRef100 [46], is adopted as the training data of transformer-based487 language model [10], i.e., ProtFound-V here. The model architecture of ProtFound-488 V is similar to BERTLARGE [47], except that the next sentence prediction task is489 not adopted. The configuration is as follows: 24-layer, 1,024 hidden dimension, 16490 attention heads, and 340 million parameters. Inspired by LAMB [48], we adopt the491 two-step training strategy which facilitates the convergence of the model and promotes492 the training efficiency when dealing with longer sequences. In the first training stage,493 protein sequences with a maximum length of 128 are used for model training with494 a learning rate of 0.0004, warm-up steps of 40,000, and epochs of 65. In the second495 training stage, protein sequences with a maximum length of 256 are used for model496\ntraining with a learning rate of 0.0001 and epochs of 35. It is pretrained with only the497 objective of masked language modeling with a masking probability of 15%. The LAMB498 optimizer [48] and weight decay of 0.01 are adopted. The batch size of single Neural499 Processing Units (NPU) is 64, and the global batch size is 16,384. With the assistant500 of high performance computing strategy, our protein language model is pretrained501 with 256 NPUs and the training time is significantly reduced to about 100 hours.502 After the masked language modeling with negative log-likehood (NLL) loss function,503 the predicted probabilities per position in the protein sequences are obtained, which504 carry the reasonable distribution of amino acids in the natural protein sequence.505\n4.3 Sequence tokenizing and embedding506\nAn input protein sequence is treated as a sequence of tokens (residues). The tokenizer507 maps every token to its corresponding index. For protein sequences with different508 lengths, the tokenizer uses the reserved token \u2019[PAD]\u2019 to fill the sequence to the longest,509 and set the value of the corresponding attention mask to 0. The features of amino510 acids are extracted in the form of embeddings, i.e., vector representations. Specifically,511 let l be the longest length of sequences and d be the dimension of the embedding. The512 length of the tokenized sequence is l, and the shape of the embedding for each RBD513 sequence is d\u00d7 l.514 It is worth noting that there are usually only a few mutations on the RBD515 sequences, so the sequences input into the pretrained protein language models are516 usually similar to each other, which makes the embeddings of different sequences very517 similar. So after extracting embeddings, we conduct standardization to enlarge the dif-518 ference between different sequences. Specifically, let Xd\u00d7l be the extracted embedding519 feature, and N be the number of sequences. First, the embeddings are flattened into520 one dimension Xd\u00d7l \u2212\u2192 Xf1\u00d7dl. For feature dimension i \u2208 [1, dl], the standardization521 is as follows522  stdi = STDN (X f i ), meani = MEANN (X f i ),\nX\u0303fi = Xfi \u2212meani stdi ,\n(2)\nwhere STDN (\u00b7) and MEANN (\u00b7) denote the standard deviation calculation and the523 mean calculation among all the N embeddings respectively. After standardization, the524 flattened embedding X\u0303f1\u00d7dl is reshaped into the original dimension X\u0303 f 1\u00d7dl \u2212\u2192 X\u0303d\u00d7l.525 For antibody escape prediction, the input involves RBD sequences and anti-526 body sequences, and each antibody includes heavy chain sequence and light chain527 sequence. We do the operations separately to the RBD sequences, antibody heavy528 chain sequences and antibody light chain sequences as described above. We pad the529 heavy chain sequences and light chain sequences to 136, which is the largest length of530 all the antibody chains. We also conduct standardization to the extracted embeddings531 of antibody heavy chain sequences and light chain sequences.532\n4.4 Local-global feature coupling533\nAfter feature embedding and standardization, we use ContextPool-attention and534 CNN to capture the long-range dependencies and the near-range dependencies of the535 sequence embeddings, respectively.536\n4.4.1 ContextPool-attention537\nSelf-attention [10] is widely used in language modeling. However, it needs to assume538 that each individual token constructs the query and key vectors over a fixed gran-539 ularity, which may be sub-optimal for modeling context in different scales. Context540 pooling [18] adopts an explicit way to learn context-aware token features by learning541 to pool neighboring features for each token. After that, self-attention between such542 pooled features can thus be context-aware and model high-level dependencies.543 Specifically, we learn a mapping function m(\u00b7) to predict both the pooling weights544 w \u2208 Rn\u00d71 and sizes s \u2208 Rn\u00d71 for l input token features X\u0303d\u00d7l, i.e. {w, s} = m(X\u0303).545 Here, as in Huang et al. [18], we implement m(\u00b7) using three convolutional layers546 with softmax function behind, and the output channel size of the last layer is set547 to 2 to generate the vectors w and s simultaneously. The normalized pooling size548 si is transformed into standard deviation of Gaussian mask g\ni \u223c N(i, \u03c32i ), where549 \u03c3i = rn \u2217 si, and r is a scalar empirically set as 0.1.550 The final context pooling function multiplies the learned pooling weights w with551 Gaussian mask gi for token x\u0303i. Specifically, the context pooling function is calculated552 as553\nyi = fave(X\u0303 \u2299 \u03b3(w)\u2299 \u03b3(gi)) = n\u2211\nj=1\nx\u0303j \u00b7 wj \u00b7 gij , (3)\nwhere yi \u2208 Y represents the context pooling features, fave denotes average pooling554 function, and \u03b3(\u00b7) is broadcasting function for element-wise multiplication \u2299.555 During the actual training of the above model, the value of the Gaussian mask is556 relatively large, and the training loss is large at the beginning of training. In order557 to improve the training stability, we normalize the Gaussian mask vector by dividing558 it by its maximum value. Specifically, for the Gaussian mask of the ith place, the559 normalization is calculated as560\ng\u0303i = gi\nMAXlj=1(g i j)\n\u2208 [0, 1]. (4)\nCompared with the original module, the normalized one is more stable during561 training and the testing effect is slightly better.562\n4.4.2 CNN563\nWe adopt a one-dimension convolution neural network to extract the local dependen-564 cies between neighboring residues. The CNN architecture has been widely used to565 integrate local dependencies and capture latent information of sequences in the area of566 protein-protein interactions [49, 50] and compound-protein interactions [51, 52]. The567\nCNN here consists of 3 convolution layers with Layer Normalization and uses leaky568 rectified linear unit (Leaky ReLU) as activation function. We set the kernel size to 3569 and the stride to 1 to ensure the shape of the output is the same as the input.570\n4.4.3 Feature coupling571\nThe output of ContextPool-attention and CNN both have the same shape as the input,572 so a shortcut can be added on the both. The outputs of the both are concatenated573 together to get the final extracted features of residues, and the feature of the whole574 RBD sequence is calculated through max pooling on the residue dimension. Specifi-575 cally, for each RBD sequence, the shapes of the two outputs are both 1024\u00d7 l, and the576 shape of the concatenation is 2048\u00d7 l. The result of the max pooling is 2048\u00d7 1. The577 extracted feature of the whole RBD sequence is used for the subsequent prediction578 tasks.579 For antibody escape prediction, the embeddings of RBD, antibody heavy chain, and580 antibody light chain are input into independent local-global feature coupling channel581 respectively to capture long-range dependencies and near-range dependencies. The582 features of each kinds of sequence are calculated through max pooling on the residue583 dimension, and the results are concatenated as the feature to predict antibody escape.584 Specifically, for each kind of sequence, the shape of the max pooling is 2048\u00d7 1, and585 the concatenation of features of three kinds of sequence is 6144\u00d7 1, which is used for586 the subsequent prediction.587\n4.5 Loss function design588\nWe design Multi-task focal loss function to alleviate the imbalance between positive589 and negative samples. It consists of two focal losses according to the characteristics of590 each task, i.e. the regression focal loss and the classification focal loss.591 For classification task, the training intensity of a certain category can be adjusted592 by assigning different weights to samples with different labels in classification focal loss593 to alleviate the imbalance between positive and negative samples in training dataset.594 Specifically, the calculation of the classification focal loss is as follows595\npt =\n{ pred, label = 1,\n1\u2212 pred, label = 0. (5)\n\u03b1t =\n{ \u03b1, label = 1,\n1\u2212 \u03b1, label = 0. (6)\nClsFocalLoss = \u2212\u03b1t(1\u2212 pt)\u03b3 log pt, (7) where pred stands for the prediction of the classification model, label stands for the596 classification label. The penalty index \u03b3 indicates the punishment intensity for large597 error samples. The larger \u03b3 is, the lower the tolerance to wrong samples will be. The598 weight coefficient \u03b1 can adjust the attention of loss function to positive and negative599 samples to alleviate the imbalance between positive and negative samples in training600 dataset.601\nFor regression task, we propose the regression focal loss inspired by the above classification focal loss. The calculation of the regression focal loss is as follows. ai = (score\u2212 target)\u03b3 , \u03b3 = 1, 2, 3, ..., a\u0302l = ai\u2211 ai ,\nli = a\u0302l(score\u2212 target)2, RegFocalLoss = \u2211B i=1 li,\n(8)\nwhere score stands for the prediction of the model, target stands for the regression602 label, and a\u0302l is the scaled error in case the gradient value is too small. The penalty603 index \u03b3 is the same as that in classification focal loss.604 The final focal loss function is:605\nFocalLoss = \u00b5 \u2217RegFocalLoss+ ClsFocalLoss, (9) where \u00b5 denotes the weight of the regression loss. We set \u00b5 = 1 in the above606 experiments.607\n4.6 Metrics608\nFor regression task, we use Mean Squared Error (MSE) and Correlation Coefficient609 (Corr) between predicted and experimental Ka ratio. Specifically, let ypred be the610 predicted Ka ratio and ylabel be the experimental Ka ratio, the MSE and Corr are611 calculated as follows612 MSE = 1 N \u2211N i=1(ypredi \u2212 ylabeli)2, Corr = \u2211N\ni=1(ypredi\u2212ypred)(ylabeli\u2212ylabel)\u221a\u2211N i=1(ypredi\u2212ypred)2 \u221a\u2211N i=1(ylabeli\u2212ylabel)2 , (10)\nwhere N is the number of samples.613 For classification task, we set the label of samples with Ka ratio > 1 as True, and614 the label of samples with Ka ratio \u2264 1 as False. The metrics for classification includes615 AUC, Accuracy, F1\u2212 Score, Precision and Recall between label and prediction.616 AUC indicates the probability that the predicted score of positive samples is higher617 than the one of negative samples. Specifically, let m+ and m\u2212 be the number of618 positive and negative samples respectively. D+ and D\u2212 denote the set of positive and619 negative samples respectively. AUC is calculated as follows620\nAUC = 1\u2212 1 m+m\u2212 \u2211 x+\u2208D+ \u2211 x\u2212\u2208D\u2212 (I(f(x+) < f(x\u2212)) + 1 2 I(f(x+) = f(x\u2212))), (11)\nwhere the f(\u00b7) denotes the prediction function, and I(\u00b7) values 1 if the input is621 True, otherwise it values 0.622 For binary classification tasks, the prediction can be divided into four cases: TP623 if the prediction and label are both positive, TN if the prediction and label are both624 negative, FP if the prediction is positive and the label is negative, FN if the prediction625\nis negative and the label is positive. Therefore, the other four metrics are calculated626 as follows627  Accuracy = TP+TNTP+TN+FP+FN , P recision = TPTP+FP , Recall = TPTP+FN ,\nF1\u2212 Score = 2\u2217Precision\u2217RecallPrecision+Recall .\n(12)\nIn addition to the metrics mentioned above, we propose a new evaluation metrics,628 Ordinal Pair Proportion(OPP ) to evaluate prediction performance of relative rank-629 ings. We first combine all the prediction results into binary pairs. And for each pair,630 if the label order of the two predictions is consistent with the prediction result order,631 we call the pair \u2019ordinal\u2019, and the OPP is the proportion of the \u2019ordinal\u2019 pairs in all632 pairs. Then, for the prediction set N, n = |N |, OPP is calculated as follows633\n{ OPP = |M |C2n ,\nM = {(i, j)|\u2200i, j \u2208 N, i < j, (labeli \u2212 labelj)(predi \u2212 predj) > 0}. (13)\n4.7 In silico deep mutational scanning634\nTo explore the difference between XBB.1 and XBB.1.5 in growth advantage, we mutate635 the residue on site 486 of XBB.1 into other 19 residues. Subsequently, the ACE2636 binding prediction model, expression prediction model, and antibody escape prediction637 model are adopted to predict properties of each variant. We use the regression output638 and do normalization within each kinds of property due to the difference of output639 range between properties.640 To predict the evolutionary trends of Omicron BA.5 and XBB.1.5, we take them641 as reference sequences, and enumerate all the mutated variants with single mutation.642 The ACE2 binding, expression and antibody escape of each variants are predicted. As643 to the antibody sequence used during antibody escape prediction, we adopt BD57-644 0129 from previous work [41] as it achieves relatively strong neutralizing capability,645 and variants escaping it indicates strong escape capability. Based on the predicted646 properties of variants, we set different thresholds to do variant filtering to get possible647 high-risk variants. For the property that is more essential for variants to survive, like648 expression, we set a higher threshold. For the less vital property, we set the threshold649 relatively lower to avoid filtering out too many variants. Finally, we use classification650 output, and set the threshold of ACE2 binding as 0.25, expression as 0.7, antibody651 escape as 0.55. For the filtered variants, the number of possible mutations at each652 site is considered as indicator of evolutionary trends. In other words, the greater the653 number of possible mutations, the higher the risk of the site.654\nData availability655\nFor the pretraining of the protein language model, we downloaded UniRef 90 from656 https://www.uniprot.org/. For the training and testing of ACE2 binding prediction,657\nexpression prediction, and antibody escape, we obtained and processed deep muta-658 tional scanning data from previous work. The processed datasets are available at659 https://github.com/ZhiweiNiepku/ProtFound-V.660\nCode availability661\nThe relevant codes of model training and inference are available at662 https://github.com/ZhiweiNiepku/ProtFound-V, and we also released the pretrained663 protein language model at the link above.664\nReferences665\n[1] Kuiken, T., Fouchier, R., Rimmelzwaan, G. & Osterhaus, A. Emerging viral666 infections in a rapidly changing world. Current opinion in biotechnology 14,667 641\u2013646 (2003).668\n[2] Luo, G. G. & Gao, S.-J. Global health concerns stirred by emerging viral669 infections. Journal of medical virology 92, 399 (2020).670\n[3] Prokunina-Olsson, L. et al. Covid-19 and emerging viral infections: The case for671 interferon lambda. Journal of Experimental Medicine 217 (2020).672\n[4] Lu, R. et al. Genomic characterisation and epidemiology of 2019 novel coro-673 navirus: implications for virus origins and receptor binding. The lancet 395,674 565\u2013574 (2020).675\n[5] Jackson, C. B., Farzan, M., Chen, B. & Choe, H. Mechanisms of sars-cov-2 entry676 into cells. Nature reviews Molecular cell biology 23, 3\u201320 (2022).677\n[6] Shang, J. et al. Structural basis of receptor recognition by sars-cov-2. Nature678 581, 221\u2013224 (2020).679\n[7] Walls, A. C. et al. Structure, function, and antigenicity of the sars-cov-2 spike680 glycoprotein. Cell 181, 281\u2013292 (2020).681\n[8] Tao, K. et al. The biological and clinical significance of emerging sars-cov-2682 variants. Nature Reviews Genetics 22, 757\u2013773 (2021).683\n[9] Graham, R. L. & Baric, R. S. Recombination, reservoirs, and the modular spike:684 mechanisms of coronavirus cross-species transmission. Journal of virology 84,685 3134\u20133146 (2010).686\n[10] Vaswani, A. et al. Attention is all you need. Advances in neural information687 processing systems 30 (2017).688\n[11] Lin, Z. et al. Evolutionary-scale prediction of atomic-level protein structure with689 a language model. Science 379, 1123\u20131130 (2023).690\n[12] Elnaggar, A. et al. Prottrans: Toward understanding the language of life through691 self-supervised learning. IEEE transactions on pattern analysis and machine692 intelligence 44, 7112\u20137127 (2021).693\n[13] Wang, S., Guo, Y., Wang, Y., Sun, H. & Huang, J. Smiles-bert: large scale694 unsupervised pre-training for molecular property prediction, 429\u2013436 (2019).695\n[14] Wu, Z. et al. Knowledge-based bert: a method to extract molecular features like696 computational chemists. Briefings in Bioinformatics 23, bbac131 (2022).697\n[15] Bagal, V., Aggarwal, R., Vinod, P. & Priyakumar, U. D. Molgpt: molecular698 generation using a transformer-decoder model. Journal of Chemical Information699 and Modeling 62, 2064\u20132076 (2021).700\n[16] Chen, J. et al. Running ahead of evolution\u2014ai-based simulation for predict-701 ing future high-risk sars-cov-2 variants. The International Journal of High702 Performance Computing Applications 37, 650\u2013665 (2023).703\n[17] Gu, J. et al. Recent advances in convolutional neural networks. Pattern704 recognition 77, 354\u2013377 (2018).705\n[18] Huang, C., Talbott, W., Jaitly, N. & Susskind, J. M. Efficient representation706 learning via adaptive context pooling, 9346\u20139355 (PMLR, 2022).707\n[19] Dong, Q., Gong, S. & Zhu, X. Class rectification hard mining for imbalanced deep708 learning, 1851\u20131860 (2017).709\n[20] Sheng, H. et al. Mining hard samples globally and efficiently for person710 reidentification. IEEE Internet of Things Journal 7, 9611\u20139622 (2020).711\n[21] Lin, T.-Y., Goyal, P., Girshick, R., He, K. & Dolla\u0301r, P. Focal loss for dense object712 detection, 2980\u20132988 (2017).713\n[22] Zhang, Y. & Yang, Q. An overview of multi-task learning. National Science714 Review 5, 30\u201343 (2018).715\n[23] Zhang, Y. & Yang, Q. A survey on multi-task learning. IEEE Transactions on716 Knowledge and Data Engineering 34, 5586\u20135609 (2021).717\n[24] Nelson, G. et al. Molecular dynamic simulation reveals e484k mutation enhances718 spike rbd-ace2 affinity and the combination of e484k, k417n and n501y muta-719 tions (501y. v2 variant) induces conformational change greater than n501y mutant720 alone, potentially resulting in an escape mutant. BioRxiv (2021).721\n[25] Rezaei, S., Sefidbakht, Y. & Uskokovic\u0301, V. Comparative molecular dynamics722 study of the receptor-binding domains in sars-cov-2 and sars-cov and the effects723 of mutations on the binding affinity. Journal of Biomolecular Structure and724 Dynamics 40, 4662\u20134681 (2022).725\n[26] Luan, B., Wang, H. & Huynh, T. Enhanced binding of the n501y-mutated sars-726 cov-2 spike protein to the human ace2 receptor: insights from molecular dynamics727 simulations. FEBS letters 595, 1454\u20131461 (2021).728\n[27] Yang, Y. et al. Key residues of the receptor binding domain in the spike protein729 of sars-cov-2 mediating the interactions with ace2: a molecular dynamics study.730 Nanoscale 13, 9364\u20139370 (2021).731\n[28] Chen, C. et al. Computational prediction of the effect of amino acid changes on732 the binding affinity between sars-cov-2 spike rbd and human ace2. Proceedings of733 the National Academy of Sciences 118, e2106480118 (2021).734\n[29] Starr, T. N. et al. Deep mutational scanning of sars-cov-2 receptor binding domain735 reveals constraints on folding and ace2 binding. cell 182, 1295\u20131310 (2020).736\n[30] Starr, T. N. et al. Deep mutational scans for ace2 binding, rbd expression,737 and antibody escape in the sars-cov-2 omicron ba. 1 and ba. 2 receptor-binding738 domains. PLoS pathogens 18, e1010951 (2022).739\n[31] Zhang, R., Ghosh, S. & Pal, R. Predicting binding affinities of emerging vari-740 ants of sars-cov-2 using spike protein sequencing data: observations, caveats and741 recommendations. Briefings in Bioinformatics 23, bbac128 (2022).742\n[32] Han, J. et al. D3ai-spike: A deep learning platform for predicting binding affin-743 ity between sars-cov-2 spike receptor binding domain with multiple amino acid744 mutations and human angiotensin-converting enzyme 2. Computers in Biology745 and Medicine 151, 106212 (2022).746\n[33] Heinzinger, M. et al. Modeling aspects of the language of life through transfer-747 learning protein sequences. BMC bioinformatics 20, 1\u201317 (2019).748\n[34] Elnaggar, A. et al. Prottrans: towards cracking the language of life\u2019s code through749 self-supervised deep learning and high performance computing. arXiv preprint750 arXiv:2007.06225 (2020).751\n[35] Rives, A. et al. Biological structure and function emerge from scaling unsu-752 pervised learning to 250 million protein sequences. Proceedings of the National753 Academy of Sciences 118, e2016239118 (2021).754\n[36] Meier, J. et al. Language models enable zero-shot prediction of the effects of muta-755 tions on protein function. Advances in Neural Information Processing Systems756 34, 29287\u201329303 (2021).757\n[37] Witte, L. et al. Epistasis lowers the genetic barrier to sars-cov-2 neutralizing758 antibody escape. Nature Communications 14, 302 (2023).759\n[38] Moulana, A. et al. Compensatory epistasis maintains ace2 affinity in sars-cov-2760 omicron ba. 1. Nature Communications 13, 7011 (2022).761\n[39] Han, P. et al. Receptor binding and complex structures of human ace2 to spike762 rbd from omicron and delta sars-cov-2. Cell 185, 630\u2013640 (2022).763\n[40] Yue, C. et al. Ace2 binding and antibody evasion in enhanced transmissibility of764 xbb. 1.5. The Lancet Infectious Diseases 23, 278\u2013280 (2023).765\n[41] Cao, Y. et al. Imprinted sars-cov-2 humoral immunity induces convergent omicron766 rbd evolution. Nature 614, 521\u2013529 (2023).767\n[42] Yisimayi, A. et al. Repeated omicron infection alleviates sars-cov-2 immune768 imprinting. bioRxiv 2023\u201305 (2023).769\n[43] Steinegger, M. & So\u0308ding, J. Clustering huge protein sequence sets in linear time.770 Nature communications 9, 1\u20138 (2018).771\n[44] Greaney, A. J., Starr, T. N. & Bloom, J. D. An antibody-escape estimator for772 mutations to the sars-cov-2 receptor-binding domain. Virus evolution 8, veac021773 (2022).774\n[45] Ferruz, N. & Ho\u0308cker, B. Controllable protein design with language models. Nature775 Machine Intelligence 4, 521\u2013532 (2022).776\n[46] Suzek, B. E. et al. Uniref clusters: a comprehensive and scalable alternative for777 improving sequence similarity searches. Bioinformatics 31, 926\u2013932 (2015).778\n[47] Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. Bert: Pre-training of779 deep bidirectional transformers for language understanding. arXiv preprint780 arXiv:1810.04805 (2018).781\n[48] You, Y. et al. Large batch optimization for deep learning: Training bert in 76782 minutes. arXiv preprint arXiv:1904.00962 (2019).783\n[49] Litfin, T., Yang, Y. & Zhou, Y. Spot-peptide: template-based prediction784 of peptide-binding proteins and peptide-binding sites. Journal of chemical785 information and modeling 59, 924\u2013930 (2019).786\n[50] Lei, Y. et al. A deep-learning framework for multi-level peptide\u2013protein787 interaction prediction. Nature communications 12, 1\u201310 (2021).788\n[51] O\u0308ztu\u0308rk, H., O\u0308zgu\u0308r, A. & Ozkirimli, E. Deepdta: deep drug\u2013target binding affinity789 prediction. Bioinformatics 34, i821\u2013i829 (2018).790\n[52] Li, S. et al. Monn: a multi-objective neural network for predicting compound-791 protein interactions and affinities. Cell Systems 10, 308\u2013322 (2020).792\nAcknowledgements793\nWe thank Ming Li for his discussion of the protocol and AI for Science (AI4S)-Preferred794 Program, Peking University Shenzhen Graduate School, China. This work was finan-795 cially supported by the National Key R&D Program of China (No. 2022ZD0118201),796 Natural Science Foundation of China (No. 61972217, 32071459, 62176249, 62006133,797 62271465, 61825101, 62088102), and R&D Program of Guangzhou Laboratory (Grant798 No.SRPG22-001).799\nAuthor information800\nContributions801\nZ.N. designed the project. Z.N and X.L. trained the model, performed the analysis, and802 wrote the paper. Z.W. and F.X. contributed to the pre-training of protein language803 models. Y.L., H.S., T.D., G.S. and W.G. contributed to useful discussions. P.Z., Y.W.,804 J.C., Y.T. contributed to scheme design. J.C., Y.T. supervised this project.805\nCorresponding authors806\nCorrespondence to Jie Chen, Yonghong Tian.807\nCompeting interests808\nThe authors declare no competing interests.809\nExtended data tables810\nTable S1 Ablation experiments of our multi-task focal loss\nLoss function AUC Accuracy F1-Score Precision Recall MSE Correlation\nBCE&MSE 84.06% 57.41% 26.33% 97.14% 15.56% 0.060 0.871 BCE&RegFocalLoss 88.59% 58.52% 28.93% 100.0% 17.04% 0.059 0.873 MSE&ClsFocalLoss 93.69% 85.56% 84.92% 88.28% 82.22% 0.055 0.879 Multi-task focal loss 92.98% 91.11% 91.58% 87.43% 96.30% 0.058 0.870\nT a b le\nS 2\nP re d ic ti o n p er fo rm\na n ce\no n si n g le -s it e m u ta ti o n b en\nch m a rk\nd a ta se t\nM et h o d\nE n co\nd er\nA U C\nA cc u ra cy\nF 1 -S co\nre P re ci si o n\nR ec a ll\nM S E\nC o rr\nN N\nM M -G\nB S A\nM o le cu\nla r D y n a m ic s S im\nu la ti o n\n- 8 0 .4 1\n- -\n- 0 .2 0\n0 .7 9\nB iL S T M\n- -\n- -\n- -\n0 .2 3\n0 .1 3\nC N N\nO n eh o t\n- -\n- -\n- 0 .1 1\n0 .7 6\nP ro tF\no u n d -V\n(w it h o u t p ro te in\nL M\n1 )\nO n eh o t\n6 1 .2 2\n5 8 .8 9\n5 1 .0 7\n5 8 .0 7\n4 9 .6 3\n0 .3 0 4\n0 .2 5 5\nP r o tF\no u n d -V\n(p r o te\nin L M\n)\nS eq\nV ec\n9 1 .3 6\n8 5 .9 3\n8 6 .4 1\n8 3 .0 2\n9 0 .3 7\n0 .0 7 1\n0 .8 3 7\nP ro tT\nra n s T 5\n9 2 .8 1\n8 7 .4 1\n8 7 .4 6\n8 6 .3 1\n8 8 .8 9\n0 .0 7 9\n0 .8 1 8\nE S M -1 b\n9 5 .8 6\n9 0 .3 7\n9 0 .6 3\n8 8 .3 2\n9 3 .3 3\n0 .0 5 9\n0 .8 7 3\nE S M -1 v (m\nea n )2\n9 3 .9 0\n8 9 .1 1\n8 9 .2 7\n8 8 .0 6\n9 0 .8 1\n0 .0 6 3\n0 .8 5 3\nE S M -2\n(6 5 0 M )3\n8 8 .2 6\n8 5 .1 9\n8 5 .4 4\n8 3 .7 1\n8 8 .8 9\n0 .0 8 0\n0 .8 0 2\nE S M -2\n(1 5 B )4\n9 3 .8 3\n9 0 .3 7\n9 0 .6 6\n8 7 .6 9\n9 4 .0 7\n0 .0 5 9\n0 .8 6 3\nO u r p r o te\nin L M\n(3 4 0 M\n) 9 2 .9 8\n9 1 .1 1\n9 1 .5 8\n8 7 .4 3\n9 6 .3 0\n0 .0 5 8\n0 .8 7 0\n1 L M\nre p re se n ts\nla n g u a g e m o d el .\n2 T h e re su\nlt s o f fi v e m o d el s w it h d iff er en\nt se ed\ns o f E S M -1 v a re\na v er a g ed\nh er e.\n3 T h e E S M -2\nm o d el\nw it h 6 5 0 m il li o n p a ra m et er s is\nch o se n h er e to\no b ta in\na n em\nb ed\nd in g d im\nen si o n (1 2 8 0 ) cl o se\nto th\na t o f P ro tF\no u n d -V\n(1 0 2 4 ).\n4 T h e la rg es t E S M -2\nm o d el\nw it h 1 5 b il li o n p a ra m et er s.\nT a b le\nS 3\nP re d ic ti o n p er fo rm\na n ce\no n m u lt isi te\nm u ta ti o n b en\nch m a rk\nd a ta se t\nM et h o d\nE n co\nd er\nA U C\nA cc u ra cy\nF 1 -S co\nre P re ci si o n\nR ec a ll\nM S E\nC o rr\nP ro tF\no u n d -V\n(p ro te in\nL M\n1 )\nS eq\nV ec\n8 6 .6 8\n7 9 .5 0\n8 0 .5 1\n7 6 .8 6\n8 4 .6 0\n5 .8 6 1\n0 .8 9 8\nP ro tT\nra n s T 5\n8 4 .6 6\n7 7 .3 3\n7 9 .3 7\n7 2 .9 7\n8 7 .1 3\n6 .3 4 5\n0 .8 8 2\nE S M -1 b\n8 4 .9 3\n7 7 .1 7\n7 7 .5 4\n7 6 .3 7\n7 8 .9 3\n5 .2 5 1\n0 .9 3 7\nE S M -1 v (m\nea n )2\n8 5 .2 7\n7 8 .2 3\n7 9 .1 7\n7 6 .1 4\n8 2 .7 7\n5 .0 6 4\n0 .9 2 2\nE S M -2\n(6 5 0 M )3\n8 3 .2 5\n7 5 .8 3\n7 6 .6 5\n7 4 .2 1\n7 9 .3 3\n5 .8 1 8\n0 .9 0 3\nE S M -2\n(1 5 B )4\n8 4 .7 4\n7 6 .5 7\n7 6 .9 7\n7 5 .9 5\n7 8 .8 0\n6 .3 2 3\n0 .8 9 8\nO u r p r o te\nin L M\n(3 4 0 M\n) 8 7 .3 7\n7 9 .8 3\n7 9 .9 7\n7 9 .4 9\n8 0 .5 3\n6 .1 8 1\n0 .9 1 2\n1 L M\nre p re se n ts\nla n g u a g e m o d el .\n2 T h e re su\nlt s o f fi v e m o d el s w it h d iff er en\nt se ed\ns o f E S M -1 v a re\na v er a g ed\nh er e.\n3 T h e E S M -2\nm o d el\nw it h 6 5 0 m il li o n p a ra m et er s is\nch o se n h er e to\no b ta in\na n em\nb ed\nd in g d im\nen si o n (1 2 8 0 ) cl o se\nto th\na t o f P ro tF\no u n d -\nV (1 0 2 4 ). 4 T h e la rg es t E S M -2\nm o d el\nw it h 1 5 b il li o n p a ra m et er s.\nTable S4 ACE2 binding predictions of ProtFound-V on five VOCs.\nVOCs Alpha Beta Gamma Delta Omicron (B.1.1.529)\nPredicted Ka ratio 1.71 1.28 1.23 1.05 1.04"
        }
    ],
    "title": "Evolution-guided large language model is a predictor of virus mutation trends",
    "year": 2023
}