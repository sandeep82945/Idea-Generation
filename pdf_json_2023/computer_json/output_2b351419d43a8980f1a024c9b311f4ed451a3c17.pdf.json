{
    "abstractText": "Online social networks have become an integral aspect of our daily lives and play a crucial role in shaping our relationships with others. However, bugs and glitches, even minor ones, can cause anything from frustrating problems to serious data leaks that can have farreaching impacts on millions of users. To mitigate these risks, fuzz testing, a method of testing with randomised inputs, can provide increased confidence in the correct functioning of a social network. However, implementing traditional fuzz testing methods can be prohibitively difficult or impractical for programmers outside of the network\u2019s development team. To tackle this challenge, we present Socialz, a novel approach to social fuzz testing that (1) characterises real users of a social network, (2) diversifies their interaction using evolutionary computation across multiple, non-trivial features, and (3) collects performance data as these interactions are executed. With Socialz, we aim to provide anyone with the capability to perform comprehensive social testing, thereby improving the reliability and security of online social networks used around the world.",
    "authors": [
        {
            "affiliations": [],
            "name": "Francisco Zanartu"
        },
        {
            "affiliations": [],
            "name": "Christoph Treude"
        },
        {
            "affiliations": [],
            "name": "Markus Wagner"
        }
    ],
    "id": "SP:87995a2349909345a7527d4726415475d8fba1eb",
    "references": [
        {
            "authors": [
                "John Ahlgren",
                "Maria Eugenia Berezin",
                "Kinga Bojarczuk",
                "Elena Dulskyte",
                "Inna Dvortsova",
                "Johann George",
                "Natalija Gucevska",
                "Mark Harman",
                "Ralf Laemmel",
                "Erik Meijer"
            ],
            "title": "WES: Agent-based user interaction simulation on real infrastructure",
            "venue": "In IEEE/ACM 42nd International Conference on Software Engineering Workshops. Association for Computing Machinery,",
            "year": 2020
        },
        {
            "authors": [
                "John Ahlgren",
                "Kinga Bojarczuk",
                "Sophia Drossopoulou",
                "Inna Dvortsova",
                "Johann George",
                "Natalija Gucevska",
                "Mark Harman",
                "Maria Lomeli",
                "Simon MM Lucas",
                "Erik Meijer"
            ],
            "title": "Facebook\u2019s cyber\u2013cyber and cyber\u2013physical digital twins",
            "venue": "Association for Computing Machinery,",
            "year": 2021
        },
        {
            "authors": [
                "Nadia Alshahwan",
                "Xinbo Gao",
                "Mark Harman",
                "Yue Jia",
                "Ke Mao",
                "Alexander Mols",
                "Taijin Tei",
                "Ilya Zorin"
            ],
            "title": "Deploying Search Based Software Engineering with Sapienz at Facebook",
            "venue": "In Search-Based Software Engineering, Thelma Elita Colanzi and Phil McMinn (Eds.). Springer International Publishing,",
            "year": 2018
        },
        {
            "authors": [
                "Mahmoud A. Bokhari",
                "Brad Alexander",
                "Markus Wagner"
            ],
            "title": "Towards Rigorous Validation of Energy Optimisation Experiments",
            "venue": "Association for Computing Machinery,",
            "year": 2020
        },
        {
            "authors": [
                "Jakob Bossek",
                "Pascal Kerschke",
                "Aneta Neumann",
                "Markus Wagner",
                "Frank Neumann",
                "Heike Trautmann"
            ],
            "title": "Evolving Diverse TSP Instances by Means of Novel and Creative Mutation Operators",
            "venue": "In 15th ACM/SIGEVO Conference on Foundations of Genetic Algorithms. Association for Computing Machinery,",
            "year": 2019
        },
        {
            "authors": [
                "Jon Brodkin"
            ],
            "title": "Trump\u2019s social app marred by bugs and apparent ban on",
            "venue": "Devin Nunes cow accounts. Retrieved",
            "year": 2022
        },
        {
            "authors": [
                "Gitlab CE"
            ],
            "title": "Issue #360755: \u201cLimit the maximum number of followed",
            "venue": "users. Retrieved",
            "year": 2023
        },
        {
            "authors": [
                "Yuan Cheng",
                "Jaehong Park",
                "Ravi Sandhu"
            ],
            "title": "A user-to-user relationshipbased access control model for online social networks. In IFIP Annual conference on data and applications security and privacy",
            "year": 2012
        },
        {
            "authors": [
                "Domenico Cotroneo",
                "Antonio Ken Iannillo",
                "Roberto Natella"
            ],
            "title": "Evolutionary Fuzzing of Android OS Vendor System Services",
            "venue": "Empirical Software Engineering 24,",
            "year": 2019
        },
        {
            "authors": [
                "Carola Doerr",
                "Markus Wagner"
            ],
            "title": "Simple On-the-Fly Parameter Selection Mechanisms for Two Classical Discrete Black-Box Optimization Benchmark Problems",
            "venue": "In Genetic and Evolutionary Computation Conference. Association for Computing Machinery,",
            "year": 2018
        },
        {
            "authors": [
                "Thomas Dohmke"
            ],
            "title": "2023. 100 million developers and counting",
            "venue": "Retrieved 25 January",
            "year": 2023
        },
        {
            "authors": [
                "Nick Duffield",
                "Balachander Krishnamurthy"
            ],
            "title": "Efficient Sampling for Better OSN Data Provisioning",
            "year": 2016
        },
        {
            "authors": [
                "Daschel Franz",
                "Heather Elizabeth Marsh",
                "Jason I Chen",
                "Alan R Teo"
            ],
            "title": "Using Facebook for Qualitative Research: A Brief Primer",
            "venue": "Journal of Medical Internet Research 21,",
            "year": 2019
        },
        {
            "authors": [
                "Mark Harman",
                "Peter O\u2019Hearn"
            ],
            "title": "From Start-ups to Scale-ups: Opportunities and Open Problems for Static and Dynamic Program Analysis",
            "venue": "In IEEE 18th International Working Conference on Source Code Analysis and Manipulation (SCAM). SCAM 2018,",
            "year": 2018
        },
        {
            "authors": [
                "Muhammad Abid Jamil",
                "Muhammad Arif",
                "Normi Sham Awang Abubakar",
                "Akhlaq Ahmad"
            ],
            "title": "Software Testing Techniques: A Literature Review",
            "venue": "In 2016 6th International Conference on Information and Communication Technology for The Muslim World (ICT4M). ICT4M, Jakarta,",
            "year": 2016
        },
        {
            "authors": [
                "Yuchen Jiang",
                "Shen Yin",
                "Kuan Li",
                "Hao Luo",
                "Okyay Kaynak"
            ],
            "title": "Industrial applications of digital twins",
            "venue": "Philosophical Transactions of the Royal Society A",
            "year": 2021
        },
        {
            "authors": [
                "\u00d6zg\u00fcr Kafal\u0131",
                "Ak\u0131n G\u00fcnay",
                "P\u0131nar Yolum"
            ],
            "title": "Detecting and predicting privacy violations in online social networks",
            "venue": "Distributed and Parallel Databases 32,",
            "year": 2014
        },
        {
            "authors": [
                "Saurabh Kumar",
                "Pradeep Kumar"
            ],
            "title": "Privacy Preserving in Online Social Networks Using Fuzzy Rewiring",
            "venue": "IEEE Transactions on Engineering Management Early Access (2021),",
            "year": 2021
        },
        {
            "authors": [
                "Yuwei Li",
                "Shouling Ji",
                "Chenyang Lv",
                "Yuan Chen",
                "Jianhai Chen",
                "Qinchen Gu",
                "Chunming Wu"
            ],
            "title": "V-fuzz: Vulnerability-oriented evolutionary fuzzing",
            "venue": "arXiv preprint arXiv:1901.01142",
            "year": 2019
        },
        {
            "authors": [
                "Valentin J.M. Manes",
                "HyungSeok Han",
                "Choongwoo Han",
                "Sang Kil Cha",
                "Manuel Egele",
                "Edward J. Schwartz",
                "Maverick Woo"
            ],
            "title": "The Art, Science, and Engineering of Fuzzing: A Survey",
            "year": 2018
        },
        {
            "authors": [
                "Leonardo Mariani",
                "Mauro Pezz\u00e8",
                "Daniele Zuddas"
            ],
            "title": "Chapter Four - Recent Advances in Automatic Black-Box Testing",
            "venue": "In Advances in Computers, Atif Memon (Ed.). Vol. 99. Elsevier,",
            "year": 2015
        },
        {
            "authors": [
                "Phil McMinn"
            ],
            "title": "Search-Based Software Testing: Past, Present and Future",
            "venue": "In IEEE Fourth International Conference on Software Testing, Verification and Validation Workshops. ICSTW",
            "year": 2011
        },
        {
            "authors": [
                "George Morahan"
            ],
            "title": "The Social Media",
            "venue": "Sites That Have Lost The Most User Data. Retrieved",
            "year": 2022
        },
        {
            "authors": [
                "Aneta Neumann",
                "Wanru Gao",
                "Carola Doerr",
                "Frank Neumann",
                "Markus Wagner"
            ],
            "title": "Discrepancy-based evolutionary diversity optimization. In Genetic and Evolutionary Computation Conference, GECCO",
            "venue": "Association for Computing Machinery,",
            "year": 2018
        },
        {
            "authors": [
                "Aneta Neumann",
                "Wanru Gao",
                "Markus Wagner",
                "Frank Neumann"
            ],
            "title": "Evolutionary Diversity Optimization Using Multi-Objective Indicators",
            "venue": "In Genetic and Evolutionary Computation Conference. Association for Computing Machinery,",
            "year": 2019
        },
        {
            "authors": [
                "Lawrence Page",
                "Sergey Brin",
                "Rajeev Motwani",
                "Terry Winograd"
            ],
            "title": "The PageRank citation ranking: Bringing order to the web",
            "venue": "Technical Report. Stanford InfoLab",
            "year": 1999
        },
        {
            "authors": [
                "Ra\u00fal Pardo",
                "Gerardo Schneider"
            ],
            "title": "Model checking social network models",
            "venue": "arXiv preprint arXiv:1709.02105",
            "year": 2017
        },
        {
            "authors": [
                "Mina Young Pedersen",
                "Marija Slavkovik",
                "Sonja Smets"
            ],
            "title": "Social Bot Detection as a Temporal Logic Model Checking Problem",
            "venue": "In International Workshop on Logic, Rationality and Interaction",
            "year": 2021
        },
        {
            "authors": [
                "Adil Rasheed",
                "Omer San",
                "Trond Kvamsdal"
            ],
            "title": "Digital Twin: Values, Challenges and Enablers From a Modeling Perspective",
            "venue": "IEEE Access",
            "year": 2020
        },
        {
            "authors": [
                "Sebastian Risi",
                "Sandy D. Vanderbleek",
                "Charles E. Hughes",
                "Kenneth O. Stanley"
            ],
            "title": "How novelty search escapes the deceptive trap of learning to learn",
            "venue": "Association for Computing Machinery,",
            "year": 2009
        },
        {
            "authors": [
                "Rishi Ranjan Singh"
            ],
            "title": "Centrality Measures: A Tool to Identify Key Actors in Social Networks",
            "year": 2020
        },
        {
            "authors": [
                "Tamara Ulrich",
                "Lothar Thiele"
            ],
            "title": "Maximizing population diversity in singleobjective optimization",
            "venue": "Association for Computing Machinery,",
            "year": 2011
        },
        {
            "authors": [
                "Ziyuan Zhong",
                "Gail Kaiser",
                "Baishakhi Ray"
            ],
            "title": "Neural Network Guided Evolutionary Fuzzing for Finding Traffic Violations of Autonomous Vehicles",
            "venue": "IEEE Transactions on Software Engineering Early Access",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "To mitigate these risks, fuzz testing, a method of testing with randomised inputs, can provide increased confidence in the correct functioning of a social network. However, implementing traditional fuzz testing methods can be prohibitively difficult or impractical for programmers outside of the network\u2019s development team.\nTo tackle this challenge, we present Socialz, a novel approach to social fuzz testing that (1) characterises real users of a social network, (2) diversifies their interaction using evolutionary computation across multiple, non-trivial features, and (3) collects performance data as these interactions are executed. With Socialz, we aim to provide anyone with the capability to perform comprehensive social testing, thereby improving the reliability and security of online social networks used around the world.\nKEYWORDS Fuzz testing; graph social network; diversity optimisation. ACM Reference Format: Francisco Zanartu, Christoph Treude, and Markus Wagner. 2023. Socialz: Multi-Feature Social Fuzz Testing. In GECCO \u201923: ACM Genetic and Evolutionary Computation Conference, July 15-19, 2023, Lisbon, Portugal. ACM, New York, NY, USA, 9 pages. https://doi.org/XXXXXXX.XXXXXXX"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Online social networks (OSNs) are an integral part of modern society, influencing a wide range of aspects of our daily lives. The vast quantity of personal information shared on these platforms makes them a treasure trove for companies seeking to reach out to potential customers and for individuals looking to grow their social circle or entertain themselves. However, like any software, OSNs are prone to bugs and technical issues; consequences range from poor user experience [6, 10] to massive data breaches affecting billions of individuals [26].\nThe recent rise of social bugs in software systems has prompted the need for social testing [1]. However, for the research community,\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. GECCO\u201923, July 15-19, 2023, Lisbon, Portugal \u00a9 2023 Association for Computing Machinery. ACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00 https://doi.org/XXXXXXX.XXXXXXX\nsocial testing poses several challenges. First and foremost, obtaining data from online social networks (OSNs) can be time-consuming, resource-intensive, and requires specialised expertise, which may not be accessible to non-specialists [13]. For example, privacy policies and community guidelines may restrict access to data, making it unavailable to researchers; Furthermore, descriptions of data extraction methods are often omitted in many studies [14]. Second, researchers may be limited in their ability to conduct experiments on OSN systems if they are built using proprietary software platforms [1]. Finally, the sheer size and complexity of OSN systems can result in significant operational costs, which can impede researchers\u2019 ability to conduct large-scale experiments, limiting the scope and depth of their research.\nTo overcome these challenges, technology companies develop tools such asWeb-Enabled Simulation (WES, by Facebook/Meta) [1], allow developers to test code updates and new features in a simulated environment, without risking real user data. Social testing, which involves simulating interactions among a large community of users, can be used to uncover faults in online social networks. However, these tools are not widely available to the public and may have limitations in simulating the full spectrum of user behaviours that could result in uncovering bugs.\nTo address these limitations, we introduce Socialz, an approach for social fuzz testing, which makes the following key contributions:\n(1) characterisation of users of a real social network, (2) evolutionary diversification of community interaction with\nrespect to multiple, non-trivial features, and (3) a workflow for executing interactions in the system and\ncollecting performance data.\nSocialz aims to advance the field of social testing through diversity-based user behaviour. Our approach involves evolving diverse sets of virtual users that are diversified and unbiased across a non-trivial feature space. This allows us to cover a wider range of behaviours compared to real users, and increases the likelihood of uncovering bugs that may not be detected by a set of similar and biased virtual users."
        },
        {
            "heading": "2 RELATEDWORK",
            "text": "Software testing has evolved into a vast field that includes many different methods and techniques for assessing the performance, usability, and other attributes of software systems [18]. Testing is performed at various levels of abstraction, ranging from unit testing to system testing.\nRecently, the concept of social testing has emerged as a new level of abstraction, potentially positioned above system testing [1]. This is due to the recognition that social bugs can arise through community interactions and may not be uncovered by traditional testing that focuses solely on single user journeys [17].\nar X\niv :2\n30 2.\n08 66\n4v 3\n[ cs\n.S E\n] 1\n5 M\nar 2\n02 3\nIn the following overview, we will briefly examine prior work in the testing of social networks, the application of evolutionary methods in fuzz testing, and diversity optimisation."
        },
        {
            "heading": "2.1 Testing of Social Networks",
            "text": "Search-Based Software Testing (SBST) is a technique that leverages optimization search algorithms to solve software testing problems [25]. This approach is widely used in both industrial and academic sectors, including at Facebook, where it is employed to test the behaviour of both the system and its users [3].\nIn social networks, testing goes beyond simply assessing the behaviour of the system and involves evaluating the interactions between users facilitated by the platform. To that end, Web-Enabled Simulation (WES) simulates the behaviours of a community of users on a software platform using a set of bots [1]. Traditional tests, on the other hand, involve executing a predetermined series of input steps. WES is run in-vivo but in a shadow copy of the system, hence as a separate \u201cdigital twin\u201d, which allows for testing without risk to real user data [2]. Both \u201cdigital twins\u201d and WES have widespread industrial applications, not only in OSNs but also in robotics, manufacturing, healthcare, transport, and the energy sector, among others [19, 32].\nIn addition to testing, there are also methods for formally verifying the correctness of social network software and systems. Formal verification of social network protocols and algorithms ensures security and reliability by ensuring access to content is subject to both user-specified and system-specified policies [8, 20]. Model checking can be applied at various levels of abstraction, from high-level network properties to the implementation of individual components [30]. This approach can be used in combination with other testing techniques, such as simulation or testing with softwarecontrolled bots, to provide a more comprehensive evaluation of a social network\u2019s behaviour and performance [31]."
        },
        {
            "heading": "2.2 Evolutionary Fuzzing",
            "text": "We adopt definitions fromManes et al. [23], who define fuzzing as a software testing methodology that involves injecting unexpected or randomised input data into a program under test (PUT) to uncover defects or bugs. A specific application of fuzzing, called fuzz testing, evaluates the security policy violations of the PUT. The tool used to perform fuzz testing is known as a fuzzer.\nhe inputs used in fuzzing can be selected either randomly, with each element having an equal chance of being chosen, or through guided methods such as syntactic or semantic models [24]. Evolutionary fuzz testing is a variant of fuzz testing that leverages evolutionary algorithms to optimise the input data. Research has shown that it is effective in discovering a wide range of vulnerabilities, including those that are difficult to detect with traditional testing methods. For example, Li et al. [22] demonstrate V-Fuzz, a vulnerability-oriented evolutionary fuzzing framework that combines vulnerability prediction with evolutionary fuzzing to reach potentially vulnerable code. They test various open-source Linux applications and fuzzing benchmarks. For Android applications, Cotroneo et al. [9] introduce a coverage-guided fuzzing platform that demonstrated to be more efficient than blind fuzzing. Similarly, Zhong et al. [36] employ a constrained neural network evolutionary\nsearch method to optimise the testing process and efficiently search for traffic violations.\nAlthough fuzzing is widely used, to the best of our knowledge, no research has been conducted to fuzz test a social network system."
        },
        {
            "heading": "2.3 Diversity Optimisation",
            "text": "When it comes to social fuzz testing, the actions a user takes need to be determined, such as \u201cfollow a particular person\u201d. In such a case, defining a user that achieves exactly this goal is straightforward, as a developer can easily translate the desired outcome into a specific interaction. However, creating a user with a more complex behaviour, such as \u201ca virtual user should be highly active but not very central to everything that is going on\u201d, is not as simple. Mapping the desired interaction between the virtual user and its environment is challenging due to the intricate interplay, even though the activity and centrality calculations may not be black boxes by definition. This issue becomes even more complex when designing a group of virtual users within a social network that can interact with each other in various ways.\nWe propose a practical solution to addressing such issues by treating the functions as black boxes and utilising heuristic approaches like novelty search [33] or evolutionary diversity optimisation [35].\nThis is made possible by recent algorithmic advancements in diversity optimisation with multiple features. First, the discrepancybased approach by Neumann et al. [27] aims to minimise the discrepancy of the solution set in the feature space. The discrepancy measure is effectively an indicator function that assesses the set of given solutions in the feature space. Second, Neumann et al. [28] present approaches based on space transformations and multiobjective performance indicators. Among other, these formulations are used to create (1) sets of images (with 149x149 pixels) that are diverse with respect to aesthetic or general features and (2) sets of instances for the travelling salesperson problem (with up to 2000 cities) that are diverse with respect to features like hull ratio and minimum spanning tree depth [5]."
        },
        {
            "heading": "3 THE METHODOLOGY OF SOCIALZ: OVERVIEW",
            "text": "In the following, we present our three-stage approach for fuzz testing OSNs. Each of the following three sections is dedicated to one of the stages, and each of them has a distinct methodology.\nIn Stage 1/3: Characterisation of User Behaviour, we detail the process of obtaining and analysing data from a real OSNs to understand the behaviour of users on the network. Stage 2/3: Evolutionary Diversification of Community Interaction employs evolutionary algorithms to improve the diversity of user behaviour in the network, and Stage 3/3: Execution involves evaluating the evolved network.\nTarget platform. To demonstrate Socialz, we need to choose a server as the PUT. The server needs to meet certain requirements to ensure that the case study can be conducted:\n\u2022 A freely available, open-source server that can be self-hosted. \u2022 An API connection or similar to impersonate users on a network and to simulate events being created by them, necessary for accurately modelling the behaviour of users in a network. \u2022 The ability to gather system performance data.\n\u2022 The option to easily restart the system in order to achieve comparable conditions for the eventual evaluation. \u2022 A well-documented application for easy troubleshooting of problems that may arise during installation, maintenance, and operation of the system.\nGitLab Community Edition (GitLab CE) is the best fit for these requirements. GitLab CE is the free and open source edition of GitLab, which is a platform that has over 30 million registered users [15]. Despite being free, GitLab CE provides a comprehensive set of performancemetrics that are continually stored on an internal Prometheus time-series database. This database can be scrapped with a comprehensive set of pre-defined Grafana dashboards [16], providing a wide range of performance metrics for our purposes (see Figure 1 for an example).\nTo easily reset the system to well-defined states, we run GitLab CE 15.5 in a Docker container, allowing us to delete the data and to restart the container to achieve a reproducible, initial state. Our system runs on a Ubuntu 22.04 virtual machine with 4 CPU cores, 100 GB of hard drive disk space, and 8 GB of RAM."
        },
        {
            "heading": "4 CHARACTERISATION OF USER BEHAVIOUR (STAGE 1/3)",
            "text": "As the starting point for this project, we populate our GitLab server with real-world data fromGitHub.We use GitHub as our data source because it offers the largest repository of information with the kind of data that our platform needs and because it has a large community of over 100 million developers [12], as well as comprehensive documentation that makes it easy to get started, troubleshoot issues, and find help. There are also several Git server alternatives, such as Gitea and Gogs1, that can be self-hosted and have similar features (but not the server performance metrics that GitLab offers), making it easy to transfer solutions from GitHub to them in terms of data. Additionally, projects exist that provide access to user interaction data that is similar to that found in online social networks (OSNs). These interactions include creating and annotating content, creating networks (e.g. starring a repository, linking comments), as well as \u201cmalicious\u201d interactions (e.g. intentionally or unintentionally submitting bugs, spamming, or violating privacy). One such project\n1https://gitea.io, https://gogs.io. Retrieved 25 January 2023.\nthat provide datas on interactions is GH Archive2. It is a public dataset available on Google Big Query that has been recording the public GitHub timeline since 2011, and it makes this data easily accessible through SQL-like queries of JSON-encoded events as reported by the GitHub API.\nTo narrow down the immense amount of data available in GH Archive3, we choose a sub-community within GitHub, and after considering various sub-communities based on their size, we find that COBOL\u2019s is small enough to enable us to conduct a thorough analysis of the data. This is an intentional choice, because we target a \u201ccomplete\u201d subset of the data, i.e. not a random sample of nodes or edges from GH Archive that are not interconnected. We model the data as a graph, with users and repositories as nodes and GitHub events as edges. In this model, repositories can be thought of as groups on an online social network (OSN) where users share and contribute content.\nTo focus on content creation, content annotation, and network creation, we select the following fives types of GitHub events:\n\u2022 WatchEvents and ForkEvents can be likened to liking a public profile page. \u2022 PushEvents can be thought of as being invited to a group with permission to publish some content. \u2022 PullRequests can be thought of as requesting permission to publish something to a group. \u2022 FollowEvents represent establishing a connection or friendship with another user. Unfortunately, as of December 2013 FollowEvents have stopped being recorded in GH Archive, we need to create a workaround where connections between users are based on their similarity. To ensure consistency in our analysis, we disregard the existing FollowEvents in the GH Archive data and instead utilise only our own approach (see Section 5.2).\nFinally, to ensure that our dataset is as complete as possible, we further adjust its size by filtering it to only include events from the years 2011 to 2016. This decision allows us to compile a relatively complete dataset (i.e. starting from the beginning of GH Archive\u2019s records and going up to a particular date), rather than having more recent but incomplete data (e.g. like considering the last six years until today) in which possibly all relevant events would have occurred before the starting date of the snapshot.\nTable 1 summarises the events in the original dataset, which we represent internally as an edge list containing 6,742 events between users and repositories.\n2https://www.gharchive.org. Retrieved 25 January 2023. 3As of December 2022, the GH Archive data stored on Google BigQuery totals more than 17.23 terabytes."
        },
        {
            "heading": "5 EVOLUTIONARY DIVERSIFICATION OF",
            "text": "COMMUNITY INTERACTION (STAGE 2/3)\nIn this section, we outline the components of our evolutionary approach and how they are used to diversify a set of virtual users, which are less biased than their real-world counterparts. This process has the potential to reveal anomalies or unexpected behaviours that would otherwise be difficult to detect in sets of bots that are similar and biased."
        },
        {
            "heading": "5.1 Features of Community Interaction",
            "text": "To characterise users, we investigate three features that we consider to be non-trivial in the sense introduced in Section 2.3: given these features, a developermay struggle tomanually design a set of virtual users that exhibit a spread of desired community interactions. Our chosen features allow us to characterise how active a user is, what is its relative importance and with what kind of events occur:\n\u2022 The graph degree of centrality measure of the nodes quantifies how active a user is in the network, i.e. how many events are submitted. This measure is often used as a notion of popularity in social networks [34], as nodes with a large number of relationships are considered more powerful and central, but has a limitation in that it only takes into account local knowledge of the network topology. Hence, we introduce an additional centrality metric to supplement its analysis next. \u2022 To assess the relative importance of a particular user on the network, we utilise the PageRank algorithm [29]: it is fast to compute, well suited for a directed network such as ours and has been proven to be effective in characterising users [21]. \u2022 To characterise the types of actions a user performs \u2014 for example, a usermay submit only PushEvents, or only ForkEvents and PullRequestsEvents \u2014 we represent each of the 15 possible combinations as a binary vector and then considered the corresponding decimal value as that user\u2019s \u201cevent type\u201d.4 We consider 15 combinations, as we have four event types that are not FollowEvents, and the combination of \u201cuser does not interact at all\u201d is not allowed.\nIt is critical to emphasise that these three metrics are features, not objectives: no user is \u201cbetter\u201d or \u201cworse\u201d than another one, neither in a single-objective sense, nor in a multi-objective sense."
        },
        {
            "heading": "5.2 Solution Evaluation",
            "text": "In our evolutionary setup, each individual is an interaction graph that represents how virtual users interact in a social network. In particular, each individual is an edge list that contains all the necessary information of our graph: the source node, the target node, and the type of event.\nTo evaluate our graphs, we have defined the following steps:\n(1) We first transform the edge list into an two-dimensional adjacency matrix. This adjacency matrix has four areas that reflect the interactions repo-repo, repo-user, user-repo, and user-user (see Table 2).\n4Because the number of users is much larger than the number of possible combinations, and because we aim for diversity, we conjecture that variations to this mapping procedure only have minor effects on the overall outcomes.\nOur rationale for the decision to create FollowEvents between two users based on the simple criterion that the cosine similarity across all events (for these two users) is greater than zero is threefold and mostly based on practical considerations: (1) we make the assumption that users who create similar events may be likely to follow each other, (2) the approach is deterministic and thus saves memory (at the cost of computation time), and (3) it reduces the search space by allowing us to generate community interactions."
        },
        {
            "heading": "5.3 Evolutionary Algorithm",
            "text": "We employ a diversity optimisation approach using the stardiscrepancy measure, based on [27]. The star-discrepancy measures the regularity with which points are distributed in a hypercube, and in particular with respect to all axis-parallel boxes [0, \ud835\udc4f], \ud835\udc4f \u2208 [0, 1]\ud835\udc51 that are anchored in the origin. Hence, this metric helps us evaluate how evenly the points are distributed in the feature space. In our case, each point represents a user with its coordinates\ndefined by the three above-described metrics. We linearly scale all three metrics into [0, 1].\nWe use a (1 + 20)-EA, and in each mutation, we randomly add and delete edges, where the particular action and the particular edge are chosen uniformly at random. When deleting edges, we do not allow to disconnect nodes, in which case we resample.\nTo aid the convergence, we utilise a success-based multiplicative update scheme that can lead to faster convergence solution [11]. This scheme provides a dynamic mutation rate for the EA based on the performance of the offspring. If an iteration is successful, meaning an offspring is not worse than the current solution, the mutation rate is increased by a constant factor\ud835\udc34 = 2. If the offspring is not better, the mutation rate is decreased by a constant factor \ud835\udc4f = 0.5. The initial per-edge mutation rate is 1/\ud835\udc5b, where \ud835\udc5b is the number of edges, which is the total number of events that are not FollowEvents.\nIn summary, our evolutionary approach to diversified community interaction works as follows. We pass an interaction graph as an edge list to our evolutionary algorithm, mutate the edges as described, compute the user\u2019s similarity to add FollowEvents and then create our graph to compute PageRank and centrality for each user and map the users to the combination of events they were involved with. With these three features, we compute a graph\u2019s star-discrepancy score and, by means of our evolutionary algorithm, iteratively keep improving the graph."
        },
        {
            "heading": "5.4 Diversified Community Interaction",
            "text": "We start by using the original edge list of 6,742 events to calculate the similarity between the 1,523 users, resulting in an edge list of 201,983 events with a star-discrepancy score of 0.540 in the threedimensional feature space. Then, we use this as the initial definition of the community interaction on our server, and apply the previously described evolutionary approach. After 1,000 generations5 the process results in an evolved edge list of 533,543 events with a corresponding star-discrepancy score of 0.086 for 1,523 users.\nFigure 2 shows the evolution over time.We note that the diversity improves quickly due to a large number of mutations. We also note that the number of mutations reduces but remained relatively high (at about 100), which indicates that diversity improvements are still frequent, as otherwise the number of mutations would be close to the minimal (enforced) 1."
        },
        {
            "heading": "6 EXECUTION (STAGE 3/3)",
            "text": "In this section, we present our approach for executing and evaluating community interaction."
        },
        {
            "heading": "6.1 Benchmarking the Evolutionary Approach",
            "text": "A natural question is how to compare the Original and Evolved edge lists, because they are noteworthy different size? To compare the two, we craft additional datasets using two approaches. The first approach creates a larger version (called \u201cSimple\u201d) of the original edge list by copying only the existing events until the size of this simple version matched that of the evolved edge list. The second\n5Taking 1.5 hours on an Apple MacBook Pro (2020, M1 Chip ARM64 CPU 8 cores, 16 GB RAM). We implement multiprocessing when evaluating candidates to take advantage of all CPU cores of our machine.\napproach generates new connections at random until the edge list reached the same size as the evolved one; the resulting community interaction is called \u201cRandom\u201d. In both approaches, we ensure that the number of FollowEvents is the same, as these events are considerably more numerous than other types of events. By reducing the potential impact of differences in size on the validity of the results, these approaches allow for the creation of comparable versions of the Original and Evolved interactions.\nTable 3 shows a first comparison between the four interaction graphs: the Original one that is directly based on GitHub data, its Simple (but larger) version, the Random version, and the Evolved one. Figures 3 and 4 add to this by presenting visualisations of the four communities. Figure 3 shows a projection of the graphs into 2d. Edges refer to interactions between users and repositories. Dot size represents the degree centrality of a node with larger dots indicating a higher degree centrality. In addition, the colour of each dot represents the PageRank score of the node, where the colour range from green, less important, to yellow, higher score. As we can see, the Original and the Simple ones are (subjectively) close in structure, while the Evolved and the Random ones are also similar due to the high level of connectedness, but the Evolved one is much more diverse in terms of the distribution of PageRank scores and degree centrality.\nFigure 4 complements these observations by shoing the three features used to calculate the star-discrepancy score for each dataset, i.e. the degree of centrality of each user, their PageRank score, and\nthe combination of events they are involved with. The visualisations show that users in the Original and Simple datasets tend to cluster together and occupy a smaller space, while users in the evolved edge list and the random version of the original edge list appear to be more evenly distributed throughout the space. Interestingly, even the random version achieves a fairly diverse set of interactions, although the degree centrality seems much less covered by the random dataset when compared to the evolved dataset. Overall, this\ndata suggests that our evolutionary algorithm effectively improves the distribution of users in the feature space."
        },
        {
            "heading": "6.2 Observing effects of community interactions",
            "text": "To assess the impact that the different datasets have on the server, we consider the processing of the community interaction as an actual benchmark in itself: as the hundreds of thousands events are processed between the 1523 users on the server, we observe how the system behaves. In the following, we present the workflow used when executing the event and we present our observations.\n6.2.1 Methodology. We require an elaborate workflow (see Figure 5) as the randomised events create a broad range of situations that need to be dealt with; they would otherwise simply results in a myriad of errors. Essentially, all types of events are first validated by checking GitLab CE\u2019s database to see if the user triggering the event exists. If not, the user is created. The same process is followed for the user and/or repository targeted in the event. The flow then proceeds to the corresponding action for that event.\nA complex logic is required for pull request events, where we select or create a branch to submit a pull request. If there is already an open pull request on that branch, we try to merge it. Otherwise, we close the event. If the pull request is closed, we reopen it. To add some realism, we use a corpus of words that we extracted from the original dataset, so when we create a commit or a pull request, we add a random text from this corpus, allowing the system to check how many lines of text were added or deleted following Git logic.\nOn the technical side, we use the previously described setup with GitLab CE and the virtual machine. Our GitLab API wrapper code implements the workflow and is used to load our datasets (Original, Evolved, Simple, and Random). During the processing, we collect performance data from the internal Grafana dashboard panels and from the Prometheus database (see Section 3) included in the GitLab installation. As substantial development effort has gone into developing the evaluation environment, we make the four Docker images publicly available at https://github.com/fzanart/ Socialz/.\nThe processing of all community interactions is time-consuming: the execution of the Original/Evolved/Simple/Random datasets on four identical virtual machines takes 9.9/33.3/27.2/31.7 hours.\n6.2.2 Effects on the system. First, Figure 6 shows \u2014 over time \u2014 the memory utilisation, the CPU saturation, and the requests per second. As we can see, all four interaction graphs result in different workloads over time. We can make a few major observations. First, considering the memory consumption, the Evolved data set appears to result in fewer sharp increases than the other three; the same observation holds for the CPU saturation, where Simple and Random appear to have more and larger spikes than Evolved. Second, while the Evolved and Random ones appear to affect the system in similar ways (with the respective data being very similar), the Simple one results in a higher CPU saturation while also being able to process more requests per second; this appears to be contradictory at first, and we can only conjecture that the Simple one exercises the system differently, possibly due to the higher proportion of PushEvents, but other explanations are possible, too.\nThird, regarding the the major drop in memory consumption by 4\u20138% at about 12 hours into the processing, we conjecture that this is due to a scheduled maintenance; interestingly, this drop does not seem to have obvious effects on the CPU saturation or on the number of processed requests per second.\nIn order to gain further insight into user behaviour and its impact on the system, we present a correlation analysis in Figure 7 between three feature metrics (PageRank, Degree centrality, and Event types) and three resource utilization metrics (CPU saturation, Memory saturation, and Latency), averaging results for each user. Despite acknowledging the fluctuation in performance over time as illustrated in Figure 6, our analysis identifies several statistically significant differences. For instance, while all three community interactions involve changes to the original dataset, our findings demonstrate that sometimes these evolved interactions impact the server in a manner that is more similar to the original dataset rather than simple or random modifications.\nSpecifically, we observe strong negative Spearman correlations (coefficients \u22120.69 \u2264 \ud835\udc5f \u2264 \u22120.62, \ud835\udc5d < 1%) between PageRank and CPU, Memory, and Latency in the original data, which is maintained in the evolved dataset. Conversely, no correlation (|\ud835\udc5f | < 0.1) is observed in effects of Simple and Random. This observation leads\nus to hypothesise that a high number of connections between irrelevant users may increase system load, as the system has to execute expensive tasks such as creating these users and connections. On the other hand, relevant users with many connections have already executed several tasks that do not need to be repeated, such as creating new repositories, which could reduce system load.\nWith regards to degree centrality, while we can make out visual differences in the data, we do not observe any outstanding relationships when considering Spearman correlation; we do only observe similar ranges of CPU and Memory utilisation and Latency between the original, simple, and evolved datasets. The random dataset occupy a smaller range, while the evolved dataset looks more spread out due to optimisation. Finally, the data on the event types overlaps much due to the discrete nature of the event types; the correlations are weak (if at all) at |\ud835\udc5f | < 0.2.\n6.2.3 Characterisation of the Performance Indicators. In our first attempt to evaluate the potential usefulness of performance indicators during the evolution of community interaction in-vivo on a server, we have identified three main observations.\nFirst, we have noticed that neighboring data points in Figure 6 often show variation due to what seems to be random noise, making it challenging to compare marginal differences that may be impacted by factors outside of our control. In addition, we can see from Figure 7 that users who are similar, such as having the same PageRank or degree centrality, can still have different experiences in terms of server load, making it potentially difficult to develop a surrogate model.\nSecond, we have observed that disruptive events can be triggered by processes running on the virtual machine or by GitLab CE\u2019s own management, which can have a significant impact on the performance of the server.\nThird, we have noted significant changes in the performance of the server at the start of the evaluation, where there is a sudden change observed after just a few minutes or hours. These changes may be indicative of major state changes that occur during the evolution of community interaction on the server.\n6.2.4 Found limitation. Our case study has uncovered a crucial issue with the server that could not have been detected through a simple repetition of the Simple dataset. During our investigations, we found that GitLab imposes limits to ensure optimal performance quality. In particular, our initial experiments revealed a restriction on the maximum number of followed users to 300. This was confirmed by a review of public issue comments in GitLab, which indicated that this limit was in place to prevent the activity page for followed users from failing to load [7]. In our case, tests resulted in HTTP 304 errors when trying to load follow events for users who were following more than 300 users, causing the event creation to fail. To resolve this issue, we manually edited GitLab\u2019s source code to increase the limit to 1,523, the total number of users in our dataset. It is important to mention that for our use case, we do not access the activity page for followed users, so we conjecture that our modification of the source code does not have any adverse impact on our results. Hence, we see this as a good illustration of how diverse data can outperform original data for testing configurations: a broader range of data points offers more chances of uncovering issues, making it a valuable asset for any testing process."
        },
        {
            "heading": "7 EFFICIENT EVOLUTION AND EVALUATION",
            "text": "In conclusion of this first case study of Socialz, we would like to bring attention to two important research questions: how can we efficiently evolve and evaluate community interaction?\nEfficiently evolving community interaction is crucial as it allows for iterative improvements and explorations based on observed data, increasing the chances of identifying social bugs. It may be beneficial to evolve interactions either (1) in-vivo, i.e. small-scale interactions are evaluated \u201clive\u201d on a running server, or (2) to run comprehensive evaluations for each large community interaction akin to those performed in Section 6. However, small-scale evaluations come with the challenge of affecting the virtual machine and creating unintended side-effects, such as triggering memory clean-ups or altering the system\u2019s performance over time, as seen in Figure 6. In contrast to this, comprehensive analyses of the entire community interaction (which may involve hundreds of thousands of events like in our case) are time-consuming, taking over one day each. As a middle way between these two extremes, differential evaluations may be a solution, but only if the effects of mutations can be attributed efficiently and accurately. Currently, this presents a significant challenge both in practice and algorithmically.\nThe question of how much to evolve community interaction is closely tied to the question of which performance indicators should be used. As our data analysis has shown, there is a significant amount of noise present in the server under examination. This is a common issue in complex systems such as Android phones [4], where the targeted application shares resources with multiple processes and modern multi-core hardware. Existing validation methods, such as complete rollbacks to known states or extended repetition, are not practical due to the time and resources required. Therefore, there is a need for schemes that allow for reliable and efficient attribution of community interactions to their effects."
        },
        {
            "heading": "8 CONCLUSIONS AND FUTUREWORK",
            "text": "This study presents a new social fuzz testing method called Socialz, which is based on publicly accessible data from GitHub. The approach uses evolutionary computation to diversify community interactions, and then evaluates the results on a GitLab CE server.\nThe key takeaways of this research are:\n\u2022 Social fuzz testing is a feasible approach, although the initial setup requires significant effort. \u2022 Evolutionary diversity optimisation can generate community interactions that are significantly different from the original data or random data, potentially uncovering social bugs. \u2022 Our testing also revealed a limitation that simple data replay could not.\nIn addition to the already outlined research directions on efficient evolution and evaluation, future work in this area offers endless possibilities, such as:\n\u2022 Further characterisation and hybridisation of subcommunities. \u2022 Exploration of additional community interactions and the related features. \u2022 Integration of Socialz with traditional fuzz testing techniques that target code-level or system-level interactions.\nTo support future research in social testing, the code, data, and virtual machines used in this study are available for public use at https://github.com/fzanart/Socialz/."
        },
        {
            "heading": "Acknowledgements",
            "text": "This project has been enabled by a gift from Facebook/Meta: https://research.facebook.com/blog/2021/9/announcing-thewinners-of-the-2021-rfp-on-agent-based-user-interactionsimulation-to-find-and-fix-integrity-and-privacy-issues/"
        }
    ],
    "title": "Socialz: Multi-Feature Social Fuzz Testing",
    "year": 2023
}