{
    "abstractText": "Successful applications of distributional reinforcement learning with quantile regression prompt a natural question: can we use other statistics to represent the distribution of returns? In particular, expectile regression is known to be more efficient than quantile regression for approximating distributions, especially on extreme values, and by providing a straightforward estimator of the mean it is a natural candidate for reinforcement learning. Prior work has answered this question positively in the case of expectiles, with the major caveat that expensive computations must be performed to ensure convergence. In this work, we propose a dual expectile-quantile approach which solves the shortcomings of previous work while leveraging the complementary properties of expectiles and quantiles. Our method outperforms both quantile-based and expectile-based baselines on the MuJoCo continuous control benchmark.",
    "authors": [
        {
            "affiliations": [],
            "name": "Sami Jullien"
        },
        {
            "affiliations": [],
            "name": "Romain Deffayet"
        },
        {
            "affiliations": [],
            "name": "Maarten de Rijke"
        }
    ],
    "id": "SP:1ba6c4f76735dfff96cceab06df8941132f0fb62",
    "references": [
        {
            "authors": [
                "Philip J. Ball",
                "Laura Smith",
                "Ilya Kostrikov",
                "Sergey Levine"
            ],
            "title": "Efficient online reinforcement learning with offline data",
            "venue": "arXiv preprint arXiv:2302.02948,",
            "year": 2023
        },
        {
            "authors": [
                "Marc G. Bellemare",
                "Will Dabney",
                "R\u00e9mi Munos"
            ],
            "title": "A distributional perspective on reinforcement learning",
            "venue": "In ICML,",
            "year": 2017
        },
        {
            "authors": [
                "Marc G. Bellemare",
                "Will Dabney",
                "Mark Rowland"
            ],
            "title": "Distributional Reinforcement Learning",
            "year": 2023
        },
        {
            "authors": [
                "Jacqueline G Cavazos",
                "P Jonathon Phillips",
                "Carlos D Castillo",
                "Alice J O\u2019Toole"
            ],
            "title": "Accuracy comparison across face recognition algorithms: Where are we on measuring race bias",
            "venue": "IEEE Transactions on Biometrics, Behavior, and Identity Science,",
            "year": 2020
        },
        {
            "authors": [
                "Xinyue Chen",
                "Che Wang",
                "Zijian Zhou",
                "Keith W. Ross"
            ],
            "title": "Randomized ensembled double q-learning: Learning fast without a model",
            "venue": "In ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "Yinlam Chow",
                "Mohammad Ghavamzadeh",
                "Lucas Janson",
                "Marco Pavone"
            ],
            "title": "Risk-constrained reinforcement learning with percentile risk",
            "venue": "criteria. JMLR,",
            "year": 2017
        },
        {
            "authors": [
                "Will Dabney",
                "Georg Ostrovski",
                "David Silver",
                "R\u00e9mi Munos"
            ],
            "title": "Implicit quantile networks for distributional reinforcement learning",
            "venue": "In ICML,",
            "year": 2018
        },
        {
            "authors": [
                "Will Dabney",
                "Mark Rowland",
                "Marc Bellemare",
                "R\u00e9mi Munos"
            ],
            "title": "Distributional reinforcement learning with quantile regression",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Jingliang Duan",
                "Yang Guan",
                "Shengbo Eben Li",
                "Yangang Ren",
                "Qi Sun",
                "Bo Cheng"
            ],
            "title": "Distributional soft actor-critic: Off-policy reinforcement learning for addressing value estimation errors",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Zimeng Fan",
                "Nianli Peng",
                "Muhang Tian",
                "Brandon Fain"
            ],
            "title": "Welfare and fairness in multiobjective reinforcement learning",
            "venue": "In AAMAS,",
            "year": 2023
        },
        {
            "authors": [
                "Tuomas Haarnoja",
                "Aurick Zhou",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor",
            "venue": "arXiv preprint arXiv:1801.01290,",
            "year": 2018
        },
        {
            "authors": [
                "Xuming He"
            ],
            "title": "Quantile curves without crossing",
            "venue": "The American Statistician,",
            "year": 1997
        },
        {
            "authors": [
                "Takuya Hiraoka",
                "Takahisa Imagawa",
                "Taisei Hashimoto",
                "Takashi Onishi",
                "Yoshimasa Tsuruoka"
            ],
            "title": "Dropout q-functions for doubly efficient reinforcement learning",
            "venue": "In ICLR,",
            "year": 2022
        },
        {
            "authors": [
                "Arthur E. Hoerl"
            ],
            "title": "Application of ridge analysis to regression problems",
            "venue": "Chemical Engineering Progress,",
            "year": 1962
        },
        {
            "authors": [
                "Shengyi Huang",
                "Rousslan Fernand Julien Dossa",
                "Chang Ye",
                "Jeff Braga",
                "Dipam Chakraborty",
                "Kinal Mehta",
                "Jo\u00e3o G.M. Ara\u00fajo"
            ],
            "title": "Cleanrl: High-quality single-file implementations of deep reinforcement learning",
            "venue": "algorithms. JMLR,",
            "year": 2022
        },
        {
            "authors": [
                "Sami Jullien",
                "Mozhdeh Ariannezhad",
                "Paul Groth",
                "Maarten de Rijke"
            ],
            "title": "A simulation environment and reinforcement learning method for waste reduction",
            "venue": "TMLR,",
            "year": 2023
        },
        {
            "authors": [
                "James Kostas",
                "Yash Chandak",
                "Scott M Jordan",
                "Georgios Theocharous",
                "Philip Thomas"
            ],
            "title": "High confidence generalization for reinforcement learning",
            "venue": "In ICML,",
            "year": 2021
        },
        {
            "authors": [
                "Ilya Kostrikov",
                "Ashvin Nair",
                "Sergey Levine"
            ],
            "title": "Offline reinforcement learning with implicit q-learning",
            "venue": "In ICLR,",
            "year": 2022
        },
        {
            "authors": [
                "Yecheng Ma",
                "Dinesh Jayaraman",
                "Osbert Bastani"
            ],
            "title": "Conservative offline distributional reinforcement learning",
            "venue": "In NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "John Martin",
                "Michal Lyskawinski",
                "Xiaohu Li",
                "Brendan Englot"
            ],
            "title": "Stochastically dominant distributional reinforcement learning",
            "venue": "In ICML,",
            "year": 2020
        },
        {
            "authors": [
                "Borislav Mavrin",
                "Hengshuai Yao",
                "Linglong Kong",
                "Kaiwen Wu",
                "Yaoliang Yu"
            ],
            "title": "Distributional reinforcement learning for efficient exploration",
            "venue": "In ICML,",
            "year": 2019
        },
        {
            "authors": [
                "Alberto Maria Metelli",
                "Mirco Mutti",
                "Marcello Restelli"
            ],
            "title": "A tale of sampling and estimation in discounted reinforcement learning",
            "venue": "In Proceedings of The 26th International Conference on Artificial Intelligence and Statistics,",
            "year": 2023
        },
        {
            "authors": [
                "Volodymyr Mnih",
                "Koray Kavukcuoglu",
                "David Silver",
                "Alex Graves",
                "Ioannis Antonoglou",
                "Daan Wierstra",
                "Martin Riedmiller"
            ],
            "title": "Playing Atari with deep reinforcement learning",
            "venue": "arXiv preprint arXiv:1312.5602,",
            "year": 2013
        },
        {
            "authors": [
                "Whitney K. Newey",
                "James L. Powell"
            ],
            "title": "Asymmetric least squares estimation and testing",
            "year": 1987
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga",
                "Alban Desmaison",
                "Andreas Kopf",
                "Edward Yang",
                "Zachary DeVito",
                "Martin Raison",
                "Alykhan Tejani",
                "Sasank Chilamkurthy",
                "Benoit Steiner",
                "Lu Fang",
                "Junjie Bai",
                "Soumith Chintala"
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "venue": "In NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "Colin Philipps"
            ],
            "title": "When is an expectile the best linear unbiased estimator? SSRN",
            "year": 2021
        },
        {
            "authors": [
                "Collin Philipps"
            ],
            "title": "Interpreting expectiles",
            "venue": "SSRN,",
            "year": 2021
        },
        {
            "authors": [
                "Boris T. Polyak",
                "Anatoli B. Juditsky"
            ],
            "title": "Acceleration of stochastic approximation by averaging",
            "venue": "SIAM Journal on Control and Optimization,",
            "year": 1992
        },
        {
            "authors": [
                "Mark Rowland",
                "Robert Dadashi",
                "Saurabh Kumar",
                "R\u00e9mi Munos",
                "Marc G. Bellemare",
                "Will Dabney"
            ],
            "title": "Statistics and samples in distributional reinforcement learning",
            "venue": "In ICML,",
            "year": 2019
        },
        {
            "authors": [
                "Mark Rowland",
                "R\u00e9mi Munos",
                "Mohammad Gheshlaghi Azar",
                "Yunhao Tang",
                "Georg Ostrovski",
                "Anna Harutyunyan",
                "Karl Tuyls",
                "Marc G. Bellemare",
                "Will Dabney"
            ],
            "title": "An analysis of quantile temporal-difference learning",
            "venue": "arXiv preprint arXiv:2301.04462,",
            "year": 2023
        },
        {
            "authors": [
                "Malcolm Strens"
            ],
            "title": "A Bayesian framework for reinforcement learning",
            "venue": "In ICML,",
            "year": 2000
        },
        {
            "authors": [
                "Richard S. Sutton",
                "Andrew G. Barto"
            ],
            "title": "Reinforcement Learning: An Introduction",
            "venue": "MIT press,",
            "year": 2018
        },
        {
            "authors": [
                "Emanuel Todorov",
                "Tom Erez",
                "Yuval Tassa"
            ],
            "title": "Mujoco: A physics engine for model-based control",
            "venue": "In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems,",
            "year": 2012
        },
        {
            "authors": [
                "Nikos Vlassis",
                "Mohammad Ghavamzadeh",
                "Shie Mannor",
                "Pascal Poupart"
            ],
            "title": "Bayesian reinforcement learning",
            "venue": "Reinforcement Learning: State-of-the-Art,",
            "year": 2012
        },
        {
            "authors": [
                "Linda Schulze Waltrup",
                "Fabian Sobotka",
                "Thomas Kneib",
                "G\u00f6ran Kauermann"
            ],
            "title": "Expectile and quantile regression\u2014david and goliath",
            "venue": "Statistical Modelling,",
            "year": 2015
        },
        {
            "authors": [
                "Derek Yang",
                "Li Zhao",
                "Zichuan Lin",
                "Tao Qin",
                "Jiang Bian",
                "Tie-Yan Liu"
            ],
            "title": "Fully parameterized quantile function for distributional reinforcement",
            "venue": "learning. NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "Qiwli Yao",
                "Howell Tong"
            ],
            "title": "Asymmetric least squares regression estimation: A nonparametric approach",
            "venue": "Journal of Nonparametric Statistics,",
            "year": 1996
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Distributional reinforcement learning (RL) [3] aims to maintain an estimate of the full distribution of expected return rather than only the mean. It can be used to better capture the uncertainty in the transition matrix of the environment [2], as well as the stochasticity of the policy being evaluated, which may enable faster and more stable training by making better use of the data samples [22].\nNon-parametric approximations of the return distribution learned by quantile regression have proven very effective in several domains [7, 8, 17, 37], when combined with deep RL agents such as deep Q-networks (DQN) [24] or soft actor-critic (SAC) [12]. However, quantile-based methods sometimes require an extensive number of steps to outperform the classical agents they are based upon [9], and\n\u2217Both authors contributed equally to the paper.\nPreprint. Under review.\nar X\niv :2\n30 5.\n16 87\n7v 1\nsometimes fail at solving the task at hand [39]. In this paper, we propose an extension of the nonparametric approach to distributional RL that improves sample efficiency and overall performance. Our approach relies on combining expectile and quantile regression for policy evaluation.\nIn short, where quantiles are a generalisation of the median, expectiles are a generalisation of the mean. Interestingly, they learn different information about on the distribution: quantiles are trained using an asymmetric L1 loss and are therefore robust to outliers, while expectiles rely on an asymmetric L2 loss, which finds more stable solutions and has better optimization properties. In particular, expectile regression produces the best linear unbiased estimator (BLUE) of any point within the range of the distribution, including all quantiles of the distribution [27]. Moreover, expectile regression provides a straightforward estimator of the mean expected return, i.e., the expectile 0.5. Finally, expectiles present valuable properties for deep RL that we detail in Section 4. In particular, we show that they allow for an adaptive action selection strategy based on training errors.\nThe properties listed above make expectile regression a desirable approach for distributional RL. However, in distributional temporal difference learning [3], the distributional Bellman operator requires samples from the target distribution to compute the target during value-function training and, contrary to quantiles, obtaining such samples from expectile values is not obvious. Previous work [30] observed that a naive approach using expectile values as pseudo-samples lacks theoretical guarantees and makes the distribution collapse to the mean in practice. Rowland et al. [30] solved\nthis issue by applying an imputation step that returns samples based on expectile values, and takes the form of finding an approximate solution to an optimization problem. This solution, although effective, is extremely slow, preventing practical applications at scale. Our approach does not have this drawback, as we rely on a dual estimation of quantiles and expectiles, in which quantile values are used to generate samples from the target distribution.\nOur contributions can be summarized as follows:\n\u2022 We propose an implementation of the distributional Bellman operator based on dual estimation of quantiles and expectiles. It solves both in theory and in practice the distribution collapse that previous work observed and does not require intractable computations.\n\u2022 We show in a toy environment and at scale on continuous control tasks that our proposed operator leads to stronger results than both quantile-based and expectile-based approaches.\n\u2022 We leverage properties of the expectiles to design an adaptive action selection strategy based on training errors.\n\u2022 To support the reproducibility of this work, we publicly release the code for our approach.2"
        },
        {
            "heading": "2 Background",
            "text": ""
        },
        {
            "heading": "2.1 Distributional reinforcement learning",
            "text": "We consider an environment modeled by a Markov decision process (MDP) (S,A, R, T, \u03b3), where S and A are a state and action space, respectively, R(s, a) denotes the stochastic reward obtained by taking action a in state s, T (\u00b7 | s, a) is the probability distribution over possible next states after taking a in s, and \u03b3 is a discount factor. Furthermore, we write \u03c0(\u00b7 | s) for a (potentially stochastic) policy selecting the action depending on the current state. We consider the problem of finding a policy maximizing the average discounted return, i.e.,\n\u03c0\u2217 = argmax \u03c0 E [ \u221e\u2211 t=0 \u03b3tR(st, at) ] , (1)\nwhere at \u223c \u03c0(\u00b7 | st) and st+1 \u223c T (\u00b7 | st, at). We can define the action-value random variable for policy \u03c0 as Z\u03c0 : (s, a) 7\u2192 \u2211\u221e t=0 \u03b3\ntR(st, at), with s0 = s, a0 = a. We will refer to action-value variables as Z-functions in the remainder. Note that the Q-function, as usually defined in RL [33], is given by Q\u03c0(s, a) = E [Z\u03c0(s, a)]. In this work, we consider approaches that evaluate policies through distributional dynamic programming, i.e., by repeatedly applying the distributional Bellman operator T to a candidate Z-function:\nT Z(st, at) = R(st, at) + \u03b3Z(st+1, at+1). (2) This operator has been shown to be a contraction in the p-Wasserstein distance and therefore admits a unique fixed point Z\u03c0 [2]. A major challenge of distributional RL resides in the choice of representation for the action-value distribution, as well as the exact implementation of the distributional Bellman operator."
        },
        {
            "heading": "2.2 Quantile and expectile regression",
            "text": "Let Z be a real-valued probability distribution. The \u03b1-quantile q\u03b1 of Z is defined as a value splitting the probability mass of Z in two parts of weights \u03b1 and 1\u2212 \u03b1, respectively:\nP (z \u2264 q\u03b1) = \u03b1. (3) Therefore, the quantile function QZ : \u03b1 7\u2192 q\u03b1 is the inverse cumulative distribution function: QZ = F\u22121Z . Alternatively, quantiles are given by the minimizer of an asymmetric L1 loss:\nq\u03b1 = argmin q\nEz\u223cZ [(\u03b11z>q + (1\u2212 \u03b1)1z\u2264q) |z \u2212 q|] . (4)\nExpectiles and the expectile function EZ : \u03c4 7\u2192 e\u03c4 are defined analogously, as the \u03c4 -expectile e\u03c4 minimizes the asymmetric L2 loss:\ne\u03c4 = argmin e\nEz\u223cZ [ (\u03c41z>e + (1\u2212 \u03c4)1z\u2264e) (z \u2212 e)2 ] . (5)\n2https://anonymous.4open.science/r/DualRegressionRL\nFurthermore, while Eq. 3 characterizes quantiles using a condition on probability mass, the expectiles are characterized by a partial moment condition:\n\u03c4 \u222b \u221e e\u03c4 (z \u2212 e\u03c4 )dz = (1\u2212 \u03c4) \u222b e\u03c4 \u2212\u221e (e\u03c4 \u2212 z)dz. (6)\nIn particular, the median is the 0.5-quantile and the mean is the 0.5-expectile. A consequence is that expectile regression provides a straightforward estimator of the mean return in RL, while the (weighted) mean of quantiles has been used to approximate the mean in previous work on distributional RL with quantile regression [7, 8, 37], even though this estimator is biased when approximating a finite number of quantiles.\nAs mentioned in the introduction, the L1 and L2 losses used in quantile and expectile regression, respectively, possess different properties. The L1 loss is more robust to outliers [15] while the L2 loss provides more stable solutions, as it is continuously differentiable. Interestingly, expectile regression provides the best linear unbiased estimator of any location parameter within the range of the data distribution, including all quantiles [27]. This suggests that equipped with the right tools, expectile regression can give better approximation of the quantiles than quantile regression itself. This idea motivates our dual approach, and we verify this property on a toy example in Section 5.1.\nExpectiles possess several other interesting properties, as detailed in [28], some of which we use in our method in Section 4."
        },
        {
            "heading": "2.3 Naive expectile-based distributional RL suffers from target mismatch",
            "text": "Quantile regression has been used for distributional RL in many previous studies [7, 8, 37] where a parameterized quantile function Q\u03b8Z(s, a, \u03b1) is trained using a quantile temporal difference loss function derived from Eq. 4:3 LQ ( Q\u03b8Z(s, a, \u00b7) ) =\nN\u2211 i=1 N\u2211 j=1 lQ(qi, zj) with lQ(qi, zj)= ( \u03b1i1zj>qi + (1\u2212 \u03b1i)1zj\u2264qi ) |zj \u2212 qi| , (7)\nwhere the trainable quantile values qi = Q\u03b8Z(s, a, \u03b1i) are obtained by querying the quantile function at various quantile fractions \u03b1i, which can be fixed by the designer [8], sampled from a distribution [7], or learned during training [37]. In quantile-based temporal difference (QTD), the target samples zj can be obtained by querying the estimated quantile function at the next state-action pair: zj = r+\u03b3Q\u03b8Z(s\u2032, a\u2032, \u03b1j).4 Indeed, because the true quantile function is the inverse CDF of the action-value distribution, Dabney et al. [8] and Bellemare et al. [3] showed that quantiles at equidistant fractions minimize the 1-Wasserstein distance with the action-value distribution and that the resulting projected Bellman operator is a contraction mapping in such a distance. Rowland et al. [31] extended these results to prove the convergence of QTD learning under mild assumptions. We refer to these studies for a more detailed convergence analysis.\nIn contrast, expectile-based temporal difference (ETD) does not allow the same update rule as the one given by Eq. 7. We first write the generic ETD loss derived from Eq. 5: LE ( E\u03b8Z(s, a, \u00b7) ) =\nN\u2211 i=1 N\u2211 j=1 lE(ei, zj) with lE(ei, zj) = ( \u03c4i1zj>ei + (1\u2212 \u03c4i)1zj\u2264ei ) (zj \u2212 ei)2 , (8)\nwith ei = E\u03b8Z(s, a, \u03c4i). Here, choosing zj = r + \u03b3E\u03b8Z(s\u2032, a\u2032, \u03c4j), analogously to QTD and nondistributional TD, would cause the update to approximate a different distribution because the expectile function is in general not the inverse CDF of the return distribution. Rowland et al. [30] formalized this idea using the concept of Bellman closedness, i.e., that the Bellman operator yields the same statistics whether it is applied to the target distribution or to the implicit distribution given by statistics of the target distribution (i.e., in our case a mixture of diracs with locations given by quantiles or expectiles). They showed that quantiles are approximately Bellman closed, while expectiles are not.\n3In practice, the original quantile loss is often replaced by a smooth Huber loss, but we omit it for simplicity. 4We can have a\u2032 \u223c \u03c0(\u00b7 | s\u2032), as in actor-critic algorithms, or a\u2032 = argmaxa Q\u03b8Z(s\u2032, a, \u03b1j) as in Q-learning.\nThis section is agnostic to that choice but we refer to [3] for convergence analysis in the latter case.\nIn practice, one may observe that with the naive approach, every estimated expectile collapses to the mean after multiple updates [30], as we can show that, e.g., the expectile function of a Gaussian distribution is another Gaussian distribution with the same mean and a strictly lower variance [28]. Rowland et al. [30] yield a well-behaved update by adding an imputation step that consists in generating samples zj of the target distribution from estimated expectiles ej , which requires solving a root-finding problem. This approach becomes extremely slow as we increase the number of estimated expectiles, rendering it unusable at scale."
        },
        {
            "heading": "3 Related Work",
            "text": ""
        },
        {
            "heading": "3.1 Distributional RL",
            "text": "Distributional reinforcement learning aims to approximate the distribution of future returns Z, instead of its expectation Q. This results in several benefits \u2013 by ascribing randomness to the value of a state-action pair, an algorithm can learn more efficiently for close states and actions [22], as well as capture possible stochasticity in the environment [21].\nEstimating a parameterized distribution is a straightforward approach, and has been mostly explored from both Bayesian [32, 35] and frequentist [17] perspectives. However, this usually requires an expensive likelihood computation, as well as making a restrictive assumption on the shape of the distribution Z. For instance, assuming a normal distribution when the actual distribution is heavy-tailed can yield disappointing results.\nThus, non-parametric approaches are also used to approximate the distribution. C51 [2] quantizes the domain where Z has a non-zero density (usually in 51 atoms, hence the name), and performs weighted classification on the atoms, by computing the cross-entropy between Z and T Z. While C51 greatly increases performance over a deterministic evaluation of the Q-value, it requires the user to set the bounds of the divided interval, does not have a proper distribution structure, and relies on averaging the mass points to obtain an estimation of the mean. Yet, biased estimation of the mean of a distribution is likely to deteriorate the average performance [23]. Other work optimizes on risk metrics such as Value-at-Risk directly, in order to promote exploration safety or prevent risky outcomes [6, 18, 22].\nAnother important non-parametric approach to the estimation of a distribution is quantile regression. Quantile regression originally relies on the minimization of an asymmetric L1 loss. Estimating the quantiles of a distribution allows to approximate the action-value distribution without relying on a shape assumption. QR-DQN [8] introduced quantile regression as a way to minimize the earth mover\u2019s distance (or Wasserstein loss) between Z and T Z. Further, implicit quantile networks (IQN) [7] embed the quantile fraction, in order to process it in the Q-network of a DQN algorithm. Fully parameterized quantile functions (FQF) [37] additionally learn the optimal quantile fractions, that are then used to compute the values via IQN\u2019s embeddings. However, none of the work above addresses the problem of quantile crossing: the value of the quantile 0.2 should be inferior to the value of the quantile 0.3, to respect the monotonicity of the Cumulative Density Function of Z [13]."
        },
        {
            "heading": "3.2 Expectile regression",
            "text": "Expectiles were originally introduced as a family of estimators of location parameters for a given distribution, to palliate possible heteroskedasticity of the error terms in regression [25, 27].\nExpectiles can be seen as mean estimators under missing data [28]. Unlike quantiles, they span the entire convex hull of the distribution\u2019s support, and on this ensemble, the expectile function is strictly increasing: an expectile fraction is always associated to a unique value. Expectiles have been used in reinforcement learning successfully before [30], but in a way that requires a slow optimization step to achieve satisfactory performance. Moreover, expectile regression is subject to the same crossing issue as quantiles, albeit empirically less so [36]. Expectiles have also been used in offline reinforcement learning to compute a soft maximum over potential outcomes seen in the offline data [19].\nIn this paper, we make use of the relation between quantiles and expectiles to learn both at the same time, circumventing the theoretical weaknesses of both approaches. Moreover, our use of monotonically increasing neural networks ensures that neither quantile or expectile curves cross."
        },
        {
            "heading": "4 Method",
            "text": ""
        },
        {
            "heading": "4.1 Training expectiles and quantiles together",
            "text": "We propose to train a single parameterized Z-function using both quantile and expectile regression. We first note that the expectile function at a given state-action pair EZ(s,a) : [0, 1]\u2192 R is a strictly increasing function that spans the entire convex hull of the distribution\u2019s support. On the other hand, the quantile function QZ(s,a) : [0, 1] \u2192 R is non-decreasing and spans the distribution\u2019s support. As a consequence, there exists a strictly increasing functional mapping from quantiles to expectiles. In this work, we propose to train a mapper M : [0, 1] \u2192 [0, 1] to approximate a mapping between quantile fractions and corresponding expectile fractions. In other words, we want QZ(s,a) (\u03c4) = EZ(s,a)(M(\u03c4)). Yao and Tong [38] show that for regression with a location-scale model, quantile and expectile functions share the same feature-dependent parameters:\nQZ|X(\u03c4) = \u00b5(X) + \u03c3(X)Q\u03f5(\u03c4) and EZ|X(\u03c4) = \u00b5(X) + \u03c3(X)E\u03f5(\u03c4), (9)\nwhere \u03f5 defines the shape of the distribution, \u00b5 the location and \u03c3 the scale. As a consequence, under this model, the quantile-to-expectile mapper is independent of the state-action pair X , and we can write: \u2200s \u2208 S, a \u2208 A, \u03c4 \u2208 [0, 1] QZ(s,a) (\u03c4) = EZ(s,a)(MZ(\u03c4)). (10) Motivated by these properties, we design a practical implementation of Eq. 10 for temporal difference using a parameterized function Ss,a\u03b8 (\u03c4) and a parameterized mapper m\u03d5(\u03c4), in order to approximate the expectiles and quantiles of Z(s, a). Following Dabney et al. [7], we sample uniformly in [0, 1] a set of fractions \u03c41, . . . , \u03c4N at each training step. The expectile loss from Eq. 5 is used to train \u03b8, while the quantile loss from Eq. 4 is used to train both \u03b8 and \u03d5, that is, given a tuple of experience (s, a, r, s\u2032, a\u2032):\nL\u03b8,\u03d5(s, a) = LE(Ss,a\u03b8 (\u03c4)) + LQ (S s,a \u03b8 (m\u03d5(\u03c4)))\n= N\u2211 i=1 N\u2211 j=1 ( \u03c4i1zj>Ss,a\u03b8 (\u03c4i) + (1\u2212 \u03c4i)1zj\u2264Ss,a\u03b8 (\u03c4i) ) (zj \u2212 Ss,a\u03b8 (\u03c4i)) 2\n+ ( \u03c4i1zj>Ss,a\u03b8 (m\u03d5(\u03c4i)) + (1\u2212 \u03c4i)1zj\u2264Ss,a\u03b8 (m\u03d5(\u03c4i)) ) |zj \u2212 Ss,a\u03b8 (m\u03d5(\u03c4i))| .\n(11)\nAs we recalled in Section 2, the way we generate samples zj from the target distribution is crucial in distributional RL. While Rowland et al. [30] resorted to solving a root-finding problem at each training step, we can simply query our estimator of quantiles at the next state-action pair to yield a sound imputation step:\n\u2200j \u2208 1, . . . , N, zj = r + \u03b3 \u00b7 stop_grad ( Ss \u2032,a\u2032 \u03b8 (m\u03d5(\u03c4j)) ) . (12)\nWe refer to the algorithm based on the update described in Eqn. 11 and 12 as expectile-quantile temporal difference (EQTD), and provide pseudo-code of its value function update in Algorithm 1."
        },
        {
            "heading": "4.2 Action selection strategy for environment interactions",
            "text": "This section is concerned with the choice of statistic to be considered by the agent when interacting with the environment. Previous studies based on quantiles have considered the CVaR [20] or the (potentially weighted) mean of quantiles [8, 37], and previous work on expectile-based distributional RL used the expectile 0.5 [30], as it is a natural estimator of the mean. We take a different approach by leveraging properties of the expectiles. Namely, expectile regression offers the possibility of adapting the action selection strategy, even when our goal is to optimize for the mean return. This allows us to propose a method allowing faster propagation of the temporal difference error and, as a result, better sample-efficiency.\nGiven a tuple of experience (s, a, r, s\u2032, a\u2032), we write \u03f5\u03c4 = Ss,a\u03b8 (\u03c4)\u2212 (r + \u03b3z\u2032) for the distributional TD error for a fixed \u03c4 with r \u223c R and z\u2032 \u223c Z(s\u2032, a\u2032), and we define \u03f5 := \u03f50.5. Ideally, after training, we should have E [\u03f5] = 0. However, it is likely that during training, we have E [\u03f5] \u0338= 0, as the policy\nAlgorithm 1 Expectile-quantile temporal difference (EQTD) update pseudo-code Require: Statistic function S\u03b8, mapper m\u03d5, fractions (\u03c4i)i=1,...,N , learning rate \u03b7.\nCollect experience (s, a, r, s\u2032, a\u2032) for i = 1, . . . , N do\nCompute expectile values ei \u2190 S\u03b8(s, a, \u03c4i) and quantile values qi \u2190 S\u03b8(s, a,m\u03d5(\u03c4i)) Compute target samples zi \u2190 r + \u03b3 \u00b7 stop_grad(S\u03b8(s\u2032, a\u2032, \u03b1i))\nend for for i = 1, . . . , N do\nCompute expectile loss LE \u2190 1N \u2211N j=1 ( \u03c4i1zj>ei + (1\u2212 \u03c4i)1zj\u2264ei ) (zj \u2212 ei)2\nCompute quantile loss LQ \u2190 1N \u2211N j=1 ( \u03c4i1zj>qi + (1\u2212 \u03c4i)1zj\u2264qi ) |zj \u2212 qi| Update statistic function parameters \u03b8 \u2190 \u03b8 \u2212 \u03b7\u2207\u03b8LE Update mapper parameters \u03d5\u2190 \u03d5\u2212 \u03b7\u2207\u03d5LQ\nend for\nbeing evaluated has not been fully fitted yet. Thus, acting greedily with respect to the mean may be suboptimal, therefore hurting exploitation.\nExpectiles offer the opportunity to act optimally even if the TD error has not been fully reduced yet, by acting greedily with respect to a different expectile \u03c4act, such that E[\u03f5\u03c4act ] = 0. An interesting property of expectiles is that the expectile indexed by \u03c4 corresponds to the mean of a particular reweighted regression problem, where values above it are rescaled by a factor \u03c4 and values below by a factor 1\u2212 \u03c4 [28]. Therefore, to obtain the formula for \u03c4act, we can first write:\nE[\u03f5\u03c4 ] = \u03c4E[ \u03f5 | \u03f5 \u2265 0]Pr(\u03f5 \u2265 0) + (1\u2212 \u03c4)E[ \u03f5 | \u03f5 < 0]Pr(\u03f5 < 0). (13)\nThen the rescaled error \u03f5\u03c4 act is centered on 0 if E[\u03f5\u03c4act ] = 0, i.e.,\n\u03c4actE[ |\u03f5| | \u03f5 \u2265 0]Pr(\u03f5 \u2265 0) = (1\u2212 \u03c4act)E[ |\u03f5| | \u03f5 < 0]Pr(\u03f5 < 0), (14) which, assuming \u03f5 \u0338= 0, gives a formula for the selection of \u03c4act:\n\u03c4act = 1\n1 + E[ |\u03f5||\u03f5\u22650]Pr(\u03f5\u22650)E[ |\u03f5||\u03f5<0]Pr(\u03f5<0) . (15)\nIn practice, given a batch of training errors \u03f5, we compute empirically the expectile fraction \u03c4act and use it to act in the environment. In our case, we use soft actor-critic (SAC) [12] as the backbone for our method, so we train the actor on the task of maximizing the corresponding expectile value of the Z-function, that is, given an actor \u03c0\u03c8 parameterized by \u03c8 and a parameter \u03b1 controlling the entropy of the desired policy, we maximize the following loss by gradient ascent:\nL\u03c8(s, a) = S\u03b8(s, a\u03c8, \u03c4act)\u2212 \u03b1 log \u03c0\u03c8(a\u03c8 | s), with a\u03c8 \u223c \u03c0\u03c8(\u00b7 | s). (16) For better robustness, we perform a soft update of \u03c4act at each training step by Polyak averaging [29] with the same forgetting factor as for target networks of the Z-function. Also, we noticed in our experiments that individual errors could be very large and challenge the stability of our agent, so we clip E[ |\u03f5| | \u03f5 \u2265 0] and E[ |\u03f5| | \u03f5 \u2265 0] in Eq. 15. We perform an ablation study in Appendix B."
        },
        {
            "heading": "4.3 Implementation details",
            "text": ""
        },
        {
            "heading": "4.3.1 Baselines",
            "text": "We experimented with the following baselines to evaluate our approach:\n\u2022 Soft actor-critic (SAC): We take the SAC implementation provided by CleanRL [16], and further tune it to ensure a fair comparison. In particular, we added layer normalization to the Q-network as it gives stronger results, in line with recent research [1, 14], and re-ran the hyperparameter search.\n\u2022 Quantile temporal difference (QTD): We approximate quantiles using the general approach described in IQN [7], on top of the SAC method described above, with minor modifications detailed in the next section.\n\u2022 Expectile temporal difference (ETD): We use a similar approach as for QTD, but trained with an expectile loss and a naive imputation step as described in [30], i.e., expectile values are used as target for the temporal difference loss."
        },
        {
            "heading": "4.3.2 Network architectures and hyperparameters",
            "text": "We base all baselines and our method on the same underlying neural network, derived from CleanRL\u2019s implementation of SAC [16]. Hyperparameters can be found in Appendix A.\nFor distributional methods, we implemented the Z-function as a feed-forward neural network where weights relating to the statistic fraction are forced to be positive, in order to ensure monotonicity of the quantile and expectile functions and prevent quantile crossings [13]. We did not use the fraction proposal network introduced with FQF [37], as well as the sinusoidal embeddings proposed by IQN [7], as they did not lead to any significant improvement. Finally, we found that using layer normalization increased performance for both our method and baselines. Similarly to previous quantile-based methods, we use the N = 200 statistics sampled at each training step to generate N samples from the target distribution.\nThe mapper is implemented as a monotonic neural network. To do so, we simply force its weights to be positive, and process the output through a sigmoid function to constrain the output between 0 and 1. Since the mapper is queried to obtain both the candidate and target values, we use a mapper-specific target network updated less frequently than the live network, using Polyak averaging [29]."
        },
        {
            "heading": "5 Experiments",
            "text": ""
        },
        {
            "heading": "5.1 Chain MDP: A toy example",
            "text": "We start by observing the effect of our proposed operator in a toy environment. The MDP comprises 4 states, with each state pointing to the next through a unique action and without accumulating any reward, until the agent reaches the last state s4, where the episode terminates and the agent obtains a reward sampled from a bimodal distribution r \u223c ( 1 2N (\u22122, 1) + 1 2N (+2, 1) ) . A visual description of the MDP is given in Appendix C.\nFigure 1a highlights the advantageous properties of expectile regression that were introduced in prior work [27, 28, 36, 38]. When trying to approximate the distribution of terminal rewards directly from\nsamples (left plot), expectile regression yields more accurate estimates than quantile regression in the low-data regime (recall that the quantile function is the inverse CDF while the expectile function is in general not). Interestingly, the right plot shows that coupling expectile regression with our mapper allows us to recover the quantile function much more efficiently than quantile regression itself!5\nIn Figure 1b (left), we illustrate the deficiencies of regular QTD and ETD. QTD is sample-inefficient and fails to approximate the distribution within the given evaluation budget. However, we observe that the distribution information is propagated correctly through temporal difference updates, since the quantile functions estimated at each state coincide. On the contrary, the expectile function collapses to the mean as the error propagates from s4 to s1. This is due to the fact that expectile values at the next state-action pair cannot be used as pseudo-samples of the return distribution Z(s\u2032, a\u2032) [30] .\nFinally, Figure 1b (right) shows that our dual training method, where the pseudo-samples of Z(s\u2032, a\u2032) are the estimated quantiles Ss \u2032,a\u2032\n\u03b8 (m\u03d5(\u03c4)), solves both issues: the expectile function does not collapse anymore and the quantile function approximation is more accurate."
        },
        {
            "heading": "5.2 MuJoCo: Continuous control with stochastic policies",
            "text": "In this section, we verify that our dual approach also provides benefits at scale, on continuous control environments. We opted for the MuJoCo control benchmark [34], and in particular the set of Gymnasium [11] environments considered in previous work on SAC-based agents [5, 14]: Ant-v4, Hopper-v4, Humanoid-v4, and Walker2d-v4. We train all agents for a total of 500 thousand environment steps, and plot the average return. We repeat this process for 5 different seeds, in order to reduce the uncertainty of our results. The results of our experiments can be found in Figure 3.\nA first observation is that a well-tuned SAC agent is a hard baseline to beat, even for distributional approaches. While distributional approaches clearly beat SAC on Walker2d, they perform similarly or sometimes worse on other environments. However, our dual approach always performs similarly or better than distributional baselines, beats all baselines on Ant, and is the most sample-efficient approach on Hopper and Walker2d. In particular, our dual EQTD approach retains a strong performance when either QTD or ETD fail, suggesting it gets the best of both worlds, as shown in the toy environment."
        },
        {
            "heading": "6 Limitations and broader impact",
            "text": "Limitations. Our method implies that the action-value distribution follows a distribution from the location-scale family (Eq. 9). While this seems limiting, it does not require all states to be allocated the same distributions, only that the mapping between them remains the same. Moreover, the locationscale family is quite broad, as it includes Normal, Student, Cauchy, GEV distributions, and more. More experiments in various environments would be needed to test the robustness of this assumption. Finally, while our method adds limited overhead to distributional baselines compared to previous work [30], it is still slower than a simple SAC baseline.\nBroader impact statement. Automated decision making, when employed in resource allocation, has the potential to discriminate populations based on various factors [10] \u2013 this is even more true for minorities, that are often not present enough in the training of such algorithms [4]. While this drawback is present in most applications of RL, we believe that moving towards distributional reinforcement learning helps in accounting for edge cases. This is even more true with expectiles, that are notoriously good at representing threshold effects and outliers [27]."
        },
        {
            "heading": "7 Conclusion",
            "text": "We proposed a non-parametric approach to distributional reinforcement learning based on the simultaneous estimation of quantiles and expectiles of the action-value distribution. This approach presents the advantage of leveraging the efficiency of the expectile-based loss for both expectile and quantile estimation while solving the theoretical shortcomings of expectile-based distributional reinforcement learning, which tends to lead to a collapse of the expectile function in practice.\n5The deviation on extreme values between the estimated quantile function and the true inverse CDF is due to the fact that the mapper cannot generate quantiles outside the range of estimated expectiles.\nWe showed on a toy environment how the dual optimization affects the statistics recovered in distributional RL: in short, the quantile function is estimated more accurately than with vanilla quantile regression and the expectile function remains consistent after several steps of temporal difference training. We then assessed the performance of the dual expectile-quantile temporal difference (EQTD) at scale, on the MuJoCo benchmark. Our agent surpassed the performance of both expectile and quantile-based agents, demonstrating its effectiveness in practical scenarios.\nFor future work, we plan to investigate how the dual EQTD approach can be used in risk-aware decision-making problems, and how it performs when the goal is to minimize risk metrics such as (conditional) value-at-risk. Moreover, we plan to gather insights into what type of behavior is favored by the quantile and expectile loss, respectively."
        },
        {
            "heading": "Acknowledgements",
            "text": "This research was supported by Ahold Delhaize and the Hybrid Intelligence Center, a 10-year program funded by the Dutch Ministry of Education, Culture and Science through the Netherlands Organisation for Scientific Research. https://hybrid-intelligence-centre.nl.\nAll content represents the opinion of the authors, which is not necessarily shared or endorsed by their respective employers and/or sponsors."
        },
        {
            "heading": "A Hyperparameters and training setup",
            "text": "We use Pytorch [26] to train our models on a single gpu (NVIDIA A100 80GB). A full training procedure of 500K training steps and corresponding validation steps takes approximately 6 hours in our setup."
        },
        {
            "heading": "B Ablation study for the action selection strategy",
            "text": "We perform a short ablation study on the effect of the action selection strategy introduced in Section 4.2, to assess whether the adaptive version brings an improvement over a mean selection. Our ablation study shows that our adaptive action selection strategy may be slightly more sample efficient than a mean selection on Walker2d-v4 and Humanoid-v4, albeit not significantly on our 5 seeds. However, it improves overall performance and sample efficiency on Hopper-v4.\nThis confirms our initial idea that our adaptative action selection strategy can help in improving the sample efficiency, especially in harder environments such as Hopper."
        },
        {
            "heading": "C Toy Markov decision process",
            "text": ""
        }
    ],
    "title": "Distributional Reinforcement Learning with Dual Expectile-Quantile Regression",
    "year": 2023
}