{
    "abstractText": "This paper studies decentralized online convex optimization in a networked multi-agent system and proposes a novel algorithm, Learning-Augmented Decentralized Online optimization (LADO), for individual agents to select actions only based on local online information. LADO leverages a baseline policy to safeguard online actions for worst-case robustness guarantees, while staying close to the machine learning (ML) policy for average performance improvement. In stark contrast with the existing learning-augmented online algorithms that focus on centralized settings, LADO achieves strong robustness guarantees in a decentralized setting. We also prove the average cost bound for LADO, revealing the tradeoff between average performance and worst-case robustness and demonstrating the advantage of training the ML policy by explicitly considering the robustness requirement.",
    "authors": [
        {
            "affiliations": [],
            "name": "Pengfei Li"
        },
        {
            "affiliations": [],
            "name": "Jianyi Yang"
        },
        {
            "affiliations": [],
            "name": "Shaolei Ren"
        }
    ],
    "id": "SP:552d1ff5721408bf85a8f6624a495eb697900f95",
    "references": [
        {
            "authors": [
                "Hamidreza Shahbazi",
                "Farid Karbalaei"
            ],
            "title": "Decentralized voltage control of power systems using multi-agent systems",
            "venue": "Journal of Modern Power Systems and Clean Energy,",
            "year": 2020
        },
        {
            "authors": [
                "Yuanyuan Shi",
                "Guannan Qu",
                "Steven Low",
                "Anima Anandkumar",
                "AdamWierman"
            ],
            "title": "Stability constrained reinforcement learning for real-time voltage control",
            "venue": "arXiv preprint arXiv:2109.14854,",
            "year": 2021
        },
        {
            "authors": [
                "Saghar Hosseini",
                "Airlie Chapman",
                "Mehran Mesbahi"
            ],
            "title": "Online distributed convex optimization on dynamic networks",
            "venue": "IEEE Transactions on Automatic Control,",
            "year": 2016
        },
        {
            "authors": [
                "Songchun Fan",
                "Seyed Majid Zahedi",
                "Benjamin C. Lee"
            ],
            "title": "The computational sprinting game",
            "venue": "In ASPLOS,",
            "year": 2016
        },
        {
            "authors": [
                "Amal Feriani",
                "Ekram Hossain"
            ],
            "title": "Single and multi-agent deep reinforcement learning for ai-enabled wireless networks: A tutorial",
            "venue": "IEEE Communications Surveys & Tutorials,",
            "year": 2021
        },
        {
            "authors": [
                "Yasar Sinan Nasir",
                "Dongning Guo"
            ],
            "title": "Multi-agent deep reinforcement learning for dynamic power allocation in wireless networks",
            "venue": "IEEE Journal on Selected Areas in Communications,",
            "year": 2019
        },
        {
            "authors": [
                "Fuqiang Yao",
                "Luliang Jia"
            ],
            "title": "A collaborative multi-agent reinforcement learning anti-jamming algorithm in wireless networks",
            "venue": "IEEE wireless communications letters,",
            "year": 2019
        },
        {
            "authors": [
                "Felipe Caro",
                "J\u00e9r\u00e9mie Gallien"
            ],
            "title": "Clearance pricing optimization for a fast-fashion retailer",
            "venue": "Operations research,",
            "year": 2012
        },
        {
            "authors": [
                "Ozan Candogan",
                "Kostas Bimpikis",
                "Asuman Ozdaglar"
            ],
            "title": "Optimal pricing in networks with externalities",
            "venue": "Operations Research,",
            "year": 2012
        },
        {
            "authors": [
                "Alec Koppel",
                "Felicia Y. Jakubiec",
                "Alejandro Ribeiro"
            ],
            "title": "A saddle point algorithm for networked online convex optimization",
            "venue": "IEEE Transactions on Signal Processing,",
            "year": 2015
        },
        {
            "authors": [
                "Xuanyu Cao",
                "Tamer Ba\u015far"
            ],
            "title": "Decentralized online convex optimization based on signs of relative states",
            "year": 2021
        },
        {
            "authors": [
                "Kaixiang Lin",
                "Shu Wang",
                "Jiayu Zhou"
            ],
            "title": "Collaborative deep reinforcement learning",
            "venue": "arXiv preprint arXiv:1702.05796,",
            "year": 2017
        },
        {
            "authors": [
                "XuanyuCao",
                "Tamer Ba\u015far"
            ],
            "title": "Decentralized online convex optimizationwith feedback delays",
            "venue": "IEEE Transactions on Automatic Control,",
            "year": 2021
        },
        {
            "authors": [
                "Mehrdad Mahdavi",
                "Rong Jin",
                "Tianbao Yang"
            ],
            "title": "Trading regret for efficiency: Online convex optimization with long term constraints",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2012
        },
        {
            "authors": [
                "GautamGoel andAdamWierman"
            ],
            "title": "An online algorithm for smoothed online convex optimization",
            "venue": "SIGMETRICS Perform. Eval. Rev.,",
            "year": 2019
        },
        {
            "authors": [
                "Gautam Goel",
                "Yiheng Lin",
                "Haoyuan Sun",
                "Adam Wierman"
            ],
            "title": "Beyond online balanced descent: An optimal algorithm for smoothed online optimization",
            "venue": "In NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "Lijun Zhang",
                "Wei Jiang",
                "Shiyin Lu",
                "Tianbao Yang"
            ],
            "title": "Revisiting smoothed online learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Guanya Shi",
                "Yiheng Lin",
                "Soon-Jo Chung",
                "Yisong Yue",
                "Adam Wierman"
            ],
            "title": "Online optimization with memory and competitive control",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Weici Pan",
                "Guanya Shi",
                "Yiheng Lin",
                "Adam Wierman"
            ],
            "title": "Online optimization with feedback delay and nonlinear switching cost",
            "venue": "Proc. ACMMeas. Anal. Comput. Syst.,",
            "year": 2022
        },
        {
            "authors": [
                "Niangjun Chen",
                "Gautam Goel",
                "AdamWierman"
            ],
            "title": "Smoothed online convex optimization in high dimensions via online balanced descent",
            "venue": "In COLT,",
            "year": 2018
        },
        {
            "authors": [
                "Xiuxian Li",
                "Xinlei Yi",
                "Lihua Xie"
            ],
            "title": "Distributed online convex optimization with an aggregative variable",
            "venue": "IEEE Transactions on Control of Network Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Saghar Hosseini",
                "Airlie Chapman",
                "Mehran Mesbahi"
            ],
            "title": "Online distributed convex optimization on dynamic networks",
            "venue": "IEEE Transactions on Automatic Control,",
            "year": 2016
        },
        {
            "authors": [
                "Weiwei Kong",
                "Christopher Liaw",
                "Aranyak Mehta",
                "D. Sivakumar"
            ],
            "title": "A new dog learns old tricks: RL finds classic optimization algorithms",
            "venue": "In ICLR,",
            "year": 2019
        },
        {
            "authors": [
                "Thomas Barrett",
                "William Clements",
                "Jakob Foerster",
                "Alex Lvovsky"
            ],
            "title": "Exploratory combinatorial optimization with reinforcement learning",
            "venue": "In AAAI,",
            "year": 2020
        },
        {
            "authors": [
                "Han Zhang",
                "Wenzhong Li",
                "Shaohua Gao",
                "Xiaoliang Wang",
                "Baoliu Ye"
            ],
            "title": "Reles: A neural adaptive multipath scheduler based on deep reinforcement learning",
            "year": 2019
        },
        {
            "authors": [
                "Zhihui Shao",
                "Jianyi Yang",
                "Cong Shen",
                "Shaolei Ren"
            ],
            "title": "Learning for robust combinatorial optimization: Algorithm and application",
            "venue": "In INFOCOM,",
            "year": 2022
        },
        {
            "authors": [
                "Kaiqing Zhang",
                "Zhuoran Yang",
                "Han Liu",
                "Tong Zhang",
                "Tamer Basar"
            ],
            "title": "Fully decentralized multi-agent reinforcement learning with networked agents",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Kaiqing Zhang",
                "Zhuoran Yang",
                "Tamer Ba\u015far"
            ],
            "title": "Multi-agent reinforcement learning: A selective overview of theories and algorithms",
            "venue": "Handbook of Reinforcement Learning and Control,",
            "year": 2021
        },
        {
            "authors": [
                "Afshin Oroojlooy",
                "Davood Hajinezhad"
            ],
            "title": "A review of cooperative multi-agent deep reinforcement learning",
            "venue": "Applied Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Pengfei Li",
                "Jianyi Yang",
                "Shaolei Ren"
            ],
            "title": "Robustified learning for online optimization with memory costs",
            "venue": "In INFOCOM,",
            "year": 2023
        },
        {
            "authors": [
                "Nicolas Christianson",
                "Tinashe Handina",
                "Adam Wierman"
            ],
            "title": "Chasing convex bodies and functions with black-box advice",
            "venue": "In COLT,",
            "year": 2022
        },
        {
            "authors": [
                "Joan Boyar",
                "Lene M. Favrholdt",
                "Christian Kudahl",
                "Kim S. Larsen",
                "Jesper W. Mikkelsen"
            ],
            "title": "Online algorithms with advice: A survey",
            "venue": "SIGACT News,",
            "year": 2016
        },
        {
            "authors": [
                "Alexander Wei",
                "Fred Zhang"
            ],
            "title": "Optimal robustness-consistency trade-offs for learning-augmented online algorithms",
            "venue": "In NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "\u00c9tienne Bamas",
                "Andreas Maggiori",
                "Ola Svensson"
            ],
            "title": "The primal-dual method for learning augmented algorithms",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Thodoris Lykouris",
                "Sergei Vassilvitskii"
            ],
            "title": "Competitive caching with machine learned advice",
            "venue": "J. ACM,",
            "year": 2021
        },
        {
            "authors": [
                "Zhipeng Tu",
                "Xi Wang",
                "Yiguang Hong",
                "Lei Wang",
                "Deming Yuan",
                "Guodong Shi"
            ],
            "title": "Distributed online convex optimization with compressed communication",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Yiheng Lin",
                "Judy Gan",
                "Guannan Qu",
                "Yash Kanoria",
                "AdamWierman"
            ],
            "title": "Decentralized online convex optimization in networked systems",
            "venue": "Proceedings of the 39th International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "YifanWu",
                "Roshan Shariff",
                "Tor Lattimore",
                "Csaba Szepesv\u00e1ri"
            ],
            "title": "Conservative bandits",
            "venue": "In International Conference on Machine Learning,",
            "year": 2016
        },
        {
            "authors": [
                "Yunchang Yang",
                "Tianhao Wu",
                "Han Zhong",
                "Evrard Garcelon",
                "Matteo Pirotta",
                "Alessandro Lazaric",
                "Liwei Wang",
                "Simon Shaolei Du"
            ],
            "title": "A reduction-based framework for conservative bandits and reinforcement learning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "EvrardGarcelon",
                "MohammadGhavamzadeh",
                "Alessandro Lazaric",
                "andMatteo Pirotta"
            ],
            "title": "Conservative exploration in reinforcement learning",
            "venue": "Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics,",
            "year": 2020
        },
        {
            "authors": [
                "M. Lin",
                "A. Wierman",
                "L.L.H. Andrew",
                "E. Thereska"
            ],
            "title": "Dynamic right-sizing for power-proportional data centers",
            "venue": "In INFOCOM,",
            "year": 2011
        },
        {
            "authors": [
                "Mohammad A. Islam",
                "Kishwar Ahmed",
                "Hong Xu",
                "Nguyen H. Tran",
                "Gang Quan",
                "Shaolei Ren"
            ],
            "title": "Exploiting spatio-temporal diversity for water saving in geo-distributed data centers",
            "venue": "IEEE Transactions on Cloud Computing,",
            "year": 2018
        },
        {
            "authors": [
                "Gautam Goel",
                "Yiheng Lin",
                "Haoyuan Sun",
                "Adam Wierman"
            ],
            "title": "Beyond online balanced descent: An optimal algorithm for smoothed online optimization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Richard Cheng",
                "Abhinav Verma",
                "Gabor Orosz",
                "Swarat Chaudhuri",
                "Yisong Yue",
                "Joel Burdick"
            ],
            "title": "Control regularization for reduced variance reinforcement learning",
            "venue": "Proceedings of the 36th International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Hoang M. Le",
                "Andrew Kang",
                "Yisong Yue",
                "Peter Carr"
            ],
            "title": "Smooth imitation learning for online sequence prediction",
            "venue": "In ICML,",
            "year": 2016
        },
        {
            "authors": [
                "Yunchang Yang",
                "Tianhao Wu",
                "Han Zhong",
                "Evrard Garcelon",
                "Matteo Pirotta",
                "Alessandro Lazaric",
                "Liwei Wang",
                "Simon Shaolei Du"
            ],
            "title": "A reduction-based framework for conservative bandits and reinforcement learning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Jakub Ch\u0142\u0119dowski",
                "Adam Polak",
                "Bartosz Szabucki",
                "Konrad Tomasz \u017bo\u0142na"
            ],
            "title": "Robust learning-augmented caching: An experimental study",
            "venue": "In ICML,",
            "year": 2021
        },
        {
            "authors": [
                "Keerti Anand",
                "Rong Ge",
                "Amit Kumar",
                "Debmalya Panigrahi"
            ],
            "title": "A regression approach to learning-augmented online algorithms",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Manish Purohit",
                "Zoya Svitkina",
                "Ravi Kumar"
            ],
            "title": "Improving online algorithms via ml predictions",
            "venue": "In NeurIPS,",
            "year": 2018
        },
        {
            "authors": [
                "Goran Zuzic",
                "Di Wang",
                "Aranyak Mehta",
                "D. Sivakumar"
            ],
            "title": "Learning robust algorithms for online allocation problems using adversarial training",
            "venue": "In https: // arxiv. org/ abs/",
            "year": 2010
        },
        {
            "authors": [
                "Daan Rutten",
                "Nicolas Christianson",
                "Debankur Mukherjee",
                "AdamWierman"
            ],
            "title": "Smoothed online optimization with unreliable predictions",
            "venue": "Proc. ACMMeas. Anal. Comput. Syst.,",
            "year": 2023
        },
        {
            "authors": [
                "Antonios Antoniadis",
                "Christian Coester",
                "Marek Elias",
                "Adam Polak",
                "Bertrand Simon"
            ],
            "title": "Online metric algorithms with untrusted predictions",
            "venue": "In ICML,",
            "year": 2020
        },
        {
            "authors": [
                "Yiheng Lin",
                "Yang Hu",
                "Guannan Qu",
                "Tongxin Li",
                "Adam Wierman"
            ],
            "title": "Bounded-regret MPC via perturbation analysis: Prediction error, constraints, and nonlinearity",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Akshay Agrawal",
                "Brandon Amos",
                "Shane Barratt",
                "Stephen Boyd",
                "Steven Diamond",
                "J. Zico Kolter"
            ],
            "title": "Differentiable convex optimization layers",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "AnaRadovanovi\u0107",
                "Ross Koningstein",
                "Ian Schneider",
                "BokanChen",
                "AlexandreDuarte",
                "Binz Roy",
                "Diyue Xiao",
                "Maya Haridasan",
                "Patrick Hung",
                "Nick Care",
                "Saurav Talukdar",
                "Eric Mullen",
                "Kendal Smith",
                "MariEllen Cottman",
                "Walfredo Cirne"
            ],
            "title": "Carbon-aware computing for datacenters",
            "venue": "IEEE Transactions on Power Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Le Yi Wang",
                "Caisheng Wang",
                "George Yin",
                "Feng Lin",
                "Michael P. Polis",
                "Caiping Zhang",
                "Jiuchun Jiang"
            ],
            "title": "Balanced control strategies for interconnected heterogeneous battery systems",
            "venue": "IEEE Transactions on Sustainable Energy,",
            "year": 2016
        },
        {
            "authors": [
                "Eli Cortez",
                "Anand Bonde",
                "Alexandre Muzio",
                "Mark Russinovich",
                "Marcus Fontoura",
                "Ricardo Bianchini"
            ],
            "title": "Resource central: Understanding and predicting workloads for improved resource management in large cloud platforms",
            "venue": "In Proceedings of the 26th Symposium on Operating Systems Principles,",
            "year": 2017
        },
        {
            "authors": [
                "Manajit Sengupta",
                "Yu Xie",
                "Anthony Lopez",
                "Aron Habte",
                "Galen Maclaurin",
                "James Shelby"
            ],
            "title": "The national solar radiation data base (nsrdb)",
            "venue": "Renewable and sustainable energy reviews,",
            "year": 2018
        },
        {
            "authors": [
                "CanWan",
                "Jian Zhao",
                "Yonghua Song",
                "Zhao Xu",
                "Jin Lin",
                "ZechunHu"
            ],
            "title": "Photovoltaic and solar power forecasting for smart grid energy management",
            "venue": "CSEE Journal of Power and Energy Systems,",
            "year": 2015
        },
        {
            "authors": [
                "Asis Sarkar",
                "Dhiren Kumar Behera"
            ],
            "title": "Wind turbine blade efficiency and power calculation with electrical analogy",
            "venue": "International Journal of Scientific and Research Publications,",
            "year": 2012
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "This paper studies the problem of decentralized online convex optimization in networks, where inter-connected agents must individually select actions with sequentially-revealed local online information and delayed feedback from their neighboring agents. We consider a setting where, at each step, agents must decide on an action using local information while collectively seeking to minimize a global cost consisting of the sum of (i) the agents\u2019 node costs, which capture the local instantaneous effects of the individual actions; (ii) temporal costs, which capture the (inertia) effects of local temporal action changes; and (iii) spatial costs, which characterize the loss due to unaligned actions of two connected neighboring agents in the network. This problem models a wide variety of networked systems with numerous applications, such as decentralized control in power systems [1, 2, 3], capacity allocation in multi-rack data centers [4], spectrum management in multi-user wireless networks [5, 6, 7], multi-product pricing in revenue management [8, 9], among many others.\nWhile a centralized algorithm can better minimize the global cost, decentralized optimization offers a number of important benefits such as avoiding a single point of failure and lower computational complexities [10, 11]. Nonetheless, the decentralized setting is significantly more challenging than its centralized counterpart and, despite recent progress (e.g., [12, 13]), many problems still remain open. The key challenges come from information inefficiency and uncertainties, both temporally and spatially. Concretely, an agent\u2019s actions are coupled over time due to the temporal cost that captures the need of avoiding abruptly large changes, and meanwhile, agents are spatially coupled in the network. As a result, to minimize the global cost, agents must know all the future cost functions as well as their neighboring agents\u2019 actions. But, neither perfect knowledge of all the future cost functions nor all the agents\u2019 actions are available to individual agents in a decentralized online setting.\nTo address these challenges, decentralized online convex optimization has been studied under various settings. For example, online algorithms for the special single-agent case [14, 15, 16, 17, 18, 19, 20] and more recently for the multi-agent case [10, 12, 13, 21, 11, 22] have been proposed to minimize the worst-case regret or competitive ratio. However, because these algorithms must make conservative decisions to mitigate potentially adversarial uncertainties, they often do not perform well in terms of the average cost. In contrast, online optimizers based on machine learning (ML) can improve the average performance by exploiting the distributional information for various problems, e.g., [23, 24, 25, 26], including multi-agent networked systems [27, 28, 29]. But, ML-based optimizers typically lack robustness guarantees and can result in a very high cost in the worst case (due to, e.g., out-of-distribution inputs), which makes them unsuitable for mission-critical applications.\nThe field of learning-augmented algorithms has emerged in recent years with the goal of providing \u201cbest of both worlds\u201d guarantees, i.e., algorithms that ensure near-optimal performance (when ML predictions are perfect) while still achieving adversarial robustness guarantees (when ML predictions are of a low quality). Learning-augmented algorithms have achieved that goal in a variety of online problems, e.g., [30, 31, 32, 33, 32, 34, 35]. However, existing algorithms all focus on centralized settings. Further, they predominantly focus on worst-case performance bounds and do not provide guarantees on average cost performance.\nar X\niv :2\n30 6.\n10 15\n8v 2\n[ cs\n.L G\n] 2\n3 Se\np 20\n23\nContributions. We introduce and analyze a novel learning-augmented algorithm for decentralized online optimization. Our algorithm, LADO (Learning-Augmented Decentralized Online optimization), exploits ML predictions to improve average-case cost performance while still providing the adversarial robustness guarantees with respect to any given baseline/expert policy. The key idea behind LADO is to leverage a baseline policy to safeguard online actions to avoid too greedily following ML predictions that may not be robust. Unlike in a centralized setting, the key design challenge for our decentralized setting is determining how to manage the spatial information inefficiency (i.e., not knowing the neighboring agents\u2019 actions in advance). To address this, we propose a novel spatial cost decomposition to split the shared spatial cost in an adaptive manner between connected agents so that each agent can safeguard its own actions based on local information. We also introduce temporal reservation costs to address the worst-case future uncertainties.\nOur main results provide worst-case and average cost bounds for LADO (see Theorems 5.1, 5.2, and 5.3). Importantly, our results also provide bounds for two approaches to ML training in LADO: training that is aware of the robustness step performed by LADO and training that is unaware of how the ML policy will be used. Most of the prior work on learning-augmented algorithms considers ML models unaware of how they will be used. Our results quantify the improvement obtained by explicitly accounting for the robustness step in ML training.\nTo summarize, the main contributions of our work are as follows. First, LADO significantly differs from the existing learning-augmented algorithms by studying a more challenging setting \u2014 decentralized optimization where an agent chooses online actions with only delayed information about the other neighboring agents\u2019 actions. Second, to guarantee worst-case robustness of LADO against a given policy in our decentralized setting, our design of robust action sets includes novel adaptive spatial cost splitting, which is a novel technique and differs from the design in a centralized setting. Last but not least, we rigorously demonstrate the benefit of training the ML policy by explicitly accounting for the robustness constraint in terms of the average cost bound."
        },
        {
            "heading": "2 Related Work",
            "text": "Decentralized online convex optimization. Online convex optimization in a centralized single-agent setting is a classic problem for which many algorithms have been designed to bound the worst-case performance, e.g., [15, 16, 17, 18, 19, 20]. Recently, a growing literature has begun to study online convex optimization in a decentralized networked system [36, 13, 37, 10]. Compared to the centralized setting, the decentralized setting is significantly more challenging, since an agent has no access to the information of other agents before making its action at each step. In this context, a recent work [37] proposes an online algorithm with a bounded competitive ratio and shows the dependency of the competitive ratio on cost predictions. Several other studies [10, 13, 21, 11, 22] propose algorithms with bounded regrets. In all cases, these studies focus on the worst-case performance, which leads to conservative algorithms that may not achieve a low average global cost.\nML-based optimizers. ML policies have been used for exploiting the statistical information and improve the average performance of various (online) optimization problems, including scheduling, resource management, and secretary problems [23, 24, 25, 26]. There also exist ML-based optimizers, such as multi-agent learning [27, 28, 29], in the context of decentralized optimization. However, a crucial drawback of pure ML-based optimizers is that they may have very high or even unbounded costs in the worst case, making them unsuitable for mission-critical applications. We provide an approach to empowering such ML-based algorithms with worst-case robustness guarantees.\nLearning-augmented algorithms. Learning-augmented algorithms have been proposed as a way to add worstcase robustness guarantees on top of ML policies [30, 31, 32, 33, 32, 34, 35]. However, to this point, learningaugmented algorithms have not been designed for decentralized settings. Thus, LADO bridges the gap and designs a novel learning-augmented algorithm with guaranteed worst-case robustness in a decentralized setting. Our work differs from the standard constrained online optimization (e.g., [14]) in that LADO is essentially a meta algorithm leveraging one robust policy to safeguard another policy which can perform better on average. Additionally, LADO considers worst-case robustness and hence substantially differs from conservative bandits/reinforcement learning that focus on average or high-probability performance constraints [38, 39, 40]."
        },
        {
            "heading": "3 Problem Formulation",
            "text": "We study the setting introduced in [37] (where there is no ML policy augmentation) and consider decentralized online convex optimization in a networked system with V = |V| agents/nodes belonging to the set V . If two agents have interactions with each other, there exists an edge between them. Thus, the networked system can be represented\nby an undirected graph (V, E), with E being the set of edges. Each problem instance (a.k.a. episode) consists of T sequential time steps.\nAt step t = 1, \u00b7 \u00b7 \u00b7 , T , each agent v selects an irrevocable action xvt \u2208 Rn. We denote xt = [x1t , \u00b7 \u00b7 \u00b7 , xVt ] as the action vector for all agents at step t, where the superscript v represents the agent index whenever applicable. After xt is selected for step t, the network generates a global cost gt(xt)which consists of the following three parts.\n\u2022 Node cost fvt (xvt ): Each individual agent incurs a node cost denoted as fvt (xvt ), which only relies on the action of a single agent v at step t and measures the effect of the agent\u2019s decision on itself.\n\u2022 Temporal cost cvt (xvt , xvt\u22121): It couples the two temporal-adjacent actions of a single agent v and represents the effect of temporal interactions to smooth actions over time.\n\u2022 Spatial cost s(v,u)t (xvt , xut ): It is incurred if an edge exists between two agents v and u, capturing the loss due to unaligned actions of two connected agents.\nThis formulation applies directly to many real-world applications [37]. For example, in geo-distributed cloud resource management, each data center is an agent whose server provisioning decision (i.e., the number of on/off servers) incurs a node cost that captures its local operational cost [41]. The temporal cost penalizes frequent servers on/off to avoid excessive wear-and-tear (a.k.a., switching costs) [41]. Meanwhile, each data center\u2019s decision results in an environmental footprint (e.g., carbon emission and water consumption) [42]. Thus, the added spatial cost mitigates inequitable environmental impacts in different locations to achieve environmental justice, which is a crucial consideration in many corporates\u2019 Environmental, Social, and Governance (ESG) strategies [43]. We provide more details about the modeling of this application in the appendix, where we also describe and experiment with another application to decentralized battery management for sustainable computing.\nNext, we make the following assumptions commonly adopted for online optimization, e.g., [44, 37]. Assumption 3.1. The node cost fvt : Rn \u2192 R\u22650 is convex and \u2113f -smooth.\nAssumption 3.2. The temporal interaction cost cvt : Rn \u00d7 Rn \u2192 R\u22650 is convex and \u2113T -smooth.\nAssumption 3.3. The spatial interaction cost sv,ut : Rn \u00d7 Rn \u2192 R\u22650 is convex and \u2113S-smooth.\nThe convexity assumption is standard and needed for analysis, while smoothness (i.e., Lipschitz-continuous gradients) ensures that the costs will not vary unboundedly when the actions change.\nThe networked agents collaboratively minimize the total global cost over T time steps defined as:\ncost(x1:T ) = T\u2211 t=1 gt(xt) = T\u2211 t=1 \u2211 v\u2208V fvt (x v t ) + T\u2211 t=1 \u2211 v\u2208V cvt (x v t , x v t\u22121) + T\u2211 t=1 \u2211 (v,u)\u2208E s (v,u) t (x v t , x u t ),\nwhere gt(xt) = \u2211 v\u2208V f v t (x v t ) + \u2211 v\u2208V c v t (x v t , x v t\u22121) + \u2211 (v,u)\u2208E s (v,u) t (x v t , x u t ) is the total cost at time t. With a slight\nabuse of notation, we also denote gt = {fvt , cvt , s(u,v)t , v \u2208 V, (u, v) \u2208 E} as the cost function information for step t, and g1:T = [g1, \u00b7 \u00b7 \u00b7 , gT ] \u2208 G as all the information for the entire problem instance where G is the set of all possible g1:T .\nOur goal is to find a decentralized learning-augmented online policy \u03c0v for each agent v that maps the local available information (to be specified in Section 4.1) to its action xvt at time t.\nFor notational convenience, we also denote \u03c0 = [\u03c01, \u00b7 \u00b7 \u00b7 , \u03c0V ] as the combined policy for the entire networks."
        },
        {
            "heading": "3.1 Performance Metrics",
            "text": "We consider the following two performance metrics \u2014 average cost and \u03bb-robustness. Definition 3.4 (Average cost). Given a decentralized online policy \u03c0 = [\u03c01, \u00b7 \u00b7 \u00b7 , \u03c0V ], the average cost is AV G(\u03c0) = Eg1:T [cost(\u03c0, g1:T )], where the information g1:T follows a distribution Pg1:T .\nDefinition 3.5 (\u03bb-robust to \u03c0\u2020). For \u03bb > 0, a decentralized online policy \u03c0 = [\u03c01, \u00b7 \u00b7 \u00b7 , \u03c0V ] is \u03bb-robust against a baseline policy \u03c0\u2020 if cost(\u03c0, g1:T ) \u2264 (1 + \u03bb)cost(\u03c0\u2020, g1:T )) holds for any g1:T \u2208 G.\nThe average cost measures the decision quality of the decentralized policy \u03c0 in typical cases, whereas the \u03bbrobustness shows theworst-case competitiveness in terms of the cost ratio of the global cost \u03c0 to a given baseline policy \u03c0\u2020 (which is also referred to as an expert policy). Our definition of\u03bb-robustness against \u03c0\u2020 is both general and common in the literature on learning-augmented online algorithms as well as online control [31, 45, 46], where robustness is defined against a given baseline policy \u03c0\u2020 [31]. Importantly, for our problem, there exist various expert policies \u03c0\u2020\nAlgorithm 1 Online Algorithm for Agent v \u2208 V Require: Expert policy \u03c0\u2020v , and ML policy \u03c0\u0303v 1: for t = 1, \u00b7 \u00b7 \u00b7 , T do 2: Collect local online information Ivt . 3: Obtain ML prediction x\u0303vt and expert action xv,\u2020t based on Ivt , respectively. 4: Choose action xt = \u03c8\u03bb(x\u0303vt ) by (2). 5: end for\n(e.g., localized prediction control [37]) with bounded cost ratios against the oracle policy OPT that minimizes the global cost with all the offline information. As a result, by considering an expert policy with a competitive ratio of \u03c1\u03c0\u2020 , our policy \u03c0 is also competitive against the optimal oracle, i.e., cost(\u03c0, g1:T ) \u2264 \u03c1\u03c0\u2020(1 + \u03bb)cost(OPT, g1:T )) for any g1:T \u2208 G. Alternatively, the expert policy \u03c0\u2020 can be viewed as a policy prior currently in use [47], while the new learning-augmented policy \u03c0 must no worse than (1 + \u03bb)-times the policy prior in terms of the cost for any problem instance.\nThe average cost and worst-case robustness metrics are different and complementary to each other [48, 31]. Here, we take a robustness-constrained approach. Specifically, given both an ML-based optimizer and an expert algorithm as advice, we aim to find a learning-augmented policy \u03c0 = [\u03c01, \u00b7 \u00b7 \u00b7 , \u03c0V ] to minimize the average cost subject to the \u03bb-robustness constraint:\nmin \u03c0\nEg1:T [cost(\u03c0, g1:T )] , s.t., cost(\u03c0, g1:T ) \u2264 (1 + \u03bb)cost(\u03c0\u2020, g1:T ), \u2200g1:T \u2208 G. (1)"
        },
        {
            "heading": "4 Robust Decentralized Online Optimization",
            "text": "We now present LADO, a learning-augmented decentralized online optimization algorithm that exploits the benefits of ML while guaranteeing \u03bb-robustness against any given expert policy \u03c0\u2020 in a network."
        },
        {
            "heading": "4.1 A Learning-Augmented Decentralized Algorithm",
            "text": "At time t = 1, \u00b7 \u00b7 \u00b7 , T , each agent v \u2208 V has access to a decentralized online ML policy \u03c0\u0303v and a decentralized online expert policy \u03c0\u2020v , which produce actions x\u0303vt and xv,\u2020t , respectively, based on local online information. Then, given x\u0303vt and xv,\u2020t , the agent v chooses its actual action xvt using LADO."
        },
        {
            "heading": "4.1.1 Local Information Availability",
            "text": "We consider a decentralized setting where only local online information is available to the agents. In particular, the following online information is revealed to each agent v at step t: node cost function fvt , temporal cost function cvt , spatial cost function s(v,u)t\u22121 , connected agents\u2019 actions xut\u22121 and their corresponding expert actions xu,\u2020t\u22121 for (v, u) \u2208 E . That is, at the beginning of step t, each agent v receives its own node cost and temporal cost functions for time t, and also the spatial cost along with the actual/expert actions from the neighboring agents connected to agent v for time t\u2212 1. Thus, before choosing an action at time t, all the local information available to agent v can be summarized as Ivt = {fv1:t, cv1:t, s (v,u) 1:t\u22121, x u 1:t\u22121, x u,\u2020 1:t\u22121, Z v t , (v, u) \u2208 E}, where Zvt captures the other applicable information (e.g., agent v\u2019s own actual/ML/expert actions in the past). Moreover, knowledge of cost functions over the next k temporal steps and/or r-hop agents in the network can further improve the competitiveness of expert policies [37] and, if available, be included in Zvt . Without loss of generality, we use Ivt as the locally available information for agent v at time t. Additionally, the smoothness parameters \u2113f , \u2113c, and \u2113s and robustness parameter \u03bb are known to the agents as shared information.\nMost importantly, unlike in a centralized setting, an agent v must individually choose its (irrevocable) action xvt on its own \u2014 it cannot communicate its action xvt or its expert action xv,\u2020t to its connected agent u until the next time step t+ 1. The one-step delayed feedback of the spatial costs and the actual/expert actions from the connected agents is commonly studied in decentralized online convex optimization [37] and crucially differentiates our work from the prior centralized learning-augmented algorithms, adding challenges for ensuring the satisfaction of the \u03bb-robustness requirement."
        },
        {
            "heading": "4.1.2 Algorithm Design",
            "text": "We present our learning-augmented decentralized online algorithm, LADO, in Algorithm 1, where an ML policy is trained offline and deployed online by each agent v. The assumption of an offline-trained predictor (i.e., ML policy in our case) is standard in learning-augmented algorithms [49, 50, 32, 30] as well as general learning-based optimizers [23, 51, 26]. For our problem, approaches such as multi-agent reinforcement learning [27, 28, 29] can be used to train ML policies for each agent.\nAt time t, each agent v feeds its locally available information Ivt as input into its ML policy \u03c0\u0303v and expert policy \u03c0\u2020v to receive x\u0303vt and xv,\u2020t , respectively, based on which it chooses xvt as the actual action. When the context is clear, we also use ML prediction to refer to the ML action. Note that, in Algorithm 1, we run the expert policy (e.g., the localized policy proposed in [37]) independently as if it is applied alone. Thus, the expert policy \u03c0\u2020v does not need to use all the information in Ivt .\nThe crux of LADO is to carefully leverage ML predictions while being close enough to expert actions. To achieve this, we design a novel robust action set that addresses the key challenge that only local online information Ivt is available to each agent v in our decentralized setting. By choosing an action that falls into the robust action set while staying close to the ML prediction, LADO guarantees \u03bb-robustness and exploits the benefits of ML predictions, achieving the best of both worlds. Concretely, we project the ML prediction x\u0303vt into the robust action set denoted by X v\u03bb,t as follows\nxvt = arg min x\u2208Xv\u03bb,t \u2225x\u2212 x\u0303vt \u22252, (2)\nwhere the robust action set X v\u03bb,t is convex and will be specified in Section 4.2. Thus, the projection in (2) can be efficiently performed by solving convex optimization at each individual agent v."
        },
        {
            "heading": "4.2 Designing a Robust Action Set",
            "text": "The core of LADO is an action set that \u201crobustifies\u201d ML predictions for \u03bb-robustness. This is challenging due to the temporal and spatial information inefficiency \u2014 the \u03bb-robustness requirement in (1) is imposed over the total global cost over T steps, whereas each agent must choose its action based on local and online information Ivt . To address this challenge, we propose novel adaptive spatial cost decomposition and introduce reservation costs to safeguard the online actions for \u03bb-robustness."
        },
        {
            "heading": "4.2.1 Spatial Cost Decomposition",
            "text": "Due to the decentralized setting, we first decompose the global cost gt(xt) at time t into locally computable costs for individual agents v \u2208 V expressed as\ngvt (x v t ) =f v t (x v t ) + c v t (x v t , x v t\u22121) + \u2211 (u,v)\u2208E \u03ba (v,u) t s (u,v) t (x v t , x u t ), (3)\nwhere we use the weights \u03ba(v,u)t \u2265 0 and \u03ba(v,u)t \u2265 0, such that \u03ba(v,u)t + \u03ba(u,v)t = 1 for (v, u) \u2208 E , to adaptively split the shared spatial cost s(u,v)t (xvt , xut ) between the two connected agents (i.e., \u03ba(u,v) for agent v and \u03ba(v,u) for agent u). We specify the choice of the weight \u03ba(v,u)t in (6) later.\nDue to the cost decomposition in (3), the global \u03bb-robustness constraint in (1) can be guaranteed if the action of each node v satisfies the following local constraint:\nt\u2211 i=1 gvi (x v i ) \u2264 (1 + \u03bb) t\u2211 i=1 gvi (x v,\u2020 i ). (4)\nAt step t, however, agent v cannot evaluate its local cost gvt (xvt ), because it has no access to the actions xut and expert actions xu,\u2020t of its connected neighbors u and hence cannot calculate the actual or expert\u2019s spatial costs for (v, u) \u2208 E . Additionally, even if agent v has the knowledge of gvt (xvt ), simply satisfying (4) at time step t cannot guarantee that a feasible action exists to satisfy the local constraints for future steps t+ 1, \u00b7 \u00b7 \u00b7 , T due to the temporal cost. To see this, consider a toy example with T = 2 and cvt = \u2225xvt \u2212 xvt\u22121\u22252. Assume that xv1 is selected such that the first-step local constraint is satisfied by equality, i.e., gv1(xv1) = (1 + \u03bb)gvi (xv,\u20201 ). Then, at the second step t = 2, it can happen that the node costs satisfy fv2 (xv,\u20201 ) = 0 and fv2 (xv1) > 0, while the spatial costs are all zero. Then, with the expert action xv,\u20202 = xv,\u20201 , it follows that gv2(xv2) > (1 + \u03bb)gv2(xv,\u20202 ) = 0 for any xv2 \u2208 X , thus violating the local constraint (4) for agent v. By the same reasoning, the \u03bb-robustness constraint can be violated for the whole network.\nt\u2211 \u03c4=1 fv\u03c4 (x v \u03c4 ) + t\u2211 \u03c4=1 cv\u03c4 (x v \u03c4 , x v \u03c4\u22121) + t\u22121\u2211 \u03c4=1 \u2211 (v,u)\u2208E \u03ba(v,u)\u03c4 \u00b7 s(v,u)\u03c4 (xv\u03c4 , xu\u03c4 ) +R(xvt , x v,\u2020 t )\n\u2264(1 + \u03bb) ( t\u2211 \u03c4=1 fv\u03c4 (x v,\u2020 \u03c4 ) + t\u2211 \u03c4=1 cv\u03c4 (x v,\u2020 \u03c4 , x v,\u2020 \u03c4\u22121) + t\u22121\u2211 \u03c4=1 \u2211 (v,u)\u2208E \u03ba(v,u)\u03c4 \u00b7 s(v,u)\u03c4 (xv,\u2020\u03c4 , xu,\u2020\u03c4 ) ) (5)"
        },
        {
            "heading": "4.2.2 Robust Action Sets via Reservation Costs",
            "text": "To ensure non-empty sets of feasible actions satisfying the local constraints (5) for each time step t, we propose a reservation cost that safeguards each agent v\u2019s action against any possible uncertainties (e.g., connected agent u\u2019s current actions and future cost functions). With only local online information Ivt available to agent v, the key insight of our added reservation cost at each time step t is to bound the maximum possible cost difference between agent v\u2019s cost\u2211ti=1 gvi (xvi ) and its corresponding cost constraint (1 + \u03bb)\u2211ti=1 gvi (xv,\u2020i ) for future time steps. More concretely, we use a new constraint in (5) to define the robust action set for agent v at step t. In (5), the weight \u03ba(v,u)t (attributed to agent v) for adaptively splitting the spatial cost s(v,u)t between agent v and agent u is\n\u03ba (v,u) t =\n\u2225xvt \u2212 x v,\u2020 t \u22252\n\u2225xvt \u2212 x v,\u2020 t \u22252 + \u2225xut \u2212 x u,\u2020 t \u22252\n, (6)\nand the reservation cost is R(xvt , x v,\u2020 t ) =\n\u2113T + \u2113S \u00b7Dv 2 (1 + 1 \u03bb0 )\u2225xvt \u2212 x v,\u2020 t \u22252, (7)\nwhere \u2113T and \u2113S are smoothness parameters for the temporal and spatial cost functions, Dv is the degree of agent v (i.e., the number of agents connected to agent v), and 0 < \u03bb0 \u2264 \u03bb is a hyperparameter to adjust the size of the robust action set (and will be optimally chosen as \u03bb0 = \u221a 1 + \u03bb\u2212 1 in Theorems 5.2 and 5.3). In the special case when both xvt = x v,\u2020 t and xut = xu,\u2020t , we set \u03ba(v,u)t = 12 in (6).\nImportantly, the new constraint (5) for agent v can be calculated purely based on local online information Ivt ; it only depends on the cumulative node and temporal costs up to time t, as well as the spatial costs (including the feedback of the connected neighboring agents\u2019 actions and their expert actions) up to time t \u2212 1. Moreover, the reservation cost R(xvt , xv,\u2020t ) safeguards agent v\u2019s action not only against uncertainties in future temporal cost functions in online optimization, but also against delayed spatial costs resulting from decentralized optimization, which we further explain as follows.\n\u2022 Temporal uncertainties. The temporal cost couples each agent\u2019s actions over time, but the online action needs to be chosen without knowing all the future costs. Consequently, as shown in the example in Section 4.2.1, simply satisfying the \u03bb-robustness in terms of the cumulative cost up to t does not necessarily ensure \u03bb-robustness in the future. To hedge against temporal uncertainties, our reservation cost R(xvt , xv,\u2020t ) in (7) includes the term \u2113T2 (1 + 1 \u03bb0 )\u2225xvt \u2212 x v,\u2020 t \u22252, which bounds the maximum cost disadvantage for agent v: cvt (xvt , xvt+1)\u2212 (1 + \u03bb)cvt (xv,\u2020t , xv,\u2020t+1) \u2264 \u2113T 2 (1 + 1 \u03bb0 )\u2225xvt \u2212 x v,\u2020 t \u22252. Thus, xvt+1 = xv,\u2020t+1 is always a feasible robust action for agent v at time t+ 1.\n\u2022 Spatial uncertainties. In our decentralized setting, agent v chooses its action based on the local online information Ivt , which creates spatial uncertainties regarding its connected neighboring agents\u2019 actions and spatial costs. In our design, with the splitting weight \u03ba(v,u)t in (6) and the term \u2113S \u00b7Dv2 (1 + 1\u03bb0 )\u2225xvt \u2212 x v,\u2020 t \u22252 added to the reservation cost (7), we ensure that our constraint in (5), if satisfied, can always guarantee the local constraint in (4) and hence also the \u03bb-robustness constraint, due to the following inequality:\u2211\n(v,u)\u2208E\n\u03ba (v,u) t ( s (v,u) t (x v t , x u t )\u2212 (1 + \u03bb)s (v,u) t (x v,\u2020 t , x u,\u2020 t ) )\n\u2264 \u2211\n(v,u)\u2208E\n\u03ba (v,u) t \u2113S 2 (1 + 1 \u03bb0 ) ( \u2225xvt \u2212 x v,\u2020 t \u22252 + \u2225xut \u2212 x u,\u2020 t \u22252 ) = \u2113S \u00b7Dv 2 (1 + 1 \u03bb0 )\u2225xvt \u2212 x v,\u2020 t \u22252.\nNote that, as the degreeDv of node v increases, more agents are connected to agent v and hence spatial uncertainties also naturally increase, resulting in an increased reservation cost in (7).\nIn summary, our novel robust action set for agent v at time step t is designed as X v\u03bb,t = {xvt | xvt satisfies (5) for step t}, (8)\nwhich, by convexity of cost functions, is convex and leads to computationally-efficient projection (2)."
        },
        {
            "heading": "5 Performance Bounds",
            "text": "We now analyze LADO in terms of its robustness and average cost, proving that LADO is \u03bb-robust against any given expert policy in the worst case and can simultaneously improve the average performance over the expert policy (if the ML policy is properly trained in Theorem 5.3)."
        },
        {
            "heading": "5.1 Robustness",
            "text": "We formally state the robustness guarantee of LADO as follows. A proof is provided in Appendix C. Theorem 5.1. (\u03bb-robustness) Given any ML policy \u03c0\u0303 and expert policy \u03c0\u2020, for any \u03bb > 0 and \u03bb0 \u2208 (0, \u03bb] in the robust action set in (8), the cost of LADO satisfies cost(LADO, g1:T ) \u2264 (1 + \u03bb) \u00b7 cost(\u03c0\u2020, g1:T ) for any problem instance g1:T \u2208 G.\nTheorem 5.1 guarantees that, for any problem instance g1:T \u2208 G, the total global cost of LADO is always upper bounded by (1 + \u03bb) times the global cost of the expert policy \u03c0\u2020, regardless of the quality of ML predictions. This robustness guarantee is the first in the context of decentralized learning-augmented algorithms and attributed to our novel design of locally computable robust action sets in (8), based on which each agent individually safeguards its own online actions. Moreover, for our setting, there exist online policies (e.g., localized policy for multi-agent networks [37]) that have bounded competitive ratios against the offline oracle and hence can be readily applied as expert policies in LADO. Thus, their competitive ratios immediately translate with a scaling factor of (1 + \u03bb) into competitiveness of LADO against the offline oracle."
        },
        {
            "heading": "5.2 Average Cost",
            "text": "The prior literature on learning-augmented algorithms consider consistency (i.e., the worst-case competitive ratio with respect to the ML policy) as a measure to study how well the ML policy\u2019s performance is retained [52, 53, 31]. Nonetheless, in practice, the key benefit of utilizing anML policy is to improve the average performance. Additionally, given our \u03bb-robustness guarantee, it is impossible to simultaneously achieve 1-consistency (or any consistency better than the pure expert) even for the single-agent setting due to the fundamental challenge of online convex optimization with temporal costs [52].\nThus, we turn to the average performance of LADO and consider two cases: an ML policy trained as a standalone optimizer without being aware of the projection in LADO (Line 4 in Algorithm 1), and the optimal ML policy trained with explicit awareness of the projection operator."
        },
        {
            "heading": "5.2.1 A Projection-Unaware ML Policy",
            "text": "It is a common assumption in learning-augmented algorithms (e.g., [31]) that the ML policy is trained offline as a standalone optimizer in a projection-unaware manner and provided to the downstream algorithm as a black box. Consequently, we should run the ML policy \u03c0\u0303 as if it is applied independently (i.e., not using the actual action xvt\u22121 as the input to the ML policy \u03c0\u0303). By doing so, the only modification applied to the ML policy \u03c0\u0303 is the projection operation in LADO; otherwise, using the actual action xvt\u22121 as input to the ML policy also modifies the input to the ML policy and can introduce additional perturbation, resulting in potential cost increases [54].\nGiven a projection-unaware ML policy \u03c0\u0303, we denote LADO as LADO(\u03c0\u0303) to highlight its dependency on \u03c0\u0303 when applicable. Next, we bound the average cost of LADO(\u03c0\u0303). Theorem 5.2. (Average Cost of LADO(\u03c0\u0303)) Given a projection-unaware ML policy \u03c0\u0303, for any \u03bb > 0, by optimally setting \u03bb0 = \u221a 1 + \u03bb\u2212 1, the average cost of LADO(\u03c0\u0303) is upper bounded by\nAV G(LADO(\u03c0\u0303)) \u2264 min { (1 + \u03bb)AV G(\u03c0\u2020), (\u221a AVG(\u03c0\u0303) + \u221a \u2113f + 2\u2113T + \u2113SDv\n2 \u2126(\u03bb, \u03c0\u0303, \u03c0\u2020)\n)2} ,\nwhere AV G(\u03c0\u2020) and AV G(\u03c0\u0303) are the average costs of the expert policy and the ML policy, respectively, and \u2126(\u03bb, \u03c0\u0303, \u03c0\u2020) =\u2211 v\u2208V \u2211T t=1 Eg1:T {[ \u2225x\u0303vt \u2212 x v,\u2020 t \u22252 \u2212 2( \u221a 1+\u03bb\u22121)2 \u2113f+2\u00b7\u2113T+\u2113S \u00b7Dv \u00b7 cost \u2020 v,t ]+} in which cost\u2020v,t is the expert\u2019s cost for node v at time t.\nTheorem 5.2 quantifies the tradeoff between exploiting a projection-unaware ML policy for average cost performance and following an expert policy for worst-case robustness in a decentralized setting. Specifically, the average cost bound of LADO(\u03c0\u0303) is a minimum of two terms. The first term holds due to the guaranteed \u03bb-robustness against\nthe expert policy. The second term shows that, due to the robustness requirement, LADO(\u03c0\u0303) can deviate from the ML policy and hence have a higher average cost than AV G(\u03c0\u0303). The cost difference mainly depends on an auxiliary term \u2126(\u03bb, \u03c0\u0303, \u03c0\u2020) that measures how well LADO follows the ML policy (i.e., the expected distance between actions made by LADO and actions made by the pure ML policy). The proof relies on our novel spatial cost decomposition (Section 4.2.1) and is deferred to the appendix.\nMore concretely, \u2126(\u03bb, \u03c0\u0303, \u03c0\u2020) decreases and LADO can better exploit the benefits of ML when \u03bb increases (i.e., the robustness requirement is less stringent), the network degree decreases (i.e., agents are less spatially connected and hence there is less spatial uncertainty), and/or \u2225x\u0303vt \u2212 xv,\u2020t \u22252 is smaller (i.e., the ML policy itself is closer to the expert policy with a smaller distance). In these cases, \u2126(\u03bb, \u03c0\u0303, \u03c0\u2020) is smaller, making LADO(\u03c0\u0303) follow the ML policy more closely and hence have an average cost bound closer to AV G(\u03c0\u0303)while still guaranteeing the worst-case \u03bb-robustness. Another insight is that \u2126(\u03bb, \u03c0\u0303, \u03c0\u2020) decreases when the expert policy has a higher cost, which naturally provides more freedom to LADO to follow ML while still being able to satisfy the \u03bb-robustness requirement.\nTheorem 5.2 applies to any ML policies, including:\n\u03c0\u0303\u2217 = argmin \u03c0 Eg1:T [cost(\u03c0, g1:T )] , (9)\nwhich is optimal in terms of the average cost but disregards the \u03bb-robustness constraint. Thus, there is a mismatch between the training objective of \u03c0\u0303\u2217 in (9) and the actual online inference in LADO."
        },
        {
            "heading": "5.2.2 Optimal Projection-Aware ML Policy",
            "text": "To further improve the average cost performance, we consider the following ML policy \u03c0\u0303\u25e6\u03bb that is optimally trained with explicit consideration of the downstream projection:\n\u03c0\u0303\u25e6\u03bb = argmin \u03c0 Eg1:T [cost(LADO(\u03c0), g1:T )] , (10)\nwhere the projected ML prediction by LADO is explicitly used as the action in the cost. The policy \u03c0\u0303\u25e6\u03bb can be trained offline using implicit differentiation (i.e., the added projection in Line 4 of Algorithm 1 can be implicitly differentiated based on KKT conditions) [55]. Like in other learning-augmented algorithms [31], we consider that \u03c0\u0303\u25e6\u03bb is already available for online inference by individual agents. Next, we use LADO(\u03c0\u0303\u25e6\u03bb) to emphasize the usage of \u03c0\u0303\u25e6\u03bb in LADO, and show the average its cost bound. The proof is deferred to Appendix D.2. Corollary 5.3 (Average cost of LADO(\u03c0\u0303\u25e6\u03bb)). Given the optimal projection-aware ML policy \u03c0\u0303\u25e6\u03bb, for any \u03bb > 0, by optimally setting \u03bb0 = \u221a 1 + \u03bb\u2212 1, the average cost of LADO(\u03c0\u0303\u25e6\u03bb) is upper bounded by\nAV G(LADO(\u03c0\u0303\u25e6\u03bb)) \u2264 min { (1\u2212 \u03b1\u03bb)AV G(\u03c0\u2020) + \u03b1\u03bbAV G(\u03c0\u0303\u2217),(\u221a\nAVG(\u03c0\u0303\u2217) + \u221a \u2113f + 2\u2113T + \u2113SDv\n2 \u2126(\u03bb, \u03c0\u0303\u2217, \u03c0\u2020) )2} (11) where AV G(\u03c0\u2020) and AV G(\u03c0\u0303\u2217) are the average costs of the expert policy and the optimal projection-unaware ML policy (9), respectively, the weight\u03b1\u03bb = min { ( \u221a 1 + \u03bb\u2212 1) \u221a 2 \u2113T+\u2113f+D\u00b7\u2113S \u00b7 C\u0302, 1 } with C\u0302 = ming1:T\u2208G minv\u2208V,t\u2208[1,T ] costv(xv,\u20201:t )\u2211t i=1 \u2225x v,\u2020 i \u2212x\u0303 v,\u2217 i \u22252 being the expert\u2019s minimum single-node cumulative cost normalized by the cumulative expert-ML action distance, and\n\u2126(\u03bb, \u03c0\u0303\u2217, \u03c0\u2020) = \u2211 v\u2208V \u2211T t=1 Eg1:T {[ \u2225x\u0303v,\u2217t \u2212 x v,\u2020 t \u22252 \u2212 2( \u221a 1+\u03bb\u22121)2 \u2113f+2\u00b7\u2113T+\u2113S \u00b7Dv \u00b7 cost \u2020 v,t ]+} in which cost\u2020v,t is the expert\u2019s cost for\nnode v at time t.\nCorollary 5.3 formally demonstrates the benefits of using the optimal projection-aware ML policy (10) compared to the optimal projection-unaware ML policy (9). Specifically, by the optimality of AV G(\u03c0\u0303\u2217) in (9) without \u03bbrobustness, we naturally have AV G(\u03c0\u0303\u2217) \u2264 AV G(\u03c0\u2020). Thus, the first term in (11) shows that, LADO(\u03c0\u0303\u25e6\u03bb) can achieve a smaller average cost than the expert by using the projection-aware ML policy \u03c0\u0303\u25e6\u03bb. This is because the expert policy is intuitively a feasible solution in our \u03bb-robustness ML policy space, while the policy \u03c0\u0303\u25e6\u03bb in (10) is the optimal one that specifically minimizes the average cost of LADO(\u03c0\u0303\u25e6\u03bb). By contrast, even by using the optimal projection-unaware ML policy \u03c0\u0303\u2217, the average cost of LADO(\u03c0\u0303\u2217) is bounded by (1 + \u03bb)AV G(\u03c0\u2020) in the first term of Theorem 5.2, since the added projection during actual inference can void the optimality of \u03c0\u0303\u2217 and result in a higher average cost up to (1 + \u03bb times of the expert\u2019s cost. The root reason for the advantage of the optimal projection-aware ML policy (10) in terms of the average cost is that its ML prediction is specifically customized to LADO. On the other hand, even though\n\u03c0\u0303\u2217 in (9) is the optimal-unconstrained ML policy on its own, its optimality can no longer hold when modified by LADO for \u03bb-robustness during actual online inference.\nFinally, the second term inside min in Corollary 5.3 shows that the average cost of LADO(\u03c0\u0303\u25e6\u03bb) with the optimal projection-aware ML policy \u03c0\u0303\u25e6\u03bb in (10) is upper bounded by that of LADO(\u03c0\u0303\u2217\u03bb), since LADO(\u03c0\u0303\u2217\u03bb) is a feasible policy satisfying \u03bb-robustness by our design. Like in Theorem 5.2, it reinforces the insight that LADO can better exploit the potential of ML predictions when \u03bb > 0 increases."
        },
        {
            "heading": "6 Concluding Remarks",
            "text": "This paper proposes LADO, a learning-augmented online algorithm that improves the average performance while guaranteeing worst-case robustness in a decentralized setting. LADO addresses the key challenges of temporal and spatial information inefficiency, and constructs novel robust action sets that allow agents to choose individual actions based on local online information. We prove bounds on the average performance for LADO under different ML policy training methods, demonstrating the advantage of training the ML policy with awareness of the robustness requirement.\nLimitations. We conclude our study by discussing key limitations of LADO. First, LADO relies on a few assumptions on cost functions (e.g., smoothness) that might limit the applicability of LADO, although these assumptions are common in the literature to facilitate theoretical analysis. Second, LADO focuses on a specific type of online networked optimization that has both temporal and spatial costs. LADO also opens up new interesting problems, including how to train the ML policy in an online manner given downstream projection? How to handle further delayed or even missing spatial information about neighbors\u2019 actions? And, how to incorporate predictions from multiple ML policies?"
        },
        {
            "heading": "A Application Examples",
            "text": "To make our model concrete, we present application examples from geographic server provisioning with environmental justice and decentralized battery management for sustainable computing. Readers are also referred to [37] for a discussion of other application examples such as multi-product pricing in revenue management.\nGeographic server provisioning with environmental justice. Online service providers commonly rely on geographically-distributed data centers in the proximity of end users to minimize service latency. Nonetheless, data centers are notoriously energy-intensive. Thus, given time-varying workload demands, the data center capacity (i.e., the number of active servers) needs to be dynamically adjusted to achieve energy-proportional computing and minimize the operational cost [41]. More specifically, each data center dynamically provisions its servers in a decentralized manner, based on which the incoming workloads are scheduled [56]. Naturally, turning on more servers in a data center can provide better service quality in general, but it also consumes more energy and hence negatively results in a higher environmental footprint (e.g., carbon and water, which both roughly increase with the energy consumption proportionally [43, 42]). While it is important to reduce the total environmental footprint across geo-distributed data centers, addressing environmental justice \u2014 mitigating locational disparity in terms of negative environmental consequences caused by data center operation [57] \u2014 is also crucial as injustice can create significant business risks and unintended societal impacts [43]. To address environmental justice, we view each data center as a node v in our model. The data center v makes its own dynamic server provisioning decision xvt (i.e., the number of active servers, which can be treated as a continuous variable due to tens of thousands of servers in data centers), and incurs a node cost fvt (xvt ) that captures the local energy cost, environmental footprint, and service quality [41]. The temporal cost cvt (xvt , xvt\u22121) = \u2225xvt \u2212 xvt\u22121\u22252 captures the negative impact of switching servers on and off (e.g., wear-and-tear), which is also referred to as the switching cost in the data center literature [41]. Additionally, the spatial cost s(v,u)t (xvt , xut ) can be written as s(v,u)t (xvt , xut ) = \u2225evt xvt \u2212 eut xvt \u22252 where evt is the weighted environmental \u201cprice\u201d (e.g., water usage efficiency scaled by the average per-server energy) in data center v. Thus, the spatial cost addresses environmental justice concerns by penalizing difference between data center v and data center u in terms of their environmental footprint. As a result, by considering weighted sums of the node costs, temporal costs, and spatial costs, our model applies to the problem of geographic server provisioning with environmental justice, which is emerging as a critical concern in the wake of increasingly hyperscale data centers that may leave certain local communities to disproportionately bear the negative environmental consequences.\nDecentralized battery management for sustainable computing. Traditionally, data centers rely on fossil fuels such as coal or natural gas to power their operation. Thus, with the proliferating demand for cloud computing and artificial intelligence services, there have been increasing environmental concerns with data centers\u2019 growing carbon emissions. As such, it is important to find ways to reduce data centers\u2019 carbon footprint and mitigate their environmental impact. While renewable energy sources, such as solar and wind, are natural alternatives for sustainable data centers, their availability can be highly fluctuating subject to weather conditions, thus imposing significant challenges to meet data centers\u2019 energy demands. Consequently, large energy storage consisting of multiple battery units has become essential to leverage intermittent renewable energy to power data centers for sustainable computing. Nonetheless, it is challenging to manage a large energy storage system to achieve optimal efficiency. Specifically, while each battery unit is responsible for its own charging/discharging decisions to keep the energy level within a desired range (e.g., 20-80%) in decentralized battery management, the state-of-charge (SoC) levels across different battery units should also be maintained as uniform as possible to extend the overall battery lifespan and energy efficiency [58]. This problem can be well captured by converting a canonical form into our model: each battery unit decides its SoC level by charging/discharging and incurs a node cost (i.e., SoC level deviating from the desired range) and a temporal cost (i.e., SoC changes due to charging/discharging), and meanwhile there is a spatial cost due to SoC differences across different battery units.\nMore concretely, we consider an energy storage system that includes a set of battery units V interconnected through physical connections E . For a battery unit v \u2208 V , the goal is to minimize the difference between the current SoC and a nominal value x\u0304v plus a power grid\u2019s usage cost, which can be defined as a local objective: minuv,1:T \u2211T t=1 \u2225xv,t \u2212 x\u0304v\u22252 + \u2211T t=1 b\u2225\u03bev,t\u22252, where \u03bev,t is the charging/discharging schedule from the power grid (i.e., \u03bev,t > 0 means drawing energy from the grid and \u03bev,t < 0 means returning energy to the grid) and b is the power grid\u2019s usage penalty cost. The time index for the first term starts at t = 2 as we assume a given initial state xv,1 (i.e., the SoC cost at t = 1 is already given). The canonical form of the battery SoC dynamics follows by xv,t = Avxv,t\u22121 + Bv\u03bev,t + wv,t, where Av denotes the self-degradation coefficient, Bv denotes the charging efficiency, wv,t is the data center\u2019s net energy demand from battery unit v (i.e., wv,t > 0 means the data center\u2019s\nenergy demand exceeds the available renewables and wv,t < 0 otherwise). Based on the physical connection (u, v) \u2208 E , the SoC difference between battery units u and v can lead to reduced performance and lifespan. For instance, the battery voltage difference caused by different SoCs may cause overheating problems or even battery damage [58]. Thus, to penalize the SoC difference between two internnected battery units, we add a spatial cost\u2211(v,u)\u2208E c \u00b7 \u2225xvt \u2212 xut \u22252, where c is the SoC difference penalty coefficient. Thus, the total control cost is\nmin {uv,1:T ,\u2200v\u2208V} T\u2211 t=1 \u2211 v\u2208V \u2225xv,t \u2212 x\u0304v\u22252 + T\u2211 t=1 \u2211 v\u2208V b\u2225\u03bev,t\u22252 + T\u2211 t=1 \u2211 (v,u)\u2208E c\u2225xvt \u2212 xut \u22252. (12)\nNext, we convert (12) into our formulation decentralized online convex optimization. At time t, we define yv,t = x\u0304v \u2212Atvxv,1\u2212 \u2211t i=1A t\u2212i v wv,i as the context parameter determined by all the previous states and online inputs,\nand av,t = \u2211t i=1A t\u2212i v Bv\u03bev,i as the corresponding node v\u2019s online action in our model. Then, we define the node cost for v as fvt (av,t) = \u2225av,t \u2212 yv,t\u22252 = \u2225xv,t \u2212 x\u0304v\u22252, the temporal cost for v as cvt (av,t, av,t\u22121) = bB2v \u2225av,t \u2212Avav,t\u22121\u2225 2 = b\u2225\u03bev,t\u22252, and the spatial cost for edge (v, u) as s(v,u)t = c\u2225(av,t \u2212 au,t)\u2212 (yv,t \u2212 yu,t) + x\u0304v \u2212 x\u0304u\u22252 = c\u2225xvt \u2212 xut \u22252. By combining these three costs together, the total global cost becomes\nmin {av,1:T ,\u2200v\u2208V} T\u2211 t=1 (\u2211 v\u2208V \u2225av,t \u2212 yv,t\u22252 + \u2211 v\u2208V b B2v \u2225av,t \u2212Avav,t\u22121\u22252+\n\u2211 (v,u)\u2208E c\u2225(av,t \u2212 au,t)\u2212 (yv,t \u2212 yu,t) + x\u0304v \u2212 x\u0304u\u22252 ) ,\n(13)\nwhich has the same form as our formulation (1) if we view av,t as node v\u2019s online action at time t."
        },
        {
            "heading": "B Experiments",
            "text": "To demonstrate the empirical benefits of LADO over existing baseline algorithms, we conduct experiments with the application of decentralized battery management for sustainable computing.\nB.1 Settings Following the literature on sustainable data centers [41], we use a trace-based simulation in our experiments. The data center workload trace is taken from Microsoft Azure [59], which contains the CPU utilization of 2,695,548 virtual machines (VM) for each 5-minute window. We estimate the energy consumption Pd,t by summing up the CPU utilization of all VMs. The weather-related parameters, i.e., wind speed, solar radiation and temperature data, are all collected from the National Solar Radiation Database [60]. Based on the weather information, we use empirical equations to model the wind and solar renewables generated at time step t. Specifically, the amount of solar energy generated at step t is given based on [61] by Psolar,t = 12\u03basolarAarrayIrad,t(1\u2212 0.05 \u2217 (Tempt \u2212 25)), where Aarray is the solar array area (m2), Irad,t is the solar radiation (kW/m2), and Tempt is the temperature (\u25e6C) at time t, and \u03basolar is the conversion efficiency (%) of the solar panel. The amount of wind energy is modeled based on [62] as Pwind,t = 12\u03bawind\u03f1AsweptV 3wind,t, where \u03f1 is the air density (kg/m3), Aswept is the swept area of the turbine (m2), \u03bawind is the conversion efficiency (%) of wind energy, and Vwind,t is the wind speed (kW/m2) at time t. Thus, at time t, the total energy generated by the solar and wind renewables is Pr,t = Pwind,t + Psolar,t. By subtracting the renewables Pr,t from the data center\u2019s energy demand Pd,t, we obtain the net demand as Pn,t = Pd,t \u2212 Pr,t, which is then normalized to [\u22121, 1]. Then, we use a sliding window to generate 24-hour net demand sequences as the datasets, where each sequence has 25 successive normalized net demands (from hour 0 to hour 24). In our experiment, we consider three fully-connected battery units, each being a node in our model. The initial SoC states xv0 and nominal SoC values x\u0304v are set as the same for all the three nodes. Each battery unit has its own self-degradation coefficient Av , which are set as 0.9, 0.93, 0.95, respectively, and the charging efficiency is set as Bv = 1 for v = 1, 2, 3.\nWe compare LADOwith the following baselines. \u2022 Offline optimal (OPT): OPT obtains the offline optimal solution to (13) with the complete information for each problem instance. \u2022 Expert: The state-of-the-art online algorithm for our problem is the localized prediction policy [37]. Here, we set the prediction window as 1 and refer to it as Expert.\n\u2022 ML optimizer (ML): ML uses the same recursive neural network (RNN) model used by LADO, but is trained as a standalone policy without considering LADO. \u2022 Hitting cost optimizer (HitOnly): HitOnly solely optimizes the node cost for each node, which aims at tracking the nominal SoC value exactly. \u2022 Single-step cost optimizer (Greedy): Greedy myopically minimizes the node cost and temporal cost at each time for each node.\nWe also consider LADO-OPT by considering the optimal projection-aware ML policy (Section 5.2.2). For the ML model architecture, we choose a RNN with 2 recurrent layers, each with 8 hidden features. In all our experiments, each problem instance spans 24 hours, and each time step represents one hour. For the training processes, we use the net energy demand trace from the first two months of 2015, which contains 1440 hourly data samples and produces a total of 1416 24-hour sequences. The ML model is optimized by Adam with a learning rate of 10\u22123 for 60 epochs in total. After training, the weights of ML model are shared between all the nodes with different coefficients Av . On average, the training process takes 3 minutes on a 2020 MacBook Air with 8GB memory. For testing, we use the net demand traces from April to March.\nB.2 Results We first show the empirical average (AVG) and competitive ratio (CR) values in Table 1. The CR values are the empirically worst cost ratio of an algorithm\u2019s cost to OPT\u2019s cost in our testing dataset. We see from the table that Expert achieves the best empirical CR, but its average performance is not as good as ML due to its conservativeness.\nAlthough the pureML-based optimizer achieves better average cost by leveraging historical data, its CR is significantly higher than Expert and even higher than Greedy.\nBy projecting the ML actions into carefully designed robust action sets, LADO can significantly reduce the CR compared to ML, while improving the average cost performance compared to Expert. Our analysis in Theorem 5.2 proves that, with a larger \u03bb, the average cost of LADO is closer to that of ML. This point is also reflected by our experiments. Interestingly, by setting \u03bb = 2, LADO can even achieve a lower average cost than ML, while still having a lower CR. The reason is that Expert performs much better than ML for some problem instances. Thus, the inclusion of Expert in LADO avoids those instances that would otherwise have a high cost if ML were used, and meanwhile a large \u03bb = 2 also provides enough flexibility for LADO to exploit the benefits of ML in most other cases. Moreover, by training an ML policy that is explicitly aware of the projection, LADO-OPT can further reduce the average cost compared to LADOwhile having the same robustness guarantees.\nFinally, we show the boxplots of total costs and cost ratios of different algorithms in Fig. 1. Here, the cost ratio is with respect to OPT. Note that the worst-case cost and cost ratio are different: a higher worst-case cost ratio of an algorithm ti OPT does not necessarily mean a higher worst-case cost. Our results complement Table 1 and show that LADO can achieve a lower cost than Expert in most problem instances while still having a significantly lower cost ratio than ML."
        },
        {
            "heading": "C Proof of Robustness in Theorem 5.1",
            "text": "We first prove the following lemma. Lemma C.1. If the spatial cost is non-negative, convex and \u2113S-smooth w,r,t the vector (xv, xu), then for any \u03bb > 0, it holds that\ns (v,u) t (x v t , x u t )\u2212 (1 + \u03bb)s (v,u) t (x v,\u2020 t , x u,\u2020 t ) \u2264 \u2113S 2 (1 + 1 \u03bb ) ( \u2225xvt \u2212 x v,\u2020 t \u22252 + \u2225xut \u2212 x u,\u2020 t \u22252 ) . (14)\nProof. By the definition of smoothness, we have\ns (v,u) t (x v t , x u t )\n\u2264s(v,u)t (x v,\u2020 t , x u,\u2020 t )+ < \u2207s (v,u) t (x v,\u2020 t , x u,\u2020 t ), (x v t \u2212 x v,\u2020 t , x u t \u2212 x u,\u2020 t ) > + \u2113S 2 \u2225(xvt , xut )\u2212 (x v,\u2020 t , x u,\u2020 t )\u22252\n\u2264s(v,u)t (x v,\u2020 t , x u,\u2020 t ) + \u2225\u2207s (v,u) t (x v,\u2020 t , x u,\u2020 t )\u2225 \u00b7 \u2225(xvt \u2212 x v,\u2020 t , x u t \u2212 x u,\u2020 t )\u2225+ \u2113S 2 \u2225(xvt , xut )\u2212 (x v,\u2020 t , x u,\u2020 t )\u22252\n\u2264s(v,u)t (x v,\u2020 t , x u,\u2020 t ) +\n\u03bb\n2\u2113S \u2225\u2207s(v,u)t (x v,\u2020 t , x u,\u2020 t )\u22252 + (1 +\n1 \u03bb ) \u2113S 2 \u2225(xvt , xut )\u2212 (x v,\u2020 t , x u,\u2020 t )\u22252\n(15)\nThe second inequality comes from the property of inner product. The third inequality is based on AM-QM inequality. Besides, if (x\u0302vt , x\u0302ut ) is a minimizer of the spatial cost, by Lemma 2.9 in [63], we have\ns (v,u) t (x v,\u2020 t , x u,\u2020 t ) \u2265 s (v,u) t (x\u0302 v t , x\u0302 u t ) + 0 +\n1\n2\u2113S \u2225\u2207s(v,u)t (x v,\u2020 t , x u,\u2020 t )\u22252 \u2265\n1\n2\u2113S \u2225\u2207s(v,u)t (x v,\u2020 t , x u,\u2020 t )\u22252 (16)\nBy substituting Eqn (16) back to Eqn (15), we have\ns (v,u) t (x v t , x u t ) \u2264 (1 + \u03bb) \u00b7 s (v,u) t (x v,\u2020 t , x u,\u2020 t ) + (1 +\n1 \u03bb ) \u2113S 2 (\u2225(xvt \u2212 x v,\u2020 t \u22252 + \u2225xut \u2212 x u,\u2020 t \u22252). (17)\nProof of Theorem 5.1. Proof. To prove Theorem 5.1, the key point is to guarantee the robust action set (8) is non-empty. We will prove this through induction. For t = 1, it is obvious that xv1 = xv,\u20201 satisfies the constraint. We assume that the robustness constraint is satisfied up to time step t\u2212 1, which is\nt\u22121\u2211 \u03c4=1 fv\u03c4 (x v \u03c4 ) + t\u22122\u2211 \u03c4=1 \u2211 (v,u)\u2208E \u03ba(v,u)\u03c4 \u00b7 s(v,u)\u03c4 (xv\u03c4 , xu\u03c4 ) + \u2113T + \u2113S \u00b7Dv 2 (1 + 1 \u03bb0 ) \u00b7 \u2225xvt\u22121 \u2212 x v,\u2020 t\u22121\u22252\n+ t\u22121\u2211 \u03c4=1 cv\u03c4 (x v \u03c4 , x v \u03c4\u22121) \u2264 (1 + \u03bb) (t\u22121\u2211 \u03c4=1 fv\u03c4 (x v,\u2020 \u03c4 ) + t\u22121\u2211 \u03c4=1 cv\u03c4 (x v,\u2020 \u03c4 , x v,\u2020 \u03c4\u22121) + t\u22122\u2211 \u03c4=1 \u2211 (v,u)\u2208E \u03ba(v,u)\u03c4 \u00b7 s(v,u)\u03c4 (xv,\u2020\u03c4 , xu,\u2020\u03c4 ) ) (18)\nBased on Lemma C.1 and \u03ba(v,u)t\u22121 = \u2225xvt\u22121\u2212x v,\u2020 t\u22121\u2225 2\n\u2225xvt\u22121\u2212x v,\u2020 t\u22121\u22252+\u2225xut\u22121\u2212x u,\u2020 t\u22121\u22252\n, we have\n\u03ba (v,u) t\u22121 \u00b7 ( s (v,u) t\u22121 (x v t\u22121, x u t\u22121)\u2212 (1 + \u03bb)s (v,u) t\u22121 (x v,\u2020 t\u22121, x u,\u2020 t\u22121) ) \u2264 \u2113S\n2 (1 +\n1 \u03bb )\u2225xvt\u22121 \u2212 x v,\u2020 t\u22121\u22252 (19)\nFor time step t, if we choose xvt = xv,\u2020t , by the smoothness assumption, we have\ncvt (x v,\u2020 t , x v t\u22121)\u2212 (1 + \u03bb)cvt (x v,\u2020 t , x v,\u2020 t\u22121) \u2264 \u2113T 2 (1 + 1 \u03bb )\u2225xvt\u22121 \u2212 x v,\u2020 t\u22121\u22252 (20)\nSince the node cost is non-negative, by (19) and (20), we have\nfvt (x v,\u2020 t ) + \u2211 (v,u)\u2208E \u03ba (v,u) t\u22121 \u00b7 s (v,u) t\u22121 (x v t\u22121, x u t\u22121) + c v t (x v,\u2020 t , x v t\u22121)\u2212 \u2113T + \u2113S \u00b7Dv 2 (1 + 1 \u03bb0 )\u2225xvt\u22121 \u2212 x v,\u2020 t\u22121\u22252\n\u2212 (1 + \u03bb) ( fvt (x v,\u2020 t ) + c v t (x v,\u2020 t , x v,\u2020 t\u22121) + \u2211 (v,u)\u2208E \u03ba (v,u) t\u22121 \u00b7 s (v,u) t\u22121 (x v,\u2020 t\u22121, x u,\u2020 t\u22121) ) \u2264\u2113T + \u2113S \u00b7Dv\n2 (1 +\n1 \u03bb )\u2225xvt\u22121 \u2212 x v,\u2020 t\u22121\u22252 \u2212 \u2113T + \u2113S \u00b7Dv 2 (1 + 1 \u03bb0 )\u2225xvt\u22121 \u2212 x v,\u2020 t\u22121\u22252\n\u22640,\n(21)\nwhere the last inequality holds by \u03bb \u2265 \u03bb0. By adding Eqn (21) back to Eqn (18) and moving items, we recover the robustness constraint for time step t if xvt = xv,\u2020t ,\nt\u2211 \u03c4=1 fv\u03c4 (x v \u03c4 ) + t\u22121\u2211 \u03c4=1 \u2211 (v,u)\u2208E \u03ba(v,u)\u03c4 \u00b7 s(v,u)\u03c4 (xv\u03c4 , xu\u03c4 ) + t\u2211 \u03c4=1 cv\u03c4 (x v \u03c4 , x v \u03c4\u22121) + \u2113T + \u2113S \u00b7Dv 2 (1 + 1 \u03bb0 )\u2225xvt \u2212 x v,\u2020 t \u22252\n\u2264 (1 + \u03bb) ( t\u2211 \u03c4=1 fv\u03c4 (x v,\u2020 \u03c4 ) + t\u2211 \u03c4=1 cv\u03c4 (x v,\u2020 \u03c4 , x v,\u2020 \u03c4\u22121) + t\u22121\u2211 \u03c4=1 \u2211 (v,u)\u2208E \u03ba(v,u)\u03c4 \u00b7 s(v,u)\u03c4 (xv,\u2020\u03c4 , xu,\u2020\u03c4 ) ) (22)\nIn other words, the expert\u2019s action xv,\u2020t is always an action in the corresponding robust action set (8). Thus the robust action set is non-empty.\nSince \u03ba(v,u)t + \u03ba(u,v)t = 1 holds for (v, u) \u2208 E , if all the nodes select actions from the robust action set (8) at each step, we can guarantee that cost(LADO, g1:T ) \u2264 (1 + \u03bb) \u00b7 cost(\u03c0\u2020, g1:T ) is satisfied."
        },
        {
            "heading": "D Proof of Average Cost Bounds",
            "text": "D.1 Proof of Theorem 5.2 Lemma D.1. We denote the actual actions from LADO as xv1:T = (xv1, \u00b7 \u00b7 \u00b7 , xvT ), the squared distance between actual action and ML advice is bounded by\nT\u2211 t=1 \u2225xvt \u2212 x\u0303vt \u22252 \u2264 T\u2211 t=1 [ \u2225x\u0303vt \u2212 x v,\u2020 t \u22252 \u2212 \u03bb\u2212 \u03bb0 1 + 1\u03bb0 \u00b7 2 \u2113f + 2 \u00b7 \u2113T + \u2113S \u00b7Dv \u00b7 cost\u2020v,t ]+ where \u03bb, \u03bb0, D, and \u03c1 are defined in Theorem 5.2.\nProof. To prove this lemma, we first construct a sufficient condition to satisfy the original constraint in Eqn (5). Then we prove a distance bound in this sufficient condition, where the bound still holds for the original problem.\nAt time step t, we know the constraint in time step t\u2212 1 is already satisfied, so we obtain the following sufficient condition of the satisfaction of (5) as\nfvt (x v t ) + \u2211 (v,u)\u2208E \u03ba (v,u) t\u22121 \u00b7 s (v,u) t\u22121 (x v t\u22121, x u t\u22121) + \u2113T + \u2113S \u00b7Dv 2 (1 + 1 \u03bb0 ) ( \u2225xvt \u2212 x v,\u2020 t \u22252 \u2212 \u2225xvt\u22121 \u2212 x v,\u2020 t\u22121\u22252 ) +cvt (x v t , x v t\u22121) \u2264 (1 + \u03bb) ( fvt (x v,\u2020 t ) + c v t (x v,\u2020 t , x v,\u2020 t\u22121) +\n\u2211 (v,u)\u2208E \u03ba (v,u) t\u22121 \u00b7 s (v,u) t\u22121 (x v,\u2020 t\u22121, x u,\u2020 t\u22121) ) (23)\nWith the convexity and smoothness assumptions, we have\ncvt (x v,\u2020 t , x v t\u22121)\u2212 (1 + \u03bb0)cvt (x v,\u2020 t , x v,\u2020 t\u22121) \u2264 \u2113T 2 (1 + 1 \u03bb0 ) ( \u2225xvt \u2212 x v,\u2020 t \u22252 + \u2225xvt\u22121 \u2212 x v,\u2020 t\u22121\u22252 ) fvt (x v t )\u2212 (1 + \u03bb0)fvt (x v,\u2020 t ) \u2264\n\u2113f 2 (1 + 1 \u03bb0 )\u2225xvt \u2212 x v,\u2020 t \u22252\n(24)\nThen a sufficient condition that (23) holds becomes\u2211 (v,u)\u2208E \u03ba (v,u) t\u22121 \u00b7 s (v,u) t\u22121 (x v t\u22121, x u t\u22121) + (1 + 1 \u03bb0 ) ( \u2113f + 2 \u00b7 \u2113T + \u2113S \u00b7Dv 2 \u2225xvt \u2212 x v,\u2020 t \u22252 \u2212 \u2113S \u00b7Dv 2 \u2225xvt\u22121 \u2212 x v,\u2020 t\u22121\u22252 ) \u2264 (\u03bb\u2212 \u03bb0) ( fvt (x v,\u2020 t ) + c v t (x v,\u2020 t , x v,\u2020 t\u22121) ) + (1 + \u03bb)\n\u2211 (v,u)\u2208E \u03ba (v,u) t\u22121 \u00b7 s (v,u) t\u22121 (x v,\u2020 t\u22121, x u,\u2020 t\u22121)\n(25)\nFrom Eqn (19), we can further cancel out the spatial costs and get the sufficient condition as\n(1 + 1\n\u03bb0 )\n( \u2113f + 2 \u00b7 \u2113T + \u2113S \u00b7Dv\n2 \u2225xvt \u2212 x v,\u2020 t \u22252 ) \u2264 (\u03bb\u2212 \u03bb0) ( fvt (x v,\u2020 t ) + c v t (x v,\u2020 t , x v,\u2020 t\u22121) +\n\u2211 (v,u)\u2208E \u03ba (v,u) t\u22121 \u00b7 s (v,u) t\u22121 (x v,\u2020 t\u22121, x u,\u2020 t\u22121) ) (26) Therefore, for time step t, constraint (5) is satisfied if the action xvt satisfies\n\u2225xvt \u2212 x v,\u2020 t \u22252 \u2264 \u03bb\u2212 \u03bb0 1 + 1\u03bb0 \u00b7 2 \u2113f + 2 \u00b7 \u2113T + \u2113S \u00b7Dv \u00b7 cost\u2020v,t (27)\nwhere cost\u2020v,t denotes the cost of node v at time t. Since actions that satisfy (27) must satisfy the original constraint (5) and xvt is obtained by the projection from x\u0303vt into the original constraint (5), we have\n\u2225xvt \u2212 x\u0303vt \u2225 \u2264 [ \u2225x\u0303vt \u2212 x v,\u2020 t \u2225 \u2212 \u221a \u03bb\u2212 \u03bb0 1 + 1\u03bb0 \u00b7 2 \u2113f + 2 \u00b7 \u2113T + \u2113S \u00b7Dv \u00b7 cost\u2020v,t ]+\n(28)\nBesides, it\u2019s obvious that [\u2225x\u0303vt \u2212 xv,\u2020t \u2225 \u2212 \u221a \u03bb\u2212 \u03bb0 1 + 1\u03bb0 \u00b7 2 \u2113f + 2 \u00b7 \u2113T + \u2113S \u00b7Dv \u00b7 cost\u2020v,t ]+2\n\u2264 [ \u2225x\u0303vt \u2212 x v,\u2020 t \u2225 2 \u2212 \u03bb\u2212 \u03bb0\n1 + 1\u03bb0 \u00b7 2 \u2113f + 2 \u00b7 \u2113T + \u2113S \u00b7Dv\n\u00b7 cost\u2020v,t ]+ (29)\nBy summing up the inequalities over time, we complete the proof. Proof of Theorem 5.2\nProof. Based on the smoothness constraint, for any \u03bb2 > 0 we have( T\u2211\n\u03c4=1\nfv\u03c4 (x v \u03c4 ) + T\u2211 \u03c4=1 cv\u03c4 (x v \u03c4 , x v \u03c4\u22121)\n) \u2212 (1 + \u03bb2) ( T\u2211\n\u03c4=1\nfv\u03c4 (x\u0303 v \u03c4 ) + T\u2211 \u03c4=1 cv\u03c4 (x\u0303 v \u03c4 , x\u0303 v \u03c4\u22121)\n)\n\u2264(1 + 1 \u03bb2 ) ( \u2113f 2 T\u2211 \u03c4=1 \u2225xv\u03c4 \u2212 x\u0303v\u03c4\u22252 + \u2113T 2 T\u2211 \u03c4=1 \u2225xv\u03c4 \u2212 x\u0303v\u03c4\u22252 + \u2113T 2 T\u22121\u2211 \u03c4=0 \u2225xv\u03c4 \u2212 x\u0303v\u03c4\u22252 )\n\u2264(1 + 1 \u03bb2 ) \u2113f + 2 \u00b7 \u2113T 2 T\u2211 \u03c4=1 \u2225xv\u03c4 \u2212 x\u0303v\u03c4\u22252\n(30)\nSimilarly, according to Eqn (19), by summing up the inequalities over time, we have\u2211 (v,u)\u2208E T\u2211 \u03c4=1 \u03ba(v,u)\u03c4 \u00b7 s(v,u)\u03c4 (xv\u03c4 , xu\u03c4 )\u2212 (1 + \u03bb2) \u2211 (v,u)\u2208E T\u2211 \u03c4=1 \u03ba(v,u)\u03c4 \u00b7 s(v,u)\u03c4 (x\u0303v\u03c4 , x\u0303u\u03c4 )\n\u2264(1 + 1 \u03bb2 ) \u2113S 2 \u2211 (v,u)\u2208E T\u2211 \u03c4=1 \u2225xv\u03c4 \u2212 x\u0303v\u03c4\u22252 = (1 + 1 \u03bb2 ) Dv \u00b7 \u2113S 2 T\u2211 \u03c4=1 \u2225xv\u03c4 \u2212 x\u0303v\u03c4\u22252 (31)\nBy adding Eqn (30) and Eqn (31), we can bound the cost error of node v as\ncostv(xv1:T )\u2212 (1 + \u03bb2)costv(x\u0303v1:T ) \u2264 (1 + 1 \u03bb2 ) \u2113f + 2 \u00b7 \u2113T +Dv \u00b7 \u2113S 2 T\u2211 \u03c4=1 \u2225xv\u03c4 \u2212 x\u0303v\u03c4\u22252 (32)\nBy substituting Lemma D.1 into Eqn (32), we have\ncostv(xv1:T )\u2212 (1 + \u03bb2)costv(x\u0303v1:T ) \u2264 (1 + 1 \u03bb2 ) \u2113f + 2 \u00b7 \u2113T + \u2113S \u00b7Dv 2 \u03c9v(\u03bb) (33)\nwe denote \u03c9v(\u03bb) = \u2211T\nt=1 [ \u2225x\u0303vt \u2212 x v,\u2020 t \u22252 \u2212 2\u2113f+2\u00b7\u2113T+\u2113S \u00b7Dv \u00b7\n\u03bb\u2212\u03bb0 1+ 1\u03bb0\n\u00b7 cost\u2020v,t ]+\nas the auxiliary cost brought by the robustification process. By summing up the cost all nodes v \u2208 V and taking the expectation over g1:T \u2208 G over the data distribution, the average cost of our algorithm is bounded by\nAV G(LADO(\u03c0\u0303))\u2212 (1 + \u03bb2)AV G(\u03c0\u0303) \u2264 (1 + 1 \u03bb2 ) \u2113f + 2 \u00b7 \u2113T + \u2113S \u00b7Dv 2 \u00b7 \u2126(\u03bb, \u03c0\u0303, \u03c0\u2020) (34)\nwhere \u2126(\u03bb, \u03c0\u0303, \u03c0\u2020) = Eg1:T [ \u2211 v\u2208V \u03c9v(\u03bb)]. By optimally setting \u03bb2 = \u221a \u2113f+2\u2113T+\u2113SDv 2 \u00b7 \u2126(\u03bb,\u03c0\u0303,\u03c0\u2020) AVG(\u03c0\u0303) , we finish the proof for the second term in the min operator\nAV G(LADO(\u03c0\u0303)) \u2264 (\u221a AV G(\u03c0\u0303) + \u221a \u2113f + 2\u2113T + \u2113SDv\n2 \u00b7 \u2126(\u03bb, \u03c0\u0303, \u03c0\u2020)\n)2 (35)\nThe first term in the min operator can be directly obtained by Theorem 5.1.\nD.2 Proof of Corollary 5.3 Proof. In Corollary 5.3, we assume that \u03c0\u25e6\u03bb in Eqn. (10) is used in LADO. To bound the average cost of LADO(\u03c0\u0303\u25e6\u03bb), we construct a policy that satisfies the constraint (5) for each step in each sequence. Then the average cost bound of the constructed policy is also the average cost upper bound of LADO(\u03c0\u0303\u25e6\u03bb) since LADO(\u03c0\u0303\u25e6\u03bb) is the policy that minimizes average cost while satisfying the constraint (5) for each step in each sequence if we assume that the ML model can represent any policy. The feasible policy is constructed as \u03c0\u0302 = (1 \u2212 \u03b1)\u03c0\u2020 + \u03b1\u03c0\u0303\u2217 which gives action x\u0302vt = (1 \u2212 \u03b1)xv,\u2020t + \u03b1x\u0303vt , \u03b1 \u2208 [0, 1], where x\u2020t , x\u0303t denotes expert action and the prediction from projection-unaware ML model \u03c0\u0303\u2217, respectively. We need to find the \u03b1 that guarantees the satisfaction of the constraint (3). To do that, we rewrite the constraint as\nt\u2211 \u03c4=1 ( fv\u03c4 (x\u0302 v \u03c4 )\u2212 (1 + \u03bb0)fv\u03c4 (x\u2020\u03c4 ) ) + t\u2211 \u03c4=1 ( cv(x\u0302v\u03c4 , x\u0302 v \u03c4\u22121)\u2212 (1 + \u03bb0)cv(xv,\u2020\u03c4 , x v,\u2020 \u03c4\u22121) ) + t\u22121\u2211 \u03c4=1 \u2211 (v,u)\u2208E \u03ba(v,u)\u03c4 \u00b7\n( s(v,u)\u03c4 (x\u0302 v \u03c4 , x\u0302 u \u03c4 )\u2212 (1 + \u03bb0)s(v,u)\u03c4 (xv,\u2020\u03c4 , xu,\u2020\u03c4 ) ) + (1 + 1\n\u03bb0 ) \u2113T +Dv \u00b7 \u2113S 2 \u2225xvt \u2212 x v,\u2020 t \u22252\n\u2264(\u03bb\u2212 \u03bb0) ( t\u2211 \u03c4=1 fv\u03c4 (x v,\u2020 \u03c4 ) + t\u2211 \u03c4=1 cv(xv,\u2020\u03c4 , x v,\u2020 \u03c4\u22121) + t\u22121\u2211 \u03c4=1 \u2211 (v,u)\u2208E \u03ba(v,u)\u03c4 s (v,u) \u03c4 (x v,\u2020 \u03c4 , x u,\u2020 \u03c4 ) ) ,\u2200t \u2208 [1, T ]\n(36)\nBased on the smoothness assumption, we have\nfvt (x\u0302 v t )\u2212 (1 + \u03bb0)fvt (x v,\u2020 t ) \u2264(1 +\n1 \u03bb0 ) \u2113f 2 \u2225x\u0302vt \u2212 x v,\u2020 t \u22252\ncv(x\u0302vt , x\u0302 v t\u22121)\u2212 (1 + \u03bb0)cv(x v,\u2020 t , x v,\u2020 t\u22121) \u2264(1 +\n1 \u03bb0 ) \u2113T 2 (\u2225x\u0302vt \u2212 x v,\u2020 t \u22252 + \u2225x\u0302vt\u22121 \u2212 x v,\u2020 t\u22121\u22252)\n\u03ba (v,u) t ( s (v,u) t (x\u0302 v t , x\u0302 u t )\u2212 (1 + \u03bb0)s (v,u) t (x v,\u2020 t , x u,\u2020 t ) ) \u2264(1 + 1\n\u03bb0 ) \u2113S 2 (\u2225x\u0302vt \u2212 x v,\u2020 t \u22252)\n(37)\nThen, a sufficient condition of Eqn (36) is\n(1 + 1 \u03bb0 ) \u2113f + 2\u2113T +Dv\u2113S 2 t\u2211 \u03c4=1 \u2225x\u0302v\u03c4 \u2212 xv,\u2020\u03c4 \u22252 \u2264 (\u03bb\u2212 \u03bb0)costv(xv,\u20201:t ),\u2200t \u2208 [1, T ] (38)\nSince D = maxv\u2208V Dv is the maximum node degree in the whole graph, then\n\u03b12 t\u2211\n\u03c4=1\n\u2225x\u0303v\u03c4 \u2212 xv,\u2020\u03c4 \u22252 \u2264 2 \u2113f + 2\u2113T +D \u00b7 \u2113S \u00b7 \u03bb\u2212 \u03bb0 1 + 1\u03bb0 costv(xv,\u20201:t ),\u2200t \u2208 [1, T ] (39)\nWe define C\u0302 = minv\u2208V,t\u2208[1,T ] costv(x v,\u2020 1:t )\u2211t\ni=1 \u2225x v,\u2020 i \u2212x\u0303vi \u22252\nas the minimum normalized baseline cost, then we can have\n\u03b1 \u2264 min { 1, \u221a 2\n\u2113f + 2\u2113T +D \u00b7 \u2113S \u00b7 \u03bb\u2212 \u03bb0 1 + 1\u03bb0 \u00b7 C\u0302\n} = \u03b1\u03bb (40)\nIn other words, as long as \u03b1 \u2208 [0, \u03b1\u03bb], the robustness constraint is always satisfied. Based on the convex assumption on hitting cost, temporal cost and spatial cost, we have\ncostv(x\u0302v1:t) = costv((1\u2212 \u03b1)xv,\u20201:t + \u03b1x\u0303v,\u20201:t ) \u2264 (1\u2212 \u03b1)costv(xv,\u20201:t ) + \u03b1 \u00b7 costv(x\u0303v1:t). (41)\nBy setting \u03b1 = \u03b1\u03bb and taking expectation of both side over the data distribution, we finish the proof of the first term in Theorem 5.3."
        }
    ],
    "title": "Learning-Augmented Decentralized Online Convex Optimization in Networks",
    "year": 2023
}