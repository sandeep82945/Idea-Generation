{
    "abstractText": "The challenges in recovering underwater images are the presence of diverse degradation factors and the lack of ground truth images. Although synthetic underwater image pairs can be used to overcome the problem of inadequately observing data, it may result in over-fitting and enhancement degradation. This paper proposes a model-based deep learning method for restoring clean images under various underwater scenarios, which exhibits good interpretability and generalization ability. More specifically, we build up a multi-variable convolutional neural network model to estimate the clean image, background light and transmission map, respectively. An efficient loss function is also designed to closely integrate the variables based on the underwater image model. The meta-learning strategy is used to obtain a pre-trained model on the synthetic underwater dataset, which contains different types of degradation to cover the various underwater environments. The pre-trained model is then fine-tuned on real underwater datasets to obtain a reliable underwater image enhancement model, called MetaUE. Numerical experiments demonstrate that the pre-trained model has good generalization ability, allowing it to remove the color degradation for various underwater attenuation images such as blue, green and yellow, etc. The fine-tuning makes the model able to adapt to different underwater datasets, the enhancement results of which outperform the state-of-the-art underwater image restoration methods. All our codes and data are available at https://github.com/Duanlab123/MetaUE.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zhenwei Zhang"
        },
        {
            "affiliations": [],
            "name": "Haorui Yan"
        },
        {
            "affiliations": [],
            "name": "Ke Tang"
        },
        {
            "affiliations": [],
            "name": "Yuping Duan"
        }
    ],
    "id": "SP:707a48b09bf1215cb944d1ae8ed435d37e55f543",
    "references": [
        {
            "authors": [
                "M. Jian",
                "X. Liu",
                "H. Luo",
                "X. Lu",
                "H. Yu",
                "J. Dong"
            ],
            "title": "Underwater image processing and analysis: A review",
            "venue": "Signal Processing: Image Communication, vol. 91, pp. 116\u2013128, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "D. Akkaynak",
                "T. Treibitz"
            ],
            "title": "Sea-Thru: A method for removing water from underwater images",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 1682\u20131691.",
            "year": 2019
        },
        {
            "authors": [
                "J. Liu",
                "W. Liu",
                "J. Sun",
                "T. Zeng"
            ],
            "title": "Rank-one prior: Toward realtime scene recovery",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 14 802\u201314 810.",
            "year": 2021
        },
        {
            "authors": [
                "J. Liu",
                "R.W. Liu",
                "J. Sun",
                "T. Zeng"
            ],
            "title": "Rank-one prior: Real-time scene recovery",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "X. Ding",
                "Z. Liang",
                "Y. Wang",
                "X. Fu"
            ],
            "title": "Depth-aware total variation regularization for underwater image dehazing",
            "venue": "Signal Processing: Image Communication, vol. 98, pp. 116\u2013128, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "G. Hou",
                "J. Li",
                "G. Wang",
                "Z. Pan",
                "X. Zhao"
            ],
            "title": "Underwater image dehazing and denoising via curvature variation regularization",
            "venue": "Multimedia Tools and Applications, vol. 79, no. 27, pp. 20 199\u201320 219, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Q. Jiao",
                "M. Liu",
                "P. Li",
                "L. Dong",
                "M. Hui",
                "L. Kong",
                "Y. Zhao"
            ],
            "title": "Underwater image restoration via non-convex non-smooth variation and thermal exchange optimization",
            "venue": "Journal of Marine Science and Engineering, vol. 9, no. 6, pp. 570\u2013581, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J. Li",
                "K.A. Skinner",
                "R.M. Eustice",
                "M. Johnson-Roberson"
            ],
            "title": "Water Gan: Unsupervised generative network to enable real-time color correction of monocular underwater images",
            "venue": "IEEE Robotics and Automation Letters, vol. 3, no. 1, pp. 387\u2013394, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "C. Li",
                "J. Guo",
                "C. Guo"
            ],
            "title": "Emerging from water: Underwater image color correction based on weakly supervised color transfer",
            "venue": "IEEE Signal Processing Letters, vol. 25, no. 3, pp. 323\u2013327, 2018. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 12",
            "year": 2018
        },
        {
            "authors": [
                "X. Ye",
                "Z. Li",
                "B. Sun",
                "Z. Wang",
                "R. Xu",
                "H. Li",
                "X. Fan"
            ],
            "title": "Deep joint depth estimation and color correction from monocular underwater images based on unsupervised adaptation networks",
            "venue": "IEEE Transactions on Circuits and Systems for Video Technology, vol. 30, no. 11, pp. 3995\u2013 4008, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "M. Yang",
                "K. Hu",
                "Y. Du",
                "Z. Wei",
                "Z. Sheng",
                "J. Hu"
            ],
            "title": "Underwater image enhancement based on conditional generative adversarial network",
            "venue": "Signal Processing: Image Communication, vol. 81, pp. 115\u2013123, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "R. Liu",
                "Z. Jiang",
                "S. Yang",
                "X. Fan"
            ],
            "title": "Twin adversarial contrastive learning for underwater image enhancement and beyond",
            "venue": "IEEE Transactions on Image Processing, vol. 31, pp. 4922\u20134936, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "C. Fabbri",
                "M.J. Islam",
                "J. Sattar"
            ],
            "title": "Enhancing underwater imagery using generative adversarial networks",
            "venue": "2018 IEEE International Conference on Robotics and Automation (ICRA), vol. 5, no. 3, 2018, pp. 7159\u20137165.",
            "year": 2018
        },
        {
            "authors": [
                "M.J. Islam",
                "Y. Xia",
                "J. Sattar"
            ],
            "title": "Fast underwater image enhancement for improved visual perception",
            "venue": "IEEE Robotics and Automation Letters, vol. 5, no. 2, pp. 3227\u20133234, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "C. Li",
                "S. Anwar",
                "F. Porikli"
            ],
            "title": "Underwater scene prior inspired deep underwater image and video enhancement",
            "venue": "Pattern Recognition, vol. 98, pp. 107\u2013118, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "C. Li",
                "C. Guo",
                "W. Ren",
                "R. Cong",
                "J. Hou",
                "S. Kwong",
                "D. Tao"
            ],
            "title": "An underwater image enhancement benchmark dataset and beyond",
            "venue": "IEEE Transactions on Image Processing, vol. 29, no. 2, pp. 4376\u20134389, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "X. Xue",
                "Z. Hao",
                "L. Ma",
                "Y. Wang",
                "R. Liu"
            ],
            "title": "Joint luminance and chrominance learning for underwater image enhancement",
            "venue": "IEEE Signal Processing Letters, vol. 28, no. 1, pp. 818\u2013822, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "C. Li",
                "S. Anwar",
                "J. Hou",
                "R. Cong",
                "C. Guo",
                "W. Ren"
            ],
            "title": "Underwater image enhancement via medium transmission-guided multi-color space embedding",
            "venue": "IEEE Transactions on Image Processing, vol. 30, pp. 4985\u2013 5000, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "K. Zhang",
                "W. Zuo",
                "L. Zhang"
            ],
            "title": "Deep plug-and-play super-resolution for arbitrary blur kernels",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 1671\u20131681.",
            "year": 2019
        },
        {
            "authors": [
                "K. Zhang",
                "Y. Li",
                "W. Zuo",
                "L. Zhang",
                "L. Van Gool",
                "R. Timofte"
            ],
            "title": "Plugand-play image restoration with deep denoiser prior",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 10, pp. 6360\u2013 6376, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Fang",
                "H. Zhang",
                "H.S. Wong",
                "T. Zeng"
            ],
            "title": "A robust non-blind deblurring method using deep denoiser prior",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 735\u2013744.",
            "year": 2022
        },
        {
            "authors": [
                "J. Ho",
                "A. Jain",
                "P. Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "Advances in Neural Information Processing Systems, vol. 33, pp. 6840\u2013 6851, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "P. Dhariwal",
                "A. Nichol"
            ],
            "title": "Diffusion models beat GANs on image synthesis",
            "venue": "Advances in Neural Information Processing Systems, vol. 34, pp. 8780\u20138794, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "R. Rombach",
                "A. Blattmann",
                "D. Lorenz",
                "P. Esser",
                "B. Ommer"
            ],
            "title": "Highresolution image synthesis with latent diffusion models",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 10 684\u201310 695.",
            "year": 2022
        },
        {
            "authors": [
                "J. Wyatt",
                "A. Leach",
                "S.M. Schmon",
                "C.G. Willcocks"
            ],
            "title": "Anoddpm: Anomaly detection with denoising diffusion probabilistic models using simplex noise",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 650\u2013656.",
            "year": 2022
        },
        {
            "authors": [
                "H. Chung",
                "E.S. Lee",
                "J.C. Ye"
            ],
            "title": "Mr image denoising and superresolution using regularized reverse diffusion",
            "venue": "IEEE Transactions on Medical Imaging, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "E. Hoogeboom",
                "D. Nielsen",
                "P. Jaini",
                "P. Forr\u00e9",
                "M. Welling"
            ],
            "title": "Argmax flows and multinomial diffusion: Learning categorical distributions",
            "venue": "Advances in Neural Information Processing Systems, vol. 34, pp. 12 454\u2013 12 465, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A. Santoro",
                "S. Bartunov",
                "M. Botvinick",
                "D. Wierstra",
                "T. Lillicrap"
            ],
            "title": "Meta-learning with memory-augmented neural networks",
            "venue": "International Conference on Machine Learning. PMLR, 2016, pp. 1842\u20131850.",
            "year": 2016
        },
        {
            "authors": [
                "Y.-X. Wang",
                "D. Ramanan",
                "M. Hebert"
            ],
            "title": "Meta-learning to detect rare objects",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 9925\u20139934.",
            "year": 2019
        },
        {
            "authors": [
                "T. Hospedales",
                "A. Antoniou",
                "P. Micaelli",
                "A. Storkey"
            ],
            "title": "Meta-learning in neural networks: A survey",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 9, pp. 5149\u20135169, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "H. Zhu",
                "L. Li",
                "J. Wu",
                "W. Dong",
                "G. Shi"
            ],
            "title": "MetaIQA: Deep metalearning for no-reference image quality assessment",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 14 143\u201314 152.",
            "year": 2020
        },
        {
            "authors": [
                "H. Zhao",
                "F. Ji",
                "Q. Li",
                "Q. Guan",
                "S. Wang",
                "M. Wen"
            ],
            "title": "Federated meta- Learning enhanced acoustic radio cooperative framework for ocean of things",
            "venue": "IEEE Journal of Selected Topics in Signal Processing, vol. 16, no. 3, pp. 474\u2013486, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "W. Wang",
                "L. Duan",
                "Y. Wang",
                "Q. En",
                "J. Fan",
                "Z. Zhang"
            ],
            "title": "Remember the difference: Cross-domain few-shot semantic segmentation via metamemory transfer",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 7065\u20137074.",
            "year": 2022
        },
        {
            "authors": [
                "L. Hong",
                "X. Wang",
                "Z. Xiao",
                "G. Zhang",
                "J. Liu"
            ],
            "title": "Wsuie: Weakly supervised underwater image enhancement for improved visual perception",
            "venue": "IEEE Robotics and Automation Letters, vol. 6, no. 4, pp. 8237\u20138244, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "I.H. Laradji",
                "A. Saleh",
                "P. Rodriguez",
                "D. Nowrouzezahrai",
                "M.R. Azghadi",
                "D. Vazquez"
            ],
            "title": "Weakly supervised underwater fish segmentation using affinity lcfcn",
            "venue": "Scientific reports, vol. 11, no. 1, pp. 173\u2013179, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A. Saleh",
                "M. Sheaves",
                "D. Jerry",
                "M.R. Azghadi"
            ],
            "title": "Transformer-based self-supervised fish segmentation in underwater videos",
            "venue": "arXiv preprint arXiv:2206.05390, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "S. Cai",
                "G. Li",
                "Y. Shan"
            ],
            "title": "Underwater object detection using collaborative weakly supervision",
            "venue": "Computers and Electrical Engineering, vol. 102, p. 108159, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "P.M. Uplavikar",
                "Z. Wu",
                "Z. Wang"
            ],
            "title": "All-in-One underwater image enhancement using domain-adversarial learning",
            "venue": "CVPR Workshops, 2019, pp. 1\u20138.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Liu",
                "H. Xu",
                "B. Zhang",
                "K. Sun",
                "J. Yang",
                "B. Li",
                "C. Li",
                "X. Quan"
            ],
            "title": "Model-based underwater image simulation and learning-based underwater image enhancement method",
            "venue": "Information, vol. 13, no. 4, pp. 2176\u20132289, 2022. [Online]. Available: https: //www.mdpi.com/2078-2489/13/4/187",
            "year": 2022
        },
        {
            "authors": [
                "Z. Wang",
                "L. Shen",
                "M. Xu",
                "M. Yu",
                "K. Wang",
                "Y. Lin"
            ],
            "title": "Domain adaptation for underwater image enhancement",
            "venue": "IEEE Transactions on Image Processing, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "Y. Wang",
                "J. Guo",
                "H. Gao",
                "H. Yue"
            ],
            "title": "UIEC2-Net: CNN-based underwater image enhancement using two color space",
            "venue": "Signal Processing: Image Communication, vol. 96, pp. 116\u2013130, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S. Liu",
                "H. Fan",
                "S. Lin",
                "Q. Wang",
                "N. Ding",
                "Y. Tang"
            ],
            "title": "Adaptive learning attention network for underwater image enhancement",
            "venue": "IEEE Robotics and Automation Letters, vol. 7, no. 2, pp. 5326\u20135333, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "W.-H. Zhou",
                "D.-M. Zhu",
                "M. Shi",
                "Z.-X. Li",
                "M. Duan",
                "Z.-Q. Wang",
                "G.-L. Zhao",
                "C.-D. Zheng"
            ],
            "title": "Deep images enhancement for turbid underwater images based on unsupervised learning",
            "venue": "Computers and Electronics in Agriculture, vol. 202, pp. 107\u2013122, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "S. Chai",
                "Z. Fu",
                "Y. Huang",
                "X. Tu",
                "X. Ding"
            ],
            "title": "Unsupervised and untrained underwater image restoration based on physical image formation model",
            "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 2774\u20132778.",
            "year": 2022
        },
        {
            "authors": [
                "Z. Guo",
                "D. Guo",
                "Z. Gu",
                "H. Zheng",
                "B. Zheng",
                "G. Wang"
            ],
            "title": "Unsupervised underwater image clearness via transformer",
            "venue": "OCEANS 2022- Chennai. IEEE, 2022, pp. 1\u20134.",
            "year": 2022
        },
        {
            "authors": [
                "Q. Jiang",
                "Y. Zhang",
                "F. Bao",
                "X. Zhao",
                "C. Zhang",
                "P. Liu"
            ],
            "title": "Twostep domain adaptation for underwater image enhancement",
            "venue": "Pattern Recognition, vol. 122, pp. 108\u2013124, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "A.R. Espinosa",
                "D. McIntosh",
                "A.B. Albu"
            ],
            "title": "An efficient approach for underwater image improvement: Deblurring, dehazing, and color correction",
            "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2023, pp. 206\u2013215.",
            "year": 2023
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 770\u2013778.",
            "year": 2016
        },
        {
            "authors": [
                "N. Silberman",
                "R. Fergus"
            ],
            "title": "Indoor scene segmentation using a structured light sensor",
            "venue": "2011 IEEE International Conference on Computer Vision Workshops (ICCV workshops). IEEE, 2011, pp. 601\u2013 608.",
            "year": 2011
        },
        {
            "authors": [
                "X. Zhao",
                "T. Jin",
                "S. Qu"
            ],
            "title": "Deriving inherent optical properties from background color and underwater image enhancement",
            "venue": "Ocean Engineering, vol. 94, pp. 163\u2013172, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "C.D. Mobley"
            ],
            "title": "Light and water: radiative transfer in natural waters",
            "year": 1994
        },
        {
            "authors": [
                "A. Paszke",
                "S. Gross",
                "S. Chintala",
                "G. Chanan",
                "E. Yang",
                "Z. DeVito",
                "Z. Lin",
                "A. Desmaison",
                "L. Antiga",
                "A. Lerer"
            ],
            "title": "Automatic differentiation in pytorch",
            "venue": "2017.",
            "year": 2017
        },
        {
            "authors": [
                "W. Zhang",
                "Y. Wang",
                "C. Li"
            ],
            "title": "Underwater image enhancement by attenuated color channel correction and detail preserved contrast enhancement",
            "venue": "IEEE Journal of Oceanic Engineering, 2022. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 13",
            "year": 2022
        },
        {
            "authors": [
                "W. Zhang",
                "P. Zhuang",
                "H.-H. Sun",
                "G. Li",
                "S. Kwong",
                "C. Li"
            ],
            "title": "Underwater image enhancement via minimal color loss and locally adaptive contrast enhancement",
            "venue": "IEEE Transactions on Image Processing, vol. 31, pp. 3997\u20134010, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "H. Li",
                "J. Li",
                "W. Wang"
            ],
            "title": "A fusion adversarial underwater image enhancement network with a public test dataset",
            "venue": "arXiv preprint arXiv:1906.06819, 2019.",
            "year": 1906
        },
        {
            "authors": [
                "A. Duarte",
                "F. Codevilla",
                "J.D.O. Gaya",
                "S.S. Botelho"
            ],
            "title": "A dataset to evaluate underwater image restoration methods",
            "venue": "OCEANS 2016- Shanghai. IEEE, 2016, pp. 1\u20136.",
            "year": 2016
        },
        {
            "authors": [
                "M. Yang",
                "A. Sowmya"
            ],
            "title": "An underwater color image quality evaluation metric",
            "venue": "IEEE Transactions on Image Processing, vol. 24, no. 12, pp. 6062\u20136071, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "K. Panetta",
                "C. Gao",
                "S. Agaian"
            ],
            "title": "Human-visual-system-inspired underwater image quality measures",
            "venue": "IEEE Journal of Oceanic Engineering, vol. 41, no. 3, pp. 541\u2013551, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "A. Galdran",
                "D. Pardo",
                "A. Pic\u00f3n",
                "A. Alvarez-Gila"
            ],
            "title": "Automatic redchannel underwater image restoration",
            "venue": "Journal of Visual Communication and Image Representation, vol. 26, pp. 132\u2013145, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "L. Ma",
                "T. Ma",
                "R. Liu",
                "X. Fan",
                "Z. Luo"
            ],
            "title": "Toward fast, flexible, and robust low-light image enhancement",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 5637\u20135646.",
            "year": 2022
        },
        {
            "authors": [
                "M. Wang",
                "Y. Huang",
                "J. Xiong",
                "W. Xie"
            ],
            "title": "Low-light images in-thewild: A novel visibility perception-guided blind quality indicator",
            "venue": "IEEE Transactions on Industrial Informatics, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Yang",
                "C. Wang",
                "R. Liu",
                "L. Zhang",
                "X. Guo",
                "D. Tao"
            ],
            "title": "Selfaugmented unpaired image dehazing via density and depth decomposition",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 2037\u20132046.",
            "year": 2022
        },
        {
            "authors": [
                "R.W. Liu",
                "Y. Guo",
                "Y. Lu",
                "K.T. Chui",
                "B.B. Gupta"
            ],
            "title": "Deep networkenabled haze visibility enhancement for visual iot-driven intelligent transportation systems",
            "venue": "IEEE Transactions on Industrial Informatics, vol. 19, no. 2, pp. 1581\u20131591, 2022.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014Meta-learning, model-based learning, image enhancement, underwater image\nI. INTRODUCTION\nUNDERWATER imaging has played an important role indeep-sea exploration, underwater robot technology and marine ecological monitoring. However, underwater optical images are vulnerable to underwater turbulence, diffusion, severe absorption, scattering of water bodies, various noises, low contrast, uniform illumination, monotonous color, and complex underwater scenes [1]. Four typical underwater images are displayed in Fig. 1, which makes the high-level image processing tasks based on underwater images become very challenging.\nThe work was partially supported by the National Natural Science Foundation of China (NSFC 12071345, 11701418). Asterisk indicates the corresponding author.\nZ. Zhang and H. Yan are with Center for Applied Mathematics, Tianjin University, Tianjin 300072, China.\nK. Tang is with the Guangdong Key Laboratory of Brain-Inspired Intelligent Computation, Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen 518055, China, and also with the Research Institute of Trustworthy Autonomous Systems, Southern University of Science and Technology, Shenzhen 518055, China. E-mail: tangk3@sustech.edu.cn.\nY. Duan is with Center for Applied Mathematics, Tianjin University, Tianjin 300072, China. E-mail: doveduan@gmail.com.\n(a) Blue image (b) Haze image\n(c) Green image (d) Low luminance image\nFig. 1. Illustration of typical underwater images and our enhancement results.\nUnderwater image degradation is a complex process, mainly influenced by light absorption and scattering. Various methods have been developed to recover clean images from underwater images including traditional methods and learning-based methods. Akkaynak et al. [2], [3] proposed an accurate underwater image formation model and used mathematical methods to estimate the coefficients, which can consistently remove water from underwater images. By a rank-one transmission prior, Liu et al. [4], [5] presented a real-time light correction method to recover the degraded scenes of underwater, sandstorms and haze, etc. The regularization methods were also used to recover underwater images, such as depth-aware total variation regularization [6], curvature variation regularization [7], and adaptive non-convex non-smooth variation regularization [8], etc. In recent years, the deep learning methods have provided new solutions for underwater image enhancement, which can be roughly divided into Generative Adversarial Network (GAN)-based methods and Convolutional Neural Network (CNN)-based methods. Since the discriminator of GAN can eliminate unreliable data, Li et al. [9] generated realistic ar X\niv :2\n30 3.\n06 54\n3v 1\n[ cs\n.C V\n] 1\n2 M\nar 2\n02 3\nunderwater images from the in-air images for color correction of underwater images. Later, GAN-based methods have been intensively studied for underwater image enhancement [10]\u2013 [15]. On the other hand, Li et al. [16] presented a CNN model using the scene prior for underwater image enhancement on the synthesized underwater datasets. Since then, various CNNbased models have been developed such as the Water-Net [17], JLCL-Net [18], Ucolor [19], etc. However, the existing deep learning-based underwater image enhancement methods may lose their effects on images of unknown water, which limits their usages in real applications. Indeed, model-driven deep learning methods are well-known for their high interpretability and abilities in dealing with different image processing tasks. The plug-and-play strategy as a typical model-driven method has been used for image super resolution [20], image denoising [21], image deblurring [22], etc. Most recently, Jonathan, Jain and Abbeel [23] presented the diffusion probabilistic models inspired by considerations from nonequilibrium thermodynamics, which have emerged as a powerful new family of deep generative models with a record-breaking performance in many applications, including image synthesis [24], [25], denoising [26], [27], and segmentation [28], etc.\nDeep meta-learning [29]\u2013[31] is a knowledge-driven machine learning framework that attempts to solve the problem of how to learn. Meta-learning has achieved remarkable results on no or fewer reference image processing problems. For instance, Zhu et al. [32] presented a no-reference image quality assessment metric based on deep meta-learning to learn the meta-knowledge shared by human when evaluating the quality of images with various distortions. Zhao et al. [33] proposed a novel federated meta-learning enhanced acoustic radio cooperative framework to do the transfer. Wang et al. [34] proposed a meta memory bank to improve the generalization of segmentation networks by bridging the domain gap between source and target domains. The model-agnostic nature of meta-learning can facilitate underwater image enhancement tasks, to produce good generalization performance in diverse underwater environments.\nIn this paper, we propose a task-tailored deep learning method based on the physical model of underwater images to recover clean underwater images, which can accurately describe the underwater image distortions. Our network consists of three sub-networks to estimate the clean image, background light, and transmission map, respectively, which are combined according to the underwater image model to obtain the supervision loss. Meta-learning is used to obtain a pre-trained model based on synthetic images, which is then fine-tuned on real underwater images. The pre-trained model is made to capture the shared prior knowledge of the degraded underwater images, while the fine-tuned model can make the pre-trained model easy to adapt to unknown distortions. To sum up, our major contributions are concluded as follows\n1) We propose a model-driven multi-variable convolution neural network based on the underwater image degradation model, which improves the interpretability by estimating different variables through sub-networks. 2) We implement the meta-learning strategy to learn the shared prior knowledge among different types of dis-\ntortions on a sophisticated underwater image dataset generated from in-air images to cover a wide range of degradation images, which greatly promotes the generalization ability on diversified distortions. 3) We evaluate our meta-learning underwater image enhancement model (shorted by MetaUE) on different underwater image datasets, which outperforms the stateof-the-art methods in terms of enhancement qualities and generalization ability, particularly for images with strong light scattering and insufficient lighting.\nThe rest of the paper is organized as follows. Section II reviews the most important supervised and unsupervised learningbased methods for underwater image enhancement tasks. In Section III, we propose the underwater image enhancement method based on the underwater image physical model. The meta-learning strategy is presented in Section IV, where a synthetic underwater image dataset is developed for training the pre-trained model. Numerical experiments are provided in Section V by comparing with the state-of-the-art underwater enhancement methods. We conclude the work in Section VI and discuss its possible applications for other low-quality image enhancement problems."
        },
        {
            "heading": "II. RELATED WORKS",
            "text": ""
        },
        {
            "heading": "A. Supervised learning-based methods",
            "text": "With the development of deep learning methods, it is possible to restore clean images from complex underwater environments. Supervised learning-based methods can effectively extract feature expressions from degraded underwater images to high-quality images. As a pioneering work, Li et al. [10] proposed a weakly supervised color transfer (WSCT) method by learning a cross-domain mapping function between ground-truth images and underwater images. Hong et al. [35] proposed a weakly supervised underwater enhancement model, where a GAN-based architecture is designed to enhance underwater images by unpaired image-to-image transformation. Similarly, weakly supervised methods are proposed to deal with underwater tasks, such as underwater fish segmentation [36], [37], underwater object detection [38].\nHowever, these weakly supervised methods still have much room for improvement to obtain satisfactory underwater images. Using the paired underwater/clean images to train the supervised methods is a good strategy to improve the performance of underwater problems. Li et al. [16] presented an underwater image enhancement convolutional neural network (UWCNN) based on the synthesized underwater dataset, which was built up based on the physical underwater image model and Jelov water type [34]. The training dataset in [16] was also used in [39], where the all-in-one network adversatively learned the domain-agnostic features to generate enhanced underwater images from the degraded images of ten different water types. Liu et al. [40] derived a new physical underwater image model to generate a wide range of degradation images from the clean underwater images, where the proposed simulation images were selected by color deviations and the Sobel edge map to generate reliable dataset. Wang et al. [41] noticed that only training on simulation data makes many\nlearning methods fail in real underwater images. Thus, they proposed a novel two-phase underwater domain adaptation network to simultaneously minimize the synthetic data and real underwater data gap.\nSince there exist significant differences between synthetic and real underwater datasets, which may cause poor generalization in real underwater environments. Li et al. [17] constructed an underwater enhancement benchmark (UIEB) for real underwater images, which obtained real underwater images by using the existing enhancement methods. Based on the UIEB [17], the following effective deep-learning networks are proposed to recover the underwater images. Xue et al. [18] proposed the joint luminance and chrominance learning network (JLCL-Net). Wang et al. [42] developed the UIEC2Net using two color spaces to improve luminance and saturation, respectively. Li et al. [19] proposed a network containing the multi-color spaces guided by the medium transmission to cope with color casts and low contrast problems. Liu et al. [43] developed an adaptive learning attention network for underwater image enhancement based on supervised learning."
        },
        {
            "heading": "B. Unsupervised learning-based methods",
            "text": "On the other hand, unsupervised methods were also intensively studied for underwater image enhancement. Fabbri et al. [14] used the cycle-consistent adversarial network (CycleGAN) to directly learn to generate degraded underwater images based on two separate groups of clean and degraded real-world images, which are then used to train a UNet for underwater image enhancement. Islam et al. [15] utilized the same CycleGAN-based method to build up a large dataset called EUVP and proposed a lightweight conditional GAN to recover the underwater images (FUnIE-GAN). Yang et al. [12] proposed a conditional generative adversarial network to improve the quality of the underwater images. Zhou et al. [44] proposed an unsupervised underwater loop enhancement network (ULENet) to improve the turbid underwater images. Based on the underwater image formation model, Chai et al. [45] proposed an unsupervised method to estimate the components of the physical image model, such as scene, backscatter, etc. Guo et al. [46] used the transformer module to capture the global information to train the unsupervised network. Liu et al. [13] proposed an unsupervised method called twin adversarial contrastive learning-based underwater enhancement method (TACL) to enhance the quality of the underwater images. Jiang et al. [47] proposed a novel domain adaptation framework for real-world underwater image enhancement inspired by transfer learning, which transfers in-air image dehazing to real-world underwater image enhancement. Alejandro et al. [48] presented a novel state-of-the-art end-to-end deep learning architecture for underwater image enhancement focused on solving key image degradations related to blur, haze, and color casts and inference efficiency."
        },
        {
            "heading": "III. OUR APPROACH",
            "text": ""
        },
        {
            "heading": "A. Underwater image formation",
            "text": "One crucial challenge of underwater image enhancement is the endless diversity versus the limited diversity of the training\nsamples, which harms the generalization ability of the conventional underwater methods trained on the synthesized dataset. As shown in Fig. 2, the radiance perceived by the camera I\u03bb is the sum of direct signal and background scattering, which is caused by light reflected by particles suspended in the water column. By ignoring artificial light, the underwater images can be modeled as follows\nI\u03bb(x) = J\u03bb(x)t\u03bb(x) +B\u03bb(1\u2212 t\u03bb(x)), \u03bb \u2208 {R,G,B}, (1)\nwhere J\u03bb(x) is the clean image, B\u03bb is the background light and t\u03bb(x) is the transmission map defined by t\u03bb(x) = e\u2212c\u03bbd(x) with c\u03bb being the attenuation coefficients and d(x) being the transmission distance. We propose a multi-variable CNN model to simultaneously estimate the clean image, background light, and transmission map according to the underwater image model (1), which can ideally adapt to complex and diversified underwater environments."
        },
        {
            "heading": "B. Network architecture",
            "text": "The overall framework of our model-based meta-learning model is illustrated in Fig. 3, which contains three subnetworks, i.e., the clean image estimation sub-network NJ , background light estimation sub-network NB , and the transmission map estimation sub-network NT . More specifically, the clean image estimation sub-network is used to estimate the clean image J\u03bb from the observed underwater image as follows\nJ\u03bb = NJ(I\u03bb; \u03b8J),\nwhere I\u03bb is the input image and \u03b8J represents the parameters of network NJ . The background light estimation subnetwork is used to evaluate the background light B\u03bb\nB\u03bb = NB(I\u03bb; \u03b8B),\nwhere \u03b8B represents the parameters of network NB . Finally, the transmission map estimation sub-network aims to estimate the transmission map t\u03bb(x) from an underwater image as below\nt\u03bb = NT (I\u03bb; \u03b8T ),\nwhere \u03b8T is the parameters of network NT . All the subnetworks are developed based on an encoder-decoder architecture with skip connections and shortcut connections [49] to perform the translations from degraded images to target images. The detailed network architectures are provided in Fig. 4. As can be seen, the network consists of four encoder blocks and four decoder blocks with the skip connections from each encoder block to the corresponding decoder block. To ensure that useful information is well preserved in the block outputs, we also build an open path by shortcut connection in each block. In the encoder, the image is finally downsampled into 512 feature maps, while we add a bilinear interpolation operation to up-sample the input feature maps in the decoder part."
        },
        {
            "heading": "IV. META-TRAINING STRATEGY",
            "text": ""
        },
        {
            "heading": "A. Meta-learning method",
            "text": "As shown in Fig. 3, the meta-learning strategy is used to build up our underwater image enhancement model. More specifically, we generate the meta training set Dp(\u03c4)meta =\n{D\u03c4ns ,D\u03c4nq }Nn=1, where D\u03c4ns and D\u03c4nq are the support set and query set of each task, and N is the total number of tasks. To learn a generalized model from different tasks, the sample k tasks are randomly chosen as a mini-batch from the meta training set. When the model is applied to a new task \u03c4i, the parameters of our model are updated according to the task requirements, with the loss function of the i-th support set D\u03c4is denoted by L\u03c4i for i \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , k}. To quickly obtain such ability, we use a two-level gradient descent method to update the model parameters on the support set and query set accordingly as follows{\n\u03b8\u2032J,i \u2190 \u03b8J \u2212 \u03b1\u2207L\u03c4i(NJ(\u03b8J)); on the support set \u03b8J,i \u2190 \u03b8\u2032J,i \u2212 \u03b1\u2207L\u03c4i(NJ(\u03b8\u2032J,i)); on the query set\nwhere \u03b1 is the step size of meta-learning. The gradients of all tasks are integrated to obtain the final update for the model parameters such as\n\u03b8J \u2190 \u03b8J \u2212 \u03b2 1\nk k\u2211 i=1 (\u03b8J \u2212 \u03b8J,i),\nwhere \u03b2 is the outer learning rate. Similarly, the parameters \u03b8B and \u03b8T are also updated by two-level gradient descent method.\nTo adapt to real underwater environments, we fine-tune the pre-trained model on real underwater images with unknown distortions. Then, we leverage the Adam optimizer to update the enhancement network NJ on real underwater images as follows\n\u03b8J \u2190 \u03b8J \u2212 \u03b1f\u2207L(NJ(\u03b8J)),\nwhere \u03b1f is the learning rate of fine-tuning and the other network parameters \u03b8B and \u03b8T are updated in the similar way. Then, the fine-tuned model can be obtained for enhancing the underwater images with unknown distortions. The implementation details of MetaUE are summarized in Algorithm 1.\nAlgorithm 1: Model-based Meta-learning for Underwater Enhancement (MetaUE)\nInput: Meta-trained set Dp(\u03c4)meta = {D\u03c4ns ,D\u03c4nq }Nn=1 Output: J\u03bb, B\u03bb and t\u03bb /* Pre-trained on synthetic image\ndataset */ 1 for epoch = 1, 2, ... do 2 Sample a mini-batch of k tasks in D\u03c4imeta with i \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , k}; 3 for i = 1, 2, \u00b7 \u00b7 \u00b7 , k do 4 Compute \u03b8J,i \u2190 \u03b8J \u2212 \u03b1\u2207L\u03c4i(NJ(\u03b8J)); 5 Compute \u03b8B,i \u2190 \u03b8B \u2212 \u03b1\u2207L\u03c4i(NB(\u03b8B)); 6 Compute \u03b8T,i \u2190 \u03b8T \u2212 \u03b1\u2207L\u03c4i(NT (\u03b8T )); 7 end 8 update \u03b8J \u2190 \u03b8J \u2212 \u03b2 1k \u2211k i=1(\u03b8J \u2212 \u03b8J,i);\n9 update \u03b8B \u2190 \u03b8B \u2212 \u03b2 1k \u2211k i=1(\u03b8B \u2212 \u03b8B,i);\n10 update \u03b8T \u2190 \u03b8T \u2212 \u03b2 1k \u2211k i=1(\u03b8T \u2212 \u03b8T,i); 11 end /* Fine-tuned on real image dataset */ 12 Fine-tuning the model \u03b8J , \u03b8B , and \u03b8T as \u03b8J \u2190 \u03b8J \u2212 \u03b1f\u2207L(NJ(\u03b8J)); \u03b8B \u2190 \u03b8B \u2212 \u03b1f\u2207L(NB(\u03b8B)); \u03b8T \u2190 \u03b8T \u2212 \u03b1f\u2207L(NT (\u03b8T ))."
        },
        {
            "heading": "B. Synthetic underwater image dataset",
            "text": "In the following, we build up an synthetic underwater image dataset to cover different underwater degradation images for implementing Algorithm 1. As shown in Fig.2, the incident light passing through the surface of water is the major source of illumination in an underwater environment. Here, we use the NYU-V1 dataset [50] as a benchmark dataset to generate various underwater images. Due to beam attenuation, the incident light passing through the water may make the underwater environment appearing blue, green or yellow. On the other hand, the influence of artificial light should be also considered to overcome the disadvantage of insufficient underwater illumination. Therefore, the underwater illumination can be simplified as\nE\u03bb(x) = \u03c9aE S \u03bb e \u2212c\u03bbD + \u03c9bE A \u03bb e \u2212c\u03bbd(x), (2)\nwhere \u03c9a and \u03c9b are two weights, ES\u03bb is light on the water surface, EA\u03bb is artificial light, d(x) is the distance form object to the camera and D is the water depth. The backscattering component mainly contributes to the hazy look of underwater\nimages. An efficient model was proposed in [51] to calculate the background light B\u03bb by\nB\u03bb(x) = \u03baE\u03bb(x)/c\u03bb,\nwhere \u03ba is a scalar defined by the camera system. Then the simulation formula from the scene Jgt without attenuation to the underwater image I\u03bb can be expressed as follows\nI\u03bb(x) = t\u03bb(x)J gt \u03bb (x)\nE\u03bb(x) ES\u03bb + \u03baE\u03bb(x) c\u03bb (1\u2212 t\u03bb(x)). (3)\nHere, depending on the camera principle, Jgt\u03bb (x) E\u03bb(x)\nES\u03bb is to\nremove the light ES\u03bb at x of clean image J gt \u03bb and use E\u03bb(x) as the new lighting condition. Simulation of lighting parameters E\u03bb(x). To effectively simulate various illumination conditions, the range of air light ES\u03bb is set to [0.7,1] and the depth variable D is randomly selected from [5m,20m]. The artificial illumination is the only light source to provide uneven brightness and limited visibility. Here, we use two-dimensional Gaussian distribution to simulate the artificial light in the water, where beam pattern EA\u03bb = P(x\u0303|Eart\u03bb , \u03c3) with a peak value Eart\u03bb \u2208 [0.7, 1] and standard deviation \u03c3 proportional to the width of the image by a random rate in [0.2, 1.1]. The light source x\u0303 is randomly picked from the image.\nSimulation of channel-wise attenuation coefficients c\u03bb. The attenuation coefficients of Jerlov water types [16], [52] are used to generate different water types. To better adapt to complex underwater situations, we remove the clearest water quality Type 1 and the most turbid Type 9 and introduce three kinds of water quality for the three common underwater colors (i.e., Blue, Green, and Yellow), named Type B, G, Y. The corresponding values of c\u03bb for all nine types of underwater images are presented in Table I.\nSimulation of transmission distance d(x). We use the NYU-V1 dataset [50] to generate the synthetic underwater image dataset, which contains a total of 3733 RGB images and corresponding depth maps.\nTo sum up, we use the parameters in Table II to build up a synthetic underwater image dataset based on the RGB-D NYU-V1 dataset. In particular, we randomly selected parameters three times for each water type to obtain a total of 27 specific underwater distortion types and 100791 images."
        },
        {
            "heading": "C. Loss function",
            "text": "To effectively train our multi-variable CNN model, we introduce a novel task-specific loss function as below.\nClean image supervision loss. To remove water, we compensate the disparities of wavelength attenuation for traversing\nthe water depth to the surface of water. To be specific, we use L1 loss function to measure supervision loss of clean image\nLJ = \u2016J\u03bb \u2212 Jgt\u03bb \u20161,\nwhere Jgt\u03bb denotes the clean image without attenuation. Background light supervision loss. Similarly, the background light is supervised by the L1 loss function as follows\nLB = \u2016B\u03bb \u2212Bgt\u03bb \u20161,\nwhere Bgt\u03bb = \u03baE\u03bb/c\u03bb can be regarded as the ground truth of the background light.\nTransmission map supervision loss. We penalize the difference between t\u03bb and the reference transmission map t gt \u03bb by\nLT = \u2016t\u03bb \u2212 tgt\u03bb \u20161,\nwhere the reference transmission map is given by tgt\u03bb = e \u2212c\u03bbd.\nUnderwater supervision loss. Based on the physical model (1), the underwater image can be calculated by\nI\u0303\u03bb(x) = J\u03bbt\u03bb +B\u03bb(1\u2212 t\u03bb).\nThus, the loss based on the physical process is defined to minimize the difference between the underwater I\u03bb and the physics-based estimation solution as follows\nLI = \u2016I\u03bb \u2212 I\u0303\u03bb\u20161.\nFinally, our multi-variable CNN model is jointly optimized by a weighted combination of LJ , LB ,LT , and LI as follows\nL = cJLJ + cBLB + cTLT + cILI , (4)\nwhere cJ , cB , cT , cI are the weights used to balance different terms. During the meta-learning implementation, the loss function L\u03c4i of i\u2212th support set is defined as (4). The weights in our loss function (4) are empirically set as cJ = cB = cT = 1 and cI = 0.5 for the pre-trained model. Due to the lack of ground truth of transmission map and background lights for underwater images, we set the weights as cJ = 1, cB = cT = 0 and cI = 1 when we fine-tune the model."
        },
        {
            "heading": "V. EXPERIMENTAL RESULTS",
            "text": "A. Implementation details\nIn the proposed model, all training images are cropped to 256\u00d7 256 pixel patches to feed into the proposed model. We train our model using bi-level gradient optimization with an inner learning rate \u03b1 of 1e \u2212 4 and outer learning rate \u03b2 of 5e\u22125, which is implemented based on Pytorch [53]. The total epoch is set to 40, and the batch size of training data is fixed to 8. We set the fine-tuned learning rate \u03b1f to 1e \u2212 5 and the total epoch is 30. These learning rates drop to a factor of 0.8 after every five epochs. The mini-batch size is set as k = 5. The learning rates drop to a factor of 0.8 after every two epochs."
        },
        {
            "heading": "B. Comparison methods",
            "text": "The performance of our MetaUE is compared against eight state-of-the-art underwater image enhancement methods, including three model-based methods, i.e., ACDC [54], Rank1 [4] and MMLE [55], and five learning-based methods, i.e., UWCNN [16], Water-Net [17], FUnIE-GAN [15], Ucolor [19], and TACL [13]. Note that both Water-Net [17] and Ucolor [19] were trained on the UIEB dataset, while FUnIE-GAN [15] was trained on the EUVP dataset. The UWCNN [16] was trained in simulation of multiple water types, while TACL [13] was trained on UIEB and BSD500 to train the twin adversarial enhancement network.\nTraining samples. The pre-trained model is fine-tuned on two underwater image datasets with reference images, i.e., UIEB [17] and EUVP [15], respectively. For a fair comparison, we also retrained the Water-Net and Ucolor on the EUVP dataset and retrained FUnIE-GAN on the UIEB dataset. Since the trainable codes of both UWCNN and TACL are not available, we use the public models directly in comparison.\nTesting samples. For testing the learning ability and generalization ability of our MetaUE, six underwater image datasets are used to evaluate the performance, which are UIEB [17], EUVP [15], U45 [56], UIEB-60 [17], EUVPUN [15], UCCS [57]. In particular, it contains 90 paired images of UIEB and 200 paired images of EUVP, respectively. We randomly selected 150 unpaired images from EUVPUN, 20 images from\nUCCS, and all real underwater images of U45 and UIEB-60 for comparison."
        },
        {
            "heading": "C. Quantitative evaluation",
            "text": "Full Reference Evaluation: For the test datasets with reference images, we conduct a full reference evaluation using three commonly-used metrics, PSNR, SSIM, and MSE. These results of full-reference image quality evaluation by using the reference images can provide realistic feedback on the performance of different methods to some extent. A higher PSNR and SSIM or lower MSE indicates that the result is close to the reference image.\nNon-reference Evaluation: For the test datasets without references, we employ two non-reference metrics, i.e., UCIQE [58] and UIQM [59]. A higher UCIQE or UIQM score indicates a better human visual perception. The UCIQE is a linear combination of chroma \u03c3c, contrast conl, and saturation \u00b5s defined as follows\nUCIQE = c1 \u00d7 \u03c3c + c2 \u00d7 conl + c3 \u00d7 \u00b5s\nwith c1 = 0.4680, c2 = 0.2745 and c3 = 0.2575 as in [58]. A higher UCIQE gives a better tradeoff among chroma, contrast and saturation. The UIQM is a linear composition of UICM, UISM, and UIConM, which represent the metrics of colorfulness, sharpness and contrast, respectively. The formula of UIQM is given as follows\nUIQM = c1 \u00d7 UICM + c2 \u00d7 UISM + c3 \u00d7 UIconM\nwith c1 = 0.0282, c2 = 0.2953, c3 = 3.5753 as suggest in [59]. A higher UIQM indicates a better tradeoff among colorfulness, sharpness and contrast."
        },
        {
            "heading": "D. Comparison between the pre-trained and fine-tuned models",
            "text": "We use meta-learning to obtain a pre-trained model by learning the shared knowledge among different types of distortions on the synthetic underwater image dataset. Based on the pre-trained model, we then fine-tune two image enhancement models on the real underwater image dataset, i.e., UIEB and EUVP, where F-UIEB and F-EUVP are used to denote the fine-tuned models on UIEB and EUVP, respectively. For simplicity, we use P-UIEB and P-EUVP to represent the results of the pre-trained model on the UIEB and EUVP datasets, respectively. As demonstrated in Fig. 5, we monitor the training process by tracking the validation loss and averaged PSNR on the test datasets of UIEB and EUVP. We can observe that both pre-trained models converge in around 30 epochs, while the fine-tuned models require about 20 epochs to reach convergence. More importantly, the values of loss function and PSNR have been significantly improved by the finetuning process on real underwater images, which convinces the merit of meta-learning strategy in dealing with such image restoration problems.\nGeneralization is an important capability for the underwater image enhancement model, which can equip it to handle\ndiversified distortions. As shown in Fig. 6, we evaluate the performance of the pre-trained model and fine-tuned model on typical underwater attenuation images including haze, blue, green, yellow, and turbid. We notice that our model-driven pretrained model can well remove the background color of the underwater images, while the fine-tuned models can adapt to the characteristics of real underwater datasets. For quantitative comparison, Table III provides the averaged UCIQE and UIQM as well as the component scores of UCIQE and UIQM for the test images in Fig. 6, where the pre-trained model gives higher UCIQE and UIQM than the two fine-tuned models. It confirms that the pre-trained model can provide enhancement results with better human visual perception. By analyzing the composition of these indicators, the pre-trained model is superior to other models in contrast and sharpness, but the color saturation needs to be further improved. Therefore, the model-based pre-trained model can remove the color degradation from various underwater scenarios. Since the finetuned models learned features from a specific real underwater dataset, they may have some performance degradation when the water environment changes."
        },
        {
            "heading": "E. Comparison with other underwater image enhancement methods",
            "text": "We further evaluate our MetaUE model by comparing it with state-of-the-art underwater image enhancement methods. The quantitative comparison is displayed in Table IV, where our pre-trained model is fine-tuned on the UIEB dataset. As shown in Table IV, our MetaUE model outperforms other comparison methods in terms of PSNR, SSIM, and MSE, demonstrating the advantages of our model-based metalearning strategy. The learning-based methods, Water-Net and Ucolor, perform poorly on other underwater image datasets except for UIEB, which convinces the advantages of building up the deep learning model based on the physical model for underwater images. For the unpaired datasets, the UIQM and UCIQE are the main evaluation metrics. It can be observed that both the pre-trained model and MetaUE provide better results than other comparison methods. Furthermore, it is shown that the pre-trained model presents its generalization ability on real underwater datasets. When we fine-tune the pre-trained model on UIEB, both PSNR and SSIM on the test dataset are significantly improved on UIEB, but it loses generalization on the EUVP dataset. Similar results can be observed in Table V, for which we fine-tuned the learning-based models on the EUVP dataset.\nIn Fig. 7, we exhibit the visual enhancement results among different methods on the degraded underwater images including scenes such as haze, green, blue, and low light. As can be observed, for the comparison methods, some work well on low-light images or yellow images, but none of them can provide satisfactory results on all applications. Our MetaUE presents good generalization ability on various underwater images, where the visual comparison is consistent with the quantitative results in Table IV. Furthermore, the pre-trained model is shown to be able to remove the water to obtain clear images, which provides excellent results for low-brightness images by restoring the objects in the shadow. And the MetaUE produces similar restoration results as the reference images provided by the UIEB dataset, which reflects the effect\nof the fine-tuning process. More specifically, the first two examples in Fig. 8 reveal that the pre-trained model may lose color information due to the limited coverage of the synthetic dataset. On the other hand, the fine-tuned model can learn the color features from real underwater images. Similar results can be observed in Fig. 8, where the pre-trained model shows good generalization ability by removing the color degradation and repairing the insufficient illumination. The fine-tuning can effectively compensate for the underwater characteristic to make the enhanced images more realistic.\nSome physical model-based methods [3], [4], [54], [60] assume the background light to be a constant function, which leads to inaccurate estimation. Actually, both background light and transmission map are smooth functions [3]. As shown in Fig. 9, we provide two examples to illustrate the solutions of the background light and transmission map in our model. As can be seen, the clean image, transmission map and background light are correctly estimated by our method. In particular, the residual image between the reference image Jgt\u03bb and the estimated clean image J\u03bb indicates that the pre-trained model can accurately remove the background color."
        },
        {
            "heading": "F. Ablation study",
            "text": "The ablation studies are presented to explore the effectiveness of the model-driven meta learning method for underwater image enhancement. More specifically, we build up three deep learning models for comparison, namely the Plain UNet, Meta UNet, and MetaUE. Note that the \u2018Plain UNet\u2019 is realized by random initialization and trained on UIEB and EUVP datasets, respectively. The \u2018Meta UNet\u2019 model is introduced to evaluate the effectiveness of meta-learning, where only the clean image NJ(I; \u03b8J) was trained by meta-learning. As shown in Table VI, the pre-trained strategy plays a major role in improving underwater image quality, while the physical image model can guide the deep-learning model to recover clean images from the observations. Fig. 10 provides the visual comparison of the three methods, where the first to fourth columns are the results from the UIEB dataset and the fifth to eighth columns are the\nresults from the EUVP. We can observe that the fine-tuned models based on the physical model give the best results of the three models, confirming the effectiveness of our model."
        },
        {
            "heading": "VI. CONCLUSION AND DISCUSSION",
            "text": "In this paper, we presented a model-based deep-learning model to improve the visual qualities of underwater images, which was demonstrated with better interpretability and generalization ability. To be specific, we built up a multi-variable convolution model to learn the composition of the underwater image, including the clean image, transmission map, and background light. The meta-learning was used to first capture the shared prior knowledge of the degraded underwater images and then learn to adapt to a specific underwater environment by fine-tuning on the real underwater image dataset. Numerical experiments showed that our MetaUE outperformed several SOTA underwater image enhancement methods in both quality and robustness.\nIn addition to underwater images, there exist other degraded images that could be regarded as a superimposition of a clean image with environmental conditions such as sand dust, haze, and low light, etc. Various methods have been investigated to deal with such image restoration problems, e.g., low light enhancement [61], [62] and image dehazing [63], [64]. Indeed, these problems can be also modeled by the physical model (1) and recovered by our model-based deep learning model. As illustrated in Fig. 11, our pre-trained model works well on these challenging applications, which outperforms the recent\nRank1 prior method for scene recovery in [4]. Thus, it is beneficial to construct the network model based on a solid physical model, which can help to guide the network towards achieving desired outcomes. Although our MetaUE has demonstrated the improved generalization capabilities, there is still an obvious disparity in the performance of the fine-tuned model across different datasets; see Table IV and Table V. Our future work includes developing more efficient underwater image enhancement models in dealing with various real underwater images."
        }
    ],
    "title": "MetaUE: Model-based Meta-learning for Underwater Image Enhancement",
    "year": 2023
}