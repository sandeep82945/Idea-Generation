{
    "abstractText": "With the rapid progress of Large language models (LLMs) and the huge amount of text they generated, it becomes more and more impractical to manually distinguish whether a text is machine-generated. Given the growing use of LLMs in social media and education, it prompts us to develop methods to detect machine-generated text, preventing malicious usage such as plagiarism, misinformation, and propaganda. Previous work has studied several zero-shot methods, which require no training data. These methods achieve good performance, but there is still a lot of room for improvement. In this paper, we introduce two novel zero-shot methods for detecting machinegenerated text by leveraging the log rank information. One is called DetectLLM-LRR, which is fast and efficient, and the other is called DetectLLM-NPR, which is more accurate, but slower due to the need for perturbations. Our experiments on three datasets and seven language models show that our proposed methods improve over the state of the art by 3.9 and 1.75 AUROC points absolute. Moreover, DetectLLM-NPR needs fewer perturbations than previous work to achieve the same level of performance, which makes it more practical for real-world use. We also investigate the efficiency\u2013performance trade-off based on users preference on these two measures and we provide intuition for using them in practice effectively. We release the data and the code of both methods in https://github.com/ mbzuai-nlp/DetectLLM.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jinyan Su"
        },
        {
            "affiliations": [],
            "name": "Terry Yue Zhuo"
        },
        {
            "affiliations": [],
            "name": "Di Wang"
        },
        {
            "affiliations": [],
            "name": "Preslav Nakov"
        },
        {
            "affiliations": [],
            "name": "Mohamed bin Zayed"
        }
    ],
    "id": "SP:f241911fd2c4fd9ac434c30b0abd6161c64f1827",
    "references": [
        {
            "authors": [
                "Sahar Abdelnabi",
                "Mario Fritz."
            ],
            "title": "Adversarial watermarking transformer: Towards tracing text provenance with data hiding",
            "venue": "2021 IEEE Symposium on Security and Privacy (SP), pages 121\u2013140. IEEE.",
            "year": 2021
        },
        {
            "authors": [
                "Sameer Badaskar",
                "Sachin Agarwal",
                "Shilpa Arora"
            ],
            "title": "Identifying real or fake articles",
            "year": 2008
        },
        {
            "authors": [
                "Anton Bakhtin",
                "Sam Gross",
                "Myle Ott",
                "Yuntian Deng",
                "Marc\u2019Aurelio Ranzato",
                "Arthur Szlam"
            ],
            "title": "Real or fake? learning to discriminate machine from human",
            "year": 2019
        },
        {
            "authors": [
                "Daria Beresneva."
            ],
            "title": "Computer-generated text detection using machine learning: A systematic review",
            "venue": "Natural Language Processing and Information Systems: 21st International Conference on Applications of Natural Language to Information Systems, NLDB",
            "year": 2016
        },
        {
            "authors": [
                "Samuel Weinbach"
            ],
            "title": "Gpt-neox-20b: An opensource autoregressive language model",
            "year": 2022
        },
        {
            "authors": [
                "Sid Black",
                "Gao Leo",
                "Phil Wang",
                "Connor Leahy",
                "Stella Biderman."
            ],
            "title": "GPT-Neo: Large Scale Autoregressive Language Modeling with MeshTensorflow",
            "venue": "If you use this software, please cite it using these metadata.",
            "year": 2021
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Tiziano Fagni",
                "Fabrizio Falchi",
                "Margherita Gambini",
                "Antonio Martella",
                "Maurizio Tesconi."
            ],
            "title": "Tweepfake: About detecting deepfake tweets",
            "venue": "Plos one, 16(5):e0251415.",
            "year": 2021
        },
        {
            "authors": [
                "Angela Fan",
                "Mike Lewis",
                "Yann Dauphin."
            ],
            "title": "Hierarchical neural story generation",
            "venue": "arXiv preprint arXiv:1805.04833.",
            "year": 2018
        },
        {
            "authors": [
                "Luciano Floridi",
                "Massimo Chiriatti."
            ],
            "title": "Gpt-3: Its nature, scope, limits, and consequences",
            "venue": "Minds and Machines, 30:681\u2013694.",
            "year": 2020
        },
        {
            "authors": [
                "Leo Gao",
                "Stella Biderman",
                "Sid Black",
                "Laurence Golding",
                "Travis Hoppe",
                "Charles Foster",
                "Jason Phang",
                "Horace He",
                "Anish Thite",
                "Noa Nabeshima"
            ],
            "title": "The pile: An 800gb dataset of diverse text for language modeling",
            "venue": "arXiv preprint arXiv:2101.00027",
            "year": 2020
        },
        {
            "authors": [
                "Sebastian Gehrmann",
                "Hendrik Strobelt",
                "Alexander M Rush."
            ],
            "title": "Gltr: Statistical detection and visualization of generated text",
            "venue": "arXiv preprint arXiv:1906.04043.",
            "year": 2019
        },
        {
            "authors": [
                "Alexei Grinbaum",
                "Laurynas Adomaitis."
            ],
            "title": "The ethical need for watermarks in machine-generated language",
            "venue": "arXiv preprint arXiv:2209.03118.",
            "year": 2022
        },
        {
            "authors": [
                "Biyang Guo",
                "Xin Zhang",
                "Ziyuan Wang",
                "Minqi Jiang",
                "Jinran Nie",
                "Yuxuan Ding",
                "Jianwei Yue",
                "Yupeng Wu."
            ],
            "title": "How close is chatgpt to human experts? comparison corpus, evaluation, and detection",
            "venue": "arXiv preprint arXiv:2301.07597.",
            "year": 2023
        },
        {
            "authors": [
                "Xinlei He",
                "Xinyue Shen",
                "Zeyuan Chen",
                "Michael Backes",
                "Yang Zhang."
            ],
            "title": "Mgtbench: Benchmarking machine-generated text detection",
            "venue": "arXiv preprint arXiv:2303.14822.",
            "year": 2023
        },
        {
            "authors": [
                "Ari Holtzman",
                "Jan Buys",
                "Li Du",
                "Maxwell Forbes",
                "Yejin Choi."
            ],
            "title": "The curious case of neural text degeneration",
            "venue": "arXiv preprint arXiv:1904.09751.",
            "year": 2019
        },
        {
            "authors": [
                "Daphne Ippolito",
                "Daniel Duckworth",
                "Chris CallisonBurch",
                "Douglas Eck."
            ],
            "title": "Automatic detection of generated text is easiest when humans are fooled",
            "venue": "arXiv preprint arXiv:1911.00650.",
            "year": 2019
        },
        {
            "authors": [
                "Ganesh Jawahar",
                "Muhammad Abdul-Mageed",
                "Laks VS Lakshmanan."
            ],
            "title": "Automatic detection of machine generated text: A critical survey",
            "venue": "arXiv preprint arXiv:2011.01314.",
            "year": 2020
        },
        {
            "authors": [
                "John Kirchenbauer",
                "Jonas Geiping",
                "Yuxin Wen",
                "Jonathan Katz",
                "Ian Miers",
                "Tom Goldstein."
            ],
            "title": "A watermark for large language models",
            "venue": "arXiv preprint arXiv:2301.10226.",
            "year": 2023
        },
        {
            "authors": [
                "Kalpesh Krishna",
                "Yixiao Song",
                "Marzena Karpinska",
                "John Wieting",
                "Mohit Iyyer."
            ],
            "title": "Paraphrasing evades detectors of ai-generated text, but retrieval is an effective defense",
            "venue": "arXiv preprint arXiv:2303.13408.",
            "year": 2023
        },
        {
            "authors": [
                "Thomas Lavergne",
                "Tanguy Urvoy",
                "Fran\u00e7ois Yvon."
            ],
            "title": "Detecting fake content with relative entropy scoring",
            "venue": "PAN, 8:27\u201331.",
            "year": 2008
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Eric Mitchell",
                "Yoonho Lee",
                "Alexander Khazatsky",
                "Christopher D Manning",
                "Chelsea Finn."
            ],
            "title": "Detectgpt: Zero-shot machine-generated text detection using probability curvature",
            "venue": "arXiv preprint arXiv:2301.11305.",
            "year": 2023
        },
        {
            "authors": [
                "Shashi Narayan",
                "Shay B Cohen",
                "Mirella Lapata."
            ],
            "title": "Don\u2019t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization",
            "venue": "arXiv preprint arXiv:1808.08745.",
            "year": 2018
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "gpt: Optimizing language models for dialogue",
            "venue": "http://web.archive.org/web/ 20230109000707/https://openai.com/ blog/chatgpt/.",
            "year": 2022
        },
        {
            "authors": [
                "Artidoro Pagnoni",
                "Martin Graciarena",
                "Yulia Tsvetkov."
            ],
            "title": "Threat scenarios and best practices to detect neural fake news",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, pages 1233\u20131249.",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Jian Zhang",
                "Konstantin Lopyrev",
                "Percy Liang."
            ],
            "title": "Squad: 100,000+ questions for machine comprehension of text",
            "venue": "arXiv preprint arXiv:1606.05250.",
            "year": 2016
        },
        {
            "authors": [
                "Alexander Ratner",
                "Stephen H Bach",
                "Henry Ehrenberg",
                "Jason Fries",
                "Sen Wu",
                "Christopher R\u00e9."
            ],
            "title": "Snorkel: Rapid training data creation with weak supervision",
            "venue": "Proceedings of the VLDB Endowment. International Conference on Very Large Data Bases,",
            "year": 2017
        },
        {
            "authors": [
                "Joshua Robinson",
                "Christopher Michael Rytting",
                "David Wingate."
            ],
            "title": "Leveraging large language models for multiple choice question answering",
            "venue": "arXiv preprint arXiv:2210.12353.",
            "year": 2022
        },
        {
            "authors": [
                "Kalhan Rosenblatt."
            ],
            "title": "Chatgpt passes mba exam given by a wharton professor",
            "venue": "Retrieved Jan, 25:2023.",
            "year": 2023
        },
        {
            "authors": [
                "Vinu Sankar Sadasivan",
                "Aounon Kumar",
                "Sriram Balasubramanian",
                "Wenxiao Wang",
                "Soheil Feizi"
            ],
            "title": "Can ai-generated text be reliably detected? arXiv preprint arXiv:2303.11156",
            "year": 2023
        },
        {
            "authors": [
                "Teven Le Scao",
                "Angela Fan",
                "Christopher Akiki",
                "Ellie Pavlick",
                "Suzana Ili\u0107",
                "Daniel Hesslow",
                "Roman Castagn\u00e9",
                "Alexandra Sasha Luccioni",
                "Fran\u00e7ois Yvon",
                "Matthias Gall\u00e9"
            ],
            "title": "Bloom: A 176bparameter open-access multilingual language model",
            "year": 2022
        },
        {
            "authors": [
                "Irene Solaiman",
                "Miles Brundage",
                "Jack Clark",
                "Amanda Askell",
                "Ariel Herbert-Voss",
                "Jeff Wu",
                "Alec Radford",
                "Gretchen Krueger",
                "Jong Wook Kim",
                "Sarah Kreps"
            ],
            "title": "Release strategies and the social impacts of language models",
            "year": 2019
        },
        {
            "authors": [
                "Chris Stokel-Walker"
            ],
            "title": "2022. Ai bot chatgpt writes smart essays-should academics worry? Nature",
            "year": 2022
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Adaku Uchendu",
                "Thai Le",
                "Kai Shu",
                "Dongwon Lee."
            ],
            "title": "Authorship attribution for neural text generation",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8384\u20138395.",
            "year": 2020
        },
        {
            "authors": [
                "Ben Wang",
                "Aran Komatsuzaki."
            ],
            "title": "GPTJ-6B: A 6 Billion Parameter Autoregressive Language Model",
            "venue": "https://github.com/ kingoflolz/mesh-transformer-jax.",
            "year": 2021
        },
        {
            "authors": [
                "Ann Yuan",
                "Andy Coenen",
                "Emily Reif",
                "Daphne Ippolito."
            ],
            "title": "Wordcraft: story writing with large language models",
            "venue": "27th International Conference on Intelligent User Interfaces, pages 841\u2013852.",
            "year": 2022
        },
        {
            "authors": [
                "Wang",
                "Luke Zettlemoyer"
            ],
            "title": "Opt: Open pretrained transformer language models",
            "year": 2022
        },
        {
            "authors": [
                "Xuandong Zhao",
                "Yu-Xiang Wang",
                "Lei Li."
            ],
            "title": "Protecting language generation models via invisible watermarking",
            "venue": "arXiv preprint arXiv:2302.03162.",
            "year": 2023
        },
        {
            "authors": [
                "Rank (Gehrmann"
            ],
            "title": "2019): This approach evaluates the average rank of each token of the text and classifies text with smaller average rank to be machine generated",
            "year": 2019
        },
        {
            "authors": [
                "Radford"
            ],
            "title": "2019) is the 1.5B parameter version of GPT2 trained on a dataset of 8 million web pages called WebText (Radford et al., 2019), whose objective is to predict the next word given previous words within the text. GPT2-xl surpasses many other language models trained on specific domain",
            "year": 2019
        },
        {
            "authors": [
                "Wang",
                "Komatsuzaki"
            ],
            "title": "2021), which was also trained on Pile (Gao et al., 2020), exhibits zero-shot performance roughly comparable to GPT-3 of comparable size. In addition, the performance gap from GPT-3 of similar size is closer than the GPT-Neo models",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Large language models (LLMs) have made rapid advancement in recent years, and are now able to generate text with significantly improved diversity, fluency, and quality. Models such as ChatGPT (OpenAI, 2022), GPT-3 (Brown et al., 2020), LLaMa (Touvron et al., 2023) and BLOOM (Scao et al., 2022) demonstrate exceptional performance\nin answering questions (Robinson et al., 2022), writing stories (Fan et al., 2018; Yuan et al., 2022), composing emails, analyzing program code, and thus facilitating daily life and improving work efficiency. However, LLMs can also be misused for generating plagiarized text, misinformation, and propaganda, which can lead to negative consequences. For instance, students might use LLMs to write their essays and assignments (Rosenblatt, 2023), making fair evaluation difficult for teachers, and in the long run, undermining the integrity of the entire education system. Malicious actors might generate fake news articles to spread misinformation and propaganda or to manipulate the public opinion, which is dangerous, especially when it comes to politics (Floridi and Chiriatti, 2020; Stokel-Walker, 2022).\nWith the proliferation of LLMs and the increasing amount of texts it produced, it is challenging for humans to accurately identify machine-generated texts (Gehrmann et al., 2019). Moreover, it is unrealistic to hire humans to manually identify machinegenerated text at scale due to the prohibitively high costs and the efficiency requirements in real-time applications, e.g., in social media. Thus, it is essential to develop tools and strategies to automatically identify machine-generated text and to mitigate the potential negative impact of LLMs.\nThe problem of distinguishing machinegenerated from human-written text is commonly formulated as a binary classification task (Jawahar et al., 2020). Most previous work has focused on the black-box scenario, where the detector has access to the output of the LLMs only and cannot make use of its internal representations and states. They train or fine-tune a supervised binary classification model using the output of the LLMs. Such methods lack flexibility since they need to be retrained from scratch to be able to recognize the output of a new LLM (Mitchell et al., 2023). Given the speed at which new LLMs\nar X\niv :2\n30 6.\n05 54\n0v 1\n[ cs\n.C L\n] 2\n3 M\nay 2\n02 3\nare developed, black-box methods are becoming more and more expensive and impractical. In cases when the access to the LLM is via an API only, one possibility is for the LLM owner to record all content it has generated, or to watermark all texts it has generated (Kirchenbauer et al., 2023; Zhao et al., 2023). However, such solutions are not feasible for third-parties.\nWe therefore consider a white-box setting, where the detector has full access to the LLMs. More specifically, we focus on zero-shot methods, where we use the LLM itself, without additional training. Generally speaking, zero-shot methods uses the source LLM to extract statistics such as the average per-token log probability or the average rank of each token in the ranked list of possible choices, and then to make a prediction by comparing it to a threshold (Solaiman et al., 2019; Ippolito et al., 2019; Gehrmann et al., 2019). Recently, Mitchell et al. (2023) observed that machine-generated text tends to lie in the negative curvature of the log likelihood of the text, proposing a perturbation-based zero-shot method called DetectGPT and achieving the best performance at the expense of efficiency.\nHere, we introduce two novel zero-shot methods, which extensively exploit the potential of the log rank information. The first one is using LogLikelihood Log-Rank ratio (LRR), which complements Log-Likelihood with Log-Rank to enhance the performance. The second one uses a Normalized perturbed log rank (NPR), which is based on the intuition that machine-generated texts are more sensitive to minor rewrites (or say, small perturbations). We called these two methods DetectLLM-LRR and DetectLLM-NPR respectively.\nIn summary, our contributions are as follows:\n\u2022 We propose two novel zero-shot approaches based on log rank statistics, which improve over the state of the art. On average, the proposed two methods improved upon the previous best zero-shot methods by 3.9 and 1.75 AUROC points absolute.\n\u2022 We investigate the efficacy of existing zeroshot methods and explore their boundaries and limits as the size of the LLMs increases from 1.5 to 20 billion.\n\u2022 We conduct comprehensive experiments to better understand the efficiency\u2013performance\ntrade-offs in zero-shot methods, thereby providing interesting insights on how to choose among different categories of zero-shot methods based on users\u2019 preference on performance or efficiency."
        },
        {
            "heading": "2 Related Work",
            "text": "The detection of machine-generated text is commonly formulated as a classification task (Jawahar et al., 2020; Fagni et al., 2021; Bakhtin et al., 2019; Sadasivan et al., 2023). One way of solving it is to use supervised learning, where a classification model is trained on a dataset containing both machine-generated and human-written texts. For example, GPT2 Detector (Solaiman et al., 2019) fine-tunes RoBERTa (Liu et al., 2019) on the output of GPT2, while the ChatGPT Detector (Guo et al., 2023) fine-tunes RoBERTa on the HC3 (Guo et al., 2023) dataset. However, models trained explicitly to detect machine-generated texts may overfit to their training distribution of the domains (Bakhtin et al., 2019; Uchendu et al., 2020).\nAnother stream of work attempts to distinguish machine-generated from human-written texts based on statistical irregularities in the entropy (Lavergne et al., 2008), perplexity (Beresneva, 2016) or in the n-gram frequencies (Badaskar et al., 2008). Gehrmann et al. (2019) introduced hand-crafted statistical features to assist humans in detectingmachine generated texts. Moreover, (Solaiman et al., 2019) noted the efficacy of simple zero-shot methods for detecting machine-generated text by evaluating the per-token log probability of texts and using thresholding. Mitchell et al. (2023) observed that machine-generated texts tend to lie in the local curvature of the log probability and proposed DetectGPT, whose prominent performance can only\nbe guaranteed by the large size of the perturbation function and by a large number of perturbations, and thus costs more computational resources.\nOther work explored watermarking, which imprints specific patterns of the LLM output text that can be detected by an algorithm while being imperceptible to humans. Grinbaum and Adomaitis (2022) and Abdelnabi and Fritz (2021) watermarked machine-generated text using syntax tree manipulation, while Kirchenbauer et al. (2023) required access to the LLM\u2019s logits at each time step to add the watermark."
        },
        {
            "heading": "3 Improved Zero-Shot Approaches by Leveraging Log Rank Information",
            "text": "In this section, we introduce the Log-Likelihood Log-Rank Ratio (LRR) and the Normalized Perturbed log-Rank (NPR). LRR combines logrank and log-likelihood as they provide complimentary information about the evaluated text. NPR is based on the idea that the log rank of machinegenerated texts should be more sensitive to smaller perturbations."
        },
        {
            "heading": "3.1 Log-Likelihood Log-Rank Ratio (LRR)",
            "text": "We define Log-Likelihood Log-Rank Ratio as\nLRR = \u2223\u2223\u2223\u2223\u2223 1t \u2211t i=1 log p\u03b8(xi|x<i) 1 t \u2211t i=1 log r\u03b8(xi|x<i) \u2223\u2223\u2223\u2223\u2223 =\u2212\n\u2211t i=1 log p\u03b8(xi|x<i)\u2211t i=1 log r\u03b8(xi|x<i) ,\nwhere r\u03b8(xi|x<i) \u2265 1 is the rank of token xi conditioned on the previous tokens.\nThe Log-Likelihood in the numerator represents the absolute confidence for the correct token, while the Log-Rank in the denominator accounts for the relative confidence, which reveals complimentary information about the texts. As illustrated in Figure 1, LRR is generally larger for machinegenerated text, which can be used for distinguish-\ning machine-generated from human-written text. One plausible reason might be that for machinegenerated text, the log rank is more discernible than the log likelihood, so LRR illustrates this pattern for machine-generated text. In Sections 4 and 6, we experimentally show that LRR is a better discriminator than either the log likelihood or the log rank. We call the zero-shot method using LRR as a detection feature as DetectLLM-LRR, and use abbreviation LRR in the rest of the paper."
        },
        {
            "heading": "3.2 Normalized Log-Rank Perturbation (NPR)",
            "text": "We define the normalized perturbed log rank as\nNPR = 1 n\n\u2211n p=1 log r\u03b8(x\u0303p)\nlog r\u03b8(x) ,\nwhere small perturbations are applied on the target text x to produce the perturbed text x\u0303p. Here, perturbation means minor rewrites of the texts, such as replacing some of the words. We call the zeroshot method using NPR as a detection feature as DetectLLM-NPR, and use abbreviation NPR in the rest of the paper.\nThe motivation for NPR is that machinegenerated and human-written texts are both negatively affected by small perturbations, i.e., the log rank score will increase after perturbations, but machine-generated text is more susceptible to perturbations and thus increasing more on log rank score after perturbation, which suggests higher NPR score for machine-generated texts. As shown in Figure 1, NPR can be a discernible signal for distinguishing machine-generated from humanwritten text. DetectGPT (Mitchell et al., 2023) uses a similar idea, but experimentally, we find NPR to be more efficient and to perform better. Details and comparisons are given in Section 4."
        },
        {
            "heading": "4 Experimental Setup",
            "text": "In this section, we conduct comprehensive experiments to evaluate the performance of LRR and NPR in comparison to several methods previosuly proposed in the literatu. We experiment with LLMs sizes varying from 1.5B to 20B parameters, probing the boundary of zero-shot methods when LLMs continue to grow in size. We further study the impact of the perturbation function used, the number of perturbations (specially for NPR and DetectGPT), the decoding strategy, and the temperature used."
        },
        {
            "heading": "4.1 Data",
            "text": "Following (Mitchell et al., 2023), we use three datasets: XSum (Narayan et al., 2018), SQuAD (Rajpurkar et al., 2016), WritingPrompts (Fan et al., 2018), containing news articles, Wikipedia paragraphs and prompted stories, respectively as human-written texts while attain machinegenerated texts using LLMs. These datasets are chosen to represent the areas where LLMs could have a negative impact. For each experiment, we evaluate 300 machine-generated and humanwritten texts pairs by prompting the LLMs with the first 30 tokens of the human-written text. The whole data-generation process is released with our codes."
        },
        {
            "heading": "4.2 Evaluation Measure",
            "text": "Following previous work (Mitchell et al., 2023; He et al., 2023; Krishna et al., 2023), we measure the performance using the area under the receiver operating characteristic curve (AUROC), which is the probability that a classifier correctly ranks the machine-generated example higher than humanwritten example. Since for zero-shot methods, detection rates are heavily dependent on the threshold when using discriminative statistics, the AUROC metric is commonly used to measure zero-shot detector performance, which considers the range of all possible thresholds (Krishna et al., 2023)."
        },
        {
            "heading": "4.3 Methods",
            "text": ""
        },
        {
            "heading": "4.3.1 Zero-Shot Methods",
            "text": "We compare the following zero-shot methods:\n\u2022 log p(x): the idea is that a passage with a high average log probability is more likely to have been generated by the target LLM;\n\u2022 Rank: the idea is that a passage with a higher average rank is more likely to have been generated by the target LLM;\n\u2022 Log-Rank: passage with higher average observed log rank is more likely to have been generated by the target LLM;\n\u2022 Entropy: machine-generated text has higher entropy;\n\u2022 DetectGPT: machine-generated text has more negative log probability curvature.\nMore detail and exact definition of these methods can be found in Appendix A.\nThese zero-shot baselines, along with our newly proposed LRR and NPR, can be categorized into two groups:\n\u2022 Perturbation-free: log p(x), Rank, LogRank, Entropy, LRR. These methods only query the LLM for statistics about the target text x.\n\u2022 Perturbation Based: DetectGPT and NPR. These methods query the LLM not only for the target text x, but also for perturbed versions thereof x\u03031, \u00b7 \u00b7 \u00b7 , x\u0303p.\nSince perturbation-based methods generally perform better (but are also more time-consuming), for fair comparison, we compare them within their own group."
        },
        {
            "heading": "4.3.2 Supervised Methods",
            "text": "We also experiment with two supervised detectors: RoBERTa-base and RoBERTa-Large. However, as these are not central for our narrative, we put the results and the analysis in Appendix B."
        },
        {
            "heading": "4.4 Experimental Details",
            "text": "For the perturbation-based methods (DetectGPT and NPR), we use T5-3B as the perturbation model and we perturb the input text 50 times for all the experiments, unless specified otherwise. For all zero-shot methods, we use sampling with a temperature of 1, unless specified otherwise. More details about the experiments are given in Appendix A."
        },
        {
            "heading": "5 Evaluation Results",
            "text": ""
        },
        {
            "heading": "5.1 Zero-Shot Results",
            "text": "Table 1 shows a comparison of the five baseline zero-shot approaches to our proposed LRR and NPR, grouped as perturbation-based and perturbation-free. We can see that for the perturbation based methods, NPR consistently outperforms DetectGPT on all datasets and LLMs, except for one case, with an average improvement of 0.90, 2.03, 2.32 AUROC points absolute on XSum, SQuAD, and WritingPrompts, respectively, (using the same perturbation function and the same number of perturbations). For the experiments among perturbation-free methods, on average, our method achieves the best performance and improves by 2.15, 8.27, 1.28 AUROC points absolute\nover the second-best perturbation-free method (i.e., log rank) on XSum, SQuAD, and WritingPrompts, respectively. Moreover, we find that in some cases, LRR can even perform better than perturbation based methods, e.g., on SQuAD, LRR outperforms DetectGPT by 4.23 AUROC point absolute and outperforms NPR by 2.20 AUROC points."
        },
        {
            "heading": "5.2 Comparing DetectGPT to NPR",
            "text": "Equipped with large perturbation functions and adequate amount of perturbations, perturbation-based methods generally outperform perturbation-free\nones, e.g., using T5-3b as the perturbation function and perturb 50 times as in Table 1. However, in practice, due to time and resource constraint, not all users can afford these models and large amount of perturbations. Thus, it is important to investigate how NPR and DetectGPT behave with smaller perturbation function size and fewer perturbations.\nDifferent Number of Perturbations. Figure 2 shows the averaged performance of DetectGPT and NPR with varying number of perturbations. We can see that NPR consistently performs better than DetectGPT when using the same number of pertur-\nFunction Perturbation Dataset # (Perturbations) 10 20 50 100 NPR (ours) T5-large 86.69 88.00 88.74 88.94 T5-3b 91.39 92.35 93.04 93.20 Diff 4.70 4.35 4.30 4.26\nDetectGPT T5-large 77.94 81.12 83.90 84.54 T5-3b 86.70 89.57 91.38 92.10 Diff 8.76 8.45 7.48 7.56\nTable 3: Perturbation analysis. Comparing DetectGPT to NPR using different perturbations (AUROC scores).\nbations. In other words, NPR can achieve a comparable or better performance but with significantly fewer perturbations. For example, in SQuAD and WritingPrompts dataset, NPR achieves 85 points and 95 points using approximately 10 perturbations while DetectGPT requires around 100 perturbations, which highlights the effectiveness and efficiency of NPR. More complete results for each dataset and models can be found in Figure 6 and Figure 7 of Appendix C.\nDifferent Perturbation Functions. In Table 3, we compare NPR with DetectGPT using smaller perturbation model T5-large, and the result is averaged over 6 LLMs and 3 datasets. We found that, not surprisingly, replacing T5-3b to smaller models harms the performance of both NPR and DetectGPT, and the performance degradation can\u2019t be mitigated by increasing the number of perturbations. For both NPR and DetectGPT, the average performance of 100 perturbations with T5-large is still worse than 10 perturbations with T5-3b (emphasized with gray box in Table 3). Moreover, one can observe that, NPR is less affected by the reduced perturbation function size: when replacing T5-3b to T5-large, the performance degradation averaged over 10, 20, 50, 100 perturbations for NPR is 4.40 point, much smaller compared to that of 8.06 point for DetectGPT. The complete results on 6 LLMs and 3 datasets can be found in Figure 8 of Appendix C."
        },
        {
            "heading": "5.3 Different Decoding Strategy and Temperature",
            "text": "In this subsection, we study different decoding strategy and temperature to see how these factors impact different zero-shot detectors.\nAlternative Decoding Strategies. In line with prior work (Pagnoni et al., 2022), we experimented\nwith top-k sampling (Fan et al., 2018) and top-p sampling (Holtzman et al., 2019). Top-k sampling generates from top-k most likely words according to the LLM. Top-p sampling (nucleus sampling) samples from the set of words that collectively accounts for a total mass probability p. The result (averaged across 4 LLMs) are shown in Table 2, and complete results can be found in Table 8 of Appendix D. We find that, although almost all the zero-shot methods performs better when using topk and top-p sampling than temperature sampling, Log Rank and Log Likelihood method are more in favor of top-p sampling, while LRR is stable in both top-p and top-k sampling. For top-k decoding, LRR improves 4.06, 9.33, 1.38 points over the second best zero-shot method baseline on three datasets, respectively. LRR performance also improves when using top-p decoding strategy, but due to the unstable performance surge of Log Rank method, LRR become slightly behind Log Rank method, with a minor difference of 0.36 and 0.19 points on XSum and WritingPrompts dataset, respectively. For perturbation based methods, their behavior is consistent with previous results, where NPR outperforms DetectGPT for both top-p and top-k sampling strategies.\nDifferent Temperature. Temperature controls the degree of randomness of the generation process. Increasing the temperature leads to more randomness and creativity, while reducing it leads to more conservation and less novelty. In practice, people adjust temperature for their specific purposes. For example, students might set a high temperature to encourage more original and diverse output when writing a creative essay, whereas fake news producers might set lower temperatures to generate seemingly convincing news articles for their deceptive purposes. Based on our experiments in Table 4, we found that Log Likelihood (log p), Log Rank and LRR is highly sensitive to the temperature and can get even better results than perturbation based methods when the temperature is relatively low. In addition, the performance improvement of Rank method with the increased temperature is negligible compared to Log Likelihood, Log Rank and LRR, while the performance of entropy method seems to be positively correlated to the temperature. We conjure that the abnormal behavior of Entropy method might be because of the assumption \u201cmachine generated text has higher entropy\" (Mitchell et al., 2023), which, from our experi-\nments, doesn\u2019t stand for high temperature. As for perturbation based method, the impact of temperature is not so clear as perturbation-free method. But in general, the results suggest the temperature has only minor effects on DetectGPT while it improves the performance of NPR. Another observation is that, perturbation-free method performs better than perturbation based method in low temperature, for example, for temperature is smaller than 0.95, perturbation based methods get better detection accuracy while being efficient."
        },
        {
            "heading": "6 Analysis of the Efficiency",
            "text": "Though in Table 1, perturbation based methods appear to be significantly better than perturbationfree methods, it is important to note that their superior performances can only be achieved with large perturbation functions and multiple number of perturbations, which leads to intensive demand for computational resources and longer computational time. Thus, while performance is an important factor, it is crucial to consider the efficiency of these zero-shot methods as well."
        },
        {
            "heading": "6.1 Computational Cost Analysis",
            "text": "To get an idea of how costly different zero-shot methods are to achieve their performance in Table 1, we estimated the computational time (per sample) for each zero-shot method (Detailed results can be found in Table 9 of Appendix E). For perturbation based methods, we used 50 perturbations with T5-3b as in the main experiment for the estimation. We observed that the computational time of Log Likelihood, Rank, Log Rank and Entropy are almost the same, while LRR runs approximately 2 times longer than these methods, since it requests both the Log Rank and Log Likelihood statistics. For perturbation based methods, the running time is at least 50 times longer compared to Log Likelihood, Rank, Log Rank, Entropy method, since they\ncalculate the Log Likelihood or Log Rank for not only the target text, but also perturbed samples.\nComposition of the Computational Time. In general, for perturbation-free zero-shot methods, the computational time only depends on the size of LLM and the complexity of statistics. LRR is twice as complex as simple statistics such as Log Rank and Log Likelihood, so it takes approximately twice as long to compute. As for LLM size, intuitively, larger models usually takes more time to compute, which can also be observed in Table 9. The additional computational time of perturbation based methods comes from two folds: (1) The total time for perturbation, which depends on the perturbation function we use and the number of perturbations. (2) The total time for calculating statistics of the perturbed texts, which depends on the number of perturbations, the size of LLM and the complexity of statistics. To reduce the computational time of perturbation based method, we could either choose smaller size of perturbation function or reduce the number of perturbations.\nFormula for Estimating the Computational Time. Let tp be the time of perturbing 1 sample, tm be the time of calculating a simple statistics (such as log likelihood) of one sample for a particular LLM and n be the number of perturbations. The computational time for log likelihood, rank, log rank, entropy is approximately tm, the estimated time for LRR is 2 \u00b7 tm, while the estimated computational time for perturbation based method is n \u00b7 tp + (n+ 1) \u00b7 tm. The estimated value of tp and tm are illustrated in Table 5, which can help us estimate the total running time (in seconds) of different zero-shot methods."
        },
        {
            "heading": "6.2 Balancing Efficiency and Performance",
            "text": "In this subsection, we provide additional experiments on LRR (the best perturbation-free method) and NPR (the best perturbation based method, more\ntime consuming than LRR but also rather satisfactory performance) to provide users some intuition on setting parameters of NPR and choosing among between these two methods according to user\u2019s preference of efficiency and performance.\nFirst, we study the perturbation function used for NPR. Different from Section 5.2, where the focus is to illustrate the advanced performance of NPR compared with DetectGPT, here, we mainly focus on the efficiency performance trade-off perspective and provide some intuition on choosing perturbation functions.\nT5-small and T5-base are not good candidates for perturbation functions. T5-small and T5base are 2 or 3 times faster than larger models such as T5-large (as shown in Table 5), one might wonder if it is possible to trade the saved time with more perturbations for a better performance? We give a negative answer to this. We observe in Figure 3 that using T5-base and T5-small performs worse than LRR even with 50 to 100 perturbations, which suggests that LRR can be at least 50 to 100 times faster while outperform perturbation based methods. So, if the user can only afford T5-small or T5-base as perturbation function, they should choose LRR with no hesitation since it achieves both better efficiency and better performance.\nCost-Effectiveness on More Perturbations and Larger Perturbation Function. In Figure 4, we illustrate the effectiveness of LRR compared to NPR with T5-large and T5-3b as perturbation function respectively, from which, we find that (1) T53b has a higher performance upper limits compared with T5-large. So, if resources are allowed (enough memory and adequate perturbation time), t5-3b would be a better choice, especially for users that prioritize performance. (2) To achieve the same performance as LRR, generally we only need less than 10 perturbations using T5-3b as perturbation function. This estimate could help us choose whether to use NPR or LRR on validation set: setting the number of perturbation to be 10, if LRR outperforms NPR, we would suggest use LRR, otherwise,\nNPR would be a better option. (3) To achieve the same performance, using T5-large takes more than 2 times perturbations than using T5-3b, while the perturbation time using T5-3b is less than twice of the time using T5-large, so using large perturbation functions such as T5-3b is much more efficient than using smaller ones such as T5-large. The only concern is the memory.\nIn summary, when parameterizing perturbation function and the number of perturbations, we suggest using the lager perturbation functions if memory permits, which is more cost-effective: less timeconsuming for achieving the same performance and has a high performance upper limit. In addition, setting the number of perturbation to be 10 would be a good threshold on the validation set to decide whether to use NPR or LRR."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this paper, we proposed two simple but effective zero-shot machine generated text detection methods by leveraging the log rank information. The methods we proposed \u2014LRR and NPR\u2014, achieve state-of-the-art performance within their respective category. In addition, we explored different settings such as decoding strategy and temperatures, as well as different perturbation functions and number of perturbations to better understand the advantages and the disadvantages of different zero-shot methods. Then, we analyzed the computational costs of these methods, and we provided guidance on balancing efficiency and performance."
        },
        {
            "heading": "8 Limitations and Future Directions",
            "text": "One of the limitation of zero-shot methods is the white box assumption that we can have some statistics about the source model. This induces two problems: for close-source models (such as GPT3), these statistics might not have been provided. Moreover, in practice, the detector might have to run the model locally to get the statistics for the purpose of detection, which requires that the detector have enough resources to use the LLM for inference.\nBased on the limitations of zero-shot methods, we consider weak supervised learning (Ratner et al., 2017) as an important direction for future work. Though many papers in MGTD assume knowing the source LLM where the text is generated from, in realistic, the source LLM might be unknown, so it is worth combining weak supervised learning as well as weak supervision sources (other LLMs at hand that might not be the target LLM) to weakly train a classifier. With the flexibility of the weak supervision sources, the limitations of our work could possibly be addressed: (1) Since the weak supervision sources do not have to be from the same target model, there is no need to assume that the target LLM is known. (2) Since the weak supervision sources are classifiers, we could only use statistics that are within reach, or even statistics from other open-source LLMs. (3) The weak supervision sources can be from smaller LLMs, rather than the target LLM, this relaxes the requirement for running an extremely large LLM locally."
        },
        {
            "heading": "A Experimental Details and Baselines",
            "text": "Details on Baselines. We mainly compare the proposed methods with zero-shots methods, which utilize the source model itself to extract distinguishable statistic features, including:\n\u2022 Log Likelihood (log p) (Solaiman et al., 2019): This approach evaluates the average token-wise log probability of the text and classifies text with higher Log Likelihood to be machine generated.\n\u2022 Rank (Gehrmann et al., 2019): This approach evaluates the average rank of each token of the text and classifies text with smaller average rank to be machine generated.\n\u2022 Log Rank (Mitchell et al., 2023): Instead of using the Rank score directly, this approach evaluates the average Log Rank of each token of the text and classifies text with smaller average Log Rank to be machine generated.\n\u2022 Entropy (Gehrmann et al., 2019): This approach is inspired by the hypothesis that machine generated texts are more likely to have over-confident (thus low entropy) predictive distributions. In practice, (Mitchell et al., 2023) discovered that entropy to be positively correlated with passage fakeness, therefore, following their convention, we use high average entropy as a signal of machine generated text.\n\u2022 DetectGPT (Mitchell et al., 2023): DetectGPT is based on the hypothesis that when applying small perturbations to a passage x and produce the perturbed text x\u0303, the quantity log p\u03b8(x) \u2212 log p\u03b8(x\u0303) is relatively larger for machine generated samples than human written one. In practice, the performance of this approach depends heavily on the external perturbation function and the number of perturbations.\nDetails on LLMs used. We used 7 LLMs ranging from 1.5B parameters to 20B parameters in our main experiments.\n\u2022 GPT2-xl (Radford et al., 2019) is the 1.5B parameter version of GPT2 trained on a dataset of 8 million web pages called WebText (Radford et al., 2019), whose objective is to predict the next word given previous words within the text. GPT2-xl surpasses many other language models trained on specific domain (such as books, news, Wikipedia) without using domain-specific training dataset.\n\u2022 GPT-Neo-2.7B (Black et al., 2021) was trained as an autoregressive language model on Pile (Gao et al., 2020) dataset with EleutherAI\u2019s replication of the GPT-3 architecture.\n\u2022 OPT-2.7B and OPT-13B are two models among a collection of decoder-only pre-trained transformers introduced in (Zhang et al., 2022), with the performance roughly match GPT-3 of the same size.\n\u2022 GPT-j-6B (Wang and Komatsuzaki, 2021), which was also trained on Pile (Gao et al., 2020), exhibits zero-shot performance roughly comparable to GPT-3 of comparable size. In addition, the performance gap from GPT-3 of similar size is closer than the GPT-Neo models.\n\u2022 Llama-13b is the 13B parameter model from Llama models (Touvron et al., 2023): a collection of models ranging from 7B to 65B parameters trained with publicly available dataset. Llama-13B outperforms GPT-3 (175B) on most benchmarks, and all the models are released to the research community.\n\u2022 NeoX-20B (Black et al., 2022) is a 20B autoregressive model trained on Pile, whose weights have been released openly to the public.\nExperimental Details. For small models such as GPT2-xl, Neo-2.7, OPT-2.7, GPT-j, we use 1 NVIDIA A100 GPU (with total memory 40G) in our experiments; for larger models such as OPT-13b and Llama-13, we use 3 A100 GPUs (total memory 120 G) while using 4 A100 GPUs (total memory 160 G) for the largest model NeoX-20."
        },
        {
            "heading": "B Supervised Methods",
            "text": "Main results for supervised methods. Comparing Table 1 with Table 6, we found that, on average, our best zero shot method (either LRR on SQuAD dataset or NPR on XSum and WritingPrompts dataset) can exceed supervised model fine-tuned on roberta-base. For larger model roberta-large, only on writing dataset, perturbation-based method DetectGPT and NPR outperforms roberta-large model, by a margin of 0.55% and 2.87% respectively.\nSupervised Method with Different Decoding Strategy. We experimented the 4 models used in zeroshot methods with top-p and top-k decoding strategy for supervised method and found that using top-p decoding strategy performs better than using top-k. (See Table 7). Compared to zero-shot methods, the best zero-shot method NPR can outperform roberta-base model while being comparable to roberta-large model.\nSupervised Method with Different Temperature. Supervised methods also perform better with lower temperature, but zero-shot methods such as Log Rank and Log Likelihood methods might exceed supervised methods in low temperature. Moreover, we found that the performance gap of roberta-base and roberta-large would be narrowed with lower temperature. The results are illustrated in Figure 5."
        },
        {
            "heading": "C Comparing NPR and DetectGPT",
            "text": "Different Number of Perturbations. The results for models smaller than or equal to 13B parameters are shown in Figure 6. For NeoX-20b model, we don\u2019t have enough computation resources to perform 100 perturbations, so we show it separately in Figure 7 with 1, 10, 20, and 50 perturbations. For XSum dataset, NPR and DetectGPT almost coverages with 100 perturbations, but for SQuAD and WritingPrompts dataset, NPR still outperforms DetectGPT even with 100 perturbations. For SQuAD dataset with Llama-13b model, DetectGPT exhibits abnormality while NPR maintains stably improved performance as the number of perturbations increases. In addition, in nearly all the dataset and models, NPR outperforms DetectGPT except GPT-j on XSum dataset, demonstrating the effectiveness of NPR compared to DetectGPT.\n70 75 80 85 90 95 100\n0.5\n0.7\n0.9 0.95 Te m pe ra tu\nre Neo-2.7\n99.29\n99.42\n98.17\n97.85\n99.84\n99.91\n99.80\n99.76 XSum\n70 75 80 85 90 95 100\n0.5\n0.7\n0.9\n0.95\n99.71\n99.24\n97.30\n95.94\n99.66\n99.43\n98.99\n98.52 SQuAD\n70 75 80 85 90 95 100\n0.5\n0.7\n0.9\n0.95\n99.27\n99.78\n97.06\n96.83\n99.70\n99.44\n99.01\n98.98 WritingP\n70 75 80 85 90 95 100\n0.5\n0.7\n0.9\n0.95\nTe m\npe ra\ntu re\nOPT-2.7\n99.38\n98.94\n97.29\n95.78\n99.79\n99.33\n99.19\n98.32\n70 75 80 85 90 95 100\n0.5\n0.7\n0.9\n0.95\n99.89\n99.59\n97.27\n95.32\n99.66\n99.47\n98.50\n97.73\n70 75 80 85 90 95 100\n0.5\n0.7\n0.9\n0.95\n99.03\n98.68\n94.16\n91.97\n99.21\n98.10\n96.52\n95.84\n70 75 80 85 90 95 100\n0.5\n0.7\n0.9\n0.95\nTe m\npe ra\ntu re\nOPT-13\n99.44\n98.26\n92.93\n90.11\n99.67\n98.17\n95.78\n94.85\n70 75 80 85 90 95 100\n0.5\n0.7\n0.9\n0.95\n99.47\n98.44\n93.06\n92.32\n99.35\n98.00\n93.56\n94.43\n70 75 80 85 90 95 100\n0.5\n0.7\n0.9\n0.95\n99.06\n97.67\n90.25\n88.68\n98.51\n96.63\n92.36\n91.14\n70 75 80 85 90 95 100 perfromance(%)\n0.5\n0.7\n0.9\n0.95\nTe m\npe ra\ntu re\nLlama-13\n97.98\n95.01\n85.16\n83.22\n97.65\n94.32\n89.17\n86.31\nroberta-base roberta-large\n70 75 80 85 90 95 100 perfromance(%)\n0.5\n0.7\n0.9\n0.95\n94.97\n93.05\n83.35\n79.40\n94.83\n92.50\n84.75\n82.43\n70 75 80 85 90 95 100 perfromance(%)\n0.5\n0.7\n0.9\n0.95\n98.97\n97.02\n88.22\n85.72\n98.20\n95.52\n90.03\n88.29\nFigure 5: Comparing supervised methods with different temperature (AUROC score).\nUsing T5-large as Perturbation Function. We illustrate the performance of NPR and DetectGPT in Figure 8 with different combination of dataset and LLMs using T5-large as perturbation function. Compared to T5-3b illustrated in Figure 6, the superiority of NPR over DetectGPT becomes more distinct with T5-large being the perturbation function, where in almost all the LLMs, datasets and different number of perturbations (except with Llama-13b on SQuAD), NPR outperforms DetectGPT by a large margin. In addition, we could also observe that NPR achieves comparable or even better result with only 10 perturbations to that of DetectGPT with 100 perturbations, which indicates that NPR is more efficient and can achieve similar level of performance with significantly fewer number of perturbations."
        },
        {
            "heading": "D Alternative Sampling Strategies and Temperature",
            "text": "Different Sampling Strategy. In Table 8, we illustrate the complete results with different zero-shot methods with four LLMs using top-p and top-k sampling. For perturbation based methods, even with different sampling strategy, NPR provides clearer signal for machine generated text detection than DetectGPT. Moreover, we find that although LRR is more stable than Log Rank and Log Likelihood methods: when replacing temperature sampling to top-p and top-k sampling, all the above-mentioned three zero-shot methods\u2019 performance improve, however, LRR improves approximately the same for both\ntop-k and top-p sampling while the other two is more in favor of top-p sampling.\nDifferent Temperature. Here, we investigate how temperature used for machine generated texts affects detection accuracy of different zero-shot methods. From Figure 9, we find that, all the perturbation-free zero-shot methods improved their performance with the decreasing temperature. In particular, for Log Rank and Log Likelihood method, the performance can become extremely high when the temperature drops, even exceeding NPR and achieving approximately 100 points detection accuracy. For example, in Neo-2.7 and OPT-13 with temperature 0.5, log p method and Log Rank method achieves an accuracy of 100 points on WritingPrompts dataset, this prevalent performance can be observed notably in smaller models with relatively high temperature (such as GPT2-xl and Neo-2.7 with high temperature such as 0.7) or in large models with relatively lower temperature such as OPT-13 with temperature 0.5 as we demonstrated in Figure 9. Though we omit entropy method because it gets an accuracy worse than random guessing, one of the observation from our experiments is that, using the assumption \u201cmachine generated text has higher entropy\" suggested in (Mitchell et al., 2023), the performance of entropy method improve with the increasing temperature with absolute accuracy smaller than 50 points, which suggests that for low temperature, we should use the assumption \u201cmachine generated text has lower entropy\" for detection machine generated text. In general, Entropy method performs worse than random and is not an implementable detection method.\nFor perturbation based methods (Figure 10), while DetectGPT does not exhibit a clear trend with respect to temperature, the performance of NPR improves with the decreasing temperature most of the time. However, this trend is not as clearly as Log Rank and log likelihood method, especially when the temperature becomes too low. This behavior suggests that perturbation based method is more suitable for high temperature, while perturbation-free method is more suitable for low temperature."
        },
        {
            "heading": "E Computational Time",
            "text": "The estimated computational time of different zero-shot methods is illustrated in Table 9. The time is estimated over the average of 10 samples. For perturbation based methods, since the time depends on the perturbation function and the number of perturbation, we used T5-3b as perturbation function and use 50 perturbations since this is the setting used for the main results in Table 1, we want to provide an idea of how much more it costs for perturbation based method to achieve exceptional performance in Table 1.\n40 50 60 70 80 90 100\n0.5\n0.7\n0.9 0.95 Te m pe ra tu\nre Neo-2.7\n99.97\n99.63\n95.71\n92.59\n80.02\n78.78\n78.81\n77.84\n100.00\n99.90\n97.25\n94.80\n99.26\n99.52\n96.77\n95.40\nXSum\n40 50 60 70 80 90 100\n0.5\n0.7\n0.9\n0.95\n100.00\n99.60\n94.25\n90.14\n82.17\n81.58\n80.86\n80.18\n100.00\n99.93\n96.95\n93.79\n100.00\n99.84\n98.70\n97.11\nSQuAD\n40 50 60 70 80 90 100\n0.5\n0.7\n0.9\n0.95\n100.00\n100.00\n98.84\n97.69\n84.20\n83.83\n83.06\n83.10\n100.00\n100.00\n99.31\n98.69\n97.96\n98.46\n99.21\n99.06\nWritingP\n40 50 60 70 80 90 100\n0.5\n0.7\n0.9\n0.95\nTe m\npe ra\ntu re\nOPT-2.7\n99.70\n99.35\n94.56\n90.94\n79.04\n78.42\n77.68\n77.32\n99.96\n99.61\n95.63\n92.47\n99.80\n99.56\n94.78\n92.13\n40 50 60 70 80 90 100\n0.5\n0.7\n0.9\n0.95\n99.99\n99.72\n95.10\n92.02\n83.86\n83.04\n82.17\n81.74\n100.00\n99.98\n97.25\n95.15\n99.70\n99.96\n98.50\n97.80\n40 50 60 70 80 90 100\n0.5\n0.7\n0.9\n0.95\n100.00\n99.96\n98.07\n97.23\n85.47\n84.96\n84.33\n84.27\n100.00\n99.99\n98.75\n98.25\n99.32\n99.15\n98.36\n98.21\n40 50 60 70 80 90 100\n0.5\n0.7\n0.9\n0.95\nTe m\npe ra\ntu re\nOPT-13\n99.87\n98.89\n92.41\n87.44\n77.05\n76.40\n74.46\n74.45\n100.00\n99.40\n94.17\n89.39\n99.68\n99.21\n92.79\n87.87\n40 50 60 70 80 90 100\n0.5\n0.7\n0.9\n0.95\n99.93\n98.91\n91.22\n86.26\n80.36\n79.71\n78.33\n78.15\n100.00\n99.61\n93.96\n89.90\n99.62\n99.83\n96.33\n93.68\n40 50 60 70 80 90 100\n0.5\n0.7\n0.9\n0.95\n100.00\n99.90\n97.79\n95.90\n85.33\n84.68\n84.29\n84.58\n100.00\n99.94\n98.41\n96.84\n97.99\n98.82\n97.84\n96.86\n40 50 60 70 80 90 100 perfromance(%)\n0.5\n0.7\n0.9\n0.95\nTe m\npe ra\ntu re\nLlama-13\n98.52\n93.12\n72.08\n63.35\n54.64\n52.81\n50.71\n49.71\n99.50\n96.51\n77.43\n68.74\n99.58\n98.08\n84.74\n78.10\nlog p Rank Log Rank LRR(ours)\n40 50 60 70 80 90 100 perfromance(%)\n0.5\n0.7\n0.9\n0.95\n86.72\n75.42\n56.93\n50.00\n60.73\n59.14\n56.55\n55.47\n92.01\n81.88\n62.68\n55.59\n99.05\n94.90\n79.98\n73.88\n40 50 60 70 80 90 100 perfromance(%)\n0.5\n0.7\n0.9\n0.95\n99.99\n99.60\n93.57\n90.22\n81.53\n80.37\n78.65\n78.38\n100.00\n99.86\n95.59\n92.97\n98.83\n98.69\n96.03\n94.33\nFigure 9: Comparison of perturbation-free methods using different temperature (AUROC score)."
        }
    ],
    "title": "DetectLLM: Leveraging Log Rank Information for Zero-Shot Detection of Machine-Generated Text",
    "year": 2023
}