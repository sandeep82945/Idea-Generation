{
    "abstractText": "Dual encoders are now the dominant architecture for dense retrieval. Yet, we have little understanding of how they represent text, and why this leads to good performance. In this work, we shed light on this question via distributions over the vocabulary. We propose to interpret the vector representations produced by dual encoders by projecting them into the model\u2019s vocabulary space. We show that the resulting projections contain rich semantic information, and draw connection between them and sparse retrieval. We find that this view can offer an explanation for some of the failure cases of dense retrievers. For example, we observe that the inability of models to handle tail entities is correlated with a tendency of the token distributions to forget some of the tokens of those entities. We leverage this insight and propose a simple way to enrich query and passage representations with lexical information at inference time, and show that this significantly improves performance compared to the original model in zero-shot settings, and specifically on the BEIR benchmark.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Ori Ram"
        },
        {
            "affiliations": [],
            "name": "Liat Bezalel"
        },
        {
            "affiliations": [],
            "name": "Adi Zicher"
        },
        {
            "affiliations": [],
            "name": "Yonatan Belinkov"
        },
        {
            "affiliations": [],
            "name": "Jonathan Berant"
        },
        {
            "affiliations": [],
            "name": "Amir Globerson"
        }
    ],
    "id": "SP:7ee6d4a164fb1040d0ce4cb80481dc911a966aa5",
    "references": [
        {
            "authors": [
                "Leonard Adolphs",
                "Michelle Chen Huebscher",
                "Christian Buck",
                "Sertan Girgin",
                "Olivier Bachem",
                "Massimiliano Ciaramita",
                "Thomas Hofmann"
            ],
            "title": "Decoding a neural retriever\u2019s latent space for query suggestion",
            "year": 2022
        },
        {
            "authors": [
                "Yang Bai",
                "Xiaoguang Li",
                "Gang Wang",
                "Chaoliang Zhang",
                "Lifeng Shang",
                "Jun Xu",
                "Zhaowei Wang",
                "Fangshan Wang",
                "Qun Liu"
            ],
            "title": "SparTerm: Learning termbased sparse representation for fast text retrieval",
            "year": 2020
        },
        {
            "authors": [
                "Petr Baudi\u0161",
                "Jan \u0160ediv\u00fd."
            ],
            "title": "Modeling of the question answering task in the YodaQA system",
            "venue": "Proceedings of the 6th International Conference on Experimental IR Meets Multilinguality, Multimodality, and Interaction - Volume 9283, CLEF\u201915, page",
            "year": 2015
        },
        {
            "authors": [
                "Jonathan Berant",
                "Andrew Chou",
                "Roy Frostig",
                "Percy Liang."
            ],
            "title": "Semantic parsing on Freebase from question-answer pairs",
            "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1533\u20131544, Seattle, Wash-",
            "year": 2013
        },
        {
            "authors": [
                "Michele Bevilacqua",
                "Giuseppe Ottaviano",
                "Patrick Lewis",
                "Wen-tau Yih",
                "Sebastian Riedel",
                "Fabio Petroni"
            ],
            "title": "Autoregressive search engines: Generating substrings as document identifiers",
            "year": 2022
        },
        {
            "authors": [
                "Jack Clark",
                "Christopher Berner",
                "Sam McCandlish",
                "Alec Radford",
                "Ilya Sutskever",
                "Dario Amodei."
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2020
        },
        {
            "authors": [
                "Wei-Cheng Chang",
                "Felix X. Yu",
                "Yin-Wen Chang",
                "Yiming Yang",
                "Sanjiv Kumar."
            ],
            "title": "Pre-training tasks for embedding-based large-scale retrieval",
            "venue": "8th International Conference on Learning Representations, ICLR 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Xilun Chen",
                "Kushal Lakhotia",
                "Barlas O\u011fuz",
                "Anchit Gupta",
                "Patrick Lewis",
                "Stan Peshterliev",
                "Yashar Mehdad",
                "Sonal Gupta",
                "Wen-tau Yih"
            ],
            "title": "Salient phrase aware dense retrieval: Can a dense retriever imitate a sparse one",
            "year": 2022
        },
        {
            "authors": [
                "Guy Dar",
                "Mor Geva",
                "Ankit Gupta",
                "Jonathan Berant"
            ],
            "title": "Analyzing transformers in embedding space",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Thibault Formal",
                "Benjamin Piwowarski",
                "St\u00e9phane Clinchant."
            ],
            "title": "SPLADE: Sparse lexical and expansion model for first stage ranking",
            "venue": "Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval,",
            "year": 2021
        },
        {
            "authors": [
                "Mor Geva",
                "Avi Caciularu",
                "Kevin Ro Wang",
                "Yoav Goldberg"
            ],
            "title": "Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space",
            "year": 2022
        },
        {
            "authors": [
                "Mor Geva",
                "Roei Schuster",
                "Jonathan Berant",
                "Omer Levy."
            ],
            "title": "Transformer feed-forward layers are keyvalue memories",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5484\u20135495, Online and Punta Cana,",
            "year": 2021
        },
        {
            "authors": [
                "Gautier Izacard",
                "Mathilde Caron",
                "Lucas Hosseini",
                "Sebastian Riedel",
                "Piotr Bojanowski",
                "Armand Joulin",
                "Edouard Grave."
            ],
            "title": "Unsupervised dense information retrieval with contrastive learning",
            "venue": "Transactions on Machine Learning Research.",
            "year": 2022
        },
        {
            "authors": [
                "Jeff Johnson",
                "Matthijs Douze",
                "Herv\u00e9 J\u00e9gou."
            ],
            "title": "Billion-scale similarity search with GPUs",
            "venue": "IEEE Transactions on Big Data, 7(3):535\u2013547.",
            "year": 2021
        },
        {
            "authors": [
                "Mandar Joshi",
                "Eunsol Choi",
                "Daniel Weld",
                "Luke Zettlemoyer."
            ],
            "title": "TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Vol-",
            "year": 2017
        },
        {
            "authors": [
                "Euna Jung",
                "Jungwon Park",
                "Jaekeol Choi",
                "Sungyoon Kim",
                "Wonjong Rhee"
            ],
            "title": "Isotropic representation can improve dense retrieval",
            "year": 2022
        },
        {
            "authors": [
                "Vladimir Karpukhin",
                "Barlas Oguz",
                "Sewon Min",
                "Patrick Lewis",
                "Ledell Wu",
                "Sergey Edunov",
                "Danqi Chen",
                "Wen-tau Yih."
            ],
            "title": "Dense passage retrieval for opendomain question answering",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural",
            "year": 2020
        },
        {
            "authors": [
                "Omar Khattab",
                "Matei Zaharia."
            ],
            "title": "ColBERT: Efficient and effective passage search via contextualized late interaction over BERT",
            "venue": "Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval,",
            "year": 2020
        },
        {
            "authors": [
                "Uszkoreit",
                "Quoc Le",
                "Slav Petrov."
            ],
            "title": "Natural questions: A benchmark for question answering research",
            "venue": "Transactions of the Association for Computational Linguistics, 7:452\u2013466.",
            "year": 2019
        },
        {
            "authors": [
                "Kenton Lee",
                "Ming-Wei Chang",
                "Kristina Toutanova."
            ],
            "title": "Latent retrieval for weakly supervised open domain question answering",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6086\u20136096, Florence, Italy.",
            "year": 2019
        },
        {
            "authors": [
                "Patrick Lewis",
                "Ethan Perez",
                "Aleksandra Piktus",
                "Fabio Petroni",
                "Vladimir Karpukhin",
                "Naman Goyal",
                "Heinrich K\u00fcttler",
                "Mike Lewis",
                "Wen-tau Yih",
                "Tim Rockt\u00e4schel",
                "Sebastian Riedel",
                "Douwe Kiela"
            ],
            "title": "Retrieval-augmented generation for knowledge",
            "year": 2020
        },
        {
            "authors": [
                "Patrick Lewis",
                "Pontus Stenetorp",
                "Sebastian Riedel."
            ],
            "title": "Question and answer test-train overlap in opendomain question answering datasets",
            "venue": "Proceedings of the 16th Conference of the European Chapter of",
            "year": 2021
        },
        {
            "authors": [
                "Jimmy Lin",
                "Xueguang Ma",
                "Sheng-Chieh Lin",
                "JhengHong Yang",
                "Ronak Pradeep",
                "Rodrigo Nogueira."
            ],
            "title": "Pyserini: A python toolkit for reproducible information retrieval research with sparse and dense representations",
            "venue": "Proceedings of the 44th Inter-",
            "year": 2021
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov"
            ],
            "title": "RoBERTa: A robustly optimized bert pretraining approach",
            "year": 2019
        },
        {
            "authors": [
                "Sean MacAvaney",
                "Sergey Feldman",
                "Nazli Goharian",
                "Doug Downey",
                "Arman Cohan."
            ],
            "title": "ABNIRML: Analyzing the behavior of neural IR models",
            "venue": "Transactions of the Association for Computational Linguistics, 10:224\u2013239.",
            "year": 2022
        },
        {
            "authors": [
                "Yuning Mao",
                "Pengcheng He",
                "Xiaodong Liu",
                "Yelong Shen",
                "Jianfeng Gao",
                "Jiawei Han",
                "Weizhu Chen."
            ],
            "title": "Generation-augmented retrieval for opendomain question answering",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Compu-",
            "year": 2021
        },
        {
            "authors": [
                "Niklas Muennighoff",
                "Nouamane Tazi",
                "Lo\u00efc Magne",
                "Nils Reimers"
            ],
            "title": "MTEB: Massive text embedding benchmark",
            "year": 2022
        },
        {
            "authors": [
                "Krueger",
                "David Schnurr",
                "Felipe Petroski Such",
                "Kenny Hsu",
                "Madeleine Thompson",
                "Tabarak Khan",
                "Toki Sherbakov",
                "Joanne Jang",
                "Peter Welinder",
                "Lilian Weng"
            ],
            "title": "Text and code embeddings by contrastive pre-training",
            "year": 2022
        },
        {
            "authors": [
                "Jianmo Ni",
                "Chen Qu",
                "Jing Lu",
                "Zhuyun Dai",
                "Gustavo Hern\u00e1ndez \u00c1brego",
                "Ji Ma",
                "Vincent Y. Zhao",
                "Yi Luan",
                "Keith B. Hall",
                "Ming-Wei Chang",
                "Yinfei Yang"
            ],
            "title": "Large dual encoders are generalizable retrievers",
            "year": 2022
        },
        {
            "authors": [
                "Haifeng Wang."
            ],
            "title": "RocketQA: An optimized training approach to dense passage retrieval for opendomain question answering",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
            "year": 2021
        },
        {
            "authors": [
                "Alec Radford",
                "Jeff Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "year": 2019
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Jian Zhang",
                "Konstantin Lopyrev",
                "Percy Liang."
            ],
            "title": "SQuAD: 100,000+ questions for machine comprehension of text",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383\u20132392, Austin,",
            "year": 2016
        },
        {
            "authors": [
                "Ori Ram",
                "Gal Shachaf",
                "Omer Levy",
                "Jonathan Berant",
                "Amir Globerson."
            ],
            "title": "Learning to retrieve passages without supervision",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
            "year": 2022
        },
        {
            "authors": [
                "Revanth Gangi Reddy",
                "Vikas Yadav",
                "Md Arafat Sultan",
                "Martin Franz",
                "Vittorio Castelli",
                "Heng Ji",
                "Avirup Sil"
            ],
            "title": "Towards robust neural retrieval models with synthetic pre-training",
            "year": 2021
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Stephen Robertson",
                "Hugo Zaragoza."
            ],
            "title": "The probabilistic relevance framework: BM25 and beyond",
            "venue": "Found. Trends Inf. Retr., 3(4):333\u2013389.",
            "year": 2009
        },
        {
            "authors": [
                "Joseph Rocchio."
            ],
            "title": "Relevance feedback in information retrieval",
            "venue": "The SMART retrieval system: experiments in automatic document processing, pages 313\u2013323.",
            "year": 1971
        },
        {
            "authors": [
                "Christopher Sciavolino",
                "Zexuan Zhong",
                "Jinhyuk Lee",
                "Danqi Chen."
            ],
            "title": "Simple entity-centric questions challenge dense retrievers",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6138\u20136148, Online",
            "year": 2021
        },
        {
            "authors": [
                "Kaitao Song",
                "Xu Tan",
                "Tao Qin",
                "Jianfeng Lu",
                "TieYan Liu."
            ],
            "title": "MPNet: Masked and permuted pretraining for language understanding",
            "venue": "Advances in Neural Information Processing Systems, volume 33, pages 16857\u201316867. Curran Associates, Inc.",
            "year": 2020
        },
        {
            "authors": [
                "Karen Sparck Jones."
            ],
            "title": "A statistical interpretation of term specificity and its application in retrieval",
            "venue": "Journal of Documentation, volume 28 no. 1, pages 11\u201321.",
            "year": 1972
        },
        {
            "authors": [
                "Yi Tay",
                "Vinh Q. Tran",
                "Mostafa Dehghani",
                "Jianmo Ni",
                "Dara Bahri",
                "Harsh Mehta",
                "Zhen Qin",
                "Kai Hui",
                "Zhe Zhao",
                "Jai Gupta",
                "Tal Schuster",
                "William W. Cohen",
                "Donald Metzler."
            ],
            "title": "Transformer memory as a differentiable search index",
            "venue": "Advances in Neural",
            "year": 2022
        },
        {
            "authors": [
                "Nandan Thakur",
                "Nils Reimers",
                "Andreas R\u00fcckl\u00e9",
                "Abhishek Srivastava",
                "Iryna Gurevych."
            ],
            "title": "BEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models",
            "venue": "Proceedings of the Neural Information Processing Systems Track on",
            "year": 2021
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141 ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems 30, pages 5998\u20136008.",
            "year": 2017
        },
        {
            "authors": [
                "Ellen M. Voorhees."
            ],
            "title": "Query expansion using lexical-semantic relations",
            "venue": "Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR \u201994, page 61\u201369, Berlin, Heidelberg.",
            "year": 1994
        },
        {
            "authors": [
                "Teven Le Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander Rush."
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System",
            "year": 2020
        },
        {
            "authors": [
                "Le Zhao",
                "Jamie Callan."
            ],
            "title": "Automatic term mismatch diagnosis for selective query expansion",
            "venue": "Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR \u201912, page 515\u2013524, New",
            "year": 2012
        },
        {
            "authors": [
                "Thakur"
            ],
            "title": "The license and number of test example in each of the datasets used in the paper. in our work",
            "venue": "For the BEIR benchmark,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Dense retrieval models based on neural text representations have proven very effective (Karpukhin et al., 2020; Qu et al., 2021; Ram et al., 2022; Izacard et al., 2022a,b), improving upon strong traditional sparse models like BM25 (Robertson and Zaragoza, 2009). However, when applied off-theshelf (i.e., in out-of-domain settings) they often experience a severe drop in performance (Thakur et al., 2021; Sciavolino et al., 2021; Reddy et al., 2021). Moreover, the reasons for such failures are poorly understood, as the information captured in their representations remains under-investigated.\n\u2217Supported by the Viterbi Fellowship in the Center for Computer Engineering at the Technion.\n1Our code is publicly available at https://github. com/oriram/dense-retrieval-projections.\nIn this work, we present a new approach for interpreting and reasoning about dense retrievers, through distributions induced by their query2 and passage representations when projected to the vocabulary space, namely distributions over their vocabulary space (Figure 1). Such distributions enable a better understanding of the representational nature of dense models and their failures, which paves the way to simple solutions that improve their performance.\n2Throughout the paper, we use query and question interchangeably.\nar X\niv :2\n21 2.\n10 38\n0v 2\n[ cs\n.C L\n] 2\n4 M\nay 2\n02 3\nWe begin by showing that dense retrieval representations can be projected to the vocabulary space, by feeding them through the masked language modeling (MLM) head of the pretrained model they were initialized from without any further training. This operation results in distributions over the vocabulary, which we refer to as query vocabulary projections and passage vocabulary projections.\nSurprisingly, we find these projections to be highly interpretable to humans (Figure 2; Table 1). We analyze these projections and draw interesting connections between them and well-known concepts from sparse retrieval (\u00a75). First, we highlight the high coverage of tokens shared by the query and the passage in the top-k of their projections. This obersvation suggests that the lexical overlap between query and passages plays an important role in the retrieval mechanism. Second, we show that vocabulary projections of passages they are likely to contain words that appear in queries about the given passage. Thus, they can be viewed as predicting the questions one would ask about the passage. Last, we show that the model implicitly implements query expansion (Rocchio, 1971). For example, in Figure 2 the query is \u201cHow many judges currently serve on the Supreme court?\u201d, and the words in the query projection Q include \u201cjustices\u201d (the common way to refer to them) and \u201cnine\u201d (the correct answer).\nThe above findings are especially surprising due to the fact that these retrieval models are fine-tuned\nin a contrastive fashion, and thus do not perform any prediction over the vocabulary or make any use of their language modeling head during finetuning. In addition, these representations are the result of running a deep transformer network that can implement highly complex functions. Nonetheless, model outputs remain \u201cfaithful\u201d to the original lexical space learned during pretraining.\nWe further show that our approach is able to shed light on the reasons for which dense retrievers struggle with simple entity-centric questions (Sciavolino et al., 2021). Through the lens of vocabulary projections, we identify an interesting phenomenon: dense retrievers tend to \u201cignore\u201d some of the tokens appearing in a given passage. This is reflected in the ranking assigned to such tokens in the passage projection. For example, the word \u201cmichael\u201d in the bottom example of Figure 2 is ranked relatively low (even though it appears in the passage title), thereby hindering the model from retrieving this passage. We refer to this syndrome as token amnesia (\u00a76).\nWe leverage this insight and suggest a simple inference-time fix that enriches dense representations with lexical information, addressing token amnesia. We show that lexical enrichment significantly improves performance compared to vanilla models on the challenging BEIR benchmark (Thakur et al., 2021) and additional datasets. For example, we boost the performance of the strong MPNet model on BEIR from 43.1% to 44.1%.\nTaken together, our analyses and results demon-\nstrate the great potential of vocabulary projections as a framework for more principled research and development of dense retrieval models."
        },
        {
            "heading": "2 Background",
            "text": "In this work, we suggest a simple framework for interpreting dense retrieves, via projecting their representations to the vocabulary space. This is done using the (masked) language modeling head of their corresponding pretrained model. We begin by providing the relevant background."
        },
        {
            "heading": "2.1 Masked Language Modeling",
            "text": "Most language models based on encoder-only transformers (Vaswani et al., 2017) are pretrained using some variant of the masked language modeling (MLM) task (Devlin et al., 2019; Liu et al., 2019; Song et al., 2020), which involves masking some input tokens, and letting the model reconstruct them.\nSpecifically, for an input sequence x1, ..., xn, the transformer encoder is applied to output contextualized token representations h1, ...,hn \u2208 Rd. Then, to predict the missing tokens, an MLM head is applied to their contextualized representations. The MLM head is a function that takes a vector h \u2208 Rd as input and returns a distribution P over the model\u2019s vocabulary V , defined as follows:\nMLM-Head(h)[i] = exp(v\u22a4i g(h))\u2211 j\u2208V exp(v \u22a4 j g(h)) (1)\ng : Rd \u2192 Rd is a potentially non-linear function (e.g., a fully connected layer followed by a LayerNorm for BERT; Devlin et al. 2019), and vi \u2208 Rd corresponds to the static embedding of the i-th item in the vocabulary."
        },
        {
            "heading": "2.2 Dense Retrieval",
            "text": "In dense retrieval, we are given a corpus of passages C = {p1, ..., pm} and a query q (e.g., a question or a fact to check), and we wish to compute query and passage representations (eq and ep, respectively) such that similarity in this space implies high relevance of a passage to the query. Formally, let EncQ be a query encoder and EncP a passage encoder. These encoders are mappings from the input text to a vector in Rd, and are obtained by fine-tuning a given LLM. Specifically, they return a pooled version of the LLM contextualized embeddings (e.g., the [CLS] embedding or mean pooling). We denote the embedding of the query and passage\nvectors as follows:\neq = EncQ(q) ep = EncP (p) (2)\nTo fine-tune retrievers, a similarity measure s(q, p) is defined (e.g., the dot-product between eq and eq or their cosine similarity) and the model is trained in a contrastive manner to maximize retriever accuracy (Lee et al., 2019; Karpukhin et al., 2020). Importantly, in this process, the MLM head function does not change at all."
        },
        {
            "heading": "3 Vocabulary Projections",
            "text": "We now describe our framework for projecting query and passage representations of dense retrievers to the vocabulary space. Given a dense retrieval model, we utilize the MLM head of the model it was initialized from to map from encoder output representations to distributions over the vocabulary (Eq. 1). For example, for DPR (Karpukhin et al., 2020) we take BERT\u2019s MLM head, as DPR was initialized from BERT. Given a query q, we use the query encoder EncQ to obtain its representation eq as in Eq. 2. Similarly, for a passage p we apply the passage encoder EncP to get ep. We then apply the MLM head as in Eq. (1) to obtain the vocabulary projection:\nQ = MLM-Head(eq) P = MLM-Head(ep) (3)\nNote that it is not clear a-priori that Q and P will be meaningful in any way, as the encoder model has been changed since pretraining, while the MLMhead function remains fixed. Moreover, the MLM function has not been trained to decode \u201cpooled\u201d sequence-level representations (i.e., the results of CLS or mean pooling) during pretraining. Despite this intuition, in this work we argue that P and Q are actually highly intuitive and can facilitate a better understanding of dense retrievers."
        },
        {
            "heading": "4 Experiment Setup",
            "text": "To evaluate our framework and method quantitatively, we consider several dense retrieval models and datasets."
        },
        {
            "heading": "4.1 Models",
            "text": "We now list the retrievers used to demonstrate our framework and method. All dense models share the same architecture and size (i.e., that of BERTbase; 110M parameters), and all were trained in\nQuestion top-20 in Q Passage top-20 in P\nwhere do the great lakes meet the ocean (A: the saint lawrence river) lakes lake shore ocean confluence river water north canada meet east land rivers canoe sea border michigan connecting both shores the great lakes , also called the laurent ##ian great lakes and the great lakes of north america , are a series of inter ##connected freshwater lakes located primarily in the upper mid - east region of north america , on the canada \u2013 united states border , which connect to the atlantic ocean through the saint lawrence river . they consist of lakes superior , michigan , huron ... lakes lake the canada great freshwater water region ontario these central river rivers large basin core area erie all four\nsouthern soul was considered the sound of what independent record label (A: motown) southern music label soul motown blues nashville vinyl sound independent labels country records genre dixie record released gospel jazz south soul music . the key sub ##gen ##res of soul include the detroit ( motown ) style , a rhythmic music influenced by gospel ; \" deep soul \" and \" southern soul \" , driving , energetic soul styles combining r & b with southern gospel music sound ; ... which came out of the rhythm and blues style ... soul music jazz funk blues rock musical fusion genre black pure classical genres pop southern melody art like rich urban\nwho sings does he love me with re ##ba (A: linda davis) duet song love music solo re he motown me his \" pa album songs honey reprise bobby i peggy blues \" does he love you \" is a song written by sandy knox and billy st ##rit ##ch , and recorded as a duet by american country music artists re ##ba mc ##ent ##ire and linda davis ... he you him i it she his john we love paul who me does did yes why they how this\nTable 1: Examples of questions and gold passages from the development set of Natural Questions, along with their 20 top-scored tokens in projections of DPR representations. Green tokens represent the lexical overlap signal (i.e., tokens that appear in both the question and the passage). Blue tokens represent query expansion (i.e., tokens that do not appear in the question but do appear in the passage).\na contrastive fashion with in-batch negatives\u2014the prominent paradigm for training dense models (Lee et al., 2019; Karpukhin et al., 2020; Chang et al., 2020; Qu et al., 2021; Ram et al., 2022; Izacard et al., 2022a; Ni et al., 2022; Chen et al., 2022). For the analysis, we use DPR (Karpukhin et al., 2020) and BERT (Devlin et al., 2019) as its pretrained baseline. For the results of our method, we also use S-MPNet (Reimers and Gurevych, 2019) and Spider (Ram et al., 2022). Our sparse retrieval model is BM25 (Robertson and Zaragoza, 2009). We refer the reader to App. A for more details."
        },
        {
            "heading": "4.2 Datasets",
            "text": "We follow prior work (Karpukhin et al., 2020; Ram et al., 2022) and consider six common open-domain question answering (QA) datasets for the evaluation of our framework: Natural Questions (NQ; Kwiatkowski et al. 2019), TriviaQA (Joshi et al., 2017), WebQuestions (WQ; Berant et al. 2013), CuratedTREC (TREC; Baudi\u0161 and \u0160ediv\u00fd 2015), SQuAD (Rajpurkar et al., 2016) and EntityQuestions (EntityQs; Sciavolino et al. 2021). We also consider the BEIR (Thakur et al., 2021) and the MTEB (Muennighoff et al., 2022) benchmarks."
        },
        {
            "heading": "4.3 Implementation Details",
            "text": "Our code is based on the official repository of DPR (Karpukhin et al., 2020), built on Hugging Face\nTransformers (Wolf et al., 2020). For the six QA datasets, we use the Wikipedia corpus standardized by Karpukhin et al. (2020), which contains roughly 21 million passages of a hundred words each. For dense retrieval over this corpus, we apply exact search using FAISS (Johnson et al., 2021). For sparse retrieval we use Pyserini (Lin et al., 2021)."
        },
        {
            "heading": "5 Analyzing Dense Retrievers via Vocabulary Projections",
            "text": "In Section 3, we introduce a new framework for interpreting representations produced by dense retrievers. Next, we describe empirical findings that shed new light on what is encoded in these representations. Via vocabulary projections, we draw connections between dense retrieval and well-known concepts from sparse retrieval like lexical overlap (\u00a75.1), query prediction (\u00a75.2) and query expansion (\u00a75.3)."
        },
        {
            "heading": "5.1 The Dominance of Lexical Overlap",
            "text": "Tokens shared by questions and their corresponding gold passages constitute the lexical overlap signal in retrieval, used by sparse models like BM25. We start by asking: how prominent are they in vocabulary projections? Figure 3 illustrates the coverage of these tokens in Q and P for DPR after training, compared to its initialization before training\n(i.e., BERT with mean or CLS pooling). In other words, for each k we check what is the percentage of shared tokens ranked in the top-k of Q and P . Results suggest that after training, the model learns to rank shared tokens much higher than before. Concretely, 63% and 53% of the shared tokens appear in the top-20 tokens of Q and P respectively, compared to only 16% and 8% in BERT (i.e., before training). These numbers increase to 78% and 69% of the shared tokens that appear in the top-100 tokens of Q and P . In addition, we observed that for 71% of the questions, the topscored token in Q appears in both the question and the passage (App. B). These findings suggest that even for dense retrievers\u2014which do not operate at the lexical level\u2014lexical overlap remains a highly dominant signal."
        },
        {
            "heading": "5.2 Passage Encoders as Query Prediction",
            "text": "Our next analysis concerns the role of passage encoders. In \u00a75.1, we show that tokens shared by the question and its gold passage are ranked high in both Q and P . However, passages contain many tokens, and the shared tokens constitute only a small fraction of all tokens. We hypothesize that out of passage tokens, those that are likely to appear in relevant questions receive higher scores in P than others. If this indeed the case, it implies that passage encoders implicitly learn to predict which of the passage tokens will appear in relevant questions. To test our hypothesis, we analyze the ranks\nrankP (t) .\nof question and passage tokens in passage vocabulary projections, P . Formally, let Tq and Tp be the sets of tokens in a question q and its gold passage p, respectively. Table 2 shows the token-level mean reciprocal rank (MRR) of these sets in P . We observe that tokens shared by q and p (i.e., Tq \u2229 Tp) are ranked significantly higher than other passage tokens (i.e., Tp). For example, in DPR the MRR of shared tokens is 26.1, while that of other passage tokens is only 3.0. In addition, the MRR of shared tokens in BERT is only 1.4. These findings support our claim that tokens that appear in relevant questions are ranked higher than others, and that this behavior is acquired during fine-tuning."
        },
        {
            "heading": "5.3 Query Encoders Implement Query Expansion",
            "text": "To overcome the \u201cvocabulary mismatch\u201d problem (i.e., when question-document pairs are semantically relevant, but lack significant lexical overlap), query expansion methods have been studied extensively (Rocchio, 1971; Voorhees, 1994; Zhao and Callan, 2012; Mao et al., 2021). The main idea is to expand the query with additional terms that will better guide the retrieval process. We define a token as a query expansion if it does not appear in the query itself but does appear in the query projection Q, and also in the gold passage of that query p (excluding stop words and punctuation marks). Figure 4 shows the percentage of queries with at least one query expansion token in the top-k as a function of k for DPR and the BERT baseline (i.e., before DPR training). We observe that after training, the model promotes query expansion tokens to higher ranks than before. In addition, we found that almost 14% of the tokens in the top-5 of Q are query expansion tokens (cf. App B).\nWe note that there are two interesting classes of query expansion tokens: (1) synonyms of ques-\ntion tokens, as well as tokens that share similar semantics with tokens in q (e.g., \u201cmichigan\u201d in the first example of Table 1). (2) \u201canswer tokens\u201d which contain the answer to the query (e.g., \u201cmotown\u201d in the second example of Table 1). The presence of such tokens may suggest the model already \u201cknows\u201d the answer to the given question, either from pretraining or from similar questions seen during training (Lewis et al., 2021).\nGiven these findings, we conjecture that the model \u201cuses\u201d these query expansion tokens to introduce a semantic signal to the retrieval process."
        },
        {
            "heading": "6 Token Amnesia",
            "text": "The analysis in Section 5 shows that vocabulary projections of passages (i.e., P ) predict which of the input tokens are likely to appear in relevant questions. However, in some cases these predictions utterly fail. For example, in Figure 2 the token \u201cmichael\u201d is missing from the top-k of the passage projection P . We refer to such cases as token amnesia. Here we ask, do these failure in query prediction hurt retrieval?\nNext, we demonstrate that token amnesia indeed correlates with well-known failures of dense retrievers (\u00a76.1). To overcome this issue, we suggest a lexical enrichment procedure for dense representations (\u00a76.2) and demonstrate its effectiveness on downstream retrieval performance (\u00a76.3)."
        },
        {
            "heading": "6.1 Token Amnesia is Correlated with Retriever Failures",
            "text": "Dense retrievers have shown difficulties in out-ofdomain settings (Sciavolino et al., 2021; Thakur et al., 2021), where even sparse models like BM25 significantly outperform them. We now offer an intuitive explanation to these failures via token amnesia. We focus on setups where BM25 outperforms dense models and ask: why do dense retrievers fail to model lexical overlap signals? To answer this question, we consider subsets of NQ and EntityQs where BM25 is able to retrieve the correct passage in its top-5 results. We focus on these subsets as they contain significant lexical overlap between questions and passages (by definition, as BM25 successfully retrieved the correct passage). Let q be a question and p the passage retrieved by BM25 for q, and Q and P be their corresponding vocabulary projections for some dense retriever. Also, let T \u2286 V be the set of tokens that appear in both q and p (excluding stop words). Figure 5 shows the maximum (i.e., lowest) rank of tokens from T in the distributions P (left) and Q (right) as a function of whether DPR is able to retrieve this passage (i.e., the rank of p in the retrieval results of DPR). Indeed, the median max-rank over questions for which DPR succeeds to fetch p in its top-5 results (blue box) is much lower than that of questions for which DPR fails to retrieve the passage (red box). As expected (due to the fact that questions contain less tokens than passages), the ranks of shared tokens in question projections Q are much higher. However, the trend is present in Q as well. Additional figures (for EntityQs; as well as median ranks instead of max ranks) are given in App. C.\nOverall, these findings indicate a correlation between token amnesia and failures of DPR. Next, we introduce a method to address token amnesia in dense retrievers, via lexical enrichment of dense representations."
        },
        {
            "heading": "6.2 Method: Lexical Enrichment",
            "text": "As suggested by the analysis in \u00a76.1, dense retrievers have the tendency to ignore some of their input tokens. We now leverage this insight to improve these models. We refer to our method as lexical enrichment (LE) because it enriches text encodings with specific lexical items.\nIntuitively, a natural remedy to the \u201ctoken amnesia\u201d problem is to change the retriever encoding such that it does include these tokens. For example,\nassume the query q is \u201cWhere was Michael Jack born?\u201d and the corresponding passage p contains the text \u201cMichael Jack was born in Folkestone, England\u201d. According to Figure 2, the token \u201cmichael\u201d is ranked relatively low in P , and DPR fails to retrieve the correct passage p. We would like to modify the passage representation ep and get an enriched version e\u2032p that does have this token in its top-k projected tokens, while keeping most of the other projected tokens intact. This is our goal in LE, and we next describe the approach. We focus on enrichment of passage representations, as query enrichment works similarly. We first explain how to enrich representations with a single token, and then extend the process to multiple tokens.\nSingle-Token Enrichment Assume we want to enrich a passage representation ep with a token t (e.g., t = \u201cmichael\u201d in the above example). If there were no other words in the passage, we\u2019d simply want to find an embedding such that feeding it into the MLM would produce t as the top token.3 We refer to this embedding as the single-token enrichment of t, denote it by st and define it as:4\nst = argmax s\u0302 logMLM-Head(s\u0302)[t] (4)\n3Note that feeding the token input embedding vt does not necessarily produce t as the top token, as the MLM head applies a non-linear function g (Eq. 1).\n4This is equivalent to the cross-entropy loss between a one-hot vector on t and the output distribution MLM(s\u0302).\nIn order to approximately solve the optimization problem in Eq. 4 for each t in the vocabulary, we use Adam with a learning rate of 0.01.5 We stop when a (cross-entropy) loss threshold of 0.1 is reached for all tokens. We then apply whitening (Jung et al., 2022), which was proven effective for dense retrieval.\nMulti-Token Enrichment Now suppose we have an input x (either a question or a passage) and we\u2019d like to enrich its representation with its tokens x = [x1, .., xn], such that rare tokens are given higher weights than frequent ones (as in BM25). Then, we simply take its original representation ex and add to it a weighted sum of the single-token enrichments (Eq. 4). Namely, we define:\nelexx = 1\nn n\u2211 i=1 wxisxi\ne\u2032x = ex + \u03bb \u00b7 elexx\n||elexx ||\n(5)\nHere \u03bb is a hyper-parameter chosen via cross validation. We use the inverse document frequency (Sparck Jones, 1972) of tokens as their weights: wxi = IDF(xi). The relevance score is then defined on the enriched representations.\n5For S-MPNet, we used a learning rate of 10\u22123."
        },
        {
            "heading": "6.3 Results",
            "text": "Our experiments demonstrate the effectiveness of our method for multiple models, especially in zeroshot settings. Table 3 shows the results of several models with and without our enrichment method, LE. Additional results are given in App. D. The results demonstrate the effectiveness of LE when added to all baseline models. Importantly, our method improves the performance of S-MPNet\u2014 the best base-sized model on the MTEB benchmark to date (Muennighoff et al., 2022)\u2014on MTEB and BEIR by 1.1% and 1.0%, respectively. When considering EntityQs (on which dense retrievers are known to struggle), we observe significant gains across all models, and S-MPNet and Spider obtain higher accuracy than BM25 that operates on the same textual units (i.e., BM25 with BERT vocabulary). This finding indicates that they are able to integrate semantic information (from the original representation) with lexical signals. Yet, vanilla BM25 is still better than LE models on EntityQs and SQuAD, which prompts further work on how to incorporate lexical signals in dense retrieval. Overall, it is evident that LE improves retrieval accuracy compared to baseline models for all models and datasets (i.e., zero-shot setting)."
        },
        {
            "heading": "6.4 Ablation Study",
            "text": "We carry an ablation study to test our design choices from \u00a76.2. We evaluate four elements of our method: (1) The use of IDF to highlight rare tokens, (2) Our approach for deriving single-token representations, (3) The use of whitening, and (4) The use of unit normalization.\nIDF In our method, we create lexical representations of questions and passages, elexx . These lexical representations are the average of token embeddings, each multiplied by its token\u2019s IDF. We validate that IDF is indeed necessary \u2013 Table 4 demonstrates that setting wxi = 1 in Eq. 5 leads to a significant degradation in performance on EntityQs. For example, top-20 retrieval accuracy drops from 65.2% to 57.7%.\nSingle-Token Enrichment Eq. 4 defines our single-token enrichment: for each item in the vocabulary v \u2208 V , we find an embedding which gives a one-hot vector peaked at v when fed to the MLM head. We confirm that this is necessary by replacing Eq. 4 with the static embeddings of the pretrained model (e.g., BERT in the case of DPR). We find that our approach significantly improves over BERT\u2019s embeddings on EntityQs (e.g., the margin in top-20 accuracy is 3.4%).\nWhitening & Normalization Last, we experiment with removing the whitening and \u21132 normalization. It is evident that they are both necessary, as removing either of them causes a dramatic drop in performance (3.8% and 2.2% in top-20 accuracy on EntityQs, respectively)."
        },
        {
            "heading": "7 Related Work",
            "text": "Projecting representations and model parameters to the vocabulary space has been studied previously mainly in the context of language models. The approach was initially explored by nostalgebraist (2020). Geva et al. (2021) showed that feedforward layers in transformers can be regarded as\nkey-value memories, where the value vectors induce distributions over the vocabulary. Geva et al. (2022) view the token representations themselves as inducing such distributions, with feed-forward layers \u201cupdating\u201d them. Dar et al. (2022) suggest to project all transformer parameters to the vocabulary space. Dense retrieval models, however, do not have any language modeling objective during fine-tuning, yet we show that their representations can still be projected to the vocabulary.\nDespite the wide success of dense retrievers recently, interpreting their representations remains under-explored. MacAvaney et al. (2022) analyze neural retrieval models (not only dense retrievers) via diagnostic probes, testing characteristics like sensitivity to paraphrases, styles and factuality. Adolphs et al. (2022) decode the query representations of neural retrievers using a T5 decoder, and show how to \u201cmove\u201d in representation space to decode better queries for retrieval.\nLanguage models (and specifically MLMs) have been used for sparse retrieval in the context of termweighting and lexical expansion. For example, Bai et al. (2020) and Formal et al. (2021) learn such functions over BERT\u2019s vocabulary space. We differ by showing that dense retrievers implicitly operate in that space as well. Thus, these approaches may prove effective for dense models as well. While we focus in this work on dense retrievers based on encoder-only models, our framework is easily extendable for retrievers based on autoregressive decoder-only (i.e., left-to-right) models like GPT (Radford et al., 2019; Brown et al., 2020), e.g., Neelakantan et al. (2022) and Muennighoff (2022)."
        },
        {
            "heading": "8 Conclusion",
            "text": "In this work, we explore projecting query and passage representations obtained by dense retrieval to the vocabulary space. We show that these projections facilitate a better understanding of the mechanisms underlying dense retrieval, as well as their failures. We also demonstrate how projections can help improve these models. This understanding is likely to help in improving retrievers, as our lexical enrichment approach demonstrates.\nLimitations\nWe point to several limitations of our work. First, our work considers a popular family of models referred to as \u201cdense retrievers\u201d, but other approaches for retrieval include sparse retrievers (Robertson and Zaragoza, 2009; Bai et al., 2020; Formal et al., 2021), generative retrievers (Tay et al., 2022; Bevilacqua et al., 2022), late-interaction models (Khattab and Zaharia, 2020), inter alia. While our work draws interesting connections between dense and sparse retrieval, our main focus is on understanding and improving dense models. Second, all three dense models we analyze are bidirectional and were trained in a contrastive fashion. While most dense retrievers indeed satisfy these properties, there are works that suggested other approaches, both in terms of other architectures (Muennighoff, 2022; Neelakantan et al., 2022; Ni et al., 2022) and other training frameworks (Lewis et al., 2020; Izacard et al., 2022b). Last, while our work introduces new ways to interpret and analyze dense retrieval models, we believe our work is the tip of the iceberg, and there is still much work to be done in order to gain a full understanding of these models.\nEthics Statement\nRetrieval systems have the potential to mitigate serious problems caused by language models, like factual inaccuracies. However, retrieval failures may lead to undesirable behavior of downstream models, like wrong answers in QA or incorrect generations for other tasks. Also, since retrieval models are based on pretrained language models, they may suffer from similar biases."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank Ori Yoran, Yoav Levine, Yuval Kirstain, Mor Geva and the anonymous reviewers for their valuable feedback. This project was funded by the European Research Council (ERC) under the European Unions Horizon 2020 research and innovation programme (grant ERC HOLI 819080), the Blavatnik Fund, the Alon Scholarship, the Yandex Initiative for Machine Learning, Intel Corporation, ISRAEL SCIENCE FOUNDATION (grant No. 448/20), Open Philanthropy, and an Azrieli Foundation Early Career Faculty Fellowship."
        },
        {
            "heading": "A Models: Further Details",
            "text": "DPR (Karpukhin et al., 2020) is a dense retriever that was trained on Natural Questions (Kwiatkowski et al., 2019). It was initialized from BERT-base (Devlin et al., 2019). Thus, we use the public pretrained MLM head of BERT-base to project DPR representations.\nBERT (Devlin et al., 2019) We use BERT for dense retrieval, mainly as a baseline for DPR, as DPR was initialized from BERT. This allows us to track where behaviors we observe stem from: pretraining or retrieval fine-tuning. We use both CLS and mean pooling for BERT.\nS-MPNet is a supervised model trained for Sentence Transformers (Reimers and Gurevych, 2019) using many available datasets for retrieval, sentence similarity, inter alia. It uses cosine similarity, rather than dot product, for relevance scores. It was initialized from MPNet-base (Song et al., 2020), and thus we use this model\u2019s MLM head.\nSpider (Ram et al., 2022) is an unsupervised dense retriever trained using the recurring span retrieval pretraining task. It was also initialized from BERT-base, and we therefore use the same MLM head for projection as the one used for DPR.\nBM25 (Robertson and Zaragoza, 2009) is a lexical model based on tf-idf. We use two variants of BM25: (1) vanilla BM25, and (2) BM25 over BERT/MPNet tokens (e.g., \u201cReba\u201d \u2192 \u201cre ##ba\u201d).6 We consider this option to understand whether the advantages of BM25 stem from its use of different word units from the transformer models."
        },
        {
            "heading": "B Analysis: Further Results",
            "text": "Figure 6 gives an analysis of the top-k tokens in the question projection Q and passage projection P ."
        },
        {
            "heading": "C Token Amnesia: Further results",
            "text": "Figure 7 gives further analyses of token amnesia: It contains the results for EntityQuestions, as well as analysis of median ranks in addition to max ranks (complements Figure 5)."
        },
        {
            "heading": "D Lexical Enrichment: Further Results",
            "text": "Table 9 gives the results of our method on the BEIR and MTEB benchmarks for all 19 datasets (complements Table 3). Table 6, Table 7 and Table 8 give the zero-shot results for k \u2208 {1, 5, 100}, respectively (complement Table 3)."
        },
        {
            "heading": "E Dataset Statistics & Licenses",
            "text": "Table 5 details the license and number of test example for each of the six open-domain datasets used\n6BERT and MPNet use essentially the same vocabulary, up to special tokens.\nin our work. For the BEIR benchmark, we refer the reader to Thakur et al. (2021) for number of examples and license of each of their datasets."
        },
        {
            "heading": "F Computational Resources",
            "text": "Our method (LE) does not involve training models at all. Our computational resources have been used to evaluate LE on the BEIR benchmark, i.e., computing passage embeddings for each corpus and each model. We used eight Quadro RTX 8000 GPUs. Each experiment took several hours."
        }
    ],
    "title": "What Are You Token About? Dense Retrieval as Distributions Over the Vocabulary",
    "year": 2023
}