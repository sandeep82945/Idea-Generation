{
    "abstractText": "Quantum machine learning (QML) has received a lot of attention according to its light training parameter numbers and speeds; and the advances of QML lead to active research on quantum multi-agent reinforcement learning (QMARL). Existing classical multi-agent reinforcement learning (MARL) features non-stationarity and uncertain properties. Therefore, this paper presents a simulation software framework for novel QMARL to control autonomous multi-drones, i.e., quantum multi-drone reinforcement learning. Our proposed framework accomplishes reasonable reward convergence and service quality performance with fewer trainable parameters. Furthermore, it shows more stable training results. Lastly, our proposed software allows us to analyze the training process and results.",
    "authors": [
        {
            "affiliations": [],
            "name": "Pyoung Kim"
        },
        {
            "affiliations": [],
            "name": "Joon Yun"
        },
        {
            "affiliations": [],
            "name": "Jae Pyoung Kim"
        }
    ],
    "id": "SP:2d4a6e5f377fdbbccc60d29debc2f64f351625e9",
    "references": [
        {
            "authors": [
                "Y. Kwak",
                "W.J. Yun",
                "J.P. Kim",
                "H. Cho",
                "J. Park",
                "M. Choi",
                "S. Jung",
                "J. Kim"
            ],
            "title": "Quantum distributed deep learning architectures: Models",
            "venue": "discussions, and applications, ICT Express ",
            "year": 2022
        },
        {
            "authors": [
                "S. Jerbi",
                "C. Gyurik",
                "S. Marshall",
                "H. Briegel",
                "V. Dunjko"
            ],
            "title": "Parametrized quantum policies for reinforcement learning",
            "venue": "in: Proc. NeurIPS, Virtual",
            "year": 2021
        },
        {
            "authors": [
                "W.J. Yun",
                "Y. Kwak",
                "J.P. Kim",
                "H. Cho",
                "S. Jung",
                "J. Park",
                "J. Kim"
            ],
            "title": "Quantum multi-agent reinforcement learning via variational quantum circuit design",
            "venue": "in: Proc. IEEE International Conference on Distributed Computing Systems (ICDCS), Bologna, Italy",
            "year": 2022
        },
        {
            "authors": [
                "J. Kamel",
                "M.R. Ansari",
                "J. Petit",
                "A. Kaiser",
                "I.B. Jemaa",
                "P. Urien"
            ],
            "title": "Simulation framework for misbehavior detection in vehicular networks",
            "venue": "IEEE Transactions on Vehicular Technology 69 (6) ",
            "year": 2020
        },
        {
            "authors": [
                "Y. Zhang",
                "S. Lu",
                "Y. Yang",
                "Q. Guo"
            ],
            "title": "Internet-distributed vehiclein-the-loop simulation for HEVs",
            "venue": "IEEE Transactions on Vehicular Technology 67 (5) ",
            "year": 2018
        },
        {
            "authors": [
                "Y. Hou",
                "Y. Zhao",
                "A. Wagh",
                "L. Zhang",
                "C. Qiao",
                "K.F. Hulme",
                "C. Wu",
                "A.W. Sadek",
                "X. Liu"
            ],
            "title": "Simulation-based testing and evaluation tools for transportation cyber\u2013physical systems",
            "venue": "IEEE Transactions on Vehicular Technology 65 (3) ",
            "year": 2016
        },
        {
            "authors": [
                "G. Brockman",
                "V. Cheung",
                "L. Pettersson",
                "J. Schneider",
                "J. Schulman",
                "J. Tang",
                "W. Zaremba"
            ],
            "title": "Openai gym",
            "venue": "arXiv preprint arXiv:1606.01540 ",
            "year": 2016
        },
        {
            "authors": [
                "J. Terry",
                "B. Black",
                "N. Grammel",
                "M. Jayakumar",
                "A. Hari",
                "R. Sullivan",
                "L.S. Santos",
                "C. Dieffendahl",
                "C. Horsch"
            ],
            "title": "R",
            "venue": "Perez-Vicente, et al., Pettingzoo: Gym for multi-agent reinforcement learning, in: Proc. NeurIPS, Vol. 34, Virtual",
            "year": 2021
        },
        {
            "authors": [
                "W.J. Yun",
                "S. Yi",
                "J. Kim"
            ],
            "title": "Multi-agent deep reinforcement learning using attentive graph neural architectures for real-time strategy games",
            "venue": "in: Proc. IEEE International Conference on Systems, Man, and Cybernetics (SMC), Virtual",
            "year": 2021
        },
        {
            "authors": [
                "H. Wang",
                "Y. Ding",
                "J. Gu",
                "Y. Lin",
                "D.Z. Pan",
                "F.T. Chong",
                "S. Han"
            ],
            "title": "QuantumNAS: Noise-adaptive search for robust quantum circuits",
            "venue": "in: Proc. IEEE International Symposium on High- Performance Computer Architecture (HPCA), Los Alamitos, CA, USA",
            "year": 2022
        },
        {
            "authors": [
                "G. Aleksandrowicz",
                "T. Alexander",
                "P. Barkoutsos",
                "L. Bello",
                "Y. Ben-Haim",
                "D. Bucher",
                "F.J. Cabrera-Hern\u00e1ndez",
                "J. Carballo- Franquis"
            ],
            "title": "A",
            "venue": "Chen, C.-F. Chen, et al., Qiskit: An open-source framework for quantum computing ",
            "year": 2019
        },
        {
            "authors": [
                "M. Shin",
                "J. Kim",
                "M. Levorato"
            ],
            "title": "Auction-based charging scheduling with deep learning framework for multi-drone networks",
            "venue": "IEEE Transactions on Vehicular Technology 68 (5) ",
            "year": 2019
        },
        {
            "authors": [
                "H. Lee",
                "S. Kwon",
                "S. Jung",
                "J. Kim"
            ],
            "title": "Neural Myerson auction for truthful and energy-efficient autonomous aerial data delivery",
            "venue": "Journal of Communications and Networks 24 (6) ",
            "year": 2022
        },
        {
            "authors": [
                "N. Gupta",
                "S. Agarwal",
                "D. Mishra"
            ],
            "title": "Multi-UAV replacement and trajectory design for coverage continuity",
            "venue": "in: Proc. IEEE International Conference on Communications (ICC)",
            "year": 2022
        },
        {
            "authors": [
                "S. Jung",
                "W.J. Yun",
                "M. Shin",
                "J. Kim",
                "J.-H. Kim"
            ],
            "title": "Orchestrated scheduling and multi-agent deep reinforcement learning for cloud-assisted multi-UAV charging systems",
            "venue": "IEEE Transactions on Vehicular Technology 70 (6) ",
            "year": 2021
        },
        {
            "authors": [
                "D. Bouwmeester",
                "A. Zeilinger"
            ],
            "title": "The physics of quantum information: Basic concepts",
            "venue": "Springer",
            "year": 2000
        },
        {
            "authors": [
                "A. P\u00e9rez-Salinas",
                "A. Cervera-Lierta",
                "E. Gil-Fuster",
                "J.I. Latorre"
            ],
            "title": "Data re-uploading for a universal quantum classifier",
            "venue": "Quantum 4 ",
            "year": 2020
        },
        {
            "authors": [
                "C. Park",
                "H. Lee",
                "W.J. Yun",
                "S. Jung",
                "C. Cordeiro",
                "J. Kim"
            ],
            "title": "Cooperative multi-agent deep reinforcement learning for reliable and energy-efficient mobile access via multi-UAV control",
            "venue": "arXiv preprint arXiv:2210.00945 ",
            "year": 2022
        },
        {
            "authors": [
                "C. Park",
                "J.P. Kim",
                "W.J. Yun",
                "S. Jung"
            ],
            "title": "J",
            "venue": "Kim, Visual simulation software demonstration for quantum multi-drone reinforcement learning ",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Quantum machine learning (QML) has received a lot of attention according to its light training parameter numbers and speeds; and the advances of QML lead to active research on quantum multi-agent reinforcement learning (QMARL). Existing classical multi-agent reinforcement learning (MARL) features non-stationarity and uncertain properties. Therefore, this paper presents a simulation software framework for novel QMARL to control autonomous multi-drones, i.e., quantum multi-drone reinforcement learning. Our proposed framework accomplishes reasonable reward convergence and service quality performance with fewer trainable parameters. Furthermore, it shows more stable training results. Lastly, our proposed software allows us to analyze the training process and results.\nKeywords: Drone, Quantum Machine Learning, Reinforcement Learning, Simulations, Visualization."
        },
        {
            "heading": "1. Introduction",
            "text": "Background and Motivation. Spurred by the recent advances in quantum computing (QC), attempts to reimplement existing machine learning (ML) have been presented to discover quantum advantages [1, 2]. Primarily, multi-agent reinforcement learning (MARL) is one of the most challenging ML fields due to scalability and non-stationarity [3]. Yun et al. have proposed QMARL that can perform reasonably by using a small number of trainable parameters compared to classical neural networks [3]. In addition, meta QMARL has been proposed, which enables adaptation to context changes by using a tiny size of memory (known as pole memory in QC) compared to the classical method [3]. Although these studies have shown the effectiveness and feasibility, user-friendly simulation software is not considered and designed yet, whereas the software simulator implementation and visualization research is con-\n\u2217Corresponding authors Email addresses: cosdeneb@korea.ac.kr (\u2020Chanyoung\nPark), paulkim436@korea.ac.kr (\u2020Jae Pyoung Kim), ywjoon95@korea.ac.kr (\u2020Won Joon Yun), soohyun828@korea.ac.kr (\u2020Soohyun Park), sjung@ajou.ac.kr (\u00a7Soyi Jung), joongheon@korea.ac.kr (\u2020Joongheon Kim)\n1This research was funded by the National Research Foundation of Korea (2022R1A2C2004869). Chanyoung Park and Jae Pyoung Kim contributed equally (first authors).\nsidered as one of key topics in network and mobility research fields [4, 5, 6].\nFor existing classical RL, OpenAI has already developed and publicized Gym [7], which helps compare the performance between various RL algorithms by implementing communication between the algorithms and the environments. Furthermore, it has become standard to evaluate RL algorithms via Gym environments. Many versions of Gym exist for different frameworks of RL. For example, Gazebo is a library for 3D robotics simulation in which RL can be utilized. Petting Zoo [8] and Starcraft multi-agent challenge (SMAC) [9] are both environments for MARL, equipped with multiple agents. Next, for QML frameworks, a variety of tools such as TorchQuantum [10], and Qiskit [11] has also been developed and publicized in recent years. These libraries significantly contributed to the research of many current QML algorithms. For example, the QMARL framework mentioned above [3] also utilizes TorchQuantum for the implementation.\nSimilar to the frameworks introduced above, we have designed a QMARL software framework for visualizing a multi-drone communication environment [12, 13, 14, 15], so-called quantum multi-drone reinforcement learning (QMDRL). The MDRL framework proposed in this paper is a hybrid model which utilizes both classic and QC. Classical computing is used to implement\nPreprint submitted to ICT Express February 6, 2023\nar X\niv :2\n21 1.\n15 37\n5v 3\n[ qu\nan t-\nph ]\n3 F\neb 2\n02 3\nthe Trainer in Fig. 1, which computes the loss function, performs optimization via gradient descent, and updates the target values. QC is used to implement the quantumbased policy (Q-policy) in Q-policy Layer, which computes the action distribution of all the drone agents. In this process, the data re-uploading technique encodes both classical data and compute output data. Leveraging QC in our proposed model has not only increased the computational speed exponentially but also shown a higher total reward value after training. Therefore, the proposed model outperforms the previous model and other benchmarks, corroborating our methodology\u2019s efficiency. Moreover, we can analyze multi-agent training processes and results thanks to our proposed visual simulation software. Contributions. The contributions of this work are as follows: (i) A proposed QMDRL framework for a multidrone environment is designed to maximize users\u2019 support rate and quality of service (QoS). Based on the proposed framework, we construct visualized humancomputer/drone interaction (HCI) interface for better understanding to the system designers and engineers; (ii) We validate that our proposed QMDRL training algorithm has more stable and effective performances than the classical MARL based on our proposed visualization software framework; (iii) A Q-policy network that computes drone agents\u2019 action distribution via data re-uploading is proposed. The data re-uploading scheme helps our Q-Policy to stably handle multiple qubits. Organization. The rest of this paper is organized as follows. Sec. 2 introduces the basic concepts of quantum machine learning. Sec. 3 proposes a software simulator/tool for quantum multi-drone reinforcement learning. Sec. 4 evaluates and demonstrates the software simulator of quantum multi-drone reinforcement learning. Sec. 5 concludes this paper."
        },
        {
            "heading": "2. Basics of Quantum Computing",
            "text": "Qubits. In QC, a qubit is used as the basic unit of information instead of bits. While bits can only be expressed by 0 or 1, qubits can be expressed as a combination of two bases, |0\u3009 and |1\u3009. This nature is also known as superposition [16]. Furthermore, quantum states can be expressed as qubits, and entanglement can occur between qubits which causes individual qubits to be strongly correlated. Due to these differences, qubits can express more information than classical bits. Assuming there is a q qubits system, the quantum state defined within the Hilbert state can be defined as follows, |\u03c8\u3009 = \u03b11|0 \u00b7 \u00b7 \u00b7 0\u3009+ \u00b7 \u00b7 \u00b7+\u03b12q |1 \u00b7 \u00b7 \u00b7 1\u3009, where \u03b1 represents the probability amplitude and \u22112q i=1 \u03b1 2 i = 1; finally,\nthe quantum state can be graphically expressed on the Bloch sphere. Basic Quantum Gates. Although bits and qubits are different, classical data can be encoded into qubits via quantum state encoders composed of basic quantum rotation gates expressed as Rx, Ry, and Rz. These rotation gates encode classical bits into qubits and perform unitary operations on a qubit by rotating it in the direction of x-, y-, and z- axes, respectively. Additionally, as mentioned above, qubit entanglement between two qubits can be achieved by a CNOT gate. This is done by performing the XOR operation on one qubit and using the other as the control qubit. All the gates introduced above are used to compose quantum neural networks. Quantum Neural Networks. The structure of quantum neural networks (QNN) comprises the state encoder, parameterized quantum circuit (PQC), and the measurement layer. Since classical data X is incompatible with quantum circuits, it must be converted into quantum states before being used as input data. In this paper, the data re-uploading [17] method will be used in implementing the state encoder as shown in Fig. 1. Data re-uploading and quantum state encoding are jointly achieved by passing q number of |0\u3009 qubits through a sequence of unitary operation gates containing the information of X and trainable parameters of the encoder, denoted as \u03b8enc. In order to imbue each gate with information of X, it has to be split into [x1 \u00b7 \u00b7 \u00b7 xN]. Then, these gates will repeatedly rotate the initial qubits, and the output quantum states produced successfully convert X into quantum states. The process of state encoding can be expressed as follows, |\u03c8enc\u3009 = U(\u03b8N)U(xN) \u00b7 \u00b7 \u00b7U(\u03b81)U(x1)|\u03c80\u3009, where N is the number of split data, U(\u00b7) represents a unitary operation of qubit rotation, |\u03c80\u3009 is the initial quantum state, |\u03c8enc\u3009 is the encoded qubit and \u03b8 is the trainable parameters of the state encoder. The PQC will then process the converted qubits. As shown in Fig. 1, the PQC is composed of the Rx, Ry, Rz and the CNOT gates which contain trainable parameters \u03b8PQC . By rotating and entangling the input qubits, we will be able to obtain the values needed for MARL. This can be expressed as |\u03c8PQC\u3009 = U(\u03b8PQC)|\u03c8enc\u3009. Finally, the produced qubits will proceed into the measurement layer, where they will be measured. Measurement can be carried out by applying a projection matrix M \u2208 M \u2261 {M1, \u00b7 \u00b7 \u00b7 ,Mc, \u00b7 \u00b7 \u00b7 ,MC} onto the z\u2212 axis. The value produced from this operation is known as observable, denoted as \u3008V\u3009\u03b8 \u2208 [\u22121, 1]C . The measurement operation can be expressed as follows, \u3008Vc\u3009\u03b8 = \u30080|U\u2020(x)U\u2020(\u03b8)McU(\u03b8)U(x)|0\u3009 = \u3008\u03c8|Mc|\u03c8\u3009, where (\u00b7)\u2020 is the complex conjugate operator. The observable produced from the quantum layer is the action\ndistribution of the actors and is used to calculate the loss function. However, the function cannot be trained by backpropagation because quantum states within the QNN cannot be measured and will collapse when the chain rule is applied. Therefore, training is carried out by computing the loss gradient from the symmetric difference quotient of the loss function."
        },
        {
            "heading": "3. Software Simulator for Quantum Multi-Drone Reinforcement Learning",
            "text": "Framework of Proposed QMARL. We consider the MARL environment in [18], where drone agents try to learn a policy that maximizes the total reward. We describe the sequential framework of our proposed QMDRL based on Fig. 1.\n1. In Drone Environment Layer [18], all M drone agents collect observation information denoted as the set of states, which is S \u2261 [o1, \u00b7 \u00b7 \u00b7 , om, \u00b7 \u00b7 \u00b7 , oM].\n2. With drone agents\u2019 state, they take actions in time step t based on their policy (i.e., actor), and then the state is transitioned to next time step t + 1 in MDRL Layer. Here, all actions and rewards that drone agents take are denoted as the following sets, A \u2261 [a1, \u00b7 \u00b7 \u00b7 , am, \u00b7 \u00b7 \u00b7 , aM] and R \u2261 [r1, \u00b7 \u00b7 \u00b7 , rm, \u00b7 \u00b7 \u00b7 , rM].\n3. Loss value is calculated by criticizing the reward that drone agents get at time step t with the return by target actor network in Trainer Layer. After that, the optimizer updates the parameters of actor net-\nworks in the direction of decreasing the value of the loss function. In the case of the target network, its parameters are updated at a specific time intermittently. The detailed training processes are described in Sec. 3.\n4. The Quantum Layer, also known as the Q-policy network, produces the action distribution of all the drone agents by taking the state data as input. The output data is computed by the encoders and parameterized quantum circuits which are the components of the Q-policy network. Then, the action distribution values will be used to calculate the loss function utilized in the trainer layer via the mean squared error function. Finally, the gradient of the loss function is calculated to perform gradient descent to update the actor and the target.\n5. At the same time, all drone agent\u2019s trajectories are visualized while training their policies by tensorboard in the Visualization Layer as shown in Fig. 4.\nDescription of Q-policy. Each drone agent has our proposed Q-policy as shown in Fig. 2. Especially, the algorithm for the Q-policy network is as shown in Fig. 2(a). Firstly, (Lines 4\u20135) initializes the input dimensions as the size of the observation data of the drone agents and the initial quantum state dimension as the output size equivalent to the total number of actions. Next, (Line 6) defines the quantum device where the encoders and quantum layers will be placed. (Lines 7\u20139) and (Line 14) define the encoder and quantum layer, respectively. As shown in the figure, the encoder is composed of the pre-defined general encoder and re-uploading encoder, while the quantum layer is made up of Controlled U3\ngates. In (Line 10), the number of repetitions of data reuploading is set as the ceiling value of the input dimension divided by the number of initial qubits. (Lines 11\u2013 13) defines the parameter values of the Controlled U3 gate. Finally, (Line 16) defines the measurement layer as applying Pauli-Z gate to all the qubits within the circuit. Fig. 2(a) contains the code for the forward function of the Q-policy network. (Lines 19\u201320) shows the process of reshaping the input data into a single-column matrix. Then, (Line 21) is the part where data re-uploading occurs. The process is repeated for the number of repetitions initialized above. Every iteration passes the input data through one encoder and one quantum-layer (Qlayer). Note that the first and the last iterations use encoders with different input dimensions. Lastly, (Lines 22\u201323) returns the action distribution by measuring the output produced by data re-uploading.\nFig. 2 also shows the structure of the Q-policy network, which is designed to perform data re-uploading. As shown in the figure, data re-uploading passes an initial quantum state through a repeated sequence of encoders and Q-layers. According to the algorithm elaborated above, the quantum gates in the encoders are imbued with the input state data. At the same time, the Q-layer is made up of Controlled U3 gates containing parameters such as the number of blocks, the number of layers per block, and the number of qubits. After data reuploading has been performed, the output quantum state is measured by projecting the quantum state onto the reference z-axis to obtain the action distribution data. Training Process of Drone Agents. Fig. 3 shows the training process of drone agents in our MARL environment with their policies. (Lines 2\u20139) performs the\ninitialization of the policy\u2019s parameters, where each drone agent is given a unique identification to distinguish drone agents in (Line 3). (Lines 7\u20138) creates two neural networks corresponding to the drone agent\u2019s policy; actor-network and target actor-network. Using the target network, the learning process becomes more stable by making the target value independent to the training parameters in actor-network. (Line 13) converts the drone agent\u2019s observation into the hidden state by encoding the parameters of the model machined to be suitable to use PyTorch-related tools (i.e., TorchQuantum and Qiskit). After that, when the actor-network object is called with a hidden state in (Line 14), the drone agent executes the forward function to return the action set of the drone. In (Lines 15\u201317), softmax in PyTorch is a\nfunction that outputs probabilities of all actions, and the drone agent returns the action with the highest probability. In RL, the drone agent\u2019s training is performed after state transition, where the transition pair ( S,A,R,S\u2032 ) is encoded into hidden states in (Lines 20\u201323). Denotation \u2032 is to distinguish whether the set is for t + 1. (Lines 24\u2013 25) calculates the Q-function of actor-network and target actor-network. In (Lines 27\u201330), the optimizer updates the parameters of actor networks to decrease the value of the loss function by Trainer in Fig. 1."
        },
        {
            "heading": "4. Performance Evaluation",
            "text": "Visualization of Drone Agents\u2019 Training Results. Thanks to our visualization using tensorboard, we can\nanalyze drone agents\u2019 decisions over time. Fig. 5(a)\u2013(c) show the total reward drone agents get, the support rate among all users, and users\u2019 QoS in all epochs, respectively. All results show the common tendencies, where drone agents in the classical MARL get larger values in the intermediate step of learning and a faster increase rate in the early step of learning. However, users in the QMDRL finally get more considerable value at the end of the training. In other words, our proposed QMDRL shows a more stable convergence rate with little fluctuations and higher values at the end of the learning. We can also observe drone agents\u2019 trajectories while learning their policies in Fig. 6(a)\u2013(i), where all drone agents try to provide high-quality wireless communication service to as many people as possible. When there is no malfunctioning, drone agents move to areas where any drone doesn\u2019t provide service to users. Conversely, when any malfunction occurs, the drone agents move to areas where the malfunctioned non-drone/agent has provided service to users. By our proposed visual simulation software framework, we can validate that the QMDRL can make a reasonable performance with a smaller number of trainable parameters compared to classical MARL in reward convergence and system quality. Demonstration Video. The video demonstration for our proposed visual simulation software framework and simulation results are in [19]. First of all, this demo shows the architecture of our Q-Policy network, and it proposes a visualization framework of software simulation for QMDRL visualization. After that, we show the HCI process sequentially using our framework."
        },
        {
            "heading": "5. Concluding Remarks",
            "text": "We present a novel QMDRL framework for MARL-based autonomous multi-drone mobility control/coordination and analyze the performances by visualization using tensorboard. In addition, we show how our proposed QMDRL could be used to train the multidrone/agent system efficiently. As future work, we plan to investigate other applications of QMDRL in versatile situations using our proposed visualization framework."
        }
    ],
    "title": "Software Simulation and Visualization of Quantum Multi-Drone Reinforcement Learning",
    "year": 2023
}