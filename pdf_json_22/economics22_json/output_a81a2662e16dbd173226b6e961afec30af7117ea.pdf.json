{
    "abstractText": "The most popular methods for measuring importance of the variables in a black-box prediction algorithm make use of synthetic inputs that combine predictor variables from multiple observations. These inputs can be unlikely, physically impossible, or even logically impossible. As a result, the predictions for such cases can be based on data very unlike any the black box was trained on. We think that users cannot trust an explanation of the decision of a prediction algorithm when the explanation uses such values. Instead we advocate a method called Cohort Shapley that is grounded in economic game theory and uses only actually observed data to quantify variable importance. Cohort Shapley works by narrowing the cohort of observations judged to be similar to a target observation on one or more features. We illustrate it on an algorithmic fairness problem where it is essential to attribute importance to protected variables that the model was not trained on.",
    "authors": [
        {
            "affiliations": [],
            "name": "Masayoshi Mase"
        },
        {
            "affiliations": [],
            "name": "Benjamin B. Seiler"
        }
    ],
    "id": "SP:6898b15fe8a141fa58a8d31ecc3fa75f33f717db",
    "references": [
        {
            "authors": [
                "K. Aas",
                "M. Jullum",
                "A. L\u00f8land"
            ],
            "title": "Explaining individual predictions when features are dependent: More accurate approximations to Shapley values",
            "venue": "Technical report, arXiv:1903.10464.",
            "year": 2019
        },
        {
            "authors": [
                "R. Agarwal",
                "L. Melnick",
                "N. Frosst",
                "X. Zhang",
                "B. Lengerich",
                "R. Caruana",
                "G. Hinton"
            ],
            "title": "Neural additive models: Interpretable machine learning with neural nets",
            "venue": "In 35th Conference on Neural Information Processing Systems (NeurIPS",
            "year": 2021
        },
        {
            "authors": [
                "J. Angwin",
                "J. Larson",
                "S. Mattu",
                "L. Kirchner"
            ],
            "title": "Machine bias: there\u2019s software used across the country to predict future criminals",
            "venue": "and it\u2019s biased against blacks.",
            "year": 2016
        },
        {
            "authors": [
                "R.J. Aumann",
                "L.S. Shapley"
            ],
            "title": "Values of Non-Atomic Games",
            "venue": "Princeton University Press, Princeton, NJ.",
            "year": 1974
        },
        {
            "authors": [
                "R. Berk",
                "H. Heidari",
                "S. Jabbari",
                "M. Kearns",
                "A. Roth"
            ],
            "title": "Fairness in criminal justice risk assessments: The state of the art",
            "venue": "Sociological Methods & Research.",
            "year": 2018
        },
        {
            "authors": [
                "R. Berman"
            ],
            "title": "Beyond the last touch: Attribution in online advertising",
            "venue": "Marketing Science, 37(5):771\u2013792.",
            "year": 2018
        },
        {
            "authors": [
                "K.A. Bollen",
                "J. Pearl"
            ],
            "title": "Eight myths about causality and structural equation models",
            "venue": "Morgan, S. L., editor, Handbook of causal analysis for social research, pages 301\u2013328. Springer, Dordrecht, NL.",
            "year": 2013
        },
        {
            "authors": [
                "L. Breiman"
            ],
            "title": "Random forests",
            "venue": "Machine learning, 45(1):5\u201332.",
            "year": 2001
        },
        {
            "authors": [
                "T. Brennan",
                "W. Dieterich",
                "B. Ehret"
            ],
            "title": "Evaluating the predictive validity of the COMPAS risk and needs assessment system",
            "venue": "Criminal Justice and Behavior.",
            "year": 2009
        },
        {
            "authors": [
                "F. Campolongo",
                "J. Cariboni",
                "A. Saltelli"
            ],
            "title": "An effective screening design for sensitivity analysis of large models",
            "venue": "Environmental modelling & software, 22(10):1509\u20131518.",
            "year": 2007
        },
        {
            "authors": [
                "D. Chan",
                "M. Perry"
            ],
            "title": "Challenges and opportunities in media mix modeling",
            "year": 2017
        },
        {
            "authors": [
                "G. Chastaing",
                "F. Gamboa",
                "C. Prieur"
            ],
            "title": "Generalized Hoeffding-Sobol\u2019 decomposition for dependent variables-application to sensitivity analysis",
            "venue": "Electronic Journal of Statistics, 6:2420\u2013 2448.",
            "year": 2012
        },
        {
            "authors": [
                "A. Chouldechova"
            ],
            "title": "Fair prediction with disparate impact: A study of bias in recidivism prediction instruments",
            "venue": "Big data, 5(2):153\u2013163.",
            "year": 2017
        },
        {
            "authors": [
                "A. Chouldechova",
                "A. Roth"
            ],
            "title": "The frontiers of fairness in machine learning",
            "venue": "Technical report, arXiv:1810.08810.",
            "year": 2018
        },
        {
            "authors": [
                "W.G. Cochran"
            ],
            "title": "The effectiveness of adjustment by subclassification in removing bias in observational studies",
            "venue": "Biometrics, pages 295\u2013313.",
            "year": 1968
        },
        {
            "authors": [
                "S. Corbett-Davies",
                "S. Goel"
            ],
            "title": "The measure and mismeasure of fairness: A critical review of fair machine learning",
            "venue": "Technical report, arXiv:1808.00023.",
            "year": 2018
        },
        {
            "authors": [
                "S. Corbett-Davies",
                "E. Pierson",
                "A. Feller",
                "S. Goel",
                "A. Huq"
            ],
            "title": "Algorithmic decision making and the cost of fairness",
            "venue": "Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, pages 797\u2013806.",
            "year": 2017
        },
        {
            "authors": [
                "D.R. Cox"
            ],
            "title": "Interaction",
            "venue": "International Statistical Review, 52(1):1\u201324.",
            "year": 1984
        },
        {
            "authors": [
                "S. Da Veiga",
                "F. Gamboa",
                "B. Iooss",
                "C. Prieur"
            ],
            "title": "Basics and Trends in Sensitivity Analysis: Theory and Practice in R",
            "year": 2021
        },
        {
            "authors": [
                "A. Datta",
                "S. Sen",
                "Y. Zick"
            ],
            "title": "Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems",
            "venue": "Proceedings of the 2016 IEEE symposium on security and privacy (SP), pages 598\u2013617. IEEE.",
            "year": 2016
        },
        {
            "authors": [
                "A.P. Dawid",
                "M. Musio"
            ],
            "title": "Effects of causes and causes of effects",
            "venue": "Annual Review of Statistics and Its Application, 9.",
            "year": 2021
        },
        {
            "authors": [
                "A.B. De Gonzalez",
                "D.R. Cox"
            ],
            "title": "Interpretation of interaction: A review",
            "venue": "The Annals of Applied Statistics, 1(2):371\u2013385.",
            "year": 2007
        },
        {
            "authors": [
                "W. Dieterich",
                "C. Mendoza",
                "T. Brennan"
            ],
            "title": "COMPAS risk scales: Demonstrating accuracy equity and predictive parity",
            "venue": "Technical report, Northpoint Inc.",
            "year": 2016
        },
        {
            "authors": [
                "D.L. Donoho"
            ],
            "title": "What\u2019s missing from today\u2019s machine intelligence juggernaut? Harvard Data",
            "venue": "Science Review,",
            "year": 2019
        },
        {
            "authors": [
                "F. Doshi-Velez",
                "M. Kortz",
                "R. Budish",
                "C. Bavitz",
                "S. Gershman",
                "D. O\u2019Brien",
                "K. Scott",
                "S. Schieber",
                "J. Waldo",
                "D. Weinberger",
                "A. Weller",
                "A. Wood"
            ],
            "title": "Accountability of AI under the law: The role of explanation",
            "venue": "Technical report,",
            "year": 2017
        },
        {
            "authors": [
                "C. Dwork",
                "M. Hardt",
                "T. Pitassi",
                "O. Reingold",
                "R. Zemel"
            ],
            "title": "Fairness through awareness",
            "venue": "Proceedings of the 3rd innovations in theoretical computer science conference, pages 214\u2013226.",
            "year": 2012
        },
        {
            "authors": [
                "B. Efron",
                "C. Stein"
            ],
            "title": "The jackknife estimate of variance",
            "venue": "Annals of Statistics, 9(3):586\u2013 596.",
            "year": 1981
        },
        {
            "authors": [
                "R.A. Fisher",
                "W.A. Mackenzie"
            ],
            "title": "The manurial response of different potato varieties",
            "venue": "Journal of Agricultural Science, xiii:311\u2013320.",
            "year": 1923
        },
        {
            "authors": [
                "A.W. Flores",
                "K. Bechtel",
                "C.T. Lowenkamp"
            ],
            "title": "False positives, false negatives, and false analyses: A rejoinder to machine bias: There\u2019s software used across the country to predict future criminals",
            "venue": "and it\u2019s biased against blacks. Federal Probation, 80(2):38\u201346.",
            "year": 2016
        },
        {
            "authors": [
                "S.A. Friedler",
                "C. Scheidegger",
                "S. Venkatasubramanian",
                "S. Choudhary",
                "E.P. Hamilton",
                "D. Roth"
            ],
            "title": "A comparative study of fairness-enhancing interventions in machine learning",
            "venue": "Proceedings of the conference on fairness, accountability, and transparency, pages 329\u2013338.",
            "year": 2019
        },
        {
            "authors": [
                "C. Frye",
                "D. de Mijolla",
                "T. Begley",
                "L. Cowton",
                "M. Stanley",
                "I. Feige"
            ],
            "title": "Shapley explainability on the data manifold",
            "venue": "In Proceedings of the 10th International Conference on Learning Representations",
            "year": 2021
        },
        {
            "authors": [
                "A. Gelman",
                "D.K. Park"
            ],
            "title": "Splitting a predictor at the upper quarter or third and the lower quarter or third",
            "venue": "The American Statistician, 63(1):1\u20138.",
            "year": 2009
        },
        {
            "authors": [
                "A. Ghorbani",
                "J. Zou"
            ],
            "title": "Data Shapley: Equitable valuation of data for machine learning",
            "venue": "International Conference on Machine Learning, pages 2242\u20132251. PMLR.",
            "year": 2019
        },
        {
            "authors": [
                "D. Harrison",
                "D.L. Rubinfeld"
            ],
            "title": "Hedonic prices and the demand for clean air",
            "venue": "Journal of Environmental Economics and Management, 5:81\u2013102.",
            "year": 1978
        },
        {
            "authors": [
                "P.W. Holland"
            ],
            "title": "Statistics and causal inference",
            "venue": "Journal of the American statistical Association, 81(396):945\u2013960.",
            "year": 1986
        },
        {
            "authors": [
                "P.W. Holland"
            ],
            "title": "Causal inference, path analysis and recursive structural equations models",
            "venue": "Technical Report 88\u201381, ETS Research Report Series.",
            "year": 1988
        },
        {
            "authors": [
                "G. Hooker"
            ],
            "title": "Generalized functional ANOVA diagnostics for high-dimensional functions of dependent variables",
            "venue": "Journal of Computational and Graphical Statistics.",
            "year": 2012
        },
        {
            "authors": [
                "G. Hooker",
                "L. Mentch"
            ],
            "title": "Please stop permuting features: An explanation and alternatives",
            "venue": "Technical report, arXiv:1905.03151.",
            "year": 2019
        },
        {
            "authors": [
                "S. Hooker",
                "D. Erhan",
                "Kindermans",
                "P.-J.",
                "B. Kim"
            ],
            "title": "A benchmark for interpretability methods in deep neural networks",
            "venue": "Advances in neural information processing systems, 32.",
            "year": 2019
        },
        {
            "authors": [
                "E. Jackson",
                "C. Mendoza"
            ],
            "title": "Setting the Record Straight: What the COMPAS Core Risk and Need Assessment Is and Is Not",
            "venue": "Harvard Data Science Review, 2(1). https://hdsr.mitpress.mit.edu/pub/hzwo7ax4.",
            "year": 2020
        },
        {
            "authors": [
                "M.J.W. Jansen"
            ],
            "title": "Analysis of variance designs for model output",
            "venue": "Computer Physics Communications, 117(1\u20132):35\u201343.",
            "year": 1999
        },
        {
            "authors": [
                "T. Jiang",
                "A.B. Owen"
            ],
            "title": "Quasi-regression with shrinkage",
            "venue": "Mathematics and Computers in Simulation, 62(3-6):231\u2013241.",
            "year": 2003
        },
        {
            "authors": [
                "J. Kleinberg",
                "S. Mullainathan",
                "M. Raghavan"
            ],
            "title": "Inherent trade-offs in the fair determination of risk scores",
            "venue": "Technical report, arXiv:1609.05807.",
            "year": 2016
        },
        {
            "authors": [
                "I.E. Kumar",
                "S. Venkatasubramanian",
                "C. Scheidegger",
                "S. Friedler"
            ],
            "title": "Problems with Shapley-value-based explanations as feature importance measures",
            "venue": "Proceedings of the 37th International Conference on Machine Learning (ICML 2020).",
            "year": 2020
        },
        {
            "authors": [
                "R.H. Lindeman",
                "P.F. Merenda",
                "R.Z. Gold"
            ],
            "title": "Introduction to bivariate and multivariate analysis",
            "venue": "Scott, Foresman and Company, Homewood, IL.",
            "year": 1980
        },
        {
            "authors": [
                "S.M. Lundberg",
                "G. Erion",
                "H. Chen",
                "A. DeGrave",
                "J.M. Prutkin",
                "B. Nair",
                "R. Katz",
                "J. Himmelfarb",
                "N. Bansal",
                "Lee",
                "S.-I."
            ],
            "title": "From local explanations to global understanding with explainable ai for trees",
            "venue": "Nature Machine Intelligence, 2(1):2522\u20135839.",
            "year": 2020
        },
        {
            "authors": [
                "S.M. Lundberg",
                "Lee",
                "S.-I."
            ],
            "title": "A unified approach to interpreting model predictions",
            "venue": "Advances in Neural Information Processing Systems 30 (NIPS 2017), pages 4765\u20134774.",
            "year": 2017
        },
        {
            "authors": [
                "M. Mase",
                "A.B. Owen",
                "B.B. Seiler"
            ],
            "title": "Explaining black box decisions by Shapley cohort refinement",
            "venue": "Technical report, arXiv:1911.00467.",
            "year": 2019
        },
        {
            "authors": [
                "M. Mase",
                "A.B. Owen",
                "B.B. Seiler"
            ],
            "title": "Cohort Shapley value for algorithmic fairness",
            "venue": "Technical report, arXiv:2105.07168.",
            "year": 2021
        },
        {
            "authors": [
                "T.P. Michalak",
                "K.V. Aadithya",
                "P.L. Szczepanski",
                "B. Ravindran",
                "N.R. Jennings"
            ],
            "title": "Efficient computation of the shapley value for game-theoretic network centrality",
            "venue": "Journal of Artificial Intelligence Research,",
            "year": 2013
        },
        {
            "authors": [
                "J.S. Mill"
            ],
            "title": "A system of logic, ratiocinative and inductive",
            "venue": "John Parker, London, third edition in project gutenberg edition.",
            "year": 1851
        },
        {
            "authors": [
                "R. Mitchell",
                "J. Cooper",
                "E. Frank",
                "G. Holmes"
            ],
            "title": "Sampling permutations for shapley value estimation",
            "year": 2022
        },
        {
            "authors": [
                "N. Moehle",
                "S. Boyd",
                "A. Ang"
            ],
            "title": "Portfolio performance attribution via shapley value",
            "venue": "Technical report, arXiv:2102.05799.",
            "year": 2021
        },
        {
            "authors": [
                "C. Molnar"
            ],
            "title": "Interpretable machine learning: A Guide for Making Black Box Models Explainable",
            "venue": "Leanpub.",
            "year": 2018
        },
        {
            "authors": [
                "M.D. Morris"
            ],
            "title": "Factorial sampling plans for preliminary computational experiments",
            "venue": "Technometrics, 33(2):161\u2013174.",
            "year": 1991
        },
        {
            "authors": [
                "M. Neuh\u00e4user",
                "M. Thielmann",
                "G.D. Ruxton"
            ],
            "title": "The number of strata in propensity score stratification for a binary outcome",
            "venue": "Archives of medical science: AMS, 14(3):695.",
            "year": 2018
        },
        {
            "authors": [
                "M.A. Newton",
                "A.E. Raftery"
            ],
            "title": "Approximate Bayesian inference with the weighted likelihood bootstrap",
            "venue": "Journal of the Royal Statistical Society: Series B (Methodological), 56(1):3\u2013",
            "year": 1994
        },
        {
            "authors": [
                "A.B. Owen"
            ],
            "title": "Latin supercube sampling for very high dimensional simulations",
            "venue": "ACM Transactions on Modeling and Computer Simulation, 8(2):71\u2013102.",
            "year": 1998
        },
        {
            "authors": [
                "A.B. Owen"
            ],
            "title": "Sobol\u2019 indices and Shapley value",
            "venue": "Journal on Uncertainty Quantification, 2:245\u2013251.",
            "year": 2014
        },
        {
            "authors": [
                "A.B. Owen",
                "C. Prieur"
            ],
            "title": "On Shapley value for measuring importance of dependent inputs",
            "venue": "SIAM/ASA Journal on Uncertainty Quantification, 5(1):986\u20131002.",
            "year": 2017
        },
        {
            "authors": [
                "J. Pearl"
            ],
            "title": "Causal inference in statistics: An overview",
            "venue": "Statistics surveys, 3:96\u2013146.",
            "year": 2009
        },
        {
            "authors": [
                "E. Plischke",
                "G. Rabitti",
                "E. Borgonovo"
            ],
            "title": "Computing Shapley effects for sensitivity analysis",
            "venue": "SIAM/ASA Journal on Uncertainty Quantification, 9(4):1411\u20131437.",
            "year": 2021
        },
        {
            "authors": [
                "S. Razavi",
                "A. Jakeman",
                "A. Saltelli",
                "C. Prieur",
                "B. Iooss",
                "E. Borgonovo",
                "E. Plischke",
                "S.L. Piano",
                "T. Iwanaga",
                "W. Becker",
                "S. Tarantola",
                "J.H.A. Guillaume",
                "J. Jakeman",
                "H. Gupta",
                "N. Milillo",
                "G. Rabitti",
                "V. Chabridon",
                "Q. Duan",
                "X. Sun",
                "S. Smith",
                "R. Sheikholeslami",
                "N. Hosseini",
                "M. Asadzadeh",
                "A. Puy",
                "S. Kucherenko",
                "H.R. Maier"
            ],
            "title": "The future of sensitivity analysis: An essential discipline for systems modeling and policy support",
            "venue": "Environmental Modelling",
            "year": 2021
        },
        {
            "authors": [
                "M.T. Ribeiro",
                "S. Singh",
                "C. Guestrin"
            ],
            "title": "Why should I trust you?: Explaining the predictions of any classifier",
            "venue": "Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pages 1135\u20131144.",
            "year": 2016
        },
        {
            "authors": [
                "D.B. Rubin"
            ],
            "title": "Estimating causal effects of treatments in randomized and nonrandomized studies",
            "venue": "Journal of educational Psychology,",
            "year": 1974
        },
        {
            "authors": [
                "D.B. Rubin"
            ],
            "title": "The Bayesian bootstrap",
            "venue": "The annals of statistics, 9(1):130\u2013134.",
            "year": 1981
        },
        {
            "authors": [
                "C. Rudin",
                "C. Wang",
                "B. Coker"
            ],
            "title": "The Age of Secrecy and Unfairness in Recidivism Prediction",
            "venue": "Harvard Data Science Review, 2(1). https://hdsr.mitpress.mit.edu/pub/7z10o269.",
            "year": 2020
        },
        {
            "authors": [
                "A. Saltelli",
                "M. Ratto",
                "T. Andres",
                "F. Campolongo",
                "J. Cariboni",
                "D. Gatelli",
                "M. Saisana",
                "S. Tarantola"
            ],
            "title": "Global sensitivity analysis: the primer",
            "venue": "John Wiley & Sons.",
            "year": 2008
        },
        {
            "authors": [
                "S.L. Scott",
                "H.R. Varian"
            ],
            "title": "Predicting the present with Bayesian structural time series",
            "venue": "International Journal of Mathematical Modelling and Numerical Optimisation, 5(1-2):4\u201323.",
            "year": 2014
        },
        {
            "authors": [
                "L.S. Shapley"
            ],
            "title": "A value for n-person games",
            "venue": "Kuhn, H. W. and Tucker, A. W., editors, Contribution to the Theory of Games II (Annals of Mathematics Studies 28), pages 307\u2013317. Princeton University Press, Princeton, NJ.",
            "year": 1953
        },
        {
            "authors": [
                "D. Slack",
                "S. Hilgard",
                "E. Jia",
                "S. Singh",
                "H. Lakkaraju"
            ],
            "title": "Fooling lime and shap: Adversarial attacks on post hoc explanation methods",
            "venue": "AAAI/ACM Conference on AI, Ethics, and Society (AIES).",
            "year": 2020
        },
        {
            "authors": [
                "I.M. Sobol"
            ],
            "title": "Multidimensional Quadrature Formulas and Haar Functions. Nauka, Moscow",
            "year": 1969
        },
        {
            "authors": [
                "I.M. Sobol"
            ],
            "title": "Sensitivity estimates for nonlinear mathematical models",
            "venue": "Mathematical Modeling and Computational Experiment,",
            "year": 1993
        },
        {
            "authors": [
                "E. Song",
                "B.L. Nelson",
                "J. Staum"
            ],
            "title": "Shapley effects for global sensitivity analysis: Theory and computation",
            "venue": "SIAM/ASA Journal on Uncertainty Quantification, 4(1):1060\u20131083.",
            "year": 2016
        },
        {
            "authors": [
                "C.J. Stone"
            ],
            "title": "The use of polynomial splines and their tensor products in multivariate function estimation",
            "venue": "The Annals of Statistics, 22(1):118\u2013184.",
            "year": 1994
        },
        {
            "authors": [
                "E. \u0160trumbelj",
                "I. Kononenko"
            ],
            "title": "An efficient explanation of individual classifications using game theory",
            "venue": "Journal of machine learning research, 11:1\u201318.",
            "year": 2010
        },
        {
            "authors": [
                "E. \u0160trumbelj",
                "I. Kononenko",
                "M.R. \u0160ikonja"
            ],
            "title": "Explaining instance classifications with interactions of subsets of feature values",
            "venue": "Data & Knowledge Engineering, 68(10):886\u2013904.",
            "year": 2009
        },
        {
            "authors": [
                "M. Sundararajan",
                "A. Najmi"
            ],
            "title": "The many Shapley values for model explanation",
            "venue": "Proceedings of the 37th International Conference on Machine Learning (ICML 2020).",
            "year": 2020
        },
        {
            "authors": [
                "M. Sundararajan",
                "A. Taly",
                "Q. Yan"
            ],
            "title": "Axiomatic attribution for deep networks",
            "venue": "International conference on machine learning, pages 3319\u20133328. PMLR.",
            "year": 2017
        },
        {
            "authors": [
                "S. Tan",
                "R. Caruana",
                "G. Hooker",
                "Y. Lou"
            ],
            "title": "Distill-and-compare",
            "venue": "Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society. ACM.",
            "year": 2018
        },
        {
            "authors": [
                "P. Wei",
                "Z. Lu",
                "J. Song"
            ],
            "title": "Variable importance analysis: A comprehensive review",
            "venue": "Reliability Engineering & System Safety, 142:399\u2013432.",
            "year": 2015
        },
        {
            "authors": [
                "A. Xiang"
            ],
            "title": "Reconciling legal and technical approaches to algorithmic bias",
            "venue": "Tennessee Law Review, 88(3):2021.",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Black-box models commonly attain state of the art prediction accuracy as measured by predictions of held-out response values. In many settings, it is not enough to know that they are accurate. We want to interpret them too. We might derive a useful scientific insight from understanding how a prediction works, or we might spot a flaw in the model that convinces us that it will not generalize well beyond our data set. We might also find that the algorithm works in a way that is unfair to some people.\nA first step in interpreting a black-box model is to discern what are the important variables in that model. This is already a hard problem, and as we note below there are several large branches of the literature devoted to this problem. Most of the methods in those areas work by making changes to some, but not necessarily all, of the input variables in a model and taking note of how the predictions change in response. A severe problem with those approaches is that they will use unlikely or even impossible combinations of inputs. For instance, if one combines the youngest age of a criminal defendant with the greatest observed number of prior arrests, the result is an extremely unlikely synthetic data point. Combining features from different observations can lead to synthetic points describing people who graduated from high school before they were born, or who live in both Idaho and Los Angeles County. In medical settings, one can get impossible patients with systolic blood pressures below their diastolic blood pressure, or whose minimum O2 saturation level is above their average.\nFigure 1 shows what this can look like when levels of one variable are permuted with respect to levels of another variable. The left panel uses the well known Boston housing data of Harrison and\nar X\niv :2\n20 5.\n15 75\n0v 3\n[ cs\n.L G\n] 1\n3 A\npr 2\nRubinfeld (1978). The right panel uses the bodyfat data from the Data Analysis and Story Library (https://dasl.datadescription.com) after converting the waist measurements there from inches to centimeters. We see that the synthetic variable combinations can be quite unlike the real data. Those combinations may simply have nothing to do with any parts of Boston or with any likely human physique. This poses a problem: a black-box prediction function evaluated at such points is then being used far from where it was trained. Such extrapolations are at a minimum unreliable, and potentially meaningless. Furthermore, there are adversarial attacks on variable importance measures that use such inputs. Slack et al. (2020) report how an algorithm that makes unfair predictions at observed data values can be rated as fair due to predictions made at synthetic data points formed by mixing and matching inputs. This applies to two of the leading methods in machine learning: the Local Interpretable Model agnostic Interpretation (LIME) of Ribeiro et al. (2016) and the SHapley Additive exPlanations (SHAP) of Lundberg and Lee (2017). In contrast, the TreeSHAP method (Lundberg et al., 2020) uses only observed values; we describe below how it differs from our proposal. For now, we note that it is only defined for tree structured predictions.\nFor the reasons above, we need a variable importance measure that does not use impossible data. While there are algorithmic methods to judge when a data point is \u2018out of distribution,\u2019 there is not a reliable way to say exactly where the support of a distribution ends. In this article, we present the cohort Shapley measure from Mase et al. (2019) as one solution. Like many of the methods we discuss, this algorithm is grounded in axioms of economic game theory from Shapley (1953). However, the way the underlying game value is defined allows us to only use observed combinations of predictor variables.\nThis article is organized as follows: Section 2 compares properties of cohort Shapley to some\nwidely adopted variable importance methods and introduces a running example for illustration. Section 3 provides a brief discussion of the problems of variable importance and points to literature surveys in three areas: statistics, machine learning and global sensitivity analysis. At first sight, variable importance looks to be easy because all the counterfactuals we might want to consider can usually be directly computed. At a second look, variable importance is actually extremely difficult because it seeks to identify causes of effects instead of effects of causes. We explain that distinction making reference to Dawid and Musio (2021). Section 4 presents functional ANOVA, Sobol\u2019 indices from global sensitivity analysis, and Shapley value from game theory. Section 5 describes how Shapley value has been used to quantify variable importance and presents the cohort Shapley method. Section 6 illustrates cohort Shapley on an algorithmic fairness problem using data from Angwin et al. (2016) on the COMPAS algorithm for predicting recidivism. The key issue there is whether Black defendants were treated unfairly. For every defendant, cohort Shapley can estimate the importance to their prediction of each of their feature variables. We are able to do this even though the algorithm was trained without using race as a variable. Furthermore, we are able to do this just using the algorithm\u2019s predictions, which is an advantage because the algorithm is proprietary and not available to us. We can aggregate importance over subsets of observations, consistent with an additivity axiom. We also show how to use the Bayesian bootstrap of Rubin (1981) to get confidence intervals for variable importance of individuals and of aggregates. Section 7 has some final remarks. Python code for the cohort Shapley method is available at https://github.com/cohortshapley/cohortshapley."
        },
        {
            "heading": "2 Comparisons and a running example",
            "text": "Here we compare cohort Shapley to some other methods that we will describe in later sections. Variable importance methods differ primarily in how they define importance. We base our comparisons on which useful properties they have or do not have, as shown in Table 1. We have phrased the properties in a way that makes positive values desirable. Attributing importance to variables that are not in the model is controversial. We think that the ability to do that is desirable; cohort Shapley allows that but does not require it. TreeSHAP is fully automatic. Baseline Shapley and most other methods require defining a baseline point to compare the target point to. Cohort Shapley also requires a user-provided notion of variable similarity. We have listed LIME as requiring a second black box because it has a regularization process in determining how local an observation must be to be similar to the target. Kernel SHAP is listed as not computing the Shapley value because it computes a kernelized approximation to it. Similarly, on-manifold SHAP and conditional kernel SHAP do not compute the Shapley value; cohort and baseline Shapley do compute it. We include Monte Carlo versions of them because they are consistent for the Shapley value as computation increases. LIME requires the choice a surrogate model and a kernel, so we do not consider it to be automatic.\nOf the methods in Table 1, only cohort Shapley can be used on the COMPAS data that consist only of the predictions from a proprietary algorithm with no information on the model itself. It is similarly the only method that works with residuals to compare actual versus predicted levels of recidivism. The residuals are important because they let us study how an algorithm unfairly over-estimates or under-estimates a risk. Also if a variable is important for the residual it suggests that the model did not use that variable well. Cohort Shapley is one of only two that can attribute importance to an unused variable and one of only two that never uses unobserved data.\nAll the methods that we compare in Table 1 can be used without retraining the model on\ndifferent feature sets. They are aimed at attributing a specific point\u2019s prediction to one of their features, which is the task we consider in our numerical examples. Retraining with and without certain features present has been used, for example by S\u030ctrumbelj et al. (2009) and by Hooker et al. (2019) for their RemOve And Retrain (ROAR) method.\nTable 2 shows a small example that we will use to illustrate cohort Shapley. It has three binary predictors, two data distributions (one dependent and one independent) and two response functions (one additive and one with interaction). The dependent data distribution features an impossible combination. We can think of a setting where one binary variable specifies whether somebody has any children and a second whether their first child is male. Then the (0, 1) combination is impossible. Or we can think of a setting for the Boston housing data describing a region with many large lots and also high crime which while not logically impossible is quite improbable. In the given example, we have made a three variable combination impossible. We have also left variable three out of f1. Variable 3 could be something like the race of a person which is not used by the algorithm at all, but might have some importance from association with variables that are used."
        },
        {
            "heading": "3 Variable importance",
            "text": "We begin with some notation. We will consider a data set with n observations indexed by i \u2208 1:n \u2261 {1, . . . , n}. For observation i there is a d-tuple xi = (xi1, . . . , xid) of predictor variables. The component variables xij do not always have to be real-valued so xi is not necessarily a vector in Rd. The data set also includes a response yi for observation i. The models we consider predict yi from xi. We focus on the case yi \u2208 R. Sometimes we omit the observation\u2019s index and speak of predicting y from x. In some contexts we care about explaining the predictions for one specific target who then has index t.\nIn our formal discussions, we will need some special notation to study the changes that methods\nmake to x. For u \u2286 1:d we let xu be the tuple of variables xj for j \u2208 u. We will use |u| for the cardinality of u and \u2212u = 1:d \\ u for the complement of u. For tuples x and x\u0303 we let xu:x\u0303\u2212u be the hybrid point x\u2032 with x\u2032j = xj for j \u2208 u and x\u2032j = x\u0303j for j 6\u2208 u. We abbreviate x{j} and x\u2212{j} to xj and x\u2212j , respectively."
        },
        {
            "heading": "3.1 Goals and simple examples",
            "text": "Our goal is to quantify the importance of the individual variables in a model that predicts y from x. Importance can be interpreted several ways. For instance, changing xij could have a causal impact on yi, removing predictor xj from a model to predict y could degrade the accuracy of those predictions, or changing xij could bring a large change to the prediction y\u0302i (Jiang and Owen, 2003). These are distinct issues about how xj relates to y. We are most interested in understanding specific decisions made by a given algorithm. For instance, which variables best explain why an algorithm may have recommended against lending money to a target applicant t, or may have recommended in favor of sending patient t to an intensive care unit? For problems of this type, it is the third measure of importance that we want to study. In particular, an algorithm might have placed significant importance on a predictor known to have no causal effect, and we would want to detect that by showing how important that variable was to the predictions. It is for this goal that we especially want a method that avoids using impossible input combinations. In our conclusions we consider the consequences of including impossible combinations when judging whether variable j is important for accurate prediction.\nThe models we consider predict y by y\u0302 = f(x) \u2208 R. Importance of a variable can be \u2018local\u2019, meaning that we want to explain y\u0302t = f(xt) for a target point t \u2208 1:n. It can also be global, meaning that we want to quantify the importance of variables in aggregate for a set of points.\nTo measure the importance of xj to f(x) it is usual to change xj , so x becomes x\u0303, and to record the corresponding change f(x\u0303)\u2212f(x). There are an enormous number of ways to do this. We have to choose the value or values of xj to start from, and the value or values to change it to. Then we have to decide whether or not to also change xj\u2032 for j\n\u2032 6= j while we are changing xj , and if we choose to change xj\u2032 we must specify how. When multiple changes are under consideration, we\nmust find a way to aggregate them and then compare or rank the aggregate measures for variables j \u2208 1:d.\nHere are a few concrete examples from among many choices. Classical sensitivity analysis based on |\u2202f(x0)/\u2202xj | starts with a default point x0 \u2208 Rd and considers infinitesimally small changes there, assuming that f is differentiable and the xj have been properly scaled relative to each other. The method of Morris (1991) makes small (local) changes of size \u2206j to xj from a fine grid of starting points for xj with xj\u2032 for j\n\u2032 6= j sampled independently and uniformly over their values. It looks at the mean, standard deviation and even the whole cumulative distribution function of the resulting f(x\u0303) \u2212 f(x) values. Campolongo et al. (2007) propose to aggregate |f(x\u0303) \u2212 f(x)| instead. Global sensitivity analysis (Razavi et al., 2021), about which we say more below, considers not just local changes but all possible changes from random x to random x\u0303 with some but not all x\u0303j = xj , aggregating the changes by second moments. The variable importance measure in random forests of Breiman (2001) makes a random permutation of x1j , . . . , xnj with respect to the other components of x1, . . . ,xn and records how prediction accuracy changes in response to permuting the jth variable.\nThere is not room to survey all variable importance methods and we must therefore leave many of them out and focus on the ones most closely related to our proposed method. Wei et al. (2015) provide a comprehensive survey of variable importance measures in statistics, organized around a taxonomy. They cite 197 references of which 24 are themselves surveys. The global sensitivity analysis literature is focused on climate models, computer aided design models and similar tools that often have a strong physical sciences emphasis. In addition to the recent survey of Razavi et al. (2021), with over 350 references, there are also textbooks of Saltelli et al. (2008) and Da Veiga et al. (2021). Molnar (2018) surveys the explainable AI (XAI) literature. Two prominent methods discussed there are LIME (Ribeiro et al., 2016) and SHAP (Lundberg and Lee, 2017). A survey of methods based on Shapley value appears in Sundararajan and Najmi (2020), which appeared as we were completing Mase et al. (2019)."
        },
        {
            "heading": "3.2 Attribution versus prediction",
            "text": "Our objective is to study f to explain it, and this is easier than studying a ground truth quantity like E(Y |x) because f is in our computer while E(Y |x) must ordinarily be estimated by gathering real-world data. A question about what changing xj to x\u0303j would do to f(\u00b7) can be directly answered by computing f(x\u2212j :x\u0303j) \u2212 f(x) for any setting x\u2212j of the other d \u2212 1 variables. Problems about causal inference when directed towards studying which variables cause f to change are amenable to direct computational solutions. On the other hand, explaining why f(x) = 1 instead of f(x) = 0 is harder. There could be several j \u2208 1:d that would have given rise to f(x\u2212j :x\u0303j) = 0 had we changed xj to some other value x\u0303j . In some problems every j could have changed f . We might also have to consider the effects of changing more than one component of x at a time. We then have to consider which changes to one or more components x are most worthy of consideration.\nThe variable importance problem is one of studying \u2018causes of effects\u2019 and not \u2018effects of causes\u2019. This distinction has been made repeatedly by Paul Holland; see, for example, Holland (1986, 1988) citing Mill (1851) and other philosophers. Holland\u2019s view is that statistical analysis is better suited to studying effects of causes. There is an excellent discussion of the difference between causes of effects and effects of causes in Dawid and Musio (2021) that also considers legal uses, such as a case about attributing a person\u2019s development of type II diabetes to past consumption of Lipitor. They consider at length the difficulties of using potential outcomes (Rubin, 1974) or structural causal\nmodels (Pearl, 2009) in settings where there is only one variable being considered as the possible cause of some effect. In Section 16, they mention briefly that the setting with multiple putative causes brings further difficulties. That is precisely the problem one faces in variable importance settings. The following quote is from their conclusions:\nWe have presented a thorough account of a number of ways in which the statistical problems of effects of causes (EoC) and of causes of effects (CoE) have been formulated. Although most treatments of statistical causality use essentially identical tools to address both these problem areas, we consider that this is inappropriate.\nIdentifying causes of effects is a problem of attribution. We seek to measure the extent to which different variables xj have contributed to the value of f(x). The big differences between attribution methods come down to their different definitions, which have different strengths and weaknesses. We cannot expect a \u2018bakeoff\u2019 where algorithms compete to estimate some known true attributions to settle a discussion about which definition is most appropriate. This means that the common task framework (Donoho, 2019) that has been so effective in improving prediction methods has limited utility for attribution. A second issue that Holland raises about \u2018causes of effects\u2019 is that the cause we identify might itself be an effect of some other cause. In causal inference, we can avoid adjusting for variables whose values became known after an intervention. For attribution, it is not so clear how to separate proximate causes from some prior causes. We might also have to think of even earlier ultimate causes of those prior causes.\nWe are familiar with the problem of attribution in real life. If a candidate lost an election by 10,000 votes and some issue caused an unfavorable vote swing of 11,000 votes, then we could at first reasonably attribute her loss to that issue. However, there may be ten such issues that individually or in combination brought a swing of over 10,000 votes. It is then less clear that the first issue is \u2018the reason\u2019. The attribution issue has been intensely studied in advertising. In media mix modeling (MMM), users are told what percentage of their sales to attribute to each advertising channel; see Chan and Perry (2017) for a discussion of challenges in MMM. In online advertising, Shapley values are now being used instead of simply attributing a sale or other customer conversion to the last click prior to that event (Berman, 2018). Shapley values are also being used in financial profit-and-loss attribution (Moehle et al., 2021).\nOne maxim in causal inference is that there is \u2018no causation without manipulation\u2019; see, for instance, Holland (1986). Under this maxim, if we cannot in principal intervene to change a variable then we cannot make claims about its causal effect, at least not via the potential outcomes framework where both a treatment and an alternative must be possible. Judea Pearl disagrees with this maxim. For instance Bollen and Pearl (2013) remark that this principal would lead to a conclusion that the moon does not cause tides. In problems of attribution, we cannot avoid discussing the importance of variables, such as a specific person\u2019s birth year or ancestry, that an investigator cannot intervene to change."
        },
        {
            "heading": "3.3 Challenges to variable importance: interaction and dependence",
            "text": "Now we turn to quantifying the effect of xj on f(x). The setting is simplest for an additive model of independent random inputs. Suppose that\nf(x) = d\u2211 j=1 fj(xj) (1)\nwhere xj has support Xj and x1, . . . , xd are independent random quantities with fj : Xj \u2192 R. Global importance measures can then be defined in terms of semi-norms of the fj . We could choose var(fj(xj)) or var(fj(xj))\n1/2 or supxj\u2208Xj fj(xj)\u2212 infxj\u2208Xj fj(xj). If Xj \u2286 R and fj has derivative f \u2032j , then we could choose supxj\u2208Xj |f \u2032(xj)| or the total variation \u222b Xj |f\n\u2032(xj)|dxj or E(|f \u2032(xj)|). Semi-norms make sense because replacing fj(xj) by fj(xj) + c and fj\u2032(xj\u2032) by fj\u2032(xj\u2032)\u2212 c does not reasonably change the importance of xj and xj\u2032 . Implicit in each of these measures is a choice of how to combine the effects of changes in fj and hence in f over pairs xj , x\u0303j \u2208 Xj . These various semi-norms quantify importance in different ways, but the setting of equation (1) is the one where it is most straightforward to choose a definition for a given problem for global and also local variable importance. The effect of changing xj is unrelated to the value of xj\u2032 and given a value for xj we have no reason to direct our attention about xj\u2032 to any specific part of Xj\u2032 .\nVariable importance becomes much more difficult when the additivity condition (1) does not hold. In that case the variables have interactions, and the importance of those interactions has to be shared somehow among the variables that contribute. A second complication is that the distribution of x strongly affects which variable is important. To illustrate this effect, note that according to the CDC https://www.cdc.gov/measles/about/faqs.html about 3% of people immunized against measles will get measles if they are exposed to it. Let\u2019s assume that anybody not exposed does not get measles and also, since it is very contagious, that 90% of people not immunized will get it if exposed. Table 3 illustrates how an interaction can complicate the relative importance of variables. If almost everybody is in the upper left cell, then exposure is the more important variable. If almost everybody is in the lower right cell then immunity is the more important variable. Note that this second complication from interactions is present already in the setting of independent input variables. For a thorough account of the phenomenon of interaction, see Cox (1984) and De Gonzalez and Cox (2007).\nWhile interactions can be conveniently handled by Sobol\u2019 indices based on the ANOVA decomposition as we will see in Section 4, dependence among the components of x is far harder to deal with. In that case, the reasonable changes to xj can depend on all components in x\u2212j and their combinations. The Boston housing data illustration in Figure 1 shows a setting with two variables where at least one must be near a default value. An extreme version of this problem is given in Owen and Prieur (2017) where x \u2208 {0, 1}2 but the only possible values of x are {0, 0}, {0, 1} and {1, 0}. If x = {0, 1} then it is not meaningful to change x1 while leaving x2 unchanged, so such a change cannot contribute to the importance of variable 1. Higher dimensional patterns will be much harder to detect.\nThe challenges brought by dependence are not easy to handle via the ANOVA. See Chastaing et al. (2012) and Owen and Prieur (2017). When there are dependencies among variables, we can turn from ANOVA to the Shapley value that we describe in Section 5. Shapley value opens up some conceptually attractive solutions that nonetheless have computational challenges such as estimating an exponentially large number of conditional expectations.\nOur approach of studying changes to f does not capture every variable importance measure in widespread use. For instance the importance of predictors in spike-and-slab regression models is often measured by the posterior probability that \u03b2j 6= 0 which is not directly obtainable from f(x) = E\u0302(y |x) (Scott and Varian, 2014). Such measures provide a useful aggregate for the role of xj but do not easily explain local decisions. Similarly, counts of the number of times a variable is used to define a split in a tree or forest model do not have a direct interpretation in terms of the value of f ."
        },
        {
            "heading": "4 ANOVA, Sobol\u2019 indices, and Shapley value",
            "text": "In this section we describe some of the tools we need in order to present the cohort Shapley measure. These are general analysis of variance (ANOVA) decomposition, Sobol\u2019 indices, and Shapley value."
        },
        {
            "heading": "4.1 General ANOVA",
            "text": "When the components of x are independent and E(f(x)2) < \u221e, there is a convenient analysis of variance (ANOVA) decomposition of f . This ANOVA can be used to define variance components and subsequently Sobol\u2019 indices. Sobol\u2019 indices provide a very powerful way to quantify global importance of variables. They do not provide local explanations, and, as we will see below, they have difficulty with dependent data settings.\nThe familiar ANOVA used in experimental design applies to tabular data defined in terms of categorical xj . It goes back to Fisher and Mackenzie (1923). A generalization to x \u223c U[0, 1]d was used by Hoeffding (1948), Sobol\u2019 (1969) and Efron and Stein (1981) for U-statistics, integration and the jackknife, respectively. The ANOVA decomposes \u03c32 = var(f(x)) into components for each u \u2286 1:d. For d =\u221e see Owen (1998).\nTo understand the generalized ANOVA, we recall how the orginal one works. We are given f(x) where now xj takes kj levels and we have all N = \u220fd j=1 kj possible evaluations of f . The xj are then independent random variables under equal weighting of those N values. We first compute a grand mean \u00b5 averaging all the values f(x1), . . . , f(xN ). Next, to define the main effect of variable j, we subtract \u00b5 from each f(xi) and at each level of xj we average those differences over N/kj settings of the other d \u2212 1 variables. The idea is that only differences merit an explanation. For an interaction between variables j and j\u2032, we subtract the grand mean and both main effects from f(xi) and for each of the kjkj\u2032 combinations of xj and xj\u2032 we average the results over all N/(kjkj\u2032) settings of the other d \u2212 2 variables. The general rule is like this: we don\u2019t attribute to xu what can be explained by xv for v ( u, then to get a function of xu we average over x\u2212u.\nThe general ANOVA replaces averages by expectations. It begins with a grand mean function f\u2205(x) which is everywhere equal to the constant \u00b5 \u2261 E(f(x)). The main effect for variable j is the function\nf{j}(x) = E(f(x)\u2212 \u00b5 |xj) = E(f(x) |xj)\u2212 \u00b5.\nFor a subset of variables u \u2286 1:d, we define\nfu(x) = E ( f(x)\u2212 \u2211 v(u fv(x) \u2223\u2223xu) = E(f(x) | xu)\u2212\u2211 v(u fv(x). (2)\nThe first expression in (2) shows that we do not attribute to xu what can be explained by xv for any proper subset v of u. The second expression follows because fv is not random given xu. While fu is defined on the whole domain of x its value only depends on xu.\nWhen |u| > 1, the effect fu is a |u|-fold interaction. The effect fu has variance component \u03c32u = var(fu(x)). We easily find that\nf(x) = \u2211 u\u22861:d fu(x) and \u03c3 2 = \u2211 u\u22861:d \u03c32u."
        },
        {
            "heading": "4.2 Sobol\u2019 indices",
            "text": "Sobol\u2019 indices (Sobol\u2019, 1993) measure the importance of the subvector xu not just individual variables xj . There are two versions, the closed index and the total index, respectively,\n\u03c42u = \u2211 v\u2286u \u03c32v and \u03c4 2 u = \u2211 v:v\u2229u 6=\u2205 \u03c32v .\nThese are usually divided by \u03c32 to give a proportion of variance explained. We easily see that 0 6 \u03c42u 6 \u03c4 2 u = \u03c3\n2 \u2212 \u03c42\u2212u. The total index \u03c42u includes interactions between xu and x\u2212u that the closed index \u03c42u excludes. If \u03c4 2 u is relatively large then we interpret xu as important while if \u03c4 2 u is small then xu can be interpreted as unimportant. One can show that \u03c4 2 u = var(E(f(x) |xu)) and \u03c42u = E(var(f(x) |x\u2212u)). Sobol\u2019 indices have been intensely studied in the global sensitivity analysis literature. They are very conveniently estimated by sampling methods based on identities like\n\u03c42u = E ( f(x)f(xu:z\u2212u) ) \u2212 \u00b52 and \u03c42u = 1 2 E ( (f(x)\u2212 f(x\u2212u:zu))2 ) due to Sobol\u2019 (1993) and Jansen (1999), respectively.\nSobol\u2019 indices provide a very good global measure of how important a variable or even a set of variables is. They are not aimed at variable importance at a specific point x, so they are not suited to local importance.\nWe mentioned earlier that the ANOVA does not extend well to dependent data settings. Since Sobol\u2019 indices are based on the ANOVA, this causes difficulties for them. The ANOVA for dependent data has been considered by Chastaing et al. (2012) building on work of Hooker (2012) who builds on that of Stone (1994). One of the challenges is that some ways to define ANOVA lead to negative analogues of variance components for dependent data. A key condition in some of the definitions is that the density p(x) must be bounded below by c\u00d7 pu(xu)\u00d7 p\u2212u(x\u2212u) for some c > 0 and all x. This rules out distributions with \u2018holes\u2019 where p(x) = 0 on a rectangle within the support of p. It also rules out multivariate Gaussian distributions with nonzero correlations.\nThe two aforementioned problems with Sobol\u2019 indices do not apply to Shapley value which we consider next."
        },
        {
            "heading": "4.3 Shapley value",
            "text": "The Shapley value (Shapley, 1953) is used in cooperative game theory to define a fair allocation of rewards to team members who have jointly produced some value. It has seen many uses in defining variable importance measures. See Sundararajan and Najmi (2020) and Molnar (2018) for surveys\nof uses in XAI and Song et al. (2016) in global sensitivity analysis. The analogy is to view the variables xj in a model f(x) as team members cooperating on a goal such as predicting y.\nIn Shapley\u2019s formulation, there is a set 1:d of entities. Together they produce a value that we denote by \u03bd(1:d). We suppose that we can find the value \u03bd(u) that would have been produced by any u \u2286 1:d. It is customary to assume that \u03bd(\u2205) = 0. Shapley proposes four quite reasonable axioms on the basis of which there is then a unique value \u03c6j = \u03c6j [\u03bd] that should be attributed to player j on the team. A key quantity in Shapley\u2019s formulation is the incremental value that would be brought to a team u if player j 6\u2208 u were to join it. This incremental value of player j given that players in u are already on the team is\n\u03bd(j |u) = \u03bd(u \u222a {j})\u2212 \u03bd(u).\nShapley\u2019s four axioms are: 1) Efficiency: \u2211d j=1 \u03c6j = \u03bd(1:d).\n2) Dummy: If \u03bd(j |u) = 0 whenever j 6\u2208 u, then \u03c6j = 0. 3) Symmetry: If \u03bd(j |u) = \u03bd(j\u2032 |u) whenever j, j\u2032 6\u2208 u then \u03c6j = \u03c6j\u2032 . 4) Additivity: For games \u03bd and \u03bd\u2032, \u03c6j [\u03bd + \u03bd \u2032] = \u03c6j [\u03bd] + \u03c6j [\u03bd \u2032].\nThe second and third axioms are compelling. Molnar (2018) suggests that the first axiom might even be a requirement in settings where one is required by law to provide an explanation. See also Doshi-Velez et al. (2017). The fourth axiom is more debatable but is usually included in variable importance applications and we will use it. Under these four axioms there is a unique solution\n\u03c6j = 1\nd \u2211 u\u2286\u2212j ( d\u2212 1 |u| )\u22121 \u03bd(j |u). (3)\nAn intuitive explanation of (3) is as follows: Suppose that we build a team from \u2205 to 1:d in one of d! possible orders and track the incremental value that arises at the moment when player j joins this team. Then \u03c6j is the average of those d! incremental values. Actually averaging all permutations would have a cost that is O(d!). The direct calculation of (3) depends on only d2d\u22121 incremental values: each of 2d sets u has d neighboring sets that differ by inclusion or deletion of exactly one of the d team members. Every pair (u, j) with u \u2282 1:d and j 6\u2208 u is counted twice this way.\nFor illustration, suppose that d = 3 as in our running example. Then there are 6 permutations of the three variables. Variable 1 enters first in two of them, following \u2205. It enters last in two of them, following {2, 3}. It enters once after just {2} is present and once after just {3} is present. Therefore\n\u03c61 = 1\n6\n( 2 [ \u03bd({1})\u2212 \u03bd(\u2205)] + 2 [ \u03bd({1, 2, 3})\u2212 \u03bd({2, 3})] + [ \u03bd({1, 2})\u2212 \u03bd({2})] + [ \u03bd({1, 3})\u2212 \u03bd({3})] ) (4)\nand the other \u03c6j are similar. The Shapley values \u03c6j are unchanged if we add a constant c to all of the values \u03bd(u). This follows because c cancels out of the incremental values. As remarked above, it is common to assume that \u03bd(\u2205) = 0. Sometimes it is simpler to define a value function in a way that does not force \u03bd(\u2205) = 0. In such cases the Shapley values attribute the incremental value \u03bd(1:d) \u2212 \u03bd(\u2205) of the whole team to the members of that team.\nThe cost to compute Shapley values exactly grows exponentially in d. The representation via permutations opens up a natural Monte Carlo strategy of using an average over a sample of randomly generated permutations. Monte Carlo is routinely used in settings such as integration where an exact computation would have infinite cost. Its effectiveness depends on the variance of the incremental values and not on the number d2d\u22121 of incremental values needed for exact calculation. Michalak et al. (2013) and Mitchell et al. (2022) describe some value functions where they find that Monte Carlo is inefficient. For cohort Shapley, Monte Carlo would be unfavorable if only a vanishing fraction of the incremental values \u03bd(j |u) were non-negligible. We consider this unlikely and have not seen it in our experience.\nThe integrated gradients method of Sundararajan et al. (2017) is based on the Aumann-Shapley value of Aumann and Shapley (1974). Where the Shapley value averages difference over paths on the edges of {0, 1}d, integrated gradients average a gradient over the diagonal of [0, 1]d which can be much faster to compute."
        },
        {
            "heading": "5 Using Shapley for importance",
            "text": "In this section, we show how Shapley values have been used to study variable importance. Then we present the cohort Shapley algorithm that only uses actually observed data for variable importance. We also include a discussion of some tradeoffs and choices that come up when using Shapley values for variable importance. There are many different ways to formulate a game with which to apply the Shapley formulations. The axioms have considerable logical force, conditional on the value function being a reasonable one. As a result, the important differences among methods based on Shapley values stem from the choice of value function."
        },
        {
            "heading": "5.1 Some past uses of Shapley value",
            "text": "Lindeman et al. (1980) propose a measure of the importance of including a variable in a linear regression model. Their proposal is equivalent to using the Shapley value with \u03bd(u) is proportion of the variance explained by a linear model (usually denoted R2) using the variables xu. S\u030ctrumbelj and Kononenko (2010) used Shapley value to explain the value of f(xt) in a local explanation for target observation t. In their equation (1), they use the value function\n\u03bd(u) = E(f(xt,u:x\u2212u))\u2212 E(f(x))\nwhere the first expectation is taken with respect to independent discrete random variables xj for j 6\u2208 u and the second one has all xj from independent discrete distributions. Those discrete distributions could be the empirical marginal distributions. They also consider random sampling approximation that allows continuous distributions.\nFor x with independent components, Owen (2014) takes \u03bd(u) to be the variance explained by xu, var(E(f(x) |xu)) = \u03c42u. The result is that\n\u03c6j = \u2211 u\u22861:d 1{j \u2208 u}\u03c32u |u| .\nEvery variance component \u03c32u is shared equally among the |u| variables that contribute to it. It follows that the easily estimated Sobol\u2019 indices bracket the Shapley value:\n\u03c42j 6 \u03c6j 6 \u03c4 2 j .\nPlischke et al. (2021) note that the upper bound can be improved to (\u03c42j + \u03c4 2 j )/2.\nSong et al. (2016) propose the use of Shapley value with \u03bd(u) = var(E(f(x) |xu)) for a setting where components of x may be dependent. They present a computational algorithm based on permutations and they show that the same Shapley values arise using \u03bd(u) = E(var(f(x) |x\u2212u)). Owen and Prieur (2017) describe how Shapley values solve the conceptual problems that dependence causes for Sobol\u2019 indices, though computational challenges remain.\nBaseline Shapley (see Sundararajan and Najmi (2020)) explains f(xt)\u2212f(xb) where t is a target point and xb is a \u2018baseline\u2019 vector of predictors. It could be made of default values or it could be x\u0304 = (1/n) \u2211n i=1 xi, although some components of x\u0304 might not be possible feature values. In baseline Shapley, the value function is \u03bd(u) = f(xt,u:xb,\u2212u) and unobserved hybrid values xt,u:xb,\u2212u are to be expected.\nMany of the alternatives to baseline Shapley use a \u2018conditional expectation Shapley\u2019 with\n\u03bd(u) = E(f(xt) |xt,u)\ndiffering according to which distribution of xt,\u2212u given xt,u they use to define the expectation. For instance the above mentioned choice of S\u030ctrumbelj and Kononenko (2010) is of this type assuming independent predictors. The quantitative input influence of Datta et al. (2016) takes \u03bd(u) = E(f(xu:z\u2212u) | xu) under a sampling model where z\u2212u has the empirical marginal distribution of x\u2212u. That will generally produce unobserved hybrid points. Kernel SHAP (Lundberg and Lee, 2017), similarly uses an independence assumption on inputs to compute a conditional Shapley value.\nThe TreeSHAP method (Lundberg et al., 2020) is designed to be especially computationally efficient on tree structured models but can only be used on such models. If the prediction is a weighted sum of tree predictions then, using additivity of the Shapley value one can take a weighted sum of Shapley values of single trees. For a single tree, the value is E(f(x) |xu) where the distribution of f(x\u2212u) given xu is uniform over all the values of f(xi) in all the leaves of the tree that are compatible with xu. It only uses observed data and it cannot attribute importance to variables that are not used by the model.\nFrye et al. (2021) share our concerns about using impossible data. They use a variational autoencoder (VAE) to represent the d predictors as being scattered around a lower dimensional manifold. They compute E(f(x) |xu) using their model to define the distribution of x\u2212u given xu, but not every point x they use will have been observed, so whether those are reasonable depends on performance of the chosen VAE. Similarly, Aas et al. (2019) propose a conditional kernel SHAP that uses weights from an ellipsoidally shaped kernel based on a Mahalanobis distance between target and reference points using the sample covariance. They need a scaling parameter for each subset of input variables. Their kernel puts positive weight on unobserved variable combinations.\nFinally, we note that Ghorbani and Zou (2019) use Shapley value to measure the value of individual data points in a model. They compare this to leave-one-out methods and leverage scores and find that it helps to identify outliers and corrupted data. For instance, data points with a negative Shapley value for model accuracy are worthy of inspection; perhaps the response value was mislabeled or there is some other error in that data.\nSome more uses of Shapley value in XAI are surveyed in Sundararajan and Najmi (2020), Molnar (2018) and Kumar et al. (2020)."
        },
        {
            "heading": "5.2 Cohort Shapley",
            "text": "The cohort Shapley method quantifies variable importance using only actually observed data values. Our description here is based on Mase et al. (2019).\nFor target t and variable j we define a function st,j where st,j(xij) = 1 if xij is similar to xtj and is zero otherwise. It must always be true that st,j(xtj) = 1. For binary variables, similarity is just whether xij = xtj . For real valued features j, similarity could be that |xij \u2212 xtj | 6 \u03b4 or |xij \u2212 xtj | 6 \u03b4|xtj | for some \u03b4 > 0, or we could discretize xij values into a set of ranges and take observations to be similar if they are in the same range. This last choice makes similarity transitive, which the other choices do not impose.\nFor observation t and u \u2286 1:d, define the cohort\nCt(u) = {i \u2208 1:n | st,j(xij) = 1, all j \u2208 u}, (5)\nwith Ct(\u2205) = 1:n by convention. The value function in cohort Shapley is\n\u03bd(u) = 1 |Ct(u)| \u2211\ni\u2208Ct(u)\nf(xi),\nfor local explanation for observation t. These values are all well defined because t \u2208 Ct(u) always holds. In the random permutation framework, we begin with the cohort Ct(\u2205) of all observations whose mean is then the global mean (1/n) \u2211n i=1 f(xi). We then refine the cohort, imposing similarity with respect to all d variables in order. An important variable is one that brings a relatively large change to the cohort mean when we refine on it. The cohort Shapley values \u03c6j explain the difference between the mean of f(xi) over the fully refined cohort Ct(1:d) and the global mean. If all input variables have been binned, then cohort Shapley becomes a conditional expectation Shapley defined in terms of the binned variables.\nIn applying cohort Shapley, one is forced to define what it means for xij to be similar to xtj . This is difficult for continuous variables. Modest numbers of levels are usually recommended in similar settings that require \u2018binning\u2019 a continuous variable. Gelman and Park (2009) use means over the largest and smallest of just three groups to approximate a regression slope with small loss of efficiency. Cochran (1968) finds diminishing returns to using more than five strata in sampling problems. Cochran\u2019s results are based on linear regression and nonlinear relationships would reasonably require a few more strata. For a modestly nonlinear setting like propensity scores for logistic regression, Neuha\u0308user et al. (2018) find diminishing returns at around ten strata.\nWhile binning a variable can seem arbitrary we think that it is more transparent than employing yet another black box, such as a VAE, to approximate empirical conditional distributions.\nWe can understand TreeSHAP in terms of cohorts. For a single tree it uses cohorts defined by a union of leaves of that tree. Such a union of leaves need not define a spatially connected set when X = Rd. For aggregates of multiple trees the notion of similarity will generally differ between the trees in that aggregate. Our concern with TreeSHAP is that it uses a notion of variable similarity defined in part by the response values it is fitting. This makes it harder to interpret or explain the underlying similarity concept. In a high stakes setting, such as whether a given algorithm was unfair to one or more people, we would be interested to know who those people were compared to. It is also not able to attribute importance to a variable that the model does not use.\nCategorical variables with many levels may need to be coarsened similarly to continuous variables. If all of the variables are categorical or have been made categorical by stratification then\ncohort Shapley matches conditional expectation Shapley based on the empirical distribution of the data."
        },
        {
            "heading": "5.3 Running example revisited",
            "text": "Table 4 shows the cohorts of target observation 1 for our running example. Potential observation 7 is unobserved in one of the data distributions and so it is removed from the two cohorts that it would have appeared in had all 8 x values been observed. That table also records the cohort means for our two distributions and two example functions, f1(x) = x1 \u2212 2x2 and f2(x) = x1x2(1\u2212 x3).\nTable 5 shows cohort Shapley values for our running example. For independent data and f1(x) = x1 \u2212 2x2 the average value of f(xi) over all data (cohort C1(\u2205)) is \u03bd(\u2205) = \u22121/2. For target observation t = 1 and cohort C1({1, 2, 3}) = {1} we get \u03bd({1, 2, 3}) = 0. Therefore we have to explain why f(xt) is 1/2 unit above average. There are six equally weighted paths from \u2205 to {1, 2, 3}. As described at equation (4), we get\n\u03c61 = 1\n6\n( 2 [ \u22121/2] + 2 [ \u22121/2] + [ \u22121/2] + [ \u22121/2] ) = \u22121\n2 .\nSimilarly, \u03c62 = 1 and \u03c63 = 0. So the target observation t = 1 has f that is 1/2 unit above average and the cohort Shapley attributions are that this arises from a \u22121/2 effect of having x1 = 0 offset by a +1 effect of having x2 = 0 while x3 does nothing. We see that \u03c6j takes account of not just the sign of the coefficient of xj but also whether xtj is at the low or high value of xj .\nIf x = (1, 1, 0) is impossible, as in distribution P\u2217, then that introduces some amount of dependence between x3 and the other variables. Now \u03c63 6= 0 for the target observation and the linear function f1 = x1 \u2212 2x2 that does not involve x3. For f2 = x1x2(1 \u2212 x3) under the independence distribution P , the target observation has f2(xt) = 0 while the average of f2 is 1/8. The Shapley attributions are \u22121/12 for both x1 and x2 and +1/24 for x3 owing to the factor 1\u2212 x3. Under P\u2217, f2 is only nonzero at the impossible combination and then all \u03c6j = 0."
        },
        {
            "heading": "5.4 Tradeoffs and issues",
            "text": "There are several difficult choices to consider in rating variable importance generally and some difficult choices specific to methods based on Shapley value. These stem from \u2018importance\u2019 meaning different things in different contexts. Kumar et al. (2020) present several qualms about the use of Shapley value in any of its forms for variable importance.\nOne consequential decision is whether to attribute any importance to a variable xj that is not used at all in f(x). In a case like this, changes to xtj cannot possibly change the prediction y\u0302t = f(xt) so it might seem unreasonable to attribute importance to xtj . Suggesting that person t change xtj to qualify for a loan would be bad advice. However there are settings in algorithmic fairness where a protected variable such as race or gender that is not used by f(\u00b7) might still play a role through its association with other variables that were used. In those applications, it is essential to consider the possibility that xj could be important. Fairness by unawareness is not considered reliable (Dwork et al., 2012). Cohort Shapley can be used to identify importance for variables not used by the function. To do that, one includes that variable in the set of predictors under study. We illustrate this in Section 6, in an example involving race.\nA potential problem with Shapley values arises when two variables xj and xj\u2032 are very strongly related. This can cause them to \u2018share their importance\u2019 which might make both of them appear unimportant. Or, as Kumar et al. (2020) note, if one variable denotes a protected category and the other does not, including both in the Shapley calculations could make the protected variable look less important than it really is."
        },
        {
            "heading": "6 Algorithmic Fairness and the COMPAS data",
            "text": "Algorithmic fairness makes use of variable importance. For instance, it is unfair when a protected variable such as race or gender is improperly important in a decision that affects a person. In this section, we conduct a detailed investigation of the data from Angwin et al. (2016) regarding the fairness of the COMPAS tools and their use in recidivism prediction. The focal issue is whether COMPAS is unfair to Black defendants, and if so, by how much and to which defendants? The results in this section were presented earlier in Mase et al. (2021), which this article supercedes.\nThere are several aspects of cohort Shapley that make it very suitable to analyzing fairness of the COMPAS data: 1) it is able to judge fairness with respect to race for an algorithm that was constructed without\n2) it is able to work without access to f , using instead its predictions, which in this instance were made by a proprietary algorithm that is not available to researchers; 3) the additivity property of Shapley value means that the Shapley values for residuals (actual versus predicted response) equal the corresponding differences between Shapley values for observations and their predictions; 4) the additivity property of Shapley value lets us aggregate values over a set of defendants, such as all Black women; 5) we can work with a data set that is not necessarily the one the algorithm was trained on; 6) because we have samples of (xi, yi, y\u0302i) for defendants i we can use bootstrap sampling to get\na measure of sampling uncertainty in our Shapley values for either individuals or aggregates of individuals.\nCohort Shapley is the unique method from Table 1 to satisfy condition 2 and one of only two that satisfy condition 1. The uncertainty in condition 6 is distinct from uncertainty due to random sampling of permutations. In this analysis we are able to compute cohort Shapley exactly and the uncertainty arises from viewing the defendants as a sample from a larger population. This setting is simpler to bootstrap because the model was trained on a separate data set."
        },
        {
            "heading": "6.1 Background on fairness",
            "text": "Just as there are many ways to define variable importance, there are multiple ways to define what fairness means. Some of those definitions are mutually incompatible and some of them differ from legal definitions. Here we present just a few of the issues as a prelude to studying the COMPAS data. See Corbett-Davies and Goel (2018), Chouldechova and Roth (2018), Berk et al. (2018), and Friedler et al. (2019) for surveys. We do not make assertions about which definitions are preferable.\nFor y, y\u0302 \u2208 {0, 1}, let nyy\u0302 be the number of observations with yi = y and y\u0302i = y\u0302. These four counts and their derived properties can be computed for any subset of the observations. The false positive rate (FPR) is n01/n0\u2022, where a bullet indicates that we are summing over the levels of that index. We ignore uninteresting corner cases such as n0\u2022 = 0; when there are no observations with y = 0 then we have no interest in the proportion of them with y\u0302 = 1. The false negative rate (FNR) is n10/n1\u2022. The prevalence of the trait under study is p = n1\u2022/n\u2022\u2022. The positive predictive value (PPV) is n11/n\u20221. As Chouldechova (2017, equation (2.6)) notes, these values satisfy\nFPR = p 1\u2212 p 1\u2212 PPV PPV (1\u2212 FNR). (6)\nSee also Kleinberg et al. (2016). Equation (6) shows how some natural definitions of fairness conflict. FPR and FNR describe y\u0302 | y, while PPV describes y | y\u0302. If two subsets of observations have the same PPV but different prevalences p, then they cannot also match up on FPR and FNR. Fairness in y | y\u0302 terms and fairness in y\u0302 |y terms can only coincide in trivial settings such as when y\u0302 = y always or empirically unusual settings with equal prevalence between observations having different values of a protected variable.\nThere is some debate about when or whether using protected variables can lead to improved fairness. See Xiang (2020) who gives a summary of legal issues surrounding fairness and CorbettDavies et al. (2017) who study whether imposing calibration or other criteria might adversely affect the groups they are meant to help.\nDatta et al. (2016) attributes demographic parity of the prediction y\u0302 to input variables through aggregated Shapley value. We quantify individual level bias on protected variables using cohort\nShapley and then aggregate to a measure for any group of interest. Cohort Shapley can measure quantities of various fairness definitions that are conventionally quantified by group level statistical measures."
        },
        {
            "heading": "6.2 COMPAS recidivism data",
            "text": "The COMPAS Core Risk and Needs Assessment tool from Northpointe Inc. includes the General Recidivism Risk Scale (GRRS) and Violent Recidivism Risk Scale (VRRS) which were investigated by Angwin et al. (2016). A more complete description of these scales can be found in Brennan et al. (2009) and Northpointe (2019). We will focus on the GRRS data collected and published by Angwin et al. (2016) in the analysis that follows when referring to the \u201cCOMPAS data\u201d. Each defendant is rated into one of ten deciles with higher deciles considered higher risk of reoffending. Angwin et al. (2016) investigated whether that algorithm was biased against Black people. They obtained data for defendants in Broward County Florida, including the COMPAS decile, the defendants\u2019 race, age, gender, number of prior arrests and whether the crime for which they were charged is a felony or not. Angwin et al. (2016) describe how they processed their data including how they found followup data on offences committed and how they matched those to defendants for whom they had prior COMPAS scores. They also note that race was not one of the variables used in the COMPAS predictions, and the original model for those predictions was trained on more features than those available and included in this Broward County data. Angwin et al. (2016) look at pretrial defendants and measure recidivism as being arrested for another crime within two years, and we will use the same metrics and data for our analysis so that it can be compared to the original work. This example is controversial. For a more complete discussion of the appropriateness of using this data to evaluate COMPAS including the important discrepancies between pre-trial vs probation or parole use and the time frame for reoffending, see Rudin et al. (2020), Flores et al. (2016), Dieterich et al. (2016), and Jackson and Mendoza (2020). While this data is therefore likely inadequate for the purposes of evaluating the true merits of COMPAS, we include it as an example here given its place in the literature as a known benchmark for meaningful comparison.\nAngwin et al. (2016) find that COMPAS is biased because it gave a higher rate of false positives for Black defendants and a higher rate of false negatives for White defendants. Flores et al. (2016) and Dieterich et al. (2016) disagree, raising the issue of y\u0302 | y fairness versus y | y\u0302 fairness. The prevalence of reoffences differed between Black and White defendants forcing y | y\u0302 and y\u0302 |y notions to be incompatible. Tan et al. (2018) find some evidence of racial bias when comparing an interpretable surrogate model trained to predict y\u0302 to one trained on y, but stop short of attributing that bias to COMPAS itself instead pointing to the difference between features used to train COMPAS and those present in the data set. Agarwal et al. (2021) also find evidence of racial bias in an interpretable surrogate model trained to predict COMPAS scores. Rudin et al. (2020) find no evidence that the COMPAS scores depend on race except through age and criminal history. Further they find that the same racial FPR/FNR discrepancies detected by Angwin et al. (2016) when looking at the COMPAS scores would exist when using a model that relied solely on age.\nOur cohort Shapley analysis below finds statistically significant racial effects. White defendants tend to have higher than predicted rates of recidivism while Black defendants tend to have lower than predicted rates of recidivism. The magnitudes of those effects are in the range of 3.5 to 5.5 percent when measured by the average of y\u2212 y\u0302. Consistent with some of the earlier findings we see larger effects of race on the false positive and false negative rates.\nBeyond these aggregate findings, our analysis provides local (i.e., individual level) racial effects.\nThis would be useful in looking at a specific case of interest or, as we show below, for finding that the impact of race varies by gender. While some previous work like Angwin et al. (2016) and Rudin et al. (2020) have examined specific individuals or pairs of individuals as examples, this work is the first to present a method that quantifies this local fairness systematically.\nFollowing Chouldechova (2017) we focus on just Black and White defendants. That provides a sample of 5278 defendants from among the original 6172 defendants. As in that paper we record the number of prior arrests as a categorical variable with five levels: 0, 1\u20133, 4\u20136, 7\u201310 and >10. Following Angwin et al. (2016), we record the defendants\u2019 ages as a categorical variable with three levels: <25, 25\u201345 and >45. Also, following Chouldechova (2017), we consider the prediction y\u0302i to be 1 if defendant i is in deciles 5\u201310 and y\u0302i = 0 for defendant i in deciles 1\u20134.\nFigure 2 shows some conventional group fairness metrics for the COMPAS data set. Horizontal bars show group specific means and the vertical dashed lines show population means. We see that Black defendants had a higher average value of y\u0302 than White defendants. Black defendants also had a higher average of y but a lower average residual y \u2212 y\u0302. Using B and W to denote the two racial groups, E\u0302(y \u2212 y\u0302 | B) .= \u22120.035 and E\u0302(y \u2212 y\u0302 |W ) .= 0.054. The FPR was higher for Black defendants and the FNR was higher for White defendants."
        },
        {
            "heading": "6.3 Exploration of the COMPAS data",
            "text": "In this section we use cohort Shapley to study some fairness issues in the COMPAS data, especially individual level metrics. We have selected what we found to be the strongest and most interesting findings related to race and gender from Mase et al. (2021). Histograms there also show effects due to age and the number of prior arrests.\nWe have computed Shapley impacts for these responses: yi, y\u0302i, yi\u2212 y\u0302i, FPi = 1{yi = 0 & y\u0302i = 1} and FNi = 1{y1 = 1 & y\u0302i = 1}. If FPi = 1 then defendant i received a false positive prediction.\nNote that the sample average value of FPi\nE\u0302(FPi) = n01 n\u2022\u2022 = FPR\u00d7 n0\u2022 n\u2022\u2022 = FPR\u00d7 (1\u2212 p), and E\u0302(FNi) = FNR\u00d7 p"
        },
        {
            "heading": "6.4 Graphical analysis",
            "text": "Figure 3 shows histograms of Shapley impacts of race for the defendants in the COMPAS data. The first panel there shows a positive impact for every Black defendant and a negative one for every White defendant for the prediction y\u0302. For the actual response y, the histograms overlap slightly. By additivity of Shapley value the impacts for y \u2212 y\u0302 can be found by subtracting the impact for y\u0302 from that for y for each defendant i = 1, . . . , n. The histograms of yi \u2212 y\u0302i show that the impact of race on the residual is typically positive for White defendants and negative for Black defendants.\nThe histograms of Shapley impacts for race overlap for the two metrics, FPR and FNR. There, a small number of adversely affected White defendants and beneficially affected Black defendants are observed. We can inspect the corresponding defendants to see which conditions are the exceptional cases. For example, the 469\u2019th defendant (Black) and the 486\u2019th defendant (White) have the same values for all other features. They both had 4-6 prior arrests, age <25, crime severity of felony, and both were female. In Section 6.6 we give an example with a confidence interval for the Shapley values of an individual defendant. The same could be done for a difference between two defendant\u2019s Shapley values.\nFigure 4 shows histograms of Shapley impacts for race color-coded by the defendants\u2019 gender. We see that the impact on y\u0302 is bimodal by race for both male and female defendants, but the effect is larger in absolute value for male defendants. The impacts for the response, the residual and false positive values do not appear to be bimodal by race for female defendants, but they do for male defendants. The impacts for false negatives do not appear bimodal for either race. It is clear from these figures that the race differences we see are much stronger among male defendants."
        },
        {
            "heading": "6.5 Tabular summary and bootstrap",
            "text": "We take a particular interest in the residual, or error, yt\u2212 y\u0302t. It equals 1 for false negatives and \u22121 for false positives and 0 when the prediction was correct. Table 6 shows race and gender Shapley values for the residual yi \u2212 y\u0302i aggregated over all four race-gender subsets of defendants.\n0.1 0.0 0.1 Impact (prediction Y)\n0\n1000\n2000\n0.1 0.0 0.1 Impact (response Y)\n0\n1000\n2000\n0.1 0.0 0.1 Impact (residual Y Y)\n0\n1000\n2000\nWhat we see there is that revealing that a defendant is Black tells us that, on average, that defendant\u2019s residual yt\u2212 y\u0302t is decreased by 3.6%. By this measure, the Black defendants offend less than predicted. Revealing that the defendant is White increases the residual by 5.4%. Revealing race makes very little difference to the residual averaged over male or over female defendants (of both races). Revealing gender makes a relatively large difference of \u22128.2% for female defendants and +2.0% for male defendants.\nTo judge the uncertainty in the values in Table 6 we applied the Bayesian bootstrap of Rubin (1981). That algorithm randomly reweights each data point by a unit mean exponential random variable. There is an asymptotic justification in Newton and Raftery (1994). Figure 5 shows violin plots of 1000 bootstrapped cohort Shapley values. We see that the mean Shapley value of race on the residual is significantly negative for Black defendants and significantly positive for White defendants. These differences persist when disaggregated by gender. The average effect of race is near zero for male defendants and for female defendants due to race effects nearly cancelling.\nBecause the Bayesian bootstrap never fully deletes any of the data we can use it to quantify the statistical uncertainty in the cohort Shapley values for an individual defendant. Figure 6 shows a Bayesian bootstrap violin plot of cohort Shapley values on the residual for the 2999\u2019th defendant. This is the Black defendant whose Shapley value for race on the residual y \u2212 y\u0302 was most negative, in the region overlapping Shapley values of White defendants. As one would expect there is greater uncertainty on the impacts for an individual than for an aggregate. Our analysis has not taken account of the fact that this defendant was selected based on their data."
        },
        {
            "heading": "6.6 FPR and FNR revisited",
            "text": "The risk of being falsely predicted to reoffend involves two factors: having yi = 0 and having y\u0302i = 1 given that yi = 0. FPR is commonly computed only over defendants with yi = 0. Accordingly, in\nthis section we study it by subsetting the defendants to {i | yi = 0} and finding cohort Shapley value of y\u0302i = 1 for the features. That provides a different way to quantify the importance of race on the incidence of false positives.\nFigure 7 shows the cohort Shapley impact of race conditioned for the FPR and FNR after working with subsets of defendants as described above. The distribution of impacts conditioned on race are clearly separated in the figure. This conditional analysis shows a stronger disadvantage for Black defendants than in Figure 3."
        },
        {
            "heading": "7 Discussion",
            "text": "There are many incompatible definitions of importance. Local importance measures address a causes of effects question, which means that we have to carefully consider which counterfactuals to use and how to use them. When attributing the value of f(xi) to features xij , we have avoided using any impossible data by only using actually observed data. We think it is important to have such a choice as otherwise the attribution could be based on extremely unlikely or even impossible combinations. Hooker and Mentch (2019) also point to problems with mixing and matching variable inputs.\nThere could be good uses for possible but hitherto unobserved input combinations. Using only observed values means that we rely on data cleaning to have eliminated any impossible combinations. An alternative that uses input values that pass a \u2018within distribution\u2019 test could be used but in our view it is exceedingly difficult to draw the line between possible and impossible values based on a sample. Kumar et al. (2020) also raise this point.\nThere may be a use for impossible data when explaining whether a predictor affects accuracy. For\ninstance, when judging whether a predictor helps predictions, Breiman (2001) studies what happens when x1j , . . . , xnj are subjected to a random permutation \u03c0(\u00b7) with respect to the other variables, producing hypothetical values x\u0303i = xi,\u2212j :x\u03c0(i),j . He then looks at the accuracy of predicting yi by f(x\u0303i). If accuracy does not change much, then it is reasonable to suppose that xj has only a minimal role in predictions. If accuracy does change a lot however, it could be because the fitted model extrapolates poorly into the low probability or impossible regions of the space. Breiman\u2019s method appears then to be a \u2018one way diagnostic\u2019, persuasive when it indicates low importance but otherwise not.\nDespite these possibilities, we adopted a constraint of not using any unobserved values in order to define a measure of importance. As mentioned in the introduction this counters adversarial approaches that can make changes to predictions at unobserved points in order to change the fairness measures of an algorithm.\nIn defining cohort Shapley we have required a human in the loop to define a similarity measure for the variables. We acknowledge that this is a burden. However, the alternatives we have seen use additional black boxes or use the response itself to define similarity and we think those choices\nmake the final result harder to understand. We think that people would not accept an automatic algorithmic declaration that some black box is or is not fair without having the ability to see how it defines the similar cases that should be treated similarly. Also, the human or humans making and discussing those choices can decide on similarity for one variable at a time. Those choices then define the conditional expectations needed by the algorithm.\nIn addition to transparency and avoiding impossible data, cohort Shapley brings some other advantages: the user can opt to measure importance of variables that were not used in the model, the method does not require access to the black box beyond the predictions it makes, and having defined cohorts we can study means or medians or other quantities of interest. It can also be applied to data on which the algorithm was not trained, as for instance with the COMPAS data. This allows one to quantify external validity properties directly. All conditional expectation Shapley value methods use estimates of conditional means and these become challenging as the conditioning dimension increases and the sample size decreases. For a setting where the black box is used on fresh data that it was not trained on, we have illustrated a convenient bootstrap method to quantify the uncertainty that arises from sampling observations.\nWith the COMPAS data we have illustrated how cohort Shapley values can be used to explore data surrounding a complex fairness issue. There are many more analyses that could be done, and we expect that domain experts would not be unanimous on which is best. We do not have the data necessary to compare fairness of COMPAS to fairness of human judges."
        },
        {
            "heading": "Acknowledgments",
            "text": "This work was supported by Hitachi, Ltd. and by the National Science Foundation grant IIS1837931. We thank Sharad Goel, Jessica Hwang, Alexandra Chouldechova and Alice Xiang for helpful discussions. The opinions in this article are our own and not necessarily shared by those we acknowledge."
        }
    ],
    "title": "Variable importance without impossible data",
    "year": 2023
}