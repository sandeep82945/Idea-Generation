{
    "abstractText": "As the size of the dataset used in deep learning tasks increases, the noisy label problem, which is a task of making deep learning robust to the incorrectly labeled data, has become an important task. In this paper, we propose a method of learning noisy label data using the label noise selection with test-time augmentation (TTA) cross-entropy and classifier learning with the NoiseMix method. In the label noise selection, we propose TTA cross-entropy by measuring the cross-entropy to predict the test-time augmented training data. In the classifier learning, we propose the NoiseMix method based on MixUp and BalancedMix methods by mixing the samples from the noisy and the clean label data. In experiments on the ISIC-18 public skin lesion diagnosis dataset, the proposed TTA cross-entropy outperformed the conventional cross-entropy and the TTA uncertainty in detecting label noise data in the label noise selection process. Moreover, the proposed NoiseMix not only outperformed the state-of-the-art methods in the classification performance but also showed the most robustness to the label noise in the classifier learning.",
    "authors": [
        {
            "affiliations": [],
            "name": "Hansang Lee"
        },
        {
            "affiliations": [],
            "name": "Haeil Lee"
        },
        {
            "affiliations": [],
            "name": "Helen Hong"
        },
        {
            "affiliations": [],
            "name": "Junmo Kim"
        }
    ],
    "id": "SP:18413621dbc74e09f4b446540d815393caa179c4",
    "references": [
        {
            "authors": [
                "N.C.F. Codella",
                "D. Gutman",
                "M.E. Celebi",
                "B. Helba",
                "M.A. Marchetti",
                "S.W. Dusza",
                "A. Kalloo",
                "K. Liopyris",
                "N. Mishra",
                "H. Kittler",
                "A. Halpern"
            ],
            "title": "Skin lesion analysis toward melanoma detection: A challenge at the 2017 international symposium on biomedical imaging (isbi), hosted by the international skin imaging collaboration (isic)",
            "venue": "2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018). pp. 168\u2013172",
            "year": 2018
        },
        {
            "authors": [
                "N.C.F. Codella",
                "D. Gutman",
                "M.E. Celebi",
                "B. Helba",
                "M.A. Marchetti",
                "S.W. Dusza",
                "A. Kalloo",
                "K. Liopyris",
                "N.K. Mishra",
                "H. Kittler",
                "A. Halpern"
            ],
            "title": "Skin lesion analysis toward melanoma detection: A challenge at the 2017 international symposium on biomedical imaging (isbi), hosted by the international skin imaging collaboration (ISIC)",
            "venue": "CoRR abs/1710.05006",
            "year": 2017
        },
        {
            "authors": [
                "N.C.F. Codella",
                "V. Rotemberg",
                "P. Tschandl",
                "M.E. Celebi",
                "S.W. Dusza",
                "D. Gutman",
                "B. Helba",
                "A. Kalloo",
                "K. Liopyris",
                "M.A. Marchetti",
                "H. Kittler",
                "A. Halpern"
            ],
            "title": "Skin lesion analysis toward melanoma detection 2018: A challenge hosted by the international skin imaging collaboration (ISIC)",
            "venue": "CoRR abs/1902.03368",
            "year": 2019
        },
        {
            "authors": [
                "M. Combalia",
                "N.C.F. Codella",
                "V. Rotemberg",
                "B. Helba",
                "V. Vilaplana",
                "O. Reiter",
                "C. Carrera",
                "A. Barreiro",
                "A.C. Halpern",
                "S. Puig",
                "J. Malvehy"
            ],
            "title": "Dermoscopic lesions in the wild",
            "year": 2000
        },
        {
            "authors": [
                "E. Englesson",
                "H. Azizpour"
            ],
            "title": "Consistency regularization can improve robustness to label noise",
            "venue": "CoRR abs/2110.01242",
            "year": 2021
        },
        {
            "authors": [
                "A. Galdran",
                "G. Carneiro",
                "M.A. Gonz\u00e1lez Ballester"
            ],
            "title": "Balanced-mixup for highly imbalanced medical image classification",
            "venue": "de Bruijne, M., Cattin, P.C., Cotin, S., Padoy, N., Speidel, S., Zheng, Y., Essert, C. (eds.) Medical Image Computing and Computer Assisted Intervention \u2013 MICCAI 2021. pp. 323\u2013333. Springer International Publishing, Cham",
            "year": 2021
        },
        {
            "authors": [
                "B. Han",
                "Q. Yao",
                "X. Yu",
                "G. Niu",
                "M. Xu",
                "W. Hu",
                "I. Tsang",
                "M. Sugiyama"
            ],
            "title": "Coteaching: Robust training of deep neural networks with extremely noisy labels",
            "venue": "Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., Garnett, R. (eds.) Advances in Neural Information Processing Systems. vol. 31. Curran Associates, Inc.",
            "year": 2018
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2016
        },
        {
            "authors": [
                "D. Hendrycks",
                "N. Mu",
                "E.D. Cubuk",
                "B. Zoph",
                "J. Gilmer",
                "B. Lakshminarayanan"
            ],
            "title": "AugMix: A simple data processing method to improve robustness and uncertainty",
            "venue": "Proceedings of the International Conference on Learning Representations (ICLR)",
            "year": 2020
        },
        {
            "authors": [
                "L. Ju",
                "X. Wang",
                "L. Wang",
                "D. Mahapatra",
                "X. Zhao",
                "Q. Zhou",
                "T. Liu",
                "Z. Ge"
            ],
            "title": "Improving medical images classification with label noise using dual-uncertainty estimation",
            "venue": "IEEE Transactions on Medical Imaging 41(6), 1533\u20131546",
            "year": 2022
        },
        {
            "authors": [
                "D. Karimi",
                "H. Dou",
                "S.K. Warfield",
                "A. Gholipour"
            ],
            "title": "Deep learning with noisy labels: Exploring techniques and remedies in medical image analysis",
            "venue": "Medical Image Analysis 65, 101759",
            "year": 2020
        },
        {
            "authors": [
                "J. Li",
                "R. Socher",
                "S.C. Hoi"
            ],
            "title": "Dividemix: Learning with noisy labels as semisupervised learning",
            "venue": "International Conference on Learning Representations",
            "year": 2020
        },
        {
            "authors": [
                "C. Szegedy",
                "S. Ioffe",
                "V. Vanhoucke",
                "A.A. Alemi"
            ],
            "title": "Inception-v4, inception-resnet and the impact of residual connections on learning",
            "venue": "Proceedings of the ThirtyFirst AAAI Conference on Artificial Intelligence. p. 4278\u20134284",
            "year": 2017
        },
        {
            "authors": [
                "P. Tschandl",
                "C. Rosendahl",
                "H. Kittler"
            ],
            "title": "The ham10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions",
            "venue": "Scientific Data 5(1), 180161",
            "year": 2018
        },
        {
            "authors": [
                "Y. Xue",
                "K. Whitecross",
                "B. Mirzasoleiman"
            ],
            "title": "Investigating why contrastive learning benefits robustness against label noise",
            "venue": "CoRR abs/2201.12498",
            "year": 2022
        },
        {
            "authors": [
                "H. Zhang",
                "M. Cisse",
                "Y.N. Dauphin",
                "D. Lopez-Paz"
            ],
            "title": "mixup: Beyond empirical risk minimization",
            "venue": "International Conference on Learning Representations",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "Keywords: Classification \u00b7 Label noise \u00b7 Skin lesion diagnosis \u00b7 Test time augmentation \u00b7 Mixup"
        },
        {
            "heading": "1 Introduction",
            "text": "The noisy label problem represents the task of training a machine learner, mainly a deep neural network, on the training data, which consists of incorrectly labeled data [11]. As the size of the dataset used in deep learning tasks increases, the risk of mislabeled label noise data being included in the dataset increases. In addition, when labels are automatically generated from radiological reports to compose large-scale medical image datasets, there is a potential risk for label noise in medical image datasets [10]. Since the machine learner generally aims to make accurate predictions on all training data, the training data corrupted by ? Corresponding author\nar X\niv :2\n21 2.\n00 47\n9v 1\n[ cs\n.C V\n] 1\nthe label noise leads to deterioration of the performance of the machine learner. Due to the excellent memorization characteristics of the deep neural networks, the noisy label learning of deep neural networks can be especially challenging.\nSeveral works have been proposed to improve the learning efficiency for noisy label data. Most of them can be divided into two categories, sample-based and model-based methods. The sample-based method consists of the label noise selection that finds the incorrect label data from the training data, and the classifier learning that trains the classifier on the clean label data. Han et al. proposed the Co-Teaching method, which selects the label noise data by ranking the crossentropy loss and trains two networks by teaching each other to improve the robustness [7]. Ju et al. suggested the label noise selection with uncertainty estimation and the classifier learning with curriculum learning [10]. The model-based method aims to improve the robustness of the network to the noisy labels without the selection of label noise data. Englesson et al. proposed that the consistency regularization, e.g., MixUp [16] and AugMix [9] can enhance the robustness of the network to the label noise [5]. Xue et al. suggested that self-supervised contrastive learning also can improve the network robustness to the noisy label problems [15].\nIn this paper, we propose a sample-based method of noisy label learning in medical images to improve both the accuracy of the label noise selection and the robustness of the classifier learning. To achieve this, we propose a test-time augmentation (TTA) cross-entropy for the label noise selection and a NoiseMix method for classifier learning. In the label noise selection, we propose TTA crossentropy by measuring the cross-entropy to predict the test-time augmented training data. The proposed TTA cross-entropy can avoid the memorization problem of the conventional cross-entropy while improving the label noise detection performance of TTA uncertainty. In the classifier learning, we propose the NoiseMix technique based on MixUp [16] and BalancedMix [6] methods by mixing the samples from the noisy and the clean label data. By modifying the mixing rate of the label noise data in MixUp training, we can further improve the robustness of the classifier to the label noise. We validate the effectiveness of the proposed method on the ISIC-18 public skin lesion diagnosis dataset [1]. In experiments, the proposed TTA cross-entropy outperformed the conventional cross-entropy and the TTA uncertainty in detecting label noise data in the label noise selection process. Moreover, the proposed NoiseMix not only outperformed the state-of-the-art methods in the classification performance but also showed the most robustness to the label noise."
        },
        {
            "heading": "2 Methods",
            "text": "As shown in Fig. 1, our method consists of two main steps. First, we perform label noise selection to separate the clean and the noise label data from the noisy label data using TTA cross-entropy. Second, we perform classifier learning on the noisy training data and the clean label data using the NoiseMix method."
        },
        {
            "heading": "2.1 Label Noise Selection with Test-Time Augmentation Cross-Entropy",
            "text": "In label noise selection, our aim is to separate the incorrectly-labeled label noise data SN and the correctly-labeled clean label data SC from the entire noisy label data S = {SN , SC}. As shown in Fig. 2, our label noise selection method consists of three steps, including (1) warm-up for weak classifier learning, (2) testtime augmentation and weak classifier prediction, and (3) TTA cross-entropy computation. Warm-Up for Weak Classifier Training. First, we perform warm-up that trains the weak classifier which will provide the prediction scores of the training data for the label noise selection process. The weak classifier fw() is trained on the entire training data S for a few epochs to prevent the overfitting of the classifier to the incorrect label noise. In experiments, we perform a warm-up on the weak classifier for two epochs.\nTTA and Weak Classifier Prediction. Using the trained weak classifier, we compute the prediction scores of the training data to separate the label noise and the clean label data. In the warm-up process, the weak classifier is trained not to prevent the overfitting of the incorrect label noise, but the weak classifier still has a risk of memorizing the label noise data. To avoid the memorization problem of a weak classifier, we perform weak classifier prediction on the augmented training data through affine transformation-based TTA instead of the training data itself.\nFor a training image-label pair (x, y) \u2208 S, we form a set of augmented data X = {x1, x2, ..., xN} with affine transformation T ():\nxn = T (x, \u03b8n) (1)\nwhere xn is the n-th augmented data and \u03b8n is the n-th parameter setting for affine transformation. We then perform weak classifier prediction on these augmented data to have a set of predicted labels Y = {y1, y2, ..., yN} as follows:\nyn = fw(xn) (2)\nwhere yn is the n-th predicted label of the augmented training data. TTA Cross-Entropy Computation. We compute the prediction score using the weak classifier prediction of the augmented training data and select the incorrect label noise data from the training data according to the prediction score. As an efficient prediction score to distinguish the label noise data from the clean label data, we propose a TTA cross-entropy.\nFor the set of predicted labels of the augmented training data Y we can form a probability distribution pY for unique labels m = 1, 2, ...,M where pY(m) is the ratio of the number of yn with label m among the N predicted labels. For the probability distribution pY , the conventional TTA uncertainty is computed as the entropy of the distribution:\nH(Y) = \u2212 M\u2211\nm=1\npY(m)ln (pY(m)). (3)\nThe TTA uncertainty is relatively robust to the memorization problem of the weak classifier compared to the conventional cross-entropy of the training data prediction [10]. However, the TTA uncertainty only evaluates the instability of the label prediction of the augmented data, not whether the training label is correct or incorrect. Thus, it has limitations in missing the cases with incorrect labels but relatively low label uncertainty. To overcome the limitation, we propose a TTA cross-entropy to reflect the correctness of training labels to the TTA uncertainty as follows:\nCE(Y, y) = \u2212 M\u2211\nm=1\npy(m)ln (pY(m)), (4)\nwhere py is the probability distribution of unique labels for the training label y, where py(m) = 1 if the training label y = m and py(m) = 0 if y 6= m.\nThe proposed TTA cross-entropy considers both the label instability of the weak classifier prediction of the augmented training data and the correctness of the training data labels. Thus, the TTA cross-entropy can improve the efficiency of the label noise selection compared to the conventional training data crossentropy and TTA uncertainty."
        },
        {
            "heading": "2.2 Classifier Training with NoiseMix",
            "text": "We re-train the classifier with the noisy label data and the clean label data obtained from the label noise selection process. We aim to improve the learning efficiency of the clean label data learner with the label noise data while preventing overfitting of the label noise. Inspired by the BalancedMix [6] method for class imbalance learning, we propose the NoiseMix method for noisy label learning by combining two data sampling strategies by means of MixUp [16].\nIn NoiseMix, we form a mixed training data (x\u0302, y\u0302) by mixing the clean label data (xC , yC) \u2208 SC with the original training data (x, y) \u2208 S as follows:\nx\u0302 = \u03bbx+ (1\u2212 \u03bb)xC , y\u0302 = \u03bby + (1\u2212 \u03bb)yC , (5)\nwhere \u03bb is the MixUp coefficient determined as \u03bb \u223c Beta(\u03b1, 1). The NoiseMix training with (x\u0302, y\u0302) in 5 not only enables the regularized learning of the clean label data through MixUp but also reflects the effect of re-weighting of the label noise data by mixing them only with the clean label data."
        },
        {
            "heading": "3 Experiments and Results",
            "text": ""
        },
        {
            "heading": "3.1 Datasets and Experimental Details",
            "text": "The proposed method was validated on the public skin lesion diagnosis dataset of ISIC-18 [14,4,2,1,3] 3. ISIC-18 dataset consists of 10,208 dermoscopic skin lesion images (10,015 for training and 193 for validation) for seven skin disease classes. In this paper, we formulate a binary classification task to classify images of seven skin lesions into benign (8388 for training and 157 for validation) and malignant (1627 for training and 36 for validation,) The benign class includes five skin diseases of AKIEC, BKL, DF, NV, and VASC, whereas the malignant class includes two skin diseases of BCC and MEL.\nTo construct the noisy label dataset from the ISIC-18 dataset, we generated the instance-dependent label noise data [10] instead of random label noise. First, we trained a ResNet-50 on the training data for two epochs with a mini-batch size of 16. Second, using this weak classifier, we measured the cross-entropy loss of the training data. Third, the labels of the r% of training data with the highest losses were replaced with the labels predicted by the weak classifier, where r is the ratio of the label noise data. This instance-dependent label noise enables evaluation in a setting similar to a real noisy label environment compared to the 3 https://challenge2018.isic-archive.com/\nrandom label noise. Here, the ratios of the label noise data to the training data were set to r = {10%, 30%, 50%}.\nIn label noise selection of the proposed method, we trained a ResNet-50 for two epochs with a mini-batch size of 16 for weak classifier training. In TTA, a random horizontal flip, a random vertical flip, a random rotation with a degree of \u221245\u25e6 \u2264 \u03b8 \u2264 +45\u25e6, a random translation with a shift rate of (0.1,0.1), and a random scaling with a factor of 1 \u2264 \u03c3 \u2264 1.2 were applied. In NoiseMix, the mixing hyper-parameter \u03b1 determining the MixUp weights \u03bb \u223c Beta(\u03b1, 1) was set as 0.2.\nIn experiments, we evaluated (1) the effect of the proposed TTA cross-entropy for the label noise selection and (2) the effect of the proposed NoiseMix for classifier learning. In label noise selection, we compared the proposed TTA crossentropy with the conventional cross-entropy [5] and the TTA uncertainty [10] by observing the ROC curve and the AUC for detecting the label noise data in the noisy training data. In classifier learning, we compared the accuracy of the proposed NoiseMix with the results of (1) the baseline ResNet-50 [8] trained on the noisy label data S, (2) the DivideMix, a state-of-the-art method for noisy label learning, trained on S, (3) the baseline ResNet-50 trained on the clean label data SC selected by the proposed TTA cross-entropy, and (4) the baseline MixUp [16] trained on the SC . Both baseline ResNet-50 trained on S and SC were trained for 100 epochs with a mini-batch size of 8. In DivideMix, the InceptionResNet-v2 [13] was trained for 112 epochs with a mini-batch size of 8. In MixUp, the ResNet-18 was trained for 100 epochs with a mini-batch size of 8 and the mixup weight parameter \u03b1 of 0.2."
        },
        {
            "heading": "3.2 Results",
            "text": "Fig. 3 shows the histograms of cross-entropy, TTA uncertainty, and the TTA cross-entropy for the label noise and the clean label data. The prediction score can be considered more valuable as (1) the in-set distribution is concentrated in a limited period and (2) the distributions between two sets are far from each other, so they can be easily separated. In Fig. 3 (a), The cross-entropy of the\nclean label data is concentrated at the low values, but the cross-entropy of the label noise data is also distributed in the low values, making separation difficult. In Fig. 3 (b), The TTA uncertainty of the label noise data is concentrated at the high values, but the uncertainty of the clean label data is distributed over a wide range of periods, making incorrect detection in label noise selection. In Fig. 3 (c), the distributions of TTA cross-entropy seem to be a mixture of the crossentropy of clean label data and the TTA uncertainty of the label noise data. Each distribution is distributed in a narrower period, and the overlap between the distributions is smaller than the cross-entropy and the TTA uncertainty. Due to these histogram characteristics, it can be seen that the proposed TTA cross-entropy can be considered a better prediction score for distinguishing the label noise data from the clean label data compared to the cross-entropy and the TTA uncertainty.\nFig. 4 shows the ROC curves and AUCs of proposed and comparative label noise selection methods for detecting the label noise data. It can be observed that the label noise selection with cross-entropy shows low detection performance in separating label noise and clean label data. This is because the network can be easily overfitted on the medical images due to similar appearance, and the cross-entropy is vulnerable to the memorization problem of the overfitted weak classifier. The memorization problem seems mostly avoided by applying TTA\nuncertainty, and the proposed TTA cross-entropy further improves the detection performance of the label noise data regardless of label noise ratios r.\nTable 1 shows the accuracies of the proposed and the comparative methods in classifier learning for different label noise ratios r. The performance of noisy label learning can be evaluated from two perspectives: (1) For each label noise ratio, the method with the highest accuracy can be considered as the best noisy label learner. (2) For the increase in the label noise ratio, the method with the smallest decrease in performance can be considered as themost robust noisy label learner. From the highest accuracy perspective, it can be observed that the proposed method with NoiseMix outperformed not only the baseline ResNet-50 and MixUp but also the state-of-the-art DivideMix regardless of label noise ratios. In the noise robustness perspective, when the label noise ratio r increases from 10% to 50%, the MixUp and DivideMix decrease by 10.3%p and 7.8%p, respectively, while the performance of the proposed method decreases only 1.2%p. It can be confirmed that the proposed NoiseMix enables not only the best performance but also the most robust noisy label learning."
        },
        {
            "heading": "4 Conclusions",
            "text": "In this paper, we proposed a method of learning noisy label data using the label noise selection with TTA cross-entropy and the classifier learning with the NoiseMix method. In the label noise selection, the proposed TTA crossentropy improved the accuracy of selecting label noise data by preventing the memorization problem of the conventional cross-entropy and reflecting the label correctness to the TTA uncertainty. In the classifier learning, the proposed NoiseMix enhanced the robustness by reflecting the effect of re-weighting the label noise data to the conventional MixUp. As a result, our TTA cross-entropy outperformed the conventional cross-entropy and TTA uncertainty in noisy label noise selection. Furthermore, our NoiseMix outperformed the existing training techniques without designing loss functions or weighting schemes.\nAcknowledgments This work was supported by the National Research Foundation of Korea(NRF) grant funded by the Korea government(MSIT) (No. 2020 R1A2C1102140), and the Korea Medical Device Development Fund grant funded by the Korea government (the Ministry of Science and ICT, the Ministry of Trade, Industry and Energy, the Ministry of Health & Welfare, the Ministry of Food and Drug Safety) (Project Number: 9991007550, KMDF_PR_20200901_0269)"
        }
    ],
    "title": "Noisy Label Classification using Label Noise Selection with Test-Time Augmentation Cross-Entropy and NoiseMix Learning",
    "year": 2022
}