{
    "abstractText": "Speaker adaptation is important to build robust automatic speech recognition (ASR) systems. In this work, we investigate various methods for speaker adaptive training (SAT) based on feature-space approaches for a conformer-based acoustic model (AM) on the Switchboard 300h dataset. We propose a method, called Weighted-Simple-Add, which adds weighted speaker information vectors to the input of the multi-head self-attention module of the conformer AM. Using this method for SAT, we achieve 3.5% and 4.5% relative improvement in terms of WER on the CallHome part of Hub5\u201900 and Hub5\u201901 respectively. Moreover, we build on top of our previous work where we proposed a novel and competitive training recipe for a conformerbased hybrid AM. We extend and improve this recipe where we achieve 11% relative improvement in terms of word-error-rate (WER) on Switchboard 300h Hub5\u201900 dataset. We also make this recipe efficient by reducing the total number of parameters by 34% relative.",
    "authors": [
        {
            "affiliations": [],
            "name": "Mohammad Zeineldeen"
        },
        {
            "affiliations": [],
            "name": "Jingjing Xu"
        },
        {
            "affiliations": [],
            "name": "Christoph L\u00fcscher"
        },
        {
            "affiliations": [],
            "name": "Ralf Schl\u00fcter"
        },
        {
            "affiliations": [],
            "name": "Hermann Ney"
        }
    ],
    "id": "SP:c606ded9aaf066d05e123e944ff6cffd9171ed31",
    "references": [
        {
            "authors": [
                "M. Zeineldeen",
                "J. Xu",
                "C. L\u00fcscher",
                "W. Michel",
                "A. Gerstenberger",
                "R. Schl\u00fcter",
                "H. Ney"
            ],
            "title": "Conformer-based Hybrid ASR System for Switchboard Dataset",
            "venue": "ICASSP, Singapore, May 2022, pp. 7437\u20137441.",
            "year": 2022
        },
        {
            "authors": [
                "W. Zhou",
                "W. Michel",
                "K. Irie",
                "M. Kitza",
                "R. Schl\u00fcter",
                "H. Ney"
            ],
            "title": "The RWTH ASR System for TED-LIUM Release 2: Improving Hybrid HMM with SpecAugment",
            "venue": "ICASSP, Barcelona, Spain, May 2020, pp. 7839\u20137843.",
            "year": 2020
        },
        {
            "authors": [
                "C. L\u00fcscher",
                "E. Beck",
                "K. Irie",
                "M. Kitza",
                "W. Michel",
                "A. Zeyer",
                "R. Schl\u00fcter",
                "H. Ney"
            ],
            "title": "RWTH ASR Systems for LibriSpeech: Hybrid vs Attention",
            "venue": "INTERSPEECH, Graz, Austria, Sep. 2019, pp. 231\u2013235.",
            "year": 2019
        },
        {
            "authors": [
                "S. Hochreiter",
                "J. Schmidhuber"
            ],
            "title": "Long Short-term Memory",
            "venue": "Neural Computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.",
            "year": 1997
        },
        {
            "authors": [
                "O. Abdel-Hamid",
                "A. Mohamed",
                "H. Jiang",
                "L. Deng",
                "G. Penn",
                "D. Yu"
            ],
            "title": "Convolutional Neural Networks for Speech Recognition",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 22, no. 10, pp. 1533\u20131545, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "V. Peddinti",
                "D. Povey",
                "S. Khudanpur"
            ],
            "title": "A Time Delay Neural Network Architecture for Efficient Modeling of Long Temporal Contexts",
            "venue": "INTERSPEECH, Dresden, Germany, Sep. 2015, pp. 3214\u20133218.",
            "year": 2015
        },
        {
            "authors": [
                "M. Sperber",
                "J. Niehues",
                "G. Neubig",
                "S. St\u00fcker",
                "A. Waibel"
            ],
            "title": "Self-Attentional Acoustic Models",
            "venue": "INTERSPEECH, Hyderabad, India, Sep. 2018, pp. 3723\u20133727.",
            "year": 2018
        },
        {
            "authors": [
                "A. Gulati",
                "J. Qin",
                "C. Chiu",
                "N. Parmar",
                "Y. Zhang",
                "J. Yu",
                "W. Han",
                "S. Wang",
                "Z. Zhang",
                "Y. Wu",
                "R. Pang"
            ],
            "title": "Conformer: Convolution-augmented Transformer for Speech Recognition",
            "venue": "INTERSPEECH, Shanghai, China, Oct. 2020, pp. 5036\u20135040.",
            "year": 2020
        },
        {
            "authors": [
                "Z. T\u00fcske",
                "G. Saon",
                "B. Kingsbury"
            ],
            "title": "On the Limit of English Conversational Speech Recognition",
            "venue": "INTERSPEECH, Brno, Czechia, Sep. 2021, pp. 2062\u20132066.",
            "year": 2021
        },
        {
            "authors": [
                "G. Saon",
                "H. Soltau",
                "D. Nahamoo",
                "M. Picheny"
            ],
            "title": "Speaker Adaptation of Neural Network Acoustic Models Using I-Vectors",
            "venue": "ASRU, Olomouc, Czech Republic, Dec. 2013, pp. 55\u201359.",
            "year": 2013
        },
        {
            "authors": [
                "C. Leggetter",
                "P. Woodland"
            ],
            "title": "Maximum Likelihood Linear Regression for Speaker Adaptation of Continuous Density Hidden Markov Models",
            "venue": "Computer Speech & Language, vol. 9, no. 2, pp. 171\u2013185, 1995. [Online]. Available: https://www. sciencedirect.com/science/article/pii/S0885230885700101",
            "year": 1995
        },
        {
            "authors": [
                "N. Dehak",
                "P.J. Kenny",
                "R. Dehak",
                "P. Dumouchel",
                "P. Ouellet"
            ],
            "title": "Front-End Factor Analysis for Speaker Verification",
            "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 19, no. 4, pp. 788\u2013798, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "D. Snyder",
                "D. Garcia-Romero",
                "G. Sell",
                "D. Povey",
                "S. Khudanpur"
            ],
            "title": "X-Vectors: Robust DNN Embeddings for Speaker Recognition",
            "venue": "ICASSP, Calgary, Alberta, Canada, 2018, pp. 5329\u20135333.",
            "year": 2018
        },
        {
            "authors": [
                "F. Seide",
                "G. Li",
                "X. Chen",
                "D. Yu"
            ],
            "title": "Feature Engineering in Context-dependent Deep Neural Networks for Conversational Speech Transcription",
            "venue": "ASRU, Waikoloa, HI, USA, 2011, pp. 24\u201329.",
            "year": 2011
        },
        {
            "authors": [
                "R. Gemello",
                "F. Mana",
                "S. Scanzio",
                "P. Laface",
                "R. De Mori"
            ],
            "title": "Adaptation of Hybrid ANN/HMM Models using Linear Hidden Transformations and Conservative Training",
            "venue": "ICASSP, vol. 1. Toubouse, France: IEEE, May 2006, pp. I\u2013I.",
            "year": 2006
        },
        {
            "authors": [
                "B. Li",
                "K.C. Sim"
            ],
            "title": "Comparison of Discriminative Input and Output Transformations for Speaker Adaptation in the Hybrid NN/HMM Systems",
            "venue": "Eleventh Annual Conference of the International Speech Communication Association, Makuhari, Chiba, Japan, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "H. Zhu",
                "L. Wang",
                "P. Zhang",
                "Y. Yan"
            ],
            "title": "Multi-accent Adaptation Based on Gate Mechanism",
            "venue": "INTERSPEECH, Graz, Austria, Sep. 2019, pp. 744\u2013748.",
            "year": 2019
        },
        {
            "authors": [
                "X. Gong",
                "Y. Lu",
                "Z. Zhou",
                "Y. Qian"
            ],
            "title": "Layer-wise Fast Adaptation for End-to-End Multi-accent Speech Recognition",
            "venue": "IN- TERSPEECH, Brno, Czechia, 2021, pp. 1274\u20131278.",
            "year": 2021
        },
        {
            "authors": [
                "K. Deng",
                "S. Cao",
                "L. Ma"
            ],
            "title": "Improving Accent Identification and Accented Speech Recognition under a Framework of Selfsupervised Learning",
            "venue": "INTERSPEECH, Brno, Czechia, Sep. 2021, pp. 1504\u20131508.",
            "year": 2021
        },
        {
            "authors": [
                "M.A.T. Turan",
                "E. Vincent",
                "D. Jouvet"
            ],
            "title": "Achieving Multiaccent ASR via Unsupervised Acoustic Model Adaptation",
            "venue": "INTERSPEECH, Shanghai, China, Oct. 2020, pp. 1286\u20131290.",
            "year": 2020
        },
        {
            "authors": [
                "S. Ioffe",
                "C. Szegedy"
            ],
            "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
            "venue": "ICML, vol. 37, Lille, France, 2015, pp. 448\u2013456.",
            "year": 2015
        },
        {
            "authors": [
                "L.J. Ba",
                "J.R. Kiros",
                "G.E. Hinton"
            ],
            "title": "Layer Normalization",
            "venue": "CoRR, vol. abs/1607.06450, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "V. Peddinti",
                "G. Chen",
                "V. Manohar",
                "T. Ko",
                "D. Povey",
                "S. Khudanpur"
            ],
            "title": "JHU ASpIRE System: Robust LVCSR with TDNNS, IVector Adaptation and RNN-LMS",
            "venue": "ASRU. Scottsdale, AZ, USA: IEEE, Dec. 2015, pp. 539\u2013546.",
            "year": 2015
        },
        {
            "authors": [
                "M. Kitza",
                "P. Golik",
                "R. Schl\u00fcter",
                "H. Ney"
            ],
            "title": "Cumulative Adaptation for BLSTM Acoustic Models",
            "venue": "INTERSPEECH, Graz, Austria, Sep. 2019, pp. 754\u2013758.",
            "year": 2019
        },
        {
            "authors": [
                "S. Yoo",
                "I. Song",
                "Y. Bengio"
            ],
            "title": "A Highly Adaptive Acoustic Model for Accurate Multi-dialect Speech Recognition",
            "venue": "ICASSP, Brighton, UK, May 2019, pp. 5716\u20135720.",
            "year": 2019
        },
        {
            "authors": [
                "J. Rownicka",
                "P. Bell",
                "S. Renals"
            ],
            "title": "Embeddings for DNN Speaker Adaptive Training",
            "venue": "ASRU. Singapore: IEEE, Dec. 2019, pp. 479\u2013486.",
            "year": 2019
        },
        {
            "authors": [
                "J. Godfrey",
                "E. Holliman",
                "J. McDaniel"
            ],
            "title": "Switchboard: Telephone Speech Corpus for Research and Development",
            "venue": "ICASSP, vol. 1, San Francisco,USA, Mar. 1992, pp. 517\u2013520.",
            "year": 1992
        },
        {
            "authors": [
                "A. Zeyer",
                "T. Alkhouli",
                "H. Ney"
            ],
            "title": "RETURNN as a Generic Flexible Neural Toolkit with Application to Translation and Speech Recognition",
            "venue": "Annual Meeting of the Assoc. for Computational Linguistics, Melbourne, Australia, Jul. 2018.",
            "year": 2018
        },
        {
            "authors": [
                "S. Wiesler",
                "A. Richard",
                "P. Golik",
                "R. Schl\u00fcter",
                "H. Ney"
            ],
            "title": "RASR/NN: The RWTH Neural Network Toolkit for Speech Recognition",
            "venue": "ICASSP, Florence, Italy, May 2014, pp. 3313\u2013 3317.",
            "year": 2014
        },
        {
            "authors": [
                "J. Long",
                "E. Shelhamer",
                "T. Darrell"
            ],
            "title": "Fully Convolutional Networks for Semantic Segmentation",
            "venue": "CVPR, Boston, USA, Jun. 2015, pp. 3431\u20133440.",
            "year": 2015
        },
        {
            "authors": [
                "R. Schluter",
                "I. Bezrukov",
                "H. Wagner",
                "H. Ney"
            ],
            "title": "Gammatone Features and Feature Combination for Large Vocabulary Speech Recognition",
            "venue": "ICASSP, Honolulu, USA, Apr. 2007, pp. 649\u2013 652.",
            "year": 2007
        },
        {
            "authors": [
                "A. Rouhe",
                "T. Kaseva",
                "M. Kurimo"
            ],
            "title": "Speaker-aware Training of Attention-based End-to-end Speech Recognition Using Neural Speaker Embeddings",
            "venue": "ICASSP, Barcelona, Spain, May 2020, pp. 7064\u20137068.",
            "year": 2020
        },
        {
            "authors": [
                "M. Gibson",
                "T. Hain"
            ],
            "title": "Hypothesis Spaces for Minimum Bayes Risk Training in Large Vocabulary Speech Recognition",
            "venue": "IN- TERSPEECH, Pittsburgh, USA, Sep. 2006.",
            "year": 2006
        },
        {
            "authors": [
                "E. Beck",
                "W. Zhou",
                "R. Schl\u00fcter",
                "H. Ney"
            ],
            "title": "LSTM Language Models for LVCSR in First-pass Decoding and Lattice-Rescoring",
            "venue": "CoRR, vol. abs/1907.01030, 2019. [Online]. Available: http://arxiv.org/abs/1907.01030",
            "year": 1907
        },
        {
            "authors": [
                "K. Okabe",
                "T. Koshinaka",
                "K. Shinoda"
            ],
            "title": "Attentive Statistics Pooling for Deep Speaker Embedding",
            "venue": "INTERSPEECH, Hyderabad, India, Sep. 2018, pp. 2252\u20132256.",
            "year": 2018
        },
        {
            "authors": [
                "Z. T\u00fcske",
                "G. Saon",
                "K. Audhkhasi",
                "B. Kingsbury"
            ],
            "title": "Single Headed Attention Based Sequence-to-sequence Model for Stateof-the-Art Results on Switchboard",
            "venue": "INTERSPEECH, Shanghai, China, Sep. 2020, pp. 551\u2013555.",
            "year": 2020
        },
        {
            "authors": [
                "S. Hu",
                "X. Xie",
                "S. Liu",
                "J. Yu",
                "Z. Ye",
                "M. Geng",
                "X. Liu",
                "H. Meng"
            ],
            "title": "Bayesian Learning of LF-MMI Trained Time Delay Neural Networks for Speech Recognition",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 1514\u20131529, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "G. Saon",
                "Z. T\u00fcske",
                "D. Bola\u00f1os",
                "B. Kingsbury"
            ],
            "title": "Advancing RNN Transducer Technology for Speech Recognition",
            "venue": "ICASSP. Toronto, Canada: IEEE, 2021, pp. 5654\u20135658.",
            "year": 2021
        },
        {
            "authors": [
                "S. Sigtia",
                "E. Marchi",
                "S. Kajarekar",
                "D. Naik",
                "J. Bridle"
            ],
            "title": "Multi- Task Learning for Speaker Verification and Voice Trigger Detection",
            "venue": "ICASSP. Barcelona, Spain: IEEE, May 2020, pp. 6844\u2013 6848.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Yang",
                "P. Wang",
                "D. Wang"
            ],
            "title": "A Conformer Based Acoustic Model for Robust Automatic Speech Recognition",
            "venue": "CoRR, vol. abs/2203.00725, 2022. [Online]. Available: https: //doi.org/10.48550/arXiv.2203.00725",
            "year": 2022
        },
        {
            "authors": [
                "M. Kitza",
                "R. Schl\u00fcter",
                "H. Ney"
            ],
            "title": "Comparison of BLSTM-layerspecific Affine Transformations for Speaker Adaptation",
            "venue": "IN- TERSPEECH, Hyderabad, India, Sep. 2018, pp. 877\u2013881.",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction & Related Work",
            "text": "Hybrid neural network (NN)-Hidden Markov model (HMM) has been widely used to build competitive automatic speech recognition (ASR) systems on different datasets [1, 2, 3]. The system consists of an acoustic model (AM) and a language model (LM). Due to the tremendous success of deep learning, NN architectures are used for acoustic modeling. Such NN architectures include Bidirectional long short-term long memory (BLSTM) [4], convolution neural networks (CNN) [5], and time-delay neural networks (TDNN) [6]. The NN acoustic models are often trained with cross-entropy using a frame-wise alignment generated by a Gaussian mixture model (GMM)HMM system.\nRecently, self-attention models [7], specifically, conformerbased models [8], have achieved state-of-the-art performance on different datasets [8, 9]. In [1], we proposed a novel and competitive training recipe for building conformer-based hybridNN-HMM systems. We studied different training methods to improve performance as well as to speed up training time. In this work, we extend and improve our training recipe by making it more efficient in terms of training speed and memory consumption as well as having a significant improvement in terms of word-error-rate (WER).\nMoreover, speaker adaptation methods have been used to build robust ASR systems [10]. These methods can be classified as feature-space approaches and model-space approaches. Feature-space approaches include using speaker adaptive features [11] or augmenting speaker information such as i-vectors\n\u2217Equal contribution\n[12] or x-vectors [13] into the network. On the other hand, the model-space approaches modify the acoustic model to match the testing conditions by learning a speaker or environment dependent linear transformation to transform the input, output, or hidden representations of the network [14, 15, 16]. There have been some attempts to apply feature-space approaches to state-of-the-art conformer-based models [9]. In [9], they concatenate i-vectors to the input of each feed-forward layer in the conformer block, however, this gives only minor improvements and adds many additional parameters. In addition, there has been other work related to multi-accent adaptation which studied also different ways to integrate accent information into the network [17, 18, 19, 20]. However, in all these approaches, the conformer architecture was not used. Therefore, in this work, we focus on feature-space approaches and we investigate better ways to integrate speaker information into the conformer AM.\nThe main contributions of this paper are: (1) We improve our training recipe for conformer-based hybrid-HMM-NN models [1] which lead to 11% relative improvement in terms of WER on Switchboard 300h Hub5\u201900 dataset, (2) We build an efficient conformer AM with 34% relative reduction of the total number of parameters compared to our previous baseline [1], (3) We propose a better method to integrate i-vectors into the conformer AM and achieved 3.5% relative improvement in terms of WER on CallHome part of Switchboard 300h. We call this method Weighted-Simple-Add and it adds weighted speaker information vectors to the input of the multi-head selfattention module of the conformer AM."
        },
        {
            "heading": "2. Model Architecture",
            "text": "Our AM is a variant of the conformer architecture [8]. The training recipe is built on top of our previous work [1]. The main differences are: (1) we replace all batch normalization (BN) [21] layers with layer normalization (LN) [22] layers, (2) we add another convolution module following Macaron style [8]. Each conformer block consists of one or more of these modules: feed-forward (FFN) module, multi-head self-attention (MHSA) module, and convolution (Conv) module. The proposed conformer block is illustrated in Figure 1. More details and experimental results are discussed in Section 5."
        },
        {
            "heading": "3. Speaker Adaptation",
            "text": "The main goal of speaker adaptation is to reduce the mismatch between train and test data by adapting the model to the unseen speakers of the test data. In this way, these methods adapt the speaker-independent model to be more robust to different\nar X\niv :2\n20 6.\n12 95\n5v 1\n[ cs\n.C L\n] 2\n6 Ju\nn 20\n22\nspeakers observed in the test data. In this work, we focus on feature-space adaptation methods.\nFeature-space methods adapt the features of the network to be speaker-dependent. I-vectors [12] have been successfully applied to NN-based ASR systems [10, 23]. However, the integration of such vectors depends on the model architecture itself and it is not always clear what is the best way to do such an integration. For the BLSTM model, it was observed that concatenating the i-vectors to the network features gives significant improvement [24]. But this did not work well for the conformer model as observed in [9]. In this work, we investigate and propose better ways to integrate the i-vectors into the conformer AM. We develop a method to combine i-vectors into the MHSA module of the conformer AM which gives the best improvement.\nLet zT1 be the input to a module in some conformer block (more details can be found in Section 5.2), and let v represents a speaker representation vector such as i-vector or x-vector for the input utterance. The methods to integrate v into the network can be defined as:\n\u2022 Concat: concatenate the hidden representation zt with v\nz\u0303t = [zt; v] \u2200t \u2208 [1, T ] (1)\n\u2022 Simple-Add: U and b are trainable parameters.\nz\u0303t = zt + Uv + b \u2200t \u2208 [1, T ] (2)\n\u2022 Complex-Add: W , U , and b are trainable parameters.\nz\u0303t =Wzt + Uv + b \u2200t \u2208 [1, T ] (3)\n\u2022 Gated-Add: learn a scale and shift parameters conditioned on the speaker information v [25, 26].\nz\u0303t = zt \u2297 \u03b3 + \u03b2 \u2200t \u2208 [1, T ] (4) \u03b3 = tanh(Wv) + b1 (5) \u03b2 = tanh(Uv) + b2 (6)\nwhere W,U, b1, b2 are trainable parameters.\n\u2022 Weighted-Simple-Add: The idea is to compute a score between each hidden representation zt and the speaker representation vector v. Sigmoid function is used to get a score between 0 and 1. Inspired from [19], we also apply a threshold k on the learned weights. We use a value of 0.4 for k.\nwt = \u03c3(zt \u00b7 (tanh(W \u00b7 v) + b1)) \u2208 R (7)\nwt = { wt, wt \u2265 k 0, wt < k\n(8)\nz\u0303t = zt + wt (Uv + b2) \u2200t \u2208 [1, T ] (9)\nwhere \u03c3 is the sigmoid function. \u00b7 and are the dotproduct and element-wise product operations respectively. W , U , b1, and b2 are trainable parameters.\nNote that the representation capability (learnable space of parameters) is almost similar in nature between methods Concat, Simple-Add, and Complex-Add (see Section 5.2)."
        },
        {
            "heading": "4. Experimental Setup",
            "text": "Experiments are conducted on the Switchboard 300h dataset [27] which consists of English telephony conversations. We use Hub5\u201900 as a development set for tuning the search hyperparameters. It consists of both Switchboard (SWB) and CallHome\n(CH) parts. CH part is much noisier. We use Hub5\u201901 as test set. RETURNN [28] is used to train the acoustic models and RASR [29] is used for decoding. All our configs files and code to reproduce our results can be found online1."
        },
        {
            "heading": "4.1. Baseline",
            "text": "We use the baseline from our previous work [1]. It has 12 conformer blocks and we do time down-sampling by a factor of 3. Up-sampling is applied to the output layer using transposed convolution [30] to match the target fixed alignment. The total number of parameters is 88M. More details regarding hyperparameters and training methods can be found in [1]. We noticed that the momentum and epsilon parameters used in the BN layer were not optimal. After updating these parameters, our baseline achieves now WER of 11.7% on Hub5\u201900. The details for applying speaker adaptation are explained in Section 5.2."
        },
        {
            "heading": "4.2. I-vectors Extraction",
            "text": "Our i-vector extraction pipeline follows the recipe described in [24]. To train the universal background model (UBM), 40-dimensional Gammatone features [31] with a context of 9 frames are concatenated and then reduced to a dimension of 60 using linear discriminant analysis (LDA). Normalization and LDA are usually applied after the extraction. We do not scale the i-vectors as done in [24] since we found this is crucial to get improvement. The same observation is also stated in [32]. For the final LDA dimension reduction, we have compared using i-vectors with dimensions 100, 200, and 300. Using 200- dimensional i-vectors is the best for our settings."
        },
        {
            "heading": "4.3. Sequence Discriminative Training (seq. train)",
            "text": "We use the lattice-based version of state-level minimum Bayes risk (sMBR) criterion [33]. The lattices are generated using a bigram LM. A small constant learning rate with value 1e-5 is used. At the final output layer, we use CE loss smoothing with a factor of 0.1. The sMBR loss scale is set to 0.9."
        },
        {
            "heading": "4.4. Language Models",
            "text": "In recognition, we use 4-gram count-based language model (LM) and LSTM LM as first pass decoding [34]. The LSTM LM has 51.3 perplexity (PPL) on Hub5\u201900 dataset. We use a transformer (Trafo) LM for lattice rescoring having PPL of 48.1 on Hub5\u201900 dataset."
        },
        {
            "heading": "5. Experimental Results",
            "text": ""
        },
        {
            "heading": "5.1. Improved Baseline",
            "text": "Our conformer acoustic model training recipe is built on top of our previous work [1]. The improved results are reported in Table 1. We replace BN with LN for all convolution modules. This also makes the model fits better for streaming mode. This leads to 3% relative improvement in terms of WER on Hub5\u201900. Moreover, we study the effect of adding another convolution module in each conformer block following the Macaron style as suggested in [8]. This improves the system by 4% relative in terms of WER on Hub5\u201900. In addition, we reduce the number of parameters by 40% relative which makes training more efficient and requires less memory usage. To achieve that, we use an attention dimension of value 384 with 6 attention heads. The dimension of the feed-forward module is 1536. We also remove LongSkip [1]. The total number of parameters is then only 58M . The initial learning value is set to 1e-5. We apply linear warmup over 5 epochs with a peak learning rate of 8e-4. Table 2 shows that the improved baseline with reduced dimensions is 27% relatively faster in terms of training speed.\n1https://github.com/rwth-i6/returnn-experiments/ tree/master/2022-swb-conformer-hybrid-sat"
        },
        {
            "heading": "Ti GPU.",
            "text": ""
        },
        {
            "heading": "5.2. Speaker Adaptation Results",
            "text": "We use the best conformer AM as described in Section 5.1 for speaker adaptive training (SAT). We apply SAT by integrating i-vectors into the network. We load the pretrained model and continue training with i-vectors while adapting SpecAugment and applying learning rate reset to escape from a local optimum as described in [2]. We also do not apply any learning rate warmup because we start from a pretrained model.\nIn Table 3, we compare different methods to integrate ivectors into the conformer AM which are described in Section 3. First, we can observe that we get more improvement on CH part compared to SWB part since CH part is much noisier and contains more unseen speakers. We can observe that Weighted-Simple-Add gives the larger improvement. Using this method, we achieve 3.5% relative improvement in terms of WER on CH part of Hub5\u201900 and 3% relative improvement overall. On SWB part, all methods give a similar improvement which is only 0.1% absolute.\nFor all SAT methods, we integrate the i-vectors into the first conformer block. In Table 4, we compare integrating i-vectors at different blocks of the conformer where Block 0 refers to the front-end VGG network. We can observe that integrating ivectors at the first conformer block gives the best improvement and the performance degrades as we use deeper blocks. We can also observe that integrating at many blocks at the same time does not help. These results are done using the best method Weighted-Simple-Add.\nFurthermore, we integrate i-vectors into the input of the MHSA module of the first conformer block. This means zt in the equations of Section 3 represents the input to the MHSA module. We decide to use this module since it captures global context which can potentially carry more speaker information. To measure this experimentally, we report in Table 5 the speaker error rates when adding a speaker identification loss on top of each module in the first conformer block. We can observe that the largest relative improvement is achieved at MHSA module. The settings for speaker identification experiments are described in Section 6."
        },
        {
            "heading": "5.3. Comparison between i-vector and x-vector",
            "text": "The x-vectors are generated following the TDNN architecture in [13]. The training data is split into train and dev sets. We use attentive pooling [35] instead of statistical pooling which improves the speaker identification accuracy from 87.5% to"
        },
        {
            "heading": "1 7.0 13.8 10.4",
            "text": ""
        },
        {
            "heading": "2 7.1 14.1 10.6",
            "text": ""
        },
        {
            "heading": "3 7.2 14.2 10.7",
            "text": "93.2% on the dev set. The comparison between using i-vectors and x-vectors for speaker adaptation using Weighted-SimpleAdd method is reported in Table 6. Using x-vectors did not improve the performance but instead, lead to minor degradation on CH part. On possible explanation is that x-vectors mainly focus on the speaker characteristics whereas i-vectors are designed to capture both speaker and channel characteristics which might be important in this case [26]. Further investigations are needed for experiments with x-vectors."
        },
        {
            "heading": "5.4. Overall Results",
            "text": "We summarize our results in Table 7 and compare with different modeling approaches and architectures from the literature. Compared to our previous work [1], this work achieves 11.0% and 4.1% relative improvement with transformer LM on Hub5\u201900 and Hub5\u201901 datasets respectively. Moreover, our model outperforms a well-optimized BLSTM attention system [36] by 3% and 5% with LSTM LM on Hub5\u201900 and Hub5\u201901 datasets respectively. In addition, our model outperforms the TDNN baseline by 9% relative on Hub5\u201900 dataset. Our best conformer AM is still behind the state-of-the-art results but not with a big margin. We argue that our baseline can be further improved by applying speed perturbation [36] and also longer training. Surprisingly, sequence training only give minor improvement which requires further investigation."
        },
        {
            "heading": "6. Speaker Identification Error Analysis",
            "text": "It has been observed empirically that the improvement by speaker adaptation is not significant when using a conformerbased AM [9]. However, BLSTM-based AMs benefit much more from speaker adaptation [24]. We argue that the reason behind this is that the conformer model is able to better learn speaker information in the lower layers which make it already robust to different speakers. To verify this, we conduct the fol-\nlowing experiment using the Switchboard 300h training dataset. We added a speaker identification loss on top of the output of each layer for both the conformer model and BLSTM model. The BLSTM model consists of 6 layers following [1, 24]. Then, we freeze all the AM parameters except the speaker identification loss parameters. We use an attention mechanism as [39] to get a soft speaker representation from the last output layer of the AM. This can also learn to skip unnecessary information such as silence. The total number of speakers is 520. We split the training dataset into train and dev parts. The speaker identification error rates on the dev set are reported in Figure 2. We can observe that the conformer model has a much better speaker identification accuracy using the lower layers compared to the BLSTM model. It was able to achieve using the second block a speaker identification error rate of 13% while the lowest error rate that the BLSTM achieves is 43%. These figures also show that lower layers are more important for learning speaker representations. For the conformer, we also observe that having the speaker loss on top of the MHSA module is enough to achieve such a low speaker error rate. This also explains why the proposed method to integrate the i-vectors into the MHSA module works the best for us. Moreover, the speaker error starts to increase for higher layers which is most probably because the model starts to filter out unnecessary speaker information to learn high-level features."
        },
        {
            "heading": "7. Conclusions & Future Work",
            "text": "In this work, we improved our training recipe for conformerbased hybrid Neural Network-Hidden Markov Model. We achieved 11% relative improvement in terms of word-errorrate (WER) on the Switchboard 300h Hub5\u201900 dataset. We also developed an efficient training recipe with 34% relative reduction in total number of parameters. Moreover, we investigated different ways to apply speaker adaptive training (SAT) for conformer-based acoustic models. We propose a method, called Weighted-Simple-Add to add weighted speaker information vectors to the input of multi-head self-attention module of the conformer AM. Using this method for SAT, we achieve 3.5% and 4.5% relative improvement in terms of WER on the CallHome part of Hub5\u201900 and Hub5\u201901 respectively. We also perform some analysis and showed that the conformer-based AM is already good enough for speaker identification task. For future work, we plan to apply our recipe to a noisier dataset\nsuch as CHiME-4 [40] as well as applying model-space SAT approaches [41]."
        },
        {
            "heading": "8. Acknowledgements",
            "text": "This work was partially supported by the project HYKIST funded by the German Federal Ministry of Health on the basis of a decision of the German Federal Parliament (Bundestag) under funding ID ZMVI1-2520DAT04A. We thank Wilfried Michel, Zolta\u0301n Tu\u0308ske, and Wei Zhou for useful discussions. We also thank Alexander Gerstenberger for performing lattice rescoring with Transformer."
        },
        {
            "heading": "9. References",
            "text": "[1] M. Zeineldeen, J. Xu, C. Lu\u0308scher, W. Michel, A. Gerstenberger,\nR. Schlu\u0308ter, and H. Ney, \u201cConformer-based Hybrid ASR System for Switchboard Dataset,\u201d in ICASSP, Singapore, May 2022, pp. 7437\u20137441.\n[2] W. Zhou, W. Michel, K. Irie, M. Kitza, R. Schlu\u0308ter, and H. Ney, \u201cThe RWTH ASR System for TED-LIUM Release 2: Improving Hybrid HMM with SpecAugment,\u201d in ICASSP, Barcelona, Spain, May 2020, pp. 7839\u20137843.\n[3] C. Lu\u0308scher, E. Beck, K. Irie, M. Kitza, W. Michel, A. Zeyer, R. Schlu\u0308ter, and H. Ney, \u201cRWTH ASR Systems for LibriSpeech: Hybrid vs Attention,\u201d in INTERSPEECH, Graz, Austria, Sep. 2019, pp. 231\u2013235.\n[4] S. Hochreiter and J. Schmidhuber, \u201cLong Short-term Memory,\u201d Neural Computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.\n[5] O. Abdel-Hamid, A. Mohamed, H. Jiang, L. Deng, G. Penn, and D. Yu, \u201cConvolutional Neural Networks for Speech Recognition,\u201d\nIEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 22, no. 10, pp. 1533\u20131545, 2014.\n[6] V. Peddinti, D. Povey, and S. Khudanpur, \u201cA Time Delay Neural Network Architecture for Efficient Modeling of Long Temporal Contexts,\u201d in INTERSPEECH, Dresden, Germany, Sep. 2015, pp. 3214\u20133218.\n[7] M. Sperber, J. Niehues, G. Neubig, S. Stu\u0308ker, and A. Waibel, \u201cSelf-Attentional Acoustic Models,\u201d in INTERSPEECH, Hyderabad, India, Sep. 2018, pp. 3723\u20133727.\n[8] A. Gulati, J. Qin, C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu, and R. Pang, \u201cConformer: Convolution-augmented Transformer for Speech Recognition,\u201d in INTERSPEECH, Shanghai, China, Oct. 2020, pp. 5036\u20135040.\n[9] Z. Tu\u0308ske, G. Saon, and B. Kingsbury, \u201cOn the Limit of English Conversational Speech Recognition,\u201d in INTERSPEECH, Brno, Czechia, Sep. 2021, pp. 2062\u20132066.\n[10] G. Saon, H. Soltau, D. Nahamoo, and M. Picheny, \u201cSpeaker Adaptation of Neural Network Acoustic Models Using I-Vectors,\u201d in ASRU, Olomouc, Czech Republic, Dec. 2013, pp. 55\u201359.\n[11] C. Leggetter and P. Woodland, \u201cMaximum Likelihood Linear Regression for Speaker Adaptation of Continuous Density Hidden Markov Models,\u201d Computer Speech & Language, vol. 9, no. 2, pp. 171\u2013185, 1995. [Online]. Available: https://www. sciencedirect.com/science/article/pii/S0885230885700101\n[12] N. Dehak, P. J. Kenny, R. Dehak, P. Dumouchel, and P. Ouellet, \u201cFront-End Factor Analysis for Speaker Verification,\u201d IEEE Transactions on Audio, Speech, and Language Processing, vol. 19, no. 4, pp. 788\u2013798, 2011.\n[13] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudanpur, \u201cX-Vectors: Robust DNN Embeddings for Speaker Recognition,\u201d in ICASSP, Calgary, Alberta, Canada, 2018, pp. 5329\u20135333.\n[14] F. Seide, G. Li, X. Chen, and D. Yu, \u201cFeature Engineering in Context-dependent Deep Neural Networks for Conversational Speech Transcription,\u201d in ASRU, Waikoloa, HI, USA, 2011, pp. 24\u201329.\n[15] R. Gemello, F. Mana, S. Scanzio, P. Laface, and R. De Mori, \u201cAdaptation of Hybrid ANN/HMM Models using Linear Hidden Transformations and Conservative Training,\u201d in ICASSP, vol. 1. Toubouse, France: IEEE, May 2006, pp. I\u2013I.\n[16] B. Li and K. C. Sim, \u201cComparison of Discriminative Input and Output Transformations for Speaker Adaptation in the Hybrid NN/HMM Systems,\u201d in Eleventh Annual Conference of the International Speech Communication Association, Makuhari, Chiba, Japan, 2010.\n[17] H. Zhu, L. Wang, P. Zhang, and Y. Yan, \u201cMulti-accent Adaptation Based on Gate Mechanism,\u201d in INTERSPEECH, Graz, Austria, Sep. 2019, pp. 744\u2013748.\n[18] X. Gong, Y. Lu, Z. Zhou, and Y. Qian, \u201cLayer-wise Fast Adaptation for End-to-End Multi-accent Speech Recognition,\u201d in INTERSPEECH, Brno, Czechia, 2021, pp. 1274\u20131278.\n[19] K. Deng, S. Cao, and L. Ma, \u201cImproving Accent Identification and Accented Speech Recognition under a Framework of Selfsupervised Learning,\u201d in INTERSPEECH, Brno, Czechia, Sep. 2021, pp. 1504\u20131508.\n[20] M. A. T. Turan, E. Vincent, and D. Jouvet, \u201cAchieving Multiaccent ASR via Unsupervised Acoustic Model Adaptation,\u201d in INTERSPEECH, Shanghai, China, Oct. 2020, pp. 1286\u20131290.\n[21] S. Ioffe and C. Szegedy, \u201cBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,\u201d in ICML, vol. 37, Lille, France, 2015, pp. 448\u2013456.\n[22] L. J. Ba, J. R. Kiros, and G. E. Hinton, \u201cLayer Normalization,\u201d CoRR, vol. abs/1607.06450, 2016.\n[23] V. Peddinti, G. Chen, V. Manohar, T. Ko, D. Povey, and S. Khudanpur, \u201cJHU ASpIRE System: Robust LVCSR with TDNNS, IVector Adaptation and RNN-LMS,\u201d in ASRU. Scottsdale, AZ, USA: IEEE, Dec. 2015, pp. 539\u2013546.\n[24] M. Kitza, P. Golik, R. Schlu\u0308ter, and H. Ney, \u201cCumulative Adaptation for BLSTM Acoustic Models,\u201d in INTERSPEECH, Graz, Austria, Sep. 2019, pp. 754\u2013758.\n[25] S. Yoo, I. Song, and Y. Bengio, \u201cA Highly Adaptive Acoustic Model for Accurate Multi-dialect Speech Recognition,\u201d in ICASSP, Brighton, UK, May 2019, pp. 5716\u20135720.\n[26] J. Rownicka, P. Bell, and S. Renals, \u201cEmbeddings for DNN Speaker Adaptive Training,\u201d in ASRU. Singapore: IEEE, Dec. 2019, pp. 479\u2013486.\n[27] J. Godfrey, E. Holliman, and J. McDaniel, \u201cSwitchboard: Telephone Speech Corpus for Research and Development,\u201d in ICASSP, vol. 1, San Francisco,USA, Mar. 1992, pp. 517\u2013520.\n[28] A. Zeyer, T. Alkhouli, and H. Ney, \u201cRETURNN as a Generic Flexible Neural Toolkit with Application to Translation and Speech Recognition,\u201d in Annual Meeting of the Assoc. for Computational Linguistics, Melbourne, Australia, Jul. 2018.\n[29] S. Wiesler, A. Richard, P. Golik, R. Schlu\u0308ter, and H. Ney, \u201cRASR/NN: The RWTH Neural Network Toolkit for Speech Recognition,\u201d in ICASSP, Florence, Italy, May 2014, pp. 3313\u2013 3317.\n[30] J. Long, E. Shelhamer, and T. Darrell, \u201cFully Convolutional Networks for Semantic Segmentation,\u201d in CVPR, Boston, USA, Jun. 2015, pp. 3431\u20133440.\n[31] R. Schluter, I. Bezrukov, H. Wagner, and H. Ney, \u201cGammatone Features and Feature Combination for Large Vocabulary Speech Recognition,\u201d in ICASSP, Honolulu, USA, Apr. 2007, pp. 649\u2013 652.\n[32] A. Rouhe, T. Kaseva, and M. Kurimo, \u201cSpeaker-aware Training of Attention-based End-to-end Speech Recognition Using Neural Speaker Embeddings,\u201d in ICASSP, Barcelona, Spain, May 2020, pp. 7064\u20137068.\n[33] M. Gibson and T. Hain, \u201cHypothesis Spaces for Minimum Bayes Risk Training in Large Vocabulary Speech Recognition,\u201d in INTERSPEECH, Pittsburgh, USA, Sep. 2006.\n[34] E. Beck, W. Zhou, R. Schlu\u0308ter, and H. Ney, \u201cLSTM Language Models for LVCSR in First-pass Decoding and Lattice-Rescoring,\u201d CoRR, vol. abs/1907.01030, 2019. [Online]. Available: http://arxiv.org/abs/1907.01030\n[35] K. Okabe, T. Koshinaka, and K. Shinoda, \u201cAttentive Statistics Pooling for Deep Speaker Embedding,\u201d in INTERSPEECH, Hyderabad, India, Sep. 2018, pp. 2252\u20132256.\n[36] Z. Tu\u0308ske, G. Saon, K. Audhkhasi, and B. Kingsbury, \u201cSingle Headed Attention Based Sequence-to-sequence Model for Stateof-the-Art Results on Switchboard,\u201d in INTERSPEECH, Shanghai, China, Sep. 2020, pp. 551\u2013555.\n[37] S. Hu, X. Xie, S. Liu, J. Yu, Z. Ye, M. Geng, X. Liu, and H. Meng, \u201cBayesian Learning of LF-MMI Trained Time Delay Neural Networks for Speech Recognition,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 1514\u20131529, 2021.\n[38] G. Saon, Z. Tu\u0308ske, D. Bolan\u0303os, and B. Kingsbury, \u201cAdvancing RNN Transducer Technology for Speech Recognition,\u201d in ICASSP. Toronto, Canada: IEEE, 2021, pp. 5654\u20135658.\n[39] S. Sigtia, E. Marchi, S. Kajarekar, D. Naik, and J. Bridle, \u201cMultiTask Learning for Speaker Verification and Voice Trigger Detection,\u201d in ICASSP. Barcelona, Spain: IEEE, May 2020, pp. 6844\u2013 6848.\n[40] Y. Yang, P. Wang, and D. Wang, \u201cA Conformer Based Acoustic Model for Robust Automatic Speech Recognition,\u201d CoRR, vol. abs/2203.00725, 2022. [Online]. Available: https: //doi.org/10.48550/arXiv.2203.00725\n[41] M. Kitza, R. Schlu\u0308ter, and H. Ney, \u201cComparison of BLSTM-layerspecific Affine Transformations for Speaker Adaptation,\u201d in INTERSPEECH, Hyderabad, India, Sep. 2018, pp. 877\u2013881."
        }
    ],
    "title": "Improving the Training Recipe for a Robust Conformer-based Hybrid Model",
    "year": 2022
}