{
    "abstractText": "Tuberculosis (TB) presents a substantial health risk to autopsy staff, given its three to five times higher incidence of TB compared to clinical staff. This risk is notably accentuated in South Korea, which reported the highest TB incidence rate and the third highest TB mortality rate among OECD member countries in 2020. The standard TB diagnostic method, histopathological examination of sputum or tissue for acid-fast bacilli (AFB) using Ziehl\u2013Neelsen staining, demands microscopic examination of slides at 1000\u00d7 magnification, which is labor-intensive and time-consuming. This article proposes a computer-aided diagnosis (CAD) system designed to enhance the efficiency of TB diagnosis at magnification less than 1000\u00d7. By training nine neural networks with images taken from 30 training slides and 10 evaluation slides at 400\u00d7 magnification, we evaluated their ability to detect M. tuberculosis. The N model achieved the highest accuracy, with 99.77% per patch and 90% per slide. We discovered that the model could aid pathologists in preliminary TB screening, thereby reducing diagnostic time. We anticipate that this research will contribute to minimizing autopsy staff\u2019s infection risk and rapidly determining the cause of death.",
    "authors": [
        {
            "affiliations": [],
            "name": "Joong Lee"
        },
        {
            "affiliations": [],
            "name": "Junghye Lee"
        }
    ],
    "id": "SP:8597c410a368d2138c8edc0fe2c1a1bea4df8632",
    "references": [
        {
            "authors": [
                "H. Lee",
                "J. Kim",
                "H. In",
                "Y. Kim"
            ],
            "title": "Characteristics and Trends in Deaths from Tuberculosis in the Republic of Korea, 2001\u20132020",
            "venue": "Public Health Wkly. Rep. PHWR 2021,",
            "year": 2021
        },
        {
            "authors": [
                "K. Kim"
            ],
            "title": "Automated Single-Cell Tracking Microscope System for Rapid Drug Susceptibility Test of M. Tuberculosis",
            "venue": "Ph.D. Thesis, Seoul National University, Seoul, Republic of Korea,",
            "year": 2016
        },
        {
            "authors": [
                "H. Lee",
                "J. Kim",
                "H. In",
                "Y. Kim"
            ],
            "title": "Review on Global Burden of Tuberculosis in 2020\u2014Global Tuberculosis Report 2021 WHO",
            "venue": "Public Health Wkly. Rep. PHWR 2022,",
            "year": 2022
        },
        {
            "authors": [
                "The Korean Academy of Tuberculosis",
                "Respiratory Diseases. Korean Guidelines For Tuberculosis, 4th ed.",
                "The Korean Academy of Tuberculosis",
                "Respiratory Diseases"
            ],
            "title": "Seoul, Republic of Korea, 2020",
            "venue": "Available online: https://www.cdc.go.kr/board/ board.es?mid=a20507020000&bid=0019#",
            "year": 2023
        },
        {
            "authors": [
                "E. Choi",
                "C. Jung",
                "H. Seong",
                "T. Kim",
                "W. Lee"
            ],
            "title": "A Comparative Analysis on The Efficiency of Various Clinical Methods for Diagnosis of Tuberculosis",
            "venue": "Korean J. Biomed. Lab. Sci. 1999,",
            "year": 1999
        },
        {
            "authors": [
                "Y.J. Ryu"
            ],
            "title": "Diagnosis and Treatment of Pulmonary Tuberculosis",
            "venue": "J. Korean Med. Assoc",
            "year": 2014
        },
        {
            "authors": [
                "K.R. Steingart",
                "M. Henry",
                "V. Ng",
                "P.C. Hopewell",
                "A. Ramsay",
                "J. Cunningham",
                "R. Urbanczik",
                "M. Perkins",
                "M.A. Aziz",
                "M. Pai"
            ],
            "title": "Fluorescence versus conventional sputum smear microscopy for tuberculosis: A systematic review",
            "venue": "Lancet Infect. Dis",
            "year": 2006
        },
        {
            "authors": [
                "E.G. Dzodanu",
                "J. Afrifa",
                "D.O. Acheampong",
                "I. Dadzie"
            ],
            "title": "Diagnostic Yield of Fluorescence and Ziehl-Neelsen Staining Techniques in the Diagnosis of Pulmonary Tuberculosis: A Comparative Study in a District Health Facility",
            "venue": "Tuberc. Res. Treat",
            "year": 2019
        },
        {
            "authors": [
                "Y.J. Ryu"
            ],
            "title": "Diagnosis of Pulmonary Tuberculosis: Recent Advances and Diagnostic Algorithms",
            "venue": "Tuberc. Respir. Dis",
            "year": 2015
        },
        {
            "authors": [
                "C.H. Collins",
                "J.M. Grange"
            ],
            "title": "Tuberculosis acquired in laboratories and necropsy",
            "venue": "rooms. Commun. Dis. Public Health 1999,",
            "year": 1999
        },
        {
            "authors": [
                "D. Wilkins",
                "A.J. Woolcock",
                "Y.E. Cossart"
            ],
            "title": "Tuberculosis: Medical students at risk",
            "year": 1994
        },
        {
            "authors": [
                "M. Sugita",
                "Y. Tsutsumi",
                "M. Suchi",
                "H. Kasuga",
                "T. Ishiko"
            ],
            "title": "Pulmonary tuberculosis: An occupational hazard for pathologists and pathology technicians in Japan",
            "venue": "Acta Pathol. Jpn. 1990,",
            "year": 1990
        },
        {
            "authors": [
                "R.J. Flavin",
                "N. Gibbons",
                "D.S. O\u2019Briain"
            ],
            "title": "Mycobacterium tuberculosis at autopsy\u2014Exposure and protection: An old adversary revisited",
            "venue": "J. Clin. Pathol",
            "year": 2006
        },
        {
            "authors": [
                "J.L. Burton"
            ],
            "title": "Health and safety at necropsy",
            "venue": "J. Clin. Pathol",
            "year": 2003
        },
        {
            "authors": [
                "A.U. Ibrahim",
                "E. Guler",
                "M. Guvenir",
                "K. Suer",
                "S. Serte",
                "M. Ozsoz"
            ],
            "title": "Automated detection of Mycobacterium tuberculosis using transfer learning",
            "venue": "J. Infect. Dev. Ctries. 2021,",
            "year": 2021
        },
        {
            "authors": [
                "J. Chang",
                "P. Arbel\u00e1ez",
                "N. Switz",
                "C. Reber",
                "A. Tapley",
                "J.L. Davis",
                "A. Cattamanchi",
                "D. Fletcher",
                "J. Malik"
            ],
            "title": "Automated Tuberculosis Diagnosis Using Fluorescence Images from a Mobile Microscope",
            "venue": "Med. Image Comput. Comput. Assist. Interv",
            "year": 2012
        },
        {
            "authors": [
                "Y. Xiong",
                "X. Ba",
                "A. Hou",
                "K. Zhang",
                "L. Chen",
                "T. Li"
            ],
            "title": "Automatic detection of mycobacterium tuberculosis using artificial intelligence",
            "venue": "J. Thorac. Dis. 2018,",
            "year": 1936
        },
        {
            "authors": [
                "S. Lopez-Garnier",
                "P. Sheen",
                "M. Zimic"
            ],
            "title": "Automatic diagnostics of tuberculosis using convolutional neural networks analysis of MODS digital images",
            "venue": "PLoS ONE 2019,",
            "year": 2120
        },
        {
            "authors": [
                "C.F.F.C. Filho",
                "P.C. Levy",
                "C.D.M. Xavier",
                "L.B.M. Fujimoto",
                "M.G.F. Costa"
            ],
            "title": "Automatic identification of tuberculosis",
            "venue": "mycobacterium. Res. Biomed. Eng",
            "year": 2015
        },
        {
            "authors": [
                "Y. Zaizen",
                "Y. Kanahori",
                "S. Ishijima",
                "Y. Kitamura",
                "H.-S. Yoon",
                "M. Ozasa",
                "H. Mukae",
                "A. Bychkov",
                "T. Hoshino",
                "J. Fukuoka"
            ],
            "title": "Deep-Learning-Aided Detection of Mycobacteria in Pathology Specimens Increases the Sensitivity in Early Diagnosis of Pulmonary Tuberculosis Compared with Bacteriology Tests",
            "venue": "Diagnostics 2022,",
            "year": 2022
        },
        {
            "authors": [
                "M. El-Melegy",
                "D. Mohamed",
                "T. ElMelegy",
                "M. Abdelrahman"
            ],
            "title": "Identification of Tuberculosis Bacilli in ZN-Stained Sputum Smear Images: A Deep Learning Approach",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops",
            "year": 2019
        },
        {
            "authors": [
                "M. El-Melegy",
                "D. Mohamed",
                "ElMelegy, T. Automatic Detection of Tuberculosis Bacilli from Microscopic Sputum Smear Images Using Faster R-CNN, Transfer Learning",
                "Augmentation. In Proceedings of the Pattern Recognition",
                "Image Analysis"
            ],
            "title": "9th Iberian Conference, IbPRIA 2019, Madrid, Spain, 1\u20134 July 2019; Part I",
            "venue": "pp. 270\u2013278. Available online: https: //dl.acm.org/doi/proceedings/10.1007/978-3-030-31332-6",
            "year": 2023
        },
        {
            "authors": [
                "S.R. Reshma",
                "T.R. Beegum"
            ],
            "title": "Microscope image processing for TB diagnosis using shape features and ellipse fitting",
            "venue": "In Proceedings of the 2017 IEEE International Conference on Signal Processing, Informatics, Communication and Energy Systems (SPICES), Kollam, India,",
            "year": 2017
        },
        {
            "authors": [
                "M. Zachariou",
                "O. Arandjelovi\u0107",
                "W. Sabiiti",
                "B. Mtafya",
                "D. Sloan"
            ],
            "title": "Tuberculosis Bacteria Detection and Counting in Fluorescence Microscopy Images Using a Multi-Stage Deep Learning Pipeline",
            "year": 2022
        },
        {
            "authors": [
                "Y. Taigman",
                "M. Yang",
                "M. Ranzato",
                "L. Wolf"
            ],
            "title": "DeepFace: Closing the Gap to Human-Level Performance in Face Verification",
            "venue": "In Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition, Columbus, OH, USA,",
            "year": 2014
        },
        {
            "authors": [
                "K.S. Suvarna",
                "L. Christopher",
                "J.D. Bancroft"
            ],
            "title": "Bancroft\u2019s Theory and Practice of Histological Techniques",
            "year": 2012
        },
        {
            "authors": [
                "A. Esteva",
                "B. Kuprel",
                "R.A. Novoa",
                "J. Ko",
                "S.M. Swetter",
                "H.M. Blau",
                "S. Thrun"
            ],
            "title": "Dermatologist-level classification of skin cancer with deep neural networks",
            "venue": "Nature 2017,",
            "year": 2017
        },
        {
            "authors": [
                "V. Gulshan",
                "L. Peng",
                "M. Coram",
                "M.C. Stumpe",
                "D. Wu",
                "A. Narayanaswamy",
                "S. Venugopalan",
                "K. Widner",
                "T. Madams",
                "J Cuadros"
            ],
            "title": "Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs",
            "venue": "JAMA",
            "year": 2016
        },
        {
            "authors": [
                "P. Rajpurkar",
                "J. Irvin",
                "K. Zhu",
                "B. Yang",
                "H. Mehta",
                "T. Duan",
                "D. Ding",
                "A. Bagul",
                "C. Langlotz",
                "K Shpanskaya"
            ],
            "title": "CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning",
            "venue": "arXiv 2017,",
            "year": 2017
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep Residual Learning for Image Recognition",
            "venue": "In Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV,",
            "year": 2016
        },
        {
            "authors": [
                "C. Szegedy",
                "V. Vanhoucke",
                "S. Ioffe",
                "J. Shlens",
                "Z. Wojna"
            ],
            "title": "Rethinking the Inception Architecture for Computer Vision",
            "venue": "In Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2016
        },
        {
            "authors": [
                "F. Chollet"
            ],
            "title": "Xception: Deep Learning with Depthwise Separable Convolutions",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, HI, USA,",
            "year": 2017
        },
        {
            "authors": [
                "G. Huang",
                "Z. Liu",
                "L. Van Der Maaten",
                "K.Q. Weinberger"
            ],
            "title": "Densely connected convolutional networks",
            "venue": "In Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, HI, USA,",
            "year": 2017
        },
        {
            "authors": [
                "M. Tan",
                "Q. EfficientNet Le"
            ],
            "title": "Rethinking Model Scaling for Convolutional Neural Networks",
            "venue": "In Proceedings of the 36th International Conference on Machine Learning, Long Beach, CA, USA, 9\u201315 June 2019; Volume 97, pp. 6105\u20136114. Available online: https://proceedings.mlr.press/v97/tan19a.html",
            "year": 2023
        },
        {
            "authors": [
                "I. Radosavovic",
                "R.P. Kosaraju",
                "R. Girshick",
                "K. He",
                "P. Dollar"
            ],
            "title": "Designing Network Design Spaces",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2020
        },
        {
            "authors": [
                "B. Zoph",
                "V. Vasudevan",
                "J. Shlens",
                "Q.V. Le"
            ],
            "title": "Learning Transferable Architectures for Scalable Image Recognition",
            "venue": "In Proceedings of the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, USA,",
            "year": 2018
        },
        {
            "authors": [
                "A. Dosovitskiy",
                "L. Beyer",
                "A. Kolesnikov",
                "D. Weissenborn",
                "X. Zhai",
                "T. Unterthiner",
                "M. Dehghani",
                "M. Minderer",
                "G. Heigold",
                "S Gelly"
            ],
            "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
            "venue": "arXiv 2021,",
            "year": 2021
        },
        {
            "authors": [
                "T. DeVries",
                "G.W. Taylor"
            ],
            "title": "Improved Regularization of Convolutional Neural Networks with Cutout",
            "venue": "arXiv 2017,",
            "year": 2023
        },
        {
            "authors": [
                "H. Zhang",
                "M. Cisse",
                "Y.N. Dauphin",
                "D. Lopez-Paz"
            ],
            "title": "Mixup: Beyond Empirical Risk Minimization",
            "venue": "arXiv 2018,",
            "year": 2018
        },
        {
            "authors": [
                "D. Anguita",
                "L. Ghelardoni",
                "A. Ghio",
                "L. Oneto",
                "S. Ridella"
            ],
            "title": "The \u2018K\u2019 in K-fold Cross Validation",
            "venue": "In Proceedings of the European Symposium on Artificial Neural Networks, Bruges, Belgium,",
            "year": 2012
        }
    ],
    "sections": [
        {
            "text": "Citation: Lee, J.; Lee, J. A Study of\nMycobacterium tuberculosis Detection\nUsing Different Neural Networks in\nAutopsy Specimens. Diagnostics 2023,\n13, 2230. https://doi.org/10.3390/\ndiagnostics13132230\nAcademic Editor: Hiroshi Ikegaya\nReceived: 30 May 2023\nRevised: 22 June 2023\nAccepted: 26 June 2023\nPublished: 30 June 2023\nCopyright: \u00a9 2023 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\nKeywords: acid-fast stain; automatic bacillus identification; computer-aided diagnosis (CAD); low resolution microscopic slide image; Mycobacterium tuberculosis; postmortem examination"
        },
        {
            "heading": "1. Introduction",
            "text": "Tuberculosis (TB) is a chronic, highly infectious, airborne disease caused by the 1- to 4-\u00b5m-long rod-shaped bacterium Mycobacterium tuberculosis. It ranks as the 13th leading cause of death worldwide, claiming 1.49 million lives, with 86% of all TB patients residing in India (26%), China (8.5%), and Indonesia (8.4%). The World Health Organization (WHO) forecasted an additional 137,000 TB fatalities globally for 2020 and 2021 due to the detrimental effects of COVID-19 on TB control measures. Despite a continuous decline in the incidence rate, South Korea still had the highest TB incidence rate and the third highest TB mortality rate among the 38 member countries of the Organization for Economic Cooperation and Development (OECD) in 2020. With new TB cases reaching 19,933 in 2020, the situation still presents a significant challenge [1\u20135]. Currently, several techniques are employed to diagnose TB infection. The commonly used methods include antibacterial plain microscopic smear (Ziehl\u2013Neelsen (ZN) stain), fluorescence microscopic smear (auramine O, auramine\u2013rhodamine stain), molecular tests (transcription-mediated amplification, strand-displacement amplification, conventional PCR, Xpert MTB/RIF), mycobacterial culture, drug susceptibility tests, histopathologic examination, and immunologic tests (tuberculin skin test (TST), interferon-gamma releasing assay (IGRA)). The histopathological examination using Ziehl\u2013Neelsen (ZN) staining, being the standard method, is widely used for the diagnosis of pulmonary TB due to its affordability, simplicity, and rapidity of results [6\u201311].\nDiagnostics 2023, 13, 2230. https://doi.org/10.3390/diagnostics13132230 https://www.mdpi.com/journal/diagnostics\nDiagnostics 2023, 13, 2230 2 of 14\nAutopsy plays a crucial role in both clinical and medicolegal systems by systematically examining the deceased to determine the cause of death, offering valuable medical insights, and gathering evidence for legal investigations when needed. However, autopsy personnel can face serious health risks due to potential exposure to various infectious agents, including HIV, hepatitis B and C viruses, and Mycobacterium tuberculosis. The Mycobacterium tuberculosis bacterium is known to be robust enough to survive in diverse environments for weeks to months under favorable conditions. Viable TB bacteria have been identified in the tissues of the deceased up to 36 days postmortem. In an autopsy study of 138 formalin-fixed lung tissues with histologic evidence of TB, three specimens were cultured positive for TB, suggesting the extended risk of TB infection from handling formalin-fixed tissues. TB infection can be spread via direct contact with contaminated body fluids or tissue, but it can also be disseminated by infectious aerosols, which are airborne particles approximately 1 to 5 \u00b5m in diameter that can stay suspended in the air for extended periods. Upon inhalation, these particles traverse the upper respiratory tract and reach the alveoli. Moving and manipulating a body can expel air from the lungs of an infected individual, aerosolizing the bacilli. Aerosols can also be generated by fluid aspirator hoses discharged into sinks, oscillating saws used on bone and soft tissue, and water sprayed by hoses onto tissue surfaces. In 1979, eight of 35 medical students at the University of Sydney who attended the autopsy of an immunosuppressed patient with active tuberculosis contracted the disease. Exposure during the autopsy, even as brief as 10 min, resulted in the spread of the disease. In Finland and Japan, studies have shown that pathologists who perform autopsies are more likely to contract occupational tuberculosis than those who do not perform autopsies and those who work in university departments of preventive medicine and public health. These data emphasize the need for appropriate infection prevention measures and rapid testing [12\u201316]. In the field of medical image analysis, convolutional neural networks (CNNs) have emerged as powerful tools for disease detection. CNN models excel in analyzing medical images, such as chest X-rays and mammograms, enabling the identification of lung diseases, cancers, and specific abnormalities. Given the global burden of TB and the need for rapid screening, this study focuses on leveraging deep learning networks to classify lowresolution slide images at 400\u00d7 magnification. We evaluated the performance of nine deep learning models, including NASNet-A Large, aiming to provide a preliminary screening tool to assist pathologists in shortening the diagnosis time. By improving the efficiency of TB diagnosis, this research contributes to enhancing healthcare outcomes. This study not only addresses the challenges associated with rapid TB screening at autopsy, but it also demonstrates the potential of deep learning networks in medical image analysis for disease detection. By harnessing the power of AI models, including CNNs, we aim to improve the accuracy and efficiency of TB diagnosis, ultimately leading to better healthcare practices and outcomes."
        },
        {
            "heading": "2. Related Works",
            "text": "Abdullahi Umar Ibrahim et al. cropped 178, 524, and 530 ZN slide sets into 227 \u00d7 227 \u00d7 3 patches with a 70% training and 30% testing split. They then performed transfer learning using AlexNet. As a result, they achieved accuracies of 98.15%, 98.09%, and 98.73% in experiments A, B, and C, respectively, demonstrating an analytical ability comparable to that of a pathologist [17]. Jeannette Chang et al. used 390 fluorescence microscopy slide images (92 positive, 298 negative) and achieved accuracy of 89.2% using a support vector machine (SVM) as a classifier after Otsu binarization. They used Hu moment and a histogram of oriented gradients (HOG) as feature vectors [18]. Yan Xiong et al. used 45 ZN slides (30 positive samples, 15 negative samples) cropped into 32 \u00d7 32 \u00d7 3 patches, and they applied a CNN (convolutional neural network) model to achieve 97.94% accuracy [19]. Santiago Lopez-Garnier et al. reported that they cropped 12,510 (4849 positive, 7661 negative) images of microscopic observed drug susceptibility (MODS) 2048 \u00d7 1536 into 224 \u00d7 224 \u00d7 1 patches and trained VGG16\u2019s CNN model with them, achieving 95.76% accuracy and\nDiagnostics 2023, 13, 2230 3 of 14\n94.27% sensitivity [20]. Costa Filho et al. reported that they segmented 120 ZN microscope images using RGB, HSI, YCbCr, and Lab color separation information and classified them using SVM, achieving 97.77% accuracy [21]. Yoshiaki Zaizen et al. trained the DenseNet-121 model using 40 negative slides after annotating two positive slides and tested the results with 42 patients\u2019 slides, achieving accuracy of 86% [22]. Moumen El-Melegy et al. used the ZNSM-iDB public database (ZN stained microscopy images) to detect regions using the Faster R-CNN (faster region-based convolutional neural network) model after cropping into 400 \u00d7 400 \u00d7 3 patches. They then resized the selected region to 20 \u00d7 20 and binary classified it with five layers of CNNs, achieving accuracy of 98.4% [23,24]. Reshma SR et al. demonstrated that contour extraction, ellipse detection, and ellipse merging techniques could be used to count M. tuberculosis bacteria on 176 images with 91.5% accuracy [25]. Marios Zachariou et al. demonstrated that ResNet50 can detect M. tuberculosis with 99.74% accuracy in 230 fluorescently stained microscope slides after segmentation by Cycle-Gan and classification of the extracted regions using ResNet, DenseNet, and SqueezeNet [26]."
        },
        {
            "heading": "3. Materials and Methods",
            "text": "To diagnose tuberculosis, sputum or tissue is stained with Ziehl\u2013Neelsen staining and examined under a microscope: the cells appear blue and complex in shape, while the waxy lipids in the cell wall of the bacillus appear red. The South Korean TB diagnosis guidelines suggest that the slides should be examined at a 1000\u00d7 magnification, which is not an easy task even for pathologists [27]. In a microscopic examination, Mycobacterium tuberculosis is often difficult to identify due to its small size and irregular shape. The conditions of the tissue specimen and various artifacts, such as low contrast background, variations in the degree of staining, and tissue folding, can also make the bacillus even more difficult to identify. In addition, a large number of slides or small number of bacilli on a slide can require a long time to examine, which can lead to misdiagnosis due to the fatigue of the reading pathologist. Therefore, there have been requests and efforts to expedite reading with the help of artificial intelligence assistants. Recently, several models of CNNs, a new and powerful form of deep learning, have been used to detect areas of TB bacteria. Since equipment that digitizes slides at 1000\u00d7 magnification is expensive, equipment that digitizes at 200/400\u00d7 magnification is commonly used in practice. The ability to read at lower magnifications than the prescribed 1000\u00d7 would be a major advantage in terms of applicability and time. To the authors\u2019 knowledge, no successful studies of automated reading at lower resolutions have been published so far. The authors investigated techniques for effective detection of Mycobacterium tuberculosis on low-resolution digitized slide images. As described in Section 3.2, we utilized various CNN and ViT (vision transformer) models to compare their performance and explore their applicability."
        },
        {
            "heading": "3.1. Materials",
            "text": "Samples from 14 TB-infected lungs and 26 normal lungs were collected during autopsy at the National Forensic Service of Korea. All specimens were fixed, sectioned, and stained with Ziehl\u2013Neelsen stain according to laboratory regulations to be reviewed by a pathologist. All specimens were further analyzed for tuberculosis nucleic acid amplification testing (TB-PCR) were and used in the experiment only if a match was found. All slides were scanned using a digital pathology scanner (Roche Co., Basel, Switzerland, VENTANA DP 200 slide scanner). Patch-based dataset construction was performed by creating a 224 \u00d7 224 \u00d7 3 patch from an image scanned at 400\u00d7 magnification, excluding the background area and including the actual tissue area. Nine of 14 positive slide samples were labeled as simple true/false by the pathologist, identifying which of the patches contained M. tuberculosis. Of the 351,875 positive spots, only 47,017 actually contained TB bacteria. From a total of 3.1 million negative spots, the same number of negative spots as the positive samples\nDiagnostics 2023, 13, 2230 4 of 14\nwere randomly selected to form the complete dataset (98,034). Figure 1 shows a patch of a positive slide and the location of the tubercle bacilli.\nDiagnostics 2023, 13, x FOR PEER REVIEW 4 of 14\n3.1 million negative spots, the same number of negative spots as the positive samples were randomly selected to form the complete dataset (98,034). Figure 1 shows a patch of a positive slide and the location of the tubercle bacilli.\n(a) (b) (c)\nFigure 1. Examples of annotations. (a) Whole slide image of lung tissue containing Mycobacterium tuberculosis; Ziehl\u2013Neelsen staining, scanning magnification view; (b) cropped patch image of (a), 400\u00d7; (c) annotated short, rod-shaped bacilli.\n3.2. Neural Network Model CNN models are popular because they automatically extract features using a convolution layer; reduce computational complexity using a pooling layer; introduce non-linearity into the model using activation functions, such as ReLUs (rectified linear units); and improve feature extraction overall, making them more effective at learning complex patterns in large datasets than machine learning methods, such as logistic regression or decision trees. In facial recognition, for example, the Facebook team used a CNN called DeepFace to perform human-level face recognition. Andre Esteva et al. demonstrated dermatologist-level classification performance in skin cancer detection using a CNN called Inception-v3 on a large dataset of skin cancer images. Gulshan et al. outperformed the average human ophthalmologist at detecting diabetic retinopathy using Inception-v3. Pranav Rajpurkar et al. demonstrated radiologist-level performance in detecting 14 diseases from chest X-rays using a 121-layer CNN called CheXNet [28\u201331]. This section provides a brief overview of the deep learning network models used in the comparison, as shown in Table 1.\nFigure 1. Examples of annotations. (a) Whole slide image of lung tissue containing Mycobacterium tuberculosis; Ziehl\u2013Neelsen staining, scanning magnification view; (b) cropped patch image of (a), 400\u00d7; (c) annotated short, rod-shaped bacilli."
        },
        {
            "heading": "3.2. eural et ork odel",
            "text": "els re l r ec se t e t atically extract features using a convolution layer; reduce computational complexity using a pooling layer; introduce non-li earity into the model using activation fu ctions, such as ReLUs (rectified linear units); and improve f ature extraction overall, making them ore effective at learning complex patterns in large datasets than machine learning methods, such as logistic reg ssion r decision trees. In facial recognitio , f r example, the Fac book team used a CNN called De pFac to perform human-level fac recognition. Andre Esteva et al. d monstrated dermatologistlevel classification performance in skin cancer detection using a CNN c lled Inception-v3 on a large dat set of skin cancer images. Gulshan et al. outp rformed the average human ophthalmologist t detecting diab tic retinopathy using Inception-v3. Pranav Rajpurkar et al. demonstrat d radiologist-level performance in detecting 14 diseases from che t X-rays using a 121-layer CNN called heXNet [28\u201331]. is secti r i es rief er ie f t e l r i et r els se i t e co ariso , as s o i able 1.\nl . er ie f t e ee l r i t r l i i .\nDiagnostics 2023, 13, 2230 5 of 14\n3.2.1. ResNet50\nKaiming He and colleagues at Microsoft encountered a problem with the gradient disappearing as the network depth increased. To address this issue, they utilized residual learning with skip connections, which enabled deep learning to occur effectively [32]. The structure is depicted in Figure 2.\nDiagnostics 2023, 13, x FOR PEER REVIEW 5 of 14\n3.2.1. ResNet50 Kaiming He and colleagues at Microsoft encountered a problem with the gradient disappearing as the network depth increased. To address this issue, they utilized residual learni g with skip connections, which enabled eep learni g to ccur effectively [32]. The structure is depicted in Figure 2.\nFigure 2. The architecture of the Resnet50 model.\n3.2.2. Inception v3 Szegedy et al. created a method to learn multi-level features more effectively. This goal was accomplished by employing modules with parallel convolutions of varying sizes, which minimized computation while also addressing the problem of overfitting [33]. The structure is shown in Figure 3.\nFigure 3. The architecture of the Inception V3 model.\n3.2.3. Xception The Xception architecture, an advanced form of the Inception module, effectively separates pointwise and depthwise convolutions, as illustrated in Figure 4. This arrangement allows for independent computation of cross-channel and spatial correlations. Consequently, the network can learn spatial and channel-specific features separately, boosting the efficiency of image representation learning [34].\nFigure 2. The architecture of the Resnet50 model.\n3.2.2. Inception v3\nSzegedy et al. created a method to learn multi-level features more effectively. This goal was accomplished by employing modules with parallel convolutions of varying sizes, which minimized computation while also addressing the problem of overfitting [33]. The structure is shown in Figure 3.\nDiagnostics 2023, 13, x FOR PEER REVIEW 5 of 14\n3.2.1. ResNet50 Kaiming He and colleagues at Microsoft encountered a problem with the gradient disappearing as the etwork depth increased. To address this issue, they utilized resi ual learning with skip co nections, which enabled deep learning to occur effectively [32]. The structure is depicted in Figure 2.\nFigure 2. The architecture of the Resnet50 model.\n3.2.2. Inception v3 Szegedy et al. created a ethod to learn ulti-level features ore effectively. This goal as acco plished by employing modules with parallel convolutions of varying sizes, which inimized computation while also addressing the problem of overfitting [33]. The structure is shown in Figure 3.\nFigure 3. The architecture of the Inception V3 model.\n3.2.3. Xception The Xception architecture, an advanced form of the Inception module, effectively separates pointwise and depthwise convolutions, as illustrated in Figure 4. This arrangement allows for independent computation of cross-channel and spatial correlations. Consequently, the network can learn spatial and channel-specific features separately, boosting the efficiency of image representation learning [34].\nFigure 3. The architecture of the Inception V3 odel.\n3.2.3. Xception\nThe Xception architecture, an advanced form of the Inception module, effectively separates pointwise and depthwise convolutions, as illustrated in Figure 4. This arrangement allows for independent computation of cross-channel and spatial correlations. Consequently, the network can learn spatial and channel-specific features separately, boosting the efficiency of image representation learning [34].\nDiagnostics 2023, 13, 2230 6 of 14 Diagnostics 2023, 13, x FOR PEER REVIEW 6 of 14\nFigure 4. The architecture of the Xception.\n3.2.4. DenseNet DenseNet (densely connected convolutional networks), proposed by Huang et al., is a network structure that connects each layer\u2019s feature map to the feature maps of all subsequent layers, as illustrated in Figure 5. This connection pattern enhances feature propagation and improves parameter efficiency [35].\nFigure 5. The architecture of the DenseNet.\n3.2.5. EfficientNet Introduced by Tan and Le, EfficientNet systematically explores model scaling (in terms of depth, width, and resolution) as a way to achieve maximal efficiency with limited resources. The model selects efficient combinations to deliver better performance, even with smaller models [36]. The structure is shown in Figure 6.\nFigure 6. The architecture of EfficientNet.\nFigure 4. The architecture of the Xception.\n3.2.4. DenseNet\nDenseNet (densely connected convolutional networks), proposed by Huang et al., is a etwork structure that connects each layer\u2019s feature ma to the feature maps of all subsequent lay rs, as illustrated in Figure 5. This connection pattern enhances feature propagation and improves param ter efficiency [35].\nDiagnostics 2023, 13, x FOR PEER REVIEW 6 of 14\nFigure 4. The architecture of the Xception.\n3.2.4. DenseNet DenseNet (densely connected convolutional networks), prop sed by Huang et al., is a network struc ur that connects ach yer\u2019s f atur map to the f ature m ps of all subs quent layers, as illustrated in Figure 5. This conn cti patter enhances featur propagati n nd improve aramete effici ncy [35].\nFigure 5. The architecture of the DenseNet.\n3.2.5. EfficientNet Introduced by Tan and Le, EfficientNet systematically explores model scaling (in terms of depth, width, and resolution) as a wa to achieve ma imal efficiency with lim ted esources. The model selects efficient combinations to deliver b tter performance, even with smaller odels [36]. The struc ure is shown in Figure 6.\nFigure 6. The architecture of EfficientNet.\nFigure 5. The architecture of the DenseNet.\n3.2.5. EfficientNet\nIntroduced by Tan and Le, EfficientNet systematically explores model scaling (in terms of depth, width, and resolution) as a way to achieve maximal efficiency with limited resources. The model selects efficient combinations to deliver better performance, even with smaller models [36]. The structure is shown in Figure 6.\nDiagnostics 2023, 13, x FOR PEER REVIEW 6 of 14\nFigure 4. The architecture of the Xception.\n3.2.4. DenseNet DenseNet (densely connected convolutional networks), proposed by Huang et al., is a network structure that connects each layer\u2019s feature map to the feature maps of all subsequent layers, as illustrated in Figure 5. This connection pattern enhances feature propagation and improves parameter efficiency [35].\nFigure 5. The architecture of the DenseNet.\n3.2.5. EfficientNet Introduced by Tan d Le, EfficientNet systematic lly explores model scaling (in terms of depth, width, and resolution) as a w y to achieve maximal efficiency with limited resources. The model s ects efficient combinations to deliv r better performance, even with smaller models [36]. The struct re is shown in Figure 6.\nFigure 6. The architecture of EfficientNet. Figure 6. The architecture of Efficien Net.\n3.2.6. RegNet\nRadosavovic et al. proposed RegNet, a class of models that utilize a simple design space to represent regularized network designs. The model aims to find regular (i.e., evenly\nDiagnostics 2023, 13, 2230 7 of 14\ndistributed) networks that exhibit a simple and predictable structure while still delivering high performance [37].\n3.2.7. NASNet\nNASNet is a model discovered through the neural architecture search (NAS), a method proposed by Zoph et al. NAS automates the design of neural networks. In contrast to models such as ResNet and Inception, which are manually designed by human engineers who specify and stack blocks to build the models, NASNet\u2019s architecture consists of convolutional blocks that were automatically generated. The process of discovery used reinforcement learning and RNNs to navigate the space of possible architectures, leading to the final structure of NASNet [38].\n3.2.8. Vision Transformer (ViT)\nDosovitsky et al. proposed ViT, which applies transformers from the realm of natural language processing (NLP) to image classification. This model treats patches on an image as sequence elements, demonstrating that the scalability of transformers can also be leveraged for image classification tasks. This approach can deliver better results than CNN architectures and requires fewer computational resources for model training [39].\n3.2.9. Swin Transformer\nProposed by Liu et al., the Swin transformer is a transformer variant specifically designed for vision tasks. Like vision transformer (ViT), it breaks an image into nonoverlapping patches, but it also incorporates a moving window mechanism to capture both local and global information. The Swin transformer has achieved state-of-the-art results for several vision benchmarks, including image classification, object detection, and semantic segmentation [40]."
        },
        {
            "heading": "3.3. Experiments",
            "text": "In this study, we employed seven CNN architectures and two vision transformer models, as detailed in Table 1, with the aim of classifying slide images into two categories: detection and non-detection of Mycobacterium tuberculosis. All models were set with a mini-batch size of 24, except for NasNet, which utilized a size of 12. We adopted a learning rate of 0.001 for all models, with the exception of the two Vit-series models, which used a rate of 0.000001. The Adam optimization algorithm was implemented as the optimizer across all models. Additionally, we set the epoch count at 20 and applied an L2 regularizer with a value of 0.0001. The training process for all models was conducted utilizing the cosine annealing scheduler. The study used RGB color images, with their sizes adjusted based on the model in use. For instance, the Inception v3, Xception, and NASNet models were supplied with images sized 299 \u00d7 299, although NASNet usually recommends a larger size. All other models were provided with images sized 224 \u00d7 224. We applied various data augmentation techniques to the training set images, aiming to enhance model generalization and reduce overfitting risk. These techniques encompassed rotation within a range of \u22125 to 5 degrees, scaling from \u221210 to 10%, vertical and horizontal flipping, and contrast adjustment up to 10%. Additional methods included brightness adjustment, cutout augmentation, and mixup augmentation. Cutout augmentation, which erases random square sections of the input image, encourages the network to interpret more context for each pixel during training. Mixup augmentation creates synthetic examples in the input space through linear interpolation between randomly paired training examples, smoothing the model\u2019s decision boundaries. This process reduces the overfitting likelihood and enhances the model\u2019s generalization capabilities. For mixup augmentation, we used a mix ratio of 0.25 and an alpha value of 0.2 [41,42]. To preclude any overlap or bias between the training and test datasets and to effectively evaluate model performance, we implemented K-fold cross-validation (K = 5). The entire\nDiagnostics 2023, 13, 2230 8 of 14\ndataset was partitioned into five mutually exclusive subsets of equal size. Each subset served as a test set once, with the remaining four subsets used for training. This process enabled us to train and test on every dataset, and the average predictive performance is often used as an indicator of the model\u2019s generalization ability. We designated 10% of the training data as the validation set and assessed the test set using the model that had the best performance during the validation phase [43]. To evaluate the performance of the proposed methodology, we employed the following performance metrics: accuracy, precision, sensitivity, and F1-score, which are defined in Equations (1)\u2013(4), respectively [44].\nAccuracy is the ratio of correct predictions to the total data.\nAccuracy = TP + TN/(TP + TN + FP + FN) (1)\nPrecision is the ratio of true positive predictions to all positive predictions.\nPrecision = TP/(TP + FP) (2)\nSensitivity, also known as recall, is the ratio of true positives that are correctly predicted to be positive.\nSensitivity = TP/(TP + FN) (3)\nF1 score is the harmonic mean of precision and sensitivity.\nF1 score = 2 \u00d7 (Precision \u00d7 Sensitivity) (Precision + Sensitivity)\n(4)"
        },
        {
            "heading": "4. Results",
            "text": "The results of evaluating the performance of nine models using pathologist-labeled patches from 30 slides are summarized in Table 2. Notably, NASNet-A Large achieved the highest accuracy of 99.777%. We further assessed the models on a slide-by-slide basis using five negative and five positive slides, which were labeled by pathologists at the slide level rather than on a patch-by-patch basis. The evaluation was performed using the average ensemble of the five-fold models of NASNet-A Large and Densenet169 each, and the results are presented in Figure 7.\nDiagnostics 2023, 13, 2230 9 of 14 Diagnostics 2023, 13, x FOR PEER REVIEW 9 of 14\nFigure 7. Number of positive patches per slide at two different thresholds. Left: 0.5 (threshold); Right: 0.7 (threshold).\nWhen considering only informative negative slides, the patch detection accuracy reached 99.978% and 99.820% for the NASNet-A Large and Densenet169 models, respectively. Since the slides were not labeled at the patch level, we relied on the number of positive patches for evaluation. The classification accuracy of NASNet-A Large for determining whether slides were positive or negative was 90%, while the Densenet169 model achieved 100% accuracy. Figure 7 shows the results for a threshold of 0.5 (left) and for a threshold of 0.7 (right). Slides numbered 1 to 5 correspond to positive slides, while slides 6 to 10 represent negative slides. As depicted in Figure 7, DenseNet169 produced more estimated positives on the positive slides compared to the number of estimated positive patches on the negative slides. Increasing the threshold led to a decrease in the number of positive patches, but the number of estimated positives on the positive slides remained higher than that on the negative slides. For the thresholds used in the experiment (0.5, 0.6, 0.7, 0.8, and 0.9), the number of false positives on the perfectly positive slide was higher than the number of false negatives. On the other hand, NASNet-A Large demonstrated that one of the positive slides had fewer false positives than the negative slide, and this error rate remained consistent regardless of the threshold. Thus, the threshold did not have a significant impact on the performance. Some of the presumed positive patches that were actually negative are shown in Figure 8. These errors included the appearance of clusters of M. tuberculosis bacteria despite the absence of M. tuberculosis bacteria, blurred foci of M. tuberculosis bacteria, and poor post-stain washing of M. tuberculosis bacteria.\nFigure 8. False positive patches.\nFigure 7. Number of positive patches per slide at two different t resholds. Left: 0.5 (threshold); Right: 0.7 (threshold).\nWhen considering only informative negative slides, the patch detection accuracy reached 99.978% and 99.820% for the NASNet-A Large and Densenet169 models, respectively. Since the slides were not labeled at the patch level, we relied on the number of positive patches for evaluation. The classification accuracy of NASNet-A Large for determining whether slides were positive or negative was 90%, while the Densenet169 model achieved 100% accuracy. Figure 7 shows the results for a threshold of 0.5 (left) and for a threshold of 0.7 (right). Slides numbered 1 to 5 correspond to positive slides, while slides 6 to 10 represent negative slides. As depicted in Figure 7, DenseNet169 produced more estimated positives on the positive slides compared to the number of estimated positive patches on the negative slides. Increasing the threshold led to a decrease in the number of positive patches, but the number of estimated positives on the positive slides remained higher than that on the negative slides. For the thresholds used in the experiment (0.5, 0.6, 0.7, 0.8, and 0.9), the number of false positives on the perfectly positive slide was higher than the number of false negatives. On the other hand, NASNet-A Large demonstrated that one of the positive slides had fewer false positives than the negative slide, and this error rate remained consistent regardless of the threshold. Thus, the threshold did not have a significant impact on the performance. Some of the presumed positive patches that were actually negative are shown in Figure 8. These errors included the appearance of clusters of M. tuberculosis bacteria despite the absence of M. tuberculosis bacteria, blurred foci of M. tuberculosis bacteria, and poor post-stain washing of M. tuberculosis bacteria.\nDiagnostics 2023, 13, x FOR PEER REVIEW 9 of 14\nFigure 7. Number of positive patches per slide at two different thresholds. Left: 0.5 (threshold); Right: 0.7 (threshold).\nWhen considering only informative negative slides, the patch detection accuracy reached 99.978% and 99.820% for the NASNet-A Large and Densenet169 models, respectively. Since the slides were not labeled at the patch level, we relied on the number of positive patches for evaluation. The classification accuracy of NASNet-A Large for determining whether slid s were positiv or negative was 90%, while the Densenet169 mod l achieved 100% accuracy. Figure 7 shows the results for a threshold of 0.5 (left) and for a threshold of 0.7 (right). Slides numbered 1 to 5 correspond to po itive slides, while slides 6 to 10 represent negative slides. As depicted in Figure 7, Den eN t169 produced more estimated positives on the positive slides compared to the umber of estimat positive patches on the negative slides. Increa ing the threshold led to a d crease in the number of positive patch s, but the number of estimate positives on th positive slides rema ned higher than at on th negative slides. For the thresholds used in the experiment (0.5, 0.6, 0.7, 0.8, and 0.9), the number of false p sitives on the perfectly positive slide was higher than the umber of false n gatives. On the other hand, NASNet-A Larg demonstrated that one of the positiv slides had fewer false positives than the negative slide, and this error rate remained consistent regardless of the threshold. Thus, the threshold did not have a significant impact on the performance. Some of the presumed positive patches that were actually negative are shown in Figure 8. These errors included the appearance of clusters of M. tuberculosis bacteria despite the absence of . tuberculosis bacteria, blurred foci of . tuberculosis bacteria, and poor post-stain washing of M. tuberculosis bacteria.\nWe also utilized gradient-weighted class activation mapping (Grad-CAM) to pinpoint\non what exactly the network was focusing. We calculated the gradient of a class\u2019s score with respect to the feature map of the penultimate convolutional layer, then globally averaged\nDiagnostics 2023, 13, 2230 10 of 14\nthis gradient to generate the weights, and finally created the weighted combination of the activation map. If the training has been successful, the activation map will highlight the presence of M. tuberculosis in the patch. The degree of activation is represented by a jet color map, where blue signifies the lowest activation and red the highest. As depicted in Figure 9, there was a high level of activation for M. tuberculosis.\nDiagnostics 2023, 13, x FOR PEER REVIEW 10 of 14\nWe also utilized gradient-weighted class activation mapping (Grad-CAM) to pinpoint on what exactly the network was focusing. We calculated the gradient of a class\u2019s score with respect to the feature map of the penultimate convolutional layer, then globally average this gradient to generate the weights, and finally creat d th weighted c mbination of the activation map. If the tr ining has been successful, the activ tion map will ig - light the presence of M. tuberculosis in the patch. The degree of activation is represented by a jet color map, where blue signifies the lowest activation and red the highest. As depicted in Figure 9, there was a high level of activation for M. tuberculosis.\nFigure 9. (Left): input image; (right): heat map superimposed on left image.\n5. Discussion In this study, we evaluated the performance of seven unique convolutional neural network (CNN) architectures and two vision transformer models in classifying M. tuberculosis slide images into two categories: detection and non-detection. This study represents a novel application of deep learning networks to the task of identifying M. tuberculosis on low-resolution slide scans, an approach that shows potential for revolutionizing TB diagnosis. Among the models tested, NASNet-A Large exhibited exceptional performance, attaining accuracy of 99.777%. This superior result implies that this model could potentially provide invaluable assistance to pathologists. Additionally, our innovative incorporation of Grad-CAM to interpret the training process of the CNN architecture is a noteworthy contribution to the field. The heat map visualizations generated by Grad-CAM accurately emphasized the regions within each image patch that contained TB bacteria during training. This process provides a compelling possibility of the enhanced detection of small area lesions.\nFigure 9. (Left): input image; (right): heat map superimposed on left image."
        },
        {
            "heading": "5. Discussion",
            "text": "In this study, we evaluated the performance of seven unique convolutional neural network (CNN) architectures and two vision transformer models in classifying M. tuberculosis slide images into two categories: detection and non-detection. This study represents a novel application of deep learning networks to the task of identifying M. tuberculosis on low-resolution slide scans, an approach that shows potential for revolutionizing TB diagnosis. Among the models tested, NASNet-A Large exhibited exceptional performance, attaining accuracy of 99.777%. This superior result implies that this model could potentially provide invaluable assistance to pathologists. Additionally, our innovative incorporation of Grad-CAM to interpret the training process of the CNN architecture is a noteworthy contribution to the field. The heat map visualizations generated by Grad-CAM accurately emphasized the regions within each image patch that contained TB bacteria during training. This process provides a compelling possibility of the enhanced detection of small area lesions. Regarding performance, the ensemble deep learning networks of Densen169 and NASNET exhibited impressive potential, with Densen169 achieving 100% positive and 100% negative separation on a slide-by-slide basis. NASNET was able to assist pathologists in making accurate judgements by requiring the examination of only a few dozen positive patches. This model could potentially increase the efficiency of TB diagnosis significantly.\nDiagnostics 2023, 13, 2230 11 of 14\nNevertheless, it is important to acknowledge the limitations of our current approach. Despite the high per-slide accuracy levels of 90% (NASNet-A Large) and 100% (Densenet169), the limited number of slides and the narrow threshold distinguishing negative and positive cases prevent broader generalization. Moreover, upon reviewing the 40 slides represented in Figure 10, the need for an increased number of training images for improved generalization became evident. Some slides were excessively thin, displayed faint images, or showed varying staining degrees, pointing to a more diverse range of real-world cases. To create a robust model capable of effectively handling these varied cases, extensive training with a larger and more diverse set of slide images is indispensable.\nDiagnostics 2023, 13, x FOR PEER REVIEW 11 of 14\nRegarding performance, the ensemble deep learning networks of Densen169 and NASNET exhibited impressive potential, with Densen169 achieving 100% positive and 100% negative separation on a slide-by-slide basis. NASNET was able to assist pathologists in making accurate judgements by requiring the examination of only a few dozen positive patches. This model could potentially increase the efficiency of TB diagnosis significantly.\nevert eless, it is i orta t to ack o le ge t e li itatio s of o r c rre t a roac . espite the high per-slide accuracy levels of 90% ( ASNet-A Large) and 100% (Densenet169), the li ited number of slides and the narrow threshold distinguishing negative and positive cases prevent broader generalization. Moreover, upon reviewing the 40 slides represented in Figure 10, the need for an increased number of training images for improved generalization became evident. Some slides were excessively thin, displayed faint images, or showed varying staining degrees, pointing to a more diverse range of realworld cases. To create a robust model capable of effectively handling these varied cases, extensive training with a larger and more diverse set of slide images is indispensable.\nFigure 10. Top: negative slide images; bottom: positive slide images.\nIn our future work, we aim to collect additional slide samples to improve the model\u2019s performance and applicability to low-resolution slides at magnification of 200\u00d7.\n6. Conclusions Tuberculosis (TB) remains a serious global health issue, claiming approximately 1.49 million lives annually. The risk is high even in countries with a high incidence of TB, such as South Korea, particularly during autopsies. Standard TB testing involves a labor-intensive and time-consuming examination of slides at 1000\u00d7 magnification following Ziehl\u2013 Neelsen staining. In response to this challenge, there have been attempts to develop automated TB sputum tests using high-resolution microscopic images. However, these techniques often prove inadequate in settings where slides can only be digitized using low-resolution scanners. In our study, we evaluated the performance of nine deep learning networks in classifying low-resolution slide images at 400\u00d7 magnification. The NASNet-A Large model achieved remarkable accuracy of 99.777%, demonstrating its potential for automated testing. Furthermore, Densen169 showed 100% accuracy on a slide-by-slide basis, and NASNET enabled pathologists to make accurate judgements swiftly, as they only needed to inspect a few dozen positive patches. This ability demonstrates potential support for preliminary testing by pathologists to reduce diagnostic time, which could in turn enhance the efficiency of TB diagnosis and contribute to preventing TB.\nFigure 10. Top: negative slide images; bottom: positive slide images.\nIn our future ork, e ai to collect additional slide sa ples to i prove the odel\u2019s perfor ance and applicability to low-resolution slides at magnification of 200\u00d7.\n6. l si s\nuberc l sis re ains a serious global health issue, claiming approximately 1.49 million lives annually. The risk is high even in countries with a high incidence of TB, such as South Korea, particularly during autopsies. Standard TB testing involves a labor intensive and time-consuming ex mi ation of slides at 1000\u00d7 magnificati n following Zi hl\u2013Neelsen staining. I r s s t this challenge, there have been attempts to develop automated TB sputum tests using high-resolution microscopic images. However, t ese tec i es often prove inadequate in settings where slides can only be digitized using low-resolution scanners. In our study, we evaluated the performance of nine deep learning networks in classifying low-resolution slide images at 400\u00d7 magnification. The NASNet-A Large model achieved remarkable accuracy of 99.777%, demonstrating its potential for automated testing. Furthermore, Densen169 showed 100% accuracy on a slide-by-slide basis, and NASNET enabled pathologists to make accurate judgements swiftly, as they only needed to inspect a few dozen positive patches. This ability demonstrates potential support for preliminary testing by pathologists to reduce diagnostic time, which could in turn enhance the efficiency of TB diagnosis and contribute to preventing TB. Moving forward, we plan to improve the performance of our methodology by expanding the training dataset with additional samples from various hospitals and laboratories. This comprehensive dataset will facilitate the training of more generalized and robust deep networks, ultimately providing a more efficient tool for the rapid and precise diagnosis of TB. We are optimistic that our efforts will make significant strides toward faster and more accurate TB diagnosis.\nDiagnostics 2023, 13, 2230 12 of 14\nAuthor Contributions: Conceptualization, J.L. (Joong Lee) and J.L. (Junghye Lee); methodology, J.L. (Joong Lee); software, J.L. (Joong Lee); validation, J.L. (Joong Lee) and J.L. (Junghye Lee); formal analysis, J.L. (Joong Lee); investigation, J.L. (Joong Lee) and J.L. (Junghye Lee); resources, J.L. (Junghye Lee); data curation, J.L. (Joong Lee) and J.L. (Junghye Lee); writing\u2014original draft preparation, J.L. (Joong Lee); writing\u2014review and editing, J.L. (Joong Lee) and J.L. (Junghye Lee); visualization, J.L. (Joong Lee); supervision, J.L. (Joong Lee) and J.L. (Junghye Lee); project administration, J.L. (Joong Lee) and J.L. (Junghye Lee); funding acquisition, J.L. (Junghye Lee). All authors have read and agreed to the published version of the manuscript.\nFunding: This research was funded by National Forensic Service (NFS2023MED05), Ministry of the Interior and Safety, Republic of Korea.\nInstitutional Review Board Statement: The study was conducted in accordance with the Declaration of Helsinki and approved by the Institutional Review Board of National Forensic Service (906-230131- BR-011-01/ approved 31 January 2023).\nInformed Consent Statement: Patient consent was waived due to the inability to receive consent from the deceased.\nData Availability Statement: Data are available from the first author and the corresponding author upon request.\nConflicts of Interest: The authors declare no conflict of interest."
        }
    ],
    "title": "A Study of Mycobacterium tuberculosis Detection Using Different Neural Networks in Autopsy Specimens",
    "year": 2023
}