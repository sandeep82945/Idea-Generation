{
    "abstractText": "One of the promising ways for the representation learning is contrastive learning. It enforces that positive pairs become close while negative pairs become far. Contrastive learning utilizes the relative proximity or distance between positive and negative pairs. However, contrastive learning might fail to handle the easily distinguished positive-negative pairs because the gradient of easily divided positive-negative pairs comes to vanish. To overcome the problem, we propose a dynamic mixed margin (DMM) loss that generates the augmented hard positive-negative pairs that are not easily clarified. DMM generates hard positive-negative pairs by interpolating the dataset with Mixup. Besides, DMM adopts the dynamic margin incorporating the interpolation ratio, and dynamic adaptation improves representation learning. DMM encourages making close for positive pairs far away, whereas making a little far for strongly nearby positive pairs alleviates overfitting. Our proposed DMM is a plug-and-play module compatible with diverse contrastive learning loss and metric learning. We validate that the DMM is superior to other baselines on various tasks, video-text retrieval, and recommender system task in unimodal and multimodal settings. Besides, representation learned from DMM shows better robustness even if the modality missing occurs that frequently appears on the real-world dataset. Implementation of DMM at downstream tasks is available here: https://github.com/teang1995/DMM INDEX TERMS Multimodal learning, contrastive learning, retrieval, video representation, recommender system, robustness.",
    "authors": [
        {
            "affiliations": [],
            "name": "JUNHYUK SO"
        },
        {
            "affiliations": [],
            "name": "YONGTAEK LIM"
        },
        {
            "affiliations": [],
            "name": "YEWON KIM"
        },
        {
            "affiliations": [],
            "name": "KYUNGWOO SONG"
        }
    ],
    "id": "SP:75c9c824a99ddb9c824369ce945dff79df053622",
    "references": [
        {
            "authors": [
                "S. Ging",
                "M. Zolfaghari",
                "H. Pirsiavash",
                "T. Brox"
            ],
            "title": "COOT: Cooperative hierarchical transformer for video-text representation learning",
            "venue": "Proc. NeurIPS, vol. 33, 2020, pp. 22605\u201322618.",
            "year": 2020
        },
        {
            "authors": [
                "A. Radford",
                "J.W. Kim",
                "C. Hallacy",
                "A. Ramesh",
                "G. Goh",
                "S. Agarwal",
                "G. Sastry",
                "A. Askell",
                "P. Mishkin",
                "J. Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "Proc. Int. Conf. Mach. Learn., 2021, pp. 8748\u20138763.",
            "year": 2021
        },
        {
            "authors": [
                "A. Jaegle",
                "S. Borgeaud",
                "J.-B. Alayrac",
                "C. Doersch",
                "C. Ionescu",
                "D. Ding",
                "S. Koppula",
                "D. Zoran",
                "A. Brock",
                "E. Shelhamer",
                "O. H\u00e9naff",
                "M.M. Botvinick",
                "A. Zisserman",
                "O. Vinyals",
                "J. Carreira"
            ],
            "title": "Perceiver IO: A general architecture for structured inputs & outputs",
            "venue": "2021, arXiv:2107.14795.",
            "year": 2021
        },
        {
            "authors": [
                "S. Bae",
                "H. Byun",
                "C. Oh",
                "Y.-S. Cho",
                "K. Song"
            ],
            "title": "Graph perceiver IO: A general architecture for graph structured data",
            "venue": "2022, arXiv:2209.06418.",
            "year": 2022
        },
        {
            "authors": [
                "K. Simonyan",
                "A. Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "2014, arXiv:1409.1556.",
            "year": 2014
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2016, pp. 770\u2013778.",
            "year": 2016
        },
        {
            "authors": [
                "C. Szegedy",
                "S. Ioffe",
                "V. Vanhoucke",
                "A.A. Alemi"
            ],
            "title": "Inception-v4, Inception-Resnet and the impact of residual connections on learning",
            "venue": "Proc. 31st AAAI Conf. Artif. Intell., 2017, pp. 4278\u20134284.",
            "year": 2017
        },
        {
            "authors": [
                "K. Weiss",
                "T. Khoshgoftaar",
                "D. Wang"
            ],
            "title": "A survey of transfer learning",
            "venue": "J. Big Data, vol. 3, pp. 1\u201340, May 2016.",
            "year": 2016
        },
        {
            "authors": [
                "M. Hussain",
                "J.J. Bird",
                "D.R. Faria"
            ],
            "title": "A study on CNN transfer learning for image classification",
            "venue": "Proc. U.K. Workshop Comput. Intell. Cham, Switzerland: Springer, 2018, pp. 191\u2013202.",
            "year": 2018
        },
        {
            "authors": [
                "S. Tammina"
            ],
            "title": "Transfer learning using VGG-16 with deep convolutional neural network for classifying images",
            "venue": "Int. J. Sci. Res. Publications (IJSRP), vol. 9, no. 10, pp. 143\u2013150, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "D. Mahajan",
                "R. Girshick",
                "V. Ramanathan",
                "K. He",
                "M. Paluri",
                "Y. Li",
                "A. Bharambe",
                "L.V.D. Maaten"
            ],
            "title": "Exploring the limits of weakly supervised pretraining",
            "venue": "Proc. Eur. Conf. Comput. Vis. (ECCV), Sep. 2018, pp. 181\u2013196.",
            "year": 2018
        },
        {
            "authors": [
                "Y. Feng",
                "J. Jiang",
                "M. Tang",
                "R. Jin",
                "Y. Gao"
            ],
            "title": "Rethinking supervised pretraining for better downstream transferring",
            "venue": "2021, arXiv:2110.06014.",
            "year": 2021
        },
        {
            "authors": [
                "T. Wang",
                "P. Isola"
            ],
            "title": "Understanding contrastive representation learning through alignment and uniformity on the hypersphere",
            "venue": "Proc. Int. Conf. Mach. Learn., 2020, pp. 9929\u20139939.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Kalantidis",
                "M.B. Sariyildiz",
                "N. Pion",
                "P. Weinzaepfel",
                "D. Larlus"
            ],
            "title": "Hard negative mixing for contrastive learning",
            "venue": "Proc. Adv. Neural Inf. Process. Syst., vol. 33, H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, Eds. Red Hook, NY, USA: Curran Associates, 2020, pp. 21798\u201321809.",
            "year": 2020
        },
        {
            "authors": [
                "T. Chen",
                "S. Kornblith",
                "M. Norouzi",
                "G. Hinton"
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "inProc. Int. Conf.Mach. Learn., 2020, pp. 1597\u20131607.",
            "year": 2020
        },
        {
            "authors": [
                "J.-B. Grill"
            ],
            "title": "Bootstrap your own latent\u2014A new approach to selfsupervised learning",
            "venue": "inProc. Adv. Neural Inf. Process. Syst., vol. 33, 2020, pp. 21271\u201321284.",
            "year": 2020
        },
        {
            "authors": [
                "H. Zhang",
                "J.Y. Koh",
                "J. Baldridge",
                "H. Lee",
                "Y. Yang"
            ],
            "title": "Cross-modal contrastive learning for text-to-image generation",
            "venue": "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2021, pp. 833\u2013842.",
            "year": 2021
        },
        {
            "authors": [
                "Z. Wu",
                "S. Wang",
                "J. Gu",
                "M. Khabsa",
                "F. Sun",
                "H. Ma"
            ],
            "title": "CLEAR: Contrastive learning for sentence representation",
            "venue": "2020, arXiv:2012.15466.",
            "year": 2020
        },
        {
            "authors": [
                "A. Shen",
                "X. Han",
                "T. Cohn",
                "T. Baldwin",
                "L. Frermann"
            ],
            "title": "Contrastive learning for fair representations",
            "venue": "2021, arXiv:2109.10645.",
            "year": 2021
        },
        {
            "authors": [
                "C. Oh",
                "H. Won",
                "J. So",
                "T. Kim",
                "Y. Kim",
                "H. Choi",
                "K. Song"
            ],
            "title": "Learning fair representation via distributional contrastive disentanglement",
            "venue": "Proc. 28th ACM SIGKDD Conf. Knowl. Discovery Data Mining, Aug. 2022, pp. 1295\u20131305.",
            "year": 2022
        },
        {
            "authors": [
                "A. van den Oord",
                "Y. Li",
                "O. Vinyals"
            ],
            "title": "Representation learning with contrastive predictive coding",
            "venue": "2018, arXiv:1807.03748.",
            "year": 2018
        },
        {
            "authors": [
                "E. Hoffer",
                "N. Ailon"
            ],
            "title": "Deep metric learning using triplet network",
            "venue": "Proc. Int. Workshop Similarity-Based Pattern Recognit. Cham, Switzerland: Springer, 2015, pp. 84\u201392.",
            "year": 2015
        },
        {
            "authors": [
                "X. Dong",
                "J. Shen"
            ],
            "title": "Triplet loss in Siamese network for object tracking",
            "venue": "Proc. Eur. Conf. Comput. Vis. (ECCV), Sep. 2018, pp. 459\u2013474.",
            "year": 2018
        },
        {
            "authors": [
                "H. Zhang",
                "M. Cisse",
                "Y.N. Dauphin",
                "D. Lopez-Paz"
            ],
            "title": "Mixup: Beyond empirical risk minimization",
            "venue": "2017, arXiv:1710.09412.",
            "year": 2017
        },
        {
            "authors": [
                "V. Verma",
                "A. Lamb",
                "C. Beckham",
                "A. Najafi",
                "I. Mitliagkas",
                "D. Lopez-Paz",
                "Y. Bengio"
            ],
            "title": "Manifold mixup: Better representations by interpolating hidden states",
            "venue": "Proc. Int. Conf. Mach. Learn., 2019, pp. 6438\u20136447.",
            "year": 2019
        },
        {
            "authors": [
                "S. Chopra",
                "R. Hadsell",
                "Y. LeCun"
            ],
            "title": "Learning a similarity metric discriminatively, with application to face verification",
            "venue": "Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2005, pp. 539\u2013546.",
            "year": 2005
        },
        {
            "authors": [
                "F. Schroff",
                "D. Kalenichenko",
                "J. Philbin"
            ],
            "title": "FaceNet: A unified embedding for face recognition and clustering",
            "venue": "Proc. IEEEConf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2015, pp. 815\u2013823.",
            "year": 2015
        },
        {
            "authors": [
                "K. Sohn"
            ],
            "title": "Improved deep metric learning with multi-class n-Pair loss objective",
            "venue": "Proc. Adv. Neural Inf. Process. Syst., vol. 29, D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, R. Garnett, Eds. RedHook, NY,USA: Curran Associates, 2016, pp. 1\u20139.",
            "year": 2016
        },
        {
            "authors": [
                "N. Frosst",
                "N. Papernot",
                "G. Hinton"
            ],
            "title": "Analyzing and improving representations with the soft nearest neighbor loss",
            "venue": "Proc. Int. Conf. Mach. Learn., 2019, pp. 2012\u20132020.",
            "year": 2019
        },
        {
            "authors": [
                "H. Hotelling"
            ],
            "title": "Relations between two sets of variates",
            "venue": "Biometrika, vol. 28, nos. 3\u20134, pp. 321\u2013377, Dec. 1936.",
            "year": 1936
        },
        {
            "authors": [
                "M. Hodosh",
                "P. Young",
                "J. Hockenmaier"
            ],
            "title": "Framing image description as a ranking task: Data, models and evaluation metrics",
            "venue": "J. Artif. Intell. Res., vol. 47, pp. 853\u2013899, Aug. 2013.",
            "year": 2013
        },
        {
            "authors": [
                "R. Kiros",
                "R. Salakhutdinov",
                "R.S. Zemel"
            ],
            "title": "Unifying visualsemantic embeddings with multimodal neural language models",
            "venue": "2014, arXiv:1411.2539.",
            "year": 2014
        },
        {
            "authors": [
                "I. Vendrov",
                "R. Kiros",
                "S. Fidler",
                "R. Urtasun"
            ],
            "title": "Order-embeddings of images and language",
            "venue": "2015, arXiv:1511.06361. VOLUME 11, 2023 60221 J. So et al.: Robust Contrastive Learning With Dynamic Mixed Margin",
            "year": 2015
        },
        {
            "authors": [
                "F. Faghri",
                "D.J. Fleet",
                "J.R. Kiros",
                "S. Fidler"
            ],
            "title": "VSE++: Improving visualsemantic embeddings with hard negatives",
            "venue": "2017, arXiv:1707.05612.",
            "year": 2017
        },
        {
            "authors": [
                "L. Wang",
                "Y. Li",
                "J. Huang",
                "S. Lazebnik"
            ],
            "title": "Learning two-branch neural networks for image-text matching tasks",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 41, no. 2, pp. 394\u2013407, Feb. 2019.",
            "year": 2019
        },
        {
            "authors": [
                "X. Li"
            ],
            "title": "Oscar: Object-semantics aligned pre-training for vision-language tasks",
            "venue": "Proc. Eur. Conf. Comput. Vis. Cham, Switzerland: Springer, 2020, pp. 121\u2013137.",
            "year": 2020
        },
        {
            "authors": [
                "W. Kim",
                "B. Son",
                "I. Kim"
            ],
            "title": "ViLT: Vision-and-language transformer without convolution or region supervision",
            "venue": "Int. Conf. Mach. Learn., 2021, pp. 5583\u20135594.",
            "year": 2021
        },
        {
            "authors": [
                "J. Li",
                "R. Selvaraju",
                "A. Gotmare",
                "S. Joty",
                "C. Xiong",
                "S.C.H. Hoi"
            ],
            "title": "Align before fuse: Vision and language representation learning with momentum distillation",
            "venue": "Proc. Adv. Neural Inf. Process. Syst. (NeurIPS), vol. 34, 2021, pp. 9694\u20139705.",
            "year": 2021
        },
        {
            "authors": [
                "A. Singh",
                "R. Hu",
                "V. Goswami",
                "G. Couairon",
                "W. Galuba",
                "M. Rohrbach",
                "D. Kiela"
            ],
            "title": "FLAVA: A foundational language and vision alignment model",
            "venue": "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2022, pp. 15617\u201315629.",
            "year": 2022
        },
        {
            "authors": [
                "F. Chen",
                "X. Chen",
                "J. Shi",
                "D. Zhang",
                "J. Chang",
                "Q. Tian"
            ],
            "title": "HiVLP: Hierarchical vision-language pre-training for fast image-text retrieval",
            "venue": "2022, arXiv:2205.12105.",
            "year": 2022
        },
        {
            "authors": [
                "S. Venugopalan",
                "H. Xu",
                "J. Donahue",
                "M. Rohrbach",
                "R. Mooney",
                "K. Saenko"
            ],
            "title": "Translating videos to natural language using deep recurrent neural networks",
            "venue": "2014, arXiv:1412.4729.",
            "year": 2014
        },
        {
            "authors": [
                "R. Krishna",
                "K. Hata",
                "F. Ren",
                "L. Fei-Fei",
                "J.C. Niebles"
            ],
            "title": "Densecaptioning events in videos",
            "venue": "inProc. IEEE Int. Conf. Comput. Vis. (ICCV), Oct. 2017, pp. 706\u2013715.",
            "year": 2017
        },
        {
            "authors": [
                "B. Zhang",
                "H. Hu",
                "F. Sha"
            ],
            "title": "Cross-modal and hierarchical modeling of video and text",
            "venue": "Proc. Eur. Conf. Comput. Vis. (ECCV), Sep. 2018, pp. 374\u2013390.",
            "year": 2018
        },
        {
            "authors": [
                "H. Luo",
                "L. Ji",
                "M. Zhong",
                "Y. Chen",
                "W. Lei",
                "N. Duan",
                "T. Li"
            ],
            "title": "CLIP4Clip: An empirical study of CLIP for end to end video clip retrieval",
            "venue": "2021, arXiv:2104.08860.",
            "year": 2021
        },
        {
            "authors": [
                "H. Fang",
                "P. Xiong",
                "L. Xu",
                "Y. Chen"
            ],
            "title": "CLIP2 Video: Mastering videotext retrieval via image CLIP",
            "venue": "2021, arXiv:2106.11097.",
            "year": 2021
        },
        {
            "authors": [
                "P. Poklukar",
                "M. Vasco",
                "H. Yin",
                "F.S. Melo",
                "A. Paiva",
                "D. Kragic"
            ],
            "title": "Geometric multimodal contrastive representation learning",
            "venue": "2022, arXiv:2202.03390.",
            "year": 2022
        },
        {
            "authors": [
                "S. Rendle",
                "C. Freudenthaler",
                "Z. Gantner",
                "L. Schmidt-Thieme"
            ],
            "title": "BPR: Bayesian personalized ranking from implicit feedback",
            "venue": "2012, arXiv:1205.2618.",
            "year": 2012
        },
        {
            "authors": [
                "C.-K. Hsieh",
                "L. Yang",
                "Y. Cui",
                "T.-Y. Lin",
                "S. Belongie",
                "D. Estrin"
            ],
            "title": "Collaborative metric learning",
            "venue": "Proc. 26th Int. Conf. World Wide Web, Apr. 2017, pp. 193\u2013201.",
            "year": 2017
        },
        {
            "authors": [
                "S. Zhang",
                "Y. Tay",
                "L. Yao",
                "A. Sun"
            ],
            "title": "Next item recommendation with self-attention",
            "venue": "2018, arXiv:1808.06414.",
            "year": 2018
        },
        {
            "authors": [
                "R. Ying",
                "R. He",
                "K. Chen",
                "P. Eksombatchai",
                "W.L. Hamilton",
                "J. Leskovec"
            ],
            "title": "Graph convolutional neural networks for web-scale recommender systems",
            "venue": "Proc. 24th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, Jul. 2018, pp. 974\u2013983.",
            "year": 2018
        },
        {
            "authors": [
                "Z. Gao",
                "Z. Cheng",
                "F. Perez",
                "J. Sun",
                "M. Volkovs"
            ],
            "title": "MCL: Mixed-centric loss for collaborative filtering",
            "venue": "Proc. ACM Web Conf., Apr. 2022, pp. 2339\u20132347.",
            "year": 2022
        },
        {
            "authors": [
                "Q. Wang",
                "H. Yin",
                "H. Wang",
                "Q.V.H. Nguyen",
                "Z. Huang",
                "L. Cui"
            ],
            "title": "Enhancing collaborative filteringwith generative augmentation",
            "venue": "inProc. 25th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, Jul. 2019, pp. 548\u2013556.",
            "year": 2019
        },
        {
            "authors": [
                "M.L. Ha",
                "V. Blanz"
            ],
            "title": "Deep ranking with adaptive margin triplet loss",
            "venue": "2021, arXiv:2107.06187.",
            "year": 2021
        },
        {
            "authors": [
                "J. So",
                "C. Oh",
                "Y. Lim",
                "H. Byun",
                "M. Shin",
                "K. Song"
            ],
            "title": "Geodesic multimodal mixup for robust fine-tuning",
            "venue": "2022, arXiv:2203.03897.",
            "year": 2022
        },
        {
            "authors": [
                "P. Zhu",
                "R. Abdal",
                "J. Femiani",
                "P. Wonka"
            ],
            "title": "Mind the gap: Domain gap control for single shot domain adaptation for generative adversarial networks",
            "venue": "2021, arXiv:2110.08398.",
            "year": 2021
        },
        {
            "authors": [
                "T. Chen",
                "C. Luo",
                "L. Li"
            ],
            "title": "Intriguing properties of contrastive losses",
            "venue": "Adv. Neural Inf. Process. Syst., vol. 34, pp. 11834\u201311845, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A. Miech",
                "D. Zhukov",
                "J. Alayrac",
                "M. Tapaswi",
                "I. Laptev",
                "J. Sivic"
            ],
            "title": "HowTo100M: Learning a text-video embedding by watching hundred million narrated video clips",
            "venue": "Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2019, pp. 2630\u20132640.",
            "year": 2019
        },
        {
            "authors": [
                "K. Hara",
                "H. Kataoka",
                "Y. Satoh"
            ],
            "title": "Can spatiotemporal 3D CNNs retrace the history of 2D CNNs and ImageNet?\u2019",
            "venue": "in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.,",
            "year": 2018
        },
        {
            "authors": [
                "J.Wang",
                "W. Jiang",
                "L.Ma",
                "W. Liu",
                "andY.Xu"
            ],
            "title": "Bidirectional attentive fusion with context gating for dense video captioning",
            "venue": "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2018, pp. 7190\u20137198.",
            "year": 2018
        },
        {
            "authors": [
                "L. Zhou",
                "C. Xu",
                "J.J. Corso"
            ],
            "title": "Towards automatic learning of procedures fromweb instructional videos",
            "venue": "inProc. 3rd AAAI Conf. Artif. Intell., 2018, pp. 7590\u20137598.",
            "year": 2018
        },
        {
            "authors": [
                "Y.-H.-H. Tsai",
                "S. Bai",
                "P.P. Liang",
                "J.Z. Kolter",
                "L.-P. Morency",
                "R. Salakhutdinov"
            ],
            "title": "Multimodal transformer for unaligned multimodal language sequences",
            "venue": "Proc. 57th Annu. Meeting Assoc. Comput. Linguistics, 2019, p. 6558.",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "INDEX TERMS Multimodal learning, contrastive learning, retrieval, video representation, recommender system, robustness.\nI. INTRODUCTION The machine learning algorithm demonstrates notable advancements in performance across various tasks, datasets, and experimental environments. Many works show remarkable results on various tasks such as retrieval, classification, and recommender system. There are recent works that handle diverse datasets simultaneously, multimodal learning [1], [2], [3], [4]. Besides, machine learning in real-world environments, such as a limited number of datasets or modalities missing, becomes essential, and there are many efforts to handle problematic experimental environment settings.\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Li He .\nThe significant performance improvements stem from representation learning. Representation learning that captures the important low-dimensional hidden features plays a key role in interpreting the model mechanism and improves the diverse downstream task performance. Traditional representation learning relies on learning the relationship between the dataset instance and its corresponding labels with neural networks. There are diverse research works to enhance representation learning and improve performance [5], [6], [7]. Besides, there have been many efforts to transfer the learned representation to diverse downstream tasks [8], [9], [10].\nNonetheless, the traditional representation learning relying on the relationship between the data instance and labels is not enough. Recently, there have been many works that\nVOLUME 11, 2023 This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/ 60211\ntraditional representation learning has weak transferability, and the performances on downstream tasks are limited [11], [12]. Another promising way for representation learning is to capture the relationship between the dataset instances. A pairwise loss that learns the dataset instance relationship, such as contrastive learning and triplet loss, utilizes the relationship between the dataset instances instead of the relationship between the dataset instance and its corresponding labels. Positive pairs are encouraged to move closer, while negative pairs are encouraged to go farther by contrastive learning. Besides, contrastive learning or triplet loss adopts the margin to adjust the distance between the positive pairs and negative pairs. There are recent researches that models learned by the contrastive learning loss have a better representation [13], [14], and it exhibits enhanced performance on downstream tasks such as vision [15], [16], text [17], [18], and fairness [19], [20].\nHowever, pairwise loss, such as contrastive learning and triplet loss, might fail to learn robust representation learning when the number of the dataset is limited and the positive and negative pairs are easily distinguished. First, contrastive learning loss relies on the relationship between the dataset instances. In other words, it is hard to capture the representation when the number of data instances that can be considered as a relationship is limited. Second, contrastive learning calculates the gradient with respect to the loss that depends on the difference between the positive and negative pairs. In other words, the gradients come to vanish when the positive and negative pairs are easily distinguished.\nIn this paper, we provide the novel contrastive learning loss, dynamic mixed margin (DMM). DMM compares the positive and negative pairs to learn the representation similar to the previous contrastive learning [15], [21] and triplet loss [22], [23]. However, DMM generates augmented hard positive-negative pairs that are not easily distinguished. To generate the hard positive-negative pairs, DMM interpolates the two datasets on the embedding space with mixup [24], [25].With the augmented dataset, DMMdynamically adopts the margin based on the original base margin and interpolation ratio to handle the hardness of the augmented dataset.\nDMM generates the augmented dataset with a dynamically adapted margin is simple yet effective. First, DMM contributes to getting closer for the far-away positive pairs while getting slightly far for the strongly closed positive pairs. In other words, DMM alleviates underfitting and overfitting by dynamically adapting the margin with an augmented dataset. Second, DMM generates the natural hard positive-negative pairs and learns the generated dataset as well as the given dataset. Therefore, DMM shows significantly improved performance when the training dataset is relatively limited. Third, DMM is compatible with the diverse loss and backbone network. We validate DMM on the diverse backbone (Transformer, matrix\nfactorization, graph convolutional network) and tasks (videotext retrieval, recommender system, sentiment classification) in unimodal and multimodal settings. Besides, we validate that DMM still captures the important features of a given dataset even if the missing modality occurs in multimodal learning.\nFollowing is a summary of this paper\u2019s contributions.\n\u2022 We analyze that the current contrastive learning loss might fail to learn robust representation learning when the positive-negative pairs are easily distinguished. \u2022 We propose a new type of loss, DMM, that generates the hard positive-negative pairs with a dynamically adapted margin. It is noted that DMM is compatible with diverse contrastive loss and metric learning loss. \u2022 We validate that DMM contributes to learning robust representation learning, and it improves the diverse downstream tasks on unimodal and multimodal datasets.\nII. RELATED WORKS A. CONTRASTIVE LEARNING Contrastive learning has been broadly adopted as a methodology for unsupervised learning in multiple domains. The main purpose of contrastive learning is to learn a better representation, where semantically close samples are placed close that can be transferred well to other tasks.\nTraditionally, contrastive learning constructs a positive pair, a semantically similar pair, and a negative pair. Besides, contrastive learning pulls the positive pairs and pushes the negative pair by minimizing the positive pair\u2019s distance while maximizing the negative pair\u2019s distance. There are various forms of contrastive learning, depending on what kind of pair is used or how to consider the relationship between them.\nPairwise Ranking Loss [26] is one of the first proposed metric learning methods. Pairwise Ranking Loss uses only one positive or negative pair for each iteration. Triplet loss is originally suggested to learn face representation for face recognition tasks in FaceNet [27]. Triplet loss simultaneously considers positive and negative pairs for each iteration. Multi-Class N pair loss [28] generalized Triplet loss for multiple negative pairs. It induces positive pairs to exist closer than N negative pairs. NT-Xent loss (or InfoNCE) [15], [21] generalizes multi-class N pair loss and suggests using a temperature-scaled logit. Soft-Nearest Neighbors [29] Loss extends it to include multiple positive samples. There have been many variants for contrastive learning, but we find that the current contrastive learning loss can be improved with hard positive-negative pairs and proper margin. In this work, we propose a updated contrastive learning loss, dynamic mixed margin (DMM), to encourage robust representation learning that generates the augmented hard positive-negative pairs and dynamically adapted margin.\n60212 VOLUME 11, 2023\nB. IMAGE-TEXT RETRIEVAL As technology explosively progresses, huge amounts of multimodal data including images and text, have overflowed on the Web. With that rich available data source, developing a robust image-text retrieval system has become an urgent problem. In some early works on image-text retrieval, Canonical Correlation Analysis (CCA) [30] was used to get joint embedding space of vision and language [31]. Then, the representation learning methods that adopt pairwise ranking loss [32], [33], [34], [35] gained popularity with their empirical success.\nRecently, large-scale vision-language pre-trained models [36], [37], [38], [39], [40] show remarkable performance on diverse vision-language tasks, like visual-based question answering, language-based visual reasoning, image captioning, and image-text retrieval. By leveraging plentiful image-text paired examples, they induce powerful embedding space modeling the interaction of diverse levels between different modalities so that the image-text retrieval is easily performed. However, they can not be directly applied to video-text learning, another crucial research topic, because they lack the spatial-temporal notion, which is important for video context modeling. We describe some works on them in the following paragraph.\nC. VIDEO-TEXT RETRIEVAL Some works utilize the recurrent network to learn visual and textual features to incorporate the temporal information from the video. LSTM of Youtube (LSTM-YT) [41] generates captions using a sequence-to-sequence model. DENSE [42] proposed captioning module to utilize the rich context from all the events and generate text describing the video.\nSeveral works have conducted video-text retrieval as an alignment task, and there have been some works that adopt the triplet loss to align video and text embeddings. Crossmodal hierarchical sequence embedding (CMHSE) [43] presented a broad framework for embedding text and video data into vector spaces with hierarchical structure, considering both implicit and explicit connection information. It uses triplet loss to align video and text embeddings. Cooperative hierarchical Transformer (COOT) is the extension work of CMHSE by introducing the cross-modal cycle consistency loss and transformer architecture. Recently, there have been several works that adopt the pre-trained model, contrastive language-image pre-training (CLIP) [2], for video-text learning such as CLIP4Clip [44] and CLIP2Video [45]. CLIP4CLIP and CLIP2Video utilize the pre-trained CLIP to solve frame-level input video retrieval tasks or video understanding. In this paper, we propose a modified contrastive learning method, dynamic mixed margin (DMM), that is compatible with diverse previous works. To validate our method, we apply our methods on COOT [1] and other\nvariants [46] for video-text retrieval and video classification task.\nD. RECOMMENDER SYSTEM Traditionally, CF adopts an inner product to capture the user\u2019s preference of item. Vanilla CF trains the model with the output, inner product, and ground-truth label by utilizing the cross-entropy loss. Similarly, Bayesian personalized ranking (BPR) Loss [47] adopts the dot product to calculate the similarity, and they compare the similarities between the positive and negative pairs. BPR maximizes the difference between them. However, some studies raise the question of whether CF models capture relationships between users and items [48], [49]. Diverse metric learning methods such as collaborative metric learning (CML) emerged to encourage relationship learning between users and items. [48]. CML suggests the concept of an impostor, which indicates the item that the user does not prefer but exists user nearby. CML method pushes the impostor far away from the user.\nHard negative samples play an important role in the convergence of CF models [50]. However, mining hard negative samples is time-consuming and cumbersome. Recent studies propose hard negative and positive mining methods [51], and there is related work for augmenting datasets [52].\nIn this paper, we provide a new version of contrastive learning approach, DMM, which is a simple yet effective method. Different from the previous works, we generate hard positive-negative pairs, and it encourages robust representation learning with a dynamically adapted margin. Our method is compatible with diverse objective functions from previous works such as BPR or CML.\nIII. PRELIMINARY A. TRIPLET LOSS Triplet loss [27] is one of the contrastive learning methods, forcing the positive sample to get close and the negative samples far away. Let (xanc, ypos) be positive pair P and (xanc, xneg) be negative pair N . \u03b1 is margin that ensures the minimum distance between the positive pair and the negative pair. D is a distance function. We can define triplet loss as follows:\nL(P,N , \u03b1) = max(0, \u03b1 + D(xanc, ypos) \u2212 D(xanc, yneg)) (1)\nHowever, the current triplet loss has a few elements that can be improved. First, the decision boundary of the positive and negative samples is strict. The model could be overconfident with the strict decision boundary and might encounter overfitting problems. Second, the fixed margin could negatively influence learning features. Distance between anchor and samples differentiates with prediction difficulty. Easy samples are relatively closewith anchor; conversely, hard samples are far apart. The hard sample is difficult to be predicted, whether positive or negative. When the fixed margin of the\nVOLUME 11, 2023 60213\nhard sample is too small, it might cause a degradation of performance. Therefore, the proper value of the margin is needed for each sample. Third, triplet loss needs a sufficient training sample as well as hard positive-negative pairs. Triplet loss learns the relationship between the data. Therefore, a number of enough data are necessary. Besides, when positive and negative pairs are too easily distinguished, the training loss will converge to zero [53], and it might induce the underfitting problem. In light of the aforementioned problems, we propose DMM that adopts the augmented hard positive-negative pairs and dynamic margin loss.\nB. MIXUP Mixup is a data augmentation method by interpolating in the input space, or embedding space [24], [25]. Mixup constructs virtual training samples by respectively mixing data, or embedding, x, and label, y, with ratio \u03bb sampled from Beta distribution, respectively. Equation 2 denotes the formulation of Mixup. It is noted that (xi, yi) and (xj, yj) are random samples from the training data.\nxmixed = \u03bbxi + (1 \u2212 \u03bb)xj ymixed = \u03bbyi + (1 \u2212 \u03bb)yj (2)\nThrough Mixup, DMM generates a hard-negative sample between a positive sample and a negative sample.\nIV. METHODOLOGY In this section, we introduce our dynamic mixed margin method for robust representation learning. DMM has two components to improve contrastive learning. First, DMM generates the hard positive-negative pairs to encourage robust learning. While the dataset is limited or positive, and negative pairs are easily distinguished. Second, DMM adopts the dynamically adapted margin to effectively incorporate the newly augmented dataset. This paper focuses on video understanding, such as retrieval and classification, and recommender systems. All tasks include the two modalities; video understanding includes the video and text, and the recommender system has user and item. Therefore, we formulate the DMM by focusing on the environmental settings with two modalities. In the following section, we will elaborate our DMM and formulate all objective functions for video understanding and recommender systems, respectively.\nA. DMM We assume that there are two modalities, x and y (i.e., video and text). For the training phase, we have the positive pair (xi, yi) and the negative pair (xi, yj). To generate the augmented hard positive-negative pairs, we sample the interpolation ratio \u03bb from the uniform distribution. With the randomly sampled \u03bb, we mix the two first modality instances, xi and xi\u2032 \u0338=i. After that, we adjust the margin as \u03bb\u03b1 instead of \u03b1 to incorporate the augmented dataset effectively. Equation 3 denotes the DMM formulation, and D(x, y) means\nthe cosine distance between two instances. P indicates the positive pairs set {(xi, yi)} and N indicates the negative pairs set {(xi, yi\u2032 ), (xi\u2032 , yi)}.\nL(P,N , \u03b1, \u03bb) = max(0, \u03bb\u03b1 + D(\u03bbxi + (1 \u2212 \u03bb)xj\u0338=i, yi)\n\u2212 D(\u03bbxi + (1 \u2212 \u03bb)xj\u0338=i, yi\u2032 \u0338=i)) (3)\nEquation 1 and equation 3 denotes formulation of Triplet Loss and DMM, respectively. When \u03bb becomes 1, Equation 3 and Equation 1 become identical. Therefore, we can assert that DMM generalizes the Triplet Loss. Figure 1 represents the overall working mechanism of DMM on two modalities, video and text. When the distance between positive and negative is too close, it implies the hardness of prediction, whether it is positive or negative. If the margin is too small, the hard sample is not sufficient, and it is hard to train the model effectively. Conversely, when the margin is too big, too many samples may be produced, and convergence of model could be very slow [53]. Also, the fixed margin may have difficulty with distinguishing test samples of similar classes [22]. In fact, using the fixed margin may harm the performance. Therefore the dynamic margin could adapt with different distance, and solve these problems. Dynamicmargin can push boundaries between neighbor classes, they become more separately as mentioned in [56]. Figure 1a is the case when the margin is too small in triplet loss. As the distance between positive and negative pairs reduced, only a few samples could participate in training. However, as shown in Figure 1b, DMM alleviates the problem by generating an augmented positive and negative pair, and it reduces the distance between positive and negative pair. Additionally, smaller margin let negative sample become sufficient and close enough to participate in training. Another case is when margin is too large like Figure 1c. Positive video text pairs are too close, but negative pairs are too far from each other. It may cause overconfident model. By mixing video samples xi and xj with DMM, mixed sample xmixed works as positive pair with yi. Therefore yi move closer to xmixed . In fact distance between xi and yi get closer, and also we can get small margin.\nB. VIDEO-TEXT RETRIEVAL We propose a DMM-based hierarchical loss inspired by the COOT [1] for video-text retrieval. For video and text alignment, we use contrastive loss based on the DMM. We let positive samples be close to each other and far apart from negative samples. Similar to [1], we align clipsentence (vki , s k i ), video-paragraph (v\nk , sk ) and global context (gv, gs) level representations. For vki and s k i , the subscript k indicates the video and paragraph, and i denotes the clip and sentence, respectively. Equation 4 denotes the low-level (clip-sentence), high-level (video-paragraph) alignment, and global context level alignment, that is aggregated from of frame features (vkg) and word features (s k g) of k-th video. It is noted that (\u00b7k , \u00b7k ), (\u00b7ki , \u00b7 k i ) and (\u00b7 k , \u00b7k \u2032 ), (\u00b7ki , \u00b7 k \u2032 i ) denote the\n60214 VOLUME 11, 2023\npositive pair and negative pair, respectively. Llow = \u2211\nk,i,k \u2032 \u0338=k,i\u2032 \u0338=i\nL({(vki , s k i )}, {(v k i , s k \u2032 i\u2032 ), (v k \u2032 i\u2032 , s k i )}, \u03b1, \u03bb)\nLhigh = \u2211\nk\u2208,k \u2032 \u0338=k\nL({(vk , sk )}, {(vk , sk \u2032 ), (vk \u2032 , sk )}, \u03b1, \u03bb)\nLglobal = \u2211 k,k \u2032 \u0338=k L({(vkg, s k g)}, {(v k g, s k \u2032 g ), (v k \u2032 g , s k g)}, \u03b1, \u03bb = 1.0)\n(4)\nBesides, it has been known that semantic clustering loss improves the video-retrieval tasks [1], [43]. Therefore, we adopt the additional clustering loss to push negative samples farther apart in both low-level and high-level representations by following [43]. Equation 5 represents the clustering loss for the video-retrieval task. The ordered pair (1,1) indicates the fixed positive pairs.\nLcluster = \u2211\nk,i,k \u2032 \u0338=k,i\u2032 \u0338=i\nL({(1,1)},{(vki , v k \u2032 i\u2032 ), (s k \u2032 i\u2032 , s k i )}, \u03b1, \u03bb=1.0)\n+ \u2211 k,k \u2032 \u0338=k L({(1, 1)}, {(vk , vk \u2032 ), (sk \u2032 , sk )}, \u03b1, \u03bb=1.0)\n(5)\nFurthermore, we utilize the cross-modal cycle consistency (CMC) loss for coherence between video clips and sentences at embedding space, [1] as shown in Equation 6.\nThe sentence embedding {si} is semantically cycle consistent, so it cycles back to the starting point i. Therefore we can express i = \u00b5 and loss term.\nv\u0304si = \u2211 j=1 w(1)j vj where w (1) j = exp(\u2212||si \u2212 vj||2)\u2211 k=1 exp(\u2212||si \u2212 vk ||2)\n\u00b5 = \u2211 j=1 w(2)j j where w (2) j = exp(\u2212||v\u0304\u2212 sj||2)\u2211 k=1 exp(\u2212||v\u0304\u2212 sk ||2)\nLcmc = ||i\u2212 \u00b5||2 (6)\nIn summary, our total training objective for the video-text retrieval is shown in Equation 7. The overall loss term is composed by aggregating the terms from Equation 4, Equation 5 and Equation 6 to consider the diverse hierarchical level of features. It is noted that the training loss is for the augmented positive-negative pairs as well as the original positive-negative pairs.\nLCOOT+DMM = Llow + Lhigh + Lglobal + Lcluster + Lcmc (7)\nC. RECOMMENDER SYSTEM Our method, DMM, is a plug-and-play algorithm compatible with diverse models. To validate our method, we apply our DMM loss to the recommender system that consists of user and product embedding. In this paragraph, we provide the DMM objective for the recommender system. We adopt two baselines, Bayesian personalized ranking (BPR) [47] and collaborative metric learning (CML) [48]. First, BPR aims to maximize the distinction in similarity between positive pairs and negative pairs. Secondly, CML pulls positive item and push negative item from the user\u2019s perspective. It maximizes the difference between the squared Euclidian distance, D, of positive pairs and negative pairs. Equation 1 shows triplet loss for different modalities. Since we newly interpret CML as the Triplet Loss applied in recommendation systems, we can express formulation of CML following Equation 1. we can summarize the CMLobjective for the user and product embedding u and p, respectively as follow. The notation (\u00b7k , \u00b7k ) represents a positive pair, while (\u00b7k , \u00b7k \u2032 ) denotes a\nVOLUME 11, 2023 60215\nnegative pair. LCML = \u2211\nk,k \u2032 \u0338=k,\nL({(pk , uk )}, {(pk , uk \u2032 ), (pk \u2032 , uk )}, \u03b1) (8)\nThe CML has similar problems with traditional contrastive learning loss. It does not handle the hard positive-negative pairs and adopts the constant margin \u03b1. Identical to the videorelated tasks, we apply our method on CML to improve the recommender system performance. We propose a modified contrastive learning method for the recommender system, combining of our DMM and CML. In this way, users\u2019 preferences can be identified, and the model may not be easily over-fitted. Since we interpret CML as a type of Triplet Loss, we can apply DMM to CML directly. Different from Equation 7, DMM with CML has a single term, as shown in Equation 9.\nLCML+DMM = \u2211\nk,k \u2032 \u0338=k,\nL({(pk , uk )}, {(pk , uk \u2032 ), (pk \u2032 , uk )}, \u03b1, \u03bb)\n(9)\nV. THEORETICAL ANALYSIS In this section, we theoretically analyzed the advantages of DMM loss LDMM compared to the existing traditional triplet loss LTriplet . We show that our methods LDMM encourage to utilize the diverse negative samples as a training dataset, while the LTriplet ignores diverse negative samples. Therefore, LDMM enhances learning positive sample embedding by utilizing the negative samples. In other words, there exists that the LDMM gradient with respect to the positive samples is not zero, while LTriplet loss gradients become zeros. Despite the LTriplet reaching zero, our application of DMM demonstrates that our LDMM does not achieve the same result. Here, we provide a mathematical proof for the scenario:\nTheorem 1: Let\u2019s assume that (yi\u2032 \u0338=i \u2212 yi)(yi\u2032 \u0338=i + yi \u2212 2 xj\u0338=i) < 0, \u03bb \u2208 (0, 1), and xi = \u2212(\u03b1 \u2212 y2i\u2032 \u0338=i + y 2 i )/(2yi\u2032 \u0338=i \u2212 2 yi), where xi, xj, yi\u2032 , yi \u2208 R. Then, (i) LTriplet = 0,\n\u2202LTriplet \u2202xi = 0\n(ii) LDMM > 0, \u2202LDMM\n\u2202xi \u0338= 0\nProof: First, xi = \u2212(\u03b1\u2212y2i\u2032 \u0338=i+y 2 i )/(2yi\u2032 \u0338=i\u22122 yi) implies\nLTriplet = 0, and \u2202LTriplet/\u2202xi = \u2202(0)/\u2202xi = 0.\nSecond, we can rewrite LDMM as (\u03bb\u22121)(yi\u2032 \u0338=i\u2212yi)(yi\u2032 \u0338=i+yi\u2212 2 xj\u0338=i) > 0, because of xi condition. It is noted that yi\u2032 \u0338=i \u0338= yi, because yi\u2032 \u0338=i and yi are negative and positive pair of xi, respectively. Besides, \u03bb \u2212 1 < 0 always hold, because \u03bb \u2208 (0, 1). Therefore, LDMM > 0 and \u2202LDMM/\u2202xi = 2\u03bb(yi\u2032 \u0338=i \u2212 yi) \u0338= 0, while LTriplet = 0 and \u2202LTriplet/\u2202xi = \u2202(0)/\u2202xi = 0.\n\u25a1\nVI. RESULTS A. SYNTHETIC DATASET EXPERIMENTS To verify whether DMM functions well as expected, we conduct experiments on synthetic datasets. We use Figure 1a and Figure 1c to create the synthetic dataset. First, for Figure 1a, we fix xi, yi and yj, while we vary xj and \u03bb to investigate the gradient with respect to the yj. Figure 2 shows results with \u03bb in {1.0, 0.6, 0.3, 0.1}. When \u03bb = 1.0, as mentioned in Section IV-A, our model DMMbecomes equivalent to Triplet Loss. In this case, the magnitude of gradients is zero because the distance between xi and yi is sufficiently smaller than that of xi and xj. In other words, the negative samples are not utilized for training, and it might be harmful to increase the model performance. However, when we utilize DMM with reduced \u03bb, as shown in Figure 2b \u223c Figure 2d, the negative data is started to be utilized for training. In other words, the magnitude of the gradient is not zero for a larger number of data points. Besides, Figure 2e shows gradient direction with respect to yi as \u03bb varying in {1.0, 0.6, 0.3, 0.1}. Without DMM, \u03bb = 1.0, the traditional Triplet loss enforces minimizing the distance between positive pairs, xi, yi. However, the positive pairs are already too close, and additional closeness might be related to the overfitting problems. However, our methods DMM, \u03bb \u0338= 1.0, guarantees that the distance between positive pairs keeps appropriate, and it prevents overfitting problems.\nB. VIDEO-PARAGRAPH RETRIEVAL In this subsection, we provide the quantitative and qualitative results for the video to paragraph and paragraph to video retrieval tasks on diverse datasets, including YouCook2 and ActivityNet."
        },
        {
            "heading": "1) EXPERIMENT SETUP",
            "text": "We use the same experimental setting with COOT [1]. We set margin \u03b1 = 0.2 and set the batch size as 64 for both video\n60216 VOLUME 11, 2023\nand paragraph pairs. We sample the value of \u03bb from uniform distribution and constrain to be over 0.5.\nWe utilize the instances that belong to the same batches as a negative dataset to construct contrastive loss. Following the COOT [1], we sample one clip from video and one sentence from paragraph. We use the same video and text encoder with COOT for a fair comparison. For the text encoder, we use BERT-base pre-trained model. As video encoder, for ActivityNet, we use 2048 dimensions feature from [43], and for YouCook2, we use concatenation of pre-trained Resnet-152 [6] and ResNext-101 [58]. Besides, we also implement with provided video embedding network that is pre-trained with the HowTo100M dataset [57]. We use the Adam optimizer with learning rate 1e-3 and weight decay 2e-5. We repeated the experiments five times with different random seeds. All experiments are conducted on RTX A6000 GPU. Appendix. CODE IMPLEMENTATION provides the implementation code for our experiments."
        },
        {
            "heading": "2) DATASETS AND EVALUATION METRICS",
            "text": "To validate the video retrieval capability, we evaluate our methods on ActivityNet-caption [59] and YouCook2 [60] dataset. ActivityNet-caption is the benchmark that contains 20K untrimmed videos with 100k annotations. The total duration of the videos is 849 hours, and they are collected from YouTube. There are 10,009 videos in the train set and 4,917 in the validation set. YouCook2 is one of the widely utilized video benchmarks for video-language multimodal research. It comprises 2,000 lengthy uncut films from 89 different\ncooking recipes downloaded from YouTube. Each recipe contains 22 videos in average. The videos are all in the third person. There are about 9,600 train data and 3,200 in validation. To estimate performance on retrieval tasks, we use diverse retrieval metrics, such as recall and median rank (MR)."
        },
        {
            "heading": "3) RESULT",
            "text": "Table 1 shows the video-paragraph retrieval results on ActivityNet-caption dataset.We gained 4.7% better performance at the R@1 metric compared to COOT. We achieve high recall performance. Figure 4a present our result of top-1 video-paragraph retrieval. DMM found the paragraph of the exact matching pair of video for R@1. Also, our method has improved performance in paragraph-video retrieval. Although Figure 5a is not ground truth, DMM searched out a considerably similar video to the corresponding paragraph compared to COOT. Table 2 shows the result of YouCook2 dataset. We compare our method under two settings as COOT [1]. We first compare the model without HowTo100M pre-trained features. With our method and, COOT+DMM achieves the best performance in both paragraph-video and sentence-video retrieval tasks. Second, we compare the quantitative results with the pre-trained feature of the HowTo100M setting.\nWith the pre-trained feature, DMM also shows the best performance. We conjecture from the extensive experimental result that mixing the samples to generate hard positive-negative pairs with dynamically adapted margins\nVOLUME 11, 2023 60217\ncontributes to robust representation learning. Besides, Our method improves performance when the training dataset is limited."
        },
        {
            "heading": "4) ABLATION STUDY",
            "text": "We study the performance change on mixed visual samples. Referring to Equation 3, our method mixes both negative pairs and positive pairs, respectively, during training. Tomeasure the influence of the mixing factor, we conduct an experiment mixing only each positive sample and negative sample as the following:\nLpmix(P,N , \u03b1, \u03bb) = max(0, \u03bb\u03b1 + D(\u03bbxi + (1 \u2212 \u03bb)xj\u0338=i, yi)\n\u2212 D(xi, yi\u2032 \u0338=i)) (10)\nLnmix(P,N , \u03b1, \u03bb) = max(0, \u03bb\u03b1 + D(xi, yi) \u2212 D((\u03bbxi + (1 \u2212 \u03bb)xj\u0338=i, yi\u2032 \u0338=i)) (11)\nFigure 6 shows that performance on mixing only negative or positive pairs significantly drops compare to mixing both\npairs. Mixing only either negative or positive pairs may reduce performance because of a mismatch in the contrastive loss.\nC. VIDEO SENTIMENT CLASSIFICATION Recently, Poklukar et al. [46] proposed a new contrastive learning objective called GMC to learn a multimodal representation that satisfies two criteria: informativeness and robustness. By inducing the strong alignment between multimodal joint representation and unimodal representations, it learned semantically rich embedding space and achieved state-of-the-art performance on video clip classification in missing modalities setting. Here, we validate our DMM in terms of multimodal representation learning for sentiment classification of video clips. Specifically, we evaluate the classification performance improvement by adopting the DMM objective to the baseline."
        },
        {
            "heading": "1) METHODS",
            "text": "We first consider the Multimodal Transformer [61] (MulT), which is the strong baseline for classification on the CMU-MOSEI.MulT is constructedwithmultiple crossmodal transformers for all possible modality-pairs. By adopting crossmodal attention modules, MulT successfully fuses the information of multiple modalities and achieves superior performance than standard early fusion and late fusion approaches. Poklukar et al. [46] further enhance the learned representation of MulT with their Geometric Multimodal Contrastive (GMC) Loss (e.g., LCE +LGMC ) and accomplish new state-of-the-art performance when there are any missing\n60218 VOLUME 11, 2023\nmodalities in observed data instances. For DMM, all of the experimental environments are set the same with [46], and we just add our Dynamic Mixed Margin Loss on GMC with hyperparameter \u03be (e.g., LCE + LGMC + \u03beLDMM ) that control the relative magnitude between DMM and other loss terms. We simply set \u03be = 1.0 for CMU-MOSEI."
        },
        {
            "heading": "2) DATASETS AND EVALUATION METRICS",
            "text": "To measure the sentiment analysis performance from multimodal input data, following [46], we employ CMU-MOSEI [62] dataset. CMU-MOSEI contains 23K video clips from YouTube with image, text, and audio annotation. The goal of the task is to score of emotional states of each video clip labeled in seven discretized sentiment degrees (from strong negative to strong positive). For comparison between the quality of learned embedding from methods, we consider the binary accuracy as in [61]."
        },
        {
            "heading": "3) RESULTS",
            "text": "Table 3 shows the results of CMU-MOSEI classification. While MulT shows the best performance given the complete modality observation, both GMC and GMC with DMM consistently beat MulT with a large performance gap under the partial (missing) modality setting. Furthermore, DMM additionally boosts the classification performance of GMC\nVOLUME 11, 2023 60219\nby introducing the hardness-aware dynamic margin with a mixup for robust contrastive learning.\nD. RECOMMENDER SYSTEM In this subsection, we provide the recommender system performance on MovieLens-1M and Amazon Music dataset. Similar to the video-paragraph retrieval tasks, we can interpret the recommender system as a user-item retrieval task. Therefore, we can easily apply DMM to the recommender system by generating the augmented user embedding by mixing two random user embeddings with a mixing ratio \u03bb with a dynamically adapted margin."
        },
        {
            "heading": "1) EXPERIMENTAL SETUP",
            "text": "To evaluate our method comparison to baselines, we use two datasets MovieLens-1M and Amazon-digital-music. For MovieLens-1Mwe follow [48], use ratings with 4 or 5 as positive feedbacks, and use users who rated more than 10 items. For Amazon-digital-music we follow [51], divide the dataset into training, validation, and test sets, ensuring a distribution ratio of 8:1:1. We use three evaluation metrics Recall@K, NDCG@K, HitRatio@K with K in {1, 5, 10}. We sample the value of \u03bb at the same condition as the video-paragraph retrieval experiments."
        },
        {
            "heading": "2) RESULTS",
            "text": "Table 4 shows the performances comparing with other methods. DMM show the best performance all metrics except R@1 on amazon Music. As mentioned in Hsieh et al. [48], the way that BPRminimizes the error between a user and user preferred item has a critical shortage. BPR might have problems handling the low-ranking unpopular item; therefore, item-item, user-user relationships might not be learned well. Figure 7 shows that BPR loss separates user and item embeddings; the two large clusters may harm the performance. CML handles the low-ranking item relatively well, but CML show many clustered groups still. It might cause overfitting problem such as recommend similar items to the user\nwith low diversity [51]. However, DMM learns more robust representation with large uniformity and induces improved performance. This is because ourmodel can get hard-negative samples with Mixup and more clear decision boundary with an adaptive margin. For more detailed analysis, we compare the recommendation results of CML and CML with DMM in Table 5. While the CML\u2019s recommendations are based on the user\u2019s history-prominent movie genre and are less accurate, DMM shows more accurate and diverse item recommendations.\nVII. CONCLUSION We find that the traditional triplet loss and contrastive loss might fail to learn robust representation when the training dataset is limited, and the modality missing occurs. From our new findings, we propose a novel dynamic mixed margin that generates the hard positive-negative pairs with a dynamically adapted margin. Our method learns the robust representation that is transferable to diverse downstream tasks. We provide extensive experimental results and validate that our proposed DMM consistently improve the downstream performance on numerous tasks.\nCONFLICT OF INTEREST There is no conflict of interest for this paper.\nAPPENDIX. ADDITIONAL RESULTS In this section, we provide the additional qualitative result of our method. Figure 8 represents the mixed videos by DMM. DMM mixes the videos on the embedding space, and we retrieve the original video that has the most similar embeddings. \u03bb is the mixing ratio of 8a. For large \u03bb, Figure 8c the closest video contains orange kayak as 8a. In Figure 8d and 8e, as \u03bb gets smaller drawing content appears in video as 8b.\nAPPENDIX. T-SNE EMBEDDING FOR AMAZON-DIGITAL MUSIC DATASET We provide the additional qualitative results for the recommender system onAmazonmusic dataset. Figure 9 visualizes the learned embedding\nAPPENDIX. CODE IMPLEMENTATION In this section, we provide the implement code for our dynamic mixed margin on video representation learning. Algorithm 1 provides the PyTorch style implementation code, and we will share our full implementation code upon request.\n60220 VOLUME 11, 2023\nAlgorithm 1 PyTorch-like Code for DynamicMixed Margin def dmm(neg_emb, pos_emb, margin):\nlamb = random.random() while lamb > 0.5:\nlamb = random.random() neg_emb = lamb * pos_emb + (1 - lamb) * neg_emb margin = margin * lamb return neg_emb, margin\n# implementation code for training COOT, video-text model using DMM def train_COOT_DMM(use_dmm=False, batch, model, default_margin, epoch):\nvisual_data = model.encode_visual(batch) text_data = model.encode_text(batch) contr_loss = compute_total_contrastive_loss(visual_data, text_data, default_margin) cc_loss = compute_cyclecons_loss(visual_data, text_data loss = contr_loss + cc_loss if use_dmm:\nnew_batch = copy.deepcopy(batch) flipped_batch = torch.flip(new_batch.vid_feat,dims=[0]) flipped_visual_data = model.encode_visual(flipped_batch) mixed_visual_data, margin = dmm(flipped_visual_data, visual_data, default_margin) contr_loss_mixed = compute_total_contrastive_loss (mixed_visual_data, text_data, margin) loss += contr_loss_mixed*(epoch/(epoch + 30)\nreturn loss\nACKNOWLEDGMENT (Junhyuk So, Yongtaek Lim, and Yewon Kim contributed equally to this work.)\nREFERENCES [1] S. Ging, M. Zolfaghari, H. Pirsiavash, and T. Brox, \u2018\u2018COOT: Cooperative\nhierarchical transformer for video-text representation learning,\u2019\u2019 in Proc. NeurIPS, vol. 33, 2020, pp. 22605\u201322618. [2] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, and J. Clark, \u2018\u2018Learning transferable visual models from natural language supervision,\u2019\u2019 in Proc. Int. Conf. Mach. Learn., 2021, pp. 8748\u20138763. [3] A. Jaegle, S. Borgeaud, J.-B. Alayrac, C. Doersch, C. Ionescu, D. Ding, S. Koppula, D. Zoran, A. Brock, E. Shelhamer, O. H\u00e9naff, M. M. Botvinick, A. Zisserman, O. Vinyals, and J. Carreira, \u2018\u2018Perceiver IO: A general architecture for structured inputs & outputs,\u2019\u2019 2021, arXiv:2107.14795. [4] S. Bae, H. Byun, C. Oh, Y.-S. Cho, and K. Song, \u2018\u2018Graph perceiver IO: A general architecture for graph structured data,\u2019\u2019 2022, arXiv:2209.06418. [5] K. Simonyan and A. Zisserman, \u2018\u2018Very deep convolutional networks for large-scale image recognition,\u2019\u2019 2014, arXiv:1409.1556. [6] K. He, X. Zhang, S. Ren, and J. Sun, \u2018\u2018Deep residual learning for image recognition,\u2019\u2019 in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2016, pp. 770\u2013778. [7] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi, \u2018\u2018Inception-v4, Inception-Resnet and the impact of residual connections on learning,\u2019\u2019 in Proc. 31st AAAI Conf. Artif. Intell., 2017, pp. 4278\u20134284. [8] K. Weiss, T. Khoshgoftaar, and D. Wang, \u2018\u2018A survey of transfer learning,\u2019\u2019 J. Big Data, vol. 3, pp. 1\u201340, May 2016.\n[9] M. Hussain, J. J. Bird, and D. R. Faria, \u2018\u2018A study on CNN transfer learning for image classification,\u2019\u2019 in Proc. U.K. Workshop Comput. Intell. Cham, Switzerland: Springer, 2018, pp. 191\u2013202. [10] S. Tammina, \u2018\u2018Transfer learning using VGG-16 with deep convolutional neural network for classifying images,\u2019\u2019 Int. J. Sci. Res. Publications (IJSRP), vol. 9, no. 10, pp. 143\u2013150, 2019. [11] D. Mahajan, R. Girshick, V. Ramanathan, K. He, M. Paluri, Y. Li, A. Bharambe, and L. V. D. Maaten, \u2018\u2018Exploring the limits of weakly supervised pretraining,\u2019\u2019 in Proc. Eur. Conf. Comput. Vis. (ECCV), Sep. 2018, pp. 181\u2013196. [12] Y. Feng, J. Jiang, M. Tang, R. Jin, and Y. Gao, \u2018\u2018Rethinking supervised pretraining for better downstream transferring,\u2019\u2019 2021, arXiv:2110.06014. [13] T. Wang and P. Isola, \u2018\u2018Understanding contrastive representation learning through alignment and uniformity on the hypersphere,\u2019\u2019 in Proc. Int. Conf. Mach. Learn., 2020, pp. 9929\u20139939. [14] Y. Kalantidis, M. B. Sariyildiz, N. Pion, P. Weinzaepfel, and D. Larlus, \u2018\u2018Hard negative mixing for contrastive learning,\u2019\u2019 in Proc. Adv. Neural Inf. Process. Syst., vol. 33, H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, Eds. Red Hook, NY, USA: Curran Associates, 2020, pp. 21798\u201321809. [15] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, \u2018\u2018A simple framework for contrastive learning of visual representations,\u2019\u2019 inProc. Int. Conf.Mach. Learn., 2020, pp. 1597\u20131607. [16] J.-B. Grill, \u2018\u2018Bootstrap your own latent\u2014A new approach to selfsupervised learning,\u2019\u2019 inProc. Adv. Neural Inf. Process. Syst., vol. 33, 2020, pp. 21271\u201321284. [17] H. Zhang, J. Y. Koh, J. Baldridge, H. Lee, and Y. Yang, \u2018\u2018Cross-modal contrastive learning for text-to-image generation,\u2019\u2019 in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2021, pp. 833\u2013842. [18] Z. Wu, S. Wang, J. Gu, M. Khabsa, F. Sun, and H. Ma, \u2018\u2018CLEAR: Contrastive learning for sentence representation,\u2019\u2019 2020, arXiv:2012.15466. [19] A. Shen, X. Han, T. Cohn, T. Baldwin, and L. Frermann, \u2018\u2018Contrastive learning for fair representations,\u2019\u2019 2021, arXiv:2109.10645. [20] C. Oh, H. Won, J. So, T. Kim, Y. Kim, H. Choi, and K. Song, \u2018\u2018Learning fair representation via distributional contrastive disentanglement,\u2019\u2019 in Proc. 28th ACM SIGKDD Conf. Knowl. Discovery Data Mining, Aug. 2022, pp. 1295\u20131305. [21] A. van den Oord, Y. Li, and O. Vinyals, \u2018\u2018Representation learning with contrastive predictive coding,\u2019\u2019 2018, arXiv:1807.03748. [22] E. Hoffer and N. Ailon, \u2018\u2018Deep metric learning using triplet network,\u2019\u2019 in Proc. Int. Workshop Similarity-Based Pattern Recognit. Cham, Switzerland: Springer, 2015, pp. 84\u201392. [23] X. Dong and J. Shen, \u2018\u2018Triplet loss in Siamese network for object tracking,\u2019\u2019 in Proc. Eur. Conf. Comput. Vis. (ECCV), Sep. 2018, pp. 459\u2013474. [24] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz, \u2018\u2018Mixup: Beyond empirical risk minimization,\u2019\u2019 2017, arXiv:1710.09412. [25] V. Verma, A. Lamb, C. Beckham, A. Najafi, I. Mitliagkas, D. Lopez-Paz, and Y. Bengio, \u2018\u2018Manifold mixup: Better representations by interpolating hidden states,\u2019\u2019 in Proc. Int. Conf. Mach. Learn., 2019, pp. 6438\u20136447. [26] S. Chopra, R. Hadsell, and Y. LeCun, \u2018\u2018Learning a similarity metric discriminatively, with application to face verification,\u2019\u2019 in Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2005, pp. 539\u2013546. [27] F. Schroff, D. Kalenichenko, and J. Philbin, \u2018\u2018FaceNet: A unified embedding for face recognition and clustering,\u2019\u2019 in Proc. IEEEConf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2015, pp. 815\u2013823. [28] K. Sohn, \u2018\u2018Improved deep metric learning with multi-class n-Pair loss objective,\u2019\u2019 in Proc. Adv. Neural Inf. Process. Syst., vol. 29, D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, R. Garnett, Eds. RedHook, NY,USA: Curran Associates, 2016, pp. 1\u20139. [29] N. Frosst, N. Papernot, and G. Hinton, \u2018\u2018Analyzing and improving representations with the soft nearest neighbor loss,\u2019\u2019 in Proc. Int. Conf. Mach. Learn., 2019, pp. 2012\u20132020. [30] H. Hotelling, \u2018\u2018Relations between two sets of variates,\u2019\u2019 Biometrika, vol. 28, nos. 3\u20134, pp. 321\u2013377, Dec. 1936. [31] M. Hodosh, P. Young, and J. Hockenmaier, \u2018\u2018Framing image description as a ranking task: Data, models and evaluation metrics,\u2019\u2019 J. Artif. Intell. Res., vol. 47, pp. 853\u2013899, Aug. 2013. [32] R. Kiros, R. Salakhutdinov, and R. S. Zemel, \u2018\u2018Unifying visualsemantic embeddings with multimodal neural language models,\u2019\u2019 2014, arXiv:1411.2539. [33] I. Vendrov, R. Kiros, S. Fidler, and R. Urtasun, \u2018\u2018Order-embeddings of images and language,\u2019\u2019 2015, arXiv:1511.06361.\nVOLUME 11, 2023 60221\n[34] F. Faghri, D. J. Fleet, J. R. Kiros, and S. Fidler, \u2018\u2018VSE++: Improving visualsemantic embeddings with hard negatives,\u2019\u2019 2017, arXiv:1707.05612. [35] L. Wang, Y. Li, J. Huang, and S. Lazebnik, \u2018\u2018Learning two-branch neural networks for image-text matching tasks,\u2019\u2019 IEEE Trans. Pattern Anal. Mach. Intell., vol. 41, no. 2, pp. 394\u2013407, Feb. 2019. [36] X. Li, \u2018\u2018Oscar: Object-semantics aligned pre-training for vision-language tasks,\u2019\u2019 in Proc. Eur. Conf. Comput. Vis. Cham, Switzerland: Springer, 2020, pp. 121\u2013137. [37] W. Kim, B. Son, and I. Kim, \u2018\u2018ViLT: Vision-and-language transformer without convolution or region supervision,\u2019\u2019 in Int. Conf. Mach. Learn., 2021, pp. 5583\u20135594. [38] J. Li, R. Selvaraju, A. Gotmare, S. Joty, C. Xiong, and S. C. H. Hoi, \u2018\u2018Align before fuse: Vision and language representation learning with momentum distillation,\u2019\u2019 in Proc. Adv. Neural Inf. Process. Syst. (NeurIPS), vol. 34, 2021, pp. 9694\u20139705. [39] A. Singh, R. Hu, V. Goswami, G. Couairon, W. Galuba, M. Rohrbach, and D. Kiela, \u2018\u2018FLAVA: A foundational language and vision alignment model,\u2019\u2019 in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2022, pp. 15617\u201315629. [40] F. Chen, X. Chen, J. Shi, D. Zhang, J. Chang, and Q. Tian, \u2018\u2018HiVLP: Hierarchical vision-language pre-training for fast image-text retrieval,\u2019\u2019 2022, arXiv:2205.12105. [41] S. Venugopalan, H. Xu, J. Donahue, M. Rohrbach, R. Mooney, and K. Saenko, \u2018\u2018Translating videos to natural language using deep recurrent neural networks,\u2019\u2019 2014, arXiv:1412.4729. [42] R. Krishna, K. Hata, F. Ren, L. Fei-Fei, and J. C. Niebles, \u2018\u2018Densecaptioning events in videos,\u2019\u2019 inProc. IEEE Int. Conf. Comput. Vis. (ICCV), Oct. 2017, pp. 706\u2013715. [43] B. Zhang, H. Hu, and F. Sha, \u2018\u2018Cross-modal and hierarchical modeling of video and text,\u2019\u2019 in Proc. Eur. Conf. Comput. Vis. (ECCV), Sep. 2018, pp. 374\u2013390. [44] H. Luo, L. Ji, M. Zhong, Y. Chen, W. Lei, N. Duan, and T. Li, \u2018\u2018CLIP4Clip: An empirical study of CLIP for end to end video clip retrieval,\u2019\u2019 2021, arXiv:2104.08860. [45] H. Fang, P. Xiong, L. Xu, and Y. Chen, \u2018\u2018CLIP2 Video: Mastering videotext retrieval via image CLIP,\u2019\u2019 2021, arXiv:2106.11097. [46] P. Poklukar, M. Vasco, H. Yin, F. S. Melo, A. Paiva, and D. Kragic, \u2018\u2018Geometric multimodal contrastive representation learning,\u2019\u2019 2022, arXiv:2202.03390. [47] S. Rendle, C. Freudenthaler, Z. Gantner, and L. Schmidt-Thieme, \u2018\u2018BPR: Bayesian personalized ranking from implicit feedback,\u2019\u2019 2012, arXiv:1205.2618. [48] C.-K. Hsieh, L. Yang, Y. Cui, T.-Y. Lin, S. Belongie, and D. Estrin, \u2018\u2018Collaborative metric learning,\u2019\u2019 in Proc. 26th Int. Conf. World Wide Web, Apr. 2017, pp. 193\u2013201. [49] S. Zhang, Y. Tay, L. Yao, and A. Sun, \u2018\u2018Next item recommendation with self-attention,\u2019\u2019 2018, arXiv:1808.06414. [50] R. Ying, R. He, K. Chen, P. Eksombatchai, W. L. Hamilton, and J. Leskovec, \u2018\u2018Graph convolutional neural networks for web-scale recommender systems,\u2019\u2019 in Proc. 24th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, Jul. 2018, pp. 974\u2013983. [51] Z. Gao, Z. Cheng, F. Perez, J. Sun, and M. Volkovs, \u2018\u2018MCL: Mixed-centric loss for collaborative filtering,\u2019\u2019 in Proc. ACM Web Conf., Apr. 2022, pp. 2339\u20132347. [52] Q. Wang, H. Yin, H. Wang, Q. V. H. Nguyen, Z. Huang, and L. Cui, \u2018\u2018Enhancing collaborative filteringwith generative augmentation,\u2019\u2019 inProc. 25th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, Jul. 2019, pp. 548\u2013556. [53] M. L. Ha and V. Blanz, \u2018\u2018Deep ranking with adaptive margin triplet loss,\u2019\u2019 2021, arXiv:2107.06187. [54] J. So, C. Oh, Y. Lim, H. Byun, M. Shin, and K. Song, \u2018\u2018Geodesic multimodal mixup for robust fine-tuning,\u2019\u2019 2022, arXiv:2203.03897. [55] P. Zhu, R. Abdal, J. Femiani, and P. Wonka, \u2018\u2018Mind the gap: Domain gap control for single shot domain adaptation for generative adversarial networks,\u2019\u2019 2021, arXiv:2110.08398. [56] T. Chen, C. Luo, and L. Li, \u2018\u2018Intriguing properties of contrastive losses,\u2019\u2019 Adv. Neural Inf. Process. Syst., vol. 34, pp. 11834\u201311845, 2021. [57] A. Miech, D. Zhukov, J. Alayrac, M. Tapaswi, I. Laptev, and J. Sivic, \u2018\u2018HowTo100M: Learning a text-video embedding by watching hundred million narrated video clips,\u2019\u2019 in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2019, pp. 2630\u20132640.\n[58] K. Hara, H. Kataoka, and Y. Satoh, \u2018\u2018Can spatiotemporal 3D CNNs retrace the history of 2D CNNs and ImageNet?\u2019\u2019 in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2018, pp. 6546\u20136555. [59] J.Wang,W. Jiang, L.Ma,W. Liu, andY.Xu, \u2018\u2018Bidirectional attentive fusion with context gating for dense video captioning,\u2019\u2019 in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2018, pp. 7190\u20137198. [60] L. Zhou, C. Xu, and J. J. Corso, \u2018\u2018Towards automatic learning of procedures fromweb instructional videos,\u2019\u2019 inProc. 3rd AAAI Conf. Artif. Intell., 2018, pp. 7590\u20137598. [61] Y.-H.-H. Tsai, S. Bai, P. P. Liang, J. Z. Kolter, L.-P. Morency, and R. Salakhutdinov, \u2018\u2018Multimodal transformer for unaligned multimodal language sequences,\u2019\u2019 in Proc. 57th Annu. Meeting Assoc. Comput. Linguistics, 2019, p. 6558. [62] A. Zadeh, R. Zellers, E. Pincus, and L. Morency, \u2018\u2018Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages,\u2019\u2019 IEEE Intell. Syst., vol. 31, no. 6, pp. 82\u201388, Nov. 2016.\nJUNHYUK SO received the B.S. degree in electrical and computer engineering from the University of Seoul, in 2022. He is currently pursuing the M.S. degree in computer science and engineering with the Pohang University of Science and Technology.\nYONGTAEK LIM received the B.S. degree in electrical and computer engineering from Ajou University, in 2021. He is currently pursuing the M.S. degree in artificial intelligence with the University of Seoul.\nYEWON KIM received the B.S. degree in electrical and computer engineering from the University of Seoul, in 2022, where she is currently pursuing the M.S. degree in artificial intelligence.\nCHANGDAE OH received the B.S. degree in statistics from the University of Seoul, in 2022, where he is currently pursuing the M.S. degree in artificial intelligence.\nKYUNGWOO SONG received the B.S. degree in mathematical sciences and industrial and systems engineering and the M.S. and Ph.D. degrees in industrial and systems engineering from the Korea Advanced Institute of Science and Technology (KAIST), in 2017 and 2021. In 2017, he was a part-time Professor with Hanbat National University. In 2018, he was a Visiting Researcher with Naver Clova. From 2021 to 2023, he was an Assistant Professor with the Department of\nArtificial Intelligence, University of Seoul. He is currently an Assistant Professor with the Department of Applied Statistics and the Department of Statistics and Data Science, Yonsei University.\n60222 VOLUME 11, 2023"
        }
    ],
    "title": "Robust Contrastive Learning With Dynamic Mixed Margin",
    "year": 2023
}