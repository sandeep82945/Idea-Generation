{
    "abstractText": "Parametric model order reduction techniques often struggle to accurately represent transport-dominated phenomena due to a slowly decaying Kolmogorov n-width. To address this challenge, we propose a non-intrusive, data-driven methodology that combines the shifted proper orthogonal decomposition (POD) with deep learning. Specifically, the shifted POD technique is utilized to derive a high-fidelity, low-dimensional model of the flow, which is subsequently utilized as input to a deep learning framework to forecast the flow dynamics under various temporal and parameter conditions. The efficacy of the proposed approach is demonstrated through the analysis of oneand two-dimensional wildland fire models with varying reaction rates, and its performance is evaluated using multiple error measures. The results indicate that the proposed approach yields highly accurate results within the percent range, while also enabling rapid prediction of system states within seconds.",
    "authors": [
        {
            "affiliations": [],
            "name": "Shubhaditya Burela"
        },
        {
            "affiliations": [],
            "name": "Philipp Krah"
        },
        {
            "affiliations": [],
            "name": "Julius Reiss"
        }
    ],
    "id": "SP:a12ef088df229df3b5eb795e36ffe164d7d82caf",
    "references": [
        {
            "authors": [
                "G. DaCosta",
                "D. Derigo",
                "G. Liberta",
                "T. Durrant",
                "J. San-miguel-ayanz"
            ],
            "title": "European wildfire danger and vulnerability in a changing climate: towards integrating risk dimensions (2020)",
            "venue": "sP O D -N N sP O D -I PO D -N N",
            "year": 2000
        },
        {
            "authors": [
                "S. Kondylatos",
                "I. Prapas",
                "M. Ronco",
                "I. Papoutsis",
                "G. CampsValls",
                "M. Piles",
                "M. FernndezTorres",
                "N. Carvalhais"
            ],
            "title": "Wildfire danger prediction and understanding with deep learning",
            "venue": "Geophysical Research Letters",
            "year": 2022
        },
        {
            "authors": [
                "Y.O. Sayad",
                "H. Mousannif",
                "H. Al Moatassime"
            ],
            "title": "Predictive modeling of wildfires: A new dataset and machine learning approach",
            "venue": "Fire Safety Journal",
            "year": 2019
        },
        {
            "authors": [
                "L. Vilar",
                "S. Herrera",
                "E. Tafur-Garcia",
                "M. Yebra",
                "J. Martinez-Vega",
                "P. Echavarria",
                "M.P. Martin"
            ],
            "title": "Modelling wildfire occurrence at regional scale from land use/cover and climate change scenarios",
            "venue": "Environmental Modelling & Software 145,",
            "year": 2021
        },
        {
            "authors": [
                "M.M. Valero",
                "L. Jofre",
                "R. Torres"
            ],
            "title": "Multifidelity prediction in wildfire spread simulation: Modeling, uncertainty quantification and sensitivity analysis",
            "venue": "Environmental Modelling & Software 141,",
            "year": 2021
        },
        {
            "authors": [
                "J. Mandel",
                "L.S. Bennethum",
                "J.D. Beezley",
                "J.L. Coen",
                "C.C. Douglas",
                "M. Kim",
                "A. Vodacek"
            ],
            "title": "A wildland fire model with data assimilation",
            "venue": "Mathematics and Computers in Simulation 79(3),",
            "year": 2008
        },
        {
            "authors": [
                "P. Benner",
                "M. Ohlberger",
                "A. Cohen",
                "K. Willcox"
            ],
            "title": "Model Reduction and Approximation",
            "venue": "Computational Science & Engineering. Society for Industrial and Applied Mathematics,",
            "year": 2017
        },
        {
            "authors": [
                "J.S. Hesthaven",
                "G. Rozza",
                "B. Stamm"
            ],
            "title": "Certified Reduced Basis Methods for Parametrized Partial Differential Equations",
            "year": 2016
        },
        {
            "authors": [
                "A. Quarteroni",
                "A. Manzoni",
                "F. Negri"
            ],
            "title": "Reduced Basis Methods for Partial Differential Equations",
            "venue": "https://doi.org/",
            "year": 2015
        },
        {
            "authors": [
                "C. Greif",
                "K. Urban"
            ],
            "title": "Decay of the Kolmogorov n-width for wave problems",
            "venue": "Applied Mathematics Letters 96,",
            "year": 2019
        },
        {
            "authors": [
                "B. Unger",
                "S. Gugercin"
            ],
            "title": "Kolmogorov n-widths for linear dynamical systems",
            "venue": "Advances in Computational Mathematics",
            "year": 2019
        },
        {
            "authors": [
                "P. Krah",
                "S. Bchholz",
                "M. Hringer",
                "J. Reiss"
            ],
            "title": "Front Transport Reduction for Complex Moving Fronts",
            "venue": "Accepted for publication in J. Sci. Comput",
            "year": 2022
        },
        {
            "authors": [
                "C. Huang",
                "K. Duraisamy",
                "C. Merkle"
            ],
            "title": "Challenges in Reduced Order Modeling of Reacting Flows",
            "venue": "Joint Propulsion Conference. AIAA Propulsion and Energy Forum. American Institute of Aeronautics and Astronautics, Cincinnati, Ohio (2018)",
            "year": 2018
        },
        {
            "authors": [
                "M. Ohlberger",
                "S. Rave"
            ],
            "title": "Nonlinear reduced basis approximation of parameterized evolution equations via the method of freezing",
            "venue": "Comptes Rendus Mathematique 351(23),",
            "year": 2013
        },
        {
            "authors": [
                "D. Rim",
                "S. Moe",
                "R.J. LeVeque"
            ],
            "title": "Transport Reversal for Model Reduction of Hyperbolic Partial Differential Equations",
            "venue": "SIAM/ASA Journal on Uncertainty Quantification",
            "year": 2018
        },
        {
            "authors": [
                "N. Cagniart",
                "Y. Maday",
                "B. Stamm"
            ],
            "title": "Model Order Reduction for Problems with Large Convection Effects",
            "venue": "Computational Methods in Applied Sciences,",
            "year": 2019
        },
        {
            "authors": [
                "G. Welper"
            ],
            "title": "Transformed Snapshot Interpolation with High Resolution Transforms",
            "venue": "SIAM Journal on Scientific Computing",
            "year": 2020
        },
        {
            "authors": [
                "F. Black",
                "P. Schulze",
                "B. Unger"
            ],
            "title": "Projection-based model reduction with dynamically transformed modes. ESAIM: Mathematical Modelling and Numerical Analysis",
            "year": 2020
        },
        {
            "authors": [
                "M. Nonino",
                "F. Ballarin",
                "G. Rozza",
                "Y. Maday"
            ],
            "title": "Overcoming slowly decaying Kolmogorov n-width by transport maps: application to model order reduction of fluid dynamics and fluid\u2013structure interaction problems",
            "year": 2019
        },
        {
            "authors": [
                "P. Krah",
                "M. Sroka",
                "J. Reiss"
            ],
            "title": "Model Order Reduction of Combustion Processes with Complex Front Dynamics. In: Numerical Mathematics and Advanced Applications ENUMATH 2019",
            "venue": "Lecture Notes in Computational Science and Engineering,",
            "year": 2021
        },
        {
            "authors": [
                "F. Black",
                "P. Schulze",
                "B. Unger"
            ],
            "title": "Efficient Wildland Fire Simulation via Nonlinear Model Order Reduction",
            "venue": "Fluids 6(8),",
            "year": 2021
        },
        {
            "authors": [
                "J. Reiss",
                "P. Schulze",
                "J. Sesterhenn",
                "V. Mehrmann"
            ],
            "title": "The Shifted Proper Orthogonal Decomposition: A Mode Decomposition for Multiple Transport Phenomena",
            "venue": "SIAM Journal on Scientific Computing",
            "year": 2018
        },
        {
            "authors": [
                "K. Lee",
                "K.T. Carlberg"
            ],
            "title": "Model reduction of dynamical systems on nonlinear manifolds using deep convolutional autoencoders",
            "venue": "Journal of Computational Physics 404,",
            "year": 2020
        },
        {
            "authors": [
                "J. Reiss"
            ],
            "title": "Optimization-Based Modal Decomposition for Systems with Multiple Transports",
            "venue": "SIAM Journal on Scientific Computing",
            "year": 2021
        },
        {
            "authors": [
                "P.L. Krah"
            ],
            "title": "Non-linear reduced order modeling for transport dominated fluid systems [doctoral dissertation] (2023)",
            "year": 2023
        },
        {
            "authors": [
                "A. Mendible",
                "S.L. Brunton",
                "A.Y. Aravkin",
                "W. Lowrie",
                "J.N. Kutz"
            ],
            "title": "Dimensionality reduction and reduced-order modeling for traveling wave physics",
            "venue": "Theoretical and Computational Fluid Dynamics",
            "year": 2020
        },
        {
            "authors": [
                "D. Papapicco",
                "N. Demo",
                "M. Girfoglio",
                "G. Stabile",
                "G. Rozza"
            ],
            "title": "The Neural Network shifted-proper orthogonal decomposition: A machine learning approach for non-linear reduction of hyperbolic equations",
            "venue": "Computer Methods in Applied Mechanics and Engineering",
            "year": 2022
        },
        {
            "authors": [
                "N. Sarna",
                "S. Grundel"
            ],
            "title": "Hyper-reduction for parametrized transport dominated problems via online-adaptive reduced meshes",
            "venue": "https:",
            "year": 2021
        },
        {
            "authors": [
                "K. Carlberg",
                "C. Farhat",
                "J. Cortial",
                "D. Amsallem"
            ],
            "title": "The GNAT method for nonlinear model reduction: Effective implementation and application to computational fluid dynamics and turbulent flows",
            "venue": "Journal of Computational Physics",
            "year": 2013
        },
        {
            "authors": [
                "M. Barrault",
                "Y. Maday",
                "N.C. Nguyen",
                "A.T. Patera"
            ],
            "title": "An empirical interpolation method: application to efficient reduced-basis discretization of partial differential equations",
            "venue": "Comptes Rendus Mathematique",
            "year": 2004
        },
        {
            "authors": [
                "S. Jain",
                "P. Tiso"
            ],
            "title": "Hyper-Reduction Over Nonlinear Manifolds for Large Nonlinear Mechanical Systems",
            "venue": "Journal of Computational and Nonlinear Dynamics",
            "year": 2017
        },
        {
            "authors": [
                "S. Chaturantabut",
                "D.C. Sorensen"
            ],
            "title": "Nonlinear Model Reduction via Discrete Empirical Interpolation",
            "venue": "SIAM Journal on Scientific Computing",
            "year": 2010
        },
        {
            "authors": [
                "B. Peherstorfer"
            ],
            "title": "Model Reduction for Transport-Dominated Problems via Online Adaptive Bases and Adaptive Sampling",
            "venue": "SIAM Journal on Scientific Computing",
            "year": 2020
        },
        {
            "authors": [
                "O. Koch",
                "C. Lubich"
            ],
            "title": "Dynamical LowRank Approximation",
            "venue": "SIAM Journal on Matrix Analysis and Applications 29(2),",
            "year": 2007
        },
        {
            "authors": [
                "P.A. Etter",
                "K.T. Carlberg"
            ],
            "title": "Online adaptive basis refinement and compression for reduced-order models via vector-space sieving",
            "venue": "Computer Methods in Applied Mechanics and Engineering",
            "year": 2020
        },
        {
            "authors": [
                "M.A. Dihlmann",
                "M. Drohmann",
                "B. Haasdonk"
            ],
            "title": "Model reduction of parametrized evolution problems using the reduced basis method with adaptive time partitioning",
            "venue": "Proc. of ADMOS,",
            "year": 2011
        },
        {
            "authors": [
                "R. Zimmermann",
                "B. Peherstorfer",
                "K. Willcox"
            ],
            "title": "Geometric Subspace Updates with Applications to Online Adaptive Nonlinear Model Reduction",
            "venue": "SIAM Journal on Matrix Analysis and Applications",
            "year": 2018
        },
        {
            "authors": [
                "B. Peherstorfer",
                "K. Willcox"
            ],
            "title": "Online Adaptive Model Reduction for Nonlinear Systems via Low-Rank Updates",
            "venue": "SIAM Journal on Scientific Computing",
            "year": 2015
        },
        {
            "authors": [
                "J. Koellermeier",
                "P. Krah",
                "J. Kusch"
            ],
            "title": "Macro-micro decomposition for consistent and conservative model order reduction of hyperbolic shallow water moment equations: A study using POD-Galerkin and dynamical low rank approximation",
            "year": 2023
        },
        {
            "authors": [
                "F.J. Gonzalez",
                "M. Balajewicz"
            ],
            "title": "Deep convolutional recurrent autoencoders for learning low-dimensional feature dynamics of fluid systems",
            "year": 2018
        },
        {
            "authors": [
                "Y. Kim",
                "Y. Choi",
                "D. Widemann",
                "T. Zohdi"
            ],
            "title": "A fast and accurate physicsinformed neural network reduced order model with shallow masked autoencoder",
            "venue": "Journal of Computational Physics",
            "year": 2022
        },
        {
            "authors": [
                "M. Salvador",
                "L. Ded",
                "A. Manzoni"
            ],
            "title": "Non intrusive reduced order modeling of parametrized PDEs by kernel POD and neural networks",
            "venue": "Computers & Mathematics with Applications 104,",
            "year": 2021
        },
        {
            "authors": [
                "O. San",
                "R. Maulik"
            ],
            "title": "Neural network closures for nonlinear model order reduction",
            "venue": "Advances in Computational Mathematics",
            "year": 2018
        },
        {
            "authors": [
                "J.S. Hesthaven",
                "S. Ubbiali"
            ],
            "title": "Non-intrusive reduced order modeling of nonlinear problems using neural networks",
            "venue": "Journal of Computational Physics",
            "year": 2018
        },
        {
            "authors": [
                "Q. Wang",
                "J.S. Hesthaven",
                "D. Ray"
            ],
            "title": "Non-intrusive reduced order modeling of unsteady flows using artificial neural networks with application to a combustion problem",
            "venue": "Journal of Computational Physics",
            "year": 2019
        },
        {
            "authors": [
                "A. Brzi",
                "J. Helmig",
                "F. Key",
                "S. Elgeti"
            ],
            "title": "Standardized Non-Intrusive Reduced Order Modeling Using Different Regression Models With Application to Complex Flow Problems",
            "venue": "arXiv (2021)",
            "year": 2006
        },
        {
            "authors": [
                "M. Kast",
                "M. Guo",
                "J.S. Hesthaven"
            ],
            "title": "A non-intrusive multifidelity method for the reduced order modeling of nonlinear problems",
            "venue": "Computer Methods in Applied Mechanics and Engineering",
            "year": 2020
        },
        {
            "authors": [
                "M. Guo",
                "J.S. Hesthaven"
            ],
            "title": "Data-driven reduced order modeling for timedependent problems",
            "venue": "Computer Methods in Applied Mechanics and Engineering",
            "year": 2019
        },
        {
            "authors": [
                "K. Bhattacharya",
                "B. Hosseini",
                "N.B. Kovachki",
                "A.M. Stuart"
            ],
            "title": "Model Reduction And Neural Networks For Parametric PDEs",
            "venue": "The SMAI Journal of computational mathematics",
            "year": 2021
        },
        {
            "authors": [
                "A.T. Mohan",
                "D.V. Gaitonde"
            ],
            "title": "A Deep Learning based Approach to Reduced Order Modeling for Turbulent Flow Control using LSTM Neural Networks",
            "year": 2018
        },
        {
            "authors": [
                "J. Nagoor Kani",
                "A.H. Elsheikh"
            ],
            "title": "Reduced-Order Modeling of Subsurface Multi-phase Flow Models Using Deep Residual Recurrent Neural Networks",
            "venue": "Transport in Porous Media",
            "year": 2019
        },
        {
            "authors": [
                "S. Fresca",
                "L. Dede",
                "A. Manzoni"
            ],
            "title": "A Comprehensive Deep Learning-Based Approach to Reduced Order Modeling of Nonlinear Time-Dependent Parametrized PDEs",
            "venue": "Journal of Scientific Computing 87(2),",
            "year": 2021
        },
        {
            "authors": [
                "S. Fresca",
                "A. Manzoni"
            ],
            "title": "POD-DL-ROM: Enhancing deep learning-based reduced order models for nonlinear parametrized PDEs by proper orthogonal decomposition",
            "venue": "Computer Methods in Applied Mechanics and Engineering",
            "year": 2022
        },
        {
            "authors": [
                "A. Kovrnov",
                "P. Krah",
                "J. Reiss",
                "M. Isoz"
            ],
            "title": "Shifted Proper Orthogonal Decomposition and Artificial Neural Networks for Time-Continuous Reduced Order Models of Transport-Dominated Systems (2022)",
            "year": 2022
        },
        {
            "authors": [
                "P. Virtanen",
                "R. Gommers",
                "T.E. Oliphant",
                "M. Haberland",
                "T. Reddy",
                "D. Cournapeau",
                "E. Burovski",
                "P. Peterson",
                "W. Weckesser",
                "J. Bright",
                "S.J. van der Walt",
                "M. Brett",
                "J. Wilson",
                "K.J. Millman",
                "N. Mayorov",
                "A.R.J. Nelson",
                "E. Jones",
                "R. Kern",
                "E. Larson",
                "C.J. Carey",
                "Polat",
                "Y. Feng",
                "E.W. Moore",
                "J. VanderPlas",
                "D. Laxalde",
                "J. Perktold",
                "R. Cimrman",
                "I. Henriksen",
                "E.A. Quintero",
                "C.R. Harris",
                "A.M. Archibald",
                "A.H. Ribeiro",
                "F. Pedregosa",
                "P. van Mulbregt"
            ],
            "title": "SciPy 1.0: fundamental algorithms for scientific computing in Python",
            "venue": "Nature Methods 17(3),",
            "year": 2020
        },
        {
            "authors": [
                "J. O\u2019Rourke"
            ],
            "title": "Computational Geometry In C, 2nd edn",
            "year": 1998
        },
        {
            "authors": [
                "A. Paszke",
                "S. Gross",
                "F. Massa",
                "A. Lerer",
                "J. Bradbury",
                "G. Chanan",
                "T. Killeen",
                "Z. Lin",
                "N. Gimelshein",
                "L. Antiga",
                "A. Desmaison",
                "A. Kopf",
                "E. Yang",
                "Z. DeVito",
                "M. Raison",
                "A. Tejani",
                "S. Chilamkurthy",
                "B. Steiner",
                "L. Fang",
                "J. Bai",
                "S. Chintala"
            ],
            "title": "PyTorch: An Imperative Style, High- Performance Deep Learning Library",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "N. Karcher",
                "T. Franz"
            ],
            "title": "Adaptive sampling strategies for reduced-order modeling",
            "venue": "CEAS Aeronautical Journal",
            "year": 2022
        },
        {
            "authors": [
                "A.D. Jagtap",
                "G.E. Karniadakis"
            ],
            "title": "How important are activation functions in regression and classification? A survey, performance comparison, and future directions",
            "venue": "arXiv (2022)",
            "year": 2022
        },
        {
            "authors": [
                "C. Nwankpa",
                "W. Ijomah",
                "A. Gachagan",
                "S. Marshall"
            ],
            "title": "Activation Functions: Comparison of trends in Practice and Research for Deep Learning",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "Keywords: Model order reduction, shifted proper orthogonal decomposition, data-driven models, deep learning, artificial neural networks, wildland fires\n1\nar X\niv :2\n30 4.\n2"
        },
        {
            "heading": "Acknowledgement",
            "text": "We gratefully acknowledge the support of the Deutsche Forschungsgemeinschaft (DFG) as part of GRK2433 DAEDALUS. The authors were granted access to the HPC resources of IDRIS under allocation No. AD012A01664R1 attributed by Grand E\u0301quipement National de Calcul Intensif (GENCI). Centre de Calcul Intensif d\u2019Aix-Marseille is acknowledged for granting access to its high-performance computing resources. We also thank Tobias Breiten for his valuable comments and feedback."
        },
        {
            "heading": "1 Introduction",
            "text": "Forecasting the spread of forest fires has become an important aspect of civil protection [1]. The intensification of extreme droughts and heat waves due to climate change has increased the frequency and severity of large forest fires worldwide. Consequently, predicting the risk and identifying the causes behind these events has become crucial in comprehending the connection between climate and land surface and helping manage forest fires [2]. Numerous studies such as those referenced in [3\u20136] have been undertaken to model and forecast forest fires utilizing wildland fire models that can be simulated under various scenarios. Through this paper, we present a novel approach for predicting the spread of forest fires through a combination of dimensionality reduction and deep learning (DL).\nThe phenomena of wildland fires are usually governed by parametric non-linear time-dependent partial differential equations (PDEs) with different scenarios being achieved by changing physical parameters in the system of equations. However, solving such equations for modeling large-scale wildland fires for the numerous parameters is unaffordable in real-time emergency situations requiring significant computing power and time. To speed up the numerical simulations reduced-order modeling provides a promising strategy.\nReduced order modeling (ROM) for the parameterized PDEs usually relies on an offline-online computational splitting [7]. The expensive task of building the low-dimensional subspace from the full order model (FOM) snapshots is performed once in the so-called offline stage, and the reduced order model (ROM) approximation corresponding to any new parameter value is computed in the so-called online stage. Classical projection-based methods such as proper orthogonal decomposition (POD) perform the dimensionality reduction by constructing a low-dimensional trial subspace by the leading modes of a singular value decomposition (SVD) and then projecting the system dynamics onto the constructed low-dimensional subspace using Petrov-Galerkin methods [8, 9]. In many applications, namely with faster decay of the Kolmogorov n-width [10] (in turn the Hankel singular values [11]), a low-dimensional subspace with low approximation error if found with conventional methods like POD. But the problem arises when the dimension of the trial subspace needs to be large for a desired approximation quality. This is almost always the case\n3 with transport-dominated fluid systems (TDFS) like propagating flame fronts and traveling acoustic or shock waves [12\u201314].\nTo overcome the problem of slowly decaying singular values, model order reduction (MOR) methods for TDFS have emerged over the last couple of years. The work of [15] uses transport reversal where they do template fitting by posing a minimization problem. Along similar lines [16] tries to transform/twist the set of solutions so that the combination of a proper shift and appropriate linear combination recovers accurate approximation. Similar approaches are followed in [17, 18]. Transport maps were explored in [19], approximating the field variable with a front shape function and a level set function for efficient model reduction is explored in [12, 20]. In general, most of these methods rely on an offline-online splitting where the computationally expensive offline step comprises a non-linear projection framework to capture the high-dimensional space and the online step constructs the ROM approximation intrusively for any unseen parameter value. Intrusive parameter predictions have already been utilized for the wildland fire model in [21], where the shifted proper orthogonal decomposition (sPOD) [22] is combined with residual minimization [18, 23] to obtain an efficient ROM. More powerful formulations of the sPOD are discussed in [24, 25]. The sPOD has also been used in [26] for predicting the detonation rotation waves using UnTWIST algorithm and in [27] for the prediction of shifts in nonlinear hyperbolic equations.\nHowever, the non-linearity of the dimensional reduction for transport phenomena hinders the projection of even a linear PDE on a small basis, and thus it needs to be evaluated in the original dimension of the FOM in the online stage. This prevents any significant speedup during the simulation. In order to avoid scaling with the FOM dimension hyper-reduction techniques [28\u201330] have been developed, one such being discrete empirical interpolation method (DEIM) [31, 32] which approximates non-linear terms by evaluating them at a few, carefully selected interpolation points and approximating all other components via interpolation in a low-dimensional space. This method also serves as the starting point for further extension of the method called shifted-DEIM by [21] for treating the wildland fire model.\nAnother class of methods called online adaptive basis methods [33\u201336] have also been introduced to circumvent the problem of slow-decaying Kolmogorov n-width by exploiting the time-local low-rank structure of the TDFS. One such method called adaptive bases and adaptive sampling discrete empirical interpolation method (AADEIM) was introduced by Peherstorfer et.al (2020) in [33]. It builds upon the argument that the solutions of TDFS are typically low-dimensional if considered locally in time. This notion is then exploited by approximating the FOM solution in local low-dimensional spaces that are adapted via a low-rank basis updates over time [37, 38]. However, the problem with these adaptive versions is that they usually do not deliver speedups comparable to a traditional POD-DEIM approach, as shown for shallow water flows in [39]. To circumvent these problems, various non-intrusive methods have emerged in the recent past. These methods rely in general on projecting\n4 the high-dimensional dynamics onto a low-dimensional subspace usually by using the POD and then employing deep learning (DL) framework for modeling the reduced dynamics.\nA few attempts have been made to construct a low-dimensional subspace with artificial neural networks (ANNs) [23, 40], shallow masked encoder [41] and methods like kernel POD [42] as well which allow for a non-linear representation of the projected high-dimensional dynamics. Subsequently DL techniques such as feed-forward neural network (FNN) based regression model [42\u201346] and Gaussian process regression [47, 48] have been employed to model the reduced dynamics. Model order reduction techniques based on a deep convolutional autoencoder are proposed in [40, 49]. Other DL techniques in the literature include long short-term memory (LSTMs) [50] and recurrent neural networks (RNNs) [51].\nIn our research, we expand upon the non-intrusive parameter prediction framework introduced by Fresca et al. [52, 53]. This involves constructing a trial subspace using Proper Orthogonal Decomposition (POD) and modeling the reduced dynamics using a convolutional autoencoder coupled with a Feedforward Neural Network (FNN). However, instead of the POD, we employ the sPOD for dimensionality reduction, as it avoids the issue of slowly decaying singular values. On this base, we employ a deep FNN to learn the time and parameter-dependent amplitudes and shift transformations. This allows us to efficiently predict the states of the system for unseen parameter values. A similar technique has been presented in Kovarnova et al. [54], where they only predict in time. We apply our proposed approach to one- and two-dimensional wildland fire models, and present computational results to demonstrate its effectiveness.\nThe structure of the paper is as follows. In Section 2 we lay out the theoretical foundation concerning the mode decomposition methods, namely POD and sPOD along with the formulation describing the data-driven approach. A brief definition of the errors considered in our studies for model performance evaluation is also mentioned in Section 2. In Section 3 we present the numerical results for all the example test cases along with a timing study. Finally, the conclusions are drawn in Section 4, and auxiliary tests are provided in the Appendix."
        },
        {
            "heading": "2 Theory and background",
            "text": "In this section, we introduce the theoretical aspects of the presented methods. We study the relevant mode decomposition approaches followed by the datadriven approach. Furthermore, we scrutinize the types of errors that may arise from the implementation of the proposed method."
        },
        {
            "heading": "2.1 Mode decomposition",
            "text": "We specifically consider POD and sPOD for our analysis.\n5"
        },
        {
            "heading": "2.1.1 Proper Orthogonal Decomposition (POD)",
            "text": "POD is a method to extract optimal basis sets from a collection of snapshots. The snapshot matrix Q \u2208 RM\u00d7N comprises of snapshots q(xi, tj ,\u00b5j) arranged in a column-wise fashion for each time step tj where j = 1, . . . , N , xi \u2208 \u2126 are the spatial grid points inside the domain \u2126 and \u00b5j being the parameter dependency. The POD method approximates with the help of SVD the snapshot matrix Q Q \u2248 Q\u0303 = Ur\u03a3r(Vr)> . (1) Here, r min(N,M) is the truncation rank, \u03a3r = diag(\u03c31, . . . , \u03c3r) is a diagonal matrix containing the singular values \u03c31 \u2265 \u03c32 \u2265 . . . \u03c3r and Ur \u2208 RM\u00d7r, Vr \u2208 RN\u00d7r are orthogonal matrices containing the left and right singular vectors. The reconstruction is optimal in the sense that the time-averaged least-square error of the POD approximation\nr = \u2225\u2225\u2225Q\u2212 Q\u0303\u2225\u2225\u22252\nF = min(N,M)\u2211 k=r+1 \u03c3k (2)\nis minimized."
        },
        {
            "heading": "2.1.2 Shifted proper orthogonal decomposition (sPOD)",
            "text": "The sPOD was introduced in [22] and further algorithmic developments were presented in [24, 25] based on the optimization of singular values and in [18, 21] based on the optimization of dyadic pairs and their shifts. The sPOD method aims to decompose the snapshot matrix Q = [q(xi, tj ,\u00b5j)]ij \u2208 RM\u00d7N into multiple co-moving fields {Qk \u2208 RM\u00d7N}k=1,...,f (i.e. {qk(xi, tj ,\u00b5j)}k=1,...,f ). The decomposition follows as:\nq(x, t,\u00b5) \u2248 q\u0303(x, t,\u00b5) := f\u2211 k=1 T \u2206kqk(x, t,\u00b5) , (continuous) (3)\nQ \u2248 Q\u0303 := f\u2211 k=1 T\u2206 k Qk , (discrete) (4)\nThe interpolation based discrete transformation operators are given by {T\u2206k}k=1,...,f where \u2206k(t, \u00b5) = (\u2206k(x1, t,\u00b5), . . . ,\u2206k(xM , t,\u00b5)) is the time dependent shift such that T\u00b1\u2206 k\n(Q)ij = q(xi \u2213 \u2206k(xi, t,\u00b5)i, tj ,\u00b5j), and Q\u0303 is the approximate reconstruction of Q. The assumption is that for traveling wave systems the superposition (4) can decompose the data more efficiently than the POD. This is because the traveling wave is only changing slowly in the co-moving data frame and can therefore be well approximated with only a few modes calculated with a truncated SVD:\nQk \u2248 Ukrk\u03a3krk(V krk)> k = 1, . . . , f . (5)\n6 Here, rk N is the truncation rank of each co-moving field, \u03a3krk = diag(\u03c3k1 , . . . , \u03c3 k rk\n) is a diagonal matrix containing the singular values \u03c3k1 \u2265 \u03c3k2 \u2265 . . . \u03c3krk and U k rk \u2208 RM\u00d7rk , V krk \u2208 RN\u00d7rk are orthogonal matrices containing the left and right singular vectors. In this work we use the sPOD algorithm based on the minimization of the nuclear-one norm, which is presented in [25] algorithm 8, and the corresponding tuning parameters for the algorithm are stated in the Appendix C.\nRemark 1 In order to be more efficient than the POD, when disregarding additional degrees of freedom introduced by shifts, we assume that for the total number of\ndegrees of freedom r = \u2211f k=1 rk the truncation error of the sPOD is smaller than the POD truncation error (2)."
        },
        {
            "heading": "2.2 Data-driven approach",
            "text": "We consider a non-linear parameterized dynamical system which often stems from a discretized PDE system like:\nq\u0307(t,\u00b5) = f(t, q(t,\u00b5), \u00b5) t \u2208 [0, T ] q(0,\u00b5) = q0(\u00b5)\n(6)\nwhere, q \u2208 RM is the parameterized solution to the problem, q0 is the initial data, f is the non-linear function describing the system dynamics and \u00b5 is the parameter vector. For simplicity we only consider a single parameter \u00b5 for our study, however, real-world scenarios may depend upon multiple parameters."
        },
        {
            "heading": "2.2.1 Non-intrusive predictions with POD and sPOD",
            "text": "To develop the new method, we closely follow the POD-DL-ROM method described by Fresca et. al [53]. The said method first builds a low-dimensional subspace Col(Ur) which is spanned by the first r singular vectors of the parameter snapshot matrix Q:\nQ =  | | | |q(t1, \u00b51) . . . q(tNt , \u00b51) . . . q(t1, \u00b5Np) . . . q(tNt , \u00b5Np) | | | |  \u2208 RM\u00d7N . (7) The matrix Q is the collection of N = NpNt number of FOM snapshots computed for different parameter instances \u00b51, . . . , \u00b5Np that are sampled over different time instances t1, . . . , tNt . Using POD on Q and truncating to a rank r M , we have\nQ\u0303 = Ur\u03a3r(Vr) > where, A = \u03a3r(Vr) >\n7 and\nA =  | | |a(t1, \u00b51) a(t2, \u00b51) . . . . . . a(tNt , \u00b5Np) | | |  \u2208 Rr\u00d7N . (8) A deep learning model is used to approximate the mapping (t1, \u00b51) \u2192 a, by using A as training data along with P :\nP = [ (t1, \u00b51) . . . (tNt , \u00b51) . . . (t1, \u00b5Np) . . . (tNt , \u00b5Np) ] \u2208 R(n\u00b5+1)\u00d7N . (9)\nThe matrix P corresponds to n\u00b5 number of parameters along with the contribution of time t as an additional parameter thus making P \u2208 R(n\u00b5+1)\u00d7N . By this, the network learns the mapping from parameter and time to amplitude, which is the mapping traditionally constructed by the Galerkin approach. Subsequently, once the network is trained, the prediction step is performed in order to generate the time amplitude for any (t, \u00b5)\na(t, \u00b5) =  a1(t, \u00b5) a2(t, \u00b5)\n... ar(t, \u00b5)  \u2248 NPOD(t, \u00b5) (10) and the state q\u0303\u2032 for those unseen parameters is reconstructed as:\nq\u0303\u2032(t, \u00b5) = Ura(t, \u00b5) . (11)\nMore in-depth analysis of the POD-DL-ROM method along with the network architectures, working examples, and the error study can be found in [52, 53].\nSince the POD is not suitable for transport-dominated systems, we present a novel non-intrusive model order reduction technique named sPOD-NN that uses sPOD for constructing the desired low-dimensional subspace. The sPOD is applied on the parameter snapshot matrix Q from (7). The sPOD decomposes the snapshot matrix Q into co-moving frames Qk. The algorithm also outputs the basis vectors Ukrk for every frame with which the time amplitude matrix A k is extracted for all k = 1, . . . , f . Following (5) the time amplitudes are defined,\nAk = (Ukrk) > Qk \u2208 Rrk\u00d7N , (12)\nwhere\nAk =  | | |ak(t1, \u00b51) ak(t2, \u00b51) . . . . . . ak(tNt , \u00b5Np) | | |  . (13) We now have the time amplitude matrices Ak available for all k along with the shifts. However, we note that the shifts for any new unseen parameter set (t, \u00b5) are not available upfront. This poses a challenge because, in due course of reconstructing the final state for unseen parameter values, the shifts must be computed first.\n8 Shifts: The shifts \u2206k(t, \u00b5) = (\u2206k(x1, t, \u00b5), . . . ,\u2206 k(xM , t, \u00b5)) \u2208 RM , used to encode the transport, are part of the description. They are in general dependent on the model parameters \u00b5, time t and space x. The flame propagation speed changes with the Arrhenius factor for the wildland fire model, and it needs to be predicted by the neural network. To have a low-dimensional description, the description of the shifts needs to be low-dimensional as well. We basically study two possible scenarios, where the first can be seen as a special case of the second:\n(a) Low-dimensional shifts:\nWe look at a case where the shifts are independent of the spatial coordinates x \u2208 Rd:\n\u2206k(x, t, \u00b5) = nk\u2206\u2211 i=1 ei \u2206 k i (t, \u00b5) , where n k \u2206 \u2264 d . (14)\nHere, ei denotes the ith standard basis vector. For our one- and twodimensional examples d = 1, 2. Since nk\u2206 \u2264 d the coefficient vectors of the shifts \u2206k(t, \u00b5) = (\u2206k1(t, \u00b5), . . . ,\u2206\nk nk\u2206 (t, \u00b5)) are already low-dimensional\nand only depend on time and parameter. (b) Low-rank description of high-dimensional shifts:\nThe sPOD usually assumes a low-dimensional description of the shifts. However, for complicated systems, the shifts might depend on the spatial position x itself and are thus high-dimensional. Nevertheless, the shifts often have a low-rank structure and we thus assume, they can be well represented with the help of the POD:\n\u2206k(x, t, \u00b5) \u2248 nk\u2206\u2211 n=1 \u03a5kn(x)\u2206 k n(t, \u00b5) , with n k \u2206 N . (15)\nThus after we assemble the shift matrix [\u2206k(xi, tj , \u00b5j)]ij \u2208 RM\u00d7N using a threshold algorithm (see section 3.1.3), we decompose it with the help of the truncated SVD and obtain the low-dimensional shifts \u2206k(t, \u00b5) = (\u2206k1(t, \u00b5), . . . ,\u2206\nk nk\u2206 (t, \u00b5)).\nOnce we obtain the \u2206k(t, \u00b5) we construct the training data by stacking all\n9 Ak and \u2206k(t, \u00b5) to construct an A\u0302 \u2208 R( \u2211 k r k+ \u2211 k n k \u2206)\u00d7N matrix as shown:\nA\u0302 =  a1(t1, \u00b51) a 1(t2, \u00b51) . . . . . . a 1(tNt , \u00b5Np) ... ... ... af (t1, \u00b51) a f (t2, \u00b51) . . . . . . a f (tNt , \u00b5Np) \u22061(t1, \u00b51) \u2206 1(t2, \u00b51) . . . . . . \u2206 1(tNt , \u00b5Np) ... ... ...\n\u2206f (t1, \u00b51) \u2206 f (t2, \u00b51) . . . . . . \u2206 f (tNt , \u00b5Np)\n . (16)\nAs for training the deep learning model, we consider A\u0302 and the entries of P matrix from (9) as the training data. After the successful training of the network, the time amplitudes and the shifts are predicted for (t, \u00b5)\na\u0302(t, \u00b5) =  a1(t, \u00b5) ... af (t, \u00b5) \u22061(t, \u00b5) ...\n\u2206f (t, \u00b5)\n \u2248 NsPOD(t, \u00b5) . (17)\nBy reconstructing the shifts using (14) or (15) depending on the problem at hand the state q\u0303\u2032 for the unseen parameters can be reconstructed as\nq\u0303\u2032(t, \u00b5) = f\u2211 k=1 T\u2206 k(t,\u00b5) \u00b7 ( Ukrka k(t, \u00b5) ) . (18)\nFor later comparison studies, we define the degrees of freedom (ndof) as:\nndof =\n{ r, for POD-NN\u2211\nk r k + \u2211 k n k \u2206, for sPOD-NN\n(19)"
        },
        {
            "heading": "2.2.2 Interpolation method with sPOD",
            "text": "For comparison we introduce another approach based on sPOD which we call sPOD-I (Interpolation) which instead of using the DL techniques uses an interpolation-based method in the online phase. The sPOD-I method first extracts the P and A\u0302 matrices as explained in (9) and (16) respectively. Subsequently, it then uses the scipy [55] function scipy.interpolate.griddata() for interpolation. This function triangulates the input domain [56] and performs barycentric interpolation on each triangle to construct an interpolant. For an interpolation point (t, \u00b5) lying inside a triangle the interpolated value\n10\nis given as\na\u0302(t, \u00b5) = 3\u2211 i=1 \u03b1ia\u0302(ti, \u00b5i), (20)\nwhere \u03b1i are barycentric coordinates with \u2211 \u03b1i = 1. Once the time amplitudes and the shifts are obtained, the final state can be reconstructed as shown in (18)."
        },
        {
            "heading": "2.2.3 Network architecture",
            "text": "We employ a deep FNN for the predictions. We use PyTorch [57] for constructing the neural networks. For the architecture, we have an input layer, three hidden layers, and an output layer. The parameters of the network, the number of inputs and outputs which necessarily are the neurons for each layer are shown in Table 1. In the table, we have p as the input for the input layer of the network where p = n\u00b5 + 1 as described in (9). For our examples, we only consider a single parameter \u00b5 thus p = 2. The number of outputs u in the output layer is problem dependent and has been pointed out in the respective numerical examples. We basically rely on the L1 loss function, namely MAE\n(Mean Absolute Error):\nLMAE = { 1 ndof \u2016a(t, \u00b5)\u2212NPOD(t, \u00b5)\u20161 , for POD-NN\n1 ndof \u2016a\u0302(t, \u00b5)\u2212NsPOD(t, \u00b5)\u20161 , for sPOD-NN,\n(21)\nwhere ndof is defined in (19). For improving the prediction accuracy we use data scaling. In this work, we use Min-Max scaling to scale our training data to the interval [0, 1]. Let us consider A\u0302 from (16) and P from (9) which serve as the training data for our network. The scaling for the matrix P is given as:\nP scaledij =\nPij \u2212 min j=1,...,Ns (Pij)\nmax j=1,...,Ns (Pij)\u2212 min j=1,...,Ns\n(Pij) , for i = 1, . . . , n\u00b5 + 1 (22)\n11\nwhereas, for the matrix A\u0302 where the time amplitudes and the shifts are stacked together, the scaling is performed separately for both quantities as shown here:\nA\u0302scaled =  Aij\u2212min i,j (Aij) max i,j (Aij)\u2212min i,j (Aij)\n\u2206ij\u2212mini,j (\u2206ij)\nmax i,j (\u2206ij)\u2212mini,j (\u2206ij)\n (23)\nThe minimum and maximum values for all calculated quantities are stored and then used to rescale the prediction results back to their original scale. Neural network basics are explained in Appendix A in more detail."
        },
        {
            "heading": "2.3 Errors",
            "text": "Before we look at the numerical results it is crucial to define the errors incurred by the aforementioned approaches. Once the network is trained and the time amplitudes and the shifts are predicted, the errors are as follows: POD-NN:\nEPOD =\n\u2225\u2225\u2225Q\u2212 Q\u0303\u2225\u2225\u2225 F\n\u2016Q\u2016F , where Q\u0303 = Ur\u03a3r(Vr)\n> (24)\nEPOD\u2212NNtot = \u2016Q\u2212 UrANN\u2016F\n\u2016Q\u2016F (25)\nThe error EPOD\u2212NNtot is bounded by the POD truncation error E POD and the neural network prediction error. sPOD-NN:\nEsPOD =\n\u2225\u2225\u2225Q\u2212 Q\u0303\u2225\u2225\u2225 F\n\u2016Q\u2016F , where Q\u0303 \u2248 f\u2211 k=1 T\u2206 k Qk, (26)\nEsPOD\u2212NNtot =\n\u2225\u2225\u2225Q\u2212\u2211fk=1 T\u2206kNN (UkrkAkNN)\u2225\u2225\u2225 F\n\u2016Q\u2016F (27)\nThe final error EsPOD\u2212NNtot is bounded by the sPOD truncation error E sPOD, the neural network prediction error and the error which arises from the shift operations. sPOD-I:\nEsPOD\u2212Itot =\n\u2225\u2225\u2225Q\u2212\u2211fk=1 T\u2206kI (UkrkAkI )\u2225\u2225\u2225 F\n\u2016Q\u2016F (28)\nThe final error EsPOD\u2212Itot is bounded by the sPOD truncation error, the interpolation error, and the error due to shift operations.\n12\nWe also look at the error calculated at every time instance [53] to gain a comprehensive understanding of the performance of the method. Although this can be computed for all the three methods explained, we only focus on the sPOD-NN method for our analysis. For (tj , \u00b5) with (18) we have\nEsPOD\u2212NNj = |q(tj , \u00b5)\u2212 q\u0303\u2032(tj , \u00b5)|\u221a\n1 Nt \u2211Nt j=1 \u2016q(tj , \u00b5)\u2016 2 F\n(29)\nwhich basically gives us the error at each grid point for a particular (tj , \u00b5)."
        },
        {
            "heading": "3 Numerical results",
            "text": "In order to assess the effectiveness of the sPOD-NN method, we evaluate its numerical performance 1 by the run time and the errors outlined in Section 2. For a fair comparison, we also compare it with the POD-NN and sPOD-I methods, based on the aforementioned errors. The techniques are evaluated on 1D and 2D wildland fire models. For sPOD we use the algorithm 8 in [25]. The tuning parameters can be found in Appendix C.\nFor benchmarking, the methods were also checked on synthetically generated test data for which the results can be found in Appendix D. The importance of the test case can be inferred from the left plot in Figure 25 of Appendix D where we see that EsPOD\u2212Itot decays with the sPOD basis reconstruction error EsPOD with an increasing ndof . This implies that if the online phase is substantially accurate then the final error will be dominated only by the basis reconstruction error. This is certainly true for the sPOD-I but not so for the other methods for the given test case. This is by the design of the problem, where the time amplitudes belong to a polynomial space and the method sPOD-I interpolates using polynomials."
        },
        {
            "heading": "3.1 Wildland fire model",
            "text": "We use the model shown in [6]. Considering a domain \u2126 \u2208 Rd with d = 1, 2 and a finite time horizon T := [0, tf ] where 0 < tf < \u221e, we compute the temperature T and the fuel supply mass fraction S satisfying the coupled nonlinear PDE\n\u2202tT = \u2207 \u00b7 (k\u2207T )\u2212 v \u00b7 \u2207T + \u03b1(Sr(T, \u00b5, Ta)\u2212 \u03b3(T \u2212 Ta)), \u2202tS = \u2212S\u03b3Sr(T, \u00b5, Ta),\n(30)\nwhere \u03b3Sr(T, \u00b5, Ta) is called the reaction rate constant which is given by the modified Arrhenius law:\nr(T, \u00b5, Ta) := { exp(\u2212 \u00b5T\u2212Ta ), if T > Ta, 0, otherwise .\n(31)\n1The source code is available at https://github.com/MOR-transport/sPOD-NN-paper\n13\nVariables and their fixed values used in the model are shown in Table 2. For\nnotational convenience, we work with the relative temperature T \u2212Ta instead of the temperature which results in Ta = 0 for the rest of the study."
        },
        {
            "heading": "3.1.1 1D model",
            "text": "Here, we will motivate the application of the proposed method to perform nonlinear model order reduction of a one-dimensional wildland fire model. As an initial step, we solve the model equations (30) and (31) on a one-dimensional strip of length l = 1000m in a computational domain \u2126 = [0, 1000] for tf = 1400s yielding the time domain T = [0, 1400]. The velocity of wind v = 0. The time domain is chosen such that the traveling fronts never reach the boundaries. We consider the initial condition to be:\nT 0(x) := 1200exp ( \u2212 (x\u2212 l/2) 2\n200m2\n) and S0 \u2261 1. (32)\nThe value of Arrhenius coefficient \u00b5 = 558.49K is used for our simulation. A point to note is that we use \u00b5 as our parameter of choice for tuning the model. A lower \u00b5 results in a faster spread of the fire and a higher \u00b5 results in a slowly expanding fire. Periodic boundary conditions are used for simplicity. The spatial domain is decomposed into Nx = 3000 grid points making hx = 1/3m and discretized with finite difference method with 6th order central finite difference stencil. We solve the model for 6000 time steps. Time integration is done with the standard RK4 method for which dt is calculated with the help of CFL criteria:\ndt = cfl \u00b7\n\u221a h2x + h 2 y\nc (33)\nwhere we prescribe cfl = 0.7, hy = 0 and speed of sound c = 1. Subsequently, the obtained temperature and the supply mass fraction profiles are shown in Figure 1. In the temperature plot we observe, a localized\n14\nignition region from which flame fronts emanate and travel in opposite directions. This is a perfect example of traveling wave-type phenomena. The supply mass fraction varies in the range [0, 1] with the region left and right of the two traveling fronts respectively being still unburnt while the central region is completely burnt.\nFor constructing non-linear parametric ROMs we solve (30) and (31) for \u00b5 \u2208 {540K, 550K, 560K, 570K, 580K} and sample naively every 10th snapshot in time to construct a parametric snapshot matrix Q. More advanced sampling strategies can be found in [58]. With Nt = 6000/10 = 600 time instances, Np = 5 parameter instances of \u00b5 and M = Nx = 3000 the constructed matrix Q \u2208 RM\u00d7NtNp = R3000\u00d73000. Once the parameter snapshot matrix is assembled we perform dimensionality reduction by sPOD and obtain a low-dimensional representation of the data as shown in Figure 2. The number of co-moving frames is decided by the user and for this example it is chosen as 3 (one for each of the left and right traveling fronts and one for the initial ignition regime). The shifts for the frames required for sPOD are calculated by tracking the gradients of the field variable as explained in [21]. We note here that we only show the results for the temperature field for simplicity, however, the entire procedure can be replicated for the supply mass fraction as well, the results of which could be found in Appendix B. The motivation for using sPOD is substantiated by the basis reconstruction error results shown in Table 3. We observe that as the number of modes increases the drop in EsPOD is significant in comparison to EPOD. For eg. just for 16 + 10 + 16 = 42 modes we could get to EsPOD \u223c O(10\u22124) whereas EPOD \u223c O(10\u22122).\nAs a result of the dimensionality reduction we obtain the time amplitude matrix Ak from Qk as shown in (12) and we already have the shifts calculated\n15\nQ = T\u2206 1 Q1 + T\u2206 2 Q2 + T\u2206 3 Q3\n1We also have r = r1+r2+r3+2 where r1 and r3 are the ranks of the co-moving frames and r2 is the rank of the stationary frame and 2 accounts for two additional degrees of freedom controlling the shifts. For the comparison, we, therefore, use r modes for POD.\nfor all k. Following the procedure outlined in Section 2 for training the neural network model we assemble the time amplitude matrix A\u0302 \u2208 R42\u00d73000 and the parameter matrix P \u2208 R2\u00d73000. We only consider 2 shifts in the aforementioned assembly of the matrix A\u0302, as the shift for the second frame \u22062 = 0. The network is trained for Nepochs = 100000 with batch size Nb = 100. Early stopping criteria are imposed to prevent overfitting which stops the training if the validation loss does not decrease for 3000 consecutive epochs. For testing, we choose \u00b5 = [558.49K] and use the trained model to predict the time amplitudes and the shifts which are shown in Figure 3. For a more quantitative\n16\npicture, we look at the error estimates shown in Figure 4. In the first plot, we\nsee that as the ndof increases all three errors go down as expected. However, for both sPOD-NN and sPOD-I the Etot starts to stagnate after a certain ndof . We observe that when we increase the ndof the amount of new information added to the training data is not substantial. On the other hand, the parameter matrix P remains the same. Thus in turn the approximation capability of the network remains almost the same even while making the prediction task more and more difficult. This also seems to be the case for sPOD-I. For PODNN this effect can not be fully captured here although, slight stagnation could be seen towards the end of the curve. This effect is also mentioned in Fresca\n17\net.al (2021) [53]. sPOD-NN is able to reach to an EsPOD\u2212NNtot \u223c 0.037. We also study the error EsPOD\u2212NNj which is shown in the right plot of Figure 4 where we see max \u223c O(10\u22122) whereas mean < O(10\u22123). The full reconstructed results are plotted in Figure 5. We select two random time instances: one near the ignition and the other nearing the end of the time domain and the prediction results are shown for all the methods at these instances. We could observe that the POD-NN although being able to have EPOD\u2212NNtot \u223c 0.078, has spurious oscillations. The EPOD \u223c 0.06 thus we infer that the prediction accuracy for POD-NN is limited by the basis reconstruction error itself, which cannot be resolved unless more modes are added for the study. As for sPOD-NN and sPOD-I, we see minor oscillations near the sharp edges of the curve.\n3.1.2 2D model (without wind)\nThe model equations for the wildland fire can also be extended to a 2D case. We solve the equations (30) and (31) on a two-dimensional square of side length l = 500 in a computational domain \u2126 = [0, 500]\u00d7[0, 500] for tf = 1000s, yielding the time domain T = [0, 1000] such that the traveling fronts do not reach the boundaries. The velocity of wind v = (0, 0) results in no change in the topology of the fronts. The initial condition is given to be:\nT 0(x, y) := 1200exp ( \u2212 (\n(x\u2212 l/2)2 200m2 + (y \u2212 l/2)2 200m2\n)) and S0(x, y) \u2261 1.\n(34) Periodic boundary conditions are employed. The spatial domain is split into Nx = 500, Ny = 500 grid points making hx = 1, hy = 1 and discretized with finite difference method with 6th order central finite difference stencil. We solve the system for 1000 time steps. Time integration is done with standard RK4\n18\nscheme and the dt is computed by CFL condition shown in (33) where we prescribe cfl = 0.7 and c = 1. Similar to the 1D case we consider \u00b5 = 558.49K which also acts as a tuning parameter in the model reduction procedure. The temperature and the supply mass fraction profiles are shown in Figure 6.\nFor constructing non-linear parametric ROMs we follow the same steps as mentioned in the 1D case, solving for \u00b5 \u2208 {540K, 550K, 560K, 570K, 580K} and sampling every 10th snapshot in time to construct a parametric snapshot matrix Q. With Nt = 1000/10 = 100 time instances, Np = 5 parameter instances of \u00b5 and M = Nx \u2217 Ny = 250000 the constructed matrix Q \u2208 RM\u00d7NtNp = R250000\u00d7500. We then carry out the dimensionality reduction through sPOD. We, however, need the shifts prior to running the sPOD algorithm. To this end, we observe that the 2D problem is set up in a way that the developed flame front is perfectly circular throughout the domain. Because of this radial symmetry, we can transform the data from a Cartesian to a polar coordinate system which simplifies the problem significantly. For performing model reduction in polar coordinate system we have \u2206k = (\u2206kR,\u2206 k \u03b8) as shifts.\nLet us consider the Figure 7, the first plot shows the 1D cross-sectional views of the temperature profile at two different time instances. t = 1000s is the final instance that can be considered as the reference position. The \u2206 is\n19\ngiven as: \u2206 = x(t = 300s)\ufe38 \ufe37\ufe37 \ufe38\nR\n\u2212x(t = 1000s)\nThere is no dependency of the shifts on the \u03b8 coordinate. The \u2206 only depends upon R and t. The other two plots show the temperature profiles in the polar coordinate system for the two-time instances t = 300s and t = 1000s. With\n20\nsPOD, we now aim at decomposing the polar temperature field into two frames: the first one captures the traveling fronts, and the second one captures the initial ignition regime. For this we consider, \u22061R = \u2206, \u2206 1 \u03b8 = 0 and \u2206\n2 = (0, 0). The sPOD decomposition of the temperature is shown in Figure 8 in Cartesian coordinates.\n1We also have r = r1 + r2 + 1, where r1 is the rank of the moving frame, r2 is the rank of the stationary frame, and 1 accounts for one additional degree of freedom controlling the shift. For comparison, we therefore use r modes for the POD.\nThe basis reconstruction error results are shown in Table 4. We observe that as we keep on adding more modes for the reconstruction the drop in EsPOD is significant in comparison to EPOD. For eg. sPOD converges for 19 + 5 = 24 modes to EsPOD \u223c O(10\u22123) whereas EPOD \u223c O(10\u22122).\nAs a result of the sPOD, we obtain the time amplitudes for both the frames and we subsequently construct the time amplitude matrix A\u0302 and the parameter matrix P for training as shown in Section 2. For this problem A\u0302 \u2208 R25\u00d7500\n21\nand P \u2208 R2\u00d7500. The network is trained for Nepochs = 200000 with batch size Nb = 50. Early stopping criteria are imposed to prevent overfitting, which stops the training if the validation loss does not decrease for 4000 consecutive epochs. For testing, we choose \u00b5 = [558.49K] to predict the time amplitudes and the shifts which are shown in Figure 9. Quantitative error estimates are shown in Figure 10 where in the first plot we observe the similar stagnation behavior that was observed in the 1D wildland fire model case. The EsPOD\u2212NNtot \u223c 0.017. In the second plot, we see the error over the whole time interval EsPOD\u2212NNj where we see max < O(10\u22122) and the mean < O(10\u22124). Full reconstruction results are shown in Figure 11. Here we see the cross-sectional view of the temperature profile at two different time instances. We observe that the PODNN has spurious oscillations. The EPOD \u223c 0.089 and EPOD\u2212NNtot \u223c 0.098 thus we infer that the prediction accuracy for POD-NN is once again limited by the basis reconstruction error itself. As for sPOD-NN and sPOD-I, we see very minor oscillations near the sharp edges of the curve.\n3.1.3 2D model (with wind)\nWe consider the same model equations as in the previous case albeit with a minor modification in (30) where we prescribe v = (0.2 m/s, 0) which is the wind velocity applied only in the x direction. The two-dimensional square domain is kept the same as the previous case and the spatial domain is split into Nx = 500, Ny = 500 grid points making hx = 1, hy = 1 and discretized with finite difference method with 6th order central finite difference stencil. We solve the system for 500 time steps for tf = 500s. The temperature and the supply mass fraction profiles are shown in Figure 12.\nNon-linear parametric ROM is constructed by solving for \u00b5 \u2208 {540K, 550K, 560K, 570K, 580K} and sampling every 5th snapshot in time to construct a parametric snapshot matrix Q \u2208 R250000\u00d7500. The sPOD is subsequently applied for dimensionality reduction. On similar lines to the previous case, we convert our data to a polar coordinate system for simplicity. However, this time due to the changing topology of the flame front the calculation of shifts is no longer trivial. Similar to the previous case we have \u2206k = (\u2206kR,\u2206 k \u03b8) and \u22062 = (0, 0). However, \u22061R is now dependent on (t, \u03b8,R) for which we look at Figure 13 for detail. We see the edges of the fronts in the left plot for two different time instances from which we calculate the shifts for different values of \u03b8 as shown. With sPOD, we now aim at decomposing the polar temperature field into two frames: the first one captures the traveling fronts, and the second one captures the initial ignition regime. Subsequently we have \u22061R \u2208 R250000\u00d7N , \u22061\u03b8 = 0 where in \u22061R each column gives the amount of shift to be applied to all the grid points of the domain at instances of N . The sPOD decomposition of the temperature is shown in Figure 14 in Cartesian coordinates. The basis reconstruction error results are shown in Table 5. We could reach to EsPOD \u223c 0.01 with 17 + 9 = 26 modes whereas EPOD \u223c 0.06.\nsPOD gives us the time amplitudes for both the frames and we proceed to construct the time amplitude matrix A\u0302 for training as explained in Section 2.\n22\nThe shift truncation dimension n1\u2206 = 4 is chosen suitably keeping in mind the relative reconstruction error of the shift matrix shown in Figure 15. This is one particular case of extracting low-dimensional structure from the highdimensional shifts which is desired to proceed with the neural network training. The theory behind this is explained in detail in Section 2. For this problem A\u0302 \u2208 R30\u00d7500 and P \u2208 R2\u00d7500. The network parameters remain the same as the previous case with Nepochs = 200000 and batch size Nb = 50. Early stopping criteria are imposed to prevent overfitting after 4000 consecutive epochs. For\n23\n1We also have r = r1 + r2 + 4, where r1 is the rank of the moving frame, r2 is the rank of the stationary frame, and 4 accounts for the added number of amplitudes extracted from the shift matrix \u22061R. For comparison, we therefore use r modes for the POD.\nthe test parameter, \u00b5 = [558.49K] the time amplitude and the shift predictions are in line with the previous case. We also show the results of a parameter sweep study for the temperature for 16 different test samples where we sample the values for \u00b5 \u2208 [540, 580]. The result is shown in Figure 18. Error estimates are shown in Figure 16. In the first plot we observe the EsPOD\u2212NNtot \u223c 0.028. In the second plot, we see the error over the whole time interval EsPOD\u2212NNj where we see max < O(10\u22122). Full reconstruction results are shown in Figure 17. The cross-sectional views of the temperature profile at two different time instances are shown. We observe that the POD-NN is corrupted by oscillations but for sPOD-NN and sPOD-I we do not see any oscillations."
        },
        {
            "heading": "3.2 Timing study",
            "text": "Here we show the computational time analysis of the proposed methods. All the tests for computing the timing are run on Macbook Air M1(2020) with\n24\nan 8-core CPU and 16GB of RAM. We refer to Figure 19 where we see that for both 1D and 2D models, both sPOD-NN and sPOD-I methods are more than 100 times faster than the FOM for the multi-query scenario. However, this speedup becomes very large (more than 1000 times) for POD-NN. The reason for this could be explained by looking at the sub-steps of sPOD-NN and POD-NN. For the 1D model, the sPOD-NN timing consists of the time for evaluation of the neural network followed by that of transforming the stationary frames into co-moving frames which involves matrix-matrix multiplications for every frame and at last the time for adding the individual frames and producing the final snapshot. However, for POD-NN the timing only consists of evaluating the neural network and reconstructing the final snapshot with just one matrix-matrix multiplication operation. This difference translates into the 2D model as well except we have one more sub-step for sPOD-NN where the snapshot data has to be converted from a Cartesian coordinate system to a polar one for analysis and back again. This makes the difference in speedups\n25\nmore pronounced. The speedup shown in Figure 19 is defined as:\nSpeedup = tFOM tROM\n(35)\nwhere tROM is the time taken for the methods proposed in the paper. The tFOM = 5.2s for 1D model and 63.2s for the 2D model. Both the test cases for the 2D model described in the paper have similar computational time\n26\nconsumption. The comparison is made with the same ndof across all methods for a specific model. With the results shown, we substantiate our claims that not only is the proposed method accurate but is also extremely fast for the online phase. Just for the sake of completeness, the time study for the offline phase is shown in Table 6."
        },
        {
            "heading": "4 Conclusion",
            "text": "In this paper, we proposed a non-intrusive model reduction technique sPODNN for parametric transport-dominated systems. In particular, the technique uses sPOD for constructing the reduced basis and then subsequently extracts the time amplitudes for training a neural network for the offline phase. In the online phase, the trained model is then used to predict the time amplitudes at unseen parameter values. The core concept builds upon the ability\n27\nof sPOD, over the conventional methods like POD in constructing an optimal reduced basis for transport-dominated problems. For benchmarking we tested sPOD-NN against two other methods: sPOD-I and POD-NN. We assessed the computational performance and the prediction accuracy of the methods on non-linear time-dependent parametrized PDE systems: 1D and 2D wildland fire models. Through the numerical results shown in Section 3, we saw that sPOD-NN yielded accurate numerical approximations for the time amplitudes and the shifts and in turn the final snapshot. Along with the accurate predictions the proposed method provides substantial speedups in the online phase compared to the FOM. We also saw that the sPOD-NN outperforms POD-NN in all the examples presented owing to the ability of sPOD to better construct the optimal reduced basis which will almost always happen in transport-dominated systems. Also, based on the results we saw that the scope of improvement for sPOD-NN is huge compared to POD-NN as the total error is dominated by the neural network prediction error for sPOD-NN. Given more number of training samples the network prediction error could be further reduced.\nFor the scope of this paper, we considered models with either no wind presence or a constant unidirectional wind. Although for such test cases, the presented methods would suffice, they will have to be modified to handle more\n28\nrealistic, non-uniform, and shape-changing fronts [12]. Also, the wildland fire model considered here although is enough to capture the overall dynamics of the process broadly, more complex and detailed models could be considered as test cases for the proposed methods. As a future step, our aim is to study a more challenging test case where the front profiles change in a more complicated way as time progresses. In such cases, it becomes even more difficult to accurately compute the shifts from the snapshot data making it a perfect candidate to run our proposed methods on and substantiate their generality.\nCode and Data Availability\nIn order to facilitate reproducibility and transparency of the research presented in this paper, the source code used for the experiments and analyses is\n29\nmade publicly available. Interested parties can access the code at the following GitHub repository:\nhttps://github.com/MOR-transport/sPOD-NN-paper\nWe encourage researchers to make use of the code and to extend it for their own research purposes. In addition, the trained network-data is available upon request, and interested parties can contact the corresponding author for access.\nAuthor Contribution Statement (CRediT)\nIn the following, we declare the authors\u2019 contributions to this work.\nShubhaditya Burela: methodology, implementation of neural networks, computations, and visualizations, writing original draft, reviewing & editing Philipp Krah: initial concept, methodology, implementation of sPOD algorithm, reviewing & editing Julius Reiss: supervision, reviewing & editing"
        },
        {
            "heading": "A Neural network foundation",
            "text": "In recent years, non-intrusive model reduction methods that utilize neural networks have gained considerable attention. Despite advancements in the type and architecture of these networks, the core concept remains unchanged. In this section, we describe the specific neural network architecture used in the numerical examples presented in this work.\nFor the architecture, we use a deep FNN (Figure 20). For a clearer picture, consider a feed-forward network with L hidden layers where l \u2208 {1, . . . , L} is the index of the hidden layer, for such a setup the forward propagation is described as:\nz (l+1) i = w (l+1) i y (l) + b (l+1) i , y (l+1) i = f(z (l+1) i )\n(36)\nwhere y(l) is the vector of outputs from the layer l, z(l) is the vector of inputs into the layer l, W (l) is the weight matrix in the layer l along with a bias\n37\nvector for the layer l, b(l). The activation function is given by f(\u00b7). In the backpropagation step, the training of the network is usually carried out with a gradient descent algorithm minimizing a certain loss function J(W, b). As we deal with a regression problem in our work we basically rely on two types of loss functions, namely MAE (Mean Absolute Error) often referred to as L1 loss, and MSE (Mean Squared Error):\nMAE = 1\nn n\u2211 i=1 |yi \u2212 y\u0302i| , MSE = 1 n n\u2211 i=1 (yi \u2212 y\u0302i)2 (37)\nwhere yi and y\u0302i are the input and the target vectors respectively. The training data in our problems is heavily skewed and contains critical outliers that impact the physical significance of the problem. Using Mean Squared Error (MSE) as a loss function can penalize these outliers and produce subpar prediction results. To address this, we use Mean Absolute Error (MAE) as the preferred loss function due to its robustness in the presence of outliers. A stochastic gradient descent algorithm with momentum term is utilized for minimizing the loss function and updating the parameters W and b in every iteration as shown:\nW (l) ij \u2190W (l) ij \u2212 \u03b1\n\u2202\n\u2202W (l) ij\nJ(W, b),\nb (l) i \u2190 b (l) i \u2212 \u03b1\n\u2202\n\u2202b (l) i\nJ(W, b)\n(38)\n\u03b1 is the learning rate and can be tuned to get a better decay of the loss function. W (l) ij is an element of the weight matrix associated with the connection between a node j in layer l and a node i in layer l+ 1 and the term b (l) i is an element of the bias vector associated with the node i in layer l + 1. The Table 1 outlines\n38\nthe activation functions used. The first three layers use Exponential Linear Unit (ELU) activation, and the next layer uses Leaky Rectified Linear Unit (LeakyReLU). It is common in the literature to not use any activation function for the output layer in regression problems, which we also follow in this case. The detailed comparison of the different activation functions can be found in [59], [60]."
        },
        {
            "heading": "B Results for supply mass fraction",
            "text": "Here we present the results for the supply mass fraction for all the models presented in the text. Table 7, Table 8 and Table 9 show the offline and online errors for 1D, 2D(without wind) and 2D(with wind) models. Figure 21 shows the parameter sweep study for supply mass fraction where we test the model on the samples of \u00b5 \u2208 [540, 580]"
        },
        {
            "heading": "C sPOD parameters",
            "text": "As suggested by [21] we use the sPOD for decomposing the snapshot data of the wildland fire model. To determine the co-moving fields {Qk}k=1,...,f shown in (4) and an error term E \u2208 RM\u00d7N to capture noise, we solve the following\n39\nTable 9 Offline and online error study w.r.t number of modes for 2D (with wind) wildland fire model (supply mass fraction)\nOffline errors Online errors\nModes EsPOD EPOD EsPOD\u2212NNtot E sPOD\u2212I tot E POD\u2212NN tot\n1 + 1 7.43 \u00d7 10\u22122 3.67 \u00d7 10\u22122 7.15 \u00d7 10\u22122 7.13 \u00d7 10\u22122 3.53 \u00d7 10\u22122 2 + 2 2.95 \u00d7 10\u22122 2.92 \u00d7 10\u22122 2.09 \u00d7 10\u22122 2.08 \u00d7 10\u22122 2.73 \u00d7 10\u22122 4 + 3 1.71 \u00d7 10\u22122 2.23 \u00d7 10\u22122 1.82 \u00d7 10\u22122 1.82 \u00d7 10\u22122 2.17 \u00d7 10\u22122\n\u00b5\nE rr\nor\n545 550 555 560 565 570 575\n0.010\n0.015\n0.020\n0.025\nSupply mass fraction (1D without wind)\nEsPOD\u2212NNtot EsPOD\u2212Itot EPOD\u2212NNtot\n545 550 555 560 565 570 575\n0.010\n0.015\n0.020\n0.025\n0.030\nSupply mass fraction (2D without wind)\nEsPOD\u2212NNtot EsPOD\u2212Itot EPOD\u2212NNtot\n545 550 555 560 565 570 575\n0.016\n0.018\n0.020\n0.022\nSupply mass fraction (2D with wind)\nEsPOD\u2212NNtot EsPOD\u2212Itot EPOD\u2212NNtot\nFig. 21 Parameter sweep study for 1D and 2D supply mass fraction values\nconstraint optimization problem:\nmin Qk,E f\u2211 k=1 \u2225\u2225Qk\u2225\u2225\u2217 + \u03c2 \u2016E\u20161 s.t. Q = f\u2211 k=1 T\u2206 k (Qk) + E , (39)\nusing algorithm 8 in [25]. Here, \u2016E\u20161 = \u2211\nij |Eij | is not the usual matrix 1- norm, but the vector 1-norm of a long vector E \u2208 RM\u00d7N , \u2016A\u2016\u2217 = \u2211 i \u03c3i(A) is the Schatten one-norm and \u03c2 \u2265 0, \u03b7 > 0 are tuning parameters. For the 1D and 2D wildland fire model, we state \u03c2, \u03b7 in Table 10. It is important to note that once the computation of Qkk=1,...,f and E \u2208 RM\u00d7N is complete, E \u2208 RM\u00d7N is disregarded as it solely captures the data noise.\n40"
        },
        {
            "heading": "D Synthetic test case",
            "text": "As a proof of concept for the proposed method, we consider a 1D test case of traveling waves q1 and q2. We assume that both waves move along the path:\n\u22061(t, \u00b5) = \u00b5t, and \u22062(t, \u00b5) = \u2212\u00b5t (40)\nThe path is parameterized by the parameter \u00b5. We have \u00b5 \u2208 (\u00b51, . . . , \u00b5Np) \u2208 P \u2282 RNp which means that for different values of the parameter \u00b5 the paths of the traveling waves will change. We then assume a function q(x, t, \u00b5) which is constructed by the superposition of q1 and q2:\nq(x, t, \u00b5) = q1(x+ \u22061(t, \u00b5), t, \u00b5) + q2(x+ \u22062(t, \u00b5), t, \u00b5), (41)\nwhere (x, t, \u00b5) \u2208 [\u2212L/2, L/2[\u00d7[0, T [\u00d7P, with M number of grid points in the spatial domain, Nt time steps and Np instances of the parameter \u00b5. To be as close as possible to a realistic setting, we assume that q(x, t, \u00b5) is the solution of a high dimensional ODE resulting from a discretized PDE. We also assume that we only have access to the snapshot matrix Qi,j+Nt\u2217p = q(xi, tj , \u00b5p) and the paths \u2206k(tj) for Np parameters and where k is the number of individual waves being superposed to construct the solution. The individual waves for a single parameter instance \u00b5p are defined as:\nqk(x, t, \u00b5p) := ns\u22121\u2211 n=0 \u00b5p ( 1 + e\u22122nt ) cos (\u22122\u03c0t/T (n+ 1))hn(xk/w),\n\u3008hn, hm\u3009 = \u03b4n,m (42)\nwhere w = 0.015L and xk = x\u00b1 0.1L for k = 1, 2 respectively. Both q1 and q2 are defined in terms of dyadic pairs as shown in (42) as it enables us to tune the singular value spectra (ns being the number of singular values) in each frame. Furthermore, we choose Gauss-Hermite polynomials hn\nhn(x) = (\u22121)n\u221a 2nn! \u221a \u03c0 ex 2/2 d n dxn e\u2212x 2 = 1\u221a 2nn! \u221a \u03c0 Hn(x)e \u2212 12x 2\n(43)\nas they nicely mimic strongly localized wave structures (since w L). To build the data, we consider M = 500, Nt = 500 for L = 1 and T = 1, with \u00b5 \u2208 [0.1, 0.15, 0.2, 0.25, 0.3], k = 2, ns = 8. The superposed solution q and the\n41\nindividual stationary waves q1 and q2 for all the five different \u00b5 are shown in the three rows of Figure 22 respectively. Note that the pictures have the same structure Qi,j+Nt\u2217p = q(xi, tj , \u00b5p) as the snapshot matrices Q,Q 1, Q2.\nHowever, for further analysis, we assume that we have access to only Q for every \u00b5 so for M = 500, Nt = 500 and Np = 5 the collection of snapshots results in a matrix of size Q \u2208 RM\u00d7N = R500\u00d72500. In the first step, we apply sPOD on this matrix Q and obtain a low-dimensional representation. We let the algorithm run for optimal decomposition such that the EsPOD \u223c O(10\u22124). We use the ansatz Q \u2248 Q\u0303 = T\u22061Q1 +T\u22062Q2. The result of the decomposition is shown for \u00b50 in Figure 23. The low-dimensional description for each of the sPOD decomposed frames Qk for \u00b50 is given as:\nQk = qk(x, t, ~\u00b5) \u2248 rk\u2211 i=1 aki (t, ~\u00b5)\u03c6 k i (x), k = 1, 2 (44)\nThe time amplitude matrix Ak can be extracted as shown in (12) and also we already have access to the shifts for all k. Thus for training the neural network model, we assemble the time amplitude matrix A\u0302 and the parameter matrix P as shown in (16) and (9). For our problem A\u0302 \u2208 R18\u00d72500 and P \u2208 R2\u00d72500. We run the training loop forNepochs = 150000 with batch sizeNb = 50. For testing, we choose \u00b5 = [0.23] and use the trained model to predict the time amplitudes\n42\nand the shifts which are shown in Figure 24. We assess the prediction accuracy of the proposed methods through Figure 25. In the first plot, we see that with an increasing number of modes the EsPOD decreases rapidly compared to EPOD which substantiates our claim about using sPOD for basis construction. Here, it needs to be pointed out that Etot is bounded from below by the basis reconstruction error behavior thus we see that EPOD\u2212NNtot is always higher than\n43\nEPOD. Same is true for EsPOD\u2212NNtot . It is also noted that the prediction accuracy of the sPOD-NN is an order of magnitude better than POD-NN. Surprisingly enough for this particular setting, the neural network is not able to learn the POD modes at all. We see that with an increasing ndof the E POD\u2212NN tot remains very near to 1. The performance for the POD-NN could improve if we add more parameter samples to the training data. However, in this test case, the sPOD-I performs the best. It predicts nearly at the limit of EsPOD, without incurring any extra error in the interpolation step. This can be attributed to the fact that the basis functions for the aforementioned problem are polynomials by design and the sPOD-I uses polynomials for interpolation.\nWe can get more insight into the prediction accuracy of sPOD-NN by looking at the second plot in Figure 25. It shows the maximum, minimum, and mean of the relative error E(t, \u00b5) for all the time instances. We see that the maximum error recorded throughout the time frame is around \u223c 0.06 however, the mean error is always \u223c O(10\u22123). We now look at the final reconstructed\ndata in Figure 26. We observe that the profile with QPOD\u2212NN is completely distorted, which makes sense when we look at the errors shown in Figure 25. However, there is only a minor visible difference in the profiles of sPOD-NN and sPOD-I compared to the original.\n44"
        }
    ],
    "title": "Parametric model order reduction for a wildland fire model via the shifted POD based deep learning method",
    "year": 2023
}