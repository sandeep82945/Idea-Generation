{
    "abstractText": "Transfer learning has been a popular learning paradigm in the deep learning era, especially in annotationinsufficient scenarios. Better ImageNet pre-trained models have been demonstrated, from the perspective of architecture, by previous research to have better transferability to downstream tasks[28]. However, in this paper, we find that during the same pre-training process, models at middle epochs, which are inadequately pre-trained, can outperform fully trained models when used as feature extractors (FE), while the fine-tuning (FT) performance still grows with the source performance. This reveals that there is not a solid positive correlation between top-1 accuracy on ImageNet and the transferring result on target data. Based on the contradictory phenomenon between FE and FT that a better feature extractor fails to be fine-tuned better accordingly, we conduct comprehensive analyses on features before the softmax layer to provide insightful explanations. Our discoveries suggest that, during pre-training, models tend to first learn spectral components corresponding to large singular values and the residual components contribute more when fine-tuning.",
    "authors": [
        {
            "affiliations": [],
            "name": "Andong Deng"
        },
        {
            "affiliations": [],
            "name": "Xingjian Li"
        },
        {
            "affiliations": [],
            "name": "Di Hu"
        },
        {
            "affiliations": [],
            "name": "Tianyang Wang"
        },
        {
            "affiliations": [],
            "name": "Haoyi Xiong"
        },
        {
            "affiliations": [],
            "name": "Cheng-Zhong Xu"
        }
    ],
    "id": "SP:d0c80c39202553b7521a915e7d9c4a81119fbd4c",
    "references": [
        {
            "authors": [
                "Peter Anderson",
                "Xiaodong He",
                "Chris Buehler",
                "Damien Teney",
                "Mark Johnson",
                "Stephen Gould",
                "Lei Zhang"
            ],
            "title": "Bottom-up and top-down attention for image captioning and visual question answering",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Devansh Arpit",
                "Stanis\u0142aw Jastrz\u0119bski",
                "Nicolas Ballas",
                "David Krueger",
                "Emmanuel Bengio",
                "Maxinder S. Kanwal",
                "Tegan Maharaj",
                "Asja Fischer",
                "Aaron Courville",
                "Yoshua Bengio",
                "Simon Lacoste-Julien"
            ],
            "title": "A closer look at memorization in deep networks",
            "venue": "Proceedings of the 34th International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Lukas Bossard",
                "Matthieu Guillaumin",
                "Luc Van Gool"
            ],
            "title": "Food-101 \u2013 mining discriminative components with random forests",
            "venue": "In European Conference on Computer Vision,",
            "year": 2014
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Mathilde Caron",
                "Ishan Misra",
                "Julien Mairal",
                "Priya Goyal",
                "Piotr Bojanowski",
                "Armand Joulin"
            ],
            "title": "Unsupervised learning of visual features by contrasting cluster assignments",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton"
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "In International conference on machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "Xinlei Chen",
                "Haoqi Fan",
                "Ross Girshick",
                "Kaiming He"
            ],
            "title": "Improved baselines with momentum contrastive learning",
            "venue": "arXiv preprint arXiv:2003.04297,",
            "year": 2020
        },
        {
            "authors": [
                "Xinlei Chen",
                "Kaiming He"
            ],
            "title": "Exploring simple siamese representation learning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Xinyang Chen",
                "Sinan Wang",
                "Bo Fu",
                "Mingsheng Long",
                "Jianmin Wang"
            ],
            "title": "Catastrophic forgetting meets negative transfer: Batch spectral shrinkage for safe transfer learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Mircea Cimpoi",
                "Subhransu Maji",
                "Iasonas Kokkinos",
                "Sammy Mohamed",
                "Andrea Vedaldi"
            ],
            "title": "Describing textures in the wild",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2014
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805,",
            "year": 2018
        },
        {
            "authors": [
                "Dumitru Erhan",
                "Aaron Courville",
                "Yoshua Bengio",
                "Pascal Vincent"
            ],
            "title": "Why does unsupervised pre-training help deep learning",
            "venue": "In Proceedings of the thirteenth international conference on artificial intelligence and statistics,",
            "year": 2010
        },
        {
            "authors": [
                "Dumitru Erhan",
                "Pierre-Antoine Manzagol",
                "Yoshua Bengio",
                "Samy Bengio",
                "Pascal Vincent"
            ],
            "title": "The difficulty of training deep architectures and the effect of unsupervised pretraining",
            "venue": "In Artificial Intelligence and Statistics,",
            "year": 2009
        },
        {
            "authors": [
                "Spyros Gidaris",
                "Praveer Singh",
                "Nikos Komodakis"
            ],
            "title": "Unsupervised representation learning by predicting image rotations",
            "venue": "arXiv preprint arXiv:1803.07728,",
            "year": 2018
        },
        {
            "authors": [
                "Gregory Griffin",
                "Alex Holub",
                "Pietro Perona"
            ],
            "title": "Caltech-256 object category dataset",
            "year": 2007
        },
        {
            "authors": [
                "Jean-Bastien Grill",
                "Florian Strub",
                "Florent Altch\u00e9",
                "Corentin Tallec",
                "Pierre Richemond",
                "Elena Buchatskaya",
                "Carl Doersch",
                "Bernardo Avila Pires",
                "Zhaohan Guo",
                "Mohammad Gheshlaghi Azar"
            ],
            "title": "Bootstrap your own latent-a new approach to self-supervised learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Kaiming He",
                "Haoqi Fan",
                "Yuxin Wu",
                "Saining Xie",
                "Ross Girshick"
            ],
            "title": "Momentum contrast for unsupervised visual representation learning",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Kaiming He",
                "Ross Girshick",
                "Piotr Dollar"
            ],
            "title": "Rethinking imagenet pre-training",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2019
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Kimin Lee",
                "Mantas Mazeika"
            ],
            "title": "Using pre-training can improve model robustness and uncertainty",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Minyoung Huh",
                "Pulkit Agrawal",
                "Alexei A. Efros"
            ],
            "title": "What makes imagenet good for transfer learning",
            "year": 2016
        },
        {
            "authors": [
                "Longlong Jing",
                "Yingli Tian"
            ],
            "title": "Self-supervised visual feature learning with deep neural networks: A survey",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Dimitris Kalimeris",
                "Gal Kaplun",
                "Preetum Nakkiran",
                "Benjamin Edelman",
                "Tristan Yang",
                "Boaz Barak",
                "Haofeng Zhang"
            ],
            "title": "Sgd on neural networks learns functions of increasing complexity",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Maurice G Kendall"
            ],
            "title": "A new measure of rank correlation",
            "venue": "Biometrika, 30(1/2):81\u201393,",
            "year": 1938
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Simon Kornblith",
                "Jonathon Shlens",
                "Quoc V. Le"
            ],
            "title": "Do better imagenet models transfer better",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019",
            "year": 2019
        },
        {
            "authors": [
                "Jonathan Krause",
                "Michael Stark",
                "Jia Deng",
                "Li Fei-Fei"
            ],
            "title": "3d object representations for fine-grained categorization",
            "venue": "In 4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13), Sydney, Australia,",
            "year": 2013
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "year": 2009
        },
        {
            "authors": [
                "Yann LeCun",
                "L\u00e9on Bottou",
                "Yoshua Bengio",
                "Patrick Haffner"
            ],
            "title": "Gradient-based learning applied to document recognition",
            "venue": "Proceedings of the IEEE,",
            "year": 1998
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Silvio Savarese",
                "Steven Hoi"
            ],
            "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
            "venue": "arXiv preprint arXiv:2301.12597,",
            "year": 2023
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Caiming Xiong",
                "Steven Hoi"
            ],
            "title": "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Xingjian Li",
                "Haoyi Xiong",
                "Haozhe An",
                "Cheng-Zhong Xu",
                "Dejing Dou"
            ],
            "title": "Rifle: Backpropagation in depth for deep transfer learning through re-initializing the fully-connected layer",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Xingjian Li",
                "Haoyi Xiong",
                "Hanchao Wang",
                "Yuxuan Rao",
                "Liping Liu",
                "Zeyu Chen",
                "Jun Huan"
            ],
            "title": "Delta: Deep learning transfer using feature map with attention for convolutional networks",
            "venue": "arXiv preprint arXiv:1901.09229,",
            "year": 1901
        },
        {
            "authors": [
                "Yuanzhi Li",
                "Colin Wei",
                "Tengyu Ma"
            ],
            "title": "Towards explaining the regularization effect of initial large learning rate in training neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C Lawrence Zitnick"
            ],
            "title": "Microsoft coco: Common objects in context",
            "venue": "In Computer Vision\u2013ECCV 2014: 13th European Conference,",
            "year": 2014
        },
        {
            "authors": [
                "UK Lopes",
                "Jo\u00e3o Francisco Valiati"
            ],
            "title": "Pre-trained convolutional neural networks as feature extractors for tuberculosis detection",
            "venue": "Computers in biology and medicine,",
            "year": 2017
        },
        {
            "authors": [
                "Jiasen Lu",
                "Dhruv Batra",
                "Devi Parikh",
                "Stefan Lee"
            ],
            "title": "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Tao Luo",
                "Zheng Ma",
                "Zhi-Qin John Xu",
                "Yaoyu Zhang"
            ],
            "title": "Theory of the frequency principle for general deep neural networks",
            "venue": "arXiv preprint arXiv:1906.09235,",
            "year": 1906
        },
        {
            "authors": [
                "Subhransu Maji",
                "Esa Rahtu",
                "Juho Kannala",
                "Matthew Blaschko",
                "Andrea Vedaldi"
            ],
            "title": "Fine-grained visual classification of aircraft",
            "venue": "arXiv preprint arXiv:1306.5151,",
            "year": 2013
        },
        {
            "authors": [
                "Karttikeya Mangalam",
                "Vinay Uday Prabhu"
            ],
            "title": "Do deep neural networks learn shallow learnable examples first? 2019",
            "year": 2019
        },
        {
            "authors": [
                "Dimitrios Marmanis",
                "Mihai Datcu",
                "Thomas Esch",
                "Uwe Stilla"
            ],
            "title": "Deep learning earth observation classification using imagenet pretrained networks",
            "venue": "IEEE Geoscience and Remote Sensing Letters,",
            "year": 2015
        },
        {
            "authors": [
                "Ishan Misra",
                "C Lawrence Zitnick",
                "Martial Hebert"
            ],
            "title": "Shuffle and learn: unsupervised learning using temporal order verification",
            "venue": "In European Conference on Computer Vision,",
            "year": 2016
        },
        {
            "authors": [
                "Cuong Nguyen",
                "Tal Hassner",
                "Matthias Seeger",
                "Cedric Archambeau"
            ],
            "title": "Leep: A new measure to evaluate transferability of learned representations",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Maria-Elena Nilsback",
                "Andrew Zisserman"
            ],
            "title": "Automated flower classification over a large number of classes",
            "venue": "Sixth Indian Conference on Computer Vision, Graphics & Image Processing,",
            "year": 2008
        },
        {
            "authors": [
                "Mehdi Noroozi",
                "Paolo Favaro"
            ],
            "title": "Unsupervised learning of visual representations by solving jigsaw puzzles",
            "venue": "In European conference on computer vision,",
            "year": 2016
        },
        {
            "authors": [
                "Aaron van den Oord",
                "Yazhe Li",
                "Oriol Vinyals"
            ],
            "title": "Representation learning with contrastive predictive coding",
            "venue": "arXiv preprint arXiv:1807.03748,",
            "year": 2018
        },
        {
            "authors": [
                "Lutz Prechelt"
            ],
            "title": "Early stopping-but when? In Neural Networks: Tricks of the trade, pages 55\u201369",
            "year": 1998
        },
        {
            "authors": [
                "Hang Qi",
                "Matthew Brown",
                "David G Lowe"
            ],
            "title": "Low-shot learning with imprinted weights",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Ariadna Quattoni",
                "Antonio Torralba"
            ],
            "title": "Recognizing indoor scenes",
            "venue": "IEEE conference on computer vision and pattern recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Maithra Raghu",
                "Chiyuan Zhang",
                "Jon Kleinberg",
                "Samy Bengio"
            ],
            "title": "Transfusion: Understanding transfer learning for medical imaging",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Muhammad Rahman",
                "Yongzhong Cao",
                "Xiaobing Sun",
                "Bin Li",
                "Yameng Hao"
            ],
            "title": "Deep pre-trained networks as a feature extractor with xgboost to detect tuberculosis from chest xray",
            "venue": "Computers & Electrical Engineering,",
            "year": 2021
        },
        {
            "authors": [
                "Sivaramakrishnan Rajaraman",
                "Sameer K Antani",
                "Mahdieh Poostchi",
                "Kamolrat Silamut",
                "Md A Hossain",
                "Richard J Maude",
                "Stefan Jaeger",
                "George R Thoma"
            ],
            "title": "Pre-trained convolutional neural networks as feature extractors toward improved malaria parasite detection in thin blood smear",
            "venue": "images. PeerJ,",
            "year": 2018
        },
        {
            "authors": [
                "Hadi Salman",
                "Andrew Ilyas",
                "Logan Engstrom",
                "Ashish Kapoor",
                "Aleksander Madry"
            ],
            "title": "Do adversarially robust imagenet models transfer better",
            "venue": "In ArXiv preprint arXiv:2007.08489,",
            "year": 2007
        },
        {
            "authors": [
                "Steffen Schneider",
                "Alexei Baevski",
                "Ronan Collobert",
                "Michael Auli"
            ],
            "title": "wav2vec: Unsupervised pre-training for speech recognition",
            "venue": "arXiv preprint arXiv:1904.05862,",
            "year": 1904
        },
        {
            "authors": [
                "Yale Song",
                "Mohammad Soleymani"
            ],
            "title": "Polysemous visualsemantic embedding for cross-modal retrieval",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Yonglong Tian",
                "Dilip Krishnan",
                "Phillip Isola"
            ],
            "title": "Contrastive multiview coding",
            "venue": "In European conference on computer vision,",
            "year": 2020
        },
        {
            "authors": [
                "Yapeng Tian",
                "Jing Shi",
                "Bochen Li",
                "Zhiyao Duan",
                "Chenliang Xu"
            ],
            "title": "Audio-visual event localization in unconstrained videos",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Anh T Tran",
                "Cuong V Nguyen",
                "Tal Hassner"
            ],
            "title": "Transferability and hardness of supervised classification tasks",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Francisco Utrera",
                "Evan Kravitz",
                "N. Benjamin Erichson",
                "Rajiv Khanna",
                "Michael W. Mahoney"
            ],
            "title": "Adversarially-trained deep nets transfer better: Illustration on image classification",
            "venue": "In ArXiv preprint arXiv:2007.05869,",
            "year": 2007
        },
        {
            "authors": [
                "Laurens Van der Maaten",
                "Geoffrey Hinton"
            ],
            "title": "Visualizing data using t-sne",
            "venue": "Journal of machine learning research,",
            "year": 2008
        },
        {
            "authors": [
                "C. Wah",
                "S. Branson",
                "P. Welinder",
                "P. Perona",
                "S. Belongie"
            ],
            "title": "The Caltech-UCSD Birds-200-2011 Dataset",
            "venue": "Technical Report CNS-TR-2011-001, California Institute of Technology,",
            "year": 2011
        },
        {
            "authors": [
                "Zirui Wang",
                "Zihang Dai",
                "Barnab\u00e1s P\u00f3czos",
                "Jaime Carbonell"
            ],
            "title": "Characterizing and avoiding negative transfer",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Fangyun Wei",
                "Yue Gao",
                "Zhirong Wu",
                "Han Hu",
                "Stephen Lin"
            ],
            "title": "Aligning pretraining for detection via object-level contrastive learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Enze Xie",
                "Jian Ding",
                "Wenhai Wang",
                "Xiaohang Zhan",
                "Hang Xu",
                "Peize Sun",
                "Zhenguo Li",
                "Ping Luo"
            ],
            "title": "Detco: Unsupervised contrastive learning for object detection",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Zhi-Qin John Xu",
                "Yaoyu Zhang",
                "Tao Luo",
                "Yanyang Xiao",
                "Zheng Ma"
            ],
            "title": "Frequency principle: Fourier analysis sheds light on deep neural networks",
            "venue": "arXiv preprint arXiv:1901.06523,",
            "year": 1901
        },
        {
            "authors": [
                "Zhi-Qin John Xu",
                "Yaoyu Zhang",
                "Yanyang Xiao"
            ],
            "title": "Training behavior of deep neural network in frequency domain",
            "venue": "In International Conference on Neural Information Processing,",
            "year": 2019
        },
        {
            "authors": [
                "LI Xuhong",
                "Yves Grandvalet",
                "Franck Davoine"
            ],
            "title": "Explicit inductive bias for transfer learning with convolutional networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Ceyuan Yang",
                "Zhirong Wu",
                "Bolei Zhou",
                "Stephen Lin"
            ],
            "title": "Instance localization for self-supervised detection pretraining",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Zhilin Yang",
                "Zihang Dai",
                "Yiming Yang",
                "Jaime Carbonell",
                "Russ R Salakhutdinov",
                "Quoc V Le"
            ],
            "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Yuan Yao",
                "Lorenzo Rosasco",
                "Andrea Caponnetto"
            ],
            "title": "On early stopping in gradient descent learning",
            "venue": "Constructive Approximation,",
            "year": 2007
        },
        {
            "authors": [
                "Jason Yosinski",
                "Jeff Clune",
                "Yoshua Bengio",
                "Hod Lipson"
            ],
            "title": "How transferable are features in deep neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2014
        },
        {
            "authors": [
                "Kaichao You",
                "Yong Liu",
                "Jianmin Wang",
                "Mingsheng Long"
            ],
            "title": "Logme: Practical assessment of pre-trained models for transfer learning",
            "venue": "In ICML,",
            "year": 2021
        },
        {
            "authors": [
                "Shoubin Yu",
                "Jaemin Cho",
                "Prateek Yadav",
                "Mohit Bansal"
            ],
            "title": "Self-chained image-language model for video localization and question answering",
            "venue": "arXiv preprint arXiv:2305.06988,",
            "year": 2023
        },
        {
            "authors": [
                "Li Yuan",
                "Yunpeng Chen",
                "Tao Wang",
                "Weihao Yu",
                "Yujun Shi",
                "Zi-Hang Jiang",
                "Francis EH Tay",
                "Jiashi Feng",
                "Shuicheng Yan"
            ],
            "title": "Tokens-to-token vit: Training vision transformers from scratch on imagenet",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Jure Zbontar",
                "Li Jing",
                "Ishan Misra",
                "Yann LeCun",
                "St\u00e9phane Deny. Barlow"
            ],
            "title": "twins: Self-supervised learning via redundancy reduction",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Matthew D Zeiler",
                "Rob Fergus"
            ],
            "title": "Visualizing and understanding convolutional networks",
            "venue": "In European conference on computer vision,",
            "year": 2014
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Felix Wu",
                "Arzoo Katiyar",
                "Kilian Q Weinberger",
                "Yoav Artzi"
            ],
            "title": "Revisiting few-sample bert finetuning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Deyao Zhu",
                "Jun Chen",
                "Xiaoqian Shen",
                "Xiang Li",
                "Mohamed Elhoseiny"
            ],
            "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
            "venue": "arXiv preprint arXiv:2304.10592,",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Deep learning has achieved tremendous success in modern computer vision with the aid of the strong supervision of well-labeled datasets, such as ImageNet[11]. However, data annotation is notoriously labor-extensive and timeconsuming, especially in some specific domains where expertise is highly required. In such scenarios, transfer learning is of great interest for practitioners to train deep models with a small labeled dataset. Fortunately, existing efforts observe that when training on large-scale datasets, middle features of DNNs exhibit remarkable transferability to various downstream tasks [81, 76]. This facilitates\n\u2020Equal contribution. Work is partly done when Andong was an intern at Baidu Research. *Corresponding author.\npopular deep transfer learning paradigms of fine-tuning a pre-trained model (FT) or simply employing the pre-trained model as a feature extractor (FE). With relatively sufficient labeled examples, fine-tuning the whole network usually achieves higher performance. Despite this, FE is still important when training resources are limited, or end-toend training is not feasible. For example, some applications combine DNN features and other handcrafted features to obtain both accurate and explainable shallow classifiers [38, 56, 55].\nDespite the ubiquitous utilization of pre-trained models, it still remains mysterious how such models benefit transfer learning. Several works pioneer to explore this plausible yet essential problem. [28] systematically investigates whether better-performing models on source tasks, e.g. ImageNet, necessarily yield better performances on downstream tasks.\nar X\niv :2\n20 3.\n04 66\n8v 3\n[ cs\n.C V\n] 1\n7 A\nug 2\n02 3\nThey confirm this hypothesis for both FE and FT, over deep architectures with different capacities. However, recent works in the domain of adversarial training discover that an adversarially pre-trained model, though performs worse on ImageNet due to additional adversarial regularizations, can still transfer better than its natural (following the naming practice in [64], referring to pre-training without adversarial methods) counterpart (with the same architecture)[58, 64]. In fact, these discoveries are to some extent in contradiction to the findings in [28], which argues that worse source models may transfer better.\nOur work investigates the influence of pre-training on transfer effects from a different perspective. Specifically, we focus on the trajectory of the pre-training process, inspired by recent studies on the learning order of DNNs. Several works [2, 25, 36, 42] discover that DNNs tend to firstly learn simple and shallow features, e.g. colors and textures, which are regarded as more general and transferable across different data domains [81]. From the perspective of the frequency domain, such features lie in low-frequency spectrums. On the other hand, several other works reveal that high-frequency features obtained by a pre-trained model are likely to cause a negative transfer [9].\nThe aforementioned observations motivate a question that, does a fully pre-trained model definitely outperform its inadequately pre-trained version when transferring to target tasks (according to claims in [28] ), or is there an intermediate pre-trained checkpoint that yields a better transfer effect than that of the fully pre-trained version? To our best knowledge, very little work manages to explore how the transferability of a model is impacted by the different stages in a pre-training process.\nTo investigate this question, we run a toy experiment using CIFAR10 as the source dataset and a subset of MNIST (we randomly choose 100 data points for each digit from the official training split, resulting in a 1000-sample training set) as the target. Briefly, we train a ResNet-18[21] on CIFAR10 for 200 epochs and choose a set of checkpoints to run transfer learning in two different settings. In one setting, we treat the pre-trained model as a feature extractor (FE) and only retrain a softmax classifier, while in the other we fine-tune the whole model (FT). The retraining or fine-tuning continues for 100 epochs on the target dataset. As shown in Figure 1, the best performance of FE comes from the early 5th-epoch model, while the FT performance is higher for later checkpoints.\nTwo counter-intuitive facts can be observed from our results. One is that, a pre-trained model with higher accuracy on the source task is not necessarily better on the target task, especially when used as a feature extractor (FE). Among the checkpoints on the pre-training trajectory, there is no positive correlation between the source and target accuracy. The other observation shows inconsistent behaviors between FE\nand FT, indicating that a good starting point (FE) does not guarantee a good final result (FT). In order to explain the observed phenomenons, we investigate the spectral components of deep features before the FC layer (in Section 4.4), and observe that different parts of components contribute diversely for different pre-trained checkpoints within the same pre-training process.\nIn this paper, we conduct extensive transfer learning experiments, including ImageNet and the other 9 benchmark datasets. The results suggest that, when retraining a new classifier on top of the features extracted from pretrained models, inadequately pre-trained ImageNet models yield significantly better performance than that of the standard 90-epoch pre-trained version, but the performance still highly correlates with the source performance when finetuning. Further, we present insightful analyses to explain such a difference from the perspective of spectral components of the extracted features and find that there are specific components corresponding to pre-trained models at different pre-training stages. In summary, our main contributions are as follows:\n\u2022 Our work is the first to investigate how different checkpoints in the same pre-training process perform on transfer learning tasks. This contributes to a broader and deeper understanding of the transferability of neural networks.\n\u2022 We discover that in the same pre-training process, an inadquately pre-trained model tends to transfer better than its fully pre-trained counterpart, especially when the pre-trained model is used as a frozen feature extractor. We also further experimentally consolidate this claim beyond image classification.\n\u2022 We observe that FT prefers later pre-training checkpoints, compared with FE. Our analyses based on spectrum decomposition indicate that the learning order of different feature components leads to different preferences of pre-trained checkpoints between FE and FT.\n\u2022 We also point out the risk of utilizing transferability assessment approaches as a general tool to select pretrained models. We evaluate LogME [77], LEEP[45] and NCE[63], which are dependent on frozen pretrained models. Aiming to select the best pre-trained model among different checkpoints, scores obtained by these algorithms often show poor correlations with the actual fine-tuning performance."
        },
        {
            "heading": "2. Related Work",
            "text": "Pre-training on large datasets, such as ImageNet[11], has long been a common method for transfer learning in various kinds of downstream tasks. Due to the huge effort brought\nby data annotation, researchers have reached a consensus that supervised or unsupervised pre-training as a parameter initialization or even an important medium for representation learning on existing large datasets is beneficial[13, 24] for general downstream tasks[47, 44, 48, 61, 19, 6, 5, 18, 8, 80] or specific ones[69, 68, 73, ?]. Zeiler et al. [81] have found that retraining a softmax classifier on top of a fixed pre-trained feature would benefit the classification of target data by a large margin compared with training from scratch. In recent years, designing different kinds of pretext tasks (e.g. jigsaw puzzle[47], rotation angle prediction[15], temporal order prediction[44]) as a self-supervised pre-training method became a popular trend in this community. Later on, contrastive learning[48, 61, 19, 6] has also been demonstrated as a better self-supervised pre-training approach. Beyond the vision domain, large-scale unsupervised pretraining in speech and natural language[59, 12, 74, 53, 4] is appealing as well. Furthermore, learning universal representation and capturing cross-modal correspondence by pre-training in a multimodality setting[52, 39, 33, 32] and its downstream applications[83, 78] play an important role in the development of artificial general intelligence[16].\nWith such a powerful impact on deep learning, in the computer vision community, researchers have also been trying to understand the mechanism behind the success of pretraining, especially the ImageNet case, since ImageNet indeed has strong transferring power even to different data domains (e.g., in geoscience[43] and biomedical science[54]). Erhan et al.[14, 13] experimentally validated the role of unsupervised pre-training as a regularizer for the following supervised learning. Huh et al. [23], via designing thorough experiments, answer a series of questions about the performance difference of transferring brought by different aspects (e.g., number of training samples, number of training classes, fine-grained or coarse-grained pre-training, etc.) of ImageNet. Using the proper normalization method and extending the training time, He et al.[20] challenge this wellestablished paradigm and argue that it is possible to obtain better performance on target data from random initialization in detection and segmentation tasks. Following this work, Zoph et al. [84] further point out that self-training, with stronger data augmentation, can also lead to better transferring performance than pre-training. Nonetheless, pre-training is also viewed as a helpful training fashion for downstream tasks from different perspectives. Hendrycks et al.[22] have discovered that, in task-specific methods (e.g., label corruption, class imbalance, adversarial examples, etc.), pre-training enhances model robustness and brings consistent improvement compared with regular approaches.\nAiming to investigate what kinds of pre-training models could bring better transferring performance, Kornblith et al.[28] conduct extensive experiments on 16 different network architectures and suggest that models with higher top-\n1 accuracy on ImageNet could learn better transferable representations for target tasks. From the perspective of adversarial training, Utrera et al.[64] found that adversariallytrained models, though perform poorer on source data, actually have stronger transferability than natural models. And they further claimed that adversarially-trained models can learn more human-identifiable semantic information. Later, focusing more on model architecture, Salman et al.[58] drew the same conclusion, which further consolidates this viewpoint. In this work, we further investigate the relationship between top-1 accuracy on ImageNet and the transfer performance and found that some suboptimal models during pre-training transfer better when viewed as feature extractors, which is an analogous phenomenon with earlystopping[49, 75] in supervised learning that higher accuracy on training set does not mean higher test performance.\nIn order to further boost the performance of transfer learning, in several previous publications[72, 35, 9], new regularizers have been comprehensively investigated w.r.t. both model parameters and features. In [72], the convolutional weights are penalized to be closer to the source parameters rather than zero to avoid information loss from source data. Li et al. [35] utilize an attention mechanism to restrict the difference between the convolutional features at the same hierarchy from the source model and target one, respectively. Further, Chen et al.[9] claim that feature components corresponding to small singular values would be an impediment to knowledge transferring and then propose to suppress such components as regularization during finetuning. In this work, we also take advantage of Singular Value Decomposition on the features before the softmax layer and provide empirical analysis of the learning mechanism during the learning process."
        },
        {
            "heading": "3. Experimental Setup",
            "text": "We conduct extensive experiments on 8 representative natural image classification datasets (CIFAR10[30], CIFAR100[30], Food-101[3], FGVC Aircraft[41], Stanford Cars[29], CUB-200-2011[66], Oxford 102 Flowers[46] and MIT Indoor 67[51]) and one medical dataset (MURA [57]) based on standard pre-training on both ResNet50 [21] (90 epochs) and T2T-ViT_t-14 [79] (300 epochs), which are representative architectures for ConvNets and Transformers in image classification. The top-1 accuracies are 76.06% and 81.55% for ResNet50 and T2T-ViT_t-14, respectively. For pre-training details, we follow the standard ImageNet training configuration and the official T2T-ViT implementation for the two models, respectively."
        },
        {
            "heading": "4. Results and Analyses",
            "text": "In this section, we showcase all the experimental results of the transfer learning in two different settings: 1. Utilizing\nthe pre-trained models as a feature extractor (FE) and retraining a softmax classifier; 2. Fine-tuning (FT) the whole model. We present experimental results of FE and FT in Section 4.1 and 4.2 respectively. A key observation is that inadequately pre-trained checkpoints transfer better for FE. Besides, we find that a better FE, which can be viewed as a better initialization for the target model, does not yield a better fine-tuning result. This is confirmed in Section 4.3 by t-SNE[65] visualization of deep features before the classifier. In Section 4.4, we manage to discover the in-depth learning mechanism during fine-tuning and empirically explain the aforementioned paradox, with the help of spectral components analysis."
        },
        {
            "heading": "4.1. Inadequately Pre-training Brings Better Feature Extractors",
            "text": "Concretely, in Figure 2 and Figure 3, we can easily observe that there exists a best transferring spot before the model is fully pre-trained when viewed as a feature extrac-\ntor for different datasets, which means that the correlation between the accuracy of the pre-trained model and the quality of the feature of the penultimate layer is not as positive as claimed in [28]. We can also notice that the general trace of the FE performance is roughly a U-form curve with respect to the source performance, implying a potential tradeoff between multiple factors during the pre-training process. Some curves exhibit a form of double-U, e.g. Stanford Cars and CUB-200-2011, and the FE performance at pre-trained epoch 40 and 70 is more likely to increase. We suspect this phenomenon may relate to the learning rate annealing after the 30-th and 60-th pre-training epoch [36]. In 4.6, we will showcase some scenarios where inadequately pre-training brings advantages."
        },
        {
            "heading": "4.2. Fully Pre-training Brings Better Fine-tuning",
            "text": "Performance\nThe case for FT is quite different compared with FE. The general evolution trend for fine-tuning is still positively cor-\nrelated with the source performance, though the fully pretrained checkpoint is not always the best. And we can also find that the best FT model emerges later than the best FE model. This asynchronization is actually surprising because it is common sense that a better initialization should bring better fine-tuning results. However, our work is not the only one that challenges this intuition. Recent empirical studies [82, 34] propose to improve fine-tuning by re-initializing top layers, i.e. employing a worse feature extractor as the starting point of fine-tuning."
        },
        {
            "heading": "4.3. Visualization Analysis",
            "text": "In this subsection, we empirically visualize deep features of the best FE model (at the 5th epoch) and the fully pretrained one (at the 200th epoch). The model is pre-trained on CIFAR10 and then transferred to MNIST, by both FE and FT. Deep features on the last convolutional layer of ResNet-18, produced by MNIST images, are extracted and dimensionally reduced to a 2-d space with t-SNE[65]. The FE performances are 96.47% and 88.47%, and the FT performances are 99.30% and 99.46%(in this experiment we use the full version of MNIST). As can be seen from the top two plots in Figure 4, the visualization result is con-\nsistent with the transferring performance. When directly using the pre-trained model to extract features, data points in the embedding space of the 5-epoch model are clustered better, especially for categories corresponding to index 1 and 6; while the fully pre-trained model produces a more chaotic feature distribution that many data points are entangled with their incongruent neighbors. However, the situation becomes reversely when the whole model is finetuned. There exist a couple of misclassified data points in the feature space of the 5-epoch model, while the fully pretrained model provides highly tight and discriminative features. This phenomenon is somehow surprising because this indicates that a better initialization, i.e., more discriminative features, might lead to worse fine-tuning performance."
        },
        {
            "heading": "4.4. Spectral Component Analysis",
            "text": "Based on the observations from Figure 2 and Figure 3, two questions naturally arise: What makes an inadequately pre-trained model a better feature extractor? What makes a better initialization (FE) perform worse than a fully pre-trained model which could not produce more discriminative features at the beginning? To answer these questions, we resort to spectral analysis by Sin-\ngular Value Decomposition (SVD) for an in-depth investigation. Specifically, we first obtain the batched feature matrix before the classification layer, which we denote as F \u2208 Rb\u00d7d, where b is batch size and d is feature dimension. After this, we decompose the matrix using SVD as:\nF = U\u03a3V T , (1)\nwhere U and V are left and right singular vectors respectively, and \u03a3 is a rectangular diagonal matrix with the singular values on the diagonal. For convenience, we assume that all singular values are sorted in descending order.\nThen we divide the diagonal matrix \u03a3 as the main matrix \u03a3m and the residual matrix \u03a3r. To achieve this division, we first calculate the sum over all singular values as SK\u03c3 = \u2211K i=1 \u03c3i, and then determine the minimum k that satisfies Sk\u03c3/S K \u03c3 \u2265 0.8. \u03a3m preserves top k lines of \u03a3 and fills the remaining elements with zero. \u03a3r is then obtained by \u03a3r = \u03a3 \u2212 \u03a3m. In this way, we can get two spectral components Fm and Fr of the original F by truncated SVD reconstruction as\nFm = U\u03a3mV T (2)\nand Fr = U\u03a3rV T . (3)\nAccording to [9], Fm, as the main components of the feature matrix, represent the majority of transferring knowledge of the extracted features, while Fr is untransferable components or is hard to transfer that may do harm to the\nlearning process and further causes negative transfer[67]. To evaluate the two components, we retrain a softmax classifier with Gaussian initialization on top of Fm and Fr for 50 epochs. We set the batch size as 128, using Adam[27] optimizer, and the learning rate as 0.01.\nFor comparison, we choose the best FE model and the fully pre-trained model in this experiment. For convenience, we call the feature from the best FE model as BFE feature and the feature from the fully pre-trained model as FP feature. The first model pairs are from the CIFAR10to-MNIST experiment, and the results are shown in Table 1. Since we only analyze the features before the softmax layer, the FE models are actually identical to the corresponding pre-trained models; the FT models are fine-tuned with MNIST for 50 epochs. The best FE model is the 5- epoch pre-trained model, whose accuracy is 96.47% and is 8% higher than the fully pre-trained one; however, after fine-tuning, the fully pre-trained model outperforms the 5-epoch one, even with less discriminative initial features. Thus, we decompose the BFE feature and FP feature to investigate which part of the components contributes to their higher performance in FE and FT, respectively.\nAs can be seen from Table 1, there are several interesting discoveries as followed.\n\u2022 The quality of Fm is responsible for the FE performance, while Fr is dominant when fine-tuning the whole model. Specifically, we find that the 5-epoch model performs better as FE due to its remarkable superiority in Fm. However, in the FT setting, the 5-epoch and 200-epoch models show similar performances in Fm, and the higher Fr results in the higher overall performance of the 200-epoch model.\n\u2022 As pre-training fits source data, Fm becomes less discriminative on target data, but Fr transfers better (observed from the line of FE in Table 1). The degeneration in transferability of Fm could be caused by domain discrepancy between source and target data, as\nfully fitting source data may convert general patterns to those specific to the source domain. On the contrary, since Fr can not be well learned at earlier pre-training stages, it generally becomes more informative by further pre-training.\n\u2022 For FT, Fm is easily adapted to target data, but Fr becomes less discriminative on target data (observed from each column in Table 1). Both the 5-epoch and 200-epoch models achieve very high Fm performance (99.26% and 99.08%) after fine-tuning. This implies the underlying learning mechanism that DNNs prefer a prior fitting with main spectrums rather than residual spectrums. The performance of Fr (from FE to FT) decreases due to the information capacity w.r.t. entire F is constant. Despite the degeneration, better Fr in FE still delivers better Fr after fine-tuning, indicating that the residual components learned from the source are not completely forgotten after fine-tuning on target.\nThere might exist another explanation for the phenomenon in this spectral components analysis, which is from the perspective of the frequency domain. We can view Fm as low-frequency components of the original F , and Fr as the high-frequency one. A couple of previous publications have revealed that the neural networks are inclined to learn low-frequency information first in the training process[70, 71, 40]. In our case, during pre-training, the model rapidly learns low-frequency knowledge at the early 5 epochs, which makes it the best feature extractor for downstream tasks. When keep learning in the source domain, more high-frequency patterns, which are specific to the source domain, are gradually learned; therefore, the negative transfer happens.\nWe also illustrate the evolution of the classification performance of the two components for different pre-training epochs in the FE task (from CIFAR10 to MNIST) in Figure 5. It can be obviously noticed that Fm and Fr shows exactly opposite trends when pre-training epoch increases. With longer pre-training on CIFAR10, Fm becomes less discriminative since the model is prone to a deeper fitting to CIFAR10 with more high-frequency knowledge learned. Inversely, the residual components Fr becomes more informative for target data when memorizing more knowledge from the source domain."
        },
        {
            "heading": "4.5. Rethink Transferability Assessment Tools",
            "text": "In this subsection, we utilize several transferability assessment tools LogME[77], LEEP[45], and NCE[63] to validate whether it is possible to obtain the best checkpoint during pre-training without any training. We report these scores for three datasets at different pre-trained checkpoints and calculate the correlation coefficient and Kendall\u2019s \u03c4 coefficient[26] for FT performance. As can be seen in Figure 6, LogME shows good ability in selecting the best checkpoint on FGVC Aircraft and Flowers102, but a little bit poorer in CIFAR10; while the LEEP and NCE can hardly capture the correlation between the performance and the scores, especially in Flowers102."
        },
        {
            "heading": "4.6. Application of Inadequate Pre-training",
            "text": "Pre-trained backbone networks are frequently used as feature extractors in downstream tasks, e.g., image captioning[1], image retrieval[60], temporal action localization[62], few-shot image classification[50], etc. In this section, we leverage typical downstream tasks to validate the effectiveness of inadequately pre-trained models, demonstrating the universal advantages of our method in various scenarios.\nWe firstly focus on the image-text retrieval problem on the MSCOCO dataset[37] and choose the recent PVSE[60] to incorporate our pre-trained ResNet50 models for evaluation. As shown in Table 2, we obtain the best retrieval performance with the 70-epoch pre-trained ResNet50, which extends our conclusion beyond image classification. We also provide a simple yet effective method in this case for selecting checkpoints. We use 25% of the training data and the validation set to evaluate the models and select models according to rsum, which is the summation of the recall scores. The results in Table 3 are consistent with that obtained when the full data is in use, demonstrating the rationale and efficacy of such a method. Moreover, to further consolidate our claim, we use our pre-trained ResNet50 models to perform a few-shot image classification task. Specifically, we choose weight imprinting[50] on CUB-200-2011 in our experiment. In this method, the pretrained ResNet50 models are firstly tuned on a subset of 100\nclasses, and then the classification weights are imprinted to fit unobserved classes. We directly take the accuracy of the 100-class subset as an indicator for model selection. In Table 4, we observe that inadequately pre-trained models are still reliable feature extractors for this task. More importantly, it demonstrates that using the 100-class accuracy for model selection can also lead to the best 70-epoch model."
        },
        {
            "heading": "50 9.88 28.28 41.26 3.84 12.58 19.88 114.98",
            "text": ""
        },
        {
            "heading": "60 10.64 28.78 41.30 3.89 13.26 20.41 117.36",
            "text": ""
        },
        {
            "heading": "70 12.28 33.46 45.82 5.20 16.46 25.44 138.66",
            "text": ""
        },
        {
            "heading": "80 11.04 30.12 42.00 4.84 15.39 23.67 127.45",
            "text": ""
        },
        {
            "heading": "90 10.82 30.70 44.50 4.58 15.18 23.63 132.67",
            "text": ""
        },
        {
            "heading": "50 9.28 26.64 28.82 3.84 12.44 19.80 110.82",
            "text": ""
        },
        {
            "heading": "5. Discussion and Future Work",
            "text": "Better performance in source tasks has long been believed to be more beneficial in target tasks. However, in this paper, we find that when using pre-trained models as feature extractors and retraining a new softmax classifier, the transferring performance does not agree with the source accuracy. There always exists the best epoch in the pretraining process. Intuitively, this is possibly brought by the distribution gap between the source and target data, forming a trade-off between source and target knowledge. If pre-training is less, no sufficient (general) visual knowledge can be obtained and the feature is suboptimal, but negative transfer happens the other way around. Based on this observation, we can operate a more sophisticated checkpoint selection process when we need a good feature extractor trained from source data[38].\nMoreover, the common sense that better initialization should bring better training results is challenged given our observations. As can be seen from the difference in the evolution along the pre-training epochs between FE (view pre-trained model as a feature extractor) and FT (fine-tune the whole model), the FT performance still has a high correlation with the source performance, regardless of the Uproperty of FE performance. This means that a better feature extractor, which can be viewed as a better model initialization, does not definitely brings a better fine-tuning result. Further, in order to provide a more insightful explanation, we conduct a comparative experiment between the best FE model and the fully pre-trained one. Specifically, we delve into the spectral components of the feature before the classification layer and find that the components from top singular values contribute most to the FE, while the components with small singular values play a more critical role in the FT performance. In previous research[9], spectral components corresponding to small singular values are criticized as hard to transfer or even untransferrable. Concretely, we reach consistent conclusions but take different operations. Unlike [9], we do not drop the residual component, but investigate its discriminativeness along with the main component. In this way, we empirically reveal the reason behind\nthe paradox phenomenon that a better feature extractor fails to produce better fine-tuning results in the end. Consistent with an intuitive assumption that over-pre-training would undermine the performance of the pre-trained model as a feature extractor, we discover the main component of the target feature becomes impaired due to the model overfitting to source data with the pre-training epoch increasing.\nFrom a different perspective, we regard the main components as containing low-frequency knowledge of the feature, and the residual components as the carrier of highfrequency information. This makes sense since the residual components are generated from smaller 20% singular values, which are of high variation. In this way, our discoveries are also consistent with what has been well studied in the training mechanism of deep neural networks that the deep models learn low-frequency components before capturing high-frequency ones[70, 71].\nHowever, there are still some phenomena beyond our explanation in Table 1. For example, since the performance of Fm decreases with more pre-training (from 88.24% to 58.74%), what makes it grow much faster (40.34% vs. 11.02%), though the accuracy is a little bit lower (99.08%\nvs. 99.30%) when trained with target data? It is attractive to keep investigating the correspondence between different spectral components and different learning stages (e.g., early or late in pre-training, pre-training, or fine-tuning). We believe such research is beneficial for designing new regularizers for better transfer learning. Meanwhile, new assessment tools should be developed in the future since recent advanced methods cannot precisely select the best pretraining checkpoint during the same pre-training process.\nFurthermore, how such mechanism work in selfsupervised learning is also an interesting topic. We provide the results of self-supervised pre-training with MoCov2 [7] in Figure 7. The results illustrate that, for FE, selfsupervised pre-training does not obey the rules in the supervised case. We hypothesize that self-supervision, which is operated without explicit labels, alleviates the domain gap between source and target since it focuses more on learning an invariant mapping within the same training sample. Due to the page limit, we will investigate this difference between the two pre-training paradigms in future work. Acknowledgements. This research was supported by National Natural Science Foundation of China\n(NO.62106272), the Young Elite Scientists Sponsorship Program by CAST (2021QNRC001), in part by the Research Funds of Renmin University of China (NO. 21XNLG17) and Public Computing Cloud, Renmin University of China."
        },
        {
            "heading": "A. Datasets",
            "text": "CIFAR10 [30] and CIFAR100 [30] are two fundamental datasets in computer vision community. Both of them contain 50,000 training samples and 10,000 test samples, and all the samples are evenly distributed in each category. CIFAR10 consists of 10 common classes of objects. CIFAR100 includes 10 superclasses and each superclasses is made up of 10 fine-grained categories, and the size of each sample is 32 \u00d7 32. Food-101 [3] is a challenging food classification dataset, which consists of 101 categories. There are 250 clean test images for each class and 750 training images containing some noisy labels. FGVC Aircraft [41] is a fine-grained dataset for aircraft classification. It contains 10,000 images of 100 categories of aircraft, and the training set is 2/3 of the whole dataset. Stanford Cars [29] contains 196 classes of fine-grained cars, and there are 8,144 and 8,041 samples in the training set and test set, respectively. CUB-200-2011 [66] is a fine-grained bird classification dataset containing 200 species. There are 11,788 training samples and 5,894 test samples. Annotation of the bounding box, rough segmentation, and attributes are provided. Oxford 102 Flowers [46] contains 200 common species of flowers in United Kingdom. Each of the categories has 40 up to 258 images. There are 2,040 training samples as well as 6,149 test samples. MIT Indoor 67 [51] contains 67 indoor scene categories with in total of 15,620 images, and 80% images are used for training. MURA [57] is a dataset of musculoskeletal radiographs, containing 40,561 X-ray images from 14,863 patient studies. The goal is to distinguish normal musculoskeletal examples from abnormal ones. We follow the common setting to perform binary classification on each image."
        },
        {
            "heading": "B. Experimental setting of ResNet50",
            "text": "Pre-training We borrow the official PyTorch implementation for ImageNet training using ResNet50. The total number of training epochs is set to 90. Stochastic gradient descent with a momentum of 0.9 is used to update the model parameters. The initial learning rate is 0.1 and is multiplied by 0.1 every 30 epochs. The weight decay is 1e-4. The pre-training performance is shown in Table 5.\nTransfer learning In transfer learning, we use different training configurations to adapt to different datasets. For CIFAR10 and CIFAR100, in both FE and FT, the total training epoch is set as 150. The initial learning rate is 0.1 and is decayed by 10 times every 50 epochs. The optimizer is Adam[27]. For the rest natural datasets, we run 6,000 iterations and 9,000 iterations for FE and FT, respectively; the learning rate is set to 0.1 for FT and 0.01 for FE."
        },
        {
            "heading": "C. Experimental setting of T2T-ViT_t-14",
            "text": "Pre-training We perform pre-training following the official codes of T2T-ViT [79]. Specifically, we train T2T-ViT_t-14 on ImageNet for 300 epochs. The final model achieves a Top-1 accuracy of 81.55% on the ImageNet validation set. We choose checkpoints on epoch [20,40,60,80,100,120,150,200,250,300] for transfer learning experiments.\nTransfer learning For transfer learning, we perform the same data processing pipeline as used in ResNet50. For sufficient adaptation, the initial learning rate is set to 0.05 and decayed by a cosine annealing strategy, as suggested by the T2T-ViT paper."
        },
        {
            "heading": "D. Additional results",
            "text": "Additional FE evaluation results on DTD [10] and Caltech256 [17] are shown in table 6, which are also consistent with our claims. To further consolidate our conclusion, we also add the FE results of Swin-T in Table 7. We train Swin-T for 300 epochs following the default settings in the official repository. The results of Swin-T clearly validate our conclusion that the best feature extractors are those inadequately pre-trained models."
        }
    ],
    "title": "Towards Inadequately Pre-trained Models in Transfer Learning",
    "year": 2023
}