{
    "abstractText": "Existing large language models (LLMs) that mainly focus on Standard American English (SAE) often lead to significantly worse performance when being applied to other English dialects. While existing mitigations tackle discrepancies for individual target dialects, they assume access to high-accuracy dialect identification systems. The boundaries between dialects are inherently flexible, making it difficult to categorize language into discrete predefined categories. In this work, we propose DADA (Dialect Adaptation via Dynamic Aggregation), a modular approach to imbue SAE-trained models with multi-dialectal robustness by composing adapters which handle specific linguistic features. The compositional architecture of DADA allows for both targeted adaptation to specific dialect variants and simultaneous adaptation to various dialects. We show that DADA is effective for both single task and instruction finetuned language models, offering an extensible and interpretable framework for adapting existing LLMs to different English dialects.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Yanchen Liu"
        },
        {
            "affiliations": [],
            "name": "William Held"
        },
        {
            "affiliations": [],
            "name": "Diyi Yang"
        }
    ],
    "id": "SP:8ad5855fbda3b2900b6862ed630f3ae3224311de",
    "references": [
        {
            "authors": [
                "No\u00ebmi Aepli",
                "Antonios Anastasopoulos",
                "Adrian-Gabriel Chifu",
                "William Domingues",
                "Fahim Faisal",
                "Mihaela Gaman",
                "Radu Tudor Ionescu",
                "Yves Scherrer."
            ],
            "title": "Findings of the VarDial evaluation campaign 2022",
            "venue": "Proceedings of the Ninth Workshop on NLP",
            "year": 2022
        },
        {
            "authors": [
                "No\u00ebmi Aepli",
                "\u00c7agri \u00c7\u00f6ltekin",
                "Rob van der Goot",
                "T. Jauhiainen",
                "Mourhaf Kazzaz",
                "Nikola Ljubesic",
                "Kai North",
                "Barbara Plank",
                "Yves Scherrer",
                "Marcos Zampieri."
            ],
            "title": "Findings of the vardial evaluation campaign 2023",
            "venue": "Workshop on NLP for Similar",
            "year": 2023
        },
        {
            "authors": [
                "ing",
                "Yuhuai Wu",
                "Kelvin Xu",
                "Yunhan Xu",
                "Linting Xue",
                "Pengcheng Yin",
                "Jiahui Yu",
                "Qiao Zhang",
                "Steven Zheng",
                "Ce Zheng",
                "Weikang Zhou",
                "Denny Zhou",
                "Slav Petrov",
                "Yonghui Wu"
            ],
            "title": "Palm 2 technical report",
            "year": 2023
        },
        {
            "authors": [
                "Akari Asai",
                "Mohammadreza Salehi",
                "Matthew Peters",
                "Hannaneh Hajishirzi."
            ],
            "title": "ATTEMPT: Parameter-efficient multi-task tuning via attentional mixtures of soft prompts",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natu-",
            "year": 2022
        },
        {
            "authors": [
                "Steven Bird."
            ],
            "title": "Local languages, third spaces, and other high-resource scenarios",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
            "year": 2022
        },
        {
            "authors": [
                "Terra Blevins",
                "Robert Kwiatkowski",
                "Jamie MacBeth",
                "Kathleen McKeown",
                "Desmond Patton",
                "Owen Rambow."
            ],
            "title": "Automatically processing tweets from gang-involved youth: Towards detecting loss and aggression",
            "venue": "Proceedings of COLING 2016, the",
            "year": 2016
        },
        {
            "authors": [
                "Su Lin Blodgett",
                "Brendan T. O\u2019Connor"
            ],
            "title": "Racial disparity in natural language processing: A case study of social media african-american english",
            "venue": "ArXiv, abs/1707.00061",
            "year": 2017
        },
        {
            "authors": [
                "Su Lin Blodgett",
                "Johnny Wei",
                "Brendan O\u2019Connor"
            ],
            "title": "Twitter Universal Dependency parsing for African-American and mainstream",
            "venue": "American English. In Proceedings of the 56th Annual Meeting of the Association",
            "year": 2018
        },
        {
            "authors": [
                "Lucia Zheng",
                "Kaitlyn Zhou",
                "Percy Liang."
            ],
            "title": "On the opportunities and risks of foundation models",
            "venue": "ArXiv.",
            "year": 2021
        },
        {
            "authors": [
                "Rich Caruana."
            ],
            "title": "Multitask learning",
            "venue": "Mach. Learn., 28(1):41\u201375.",
            "year": 1997
        },
        {
            "authors": [
                "Daniel Cer",
                "Mona Diab",
                "Eneko Agirre",
                "I\u00f1igo LopezGazpio",
                "Lucia Specia."
            ],
            "title": "SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation",
            "venue": "Proceedings of the 11th International Workshop on Semantic",
            "year": 2017
        },
        {
            "authors": [
                "Findings of the VarDial evaluation campaign"
            ],
            "title": "In Proceedings of the Eighth Workshop on NLP for Similar Languages, Varieties and Dialects, pages 1\u2013 11, Kiyv, Ukraine",
            "venue": "Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "Narang",
                "Gaurav Mishra",
                "Adams Yu",
                "Vincent Zhao",
                "Yanping Huang",
                "Andrew Dai",
                "Hongkun Yu",
                "Slav Petrov",
                "Ed H. Chi",
                "Jeff Dean",
                "Jacob Devlin",
                "Adam Roberts",
                "Denny Zhou",
                "Quoc V. Le",
                "Jason Wei"
            ],
            "title": "Scaling instruction-finetuned language",
            "year": 2022
        },
        {
            "authors": [
                "Thomas Davidson",
                "Debasmita Bhattacharya",
                "Ingmar Weber."
            ],
            "title": "Racial bias in hate speech and abusive language detection datasets",
            "venue": "Proceedings of the Third Workshop on Abusive Language Online, pages 25\u201335, Florence, Italy. Association for Com-",
            "year": 2019
        },
        {
            "authors": [
                "Dorottya Demszky",
                "Devyani Sharma",
                "Jonathan H Clark",
                "Vinodkumar Prabhakaran",
                "Jacob Eisenstein."
            ],
            "title": "Learning to recognize dialect features",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa-",
            "year": 2021
        },
        {
            "authors": [
                "Xavier Garcia",
                "Orhan Firat."
            ],
            "title": "Using natural language prompts for machine translation",
            "venue": "arXiv preprint arXiv:2202.11822.",
            "year": 2022
        },
        {
            "authors": [
                "Lisa J. Green."
            ],
            "title": "African American English: A Linguistic Introduction",
            "venue": "Cambridge University Press.",
            "year": 2002
        },
        {
            "authors": [
                "Matan Halevy",
                "Camille Harris",
                "Amy Bruckman",
                "Diyi Yang",
                "Ayanna Howard."
            ],
            "title": "Mitigating racial biases in toxic language detection with an equitybased ensemble framework",
            "venue": "Equity and Access in Algorithms, Mechanisms, and Optimization,",
            "year": 2021
        },
        {
            "authors": [
                "Matan Halevy",
                "Camille Harris",
                "Amy Bruckman",
                "Diyi Yang",
                "Ayanna M. Howard."
            ],
            "title": "Mitigating racial biases in toxic language detection with an equity-based ensemble framework",
            "venue": "Equity and Access in Algorithms, Mechanisms, and Optimization.",
            "year": 2021
        },
        {
            "authors": [
                "Junxian He",
                "Chunting Zhou",
                "Xuezhe Ma",
                "Taylor BergKirkpatrick",
                "Graham Neubig."
            ],
            "title": "Towards a unified view of parameter-efficient transfer learning",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Will Held",
                "Caleb Ziems",
                "Diyi Yang"
            ],
            "title": "Tada: Task-agnostic dialect adapters for english",
            "year": 2023
        },
        {
            "authors": [
                "Neil Houlsby",
                "Andrei Giurgiu",
                "Stanislaw Jastrzebski",
                "Bruna Morrone",
                "Quentin De Laroussilhe",
                "Andrea Gesmundo",
                "Mona Attariyan",
                "Sylvain Gelly."
            ],
            "title": "Parameter-efficient transfer learning for NLP",
            "venue": "Proceedings of the 36th International Conference",
            "year": 2019
        },
        {
            "authors": [
                "Dirk Hovy",
                "Shannon L. Spruit."
            ],
            "title": "The social impact of natural language processing",
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 591\u2013598, Berlin, Germany. Association",
            "year": 2016
        },
        {
            "authors": [
                "Anna J\u00f8rgensen",
                "Dirk Hovy",
                "Anders S\u00f8gaard."
            ],
            "title": "Learning a POS tagger for AAVE-like language",
            "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
            "year": 2016
        },
        {
            "authors": [
                "Anna J\u00f8rgensen",
                "Dirk Hovy",
                "Anders S\u00f8gaard."
            ],
            "title": "Learning a POS tagger for AAVE-like language",
            "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
            "year": 2016
        },
        {
            "authors": [
                "David Jurgens",
                "Yulia Tsvetkov",
                "Dan Jurafsky."
            ],
            "title": "Incorporating dialectal variability for socially equitable language identification",
            "venue": "Proceedings of the",
            "year": 2017
        },
        {
            "authors": [
                "David Jurgens",
                "Yulia Tsvetkov",
                "Dan Jurafsky."
            ],
            "title": "Incorporating dialectal variability for socially equitable language identification",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages",
            "year": 2017
        },
        {
            "authors": [
                "Svetlana Kiritchenko",
                "Saif Mohammad."
            ],
            "title": "Examining gender and race bias in two hundred sentiment analysis systems",
            "venue": "Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics, pages 43\u201353, New Orleans, Louisiana.",
            "year": 2018
        },
        {
            "authors": [
                "Allison Koenecke",
                "Andrew Nam",
                "Emily Lake",
                "Joe Nudell",
                "Minnie Quartey",
                "Zion Mengesha",
                "Connor Toups",
                "John R. Rickford",
                "Dan Jurafsky",
                "Sharad Goel."
            ],
            "title": "Racial disparities in automated speech recognition",
            "venue": "Proceedings of the National Academy",
            "year": 2020
        },
        {
            "authors": [
                "Brian Lester",
                "Rami Al-Rfou",
                "Noah Constant."
            ],
            "title": "The power of scale for parameter-efficient prompt tuning",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045\u20133059, Online and Punta Cana, Domini-",
            "year": 2021
        },
        {
            "authors": [
                "Xiang Lisa Li",
                "Percy Liang."
            ],
            "title": "Prefix-tuning: Optimizing continuous prompts for generation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language",
            "year": 2021
        },
        {
            "authors": [
                "Percy Liang",
                "Rishi Bommasani",
                "Tony Lee",
                "Dimitris Tsipras",
                "Dilara Soylu",
                "Michihiro Yasunaga",
                "Yian Zhang",
                "Deepak Narayanan",
                "Yuhuai Wu",
                "Ananya Kumar"
            ],
            "title": "Holistic evaluation of language models. arXiv preprint arXiv:2211.09110",
            "year": 2022
        },
        {
            "authors": [
                "Jiachang Liu",
                "Dinghan Shen",
                "Yizhe Zhang",
                "Bill Dolan",
                "Lawrence Carin",
                "Weizhu Chen"
            ],
            "title": "What makes good in-context examples for GPT-3",
            "venue": "In Proceedings of Deep Learning Inside Out (DeeLIO",
            "year": 2022
        },
        {
            "authors": [
                "Xiaodong Liu",
                "Pengcheng He",
                "Weizhu Chen",
                "Jianfeng Gao."
            ],
            "title": "Multi-task deep neural networks for natural language understanding",
            "venue": "Proceedings",
            "year": 2019
        },
        {
            "authors": [
                "Yanchen Liu",
                "Timo Schick",
                "Hinrich Sch\u00fctze"
            ],
            "title": "2022b. Semantic-oriented unlabeled priming for large-scale language models",
            "year": 2022
        },
        {
            "authors": [
                "Yanchen Liu",
                "Jing Yan",
                "Yan Chen",
                "Jing Liu",
                "Hua Wu"
            ],
            "title": "SMoA: Sparse mixture of adapters to mitigate multiple dataset biases",
            "year": 2023
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov"
            ],
            "title": "2019b. Roberta: A robustly optimized bert pretraining approach",
            "year": 2019
        },
        {
            "authors": [
                "Shervin Malmasi",
                "Marcos Zampieri",
                "Nikola Ljube\u0161i\u0107",
                "Preslav Nakov",
                "Ahmed Ali",
                "J\u00f6rg Tiedemann."
            ],
            "title": "Discriminating between similar languages and Arabic dialect identification: A report on the third DSL shared task",
            "venue": "Proceedings of the Third Work-",
            "year": 2016
        },
        {
            "authors": [
                "Stefan Martin",
                "Walt Wolfram."
            ],
            "title": "The sentence in african-american vernacular english",
            "venue": "AfricanAmerican English, pages 11\u201340. Routledge.",
            "year": 2021
        },
        {
            "authors": [
                "Marzieh Mozafari",
                "Reza Farahbakhsh",
                "No\u00ebl Crespi."
            ],
            "title": "Hate speech detection and racial bias mitigation in social media based on bert model",
            "venue": "PLoS ONE,",
            "year": 2020
        },
        {
            "authors": [
                "Augustus Odena",
                "Charles Sutton",
                "David Martin Dohan",
                "Ellen Jiang",
                "Henryk Michalewski",
                "Jacob Austin",
                "Maarten Paul Bosma",
                "Maxwell Nye",
                "Michael Terry",
                "Quoc V. Le"
            ],
            "title": "Program synthesis with large language models",
            "year": 2021
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "ChatGPT",
            "venue": "https://openai.com/ research/chatgpt.",
            "year": 2023
        },
        {
            "authors": [
                "Paul Christiano",
                "Jan Leike",
                "Ryan Lowe."
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Jonas Pfeiffer",
                "Aishwarya Kamath",
                "Andreas R\u00fcckl\u00e9",
                "Kyunghyun Cho",
                "Iryna Gurevych."
            ],
            "title": "AdapterFusion: Non-destructive task composition for transfer learning",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association",
            "year": 2021
        },
        {
            "authors": [
                "Jonas Pfeiffer",
                "Andreas R\u00fcckl\u00e9",
                "Clifton Poth",
                "Aishwarya Kamath",
                "Ivan Vuli\u0107",
                "Sebastian Ruder",
                "Kyunghyun Cho",
                "Iryna Gurevych."
            ],
            "title": "AdapterHub: A framework for adapting transformers",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Nat-",
            "year": 2020
        },
        {
            "authors": [
                "Jeff Stanway",
                "Lorrayne Bennett",
                "Demis Hassabis",
                "Koray Kavukcuoglu",
                "Geoffrey Irving"
            ],
            "title": "Scaling language models: Methods, analysis & insights from training gopher",
            "year": 2022
        },
        {
            "authors": [
                "Anthony Rios."
            ],
            "title": "Fuzze: Fuzzy fairness evaluation of offensive language classifiers on african-american english",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 34(01):881\u2013889.",
            "year": 2020
        },
        {
            "authors": [
                "drea Santilli",
                "Thibault Fevry",
                "Jason Alan Fries",
                "Ryan Teehan",
                "Teven Le Scao",
                "Stella Biderman",
                "Leo Gao",
                "Thomas Wolf",
                "Alexander M Rush"
            ],
            "title": "Multitask prompted training enables zero-shot task generalization",
            "venue": "In International Conference on Learning",
            "year": 2022
        },
        {
            "authors": [
                "Maarten Sap",
                "Dallas Card",
                "Saadia Gabriel",
                "Yejin Choi",
                "Noah A. Smith."
            ],
            "title": "The risk of racial bias in hate speech detection",
            "venue": "Annual Meeting of the Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Christopher Potts"
            ],
            "title": "Recursive deep models for",
            "year": 2013
        },
        {
            "authors": [
                "Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all",
            "year": 2017
        },
        {
            "authors": [
                "Xuhui Zhou",
                "Maarten Sap",
                "Swabha Swayamdipta",
                "Noah A. Smith",
                "Yejin Choi."
            ],
            "title": "Challenges in automated debiasing for toxic language detection",
            "venue": "ArXiv, abs/2102.00086.",
            "year": 2021
        },
        {
            "authors": [
                "Caleb Ziems",
                "Jiaao Chen",
                "Camille Harris",
                "Jessica Anderson",
                "Diyi Yang."
            ],
            "title": "VALUE: Understanding dialect disparity in NLU",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
            "year": 2022
        },
        {
            "authors": [
                "Caleb Ziems",
                "William Held",
                "Jingfeng Yang",
                "Jwala Dhamala",
                "Rahul Gupta",
                "Diyi Yang."
            ],
            "title": "MultiVALUE: A framework for cross-dialectal English NLP",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Vol-",
            "year": 2023
        },
        {
            "authors": [
                "CoLA",
                "Warstadt"
            ],
            "title": "2018)) task is a widely used benchmark that focuses on grammatical acceptability judgments. It aims to assess the ability of models to determine whether a given sentence is syntactically and semantically correct or not",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "As Natural Language Processing (NLP) becomes even more impactful, the equitable distribution of its benefits becomes an increasing concern. Specifically, NLP tooling is often trained and evaluated on dominant language variants, such as Standard American English (SAE). This results in a significant decline in the performance when these tools are applied to non-SAE dialects. Studies have revealed that SAE models tested on African American Vernacular English (AAVE) encounter difficulties in language identification (Jurgens et al., 2017a) as well as various other natural language tasks (J\u00f8rgensen et al., 2016a; Kiritchenko and Mohammad,\n1All the code, synthetic datasets, and trained adapters in this work are available at https://github.com/SALT-NLP/ DADA.\n2018; Blodgett et al., 2018). These challenges extend to automated speech recognition used by virtual assistants (Koenecke et al., 2020) and hate speech detection employed by online media platforms (Davidson et al., 2019; Sap et al., 2019; Rios, 2020; Mozafari et al., 2020; Halevy et al., 2021a; Zhou et al., 2021). Notably, even large language models are not exempt from these limitations (Bommasani et al., 2021; Solaiman and Dennison, 2021; Rae et al., 2022; Liang et al., 2022). Such performance disparities raise ethical and moral concerns regarding the potential for racial disparities in the seemingly expeditious development of language technologies (Hovy and Spruit, 2016; Blodgett and O\u2019Connor, 2017; Halevy et al., 2021b).\nExisting research to mitigate this disparity has mainly focused on dialectal adaptation targeting individual dialects of interest (Ziems et al., 2022; Garcia and Firat, 2022; Ziems et al., 2023; Sun et al., 2022). This approach is a powerful first step, but it has key limitations of missing connectedness among dialects; for instance, English alone has 77\nar X\niv :2\n30 5.\n13 40\n6v 3\n[ cs\n.C L\n] 6\nD ec\n2 02\n3\nrecognized variants that vary internally (Koenecke et al., 2020; Demszky et al., 2021). Prior adaptation methods also require highly accurate dialect identification systems for real-world uses, leading to the development of separate systems for different dialects. Such separate systems are not yet available for many dialects and related languages (Malmasi et al., 2016; Aepli et al., 2023; Chakravarthi et al., 2021; Aepli et al., 2022). Alternative approaches train models using a combination of various dialect variants in a multi-task learning manner (Caruana, 1997; Liu et al., 2019a). However, this approach requires training new models for dialectal NLP from scratch simultaneously with data from all desired dialects. This training process is prohibitive, especially given the trend towards larger language models with costs upwards of millions of dollars2. Thus, there is a pressing need for an effective and extensible approach that can adapt existing models to the multi-dialectal setting.\nPrevious linguistic works have developed a collection of lexical and morphosyntactic features that describe the differences between SAE and various other English dialects (Kortmann et al., 2020; Ziems et al., 2023). Many dialects can be described by this common set of features or linguistic rules, with each dialect expressing a subset of the feature space. In addition, dialects are not deterministic speech patterns but rather ranges of acceptable use of these features that speakers adjust based on social contexts (Ziems et al., 2023; Koenecke et al., 2020; Demszky et al., 2021). As a result, dialects do not neatly fit into predefined categories.\nTo this end, we develop a model which handles this reality by accommodating the diversity of English variants at a fine-grained level (linguistic features or linguistic rules). Concretely, we propose Dialect Adaptation via Dynamic Aggregation (DADA): a modular approach to adapt an established model trained on SAE to dialect variants by composing linguistic features. DADA captures and encapsulates each feature using adapters (Houlsby et al., 2019) trained on individual feature rules. Feature adapters dynamically aggregate at test time using adapter fusion (Pfeiffer et al., 2021), which enables the SAE model to flexibly adapt to dialects. The modular design of DADA enables targeted adaptation to specific dialect variants or simultaneous adaptation to multiple dialects. As a result\n2https://lambdalabs.com/blog/ demystifying-gpt-3\nof its compositional nature, DADA also makes it easy to re-use feature adapters regardless of dialect, speaker, or time variations in feature usage. The modular architecture ensures interpretability by enabling analysis of the components responsible for the improvement in performance.\nTo sum up, our work contributes the following:\n\u2022 We propose a modular approach DADA to adapt the standard SAE model to dialect variants via a dynamic aggregation of different linguistic features. (Sec. 3)\n\u2022 We train nearly 200 feature adapters, which can be flexibly composed to target different dialects. Moreover, we demonstrate that DADA with all the trained feature adapters can consistently improve model performance across five English dialects. (Sec. 4)\n\u2022 DADA exhibits strong interpretability. Using AAVE as an example, we illustrate that DADA possesses the capability to detect the relevant linguistic features for a given input and subsequently activate the corresponding feature adapters. (Sec. 5)\n\u2022 We show that DADA improves dialectal robustness in task-agnostic instruction-tuned LLMs using FLAN-T5 (Chung et al., 2022) (Sec. 6), which highlights the capability of DADA in learning task-agnostic features that can be applied to newer general-purpose models."
        },
        {
            "heading": "2 Related Work",
            "text": "Dialect NLP research tends to focus primarily on dominant dialects represented in \"textbook\" grammar, such as Standard American English (SAE), over lower-resource dialects. The performance disparity in resulting models is pervasive (Koenecke et al., 2020; Davidson et al., 2019; Sap et al., 2019; Rios, 2020; Mozafari et al., 2020; Halevy et al., 2021a; Zhou et al., 2021; Ziems et al., 2022, 2023; Sun et al., 2022). The existence of such performance disparities raises ethical and moral concerns where NLP can potentially exacerbate the marginalization of the speakers of these dialects (Blodgett and O\u2019Connor, 2017; Halevy et al., 2021b). Lacking a common dialectal evaluation, NLP can reinforce existing power discrepancies (Hovy and Spruit, 2016; Bommasani et al., 2021). Existing works on English dialects have mainly focused on adapting models to individual dialects, such as\nAfrican American Vernacular English (AAVE) (J\u00f8rgensen et al., 2016b; Blevins et al., 2016; Ziems et al., 2022). However, for real-world use, such systems would require another system to recognize these dialects so that the appropriate model can be used for each input. This task itself is challenging, with state-of-the-art systems showing relatively low accuracy even when distinguishing high-resource dialects of English (Zampieri et al., 2023). Our work avoids this flaw by modeling multiple dialects at once using multidialectal training data. Multidialectal training data has been shown to potentially increase robustness across all dialects in multiple prior works around data collection (Jurgens et al., 2017b) and augmentation (Ziems et al., 2023).\nParameter-Efficient Learning To efficiently transfer pretrained language models to downstream tasks, several techniques (He et al., 2022) have been proposed to update only a small number of extra parameters while keeping most parameters frozen. For example, adapter tuning (Houlsby et al., 2019; Pfeiffer et al., 2020) adapts models using small bottleneck modules. Prefix tuning (Li and Liang, 2021) and prompt tuning (Lester et al., 2021) prepend additional tunable prefix tokens to input or hidden layers. Brown et al. (2020); Liu et al. (2022a,b) prompt language models for specific tasks without any parameter updates by in-context\nlearning. Besides, several research efforts have been carried out to ensemble parameter-efficient components for multi-task learning and domain adaptation. Pfeiffer et al. (2021) propose to aggregate adapters trained on source tasks with an attentional layer to transfer acquired knowledge to a target task. Asai et al. (2022) introduce a similar method, while using soft prompts instead of adapters. To improve robustness against dataset shortcuts, Liu et al. (2023) combine adapters with gating networks. Recently, Held et al. (2023) use adapter stacking for task-agnostic robustness targeting individual dialects. However, given the inherent flexibility of dialects, there arises a necessity for a method to enhance multi-dialectal robustness. Moreover, the existence of well-defined transformation rules between dialects, which is uncommon in other domains, allows us to achieve finer-grained adaptation by aggregating linguistic features.\nInstruction Tuning Inspired by the success of prompting LLMs to adapt to various tasks (Brown et al., 2020), instruction tuning (Sanh et al., 2022; Wei et al., 2022; Ouyang et al., 2022) propose to finetune language models on a variety of tasks described through instructions to achieve the multitask capability and to enhance zero-shot performance on unseen tasks. Since instruction tuning involves prompting the language models at the input\nlevel, our approach is orthogonal to it and can be employed in conjunction to enhance model\u2019s multitask and multi-dialect abilities simultaneously."
        },
        {
            "heading": "3 DADA",
            "text": "We introduce Dialect Adaptation via Dynamic Aggregation (DADA), a modular method for adapting an existing model trained on the Standard American English (SAE) to accommodate dialect variants at a finer-grained level. Our proposed method deploys a dynamic aggregation of feature adapters, which characterize the divergence of linguistic features between SAE and its dialect variants. Specifically, DADA involves the creation of a synthetic training dataset for each individual feature using transformation rules (Ziems et al., 2023). These synthetic datasets are used to train respective adapters for each linguistic feature. Finally, we compose these feature adapters to create a single model via an additional fusion layer."
        },
        {
            "heading": "3.1 Synthetic Datasets",
            "text": "Previous works have discerned a series of linguistic divergences and devised Multi-VALUE, a collection of lexical and morphosyntactic transformation rules 3 between SAE and its 50 dialect variants (Ziems et al., 2022, 2023), including Appalachian English (AppE), Chicano English (ChcE), Colloquial Singapore English (CollSgE), Indian English(IndE), and African American Vernacular English (AAVE), among others. For instance, a well-known linguistic feature of AAVE is the use of Negative Concord, where two negative morphemes are employed to convey a single negation (Martin and Wolfram, 2021). This transformation rule is sensitive to the verb-object dependency structure and necessitates an indefinite noun object (Green, 2002). As an example, the SAE sentence \u201cHe doesn\u2019t have a camera\u201d could be rendered as \u201cHe don\u2019t have no camera\u201d in AAVE.\nLet T = {T1, T2, ...TN} denote the set of transformation rules between SAE and its dialect variants. For each transformation rule Ti \u2208 T , we can generate a corresponding synthetic dataset Di by applying the respective rule to each individual training example within the original training dataset D.\n3See Appendix A for a detailed description of each transformation rule and the statistics of the corresponding synthetic training dataset."
        },
        {
            "heading": "3.2 Feature Adapter",
            "text": "Adapter tuning is known for its ability to adapt quickly to new tasks without catastrophic forgetting (Pfeiffer et al., 2021). Given these benefits and the inherent modularity of adapters, we develop a feature adapter Ai for each of the N linguistic transformation rules Ti \u2208 T by training it on the corresponding synthetic dataset Di created in Sec. 3.1. We insert an adapter module after each feedforward layer 4 of the backbone model M that has been trained on the original SAE task datasets, in order to target specific lexical and morphosyntactic differences between SAE and its dialect variants."
        },
        {
            "heading": "3.3 Dynamic Aggregation",
            "text": "In Sec. 3.2, we described the process of training feature adapter Ai for each linguistic transformation rule to capture a specific type of linguistic difference between SAE and its dialect variants. However, it is common for multiple linguistic differences to co-occur within a single sentence in realworld scenarios, thereby necessitating the model to simultaneously consider these distinct linguistic features to varying degrees.\nTherefore, we propose to dynamically aggregate the N trained feature adapters, denoted as A = {A1, A2, ...AN}, into the SAE-trained backbone model M via an additional fusion layer (Pfeiffer et al., 2021). For this purpose, we first construct a super-synthetic training dataset D, employing the same approach as described in Sec. 3.1, but with all lexical and morphosyntactic transformation rules T = {T1, T2, ...TN} applied. After incorporating the N trained feature adapters A and a fusion layer into each layer of the backbone model, we train the fusion layers using the super-synthetic training dataset D, while keeping the feature adapters A and the backbone model M frozen.\nFollowing Pfeiffer et al. (2021), we define the fusion layer as a composition of Key, Value and Query matrices at each layer l of the transformer, denoted by Kl, Vl and Ql respectively. The output of the feedforward layer hl is taken as the query vector and the output of each feature adapter Ai, denoted as al,i is used as input to both the value and key transformations. With this attention-like fusion layer (Vaswani et al., 2017), the outputs of\n4There are different implementation variants for adapter tuning, and in our work, we follow Pfeiffer et al. (2020) by only inserting adapter modules after each feed-forward layer, while in some other works, adapters are inserted after multihead attention layers as well.\nall feature adapters are combined as followed:\nsl = softmax(h T l Ql \u00b7 aTl,iKl), i \u2208 {1, ..., N} ,\na\u2032l,i = a T l,iVl, i \u2208 {1, ..., N} , A\u2032l = [a \u2032 l,0, ...a \u2032 l,N ],\nol = s T l A \u2032 l,\nwhere [\u00b7, \u00b7] indicates the concatenation of vectors and ol is the output of the l-th fusion layer. Through training on the super-synthetic dataset D, a parameterized compositional mixture of feature adapters can be learned to identify the applied linguistic features for a given input and activate the corresponding feature adapters, thereby facilitating the effective addressing of linguistic discrepancies between SAE and its dialect variants.\nTo sum up, the compositionality of DADA enables targeted adaptation to specific dialect variants by selecting appropriate feature adapters. DADA uses modularity and compositionality to adapt a model to linguistic features present at test time since the pervasiveness of a feature can vary greatly based on its applicability and density (Demszky et al., 2021). This allows DADA to simultaneously adapt to various dialects by using a comprehensive set of feature adapters. We explore this property further in Sec. 5, using its interpretability to study individual feature adaptations utilized (see Sec. 5)."
        },
        {
            "heading": "4 Multi-Dialect Adaptation",
            "text": "In this section, we demonstrate how DADA can enable the adaptation of an existing SAE model to multiple dialect variants, taking Multi-Genre Natural Language Inference (MNLI; Williams et al. (2018)) task as an example."
        },
        {
            "heading": "4.1 Experimental Setup and Evaluation",
            "text": "As described in Sec. 3.2, we train a feature adapter for each transformation rule from Ziems et al. (2023), the collection of lexical and morphosyntactic transformation rules between SAE and its dialect variants. In total, we train nearly 200 feature adapters for downstream use. Here, we demonstrate that these features can be flexibly composed in DADA to improve model performance across multiple dialects simultaneously. We evaluate on five representative dialects: AppE, ChcE, CollSgE, IndE, AAVE. We employ RoBERTa Base (Liu et al., 2019b) that has been finetuned on the original SAE MNLI training dataset as the backbone model.\nFor each transformation rule, we generate a synthetic dataset by applying only that specific transformation rule to each example in the original MNLI training dataset. We only retain examples that differ from the original example, i.e., examples that have been transformed. Afterward, we train feature adapters using these synthetic datasets, as described in Sec. 3.2. To aggregate trained feature adapters into the backbone model, we train a large fusion layer for 5 epochs on a synthetic dataset that applies all dialectal variations simultaneously, termed Multi. Additionally, we include a null adapter that remains as the identity function. This is kept for purely SAE inputs. In Appendix B, we report full hyperparameters along with the training details. We evaluate DADA on five English dialects: AppE, ChcE, CollSgE, IndE, AAVE and report the results in Table 1. Followed by Ziems et al. (2022, 2023), we construct each dialect-specific MNLI dataset by utilizing a subset of transformation rules that correspond to the respective dialect."
        },
        {
            "heading": "4.2 Results",
            "text": "Compared to the standard SAE model trained on the original MNLI dataset (SAE baseline), DADA demonstrates significant performance improvements across all evaluated dialects and even on SAE, with an average improvement of 2.16%. Moreover, DADA delivers comparable performance to the strong baseline provided by individual further fine-tuning or adapter tuning on the SAE trained model with dialect-specific training data (Single Finetuning and Single Adapter). However, while these two approaches require a perfect dialect identification system and D models, our approach uses a single model and therefore does not rely on dialect identification. This makes DADA a simpler and more realistic method for use when the target dialect distribution is unknown.\nCompared to additional finetuning or adapter tuning Multi on standard SAE model (Multi Finetuning and Multi Adapter), DADA brings an average improvement of 0.32% and 0.47%, respectively. Moreover, it tunes fewer parameters during a single training run compared to Multi Finetuning. We confirm that the empirically strong performance of DADA stems from the effective use of the correct individual feature adapters in Sec. 5.\nNote that with DADA, in instances where a new dialect arises, the integration of this new dialect can be achieved through the identification of the\nlinguistic transformation rules that govern the shift from SAE to the new dialect, followed by the training of a feature adapter for each new transformation rule, and finally the retraining of the fusion layer. Furthermore, the potential for reusability of trained feature adapters is significant as many dialects often share common linguistic features.\nnull adapter For SAE inputs, every adapter has the potential to incorrectly change the model\u2019s original predictions. Therefore, we introduce a null adapter that which preserves the output of the original SAE model at each layer. We conduct an ablation study to evaluate the necessity of the null adapter by comparing with models where it is excluded. We denote this variant as DADAw/o null. As shown in Table 1, excluding the null adapter results in a slight drop in performance for SAE.\nNumber of feature adapters We analyze the average performance of DADA on 5 evaluated English dialects, considering different numbers of feature adapters (k) ranging from 1 to all. For each k, we select the top k feature adapters with the best performance on the evaluation set. The results in Figure 3 demonstrate an overall increasing trend, indicating that each feature adapter incorporated in DADA can contribute to performance improvement, rather than relying solely on a select few."
        },
        {
            "heading": "5 Interpretability",
            "text": "As discussed in Sec. 3, DADA can implicitly identify the relevant linguistic features for a given input and activate the corresponding feature adapters. We validate this by investigating the correlation between attention scores within each layer of DADA and the presence of linguistic features, to determine whether the contributing feature adapters are relevant to the features present."
        },
        {
            "heading": "5.1 Analyses Setup and Results",
            "text": "Here, we use the AAVE dialect and MNLI task as an example. To adapt a standard MNLI-finetuned RoBERTa Base model to target the AAVE dialect, we only need to take into account the 10 transformation rules between SAE and AAVE proposed by Ziems et al. (2022). We select the corresponding feature adapters from our collection and dynamically aggregate them by training a fusion layer on AAVE training set for 5 epochs with a learning\nrate 5e-5 and batch size 64. We evaluate the resulting model on the test split of the AAVE matched MNLI dataset as shown in Table 2. In comparison to the standard SAE model, DADA demonstrates a 3.2% and 1.4% improvement on AAVE and SAE, respectively. Moreover, DADA outperforms simple additional finetuning and adapter tuning of AAVE on SAE model by 0.4% and 0.5%, respectively, achieving the best performance of 86.6% on AAVE. These results demonstrate the superior performance of DADA over all other methods evaluated."
        },
        {
            "heading": "5.2 Correlation Analysis of Fusion Activation",
            "text": "We perform a correlation analysis of these 10 feature adapters for the linguistic features applied to the input data. For each transformation rule, we calculate the softmax activation for each adapter, for each input to which the specific linguistic feature applies, and average over all activations within the same layer calculated over all instances in the AAVE MNLI test set. For better clarity, our final metrics takes the average utilization score of each feature adapter for the entire dataset and then subtracts the average utilization score associated with each transformation rule.\nWe plot the results for layers 1, 3, 7, 11 in Figure 4. We found that significant correlations in utilization on the lower layers (0-3) are observed, while those on the middle and higher layers are found to be negligible. This is consistent with our intuition, as the primary distinction between SAE and its dialect variants lies in their linguistic features (lexical and morphosyntactic), which are mainly captured by the lower layers of the model5. This analysis demonstrates that DADA has the capability to detect which linguistic features are relevant to the given input, and subsequently trigger the corresponding feature adapters. This highlights the interpretability of DADA with regard to the underlying factors that contribute to performance improvement."
        },
        {
            "heading": "6 Multi-Task Dialect Adaptation",
            "text": "Recent LLMs such as FLAN-T5 (Chung et al., 2022) and InstructGPT (Ouyang et al., 2022) are instruction-tuned (Wei et al., 2022) for various tasks, which is orthogonal to our method, making it possible to combine the two approaches easily. In this section, we demonstrate how DADA can be\n5The linguistic feature differences are subtle. Although the absolute values of the correlation coefficients are not large, they are sufficient to indicate the existence of correlations.\nemployed to instruction-tuned LLMs to improve their task-agnostic performance on dialects."
        },
        {
            "heading": "6.1 Experimental Setup",
            "text": "Using AAVE dialect as a case study, to demonstrate the effectiveness of our method in adapting the SAE model across multiple tasks, we include the tasks from the AAVE transformed version (Ziems et al., 2022) of the GLUE Benchmark (Wang et al., 2018), including CoLA, MNLI, QNLI, QQP, SST2, and STS-B. For our backbone model, we employ a FLAN-T5 Base (Chung et al., 2022). Despite the original paper incorporates GLUE within the FLAN-T5\u2019s training data, we retrain the model on these specific tasks to enhance its suitability."
        },
        {
            "heading": "6.2 Multi-task training",
            "text": "For each transformation rule of AAVE dialect, we construct synthetic training data following the procedure described in Sec. 3.1. However, in the case of a multi-task model, we construct a synthetic dataset for each task considered and utilize the mixture to train the corresponding feature adapter. Subsequently, we proceed to fuse these feature adapters by training a fusion layer on the super-synthetic dataset Multi-Task AAVE, which is constructed by applying all the AAVE transformation rules. In Appendix D, we provide the templates used to train the FLAN-T5 model. In Appendix B, we report full hyperparameters along with the training details. We assess the performance of DADA on AAVE transformed version of the GLUE Benchmark, and compare its results with the SAE baseline and Adapter Tuning with Multi-Task AAVE."
        },
        {
            "heading": "6.3 Results",
            "text": "It is surprising to note that although single Adapter Tuning with Multi-Task AAVE demonstrates improvements in 4 out of 7 tasks, the overall average performance is even inferior to that of the SAE baseline. In contrast, DADA consistently outperforms both the SAE baseline and Adapter Tuning across all evaluated tasks, resulting in an overall improvement of 1.80/1.92 points on the AAVE GLUE benchmark, respectively. Specifically, on the relatively large datasets, DADA achieves a notable accuracy improvement of 2.0%/1.0% on MNLImm, 0.9%/1.2% on QNLI, and 1.5%/0.9% on QQP when compared to the SAE Baseline and Adapter Tuning, respectively. These results demonstrate that our proposed approach, DADA, is not limited to single-task applications but can be easily scaled\nbd di da g l nc ni ng nr u\nLayer 1\nLayer 3\nLayer 7\nLayer 11 6 4 2 0 2 4 6\nup to accommodate various tasks for use with the increasingly common multi-task instruction-tuning setup using in popular large-scale industrial systems (Ouyang et al., 2022; OpenAI, 2023a; Anil et al., 2023; OpenAI, 2023b).\nIn Table 3, we also present the results obtained with ChatGPT6 (OpenAI, 2023a). Due to budget constraints, we were only able to evaluate randomly sampled 500 examples from the development set of each task. However, even with this limited evaluation, we can still gain insights that ChatGPT performs significantly worse than the SAE FLANT5 Base model on 5 out of 7 tasks. This emphasizes that merely scaling up the model is inadequate for tackling the challenge of dialect disparities. These limitations persist even in the context of large language models. Inspired by \"expert\" prompts (Odena et al., 2021; Shi et al., 2022), we incorporate a \"Native Speaker\" Prompt for Chat-\n6Engine: gpt-3.5-turbo. We conducted our ChatGPT experiments on May 16, 2023.\nGPT: \u201cYou are a native [DIALECT_NAME]\nEnglish speaker, and here is your task:\"\nHowever, ChatGPT + \"Native Speaker\" Prompt does not yield improved results and, in fact, performs even worse than the vanilla ChatGPT on all evaluated tasks. This highlights that dialect adaptation is not solved with trivial prompt-based interventions while being simultaneously less grounded in expert linguistic resources than DADA."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this paper, we present Dialect Adaptation via Dynamic Aggregation (DADA), a fine-grained and modular approach designed to adapt an established model trained on Standard American English to its dialect variants through the compositional aggregation of linguistic features. Our experiments demonstrate that the compositionality of DADA enables targeted adaptation to specific dialects, and demonstrated improved robustness across multiple\nevaluated dialects, including AppE, ChcE, CollSgE, IndE, and AAVE. Our analysis also highlights the interpretability of DADA, as shown through its capability to identify relevant linguistic features for a given input and trigger the corresponding adapters. Furthermore, our experiments on FLANT5 illustrate the potential of applying DADA to taskagnostic instruction-tuned large language models, showcasing its generalizability.\nLimitations\nDADA involves the training for feature adapters and the fusion layer, which can make it computationally expensive, especially when dealing with a substantial number of linguistic rules. However, each training run only requires a small number of parameters to be learned, and parallelization is feasible for feature adapter training. More importantly, these trained feature adapters exhibit significant reusability; the same set of feature adapters can be reused and employed for multiple dialects, though the fusion layer would need to be retrained for these dialects. However, if a use case does not involve significant reuses, this aspect may indeed remain a limitation. We will release our trained feature adapters so that future studies will not need to reincur the up-front training cost.\nFurthermore, while DADA has the flexibility to utilize any linguistic rules, in our experiments, we specifically employed these linguistic transformation rules that are well-established in prior work for English (Ziems et al., 2022, 2023). These rules were chosen because they were curated by linguists, validated by dialect speakers, and because English has many globally relevant dialects (Bird, 2022). However, evaluating DADA for other language groups and broader sets of lexical variation is key area for future work.\nWhile DADA mainly relies on Multi-VALUE (Ziems et al., 2022, 2023), they are orthogonal processes with different assumptions about dialect use. For each dialect, Multi-VALUE defines the density of a dialectal feature as the probability of the feature occurring when it is applicable, as well as the probability of the corresponding perturbation to be used in converting a sentence from SAE into that dialect. However, the actual prevalence of a feature heavily depends also on applicability.\nDADA instead focuses on adapting to the linguistic features present in a given sentence. We learn a parameterized compositional mixture of the\ndialectal features automatically, rather than relying on static assumptions of density. This avoids what we view as a major issue: it is often difficult to determine the dialect of an input since dialects themselves vary depending on context and speaker. The density of a dialectal feature represents an approximate of density across the entire dialect, but may not be accurate to a specific speaker and context (Koenecke et al., 2020). On the other hand, DADA can dynamically recognize the applicable dialectal features for a given input and activate the corresponding feature adapters. It remains to be explored in future work how the density of dialectal features, as captured in the linguistic literature, relates to the compositional mixture of these features as learned in the fusion layer of DADA.\nEthics Statement\nPrevious linguistic works on dialectal features may not fully or accurately document the natural usage patterns of all existing dialects in terms of their linguistic rules. As a result, we acknowledge that our proposed method DADA, which relies on these dialectal features from prior literature, may not take some undocumented features associated with dialects into account. However, by curating more dialectal features, our method can be easily extended to a broader range of dialects. Additionally, as DADA is task-agnostic when applied to instruction-tuned models (Sec 6), malicious individuals might misuse it. To address this concern, we will release DADA with a license that explicitly prohibits its usage for purposes of deception, impersonation, mockery, discrimination, hate speech, targeted harassment, and cultural appropriation targeting dialect-speaking communities."
        },
        {
            "heading": "Acknowledgement",
            "text": "We would like to thank the anonymous reviewers and SALT lab members for their valuable feedback. This work was partially sponsored by the Defense Advanced Research Project Agency (DARPA) grant HR00112290103/HR0011260656, and NSF grant IIS-2247357 and IIS-2308994."
        },
        {
            "heading": "A Tranformation Rules Details",
            "text": "Ziems et al. (2022, 2023) developed a collection of lexical and morphosyntactic transformation rules that account for the differences in linguistic features between SAE and its various dialect variants. In our study, we build upon this work by training transformation adapters for each rule in this collection. In their original paper, they present a comprehensive overview of each transformation rule in Appendix B. In Tables 9-21, they provide detailed Multi-VALUE implementations, including an enumeration of the implemented dialects and features, accompanied by illustrative examples for each.\nFurthermore, we provide detailed statistics for the respective synthetic training datasets (for MNLI task) associated with each linguistic rule for the AAVE dialect in Table 4. While we do not present statistics for every linguistic feature for all dialects across all evaluated tasks, we release our code, all synthetic datasets and the trained adapters, to further improve the reproducibility."
        },
        {
            "heading": "B Training Details",
            "text": "Multi-Dialect Adaptation We train feature adapters for each transformation rule using synthetic datasets, as described in Sec. 3.2, with learning rate 3e-4 and batch size 64 followed by Houlsby et al. (2019). To prevent significant performance differences among the trained feature adapters due to varying sizes of synthetic datasets, we fix the number of training steps to 10,000. For each feature adapter, we choose the checkpoint with the highest accuracy on the validation matched split of a synthetic dataset that applies all dialectal variations simultaneously, termed Multi. For dynamic\naggregation, we train a large fusion layer for 5 epochs on Multi. We set the learning rate to 2.5e-5 and the batch size to 64.\nMulti-Task Dialect Adaptation For feature adapter training, we set the learning rate to 1e-3 and fix the number of training steps as 50000. To fuse these feature adapters, we train a fusion layer for 5 epochs using a learning rate of 8e-5.\nThroughout the process of model training (including finetuning, adapter tuning, DADA training etc.), we consistently employ the standard training objectives specific to the tasks, such as crossentropy loss for classification tasks."
        },
        {
            "heading": "C Utilization Correlation Coefficients Plots",
            "text": "In Sec. 5, we showcase the effectiveness of DADA in adapting the RoBERTa Base (Liu et al., 2019b) model that has been finetuned on the original SAE MNLI training dataset to AAVE. To demonstrate the interpretability of DADA, we conduct an analysis of the utilization correlation among the aggregated 10 transformation adapters. We present utilization correlation coefficient plots for all layers in Figure 5 and 6."
        },
        {
            "heading": "D FLAN-T5 Templates",
            "text": "We provide here the templates used in Sec. 6 to train the FLAN-T5 model for each task. In the original paper by Chung et al. (2022), they defined 10 templates for each task and randomly applied them to each training example to enhance the model\u2019s robustness to varying instruction wordings. However, in our study, our goal is to demonstrate the generalizability of our proposed method DADA to instruction-tuned models, rather than focusing on improving the model\u2019s instruction-following capability. Therefore, for each task, we fix the usage of the first template from the set of 10 templates designed in the original paper.\nCoLA The Corpus of Linguistic Acceptability (CoLA; Warstadt et al. (2018)) task is a widely used benchmark that focuses on grammatical acceptability judgments. It aims to assess the ability of models to determine whether a given sentence is syntactically and semantically correct or not. For the CoLA task, we adopt the following template:\nSentence: {sentence} Would a linguist rate this sentence to be acceptable linguistically?\nI think the answer is {answer}\nMNLI The Multi-Genre Natural Language Inference (MNLI; Williams et al. (2018)) is designed to assess the model\u2019s ability to comprehend and reason. MNLI involves determining the logical relationship - entailment, contradiction, or neutrality - between a given premise and a corresponding hypothesis. For the MNLI task, we adopt the following template:\nPremise: {premise}\nHypothesis: {hypothesis}\nDoes the premise entail the hypothesis?\n{answer}\nQNLI The Question-answering Natural Language Inference (QNLI; (Wang et al., 2018)) task is a prominent benchmark that focuses on assessing\nthe ability of models to perform sentence-level semantic matching and reasoning. In this task, given a question and a corresponding sentence, the objec-\ntive is to determine whether the sentence contains the answer to the question, considering both linguistic and logical entailment. For the QNLI task,\nwe adopt the following template: Does the sentence {sentence} answer the question {question}\n{answer}\nQQP The Quora Question Pairs7 (QQP) task is a widely recognized benchmark that focuses on question sentence similarity. The task involves determining whether a pair of questions asked on the Quora platform is semantically equivalent or not. For the QQP task, we adopt the following template:\n{question1} {question2} Would you say that these questions are the same? {answer}\nSST-2 The SST-2 (Stanford Sentiment Treebank; Socher et al. (2013)) task is a widely used benchmark for sentiment analysis. It involves classifying the sentiment of a given sentence as either positive or negative. For the SST-2 task, we adopt the following template:\nReview: {sentence} Is this movie review sentence negative or positive? The answer is: {answer}\nSTS-B The Semantic Textual Similarity Benchmark (STS-B; Cer et al. (2017)) task is a widely recognized benchmark that evaluates the ability of models to assess the semantic similarity between pairs of sentences. The task involves assigning a similarity score to pairs of sentences based on their semantic equivalence. For the STS-B task, we adopt the following template:\n{sentence1} {sentence2}\nRate the textual similarity of these two sentences on a scale from 0 to 5, where 0 is \"no meaning overlap\" and 5 is \"means the same thing\".\n{answer}\n7https://www.kaggle.com/c/quora-question-pairs"
        }
    ],
    "title": "DADA: Dialect Adaptation via Dynamic Aggregation of Linguistic Rules",
    "year": 2023
}