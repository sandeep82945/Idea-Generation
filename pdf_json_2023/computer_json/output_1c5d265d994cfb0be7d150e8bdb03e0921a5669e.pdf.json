{
    "abstractText": "AI has been widely applied in scientific scenarios, such as robots performing chemical synthetic actions to free researchers frommonotonous experimental procedures. However, there exists a gap between humanreadable natural language descriptions and machine-executable instructions, of which the former are typically in numerous chemical articles, and the latter are currently compiled manually by experts. We apply the latest technology of pre-trained models and achieve automatic transcription between descriptions and instructions. We design a concise and comprehensive schema of instructions and construct an open-source human-annotated dataset consisting of 3950 description\u2013instruction pairs, with 9.2 operations in each instruction on average. We further propose knowledgeable pre-trained transcription models enhanced by multi-grained chemical knowledge. The performance of recent popular models and products showing great capability in automatic writing (e.g., ChatGPT) has also been explored. Experiments prove that our system improves the instruction compilation efficiency of researchers by at least 42%, and can generate fluent academic paragraphs of synthetic descriptions when given instructions, showing the great potential of pre-trained models in improving human productivity.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yi-Chen Nie"
        },
        {
            "affiliations": [],
            "name": "Ning Ding"
        },
        {
            "affiliations": [],
            "name": "Qian-Jun Ding"
        },
        {
            "affiliations": [],
            "name": "Wei-Ting Ye"
        },
        {
            "affiliations": [],
            "name": "Cheng Yang"
        },
        {
            "affiliations": [],
            "name": "Maosong Sun"
        },
        {
            "affiliations": [],
            "name": "Weinan E"
        },
        {
            "affiliations": [],
            "name": "Rong Zhu"
        }
    ],
    "id": "SP:bf67eeddcdec7e6840ce7ac413c6ba3efd199cd7",
    "references": [
        {
            "authors": [
                "M. Saeidi"
            ],
            "title": "Interpretation of Natural Language Rules in Conversational Machine Reading",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2018
        },
        {
            "authors": [
                "R. Nakano",
                "J. Hilton",
                "S. Balaji",
                "J. Wu",
                "L. Ouyang",
                "C. Kim",
                "C. Hesse",
                "S. Jain",
                "V. Kosaraju",
                "W. Saunders"
            ],
            "title": "WebGPT: Browser-assisted question-answering with human",
            "venue": "feedback, arXiv,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Chemical Science\nEDGE ARTICLE\nO pe\nn A\ncc es\ns A\nrt ic\nle . P\nub lis\nhe d\non 2\n4 A\nug us\nt 2 02\n3. D\now nl\noa de\nd on\n1 /9\n/2 02\n4 11\n:1 5:\n20 P\nM .\nT hi\ns ar\ntic le\nis li\nce ns\ned u\nnd er\na C\nre at\niv e\nC om\nm on\ns A\nttr ib\nut io\nnN\non C\nom m\ner ci\nal 3\n.0 U\nnp or\nte d\nL ic\nen ce\n.\nView Article Online View Journal | View Issue\nTranscription be\naDepartment of Computer Science and Te China. E-mail: liuzy@tsinghua.edu.cn bCollege of Chemistry and Molecular Engin E-mail: rongzhu@pku.edu.cn cSchool of Computer Science, Beijing Univ Beijing, China dCenter for Machine Learning Research and University AI for Science Institute, Beijing, C \u2020 These authors contributed equally to th\nCite this: Chem. Sci., 2023, 14, 9360\nAll publication charges for this article have been paid for by the Royal Society of Chemistry\nReceived 16th May 2023 Accepted 15th August 2023\nDOI: 10.1039/d3sc02483k\nrsc.li/chemical-science\n9360 | Chem. Sci., 2023, 14, 9360\u20139\ntween human-readable synthetic descriptions and machine-executable instructions: an application of the latest pre-training technology\nZheni Zeng, \u2020a Yi-Chen Nie,\u2020b Ning Ding,\u2020a Qian-Jun Ding,b Wei-Ting Ye,b Cheng Yang,c Maosong Sun,a Weinan E,d Rong Zhu*b and Zhiyuan Liu *a\nAI has been widely applied in scientific scenarios, such as robots performing chemical synthetic actions to\nfree researchers frommonotonous experimental procedures. However, there exists a gap between human-\nreadable natural language descriptions and machine-executable instructions, of which the former are\ntypically in numerous chemical articles, and the latter are currently compiled manually by experts. We\napply the latest technology of pre-trained models and achieve automatic transcription between\ndescriptions and instructions. We design a concise and comprehensive schema of instructions and\nconstruct an open-source human-annotated dataset consisting of 3950 description\u2013instruction pairs,\nwith 9.2 operations in each instruction on average. We further propose knowledgeable pre-trained\ntranscription models enhanced by multi-grained chemical knowledge. The performance of recent\npopular models and products showing great capability in automatic writing (e.g., ChatGPT) has also been\nexplored. Experiments prove that our system improves the instruction compilation efficiency of\nresearchers by at least 42%, and can generate fluent academic paragraphs of synthetic descriptions\nwhen given instructions, showing the great potential of pre-trained models in improving human\nproductivity."
        },
        {
            "heading": "1 Introduction",
            "text": "AI in chemistry is an emerging interdisciplinary eld that has achieved impressive results in various scenarios.1\u20133 Take chemical synthesis as an example; robotic synthesis systems have recently been developed to perform chemical synthetic actions following formatted instructions to free researchers from monotonous experimental procedures.4 Nevertheless, these systems are only capable of executing deterministic instructions manually compiled by human experts, while synthetic experimental procedures in the real world are typically described with natural language in numerous chemical articles. Due to the gap between machine-executable instructions and human-readable descriptions, it is exceedingly labor-intensive to repeatedly record the same procedures in two sets of language systems. This calls for the development of automatic\nchnology, Tsinghua University, Beijing,\neering, Peking University, Beijing, China.\nersity of Posts and Telecommunications,\nSchool of Mathematical Sciences, Peking hina is work.\n373\ntranscription of synthetic procedures between descriptions in the human cognitive space and instructions in the machine operative space.\nSpecically regarding the benets of transcription, there exists a large body of chemical literature describing experimental procedures for synthetic reactions. Once we have a description-to-instruction (D2I) transcription system, the vast amount of synthetic knowledge can be documented into the instruction library efficiently to enhance the robotic synthesis platforms. Correspondingly, discrete instruction options can be predicted and prompted easier than natural language descriptions. Once we have an instruction-to-description (I2D) transcription system, it is possible for chemical researchers to program instructions quickly instead of manually writing natural language descriptions in the literature.\nHowever, transcription requires a high level of natural language intelligence and chemical knowledge. For instance, the exibility and complexity of natural language stump the intuitive solutions with low generalizability, including natural language processing (NLP) modules for specic property extraction5 and manually designed rule mapping.6 Meanwhile, it is also challenging for general AI systems without chemical expertise to recognize the chemical terms, mine and complete implicit conditions, and master the grammar of instructions.\nIn response to the above challenges, we propose a knowledgeable transcription system equipped with pre-trained\n\u00a9 2023 The Author(s). Published by the Royal Society of Chemistry"
        },
        {
            "heading": "Edge Article Chemical Science",
            "text": "O pe\nn A\ncc es\ns A\nrt ic\nle . P\nub lis\nhe d\non 2\n4 A\nug us\nt 2 02\n3. D\now nl\noa de\nd on\n1 /9\n/2 02\n4 11\n:1 5:\n20 P\nM .\nT hi\ns ar\ntic le\nis li\nce ns\ned u\nnd er\na C\nre at\niv e\nC om\nm on\ns A\nttr ib\nut io\nnN\non C\nom m\ner ci\nal 3\n.0 U\nnp or\nte d\nL ic\nen ce\n. View Article Online models (PTMs). There are three key elements to achieve this system: the task dataset, the PTM, and the knowledge enhancement."
        },
        {
            "heading": "1.1 Task dataset",
            "text": "To activate and evaluate the data-driven deep learning models better, a dataset for synthetic transcription has to be constructed. We rst dene a concise and complete instruction schema for chemical synthetic actions according to statistical information and expert knowledge, containing 16 types of operations with 18 types of corresponding hierarchical arguments, with which the experimental procedures can be clearly expressed and the target of transcription is perspicuous. Following this schema, we construct an open-source humanannotated dataset, ChemTrans, consisting of 3950 pieces of OrgSyn paragraphs and their formatted synthetic instructions, with 9.2 operations in each instruction on average."
        },
        {
            "heading": "1.2 Pre-trained model",
            "text": "To understand and generate exible and complex natural language, we apply the pre-trained language models as the backbone of our system. PTMs have already been proven to acquire versatile knowledge implicitly and scattered in the unlabeled data,7 and have achieved impressive performances in various tasks, such as the open-domain chatting with ChatGPT. In this paper, we explore two solutions: (1) the ne-tuned models based on T5,8 an encoder\u2013decoder PTM; (2) the directly used large-scale models\u2021 represented by GPT-3.5. We prove the signicance of applying PTMs in the synthetic transcription scenario."
        },
        {
            "heading": "1.3 Knowledge enhancement",
            "text": "To enhance the system with rich knowledge as human experts when facing transcription tasks, we specically designed several training tasks to teach the model multi-grained chemical domain knowledge. (1) Word-level: we conduct masked language modeling by randomly masking words in chemical articles and learning to predict them. (2) Entity-level: we train the model to recognize all chemical entities obtained with SciSpacy tools9 in the given articles. (3) Operation-level: we also enable the model to identify the trigger words or phrases of synthetic actions and accomplish verb-operation mapping constructed by manually craed rules. (4) Sequence-level: we conduct decoder language modeling on the augmented pseudo instructions specially for the D2I model to learn unied instruction grammar. On the other hand, we apply the description\u2013instruction pairs generated in D2I to the I2Dmodel training to learn the language style of synthetic descriptions.\nWe comprehensively evaluate the performance of our transcription system on the ChemTrans dataset. (1) For both D2I and I2D, our system has achieved an overall satisfying effect. In particular, the system can even achieve next-operation prediction without giving original descriptions. (2) Evaluationmetrics, including the BLEU score10 and the newly dened SeqMatch score, unanimously verify the remarkable improvements that the PTMs and multi-grained knowledge enhancement bring. (3)\n\u00a9 2023 The Author(s). Published by the Royal Society of Chemistry\nThe recently popular large PTMs (e.g., GPT-3.5) are proven to have the basic transcription capability with few or even no instances, while obtaining unsatisfactory performance on chemical details and reliability. This shows that large-scale PTMs have great potential and also challenges in scientic applications. (4) The case study and human annotator test show the practical usage of our model. Users can competently obtain machine-executable results (17+ times faster than human annotators) or human-readable descriptions and improve efficiency even if we require manual verication (42% faster).\nWe release our code and dataset to encourage more researchers to tap the potential of machine intelligence in chemistry. Besides, we provide the conversion results for over 50 000 chemical reactions to form a machine-executable instruction library that facilitates automatic chemical synthesis. In the long run, the transcription process is an essential part of the grand picture of fully automatic synthetic chemistry. Meanwhile, implementing this technique is expected to facilitate the standardization and structuralization of the raw data reporting experimental procedures, which is crucial for reproducing and optimizing chemical reactions."
        },
        {
            "heading": "2 Results and discussion",
            "text": ""
        },
        {
            "heading": "2.1 Task formalization and dataset",
            "text": "We dene the mutual transcription between descriptions and instructions as mutual sequence-to-sequence tasks. For D2I transcription, the document-level natural language texts that describe multi-step chemical synthesis are fed as input, and synthetic instructions are expected to be formatted and transcribed from the given texts. These instructions can be easily disassembled into machine-executable programs. For I2D transcription, we exchange the input and output of D2I. To formalize the transcription tasks, we rst design a schema of synthetic instructions and then construct a dataset facilitating the training process of deep learning systems. We adopt the supplementary information (SI) of Organic Syntheses (OrgSyn)\u00a7 as the source corpus to construct the schema and annotate the dataset since the SI paragraphs attached to the primary synthesis articles mention a considerable number of reactants and procedures.\n2.1.1 Synthetic instruction schema with high coverage. Overall, we design the schema for two types of objects: reagents and operations. As Fig. 2 shows, the former have properties including mass, volume, concentration, and so on, which are dened as the arguments for a specic reagent object. The latter cover 16 types of actions, such as add, set temperature, and quench, which are summarized by human experts according to verb frequency statistics. Each operation is described by several arguments such as phase and pressure. Mixture reagents may be composed of several pure reagents. For instance, \u201ca solution of 131.4 g sodium periodate (NaIO4) in 2 L water\u201d contains sodium periodate (NaIO4) and water. Reagents can also be arguments for the operations. For instance, the arguments of the WASH operation can be the retained phase and the REAGENT used for washing, which is further described by arguments such as composition and volume. More details are\nChem. Sci., 2023, 14, 9360\u20139373 | 9361\nFig. 2 The hierarchical schema is designed for transcription. The first level displays the operations, and the second level displays the corresponding arguments, in which the REAGENT is further described by arguments in the third level.\nFig. 1 The schematic diagram for the transcription pipeline. For D2I and I2D tasks, both small PTMs with knowledge enhancement and large PTMs with few-shot demonstration can achieve satisfying performances."
        },
        {
            "heading": "Chemical Science Edge Article",
            "text": "O pe\nn A\ncc es\ns A\nrt ic\nle . P\nub lis\nhe d\non 2\n4 A\nug us\nt 2 02\n3. D\now nl\noa de\nd on\n1 /9\n/2 02\n4 11\n:1 5:\n20 P\nM .\nT hi\ns ar\ntic le\nis li\nce ns\ned u\nnd er\na C\nre at\niv e\nC om\nm on\ns A\nttr ib\nut io\nnN\non C\nom m\ner ci\nal 3\n.0 U\nnp or\nte d\nL ic\nen ce\n. View Article Online\nshown in the Methods section. Fig. 1 shows an example of the transcription from a textual description to complicated formatted instructions composed of the operation names and the corresponding arguments.\nSpecic to the operation selection, our principle is to choose the frequently appearing meta operations that have clear synthetic meanings (e.g. \u201cmove\u201d describes a possible part of an action but has no synthetic meaning) and are indivisible (e.g. \u201cneutralize\u201d can be replaced by other operations such as adding acid/base). We refer to statistical information to guarantee coverage and invite experts to ensure the rigor of the schema. Details are shown in the Methods section.\n2.1.2 Manually annotated large-scale dataset. Following the above schema, we annotate altogether 3950 paragraphs of OrgSyn SI for instruction transcription, on average with 154.6 words per input paragraph and with 9.2 operations, 34.3 arguments, and 176.0 words per transcribed instruction. This\n9362 | Chem. Sci., 2023, 14, 9360\u20139373\ndataset, ChemTrans, is randomly split into training (2765), validation (395), and testing (790) sets. We analyze the distribution of the manually annotated operations in the testing set, and compare it with the sum of the automatically annotated verb mentions that accurately map to operations. Overall, as shown in Fig. 4, the proportion of occurrence frequency of various operations is generally consistent with the automatic verb recognition from the corpus, proving the rationality of concluding operations from verb frequency statistics. Meanwhile, the distribution of different operations is unbalanced in our dataset, increasing the difficulty of transcription."
        },
        {
            "heading": "2.2 Pre-trained models",
            "text": "We conduct experimental comparisons between different model architectures and training settings to verify the effectiveness of PTMs. There are overall two types of PTMs we try: (1) Small PTMs. For models such as T5,8 a text-to-text transformerbased PTM with the encoder\u2013decoder architecture, they can be applied to downstream tasks by transfer learning. In this paper, we further train the small PTMs with knowledge enhancement which is introduced in the next subsection, and then ne-tune\n\u00a9 2023 The Author(s). Published by the Royal Society of Chemistry\nFig. 4 Operation-level results analysis of D2I/I2DPTM-large models on the ChemTrans testing set. We compare our model with the rulebased operation extraction method, which is shown in gray. In this table, \u201cverb\u201d refers to the sum of automatically annotated verb mentions that can be accurately mapped to the operations; \u201clabel\u201d refers to the human-annotated statistics; \u201cdup\u201d, \u201cign\u201d and \u201cdef\u201d respectively represent the duplicated, ignored and defective error statistics. Orange indicates that the proportion of errors in this operation is significantly higher compared with others, and blue is the opposite."
        },
        {
            "heading": "Edge Article Chemical Science",
            "text": "O pe\nn A\ncc es\ns A\nrt ic\nle . P\nub lis\nhe d\non 2\n4 A\nug us\nt 2 02\n3. D\now nl\noa de\nd on\n1 /9\n/2 02\n4 11\n:1 5:\n20 P\nM .\nT hi\ns ar\ntic le\nis li\nce ns\ned u\nnd er\na C\nre at\niv e\nC om\nm on\ns A\nttr ib\nut io\nnN\non C\nom m\ner ci\nal 3\n.0 U\nnp or\nte d\nL ic\nen ce\n. View Article Online\nthe models on the ChemTrans dataset. (2) Large PTMs. For models such as GPT-3.5 (chat completion mode),11 they are proven to be good few-shot learners and can nish downstream tasks given a few instances. Meanwhile, the tuning for large PTMs is time-consuming. In this paper, we directly provide the large PTM instances and input text waiting for transcription.\nAs shown in Table 1, we study various types of models for transcription:\nTransformer refers to a simple model without any pretraining tasks, and with the same parameter scale as the\nprevious framework for a similar transcription task.12 The Transformer+ variant has been involved in all grains of knowledgeable enhancement. Both of them are tuned and evaluated on ChemTrans.\nD2IPTM and I2DPTM use the small PTM T5 as the backbone model. T5 (the w/o multi variant) is not involved in knowledgeable enhancement tasks. For the other variants, w/o seq is knowledge-enhanced on tasks except for the augmented generation; w/o o+e is enhanced on tasks except the verb mapping and chemical entity recognition; w/o word is only enhanced on the augmented generation. The original versions of D2IPTM and I2DPTM are fully involved in all the knowledgeable enhancement tasks.\nGPT-3.5 (text-davinci-003 completion mode) and GPT-3.5chat (gpt-3.5-turbo chat completion mode) are adopted as representative large PTMs, and all the versions are not specially tuned before being evaluated on ChemTrans. The original version displays the zero-shot performance; 3-shot is given 3 randomly picked training instances, and 3-shot* is given 3 training instances that have the highest similarities with the current testing instance. Notice that for D2I, the zero-shot model has never seen the grammar of instructions, therefore we provide the schema information in the task prex. Details are shown in the Methods section.\nIt is noticed that in the table, boldfaced numbers indicate a signicant advantage over the T5 results of more than 1 point in the one-sided t-test with p-value < 0.02."
        },
        {
            "heading": "2.3 Knowledge enhancement tasks",
            "text": "Inspired by human researchers transcribing with the assistance of multiple types of knowledge (as we have described in the introduction), we design the following four knowledge enhancement tasks that are associated with different granularities but are conducted simultaneously. Small PTMs are\nqMatch-A/O and EM stands for ExactMatch"
        },
        {
            "heading": "Chemical Science Edge Article",
            "text": "O pe\nn A\ncc es\ns A\nrt ic\nle . P\nub lis\nhe d\non 2\n4 A\nug us\nt 2 02\n3. D\now nl\noa de\nd on\n1 /9\n/2 02\n4 11\n:1 5:\n20 P\nM .\nT hi\ns ar\ntic le\nis li\nce ns\ned u\nnd er\na C\nre at\niv e\nC om\nm on\ns A\nttr ib\nut io\nnN\non C\nom m\ner ci\nal 3\n.0 U\nnp or\nte d\nL ic\nen ce\n. View Article Online trained on these tasks aer their general pre-training but before the task-oriented ne-tuning.\n2.3.1 Word-level masked language modeling. We expect the model to be equipped with chemical commonsense and linguistic knowledge. To do so, masked language modeling is adopted, which is one of the basic pre-training tasks for NLP models, masking 15% of the input tokens randomly and requiring the model to recover. Chemical synthesis articles are used to conduct the self-supervised training.\n2.3.2 Entity-level chemical recognition. Synthesis articles are organized around chemical entities and their various properties and operations, thus it is important to recognize and understand these entities. To do so, the model is given the article text and required to replace all the chemical entities with a specic token. Training data for this task is automatically annotated with the help of SciSpacy.9\n2.3.3 Operation-level verb mapping. There are typical expressions including specic verbs indicating synthetic operations in the chemical text (e.g. \u201cstir\u201d is most likely related to SETTEMP in our schema), and we hope to summarize the expert experience and inject the verb-operation mapping knowledge into themodel. Given the original text, the model is supposed to map those verbs that exist in our keyword list to the corresponding operations. Training data is also roughly annotated with SciSpacy. In practice, the verb mapping and chemical recognition tasks are both given the original text as the input, therefore they are combined during training.\n2.3.4 Sequence-level augmented generation. D2I models are supposed to learn the special grammar of the instructions, which is structured text that is different from natural language. Adapting the model to a new output style can be done with language modeling training on the decoder, which maximizes the probability of generating specic sequences. The training data is automatically augmented by randomly sampling and substituting the operation sequences and corresponding arguments in the ChemTrans training set. Correspondingly, I2D models are supposed to generate descriptions in the special style according to the provided instructions, and the I2D data can also be automatically augmented by applying D2I models to the mass of synthesis descriptions in the literature."
        },
        {
            "heading": "2.4 Evaluation metrics",
            "text": "For I2D transcription, we adopt the BLEU score10 and ROUGE score13 for our evaluation, which are both traditional and popular metrics for sequence-to-sequence generation tasks such as machine translation. BLEU focuses on precision while ROUGE cares more about the recall of the generation. We also adopt the Distinct unigram14 to evaluate the diversity of the generated results.\nFor D2I transcription, we propose a new solution which treats the predicted and the labeled operations as a sequence matching task, since it is hard to specify the location of the predicted item by rules and to count the accuracy. This is similar to protein sequence alignment, and one popular solution is dynamic programming, an efficient technique that helps solve problems with overlapping subproblems or optimal\n9364 | Chem. Sci., 2023, 14, 9360\u20139373\nsubstructures. When conducting dynamic programming, the reward rules for matching and mismatching have to be specied. We try two different reward rules, among which SeqMatchO focuses on the performance of operation prediction, and SeqMatch-A further takes the performance of argument prediction into consideration.\nTo be specic, SeqMatch-O is to give 1 point for the matched position and 0 for the missed/unmatched/redundant position, which only cares about the quality of operation classication. The sum of the reward scores is divided by the average length of the prediction sequence and ground-truth sequence. For instance, as shown in Fig. 3, the system predicts a 5-operation sequence while the answer is a 6-operation sequence, and the longest matching subsequence is with 4 operations. Therefore, the sum of reward scores is 4 and the average length of sequences is 5.5, and we get SeqMatch \u2212 O = 0.727. Further, SeqMatch-A is to change the reward point to the average normalized BLEU score of the corresponding arguments for the current operation. In the same instance as above, we calculate the BLEU score for the text of each argument, and also divide the sum by the average number of predicted arguments and the answer arguments. In this way, we get the reward score for the rst position (ADD) as 0.444 to replace the original 1.\nIn addition, we also adopt the BLEU score and ExactMatch score for D2I evaluation. BLEU score measures the proportion of n-grams (n words in a row) onmatches. However, the n-grams in the generated instructions are not equally important, such as a bracket delimiter and an operation. We mainly consider the operations and the key arguments, therefore we believe that the SeqMatch score is more reasonable. As for ExactMatch, we provide the proportion of the perfectly predicted items. To avoid the inuence of unimportant factors such as capitalization, spaces, reagent addition order, etc., we regard the results that have the difflib similarity with answers higher than 0.95 as perfect predictions."
        },
        {
            "heading": "2.5 Results analysis",
            "text": "We analyze the experiment results from the following aspects: the overall performance of transcription tasks, the numerical result comparison between different settings, and the impact of the amount of training data. Lastly, we provide human evaluation on a case study.\n2.5.1 Overall transcription performance. For D2I transcription, large PTMs can disassemble complex descriptions into clear and concise synthesis steps quite well, and can also comprehensively master the grammar of instructions through a few instances to some extent. However, small PTMs can be better adapted to the task by ne-tuning. Models without pretraining show unsatisfactory performance.\nSpecically, Fig. 4 shows the operation-level statistics of D2IPTM-large and I2DPTM-large on the ChemTrans testing set. There are altogether 3 types of error: (1) duplicated: the predicted operation does not exist in the answer, and is skipped in dynamic programming matching; (2) ignored: the operation mentioned in the original paragraph has not been predicted successfully; (3) defective: the matched operation has a low\n\u00a9 2023 The Author(s). Published by the Royal Society of Chemistry"
        },
        {
            "heading": "Edge Article Chemical Science",
            "text": "O pe\nn A\ncc es\ns A\nrt ic\nle . P\nub lis\nhe d\non 2\n4 A\nug us\nt 2 02\n3. D\now nl\noa de\nd on\n1 /9\n/2 02\n4 11\n:1 5:\n20 P\nM .\nT hi\ns ar\ntic le\nis li\nce ns\ned u\nnd er\na C\nre at\niv e\nC om\nm on\ns A\nttr ib\nut io\nnN\non C\nom m\ner ci\nal 3\n.0 U\nnp or\nte d\nL ic\nen ce\n. View Article Online\nquality of argument prediction, with the BLEU score lower than 0.6. To verify the challenge of ChemTrans and the necessity of adopting deep learning models, we also adopt the rule-based system for pre-training verb-operation mapping to automatically recognize operation sequences for D2I. Since this method cannot process the argument extraction, the defective error is therefore meaningless. The other 2 types of error are shown in the grey cubes of Fig. 4, which are much more than what D2IPTM-large makes.\nIntuitively, the small PTM with knowledge enhancement can successfully read and transcribe most of the basic operations with any type of overall error rate lower than 15%. For the 6807 testing set operations in total, there are 894 duplicated, 837 ignored, and 148 defective error operations. The operations WASH and EXTRACT are processed quite well, which may benet from the explicitness of the related descriptions. In contrast, several operations bring challenges to our system causing both many duplicated and ignored errors, including REFLUX, QUENCH, and COLUMN. The operation REFLUX has similar keywords and expressions to SETTEMP, and therefore is easy to be misjudged as other operations. Other operations with small sample sizes (fewer than 100 in the testing set) still have room for improvement. As for defective argument prediction, the operation ADD uses extracted reagent information as arguments, which are diverse in terms of expressions and may be hard to recognize in some cases.\nFor I2D transcription, large PTMs show their basic capability of describing synthetic instructions in uent and professional natural language, while it is difficult to accurately generate the description style we need. Meanwhile, too powerful generation ability leads to over-imagination of large PTMs, and sometimes it deviates from the given instructions. In comparison, small PTMs are ne-tuned to strictly follow instructions during generation. Similarly, models without pre-training are not up to the task and cannot even generate readable paragraphs.\nWe also provide the operation-level analysis by reversely transcribing the I2D generated results into instructions and comparing with the initial inputs. From Fig. 4 we can see that the performance of the two tasks shows a very similar distribution. I2D makes obviously less duplicated error, of which one possible reason is that the predicted descriptions do not involve extraneous details discarded by the instructions, and the reversely transcribed instructions are naturally concise without much duplication.\n2.5.2 Numerical result comparison between different settings. Experiment results are shown in Table 1. In general, all the metrics for both tasks are roughly positive correlations. We can draw the following conclusions:\n(1) Pre-training plays a vital role in the mutual transcription tasks. For D2I, the tuned transformer performs much worse than PTMs, showing unsatisfactory language understanding capability. For I2D, moreover, it even fails to generate reasonable and coherent long paragraphs considering the too-low ROUGE-4 and BLEU-4 scores. In contrast, the PTMs show an overall impressive performance.\n(2) The pre-training-ne-tuning paradigm is more suitable for special tasks in specic domains when a small set of data is\n\u00a9 2023 The Author(s). Published by the Royal Society of Chemistry\navailable. For the large PTMs which are not tuned in a targeted manner, more instances (especially the highly related instances) can help them perform better, while the number of instances is limited by the length of the allowed input. Especially for the I2D task, large PTMs generate readable and reasonable results which are not similar to the ground-truth answers that we expected. Notice that GPT-3.5-chat performs worse than GPT-3.5, and this may also be caused by the overly exible dialog capability, leading to a reduced ability to follow task instructions. Therefore, the tuned small PTMs are better choices for our task.\n(3) Multi-grained knowledge enhancement is proven to be effective. Compared with the strong T5 models involved in general pre-training, the D2IPTM we propose performs better under both base and large settings. The ablation study has shown that the knowledgeable training tasks at each granularity we propose are necessary, contributing to performance improvement evenly for D2I, and the sequence-level augmentation has the most critical impact for I2D.\n2.5.3 The impact of few-shot instances. As we can observe, large PTMs learn the schema and transcription rules from the instances much better than directly reading the schema denition. Usually, the more instances we provide, the better performances these PTMs can achieve, while the maximum input length limits the number of instances they can read. We adjust the number of GPT-3.5 few-shot learning random instances from 1 to 6 (in which the input paragraph is already over 2.5k words) and show the results in Fig. 5. The performance growth has attened out with increasing number of instances.\nBesides, it is worth mentioning that randomly picked instances are not the best choice, since they may not cover the operations that we currently need. The 3-shot* version that nds the most similar training instances can partly solve this problem. Meanwhile, we nd out combinations of three training instances that can cover all of the operations, in which the shortest have altogether 19 operations, and the longest has 68. From Fig. 5 we can see that the longest combination with more complicated instances carries more information, and this helps the large PTM achieve better D2I. Similar instances can further provide hints about language style, and bring improvement, especially for I2D. But overall, there is still a gap between the results of the large PTMs and the small ne-tuned models.\nChem. Sci., 2023, 14, 9360\u20139373 | 9365\nFig. 6 Performance for different amounts of training data."
        },
        {
            "heading": "Chemical Science Edge Article",
            "text": "O pe\nn A\ncc es\ns A\nrt ic\nle . P\nub lis\nhe d\non 2\n4 A\nug us\nt 2 02\n3. D\now nl\noa de\nd on\n1 /9\n/2 02\n4 11\n:1 5:\n20 P\nM .\nT hi\ns ar\ntic le\nis li\nce ns\ned u\nnd er\na C\nre at\niv e\nC om\nm on\ns A\nttr ib\nut io\nnN\non C\nom m\ner ci\nal 3\n.0 U\nnp or\nte d\nL ic\nen ce\n. View Article Online\n2.5.4 The impact of the amount of training data. Human experts annotating labels in the chemical domain is expensive and time-consuming, therefore it is not always possible to obtain large amounts of training data when transferring to new scenarios. To analyze the quick learning capability of models and the scale characteristics of our dataset, we take 10%, 30%, 50%, 70%, and 90% of the training set and keep other settings unchanged for the T5-base and our knowledge-enhanced base models training. From Fig. 6 conclusions may be drawn that: (1) the amount of the ChemTrans dataset we proposed basically meets the requirement for ne-tuning small PTMs so that the trend of the curve rising with the increase of the data volume slows down; (2) our models demonstrate a greater advantage in low-resource scenarios for both D2I and I2D, showing that the multi-grained knowledge enhancement enables the model to have the ability to adopt quick transfer learning.\n2.5.5 Further capability to predict synthetic actions. Currently, our system can achieve automatic transcription of natural language description and structured synthetic instruction, and this convinces us that the system may master some general laws of synthesis, remembering and even predicting the synthetic actions for the given chemicals. To this end, nextoperation prediction experiments have been conducted and are shown in Table 2.\nWe conduct this experiment still on the ChemTrans dataset. Given the beginning part of the operation sequences (no shorter than 4 operations), we require the models to predict the next synthetic operation. For state transitions between discrete operations, the traditional machine learning algorithm hidden Markov model (HMM15) is a good choice. For the PTMmethods, we test T5-large and our D2IPTM-large on this task, directly generating the names of the next operations. As shown in the results, PTMs can comprehend the synthetic process and predict the operations much better than the traditional HMM. With the knowledge enhancement, our model shows an even\ngreater improvement, and can predict the next operation precisely in nearly half of the cases.\nObserving the generated results, we can see that our model also successfully learns some general rules, and here we provide several instances for commonly generated combinations:\n* ADD sodium/bromine speed: slowly * QUENCH / EXTRACT / DRY * ADD / SETTEMP overnight / FILTER / WASH The above combinations are chemically reasonable. For example, it is an important safety reminder that introducing hazardous substances should be done slowly. It is also a commonly coupled operation to extract with ether or other organic solvents aer quenching with aqueous solutions and then drying with anhydrous magnesium sulfate or other dehydrates. This indicates that the latest pre-training technology may have greater potential in assisting synthetic chemistry."
        },
        {
            "heading": "2.6 Manual evaluation",
            "text": "For a more comprehensive and exible evaluation of our schema and model, we randomly sample several input paragraphs from the testing set and perform a manual inspection of the output results. Detailed examples are shown in Fig. 7.\n2.6.1 Fine-tuned small PTMs can transcribe the descriptions and instructions more precisely. In the rst I2D example, the ADD operation refers to the redissolving of the ltered 9- acetylanthracene, and the result of our I2DPTM is the only paragraph that expresses this process clearly. For D2I transcription, GPT-3.5 fails to generate effective instructions under the zero-shot setting, and also leaves some important information (such as the time of REFLUX) under the 3-shot setting, while D2IPTM generates a quite accurate instruction paragraph. Nevertheless, there also exists annotation error that may hurt the model performance during ne-tuning, such as the missing component description of the mixture solution in the second example.\n2.6.2 Providing appropriate instances for large PTMs can obviously improve the generation quality. In the second I2D example, GPT-3.5 does not understand that the SETTEMP operation will enable the stirring by default, while it learns this rule from the given instances and successfully generates the expression of stirring for 18 hours. Though not displayed in the gure, we have also observed that compared with the randomly sampled instances, the most similar instances with the current query have achieved higher scores and can avoid random divergent thinking of the model. As shown in the second example, the 3-shot* version model supplements a reasonable device description, and generates most of the operation correctly.\n2.6.3 The effectiveness in practical scenarios. To evaluate the practical utility of our system, we consider two scenarios: automatic transcription and semi-automatic transcription with manual verication. We invite two researchers who are familiar with chemical synthesis text and the labeled instructions to test their efficiency on the transcription task. Our system takes 12 seconds to generate a paragraph on average. For human beings, the written-from-scratch setting takes 209 seconds per\n\u00a9 2023 The Author(s). Published by the Royal Society of Chemistry\nFig. 7 Case study for ChemTrans. We show two instances and compare the mutual generated results (left for descriptions and right for instructions). Blue fragments aremissed by themodels, and orange fragments are generated redundantly. Green fragments are not in the answer although they are reasonable."
        },
        {
            "heading": "Edge Article Chemical Science",
            "text": "O pe\nn A\ncc es\ns A\nrt ic\nle . P\nub lis\nhe d\non 2\n4 A\nug us\nt 2 02\n3. D\now nl\noa de\nd on\n1 /9\n/2 02\n4 11\n:1 5:\n20 P\nM .\nT hi\ns ar\ntic le\nis li\nce ns\ned u\nnd er\na C\nre at\niv e\nC om\nm on\ns A\nttr ib\nut io\nnN\non C\nom m\ner ci\nal 3\n.0 U\nnp or\nte d\nL ic\nen ce\n. View Article Online\nparagraph on average, and the revision setting takes 122 seconds. Even if we require a manual verication process, the system can still greatly improve our efficiency.\nWe then try other similar methods for synthetic description\u2013 instruction transcription. Reversely generating descriptions has not been explored much, while there exist some D2I systems. RXN12 refers to a text-to-procedure deep learning system based on a smaller transformer, and ChemIDE16 is a pure rule-based procedure extraction system. The two systems have different schemas from ours, but we can still observe their outputs manually. As shown in Fig. 8, the rule-based ChemIDE has\n\u00a9 2023 The Author(s). Published by the Royal Society of Chemistry\nmissed lots of information about reagents (e.g., the main reactant in the rst sentence), and the recall for operation recognition is also quite low (e.g., DRY and YIELD at the end). RXN shows an overall much better performance, while still makes some mistakes, such as recognizing \u201cstep A\u201d as a reagent. Our system provides the most complete necessary information and parameters, and the expression of our schema is also very clear. For example, silica gel is used for column chromatography, which is far from simply adding the adsorbent into the system, and this information is only successfully expressed under our schema.\nChem. Sci., 2023, 14, 9360\u20139373 | 9367\nFig. 8 D2I comparison between different transcription systems."
        },
        {
            "heading": "Chemical Science Edge Article",
            "text": "O pe\nn A\ncc es\ns A\nrt ic\nle . P\nub lis\nhe d\non 2\n4 A\nug us\nt 2 02\n3. D\now nl\noa de\nd on\n1 /9\n/2 02\n4 11\n:1 5:\n20 P\nM .\nT hi\ns ar\ntic le\nis li\nce ns\ned u\nnd er\na C\nre at\niv e\nC om\nm on\ns A\nttr ib\nut io\nnN\non C\nom m\ner ci\nal 3\n.0 U\nnp or\nte d\nL ic\nen ce\n. View Article Online\nWe also compare our automatically generated instruction library with the original workup records in the Open Reaction Database (ORD) which will be introduced in the Methods section. For a paragraph of experimental procedures, only a few instructions \u201cCONCENTRATION \u2013 WASH \u2013 DRY \u2013 FILTRATION \u2013 CONCENTRATION\u201d are recorded, and the details are simply provided in the form of original text segments (e.g., \u201cwash with 0.5 M hydrochloric acid that\u201d). In contrast, our system successfully recognizes the details for the related reagents, and provides the complete \u201cADD \u2013 ADD \u2013 SETTEMP \u2013 EVAPORATE \u2013 ADD \u2013 WASH \u2013 DRY \u2013 FILTER \u2013 EVAPORATE \u2013 COLUMN\u201d pipeline with corresponding arguments (e.g., the name, concentration and mass for the three washing reagents). The instructions in our library have much higher practical value for automatic synthesis than existing records."
        },
        {
            "heading": "2.7 Discussion",
            "text": "Our system makes it possible to automatically make use of the mass articles in databases and instruct machines to carry out the preparation of known compounds. Meanwhile, it can conduct assisted writing in the eld of synthetic chemistry to signicantly improve the efficiency of researchers. To the best of our knowledge, this is the rst fully open-source work (including data, code and models) on automated synthetic transcription, which may become a signicant benchmark to inspire the standardized recording of synthetic actions based on our schema, and the development of an automatic synthetic database.\nStill, there is space for improvement in our system. Currently, the structured generation results need to be further compiled to form the lower computer instructions, like the temperature argument \u201ccold (1\u20134\u00b0)\u201d has to be translated to specic temperature-controlling actions. Besides, the schema set we designed can only express single reaction streamlines. For more\n9368 | Chem. Sci., 2023, 14, 9360\u20139373\ncomplicated situations (e.g. different fractions are multi-step processed separately and then combined), temporarily our benchmark does not have a good solution and thus ignores them. With more powerful large PTMs at hand in the future, the system is supposed to have the ability to decompose complex problems into sub-problems. Considering the impressive crossmodal comprehension capability of the recent large models (e.g., GPT-4 (ref. 17)), heterogeneous information including molecule images and reaction formulae is also expected to be recognized by the model to alleviate the information loss problem mentioned in the case study, and previous research has already explored the heterogeneous bridging problem.18"
        },
        {
            "heading": "3 Methods",
            "text": ""
        },
        {
            "heading": "3.1 Related work",
            "text": "Automatic chemical synthesis dates back half a century,19,20 targeting highly specialized substances. To free human researchers from labor-intensive actions, comprehensive platforms which can handle multiple synthetic paradigms are proposed to assist organic synthesis.21 Current robotic systems can achieve dozens of different reactions requiring multiple synthesis steps.22 Researchers go a step further and expect the systems to have automatic analysis capabilities, such as searching for new reactivity.23 Given a target compound, the systems predict the intermediate,24 search the templates and plan the synthetic actions automatically.25 For better compatibility with information such as human instructions, literature knowledge, and reaction templates, formalization of the synthetic scheme is necessary, thus a chemical programming language is proposed.26 Capturing and coding the synthesis information in articles are also emphasized.6\nSpecic to reading the literature and extracting chemical information, many algorithms rely on the manually designed\n\u00a9 2023 The Author(s). Published by the Royal Society of Chemistry"
        },
        {
            "heading": "Edge Article Chemical Science",
            "text": "O pe\nn A\ncc es\ns A\nrt ic\nle . P\nub lis\nhe d\non 2\n4 A\nug us\nt 2 02\n3. D\now nl\noa de\nd on\n1 /9\n/2 02\n4 11\n:1 5:\n20 P\nM .\nT hi\ns ar\ntic le\nis li\nce ns\ned u\nnd er\na C\nre at\niv e\nC om\nm on\ns A\nttr ib\nut io\nnN\non C\nom m\ner ci\nal 3\n.0 U\nnp or\nte d\nL ic\nen ce\n. View Article Online rules, recognizing chemical attributes sentence by sentence.27\nThis is proven to be practical and necessary when there is no large dataset for conducting supervised learning,16 but it still requires manual modication. Machine learning methods are further adopted into the processing pipeline,28\u201330 improving the performance of chemical, operation, and relation recognition to some extent.\nHowever, for the development of machine reading, the breakthrough comes in the era of deep learning. Rule-based methods still play an important role in the interpretation improvement,31 while the deep learning models achieve satisfying performance in most of the NLP scenarios, including the language\u2013action transcription that we are concerned about.12 Especially in recent years, the bigger models with transformer32 blocks and the better pre-training methods show their power and achieve surprising performances in various NLP tasks, including event detection and argument extraction33,34 which has some similarities to our task. For natural language understanding, PTMs including BERT,35 XLNet,36 and RoBERTa37 nish some tasks at a comparable level with human beings. For natural language generation, PTMs such as GPT-2,38 BART,39 and T5 (ref. 8) are proven to be effective while they still have a lot of room for improvement.\nIn this work, we require the model to decode in a pre-dened instruction space. Machine reading systems equipped with deep learning have been proven to be effective for comprehending cognitive content and generating specic operations including the utilization of search engines.40 To achieve formatted generation, the decoding process of models can be restricted in different ways, such as the pointer mechanism in NER tasks,41 and slot-lling modules in text-to-SQL tasks.42 More oen, the models autonomously learn the ability of formatted output through pre-training on large-scale domain data.43 Therefore, we choose the neat and elegant approach to provide text input and formatted output for the deep learning models. The experiment results and cases show that PTMs perfectly decode in the rule we dene."
        },
        {
            "heading": "3.2 Corpus",
            "text": "OrgSyn collects the publication of reliable methods for the preparation of organic compounds. The methods are usually described in the SI of articles in the chemical synthesis domain. Procedures in OrgSyn are open to the public and can be easily accessed. Besides, these procedures are peer-reviewed and reliable, thus they can be used as benchmarks in the evaluation of our NLP models and automatic synthesis. Permission for our research and subsequent annotation has already been provided by OrgSyn.\nData from the ORD is open-accessed.{ We download all the data, lter out short segments (string length less than 100) and meaningless characters such as URLs, timestamps, chemical formulae, etc., and perform deduplication operations (delete if the similarity of adjacent paragraphs is greater than 0.8). In this way, we get 160m pieces of text describing synthetic actions in a similar style to OrgSyn text. We use 251 689 paragraphs with 100.5 words on average for the multi-grained knowledgeable pre-\n\u00a9 2023 The Author(s). Published by the Royal Society of Chemistry\ntraining. The corpus is also applied for constructing a machineexecutable instruction library. We generate the synthetic instructions of 50 000 pieces of ORD data that ensure grammatical plausibility. The generated instructions can be queried with the chemical reactions or expected products, and are much more detailed than existing pipeline records in the original database. This library can be automatically enlarged easily in the future."
        },
        {
            "heading": "3.3 Schema and dataset",
            "text": "For the denition of the ChemTrans schema actions, we rst present the top 100 high-frequency verbs in our corpus in Table 3, which cover over 80% of all the detected 51 400 verb occurrences. For all these frequently mentioned verbs, we lter out 17.0% of the verb mentions that are unrelated to the synthesis operations. For the other 83.0% of the verb mentions, we further discard 3.3% of them that are chemical instrument actions difficult to achieve automatically (e.g., \u201cconnected\u201d), or relatively unnecessary in our scenes (e.g., \u201cevacuate\u201d is discarded since experiments were operated under a nitrogen atmosphere as default in our system, and other experimental setups we might express by adding gas). Eventually, we conclude the 16 operations in our schema from the retained verb mentions, among which 67.5% accurately correspond to an operation, and 12.2% vaguely correspond to one or more possible operations. We also supplement several operations based on the practical experience of experts, including COLUMN, TRITURATE, and PARTITION.\nCompared with other chemical synthetic action schemas, our schema selects operations with appropriate granularity and therefore achieves a balance between good coverage and concise structure, of which the latter can promise the operability of the downstream automatic synthetic platforms. Take the verb \u201cneutralize\u201d for example. Considering this action is not a frequent operation and can be further decomposed and expressed with more basic operations such as ADD, we select to ignore such verbs to simplify the framework without compromising the coverage. In this way, there is either no need for the platforms to further compile \u201cneutralize\u201d into concrete operations.\nSciSpacy9 Part-of-Speech tagging tool is applied to count the verbs that appear in the text to be annotated. We analyze, combine and lter the verbs and decide on 16 basic actions for chemical synthesis as shown in Fig. 2. Meanwhile, the reagent is dened as a special item that is usually the basis of the reaction systems. Here we explain all the schema items:\nREAGENT: solutions, gas, or other substances that join in the reaction should be annotated as REAGENT. A mixture reagent is composed of several pure reagents.\nADD: any operations that introduce effective substances into the reaction system are supposed to be an ADD (e.g. inject, charge, dissolve). The construction of the initial reaction system is also regarded as an ADD operation.\nWASH: this operation is usually targeted at a specic phase (e.g. organic phase) of the reagent. Keywords include \u201cwash\u201d and \u201crinse\u201d.\nEXTRACT: similar toWASH, the EXTRACT operation requires phase and reagent arguments.\nChem. Sci., 2023, 14, 9360\u20139373 | 9369\nUnrelated 7056 Reduced, allowed, using/used, room, followed, continued, prepared, remaining/remains, becomes, stand, begins, passed, desired, described, based, required, carried, turns, repeated, taken, adjusted, rise Discarded 1374 Connected, attached, immersed, sealed, shaken, evacuated, capped, packed, stored, discarded Accurate 28 054 Added/adding, stirred/stirring/stir, washed, dried/ame-dried/drying, cooled/cool/cooling, equipped, charged, ltered, resulting, concentrated, extracted, transferred, distilled, tted, give/gives/giving, placed, containing/ contains, dissolved, afford/affords, poured, combined, obtained, evaporated, warm/ warmed, rinsed, yielding/yield, introduced, quenched, recrystallized, boiling, eluted, maintain, lled, decanted, dropping, inserted Vague 5054 Removed/remove, heated, collected, separated, ushed, puried, diluted, kept, treated, provide, purged, saturated, separates"
        },
        {
            "heading": "Chemical Science Edge Article",
            "text": "O pe\nn A\ncc es\ns A\nrt ic\nle . P\nub lis\nhe d\non 2\n4 A\nug us\nt 2 02\n3. D\now nl\noa de\nd on\n1 /9\n/2 02\n4 11\n:1 5:\n20 P\nM .\nT hi\ns ar\ntic le\nis li\nce ns\ned u\nnd er\na C\nre at\niv e\nC om\nm on\ns A\nttr ib\nut io\nnN\non C\nom m\ner ci\nal 3\n.0 U\nnp or\nte d\nL ic\nen ce\n.\nView Article Online\nDRY: this operation is usually nished with the reagent or the heating equipment.\nFILTER: this operation leads to the difference in the object of follow-up operations (e.g. the residue of ltration).\nCOLUMN: this operation is supposed to dene the adsorbent and eluent, and keywords include \u201ccolumn\u201d and \u201cchromatography\u201d.\nDISTILL/EVAPORATE: distillation usually provides temperature and pressure. Evaporation also focuses on the two arguments, while emphasizing the removal of extra substances, such as concentrating the solutions.\nSETTEMP: this operation is accompanied by stirring. The default temperature is the room temperature if not dened.\nREFLUX: this operation also sets quite a high temperature, with the hint of keywords like \u201creux\u201d and \u201cboil\u201d.\nQUENCH/TRITURATE: the quench operation is used to terminate the reaction, with keywords like \u201cquench\u201d and \u201cpour\u201d. The trituration operation is for purication, mashing the solid in the reagent. Both are concerned about the reagent and temperature arguments.\nRECRYSTALLIZE/PARTITION: the two operations should be explicitly stated in the text to recrystallize the given reagent or to be partitioned between reagents 1 and 2.\nTRANSFER: reagent 1 is transferred into reagent 2, usually under the given temperature.\nYIELD: this operation usually appears at the end of synthesis descriptions, providing the product, appearance, purity, and other information.\nAll the annotators we hire have passed the TEM8, majored in chemistry-related disciplines, or participated in Chemistry Olympiads. As Fig. 1b shows, the arguments and operations are selected by the cursor, tagged with their types, and then linked with the arrows representing hierarchical relations. We sample\n9370 | Chem. Sci., 2023, 14, 9360\u20139373\nand check 20% of the labeled results. The sampled item is required to be revised if the operation-level accuracy is lower than 90%, and a small batch of the items is required to be relabeled if lower than 70%.\nFor the convenience and accuracy of labeling, the annotators are required to label the operations and arguments in an extractive form. The arguments are linked with corresponding reagents or operations. We pre-process the JSON le, lter out those isolated labels and those items that do not meet the schema correspondence, and then transcribe the hierarchical relationship into sequential text. If the labels are too sparse (the generated output is shorter than 0.3 multiplied by the length of input), or the correspondence error appears more than twice, then the paragraph is abandoned."
        },
        {
            "heading": "3.4 Model settings",
            "text": "The transformer model we implement is a 4-layer transformer model with a hidden size of 256, following the setting of the previous framework.12\nThe backbone model T5 (ref. 8) is one of the representative sequence-to-sequence models. For the T5-base model, there are 12 encoder layers and 12 decoder layers with a hidden size of 768, and altogether 220m parameters. For the T5-large model, there are 24 encoder layers and 24 decoder layers with a hidden size of 1024, and altogether 770m parameters. The simple baseline transformer model is a smaller T5, with the encoder and decoder layer number as 4 and hidden size as 256, which is comparable with the model applied in RXN.12\nWe transcribe various NLP tasks into a unied sequence-tosequence format and distinguish them with the prex prompts, which facilitates multi-task training. In our multi-grained knowledgeable pre-training, the combined task (verb mapping and chemical recognition) is conducted on two-thirds of the encoder\u2013decoder pre-training data, and masked language modeling is conducted on the rest. The instruction generation for the decoder training is conducted simultaneously while on separate fake data. From Fig. 1c we can observe that the automatic labeling sometimes makes mistakes, such as missing/redundant operation verbs or chemical recognition (in the color of dark red), while overall providing worth-studying information for comprehension of operations and chemical substances.\nFor the large-scale PTMs, we set the prex as follows: D2I grammar: Use \u201c[ ]\u201d, \u201c&\u201d and \u201c:\u201d to mark operations, split different segments and split the name and value of arguments, such as \u201c[OPERATION] ARGUMENT1 NAME: VALUE1 & ARGUMENT2 NAME: VALUE2 &\u201d. Operations include ADD, WASH, FILTER, DRY, EXTRACT, RECRYSTALLIZE, QUENCH, PARTITION, TRANSFER, YIELD, DISTILL, EVAPORATE, COLUMN, SETTEMP and REFLUX. Arguments include time, temperature, phase, reagent, mass, composition, speed, mole, batch, volume, concentration and note. Notice that the grammar rule is only provided for zero-shot D2I transcription.\nZero-shot D2I: Generate the synthetic instructions according to the given descriptions.\nZero-shot I2D: Generate the synthetic description according to the given instructions.\n\u00a9 2023 The Author(s). Published by the Royal Society of Chemistry"
        },
        {
            "heading": "Edge Article Chemical Science",
            "text": "O pe\nn A\ncc es\ns A\nrt ic\nle . P\nub lis\nhe d\non 2\n4 A\nug us\nt 2 02\n3. D\now nl\noa de\nd on\n1 /9\n/2 02\n4 11\n:1 5:\n20 P\nM .\nT hi\ns ar\ntic le\nis li\nce ns\ned u\nnd er\na C\nre at\niv e\nC om\nm on\ns A\nttr ib\nut io\nnN\non C\nom m\ner ci\nal 3\n.0 U\nnp or\nte d\nL ic\nen ce\n. View Article Online 3-shot and 3-shot*: + Refer to the following instances. INSTRUCTION: .DESCRIPTION: .\nChat wrapper: You are now a synthetic literature writing/ instruction generating assistant. + + Now the INSTRUCTION/DESCRIPTION is given. In any cases, please generate the DESCRIPTION/INSTRUCTION."
        },
        {
            "heading": "3.5 Training settings",
            "text": "We implement ourmethod in the PyTorch44 framework, and adopt the HuggingFace transformers.45 For the multi-grained knowledgeable pre-training and ne-tuning of the models, we take the AdamW optimizer46 which is suitable for most of the PTMs in the T5 backbone. Reported scores are the average scores under 5 different random seeds. Following the setting in the original T5, we simply mix different knowledge enhancement data sets together, and treat these tasks equally. As for the proportion of tasks, we equally divided the pre-training data to create different task sets.\nIn the stage of multi-grained knowledgeable pre-training, the corpus scale is comparably large, and the empirical batch size for multi-task training is bigger than ne-tuning. Therefore, we set the batch size as 256, with the default learning rate as 1e\u2212 3. Since other tasks pay attention to the whole model while the instruction generation task only focuses on the decoder, the two parts calculate the loss and do back-propagation separately, and the second part is multiplied by the coefficient 0.1. In the stage of ne-tuning, since the dataset is small, we adjust the batch size to 16, with the common learning rate 1e \u2212 4. These two hyper-parameters have been tried and searched within the vicinity and the best have been chosen.\nThe multi-grained knowledgeable pre-training stage takes 1000 steps and runs the data with only one epoch. For the netuning period, the models are evaluated every epoch with the validation set, generated by greedy search and compared by the SeqMatch-A score. The maximum number of epochs is 20, and the early stop number is 3. Note that for the transformer baseline, due to lack of pre-training, the learning rate is set as 5e\u2212 4 and the maximum epoch as 50 to ensure the convergence.\nThe greedy decoding strategy is applied for all the periods except for testing. When evaluating with the testing set or during the interaction, we set the model decoder beam size as 3.\nFor the practical test, 20 input paragraphs are randomly picked. The three researchers are asked to: (1) read the raw data, manually pick out the operations and arguments, and mark their types (to imitate the manual programming process for automatic chemical synthesis platforms); (2) read both the raw data and the instructions transcribed by our system, and correct the wrong parts (to imitate the manual verication of automatic transcription). Researchers rst nish the written-from-scratch setting for paragraphs 1\u201310, and try the revision setting for paragraphs 11\u201320. The sequence for the two settings is exchanged then to reduce the effect of prociency."
        },
        {
            "heading": "4 Conclusions",
            "text": "In this paper, we propose the solutions of transcription between human-readable descriptions and machine-executable\n\u00a9 2023 The Author(s). Published by the Royal Society of Chemistry\ninstructions, a practical scenario in synthetic chemistry. We provide the task denition, the instruction schema, and a human-annotated dataset, ChemTrans, in the hope to draw more attention from the machine intelligence community to this interdisciplinary eld. To apply the latest pre-training technology, we try two approaches, the small PTMs and large PTMs, on the mutual transcription tasks. For the former, we design the multi-grained knowledge enhancement method aiming at improving the capability of recognizing and generating chemical entities and operations/descriptions. Our experimental study has demonstrated the effectiveness of the proposed method. In the future, we will try to further improve the benchmark settings, the model architectures, and training methods. The potential of predicting synthetic actions and designing retrosynthesis pathways will be explored. Also, our system may get access to the actual system for use, providing practical help for researchers."
        },
        {
            "heading": "Data availability",
            "text": "All data that support the ndings of this study are available and have been deposited in Google Drive (https://drive.google.com/ drive/folders/1AT-1uUR5Ev5d8fxX2FFrIZSnQ1-jbiG6? usp=sharing). The code of this study can be obtained from GitHub https://github.com/thunlp/ChemTrans. The zip le of the code can be downloaded via the Google Drive link above."
        },
        {
            "heading": "Author contributions",
            "text": "Conceptualization: Weinan E, Rong Zhu, Zhiyuan Liu; data curation: Yi-Chen Nie, Ning Ding, Wei-Ting Ye; investigation: Yi-Chen Nie, Ning Ding, Zheni Zeng; methodology: Zheni Zeng, Ning Ding; soware: Zheni Zeng; supervision: Rong Zhu, Zhiyuan Liu, Cheng Yang; validation: Zheni Zeng, Qian-Jun Ding; visualization: Zheni Zeng, Ning Ding; writing\u2014original dra: Zheni Zeng, Ning Ding; writing\u2014review & editing: QianJun Ding, Wei-Ting Ye, Zhiyuan Liu, Rong Zhu, Weinan E, Maosong Sun."
        },
        {
            "heading": "Conflicts of interest",
            "text": "There are no conicts to declare.\nNotes and references \u2021 https://platform.openai.com/docs/model-index-for-researchers. \u00a7 https://www.orgsyn.org/. { https://github.com/open-reaction-database/ord-data.\n1 G. Chen, P. Chen, C.-Y. Hsieh, C.-K. Lee, B. Liao, R. Liao, W. Liu, J. Qiu, Q. Sun, J. Tang, et al., arXiv, 2019, preprint, arXiv:1906.09427. 2 A. F. de Almeida, R. Moreira and T. Rodrigues, Nat. Rev. Chem, 2019, 3, 589\u2013604. 3 W. P. Walters andM.Murcko, Nat. Biotechnol., 2020, 38, 143\u2013 145.\nChem. Sci., 2023, 14, 9360\u20139373 | 9371"
        },
        {
            "heading": "Chemical Science Edge Article",
            "text": "O pe\nn A\ncc es\ns A\nrt ic\nle . P\nub lis\nhe d\non 2\n4 A\nug us\nt 2 02\n3. D\now nl\noa de\nd on\n1 /9\n/2 02\n4 11\n:1 5:\n20 P\nM .\nT hi\ns ar\ntic le\nis li\nce ns\ned u\nnd er\na C\nre at\niv e\nC om\nm on\ns A\nttr ib\nut io\nnN\non C\nom m\ner ci\nal 3\n.0 U\nnp or\nte d\nL ic\nen ce\n. View Article Online 4 B. Burger, P. M. Maffettone, V. V. Gusev, C. M. Aitchison, Y. Bai, X. Wang, X. Li, B. M. Alston, B. Li, R. Clowes, et al., Nature, 2020, 583, 237\u2013241.\n5 Q. Zhu, F. Zhang, Y. Huang, H. Xiao, L. Zhao, X. Zhang, T. Song, X. Tang, X. Li, G. He, et al., Natl. Sci. Rev., 2022, 9, nwac190. 6 S. Rohrbach, M. S\u030ciauc\u030ciulis, G. Chisholm, P.-A. Pirvan, M. Saleeb, S. H. M. Mehr, E. Trushina, A. I. Leonov, G. Keenan, A. Khan, et al., Science, 2022, 377, 172\u2013180. 7 X. Han, Z. Zhang, N. Ding, Y. Gu, X. Liu, Y. Huo, J. Qiu, Y. Yao, A. Zhang, L. Zhang, et al., AI Open, 2021, 2, 225\u2013250. 8 C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, P. J. Liu, et al., J. Mach. Learn. Res., 2020, 21, 1\u201367. 9 M. Neumann, D. King, I. Beltagy and W. Ammar, Proceedings of the 18th BioNLP Workshop and Shared Task, 2019, pp. 319\u2013 327. 10 K. Papineni, S. Roukos, T. Ward andW.-J. Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics, 2002, pp. 311\u2013318. 11 T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al., Adv. Neural Inf. Process., 2020, 33, 1877\u20131901. 12 A. C. Vaucher, F. Zipoli, J. Geluykens, V. H. Nair, P. Schwaller and T. Laino, Nat. Commun., 2020, 11, 1\u201311. 13 C.-Y. Lin, Text summarization branches out, 2004, pp. 74\u201381. 14 J. Li, M. Galley, C. Brockett, J. Gao and W. B. Dolan,\nProceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2016, pp. 110\u2013119. 15 S. R. Eddy, Curr. Opin. Struct. Biol., 1996, 6, 361\u2013365. 16 S. H. M. Mehr, M. Craven, A. I. Leonov, G. Keenan and\nL. Cronin, Science, 2020, 370, 101\u2013108. 17 OpenAI arXiv, 2023, preprint, arXiv:2303.08774, DOI:\n10.48550/arXiv.2303.08774. 18 Z. Zeng, Y. Yao, Z. Liu and M. Sun, Nat. Commun., 2022, 13,\n1\u201311. 19 R. Merrield, Science, 1965, 150, 178\u2013185. 20 G. Alvarado-Urbina, G. M. Sathe, W.-C. Liu, M. F. Gillen,\nP. D. Duck, R. Bender and K. K. Ogilvie, Science, 1981, 214, 270\u2013274. 21 S. V. Ley, D. E. Fitzpatrick, R. J. Ingham and R. M. Myers, Angew. Chem., Int. Ed., 2015, 54, 3449\u20133464. 22 D. Angelone, A. J. Hammer, S. Rohrbach, S. Krambeck, J. M. Granda, J. Wolf, S. Zalesskiy, G. Chisholm and L. Cronin, Nat. Chem., 2021, 13, 63\u201369. 23 J. M. Granda, L. Donina, V. Dragone, D.-L. Long and L. Cronin, Nature, 2018, 559, 377\u2013381. 24 J. Xu, Y. Zhang, J. Han, H. Qiao, J. Tang, S. Xi, B. Sun, S. Zhai, X. Wang, Y. Wu, et al., ChemRxiv, 2021, preprint, DOI: 10.26434/chemrxiv-2021-1bhnc. 25 C. W. Coley, D. A. Thomas III, J. A. Lummiss, J. N. Jaworski, C. P. Breen, V. Schultz, T. Hart, J. S. Fishman, L. Rogers, H. Gao, et al., Science, 2019, 365, eaax1566. 26 S. Steiner, J. Wolf, S. Glatzel, A. Andreou, J. M. Granda, G. Keenan, T. Hinkley, G. Aragon-Camarasa, P. J. Kitson, D. Angelone, et al., Science, 2019, 363, eaav2211.\n9372 | Chem. Sci., 2023, 14, 9360\u20139373\n27 M. C. Swain and J. M. Cole, J. Chem. Inf. Model., 2016, 56, 1894\u20131904. 28 E. Kim, K. Huang, A. Tomala, S. Matthews, E. Strubell, A. Saunders, A. McCallum and E. Olivetti, Sci. Data, 2017, 4, 1\u20139. 29 O. Kononova, H. Huo, T. He, Z. Rong, T. Botari, W. Sun, V. Tshitoyan and G. Ceder, Sci. Data, 2019, 6, 203. 30 P. Shetty, A. C. Rajan, C. Kuenneth, S. Gupta, L. P. Panchumarti, L. Holm, C. Zhang and R. Ramprasad, npj Comput. Mater., 2023, 9, 52. 31 M. Saeidi, Interpretation of Natural Language Rules in Conversational Machine Reading, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, 2018. 32 A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser and I. Polosukhin, Advances in neural information processing systems, 2017, vol. 30. 33 R. Li, W. Zhao, C. Yang and S. Su, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 2021, pp. 2625\u20132635. 34 R. Li, W. Zhao, C. Yang and S. Su, Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, 2022, pp. 1110\u20131121. 35 J. D. M.-W. C. Kenton and L. K. Toutanova, Proceedings of NAACL-HLT, 2019, pp. 4171\u20134186. 36 Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov and Q. V. Le, Advances in neural information processing systems, 2019, vol. 32. 37 Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer and V. Stoyanov, A Robustly Optimized BERT Pretraining Approach, arXiv, 2019, preprint, DOI: 10.48550/arXiv.1907.11692. 38 A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al., Language Models are Unsupervised Multitask Learners, OpenAI blog, 2019, 1, 9, https:// d4mucfpksywv.cloudfront.net/better-language-models/ language-models.pdf. 39 M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov and L. Zettlemoyer, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 2020, pp. 7871\u20137880. 40 R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain, V. Kosaraju and W. Saunders, WebGPT: Browser-assisted question-answering with human feedback, arXiv, 2021, preprint, DOI: 10.48550/ arXiv.2112.09332. 41 H. Yan, T. Gui, J. Dai, Q. Guo, Z. Zhang and X. Qiu, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, 2021, vol. 1, pp. 5808\u20135822. 42 T. Yu, Z. Li, Z. Zhang, R. Zhang and D. Radev, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2018, vol. 2, pp. 588\u2013594. 43 Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin, T. Liu, D. Jiang, et al., Findings of the\n\u00a9 2023 The Author(s). Published by the Royal Society of Chemistry"
        },
        {
            "heading": "Edge Article Chemical Science",
            "text": "O pe\nn A\ncc es\ns A\nrt ic\nle . P\nub lis\nhe d\non 2\n4 A\nug us\nt 2 02\n3. D\now nl\noa de\nd on\n1 /9\n/2 02\n4 11\n:1 5:\n20 P\nM .\nT hi\ns ar\ntic le\nis li\nce ns\ned u\nnd er\na C\nre at\niv e\nC om\nm on\ns A\nttr ib\nut io\nnN\non C\nom m\ner ci\nal 3\n.0 U\nnp or\nte d\nL ic\nen ce\n. View Article Online Association for Computational Linguistics: EMNLP 2020, 2020, pp. 1536\u20131547.\n44 A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al., Advances in neural information processing systems, 2019, vol. 32. 45 T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz,\n\u00a9 2023 The Author(s). Published by the Royal Society of Chemistry\nJ. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. L. Scao, S. Gugger, M. Drame, Q. Lhoest and A. M. Rush, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, 2020, pp. 38\u201345. 46 I. Loshchilov and F. Hutter, International Conference on Learning Representations, 2019.\nChem. Sci., 2023, 14, 9360\u20139373 | 9373"
        }
    ],
    "title": "Transcription between human-readable synthetic descriptions and machine-executable instructions: an application of the latest pre-training technology",
    "year": 2023
}