{
    "abstractText": "Bayesian inference usually requires running potentially costly inference procedures separately for every new observation. In contrast, the idea of amortized Bayesian inference is to initially invest computational cost in training an inference network on simulated data, which can subsequently be used to rapidly perform inference (i.e., to return estimates of posterior distributions) for new observations. This approach has been applied to many real-world models in the sciences and engineering, but it is unclear how robust the approach is to adversarial perturbations in the observed data. Here, we study the adversarial robustness of amortized Bayesian inference, focusing on simulation-based estimation of multi-dimensional posterior distributions. We show that almost unrecognizable, targeted perturbations of the observations can lead to drastic changes in the predicted posterior and highly unrealistic posterior predictive samples, across several benchmark tasks and a real-world example from neuroscience. We propose a computationally efficient regularization scheme based on penalizing the Fisher information of the conditional density estimator, and show how it improves the adversarial robustness of amortized Bayesian inference.",
    "authors": [
        {
            "affiliations": [],
            "name": "Manuel Gloeckler"
        },
        {
            "affiliations": [],
            "name": "Michael Deistler"
        },
        {
            "affiliations": [],
            "name": "Jakob H. Macke"
        }
    ],
    "id": "SP:bfe8d0843f15405de6b415d5b1634e48af5b21bc",
    "references": [
        {
            "authors": [
                "H. Akrami",
                "A.A. Joshi",
                "J. Li",
                "S. Ayd\u00f6re",
                "R.M. Leahy"
            ],
            "title": "A robust variational autoencoder using beta divergence",
            "venue": "Knowledge-Based Systems,",
            "year": 2022
        },
        {
            "authors": [
                "F. Altekr\u00fcger",
                "P. Hagemann",
                "G. Steidl"
            ],
            "title": "Conditional generative models are provably robust: Pointwise guarantees for bayesian inverse problems",
            "venue": "arXiv preprint arXiv:2303.15845,",
            "year": 2023
        },
        {
            "authors": [
                "C. Andrieu",
                "J. Thoms"
            ],
            "title": "A tutorial on adaptive mcmc",
            "venue": "Statistics and computing,",
            "year": 2008
        },
        {
            "authors": [
                "B. Barrett",
                "A. Camuto",
                "M. Willetts",
                "T. Rainforth"
            ],
            "title": "Certifiably robust variational autoencoders",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2022
        },
        {
            "authors": [
                "E. Bingham",
                "J.P. Chen",
                "M. Jankowiak",
                "F. Obermeyer",
                "N. Pradhan",
                "T. Karaletsos",
                "R. Singh",
                "P. Szerlip",
                "P. Horsfall",
                "N.D. Goodman"
            ],
            "title": "Pyro: Deep Universal Probabilistic Programming",
            "venue": "Journal of Machine Learning Research,",
            "year": 2019
        },
        {
            "authors": [
                "S. Blyth"
            ],
            "title": "Local Divergence and Association",
            "year": 1994
        },
        {
            "authors": [
                "J. Boelts",
                "Lueckmann",
                "J.-M",
                "R. Gao",
                "J.H. Macke"
            ],
            "title": "Flexible and efficient simulation-based inference for models of decision-making",
            "year": 2022
        },
        {
            "authors": [
                "P. Cannon",
                "D. Ward",
                "S.M. Schmon"
            ],
            "title": "Investigating the Impact of Model Misspecification in Neural Simulationbased Inference, September 2022",
            "year": 2022
        },
        {
            "authors": [
                "J. Chan",
                "V. Perrone",
                "J.P. Spence",
                "P.A. Jenkins",
                "S. Mathieson",
                "Y.S. Song"
            ],
            "title": "A Likelihood-Free inference framework for population genetic data using exchangeable neural networks",
            "venue": "Adv Neural Inf Process Syst,",
            "year": 2018
        },
        {
            "authors": [
                "K. Cranmer",
                "J. Brehmer",
                "G. Louppe"
            ],
            "title": "The frontier of simulation-based inference",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2020
        },
        {
            "authors": [
                "M. Dax",
                "S.R. Green",
                "J. Gair",
                "J.H. Macke",
                "A. Buonanno",
                "B. Sch\u00f6lkopf"
            ],
            "title": "Real-time gravitational wave science with neural posterior estimation",
            "venue": "Physical review letters,",
            "year": 2021
        },
        {
            "authors": [
                "M. Dax",
                "S.R. Green",
                "J. Gair",
                "M. P\u00fcrrer",
                "J. Wildberger",
                "J.H. Macke",
                "A. Buonanno",
                "B. Sch\u00f6lkopf"
            ],
            "title": "Neural Importance Sampling for Rapid and Reliable Gravitational-Wave Inference, October 2022",
            "year": 2022
        },
        {
            "authors": [
                "M. Deistler",
                "P.J. Goncalves",
                "J.H. Macke"
            ],
            "title": "Truncated proposals for scalable and hassle-free simulation-based inference",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "M. Deistler",
                "J.H. Macke",
                "P.J. Gon\u00e7alves"
            ],
            "title": "Energyefficient network activity from disparate circuit parameters",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2022
        },
        {
            "authors": [
                "C. Dellaporta",
                "J. Knoblauch",
                "T. Damoulas",
                "Briol",
                "F.-X"
            ],
            "title": "Robust Bayesian Inference for Simulator-based Models via the MMD Posterior Bootstrap",
            "venue": "In Proceedings of The 25th International Conference on Artificial Intelligence and Statistics,",
            "year": 2022
        },
        {
            "authors": [
                "H.M. Dolatabadi",
                "S. Erfani",
                "C. Leckie"
            ],
            "title": "Invertible generative modeling using linear rational splines",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2020
        },
        {
            "authors": [
                "C. Finlay",
                "A.M. Oberman"
            ],
            "title": "Scaleable input gradient regularization for adversarial robustness, October 2019",
            "year": 1905
        },
        {
            "authors": [
                "G.H. Golub",
                "P.C. Hansen",
                "D.P. O\u2019Leary"
            ],
            "title": "Tikhonov regularization and total least squares",
            "venue": "SIAM journal on matrix analysis and applications,",
            "year": 1999
        },
        {
            "authors": [
                "G. Gondim-Ribeiro",
                "P. Tabacof",
                "E. Valle"
            ],
            "title": "Adversarial attacks on variational autoencoders",
            "venue": "arXiv preprint arXiv:1806.04646,",
            "year": 2018
        },
        {
            "authors": [
                "P.J. Gon\u00e7alves",
                "Lueckmann",
                "J.-M",
                "M. Deistler",
                "M. Nonnenmacher",
                "K. \u00d6cal",
                "G. Bassetto",
                "C. Chintaluri",
                "W.F. Podlaski",
                "S.A. Haddad",
                "T.P. Vogels"
            ],
            "title": "Training deep neural density estimators to identify mechanistic models of neural dynamics",
            "venue": "Elife, 9:e56261,",
            "year": 2020
        },
        {
            "authors": [
                "I.J. Goodfellow",
                "J. Shlens",
                "C. Szegedy"
            ],
            "title": "Explaining and Harnessing Adversarial Examples",
            "venue": "In International Conference on Learning Representations,",
            "year": 2015
        },
        {
            "authors": [
                "D. Greenberg",
                "M. Nonnenmacher",
                "J. Macke"
            ],
            "title": "Automatic posterior transformation for likelihood-free inference",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "A. Gretton",
                "D. Sejdinovic",
                "H. Strathmann",
                "S. Balakrishnan",
                "M. Pontil",
                "K. Fukumizu",
                "B.K. Sriperumbudur"
            ],
            "title": "Optimal kernel choice for large-scale two-sample tests",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2012
        },
        {
            "authors": [
                "P. Gr\u00fcnwald",
                "Ommen",
                "T. v"
            ],
            "title": "Inconsistency of Bayesian Inference for Misspecified Linear Models, and a Proposal for Repairing It",
            "venue": "Bayesian Analysis,",
            "year": 2017
        },
        {
            "authors": [
                "K. Hayashi",
                "Yuan",
                "K.-H",
                "L. Liang"
            ],
            "title": "On the Bias in Eigenvalues of Sample Covariance Matrix",
            "venue": "Quantitative Psychology, Springer Proceedings in Mathematics & Statistics,",
            "year": 2018
        },
        {
            "authors": [
                "S. Hayou"
            ],
            "title": "On the overestimation of the largest eigenvalue of a covariance matrix, August 2017",
            "year": 2017
        },
        {
            "authors": [
                "J. Hermans",
                "V. Begy",
                "G. Louppe"
            ],
            "title": "Likelihood-free mcmc with amortized approximate ratio estimators",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "J. Hermans",
                "A. Delaunoy",
                "F. Rozet",
                "A. Wehenkel",
                "V. Begy",
                "G. Louppe"
            ],
            "title": "A Trust Crisis In Simulation-Based Inference? Your Posterior Approximations Can Be Unfaithful, December 2022",
            "year": 2022
        },
        {
            "authors": [
                "L. Holden",
                "R. Hauge",
                "M. Holden"
            ],
            "title": "Adaptive independent Metropolis\u2013Hastings",
            "venue": "The Annals of Applied Probability,",
            "year": 2009
        },
        {
            "authors": [
                "H. Husain",
                "J. Knoblauch"
            ],
            "title": "Adversarial Interpretation of Bayesian Inference",
            "venue": "In Proceedings of The 33rd International Conference on Algorithmic Learning Theory,",
            "year": 2022
        },
        {
            "authors": [
                "D.P. Kingma",
                "M. Welling"
            ],
            "title": "Auto-Encoding Variational Bayes",
            "venue": "In International Conference on Learning Representations,",
            "year": 2014
        },
        {
            "authors": [
                "A. Kuzina",
                "M. Welling",
                "J.M. Tomczak"
            ],
            "title": "Alleviating adversarial attacks on variational autoencoders with MCMC",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "J. Latz"
            ],
            "title": "On the well-posedness of Bayesian inverse problems",
            "venue": "SIAM/ASA Journal on Uncertainty Quantification,",
            "year": 2020
        },
        {
            "authors": [
                "P. Lemos",
                "M. Cranmer",
                "M. Abidi",
                "C. Hahn",
                "M. Eickenberg",
                "E. Massara",
                "D. Yallup",
                "S. Ho"
            ],
            "title": "Robust Simulation-Based Inference in Cosmology with Bayesian Neural Networks",
            "venue": "In ICML 2022 Workshop on Machine Learning for Astrophysics,",
            "year": 2022
        },
        {
            "authors": [
                "Y. Li",
                "M. Cheng",
                "Hsieh",
                "C.-J",
                "T.C.M. Lee"
            ],
            "title": "A Review of Adversarial Attack and Defense for Classification Methods",
            "venue": "The American Statistician,",
            "year": 2022
        },
        {
            "authors": [
                "Lueckmann",
                "J.-M",
                "P.J. Goncalves",
                "G. Bassetto",
                "K. \u00d6cal",
                "M. Nonnenmacher",
                "J.H. Macke"
            ],
            "title": "Flexible statistical inference for mechanistic models of neural dynamics",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Lueckmann",
                "J.-M",
                "J. Boelts",
                "D. Greenberg",
                "P. Goncalves",
                "J. Macke"
            ],
            "title": "Benchmarking simulation-based inference",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2021
        },
        {
            "authors": [
                "A. Madry",
                "A. Makelov",
                "L. Schmidt",
                "D. Tsipras",
                "A. Vladu"
            ],
            "title": "Towards deep learning models resistant to adversarial attacks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "T. Matsubara",
                "J. Knoblauch",
                "Briol",
                "F.-X",
                "C.J. Oates"
            ],
            "title": "Robust Generalised Bayesian Inference for Intractable Likelihoods, January 2022",
            "year": 2022
        },
        {
            "authors": [
                "M.A. Medina",
                "J.L.M. Olea",
                "C. Rush",
                "A. Velez"
            ],
            "title": "On the robustness to misspecification of \u00ce\u00b1-posteriors and their variational approximations",
            "venue": "Journal of Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "T. Miyato",
                "Maeda",
                "S.-i",
                "M. Koyama",
                "K. Nakae",
                "S. Ishii"
            ],
            "title": "Distributional Smoothing with Virtual Adversarial Training",
            "venue": "In International Conference on Learning Representations,",
            "year": 2016
        },
        {
            "authors": [
                "Moon",
                "H.-S",
                "A. Oulasvirta",
                "B. Lee"
            ],
            "title": "Amortized inference with user simulations",
            "venue": "In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems,",
            "year": 2023
        },
        {
            "authors": [
                "G. Papamakarios",
                "I. Murray"
            ],
            "title": "Fast \u03b5-free inference of simulation models with bayesian conditional density estimation",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "G. Papamakarios",
                "T. Pavlakou",
                "I. Murray"
            ],
            "title": "Masked autoregressive flow for density estimation",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "G. Papamakarios",
                "D. Sterratt",
                "I. Murray"
            ],
            "title": "Sequential neural likelihood: Fast likelihood-free inference with autoregressive flows",
            "venue": "In The 22nd International Conference on Artificial Intelligence and Statistics,",
            "year": 2019
        },
        {
            "authors": [
                "P. Pope",
                "Y. Balaji",
                "S. Feizi"
            ],
            "title": "Adversarial robustness of flow-based generative models",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2020
        },
        {
            "authors": [
                "M. Pospischil",
                "M. Toledo-Rodriguez",
                "C. Monier",
                "Z. Piwkowska",
                "T. Bal",
                "Y. Fr\u00e9gnac",
                "H. Markram",
                "A. Destexhe"
            ],
            "title": "Minimal Hodgkin\u2013Huxley type models for different classes of cortical and thalamic neurons",
            "venue": "Biological Cybernetics,",
            "year": 2008
        },
        {
            "authors": [
                "A.A. Prinz",
                "C.P. Billimoria",
                "E. Marder"
            ],
            "title": "Alternative to hand-tuning conductance-based models: construction and analysis of databases of model neurons",
            "venue": "Journal of neurophysiology,",
            "year": 2003
        },
        {
            "authors": [
                "A.A. Prinz",
                "D. Bucher",
                "E. Marder"
            ],
            "title": "Similar network activity from disparate circuit parameters",
            "venue": "Nature Neuroscience,",
            "year": 2004
        },
        {
            "authors": [
                "S.T. Radev",
                "U.K. Mertens",
                "A. Voss",
                "L. Ardizzone",
                "U. K\u00f6the"
            ],
            "title": "Bayesflow: Learning complex stochastic models with invertible neural networks",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
            "year": 2020
        },
        {
            "authors": [
                "F. Ramos",
                "R.C. Possas",
                "D. Fox"
            ],
            "title": "Bayessim: adaptive domain randomization via probabilistic inference for robotics simulators, 2019",
            "year": 2019
        },
        {
            "authors": [
                "J. Rauber",
                "W. Brendel",
                "M. Bethge"
            ],
            "title": "Foolbox: A python toolbox to benchmark the robustness of machine learning models",
            "venue": "In Reliable Machine Learning in the Wild Workshop, 34th International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "A.H. Ribeiro",
                "D. Zachariah",
                "T.B. Sch\u00f6n"
            ],
            "title": "Surprises in adversarially-trained linear regression",
            "venue": "arXiv preprint arXiv:2205.12695,",
            "year": 2022
        },
        {
            "authors": [
                "F Rozet"
            ],
            "title": "Arbitrary marginal neural ratio estimation for likelihood-free inference",
            "venue": "arXiv preprint arXiv:2110.00449,",
            "year": 2021
        },
        {
            "authors": [
                "M. Schmitt",
                "B\u00fcrkner",
                "P.-C",
                "U. K\u00f6the",
                "S.T. Radev"
            ],
            "title": "Detecting Model Misspecification in Amortized Bayesian Inference with Neural Networks, November 2022",
            "year": 2022
        },
        {
            "authors": [
                "C. Shen",
                "Y. Peng",
                "G. Zhang",
                "J. Fan"
            ],
            "title": "Defending against adversarial attacks by suppressing the largest eigenvalue of fisher information matrix",
            "year": 1909
        },
        {
            "authors": [
                "M. Shen",
                "S. Ghosh",
                "P. Sattigeri",
                "S. Das",
                "Y. Bu",
                "G. Wornell"
            ],
            "title": "Reliable gradient-free and likelihood-free prompt tuning",
            "venue": "In Findings of the Association for Computational Linguistics: EACL",
            "year": 2023
        },
        {
            "authors": [
                "R. Shu",
                "H.H. Bui",
                "S. Zhao",
                "M.J. Kochenderfer",
                "S. Ermon"
            ],
            "title": "Amortized Inference Regularization",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "B. Sprungk"
            ],
            "title": "On the Local Lipschitz Stability of Bayesian Inverse Problems",
            "venue": "Inverse Problems,",
            "year": 2020
        },
        {
            "authors": [
                "C. Szegedy",
                "W. Zaremba",
                "I. Sutskever",
                "J. Bruna",
                "D. Erhan",
                "I. Goodfellow",
                "R. Fergus"
            ],
            "title": "Intriguing properties of neural networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2014
        },
        {
            "authors": [
                "A. Tejero-Cantero",
                "J. Boelts",
                "M. Deistler",
                "Lueckmann",
                "J.-M",
                "C. Durkan",
                "P.J. Gon\u00e7alves",
                "D.S. Greenberg",
                "J.H. Macke"
            ],
            "title": "sbi: A toolkit for simulation-based inference",
            "venue": "Journal of Open Source Software,",
            "year": 2020
        },
        {
            "authors": [
                "S. Trivedi",
                "J. Wang"
            ],
            "title": "The expected jacobian outerproduct: Theory and empirics",
            "year": 2020
        },
        {
            "authors": [
                "D. Tsipras",
                "S. Santurkar",
                "L. Engstrom",
                "A. Turner",
                "A. Madry"
            ],
            "title": "Robustness may be at odds with accuracy",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "M. von Krause",
                "S.T. Radev",
                "A. Voss"
            ],
            "title": "Mental speed is high until age 60 as revealed by analysis of over a million participants",
            "venue": "Nature human behaviour,",
            "year": 2022
        },
        {
            "authors": [
                "V.G. Vovk"
            ],
            "title": "Aggregating strategies",
            "venue": "In Proceedings of the third annual workshop on Computational learning theory,",
            "year": 1990
        },
        {
            "authors": [
                "D. Ward",
                "P. Cannon",
                "M. Beaumont",
                "M. Fasiolo",
                "S.M. Schmon"
            ],
            "title": "Robust neural posterior estimation and statistical model criticism",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "M.J. Willetts",
                "A. Camuto",
                "T. Rainforth",
                "S. Roberts",
                "C.C. Holmes"
            ],
            "title": "Improving {vae}s\u2019 robustness to adversarial attack",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "O. Yadan"
            ],
            "title": "Hydra - a framework for elegantly configuring complex applications",
            "venue": "Github,",
            "year": 2019
        },
        {
            "authors": [
                "H. Zhang",
                "Y. Yu",
                "J. Jiao",
                "E. Xing",
                "L. El Ghaoui",
                "M. Jordan"
            ],
            "title": "Theoretically principled trade-off between robustness and accuracy",
            "venue": "In International conference on machine learning,",
            "year": 2019
        },
        {
            "authors": [
                "C. Zhao",
                "P.T. Fletcher",
                "M. Yu",
                "Y. Peng",
                "G. Zhang",
                "C. Shen"
            ],
            "title": "The adversarial attack and detection under the fisher information metric",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Cannon"
            ],
            "title": "The expected coverage",
            "year": 2022
        },
        {
            "authors": [
                "Cannon"
            ],
            "title": "2022), we evaluate the coverage given (adversarially) perturbed data",
            "year": 2022
        },
        {
            "authors": [
                "Deistler"
            ],
            "title": "We use a Monte Carlo approximation to obtain HPR1\u2212\u03b1 as described by Rozet et al",
            "year": 2022
        },
        {
            "authors": [
                "Madry"
            ],
            "title": "Adversarial robustness of amortized Bayesian inference A2.1",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Bayesian inference is a commonly used approach for identifying model parameters that are compatible with empirical observations and prior knowledge. Classical Bayesian inference methods such as Markov-chain Monte Carlo (MCMC) can be computationally expensive at test-time, as they rely on repeated evaluations of the likelihood function and, there-\n1Machine Learning in Science, University of Tu\u0308bingen and Tu\u0308bingen AI Center, Tu\u0308bingen, Germany 2Max Planck Institute for Intelligent Systems, Department Empirical Inference, Tu\u0308bingen, Germany. Correspondence to: Manuel Gloeckler <manuel.gloeckler@uni-tuebingen.de>.\nProceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s).\nfore, require a new set of likelihood evaluations for each observation. In contrast, the idea of amortized Bayesian inference is to approximate the mapping from observation to posterior distribution by a conditional density estimator, often parameterized as a neural network. Once this density estimation network has been trained, inference on a particular observation can be performed very efficiently, requiring only a single forward-pass through the network. This amortization can be achieved by training conditional density estimators on simulated data and framing Bayesian inference as a prediction problem: For any observation, the neural network is trained to predict either the posterior directly (Papamakarios & Murray, 2016; Greenberg et al., 2019; Gonc\u0327alves et al., 2020; Radev et al., 2020) or a quantity that allows to infer the posterior without further simulations (Papamakarios et al., 2019; Hermans et al., 2020). This approach has several advantages over MCMC methods: It can be used to perform \u2018simulation-based inference\u2019, i.e., applied to models which are only implicitly given as simulators (models which allow to sample the likelihood but not to evaluate it), it does not require the model to be differentiable (as compared to, e.g., Hamiltonian Monte Carlo), and it allows application in high-throughput scenarios (Dax et al., 2021; von Krause et al., 2022; Boelts et al., 2022; Arnst et al., 2022).\nHowever, these benefits come at a cost: the posterior predicted by the neural network will not be exact (Lueckmann et al., 2021), can be overconfident (Hermans et al., 2022), and can be sensitive to misspecified models (Cannon et al., 2022; Schmitt et al., 2022). Here, we study another possible limitation of neural network-based amortized Bayesian inference: It is well known that neural networks can be susceptible to adversarial attacks, i.e., tiny but targeted perturbations to the inputs can lead to vastly different outputs (Szegedy et al., 2014). For amortized Bayesian inference, this would indicate that even minor perturbations in the observed data could lead to entirely different posterior estimates.\nAdversarial attacks have become a common technique to evaluate the robustness of ML algorithms. Attacks can be used to assess performance in the presence of small worst-case perturbations, offering valuable insights into how models perform when faced with model misspecification. Furthermore, amortized inference is increasingly used in\nar X\niv :2\n30 5.\n14 98\n4v 1\n[ cs\n.L G\n] 2\n4 M\nay 2\n02 3\nreal-world safety-critical applications such as, e.g., robotics (Ramos et al., 2019) or applications accessible to the general public (Moon et al., 2023; Shen et al., 2023). In science and engineering, users are usually domain experts, but they are often not machine learning experts and, hence, must be aware of the limitations and brittleness of any such methods.\nHere, we investigate the impact of adversarial attacks on amortized inference, focusing on a particular method for amortized Bayesian inference, namely Neural Posterior Estimation (NPE, Cranmer et al. 2020). While adversarial attacks have been extensively studied in the context of classification (Rauber et al., 2017; Croce et al., 2021; Li et al., 2022), we present an approach and benchmark problems for evaluating the adversarial robustness of neural networks approximating multi-dimensional Bayesian posterior distributions. Using this approach, we demonstrate that NPE can be highly vulnerable to adversarial attacks. Finally, we develop a computationally efficient method for improving the adversarial robustness of NPE, and demonstrate its utility on a real-world example from neuroscience.\nOur overall approach is the following (Fig. 1): Given an observation xo, we consider an adversarial perturbation (Sec. 3.1). As we will show, even barely visible adversarial perturbations can strongly change estimated posterior distributions, and lead to predictive samples which strongly deviate from the original observation. We suggest and implement a defense strategy (Sec. 3.2), and will show that it reduces the impact on the posterior estimate, in particular, such that it still contains the ground truth parameters."
        },
        {
            "heading": "2. Background and Notation",
            "text": ""
        },
        {
            "heading": "2.1. Amortized Bayesian inference",
            "text": "One method to perform amortized Bayesian inference is Neural Posterior Estimation (NPE). NPE first draws samples from the joint distribution p(\u03b8,x) and then trains a conditional density estimator q\u03d5(\u03b8|x) with learnable parameters \u03d5 to approximate the posterior distribution:\nL(\u03d5) = Ep(\u03b8,x) [\u2212 log q\u03d5(\u03b8|x)] \u2248 1\nN N\u2211 i=1 \u2212 log q\u03d5(\u03b8i|xi)\nIf the conditional density estimator is sufficiently expressive, then this is minimized if and only if q\u03d5(\u03b8|x) = p(\u03b8|x) for all x in the support of p(x) (Papamakarios & Murray, 2016)."
        },
        {
            "heading": "2.2. Adversarial attacks and defenses",
            "text": "Szegedy et al. (2014) first proposed the concept of adversarial examples to fool neural networks. Adversarial examples are typically defined as solutions to an optimization problem (Szegedy et al., 2014; Goodfellow et al., 2015)\nx\u0303 = arg max ||x\u0303\u2212x||X\u2264\u03f5 \u2206(f(x\u0303), f(x)),\nwhere \u2206 specifies a distance between the predictions of the neural network.\nMany defenses against adversarial examples have been proposed. We build upon a popular defense called TRADES (Zhang et al., 2019)\u2013 when translated to inference tasks, TRADES can be interpreted as regularizing the neural network loss with the Kullback-Leibler divergence between the clean data and an adversarially perturbed data point:\nL(\u03d5) =Ep(x\u0303,x,\u03b8)[\u2212 log q\u03d5(\u03b8|x)+ \u03b2DKL(q\u03d5(\u03b8|x)||q\u03d5(\u03b8|x\u0303))]\nHere, x\u0303 is obtained by generating an adversarial example during training. This regularization requires generating an adversarial example for every datapoint and epoch, which requires running several gradient descent steps for every datapoint x\u2013 this would be exceedingly computationally costly for our inference tasks, but we will present methods for overcoming this limitation. To simplify notation, we abbreviate the posterior estimate given clean data as q := q\u03d5(\u03b8|x) and given perturbed data as q\u0303 := q\u03d5(\u03b8|x\u0303)."
        },
        {
            "heading": "3. Methods",
            "text": ""
        },
        {
            "heading": "3.1. Adversarial attacks on amortized inference",
            "text": "Adversarial perturbations are typically studied in classification tasks, in which the perturbation makes the neural network predict a wrong class. For amortized Bayesian inference, however, the output of the neural network is a continuous probability distribution (the estimate of the posterior). We therefore define the target of the adversarial perturbation to maximize the divergence between the estimated posterior given the \u2018clean\u2019 vs. the adversarially perturbed data, i.e., DKL(q(\u03b8|x)||q(\u03b8|x + \u03b4)) (GondimRibeiro et al., 2018; Willetts et al., 2021; Dax et al., 2022; Dang-Nhu et al., 2020).\nWe here focus on the Kullback-Leibler divergence1, but any divergence or pseudo-divergence (e.g. a distance function on moments of the posterior) would be possible (GondimRibeiro et al., 2018; Willetts et al., 2021; Dax et al., 2022; Dang-Nhu et al., 2020). An attack is thus defined by the constrained optimization problem\n\u03b4\u2217 = argmax \u03b4 DKL (q\u03d5(\u03b8|x)||q\u03d5(\u03b8|x+ \u03b4)) s.t. ||\u03b4|| \u2264 \u03f5.\nTo solve it, we use projected gradient descent (PGD) as an attacking scheme (Madry et al., 2018), following work on adversarial robustness for classification. We estimate the divergence between distributions parameterized by conditional normalizing flows using Monte Carlo sampling. We use the reparameterization trick (Kingma & Welling, 2014) to estimate gradients (details in A1.1).\nWe note that small perturbations to the observed data are expected to change the true posterior distribution. A sufficiently small perturbation will, in general, only cause a minor change in the posterior distribution (Latz, 2020). Furthermore, posterior predictive samples should match the perturbed observation (Berger et al., 1994; Sprungk, 2020). In contrast, we will demonstrate that the estimated posterior will change strongly after minor changes to the data, and that predictive samples of the posterior estimate do not match the perturbed observation, implying that the attack indeed breaks the amortized posterior estimate."
        },
        {
            "heading": "3.2. An adversarial defense for amortized inference",
            "text": "How do we modify NPE to be robust against such attacks? As described in Sec. 2.2, many adversarial defenses (e.g., TRADES) rely on generating adversarial examples during training, which can be computationally costly. In particular, for expressive conditional density estimators such as normalizing flows, generating an adversarial attack requires\n1We focus on DKL(q||q\u0303) to generate and evaluate attacks, but we discuss and evaluate the effect of a different adversarial objective in Sec. A4.2.\nseveral Monte Carlo (MC) samples at every gradient step, thus rendering this approach exceedingly costly. Here, we propose a computationally efficient method based on a moving average estimate of the trace of the Fisher information matrix.\nRegularizing by the Fisher information matrix To avoid having to generate adversarial examples during training, we exploit the fact that adversarial perturbations tend to be small and apply a second-order Taylor approximation to the KL-divergence (as has been done in previous work, Zhao et al. 2019; Shen et al. 2019; Miyato et al. 2016). This results in a quadratic expression (Blyth, 1994),\nDKL(q\u03d5(\u03b8|x)||q\u03d5(\u03b8|x+ \u03b4)) \u2248 1\n2 \u03b4TIx\u03b4,\nwhere Ix is the Fisher information matrix (FIM) with respect to x, which is given by\nIx = Eq\u03d5(\u03b8|x) [ \u2207x log q\u03d5(\u03b8|x)(\u2207x log q\u03d5(\u03b8|x))T ] .\nThis suggests that the neural network is most brittle along the eigenvector of the FIM with the largest eigenvalue (in particular, for a linear Gaussian model, the optimal attack on DKL corresponds exactly to the largest eigenvalue of the FIM, Sec. A5).). To improve robustness along this direction, one can regularize with the largest eigenvalue of the FIM \u03bbmax (Zhao et al., 2019; Shen et al., 2019; Miyato et al., 2016):\nL(\u03d5) = Ep(x,\u03b8) [\u2212 log q\u03d5(\u03b8|x) + \u03b2\u03bbmax] .\nWhile this approach overcomes the need to generate adversarial examples during training, computing the largest eigenvalue of the FIM can still be costly: First, it requires estimating an expectation over q\u03d5(\u03b8|x) to obtain the FIM and, second, computing the largest eigenvalue of a potentially large matrix. Below, we address these challenges.\nReducing the number of MC samples with moving averages For expressive density estimators such as normalizing flows, the expectation over q\u03d5(\u03b8|x) cannot be computed analytically, and has to be estimated with MC sampling:\nI\u0302x = 1\nN \u2211 i [ \u2207x log q\u03d5(\u03b8i|x)(\u2207x log q\u03d5(\u03b8i|x))T ] To reduce the number of samples required, we exploit that consecutive training iterations result in small changes of the neural network, and use an exponential moving average estimator for the FIM, i.e., I\u0302(t)x = \u03b3I\u0302x + (1 \u2212 \u03b3)I\u0302(t\u22121)x , where the superscript (t) indicates the training iteration.\nUsing the trace of the Fisher information matrix as regularizer Such an exponential moving average estimator\nAlgorithm 1 FIM-regularized NPE Inputs: conditional density estimator q\u03d5(\u03b8|x) with learnable parameters \u03d5, batch size B, number of training steps T , learning rate \u03b1, regularization strength \u03b2, regularization momentum \u03b3, number of Monte Carlo samples N Initialize: g(0) = 0 for t = 1 to T do\nfor b = 1 to B do L(\u03d5) = \u2212 1B log q\u03d5(\u03b8b|xb) // NPE loss \u03b81b, . . . ,\u03b8Nb \u223c q\u03d5(\u03b8|xb) // Monte Carlo end for r = 1B \u2211 b 1 N \u2211 i \u2211 d [\u2207x log q\u03d5(\u03b8ib|xb)] 2 d // FIM Trace g(t) = \u03b3\u2207\u03d5r + (1\u2212 \u03b3)g(t\u22121) // moving average \u03d5t = \u03d5t\u22121 \u2212 \u03b1(ADAM(\u2207\u03d5L(\u03d5) + \u03b2g(t)))\nend for\ndecreases the number of required MC samples, but it would require storing the FIM for each x and computing the FIM\u2019s largest eigenvalue at every iteration. Computing the largest eigenvalue scales cubically with the number of dimensions of x (but could be scaled with power-iterations, Miyato et al. 2016) and obtaining the largest eigenvalue of a random matrix (such as the MC-estimated FIM) requires many MC samples (Hayashi et al., 2018; Hayou, 2017). To overcome these limitations, we regularize instead with the trace of the FIM, which is an upper bound to the largest eigenvalue.\nUnlike the largest eigenvalue, the trace of the FIM can be computed from MC samples quickly and without explicitly computing the FIM. Using the trace of the FIM simplifies the moving average estimator to\ntr(I\u0302(t)x ) = \u03b3tr(I\u0302x) + (1\u2212 \u03b3)tr(I\u0302(t\u22121)x ).\nTo avoid maintaining the computation graph for every x and (t), we store the average gradient with respect to the neural network parameters instead of storing tr(I\u0302(t)x ) directly,\ng(t) := \u2207\u03d5Ep(x) [ tr(I\u0302(t)x ) ] .\nSummary and illustration Our adversarial defense is summarized in Algorithm 1. At every iteration, the method computes the Monte Carlo average of the trace of the Fisher information, updates the moving average of this quantity, and uses it as a regularizer to the negative log-likelihood loss. Despite our approximations, our method performs similarly to regularizers based on the largest eigenvalue or trace of the exact FIM (comparison with a Gaussian density estimator on the VAE task in Sec. A7). Finally, we note that using the FIM-regularizer systematically changes the posterior estimate even with infinite training data and, therefore, leads to a trade-off between accuracy on clean data and robustness\nto perturbations (Sec. A3). For a generalized linear Gaussian density estimator, the bias induced by FIM-regularization can be calculated exactly (details in Sec. A6).\nWe demonstrate the method on a simple one-dimensional conditional density estimation task using a neural spline flow (Dolatabadi et al., 2020) (Fig. 2). The Fisher information is large in x-regions where q\u03d5(\u03b8|x) changes quickly as a function of x. By regularizing with the trace of the FIM, the learned density is significantly smoother."
        },
        {
            "heading": "4. Experimental results",
            "text": ""
        },
        {
            "heading": "4.1. Benchmark tasks",
            "text": "We first evaluated the robustness of Neural Posterior Estimation (NPE) and the effect of FIM-regularization on six benchmark tasks (details in Sec. A1.2). Rather than using established benchmark tasks (Lueckmann et al., 2021), we chose tasks with more high-dimensional data, which might offer more flexibility for adversarial attacks.\nVisualizing adversarial attacks We first visualized the effect of several adversarial examples on inference models trained with standard (i.e., unregularized) NPE. We trained NPE with a Masked Autoregressive Flow (MAF, Papamakarios et al. 2017) on 100k simulations and generated an adversarial attack for a held-out datapoint. Although the perturbations to the observations are hardly perceptible, the posterior estimates change drastically, and posterior predictive samples match neither the clean nor the perturbed observation (Fig. 3). This indicates that the attacked density estimator predicts a posterior distribution that does not match the true Bayesian posterior given the perturbed datapoint p(\u03b8|x\u0303), but rather it predicts an incorrect distribution.\nHow does the adversarial attack change the prediction of the neural density estimator so strongly? We investigated two possibilities for this: First, the adversarial attack could construct a datapoint x\u0303 which is misspecified. Previous work has reported that NPE can perform poorly in the presence of misspecification (Cannon et al., 2022). Indeed, on the SIR benchmark task (Fig. 3D), we find clues that are consistent with misspecification: At the end of the simulation (t > 20), the perturbed observation shows an increase in infections although they had already nearly reached zero. Such an increase cannot be modeled by the simulator and cannot be attributed to the noise model (since the noise is log-normal and, thus, small for low infection counts).\nA second possibility for the adversarial attack to strongly change the posterior estimate would be to exploit the neural network itself and generate an attack for which the network produces poor predictions. We hypothesized that, on our benchmark tasks, this possibility would dominate. To investigate this, we performed adversarial attacks on different density estimators and evaluated how similar the adversarial attacks were to each other (Fig. A3). We find that the attacks largely differ between different density estimators, suggesting that the attacks are indeed targeted to the specific neural network.\nQuantifying the impact of adversarial attacks We quantified the effect of adversarial attacks on NPE without using an adversarial defense. After training NPE with 100k simulations, we constructed adversarial attacks for 104 held-out datapoints (as described in Sec. 3.1). As a baseline, we also added a random perturbation of the same magnitude on each datapoint. We then computed the average DKL between the posterior estimates given clean and perturbed data (Fig. 4). For all tasks and tolerance levels (the scale of the perturbation), the adversarial attack increases the DKL more strongly than a random attack. In addition, for all tasks apart from the linear Gaussian task, the difference between the adversarial and the random attack is several orders of magnitude (Fig. 4A).\nAs a second evaluation-metric, we computed the expected coverage of the perturbed posterior estimates, which allows us to study whether posterior estimates are under-, or overconfident (Fig. 4B, details in Sec. A1.3) (Cannon et al., 2022). For stronger perturbations, the posterior estimates become overconfident around wrong parameter regions and show poor coverage. As expected, adversarial attacks impact the coverage substantially more strongly than random attacks.\nAdditional results for different density estimators, alternative attack definitions, and simulation budgets can be found\nin Sec. A4.2 (Figs. A4, A1, A5). The results are mostly consistent across different density estimators (with minor exceptions at low simulation budgets), indicating that more flexible estimators are not necessarily less robust.\nAdversarial defense of NPE Next, we evaluated the adversarial robustness when regularizing NPE with the moving average estimate of the trace of the Fisher Information Matrix (FIM) (Sec. 3.2). In addition, we evaluated two approaches adapted from defense methods for classification tasks\u2013 however, both of these approaches rely on generating adversarial examples during training and are, thus, more computationally expensive (details in Sec. A2, methods are labeled as \u2018Adv. training\u2019 and \u2018TRADES\u2019).\nAll adversarial defense methods significantly reduce the ability of attacks to change the posterior estimate (Fig. 5A). In addition, the FIM regularizer performs similarly to other defense methods but is computationally much more efficient and scalable (A4.1, Fig. A2, sweeps for \u03b2 in Fig. A6).\nWe evaluated the expected coverage when using FIM regularization (Fig. 5B, results for Adv. Training and TRADES in Fig. A7). For all tasks, the coverage is shifted towards the upper left corner, indicating a more conservative posterior estimate (further analysis in Sec. A3). Even for medium to high tolerance levels (i.e., strong perturbations), the posterior estimate often remains underconfident and covers the\ntrue parameter set, a behavior which has been argued to be desirable in scientific applications (Hermans et al., 2022). Other defense methods (that were not specifically developed as adversarial defenses), such as posterior ensembles or noise augmentation, barely increase the adversarial robustness of NPE (Fig. A7, Sec. A4.3). Further, we investigate this effect directly comparing against the true posterior (as estimated via MCMC for a subset of tasks) in Sec. A8, verifying that posterior approximation on adversarial perturbed data is poor but can be improved using FIM regularization.\nFinally, we studied the trade-off between robustness to adversarial perturbations and accuracy of the posterior estimate on unperturbed data (Zhang et al., 2019; Tsipras et al., 2019). We computed the accuracy on unperturbed data (evaluated as average log-likelihood) and the robustness to adversarial perturbations (measured as DKL between clean and perturbed posteriors) for a range of regularization strengths \u03b2 (Fig. 5C). For a set of intermediate values for \u03b2, it is possible to achieve a large gain in robustness while only weakly reducing accuracy (details in Sec. A3, results for other density estimators in Sec. A4.3, Figs. A6 and A8).\nOverall, FIM regularization is a computationally efficient method to reduce the impact of adversarial examples on NPE. While it encourages underconfident posterior estimates, it allows for high robustness with a relatively modest reduction in accuracy."
        },
        {
            "heading": "4.2. Neuroscience example: Pyloric network",
            "text": "Finally, we performed adversarial attacks and defenses on a real-world simulator of the pyloric network in the stomatogastric ganglion (STG) of the crab Cancer Borealis. The simulator includes three model neurons, each with eight membrane conductances and seven synapses (31 parameters in total) (Prinz et al., 2003; 2004). Prior studies have used extensive simulations from prior samples and performed amortized inference with NPE (18 million simulations in Gonc\u0327alves et al. (2020), 9 million in Deistler et al. (2022b)). Both of these studies used hand-crafted summary statistics. In contrast, we here performed inference on the raw traces (subsampled by a factor of 100 due to memory constraints).\nAfter subsampling, the data contains three voltage traces, each of length 800. We ran 8M simulations from the prior and excluded any parameters which generated physiologically implausible data (Lueckmann et al., 2017), resulting in a dataset with 750k datapoints. We used a one-dimensional convolutional neural network for each of the three traces and passed the resulting embedding through a fully-connected neural network (Fig. 6).\nThe neural density estimator, trained without regularization, is susceptible to adversarial attacks (Fig. 6A). Given unperturbed data, the posterior predictive closely matches the data, whereas for the adversarially perturbed data the posterior\npredictive no longer matches the observations. In addition, the predicted posterior given clean data strongly differs from the predicted posterior given adversarially perturbed data.\nIn contrast, when regularizing with the FIM approach, the neural density estimator becomes significantly more robust to adversarial perturbations (Fig. 6B). The posterior predictives now closely matched the data, both for clean as well as adversarially perturbed observations. In addition, the posterior estimates given clean and perturbed observations match closely.\nWe quantified these results by computing the DKL between clean and adversarially perturbed posterior estimates as well as the expected coverage (Fig. 6C). NPE without regularization has a higher DKL and is overconfident. In contrast, the FIM-regularized posterior is underconfident, even for strong perturbations. These results demonstrate that real-world simulators can strongly suffer from adversarial attacks. The results also show that our proposed FIM-regularizer scales to challenging and high-dimensional tasks."
        },
        {
            "heading": "5. Discussion",
            "text": "We showed that amortized Bayesian inference can be vulnerable to adversarial attacks. The posterior estimate can change strongly when slightly perturbing the observed data,\nleading to inaccurate inference results. This poses a difficult challenge for amortized Bayesian inference which would severely limit its utility for applications in which trustworthy posterior estimates are essential: If small changes in the input data can have a strong impact on inference results, using misspecified data, or simply data not encountered during training could also lead to severely wrong conclusions.\nTo address this issue, we propose a computationally efficient defense strategy that can be used to reduce the vulnerability of Neural Posterior Estimation to adversarial attacks. We demonstrate the effectiveness of this method and show that it can significantly improve the robustness and reliability of NPE in the presence of adversarial attacks.\nPrior work on adversarial attacks and defenses Adversarial attacks and defenses have been studied on variational autoencoders (Kuzina et al., 2022; Husain & Knoblauch, 2022; Shu et al., 2018; Barrett et al., 2022; Willetts et al., 2021; Akrami et al., 2022). Our work differs from these papers in that we focus on posterior distributions parameterized by expressive conditional density estimators such as normalizing flows. Note, crucially, that the attacks in this context are on the conditioning-variable, in contrast to previous work on flow-based models studying attacks on the output of (unconditional) flow-based models (Pope et al., 2020). To train our inference model, we use the negative log-likelihood as loss-function (as compared to the ELBO for variational autoencoders), which makes our approach applicable to non-differentiable and implicit models. Several recent studies have proposed improvements to the robustness of NPE (Dellaporta et al., 2022; Lemos et al., 2022; Ward et al., 2022; Matsubara et al., 2022; Finlay & Oberman, 2019), but none of them have considered defenses against adversarial attacks. Dax et al. (2022) proposed the\nuse of (likelihood-based) importance sampling to identify and correct poor approximations in an application from astrophysics, and in this context also evaluated the adversarial robustness of a neural posterior estimator. Concurrent theoretical work of Altekru\u0308ger et al. (2023) established basic conditions under which conditional density estimators on convergence are provably robust in particular depending on the Lipschitz constant of the inference network with respect to the observation.\nAmortized Bayesian inference We studied adversarial robustness of a particular amortized Bayesian inference algorithm, Neural Posterior Estimation (NPE). Other simulationbased inference methods can be categorized as amortized as well, e.g., Neural Ratio Estimation (NRE) or Neural Likelihood Estimation (NLE) (Cranmer et al., 2020; Hermans et al., 2020; Papamakarios et al., 2019). These methods do not require new simulations or network training for new observations, but they require a (potentially expensive) inference phase to obtain the posterior. In addition, another approach to amortized Bayesian inference would be to perform amortized variational inference, which requires a differentiable model and likelihood-evaluations. We leave the study of adversarial attacks in these methods to future work.\nModel misspecification and adversarial robustness Previous work (Cannon et al., 2022) raised concerns about the reliability of NPE on misspecified simulators. Adversarial examples exploit the brittleness of neural networks to construct examples on which NPE performs particularly poorly. As such, our study can be considered as a worstcase scenario of how minor deviations in the observed data can impact its reliability. We find that adversarial examples depend strongly on the network (for the same simulator), indicating the crucial role of the inference network.\nLimitations Using a defense against adversarial attacks comes at two costs: Increased computational cost and, potentially, broader posteriors on clean data. Our proposed regularization scheme largely reduces computational cost (by up to an order of magnitude compared to other defense methods such as TRADES), but it, nonetheless, requires drawing Monte Carlo samples from the posterior estimate and evaluating its gradient w.r.t. every datapoint at every epoch. Across six benchmark tasks, our regularizer increased training time by a factor of four (compared to standard NPE, Fig. A2). Our analysis places emphasis on inference networks that have the capability to learn summary statistics of complex data, if necessary, in an end-to-end manner using neural networks (Chan et al., 2018; Radev et al., 2020). However, in various applications, expert-crafted summary statistics are commonly employed which can be explicitly designed to be robust against certain perturbations.\nIt has been argued that, in many applications (and in particular in the natural sciences), it is desirable to have underconfident posteriors (Hermans et al., 2022) \u2013 however, posterior estimates that are systematically too broad lead to a lower rate of learning from data and, thus, slower information acquisition. While we demonstrated that our method is comparable to NPE in terms of negative log-likelihood, users might need to evaluate the trade-off between robustness and information acquisition for their applications."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank Pedro Goncalves, Jaivardhan Kapoor, and Maximilian Dax for discussions and comments on the manuscript. MG and MD are supported by the International Max Planck Research School for Intelligent Systems (IMPRS-IS). This work was supported by the German Research Foundation (DFG) through Germany\u2019s Excellence Strategy \u2013 EXCNumber 2064/1 \u2013 Project number 390727645, the German Federal Ministry of Education and Research (Tu\u0308bingen AI Center, FKZ: 01IS18039A), and the \u2018Certification and Foundations of Safe Machine Learning Systems in Healthcare\u2019 project funded by the Carl Zeiss Foundation.\nSoftware and Data We used PyTorch for all neural networks (Paszke et al., 2019) and hydra to track all configurations (Yadan, 2019). Code to reproduce results is available at https:// github.com/mackelab/RABI."
        },
        {
            "heading": "A1. Further experimental details",
            "text": "A1.1. Training procedure\nAll methods and evaluations were conducted using PyTorch (Paszke et al., 2019). We used Pyro (Bingham et al., 2019) implementations of masked autoregressive flow (MAF) or rational linear spline flow (NSF) (Papamakarios et al., 2017; Dolatabadi et al., 2020). We employed three transforms, each of which was parameterized using a two-layered ReLU multi-layer perceptron (MLP) with 100 hidden units. For higher dimensional tasks such as Hodgkin Huxley, VAE, and Spatial SIR, we used a ReLU MLP embedding network with 400, 200, and 100 hidden units and outputting a 50 dim vector.\nThe diagonal Gaussian model used two hidden layers, a hidden layer of size 100, and ReLU activations. The Multivariate Gaussian and mixture density networks used a hidden layer of size 200.\nWe trained each model with the Adam optimizer with a learning rate of 10\u22123, a batch size of 512, and a maximum of 300 epochs. If training failed, the learning rate was reduced to 10\u22124 or 10\u22125. To prevent overfitting, we used early stopping based on a validation loss evaluated on 512 hold-out samples. Each model was trained on the same set of either 103, 104, or 105 simulations.\nFor the adversarial attacks, we performed 200 projected gradient descent steps and estimated the DKL with a Monte Carlo average of 5 samples at each step (for the Gaussian models we used the analytical solution). Additionally, each adversarial example was clamped to the minimum and maximum values within the test set to avoid generating samples outside of the support of p(x). After the optimization was finished, for the evaluation of the attack, the adversarial objective was evaluated with a Monte Carlo budget of 256 samples. We note that, as the scale of data varies strongly between simulators, all tolerance levels \u03f5 were normalized by the average of the standard deviation of prior predictives. Table 1 shows the resulting tolerance levels.\nFor the results shown in Fig. 5, we used a different value for the regularization strength \u03b2 for each task, as the parameter is coupled to the magnitude of the perturbation (which is different for each task). The particular value of \u03b2 was hand-picked based on initial experiments. The values were: \u03b2 = 0.001 for Gaussian linear, \u03b2 = 0.1 for SIR, \u03b2 = 0.01 for Lotka Volterra, \u03b2 = 100 for Hodgkin Huxley, \u03b2 = 0.01 for VAE and \u03b2 = 0.1 for Spatial SIR. Sweeps for \u03b2 for each task are shown in Fig. A6. We used 5 Monte Carlo samples per iteration and a momentum of \u03b3 = 0.85 for each benchmark task for the FIM regularizer. We used a MAF for all benchmark results in the main paper and evaluated different density estimators in Figs. A4 and A1.\nFor the pyloric network task, we employed a MAF with three transforms, each parameterized by a 3-layer neural network with 200 hidden neurons. We also utilized an embedding network composed of three 1D convolutional neural networks, each with three convolutional layers that produce six, nine, and twelve output channels. These networks were applied to the voltage trace of each neuron. The results were then summarized by a 3-layer feed-forward neural network, and reduced to a 100-dimensional feature vector. We trained the model using 750,000 pre-selected simulations and evaluated its convergence on a validation set of 4096 additional datapoints. The evaluation was conducted on 10,000 separate simulations. For the FIM regularized model, we used \u03b2 = 100. Further, we set the number of Monte Carlo samples within the exponential moving average to one; the momentum remained \u03b3 = 0.85.\nA1.2. Benchmark tasks\nWe used the following benchmark tasks, which produce relatively complex and high-dimensional data. We used these tasks instead of established benchmark tasks (Lueckmann et al., 2021) because our tasks are chosen to have more high-dimensional data and, thus, might offer more flexibility for adversarial attacks.\nGaussian Linear: A simple diagonal linear mapping (entries sampled from a standard Gaussian) of a ten-dimensional vector subject to isotropic Gaussian noise:\np(x,\u03b8) = N (x;A\u03b8, \u03c32I)N (\u03b8,0, I)\nwith \u03c3 = 0.1. As a result, the posterior is also Gaussian and analytically tractable.\nLotka Volterra: An ecological predator-prey model with four parameters. It is given by the solution of the following differential equation:\ndx dt = \u03b1x\u2212 \u03b2xy dy dt = \u03b4xy \u2212 \u03b3y\nwith \u03b1 representing the growth rate of prey, \u03b2 the death rate of prey, \u03b4 the hunting efficiency of the predator and \u03b3 the death rate of the predator. The observed data are the predator and prey population densities at 50 equally spaced time points. We added normally distributed noise with \u03c3 = 0.05. The prior for the parameters is Gaussian with \u00b5 = 0. and \u03c3 = 0.5, transformed by a sigmoid function in the simulator to be positive and bounded. The resulting 4-dimensional posterior is highly correlated.\nVAE: The decoder g\u03c8(x) of a Variational Autoencoder (VAE) was used as a generative model for handwritten digits (Kingma & Welling, 2014). The prior is a three-dimensional standard Gaussian. Images generated by the decoder were used as observed data (28\u00d7 28 dimensional), with Gaussian observation noise with \u03c3 = 0.05:\np(x,\u03b8) = N (x; g\u03c8(x), \u03c32I)N (\u03b8;0, I)\nDue to the training procedure of the VAE, the posterior should be almost Gaussian.\nHodgkin Huxley: A neuroscience model that describes how action potentials in neurons are initiated and propagated. It is implemented based on Pospischil et al. (2008) and taken from Tejero-Cantero et al. (2020). We use a uniform prior constrained to biologically reasonable values. The observed data is the membrane voltage at 200 equally spaced time points, to which we added normally distributed noise with \u03c3 = 0.1, leading to a 7-dimensional posterior distribution.\nSIR: A epidemiological model with two free parameters: The rate of recovery for infected individuals \u03b3 and the rate of new infections \u03b2 for a population of N = 5. The solution satisfies the following differential equation\ndS dt = \u2212\u03b2S \u00b7 I N dI dt = \u03b2 S \u00b7 I N \u03b3 \u2212 \u03b3I dI dt = \u03b3I.\nThe observed data correspond to the number of infections at 50 equally spaced time points. We added log-normal observation noise with \u03c3 = 0.2. The prior was a Gaussian with \u03c3 = 2. transformed by a sigmoid function, resulting in a complex 2-dimensional posterior.\nSpatial SIR: An epidemiological model with a spatial dimension, similar to Hermans et al. (2022). The model is initialized with three infections at random locations on the grid. The infection then propagates to neighboring grid cells with probability \u03b2 per time step. Infected people recover with probability \u03b3 at each time step. We observed a 30\u00d7 30 grid of infected/non-infected regions, subject to a Beta noise model modeling the probability of being infected after a test. A logNormal prior with \u03c3 = 0.5 was used.\nA1.3. Metrics\nThe expected coverage was evaluated as proposed in Cannon et al. (2022); Hermans et al. (2022). Given the 100(1\u2212 \u03b1)% highest posterior density region of the posterior estimate HPR1\u2212\u03b1, we target to estimate Ep(\u03b8,x) [1{\u03b8 \u2208 HPR1\u2212\u03b1}], where 1 is the indicator function. If the model is well-calibrated, the empirical coverage should match the nominal coverage 1\u2212 \u03b1.\nAs in Cannon et al. (2022), we evaluate the coverage given (adversarially) perturbed data. Thus, the expected coverage becomes:\nEp(x\u0303) [ Ep(\u03b8|x\u0303) [1{\u03b8 \u2208 HPR1\u2212\u03b1}] ] We use a Monte Carlo approximation to obtain HPR1\u2212\u03b1 as described by Rozet et al. (2021); Deistler et al. (2022a) to efficiently estimated this quantity."
        },
        {
            "heading": "A2. Adversarial training and TRADES",
            "text": "We mainly compared against two well-known methods to defend against adversarial examples (additional defenses and results in Sec. A4.3).\nFigure A1. Expected coverages for all density estimators. Expected coverage metric for a specific density estimator trained with 105 simulations on standard NPE loss. The performance on well-specified data in black; colors indicate performance on adversarially perturbed data at certain tolerance levels.\nA2.1. Adversarial training\nMadry et al. (2018) proposed that the loss should be modified such that, instead of minimizing the negative log-likelihood (NLL) given the clean observation, one minimizes the NLL given the worst possible observation x\u0303 within an \u03f5 ball around the observation (i.e. the observation that has the highest NLL within the \u03f5 ball). Formally, this objective can be written as\nmin \u03d5\nEp(x,\u03b8) [\nmax x\u0303\u2208{x\u0303 | ||x\u0303\u2212x||\u2264\u03f5}\n\u2212 log q\u03d5(\u03b8|x\u0303) ] .\nThis scheme encourages the trained neural network to have a low NLL for any x\u0303 within the \u03f5-ball and thus encourages robustness to any adversarial perturbation.\nIn order to find the x\u0303 with the highest NLL within the \u03f5-ball, one commonly generates an adversarial example during training e.g., with projected gradient descent (Madry et al., 2018). Considering the adversarial perturbation as a distribution p(x\u0303|x,\u03b8) gives:\nL(\u03d5) = Ep(x,\u03b8) [ Ep(x\u0303|x,\u03b8) [\u2212 log q\u03d5(\u03b8|x\u0303)] ] = Ep(x\u0303,\u03b8) [\u2212 log q\u03d5(\u03b8|x\u0303)]\nThis reveals that by modifying the training scheme, q\u03d5 no longer converges to the true posterior but to a different posterior distribution given by p\u0303(\u03b8|x\u0303) \u221d p\u0303(x\u0303|\u03b8)p(\u03b8) = \u222b p(x\u0303|x,\u03b8)p(x|\u03b8)dxp(\u03b8)\ni.e., the posterior distribution given the likelihood of observing the adversarially perturbed x\u0303 given \u03b8. Therefore, adversarial training can be interpreted as a regularization scheme where the data is perturbed by the adversarial perturbation (instead of a random perturbation as in Shu et al. (2018)).\nIn our experiments, we use an \u21132 projected gradient descent attack with 20 iterations during training. After initial experiments, we hand-picked \u03f5 = 0.1 for the Gaussian linear task and \u03f5 = 1.0 for the other benchmark tasks during training.\nA2.2. TRADES\nA second method for adversarial robustness was proposed by Zhang et al. (2019). Their proposed loss function balances the trade-off between performance on clean and adversarially perturbed observations, controlled through a hyperparameter \u03b2. The resulting surrogate loss adds a Kullback-Leibler divergence regularizer in the form of\nL(\u03d5) = Ep(x\u0303,x,\u03b8) [\u2212 log q\u03d5(\u03b8|x) + \u03b2DKL(q\u03d5(\u03b8|x)||q\u03d5(\u03b8|x\u0303)] .\nThis ensures that the posterior estimate is smooth (as measured by the KL divergence). However, this approach requires the ability to evaluate the KL divergence, which, for normalizing flows, can only be approximated through Monte Carlo techniques.\nIn general, the strength of regularization is determined by both the magnitude of the adversarial example, \u03f5, and the hyperparameter \u03b2. Initial experiments showed that \u03f5 had a greater effect than \u03b2, so we only varied \u03f5. To estimate the DKL during training, we use a single Monte Carlo sample. We fixed \u03b2 = 0.1 and hand-picked, based on initial experiments, \u03f5 = 0.1 for the linear Gaussian and VAE, \u03f5 = 0.5 for Hodgkin Huxley, Lotka Volterra, and Spatial SIR. The optimization to obtain x\u0303 was run with an \u21132 projected gradient descent attack with 20 iterations."
        },
        {
            "heading": "A3. Tradeoff between posterior approximation and robustness",
            "text": "Adversarially robust models typically sacrifice accuracy on clean data in order to achieve robustness. The errors on clean and perturbed data have even been suggested to be fundamentally at odds (Zhang et al., 2019; Tsipras et al., 2019) and are subject to the required strength of adversarial robustness (Min et al., 2021), even in the infinite data limit.\nRegularizing with the Fisher Information Matrix (FIM) creates a similar trade-off between accuracy on clean and perturbed data. The standard loss of neural posterior estimation minimizes\nL(\u03d5) = Ep(x) [DKL(p(\u03b8|x)||q\u03d5(\u03b8|x))] = Ep(x,\u03b8) [\u2212 log q\u03d5(\u03b8|x)]\n10 3\n10 4\n10 5\nNumber of simulations\n0\n200\n400\n600\nR un\ntim e\n[s ]\nDiagonal Gaussian\n10 3\n10 4\n10 5\nNumber of simulations\n0\n250\n500\n750\nMultivariate Gaussian\n10 3\n10 4\n10 5\nNumber of simulations\n0\n250\n500\n750 Mixture of Gaussian's\n10 3\n10 4\n10 5\nNumber of simulations\n0\n2000\n4000 MAF\n10 3\n10 4\n10 5\nNumber of simulations\n0\n5000\n10000\nNSF\nFIM regularizer Adv. training Trades None\nFigure A2. Runtime. Average runtime in seconds for each defense, calculated across all benchmark tasks and various hyperparameters per density estimator. For the MAF estimator with 5 Monte Carlo samples, the average timings for 100k training simulations are: 60 seconds for NPE, 250 seconds for FIM, 770 seconds for adv. training, and 2790 seconds for TRADES.\nwhereas the FIM regularizer minimizes\n\u2126(\u03d5) = Ep(x,x\u0303) [DKL(q\u03d5(\u03b8|x)||q\u03d5(\u03b8|x\u0303))] .\n\u2126(\u03d5) is minimized globally if q\u03d5(\u03b8|x) = q\u03d5(\u03b8|x\u0303) for all x, x\u0303 \u223c p(x). This suggests that the optimal q\u03d5(\u03b8|x\u0303) is independent of x and, thus, indeed at odds with approximating the posterior distribution given clean data.\nThe strength of this trade-off is determined by the value of the hyperparameter \u03b2, and the effect on the posterior fit is demonstrated in Fig. 5C, were we plot the trade-off between accuracy on clean data (evaluated as average log-likelihood) and the robustness to adversarial perturbations (measured as DKL between clean and perturbed posteriors). The plotted values of \u03b2 are Pareto-optimal solutions approximately solving the multi-objective optimization problem:\nmin \u03b2\n[ L(\u03d5\u2217\u03b2),\u2126(\u03d5\u2217\u03b2) ] with \u03d5\u2217\u03b2 = argmin\n\u03d5\u03b2\nL(\u03d5\u03b2) + \u03b2Ep(x) [tr(Ix)]\nIt is clear that a large value of \u03b2 heavily regularizes \u2126, pushing it towards zero, which results in the inference model ignoring the data and increasing L. Thus, as \u03b2 grows large, q\u03d5(\u03b8|x) approaches p(\u03b8), i.e. the prior distribution, as this is the best estimate according to L which is independent of x. This is in correspondence with approaches for robust generalized Bayesian inference with, e.g., \u03b1-posteriors (Gru\u0308nwald & Ommen, 2017; Vovk, 1990; Medina et al., 2022).\nHowever, for smaller values of \u03b2, there is a plateau where the accuracy of clean data is almost constant, but the levels of robustness vary significantly. This suggests that multiple inference models exist that have similar approximation errors but differ in their robustness to adversarial examples. The regularizer in this region can effectively induce robustness without sacrificing much accuracy.\nIt is worth noting that at a certain value of \u03b2, the approximation error increases significantly while the robustness decreases only gradually. As previously discussed in the main paper, \u03b2 is closely related to the magnitude of the adversarial perturbation x\u0303 i.e. the tolerance level \u03f5. The true posterior might not be robust to such large-scale perturbations, making it an invalid solution subject to robustness constraints."
        },
        {
            "heading": "A4. Additional benchmark results",
            "text": "Here, we present additional results obtained on the benchmark tasks.\nA4.1. Runtime\nIn Figure A2, we show the average runtime of the benchmark tasks. It can be observed that FIM regularization has a slightly higher cost compared to standard NPE, whereas TRADES is substantially more expensive across various density estimators. This especially holds for normalizing flows where the DKL regularizer is estimated via Monte Carlo.\nA4.2. Additional results on attacks\nWhich observations are particularly vulnerable to adversarial attacks? In our experiments, we generated a subset of parameters \u03b8, along with well-specified data points x, and adversarial examples found on these data points, x\u0303. We\nDiagonal Gaussian Multivariate Gaussian Mixture of Gaussian's MAF NSF\n\u22125 0 5 \u03b81\n\u22124\n\u22122\n0\n2\n4\n\u03b8 2\n\u03b5=0.1\n\u22125 0 5 \u03b81\n\u22124\n\u22122\n0\n2\n4\n\u03b8 2\n\u03b5=0.1\n\u22125 0 5 \u03b81\n\u22124\n\u22122\n0\n2\n4\n\u03b8 2\n\u03b5=1.0\n\u22125 0 5 \u03b81\n\u22124\n\u22122\n0\n2\n4\n\u03b8 2\n\u03b5=1.0\nFigure A3. Parameters which generated data susceptible to adversarial perturbations. In black, we illustrate the prior over parameters \u03b8 for the SIR task. For each density estimator trained with 105 simulations, we plot the distribution of \u03b8\u2019s on which we found the 50 \u201dstrongest\u201d adversarial examples using the L2PGDAttack maximizing DKL(q||q\u0303) (left) or DKL(q\u0303||q) (right).\nnext asked which observations are particularly vulnerable to adversarial attacks and what regions these correspond to in parameter space. We selected the 10% datapoints which had the highest DKL between posterior estimates given clean and perturbed data and visualized the distribution of their corresponding ground truth parameters (Fig. A3). Attacks on different density estimators have higher efficacy on different sets of parameters, with some similarities (especially for similar density estimators such as Gaussian and Multivariate Gaussian). This indicates that the attacks not only leverage worst-case misspecification but also attack the particular neural network. Notably, parameters that generate data that is vulnerable to adversarial attacks are not necessarily found in the tails of the prior (where training data is scarce), but also in regions where many training data points are available. This observation could imply either vulnerable areas in the specific neural networks or/and susceptible regions within the generative model.\nAre more complex density estimators less robust? We evaluated the DKL (forward and backward) between posterior estimates given clean and perturbed data for several conditional density estimators. Our results show similar adversarial robustness across all tested density estimators (Fig. A4). We note that attacking simple models might appear to be more vulnerable because the adversarial objective can be computed in closed form, whereas complex models require Monte Carlo approximations.\nWe also computed the expected coverage for all density estimators (Fig. A1). Again, the expected coverages suggest a similar level of adversarial robustness across different conditional density estimators.\nDoes the adversarial objective matter? We evaluated whether using the forward vs the backward DKL as the target for the adversarial attack influences the results. Despite minor differences, there is no clear advantage of divergence over the other. Adversaries with different objectives may find different adversarial examples that are more severe, as measured by their notion of \u201ddistance\u201d between the distributions, as shown in Figure A4. As the KL divergence is locally symmetric, these differences are only noticeable for larger tolerance levels \u03f5.\nIn addition, we evaluated an attack based on the Maximum-Mean discrepancy (MMD)\n\u03b4\u2217 = argmax \u03b4 MMD2(q\u03d5(\u03b8|x)||q\u03d5(\u03b8,x+ \u03b4)) s.t. ||\u03b4|| \u2264 \u03f5.\nWe use the kernel MMD with an RBF kernel, estimated by a linear time Monte Carlo estimator as defined in Gretton et al. (2012) using ten samples. The MMD attack has a similar impact as DKL attacks, but it is significantly weaker for some tasks, such as Lotka Volterra and SIR. One potential explanation for this could be an unsuitable kernel selection. Specifically, if the length scale of the RBF kernel is too small, it is well-known that the gradients tend to vanish. This issue can be particularly noticed in the SIR task, which plateaus for larger tolerance levels (in contrast to KL-based attacks, which explode). We note, however, that the MMD attack could also be applied to implicit density estimators (such as VAEs or GANs)\nFinally, we evaluated an attack that minimizes the log-likelihood of the true parameters:\n\u03b4\u2217 = argmax \u03b4 \u2212 log q\u03d5(\u03b8o|xo + \u03b4) s.t. ||\u03b4|| \u2264 \u03f5.\n10 3\n10 7\n10 00\nLinear Gaussian\n10 \u22124\n10 0\nLotka Volterra\n10 0\n10 2\nVAE\n10 2\n10 6\nHodgkin Huxley\n10 \u22121\n10 1\nSIR\n10 \u22125\n10 \u22122\nSpatial SIR\n10 1\n10 3\nN um\n. s im\nua lti\non s\n1 00\n00\n10 0\n10 2\n10 0\n10 2\n10 3\n10 8\n10 0\n10 2\n10 \u22125\n10 \u22122\n0.1 0.2 0.3 0.5 1.0 2.0 Tolerance \u03b5\n10 1\n10 3\n10 00\n00\n0.1 0.2 0.3 0.5 1.0 2.0 Tolerance \u03b5\n10 \u22121\n10 0\n10 1\n0.1 0.2 0.3 0.5 1.0 2.0 Tolerance \u03b5\n10 0\n10 2\n0.1 0.2 0.3 0.5 1.0 2.0 Tolerance \u03b5\n10 2\n10 5\n0.1 0.2 0.3 0.5 1.0 2.0 Tolerance \u03b5\n10 0\n10 2\n0.1 0.2 0.3 0.5 1.0 2.0 Tolerance \u03b5\n10 \u22124\n10 0\nDiagonal Gaussian Multivariate Gaussian Mixture of Gaussian's MAF NSF\n10 2\n10 5\n10 00\nLinear Gaussian\n10 \u22124\n10 0\nLotka Volterra\n10 0\n10 2\nVAE\n10 \u22123\n10 2\nHodgkin Huxley\n10 \u22121\n10 1\nSIR\n10 \u22125\n10 \u22122\nSpatial SIR\n10 1\n10 3\nN um\n. s im\nua lti\non s\n1 00\n00\n10 \u22121\n10 0\n10 1\n10 0\n10 2\n10 1\n10 3\n10 0\n10 2\n10 \u22125\n10 \u22122\n0.1 0.2 0.3 0.5 1.0 2.0 Tolerance \u03b5\n10 0\n10 1\n10 2\n10 00\n00\n0.1 0.2 0.3 0.5 1.0 2.0 Tolerance \u03b5\n10 \u22121\n10 0\n10 1\n0.1 0.2 0.3 0.5 1.0 2.0 Tolerance \u03b5\n10 0\n10 2\n0.1 0.2 0.3 0.5 1.0 2.0 Tolerance \u03b5\n10 2\n10 4\n0.1 0.2 0.3 0.5 1.0 2.0 Tolerance \u03b5\n10 1\n10 4\n0.1 0.2 0.3 0.5 1.0 2.0 Tolerance \u03b5\n10 \u22124\n10 0\nFigure A4. Robustness for different density estimators. Each density estimator is trained with NPE loss with a different number of simulations, attacked by an \u21132 projected gradient descent attack trying to maximize DKL(q||q\u0303) (top) or DKL(q\u0303||q) (bottom).\nNote that his attack requires access to one true parameter, which is only available for observations generated from the model and hence is not generally applicable. Furthermore, minimizing the likelihood of a single good parameter may not inevitably decrease the likelihood of all probable parameters. This attack strongly impacts the expected coverage since the attack objective is explicitly designed to avoid the true parameter and push it away from the region of the highest density (which is precisely the quantity measured by this metric).\nA4.3. Additional results for the defenses\nRobustness of ensembles and noise augmentation In addition to adversarial training and TRADES, we investigated two defense methods that were not originally developed as defenses against adversarial attacks: Ensembles and Noise Augmentation.\nEnsembles do not use a single density estimator but K different ones. Assuming we trained all K density estimators to estimate the posterior distribution on different initialization, thus falling into different local minima. Then an Ensemble Posterior is typically defined as\nq(\u03b8|x) = K\u2211 k=1 1 K q\u03d5k(\u03b8|x)\nWe built an ensemble of 10 masked autoregressive flows and evaluated its robustness to adversarial attacks on the benchmark tasks (Figure A7). The ensemble has a similar robustness as standard NPE.\nAnother defense, called \u2018Noise Augmentation\u2019, adds random perturbations to the data during training (in contrast to adversarial training, which uses adversarial perturbations). We use random noise uniformly distributed on the \u21132 ball with \u03f5 = 1.0. Again, this defense only slightly (if at all) improved the robustness of NPE.\nOverall, these results show that these defenses are not suitable to make amortized Bayesian inference robust to adversarial\n0.1 0.2 0.3 0.5 1.0 2.0 Tolerance \u03b5\n10 \u22125\n10 \u22122\nM M\nD (q\n||q )\u0303)\n2\nLinear gaussian\n0.1 0.2 0.3 0.5 1.0 2.0 Tolerance \u03b5\n10 \u22125\n10 \u22122\nLotka volterra\n0.1 0.2 0.3 0.5 1.0 2.0 Tolerance \u03b5\n10 \u22125\n10 \u22122\nVAE\n0.1 0.2 0.3 0.5 1.0 2.0 Tolerance \u03b5\n10 \u22125\n10 \u22122\nHodgkin Huxley\n0.1 0.2 0.3 0.5 1.0 2.0 Tolerance \u03b5\n10 \u22125\n10 \u22122\nSIR\n0.1 0.2 0.3 0.5 1.0 2.0 Tolerance \u03b5\n10 \u22125\n10 \u22122\nSpatial SIR\nL2PGD L2Noise\n0.0 0.5 1.0 Expected coverage\n0.0\n0.5\n1.0\nL2 PG\nD\n0.0 0.5 1.0 Expected coverage\n0.0\n0.5\n1.0\n0.0 0.5 1.0 Expected coverage\n0.0\n0.5\n1.0\n0.0 0.5 1.0 Expected coverage\n0.0\n0.5\n1.0\n0.0 0.5 1.0 Expected coverage\n0.0\n0.5\n1.0\n0.0 0.5 1.0 Expected coverage\n0.0\n0.5\n1.0\nTolerance \u03b5 0 0.1 0.2 0.3 0.5 1.0 2.0\n0.0 0.5 1.0 Expected coverage\n0.0\n0.5\n1.0\nL2 PG\nD\n0.0 0.5 1.0 Expected coverage\n0.0\n0.5\n1.0\n0.0 0.5 1.0 Expected coverage\n0.0\n0.5\n1.0\n0.0 0.5 1.0 Expected coverage\n0.0\n0.5\n1.0\n0.0 0.5 1.0 Expected coverage\n0.0\n0.5\n1.0\n0.0 0.5 1.0 Expected coverage\n0.0\n0.5\n1.0\nTolerance \u03b5 0 0.1 0.2 0.3 0.5 1.0 2.0\n0.1 0.2 0.3 0.5 1.0 2.0 Tolerance \u03b5\n\u221210 1\n\u221210 0\n10 0\n10 1\n10 2\nN LL\nLinear gaussian\n0.1 0.2 0.3 0.5 1.0 2.0 Tolerance \u03b5\n\u221210 1\n\u221210 0\n10 0\n10 1\nLotka volterra\n0.1 0.2 0.3 0.5 1.0 2.0 Tolerance \u03b5\n\u221210 1\n\u221210 0\n10 0\n10 1\n10 2\nVAE\n0.1 0.2 0.3 0.5 1.0 2.0 Tolerance \u03b5\n10 0\n10 2\n10 4\n10 6\nHodgkin Huxley\n0.1 0.2 0.3 0.5 1.0 2.0 Tolerance \u03b5\n\u221210 2\n\u221210 0\n10 1\n10 3\nSIR\n0.1 0.2 0.3 0.5 1.0 2.0 Tolerance \u03b5\n\u221210 1\n\u221210 0\n10 0\n10 1\nSpatial SIR\nL2PGD L2Noise\nA\nB\nFigure A5. Adversarial attack using different divergence. Adversarial attacks using on each task using the MMD (A) and minimizing the true parameters log likelihood (B). On top, we show robustness DKL(q||q\u0303) at the bottom; we show the expected coverage. The performance on well-specified data is shown in black; the colors indicate performance on adversarially perturbed data at certain tolerance levels.\n10 0\n10 2\nD ia\ng. G\nau ss\nia n\nLinear Gaussian\n10 \u22121\n10 1\nLotka Volterra\n10 \u22121\n10 2\nVAE\n10 \u22125\n10 1\nHodgkin Huxley\n10 \u22122\n10 0\nSIR\n10 \u22127\n10 \u22123\nSpatial SIR\n10 0\n10 2\nM ul\ntv . G\nau ss\nia n\n10 \u22122\n10 0\n10 \u22121\n10 2\n10 \u22125\n10 1\n10 \u22121\n10 2\n10 \u22127\n10 \u22123\n10 0\n10 2\nM .o\n.G .\n10 \u22126\n10 \u22121\n10 \u22126\n10 \u22121\n10 \u22125\n10 1\n10 \u22126\n10 \u22121\n10 \u22127\n10 \u22123\n10 \u22126\n10 \u22121\nM AF\n10 \u22126\n10 \u22121\n10 \u22126\n10 \u22121\n10 \u22125\n10 1\n10 \u22126\n10 \u22121\n10 \u22127\n10 \u22123\n0.1 0.2 0.3 0.5 1.0 2.0 Tolerance \u03b5\n10 \u22126\n10 \u22121\nN SF\n0.1 0.2 0.3 0.5 1.0 2.0 Tolerance \u03b5\n10 \u22126\n10 \u22121\n0.1 0.2 0.3 0.5 1.0 2.0 Tolerance \u03b5\n10 \u22126\n10 \u22121\n0.1 0.2 0.3 0.5 1.0 2.0 Tolerance \u03b5\n10 \u22126\n10 \u22121\n0.1 0.2 0.3 0.5 1.0 2.0 Tolerance \u03b5\n10 \u22126\n10 \u22121\n0.1 0.2 0.3 0.5 1.0 2.0 Tolerance \u03b5\n10 \u22127\n10 \u22123\nHyperparameter \u03b2 0.0001 0.001 0.01 0.1\nFigure A6. Robustness of FIM. Shows DKL(q||q\u0303) for different density estimators (rows) and different values of regularization strength \u03b2 (colors).\nattacks.\nVisualizing adversarial attacks with defenses. To visualize the effect on the inference of adversarial examples, we reproduced Figure 3 but with the robust inference models (with \u03f5 = 1.0). FIM regularization is an effective defense while maintaining good accuracy on clean data, as evidenced by reasonable predictive distributions (Fig. A9). Notably, both adversarial training and TRADES do not work as well (Fig. A10, Fig. A11).\nPlease note that in these figures, we do not necessarily utilize the identical observation or posterior. Instead, we present visual representations of examples that share the same \u201drank\u201d. By sorting all adversarial perturbations based on their adversarial objective, we display the outcomes that correspond to the same index as selected for Figure 3. Pyloric network examples were additionally constrained to be biophysically realistic.\n0.1 0.2 0.3 0.5 1.0 2.0 Tolerance \u03b5\n10 0\n10 2\nD (q\n||q )\u0303) KL\nLinear Gaussian\n0.1 0.2 0.3 0.5 1.0 2.0 Tolerance \u03b5\nTolerance \u03b5\nA\nB\n10 \u22124\n10 0\nLotka Volterra\n0.1 0.2 0.3 0.5 1.0 2.0 Tolerance \u03b5\n10 \u22124\n10 0\nVAE\n0.1 0.2 0.3 0.5 1.0 2.0 Tolerance \u03b5\n10 \u22123\n10 2\nHodgkin Huxley\n0.1 0.2 0.3 0.5 1.0 2.0 Tolerance \u03b5\n10 \u22123\n10 2\nSIR\n0.1 0.2 0.3 0.5 1.0 2.0 Tolerance \u03b5\n10 \u22124\n10 0\nSpatial SIR\nFIM regularizer Adv. training Trades Noise Augmentation Ensemble [MAF]\n0.0\n0.5\n1.0\nEn se\nm bl\ne [M\nA F]\n0.0\n0.5\n1.0\n0.0\n0.5\n1.0\n0.0\n0.5\n1.0\n0.0\n0.5\n1.0\n0.0\n0.5\n1.0\n0.0\n0.5\n1.0\nN oi\nse A\nug m\nen ta\ntio n\n0.0\n0.5\n1.0\n0.0\n0.5\n1.0\n0.0\n0.5\n1.0\n0.0\n0.5\n1.0\n0.0\n0.5\n1.0\n0.0\n0.5\n1.0\nA dv\n. tr\nai ni\nng\n0.0\n0.5\n1.0\n0.0\n0.5\n1.0\n0.0\n0.5\n1.0\n0.0\n0.5\n1.0\n0.0\n0.5\n1.0\n0.0\n0.5\n1.0\nTr ad\nes\n0.0\n0.5\n1.0\n0.0\n0.5\n1.0\n0.0\n0.5\n1.0\n0.0\n0.5\n1.0\n0.0\n0.5\n1.0\n0.0 0.5 1.0 Expected coverage\n0.0\n0.5\n1.0\nFI M\nr eg\nul ar\niz er\n0.0 0.5 1.0 Expected coverage\n0.0\n0.5\n1.0\n0.0 0.5 1.0 Expected coverage\n0.0\n0.5\n1.0\n0.0 0.5 1.0 Expected coverage\n0.0\n0.5\n1.0\n0.0 0.5 1.0 Expected coverage\n0.0\n0.5\n1.0\n0.0 0.5 1.0 Expected coverage\n0.0\n0.5\n1.0\n0 0.1 0.2 0.3 0.5 1.0 2.0\nFigure A7. Results for other defenses. Robustness DKL(q||q\u0303) for all defenses with selected hyperparameters (A). Expected coverage for all defenses with selected hyperparameters (B). The performance on well-specified data is shown in black; the colors indicate performance on adversarially perturbed data at certain tolerance levels.\n0.0\n0.5\n1.0\nD ia\ngo na\nl G au\nss ia\nn\nLinear Gaussian\n0.0\n0.5\n1.0 Lotka Volterra\n0.0\n0.5\n1.0 VAE\n0.0\n0.5\n1.0 Hodgkin Huxley\n0.0\n0.5\n1.0 SIR\n0.0\n0.5\n1.0 Spatial SIR\n0.0\n0.5\n1.0\nM ul\ntiv ar\nia te\nG au\nss ia\nn\n0.0\n0.5\n1.0\n0.0\n0.5\n1.0\n0.0\n0.5\n1.0\n0.0\n0.5\n1.0\n0.0\n0.5\n1.0\n0.0\n0.5\n1.0\nM ix\ntu re\no f G\nau ss\nia n'\ns\n0.0\n0.5\n1.0\n0.0\n0.5\n1.0\n0.0\n0.5\n1.0\n0.0\n0.5\n1.0\n0.0\n0.5\n1.0\n0.0\n0.5\n1.0\nM AF\n0.0\n0.5\n1.0\n0.0\n0.5\n1.0\n0.0\n0.5\n1.0\n0.0\n0.5\n1.0\n0.0\n0.5\n1.0\n0.0 0.5 1.0 Expected coverage\n0.0\n0.5\n1.0\nN SF\n0.0 0.5 1.0 Expected coverage\n0.0\n0.5\n1.0\n0.0 0.5 1.0 Expected coverage\n0.0\n0.5\n1.0\n0.0 0.5 1.0 Expected coverage\n0.0\n0.5\n1.0\n0.0 0.5 1.0 Expected coverage\n0.0\n0.5\n1.0\n0.0 0.5 1.0 Expected coverage\n0.0\n0.5\n1.0\n0 0.1 0.2 0.3 0.5 1.0 2.0\nTolerance \u03b5\nFigure A8. Expected coverage for FIM regularization, Each row shows the expected coverage for a specified density estimator regularized with trace of FIM, at selected hyperparameters. The performance on well-specified data is shown in black; the colors indicate performance on adversarially perturbed data at certain tolerance levels.\n34 100 \u03b81\n0.3 4.1 \u03b82\n\u22120.2 0.6 \u03b83\n0.8 1.3 \u03b84\n-80 -72 \u03b85\n-10 23 \u03b86\n-120 -90 \u03b87\n0 10 20 30 40 50 60 Time [ms]\n\u221260\n\u221240\n\u221220\n0\nVo lta\nge [m\nV]\nHodgkin Huxley\n0.2 1.7 \u03b81\n\u22123.1 \u22120.8 \u03b82\n\u22121.5 0.1 \u03b83\n\u22121.4 0.4 \u03b84\n0 10 20 Time\n1\n2 3 Po pu la tio n de ns\nity\nPrey\n0 10 20 Time\nPredator Lotka Volterra\n\u22121.0 \u22120.5 \u03b81\n\u22122.50\n\u22122.25\n\u22122.00\n\u22121.75\n\u03b8 2\n0 20 40 Time\n0\n1\n2\nPo pu\nla tio\nn de\nns ity\nInfections SIR\n\u22120.8 \u22120.1 \u03b81\n0.1 0.6 \u03b82\n0.8 1.7 \u03b83\nxo x p(x|x )\u223c o\nxo\u0303 x p(x|x \u0303 )\u223c o\nVAE\n1 2 \u03b81\n0\n1\n2\n\u03b8 2\nxo x p(x|x )\u223c o\nxo\u0303 x p(x|x \u0303 )\u223c o\nSpatial SIR\nA B C\nD E\nFigure A9. Adversarial examples for each benchmark task when employing FIM regularization. Each panel shows i) the original observation (blue line) and corresponding posterior predictive samples (blue shaded) ii) the adversarial example (orange line) and posterior predictive samples based on the perturbed posterior estimate, and iii) posterior distribution plots with the posterior estimate for the original (blue) and perturbed (orange) data, and the ground-truth parameters (black dot).\n34 100 \u03b81\n1.6 5.2 \u03b82\n0.0 0.6 \u03b83\n0.8 1.3 \u03b84\n-80 -72 \u03b85\n-10 23 \u03b86\n-120 -90 \u03b87\n0 10 20 30 40 50 60 Time [ms]\n\u221275\n\u221250\n\u221225\n0\n25\nVo lta\nge [m\nV]\nHodgkin Huxley\n\u22121.1 1.2 \u03b81\n\u22121.3 1.0 \u03b82\n\u22121.6 1.1 \u03b83\n\u22121.3 1.3 \u03b84\n0 10 20 Time\n0.8\n1.0\n1.2\nPo pu\nla tio\nn de\nns ity\nPrey\n0 10 20 Time\nPredator Lotka Volterra\n\u22125 0 5 \u03b81\n\u22122\n0\n2\n4\n\u03b8 2\n0 20 40 Time\n0.0\n0.2\n0.4\nPo pu\nla tio\nn de\nns ity\nInfections SIR\n1.3 2.8 \u03b81\n\u22122.8 0.7 \u03b82\n\u22122.6 \u22121.3 \u03b83\nxo x p(x|x )\u223c o\nxo\u0303 x p(x|x \u0303 )\u223c o\nVAE\n1 2 \u03b81\n0.4\n0.6\n0.8\n\u03b8 2\nxo x p(x|x )\u223c o\nxo\u0303 x p(x|x \u0303 )\u223c o\nSpatial SIR\nA B C\nD E\nFigure A10. Adversarial examples for each benchmark task when using adversarial training. Each panel shows i) the original observation (blue line) and corresponding posterior predictive samples (blue shaded) ii) the adversarial example (orange line) and posterior predictive samples based on the perturbed posterior estimate, and iii) posterior distribution plots with the posterior estimate for the original (blue) and perturbed (orange) data, and the ground-truth parameters (black dot)."
        },
        {
            "heading": "A5. Optimal attacks for a linear Gaussian simulator",
            "text": "The only task in the benchmark with tractable ground-truth posterior for arbitrary x is the Gaussian linear task. Here we analyze this task in more detail. We will show that an attack that maximizes the DKL between clean and perturbed posterior corresponds to the strongest eigenvector of the FIM. We then will compare the analytic robustness of the ground truth posterior to the empirical attack on NPE.\nAssume the generative model is given by\np(\u03b8) = N (\u03b8;\u00b50,\u03a30) and p(x|\u03b8) = N (x;A\u03b8 + b,\u039b).\nThen the posterior distribution is well-known and given by\np(\u03b8|xo) = N (\u03b8;\u00b5p,\u03a3p)\n\u00b5p(xo) = \u00b50 +\u03a30A T (A\u03a30A T +\u039b)\u22121(xo \u2212 (A\u00b50 + b)) = (\u03a3\u221210 +AT\u039b\u22121A)\u22121(AT\u039b\u22121(xo \u2212 b) +\u03a3 \u22121 0 \u00b50)\n\u03a3p = \u03a30 \u2212\u03a30AT (A\u03a30AT +\u039b)\u22121A\u03a30 = (\u03a3\u221210 +AT\u039b\u22121A)\u22121\nAs we will show below, for this simulator, the perturbation which maximizes the DKL between clean and perturbed posterior exactly corresponds to the eigenvector of the Fisher-information matrix with the largest eigenvalue.\nAnalytical expression for the Kullback-Leibler divergence In this model, the Kullback-Leibler divergence DKL(p(\u03b8|xo)||p(\u03b8|xo + \u03b4)) between clean and perturbed posterior can be computed analytically.\nWe defined an adversarial example as a distorted observation given by x\u0303o = xo+\u03b4. This will only affect the posterior mean, as the covariance matrix is independent of xo. The KL divergence between clean and perturbed posterior can be written as\nDKL(p(\u03b8|xo)||p(\u03b8|xo + \u03b4)) = 0.5 \u00b7 ( tr(\u03a3\u22121p \u03a3p)\u2212 d+ log ( |\u03a3p| |\u03a3p| )) + 0.5(\u00b5p(xo)\u2212 \u00b5p(xo + \u03b4))T\u03a3\u22121p (\u00b5p(xo)\u2212 \u00b5p(xo + \u03b4))\n= 0.5 ( (\u00b5p(xo)\u2212 \u00b5p(xo + \u03b4))T\u03a3\u22121p (\u00b5p(xo)\u2212 \u00b5p(xo + \u03b4)) )\nand the difference between means can be written as\n(\u00b5p(xo)\u2212 \u00b5p(xo + \u03b4)) = (\u03a3\u221210 +AT\u039b\u22121A)\u22121AT\u039b\u22121\u03b4 = \u03a3pAT\u039b\u22121\u03b4.\nHence, we obtain\nDKL(p(\u03b8|xo)||p(\u03b8|xo + \u03b4)) = 0.5 \u00b7 \u03b4T\u039b\u22121A\u03a3p\u03a3\u22121p \u03a3pAT\u039b\u22121\u03b4 = 0.5 \u00b7 \u03b4T\u039b\u22121A\u03a3pAT\u039b\u22121\u03b4.\nAnalytical expression for the FIM Next, we derive a closed-form expression for the Fisher Information Matrix (FIM):\nIx = Ep(\u03b8|x) [ \u2207x log p(\u03b8|x)(\u2207x log p(\u03b8|x))T ] We can write:\n\u2207x log p(\u03b8|x) = \u22120.5\u2207x(\u00b5p(x)\u2212 \u03b8)T\u03a3\u22121p (\u00b5p(x)\u2212 \u03b8) = \u2212(\u2207x\u00b5p(x))T\u03a3\u22121p (\u00b5p(x)\u2212 \u03b8) = (\u03a3pA\nT\u039b\u22121)T\u03a3\u22121p (\u03b8 \u2212 \u00b5p(x)) = \u039b1\u2212A(\u03b8 \u2212 \u00b5p(x)) = \u039b\u22121A(\u03b8 \u2212 \u00b5p(x))\nHence the Fisher information matrix with respect to x is given by\nIx = Ep(\u03b8|x) [ \u2207x log p(\u03b8|x)(\u2207x log p(\u03b8|x))T ] = Ep(\u03b8|x)[\u039b\u22121A(\u03b8 \u2212 \u00b5p(x))(\u03b8 \u2212 \u00b5p(x))TAT\u039b\u22121] = \u039b\u22121AEp(\u03b8|x)[(\u03b8 \u2212 \u00b5p(x))(\u03b8 \u2212 \u00b5p(x))T ]AT\u039b\u22121\n= \u039b\u22121A\u03a3pA T\u039b\u22121\nand thus equivalently DKL(p(\u03b8|xo)||p(\u03b8|xo + \u03b4)) = 0.5 \u00b7 \u03b4TIx\u03b4\nThis demonstrates that the Kullback-Leibler divergence between clean and perturbed posterior directly corresponds to the Fisher information matrix in the linear Gaussian simulator.\nOptimal attack on the linear Gaussian simulator When maximizing the DKL(p(\u03b8|xo)||p(\u03b8|xo + \u03b4)), the adversary, thus, tries to solve the following problem\n\u03b4\u2217 = max \u03b4:||\u03b4||2\u2264\u03f5 0.5\u03b4TIx\u03b4.\nBy Reyleight\u2019s theorem, this is solved by \u03b4\u2217 = \u03f5vmax where vmax is the eigenvector with the largest eigenvalue of Ix and thus\nDKL(p(\u03b8|xo)||p(\u03b8|xo + \u03b4)) \u2264 0.5\u03bbmax\u03f52.\nHere \u03bbmax is the maximum eigenvalue of Ix. Thus any empirical attack on the ground truth posterior would be bounded by this quantity.\nAn attack on an inference model that successfully identifies the map to the ground truth posterior should be consistent with this result. Figure A12 illustrates the ground-truth robustness to \u21132 perturbations (black) and the empirical robustness obtained through attacking NPE inference models attempting to solve this task. As the number of simulations increases, the NPE model can more accurately capture the true posterior mapping, resulting in attacks that are bounded by this quantity. This is evident in the figure, where attacks on most models are close to this bound. However, if the model is trained on too few simulations, the results may differ greatly as an incorrect mapping to the posterior is learned, making it more susceptible to adversarial attacks. Interestingly, given insufficient training data, all density estimators tend to be more, and not less, brittle to adversarial perturbations. The attacks are weaker on Mixture models and neural spline flows, which could result from either the attack not being strong enough or the model being too smooth."
        },
        {
            "heading": "A6. Analytical expression for the FIM regularization in generalized linear models",
            "text": "We analyze the solution on a generalized linear Gaussian density estimator to investigate the bias introduced by FIM regularization. We derive the analytical solutions for the optimal parameters identified by NPE and NPE with FIM regularization and discuss their differences.\nConsider a generalized linear Gaussian inference network\nqW ,\u03a3(\u03b8|x) = N (\u03b8;W\u03d5(x),\u03a3)\nhere \u03d5 is a, possibly nonlinear, feature mapping \u03d5 : Rdx \u2192 Rd\u03d5 . The only learnable parameter is the weight matrix W \u2208 Rd\u03b8\u00d7d\u03d5 and covariance matrix \u03a3 \u2208 Rd\u03b8\u00d7d\u03b8 .\nIn this case, the NPE loss using N simulations (xi,\u03b8i), can be written as\nL(X,\u0398,W ,\u03a3) = 1 2 tr\n( (W\u03d5(X)\u2212\u0398))T \u03a3\u22121 (W\u03d5(X)\u2212\u0398) ) + N\n2 log det(\u03a3)\nhere X \u2208 Rdx\u00d7N denotes all data points represented as columns of a matrix (equivalently \u0398).\nBelow, we compute analytical expressions for the optimal parameters W ,\u03a3 for (1) NPE and (2) for NPE with FIMregularization. This allows us to quantify the bias introduced by FIM-regularization in a Gaussian GLM.\nConvergence of NPE For NPE (without regularization), we can compute the optimal parameters in closed-form:\n\u2207WL = \u03a3\u22121(W\u03d5(X)T \u2212\u0398)\u03d5(X)T ! = 0 \u21d0\u21d2 W\u0302 = \u0398\u03d5(X)T ( \u03d5(X)\u03d5(X)T )\u22121 Which is a generalized linear least square regression estimator. Equivalently we can obtain an estimator for the covariance matrix:\n\u2207\u03a3L =\u2212 1 2 \u03a3\u22121(W\u03d5(X)\u2212\u0398)(W\u03d5(X)\u2212\u0398)T\u03a3\u22121 + N 2 \u03a3\u22121 ! = 0\n\u21d0\u21d2 N\u03a3\u22121 = \u03a3\u22121(W\u03d5(X)\u2212\u0398)(W\u03d5(X)\u2212\u0398)T\u03a3\u22121\n\u21d0\u21d2 \u03a3\u0302 = 1 N (W\u03d5(X)\u2212\u0398)(W\u03d5(X)\u2212\u0398)T .\nAs W\u0302 is estimated independently of \u03a3, we can plug in W = W\u0302 to globally minimize the loss.\nConvergence of NPE with FIM regularization In this model, we can also compute the FIM regularized solution in closed-form. The FIM is given by\nIx = J\u03d5(x)TW T\u03a3\u22121WJ\u03d5(x). Here J\u03d5(x) denotes the Jacobian matrix of \u03d5 at x. Hence the FIM regularized model minimizes the loss\nLFIM (X,\u0398,W ,\u03a3, \u03b2) = L(X,\u0398,W ,\u03a3) + \u03b2\nN N\u2211 i=1 tr(Ixi).\nTo avoid clutter in notation, let \u2126(X) = 1N \u2211n i=1 J\u03d5(xi)J\u03d5(xi) T . Then we can write a solution that minimizes this loss as\n\u2207WLFIM =\u2207WL+ 2\u03b2\u03a3\u22121W\u2126(X) ! = 0 \u21d0\u21d2 W\u0302FIM = \u0398\u03d5(X)T ( \u03d5(X)\u03d5(X)T + 2\u03b2\u2126(X) )\u22121 and\n\u2207\u03a3LFIM =\u2207\u03a3L \u2212 \u03b2\u03a3\u22121W\u2126(X)W T\u03a3\u22121 ! = 0\n\u21d0\u21d2 \u03a3\u0302FIM = \u03a3\u0302+ 4\u03b2\nN W\u2126(X)W T .\nAgain we can plug in W\u0302FIM to globally minimize the loss.\nBias introduced by FIM-regularization The solution for W corresponds to a Tikhonov regularized least squares solution, which simplifies to ridge regression in the linear case (Golub et al., 1999). Recent research has shown a connection between adversarial training and ridge regression in the linear-least squares setting. (Ribeiro et al., 2022). The regularization approach based on Fisher Information Matrix (FIM) incorporates a bias term through the average Jacobian outer product, which is known to recover those directions that are most relevant to predicting the output (Trivedi & Wang, 2020). Thus, the regularization strength is directed towards the directions to which the feature mapping \u03d5 is most sensitive. This bias increases monotonically with the regularization parameter, leading to an asymptotically to smooth mean function.\nAdditionally, FIM regularization overestimates the covariance matrix for a finite number of data points. Interestingly, as N \u2192 \u221e the additive bias on the covariance matrix vanishes. This is in agreement with our empirical results of \u2018conservative\u2019 posterior approximations."
        },
        {
            "heading": "A7. FIM approximations",
            "text": "We propose an efficient approximation method to scale FIM-based approaches to complex density estimators. Here we investigate the effect of the approximation compared to an exact method. In order to compare our method to a closed-from FIM, we use a simple Gaussian density estimator of the form\nq\u03d5(\u03b8|x) = N (\u03b8;\u00b5\u03d5(x), diag(\u03c32\u03d5(x))).\nHere \u00b5 : Rdx \u2192 Rdx is the mean function parameterized as neural network and \u03c3 : Rdx \u2192 Rdx the standard deviation, which is transformed to a diagonal covariance matrix. Let us denote the distributional parameters as\n\u03b7 =\n( \u00b5\u03d5(x)\n\u03c3\u03d5(x) ) and let J be the Jacobian matrix defined by Jij = \u2202\u03b7i\u2202xi . Then we can write Ix = J\nTI\u03b7J . Notice that I\u03b7 is a Gaussian FIM with respect to the parameter \u00b5 and \u03c3 which is given by\nI\u03b7 = \u2212Eq\u03b7 [ \u22072\u03b7 log q\u03b7(\u03b8) ] = ( diag( 1\u03c32 ) 0\n0 diag( 2\u03c32 )\n) .\nThe Jacobian matrix can be computed via autograd and thus Ix can be computed for any given x.\nTo test how well our approximations work, we tested the following regularizers:\n\u2022 FIM Largest Eigenvalue: Using the regularize \u2126(x) = \u03b2 \u00b7 \u03bbmax(Ix) with the exact FIM.\n\u2022 FIM Trace: This uses \u2126(x) = \u03b2 \u00b7 tr(Ix) also with the exact FIM.\n\u2022 FIM Trace [EMA]: This uses \u2126(x) = \u03b2 \u00b7 tr(I\u0302x) estimated as described in the main paper.\nThe results on the VAE task are shown in Figure A13 using three different choices \u03b2 = 0.001, 0.01, 0.1. Notably, for small regularization strengths, all techniques work similarly. Both trace-based regularizers decrease DKL(q||q\u0303) more strongly, as expected by the fact that they are upper bounds of the largest eigenvalue. Notably, train time increased to five hours using the exact eigenvalue, compared to under two minutes using our approach.\nOverall, these results demonstrate that our approximations to the largest eigenvalue of the FIM incur only a small cost in adversarial robustness."
        },
        {
            "heading": "A8. Comparission to MCMC based posteriors",
            "text": "We emphasize metrics that can be efficiently computed without requiring direct access to the true posterior. This choice is justified because numerous tasks lack a tractable or \u201dexpensive to evaluate\u201d likelihood. Consequently, calculating the posterior individually for thousands of observations would be computationally expensive. However, it is worth noting that we have a likelihood for some of the benchmark tasks and can perform non-amortized posterior computation with a manageable computational burden.\nFor the Gaussian Linear task, we have the advantage of an analytic solution. This simplifies the computation of the posterior, as we can directly derive the necessary quantities without relying on approximation methods or sampling techniques. In the case of the SIR, VAE, and Lotka Volterra tasks, we have tractable likelihood functions. This enables us to estimate the posterior distribution using likelihood-based inference methods, such as MCMC. We use a two-stage procedure to get good posterior approximations. First, we run one hundred parallel MCMC chains initialized from the prior. For the SIR and Lotka Volterra, we used an adaptive Gaussian Metropolis Hasting MCMC method (Andrieu & Thoms, 2008). For the VAE task, we used a Slice sampler (Neal, 2003). In the second stage, these results were used to train an unconditional flow-based density estimator, which then performs Independent Metropolis Hasting MCMC to obtain the final samples (Holden et al., 2009).\nWe present an illustration of several approximated (adversarial) posteriors alongside their corresponding ground truth obtained through Markov Chain Monte Carlo (MCMC) methods (Figure A14). As discussed in the main paper, even minor perturbations can lead to significant misspecification of the SIR model. We observe substantial changes in the true posterior distribution due to adversarial perturbations (A14A). Notably, the standard NPE method appears to be highly susceptible to this scenario, often producing predictions that seem arbitrary. In contrast, the FIM regularized inference networks exhibit a similar trend as the true posterior; however, they tend to be underconfident in their predictions as expected.\nFigure A14B illustrates the behavior of the variational autoencoder (VAE) task in the presence of adversarial perturbations. Interestingly, the ground-truth posterior distribution remains remarkably unaffected by these perturbations, exhibiting a high degree of invariance. Yet, the inference network, responsible for approximating the posterior, proves to be susceptible to being deceived by these adversarial perturbations. This effect is strongly reduced for FIM regularization. We observe this similarly on the Lotka Volterra task.\nWe quantified this difference on a randomly selected set of 100 pairs (x, x\u0303) by computing MMD2(q, p) = MMD2(q\u03d5(\u03b8|x), pMCMC(\u03b8|x)) and MMD2(q, p) = MMD2(q\u03d5(\u03b8|x\u0303), pMCMC(\u03b8|x\u0303)) (Figure A15). Consistent with expectations, the MMD value for well-specified data demonstrates a relatively good agreement between the true posterior and the inference network\u2019s approximation across different tasks. However, when confronted with adversarially perturbed data, the MMD value increases significantly. This indicates that the inference network struggles to accurately capture the underlying posterior distribution in the presence of adversarial perturbations. These findings highlight the limitations of the inference network and its susceptibility to such adversarial perturbations. FIM regularization can mitigate this effect, effectively reducing the impact of adversarial perturbations on the inference network\u2019s performance. However, this improvement comes at the expense of decreased approximation quality on well-specified data.\nStrong adversarial perturbation Weak adversarial perturbationA"
        }
    ],
    "title": "Adversarial robustness of amortized Bayesian inference",
    "year": 2023
}