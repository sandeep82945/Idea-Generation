{
    "abstractText": "Numerical simulations within a cold dark matter (DM) cosmology form haloes whose density profiles have a steep inner slope (\u2018cusp\u2019), yet observations of galaxies often point towards a flat central \u2018core\u2019. We develop a convolutional mixture density neural network model to derive a probability density function (PDF) of the inner density slopes of DM haloes. We train the network on simulated dwarf galaxies from the NIHAO and AURIGA projects, which include both DM cusps and cores: line-of-sight velocities and 2D spatial distributions of their stars are used as inputs to obtain a PDF representing the probability of predicting a specific inner slope. The model reco v ers accurately the expected DM profiles: \u223c 82 per cent of the galaxies have a derived inner slope within \u00b10.1 of their true value, while \u223c 98 per cent within \u00b10.3. We apply our model to four Local Group dwarf spheroidal galaxies and find results consistent with those obtained with the Jeans modelling based code GRAVSPHERE : the Fornax dSph has a strong indication of possessing a central DM core, Carina and Sextans have cusps (although the latter with large uncertainties), while Sculptor shows a double peaked PDF indicating that a cusp is preferred, but a core cannot be ruled out. Our results show that simulation-based inference with neural networks provide a innov ati ve and complementary method for the determination of the inner matter density profiles in galaxies, which in turn can help constrain the properties of the elusive DM. Key w ords: galaxies: dw arf \u2013 galaxies: evolution \u2013 galaxies: formation \u2013 galaxies: haloes \u2013 (cosmology:) dark matter.",
    "authors": [
        {
            "affiliations": [],
            "name": "J. Exp \u0301osito-M \u0301arquez"
        },
        {
            "affiliations": [],
            "name": "C. B. Brook"
        },
        {
            "affiliations": [],
            "name": "M. Huertas-Company"
        },
        {
            "affiliations": [],
            "name": "A. V. Macci \u0300o"
        },
        {
            "affiliations": [],
            "name": "G. Battaglia"
        }
    ],
    "id": "SP:c1f35482fb1c1fbdfcd68abe109a568796caae1e",
    "references": [],
    "sections": [
        {
            "text": "Exp\u00f3sito-M\u00e1rquez, J, Brook, CB, Huertas-Company, M, Di Cintio, A, Macci\u00f2, AV, Grand, RJJ, Battaglia, G and Arjona-G\u00e1lvez, E\nA probabilistic deep learning model to distinguish cusps and cores in dwarf galaxies\nhttp://researchonline.ljmu.ac.uk/id/eprint/19370/\nArticle\nLJMU has developed LJMU Research Online for users to access the research output of the University more effectively. Copyright \u00a9 and Moral Rights for the papers on this site are retained by the individual authors and/or other copyright owners. Users may download and/or print one copy of any article(s) in LJMU Research Online to facilitate their private study or for non-commercial research. You may not engage in further distribution of the material or use it for any profit-making activities or any commercial gain.\nThe version presented here may differ from the published version or from the version of the record. Please see the repository URL above for details on accessing the published version and note that access may require a subscription.\nFor more information please contact researchonline@ljmu.ac.uk\nhttp://researchonline.ljmu.ac.uk/\nCitation (please note it is advisable to refer to the publisher\u2019s version if you intend to cite from this work)\nExp\u00f3sito-M\u00e1rquez, J, Brook, CB, Huertas-Company, M, Di Cintio, A, Macci\u00f2, AV, Grand, RJJ, Battaglia, G and Arjona-G\u00e1lvez, E (2022) A probabilistic deep learning model to distinguish cusps and cores in dwarf galaxies. Monthly Notices of the Royal Astronomical Society, 519 (3). pp. 4384-4396.\nLJMU Research Online\nMNRAS 519, 4384\u20134396 (2023) https://doi.org/10.1093/mnras/stac3799 Advance Access publication 2022 December 28\nA probabilistic deep learning model to distinguish cusps and cores in dwarf galaxies\nJ. Exp \u0301osito-M \u0301arquez, 1 , 2 \u2039 C. B. Brook, 1 , 2 M. Huertas-Company, 1 , 2 , 3 A. Di Cintio , 1 , 2 A. V. Macci \u0300o, 4 , 5 , 6 R. J. J. Grand , 1 , 2 G. Battaglia 1 , 2 and E. Arjona-G \u0301alvez 1 , 2 1 Universidad de La Laguna. Avda. Astrof \u0301\u0131sico Fco. S \u0301anchez, La Laguna, Tenerife 38206, Spain 2 Instituto de Astrof \u0301\u0131sica de Canarias, Calle Via L \u0301actea s/n, E-38206 La Laguna, Tenerife, Spain 3 LERMA, Observatoire de Paris, CNRS, PSL, Universit \u0301e de Paris, 75014, Paris, France 4 New York University Abu Dhabi, PO Box 129188, Abu Dhabi, United Arab Emirates 5 Center for Astro, Particle and Planetary Physics (CAP 3 ), New York University, Abu Dhabi , PO Box 129188, United Arab Emirates 6 Max Planck Institute f \u0308ur Astronomie, K \u0308onigstuhl 17, D-69117 Heidelberg, Germany\nAccepted 2022 December 2. Received 2022 November 24; in original form 2022 September 13\nA B S T R A C T Numerical simulations within a cold dark matter (DM) cosmology form haloes whose density profiles have a steep inner slope (\u2018cusp\u2019), yet observations of galaxies often point towards a flat central \u2018core\u2019. We develop a convolutional mixture density neural network model to derive a probability density function (PDF) of the inner density slopes of DM haloes. We train the network on simulated dwarf galaxies from the NIHAO and AURIGA projects, which include both DM cusps and cores: line-of-sight velocities and 2D spatial distributions of their stars are used as inputs to obtain a PDF representing the probability of predicting a specific inner slope. The model reco v ers accurately the expected DM profiles: \u223c 82 per cent of the galaxies have a derived inner slope within \u00b10.1 of their true value, while \u223c 98 per cent within \u00b10.3. We apply our model to four Local Group dwarf spheroidal galaxies and find results consistent with those obtained with the Jeans modelling based code GRAVSPHERE : the Fornax dSph has a strong indication of possessing a central DM core, Carina and Sextans have cusps (although the latter with large uncertainties), while Sculptor shows a double peaked PDF indicating that a cusp is preferred, but a core cannot be ruled out. Our results show that simulation-based inference with neural networks provide a innov ati ve and complementary method for the determination of the inner matter density profiles in galaxies, which in turn can help constrain the properties of the elusive DM.\nKey w ords: galaxies: dw arf \u2013 galaxies: evolution \u2013 galaxies: formation \u2013 galaxies: haloes \u2013 (cosmology:) dark matter.\n1\nD c d p p o w s a d t B\np K a t (\na o t i s i p 2 t h a 2 p o g i s\ng d\nD ow nloaded from https://academ ic.oup.com /m nras/article/519/3/4384/6964650 by Liverpool John M oores U niversity user on 18 April 2023 I N T RO D U C T I O N ark matter (DM) haloes that form in simulations within a Lambda old dark matter ( CDM) cosmological context have a characteristic ensity profile, which has a logarithmic inner slope of \u22121 (the NFW rofile Navarro, Frenk & White 1996 ). Such a steep inner density rofile has been referred to as a \u2018cusp\u2019. Ne vertheless, observ ations f dwarf galaxies inhabiting these haloes have shown discrepancies ith the predictions of the model, showing significant evidence that\neveral of these galaxies have a flat inner density profile, with slope pproaching zero, referred to as a \u2018cored\u2019 profile (Moore 1994 ). The iscrepancy between theory and observations has been referred to as he \u2018core-cusp\u2019 problem (e.g. Simon et al. 2005 ; de Blok et al. 2008 ; ullock & Boylan-Kolchin 2017 ). While o v er the years sev eral alternativ e DM models hav e been roposed to tackle this issue (e.g. Spergel & Steinhardt 2000 ; aplinghat, Tulin & Yu 2016 ; Schneider et al. 2017 ), it has been lso shown that cores can be explained within CDM considering he effect that baryons have on DM matter. Navarro, Eke & Frenk 1996 ) showed that if gas is slowly accreted on to a dwarf galaxy\nE-mail: expox7@gmail.com\n( M s\nPub\nnd then suddenly remo v ed through processes such as stellar winds r supernovae feedback, the DM distribution can expand, lowering he central density of the halo. This effect of DM heating is small n realistic conditions (Gnedin & Zhao 2002 ), but Read et al. ( 2006 ) howed that if the effect repeats o v er sev eral c ycles of star formation, t accumulates leading to a complete core formation. This core can be ermanent if the outflows are sufficiently rapid (Pontzen & Go v ernato 012 ). Modern hydrodynamical simulations of dwarf galaxies that ake into consideration baryonic feedback and have a sufficiently igh density threshold for star formation have indeed succeeded t creating DM cores (e.g. Go v ernato et al. 2010 ; Di Cintio et al. 014a ; Tollet et al. 2016 ; Chan et al. 2015 ). Still, the \u2018cusp-core\u2019 roblem is far from being completely solved, due to the difficulties f unco v ering the underlying DM distribution in observed dwarf alaxies, and significant effort has gone into the development and mpro v ement of methods to infer the inner DM density profile of uch galaxies.\nAnalysis of the rotation velocity of gas in low surface brightness alaxies, for example, allo w to deri ve and fit their underlying DM istribution suggesting the presence of a DM core in such systems e.g. Moore 1994 ; Gentile et al. 2004 ; de Blok et al. 2008 ; Lelli,\ncGaugh & Schombert 2016 ). On the other side, in pressureupported galaxies that are devoid of gas, such as the dwarf spheroidal\n\u00a9 2022 The Author(s) lished by Oxford University Press on behalf of Royal Astronomical Society\ng i t v t K 2 C e l c P w i p Z o A c i u m a\nd m d t s d R a d g a g s s s\nt S W T\n2\n2\nT w l 2 m\nc m c h 2 l m\nT i t s h t v t 1 m ( t g 2 t d 1 s f i B a i t s i o 1 2\nt D d W A M r r h\na T d t i a r f\no E i c t b b g\n1 This extrapolation is reasonable considering that AURIGA galaxies, although not resolved at r < 370 pc, consistently show a cuspy inner density, i.e. there is no sign of an artificial central DM core.\nD ow nloaded from https://academ ic.oup.com /m nras/article/519/3/4384/6964650 by Liverpool John M oores U niversity user on 18 April 2023\nalaxies (dSphs) found within the Local Group, the kinematic nformation on which dynamical modelling relies on, comes from he line-of-sight velocity distribution of their stellar component. A ariety of methods have been employed on dwarf galaxies to derive heir central DM density, such as Jeans (e.g. van der Marel 1994 ; leyna et al. 2001 ; Battaglia et al. 2008 ; Read, Walker & Steger 019 ; Collins et al. 2021 ) or Schw arzschild (e.g. Schw arzschild 1979 ; appellari et al. 2006 ; van den Bosch & de Zeeuw 2010 ; Breddels t al. 2013 ; Breddels & Helmi 2013 ) modelling. The results in the iterature seem to point to cored DM profiles being fa v oured o v er uspy ones in the Fornax dSph (e.g. Geha et al. 2006 ; Walker & e \u0303 narrubia 2011 ; Brook & Di Cintio 2015 ; Pascale et al. 2018 ), hile in the case of Sculptor, another very well studied system, it s still very much debated if its DM halo is cored or cuspy, perhaps ointing to the presence of a mild cusp (e.g. Breddels & Helmi 2013 ; hu et al. 2016 ; Hayashi, Chiba & Ishiyama 2020 ) (for a re vie w n these topics, see Battaglia et al. 2022 and references therein). central limitation of the previously mentioned models, however, omes from the uncertainty in the anisotropy of the stellar orbits, n the case of Jeans modelling, which causes a de generac y with the nderlying mass profile (Binney & Mamon 1982 ); Schwarzschild odelling, on the other end, is hampered by its sensitivity to the vailable data (Kowalczyk, \u0141okas & Valluri 2017 ). In this work, we present an alternative and innov ati ve method to iscriminate between cusps and cores in dwarf galaxies based on achine learning techniques. Namely, we use convolutional mixture ensity neural networks to determine a posterior distribution of he inner profile of DM haloes. This general approach has been uccessfully implemented for measuring cluster masses from galaxy ynamics (e.g. Ho et al. 2019 ; Kodi Ramanah et al. 2020 ; Kodi amanah, Wojtak & Arendse 2021 ). The neural network uses s inputs the phase-space mappings of positional and dynamical istributions of stars within galaxies. We use a suite of 171 dwarf alaxies from the NIHAO project with different initial conditions nd parameters (Wang et al. 2015 ; Dutton et al. 2020 ) and 12 dwarf alaxies from the AURIGA project (Grand et al. 2017 ) as a training et for the network. We then apply our no v el model to four dwarf pheroidal galaxies satellites of the Milky Way to infer the inner lope of their DM density profiles.\nThe paper is organized as follows. In Section 2 , we present he simulation data set and the machine learning architecture. In ection 3 , we show the results of the trained model on the test set. e then apply the model to observed dwarf galaxies in Section 4 .\nhe conclusions are discussed in Section 5 .\nM E T H O D S\n.1 The training set\no train our model, we need a large set of simulated dwarf galaxies ith well-known density profiles. We use fully cosmological simu-\nations from NIHAO (Wang et al. 2015 ) and AURIGA (Grand et al. 017 ) projects, in which DM and baryonic matter evolve together, aking our training set as realistic as possible. Importantly, we need to include simulations of galaxies with both usps and cores in their central region, and with various stellar asses, in order to minimize any systematic dependence of cusp and ore on properties such as mass. Indeed, the fiducial NIHAO galaxies ave a density profile highly correlated with mass (Di Cintio et al. 014b ; Macci \u0300o et al. 2020 ), which could possibly allow the machine earning code to predict cusp or core based on any indicator of total\nass, rather than by the details of the stellar velocities and positions.\no maximize the neural network\u2019s ability to find and differentiate nput data features directly related to their inner slope, it is prudent o a v oid any non-physical correlation in the data set between the inner lope and other galaxy features. We therefore use simulations that ave a range of different physical and/or parametric inputs, meaning hat our final suite of simulations includes a range of inner slopes at arious masses and sizes. We firstly include dwarf galaxies within he fiducial NIHAO model, ranging in halo mass from \u223c 10 9 M to 0 11.5 M and stellar mass from an order of 10 5 M to 10 9.5 M . This odel includes energy feedback from massive stars and supernovae Stinson et al. 2006 ), which has been shown to be able to modify he inner density profile and result in cores, particularly in simulated alaxies with stellar mass between 10 7 and 10 9 M (Di Cintio et al. 014a ). We also use simulations of dwarfs from Dutton et al. ( 2020 ) hat employs the same model as the fiducial NIHAO ones, but with ifferent star formation thresholds, ranging from \u03c1 thresh = 0.1 to 00 particles per cm \u22123 : this translates into galaxies of a similar tellar mass ending up with different density profiles, as the star ormation density threshold has been shown to be one of the most mportant parameter for core formation in baryonic simulations (see en \u0301\u0131tez-Llambay et al. 2019 ; Dutton et al. 2020 ). We further add set of simulations with no stellar feedback run from the same nitial conditions as fiducial NIHAO (Wang et al. 2015 ). The lower otal feedback energy results in different inner density profiles than imulations in which the stellar feedback is included, for the same nitial conditions, therefore further increasing the desired diversity f central DM profiles at a given galaxy mass. Finally, we include 2 simulated dwarf galaxies from the AURIGA project (Grand et al. 017 ), all of which have a central DM cusp. We have 183 simulated dwarf galaxies in total: 60 simulations from he fiducial NIHAO suite (Wang et al. 2015 ), 101 simulations from utton et al. ( 2020 ) with varying density thresholds and varying ensity profile, 10 simulations without stellar feedback also from ang et al. ( 2015 ) and 12 simulations from Grand et al. ( 2017 ). ll together, these simulations have a range in halo mass between halo = 3 \u00d7 10 9 M and M halo = 4 \u00d7 10 11 M . NIHAO simulations esolve the mass profile of galaxies to below 1 per cent of their virial adius at all masses, while AURIGA simulations are constructed to ave a maximum physical softening of \u223c370 pc. We define the DM inner slope value of the simulated galaxies s the slope at 150 pc of the DM density profile of each galaxy. his value is extrapolated from the fit of the density profile to a ouble-power law profile (Di Cintio et al. 2014b ) in order to a v oid he noise effect of the computed density profile of the simulations n inner regions very close to the softening length. 1 We end up with set of simulated dwarfs exhibiting a range of density profiles: the elationship between stellar mass and inner slope of DM halo for our ull data set can be seen in Fig. 1 .\nTo increase the size of our training set, we use three different utput time-steps for each galaxy: z = 0, z = 0.112, and z = 0.226. ach simulated galaxy is already virialized at these redshifts, and it s therefore possible to take different snapshots of the dwarf. For the osmological parameters from Planck Collaboration XIII ( 2016 ), he time between z = 0 and z = 0.112 is roughly 1.46 Gyr and etween z = 0.112 and z = 0.226 is 1.27 Gyr. These time differences etween snapshots correspond to multiple dynamical times of the alaxies from the data set, typically of the order of 10 \u22122 or 10 \u22121\nMNRAS 519, 4384\u20134396 (2023)\nM\nFigure 1. Relationship between the inner slope of the DM density profiles \u03b3 (defined as the logarithmic slope at 150 pc) and the stellar mass of the simulated galaxies in our data set. The green horizontal line marks the value of \u03b3 for a NWF profile.\nFigure 2. Example of cored and cuspy galaxies from our simulation dataset. Here, each row represents a different galaxy. Left columns: Rendering of the stars in a face-on view. Central columns: Rendering of the stars with an edgeon orientation. Right columns: DM density profiles and fit to a double-power law model (Jaffe 1983 ; Merritt et al. 2006 ).\nG t c c W u\no w w s o a s e\na g m s p l i v ( g u s t s w f l\n2\nT p p e t s n\n2\nL w c\nf\nw\no c a d t a k\nK\nw m \u03ba\nD ow nloaded from https://academ ic.oup.com /m nras/article/519/3/4384/6964650 by Liverpool John M oores U niversity user on 18 April 2023\nyr for stars at distances to the centre under which 90 per cent of he stars of the galaxies are found. While this procedure does not hange greatly the range of the obtained density profiles, it does hange the position and velocities of the stars within each galaxy. e end up with a sample of 549 galaxy snapshots which we will se as training set for our method. We show in Fig. 2 examples\nNRAS 519, 4384\u20134396 (2023)\nf stellar renderings of cored and cuspy simulated galaxies together ith their corresponding DM profiles. We then proceed to select stars ithin each galaxy snapshot. Typically, the number of stars for which pectroscopic data is available for Local Group dwarf galaxies is the rder of hundreds or thousands, while the number of star particles vailable in our simulated galaxies range from a few hundred to everal million, with a mean number of about 10 5 stellar particles in ach galaxy.\nTherefore, in order to simulate an observational sample of stars, nd to further expand our training set, we have divided each simulated alaxy\u2019s complete sample of stars into a minimum of 20 subsets, each ade of randomly selected stars. The number of stars within each ubset of a given galaxy is dependent on the total number of star articles in the simulation, with an upper limit of 10 4 stars and a ower limit of 200 stars. The stars of each subset are then projected n arbitrary sky planes to simulate galaxies observed from different iewing angles. These projected stars are defined by their position x proj , y proj ) and their line-of-sight velocity v LOS . We o v ersample some alaxies by making multiple projections to each of their subset, and ndersample some galaxies, with the objective of making the training et have a uniform distribution of inner slopes: this avoids biases in he model during training. We end up with a total of 10 273 data ets to train our model, each composed of randomly selected stars ithin different simulated galaxies and at different viewing angles, or which we stored information about their positions ( x proj , y proj ) and ine-of-sight velocities v LOS .\n.2 The information inputs\nhe inputs of our deep neural network model are continuous 2D robability density functions (PDFs) of the distribution of stars in rojected phase spaces, constructed with bi v ariate kernel density stimations (KDEs). The mapping generated with KDEs allows us o encapsulate the features of the original discrete distributions in the ame form even if each galaxy subset is represented by a different umber of stars.\n.2.1 Kernel density estimation\net X 1 , X 2 ,..., X n denote a sample of size n from a random variable ith density f , each variable being a two-dimensional vector for the\nase of a bi v ariate KDE. The kernel density estimate of f at the point x is given by\nh ( x ) = 1 n | H | 1 / 2 n \u2211 i= 1 K [ H \u22121 / 2 ( x \u2212 X i ) ] , (1)\nhere K is a kernel function and H is a 2x2 bandwidth matrix. The KDE sums up the density contributions from the collection\nf data points at the e v aluation point x , so that data points close to x ontribute significantly to the total density, while data points further way from x contribute less. The shape of those contributions is etermined by K, and their dimensions and orientation by H . Usually he kernel function K is chosen to be a probability density symmetric bout zero (Sheather 2004 ). In this work, we use a 2D Gaussian ernel:\n( u ) = (2 \u03c0) \u22123 / 2 | H | 1 / 2 exp (\n\u22121 2\nu T H \u22121 u ) , (2)\nhere u = x \u2212 X i . For the bandwidth matrix, a scaling factor \u03ba is ultiplied by the covariance matrix of the data. For the selection of , we use Scott\u2019s Rule (Scott 1992 ), which, for equally weighted\nFigure 3. Model inputs for a cored and a cuspy galaxy, each one represented face-on and edge-on. The logarithmic slope at 150 pc is \u03b3 = \u22120.20 for the cored galaxy and \u03b3 = \u22121.32 for the cuspy galaxy. From top to bottom: A 3-colour image of the stars in the galaxy; the PDF in the { x , y } phase space; and the PDF in the { \u0302 R proj , \u0302 vLOS } phase space.\np d u o\n2\nF o m\ni i\ns i v t t\nt o\nn l l t f i\nt\n2\nI ( d o p d O o\n2 The use of a double Gaussian yields more accurate predictions than using a single one. On the other hand, using more than two Gaussians does not lead to more accurate slope predictions.\nD ow nloaded from https://academ ic.oup.com /m nras/article/519/3/4384/6964650 by Liverpool John M oores U niversity user on 18 April 2023\noints and two dimensions is \u03ba = n \u22121 6 , where n is the number of ata points. This leads to a fairly strong smoothing, which interested s to reduce the rele v ance of the number of stars and strengthen the v erall e v aluation of the data as opposed to indi vidual stars. .2.2 Model inputs rom the projected information (positions in the x \u2013y plane and v LOS ) f the sample of stars representing each galaxy we have made two aps: (i) A PDF sampled at 64 x 64 points with the distribution of stars n { x , y } phase space, between \u22122 kpc and 2 kpc in each coordinate, n the reference system where ( x , y ) = (0,0) is the centre of the galaxy.\n(ii) A PDF sampled at 64 x 64 points with the distribution of tars in { \u0302 R proj , \u0302 vLOS } phase space, where \u02c6 R proj = \u221a x 2 + y 2 /R hlr\ns the radial position normalized by the half-light radius R hlr and \u02c6 LOS = v LOS /P 98 per cent is the line-of-sight velocity normalized by he 98 per cent percentile of the absolute value of v LOS of all stars of he sample. \u02c6 R proj ranges from 0 to 1, and \u02c6 vLOS ranges from \u22121 to 1.\nNote that both the 2 kpc bounds in the positional data PDF and he limit up to R hl in the velocity PDF imply ignoring star data utside these regions. During the testing phase many bounds and\normalization methods were tested. With the current data set, the imits used in the work are the ones that gave the best results. A ikely explanation is that the information provided by stars outside hese limits is negligible and their presence in the PDFs only detracts rom the stars closer to the centre of the galaxy, where the key nformation for determining the internal slope is found.\nIn Fig. 3 , we show our model inputs, as the PDFs corresponding o both maps, for a cored (left) and cuspy (right) galaxy.\n.3 The model\nn this work, we use mixture density convolutional neural networks MDCNNs) to map the input data composed of the two PDFs escribed in Section 2.2.1 into the inner slopes of the DM profiles f the galaxy associated to those two PDFs. We approximate the osterior distribution of the slopes with the sum of two Gaussian istribution whose parameters are estimated by the neural network. 2\nur model takes as input a two channel image consisting of the PDFs n the { \u0302 R proj , \u0302 vLOS } phase space and the { x , y } phase space separately.\nMNRAS 519, 4384\u20134396 (2023)\nM\nFigure 4. Schematic representation of our double channel MDCNN architecture to infer inner slopes of the DM profiles (slope at 150 pc) of galaxies from their 2D phase-space mappings of positional and dynamical distributions of stars. The MDCNN extracts the spatial features from the phase-space mappings and gradually compresses into high-order features until describing the input with only five parameters, which are used as parameters of a double Gaussian corresponding to the probability density distribution of the inner slopes values.\nT T a c p\ni a\n2\nT s T l\nL\nw p l d v t\np\nw d a w t\nm\nt p f 2 G t g T a t\nm e v a f t w m o t t e t c\n2\nT u n w u c C\nD ow nloaded from https://academ ic.oup.com /m nras/article/519/3/4384/6964650 by Liverpool John M oores U niversity user on 18 April 2023\nhe images are passed through two convolutional sequential layers. he outputs of the two convolutional branches are then concatenated nd fed into a three layer fully connected network. The final output onsists of five parameters that parametrize the joint double Gaussian osterior. A schematic view of the architecture used in this work can be seen n Fig. 4 , while a more in-depth description of the different layers nd neural network methods can be found in the Appendix. .3.1 Training and evaluation he training is done o v er a training set consisting of 10 273 galaxy ubsets with their respective inner slopes, which act as targets. he loss function to minimize during the training is the ne gativ e ogarithmic likelihood of the training sample, defined as = \u2212 ln L \u2217 = \u2212 N \u2211 i= 1 ln [ p g ( t i | \u03b8 ) ] , (3) here t i is the inner slope of the galaxy subset i and \u03b8 the set of arameters of the distribution p g . For a certain galaxy subset, the ikelihood is the value of the PDF (defined with a double Gaussian istribution as the output of the last layer) in its real inner slope alue; i.e. the probability the model predicts for the inner slope of he galaxy to be its correct value:\ng ( x| \u03b8 ) = 2 \u2211\nj= 1 \u03c6j N ( x, \u03bcj , \u03c3j ) , (4)\nhere N ( x, \u03bcj , \u03c3j ) is the j Gaussian with mean \u03bcj and standard eviation \u03c3 j , \u03c6j is the weight of the j Gaussian, so that \u2211 n g j= 1 \u03c6j = 1, nd \u03b8 is then a set of six parameters (mean, standard deviation, and eight of the two Gaussians), one of which is not independent due o the normalization criterion. The minimization of the loss function is done with the adaptative oment estimation (AD AM) optimizer , an algorithm for optimiza-\nNRAS 519, 4384\u20134396 (2023)\nion that uses the gradient descent iterative technique. Between the opular learning-method algorithms, ADAM is shown to compare a v ourably in performance and computational cost (Kingma & Ba 015 ). After training, the e v aluation of the model outputs a double aussian distribution that can be understood as an approximation o the true posterior distribution of the inner slope of a given input, iven the prior distribution of the inner slopes in the training data set. his posterior then represents the probability that the model assigns certain value of the inner slope, given the set of observables under he prior of the training set.\nUsually, the test data set for the final e v aluation of the converged odel is constructed by randomly taking a sufficient number of\nlements from the complete data set to correctly represent all feature ariety in the data. In this work, due to the limited number of galaxies vailable, remo ving too man y galaxies with varying characteristics rom the training data set is expected to worsen the performance of he model, since we do not have many different examples of galaxies ith similar characteristics to each other. To properly e v aluate the odel, we have performed multiple complete training runs using nly 10 galaxies as validation and test data sets in each one, changing he galaxies that would come out of the training data set in each of he training runs to e v aluate the network in several projections of very galaxy. This allows us to analyse the consistency of the model raining and its performance in a large number of galaxies without ompromising the training data set.\n.3.2 Representing uncertainties\nhe output posterior distribution represents the random or aleatoric ncertainty in the slope prediction of the final model, but it does ot represent the uncertainty due to the stochastic nature of the eight determination while training the neural network (epistemic ncertainty), which can lead to different models for the same training onditions when dealing with limited data. We use the Monte arlo dropout method (MC-Dropout) (Gal & Ghahramani 2015 ) to\na e s w G i p a n p f G\n3\nT m o s r a g e t i\nm\n\u03b3\np b p f f b p r\nf a a c a i\ni ( c\n\u00b1 T r\nj\nFigure 5. Difference between real and predicted value of DM profiles inner slopes (defined at 150 pc) versus real inner slope, for the simulated galaxies used in this work, defining the predicted value as the mode of the posterior distribution. Each point represents the mean \u03b3 Real \u2212 \u03b3 Pred for all the projections of each individual galaxy, while error bars span the range between the minimum and the maximum deviation amongst every possible projection of each galaxy. Coloured areas represent increasing deviation ranges, from 0.05 to 0.4.\nDeviation range ( \u00b1 X) Percentage of projections\nwith Percentage of galaxies with | | \u2264 X | | \u2264 X\n0.05 66.67 67.80 0.1 80.79 81.92 0.2 94.35 94.35 0.3 98.31 98.31 0.4 99.44 98.87\nFigure 6. Distribution of \u03b3 Real \u2212\u03b3 Pred, mode for every projection of each galaxy in our training set.\nD ow nloaded from https://academ ic.oup.com /m nras/article/519/3/4384/6964650 by Liverpool John M oores U niversity user on 18 April 2023\npproximate the epistemic uncertainty that is based on the repeated v aluation of the same input, randomly setting to 0 the weights on ome layers while doing each inference, to construct a final e v aluation ith statistical information about the epistemic uncertainty. Gal & hahramani ( 2015 ) showed that applying dropout during inference s equi v alent to an approximation to a probabilistic Deep Gaussian rocess. It means we can measure the epistemic uncertainty by pplying the dropout layer during inference for a statistically rele v ant umber of them, acquiring a predictive mean and variance for each oint of the posterior distribution. The constructed final posterior or each galaxy projection is the normalized mean of 100 double aussian posteriors inferred by the model with active dropout layers.\nRESULTS\nhe goal of our work is to infer the logarithmic inner slope of the ass density profile in the central region of a galaxy (from now\nn: inner slope) from spectroscopic data of a random sample of its tars. To do so, all simulated galaxies and their subsets of stars are andomly projected in several sky planes, to simulate several viewing ngles, and the neural network is trained to infer the inner slope of the alaxy from the positions and line-of-sight velocities of its stars. For ach galaxy the neural network outputs, a PDF which approximates he posterior probability of obtaining a specific inner slope given the nputs.\n(i) by using the mode of the posterior distribution (i.e. the aximum of the PDF): \u03b3 Pred, mode . (ii) by using the mean of the normalized posterior distribution: Pred, mean .\nThe deviation of a prediction from its true value is defined as i = \u03b3 Real \u2212 \u03b3 Pred, i , where \u03b3 Real is the real slope at 150 pc of the DM rofile of a galaxy simulation. The results for the mode method can e seen in Fig. 5 , which shows the difference between the real and redicted slopes of our simulated dwarf galaxies, \u03b3 Real \u2212 \u03b3 Pred , as a unction of the real slope. Each point represents the mean deviation or every projection of each individual galaxy, while the deviation ars indicate the minimum and the maximum value amongst every ossible projection of each galaxy. Shaded-coloured horizontal areas epresent increasing uncertainty ranges, from \u00b10.05 to \u00b10.4.\nThe mean global absolute deviation on the predicted inner slope, or all the galaxies in our set, is of \u03bc = 0.056 for the mode method nd of \u03bc = 0.068 for the second method. Note that while cuspy nd \u2018in between\u2019 galaxies are scattered around \u03b3 Real \u2212 \u03b3 Pred = 0, ored galaxies tending towards \u03b3 = 0 are necessarily only scattered t \u03b3 Real \u2212 \u03b3 Pred \u2265 0, since by construction the maximum possible nner slope is 0.\nIn Table 1 , we can see the percentages of correctly predicted nner slopes, taking into account all the projections of every galaxy middle column) and each galaxy individually (right column), for our omplete test data set, within several uncertainty ranges. Roughly, 82 per cent of the galaxies reco v er the correct, real inner slope within\n0.1, while 98 per cent of them lie within | \u03b3 Real \u2212 \u03b3 Pred | \u2264 0.3. hese ranges are clearly small enough to shed light on the discussion\negarding the presence or not of cores in dwarf galaxies. Finally, a histogram of the deviation distribution for every proection of each galaxy (i.e. 10 273 in total) can be seen in Fig. 6 ,\nMNRAS 519, 4384\u20134396 (2023)\nM\ni s t a i p o =\n3\nI f t\n\u03c3\nw m\n\u03bc\nT o t b t a n n o t a o\nf d G o a\nTable 2. Percentage of predictions within increasing \u03c3 pos ranges X, defined as | \u03b3 Real \u2212 \u03b3 Pred | \u2264 X .\nRegion Percentage of projections for which \u03b3 Real is within region\n1 \u2212\u03c3 pos 86.29 2 \u2212\u03c3 pos 97.57 3 \u2212\u03c3 pos 99.73\n\u2212 t p i S a ( c e g\n3\nM i c o t p a P t \u03b3 o b s d g c D a l c e d o c\no a p i p s i\n4\nW t n c\nD ow nloaded from https://academ ic.oup.com /m nras/article/519/3/4384/6964650 by Liverpool John M oores U niversity user on 18 April 2023\nndicating that the values of \u03b3 Real \u2212\u03b3 Pred are peaked at and roughly ymmetrically distributed around 0, except for very cored galaxies hat have by definition \u03b3 Real \u2212\u03b3 Pred \u2265 0, as already stated, and small asymmetry towards predicting stronger cores in galaxies n the range of small deviations. We showed that our method redicts accurately the expected inner slope of g alaxies reg ardless f their actual real slope, with a mostly uniform scatter of \u03c3 0.075. .2 Uncertainty in the inference n Fig. 7 , we show the standard deviation \u03c3 pos of each posterior PDF rom every galaxy in the test data set, defined as the square root of he variance of the normalized posterior: 2 pos = \u222b \u221e \u2212\u221e ( \u03b3 \u2212 \u03bcpos ) 2 P ( \u03b3 ) d \u03b3, (5) here P ( \u03b3 ) is the normalized posterior distribution and \u03bcpos is the ean of the distribution: pos = \u222b \u221e \u2212\u221e \u03b3P ( \u03b3 ) d \u03b3. (6)\nhe mean of all the \u03c3 pos of the data set, \u03bc\u03c3pos , is around 0.1 and nly 8.99 per cent of the projections have values of \u03c3 pos greater han 0.2, uncertainties that are small enough to clearly distinguish etween cores and cusps in the vast majority of cases. Fig. 7 shows hat the standard deviation \u03c3 pos of each posterior PDF is uniform cross the inner slopes values, i.e. the width of the PDFs does ot depend on the inner slope of galaxies, such that the model is ot biased towards reco v ering with higher accurac y either cusps r cores. Most galaxies show a significant variation in the size of heir uncertainties depending on the projection, indicating that the mplitude of the uncertainty is strongly correlated with the angle of bservation. Table 2 shows the percentage of the test data set projections or which the true value of their inner slope is reco v ered within ifferent multiples of \u03c3 pos . If we approximate the posteriors to single aussians (which is a proper approximation for roughly 90 per cent f the projections), a well-calibrated uncertainty should provide round 68 per cent of the outputs within a confidence level of 1\nNRAS 519, 4384\u20134396 (2023)\n\u03c3 pos . Our greater percentage ( \u223c 86 per cent ) of projections within he confidence level of 1 \u2212 \u03c3 pos indicates that the model is o v erredicting the uncertainties \u03c3 pos , yielding broader posteriors than t should. This can be an effect of a too high dropout rate (see ection A ) during training, which has been shown to have such outcome on the results of probabilistic neural network models Ghosh et al. 2022 ). As it is, our model should be interpreted as onserv ati ve, since a future, better calibrated MDCNN would provide ven tighter uncertainties in reco v ering the true inner slope of a alaxy.\n.3 Effect of viewing angle on the inference of DM slopes\nost of the posteriors for the different projections have an approxmately normal distribution (the second Gaussian disappearing or onstituting a skewness correction to the main Gaussian), but several f them have two distinct peaks. Specifically, around 30 per cent of he galaxies have double peaks in more than 10 per cent of their osteriors. 54 per cent of these galaxies are cored while 46 per cent re cuspy, indicating that the appearance of double peaks in the DFs arises in both scenarios (here, we define as cored galaxies hose with inner slope \u22120.6 < \u03b3 < 0, and cuspy any galaxy with < \u22120.6). In Figs 8 and 9 , we show the PDFs and posteriors f two galaxies at different observation angles, spanning the range etween a face-on and a edge-on view . Strikingly , these images how that the width of the PDFs as well as the appearance of ouble peaks are strongly related to the viewing angle of the alaxy. This indicates that the appearance of double peaks is a onsequence of the fact that some information on the underlying M profiles is hidden when viewing the galaxy at some particular ngle, while it is released and efficiently passed to the network when ooking at the galaxy from other angles: this finding has profound onsequences for the interpretation of \u2018cusp-cores\u2019 in dwarfs. For xample, in Fig. 9 we observe that the double peaks in the posterior istribution disappear when the galaxy is seen edge-on, while a facen configuration provides a second peak that mimics the presence of a usp.\nHo we ver, this is just an example, and we hav e sev eral cases f galaxies in which the double peaks appear in edge-on view nd disappear in face-on, so that the appearance of these multiple eaks is not related to a specific edge-on or face-on configuration: ndeed, the distribution of angles for those PDFs showing double eaks is uniform throughout the complete data set. The occurrence, ignificance and widths of the double peaked PDFs will be explored n future works, as it goes beyond the scope of this paper.\nAPPLI CATI ON TO OBSERV ED G A L A X I E S\ne proceed to test our model with real observed galaxies, in order o ensure the applicability of the model and to verify that the neural etwork is not detecting features of simulated galaxies that do not orrespond to any real physical system. We selected four dSphs for\nFigure 8. Probability density distributions used by the neural network as input in the case of one simulated galaxy subset seen at 0 \u25e6 (face-on), 45 \u25e6 and 90 \u25e6 (side-on), alongside with the Bayesian posteriors predicted by the model. Left columns: PDFs in the { x , y } phase space. Central columns: PDFs in the { \u0302 R proj , \u0302 vLOS } phase space. Right columns: predicted Bayesian posterior in the space of inner slope of the DM profile (slope at 150 pc); shaded regions represent the standard deviation of the posterior values for the MC-Dropout inferences at each slope point, while the blue vertical line shows the mode (maximum) of the posterior distribution and the black one the true value of the inner DM slope.\nw b M o s w r\n9 t m i u t o t a o t c a l c e (\n4\nW t g i\ns s w 2 e p p t\nv a b F p w k C a p a O a d O i m v\ni a o 2 i a t t t o m fi w\na w g o G c t c f a i\n4\nW i w t i t\nD ow nloaded from https://academ ic.oup.com /m nras/article/519/3/4384/6964650 by Liverpool John M oores U niversity user on 18 April 2023\nhich detailed spectroscopic samples of stellar-kinematic data have een published. At this stage, we adopt the catalogues by Walker, ateo & Olszewski ( 2009 ) to directly compare our results with those btained using the code GRAVSPHERE , as in Read et al. ( 2019 ). The elected galaxies are Carina, Se xtans, F ornax and Sculptor, for which e further use the center position, velocity, ellipticity and half-light adius as compiled in Battaglia et al. ( 2022 ). To build our input PDFs, we considered only those stars with a 0 per cent or higher probability of being part of the galaxy and we ook the mean value of the line-of-sight velocity for those stars with ultiple measurements. We do not take observational uncertainties nto account, since adding noise to the data by making use of ncertainties in the line-of-sight velocity only goes so far as to alter he mean, maximum, and width of the posteriors by an order of 10 \u22122 v er multiple iterations for these four galaxies. This may change in he future if more data sources with less accurate measurements, such s proper motion, are added. A full and formal treatment of the effect f observational uncertainties will be included in future work, but heir inclusion does not affect the results presented here. In total, we onsidered 460 stars for Carina, 1353 for Fornax, 809 for Sculptor nd 327 for Sextans, and we used their projected x \u2013y positions and ine-of-sight velocities. The x \u2013y positions are normalized using the ircularized half-light radius R \u2032 hlr = R hlr \u221a 1 \u2212 ell , where R hlr and ll are the half-light radius and ellipticity from Battaglia et al. 2022 ).\n.1 Deriving central DM density slopes of dSphs with CNNs\ne now infer the inner slope of the observed dwarfs. Fig. 10 shows he posterior distributions constructed by the model for each observed alaxy. Fornax presents a very narrow peak around \u03b3 = \u22120.38, ndicating that this galaxy has a strong central DM core, while a\necondary peak would give a 12 per cent probability that the inner lope is around \u03b3 = \u22120.81. This is consistent with several previous orks that predict a cored profile for Fornax (see Goerdt et al. 006 ; Walker & Pe \u0303 narrubia 2011 ; Brook & Di Cintio 2015 ; Pascale t al. 2018 , amongst others). For the other three galaxies, a cusp is redicted with varying degrees of certainty. The model has a clear eak around \u03b3 = \u22121.06 for Carina, which roughly corresponds to he slope of an NFW profile at 150 pc.\nSextans presents a relatively large uncertainty in the inner slope alue, as depicted by the quite broad PDFs, with a broad peak round \u03b3 = \u22121.25 and a strong right wing that does not fall elow 10 per cent of the peak value until it reaches \u03b3 = \u22120.68. inally, Sculptor peaks at \u03b3 = \u22121.08, but it has a wide secondary eak, predicting a 18 per cent probability of having a mild core ith \u03b3 = \u22120.75. A small core was derived for Sculptor by using inematical data and a mass-dependent profile fit in Brook & Di intio ( 2015 ), in agreement with the Walker & Pe \u0303 narrubia ( 2011 ) nd Agnello & Evans ( 2012 ) methods that, employing multiple stellar opulations within a galaxy, also predicted a core in such dwarf (see lso Zhu et al. 2016 ; Breddels et al. 2013 ; Hayashi et al. 2020 ). ther studies, ho we ver, surprisingly predict a cusp for Sculptor after ll (Richardson & Fairbairn 2014 ), highlighting the importance of eriving the DM density of this dSphs with se veral dif ferent methods. ur derived posterior distributions offer great versatility in interpretng the results, allowing for a more complex analysis compared to odels that only allow for uncertainty ranges around the inferred alue. We compare the results of our model with the inner slopes nferred for these same galaxies at 150 pc using GRAVSPHERE , non-parametric spherical Jeans analysis code, which make use f photometric and kinematic data from the galaxies (Read et al. 019 ). The inferred values, along with their 68 per cent confidence ntervals (in our case, taking the primary maximum as reference), re listed in Table 3 . The derived values are consistent between the wo models, within their respective uncertainty ranges, indicating hat our neural network model is making predictions similar to hose obtained by Jeans analysis. Furthermore, the accuracy of ur neural network is greater, with errors roughly an order of agnitude smaller than those of GRAVSPHERE : this preliminary nding will be expanded and explored in more detail in future ork. Compared to GRAVSPHERE and similar codes, the neural network pproach is significantly faster. In a modern laptop, GRAVSPHERE ill need about half a day to run an analysis of one of these alaxies, whereas the neural network can be trained with the amount f data used in this work in less than half an hour on a standard PU. Furthermore, the training and the e v aluation are independent alculation in a neural network model, which means that, once he model has been trained, its application to any input data to onstruct the posterior distribution is nearly instantaneous. This eature will not change no matter how much the model is expanded nd complexified to perform more complete analyses of the galaxy of nterest.\n.2 Testing the similarity of training versus observational data\nhen training a neural network with simulations to then perform nference on real data, there is al w ays the risk that the network ill detect and learn from specific features of the simulation code hat do not correspond to reality, and this would cause issues when nterpreting observational data, as they have different qualities than hose used in the training set. We can test the degree to which our\nMNRAS 519, 4384\u20134396 (2023)\nM\nFigure 9. Probability density distributions used by the neural network as input in the case of one simulated galaxy subset seen at 0 \u25e6 (face-on), 45 \u25e6 and 90 \u25e6 (side-on), alongside with the Bayesian posteriors predicted by the model. Left columns: PDFs in the { x , y } phase space. Central columns: PDFs in the { \u0302 R proj , \u0302 vLOS } phase space. Right columns: predicted Bayesian posterior in the space of inner slope of the DM profile (slope at 150 pc); shaded regions represent the standard deviation of the posterior values for the MC-Dropout inferences at each slope point. The red vertical line shows the primary maximum of the posterior distribution (the mode), the green one the secondary maximum and the black one the true value of the inner DM slope. As a blue line, the mean between primary and secondary maximum is shown, when two peaks exist (in the bottom panel, instead, the blue line represents the unique maximum). This example shows how the appearance of double peaks in the posterior distributions is strongly related to the viewing angle.\nn b s c a G w c i n d t t s\ni j H t a s p T s f e t\nD ow nloaded from https://academ ic.oup.com /m nras/article/519/3/4384/6964650 by Liverpool John M oores U niversity user on 18 April 2023\network sees observational data as equivalent to the data it has een trained on by observing the parameter space of the test data et, defined as the set of all combinations of the six parameters orresponding to each element of such data set. Namely, our outputs re defined by the mean, standard deviation and weight of two aussians: this 6D parameter space will hav e re gions populated ith points and regions completely empty, corresponding to the ombinations of parameters that do not parametrize the characterstics of any physical system found in the data set. If the neural etwork does not see differences in the input with respect to the ata it has been trained on, the resulting parameters, coming from he e v aluation of observ ational data with our model, will fall within he populated regions of the parameter space of the simulation data\nNRAS 519, 4384\u20134396 (2023)\net. t\nTo be able to visualize the 6D parameter space and test if this s the case, we use the Uniform Manifold Approximation and Proection (UMAP) for Dimension Reduction technique from McInnes, ealy & Melville ( 2018 ) to perform a dimension reduction from 6D o 2D, thus mapping each combination of means, standard deviations nd weights to only two adimensional parameters representing uch \u2018contraction\u2019, preserving the global structure of the original arameter space. This allows to visualize the parameter space in 2D. he result of the dimension reduction process from the complete test ample can be seen in Fig. 11 , alongside with the position of the our observed dwarf galaxies shown in the same parameter space, ach indicated as coloured star. As expected, the spatial location of he points in the parameter space is strongly linked to the value of heir inner slope: points with a similar inner slope cluster together,\nFigure 10. Bayesian posterior distributions in the space of inner slope of DM profiles (slope at 150 pc) predicted by our neural network model for the observed dSphs Carina, Sextans, Sculptor, and Fornax. Shaded regions represent the standard deviation of the posterior values for the MC-Dropout inferences at each slope point. In each panel, the global maximum of the posterior distribution as well as the mean value are indicated, together with primary and secondary peaks when they exist. Fornax has the strongest signature of a central DM density core, while Carina has the strongest signature of having an NFW profile. Sextans is cuspy, though with a large uncertanity, while Sculptor is cuspy with a secondary peak indicating a mild core.\nTable 3. Inner slope of the DM profile (at 150 pc) for Carina, Sextans, Sculptor and Fornax galaxies predicted by GRAVSPHERE ( \u03b3 GS ) and our neural network ( \u03b3 NN ), with their 68 per cent confidence intervals (for the neural network posterior, taking the primary maximum as reference). The agreement between the two methods is encouraging.\n\u03b3 GS \u03b3 NN\nCarina \u22121 . 23 + 0 . 39 \u22120 . 35 \u22121 . 06 + 0 . 05 \u22120 . 04 Sextans \u22120 . 95 + 0 . 25 \u22120 . 25 \u22121 . 25 + 0 . 25 \u22120 . 09 Fornax \u22120 . 30 + 0 . 21 \u22120 . 28 \u22120 . 38 + 0 . 01 \u22120 . 02 Sculptor \u22120 . 83 + 0 . 30 \u22120 . 25 \u22121 . 08 + 0 . 08 \u22120 . 04\ns o f t\nt o s p l\n5\nW d u a v c t a g\nD ow nloaded from https://academ ic.oup.com /m nras/article/519/3/4384/6964650 by Liverpool John M oores U niversity user on 18 April 2023\nhowing that the network is properly parametrizing the inner slope f galaxies during training. Interestingly, the four observed galaxies all into the regions occupied by the simulated ones, which indicates hat the model is considering them as data of equi v alent nature as\nhe test data. Ho we ver, the fact that all four are close to the edges f the simulation input parameters could indicate the presence of ome features that the model has not found in the simulations. The ossible causes of this will be explored in future work employing a arger observational sample.\nC O N C L U S I O N S\ne present a no v el model for determining the slope of the inner ensity profile of DM haloes with robust uncertainty quantification sing machine learning techniques. The goal of this work is to be ble to infer such density slopes ( \u03b3 ) by simply using positions and elocities of stars within galaxies. Our method uses mixture density onvolutional neural networks with a Gaussian density layer backend o model complex galaxy substructure. We use line-of-sight velocities nd positions of stars projected on the sky within simulated dwarf alaxies, employing Kernel Density Estimations (KDEs) to construct\nMNRAS 519, 4384\u20134396 (2023)\nM\nFigure 11. Representation of the parameter space for the test data from simulated galaxies reduced to two-dimension with a UMAP, colour coded by the real expected inner DM slope. Plotted as coloured stars are the locations in the reduced parameter space of Carina, Sextans, Sculptor and Fornax galaxies. Note that different inner slopes occupy different areas of the plot and, importantly, observed dwarf galaxies fall well within the simulation region, indicating that the neural network model is not seeing rele v ant dif ferences between the simulated data with which we have fed it and the observational data.\nc a u\nc 1 N G t p s i\nl G d t u g o\na\nf f\nc\n0 g\nP s v t\nr o R\nh t p b p\nm t a i s t m t t m\nb p 2 p u A e v a\nt g m r\nf i c\nA\nC w l d E w f\n( A F f t\nD ow nloaded from https://academ ic.oup.com /m nras/article/519/3/4384/6964650 by Liverpool John M oores U niversity user on 18 April 2023\nontinuous 2D PDFs of the distribution of such stars in { \u02c6 R proj , \u0302 vLOS } nd { x , y } phase space, which serve as input to our neural network sing a double channel architecture (Figs 3 and 4 ). We train and e v aluate our model using a large set of fully osmological simulations of dwarf galaxies with halo masses of 0 9 to 10 11.5 M , and stellar masses of 10 5 to 10 9.5 M , from the IHA O and A URIGA projects (Wang et al. 2015 ; Dutton et al. 2020 ; rand et al. 2017 ). The use of different physical models employed in hese simulations allows us to have a range of density profiles at each articular galaxy mass, including both cores and cusps (Fig. 1 ). All imulated galaxies and their subsets of stars are randomly projected n several sky planes, to simulate several viewing angles. The loss function to minimize during the training is the ne gativ e ogarithmic likelihood of the training sample, defined as a double aussian probability distribution, which is the output of our Gaussian ensity layer backend. This allows a flexible probabilistic represenation of the results, which yields accurate and statistically consistent ncertainties. For each galaxy, the neural network outputs a PDF that ives the posterior probability of a certain slope to be the inner slope f the galaxy. The main results of this work are listed here:\n(i) The inner slope of simulated galaxies is predicted with a mean bsolute deviation of \u03bc = 0.056 (where the deviation is defined as = \u03b3 Real \u2212 \u03b3 Pred , and the predicted inner slope, \u03b3 Pred , is obtained rom the mode of the PDFs) and a standard deviation of \u03c3 = 0.075 or the whole sample (Figs 5 and 6 ).\n(ii) 82 per cent (98 per cent ) of the galaxies have their inner slope orrectly determined within \u00b1 0.1 (0.3) of their true value (Table 1 ).\n(iii) The posteriors PDFs have a mean standard deviation of \u03c3 pos = .108, showing no bias towards more accuracy for cuspy or cored alaxies (Fig. 7 ).\nNRAS 519, 4384\u20134396 (2023)\n(iv) While in most cases the output of the model is a single peaked DF, in \u223c 30 per cent of the galaxies some of their projections how a double peak: we demonstrated that this is related to some iewing angles, indicating the importance of properly determining he inclination of galaxies (Figs 8 and 9 ).\n(v) When applied to a set of four observed dSphs, our model eco v ers their inner slopes yielding values consistent with those btained with the Jeans modelling based code GRAVSPHERE as in ead et al. ( 2019 ) (Table 3 ). (vi) We found that the Fornax dSph has a strong indication of aving a central DM core, Carina and Se xtans hav e cusps (although he latter with a large uncertainty), while Sculptor shows a double eaked PDF indicating that a cusp is preferred, but a core cannot e ruled out (Fig. 10 ). These results are in agreement with several re viously deri ved inner slopes for these galaxies.\nThe current architecture could be used as a basis for building odels that provide a more complete output, such as a prediction of he full density profile of galaxies. The nature of the neural network llows it to be constantly extended and impro v ed. While we hav e mplemented a network of relatively low complexity, there are a eries of interesting possibilities with a further level of sophistication hat are worth e xploring. F or e xample, the use of normalizing flows ay yield to more robust results (Kodi Ramanah et al. 2020 ) while he use of a 3D convolutional network applied to PDFs defined in he { x , y , \u02c6 vLOS } phase space has given good results in galaxy cluster\nasses inference (Kodi Ramanah et al. 2021 ). In the future, the architecture of this model could be expanded\ny including more input data, such as surface brightness profiles or roper motion of stars from missions like GAIA (Gaia Collaboration 021 ). Furthermore, the inclusion of other spectroscopic samples resent in the literature, as well as of those soon to be acquired with pcoming facilities, will certainly be beneficial for this analysis. dapting the architecture and introducing more information may nable the network to impro v e accurac y and reduce the range of ariability of the results with respect to the angle of observation, an venue that will be explored in future works.\nWe have shown that deep learning techniques provide an innovaive method for the determination of the inner DM profile in dwarf alaxies, complementary to the use of Jeans and Schwarzschild odelling, achieving great accuracy and offering a complex repesentation of uncertainties. Our ne wly de veloped neural network method is a promising tool or the study of the mass distribution within dwarf galaxies, which n turn can help discriminate between different models and, in such, onstraining the properties of the elusive DM.\nC K N OW L E D G E M E N T S\nBB led the project. JEM performed the analysis. JEM and ADC rote the manuscript. JEM and MHC developed the machine earning architecture. CBB, MHC and ADC supervised the student uring the project. AVM and RJJG provided the simulation data. AG helped with the treatment of AURIGA simulations. GB helped ith the selection of the observational data set. All authors provided eedback on the draft. CB is supported by the Spanish Ministry of Science and Innovation MICIU/FEDER) through research grant PID2021-122603NB-C22. DC is supported by a Junior Leader fellowship from \u2018La Caixa\u2019 oundation (ID 100010434), code LCF/BQ/PR20/11770010. She urther acknowledges Macquarie University for the hospitality during he preparation of this work as a Honorary Visiting Fellow. MHC\ne ( u w j a w fi S K t A v I S M S 2 t A\nD\nT t p\nR\nA B\nB B\nB B B\nB B C C\nC d\nD\nD\nD\nG G\nG G\nG G G\nG G\nH H\nJ K K\nK\nK\nK K L L M M\nM\nM N N P P P R\nR R S\nS S\nS S\nS S\nT v v W W W\nZ\nA N\nA a d a a l p\nc i a w\nD ow nloaded from https://academ ic.oup.com /m nras/article/519/3/4384/6964650 by Liverpool John M oores U niversity user on 18 April 2023\nacknowledge financial support from the State Research Agency AEIMCINN) of the Spanish Ministry of Science and Innovation nder the grant and \u2018Galaxy Evolution with Artificial Intelligence\u2019 ith reference PGC2018-100852-A-I00, from the ACIISI, Conseer \u0301\u0131a de Econom \u0301\u0131a, Conocimiento y Empleo del Gobierno de Canarias nd the European Regional Development Fund (ERDF) under grant ith reference PROID2020010057, and from IAC project P/301802, nanced by the Ministry of Science and Innovation, through the tate Budget and by the Canary Islands Department of Economy, nowledge and Employment, through the Regional Budget of he Autonomous Community. GB acknowledges support from the gencia Estatal de Investigaci \u0301on del Ministerio de Ciencia en Innoaci \u0301on (AEI-MICIN) under grant references PID2020-118778GB00/10.13039/501100011033 and grant number CEX2019-000920. Fellow RG acknowledges financial support from the Spanish inistry of Science and Innovation (MICINN) through the Spanish tate Research Agency, under the Severo Ochoa Program 2020\u2013 023 (CEX2019-000920-S). Part of this research was carried out on he High Performance Computing resources at New York University bu Dhabi (UAE).\nATA AVAILABILITY\nhe data used in this work are available upon reasonable request to he corresponding author and to the PIs of the NIHAO and AURIGA rojects.\nEFERENCES\ngnello A., Evans N. W., 2012, ApJ , 754, L39 attaglia G., Helmi A., Tolstoy E., Irwin M., Hill V., Jablonka P., 2008, ApJ ,\n681, L13 attaglia G., Taibi S., Thomas G. F., Fritz T. K., 2022, A&A , 657, A54 en \u0301\u0131tez-Llambay A., Frenk C. S., Ludlow A. D., Navarro J. F., 2019, MNRAS ,\n488, 2387 inney J., Mamon G. A., 1982, MNRAS , 200, 361 reddels M. A., Helmi A., 2013, A&A , 558, A35 reddels M. A., Helmi A., van den Bosch R. C. E., van de Ven G., Battaglia\nG., 2013, MNRAS , 433, 3173 rook C. B., Di Cintio A., 2015, MNRAS , 450, 3920 ullock J. S., Boylan-Kolchin M., 2017, ARA&A , 55, 343 appellari M. et al., 2006, MNRAS , 366, 1126 han T. K., Kere \u030cs D., O \u0303 norbe J., Hopkins P. F., Muratov A. L., Faucher-\nGigu \u0300ere C.-A., Quataert E., 2015, MNRAS , 454, 2981 ollins M. L. M. et al., 2021, MNRAS , 505, 5686 e Blok W. J. G., W alter F ., Brinks E., Trachternach C., Oh S.-H., Kennicutt\nR. C., Jr, 2008, AJ , 136, 2648 i Cintio A., Brook C. B., Macci \u0300o A. V., Stinson G. S., Knebe A., Dutton A.\nA., Wadsley J., 2014a, MNRAS , 437, 415 i Cintio A., Brook C. B., Dutton A. A., Macci \u0300o A. V., Stinson G. S., Knebe\nA., 2014b, MNRAS , 441, 2986 utton A. A., Buck T., Macci \u0300o A. V., Dixon K. L., Blank M., Obreja A.,\n2020, MNRAS , 499, 2648 aia Collaboration, 2021, A&A , 649, A1 al Y., Ghahramani Z., 2015, Proceedings of The 33rd International Confer-\nence on Machine Learning eha M. C., Guhathakurta P., Rich R. M., Cooper M. C., 2006, AJ , 131, 332 entile G., Salucci P., Klein U., Vergani D., Kalberla P., 2004, MNRAS , 351,\n903 hosh A. et al., 2022, ApJ, 935, 2 nedin O. Y., Zhao H., 2002, MNRAS , 333, 299 oerdt T., Moore B., Read J. I., Stadel J., Zemp M., 2006, MNRAS , 368,\n1073 o v ernato F. et al., 2010, Nature , 463, 203 rand R. J. J. et al., 2017, MNRAS , 467, 179\nayashi K., Chiba M., Ishiyama T., 2020, ApJ , 904, 45 o M., Rau M. M., Ntampaka M., Farahi A., Trac H., P \u0301oczos B., 2019, ApJ ,\n887, 25 affe W., 1983, MNRAS , 202, 995 aplinghat M., Tulin S., Yu H.-B., 2016, Phys. Rev. Lett. , 116, 041302 ingma D. P., Ba J., 2014, 3rd International Conference on Learning\nRepresentations, ICLR 2015, San Diego, CA, USA, May 7-9 leyna J. T., Wilkinson M. I., Evans N. W., Gilmore G., 2001, ApJ , 563,\nL115 odi Ramanah D., Wojtak R., Ansari Z., Gall C., Hjorth J., 2020, MNRAS ,\n499, 1985 odi Ramanah D., Wojtak R., Arendse N., 2021, MNRAS , 501, 4080 owalczyk K., \u0141okas E. L., Valluri M., 2017, MNRAS , 470, 3959 eCun Y., Bengio Y., Hinton G., 2015, Nature , 521, 436 elli F., McGaugh S. S., Schombert J. M., 2016, AJ , 152, 157 acci \u0300o A. V., Crespi S., Blank M., Kang X., 2020, MNRAS , 495, L46 cInnes L., Healy J., Melville J., 2018, Journal of Open Source Software, 3,\n861 erritt D., Graham A. W., Moore B., Diemand J., Terzi \u0301c B., 2006, AJ , 132,\n2685 oore B., 1994, Nature , 370, 629 avarro J. F., Eke V. R., Frenk C. S., 1996, MNRAS , 283, L72 avarro J. F., Frenk C. S., White S. D. M., 1996, ApJ , 462, 563 ascale R., Posti L., Nipoti C., Binney J., 2018, MNRAS , 480, 927 lanck Collaboration XIII, 2016, A&A , 594, A13 ontzen A., Go v ernato F., 2012, MNRAS , 421, 3464 ead J. I., Wilkinson M. I., Evans N. W., Gilmore G., Kleyna J. T., 2006,\nMNRAS , 367, 387 ead J. I., Walker M. G., Steger P., 2019, MNRAS , 484, 1401 ichardson T., Fairbairn M., 2014, MNRAS , 441, 1584 chneider A., Trujillo-Gomez S., Papastergis E., Reed D., Lake G., 2017,\nMNRAS , 470, 1542 chwarzschild M., 1979, ApJ , 232, 236 cott D., 1992, Multi v ariate Density Estimation: Theory, Practice, and\nVisualization. John Wiley & Sons, Inc., New York heather S. J., 2004, Stat. Sci., 19, 588 imon J. D., Bolatto A. D., Leroy A., Blitz L., Gates E. L., 2005, ApJ , 621,\n757 pergel D. N., Steinhardt P. J., 2000, Phys. Rev. Lett. , 84, 3760 tinson G., Seth A., Katz N., Wadsley J., Governato F., Quinn T., 2006,\nMNRAS , 373, 1074 ollet E. et al., 2016, MNRAS , 456, 3542 an den Bosch R. C. E., de Zeeuw P. T., 2010, MNRAS , 401, 1770 an der Marel R. P., 1994, MNRAS , 270, 271 alker M. G., Pe \u0303 narrubia J., 2011, ApJ , 742, 20 alker M. G., Mateo M., Olszewski E. W., 2009, AJ , 137, 3100 ang L., Dutton A. A., Stinson G. S., Macci \u0300o A. V., Penzo C., Kang X.,\nKeller B. W., Wadsley J., 2015, MNRAS , 454, 83 hu L., van de Ven G., Watkins L. L., Posti L., 2016, MNRAS , 463, 1117\nPPENDI X: DETA I LS O N T H E N E U R A L E T WO R K M O D E L\nneural network can be formally described as a trainable and flexible pproximation of a model M : d \u2192 t . The networks maps an input ata d to a prediction \u0304t of the target t . This network is parametrized by set of trainable weights and a set of hyperparameters. The weights re iteratively optimized during training to minimize a particular oss function, which provides a measure of how close the network rediction t\u0304 is to the target t . In this work, we use convolutional neural networks (CNNs), a lass of deep neural networks (DNNs), to construct a neural network n which the input data d are the two PDFs described in Section 2.2.2 , nd the targets t are the inner slopes of the galaxy subsets associated ith those two PDFs. We then make a mixture density convolutional\nMNRAS 519, 4384\u20134396 (2023)\nM\nn w\nA\nA b\nf\nw e a a t\nl i l h\ne w o\nT\nt ( u f d b i\nA\nC w a i f n f\nk a a w o w l p o o\nt t\nw fi fi m\nfi o i d t a\nb a r v ( n t i r c i s\nA\nA m l i p p i w f\nA\nT i\nl l M i i s i o n s c d p d i w u\nT\nD ow nloaded from https://academ ic.oup.com /m nras/article/519/3/4384/6964650 by Liverpool John M oores U niversity user on 18 April 2023\neural network (MDCNN) by embedding a mixture density layer ithin the CNN as the last layer.\n1 Deep neural networks\nny neural network is conformed by a set of neuron layers, defined y the following function:\n( x ) = g( W \u00b7 x + b) , (A1) here x is the input of the layer, W is the weight matrix (which each\nlement being the weight of each element of the vector x ), and b is vector called the bias parameter of the layer. g ( z ) is known as the cti v ation function, which purpose is to break the linearity between he input and the output of the neuron.\nA DNN is a neural network conformed by more than one neuron ayer. The layers between the input layer (the layer that takes as nputs the input data of the neural network) and the output layer (the ayer that gives as output the outputs of the neural network) are called idden layers. A feed-forward DNN is a DNN where the neuron layers are v aluated in sequence, passing information from layer to layer ithout recurrence, which means we can describe the output h ( l) f the l th layer as\nh ( l) = g( W ( l) \u00b7 h ( l\u22121) + b ( l) ) . (A2) he training of the model is done by optimizing the weight matrices W ( l) . A model is trained on a set of input data d for which the targets are kno wn iterati vely. In each iteration, the network performance the similarity between the outputs t\u0304 and the targets t ) is e v aluated sing a loss function, and the weights are actualized to minimize that unction by an optimization algorithm. When the loss function stops ecreasing and converges to a certain value, the network is said to e optimized. The performance e v aluation is done, then, on a set of ndependent data the model has not seen during training.\n2 Convolutional neural networks\nNNs are a particular type of DNNs especially suited for problems here spatially correlated information is crucial. The main feature of CNN is the presence of convolutional layers, which are constructed n a way that restrict neurons in one layer to receive information only rom within a small neighbourhood of the previous layer. This allows eurons to extract simple features from subsets of the previous layer, orming higher order features in subsequent layers.\nA convolutional layer is designed as follows: A convolutional ernel, commonly referred to as a filter, of a given size, encoding set of neurons, is applied to each pixel (in the case of 2D images s inputs) of the input image and its vicinity, as it scans through the hole re gion. A giv en pix el in a specific layer is only a function f the pixels in the preceding layer which are enclosed within the indow defined by the kernel, known as the receptive field of the ayer. This yields a feature map that encodes high values in the ixels which match the pattern encoded in the weights and biases f the corresponding neurons in the convolutional kernel, which are ptimized during training (Kodi Ramanah et al. 2021 ). A convolutional layer may be described as a linear operation with he discrete convolution implemented via matrix multiplication. In erms of equation ( A2 ):\nh ( l) j = g \u239b \u239d \u2211\ni\u2208 M j h ( l\u22121) \u00d7 k ( l) ij + b ( l) j\n\u239e \u23a0 , (A3)\nNRAS 519, 4384\u20134396 (2023)\nhere k is the convolutional kernel (the filter) and M j is the receptive eld of the neuron j . One convolutional layer could have multiple lters, which repeat this operation with different kernels, constructing any feature maps per layer, known as channels. The receptive field is usually defined by the dimensions of the\nlter, the stride and the existence or not of padding. The application f the filter can be described as a process of sliding it o v er the input mage of the convolutional layer. We call stride to the number and irection of pixels you move the filter at each step, and padding to he addition of empty pixels around the edges with the purpose of lleviating information loss around the edges.\nUsually, a CNN is a series of pairs of convolutional layers followed y a pooling layer as a subsampling or dimensionality reduction step, process which will reduce the initial input image to a compact epresentation of features. Then, that representation is reshaped as a ector, which is subsequently passed to a sequence of dense layers LeCun, Bengio & Hinton 2015 ). This design allows the neural etwork to autonomously extract meaningful spatial features from he input image. The stack of several convolutional layers builds an nternal hierarchical representation of features encoding the most ele v ant information from the input image. Stacking subsequent onvolutional layers naturally strengthens the sensitivity of the most nternal layers to features on increasingly larger scales, because the ize of the receptive field becomes larger as we go deeper in the CNN.\n3 Mixture density neural networks\nMDNN is a network with layers whose outputs follow a multidiension probability distribution, called mixture density layers. This ayers take as inputs n nodes, with n being the number of parameters n the desired distribution, transform their values to respect the arameter constraints of the distribution and interpret them as those arameters to construct it. When used as the last layer of the network, t allows join optimization of the features from the DNN together ith a bayesian posterior backend, combining the advantages of deep eature extraction with probabilistic representation of the results.\n4 Details on the ar chitectur e\nhe schematic view of the architecture used in this work can be seen n Fig. 4\nThe convolutional sequences are constructed using pairs of convoutional and pooling layers, followed by a dropout layer. The pooling ayers downsample their input along its spatial dimension using the\nax Pooling method, which takes the maximum value o v er a certain nput window for each channel. The dropout layers randomly set nput units to 0 with a certain frequency called the dropout rate, and cales the rest such that the sum o v er all inputs is unchanged. This s done to prev ent o v erfitting during training. The joint sequence f dense layer stars with a normalization layer that applies batch ormalization to the nodes coming from the previous convolutional equences. This normalization maintains the mean of the output lose to 0 and its standard deviation close to 1. The final mixture ensity layer gives a probability distribution defined in the range of ossible inner slopes for a given galaxy subset and transform our ouble channel CNN into an MDCNN. This probability distribution s understood as a posterior under the prior distribution of inner slopes ith which the network has trained, which allows us to evaluate the ncertainty of the individual predictions of the model.\nhis paper has been typeset from a T E X/L A T E X file prepared by the author."
        }
    ],
    "title": "A probabilistic deep learning model to distinguish cusps and cores in dwarf galaxies",
    "year": 2023
}