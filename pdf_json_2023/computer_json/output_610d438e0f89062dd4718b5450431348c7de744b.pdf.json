{
    "abstractText": "Semantic segmentation, which aims to classify every pixel in an image, is a key task in machine perception, with many applications across robotics and autonomous driving. Due to the high dimensionality of this task, most existing approaches use local operations, such as convolutions, to generate perpixel features. However, these methods are typically unable to effectively leverage global context information due to the high computational costs of operating on a dense image. In this work, we propose a solution to this issue by leveraging the idea of superpixels, an over-segmentation of the image, and applying them with a modern transformer framework. In particular, our model learns to decompose the pixel space into a spatially low dimensional superpixel space via a series of local cross-attentions. We then apply multi-head self-attention to the superpixels to enrich the superpixel features with global context and then directly produce a class prediction for each superpixel. Finally, we directly project the superpixel class predictions back into the pixel space using the associations between the superpixels and the image pixel features. Reasoning in the superpixel space allows our method to be substantially more computationally efficient compared to convolution-based decoder methods. Yet, our method achieves state-of-the-art performance in semantic segmentation due to the rich superpixel features generated by the global self-attention mechanism. Our experiments on Cityscapes and ADE20K demonstrate that our method matches the state of the art in terms of accuracy, while outperforming in terms of model parameters and latency.",
    "authors": [
        {
            "affiliations": [],
            "name": "Alex Zihao Zhu"
        },
        {
            "affiliations": [],
            "name": "Jieru Mei"
        },
        {
            "affiliations": [],
            "name": "Siyuan Qiao"
        },
        {
            "affiliations": [],
            "name": "Hang Yan"
        },
        {
            "affiliations": [],
            "name": "Yukun Zhu"
        },
        {
            "affiliations": [],
            "name": "Liang-Chieh Chen"
        },
        {
            "affiliations": [],
            "name": "Henrik Kretzschmar"
        }
    ],
    "id": "SP:c84aeb7d9df05f3530d376776d39f36820c301bc",
    "references": [
        {
            "authors": [
                "Radhakrishna Achanta",
                "Appu Shaji",
                "Kevin Smith",
                "Aurelien Lucchi",
                "Pascal Fua",
                "Sabine S\u00fcsstrunk"
            ],
            "title": "Slic superpixels compared to stateof-the-art superpixel methods",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2012
        },
        {
            "authors": [
                "Nicolas Carion",
                "Francisco Massa",
                "Gabriel Synnaeve",
                "Nicolas Usunier",
                "Alexander Kirillov",
                "Sergey Zagoruyko"
            ],
            "title": "End-to-end object detection with transformers",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Jieneng Chen",
                "Yongyi Lu",
                "Qihang Yu",
                "Xiangde Luo",
                "Ehsan Adeli",
                "Yan Wang",
                "Le Lu",
                "Alan L Yuille",
                "Yuyin Zhou"
            ],
            "title": "Transunet: Transformers make strong encoders for medical image segmentation",
            "year": 2021
        },
        {
            "authors": [
                "Liang-Chieh Chen",
                "George Papandreou",
                "Iasonas Kokkinos",
                "Kevin Murphy",
                "Alan L Yuille"
            ],
            "title": "Semantic image segmentation with deep convolutional nets and fully connected crfs",
            "venue": "In ICLR,",
            "year": 2015
        },
        {
            "authors": [
                "Liang-Chieh Chen",
                "George Papandreou",
                "Iasonas Kokkinos",
                "Kevin Murphy",
                "Alan L Yuille"
            ],
            "title": "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2017
        },
        {
            "authors": [
                "Liang-Chieh Chen",
                "George Papandreou",
                "Florian Schroff",
                "Hartwig Adam"
            ],
            "title": "Rethinking atrous convolution for semantic image segmentation",
            "year": 2017
        },
        {
            "authors": [
                "Liang-Chieh Chen",
                "Yi Yang",
                "Jiang Wang",
                "Wei Xu",
                "Alan L Yuille"
            ],
            "title": "Attention to scale: Scale-aware semantic image segmentation",
            "year": 2016
        },
        {
            "authors": [
                "Liang-Chieh Chen",
                "Yukun Zhu",
                "George Papandreou",
                "Florian Schroff",
                "Hartwig Adam"
            ],
            "title": "Encoder-decoder with atrous separable convolution for semantic image segmentation",
            "year": 2018
        },
        {
            "authors": [
                "Bowen Cheng",
                "Anwesa Choudhuri",
                "Ishan Misra",
                "Alexander Kirillov",
                "Rohit Girdhar",
                "Alexander G Schwing"
            ],
            "title": "Mask2former for video instance segmentation",
            "year": 2022
        },
        {
            "authors": [
                "Bowen Cheng",
                "Maxwell D Collins",
                "Yukun Zhu",
                "Ting Liu",
                "Thomas S Huang",
                "Hartwig Adam",
                "Liang-Chieh Chen"
            ],
            "title": "Panoptic-DeepLab: A Simple, Strong, and Fast Baseline for Bottom-Up Panoptic Segmentation",
            "year": 2020
        },
        {
            "authors": [
                "Bowen Cheng",
                "Ishan Misra",
                "Alexander G Schwing",
                "Alexander Kirillov",
                "Rohit Girdhar"
            ],
            "title": "Masked-attention mask transformer for universal image segmentation",
            "year": 2022
        },
        {
            "authors": [
                "Bowen Cheng",
                "Alex Schwing",
                "Alexander Kirillov"
            ],
            "title": "Per-pixel classification is not all you need for semantic segmentation",
            "venue": "In NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Dorin Comaniciu",
                "Peter Meer"
            ],
            "title": "Mean shift: A robust approach toward feature space analysis",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2002
        },
        {
            "authors": [
                "Marius Cordts",
                "Mohamed Omran",
                "Sebastian Ramos",
                "Timo Rehfeld",
                "Markus Enzweiler",
                "Rodrigo Benenson",
                "Uwe Franke",
                "Stefan Roth",
                "Bernt Schiele"
            ],
            "title": "The cityscapes dataset for semantic urban scene understanding",
            "year": 2016
        },
        {
            "authors": [
                "Ekin D Cubuk",
                "Barret Zoph",
                "Dandelion Mane",
                "Vijay Vasudevan",
                "Quoc V Le"
            ],
            "title": "Autoaugment: Learning augmentation policies from data",
            "year": 2019
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "In ICLR,",
            "year": 2020
        },
        {
            "authors": [
                "Brian Fulkerson",
                "Andrea Vedaldi",
                "Stefano Soatto"
            ],
            "title": "Class segmentation and object localization with superpixel neighborhoods",
            "venue": "In ICCV,",
            "year": 2009
        },
        {
            "authors": [
                "Raghudeep Gadde",
                "Varun Jampani",
                "Martin Kiefel",
                "Daniel Kappler",
                "Peter V Gehler"
            ],
            "title": "Superpixel convolutional networks using bilateral inceptions",
            "venue": "In ECCV,",
            "year": 2016
        },
        {
            "authors": [
                "Bharath Hariharan",
                "Pablo Arbel\u00e1ez",
                "Ross Girshick",
                "Jitendra Malik"
            ],
            "title": "Hypercolumns for object segmentation and fine-grained localization",
            "venue": "In CVPR,",
            "year": 2015
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "year": 2016
        },
        {
            "authors": [
                "Xuming He",
                "Richard S Zemel",
                "Miguel A Carreira-Perpin\u00e1n"
            ],
            "title": "Multiscale conditional random fields for image labeling",
            "venue": "In CVPR,",
            "year": 2004
        },
        {
            "authors": [
                "Gao Huang",
                "Yu Sun",
                "Zhuang Liu",
                "Daniel Sedra",
                "Kilian Q Weinberger"
            ],
            "title": "Deep networks with stochastic depth",
            "venue": "In ECCV,",
            "year": 2016
        },
        {
            "authors": [
                "Varun Jampani",
                "Deqing Sun",
                "Ming-Yu Liu",
                "Ming-Hsuan Yang",
                "Jan Kautz"
            ],
            "title": "Superpixel sampling networks",
            "venue": "In ECCV,",
            "year": 2018
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "In ICLR,",
            "year": 2015
        },
        {
            "authors": [
                "Philipp Kr\u00e4henb\u00fchl",
                "Vladlen Koltun"
            ],
            "title": "Efficient inference in fully connected crfs with gaussian edge potentials",
            "venue": "In NeurIPS,",
            "year": 2011
        },
        {
            "authors": [
                "L\u2019ubor Ladick\u1ef3",
                "Chris Russell",
                "Pushmeet Kohli",
                "Philip HS Torr"
            ],
            "title": "Associative hierarchical crfs for object class image segmentation",
            "venue": "In ICCV,",
            "year": 2009
        },
        {
            "authors": [
                "John Lafferty",
                "Andrew McCallum",
                "Fernando CN Pereira"
            ],
            "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
            "venue": "In ICML,",
            "year": 2001
        },
        {
            "authors": [
                "Yann LeCun",
                "L\u00e9on Bottou",
                "Yoshua Bengio",
                "Patrick Haffner"
            ],
            "title": "Gradient-based learning applied to document recognition",
            "venue": "Proceedings of the IEEE,",
            "year": 1998
        },
        {
            "authors": [
                "Zhiqi Li",
                "Wenhai Wang",
                "Enze Xie",
                "Zhiding Yu",
                "Anima Anandkumar",
                "Jose M Alvarez",
                "Ping Luo",
                "Tong Lu"
            ],
            "title": "Panoptic segformer: Delving deeper into panoptic segmentation with transformers",
            "year": 2022
        },
        {
            "authors": [
                "Ze Liu",
                "Yutong Lin",
                "Yue Cao",
                "Han Hu",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Baining Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Zhuang Liu",
                "Hanzi Mao",
                "Chao-Yuan Wu",
                "Christoph Feichtenhofer",
                "Trevor Darrell",
                "Saining Xie"
            ],
            "title": "A convnet for the 2020s",
            "year": 2022
        },
        {
            "authors": [
                "Stuart Lloyd"
            ],
            "title": "Least squares quantization in pcm",
            "venue": "IEEE transactions on information theory,",
            "year": 1982
        },
        {
            "authors": [
                "Jonathan Long",
                "Evan Shelhamer",
                "Trevor Darrell"
            ],
            "title": "Fully convolutional networks for semantic segmentation",
            "venue": "In CVPR,",
            "year": 2015
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "In ICLR,",
            "year": 2019
        },
        {
            "authors": [
                "Prajit Ramachandran",
                "Niki Parmar",
                "Ashish Vaswani",
                "Irwan Bello",
                "Anselm Levskaya",
                "Jon Shlens"
            ],
            "title": "Stand-alone self-attention in vision models",
            "venue": "In NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "Jianbo Shi",
                "Jitendra Malik"
            ],
            "title": "Normalized cuts and image segmentation",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2000
        },
        {
            "authors": [
                "Robin Strudel",
                "Ricardo Garcia",
                "Ivan Laptev",
                "Cordelia Schmid"
            ],
            "title": "Segmenter: Transformer for semantic segmentation",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Zhi Tian",
                "Chunhua Shen",
                "Hao Chen"
            ],
            "title": "Conditional convolutions for instance segmentation",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "In NeurIPS,",
            "year": 2017
        },
        {
            "authors": [
                "Huiyu Wang",
                "Yukun Zhu",
                "Hartwig Adam",
                "Alan Yuille",
                "Liang- Chieh Chen"
            ],
            "title": "Max-deeplab: End-to-end panoptic segmentation with mask transformers",
            "year": 2021
        },
        {
            "authors": [
                "Huiyu Wang",
                "Yukun Zhu",
                "Bradley Green",
                "Hartwig Adam",
                "Alan Yuille",
                "Liang-Chieh Chen"
            ],
            "title": "Axial-deeplab: Stand-alone axial-attention for panoptic segmentation",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Xiaolong Wang",
                "Ross Girshick",
                "Abhinav Gupta",
                "Kaiming He"
            ],
            "title": "Non-local neural networks",
            "venue": "In CVPR,",
            "year": 2018
        },
        {
            "authors": [
                "Xinlong Wang",
                "Rufeng Zhang",
                "Tao Kong",
                "Lei Li",
                "Chunhua Shen"
            ],
            "title": "Solov2: Dynamic and fast instance segmentation",
            "venue": "In NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Mark Weber",
                "Huiyu Wang",
                "Siyuan Qiao",
                "Jun Xie",
                "Maxwell D Collins",
                "Yukun Zhu",
                "Liangzhe Yuan",
                "Dahun Kim",
                "Qihang Yu",
                "Daniel Cremers"
            ],
            "title": "Deeplab2: A tensorflow library for deep labeling",
            "year": 2021
        },
        {
            "authors": [
                "Tete Xiao",
                "Yingcheng Liu",
                "Bolei Zhou",
                "Yuning Jiang",
                "Jian Sun"
            ],
            "title": "Unified perceptual parsing for scene understanding",
            "venue": "In ECCV,",
            "year": 2018
        },
        {
            "authors": [
                "Enze Xie",
                "Wenhai Wang",
                "Zhiding Yu",
                "Anima Anandkumar",
                "Jose M Alvarez",
                "Ping Luo"
            ],
            "title": "Segformer: Simple and efficient design for semantic segmentation with transformers",
            "venue": "NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Jiarui Xu",
                "Shalini De Mello",
                "Sifei Liu",
                "Wonmin Byeon",
                "Thomas Breuel",
                "Jan Kautz",
                "Xiaolong Wang"
            ],
            "title": "Groupvit: Semantic segmentation emerges from text supervision",
            "year": 2022
        },
        {
            "authors": [
                "Qihang Yu",
                "Huiyu Wang",
                "Dahun Kim",
                "Siyuan Qiao",
                "Maxwell Collins",
                "Yukun Zhu",
                "Hartwig Adam",
                "Alan Yuille",
                "Liang-Chieh Chen"
            ],
            "title": "Cmtdeeplab: Clustering mask transformers for panoptic segmentation",
            "year": 2022
        },
        {
            "authors": [
                "Qihang Yu",
                "Huiyu Wang",
                "Siyuan Qiao",
                "Maxwell Collins",
                "Yukun Zhu",
                "Hartwig Adam",
                "Alan Yuille",
                "Liang-Chieh Chen"
            ],
            "title": "k-means mask transformer",
            "year": 2022
        },
        {
            "authors": [
                "Yuhui Yuan",
                "Xilin Chen",
                "Jingdong Wang"
            ],
            "title": "Object-contextual representations for semantic segmentation",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Wenwei Zhang",
                "Jiangmiao Pang",
                "Kai Chen",
                "Chen Change Loy"
            ],
            "title": "K-net: Towards unified image segmentation",
            "venue": "In NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Yifan Zhang",
                "Bo Pang",
                "Cewu Lu"
            ],
            "title": "Semantic segmentation by early region proxy",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Hengshuang Zhao",
                "Jianping Shi",
                "Xiaojuan Qi",
                "Xiaogang Wang",
                "Jiaya Jia"
            ],
            "title": "Pyramid scene parsing network",
            "year": 2017
        },
        {
            "authors": [
                "Sixiao Zheng",
                "Jiachen Lu",
                "Hengshuang Zhao",
                "Xiatian Zhu",
                "Zekun Luo",
                "Yabiao Wang",
                "Yanwei Fu",
                "Jianfeng Feng",
                "Tao Xiang",
                "Philip HS Torr"
            ],
            "title": "Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers",
            "year": 2021
        },
        {
            "authors": [
                "Bolei Zhou",
                "Hang Zhao",
                "Xavier Puig",
                "Sanja Fidler",
                "Adela Barriuso",
                "Antonio Torralba"
            ],
            "title": "Scene parsing through ade20k dataset",
            "year": 2017
        },
        {
            "authors": [
                "Xizhou Zhu",
                "Weijie Su",
                "Lewei Lu",
                "Bin Li",
                "Xiaogang Wang",
                "Jifeng Dai"
            ],
            "title": "Deformable detr: Deformable transformers for end-to-end object detection",
            "venue": "In ICLR,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "I. INTRODUCTION\nThe problem of semantic segmentation, or classifying every pixel in the image, is increasingly common in many robotics applications. A dense, fine-grained, understanding of the world is necessary for navigation in cluttered environments, particularly for applications such as autonomous driving, where scene understanding is deeply safety-critical. On the other hand, many robotics systems are combinations of highly complex and specialized systems, and latency is an everpresent issue for real time operation.\nThe balance between safety and performance and latency is critical for modern robotic systems. While the state of the art in semantic segmentation is able to achieve strong performance in terms of metrics such as mean Intersection over Union (mIoU), many methods still rely on dense decoders which produce predictions for every pixel in the scene. As a result, these methods tend to be relatively expensive, and arguably produce a lot of redundant computation for nearby pixels that are often very similar.\n1Waymo LLC 2Johns Hopkins University (Work done as an intern at Waymo) 3Google Research 4ByteDance Research (Work done while at Google Research) 5Work done while at Waymo \u2217Equal contributions\nTo address this issue, we aim to bring the classical ideas surrounding superpixels into modern deep learning. The premise of using superpixels is to decompose and over-segment the image into a series of irregular patches. By grouping similar pixels into superpixels and then operating on the superpixel level, one can significantly reduce the computational cost of dense prediction tasks, such as semantic segmentation. Classical superpixel algorithms, such as SLIC [1], however, rely on hard associations between each image pixel and superpixel. This makes it hard to embed the superpixel representation into neural network architectures [29] as this association is not differentiable for back propagation. Recent works, such as superpixel sampling networks (SSN) [24], resolve this issue by turning the hard association into a soft one. While this is a step towards incorporating superpixels into neural networks, their segmentation quality still lags behind other models that adopt the per-pixel or per-mask representation.\nIn this work, we propose a novel architecture that aims to revive the differentiable superpixel generation pipeline in a modern transformer framework [40]. In place of the iterative clustering algorithms used in SLIC [1] and SSN [24], we propose to learn the superpixel representation by developing a series of local cross-attentions between a set of learned superpixel queries and pixel features. The outputs of crossattention modules act as the superpixel features, directly used for semantic segmentation prediction. As a result, the proposed transformer decoder effectively converts object queries to superpixel features, enabling the model to learn the superpixel representation end-to-end.\nOperating on the superpixel level provides a number of\nar X\niv :2\n30 9.\n16 88\n9v 2\n[ cs\n.C V\n] 2\nO ct\n2 02\n3\nnotable benefits. Conventional pixel-based approaches are limited by the high dimensionality of the pixel space, making global self-attention computationally intractable. Numerous approaches such as axial attention [42] or window attention [31] have been developed to work around these issues by relaxing the global attention to a local one. By oversegmenting the image into a small set of superpixels, we are able to efficiently apply global self-attention on the superpixels, providing full global context to the superpixel features, even when reasoning about high-resolution images. Despite applying global self-attention (vs conventional convolutional neural networks) in our model, our method is more efficient than existing methods due to the low dimensionality of the superpixel space. Finally, we directly produce semantic classes for the superpixel features, and then back-project the predicted classes onto the image space using the superpixelpixel associations.\nWe perform extensive evaluations on the Cityscapes [14] and ADE20K [56] datasets, where our method matches stateof-the-art performance, but at significantly lower computational cost.\nIn summary, the main contributions of this work are as follows:\n\u2022 The first work that revives the superpixel representation in the modern transformer framework, where the object queries are used to learn superpixel features. \u2022 A novel network architecture that uses local crossattention to significantly reduce the spatial dimensionality of pixel features to a small set of superpixel features, enabling learning the global context between them and the direct classification of each superpixel. \u2022 A superpixel association and unfolding scheme that projects each superpixel class prediction back to a dense pixel segmentation, discarding the CNN pixel decoder. \u2022 Experiments on the Cityscapes and ADE20k datasets, where our method outperforms the state of the art at substantially lower computational cost."
        },
        {
            "heading": "II. RELATED WORK",
            "text": "Superpixels for Segmentation Before the deep learning era, the superpixel representation, paired with graphical models, was the main paradigm for image segmentation. Superpixel methods [33], [37], [13], [1] are usually used in the pre-processing step to reduce the computation cost. A shallow classifier, e.g., SVM [15], predicts the semantic labels of each superpixel [18], which aggregates hand-crafted features. The graphical models, particularly conditional random fields [28], are then employed to refine the segmentation results [22], [27], [26].\nConvNets for Segmentation Convolutional neural networks (ConvNets) [29] deployed in a fully convolutional manner [34] performs semantic segmentation by pixel-wise classification. Typical ConvNet-based approaches include the DeepLab series [4], [6], [8], PSPNet [54], UPerNet [46], and OCRNet [51]. Alternatively, there are some works [19], [24] that employ superpixels to aggregate features extracted by ConvNets and show promising results.\nTransformers for Segmentation Transformers [40] and their vision variants [17] have been adopted as the backbone encoders for image segmentation [7], [55], [3]. Transformer encoders can be instantiated as augmenting ConvNets with self-attention modules [43], [42]. When used as stand-alone backbones [36], [17], [31], [47], they also demonstrate strong performance compared to the previous ConvNet baselines.\nTransformers are also used as the decoders [2] for image segmentation. A popular design is to generate masks embedding vectors from object queries and then multiply them with the pixel features to generate masks [39], [44]. For example, MaX-DeepLab [41] proposes an end-to-end mask transformer framework that directly predicts class-labeled object masks. Segmenter [38] and MaskFormer [12] tackle semantic segmentation from the view of mask classification. K-Net [52] generates segmentation masks by a group of learnable kernels. Inspired by the similarity between mask\ntransformers and clustering algorithms [33], clustering-based mask transformers are proposed to segment images [49], [48], [50]. Deformable transformer [57] is also used for improving the image segmentation as in Panoptic SegFormer [30] and Mask2Former [11].\nSimilar to this work, Region Proxy [53] (RegProxy) also incorporates the idea of superpixels into a deep segmentation network by using a CNN decoder to learn the association between each pixel and superpixel. However, RegProxy uses features on the regular pixel grid to represent each superpixel, and on which to apply self-attention. In comparison, we apply a set of learned weights, which correspond to each superpixel, and use cross-attention with the pixel features to compute the pixel-superpixel associations. Our experiments demonstrate that our methodology provides significant performance improvements.\nIn summary, all of the prior works that apply transformers to segmentation have, in some way, relied on a dense CNN decoder to generate the final dense features, and then combined these features with an attention mechanism to improve performance. Our method, in comparison, uses cross attention to reduce the image into a small set of superpixels, and only applies self-attention in this superpixel space in the decoder. This allows our method to operate on a significantly lower dimensional space (often 322\u00d7 smaller than the image resolution), while utilizing the benefits that come with global self-attention to achieve state-of-the-art performance."
        },
        {
            "heading": "III. METHOD",
            "text": "Our proposed Superpixel Transformer architecture, summarized in Figure 2, consists of four main components:\n1) Pixel Feature Extraction: A convolutional encoder backbone to generate hypercolumn features. 2) Superpixel Tokenization: A series of local dual-path cross-attentions, between a set of learned queries and pixel features, to generate a set of superpixel features. 3) Superpixel Classification: A series of multi-head selfattention layers to refine the superpixel features and produce a semantic class for each superpixel. 4) Superpixel Association: Associating the predicted superpixel classes and pixel features to obtain the final dense semantic segmentation.\nWe detail each component in the following subsections."
        },
        {
            "heading": "A. Pixel Feature Extraction",
            "text": "Typical convolutional neural networks, such as ResNet [21] and ConvNeXt [32], are employed as the encoder backbone. On top of the encoder output, we apply a multi-layer perceptron (MLP), and bilinear resize to the features after stage-1 (stride 2), stage-3 (stride 8), and stage-5 (stride 32). The multi-scale features are combined with addition to form hypercolumn features [20]. Each pixel feature is represented by their corresponding hypercolumn features, which are fed to the following Superpixel Tokenization module."
        },
        {
            "heading": "B. Superpixel Tokenization",
            "text": "Before introducing our proposed Superpixel Tokenization module, we briefly review the previous works on Differentiable SLIC [1], [24], which we modernize with the transformer framework.\nPreliminary: Differentiable SLIC Simple Linear Iterative Clustering (SLIC) [1] adopts the classical iterative k-means algorithm [33] to generate superpixels by clustering pixels based on their features (e.g., color similarity and location proximity). Given a set of pixel features Ip and initialized superpixel features S0i at iteration 0, the algorithm iterates between two steps at iteration t:\n1) (Hard) Assignment: Compute the similarity Qtpi between each pixel feature Ip and superpixel feature Sti . Assign each pixel to a single superpixel based on its maximum similarity. 2) Update: Update the superpixel features Sti based on the pixels features assigned to it.\nThe Superpixel Sampling Networks (SSN) [24] make the whole process differentiable by replacing the hard assignment between each pixel and superpixel with a soft weight:\nQtpi =e \u2212\u2225Ip\u2212St\u22121i \u2225 2\n(1)\nSti = 1\nZti n\u2211 p=1 QtpiIp, (2)\nwhere Zti = \u2211 p Qpi t is the normalization constant. In practice, at t = 0, the superpixel features are initialized as the mean feature within a set of rectangular patches that are evenly distributed in the image. In order to reduce the computational complexity and to apply a spatial locality constraint, the distance computation \u2225Ip \u2212 St\u22121i \u22252 is restricted to a local 3\u00d7 3 superpixel neighborhood around each pixel, although larger window sizes are possible.\nSuperpixel Tokenization We propose to unroll the SSN iterations and replace the k-means clustering steps with a set of local cross-attentions. We initialize the superpixel features, which are distributed on a regular grid in the image, (Figure 3) with a set of randomly-initialized, learnable queries, S0i , and perform the superpixel update step using cross-attention\nbetween superpixel features and pixel features by adapting the dual-path cross-attention [41], giving\nSti =S t\u22121 i + \u2211 p\u2208N (i) softmaxp(qSt\u22121i \u00b7 kIt\u22121p )vIt\u22121p (3)\nItp =I t\u22121 p + \u2211 i\u2208N (p) softmaxi(qIt\u22121p \u00b7 kSt\u22121i )vSt\u22121i , (4)\nwhere N (x) denotes the neighborhood of x and q, k and v are the query, key and value, generated applying a MLP to each respective feature plus an additive learned position embedding. For each superpixel neighborhood corresponding to a superpixel, we share the same set of position embeddings. For each superpixel Si, there are 9 \u00b7 h \u00b7 w pixel neighbors, where [h,w] is the size of the patch covered by one superpixel, while each pixel has 9 superpixel neighbors. We illustrate this neighborhood in Figure 3. The local dual-path cross-attention repeats n times to generate the output superpixel, Stni and pixel, Itnp , features.\nThis local dual-path cross-attention serves three purposes: \u2022 Reduce complexity compared to a full cross-attention. \u2022 Stabilize training, as the final softmax is only between\n9 superpixel features or 9 \u00b7 h \u00b7 w pixel features. \u2022 Encourage spatial locality of the superpixels, forcing\nthem to focus on a coherent, local over-segmentation."
        },
        {
            "heading": "C. Superpixel Classification",
            "text": "Given the updated superpixel features from the Superpixel Tokenization module, we directly predict a class for each superpixel using a series of self-attentions. In particular, we apply k multi-head self-attention (MHSA) layers [40] to learn global context information between superpixels, producing outputs Fi. Performing MHSA on the superpixel features is significantly more efficient than on the pixel features, since the number of superpixels is much smaller. In our experiments, we typically use a superpixel resolution that is 322\u00d7 smaller than the input resolution. Finally, we apply a linear layer as a classifier, producing a semantic class prediction for each refined superpixel feature, Ci. As opposed to the CNN pixel decoders used in other approaches [10], [12], our superpixel class predictions Ci can be directly projected back to the final pixel-level semantic segmentation output without any additional layers, as described in Section III-D."
        },
        {
            "heading": "D. Superpixel Association",
            "text": "To project the superpixel class predictions back into the pixel space, we use the outputs of the Superpixel Tokenization module, Itnp and S tn i , to compute the association between each pixel and its 9 neighboring superpixels:\nQpi =softmaxi\u2208N (p)(Itnp \u00b7 S tn i ). (5)\nThe final dense semantic segmentation, Y , is then computed at each pixel, p, as the combination of each predicted superpixel class from the Superpixel Classification module, Ci, weighted by the above associations:\nYp = \u2211\ni\u2208N (p)\nQpi \u00b7 Ci. (6)\nDuring training, the dense semantic segmentation Y is supervised by the semantic segmentation ground truth."
        },
        {
            "heading": "IV. EXPERIMENTS",
            "text": "In this section, we evaluate the size, latency and accuracy of a small (ResNet-50 backbone) and large (ConvNeXt-L backbone) variant of our model against prior works, and provide ablations for the superpixel tokenization module and a fine grained latency analysis.\nWe evaluate our work on the Cityscapes [14] and ADE20K [56]. Cityscapes is a driving dataset, consisting of 5,000 high resolution street-view images, and 19 semantic classes. ADE20K is a general scene parsing dataset, consisting of 20,210 images with 150 semantic classes.\nA. Implementation Details\nPixel Feature Extraction The hypercolumn features are generated by applying a MLP to project each encoder feature to 256 channels, and bilinear resizing to stride 8.\nSuperpixel Tokenization For both datasets, we apply 2 sequential local dual-path cross-attention to generate the superpixel embeddings, each with 256 channels and 2 heads. We use a single set of learned position embeddings for each superpixel and pixel feature, initialized at 14 resolution of each feature, bilinear upsampled to the feature resolution and added to the feature.\nSuperpixel Classification 4 multi-head attention layers are applied in the superpixel classification stage, with 4 heads each, outputting 256 channels in each layer.\nSuperpixel Association The associations between the superpixel features and pixel features are computed at stride 8. The association is then bilinear upsampled to the input resolution before applying the softmax in (5).\nTraining Our training hyperparameters closely follow prior work such as [5], [50]. Specifically, we employ the polynomial learning rate scheduler, and the backbone learning rate multiplier is set to 0.1. The AdamW optimizer [25], [35] is used with weight decay 0.05. For regularization and augmentations, we use random scaling, color jittering [16], and drop path [23] with a keep probability of 0.8 (for ConvNeXt-L). We use a softmax cross-entropy loss, applied to the top 20% of pixels of the dense segmentation output.\nCityscapes Our models are trained with global batch size 32 over 32 TPU cores for 60k iterations. The initial learning rate is 10\u22123 with 5,000 steps of linear warmup. The model accepts the full resolution 1024\u00d7 2048 images as input, and produces 128 \u00d7 256 hypercolumn features (i.e., stride 8). Taking as input the hypercolumn features, the superpixel tokenization module uses 32\u00d7 64 superpixels.\nADE20K Our models are trained with global batch size 64 over 32 TPU cores and crop size 640\u00d7 640. Our Resnet50 and ConvNeX-L models are trained for 100k and 150k iterations, respectively. The initial learning rate is 10\u22123 with 5,000 steps of linear warmup. 160\u00d7160 hypercolumn features are generated, and 40\u00d7 40 superpixels are used."
        },
        {
            "heading": "B. Results",
            "text": "1) Cityscapes Dataset: Table I compares the results of our Superpixel Transformer model to other transformer-based state-of-the-art models for semantic segmentation on the Cityscapes val set. In these experiments, we compare models with backbones roughly the same size as ResNet-50 and ConvNeXt-L. In addition to other semantic segmentation models, we also compare against the state of the art panoptic segmentation method, kMaX-DeepLab [50]. While this method is trained on a slightly different task, we find that, for computational cost, it is the most fair comparison, as our training schedule and pipeline is most similar to theirs.\nWith the smaller ResNet-50 backbone, our model is roughly half the size and latency of kMaX-DeepLab, while improving upon mIoU by 0.6 against the previous SOTA, RegProxy [53]. As the ResNet-50 backbone is relatively small, most existing models are dominated by the size of their decoders, which allows our model\u2019s reduction in decoder size to have significant impacts on the overall size and performance, with a 70% improvement in FPS compared to kMaX-DeepLab. In addition, we expect to see this effect even more for even smaller backbones.\nFor comparison, we also provided results with a larger ConvNeXt-L backbone. Here, our model has a similar absolute reduction in params and FLOPs, as compared to the equivalent kMaX-DeepLab model. However, as the model is largely dominated by the size of the backbone, the overall improvements are more modest. Nonetheless, our\nmodel is able to achieve near state-of-the-art performance, especially compared to the prior semantic segmentation methods, where we outperform most of the prior works, except for Mask2Former, where we are within 0.2mIoU. We note that, for this comparison, the equivalent Mask2Former [9] model is pre-trained with a significantly larger ImageNet-22k dataset, whereas our model is pre-trained on ImageNet-1k.\nFigure 4 provides qualitative examples of our ConvNeXt-L model. The semantic segmentation predictions suggest that the model is able recover thin structures, such as poles, despite largely operating in a 32\u00d764 superpixel space.\nWe also provide a visualization of the learned superpixels. We convert the soft association in Section III-D to a hard assignment by selecting the argmax over superpixels, i:\nQ\u0304p =argmaxi\u2208N (p)Qpi (7)\nWe visualize the boundaries of these assignments overlaid on top of the input image in the left-most column of Figure 4.\nFrom these visualizations, we can see that, despite the model being trained with a soft association, the superpixels generated by the hard assignment tightly follow the boundaries in the image. We note that this is particularly interesting as we do not provide any direct supervision to the superpixel associations, and instead these are learned implicitly by the network. In addition, we find that these boundaries tend to be more faithful to the edges of an object than the labels.\n2) ADE20K Dataset: We provide quantitative results on the ADE20K dataset in Table II. We choose one of the most\ncommonly used crop sizes (640\u00d7640) and provide results for the ResNet-50 backbone.\nFor ADE20K, we achieve the highest FPS and the second lowest # params (behind the surprisingly small RegProxy model), while outperforming RegProxy.\nHowever, we do note that the gap in performance is larger for ADE20K than Cityscapes. Our hypothesis is that the large number of classes in ADE20K (152) results in ambiguities when a pixel could belong to multiple classes. This results in inconsistencies for object classes, and in particular where\nboundaries are drawn (see Figure 5 for examples). As our superpixel tokenization module operates before any semantic prediction, and each superpixel query only operates on a local neighborhood in the pixel space, the model must learn a consistent way to divide the image into a set of superpixels. When the label boundaries are inconsistent, our model is less able to effectively learn this over-segmentation, leading to a small decrease in performance.\n3) Superpixel Tokenization Ablation: In Table III, we provide an ablation of the number of cross attention layers\nand the superpixel resolution, evaluated using our ResNet-50 backbone model on Cityscapes.\nFrom these experiments, we find that increasing the number of cross attention layers from 1 to 2 improves mIoU significantly by 2.4. Further increasing the number of these layers may have further improvements in performance, although we are currently bottlenecked by accelerator memory constraints. This is largely due to our naive TensorFlow implementation [45], which we discuss in Section IV-C.2.\nReducing the number of superpixels to 16\u00d732 also has a significant regression in mIoU of 5.5. On the other hand, increasing the superpixels to 64\u00d7128 also results in a mild regression of 0.7. We posit that this is because our pixel features used in the association are at stride 8 (128\u00d7256). As a result, having 64\u00d7128 superpixels provides each local-cross attention with a neighborhood of 128/64\u00d7 3 = 6\u00d76 pixels, which makes the receptive field for each superpixel too small to learn the oversegmentation as effectively. This can be resolved by increasing size of the neighborhood around each superpixel, but at significantly higher computational cost."
        },
        {
            "heading": "C. Discussion",
            "text": "1) Superpixel Quality: Despite our network not being trained with any explicit superpixel-based loss, we find that the associations learned by the network closely resemble classical superpixels. That is, the superpixels are aligned such that they follow the dominant edges in the image. We posit that this is due to the limited receptive field for each superpixel\u2019s cross attention. As any single superpixel may not have visibility to all of the pixels for a given mask, the model must use local edges and boundaries to separate the\nsuperpixels. 2) Latency: As seen in Tables I and II, our method provides a significant improvement in FPS. As the main processing of our model operates on the small superpixel space, this allows for a large reduction in model complexity, while achieving state of the art performance.\nHowever, we believe that there is still a large further reduction in latency available. In particular, the local cross attention operation is not efficient for standard accelerators and native TensorFlow or PyTorch implementations. This is because it requires a sliding window with overlapping patches, but with different operands in each patch (as opposed to convolutions where the weights are the same for all patches). However, as we operate on the superpixel grid level (32\u00d764 for Cityscapes and 40\u00d740 for ADE20K), this impact of this inefficiency is low enough to make our method overall faster compared to methods which operate on the dense pixel space.\nNonetheless, the Superpixel Tokenization module takes up the majority of the decoder runtime, as can be seen in Table IV, where we provide timing information for our method with a ResNet-50 backbone on a 1024\u00d72048 input. Our experiments are currently designed with a relatively naive, pure TensorFlow implementation, and involves the duplication of each superpixel or pixel 9 times (depending on the crossattention operation). We believe that a CUDA implementation could remove this redundant copy, and provide even further speedups for our method."
        },
        {
            "heading": "V. CONCLUSIONS",
            "text": "We presented a novel network architecture for semantic segmentation that leverages superpixels to project the dense image segmentation problem into a low dimensional superpixel space. Operating on this space enables us to significantly reduce the size and inference latency of our network compared to prior works, while achieving state-of-the-art performance."
        }
    ],
    "title": "Superpixel Transformers for Efficient Semantic Segmentation",
    "year": 2023
}