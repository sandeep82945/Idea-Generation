{
    "abstractText": "There has been significant progress in sensing, perception, and localization for automated driving, However, due to the wide spectrum of traffic/road structure scenarios and the long tail distribution of human driver behavior, it has remained an open challenge for an intelligent vehicle to always know how to make and execute the best decision on road given available sensing / perception / localization information. In this chapter, we talk about how artificial intelligence and more specifically, reinforcement learning, can take advantage of operational knowledge and safety reflex to make strategical and tactical decisions. We discuss some challenging problems related to the robustness of reinforcement learning solutions and their implications to the practical design of driving strategies for autonomous vehicles. We focus on automated driving on highway and the integration of reinforcement learning, vehicle motion control, and control barrier function, leading to a robust AI driving strategy that can learn and adapt safely .",
    "authors": [
        {
            "affiliations": [],
            "name": "Subramanya Nageshrao"
        },
        {
            "affiliations": [],
            "name": "Yousaf Rahman"
        },
        {
            "affiliations": [],
            "name": "Vladimir Ivanovic"
        },
        {
            "affiliations": [],
            "name": "Mrdjan Jankovic"
        },
        {
            "affiliations": [],
            "name": "Eric Tseng"
        },
        {
            "affiliations": [],
            "name": "Michael Hafner"
        },
        {
            "affiliations": [],
            "name": "Dimitar Filev"
        }
    ],
    "id": "SP:8913d14402f6ac9fa72bd070478ad41a496c1472",
    "references": [
        {
            "authors": [
                "J Ackermann",
                "T B\u00fcnte"
            ],
            "title": "Robust prevention of limit cycles for robustly decoupled car steering dynamics",
            "year": 1999
        },
        {
            "authors": [
                "M Alshiekh",
                "R Bloem",
                "R Ehlers",
                "B K\u00f6nighofer",
                "S Niekum",
                "U Topcu"
            ],
            "title": "Safe reinforcement learning via shielding",
            "year": 2017
        },
        {
            "authors": [
                "AD Ames",
                "X Xu",
                "JW Grizzle",
                "P Tabuada"
            ],
            "title": "Control Barrier Function Based Quadratic Programs for Safety Critical Systems",
            "venue": "IEEE Transactions on Automatic Control",
            "year": 2016
        },
        {
            "authors": [
                "S Aradi"
            ],
            "title": "Survey of deep reinforcement learning for motion planning of autonomous vehicles",
            "venue": "IEEE Transactions on Intelligent Transportation Systems",
            "year": 2020
        },
        {
            "authors": [
                "M Buehler",
                "K Iagnemma",
                "S Singh"
            ],
            "title": "The DARPA urban challenge: autonomous vehicles",
            "venue": "in city traffic,",
            "year": 2009
        },
        {
            "authors": [
                "C Chen",
                "A Seff",
                "A Kornhauser",
                "J Xiao"
            ],
            "title": "Deepdriving: Learning affordance for direct perception in autonomous driving",
            "venue": "Computer Vision (ICCV),",
            "year": 2015
        },
        {
            "authors": [
                "RC Coulter"
            ],
            "title": "Implementation of the pure pursuit path tracking algorithm",
            "venue": "Tech. rep., Carnegie-Mellon UNIV Pittsburgh PA Robotics INST",
            "year": 1992
        },
        {
            "authors": [
                "H Duda"
            ],
            "title": "Flight control system design considering rate saturation. Aerospace science and technology",
            "year": 1998
        },
        {
            "authors": [
                "J Erdmann"
            ],
            "title": "Lane-changing model in sumo. Proceedings of the SUMO2014 modeling mobility with open data",
            "year": 2014
        },
        {
            "authors": [
                "P Falcone",
                "M Tufo",
                "F Borrelli",
                "J Asgari",
                "HE Tseng"
            ],
            "title": "A linear time varying model predictive control approach to the integrated vehicle dynamics control problem in autonomous systems",
            "venue": "46th IEEE Conference on Decision and Control,",
            "year": 2007
        },
        {
            "authors": [
                "J Garcia",
                "F Fern\u00e1ndez"
            ],
            "title": "Safe exploration of state and action spaces in reinforcement learning",
            "venue": "Journal of Artificial Intelligence Research",
            "year": 2012
        },
        {
            "authors": [
                "J Garc\u0131a",
                "F Fern\u00e1ndez"
            ],
            "title": "A comprehensive survey on safe reinforcement learning",
            "venue": "Journal of Machine Learning Research",
            "year": 2015
        },
        {
            "authors": [
                "S Hecker",
                "D Dai",
                "L Van Gool"
            ],
            "title": "End-to-end learning of driving models with surround-view cameras and route planners",
            "venue": "European Conference on Computer Vision (ECCV)",
            "year": 2018
        },
        {
            "authors": [
                "M Jankovic"
            ],
            "title": "Robust Control Barrier Functions for Constrained Stabilization of Nonlinear Systems",
            "venue": "Automatica",
            "year": 2018
        },
        {
            "authors": [
                "IG Jin",
                "SS Avedisov",
                "CR He",
                "WB Qin",
                "M Sadeghpour",
                "G Orosz"
            ],
            "title": "Experimental validation of connected automated vehicle design among human-driven vehicles. Transportation research part C: emerging technologies",
            "year": 2018
        },
        {
            "authors": [
                "A Kesting",
                "M Treiber",
                "D Helbing"
            ],
            "title": "General lane-changing model mobil for car-following models",
            "venue": "Transportation Research Record",
            "year": 2007
        },
        {
            "authors": [
                "E Kim",
                "J Kim",
                "M Sunwoo"
            ],
            "title": "Model predictive control strategy for smooth path tracking of autonomous vehicles with steering actuator dynamics",
            "venue": "International Journal of Automotive Technology",
            "year": 2014
        },
        {
            "authors": [
                "DP Kingma",
                "J Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "year": 2014
        },
        {
            "authors": [
                "E Kreyszig"
            ],
            "title": "Advanced engineering mathematics. 482\u2013488. ny jonh wiley & sons",
            "year": 1993
        },
        {
            "authors": [
                "S Lee",
                "HE Tseng"
            ],
            "title": "Trajectory planning with shadow trolleys for an autonomous vehicle on bending roads and switchbacks",
            "venue": "IEEE Intelligent Vehicles Symposium (IV),",
            "year": 2018
        },
        {
            "authors": [
                "N Li",
                "DW Oyler",
                "M Zhang",
                "Y Yildiz",
                "I Kolmanovsky",
                "AR Girard"
            ],
            "title": "Game theoretic modeling of driver and vehicle interactions for verification and validation of autonomous vehicle control systems. IEEE Transactions on control systems technology",
            "year": 2017
        },
        {
            "authors": [
                "TP Lillicrap",
                "JJ Hunt",
                "A Pritzel",
                "N Heess",
                "T Erez",
                "Y Tassa",
                "D Silver",
                "D Wierstra"
            ],
            "title": "Continuous control with deep reinforcement learning",
            "year": 2015
        },
        {
            "authors": [
                "YC Lin",
                "ZW Hong",
                "YH Liao",
                "ML Shih",
                "MY Liu",
                "M Sun"
            ],
            "title": "Tactics of adversarial attack on deep reinforcement learning agents",
            "year": 2017
        },
        {
            "authors": [
                "X Ma",
                "K Driggs-Campbell",
                "MJ Kochenderfer"
            ],
            "title": "Improved robustness and safety for autonomous vehicle control with adversarial reinforcement learning",
            "venue": "IEEE Intelligent Vehicles Symposium (IV),",
            "year": 2018
        },
        {
            "authors": [
                "AL Maas",
                "AY Hannun",
                "AY Ng"
            ],
            "title": "Rectifier nonlinearities improve neural network acoustic models",
            "venue": "In: Proc. icml,",
            "year": 2013
        },
        {
            "authors": [
                "MM Minderhoud",
                "PH Bovy"
            ],
            "title": "Extended time-to-collision measures for road traffic safety assessment",
            "venue": "Accident Analysis & Prevention",
            "year": 2001
        },
        {
            "authors": [
                "B Mirchevska",
                "C Pek",
                "M Werling",
                "M Althoff",
                "J Boedecker"
            ],
            "title": "Highlevel decision making for safe and reasonable autonomous lane changing using reinforcement learning",
            "venue": "21st International Conference on Intelligent Transportation Systems (ITSC),",
            "year": 2018
        },
        {
            "authors": [
                "V Mnih",
                "K Kavukcuoglu",
                "D Silver",
                "AA Rusu",
                "J Veness",
                "MG Bellemare",
                "A Graves",
                "M Riedmiller",
                "AK Fidjeland",
                "G Ostrovski"
            ],
            "title": "Human-level control through deep reinforcement learning",
            "year": 2015
        },
        {
            "authors": [
                "V Mnih",
                "AP Badia",
                "M Mirza",
                "A Graves",
                "T Lillicrap",
                "T Harley",
                "D Silver",
                "K Kavukcuoglu"
            ],
            "title": "Asynchronous methods for deep reinforcement",
            "year": 2016
        },
        {
            "authors": [
                "S Nageshrao",
                "HE Tseng",
                "D Filev"
            ],
            "title": "Autonomous highway driving using deep reinforcement learning",
            "venue": "IEEE International Conference on Systems, Man and Cybernetics (SMC),",
            "year": 2019
        },
        {
            "authors": [
                "Q Ngyuen",
                "K Sreenath"
            ],
            "title": "Exponential Control Barrier Functions for Enforcing High Relative-degree Safety-Critical Constraints",
            "venue": "American Control Conference,",
            "year": 2016
        },
        {
            "authors": [
                "I Papadimitriou",
                "M Tomizuka"
            ],
            "title": "Fast lane changing computations using polynomials",
            "venue": "Proceedings of the 2003 American Control Conference, 2003., IEEE,",
            "year": 2003
        },
        {
            "authors": [
                "D Pathak",
                "P Agrawal",
                "AA Efros",
                "T Darrell"
            ],
            "title": "Curiosity-driven exploration by self-supervised prediction",
            "venue": "In: International conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "S Prajna",
                "A Jadbabaie",
                "GJ Pappas"
            ],
            "title": "A Framework for Worst-case and Stochastic Safety Verification Using Barrier Certificates",
            "venue": "Transactions on Automatic Control",
            "year": 2007
        },
        {
            "authors": [
                "Y Rahman",
                "M Jankovic",
                "MA Santillo"
            ],
            "title": "2021) Driver intent prediction with barrier functions",
            "venue": "American Control Conference",
            "year": 2021
        },
        {
            "authors": [
                "R Rajamani"
            ],
            "title": "Vehicle Dynamics and Control. Springer Science & Business Media",
            "year": 2011
        },
        {
            "authors": [
                "T Schaul",
                "J Quan",
                "I Antonoglou",
                "D Silver"
            ],
            "title": "Prioritized experience replay",
            "year": 2015
        },
        {
            "authors": [
                "D Silver",
                "J Schrittwieser",
                "K Simonyan",
                "I Antonoglou",
                "A Huang",
                "A Guez",
                "T Hubert",
                "L Baker",
                "M Lai",
                "A Bolton"
            ],
            "title": "Mastering the game of go without human knowledge",
            "year": 2017
        },
        {
            "authors": [
                "A Tamar",
                "Y Glassner",
                "S Mannor"
            ],
            "title": "Optimizing the cvar via sampling",
            "venue": "In: Twenty-Ninth AAAI Conference on Artificial Intelligence",
            "year": 2015
        },
        {
            "authors": [
                "AL Thomaz",
                "C Breazeal"
            ],
            "title": "Teachable robots: Understanding human teaching behavior to build more effective robot learners",
            "venue": "Artificial Intelligence",
            "year": 2008
        },
        {
            "authors": [
                "T Toledo",
                "D Zohar"
            ],
            "title": "Modeling duration of lane changes",
            "venue": "Transportation Research Record: Journal of the Transportation Research Board",
            "year": 2007
        },
        {
            "authors": [
                "M Treiber",
                "A Kesting"
            ],
            "title": "Traffic flow dynamics. Traffic Flow Dynamics: Data, Models and Simulation",
            "year": 2013
        },
        {
            "authors": [
                "M Treiber",
                "A Hennecke",
                "D Helbing"
            ],
            "title": "Congested traffic states in empirical observations and microscopic simulations",
            "venue": "Physical review E",
            "year": 2000
        },
        {
            "authors": [
                "HE Tseng",
                "J Asgari",
                "D Hrovat",
                "P van Der Jagt",
                "A Cherry",
                "S Neads"
            ],
            "title": "Steering robot for evasive maneuvers-experiment and analysis",
            "venue": "IFAC Proceedings Volumes",
            "year": 2002
        },
        {
            "authors": [
                "A Vahidi",
                "A Eskandarian"
            ],
            "title": "Research advances in intelligent collision avoidance and adaptive cruise control. IEEE transactions on intelligent transportation systems",
            "year": 2003
        },
        {
            "authors": [
                "H Van Hasselt",
                "A Guez",
                "D Silver"
            ],
            "title": "Deep reinforcement learning with double q-learning",
            "venue": "In: AAAI,",
            "year": 2016
        },
        {
            "authors": [
                "G Vezzani",
                "A Gupta",
                "L Natale",
                "P Abbeel"
            ],
            "title": "Learning latent state representation for speeding up exploration",
            "year": 2019
        },
        {
            "authors": [
                "Y Wang",
                "D Shen",
                "EK Teoh"
            ],
            "title": "Lane detection using spline model. Pattern Recognition Letters",
            "year": 2000
        },
        {
            "authors": [
                "M Werling",
                "J Ziegler",
                "S Kammel",
                "S Thrun"
            ],
            "title": "Optimal trajectory generation for dynamic street scenarios in a frenet frame",
            "venue": "IEEE International Conference on Robotics and Automation,",
            "year": 2010
        },
        {
            "authors": [
                "P Wieland",
                "F Allg\u00f6wer"
            ],
            "title": "Constructive Safety Using Control Barrier Functions",
            "venue": "IFAC Proceedings Volumes",
            "year": 2007
        },
        {
            "authors": [
                "L Xiao",
                "F Gao"
            ],
            "title": "A comprehensive review of the development of adaptive cruise control systems. Vehicle system dynamics",
            "year": 2010
        },
        {
            "authors": [
                "W Xiao",
                "C Belta"
            ],
            "title": "Control Barrier Functions for Systems with High Relative Degree",
            "venue": "IEEE 58th Conference on Decision and Control (CDC),",
            "year": 2019
        },
        {
            "authors": [
                "H Xu",
                "Y Gao",
                "F Yu",
                "T Darrell"
            ],
            "title": "End-to-end learning of driving models from large-scale video datasets",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2017
        },
        {
            "authors": [
                "F Ye",
                "S Zhang",
                "P Wang",
                "CY Chan"
            ],
            "title": "A survey of deep reinforcement learning algorithms for motion planning and control of autonomous vehicles",
            "year": 2021
        },
        {
            "authors": [
                "M Zhang",
                "N Li",
                "A Girard",
                "I Kolmanovsky"
            ],
            "title": "A finite state machine based automated driving controller and its stochastic optimization",
            "venue": "ASME",
            "year": 2017
        },
        {
            "authors": [
                "S Zhang",
                "H Peng",
                "S Nageshrao",
                "E Tseng"
            ],
            "title": "Discretionary lane change decision making using reinforcement learning with model-based exploration",
            "venue": "IEEE International Conference On Machine Learning And Applications (ICMLA),",
            "year": 2019
        },
        {
            "authors": [
                "S Zhang",
                "H Peng",
                "S Nageshrao",
                "HE Tseng"
            ],
            "title": "Generating socially acceptable perturbations for efficient evaluation of autonomous vehicles",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Reinforcement learning (RL) is a key branch of Artificial Intelligence (AI) at the intersection of machine learning, decision making and control. It is a method of learning from interaction with the environment and it is inspired by the human learning process. RL gained success in the last few years as the right approach to learn how to make decisions in tasks that are too complex to explain or to learn from supervised examples. In 2015 DeepMind pioneered a new approach to RL, Deep Q-Network (DQN) that approximates the Q-function (Sutton and Barto, 1998) with a deep neural network (Mnih et al, 2015, 2016; Lillicrap et al, 2015; Silver et al, 2017). The key idea of the DQN algorithm is to store the agent\u2019s experiences in a replay buffer and then randomly sample and replay these experiences to provide diverse and decorrelated training data for learning the Q-function. The DQN concept was further extended to the other RL methods, e.g. Deep Deterministic Policy Gradient (DDPG) and Proximal Policy Optimization (PPO), resulting in a new subfield aptly termed as Deep Reinforcement Learning (DRL). Following the success of the DQN\n1 *Selected portions reprinted, with permission, from (Nageshrao et al, 2019), \u00a92019 IEEE\nar X\niv :2\n20 7.\n07 82\n9v 1\n[ cs\n.R O\n] 1\n6 Ju\nl 2 02\nmethodology, DRL has become one of the most used AI methods in the development of autonomous vehicles (AV). DRL can be an essential part of the prototypical AV technology stack (Sensing, Perception, Localization, Driving Policy and Actuation) as a method for decision making in the hierarchical driving policy workstream \u2013 Fig. 1.\nThe role of the decision making and motion planning module is to transform the information about the route and the state of the AV and surrounded traffic and environment into a high-level strategy \u2013 modulation of the speed set-point, merging, lane changing, car following, aborting the current maneuver, etc. \u2013 that is further implemented by the motion control system encompassing path planning, path following, and actuator control modules. Early AV research and prototypes utilized motion planning concepts derived from classical control techniques \u2013 finite state machines, rules, heuristic strategies and Model Predictive Control (MPC) (Buehler et al, 2007, 2009; Falcone et al, 2007; Kim et al, 2014; Zhang et al, 2017). Following the massive introduction of AI methods in AV development in the last years, we can observe a considerable growth in the share of the DRL based methods for motion planning (Aradi,\n2020). The rationale behind this trend is the ability of the DRL to handle complex and ill-defined situations and environments that often occur in autonomous driving setting. Presently, alternative versions and implementations of the DRL are at the core of the AV motion planning methods (e.g., detailed surveys of the DRL applications to AV can be found in (Aradi, 2020; Ye et al, 2021)). One of the challenging problems of the DRL based decision making and motion planning for AV is improving the robustness of the algorithms and preventing the possibility of unsafe actions and accidents. The essence of this problem is the probabilistic nature of the RL concept, which does not generally imply that the maximization of the reward function can guarantee the safety and repeatability of the solution. One common approach to improving DRL robustness, we call it the probabilistic robustness, is focused on enhancing the exploration strategy during the training of the DRL algorithms. The final goal is enriching the training set by introducing, through adversarial disturbances and non-reward based exploration, unexpected scenarios and situations well beyond the space defined by the simulation model/distribution and the handcrafted reward function. The methods of this group include model-based exploration (Pathak et al, 2017; Vezzani et al, 2019; Zhang et al, 2019), adversarial perturbations of the state observations (Lin et al, 2017), infusing disturbances that are modulated by the capability of the control policy (Ma et al, 2018), generating socially acceptable perturbations (Zhang et al, 2020), maximization of the rewards associated with the high risk trajectories (Tamar et al, 2015), etc. In general, the outcome of these techniques is improved robustness of the algorithms and consequentially reducing the level of risk, but without providing deterministic guaranties for solution. An alternative methodology proposed by (Alshiekh et al, 2017) introduces the idea of constraining (\u2018shielding\u2019) the output of the RL algorithm within a safety envelope that is defined by a deterministic decisionmaking strategy. Human crafted rules improving the safety of the DRL algorithm are applied in (Nageshrao et al, 2019, 2020) and (Mirchevska et al, 2018) in order to eliminate the unsafe actions produced by the DRL algorithm. On one side, the rules have the advantage of introducing human knowledge and experience in conjunction with the machine learning approach of developing the DRL. On the other side, the deterministic envelope defined by the rules might be too conservative and limited only to the specific situations considered by the designers. In this chapter we are extending the \u2018shielding\u2019 concept by combining the DRL decision making and motion planning algorithm with a generic deterministic algorithm that is aimed to define a safety boundary, called the \u2018safety filter\u2019 at the output of the DRL algorithm. Our work started with a rule-based safety filter (Nageshrao et al, 2020) with simple motion actuation, described in Section 2, continued with representative vehicle motion control (Section 3), and extended to safety filter for wide-ranging situations (Section 4 and Section 5). The wide-ranging safety filter design is inspired by the recent progress in the theory of Control Barrier Functions (CBF) as a generic and reliable methodology for object avoidance in robotic and automotive applications."
        },
        {
            "heading": "2 Decision Making: DRL Driving Strategy for Changing Lanes",
            "text": "To ensure automated driving is capable of operating in diverse environment including varying traffic density, different driving style and norms, we develop a novel Deep Reinforcement Learning framework. In this section we first provide a basic introduction to RL and Q-learning. Following this we elaborate on our hybrid approach to solve decision-making and control problems. We introduce essential modifications to classical Qlearning and show the necessity in incorporating basic safety rules. We demonstrate the effectiveness of our approach using a comparative simulation study for the highway driving problem. Our novel hybrid architecture can effectively handle complex and possibly ill-defined situations and environments."
        },
        {
            "heading": "2.1 Reinforcement learning and deep reinforcement learning an introduction",
            "text": "In this section we will provide a brief theoretical background on decision making using deep reinforcement learning. Additionally, we elaborate on the main motivation for using the hybrid control architecture presented in this chapter.\n2.1.1 Reinforcement learning In reinforcement learning (RL), an Agent learns an optimal policy for a given cost function by directly interacting with the environment. Broadly RL constitutes a set of algorithms that efficiently solve the sequential decision making problem for an underlying Markov decision process (MDP) (Sutton and Barto, 1998). An MDP is formally defined by a tuple \u3008S,A, T ,R\u3009. Where S \u2208 Rn is the state-space, A is the action space, T : S \u00d7 A \u2192 S is the state transformation function, and R : S \u00d7A\u00d7S \u2192 R is the reward function. At each discrete time step t the learning Agent selects an action at according to some policy \u03c0 for a given system state st , i.e., at = \u03c0(st) \u2208 A, where A is a set of feasible actions. On applying this action the system transitions to a new state st+1 \u2208 S, and provides a scalar reward rt+1. This process is repeated till the policy converges. It must be noted, both state transition T and policy \u03c0 can be stochastic. The goal of an RL algorithm is to learn a policy \u03c0 so as to maximize the total cumulative reward termed as return Rt = \u2211\u221e k=0 \u03b3\nkrt+k where the scalar constant \u03b3 \u2208 (0, 1] is the discount factor. For discrete action space the optimal policy is obtained by solving for the optimal Q function, termed as the action-value function. An optimal Q function satisfies the Bellman equation:\nQ\u03c0opt(st, at) = Q \u2217(st, at) = Est+1 [ rt + \u03b3max\nat+1 Q\u2217(st+1, at+1)|(st, at)\n] .\n(1)\nFor a given state st any action at that satisfies (1) is the optimal action.\n2.1.2 Decision making and low-level actuation Inspired by an example from (Sutton and Barto, 1998) (see Chapter 1.2), we elaborate on the difference between decision making (D\u2019s) and low level actuation (C\u2019s) : \u2013 Phil wants to have breakfast, he may chose between having cereal\nor a bagel (D1), after deciding he will either walk to cupboard or to the counter (C1). \u2013 He will then either pick a choice of cereal (D2) and then walk to the fridge (C2) to get milk or chose a bagel (D3) and walk to the toaster (C3), etc. This entire process involves a series of decision making followed by lowlevel actuation. High-level decisions (i.e., D\u2019s) are decided by reward function such as pleasure or getting enough nutrition etc. Any RL methodology that tries to solve both decision making and actuation simultaneously may require a large amount of training data. In this work we simplify this process by having a clear hierarchy between highlevel decision making and low level actuation. RL is used to solve the high-level decision making problem, while the classical feedback control methods are used for low-level actuation. Our approach has considerable overlap with hierarchical RL, where both decision making and control are learned simultaneously (Frans et al, 2017). To learn optimal highlevel decisions we use a modified Double DQN (DDQN) algorithm from (Van Hasselt et al, 2016).\n2.1.3 Need for robustness & safety The trade-off between exploration and exploitation is a key characteristic that distinguish RL from other forms of machine learning algorithms (Sutton and Barto, 1998). In an unknown situation an agent needs to be curious and explore new action. Hence during the initial learning phase the Agent will invariably explore all viable actions as the entire environment is unknown. Unfortunately this curiosity can be fatal and potentially expensive (Garcia and Ferna\u0301ndez, 2012). For example, during training, using unguided exploration could potentially lead to frequent collisions resulting in slow the training process. Additionally in the inference stage, due to perrception noise, function approximation etc. a trained agent may still potentially recommend a non-safe maneuver. In order to address these issues, we augment the DDQN decision maker with an explicit short-horizon safety check that is used both during training as well as in the inference phase. Safe exploration or safe-RL is an active research topic, for a detailed survey see (Garc\u0131a and Ferna\u0301ndez, 2015). A standard DRL approach such as DDQN, may require a lot of samples before learning that certain action to be potentially dangerous in certain states. For example, consider a highway where one of the lane has been barricaded, see Fig. 2a. A standard epsilon-greedy algorithm may need to collide multiple times before learning that in certain situations going to the left-most lane, for e.g. when it is blocked, to be catastrophic. This may result in significant waste of learning effort as the Agent will spend considerable amount of time exploring irrelevant regions of the state and action space. Additionally due to function approximation there is a small\nprobability where the trained DDQN Agent may still chose unsafe actions leading to a catastrophic outcome. This can be avoided by including an explicit short-horizon safety check that evaluates the action choice by the learning Agent and provides an alternative safe action whenever it is feasible (Alshiekh et al, 2017). Our first solution to this problem called the Rule Based Safety Filter (Nageshrao et al, 2019) (see Section 2.2) uses a simple safety check based on the common sense road rules. This forces the Agent to avoid nonsafe actions in dangerous situations resulting in a faster training. Our approach is similar to a teacher who provides corrective action when it is necessary (Thomaz and Breazeal, 2008). Note that the safety filter may not be optimal, i.e., an expert in the task under consideration. Additionally, due to explicit safety check, new data can be obtained even in the inference phase which can then be used for continuous adaption of the learned network, this is further elaborated in Section 2.4.3."
        },
        {
            "heading": "2.2 DRL for Autonomous Driving",
            "text": "The DRL architecture used in this work is given in Fig. 3.\nUnlike the mediated perception method that relies on complete reconstruction of scene prior (Xu et al, 2017; Hecker et al, 2018), we use the concept of affordance indicators, i.e., state variables based on the direct perception approach from (Chen et al, 2015). For a three lane highway\nscenario, the ego vehicle (EV) can be surrounded by up to six traffic vehicles (TV), see Fig. 2b.\n2.2.1 Affordance Indicators For Autonomous driving, the Agent uses information about its location within the road coordinate system and surrounding vehicles to select an action. This information is collectively termed as affordance indicators. The action taken by the Agent is then rewarded or penalized based on transitioned state. Cumulative reward is used by the Agent to learn safe and effective driving behavior. In this work, we use affordance indicators from 6 target vehicles in the vicinity of the ego vehicle. The 6 target vehicles are designated as Front Left, Front Center, Front Right, Rear Left, Rear Center and Rear Right. To define the traffic vehicle\u2019s variable we use the following notation\nStateLaneLocation&Direction (2)\nwhere State can be distance d or velocity v, Lane is either right r, center c, or left lane l. Location is either front f or rear r and Direction is either longitudinal x or lateral y. In total the following 24 indicators are used to represent the spatiotemporal information of the six nearest traffic vehicles (see Fig. 2b), they are formulated from the ego vehicle\u2019s perspective. Here the lane occupancy of a TV and the closest car to EV are obtained using the methodology presented in (Zhang et al, 2017). In addition to the 24 traffic vehicle states we use longitudinal velocity vex , lateral position dey , and lateral velocity vey of the ego vehicle e. Since the affordance indicators are formulated w.r.t. ego vehicle we do not need a state corresponding to longitudinal position of the ego vehicle. These variables are minimal requirement for highway driving, however they are not sufficient for all highway driving tasks such as use of restricted lane, on-ramp to enter the highway, off-ramp to exit, etc.\n2.2.2 High Level Decision and Feedback Control A total of 27 affordance indicators, i.e., st \u2208 R27 is used as an input to the deep Q-network. The Q-network is trained by using a modified double deep Qlearning algorithm from (Van Hasselt et al, 2016), see Algorithm 1. For highway driving decision making, we consider four longitudinal action choices: 1. maintain 2. accelerate 3. brake 4. hard brake and three lateral action choices: 1. keep lane 2. change lane to right 3. change lane to left. The combination results in set of 12 unique actions. For each of these action choice a numerical value can be assigned (Li et al, 2017), for example for longitudinal acceleration four different discrete choices {a1, 0,\u2212a1,\u2212a2}\nmay be considered. Alternatively, one can obtain the reference for the throttle or brake controller either using the intelligent driver model (IDM) (Treiber et al, 2000) or adaptive cruise control (Vahidi and Eskandarian, 2003; Rajamani, 2011). The reference for the steering controller is selfevident, it is either stay in lane or change lane to right or left. To obtain the front wheel angle we use a simple feedback controller (see Section 3.2 for details)\n\u03bacmd = f(kroad, eyoff , e\u03c8, TLC, vex) (3)\nwhere kroad is the road curvature, e\u03c8 is the heading angle offset, TLC is the desired time to complete a lane change, and the reference eyoff is the lateral offset to the desired position. When the DDQN Agent decides to perform a lane change, the absolute value of the lane offset eyoff will be set to the lane width. The inputs to the steering feedback controller (3) can be considered as additional affordance indicators which can be obtained from the perception module.\nAlgorithm 1 A DRL based safe decision maker for autonomous highway driving\n1: Initialize: BufS, BufC, Q(\u03b8) and target network Q\u0302(\u03b8\u0302) with \u03b8\u0302 = \u03b8 2: for episode =1,\u00b7 \u00b7 \u00b7 ,Ne, do 3: Initialize: {1, \u00b7 \u00b7 \u00b7 , NT} cars randomly, obtain affordance indicator s0 4: for samples t =1,\u00b7 \u00b7 \u00b7 ,NS, or Collision, do 5: With select random action at, else at = arg maxaQ ((st, a, \u03b8t)) 6: For ego car: If at is not safe Then store (st, at, \u2217, rcol) in BufC and replace at by safe action as 7: Apply action, observe st+1 and obtain rt+1 = \u03c1(st, st+1, at) 8: if Collision then 9: Store transition (st, at, \u2217, rcol) in collision buffer BufC\n10: else 11: Store transition (st, at, st+1, rt+1) in safe buffer BufS 12: end if 13: Sample random minibatch (sj , aj , sj+1, rj+1) from BufS and BufC 14: Set\nyj =\n{ rj+1 if sample is from BufC\nrj+1 + \u03b3Q\u0302 ( sj+1, arg maxaQ (sj+1, a, \u03b8t) , \u03b8\u0302t ) if sample is from BufS\n15: Perform gradient descent on \u2016yj \u2212Q(sj , aj , \u03b8t)\u20162 w.r.t. \u03b8 16: Every NC episodes set Q\u0302 = Q 17: end for 18: end for\n2.2.3 Rule Based Safety Filter As elaborated in Section 2.1.3, we use an explicit short-horizon safety check to validate the action choice\nby DDQN. For the current action choice, the safety filter verifies common sense but well known rules of the road such as ensuring a minimum relative gap to a TV based on relative velocity\ndTV \u2212 Tmin \u00d7 vTV > dTVmin (4)\nwhere dTV, vTV are the relative distance and velocity to a given traffic vehicle, Tmin is the minimum time to collision, dTVmin is the minimum gap which must be ensured before executing the action choice by the Agent. If this condition is not satisfied and when feasible, an alternate safe action will be provided by the short-horizon safety filter. In this example we use a simple variant of the intelligent driver model (IDM) (Treiber et al, 2000) to provide safe alternative longitudinal action, it is formulated as\nas =  Hard brake if TC \u2264 THB Brake if THB < TC \u2264 TB Maintain if TB < TC\n(5)\nwhere as is the safe action, TC is the calculated time to collision. It is defined as\nTC = dTV vTV\n(6)\n(Minderhoud and Bovy, 2001), THB and TB are the thresholds above which the decision made by the DDQN Agent is considered to be safe. In particular we use the following safety check prior to performing an action given by DDQN:\n1. Instead of the in-lane longitudinal action by DDQN, chose a safe action using (5) if (4) is not satisfied and the ego vehicle is faster than the preceding vehicle. 2. If the ego vehicle is in left most lane then change lane to left is not valid, similarly for the right lane. 3. For change lane to left continuously monitor (4) for center-front car and to the left-front and the left-rear car in the target lane. If condition (4) fails then lane change is either not initiated or aborted, similarly for the change lane to right.\nAfter training, generally the trained Agent is frozen and used only in inference mode. However, in reality the Agent may encounter new information, additionally there can be a considerable variation between the training environment and the real-world experience. Also due of function approximation there can be a small probability of choosing an unsafe action even by the trained Agent, this can happen even after convergence and in the absence of any explicit exploration. In order to address these issues, in the implementation phase we augment the trained DDQN Agent with the short-horizon safety check that was used during learning. Any new safety violation data will be added to the collision buffer BufC, by using the training part of the Algorithm 1 (line 13 to 15) the learned Agent can be re-trained or adapted in a continuous manner. In the following section we apply the developed DRL based decision making Algorithm 1 for autonomous highway driving."
        },
        {
            "heading": "2.3 Vehicle Dynamics",
            "text": "Throughout this chapter, we use different vehicle dynamics models based on the modeling requirements. In this section, we present the 3 models that are used in subsequent sections.\n2.3.1 Point Mass Model When using this model, each vehicle is modeled as a computationally efficient point-mass in discrete time. For longitudinal equations of motion we use a discrete-time double integrator, and for lateral motion we use a simple kinematic model.\nx(t+ 1) = x(t) + vx(t)\u2206t,\ny(t+ 1) = y(t) + vy(t)\u2206t, (7)\nvx(t+ 1) = vx(t) + ax(t)\u2206t,\nwhere t is the time index, \u2206t is the sampling time, x \u2208 R is the longitudinal position, y \u2208 R is the lateral position of the car, and vx \u2208 R is the longitudinal velocity of the vehicle. In (7), the external control inputs ax(t) and vy(t) represents the longitudinal acceleration and lateral velocity of the vehicle, respectively. We assume ax(t) to vary from nominal acceleration, to hard brake and is discretized into four values, i.e., ax = {a1, 0,\u2212a1,\u2212a2}, with a1 = 2m/s2 and a2 = 4m/s\n2. Only in case of emergency hard braking of ax = \u2212a2 is applied. The lateral velocity vy(t) provides a reference lane for the vehicle, we assume 5 seconds to complete a lane change action (Toledo and Zohar, 2007), with an option to abort at any sampling instance. In this work we use 1Hz sampling for the driving policy.\n2.3.2 Dynamic Bicycle Model In subsequent sections, we also use a continuous dynamic bicycle model for vehicle dynamics (Rajamani, 2011). This model is required when simulating vehicle dynamics at higher frequencies.\nx\u0307 = v cos(\u03c6+ \u03b2),\ny\u0307 = v sin(\u03c6+ \u03b2),\nv\u0307lon = g\u03b1\u2212 Faero \u2212mg sin(\u03b8r)\nv\u0307lat = Ff + Fr m \u2212 vlon\u03c6\u0307\n\u03c6\u0308 = FfLf \u2212 FrLr\nIz ,\n(8)\nwhere v is the velocity, \u03c6 is the heading angle, \u03b2 is the slip angle, Lf is the distance of the front wheel to the center of mass, Lr is the distance of the rear wheel to the center of mass, \u03b4 is the front wheel angle, vlon is the longitudinal velocity, vlat is the lateral velocity, g is the acceleration due to gravity, \u03b1 is the longitudinal acceleration request in g\u2019s, Faero is the aerodynamic drag, m is the mass of the vehicle, \u03b8r is the road grade, Ff is the lateral tire force on the front tire, Ff is the lateral tire force on the rear tire, and Iz is the yaw moment of inertia of vehicle. The lateral\ntire forces are calculated as follows\nFf = 2Cf(\u03b4 \u2212 \u03b8vf), Fr = 2Cr(\u2212\u03b8vr),\n\u03b8vf = arctan (vlat + Lf \u03c6\u0307\nvlon\n) ,\n\u03b8vr = arctan (vlat \u2212 Lr\u03c6\u0307\nvlon\n) ,\n(9)\nwhere Cf is the cornering stiffness of each front tire, Cr is the cornering stiffness of each rear tire, \u03b8vf is the front tire velocity angle and \u03b8vr is the rear tire velocity angle. In all simulations in Section 4, this model is used.\n2.3.3 Simplified Bicycle Model To aid in calculating the barrier dynamics in Sections 4 and 5, we use the simplified decoupled system dynamics shown below.\nx\u0307T = vT cos(\u03c6T)\u2212 vH cos(\u03c6H), v\u0307H = g\u03b1 y\u0307T = vT sin(\u03c6T)\u2212 vH sin(\u03c6H), \u03c6\u0307H = vH LH \u03b4,\n(10)\nwhere xT and yT are the relative x and y positions of the target center with respect to the ego center in road coordinates, \u03c6T and \u03c6H are the heading angles of the target and ego vehicles w.r.t. the road coordinate system, vT and vH are the absolute velocities of the target and ego respectively, LH is the wheelbase of the ego vehicle, and \u03b4 is the front wheel angle of the ego vehicle."
        },
        {
            "heading": "2.4 Simulation Results",
            "text": "In this section we will show the applicability of our DRL based decision making Algorithm 1 for autonomous highway driving. First, we will elaborate on the training environment and evaluate the learned policy.\n2.4.1 Training environment A schematic of the simulation environment used for training is given in Fig. 4. It is a three lane circular loop and is used to approximate an infinite stretch of straight highway. At the beginning of an episode, anywhere between {1, \u00b7 \u00b7 \u00b7 , NT} number of cars are placed randomly within a distance of 250m from the ego car. In this example we chose NT to be 30. During learning stage the ego car (for e.g., white Fusion in Fig. 4) uses an -greedy RL policy to make decisions, whereas for the traffic vehicles a combination of controllers from (Li et al, 2017) and (Zhang et al, 2017) are used along with an IDM controller (Treiber et al, 2000). Additionally the traffic vehicles can randomly chose to perform lane change. For the\ntraffic vehicles, the system parameters such as maximum velocity are randomly chosen. This is to ensure a diverse traffic scenario in training and evaluation. We assume that all the traffic vehicles take into account the relative distance and velocity to preceding vehicle before making a decision, i.e., they will not rear end the preceding car in the same lane. We use Algorithm 1 to train an Agent for decision making.\n2.4.2 Reward components for DRL training In order to train the policy \u03c0 we use a reward function \u03c1 that consists of a set driving goals for the ego car. It is formulated as a function of \u2013 Desired traveling speed subject to traffic condition (11), \u2013 Desired lane and lane offset subject to traffic condition (12), \u2013 Relative distance to the preceding car based on relative velocity (13),\nrv = e \u2212 (vex\u2212vdes)\n2\n10 \u2212 1, (11)\nry = e \u2212\n(dey\u2212ydes) 2\n10 \u2212 1, (12)\nrx = e\u2212 (dlead\u2212dsafe) 2 10dsafe \u2212 1 if ex < dsafe 0 otherwise,\n(13)\nwhere vex , dey , and dlead are the ego velocity, lateral position, and the longitudinal distance to the lead vehicle respectively. Similarly, vdes, ydes, and dsafe are the desired speed, lane position, and safe longitudinal distance to the lead vehicle respectively.\nFig. 5 gives an indicative plot of the reward functions (11)-(13), it is formulated assuming vdes = 30 m/s which can be achieved in the center lane i.e., ydes = 3.8m with a minimum safe distance dsafe = 40m. The desired values are based on the traffic condition and can change depending on the scenario. For slow/fast moving traffic the peak in Fig. 5c will be adjusted based on the traffic condition. In this work we penalize the ego vehicle if it cannot maintain a minimum time headway of at least 1.3 seconds.\nDuring learning, we evaluate the (partially) trained DRL controller every 100th episode. Fig. 6 shows the average reward per decision during the training phase. It takes nearly 2000 episodes for the Agent to converge. We train the DRL Agent for a total of 10000 episodes. Where each episode lasts until 200 samples or collision, whichever is earlier. Exploration is continuously annealed from 1 to 0.2 over first 7000 episodes and then kept constant for the remaining duration of learning. The Qnetwork is a deep neural network with 2 hidden layers each having 100 fully connected leaky ReLU\u2019s (Maas et al, 2013). We train the network using Adam optimizer (Kingma and Ba, 2014) with a fixed learning rate of 1e\u2212 4. For the highway driving task, the safety filter was found to be a key component for learning a meaningful policy. Fig. 6 shows the mean and confidence bound for training with and without safety filter over 200 training iterations of Algorithm 1. Training a standard DDQN Agent without explicit safety check could not learn a decent policy and always resulted in collision. Whereas DDQN with explicit safety check was able to converge to an optimal policy. Based on (11) - (13), the maximum reward an Agent can receive is zero per decision, the average reward\nper decision obtained by our trained DDQN Agent with safety check is around \u22120.025.\nIn Fig. 7 We evaluate our trained DDQN Agent to obtain average velocity with increase in traffic density. We compare this against modified safety filter from (5), the modification provides an acceleration command when the calculated time to collision TC is higher than TA. This is referred as IDM in Fig. 7. It must be noted IDM controller from (5) cannot initiate lane change, in order to address this we integrate IDM with SUMO lane change decision making from (Kesting et al, 2007) and (Erdmann, 2014). Fig. 7, clearly demonstrates advantage of RL for high level decision making when compared to model-based approaches. With the increase in traffic density both the trained DDQN Agent and the model-based lane change controller converges to IDM controller. This is anticipated since lane change is neither safe nor advantageous in higher traffic density. Use of two explicit buffers namely BufS and BufC in Algorithm 1 to store safe and non-safe transitions is simplified version of prioritized experience replay (PER) from (Schaul et al, 2015). Fig. 8 shows the mean and confidence bound for training with two buffers and PER over 200 training iterations of Algorithm 1. For the highway driving example using two explicit buffers provides marginally better policy when compared to PER. This can be due to clear bifurcation of safe and non-safe transitions.\n2.4.3 Continuous adaptation During the implementation phase, we replace the -greedy policy, i.e., \u03c0 in Algorithm 1 line 5 by the learned policy \u03c0. Whenever the control decision by DDQN fails the short-horizon safety check, buffer BufC is updated with additional data. Using a lower learning rate than the one used for training, Q-network can be retrained (line 13 until 16). Fig. 9 shows the continuous adaptation result over 30K episodes and is obtained by averaging the data over 10k episodes using a moving average filter. Because of filtering, the mean number of safety trigger increases over first 10k episodes and stays constant for no\nadaptation scenario whereas it monotonically decreases to a smaller value thanks to continuous adaptation. Even with continuous adaptation the mean number safety trigger never converges to zero, this may be due to\n1. Use of function approximation where a trained NN can potentially chose a non-safe action,\n2. Use of rigid and static safety rules."
        },
        {
            "heading": "2.5 Summary",
            "text": "In this section we provided an introduction to basics of RL, introduced our hybrid decision making \u2013 control architecture. We explained the need for including a safety filter during RL training and inference, we also showed its effectiveness in learning a driving policy. Although the rule-based safety filter is handcrafted and predetermined it significantly improved the learning speed. Additionally the number of safety interventions reduces as the agent learns to perform optimal actions."
        },
        {
            "heading": "3 Executing DRL Decision with Motion Control Algorithm",
            "text": "While the previous section illustrated the design methodology of Deep Reinforcement Learning (DRL) for a generic driving environment, it is best to train DRL with representative motion control that executes the DRL decision. This will ensure a higher fidelity of the vehicle - environment transition probability distribution for the DRL training. In this section, we discuss longitudinal and lateral motion control algorithms and their integration aimed at executing designated actions from AI DRL strategy."
        },
        {
            "heading": "3.1 Longitudinal Motion Control",
            "text": "Longitudinal motion control systems need to provide tracking of the desired vehicle speed while maintaining a safety gap/range relative to the selected target vehicle. This functionality is provided by modern adaptive cruise control (ACC) systems which compute the longitudinal acceleration command based on the actual and set vehicle speed (Vx and Vx.set), the actual and set range 2 (R and Rset), and range rate 3 (R\u0307)\nax.ACC.cmd = fACC(Vx.set, Rset, Vx, R, R\u0307) (14)\nThe commanded acceleration is delivered by the actuation layer through modulating propulsion or brake torque. The ACC mapping function fACC can be implemented in various ways. For example, (Rajamani, 2011) described an ACC mapping function that includes both car-following mode regulating steady state range/gap and speed-control mode responding to transient situations such as target vehicle cut-in or loss of its lead vehicle. The two modes were integrated through a calibrated rangerange rate (R\u2212 R\u0307) transition diagram. (Treiber and Kesting, 2013) proposed the Intelligent Driver Model (IDM) which has been widely used in traffic simulation environments and represents human car-following behavior observed on freeway and urban traffic. (Jin et al, 2018) implemented state ([R, R\u0307]) feedback controller imitating human driving behavior and experimented on connected automated vehicle. (Treiber and Kesting, 2013) further developed Improved IDM (IIDM) and the ACC Model to avoid unrealistically large deceleration when desired speed suddenly changes, e.g., when entering a reduced speed zone or when a vehicle cuts in. For more variety of ACC a reader is referred to the survey (Xiao and Gao, 2010) and references therein. To execute DRL\u2019s lane change decision, we leverage our production ACC design. This will avoid the need of complicated spatial-temporal optimization (Lee and Tseng, 2018). By adding scenario dependent smart \u2018target selection\u2019 to existing ACC design, the vehicle speed will change during the lane change maneuvers for a smooth transition into a new gap and a new lane speed. The elements considered in the scenarios include the lane change decision, the range and range rate of surrounding vehicles, target lane speed, and the status/prediction of lateral motion. The \u2018target selection\u2019 refers to the selected lead vehicle whose range and range-rate of the ACC mapping function (Eq. (14)) will be based on. For example, in case of staying in lane decision, which represents the base ACC functionality, the target vehicle selected is the lead vehicle of the current lane. In case of lane changing decision, the target selection is considered in two phases: before and after merging into target lane. In the first phase, both the lead vehicles in current lane and target lane will be fed into the ACC mapping function, with a calibratable desired range, and the commanded acceleration will take the minimum values of the two, thus ensuring safety. In the second phase (after merging into\n2 Range R is defined as the distance or gap between the ego and target vehicle, i.e., distance between the ego vehicle\u2019s front bumper and target vehicle\u2019s rear bumper. 3 Range rate R\u0307 is defined as relative speed between the ego and target vehicle.\nthe target lane), the algorithm reverts to the base ACC functionality, i.e., selecting the lead vehicle of the current (new) lane as the target. Therefore, with the scenario-based ACC always aware of the status of the lateral motion control (to be described next), smooth and humandriver-like motion control can be expected for lane change and lane keep decisions."
        },
        {
            "heading": "3.2 Lateral Motion Control",
            "text": "For lane centering and lane changing maneuvers, we first introduce the relevant vehicle-road kinematics. Fig. 10 shows the definition of coordinate frames and variables for vehicle road/lane level localization. The vehicle motion dynamics is considered with respect to the curvilinear Frenet-Serret coordinate frame (xr-yr). The states of the vehicle-road kinematics include the travel distance along the curvilinear coordinate s, the path offset ey, and the heading offset e\u03c8, while the curvature of the road is considered as an exogenous input, denoted by \u03baroad. Vehicle motion - the longitudinal velocity Vx, lateral velocity Vy, and yaw rate r - are described in the Cartesian coordinate vehicle body frame (x \u2212 y). Vehicle motion curvature \u03ba is defined as \u03ba = r/Vx. 4 Fig. 11 shows the system block diagram. The lateral motion controller performs lane localization and computes vehicle motion curvature command. The plant includes actuator and vehicle dynamics, as well as vehicle-road kinematics/geometry. Vehicle-Road kinematics in the Frenet-Serret coordinate frame is given by (Werling et al, 2010)\ne\u0307y = Vx sin e\u03c8 \u2212 Vy cos e\u03c8 (15a) e\u0307\u03c8 = \u03baroads\u0307\u2212 r (15b)\ns\u0307 = 1\n1\u2212 \u03baroadey (Vx cos e\u03c8 \u2212 Vy sin e\u03c8). (15c)\nSince the value of heading offset, road curvature, and lateral velocity are relatively small during the lane centering and lane change maneuvers on highways, the vehicle-road kinematics can be further simplified to the following\ne\u0307y = Vxe\u03c8 (16a) e\u0307\u03c8 = Vx(\u03baroad \u2212 \u03ba) (16b) s\u0307 = Vx. (16c)\nThe path offset (ey) dynamics from (\u03baroad \u2212 \u03ba) are represented by a double integrator with crossover frequency of |Vx|. The vehicle motion\n4 Alternatively, the curvature can be computed based on the front wheel angle \u03b4 as \u03ba = \u03b4/(L + KuVx) where L is the vehicle wheelbase, and Ku is the vehicle understeer gradient in units of rad-s2/m. The understeer gradient is defined as: Ku = mf/Cf \u2212mr/Cr where mf and mr are the front and rear axle mass, Cf and Cr are the front and rear axle cornering coefficients.\ncurvature \u03ba is the system input, whereas the road curvature \u03baroad is considered as a system disturbance.\nIn typical applications, the system states ey and e\u03c8 and road curvature \u03baroad are obtained through image processing of video frames taken from a front view camera and by fitting the lane markers to a 3rd order polynomial (Wang et al, 2000). Coefficients of these lane marker polynomials are then fused to obtain the coefficients of the lane center polynomial with respect to vehicle coordinate (x-y), referred to as the path polynomial, where\nypath = a0 + a1x+ a2x 2 + a3x 3. (17)\nIn this format, coefficients a0 and a1 are equivalent to the path offset (ey) and heading offset respectively (e\u03c8) (see Fig. 10). Coefficients a2 and a3 are related to the road curvature \u03baroad and curvature rate \u03ba\u2032road = d\u03baroad/dx. Based on vector differential calculus, curvature of a parametric curve y = y(x) in the x\u2212y plane is given by (Kreyszig, 1993)\n\u03ba(x) = |y\u2032\u2032|/(1 + y\u20322)3/2 (18)\nwhere y\u2032 = dy/dx. Computing y\u2032path and y \u2032\u2032 path from Eq. (17) and replacing with y\u2032 and y\u2032\u2032 in (18) gives\n\u03ba(x) = 2a2 + 6a3x\n[1 + (a1 + 2a2x+ 3a3x2)2]3/2 . (19)\nAssuming small heading offset a1 5, the curvature and curvature rate of the path (road) at the origin (x = 0) are equal to\n\u03baroad = 2a2 (20a) \u03ba\u2032road = 6a3 (20b)\n3.2.1 Lane Centering Control Vehicle-Road kinematics for lane centering applications is given by Eqs. (16a) and (16b) where the road curvature is considered as an exogenous disturbance which can be measured. The proposed curvature command (Eq. (21)) represent a generic lane centering algorithm which includes both a feedforward term of the road curvature and the feedback terms. As such, with different coefficient design, it can represent various well-known path following algorithms/approaches including Pure Pursuit (Coulter, 1992), Bezier curves based planning (Giersiefer et al, 2019), or lookahead point design (Tseng et al, 2002). Our contribution is to introduce a novel and heuristic calibration approach for any of the above path following approaches to achieve desired closed-loop system response. The generic control law is given by\n\u03bacmd = Kff (\u03baroad + VxTprev\u03ba \u2032 road)\ufe38 \ufe37\ufe37 \ufe38\nfeedforward\n+Kyey +K\u03c8e\u03c8\ufe38 \ufe37\ufe37 \ufe38 feedback\n(21)\nwhere Kff is the feedforward gain, Tprev is the preview time that can be effectively utilized to compensate for actuation delays, Ky is the path offset gain, and K\u03c8 is the heading offset gain. Substituting \u03ba in (16b) with \u03bacmd from (21) and combining with (16a) gives the closed-loop dynamics\n[ e\u0307y e\u0307\u03c8 ] = Vx [ 0 1 \u2212Ky \u2212K\u03c8 ] [ ey e\u03c8 ] +Vx(1\u2212Kff ) [ 0 0 1 VxTprev ] [ \u03baroad \u03ba\u2032road ] . (22)\nEigenvalues of this 2nd order system are\n\u03bb1,2 = VxK\u03c8\n2 \u00b1 Vx\n\u221a Ky \u221a K2\u03c8 4Ky \u2212 1. (23)\nTo design the state feedback gains in terms of desired closed-loop metrics such as the natural frequency and damping ratio, we compare eigenvalues (23) with eigenvalues of the 2nd order system expressed in terms of the natural frequency \u03c9n and damping ratio \u03b6\n5 For highway driving nominal value of a1 \u2264 5 deg hence the error introduce by the small heading offset assumption is \u2264 1%.\n\u03bb\u22171,2 = \u2212\u03c9n\u03b6 \u00b1 \u03c9n \u221a \u03b62 \u2212 1 (24)\nDerived feedback gains are given by\nKy = \u03c92n V 2x\n(25a)\nK\u03c8 = 2\u03b6\u03c9n Vx . (25b)\nInstead of the natural frequency as the tuning parameter, for lane centering applications it is convenient to use alternative time response metrics such as the transient time to reach 95% of the target value or 5% of the initial value. This time is referred to as the response time and denoted by Tr. For practical damping ratios in range between 0.7 and 0.9, relation between the transient time, closed-loop frequency and damping ratio can be obtained by numerical optimization (with coefficient of determination R2 = 0.99) and is given by\nTr \u2248 4.3\u03b6\n\u03c9n . (26)\nSolving (26) for \u03c9n by using (25a) and (25b) gives the feedback gains in terms of the transient time as\nKy = 18.5\u03b62\nT 2r V 2x (27a)\nK\u03c8 = 8.6\u03b62\nTrVx . (27b)\nFig. 12 illustrates vehicle response with non-zero initial conditions using the proposed controller for different choice of the design parameters.\n3.2.2 Lane Change Control Lane change control systems need to provide a smooth lateral shift of a vehicle from current to target lane within specified time and with small or zero overshoot. Typical duration of lane changes6 is in the range of 5 and 7 seconds with lateral accelerations amplitudes in the range of 0.4 and 0.8 m/s2 (for making a lane change on straight roads). One can divide the problem into explicit path planning and path following, or alternatively integrate both components from the perspective of smooth transitioning into a new lane center. The latter eliminates the need of specifying way points and enables the designer to reuse the lane centering control methodology described in the previous section. In the case of integrated controller, the path is switched from the current to the target lane which introduces a step change in the path offset ey. The lane centering controller will follow the new set point by driving the vehicle towards the target lane. In order to achieve the desired transient time and damping ratio, the controller gains are scheduled\n6 Duration is defined as time required to reach path offset relative to the target lane center that is withing the range of nominal lane centering oscillations in range of 0.1 to 0.2 m. For nominal lane width of 3.4 m, this corresponds to approximately 5% of the lane width or the path offset at the lane change start.\nby using (26)-(27b), where Tr corresponds to the lane change duration. Benefits of this controller are in the ease of tuning, adaptation to initial conditions or changing conditions which may arise from lane change aborts or driver interactions.\nSince this controller is linear, the initial curvature command in response to a step change in set point is large (see Fig. 12) creating a pronounced lateral acceleration and jerk. Although the actual acceleration and jerk will be partially filtered out by the actuation and vehicle dynamics, a systematic solution is proposed below.\nThe pronounced initial acceleration and jerk can be overcome by adding amplitude and rate limits to the state feedback part of the control input. The limits need to be sufficiently large in order not to affect the performance, e.g., not to cause oscillations or even limit cycle. Methods such as describing function analysis (Slotine et al, 1991; Ackermann and Bu\u0308nte, 1999; Duda, 1998) can be applied to ensure stability at the presence of calibrated rate and amplitude limits. We propose to utilize limits\nobtained from well-known quintic polynomial path planning (see, e.g., (Papadimitriou and Tomizuka, 2003)). Path in time domain described by the quintic polynomial is given by: y(t) = a0 + a1t + a2t 2 + a3t 3 + a4t 4 + a5t 5. For given initial and final conditions (tf is final time): y(0) = 0, y\u0307(0) = 0, y\u0308(0) = 0, y(tf ) = yf , y\u0307(tf ) = 0, y\u0308(tf ) = 0, the coefficients are equal to:a0 = 0, a1 = 0, a2 = 0, a3 = 10yf/t 3 f , a4 = \u221215yf/t4f , and a5 = 6yf/t5f . From the second derivative of the polynomial, time instances at which lateral acceleration peaks occur are t1,2 = (3 \u00b1 \u221a 3)/6tf \u21d2 t1 \u2248 0.21tf , t2 \u2248 0.79tf . Accordingly, the acceleration maximum is\nay.max = 5.77yf t2f\n(28)\nMaximum jerk occurs at the beginning and end with magnitude are equal to\njmax = 60yf t3f\n(29)\nObtained lateral acceleration and jerk limits7 provide comfortable maneuver yet are high enough to prevent oscillations or limit cycles which can be proved by nonlinear simulations or describing function analysis. Fig. 13 shows simulation results with amplitude and rate limiting of the curvature command. We see that although there is a major effect on the initial transient response, the target metrics in terms of the duration and damping ratio are preserved."
        },
        {
            "heading": "3.3 Summary",
            "text": "In this section, we presented longitudinal and lateral motion control, and how they work together to execute the decision made by higher level AI (DRL). For the longitudinal control, we introduced a smart \u2018target\u2019 selection logic to leverage production ACC design, and to facilitate the integration with lateral motion control for delivering smooth and humandriver-like lane change maneuvers. For the lateral control, we introduced a unified lane centering and lane changing algorithm with a general control algorithm formula that can be calibrated to represent various wellknown path following design methodologies. We further introduced a new calibration approach that can automatically adjust the state feedback gains to achieve desired system damping, closed loop bandwidth, or 0-95% set point response time. In addition, for lane change maneuvers, we integrated both path planning and path following aspects. Since the integrated control does not try to regulate the vehicle to specific waypoints, no explicit waypoints designation is required. It provides implicit, smooth, and seamless re-planning in case of unexpected vehicle motion which may arise from unintended steering movement, and in case of abrupt path offset change (set point) change in the event of lane change abort (to return to the original lane). We further recommended a way to\n7 For lane width of 3.4 m and lane change duration of 6 s, the limits are ay.max = 0.54m/s2 and jmax = 0.94m/s 3.\nanalyze and calibrate the limit of lateral acceleration and jerk \u2013 the nonlinearities purposely added to the controller for comfort and actuation constraints \u2013 for assuring both subjective and objective performance."
        },
        {
            "heading": "4 Generic Safety Filter Design with Control Barrier Functions",
            "text": "In this section we develop a more generic safety filter than the heuristic handcrafted rules utilized in Section 2, because the latter may not be sufficient for avoid all types of collisions. The performance of an Agent depends on what it sees during training. Since the handcrafted heuristic rules are designed with certain imminent collision threats in mind, they could miss corner cases in the real-world, and even in a simulation world with long tail distribution where simulated target vehicles behaving beyond the engineers\u2019 imagination. For this reason, the rule-based safety\nfilter can be \u2018brittle\u2019. Hence a collision-avoidance intervention mechanism that is not specific to certain ego-target vehicle poses/situations is developed here to serve as a generic safety filter. Control Barrier Functions (CBF) have been used as a method to compute minimally-invasive feedback control that satisfy various prescribed system constraints. For AVs, CBFs can provide a computationally efficient approach to avoid collision by correcting ego vehicle motion based on its relative position and velocity compared with surrounding road users. The CBF framework used here assumes that a nominal control signal u0 is computed by the autonomous driver, in this case an Agent. That signal is then evaluated by the CBF for safety. If u0 is safe, the CBF safety filter does not modify it. If it is deemed unsafe, the safety filter overrides the nominal control and calculates a minimally invasive safe control that can avoid collision. The advantage of using a CBF based collision avoidance system is that it provides a safe steering and longitudinal acceleration command that is as close as possible (in the Euclidean sense) to the nominal control. This results in a much less intrusive safety filter compared to the rule based safety filter from Section 2. However, it is not straightforward to decide if the actuation of braking or steering should be used, (or how the actuation should be combined/split) to best correct vehicle motion when CBF intervenes to avoid a collision. To arbitrate between steering and/or braking,we use Contextual Selection of Decoupled CBF. This algorithm addresses the above issue by taking advantage of the structure and rules of the road, as well as the knowledge of steering/braking efficacy for different situations based on physics. Therefore, the proposed logic and resulted methodology, will determine steering and braking action required to effectively and decisively impose decoupled longitudinal and lateral CBFs in order to avoid collisions. The algorithm is designed to use the same input structure, i.e. affordance indicators as the DRL Agent. In addition to the affordance indicators used by the Agent, the CBF safety filter also uses the heading with respect to the road of the six surrounding traffic vehicles. The algorithm has been verified in extensive simulation with varying traffic and road users."
        },
        {
            "heading": "4.1 Control Barrier Functions",
            "text": "Barrier Functions have been used to enhance system robustness (Prajna et al, 2007), while Control Barrier Functions (CBFs) have been used as a method to provide minimally-invasive feedback control that satisfies various constraints prescribed on that system (Wieland and Allgo\u0308wer, 2007; Ames et al, 2016; Jankovic, 2018; Xiao and Belta, 2019). Here we mix the two concepts because we would like to apply them to systems with control inputs that are outside of the ego (the entity doing the calculations) control. Consider a control affine system\nx\u0307 = f(x) + g(x)u, (30)\nwhere x \u2208 Rn and u \u2208 Rm is the control input. We assume that a control input u0 is computed by a driver (or an Agent/controller) to achieve some\nobjective, and it is known. Let us also assume that an additional control objective is to keep the state of the system in a closed admissible set C \u2282 Rn defined as\nC 4= {x \u2208 Rn : h(x) \u2265 0},\n\u2202C 4= x \u2208 Rn : h(x) = 0,\nInt(C) 4= x \u2208 Rn : h(x) > 0,\n(31)\nwhere h : Rn \u2192 R is a twice continuously differentiable function. In addition, we assume that Int(C) 6= \u2205, where Int(C). Fig. 14 shows an example Barrier Function h(x), the admissible set C and the boundary \u2202C. h(x) is relative degree 1 with respect to the control inputs if hx \u00b7\ng(x) 6= \u2205, where hx = \u2202h(x)\u2202x . For the system (30) with a given control input u\u0304 a function h(x) is a Barrier Function (Ames et al, 2016) with respect to the admissible set C if h(x) is relative degree 1 with respect to the control inputs and\nh\u0307(x, u\u0304) + l0h(x) \u2265 0, (32)\nwhere h\u0307(x, u\u0304) = hx \u00b7 f(x) + hx \u00b7 g(x)u\u0304. If a Barrier Function is relative degree 2 with respect to the control inputs (i.e. hx \u00b7g(x) \u2261 0, as is the case later in this chapter) we can \u2013 motivated by (Ngyuen and Sreenath, 2016) \u2013 consider the second order barrier constraint: h\u0308(x)+l1h\u0307(x)+l0h(x) \u2265 0. The parameters l1, l0 are selected such that the roots of the polynomial s2 + l1s + l0 are negative real. The forward invariant set is {x : h > 0 and hx \u00b7 f(x) > \u2212\u03bbih}, where \u03bbi is either one of the two roots of s2 + l1s + l0 = 0. A barrier function h(x) is a CBF with respect to the admissible set C if h is differentiable in Int(C), h(x)\u2192 0 as x\u2192 \u2202C, and\nfor x \u2208 Int(C)\nhx \u00b7 g(x)u\u0304 = 0 =\u21d2 hx \u00b7 f(x) + l0h(x) \u2265 0 (33)\nif h(x) is relative degree 1 with respect to the control inputs and\n\u2202(hx \u00b7 f(x)) \u2202x \u00b7 g(x)u\u0304 = 0 =\u21d2 \u2202(hx \u00b7 f(x)) \u2202x\n\u00b7 f(x) + l1(hx \u00b7 f(x)) + l0h(x) \u2265 0 (34)\nif h(x) is relative degree 2 with respect to the control inputs.\n4.1.1 Decoupled Barrier Functions for Autonomous Driving We define the longitudinal collision avoidance barrier functions as\nhx,F = xT \u2212 kvvH \u2212 dx,min \u2212 LH 2 \u2212 LT 2 , hx,R = \u2212xT \u2212 kvvT \u2212 dx,min \u2212 LH 2 \u2212 LT 2 ,\n(35)\nwhere the subscript F is for the forward barrier and the subscript R is for the rearward barrier. Similarly, we define the lateral collision avoidance barrier functions as\nhy,L = \u2212yT \u2212 dy,min + cbx2T, hy,R = yT \u2212 dy,min + cbx2T, (36)\nwhere the subscript L is for the barrier to the left of the ego, the subscript R is for the barrier to the right of the ego vehicle, LT is the length of the target vehicle, dx,min is the minimum longitudinal distance allowed between vehicle bumpers, dy,min is the minimum lateral distance allowed between vehicle centers, and cb is a coefficient that determines the amount of bowing in the lateral barrier. The bowing is meant to reduce the steering effort required to satisfy the collision avoidance constraint when the target is far away from the ego. The minimum lateral distance of the barrier dy,min is only enforced when the ego is driving alongside the ego (i.e xT = 0). The possible longitudinal and lateral barriers are illustrated in Fig. 15. The dotted red line shows the position which the front/rear/left/right side of the ego vehicle must maintain in order to satisfy the barrier constraint."
        },
        {
            "heading": "4.2 Calculation of Barrier Constraints",
            "text": "4.2.1 Longitudinal Barrier We use the simplified bicycle model (10) to compute h\u0307x,F , h\u0307x,R, h\u0308x,R, which are used to compute the constraints for the longitudinal barrier. Since hx,F is relative degree 1 with respect to the control input \u03b1, only h\u0307x,F is required to calculate the constraints for vehicles ahead of the ego vehicle.\nhx,F = xT \u2212 kvvH \u2212 dx,min \u2212 LH 2 \u2212 LT 2 ,\nh\u0307x,F(\u03b1) = \u2212gkv\u03b1+ ( vTcos(\u03c6T)\u2212 vHcos(\u03c6H) ) hx,R = \u2212xT \u2212 kvvT \u2212 dx,min \u2212\nLH 2 \u2212 LT 2 ,\nh\u0307x,R = ( vTcos(\u03c6T)\u2212 vHcos(\u03c6H) ) h\u0308x,R(\u03b1) = gcos(\u03c6H)\u03b1\n(37)\n4.2.2 Lateral Barriers We use (10) to compute h\u0307L, h\u0308L, h\u0307R, and h\u0308R which are used to compute the constraints for the lateral barriers. We treat the nominal acceleration \u03b10 as a disturbance into the system.\nhy,L(cb) = \u2212yT \u2212 dy,min + cbx2T,\nh\u0307y,L(cb) = vHsin(\u03c6H)\u2212 vTsin(\u03c6T) + 2cbxT(vTcos(\u03c6T)\u2212 vHcos(\u03c6H)) h\u0308y,L(\u03b10, \u03b4, cb) = ( cos(\u03c6H) + 2cbxTsin(\u03c6H) )v2H\u03b4 LH + ( sin(\u03c6H)\u2212 2cbxTcos(\u03c6H) ) g\u03b10\n+ 2cb ( vTcos(\u03c6T)\u2212 vHcos(\u03c6H) )2 ,\n(38) and\nhy,R(cb) = yT \u2212 dy,min + cbx2T,\nh\u0307y,R(cb) = vTsin(\u03c6T)\u2212 vHsin(\u03c6H) + 2cbxT(vTcos(\u03c6T)\u2212 vHcos(\u03c6H)), h\u0308y,R(\u03b10, \u03b4, cb) = ( 2cbxTsin(\u03c6H)\u2212 cos(\u03c6H) )v2H\u03b4 LH + ( \u2212 sin(\u03c6H)\u2212 2cbxTcos(\u03c6H) ) g\u03b10\n+ 2cb ( vTcos(\u03c6T)\u2212 vHcos(\u03c6H) )2 (39)\n4.2.3 Road Keeping Barriers We also use road keeping barriers that prevent the vehicle from veering off the drivable area while avoiding a collision whenever possible\nhRK =\n[ 3wl \u2212 WH2 \u2212 yH\nyH \u2212 WH2\n] , h\u0307RK = [ \u2212vH\u03c6H vH\u03c6H ] , h\u0308RK(\u03b4) =  \u2212v2Hcos(\u03c6H)\u03b4LH v2Hcos(\u03c6H)\u03b4\nLH  (40)\nwhere yH is the y-coordinate of the ego vehicle w.r.t. a lane attached frame where the y-coordinate of the right most lane line is 0, LH is the length of the ego vehicle, and wl is the lane width."
        },
        {
            "heading": "4.3 Contextual Selection of Decoupled CBF",
            "text": "In this section, we describe a method that takes advantage of the structure and rules of the road and knowledge of steering/braking efficacy for different situations to select which barrier constraints to enforce. We assume \u03b1S = \u03b10 +\u03b1CBF where \u03b10 is the nominal longitudinal acceleration that control provided by the virtual driver and \u03b1CBF is the correction computed by the collision avoidance algorithm. Similarly \u03b4S = \u03b40 + \u03b4CBF where \u03b40 is the nominal front wheel angle and \u03b4CBF is the correction computed by the collision avoidance algorithm. The goal is then to calculate \u03b1CBF and \u03b4CBF given \u03b10 and \u03b40 so that the constraints are not violated.\n4.3.1 Provisional Selection of Decoupled CBF The algorithm has two primary mechanisms for determining whether to use CBF constraints for a target vehicle. The first mechanism is to use the below constraints to determine whether a target vehicle is a threat to ego when only using the nominal acceleration and front wheel angle proposed by the DRL Agent\nCx = { h\u0307x,F(\u03b10) + l0,xhx,F \u2265 0, if xT \u2265 0 h\u0308x,R(\u03b10) + l1,xh\u0307x,R + l0,xhx,R \u2265 0, otherwise\nCy = h\u0308y(\u03b10, \u03b40, 0) + l1,yh\u0307y(0) + l0,yhy(0) \u2265 0.\n(41)\nThese constraints arise out of the longitudinal and lateral CBF. We use the first order longitudinal barrier constraint if xT \u2265 0 and the second order barrier constraint if xT < 0 because hx,F is relative degree 1 with respect to \u03b1 and hx,R is relative degree 2. In (41), the threat assessment is performed with \u03b10, \u03b40, and cb = 0. Once a vehicle has been determined as a threat, the next step is to determine whether to brake, steer, or to do both. This is determined by the relative distance and velocity of the obstacle as shown in Fig. 16. The x-axis shows the relative velocity and the y-axis shows the distance from the target. To arbitrate between steering and braking to avoid a collision, we use the the physical limits of the vehicle to determine whether to steer around and obstacle or to brake ahead of it. The minimum distances at which the ego vehicle can brake ahead of a target db and at which the ego vehicle can steer around a target ds are given by\ndb = \u2212 v2R\n2decmax , ds = \u221a 2dy,min ay,max vR,\nwhere vR = vH \u2212 vT, decmax is the maximum deceleration and ay,max is the maximum lateral acceleration of the ego vehicle. We define vcrit as the relative velocity where db = ds. In the green shaded area we choose to brake, in the blue shaded area we choose to steer, and in the red shaded area we choose to do both. The boundary between the green and blue regions is determined by vcrit. The second mechanism that determines whether to use CBF constraints is based on whether target vehicle is\nin a lane adjacent to the ego vehicle and within a predefined longitudinal threshold distance. We preemptively apply the lateral barriers for vehicles that are in either:\n\u2013 the ego lane,\n\u2013 the lane immediately adjacent to the ego lane and within 3 car lengths from the center of the ego vehicle,\n\u2013 the lane two lanes adjacent to the ego lane and within 2 car lengths from the center of the ego vehicle.\nFig. 17 shows a geometric interpretation of the logic. If the center of a target vehicle falls within the shaded area, the appropriate lateral barrier is enforced.\nUsing these two mechanisms, the barriers to be enforced are provisionally selected, as shown in Fig. 18. The process shown in Fig. 18 is performed for the closest front and rear targets in each lane. By considering all the vehicles nearby the ego, the collision avoidance algorithm can prevent a broad range of collisions and is not limited to forward collision avoidance. This barrier selection is provisional because in the case the selection cannot be enforced due to conflicting constraints, the algorithm systematically replaces lateral constraints with longitudinal constraints until a feasible solution is found.\nOnce the barriers have been provisionally selected, we designate the threat (i.e. barrier enforced through the threat assessment mechanism) with the minimum longitudinal distance as the primary obstacle iPO. For the primary obstacle, we provisionally activate both the left and right lateral barriers. This results in 2 distinct options, going to the right or the left of the primary obstacle. For all other lateral barriers, we choose the left or right barrier based on the location of the target w.r.t. the ego vehicle."
        },
        {
            "heading": "4.3.2 Computing \u03b1CBF and \u03b4CBF with Quadratic Programs",
            "text": "We calculate the safe longitudinal acceleration \u03b1S = \u03b1CBF + \u03b10 and the safe front wheel angle \u03b4S = \u03b4CBF + \u03b40 using 3 quadratic programs, QPyL , QPyR and QPx.\n4.3.2.1 QPyL and QPyR : We define QPyL as\narg min \u03b4 CBF,L ,sRK,ssat\n[ \u03b4CBF,L sRK ssat ] Qy  \u03b4CBF,LsRK ssat  , (42) subject to:\nh\u0308y,L,iPO(\u03b10, \u03b40 + \u03b4CBF,L) + l1,yh\u0307y,L,iPO + l0,yhy,L,iPO \u2265 0,\nh\u0308y,i(\u03b10, \u03b40 + \u03b4CBF,L) + l1,yh\u0307y,i + l0,yhy,i \u2265 0, \u2200i \u2208 Y, (43)\nh\u0308RK(\u03b40 + \u03b4CBF,L) + l1,RKh\u0307RK + l0,RKhRK + [ 1 1 ] sRK \u2265 0,\n\u03b4min \u2212 \u03b40 \u2264 \u03b4CBF,L + ssat, \u03b4CBF,L \u2212 ssat \u2264 \u03b4max \u2212 \u03b40,\nwhere Y is the set of target ID\u2019s for which the lateral barrier is enforced excluding the primary obstacle, and l1,y, l0,y, l1,RK, and l0,RK are tunable parameters that determine the sensitivity of the barriers. We define JyL as the optimal quadratic cost of QPyL . Similarly, we define QPyR as\narg min \u03b4 CBF,R ,sRK,ssat\n= [ \u03b4CBF,R sRK ssat ] Qy  \u03b4CBF,RsRK ssat  , (44) subject to:\nh\u0308y,R,iPO(\u03b10, \u03b40 + \u03b4CBF,R) + l1,yh\u0307y,R,iPO + l0,yhy,R,iPO \u2265 0,\nh\u0308y,i(\u03b10, \u03b40 + \u03b4CBF,R) + l1,yh\u0307y,i + l0,yhy,i \u2265 0, \u2200i \u2208 Y, (45)\nh\u0308RK(\u03b40 + \u03b4CBF,R) + l1,RKh\u0307RK + l0,RKhRK + [ 1 1 ] sRK \u2265 0,\n\u03b4min \u2212 \u03b40 \u2264 \u03b4CBF,R + ssat, \u03b4CBF,R \u2212 ssat \u2264 \u03b4max \u2212 \u03b40,\nand we define JyR as the optimal quadratic cost of QPyR . The distinction between QPyL and QPyR is that in QPyL , the left barrier is enforced for the primary obstacle and in QPyR the right barrier is enforced for the primary obstacle. The slack variable sRK allows the algorithm to momentarily leave the road surface if such an action is necessary to avoid a collision and return when it is safe to do so. Similarly, ssat allows the algorithm to find a solution to the QPs even in the event of actuator saturation.\n4.3.2.2 Final Selection of Lateral CBF: The procedure used for the final selection of the CBFs is shown in Fig. 19. After the provisional selection of the CBFs, we attempt to solve QPyL and QPyR . We define one or both QPyL and QPyR as infeasible if any of the lateral constraints conflict (one lateral constraint requires \u03b4 > 0 and another requires \u03b4 < 0). If either QPyL or QPyR is infeasible, we replace the provisional hy,i of the target with the lowest value of Cx,i with the longitudinal CBF hx,i, and resolve. This procedure is repeated until either a solution is found or all the provisional lateral barriers have been replaced by a longitudinal barrier. Note that both QPyL and QPyR may still be infeasible after all provisional lateral barriers have been replaced. Once QPyL and QPyR have either been solved or deemed infeasible, we select \u03b4CBF based on the following criteria. \u2013 If QPyL is infeasible, \u03b4CBF = \u03b4CBF,R \u2013 If QPyR is infeasible, \u03b4CBF = \u03b4CBF,L \u2013 If QPyL and QPyR are both feasible, \u2022 if |JyL \u2212 JyR | < 1e\u2212 5, \u03b4CBF = \u03b4CBF,L , \u2022 else if JyL < JyR AND (\u03b4CBF,R was NOT selected in the last\nstep OR JyL < JyR 2\n), \u03b4CBF = \u03b4CBF,L , \u2022 else if JyR < JyL AND (\u03b4CBF,L was NOT selected in the last step\nOR JyR < JyL 2\n), \u03b4CBF = \u03b4CBF,R , \u2022 else if \u03b4CBF,L was selected in the last step, \u03b4CBF = \u03b4CBF,L , \u2022 else if \u03b4CBF,R was selected in the last step, \u03b4CBF = \u03b4CBF,R ,\n\u2013 If QPyL and QPyR are both infeasible, \u03b4CBF is infeasible. Skip 4.3.2.3 and perform max braking. The logic above chooses the cheaper option and also prevents switching between choosing left or right in sequential steps. \u03b4CBF is saturated after its computation by the dual QPs.\n4.3.2.3 QPx: We solve QPx defined below, and saturate \u03b1CBF based on the actuation limits of the ego vehicle.\n\u03b1CBF = arg min \u03b1 CBF\n\u03b12 CBF , (46)\nsubject to:\nh\u0307x,i(\u03b10 + \u03b1CBF) + l0,xhx,i,\u2265 0 \u2200i \u2208 X , (47)\nwhere X is the set of target ID\u2019s for which the longitudinal barrier is enforced and l0,x is a tunable parameter that determine the sensitivity of the barriers. For each target i, we use hx,F if xT >= 0 and hx,R if xT < 0. If QPx is infeasible due to conflicting constraints, we remove the conflicting rear constraints and resolve."
        },
        {
            "heading": "4.4 Examples",
            "text": "In all examples shown in this section, the parameters are set to the values shown in Table 1. In the position plots shown below (Figures 20, 23, and 26), the green vehicle is the ego with the collision avoidance activated and the dotted pink is the ego vehicle without the collision avoidance algorithm. All target vehicles are shown in blue. The dotted red lines show the active constraints at that time step.\n4.4.1 Aggressive/Erratic Ego Cut-in Prevention In this example, the ego vehicle attempts to change lanes while a vehicle is in its blind spot. Fig. 20 shows the position of the ego vehicle and the target vehicles at 0.5 sec intervals. Figures 21a and 21b show the front wheel angle and acceleration of the ego vehicle and Figure 22 shows the minimum Euclidean distance between the ego and the critical target vehicle (i.e. the one threatening to collide with the ego vehicle) edges. The CBF safety filter corrected the nominal steering actuation and prevented a\npotentially erratic right lane change \u2013 which is shown by the dashed-red rectangle in Fig. 20 (d)-(g).Similarly, it also prevented the ego vehicle from making a left lane change (shown by the dotted red line constraint).\n4.4.2 Target Cut-in Evasion In this example, the ego vehicle evades a target vehicle that cuts-in ahead. Fig. 23 shows the position of the ego vehicle and the target vehicles at 0.5 sec intervals. Figures 24a and 24b show the front wheel angle and acceleration of the ego vehicle and Figure 25 shows the minimum Euclidean distance between the ego and the critical target vehicle edges. The safety filter avoids the vehicle cutting in by changing lanes, and also prevents the ego vehicle from driving off the highway due to the road keeping barriers.\n4.4.3 Avoiding a Stationary Vehicle Ahead In this example, the ego vehicle evades a potential collision with an obstacle that is stationary ahead. Fig. 26 shows the position of the ego vehicle and the target vehicles (including the stationary obstacle/vehicle on the left-most lane) at 0.5 sec intervals. Figures 27a and 27b show the front wheel angle and acceleration of the ego vehicle and Figure 28 shows the minimum Euclidean distance between the ego and the critical target vehicle edges. The CBF safety filter changes lane to avoid the stationary car ahead, while slowing down to avoid colliding with the slow travelling lead car in the center lane. (Note that it further prevents erratic cut-in into the right most lane.)"
        },
        {
            "heading": "4.5 Summary",
            "text": "In this section, we introduced decoupled longitudinal and lateral Control Barrier Functions for collision avoidance, and a safety filter that utilizes contextual knowledge of steering/braking efficacy in different situations to arbitrate between the two options to avoid collisions. We also present road keeping barrier functions that are used to prevent driving off the roadway when attempting to avoid collisions. The safety filter checks if the nominal steering and throttle/brake commands are safe by evaluating for CBF constraint violation. If the nominal control is safe, the safety filter does not modify them. If they are unsafe the algorithm overrides the nominal control with a minimally intrusive safe control that prevents the constraint violation. This safe control is calculated by solving constrained quadratic optimization problems for the front wheel angle and\nthe longitudinal acceleration. Several examples demonstrating collision avoidance in common highway situations were presented including prevention of erratic cut in, evasion upon target cut in, and avoidance of stationary obstacles."
        },
        {
            "heading": "5 Integrated Driving Policy with DRL, Motion Control, and CBF Safety Filter",
            "text": "In this section we replace the rule-based safety filter from Section 2 with the CBF based collision avoidance algorithm from Section 4. Using the CBF safety filter has two primary advantages. First, it can be used to provide continuous feedback to the Agent, as opposed to binary safe/unsafe\nfeedback by rule-based safety filter. This helps the Agent to assess the\nseverity of its mistake and prevents it from repeating the same. Secondly, with a CBF safety filter the Agent can adapt to different environments after its initial training without endangering the occupants of the AV or nearby motorists."
        },
        {
            "heading": "5.1 Training Architecture with CBF Safety Filter",
            "text": "The high level decisions made by the Agent are executed by a low level motion controller algorithm (Section 3). These nominal controls and the positions and velocities of surrounding vehicles are fed to the CBF based collision avoidance system. If the control action given the situation is deemed safe, it is passed through without modification. However, if the control action is unsafe, it is overridden with a safe control calculated by the CBF safety filter. The combination of the RL Agent and the CBF based collision avoidance system is referred to as the Autonomous Driver. Fig. 29 shows the system used for training the Agent. Similar to Section 2, sensors provide affordance indicators to the Agent, which then makes a decision in the form of a high level action. The high level action is then translated to a front wheel angle, throttle and brake command by a motion controller algorithm. These low level commands and affordance indicators are then provided to the CBF Safety Filter. The safety filter computes the safe front wheel angle and throttle/brake commands. If the original path is safe, these commands are unchanged. The safe front wheel angle and throttle/brake commands are then applied to the ego vehicle. The Action Translator block translates the safe low level commands back into a safe high level action. This is used to provide feedback to the Agent about the action it has taken. If the safe action differs from the action selected by the Agent, it is penalized. In this work, the Agent makes a decision every second, and the motion controller and CBF updates the front wheel angle and throttle/brake every 10th of a second."
        },
        {
            "heading": "5.1.1 Action Translation and Penalizing Unsafe Actions",
            "text": "To provide effective feedback to the DRL Agent, the continuous safe front wheel angle and longitudinal acceleration signals must be translated back into the high level action space. For the longitudinal acceleration, this is straightforward, since both the high level action and the continuous signal have the same units (acceleration in g\u2019s). For the front wheel angle \u03b4, we use a Barrier Function based intent prediction (Rahman et al, 2021) to determine if the steering action is trying to prevent a lane change or triggering one. We use the same reward function components (11)-(13) as in Section 2, with an additional component. The addition is a safety component that determines how safe the action is by comparing it to the safe action output by the CBF\nrs = fx(ax, a\u0304x) + fy(ay, a\u0304y), (48)\nwhere ax is the longitudinal action selected by the Agent, a\u0304x is the safe longitudinal action by the CBF filter, ay is the lateral action selected by the Agent, a\u0304y is the safe lateral action by the CBF filter, and fx and fy are functions that determine the size of the penalty for unsafe longitudinal and lateral actions respectively. The size penalty on unsafe actions depends on how different the unsafe action is compared to the safe action, and also how long the decision chosen by the Agent was deemed unsafe. Figures 30 and 31 show qualitatively how the size of the penalty is determined. In Fig. 30a, the RL action is to accelerate at 0.2g, but the safe (determined by CBF Safety filter) action is to brake hard for nearly the entire duration of the RL Action. Such a case incurs a large penalty. In Fig. 30b, the RL action is again to accelerate at 0.2g. In this case, the safe action is to brake near the end of the RL Action, and so this decision incurs a smaller penalty. Similarly, in Fig. 31a, the RL action is to maintain the lane. However, at the 6th time step, the safety filter must perform a left lane change in order to avoid a collision. The time duration at which RL action becomes unsafe determines how large the penalty is.\n5.1.2 Training Results To train the Agent, we use episodes consisting of 200 seconds of highway driving. In each episode, the surround-\ning environment, i.e. the density, speed, location of the traffic is randomized. Fig. 32 shows the training performance with and without CBF, and also using only the rule based safety filter from Section 2. Whilst learning how to drive without any safety filter, the Agent has many collisions initially, and does not learn how to drive safely. With the CBF, the time taken to learn to drive acceptable driving behavior is reduced significantly, the driving behavior is better as shown by the higher reward, and the driving behavior is safer, since CBF prevents collisions in case the Agent makes an unsafe decision. Without a safety filter, it is difficult to structure the reward function in a way that guides the Agent to drive safely and in a manner that is not exceedingly conservative. Fig. 33 shows the mean over 100 episodes of the number of safe Agent actions in each episode. In each episode, 20 actions are random exploration, so the maximum number of safe actions selected by the Agent is 180. The also shows that the Agent learns to drive safer as time progresses.\n5.1.2.1 Comparison of CBF Safety Filter Intervention with RuleBased Safety Filter: Figures 34 and 35 show the longitudinal and lateral interventions of the CBF and RB safety filters as the agent is trained. For both safety filters, the number of interventions and the severity of those\ninterventions decrease over time, implying that the Agent is learning to be a safer driver, and since the reward is higher with the safety filter, this implies that the addition of the safety filter does not cause the Agent to become too conservative. The plots show that the longitudinal safety filter is more intrusive with its intervention, but this does not have an adverse effect on the reward. An advantage of using the CBF safety filter over the rule-based safety filter from Section 2 is that the CBF can be tuned to a desired degree of conservative behavior, whereas the rule-based safety filter can only choose between several predefined actions that may be either insufficient or unnecessarily excessive, which can reduce passenger confidence and also result in an uncomfortable ride."
        },
        {
            "heading": "5.1.2.2 Collision avoidance with CBF Safety Filter Intervention",
            "text": "with Rule-Based Safety Filter In this example, we show an use case where the intervention by the RB safety filter is insufficient. The AV and a target vehicle are driving at 70 mph. The RL policy initiates a lane change at t = 0 sec. At t = 2 sec, the black target vehicle decelerates at 0.35g for 3 seconds. Fig. 36 shows the response of the RB safety filter and the CBF Safety Filter. The green vehicle is the AV with the CBF safety filter and the red vehicle is the AV with the RB safety filter. The RB\nsafety filter reacts later to the decelerating target vehicle and is unable to avoid a collision.\n5.1.2.3 Retraining: One of the key advantages of the proposed approach is that it can allow an already trained Agent to continue to adapt its driving policy once on the road. For example, acceptable driving behavior in different areas of a country are often slightly different. An Agent can potentially adapt in the field and optimize to its local conditions by\nretraining itself. The actual adaptation algorithm is a subject of ongoing research and is not discussed in this paper. However, without a safety filter, an Agent making exploratory decisions may get itself into an unsafe situation. The benefit of the safety filter is twofold. If the Agent makes an unsafe decision during retraining, the decision is overridden and the safety of the AV is not compromised. Secondly, the Agent is provided immediate feedback that the decision was unsafe, therefore making it unlikely to make the same mistake again."
        },
        {
            "heading": "5.2 Summary",
            "text": "In this section we replaced the rule-based safety filter from Section 2 with the CBF based safety filter from Section 4. The amount of time taken for the Agent to learn a safe driving behavior is significantly reduced thanks to the efficient use of the Safety Filter. It overrides the agent\u2019s unsafe action and provides a safe alternative which acts as an additional feedback to the agent. To this end we modified the reward function from Section 2 to include an additional penalty based on the difference between the agent\u2019s nominal action and the safe action by the CBF. Training results show that the agent learns to drive safely and that the number of unsafe decisions reduces with training. The severity of the safety filter interventions also decrease as the training proceeds. The addition of the CBF safety filter helps to make the Autonomous Driver more robust to unsafe high level decisions and to aggressive traffic vehicles. In addition, the Autonomous Driver can adapt to new environments by learning online without forgoing on safety and comfort."
        },
        {
            "heading": "6 Summary and Conclusion",
            "text": "In this chapter we provide a practical approach to the design of RLbased driving policy for highway autonomous driving. Proposed driving policy system integrates a high level DRL-driven decision-making module with a path planning and path following strategy transforming the discrete DRL action space into a sequence of motion control commands. The benefits of the hierarchical decoupling of the RL decision logic from the algorithms for path formation and execution are the simplification of the RL algorithm design and training, and the opportunity to use versatile motion control algorithms. The latter facilitates the incorporation of engineering knowledge and experience in designing smooth human-like driving and safe lane change maneuvering. The engineers\u2019 field experience and knowledge includes the selection of lead target vehicle for longitudinal speed profile, the calibration of lateral acceleration profile and corresponding feedback control gains, the compliance of actuation constraints during lane change maneuvers, and the time required to perform a lane change. The chapter elaborates on the practical aspects of the design of the DRL decision-making strategy and motion control as critical components for real-world implementation of AI based decision policy for autonomous driving.\nThe chapter further focuses on addressing the overall robustness and safety of the output produced by the decision-making and motion control layers of the driving policy. We discuss the concept of a safety filter as an important means in autonomous driving applications of RL. The safety filter overrides the nominal front wheel angle and throttle/brake commands in case of imminent/unexpected threats while allowing the RL optimizer to focus on longer term goal. Two alternative safety filters defining the safety boundary of the produced control output are discussed. The first one uses handcrafted rules to constrain the DRL/motion control output within a predefined safety boundary. It accepts or rejects the RL decisions by enforcing traffic regulations and handcrafted rules for time headway and gap size requirement. The second solution introduces a CBF-based filter that provides a dynamic safety envelope safeguarding the control output along the vehicle trajectory. It adjusts the control actuation corresponding to RL decisions by leveraging the concept of invariant set to assess imminent threats and calculate minimal required adjustment (in steering/braking) for collision avoidance. The design and vehicle implementation of CBFs as a tool for collision avoidance and following road geometry constraints are discussed. The chapter concludes with a discussion on the pros and cons of both safety filters and their impact on the training and overall performance of the DRL algorithm, and the open opportunity of DRL \u2013 integrated with motion control and a safety filter \u2013 to adapt to new environments by learning online with corresponding safety and comfort assurance."
        }
    ],
    "title": "Robust AI Driving Strategy for Autonomous Vehicles",
    "year": 2022
}