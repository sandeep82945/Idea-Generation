{
    "abstractText": "Physics-Informed Neural Networks (PINNs) have been shown to be an effective way of incorporating physicsbased domain knowledge into neural network models for many important real-world systems. They have been particularly effective as a means of inferring system information based on data, even in cases where data is scarce. Most of the current work however assumes the availability of high-quality data. In this work, we further conduct a preliminary investigation of the robustness of physics-informed neural networks to the magnitude of noise in the data. Interestingly, our experiments reveal that the inclusion of physics in the neural network is sufficient to negate the impact of noise in data originating from hypothetical low quality sensors with high signal-to-noise ratios of up to 1. The resultant predictions for this test case are seen to still match the predictive value obtained for equivalent data obtained from high-quality sensors with potentially 10x less noise. This further implies the utility of physics-informed neural network modeling for making sense of data from sensor networks in the future, especially with the advent of Industry 4.0 and the increasing trend towards ubiquitous deployment of low-cost sensors which are typically noisier.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jian Cheng Wong"
        },
        {
            "affiliations": [],
            "name": "Pao-Hsiung Chiu"
        },
        {
            "affiliations": [],
            "name": "Chin Chun Ooi"
        }
    ],
    "id": "SP:2badc525c0892e9f94b844ca42b4befd527b5741",
    "references": [
        {
            "authors": [
                "R. Stewart",
                "S. Ermon"
            ],
            "title": "Label-free supervision of neural networks with physics and domain knowledge",
            "venue": "in: Thirty-First AAAI Conference on Artificial Intelligence",
            "year": 2017
        },
        {
            "authors": [
                "Y. Zhu",
                "N. Zabaras",
                "P.-S. Koutsourelakis",
                "P. Perdikaris"
            ],
            "title": "Physicsconstrained deep learning for high-dimensional surrogate modeling and uncertainty quantification without labeled data",
            "venue": "Journal of Computational Physics 394 ",
            "year": 2019
        },
        {
            "authors": [
                "A. Karpatne",
                "G. Atluri",
                "J.H. Faghmous",
                "M. Steinbach",
                "A. Banerjee",
                "A. Ganguly",
                "S. Shekhar",
                "N. Samatova",
                "V. Kumar"
            ],
            "title": "Theory-guided data science: A new paradigm for scientific discovery from data",
            "venue": "IEEE Transactions on Knowledge and Data Engineering 29 (10) ",
            "year": 2017
        },
        {
            "authors": [
                "L. Von Rueden",
                "S. Mayer",
                "J. Garcke",
                "C. Bauckhage",
                "J. Schuecker"
            ],
            "title": "Informed machine learning\u2013towards a taxonomy of explicit integration of knowledge into machine learning",
            "venue": "Learning 18 ",
            "year": 2019
        },
        {
            "authors": [
                "Q.T. Le",
                "C. Ooi"
            ],
            "title": "Surrogate modeling of fluid dynamics with a multigrid inspired neural network architecture",
            "venue": "Machine Learning with Applications 6 ",
            "year": 2021
        },
        {
            "authors": [
                "Y. Chen",
                "L. Lu",
                "G.E. Karniadakis",
                "L. Dal Negro"
            ],
            "title": "Physics-informed neural networks for inverse problems in nano-optics and metamaterials",
            "venue": "Optics express 28 (8) ",
            "year": 2020
        },
        {
            "authors": [
                "M. Raissi",
                "A. Yazdani",
                "G.E. Karniadakis"
            ],
            "title": "Hidden fluid mechanics: Learning velocity and pressure fields from flow visualizations",
            "venue": "Science 367 (6481) ",
            "year": 2020
        },
        {
            "authors": [
                "M. Yin",
                "X. Zheng",
                "J.D. Humphrey",
                "G.E. Karniadakis"
            ],
            "title": "Non-invasive inference of thrombus material properties with physics-informed neural networks",
            "venue": "Computer Methods in Applied Mechanics and Engineering 375 ",
            "year": 2021
        },
        {
            "authors": [
                "Z. Chen",
                "Y. Liu",
                "H. Sun"
            ],
            "title": "Physics-informed learning of governing equations from scarce data",
            "venue": "Nature Communications 12 (1) ",
            "year": 2021
        },
        {
            "authors": [
                "P.-H. Chiu",
                "J.C. Wong",
                "C. Ooi",
                "M.H. Dao",
                "Y.-S. Ong"
            ],
            "title": "CAN-PINN: A fast physics-informed neural network based on coupled-automatic\u2013 numerical differentiation method",
            "venue": "Computer Methods in Applied Mechanics and Engineering 395 ",
            "year": 2022
        },
        {
            "authors": [
                "L. Sun",
                "H. Gao",
                "S. Pan",
                "J.-X. Wang"
            ],
            "title": "Surrogate modeling for fluid flows based on physics-constrained deep learning without simulation data",
            "venue": "Computer Methods in Applied Mechanics and Engineering 361 ",
            "year": 2020
        },
        {
            "authors": [
                "M.A. Nabian",
                "H. Meidani"
            ],
            "title": "Physics-driven regularization of deep neural networks for enhanced engineering design and analysis",
            "venue": "Journal of Computing and Information Science in Engineering 20 (1) ",
            "year": 2020
        },
        {
            "authors": [
                "L. Yang",
                "X. Meng",
                "G.E. Karniadakis"
            ],
            "title": "B-PINNs: Bayesian physicsinformed neural networks for forward and inverse pde problems with noisy data",
            "venue": "Journal of Computational Physics 425 ",
            "year": 2021
        },
        {
            "authors": [
                "U. Ghia",
                "K.N. Ghia",
                "C. Shin"
            ],
            "title": "High-Re solutions for incompressible flow using the Navier-Stokes equations and a multigrid method",
            "venue": "Journal of computational physics 48 (3) ",
            "year": 1982
        },
        {
            "authors": [
                "X. Jin",
                "S. Cai",
                "H. Li",
                "G.E. Karniadakis"
            ],
            "title": "NSFnets (Navier-Stokes flow nets): Physics-informed neural networks for the incompressible Navier- Stokes equations",
            "venue": "Journal of Computational Physics 426 ",
            "year": 2021
        },
        {
            "authors": [
                "P.-H. Chiu"
            ],
            "title": "An improved divergence-free-condition compensated method for solving incompressible flows on collocated grids",
            "venue": "Computers & Fluids 162 ",
            "year": 2018
        },
        {
            "authors": [
                "P.-H. Chiu",
                "H.J. Poh"
            ],
            "title": "Development of an improved divergence-freecondition compensated coupled framework to solve flow problems with time-varying geometries",
            "venue": "International Journal for Numerical Methods in Fluids 93 (1) ",
            "year": 2021
        },
        {
            "authors": [
                "J.C. Wong",
                "C. Ooi",
                "A. Gupta",
                "Y.-S. Ong"
            ],
            "title": "Learning in sinusoidal spaces with physics-informed neural networks",
            "venue": "arXiv preprint arXiv:2109.09338 ",
            "year": 2021
        },
        {
            "authors": [
                "T.Q. Le",
                "P.-H. Chiu",
                "C. Ooi"
            ],
            "title": "U-Net-based surrogate model for evaluation of microfluidic channels",
            "venue": "International Journal of Computational Methods ",
            "year": 2021
        },
        {
            "authors": [
                "H. Wang",
                "Y. Liu",
                "S. Wang"
            ],
            "title": "Dense velocity reconstruction from particle image velocimetry/particle tracking velocimetry using a physicsinformed neural network",
            "venue": "Physics of Fluids 34 (1) ",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Robustness of Physics-Informed Neural Networks to Noise in Sensor Data\nJian Cheng Wong Department of Fluid Dynamics\nInstitute of High Performance Computing Singapore, Singapore\nwongj@ihpc.a-star.edu.sg\nPao-Hsiung Chiu Department of Fluid Dynamics\nInstitute of High Performance Computing Singapore, Singapore\nchiuph@ihpc.a-star.edu.sg\nChin Chun Ooi Department of Fluid Dynamics\nInstitute of High Performance Computing & Center for Frontier AI Research Singapore, Singapore\nooicc@ihpc.a-star.edu.sg\nMy Ha Dao Department of Fluid Dynamics\nInstitute of High Performance Computing Singapore, Singapore\ndaomh@ihpc.a-star.edu.sg\nAbstract\u2014Physics-Informed Neural Networks (PINNs) have been shown to be an effective way of incorporating physicsbased domain knowledge into neural network models for many important real-world systems. They have been particularly effective as a means of inferring system information based on data, even in cases where data is scarce. Most of the current work however assumes the availability of high-quality data. In this work, we further conduct a preliminary investigation of the robustness of physics-informed neural networks to the magnitude of noise in the data. Interestingly, our experiments reveal that the inclusion of physics in the neural network is sufficient to negate the impact of noise in data originating from hypothetical low quality sensors with high signal-to-noise ratios of up to 1. The resultant predictions for this test case are seen to still match the predictive value obtained for equivalent data obtained from high-quality sensors with potentially 10x less noise. This further implies the utility of physics-informed neural network modeling for making sense of data from sensor networks in the future, especially with the advent of Industry 4.0 and the increasing trend towards ubiquitous deployment of low-cost sensors which are typically noisier.\nIndex Terms\u2014Physics-Informed Neural Networks, Sensor networks, Inverse modeling, Data noise, Fluid dynamics\nI. INTRODUCTION\nNeural networks, particular in deep learning, have proven to be very successful in recent years, especially in computer vision and natural language processing tasks. However, they have been vastly less successful generally when applied in the science and engineering domain as data collection is often very difficult and a major bottleneck.\nIn order to overcome this data scarcity, it has been hypothesized that incorporating knowledge about the system into these deep learning models can be a more data-efficient route for systems with known governing physics. Consequently, much work has also been published in this field in recent years [1], [2].\nThere are many strategies for the incorporation of knowledge or physics into machine learning models, such as in\nthe design of physics\u2013specific model architecture or physicsguided feature selection [3]\u2013[5]. In particular, the approach of incorporating physics-based knowledge or constraints into the model as a form of regularization has yielded very promising results across diverse domains such as fluid dynamics and electromagnetics [6]\u2013[10], and is particularly effective for remedying data-absent scenarios [11], [12]. More generally, the constraints embodied in these physics-based regularization can range from governing equations in the form of differential equations to simpler empirical laws such as physical equations of state.\nWhile this use of a physics-based regularization term in the loss function in the training and construction of surrogate models for a fluid dynamical system is expected to be advantageous for data-scarce scenarios, we further explore the potential benefits of this methodology beyond improved predictive error for data-sparse or data-absent scenarios. Specifically, we describe a predominant concern over the use of purely datadriven surrogate models below, especially in the context of sparse data sets, and propose a fluid dynamical system as a case study for exploring the benefit of this methodology.\nNamely, in this current era of Industry 4.0, data acquisition frequently entails a choice between the deployment of numerous cheap, but relatively less robust and more noisy sensors at more locations, or the selective use of expensive but potentially more accurate sensors at comparatively less locations. This is exacerbated by the very specific expertise required to ensure high-quality data, for example in chemical sensing or optical imaging. This is in contrast to more intuitive domains such as computer vision, although mislabeled data can still be an issue.\nHence, a major trade-off to be considered in the development of any data-driven machine learning model is the choice between acquisition of more noisy data, and potentially overfitting to dataset noise, or the acquisition of extremely sparse, but less noisy data. While there have been other\nar X\niv :2\n21 1.\n12 04\n2v 1\n[ cs\n.L G\n] 2\n2 N\nov 2\n02 2\ndevelopments in adapting the PINN framework to handle noise via Bayesian formulations [13], we choose to more directly test the ability of the physics-based regularization to compensate for both data scarcity and data noise in this work.\nIn that context, we first develop a base physics-informed neural network in accordance with prior work in literature. We then evaluate the model across a series of scenarios where the data obtained and provided for training is either sparse and/or noisy, which is representative of many engineering systems where sensor systems might be expensive and difficult to deploy, and evaluate the extent to which inclusion of physics can improve the neural network\u2019s robustness to noise in the data."
        },
        {
            "heading": "II. METHODS",
            "text": ""
        },
        {
            "heading": "A. Fluid Dynamical Case Study",
            "text": "For this work, we simulated the canonical case of a 2-D lid-driven cavity, which is a common benchmark problem in computational fluid dynamics (CFD) [14], [15]. The following set of partial differential equations (PDEs) that govern incompressible steady-state fluid systems are thus considered to be physical laws that must be satisfied across the whole fluid domain \u2126. As these equations represent the conservation of mass and momentum, these equations are representative of physics-based laws that can be implemented in problems more generally.\n\u2202u \u2202x + \u2202v \u2202y = 0 (1a)\n\u03c1 \u2202(uu)\n\u2202x + \u03c1\n\u2202(vu)\n\u2202y = \u00b5\n( \u2202\n\u2202x ( \u2202u \u2202x ) + \u2202 \u2202y ( \u2202u \u2202y ) ) \u2212 \u2202p \u2202x (1b)\n\u03c1 \u2202(uv)\n\u2202x + \u03c1\n\u2202(vv)\n\u2202y = \u00b5\n( \u2202\n\u2202x ( \u2202v \u2202x ) + \u2202 \u2202y ( \u2202v \u2202y ) ) \u2212 \u2202p \u2202y (1c)\nEquation 1(a) is a statement of conservation of mass, and is also called the continuity equation, while Eq. 1(b)-(c) are statements of conservation of momentum in the x and y direction respectively for a 2-dimensional scenario, and are collectively referred to as the momentum equations. (u,v) refers to the two components of velocity on the Cartesian grid, p represents pressure, and \u03c1 and \u00b5 represent the density and viscosity of the fluid respectively.\nFor this particular fluid system, the physical domain simulated is of length 1 x 1 unit. Density (\u03c1) is assumed to be of constant magnitude 1 while \u00b5 is assumed to be 0.01, hence generating a scenario with a Reynolds number of 100. A velocity Dirichlet boundary condition for the top surface was used (u = 1, v = 0), while the other three surfaces were specified to be stationary surfaces (u = 0 and v = 0).\nAn incompressible CFD solver was used to solve for velocity, (u,v), and pressure [16], [17]. The velocity and pressure fields were numerically solved on a uniform 200 \u00d7 200 grid, and subsequently down-sampled to a 50 \u00d7 50 grid to simulate a potential set of synthetic \u201cexperimental\u201d data points that can be used for training. The original CFD-derived set of 200 \u00d7\n200 grid points was used as ground truth for evaluation of test mean-squared error in all subsequent numerical experiments."
        },
        {
            "heading": "B. Physics-Informed Neural Network",
            "text": "A fully-connected feed-forward neural network is used for all experiments in this work. The networks for u, v, p consist of 7 layers, with 32 nodes in the first layer, and 20 nodes per layer in the subsequent layers. The first four layers are shared for u, v and p, while each flow variable has a unique set of parameters for the subsequent three layers. In addition, all hidden layers use the \u2018sine\u2019 activation function and a He initialization, in line with prior PINN work [18].\nAll networks are trained with ADAM, and training is terminated after 105 iterations. An initial learning rate of 1e-3 is used which is reduced on plateauing until a minimum learning rate of 5e-6 is reached. Importantly, all data-driven neural networks and physics-informed neural networks compared in this work were trained under the same network architectures and training hyperparameters.\nFor the base data-driven neural network (DNN), the training loss function is defined based solely on the provided data:\nL = Lu\u2212data + Lv\u2212data + LBC (2a)\nLu\u2212data = 1\nn \u03a3ni=1(ui \u2212 ui,data)2 (2b)\nLv\u2212data = 1\nn \u03a3ni=1(vi \u2212 vi,data)2 (2c) LBC = \u2225\u2225B[u(x, y)] \u2212 g(x, y)\u2225\u22252\n\u2202\u2126 (2d)\n(2e)\nGenerally, the boundary operator, B, can be any combination of Dirichlet or Neumann boundary conditions that enforce the desired condition g(x, y) at the domain boundary \u2202\u2126.\nThe PINN training loss function includes additional physicsbased regularization in contrast to the DNN training loss function above. It is defined as:\nL = LData + \u03bbPDELPDE + LBC (3a) LData = Lu\u2212data + Lv\u2212data (3b)\nLPDE = \u2225\u2225Nx[u(x, y)]\u2225\u22252\u2126 (3c)\nLBC = \u2225\u2225B[u(x, y)] \u2212 g(x, y)\u2225\u22252\n\u2202\u2126 (3d)\nwhich includes the data loss component, LData, and the BC loss components, LBC , as per the typical data-driven neural network, and the additional physics-based regularization, LPDE , which is determined by the set of Eqs 1(a-c). The relative weight, \u03bb, controls the trade-off between different components in the loss function and is a hyper-parameter that can be tuned for performance optimization across different cases. A constant value of \u03bb = 1 is used in this work.\nThe inclusion of PDE-derived loss term, LPDE , is typically computed by evaluating the residuals of the continuity and momentum equations in Eqs 1(a-c) via the direct use of\nautomatic differentiation within Tensorflow. A schematic of the architecture used is presented in Fig. 1.\nIn this work, the utility of incorporating physics into the neural network model is tested for situations whereby the data can be sparse and/or noisy. Hence, the training data is randomly sub-sampled from the down-sampled set of 50 \u00d7 50 points (2500 total points), and random Gaussian noise of varying amplitude is introduced to the individual points. PDE residuals are separately calculated on the 2500 locations even if no data is provided at that exact location.\nTest errors are then evaluated on the original set of 200 \u00d7 200 points obtained from high-fidelity numerical simulation, which serve as the ground truth. All the different models are evaluated by the Mean-Squared Error (MSE) which is computed as:\nMSE = 1\n2n \u03a3ni=1[(ui \u2212 ui,tr)2 + (vi \u2212 vi,tr)2] (4)\nFor each experiment, the neural network\u2019s weights are randomly initialized and trained across 10 replicates. These replicate experiments span 2 different ranges of noise and data scarcity. The distribution of MSE from model predictions across these replicate experiments are presented and discussed in the following Results section."
        },
        {
            "heading": "III. RESULTS AND DISCUSSION",
            "text": "While many experiments in literature have focused on demonstrating the benefits of physics-informed learning in data-absent scenarios, there has been relatively less attention devoted to an assessment of the interplay between the amount of data provided and the magnitude of noise inherent in the data. Indeed, much of the current published work focuses more on relatively noise-free data.\nA. Inference with Varying Noise Magnitudes\nIn the first set of experiments, noise of varying magnitudes was added to a set of 2500 data points spanning the entire fluid domain. The noise was scaled to be between 10%, 20%, 30%, 40%, 50% and 100% of the standard deviation inherent in the velocity data across the domain. This scaling is representative of the commonly used Signal-to-Noise Ratio (SNR) metric used to characterize the quality of sensors in many engineering applications. Importantly, it should be noted that an SNR > 1 is a typical threshold beyond which any sensor is regarded to be generally unusable. Representative contour plots are presented in Fig. 2 to illustrate the impact of different magnitudes of noise on the data provided. In particular, it is worth noting how flow structures in the centre of the domain are completely lost due to the addition of noise.\nThese data-sets representing data acquired from sensors of varying SNR are then provided to both a vanilla datadriven neural network (DNN) and a PINN. The MSEs are then computed for all the scenarios, and presented in Fig. 3.\nIt is also worth noting that one of the purported uses for PINNs is the ability to infer missing information about particular scenarios based on partial data. In fluid dynamics in particular, a common problem encountered in the use of particle image velocimetry for characterization of specific systems is the fact that only velocity field information can be acquired [19], [20]. Pressure information is typically not simultaneously acquired and must be inferred. The use of a typical DNN is not able to provide meaningful predictions for pressure unless corresponding velocity-to-pressure models are trained or corresponding numerical methods are employed. However, the use of a PINN integrates relevant underlying knowledge with the provided velocity data for highly accurate and convenient inference of pressure, as depicted in Fig. 4.\nRepresentative contour plots of the velocity and pressure predicted by the DNN and PINN models are presented in Fig. 5 to illustrate the effectiveness of the PINN models relative to the DNN models.\nUnsurprisingly, an increase in SNR ratio causes a significant degradation in the predictive performance of a pure DNN. The test MSE rapidly increases by more than 2 orders of magnitude from less than 10\u22124 to 10\u22122. However, the use of a PINN model is able to compensate for the type of noise present in low quality sensors and restore them to the accuracy achievable by a typical DNN trained on data acquired from much more accurate sensors with low SNR of less than 10%.\nB. Inference with Noisy Data of Variable Dataset Size\nAs a further evaluation of the utility of including physics, additional experiments are conducted with down-sampled datasets of different sizes to simulate and illustrate the impact when high-quality (relatively low noise) sensors are employed but resource/experimental constraints limit the amount of data that can be acquired instead.\nRepresentative plots of the impact of smaller data-sets are presented in Fig. 6, illustrating the relative amounts of\ninformation contained when the dataset is as large as 2000 points in the domain.\nThese data-sets of different sizes are similarly provided to both a DNN and a PINN for training. Test MSEs are computed for all the scenarios, and presented in Fig. 7.\nRepresentative contour plots of the velocity and pressure predicted by the DNN and PINN models are presented in\nFig. 8 to illustrate the effectiveness of the PINN models relative to the DNN models for datasets of varying sizes.\nThe results show that the introduction of physics into the neural network is increasingly helpful as the data gets more noisy and sparse, with at least an order of magnitude improvement in the test error between the DNN and PINN. In this instance, we note orders of magnitude reduction in model test error across all cases. Importantly, even for this relatively simpler case study, we note that a halving of the amount of data provided leads to an approximately order of magnitude increase in the DNN test error. It\u2019s worth noting that the reduction in dataset size by 5x from 2500 to 500 degrades the DNN model performance to that equivalent to the use of low-quality sensors with noise larger than 50%. Such tradeoffs between the number and quality of sensors that can be acquired and deployed are dependent on the complexity of the problem, but are nonetheless important considerations in real-world problems.\nCritically, the use of a PINN can overcome this reduction in the amount of data provided and prevent any significant performance degradation. This is thus extremely promising for the use of PINNs with scarce data-sets, especially in situations where the cost of sensors or data acquisition can severely reduce the availability of data."
        },
        {
            "heading": "IV. CONCLUSION",
            "text": "In this study, we report that the use of physics-based regularization is surprisingly effective for ameliorating noise in the data acquired. In particular, the final prediction error from trained models is reduced by orders of magnitude via the incorporation of underlying governing physical laws into the training process.\nMore specifically, the inclusion of physics-based regularization effectively overcomes noise that can be range up to equivalent SNR of 1. In this worst case scenario, the PINN models in this study still produce solutions that are approximately equivalent to models trained with data acquired by hypothetical sensors with 10x better noise performance. This effectiveness can be particularly significant for real-world applications, especially since 10x better sensors are frequently not just 10x more expensive, e.g. in chemical sensing.\nInterestingly, we note that the PINN model as tested in this case study displays differential ability to overcome data scarcity, and data noise. Critically, the choice of sensors for real-world deployment and measurement frequently involves a multi-criteria optimization, whereby one wants to optimize both data quantity and quality under a limited budget (constraint). Understanding the relative sensitivity of PINN models to dataset size and noise can greatly impact the optimal sensor choice. Further tests on different scenarios and case studies to confirm the potential greater effectiveness of PINNs for resolving data noise can be beneficial for guiding this optimization."
        },
        {
            "heading": "ACKNOWLEDGMENT",
            "text": "This research is supported by A*STAR under the AME Programmatic programme: \u201dExplainable Physics-based AI for Engineering Modelling and Design (ePAI)\u201d Award No. A20H5b0142 and the AI3 HTPO seed grant C211118016 on \u201dUpside-Down Multi-Objective Bayesian Optimization for Few-Shot Design\u201d."
        }
    ],
    "title": "Robustness of Physics-Informed Neural Networks to Noise in Sensor Data",
    "year": 2022
}