{
    "abstractText": "Big data analytics has opened new avenues in economic research, but the challenge of analyzing datasets with tens of millions of observations is substantial. Conventional econometric methods based on extreme estimators require large amounts of computing resources and memory, which are often not readily available. In this paper, we focus on linear quantile regression applied to \u201cultra-large\u201d datasets, such as U.S. decennial censuses. A fast inference framework is presented, utilizing stochastic subgradient descent (S-subGD) updates. The inference procedure handles cross-sectional data sequentially: (i) updating the parameter estimate with each incoming \u201cnew observation\u201d, (ii) aggregating it as a Polyak-Ruppert average, and (iii) computing a pivotal statistic for inference using only a solution path. The methodology draws from time-series regression to create an asymptotically pivotal statistic through random scaling. Our proposed test statistic is calculated in a fully online fashion and critical values are calculated without resampling. We conduct extensive numerical studies to showcase the computational merits of our proposed inference. For inference problems as large as (n, d) \u223c (10, 10), where n is the sample size and d is the number of regressors, our method generates new insights, surpassing current inference methods in computation. Our method specifically reveals trends in the gender gap in the U.S. college wage premium using millions of observations, while controlling over 10 covariates to mitigate confounding effects.",
    "authors": [
        {
            "affiliations": [],
            "name": "SOKBAE LEE"
        },
        {
            "affiliations": [],
            "name": "YUAN LIAO"
        },
        {
            "affiliations": [],
            "name": "MYUNG HWAN SEO"
        }
    ],
    "id": "SP:c4a5408d4ef1e988e28cf3213c961354af3a57f9",
    "references": [
        {
            "authors": [
                "K.M. Abadir",
                "P. Paruolo"
            ],
            "title": "Two mixed normal densities from cointegration analysis",
            "venue": "Econometrica 65 (3), 671\u2013680.",
            "year": 1997
        },
        {
            "authors": [
                "K.M. Abadir",
                "P. Paruolo"
            ],
            "title": "Simple robust testing of regression hypotheses: A comment",
            "venue": "Econometrica 70 (5), 2097\u20132099.",
            "year": 2002
        },
        {
            "authors": [
                "J. Angrist",
                "V. Chernozhukov",
                "I. Fern\u00e1ndez-Val"
            ],
            "title": "Quantile regression under misspecification, with an application to the us wage structure",
            "venue": "Econometrica 74 (2), 539\u2013563.",
            "year": 2006
        },
        {
            "authors": [
                "M. Buchinsky"
            ],
            "title": "Changes in the U.S. wage structure 1963-1987",
            "venue": "Application of quantile regression. Econometrica",
            "year": 1994
        },
        {
            "authors": [
                "M. Buchinsky"
            ],
            "title": "Recent advances in quantile regression models: A practical guideline for empirical research",
            "venue": "Journal of Human Resources",
            "year": 1998
        },
        {
            "authors": [
                "K.S. Buckles",
                "D.M. Hungerman"
            ],
            "title": "Season of Birth and Later Outcomes: Old Questions, New Answers",
            "venue": "Review of Economics and Statistics 95 (3), 711\u2013724.",
            "year": 2013
        },
        {
            "authors": [
                "G. Chamberlain"
            ],
            "title": "Quantile regression, censoring, and the structure of wages",
            "venue": "Advances in Econometrics: Sixth World Congress, Volume 2, pp. 171\u2013209.",
            "year": 1994
        },
        {
            "authors": [
                "P. Chatterji",
                "J.S. Lee",
                "B.G. Park"
            ],
            "title": "Does seasonality of birth outcome really exist? Working Paper, University at Albany, SUNY, available at https: //sites.google.com/view/byoungpark/research",
            "year": 2022
        },
        {
            "authors": [
                "C. Chen"
            ],
            "title": "A finite smoothing algorithm for quantile regression",
            "venue": "Journal of Computational and Graphical Statistics 16 (1), 136\u2013164.",
            "year": 2007
        },
        {
            "authors": [
                "Chen",
                "L.-Y.",
                "S. Lee"
            ],
            "title": "Sparse quantile regression",
            "venue": "Journal of Econometrics 235 (2), 2195\u20132217.",
            "year": 2023
        },
        {
            "authors": [
                "X. Chen",
                "Z. Lai",
                "H. Li",
                "Y. Zhang"
            ],
            "title": "Online statistical inference for stochastic optimization via Kiefer-Wolfowitz methods",
            "venue": "arXiv preprint, http://arxiv.org/ abs/2102.03389.",
            "year": 2021
        },
        {
            "authors": [
                "X. Chen",
                "J.D. Lee",
                "X.T. Tong",
                "Y. Zhang"
            ],
            "title": "Statistical inference for model parameters in stochastic gradient descent",
            "venue": "Annals of Statistics 48 (1), 251\u2013273.",
            "year": 2020
        },
        {
            "authors": [
                "V. Chernozhukov",
                "I. Fern\u00e1ndez-Val",
                "T. Kaji"
            ],
            "title": "Extremal quantile regression: An overview",
            "venue": "arXiv preprint, https://arxiv.org/abs/1612.06850.",
            "year": 2016
        },
        {
            "authors": [
                "V. Chernozhukov",
                "I. Fern\u00e1ndez-Val",
                "B. Melly"
            ],
            "title": "Inference on counterfactual distributions",
            "venue": "Econometrica 81 (6), 2205\u20132268.",
            "year": 2013
        },
        {
            "authors": [
                "V. Chernozhukov",
                "I. Fern\u00e1ndez-Val",
                "B. Melly"
            ],
            "title": "Fast algorithms for the quantile regression process",
            "venue": "Empirical economics 62 (1), 7\u201333.",
            "year": 2022
        },
        {
            "authors": [
                "Chiappori",
                "P.-A.",
                "M. Iyigun",
                "Y. Weiss"
            ],
            "title": "Investment in schooling and the marriage market",
            "venue": "American Economic Review 99 (5), 1689\u20131713.",
            "year": 2009
        },
        {
            "authors": [
                "J. Currie",
                "H. Schwandt"
            ],
            "title": "Within-mother analysis of seasonal patterns in health at birth",
            "venue": "Proceedings of the National Academy of Sciences 110 (30), 12265\u201312270.",
            "year": 2013
        },
        {
            "authors": [
                "Y. Fang",
                "J. Xu",
                "L. Yang"
            ],
            "title": "Online bootstrap confidence intervals for the stochastic gradient descent estimator",
            "venue": "Journal of Machine Learning Research 19 (1), 1\u201321.",
            "year": 2018
        },
        {
            "authors": [
                "M. Fernandes",
                "E. Guerre",
                "E. Horta"
            ],
            "title": "Smoothing quantile regressions",
            "venue": "Journal of Business & Economic Statistics 39 (1), 338\u2013357.",
            "year": 2021
        },
        {
            "authors": [
                "Forneron",
                "J.-J."
            ],
            "title": "Estimation and inference by stochastic optimization",
            "venue": "arXiv preprint, http://arxiv.org/abs/2205.03254.",
            "year": 2022
        },
        {
            "authors": [
                "Forneron",
                "J.-J.",
                "S. Ng"
            ],
            "title": "Estimation and inference by stochastic optimization: Three examples",
            "venue": "AEA Papers and Proceedings, Volume 111, pp. 626\u201330.",
            "year": 2021
        },
        {
            "authors": [
                "S. Gadat",
                "F. Panloup"
            ],
            "title": "Optimal non-asymptotic analysis of the RuppertPolyak averaging stochastic algorithm",
            "venue": "Stochastic Processes and their Applications 156, 312\u2013348.",
            "year": 2023
        },
        {
            "authors": [
                "C. Goldin",
                "L.F. Katz",
                "I. Kuziemko"
            ],
            "title": "The homecoming of American college women: The reversal of the college gender gap",
            "venue": "Journal of Economic perspectives 20 (4), 133\u2013156.",
            "year": 2006
        },
        {
            "authors": [
                "A. Gosling",
                "S. Machin"
            ],
            "title": "The Changing Distribution of Male Wages in the U.K",
            "venue": "Meghir",
            "year": 2000
        },
        {
            "authors": [
                "P. Hall",
                "C.C. Heyde"
            ],
            "title": "Martingale Limit Theory and Its Application",
            "venue": "Academic Press, Boston.",
            "year": 1980
        },
        {
            "authors": [
                "X. He",
                "X. Pan",
                "K.M. Tan",
                "W.-X. Zhou"
            ],
            "title": "Smoothed quantile regression with large-scale inference",
            "venue": "Journal of Econometrics 232 (2), 367\u2013388.",
            "year": 2023
        },
        {
            "authors": [
                "J.L. Horowitz"
            ],
            "title": "Bootstrap methods for median regression models",
            "venue": "Econometrica 66 (6), 1327\u20131351.",
            "year": 1998
        },
        {
            "authors": [
                "W.H. Hubbard"
            ],
            "title": "The phantom gender difference in the college wage premium",
            "venue": "Journal of Human Resources 46 (3), 568\u2013586.",
            "year": 2011
        },
        {
            "authors": [
                "S. Johansen"
            ],
            "title": "Estimation and hypothesis testing of cointegration vectors in Gaussian vector autoregressive models",
            "venue": "Econometrica 59 (6), 1551\u20131580.",
            "year": 1991
        },
        {
            "authors": [
                "N.M. Kiefer",
                "T.J. Vogelsang",
                "H. Bunzel"
            ],
            "title": "Simple robust testing of regression hypotheses",
            "venue": "Econometrica 68 (3), 695\u2013714.",
            "year": 2000
        },
        {
            "authors": [
                "R. Koenker"
            ],
            "title": "Quantile Regression",
            "venue": "Cambridge University Press.",
            "year": 2005
        },
        {
            "authors": [
                "R. Koenker"
            ],
            "title": "Quantile regression: 40 years on",
            "venue": "Annual Review of Economics 9 (1), 155\u2013176.",
            "year": 2017
        },
        {
            "authors": [
                "R. Koenker",
                "G. Bassett"
            ],
            "title": "Regression quantiles",
            "venue": "Econometrica 46 (1), 33\u201350.",
            "year": 1978
        },
        {
            "authors": [
                "E. Lazarus",
                "D.J. Lewis",
                "J.H. Stock",
                "M.W. Watson"
            ],
            "title": "HAR inference: Recommendations for practice",
            "venue": "Journal of Business & Economic Statistics 36 (4), 541\u2013559.",
            "year": 2018
        },
        {
            "authors": [
                "S. Lee",
                "Y. Liao",
                "M.H. Seo",
                "Y. Shin"
            ],
            "title": "Fast and robust online inference with stochastic gradient descent via random scaling",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence 36 (7), 7381\u20137389.",
            "year": 2022
        },
        {
            "authors": [
                "S. Lee",
                "S. Ng"
            ],
            "title": "An econometric perspective on algorithmic subsampling",
            "venue": "Annual Review of Economics",
            "year": 2020
        },
        {
            "authors": [
                "S. Lee",
                "S. Ng"
            ],
            "title": "Least squares estimation using sketched data with heteroskedastic errors",
            "venue": "K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato (Eds.), Proceedings of the 39th International Conference on Machine Learning, Volume 162 of Proceedings of Machine Learning Research, pp. 12498\u201312520. PMLR.",
            "year": 2022
        },
        {
            "authors": [
                "X. Li",
                "J. Liang",
                "X. Chang",
                "Z. Zhang"
            ],
            "title": "Statistical estimation and online inference via local SGD",
            "venue": "P.-L. Loh and M. Raginsky (Eds.), Proceedings of Thirty Fifth Conference on Learning Theory, Volume 178 of Proceedings of Machine Learning Research, pp. 1613\u20131661. PMLR.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Liu",
                "Q. Hu",
                "L. Ding",
                "L. Kong"
            ],
            "title": "Online local differential private quantile inference via self-normalization",
            "venue": "A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett (Eds.), Proceedings of the 40th International Conference on Machine Learning, Volume 202 of Proceedings of Machine Learning Research, pp. 21698\u201321714. PMLR.",
            "year": 2023
        },
        {
            "authors": [
                "S. Pesme",
                "N. Flammarion"
            ],
            "title": "Online robust regression via SGD on the l1 loss",
            "venue": "H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin (Eds.), Advances in Neural Information Processing Systems, Volume 33, pp. 2540\u20132552. Curran Associates, Inc.",
            "year": 2020
        },
        {
            "authors": [
                "B.T. Polyak"
            ],
            "title": "New method of stochastic approximation type",
            "venue": "Automation and Remote Control 51 (7), 937\u2013946.",
            "year": 1990
        },
        {
            "authors": [
                "B.T. Polyak",
                "A.B. Juditsky"
            ],
            "title": "Acceleration of stochastic approximation by averaging",
            "venue": "SIAM Journal on Control and Optimization 30 (4), 838\u2013855.",
            "year": 1992
        },
        {
            "authors": [
                "S. Portnoy"
            ],
            "title": "Asymptotic Behavior of M Estimators of p Regression Parameters when p2/n is Large; II",
            "venue": "Normal Approximation. Annals of Statistics 13 (4), 1403\u2013 1417.",
            "year": 1985
        },
        {
            "authors": [
                "S. Portnoy",
                "R. Koenker"
            ],
            "title": "The Gaussian hare and the Laplacian tortoise: computability of squared-error versus absolute-error estimators",
            "venue": "Statistical Science 12 (4), 279\u2013300.",
            "year": 1997
        },
        {
            "authors": [
                "H. Robbins",
                "S. Monro"
            ],
            "title": "A Stochastic Approximation Method",
            "venue": "The Annals of Mathematical Statistics 22 (3), 400 \u2013 407.",
            "year": 1951
        },
        {
            "authors": [
                "S. Ruggles",
                "S. Flood",
                "R. Goeken",
                "M. Schouweiler",
                "M. Sobek"
            ],
            "title": "IPUMS USA: Version 12.0 [dataset]. Minneapolis, MN: IPUMS. https://doi.org/10.18128/ D010.V12.0",
            "year": 2022
        },
        {
            "authors": [
                "D. Ruppert"
            ],
            "title": "Efficient estimations from a slowly convergent Robbins\u2013Monro process",
            "venue": "Technical Report 781, Cornell University Operations Research and Industrial Engineering. available at https://ecommons.cornell.edu/bitstream/ handle/1813/8664/TR000781.pdf?sequence=1.",
            "year": 1988
        },
        {
            "authors": [
                "K. Shan",
                "Y. Yang"
            ],
            "title": "Combining regression quantile estimators",
            "venue": "Statistica Sinica 19 (3), 1171\u20131191.",
            "year": 2009
        },
        {
            "authors": [
                "Y. Sun"
            ],
            "title": "Fixed-smoothing asymptotics in a two-step generalized method of moments framework",
            "venue": "Econometrica 82 (6), 2327\u20132370.",
            "year": 2014
        },
        {
            "authors": [
                "Y. Sun",
                "P.C. Phillips",
                "S. Jin"
            ],
            "title": "Optimal bandwidth selection in heteroskedasticity\u2013autocorrelation robust testing",
            "venue": "Econometrica 76 (1), 175\u2013194.",
            "year": 2008
        },
        {
            "authors": [
                "K.M. Tan",
                "L. Wang",
                "W.-X. Zhou"
            ],
            "title": "High-dimensional quantile regression: Convolution smoothing and concave regularization",
            "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology) 84 (1), 205\u2013233.",
            "year": 2022
        },
        {
            "authors": [
                "H. Wang",
                "Y. Ma"
            ],
            "title": "Optimal subsampling for quantile regression in big data",
            "venue": "Biometrika 108 (1), 99\u2013112.",
            "year": 2021
        },
        {
            "authors": [
                "J. Yang",
                "X. Meng",
                "M. Mahoney"
            ],
            "title": "Quantile regression for large-scale applications",
            "venue": "S. Dasgupta and D. McAllester (Eds.), Proceedings of the 30th International Conference on Machine Learning, Volume 28 of Proceedings of Machine Learning Research, Atlanta, Georgia, USA, pp. 881\u2013887. PMLR.",
            "year": 2013
        },
        {
            "authors": [
                "Q. Zheng",
                "L. Peng",
                "X. He"
            ],
            "title": "High dimensional censored quantile regression",
            "venue": "Annals of Statistics 46 (1), 308\u2013343.",
            "year": 2018
        },
        {
            "authors": [
                "W. Zhu",
                "X. Chen",
                "W.B. Wu"
            ],
            "title": "Online covariance matrix estimation in stochastic gradient descent",
            "venue": "Journal of the American Statistical Association 118 (541), 393\u2013404.",
            "year": 2023
        },
        {
            "authors": [
                "Y. Zhu",
                "J. Dong"
            ],
            "title": "On constructing confidence region for model parameters in stochastic gradient descent via batch means",
            "venue": "2021 Winter Simulation Conference (WSC). available at https://ieeexplore.ieee.org/document/9715437.",
            "year": 2021
        },
        {
            "authors": [
                "Lemma A"
            ],
            "title": "The desired conclusion follows immediately from Theorem 2 of Gadat and Panloup (2023) if we verify the conditions imposed in their Theorem 2. In particular, we provide sufficient conditions for a Kurdyka\u2013\u0141ojasiewicz inequality (with r = 0 using the notation in Gadat and Panloup",
            "year": 2023
        },
        {
            "authors": [
                "LEE",
                "SEO LIAO",
                "SHIN B"
            ],
            "title": "Empirical application: full estimation results. We provide full estimation results. Empirical application: a single year for 2005, 2010, and 2015. We use a dataset composed of a single year sample for",
            "year": 2005
        }
    ],
    "sections": [
        {
            "text": "Keywords: large-scale inference, stochastic gradient descent, subgradient\nDepartment of Economics, Columbia University, New York, NY 10027, USA Department of Economics, Rutgers University, New Brunswick, NJ 08901, USA Department of Economics, Seoul National University, Seoul, 08826, Korea Department of Economics, McMaster University, Hamilton, ON L8S 4L8, Canada E-mail addresses: sl3841@columbia.edu, yuan.liao@rutgers.edu, myunghseo@snu.ac.kr, shiny11@mcmaster.ca. Date: November 2, 2023. We would like to thank the guest associate editor, two anonymous referees, Roger Koenker, and seminar participants at CIREQ 2022, Albany, Wisconsin\u2013Madison, Washington, UIUC, and SFU for helpful comments. Shin is grateful for the partial support from the Social Sciences and Humanities Research Council of Canada (SSHRC-20015659). This work was made possible by the facilities of Calcul Queb\u00e9c (www.calculquebec.ca) and Compute Canada (www.computecanada.ca). Seo acknowldges support from the Research Grant of the Center for Distributive Justice at the Institute of Economic Research, Seoul National University, and from the Ministry of Education of the Republic of Korea and the National Research Foundation of Korea (NRF-2018S1A5A2A01033487).\n1\nar X\niv :2\n20 9.\n14 50\n2v 5\n[ ec\non .E\nM ]\n3 1\nO ct\n2 02\n3"
        },
        {
            "heading": "1. Introduction",
            "text": "Quantile regression (QR) has been increasingly popular in economics since the groundbreaking work of Koenker and Bassett (1978). Many significant developments have been made to this important statistical methodology. Meanwhile, economists are now able to analyze datasets that contain tens of millions of observations. For instance, one of the important economic applications of quantile regression is to study the wage structure. The 5% sample in the 2000 U.S. Census contains more than 14 million observations and more than 4 million even after restricting the sample to a subpopulation of working White adults. One motivation of using datasets with such \u201cultra-large\u201d sample sizes is to deal with econometric models with a large number of parameters within the framework of simple parametric models. Suppose that there are d parameters to estimate with a sample of size n. A parametric regression model can be as large as d \u223c 1, 000 because a large number of controls might be necessary. In the setting of \u201cultra-large\u201d sample size with n \u223c 107, straightforward parametric estimators can be used with d \u223c 1, 000 under standard textbook assumptions without demanding conditions such as sparsity assumptions.1\nWith datasets containing tens of millions of observations, however, one major challenge of QR is that it would require huge computing powers and memory that are often not accessible using ordinary personal computers. While inference is desirable in empirical studies to quantify statistical uncertainty, it often demands more computational resources than point estimation. Typically, asymptotic normal inference with QR estimators involves solving optimization problems and computing asymptotic covariance matrices. Computationally efficient implementation of both tasks is crucial with ultra-large datasets.\nIn this paper, we focus on this challenge, by studying the QR inference problem in the context of n \u223c 107. We propose a fast statistical inference framework to analyze cross-sectional data with millions of observations, a typical sample size for the census data, embracing stochastic gradient descent techniques, one of the most active research areas in machine learning. This literature dates back to Robbins and Monro (1951). While we focus on the QR framework, whose inference has been well known to be a hard problem, our proposed method can be naturally generalized to other econometric frameworks.\n1A classical condition for a uniform normal approximation of an M -estimator is (d log n)3/2/n \u2192 0 (Portnoy, 1985).\nEmpirically, we use the data from IPUMS USA (Ruggles et al., 2022), a common data source for the U.S. wage structure, to study the trends in the gender gap in terms of the college wage premium. In our empirical application, we aim to control for work experience by flexibly interacting workers\u2019 age with gender and state fixed effects, which creates over one thousand regressors, thereby motivating the need of ultra-large datasets. We find that existing inference methods are not applicable due to time or memory constraints, while our inference method provides new insights into trends in the gender gap. There has been a large literature on college wage premium and wage structure. See, for instance, Chamberlain (1994); Buchinsky (1994, 1998); Gosling et al. (2000); Angrist et al. (2006); Chernozhukov et al. (2013) among others. To the best of knowledge, our empirical illustration is the first to obtain confidence intervals based on quantile regression with more than 4 million observations and more than 1000 regressors.\n1.1. Standard inference for quantile regression. Consider the setting of a linear quantile regression model for which data {Yi \u2261 (yi, xi) \u2208 R1+d : i = 1, . . . , n} are generated from\nyi = x \u2032 i\u03b2 \u2217 + \u03b5i,(1)\nwhere \u03b2\u2217 is a vector of unknown parameters and the unobserved error \u03b5i satisfies P (\u03b5i \u2264 0|xi) = \u03c4 for a fixed quantile \u03c4 \u2208 (0, 1). Note that \u03b2\u2217 is characterized by\n\u03b2\u2217 := arg min \u03b2\u2208Rd Q (\u03b2) ,\nwhere Q(\u03b2) := E[q(\u03b2, Yi)] with the check function q(\u03b2, Yi) := (yi\u2212x\u2032i\u03b2)(\u03c4\u2212I{yi\u2212x\u2032i\u03b2 \u2264 0}). Here, I(\u00b7) is the usual indicator function. The standard approach for estimating \u03b2\u2217 is to use the following M-estimator originally proposed by Koenker and Bassett (1978):\n\u03b2\u0302n := arg min \u03b2\u2208Rd\n1\nn n\u2211 i=1 q(\u03b2, Yi).(2)\nSee Koenker (2005) and Koenker (2017) for a monograph and a review of recent developments, respectively.\nThere have been two potential challenges for the inference of standard quantile regression. The first is the computational problem. The optimization problem is typically reformulated to a linear programming problem, and solved using interiorpoint algorithms (e.g., Portnoy and Koenker, 1997). The second is associated with\nstatistical inference. As was shown by Koenker and Bassett (1978), the asymptotic distribution of the M estimator is as follows:\n\u221a n(\u03b2\u0302n \u2212 \u03b2\u2217) d\u2192 N(0, \u03c4(1\u2212 \u03c4)H\u22121E[xix\u2032i]H\u22121), H = E[f\u03b5(0|xi)xix\u2032i],(3)\nwhere f\u03b5(\u00b7|xi) is the conditional distribution of \u03b5i given xi, assuming data are independent and identically distributed (i.i.d.). Hence, in the heteroskedastic setting, standard inference based on the \u201cplug-in\u201d method would require estimating matrix H, which involves the conditional density function f\u03b5(\u00b7|xi). An alternative inference would be based on bootstrap.\nOne of the attempts to solving the computational/inference difficulties is to rely on the smoothing idea, via either smoothing the estimating equation (Horowitz, 1998) or the convolution-type smoothing (Fernandes et al., 2021; Tan et al., 2022; He et al., 2023). Both require a choice of the smoothing bandwidth. In particular, the convolution-type smoothing (conquer as coined by He et al. (2023)) has received recent attention in the literature because the optimization problem is convex, so it is more scalable. Figure 1 of He et al. (2023) shows that conquer works well for point estimations when the sample size ranges from 103 \u223c 106 with the number of regressors d \u2248 n1/2.\nMeanwhile, in terms of inference, both types of smoothed estimators are first-order asymptotically equivalent to the unsmoothed estimator \u03b2\u0302n, and the scale of the numerical studies of these solutions is less ambitious. For instance, the largest model considered for inference in He et al. (2023) is merely (n, d) = (4000, 100) (see Figure 7 in their paper). Therefore, there is a scalability gap between point estimation and inference in the literature. In this paper, we aim to bridge this gap by proposing a method for fast inference.\n1.2. The proposed fast S-subGD framework. We focus on the inference problem at the scale up to (n, d) \u223c (107, 103), which we refer to as an \u201cultra-large\u201d quantile regression problem. This is a possible scale with IPUMS USA, but very difficult to deal with using benchmark inference procedures for quantile regression. To tackle this large-scale problem, we estimate \u03b2\u2217 via stochastic (sub)gradient descent (S-subGD). Our assumption on the asymptotic regime is that d is fixed but n grows to infinity.\nSuppose that we have i.i.d. data from a large cross-sectional survey:\nY1, ..., Yn, Yi = (xi, yi),\nwhere the ordering of these observations is randomized. Our method produces a sequence of estimators, denoted by \u03b2i, which is a solution path being updated as\n\u03b2i = \u03b2i\u22121 \u2212 \u03b3i\u2207q (\u03b2i\u22121, Yi) .\nHere Yi is a new observation in the randomized data sequence. Also, \u2207q (\u03b2i\u22121, Yi) is a subgradient of the check function with respect to the current update, and \u03b3i is a predetermined learning rate. Then, we take the average of the sequence\n\u03b2\u0304n := 1\nn n\u2211 i=1 \u03b2i,\nwhich is called as the Polyak (1990)-Ruppert (1988) average in the machine learning literature. It will be shown in this paper that \u221a n(\u03b2\u0304n \u2212 \u03b2\u2217) is asymptotically normal with the asymptotic variance the same as that of the standard Koenker and Bassett (1978) estimator \u03b2\u0302n. Therefore, it achieves the same first-order asymptotic efficiency.\nThe main novelty of this paper comes from how to conduct inference based on \u03b2\u0304n. We use a sequential transformation of \u03b2i\u2019s in a suitable way for on-line updating, and construct asymptotically pivotal statistics. That is, we studentize \u221a n ( \u03b2\u0304n \u2212 \u03b2\u2217\n) via a random scaling matrix V\u0302n whose exact form will be given later. The resulting statistic is not asymptotically normal but asymptotically pivotal in the sense that its asymptotic distribution is free of any unknown nuisance parameters; thus, its critical values are easily available. Furthermore, the random scaling quantity V\u0302n does not require any additional input other than stochastic subgradient path \u03b2i, and can be computed fast.\nThe main contribution of this paper is computational. We combine the idea of stochastic subgradient descent with random scaling and make large-scale inference for quantile regression much more practical. We provide a discussion of computational complexity and show that our algorithm is comparable to the existing ones in terms of point estimation and is superior to those in terms of inference (especially, subvector inference). See Section 2.6 for details. In addition, we demonstrate the usefulness of our method via Monte Carlo experiments. Empirically, we apply it to studying the gender gap in college wage premiums using the IPUMS dataset. The proposed inference method of quantile regression to the ultra-big dataset reveals some interesting new features. First, it shows heterogeneous effects over different quantile levels. Second, we find that the female college wage premium is significantly higher than that of male workers at the median, which is different from what have been concluded in the literature. In fact, although the S-subGD inference tends to be conservative than\nthe standard normal approximation, it reveals statistically significant results while controlling over 103 covariates to mitigate confounding effects. With more availability of such a large dataset, the S-subGD method will make it possible to obtain convincing empirical evidence for other analyses. Another topic is to examine the relationship between seasonality and birth weight (e.g., Buckles and Hungerman, 2013; Currie and Schwandt, 2013; Chatterji et al., 2022) because the quantiles of birth weight are of interest and birth data from vital statistics contain tens of millions of observations.2\n1.3. The related literature. Our estimator is motivated from the literature on online learning, where data were collected in a streaming fashion, and as a new observation Yi arrives, the estimator is updated to \u03b2i. Under the M-estimation framework with strongly convex, differentiable loss functions, Polyak and Juditsky (1992) showed that the Polyak (1990)-Ruppert (1988) average is asymptotically normal. More recently, Fang et al. (2018), Chen et al. (2020), and Zhu et al. (2023) studied the statistical inference problem and proposed methods that require the implementation of bootstrap, consistent estimation of the asymptotic variance matrix, or the use of \u201cbatch-means\u201d approach, respectively. Besides the smoothness assumption that excludes quantile regression, neither consistent estimation of the asymptotic variance matrix nor implementation of bootstrap is computationally attractive for datasets as large as those with which this paper is concerned. The batch-means approach does not seem to provide adequate finite sample coverage even if n is sufficiently large (see, e.g., Lee et al., 2022, in the context of linear mean regression and logistic regression models).\nThe idea of random scaling is borrowed from the time-series literature on fixed bandwidth heteroskedasticity and autocorrelation robust (HAR) inference (e.g., Kiefer et al., 2000; Sun et al., 2008; Sun, 2014; Lazarus et al., 2018) and has been recently adopted in the context of stochastic gradient descent (SGD) in machine-learning problems: online inference for linear mean regression (Lee et al., 2022), federated learning (Li et al., 2022), and Kiefer-Wolfowitz methods (Chen et al., 2021) among others.3 The current paper shares the basic inference idea with our previous work (Lee et al., 2022); however, quantile regression is sufficiently different from mean regression and it requires further theoretical development that is not covered in the existing work. In particular, our previous work is based on Polyak and Juditsky (1992), which 2For example, the National Bureau of Economic Research has public use data at https://www.nber. org/research/data/vital-statistics-natality-birth-data. 3Li et al. (2022) and Chen et al. (2021) applied the idea of random scaling after our previous work (Lee et al., 2022) appeared on arXiv.\nlimits its analysis to differentiable and strongly convex objective functions; however, the check function q(\u03b2, Yi) for quantile regression is non-differentiable and is not strongly convex. We overcome these difficulties by using the results given in Gadat and Panloup (2023). One could have adopted convolution-type smoothing for S-subGD as the corresponding objective function is differentiable and locally strongly convex. We do not go down this alternative route in this paper as the existence of a sequence of smoothing bandwidths converging to zero complicates theoretical analysis and it would be difficult to optimally choose the sequence of smoothing bandwidths along with the sequence of learning rates. Previously, Pesme and Flammarion (2020) studied online robust regression in the context of a Gaussian linear model with an adversarial noise; in fact, their algorithm corresponds to our median regression case (see equation (2) in their paper) but they did not study quantile regression models. Gadat and Panloup (2023) considered, as their example, the recursive quantile estimator, which corresponds to the quantile regression estimator with an intercept being only the regressor. Neither of these two papers delved into the issue of inference.\nIn the econometric literature, Forneron and Ng (2021) and Forneron (2022) developed SGD-based resampling schemes that deliver both point estimates and standard errors within the same optimization framework. The major difference is that their stochastic path is a Newton-Raphson type which requires computing the Hessian matrix or its approximation. Hence, their framework is not applicable to quantile regression because the check function is not twice differentiable. In other words, for quantile regression inferences with over millions of observations, estimating the inverse Hessian matrix H\u22121 in (3) is a task that we particularly would like to avoid.\nAs an alternative to full sample estimation, one may consider sketching (e.g., see Lee and Ng, 2020, for a review from an econometric perspective). For example, Portnoy and Koenker (1997) developed a preprocessing algorithm that can be viewed as a sketching method and Yang et al. (2013) proposed a fast randomized algorithm for large-scale quantile regression and solved the problem of size n \u223c 1010 and d = 12 by randomly creating a subsample of about n = 105. However, the theoretical analysis carried out in Yang et al. (2013) is limited to approximations of the optimal value of the check function and does not cover the issue of inference. As an alternative sketching method, one may employ Wang and Ma (2021) and construct a random subsample using data-dependent, nonuniform weights to improve the efficiency of the estimator. In mean regression models, the precision of a sketched estimator depends on the subsample size and is typically worse than that of the full sample estimator\n(e.g., Lee and Ng, 2020, 2022). We expect a similar phenomenon for quantile regression models.\n1.4. Notation. Let a\u2032 and A\u2032, respectively, denote the transpose of vector a and matrix A. Let \u2225a\u2225 denote the Euclidean norm of vector a and \u2225A\u2225 the Frobenius norm of matrix A. Also, let \u2113\u221e [0, 1] denote the set of bounded continuous functions on [0, 1]. Let I(A) be the usual indicator function, that is I(A) = 1 if A is true and 0 otherwise. For a symmetric, positive definite matrix S, let \u03bbmin(S) denote its smallest eigenvalue."
        },
        {
            "heading": "2. A Fast Algorithm for Quantile Inference",
            "text": "2.1. Stochastic subgradient descent (S-subGD). In this section, we propose our inference algorithm. Suppose that we have cross-sectional data {Y1, ..., Yn}, where Yi = (xi, yi).\nFirst, we randomize the ordering of the observed data:\nY1 = (x1, y1), ..., Yn = (xn, yn).\nWe start with an initial estimator, denoted by \u03b20, using a method which we shall discuss later. Then produce an S-subGD solution path which updates according to the rule:\n\u03b2i = \u03b2i\u22121 \u2212 \u03b3i\u2207q (\u03b2i\u22121, Yi) ,(4)\nwhere the updating subgradient depends on a single \u201cnext\u201d observation Yi:\n\u2207q(\u03b2, Yi) := xi[I{yi \u2264 x\u2032i\u03b2} \u2212 \u03c4 ].(5)\nThis is the subgradient of q (\u03b2, Yi) with respect to \u03b2. Here \u03b2i\u22121 is the \u201ccurrent\u201d estimate, which is updated to \u03b2i when the next observation Yi comes into play. Also \u03b3i is a pre-determined step size, which is also called a learning rate and is assumed to have the form \u03b3i := \u03b30i\u2212a for some constants \u03b30 > 0 and a \u2208 (1/2, 1). Then, we take the average of the sequence \u03b2\u0304n := 1n \u2211n i=1 \u03b2i as the final estimator.\nComputing this estimator does not require storing the historical data; thus, it is very fast and memory-efficient even when n \u223c 107. In fact, as we shall see below, the computational gain is much more substantial for inference.\n2.2. The pivotal statistic. It is most common to make inference based on a consistent estimator of the asymptotic variance for \u221a n(\u03b2\u0304n \u2212 \u03b2\u2217), which is computationally demanding. For instance, one needs to estimate the inverse Hessian matrix H\u22121 in\n(3). Instead of pursuing asymptotic normal inference using a consistent estimator of the asymptotic variance, we employ asymptotic mixed-normal inference based on a suitable random scaling method through the partial sum process of the solution path as done in the fixed-b standardization. Motivated by the fact that the solution path \u03b2i is sequentially updated, we apply the random scaling approach by defining:\nV\u0302n := 1\nn n\u2211 s=1 { 1\u221a n s\u2211 i=1 ( \u03b2i \u2212 \u03b2\u0304n )}{ 1\u221a n s\u2211 i=1 ( \u03b2i \u2212 \u03b2\u0304n )}\u2032 .(6)\nInference will be conducted based on the standardization using V\u0302n. Computing V\u0302n can be efficient even if n \u223c 107 or larger, since it can be sequentially computed as detailed later. Furthermore, its subvector inference involves only the associated elements, substantially reducing the computational burden.\nOnce \u03b2\u0304n and V\u0302n are obtained, the inference is straightforward. For example, for the j th component of \u03b2\u0304n, let V\u0302n,jj denote the (j, j) th diagonal entry of V\u0302n. The t-statistic is then\n\u221a n ( \u03b2\u0304n,j \u2212 \u03b2\u2217j )\u221a V\u0302n,jj ,(7)\nwhose asymptotic distribution is mixed normal and symmetric around zero. We will formally derive it in the next section. The mixed normal asymptotic distribution for the t-statistic in (7) is the same as the distribution of the statistics observed in the estimation of the cointegration vector by Johansen (1991) and Abadir and Paruolo (1997) and in the heteroskedasticity and autocorrelation robust inference in Kiefer et al. (2000). They are different statistics but converge to the identical distribution, a functional of the standard Wiener process as shown by Abadir and Paruolo (2002). We can use the t-statistic to construct the (1\u2212 \u03b1) asymptotic confidence interval for the j-th element \u03b2\u2217j of \u03b2\u2217 by\u03b2\u0304n,j \u2212 cv(1\u2212 \u03b1/2) \u221a V\u0302n,jj n , \u03b2\u0304n,j + cv(1\u2212 \u03b1/2) \u221a V\u0302n,jj n\n , where the critical value cv(1\u2212 \u03b1/2) is tabulated in Abadir and Paruolo (1997, Table I). For easy reference, the critical values are reproduced in Table 1. For instance, the critical value for \u03b1 = 0.05 is 6.747. Critical values for testing linear restrictions H0 : R\u03b2 \u2217 = c can be found in Kiefer et al. (2000, Table II).\nNote. The table gives one-sided asymptotic critical values that satisfy Pr(t\u0302 \u2264 c) = p asymptotically, where p \u2208 {0.9, 0.95, 0.975, 0.99}. Source: Abadir and Paruolo (1997,\nTable I).\n2.3. Practical details for memory efficiency. While computing (V\u0302n, \u03b2\u0304n) is straightforward for datasets of medium sample sizes, it is not practical when n \u223c 107 or even larger size, as storing the entire solution path \u03b2i can be infeasible for memory of usual personal computing devices.\nWe can construct a solution path for (V\u0302n, \u03b2\u0304n) along the way of updating \u03b2i, so all relevant quantities of the proposed inference can be obtained sequentially. Specifically, let (\u03b2\u0304i\u22121, \u03b2i\u22121) be the current update when we use up to i\u2212 1 observations. We then update using the new observation Yi by:\n\u03b2i = \u03b2i\u22121 \u2212 \u03b3i\u2207q(\u03b2i\u22121, Yi),(8)\n\u03b2\u0304i = \u03b2\u0304i\u22121 i\u2212 1 i + \u03b2i 1 i ,(9)\nV\u0302i = i \u22122 ( Ai \u2212 \u03b2\u0304ib\u2032i \u2212 bi\u03b2\u0304\u2032i + \u03b2\u0304i\u03b2\u0304\u2032i i\u2211 s=1 s2 ) ,(10)\nwhere the intermediate quantities Ai and bi are also updated sequentially:\nAi = Ai\u22121 + i 2\u03b2\u0304i\u03b2\u0304 \u2032 i, bi = bi\u22121 + i 2\u03b2\u0304i.\nTherefore computing (\u03b2\u0304n, V\u0302n) can be cast sequentially until i = n, and does not require saving the entire solution path of \u03b2i.\nA good initial value could help achieve the computational stability in practice, though it does not matter in the theoretical results. We recommend applying conquer in He et al. (2023) to the subsample (e.g. 5% or 10%) to estimate the initial value, which we adopted in our numerical experiments. Specifically, we start with a smooth quantile regression proposed in Fernandes et al. (2021) and He et al. (2023):\n\u03b20 = argmin \u2211 i\u2208S qn(\u03b2, Yi),\nwhere qn is a convolution-type-smoothed checked function, and S is a randomized subsample. The conquer algorithm is very fast to compute a point estimate of \u03b20 but\nit is much less scalable with respect to inference (see the Monte Carlo results later in the paper). For the intermediate quantities Ai and bi, we set A0 = 0 and b0 = 0.\nAnother approach is the burn-in method that applies the S-subGD to some initial observations, and throw them away for the main inference procedure. In our experiments, initial values estimated by conquer outperform those by the burn-in method because conquer tends to produce high-quality initial solutions.\n2.4. Choice of the learning rate. We introduce a practical guideline for determining the value of \u03b3i = \u03b30i\u2212a. Let \u03c3\u0302 denote the sample standard deviation of yi using subsample S. Then, we suggest to fix a = 0.501 and select the initial learning rate \u03b30 via\n\u03b30 = 1\n\u03c3\u0302 \u03d5(\u03a6\u22121(\u03c4))\u221a \u03c4(1\u2212 \u03c4) ,(11)\nwhere \u03d5(\u00b7) and \u03a6(\u00b7) are standard normal density and distribution functions, respectively. For example, if \u03c4 = 0.5, then \u03b30 \u2248 0.798/\u03c3\u0302; if \u03c4 = 0.1 or 0.9, then \u03b30 \u2248 0.585/\u03c3\u0302. The rule-of-thumb selection of \u03b30 is heuristically motivated from the asymptotic distribution of the sample quantile when the regression errors are normally distributed.\n2.5. Subvector inference. In most empirical studies, while many covariates are being controlled in the model, of interest are often just one or two key independent variables that are policy-related. In such cases, we are particularly interested in making inference for subvectors, say \u03b2\u22171 , of the original vector \u03b2\u2217. Subvector inference is not often direct target of interest in the quantile regression literature, partially because inference regarding the full vector \u03b2\u2217 and then converting to the subvector is often scalable up to the medium sample size. But this is no longer the case for the ultra-large sample size that is being considered here.\nIt is straightforward to tailor our updating algorithm to focusing on subvectors. Continue denoting \u03b2i as the i-th update of the full vector. While the full vector \u03b2i still needs to be computed in the S-subGD, both the Polyak-Ruppert average and the random scale can be updated only up to the scale of the subvector. Specifically, let \u03b2\u0304i,sub denote the subvector of \u03b2\u0304i, corresponding to the subvector of interest. Also, let\nV\u0302i,sub denote the sub-matrix of V\u0302i; both are updated according to the following rule:\n\u03b2i = updated the same as (8),(12)\n\u03b2\u0304i,sub = \u03b2\u0304i\u22121,sub i\u2212 1 i + \u03b2i,sub 1 i ,(13)\nV\u0302i,sub = i \u22122 ( Ai,sub \u2212 \u03b2\u0304i,subb\u2032i,sub \u2212 bi,sub\u03b2\u0304\u2032i,sub + \u03b2\u0304i,sub\u03b2\u0304\u2032i,sub i\u2211 s=1 s2 ) ,(14)\nAi,sub = Ai\u22121,sub + i 2\u03b2\u0304i,sub\u03b2\u0304 \u2032 i,sub, bi,sub = bi\u22121,sub + i 2\u03b2\u0304i,sub.(15)\nIn most cases, steps (13)-(15) only involve small dimensional objects.\n2.6. Computational complexity. Portnoy and Koenker (1997) demonstrated that the computational complexity of the interior point algorithm for quantile regression can be on the order of O(nd3 log2 n) (see discussions after Theorem 5.1 of Portnoy and Koenker (1997)). They further proposed an initial phase of preprocessing that effectively reduces computational complexity to O(n2/3d3 log2 n)+O(nd) (see Portnoy and Koenker, 1997, p. 290). The basic idea behind preprocessing is that using a preliminary estimate of \u03b2\u2217, one can remove observations that are above and below the quantile regression line with high probability and add two pseudo-observations called \u201cglobs\u201d, resulting in the reduction of the sample size from n to O(n2/3).4 These results indicate that the interior point algorithm works well with a large n but not with a large d. In addition, Chen (2007) developed a smoothing algorithm that has the computational complexity of O(nd2.5), which reduces the dependence on d from d3 to d2.5. In He et al. (2023), conquer is computed using gradient descent (GD), which has the computational complexity of O(ndk), where k is the number of iterations in GD.\nHowever, the comparative advantage is more pronounced in statistical inference, which incurs additional computational costs. Our inference, based on random scaling, entails an additional cost of order O(nd2). Nevertheless, it reduces to O(nd) when the object of inference is a subvector of \u03b2\u2217, with a dimension of s = O(d1/2). This reduction occurs because we only need to update a submatrix of the random scaling matrix, as discussed in the previous section. Similar gains are achieved when focusing on the inference of each element separately, as is common in applied practice. This is again because we only need to update the diagonal elements, referred to as diagonal random scaling in Table 2. We emphasize that achieving the same order of computational cost for subvector inference is generally not possible with plug-in inference. This is due to the fact that the plug-in estimator of the sandwich formula (H\u22121\u03a3H\u22121) of asymptotic variance in (3) cannot be computed using only diagonal elements or a submatrix.\nAs for the computational cost of competing methods for QR inference, first, the bootstrap inference will be time consuming as the computational cost will be a multiple of bootstrap draws. For instance, bootstrap inference with conquer has the computational complexity of O(bndk), where b is the number of bootstrap draws. Second, it\nrequires access to the entire sample to conduct standard plug-in inference based on asymptotic normality. Thus, the plug-in inference has the additional computational complexity of O(nd2) even with the conquer algorithm. This will be faster than bootstrap inference if k > d, but cannot be reduced to O(ns2) or O(nd) even for subvector inference.\nIn short, our algorithm can be as competitive as conquer in terms of computational cost, while offering the advantages of lower memory requirements and faster computations for subvector inference."
        },
        {
            "heading": "3. Asymptotic Theory",
            "text": "In this section, we present asymptotic theory that underpins our inference method. We consider the following QR model:\nyi = x \u2032 i\u03b2 \u2217 \u03c4 + \u03b5i,\nwhere P (\u03b5i \u2264 0|xi) = \u03c4 for a fixed quantile \u03c4 \u2208 (0, 1). Denote by \u03b2\u2217 = \u03b2\u2217\u03c4 for simplicity.\nAssumption 3.1 (Conditions for Quantile Regression). Suppose:\n(i) The data {Yi \u2261 (yi, xi)}ni=1 are independent and identically distributed. (ii) The conditional density f\u03b5(\u00b7|xi) of \u03b5i given xi and its partial derivative dd\u03b5f\u03b5(\u00b7|xi)\nexist, and furthermore, supb E[\u2225xi\u22253A(b, xi)] < C for some constant C < \u221e, where\nA(b, xi) := \u2223\u2223\u2223\u2223 dd\u03b5f\u03b5(x\u2032ib|xi) \u2223\u2223\u2223\u2223+ f\u03b5(x\u2032ib|xi).\n(iii) There exist positive constants \u03f5 and c0 such that\ninf |\u03b2\u2212\u03b2\u2217|<\u03f5\n\u03bbmin (E[xix\u2032if\u03b5(x\u2032i(\u03b2 \u2212 \u03b2\u2217)|xi)]) > c0.\n(iv) E[(\u2225xi\u22256 + 1) exp(\u2225xi\u22252)] < C for some constant C < \u221e. (v) \u03b3i = \u03b30i\u2212a for some 1/2 < a < 1.\nConditions (ii) imposes some moment conditions on the conditional density, and is satisfied if both f\u03b5(\u00b7|xi) and dd\u03b5f\u03b5(\u00b7|xi) are uniformly bounded along with a moment condition on xi. Both conditions (ii) and (iii) are imposed here to establish the consistency of \u03b2\u0304n.\nCondition (iii) can be viewed as a local identifiability condition, which ensures that the population loss function is locally strongly convex, and is reasonable as \u03b2 7\u2192 q(\u03b2, Yi) is convex.\nCondition (iv) requires a tail condition on the regressors. It is stronger than the usual moment conditions for standard QR inference, but is required by both consistency and the asymptotic inference for S-subGD. In the literature, it is common to adopt stronger conditions on the regressors than on the error depending on the context. For example, Zheng et al. (2018) and Chen and Lee (2023) assumed the uniform boundedness of each of the covariates; the former paper emphasized that a linear quantile regression model with compact support for the regressors is most sensible to avoid the problem of quantile crossing.\nOne of the key technical steps in the proof of such one-observation-at-a-time updating, is to show that the nonlinear updating path is linearizable. Specifically, define \u2206i = \u03b2i\u2212\u03b2\u2217, then the S-subGD updating can be rewritten using the estimation path:\n\u2206i = \u2206i\u22121 \u2212 \u03b3i\u2207Q (\u03b2i\u22121) + \u03b3i\u03bei,\nwhere \u03bei = \u2207Q(\u03b2i\u22121)\u2212\u2207q(\u03b2i\u22121, Yi) is a martingale difference sequence. We then show that the path of \u2206i can be approximated by a linear path:\n\u22061i := \u2206 1 i\u22121 \u2212 \u03b3iH\u22061i\u22121 + \u03b3i\u03bei and \u220610 = \u22060,\nwhere H = \u22072Q(\u03b2\u2217) is the population Hessian matrix. Such an linearization, as one of the technical steps in the proof, transfers the study of nonlinear SGD/S-subGD problems into linear problems.\nCondition (v) gives a requirement for the learning rate, which is standard for online learning. Polyak and Juditsky (1992) showed that a < 1 is needed for the learning rate to decrease sufficiently slow so that the asymptotic normality of the Polyak-Ruppert averaging estimator holds. Meanwhile, for nonlinear models as in the QR model, it should not degenerate too slowly so a > 1/2 is also needed.\nWe extend Lee et al. (2022) (who assumed the sample loss function to be smooth and globally convex) to the quantile regression model, and establish a functional central limit theorem (FCLT) for the aggregated solution path of the S-subGD. Under Assumption 3.1, we establish the following FCLT:\n(16) 1\u221a n [nr]\u2211 i=1 (\u03b2i \u2212 \u03b2\u2217) \u21d2 \u03a51/2W (r) , r \u2208 [0, 1] , where \u21d2 stands for the weak convergence in \u2113\u221e [0, 1], W (r) stands for a vector of the independent standard Wiener processes on [0, 1], and \u03a5 := H\u22121SH\u22121, with S := E[xix\u2032i]\u03c4(1\u2212 \u03c4) and H := E[xix\u2032if\u03b5(0|xi)].\nThe FCLT in (16) states that the partial sum of the sequentially updated estimates \u03b2i converges weakly to a rescaled Wiener process, with the scaling matrix equal to a square root of the asymptotic variance of the usual quantile regression estimator \u03b2\u0302n. Building on the FCLT, we propose a large-scale inference procedure.\nFor any \u2113 \u2264 d linear restrictions\nH0 : R\u03b2 \u2217 = c,\nwhere R is an (\u2113\u00d7 d)-dimensional known matrix of rank \u2113 and c is an \u2113-dimensional known vector, the conventional Wald test based on V\u0302n is asymptotically pivotal. We formally state the main theoretical result in the following theorem.\nTheorem 3.1 (Main Theorem). Suppose that H0 : R\u03b2\u2217 = c holds with rank(R) = \u2113. Under Assumption 3.1, the FCLT in (16) holds and\nn ( R\u03b2\u0304n \u2212 c )\u2032 ( RV\u0302nR \u2032 )\u22121 ( R\u03b2\u0304n \u2212 c ) d\u2192 W (1)\u2032(\u222b 1\n0\nW\u0304 (r)W\u0304 (r)\u2032dr )\u22121 W (1) ,\nwhere W is an \u2113-dimensional vector of the standard Wiener processes and W\u0304 (r) := W (r)\u2212 rW (1).\nAs an important special case of Theorem 3.1, the t-statistic defined in (7) converges in distribution to the following pivotal limiting distribution: for each j = 1, . . . , d,\n\u221a n ( \u03b2\u0304n,j \u2212 \u03b2\u2217j )\u221a V\u0302n,jj d\u2192 W1 (1) [\u222b 1 0 {W1 (r)\u2212 rW1 (1)}2 dr ]\u22121/2 ,(17)\nwhere W1 is a one-dimensional standard Wiener process.\n3.1. Testing for homegeneity between two quantiles. Next, consider two quantiles \u03c41, \u03c42 \u2208 (0, 1), and let \u03b2\u2217\u03c41 and \u03b2 \u2217 \u03c42\ndenote the corresponding coefficients at these two quantiles. We are interested in testing the heterogeneity at the two quantile levels:\nH0 : \u03b2 \u2217 \u03c41,sub = \u03b2\u2217\u03c42,sub.\nwhich is to test whether particular subvectors of the coefficients at the two quantile levels are the same. For instance, the subvector could be the slope of a particular regressor of interest, and we are interested in testing whether its quantile effect stays the same when the quantile level changes from \u03c41 to \u03c42.\nBoth subvector coefficients can be iteratively estimated using S-subGD, whose stochastic paths (\u03b21i , \u03b2\u03041i,sub) and (\u03b22i , \u03b2\u03042i,sub) are constructed following the Algorithm (13)-(15). In particular, the stacked random scaling matrix V\u0304i,sub corresponding to the\ntwo subvectors (\u03b2\u03041i,sub, \u03b2\u03042i,sub) is updated as:\n\u03b8i := ( \u03b2\u03041i,sub \u03b2\u03042i,sub ) ,\nAi,sub = Ai\u22121,sub + i 2\u03b8\u0304i,sub\u03b8\u0304 \u2032 i,sub, bi,sub = bi\u22121,sub + i 2\u03b8\u0304i,sub,\nV\u0304i = i \u22122 ( Ai \u2212 \u03b8\u0304i,subb\u2032i,sub \u2212 bi,sub\u03b8\u0304\u2032i,sub + \u03b8\u0304i,sub\u03b8\u0304\u2032i,sub i\u2211 s=1 s2 ) .\nIn this case, we let G := (I,\u2212I), and form a test statistic:\nn(\u03b2\u03041n,sub \u2212 \u03b2\u03042n,sub)\u2032(GV\u0304n,subG\u2032)\u22121(\u03b2\u03041n,sub \u2212 \u03b2\u03042n,sub).\nWe reject the null if the test statistic is large. The following theorem establishes the asymptotic null distribution.\nTheorem 3.2. Suppose that H0 : \u03b2\u2217\u03c41,sub = \u03b2 \u2217 \u03c42,sub holds. Under Assumption 3.1,\nn(\u03b2\u03041n,sub \u2212 \u03b2\u03042n,sub)\u2032(GV\u0304n,subG\u2032)\u22121(\u03b2\u03041n,sub \u2212 \u03b2\u03042n,sub) d\u2192 W (1)\u2032 (\u222b 1 0 W\u0304 (r)W\u0304 (r)\u2032dr )\u22121 W (1) ,\nwhere W is an 2\u00d7 dim(\u03b2\u2217\u03c41,sub)-dimensional vector of the standard Wiener processes and W\u0304 (r) := W (r)\u2212 rW (1)."
        },
        {
            "heading": "4. Monte Carlo Experiments",
            "text": "In this section we investigate the performance of the S-subGD random scaling method via Monte Carlo experiments. The main question of these numerical experiments is whether the proposed method is feasible for a large scale model.\nThe simulation is based on the following data generating process:\nyi = x \u2032 i\u03b2 \u2217 + \u03b5i for i = 1, . . . , n,\nwhere xi is a (d + 1)-dimensional covariate vector whose first element is 1 and the remaining d elements are generated from N (0, Id). The error term \u03b5i is generated from N (0, 1), and the parameter value \u03b2\u2217 is set to be (1, . . . , 1). Without loss of generality, we focus on the second element, say \u03b2\u2217(2), of \u03b2\n\u2217. That is, we focus on subvector inference: the confidence interval for \u03b2\u2217(2) as well as testing the null hypothesis of \u03b2 \u2217 (2) = 1. To accommodate a large scale model, the sample size varies in n \u2208 {105, 106, 107}, and the dimension of x varies in d \u2208 {10, 20, 40, 80, 160, 320} in the first set of experiments. Later, we extend the dimension of x into d \u2208 {500, 750, 1000, 1500}. The initial value \u03b20 is estimated by the convolution-type smoothed quantile regression (conquer) in He et al. (2023) and we do not burn in any observations. The learning rate is set to\nbe \u03b3 = \u03b30t\u2212a with \u03b30 = 1 and a = 0.501. The simulation results are summarized from 1000 replications of each design.\nWe compare the performance of the proposed method with four additional alternatives:\n(i) S-subGD: the proposed S-subGD random scaling method. (ii) QR: the classical quantile regression method. Given the scale of models, we\napply the Frisch-Newton algorithm after preprocessing and the Huber sandwich estimate under the local linearity assumption of the conditional density function by selecting \u2018pfn\u2019 and \u2018nid\u2019 options in R package quantreg (CRAN version 5.88).\n(iii) CONQUER-plugin: estimates the parameter using the conquer method, and estimates the asymptotic variance by plugging in the parameter estimates; implemented using the R package conquer (CRAN version 1.3.0). (iv) CONQUER-bootstrap: estimates the parameter using the conquer method, and applies the multiplier bootstrap method (see He et al. (2023) for details); implemented using the R package conquer. We set the number of bootstrap samples as 1000.\n(v) SGD-bootstrap: estimates using the S-subGD method and conduct the inference by an online bootstrap method (see Fang et al. (2018) for details). We set the number of bootstrap samples as 1000.\nWe use the following performance measures: the computation time, the coverage rate, and the length of the 95% confidence interval. Note that the nominal coverage probability is 0.95. The computation time is measured by 10 replications on a separate desktop computer, equipped with an AMD Ryzen Threadripper 3970X CPU (3.7GHz) and a 192GB RAM. Other measures are computed by 1000 replications on the cluster system composed of several types of CPUs. We set the time budget as 10 hours for a single estimation and the RAM budget as 192GB.\nFigures 1\u20132 summarize the simulation result. To save space, we report the results of d = 20, 80, and 320. We provide tables for all the simulation designs in the appendix. First, we observe that S-subGD is easily scalable to a large simulation design. On the contrary, some methods cannot complete the computation within the time/memory budget. For instance, CONQUER-plugin does not work for any d when n = 106 or n = 107 because of the memory budget. QR shows a similar issue when n = 107 and d = 320. CONQUER-bootstrap cannot meet the 10-hour time budget for a single replication when n = 107 and d = 320. They are all denoted as \u2018NA\u2019 in the figures.\nSecond, S-subGD outperforms the alternative in terms of computational efficiency. The bottom panel of Figure 1 shows the relative computation time, and the relative speed improvement is by a factor of 10 or 100. In case of CONQUER-plugin, it is slower than S-subGD in the scale of 1000\u2019s. If we convert them into the actual computation time, S-subGD takes 16 seconds for the design of n = 106 and d = 320. However, QR, CONQUER-bootstrap, and SGD-bootstrap take 1374 seconds (22 minutes 54 seconds), 4113 seconds (1 hour 8 minutes 33 seconds), and 1643 seconds (27 minutes 23 seconds) on average of 10 replications.\nThird, all methods shows satisfactory results in terms of the coverage rate and the length of the confidence interval. The top panel of Figure 2 reports that coverage rates are all around 0.95 although S-subGD and SGD-bootstrap are slightly under-reject and over-reject when d = 320. The average lengths of the confidence interval are reported on the bottom panel. The performance of S-subGD is comparable to QR, CONQUER-plugin, and CONQUER-bootstrap. SGD-bootstrap shows much larger CI lengths, especially when d = 320.\nIn the second set of experiments, we stretch the computational burden by increasing the size of d up to d = 1500 when n = 107. Because of the challenging model scale, we use the cluster system for these exercises and check only the performance of S-subGD focusing on inference of a single parameter. Table 3 reports the simulation result along with different sizes of d. S-subGD still completes the computation within a reasonable time range. The average computation time is under 505 seconds (8 minutes 25 seconds) even for the most challenging design. The coverage rate and CI length becomes slightly worse than those of smaller d\u2019s but they still fall in a satisfactory range.\nIn sum, S-subGD performs well when we conduct a large scale inference problem in quantile regression. It is scalable to the range where the existing alternatives cannot complete the computation within a time/memory budget. In addition to computational efficiency, the coverage rate and CI lengths are also satisfactory. Thus, S-subGD provides a solution to those who hesitate to apply quantile regression with a large scale data for the computational burden."
        },
        {
            "heading": "5. Inference for the College Wage Premium",
            "text": "The study of the gender gap in the college wage premium has been a long-standing important question in labor economics. The literature has pointed out a stylized fact that the higher college wage premium for women as the major cause for attracting more women to attend and graduate from colleges than men (e.g., Goldin et al. (2006); Chiappori et al. (2009)). Meanwhile, Hubbard (2011) pointed out a bias issue associated with the \u201ctopcoded\u201d wage data, where the use of the sample from Current Population Survey (CPS) often censors the wage data at a maximum value. As a remedy for it, Hubbard (2011) proposed to use quantile regression that is robust to censoring. Analyzing the CPS data during 1970-2008, he found no gender difference in the college wage premium later years once the topcoded bias has been accounted for.\nWe revisit this problem by estimating and comparing the college wage premium between women and men. While the data also contains the topcoded issue, the quantile regression is less sensitive to the values of upper-tail wages. By applying the new S-subGD inference to the ultra-big dataset we are using, we aim at achieving the following goals in the empirical study: (1) to identify (if any) the heterogeneous effects across quantiles; (2) to understand the trends in the college wage premium respectively for female and male; and (3) to understand the gender difference in the college wage premium.\nWe use the data from IPUMS USA (Ruggles et al., 2022) that contains several millions of workers. The main motivation of using ultra-large datasets for wage regressions is to deal with high-dimensional regressors within a simple parametric quantile regression model. As has been pointed out in the literature, work experience is an important factor in wage regressions but is difficult to be measured precisely. As career paths may be different across states and gender, one possible means of controlling for the experience is to flexibly interact workers\u2019 age, gender, and state fixed effects, which would create over one thousand regressors. Thus, a very large\nsample size would be desirable to obtain precise estimates. But the ultra-large dataset makes most existing inference procedures for quantile regression fail to work.\n5.1. The data. We use the samples over six different years (1980, 1990, 2000-2015) from IPUMS USA at https://usa.ipums.org/usa/. In the years from 1980 to 2000, we use the 5% State sample which is a 1-in-20 national random sample of the population. In the remaining years, we use the American Community Survey (ACS) each year. The sampling ratio varies from 1-in-261 to 1-in-232 in 2001-2004, but it is set to a 1-in-100 national random sample of the population after 2005. To balance the sample size, we bunch the sample every 5 year after 2001. We also provide the estimation results using separate but smaller samples in 2005, 2010, and 2015 in the appendix, which is similar to those reported in this section.\nWe restrict our sample to White, 18 \u2264 Age \u2264 65, and Wage \u2265 $62, which is a half of minimum wage earnings in 1980 ($3.10\u00d740hours\u00d71/2). Wage denotes the implied weekly wage that is computed by dividing yearly earnings by weeks worked last year. Since 2006, weeks worked last year are available only as an interval value and we use the midpoint of an interval. We only consider full-time workers who worked more than 30 hours per week. Then, we compute the real wage using the personal consumption expenditures price index (PCEPI) normalized in 1980. The data cleaning leaves us with 3.6-4.7 million observations besides 2001-2005, where we have around 2.5 million observations. Table 4 reports some summary statistics on the key variables, where Educ denotes an education dummy for some college or above. The table confirms that female college education has increased substantially over the years and female workers received more college education than male workers since 1990.\n5.2. The quantile regression model. We use the following baseline model:\nlog(Wagei) = \u03b20 + \u03b21Femalei + \u03b22Educi + \u03b23Femalei \u00b7 Educi + \u03b8\u20321Xi + \u03b8\u20322(Xi \u00b7 Femalei) + \u03b5i,\nwhere Wagei is a real weekly wage in 1980 terms, Femalei is a female dummy, Educi is an education dummy for some college or above, and Xi is a vector of additional control variables. For control variable Xi, we use 12 age group dummies with a four-year interval, 51 states dummies (including D.C.), and their interactions. Note that (Xi \u00b7 Femalei) implies that there exist up to 3-way interactions. The model contains 1226 covariates in total. We also add 4 additional year dummies for the 5-year combined samples after 2001. We estimate the model using the proposed S-subGD method over 9 different quantiles: \u03c4 = 0.1, 0.2, . . . , 0.9. We obtain the starting value of S-subGD inference by applying conquer to the 10% subsample.5 For the learning rate \u03b30t\n\u2212a, we set a = 0.501 and \u03b30 by the rule-of-thumb approach described in Section 2.4. As an sensitivity analysis, we also vary a over 17 equi-spaced points in [0.501, 0.661]. The results are quite robust to the choice of a as reported in the online appendix. We use the Compute Canada cluster system equipped with the AMD Milan CPUs whose clock speed is 2.65 GHz and with 650 GB of RAMs. For comparison, we also try to estimate the model using the standard QR method with the \u2018quantreg\u2019 R package, but it fails to compute the result in the given cluster environments.\n5.3. The results. Figures 3\u20134 and Table 5 summarize the estimation results. We also provide the full estimation results in the appendix.\nOur results share several interesting features about the estimated college wage premium. First, some degrees of heterogeneity are found over different quantiles. At the upper tail quantile, \u03c4 = 0.9, male college premium is slightly higher than the female one since 2000, while female college premiums are higher over all years in other quantiles. This shows an interesting feature about high-income individuals that cannot be viewed by mean regression, though sensible economic interpretations might be subject to debates. Also, note that the 95% confidence intervals are slightly wider for tail quantiles (\u03c4 = 0.1 and 0.9, respectively).\nSecond, college premiums increase over time for both male and female. This result coincides with the overall findings in the literature. It is interesting that college premiums get flatter since 2010 for lower quantiles (\u03c4 \u2264 0.5).\nThird, when we focus on the median (\u03c4 = 0.5) as reported in Table 5, we observe that the female-male college premium difference shows an inverse \u201cU-shape\u201d pattern. In addition, the difference is always significantly positive, which is different from the\n5Recall that Xi contains a large number of dummy variables in this empirical application. If the subsample size is too small, conquer may face a singularity issue and cannot compute the initial value. Alternatively, one may use a randomazied algorithm of Yang et al. (2013).\nresult in Hubbard (2011). He found the gender difference to be insignificant starting from around the year 2000 from the CPS data. We also note that the computation time is within a reasonable range meanwhile it is not feasible to estimate the model with the alternative approaches."
        },
        {
            "heading": "6. Discussions",
            "text": "We have proposed an inference method for large-scale quantile regression to analyze datasets whose sizes are of order (n, p) \u223c (107, 103). Based on the stochastic subgradient descent updates, our method runs fast and constructs asymptotically pivotal statistics via random scaling.\nAs an important extension, one can embed variable selections in the S-subGD framework. At the i th update, define the t-statistic ti,j = \u03b2\u0304i,j/ \u221a Vi,jj. for j = 1, ..., d.\n[0.4940,0.4984] [0.4337,0.4429] [0.0544,0.0614] Notes. The male college premium is from \u03b2\u03022 and the female college premium is from \u03b2\u03022 + \u03b2\u03023. Thus, the college premium difference between male and female workers is from \u03b2\u03023.\nThen, variable j is selected if |ti,j| > \u03bb\nfor a tuning parameter \u03bb. For instance, we can set \u03bb = 6.747 which is the critical value for the 95% confidence level of the asymptotic distribution. For each variable j, we obtain a cumulative selection path along the iteration:\nSP (j, i) = 1\ni i\u2211 k=1 1{|tk,j| > \u03bb}.\nThe SP path provides rich information about variable selections, which is particularly attractive when the signal-noise ratio is relatively weak. We would like to leave the theoretical justification of the selection path for future studies.\nBesides the variable selection, there are a few more extensions worth pursuing in future research. First, we may build on Chernozhukov et al. (2022) to develop fast inference for the quantile regression process. Second, while we focus on the regular quantile regression models where the quantile is assumed to be bounded away from both zero and one, our framework is also potentially applicable to extreme quantile regression models (see, e.g., Chernozhukov et al., 2016, for a review). The ability of handling ultra-large datasets is also appealing for extreme quantile regression models because the resulting sample size is considerably larger at the extreme quantiles. A formal treatment for these cases is out of the scope of this paper, and we leave it for\nfuture studies. Third, Shan and Yang (2009) considered, among other things, an online setting for which time series data arrive sequentially and several quantile estimators are combined with weights being updated sequentially. It is an interesting topic to study the properties of our proposed method with time series data. Finally, Liu et al. (2023) developed online inference on population quantiles with privacy concerns. It is another research topic to adapt our inference method for privacy-focused applications."
        },
        {
            "heading": "Appendix A. Proofs",
            "text": "In the appendix, we provide the proof of Theorem 3.1. Especially, we prove Theorem A.1 which is a more general version than Theorem 3.1. Theorem A.1 includes quantile regression as a special case but can be of independent interest. To do so, we first present high-level regularity conditions, which we will verify for quantile regression in Theorem A.2.\nA.1. General Theory for M-estimation that allows possibly non-smooth and locally-strongly-convex loss. We formalize a general inferential theory for M-estimations that allows non-smooth (like quantile regression) and non-globallystrongly-convex loss functions (but locally strongly convex), in the online fashion.\nRecall that \u03b2\u2217 is characterized by\n\u03b2\u2217 := arg min \u03b2\u2208Rd Q (\u03b2) ,\nwhere Q(\u03b2) := E[q(\u03b2, Yi)] is the population loss function. In this subsection, it can be the loss function for general M-estimations, not just the quantile regression. The updating rule is\n\u03b2i = \u03b2i\u22121 \u2212 \u03b3i\u2207q (\u03b2i\u22121, Yi) ,\nwhere \u2207q belongs to the subgradient of q. Next, define\n\u03bei (\u03b2) := \u2207Q (\u03b2)\u2212\u2207q (\u03b2, Yi) ,\nand \u03bei := \u03bei(\u03b2i\u22121). We impose the following conditions.\nAssumption A.1 (IID Data). Data {Yi : i = 1, . . . , n} are i.i.d.\nAssumption A.1 restricts our attention to i.i.d. data. Assumption A.1 implies among other things that the sequence {\u03bei}i\u22651 is a martingale-difference sequence (mds) defined on a probability space (\u2126,F ,Fi, P ). That is, E(\u03bei|Fi\u22121) = 0 almost surely.\nAssumption A.2 (Population Criterion). The real-valued function Q (\u03b2) := E [q (\u03b2, Yi)] is twice continuously differentiable, with:\n(i) sup\u03b2\u2208\u0398 \u2225\u22072Q(\u03b2)\u2225 < \u221e. (ii) There is K1 > 0 and \u03b5 > 0 that for all \u2225\u03b2 \u2212 \u03b2\u2217\u2225 \u2264 \u03b5,\n\u2225\u22072Q(\u03b2)\u2212\u22072Q(\u03b2\u2217)\u2225 \u2264 K1\u2225\u03b2 \u2212 \u03b2\u2217\u2225.\n(iii) In addition, \u22072Q(\u03b2) is positive semi-definite for all \u03b2.\nAssumption A.2 imposes mild regularity conditions on the population criterion function Q(\u03b2). It is similar to Assumption 3.3 of Polyak and Juditsky (1992), but with an important extension that allows for non-differentiable loss functions as in quantile regression.\nAssumption A.3 (Learning Rate). \u03b3i = \u03b30i\u2212a for some 1/2 < a < 1.\nAssumption A.3 is the standard condition on the learning rate.\nAssumption A.4 (local strong convexity). There are constants \u03f5 > 0 and c0 > 0 that satisfy\ninf \u2225\u03b2\u2212\u03b2\u2217\u2225<\u03f5\n\u03bbmin ( \u22072Q(\u03b2) ) > c0.\nThe key difference between Assumption A.4 and those of Polyak and Juditsky (1992) is that in our setting, the strong convexity is imposed only locally on a neighborhood of the true value. In contrast, Polyak and Juditsky (1992) imposed the global strong convexity. They also imposed additional global conditions, which are relaxed here. This relaxation is important for the quantile regression model, which may not be globally strongly convex.\nAssumption A.5 (Sufficient Conditions for Consistency). The sequence {\u03bei}i\u22651 satisfy the following additional conditions.\n(i) E [ \u2225\u03bei\u22256 exp (( 1 + \u2225\u03bei\u22254 )1/2) \u2223\u2223\u2223\u2223Fi\u22121] < \u221e a.s. (ii) Si (\u03b2) := E [ \u03bei (\u03b2) \u03bei (\u03b2) \u2032 |Fi\u22121 ] is uniformly Lipschitz continuous, that is,\n\u2225Si (\u03b21)\u2212 Si (\u03b22)\u2225 \u2264 L\u2225\u03b21 \u2212 \u03b22\u2225 a.s. for some L < \u221e.\n(iii) In addition, for some p \u2265 (1\u2212 a)\u22121, E \u2225\u03bei\u22252p is bounded. Here, a is defined in Assumption A.3.\nConditions (i) and (ii) in Assumption A.5 are imposed to apply Theorem 2 of Gadat and Panloup (2023). In addition, under Assumption A.5 (i)-(ii) combined with the\nlocal strong convexity in Assumption A.4, a global Kurdyka-\u0141ojasiewicz condition in Gadat and Panloup (2023) is satisfied. From here, we can establish the consistency. Condition (iii) of Assumption A.5 is an additional moment condition to strengthen the results for the FCLT.\nAssumption A.6 (Stochastic Disturbances). (i) For K2 > 0 and for all i \u2265 1,\n\u2225\u2207Q(\u03b2i\u22121)\u22252 + E(\u2225\u2207q(\u03b2i\u22121, Yi)\u22252|Fi\u22121) \u2264 K2(1 + \u2225\u03b2i\u22121\u22252) a.s.\n(ii) Denote \u2207q\u2217i := \u2207q (\u03b2\u2217, Yi). Then (a) There is a symmetric and positive definite matrix S, such that as i \u2192 \u221e,\nVar (\u2207q\u2217i |Fi\u22121) = Si(\u03b2\u2217) P\u2212\u2192 S,\nwhere Si(\u00b7) is as defined in Assumption A.5. (b) As C \u2192 \u221e,\nsup i\nE(\u2225\u2207q\u2217i \u22252I(\u2225\u2207q\u2217i \u2225 > C|Fi\u22121) P\u2212\u2192 0.\n(iii) There is a function h(\u00b7) such that h(x) \u2192 0 as \u2225x\u2225 \u2192 0, which satisfies: for all i large enough, almost surely,\nE(\u2225\u2207q (\u03b2i\u22121, Yi)\u2212\u2207q (\u03b2\u2217, Yi) \u22252|Fi\u22121) \u2264 h(\u03b2i\u22121 \u2212 \u03b2\u2217).\nWe now establish the general theorem under these assumptions.\nTheorem A.1 (General Theorem). Under Assumptions A.1-A.6,\n(18) 1\u221a n [nr]\u2211 i=1 (\u03b2i \u2212 \u03b2\u2217) \u21d2 \u03a51/2Wd (r) , r \u2208 [0, 1] , where Wd is a d-dimensional vector of the standard Wiener processes; \u21d2 stands for the weak convergence in \u2113\u221e [0, 1], and \u03a5 := H\u22121SH\u22121, with S being the probability limit of Var (\u2207q (\u03b2\u2217, Yi) |Fi\u22121) and H := \u22072Q(\u03b2\u2217).\nIn addition, under H0 : R\u03b2\u2217 = c for an \u2113\u00d7 d matrix R with rank(R) = \u2113,\nn ( R\u03b2\u0304n \u2212 c )\u2032 ( RV\u0302nR \u2032 )\u22121 ( R\u03b2\u0304n \u2212 c ) d\u2192 W\u2113 (1)\u2032(\u222b 1\n0\nW\u0304\u2113(r)W\u0304\u2113(r) \u2032dr )\u22121 W\u2113 (1) ,\nwhere W\u0304\u2113 (r) := W\u2113 (r)\u2212 rW\u2113 (1).\nA.2. Proof of Theorem A.1.\nProof of Theorem A.1. We extend the proof of Polyak and Juditsky (1992) and (Lee et al., 2022) in two aspects: allow non-globally-strong-convex and possibly nonsmooth\nloss functions. For the former, we first establish the consistency. Then we can focus on a local neighborhood of the true value. It is under Assumption A.4 that the loss function is locally strongly convex. The formal proof is divided in the following steps. Some of them are delegated to Lemma A.1 - A.3 in the online supplement.\nstep 1: consistency. Lemma A.1 shows that\n\u03b2\u0304n \u2192P \u03b2\u2217.\nstep 2: local-linearization. Rewrite (4) as\n(19) \u03b2i = \u03b2i\u22121 \u2212 \u03b3i\u2207Q (\u03b2i\u22121) + \u03b3i\u03bei.\nLet \u2206i := \u03b2i \u2212 \u03b2\u2217 and \u2206\u0304i := \u03b2\u0304i \u2212 \u03b2\u2217 to denote the errors in the i-th iterate and that in the average estimate at i, respectively. Then, subtracting \u03b2\u2217 from both sides of (19) yields that\n\u2206i = \u2206i\u22121 \u2212 \u03b3i\u2207Q (\u03b2i\u22121) + \u03b3i\u03bei.\nNow consider the following linear process \u22061i , defined as:\n\u22061i := \u2206 1 i\u22121 \u2212 \u03b3iH\u22061i\u22121 + \u03b3i\u03bei and \u220610 = \u22060,\nwhere H = \u22072Q(\u03b2\u2217). Furthermore, for r \u2208 [0, 1], introduce a partial sum process\n\u2206\u0304i (r) := i \u22121 [ir]\u2211 j=1 \u2206j, \u2206\u0304 1 i (r) := i \u22121 [ir]\u2211 j=1 \u22061j .\nThis step establishes a uniform approximation of the partial sum process \u2206ri (r) to \u2206\u03041i (r). To this end, we adopt Part 4 in the proof of Polyak and Juditsky (1992)\u2019s Theorem 2. To adopt their proof, we need to establish that the local convexity of the loss function can ensure that the nonlinear updating process \u2206i can be approximated locally (as \u03b2i \u2192P \u03b2\u2217) by the linear process \u22061i . Indeed, as Lemma A.2 shows, in the neighbourhood of \u03b2\u2217, this is the case. In fact, Lemma A.2 verifies that the required conditions, Assumptions 3.1 and 3.2 of Polyak and Juditsky (1992), hold locally. In addition, carefully examining their proof of Part 4, it is only required that the population loss function be twice differentiable, as the linear process \u22061i only depends on the Hessian of the population loss function.\nTherefore, with Lemma A.2, the proof of Part 4 in Polyak and Juditsky (1992) goes through, which establishes:\n\u221a i sup\nr\n\u2225\u2225\u2206\u0304i (r)\u2212 \u2206\u03041i (r)\u2225\u2225 = op (1) .\nHence it suffices to analyze the sequence \u2206\u03041i in place of \u2206\u0304i, and establish the weak convergence of \u2206\u03041i .\nstep 3: weak convergence of \u221a i\u2206\u03041i (r). Following the decomposition in (A10)\nin Polyak and Juditsky (1992), write \u221a i\u2206\u03041i (r) = I (1) (r) + I(2) (r) + I(3) (r) ,\nwhere\nI(1) (r) := 1\n\u03b30 \u221a i \u03b1[ir]\u22060, I (2) (r) := 1\u221a i [ir]\u2211 j=1 H\u22121\u03bej, and I(3) (r) := 1\u221a i [ir]\u2211 j=1 w [ir] j \u03bej,\nwhere \u03b1i \u2264 K and { w [ir] j } is a bounded sequence such that i\u22121 \u2211i j=1 \u2225\u2225wij\u2225\u2225\u2192 0. Then, supr\n\u2225\u2225I(1) (r)\u2225\u2225 = op (1). Suppose for now that E supr \u2225I(3)\u2225p = o(1) for some p \u2265 1. Bounding I(3) requires some involved arguments, as w[ir]j \u03bej is not mds, even though \u03bej is (note that w [ir] j \u03bej is indexed by r). The next step of the proof develops new techniques to bound this term. We now apply the FCLT for mds, see e.g. Theorem 4.2 in Hall and Heyde (1980). To verify its sufficient conditions, we prove Lemma A.3. Then Lemma A.3 is sufficient to verify the conditions in Hall and Heyde (1980) using the same argument of Part 1 of the proof in Polyak and Juditsky (1992). Hence, I(2) converges weakly to a rescaled Wiener process \u03a51/2W (r). Hence\n\u221a i\u2206\u03041i (r) \u21d2 \u03a51/2W (r) .\nstep 4: uniform convergence of I(3)(r). Let Si = \u2211i j=1w i j\u03bej. Let p > (1\u2212 a) \u22121\nand note that\nE sup r \u2225\u2225I(3) (r)\u2225\u22252p \u2264 t\u2212pE sup r \u2225\u2225S[ir]\u2225\u22252p \u2264 t\u2212p i\u2211 m=1 E \u2225Sm\u22252p .\nNote that for each m, Sm is the sum of mds. Hence, due to Burkholder\u2019s inequality (e.g., Theorem 2.10 in Hall and Heyde, 1980),\nE \u2225Sm\u22252p \u2264 CpE \u2223\u2223\u2223\u2223\u2223 m\u2211 j=1 \u2225\u2225wmj \u2225\u2225 2 \u2225\u03bej\u22252 \u2223\u2223\u2223\u2223\u2223 p\n= Cp m\u2211 j1,...,jp=1 \u2225\u2225wmj1\u2225\u22252 \u00b7 \u00b7 \u00b7 \u2225\u2225\u2225wmjp\u2225\u2225\u22252 E \u2225\u03bej1\u22252 \u00b7 \u00b7 \u00b7 \u2225\u2225\u03bejp\u2225\u22252 ,\nFAST INFERENCE FOR QUANTILE REGRESSION 33\nwhere the universal constant Cp depends only on p. Note that E \u2225\u03bej1\u2225 2 \u00b7 \u00b7 \u00b7 \u2225\u2225\u03bejp\u2225\u22252 is bounded since E \u2225\u03bej\u22252p is bounded. Also, the boundedness of\n\u2225\u2225wmj \u2225\u2225 yields\u2211mj=1 \u2225\u2225wmj \u2225\u2225b = O (\u2211m\nj=1 \u2225\u2225wmj \u2225\u2225) for any b. By Lemma 2 in Zhu and Dong (2021), \u2211mj=1 \u2225\u2225wmj \u2225\u2225 = o (ma). These facts yield that\n(20) E \u2225Sm\u22252p = O (( m\u2211 j=1 \u2225\u2225wmj \u2225\u2225 )p) = o(map),\nwhich holds uniformly for m, k. It in turn implies the desired result that\nE sup r \u2225\u2225I(3) (r)\u2225\u22252p \u2264 t\u2212p i\u2211 m=1 o (map) = o ( t1+ap\u2212p ) = o (1) .\nstep 5: FCLT. By the previous three steps:\n(21) 1\u221a n [nr]\u2211 i=1 (\u03b2i \u2212 \u03b2\u2217) = \u221a n\u2206\u0304n(r) = \u221a n\u2206\u03041n(r) + oP (1) = 1\u221a n [nr]\u2211 j=1 H\u22121\u03bej + oP (1).\nWe have 1\u221a n \u2211[nr] j=1H\n\u22121\u03bej \u21d2 \u03a51/2W (r). This establishes the FCLT in (16). step 6: random scaling. Now let Cn(r) := R 1\u221an \u2211[nr] i=1 (\u03b2i \u2212 \u03b2\u2217). Also let \u039b = (R\u03a5R\u2032)1/2. It is well-defined and invertible when l \u2264 d. By the FCLT in the previous step, for some vector of independent standard Wiener process W \u2217(r),\nCn(r) \u21d2 \u039bW \u2217(r).\nIn addition, RV\u0302nR\u2032 = 1n \u2211n s=1[Cn( s n ) \u2212 s n Cn(1)][Cn( s n ) \u2212 s n Cn(1)]\n\u2032. Here the sum is also an integral over r as Cn(r) is a partial sum process, and R(\u03b2\u0304n \u2212 \u03b2\u2217) = 1\u221anCn(1).\nHence n ( R\u03b2\u0304n \u2212 c )\u2032 ( RV\u0302nR \u2032 )\u22121 ( R\u03b2\u0304n \u2212 c )\nis a continuous functional of Cn(\u00b7). Then the continuous mapping theorem proves the theorem. \u25a1\nA.3. Proof of Theorem 3.1.\nProof of Theorem 3.1. This follows directly from Theorem A.1 because all the regularity conditions in Theorem A.1 are verified in Theorem A.2. \u25a1\nWe verify the high-level assumptions with more primitive conditions for the quantile regression model.\nTheorem A.2 (Verifying High-Level Conditions for Quantile regression). Suppose that Assumption 3.1 holds. Then, Assumptions A.1-A.6 hold with S = E[xix\u2032i]\u03c4(1\u2212 \u03c4) and H = E[xix\u2032if\u03b5(0|xi)].\nProof of Theorem A.2. Verify Assumption A.1. It is directly imposed.\nVerify Assumption A.2. It is straightforward to verify the twice differentiability, where\n\u2207Q(\u03b2) = Exi[P (\u03b5i \u2264 x\u2032i(\u03b2 \u2212 \u03b2\u2217)|xi)\u2212 \u03c4 ].\nand \u22072Q(\u03b2) = G(\u03b2) = E[xix\u2032if\u03b5(x\u2032i(\u03b2 \u2212 \u03b2\u2217)|xi)].\n(i) sup\u03b2\u2208\u0398 \u2225\u22072Q(\u03b2)\u2225 \u2264 supb\u0398 \u2225E[xix\u2032if\u03b5(x\u2032ib)|xi)]\u2225 < \u221e. (ii) For all \u2225\u03b2 \u2212 \u03b2\u2217\u2225 \u2264 \u03b5,\n\u2225\u22072Q(\u03b2)\u2212\u22072Q(\u03b2\u2217)\u2225 \u2264 \u2225\u22073Q(\u03b2\u0303)\u2225\u2225\u03b2 \u2212 \u03b2\u2217\u2225\nfor some \u03b2\u0303. Here \u22073Q(\u03b2) is a dim(\u03b2)\u00d7 dim(\u03b2)2 matrix, whose j th row is given by [vec\u22072(\u2202jQ(\u03b2))]\u2032. We note that\n\u2225\u22073Q(\u03b2\u0303)\u2225 \u2264 sup |\u03b2\u2212\u03b2\u2217|<\u03f5 E\u2225xi\u22253 \u2223\u2223\u2223\u2223 dd\u03b5f\u03b5(x\u2032i(\u03b2 \u2212 \u03b2\u2217)|xi) \u2223\u2223\u2223\u2223 < K1. The last inequality follows from Assumption 3.1(ii).\n(iii) It is clear that E[xix\u2032if\u03b5(x\u2032i(\u03b2 \u2212 \u03b2\u2217)|xi)] is globally semipositive definite.\nVerify Assumption A.3. It is directly imposed.\nVerify Assumption A.4. This is imposed by Assumption 3.1 (i) that the eigenvalues of E[xix\u2032if\u03b5(x\u2032i(\u03b2 \u2212 \u03b2\u2217)|xi)] are locally bounded away from zero.\nVerify Assumption A.5. For (i), we have for any r > 0, \u2225\u03bei\u2225r \u2264 C(\u2225xi\u2225r + E\u2225xi\u2225r). Hence\nE [ \u2225\u03bei\u22256 exp (( 1 + \u2225\u03bei\u22254 )1/2) |Fi\u22121] \u2264 CE[\u2225xi\u22256 + 1) exp(\u2225xi\u22252|Fi\u22121] < C. For (ii), it is a straightforward application of the Cauchy-Schwarz and triangular inequalities. For instance, E\u2225xi\u22252|I{\u03b5i < \u03b21}\u2212 I{\u03b5i < \u03b22}| \u2264 supb E\u2225xi\u22252f\u03b5(x\u2032ib)\u2225\u03b21\u2212 \u03b22\u2225. We omit the details. For (iii), note, for all \u03b2, E\u2225\u03bei(\u03b2)\u22252p \u2264 2\u00d7 4pE\u2225xi\u22252p < \u221e.\nVerify Assumption A.6. (i) Write\n\u2225\u2207Q (\u03b2i\u22121) \u22252 \u2264 4(E\u2225xi\u2225)2 \u2264 K E(\u2225\u2207q(\u03b2i\u22121, Yi)|2\u2225Fi\u22121) \u2264 4E(\u2225xi\u22252|Fi\u22121) \u2264 4E\u2225xi\u22252 \u2264 K.\nThis implies\n\u2225\u2207Q (\u03b2i\u22121) \u22252 + E(\u2225\u2207q(\u03b2i\u22121, Yi)\u22252|Fi\u22121) \u2264 2K \u2264 2K(1 + \u2225\u03b2i\u22121\u22252) a.s.\n(ii) Here \u2207q\u2217i = \u03bei(\u03b2\u2217) = xi[I{\u03b5i < 0} \u2212 \u03c4 ]. Clearly, E(\u03bei(\u03b2\u2217)|Fi\u22121) = 0 and (a) Var(\u03bei(\u03b2\u2217)|Fi\u22121) = E[xix\u2032i]\u03c4(1\u2212 \u03c4) = S. (b) Also, as C \u2192 \u221e,\nsup i E(\u2225\u03bei(\u03b2\u2217)\u22252I(\u2225\u03bei(\u03b2\u2217)| > C\u2225Fi\u22121) \u2264 sup i (E\u2225\u03bei(\u03b2\u2217)\u22253|Fi\u22121)2/3P (\u2225\u03bei(\u03b2\u2217)\u2225 > C|Fi\u22121)1/3\n\u2264 4(E\u2225xi\u22253)2/3E(\u2225\u03bei(\u03b2\u2217)\u2225|Fi\u22121)1/3C\u22121/3 P\u2212\u2192 0.\n(iii) Recall \u2207q(\u03b2, Yi) := xi[I{yi \u2264 x\u2032i\u03b2} \u2212 \u03c4 ]. Write\nAi = xi[I{\u03b5i < x\u2032i(\u03b2i\u22121 \u2212 \u03b2\u2217)} \u2212 I{\u03b5i < 0}].\nHence for h(x) = 2 supb E\u2225xi\u22253f\u03b5(x\u2032ib|xi)\u2225x\u2225,\nE(\u2225\u2207q (\u03b2i\u22121, Yi)\u2212\u2207q (\u03b2\u2217, Yi) \u22252|Fi\u22121) \u2264 E(\u2225Ai\u22252|Fi\u22121) \u2264 h(\u03b2i\u22121 \u2212 \u03b2\u2217).\n\u25a1\nA.4. Proof of Theorem 3.2.\nProof of Theorem 3.2. By (21), uniformly in r, and d = 1, 2,\ni\u22121/2 [ir]\u2211 j=1 [\u03b2dj,sub \u2212 \u03b2\u2217\u03c4d,sub] = 1\u221a i [ir]\u2211 j=1 (H\u22121d )sub\u03bej,d + oP (1)\nwhere (H\u22121d )sub denotes the rows of Hd = E[xix\u2032if\u03b5,d(0|xi)] corresponding to the subvector \u03b2\u2217\u03c4d,sub; f\u03b5,d(|x) denotes the conditional density of \u03b5i,d for d = 1, 2,\n\u03bei,d (\u03b2) := Exi[P (\u03b5i,d \u2264 x\u2032i(\u03b2 \u2212 \u03b2\u2217\u03c4d)|xi)\u2212 \u03c4d]\u2212 xi[I{\u03b5i,d \u2264 x \u2032 i(\u03b2 \u2212 \u03b2\u2217\u03c4d)} \u2212 \u03c4d].\nand \u03bei,d := \u03bei,d(\u03b2di\u22121). Then we have\n\u221a i\u2206\u0304i(r) := i \u22121/2 [ir]\u2211 j=1 ( \u03b21j,sub \u2212 \u03b2\u2217\u03c41,sub \u03b22j,sub \u2212 \u03b2\u2217\u03c42,sub ) = 1\u221a i [ir]\u2211 j=1 H\u0304uj+oP (1), H\u0304 = ( (H\u221211 )sub 0 0 (H\u221212 )sub )\nuj := uj(\u03b2 1 j\u22121, \u03b2 2 j\u22121), uj(\u03b2 1, \u03b22) :=\n( \u03bej,1(\u03b2 1)\n\u03bej,2(\u03b2 2) ) Then\nui(\u03b2 \u2217 \u03c41 , \u03b2\u2217\u03c42) = \u2212 ( [I{\u03b5i,1 \u2264 0} \u2212 \u03c41] [I{\u03b5i,2 \u2264 0} \u2212 \u03c42] ) \u2297 xi.\nHence, Var(ui(\u03b2\u2217\u03c41 , \u03b2 \u2217 \u03c42 )|Fi\u22121) \u2192P A\u2297 Exix\u2032i, where\nA :=\n( \u03c41(1\u2212 \u03c41) \u03c41 \u2227 \u03c42 \u2212 \u03c41\u03c42\n\u03c41 \u2227 \u03c42 \u2212 \u03c41\u03c42 \u03c42(1\u2212 \u03c42).\n) .\nNote that H\u0304uj is an mds. We apply the mds CLT to the partial sum 1\u221ai \u2211[ir]\nj=1 H\u0304uj , which converges weakly:\n\u221a i\u2206\u0304i(r) \u21d2 \u03a5\u03041/2W (r), r \u2208 [0, 1]\nwhere W (r) stands for a vector of independent standard Wiener process on [0, 1], and \u03a5\u0304 := H\u0304(A\u2297 Exix\u2032i)H\u0304. Now let Cn(r) := G \u221a n\u2206\u0304n(r), with G = (I,\u2212I). Also let \u039b = (G\u03a5\u0304G\u2032)1/2. Then for some vector of independent standard Wiener process W \u2217(r), Cn(r) \u21d2 \u039bW \u2217(r). In addition,\nGV\u0304nG \u2032 = =\n1\nn n\u2211 s=1 [Cn( s n )\u2212 s n Cn(1)][Cn( s n )\u2212 s n Cn(1)] \u2032\n= \u222b 1 0 [Cn(r)\u2212 rCn(1)][Cn(r)\u2212 rCn(1)]\u2032dr,\nthe last equality holds because Cn(r) is a partial sum. Also, under the null that \u03b2\u2217\u03c41,sub = \u03b2 \u2217 \u03c42,sub ,\n\u03b2\u03041n,sub \u2212 \u03b2\u03042n,sub = G ( \u03b2\u03041n,sub \u2212 \u03b2\u2217\u03c41,sub \u03b2\u03042n,sub \u2212 \u03b2\u2217\u03c42,sub ) = 1\u221a n Cn(1).\nHence the test statistic n(\u03b2\u03041n,sub \u2212 \u03b2\u03042n,sub)\u2032(GV\u0304n,subG\u2032)\u22121(\u03b2\u03041n,sub \u2212 \u03b2\u03042n,sub) equals\nCn(1) \u2032 (\u222b 1\n0\n[Cn(r)\u2212 rCn(1)][Cn(r)\u2212 rCn(1)]\u2032dr )\u22121 Cn(1)\nis a continuous functional of Cn(\u00b7). By the continuous mapping theorem, the test statistic converges weakly to the desired limit. \u25a1"
        },
        {
            "heading": "Appendix A. Additional Lemma\u2019s",
            "text": "Lemma A.1 (L2-Consistency). Let Assumptions A.1- A.5 hold. Then, as n \u2192 \u221e, E\u2225\u03b2\u0304n \u2212 \u03b2\u2217\u22252 = o(1).\nProof of Lemma A.1. The desired conclusion follows immediately from Theorem 2 of Gadat and Panloup (2023) if we verify the conditions imposed in their Theorem 2. In particular, we provide sufficient conditions for a Kurdyka\u2013\u0141ojasiewicz inequality (with r = 0 using the notation in Gadat and Panloup (2023)):\nlim inf |h|\u2192\u221e \u2225\u2207Q (h)\u2225 > 0 and lim sup |h|\u2192\u221e \u2225\u2207Q (h)\u2225 < +\u221e.\nAs the second condition is trivially satisfied, it suffices to show that under Assumption A.5(i)-(ii), lim inf |h|\u2192\u221e \u2225\u2207Q(h)\u2225 > 0. We write G(\u03b2) := \u22072Q(\u03b2). Then the mean-value theorem with the integral remainder yields\n\u2207Q(\u03b2) = \u222b |\u03b2\u2212\u03b2\u2217| 0 G ( (\u03b2 \u2212 \u03b2\u2217) \u2225\u03b2 \u2212 \u03b2\u2217\u2225 s+ \u03b2\u2217 ) (\u03b2 \u2212 \u03b2\u2217) \u2225\u03b2 \u2212 \u03b2\u2217\u2225 ds.\nLet A(\u03b2, s) := G (\n(\u03b2\u2212\u03b2\u2217) \u2225\u03b2\u2212\u03b2\u2217\u2225s+ \u03b2\n\u2217 ) and K(\u03b2) := \u222b \u2225\u03b2\u2212\u03b2\u2217\u2225 0\nA(\u03b2, s)ds. Then K(\u03b2) is positive semi-definite. For any \u2225\u03b2 \u2212 \u03b2\u2217\u2225 > \u03f5, write, with b := (\u03b2 \u2212 \u03b2\u2217),\n\u2225\u2207Q(\u03b2)\u22252 = |b|\u22122bTK(\u03b2)2b \u2265 \u03bb2min(K(\u03b2)) \u2265 \u03bb2min( \u222b \u03f5 0 A(\u03b2, s)ds) \u2265 inf s<\u03f5,|h|=1 \u03bb2min(G(hs+ \u03b2 \u2217))\u03f5 > \u03f5c20.\nNote that infs<\u03f5,|h|=1 \u03bbmin(G(hs+ \u03b2\u2217)) = inf |\u03b2\u2212\u03b2\u2217|<\u03f5 \u03bbmin(G(\u03b2)). Then, with Assumptions A.5, it follows from Theorem 2 of Gadat and Panloup (2023) that E\u2225\u03b2\u0304n\u2212\u03b2\u2217\u22252 = o(1). \u25a1\nLemma A.2 (Local around the True Minimizer). Let H = \u22072Q(\u03b2\u2217) and \u03a8(x) = Q(x+ \u03b2\u2217)\u2212Q(\u03b2\u2217). Suppose Assumptions A.2, A.4, A.6 hold. There exists \u03b5 > 0 that for all \u2225\u03b2 \u2212 \u03b2\u2217\u2225 \u2264 \u03b5, the following results hold.\n(i) \u03a8(0) = 0. (ii) \u03a8(\u03b2 \u2212 \u03b2\u2217) \u2265 \u03b1\u2225\u03b2 \u2212 \u03b2\u2217\u22252 for some \u03b1 > 0. (iii) |\u2207\u03a8(x)\u2212\u2207\u03a8(y)| \u2264 L\u2225x\u2212 y\u2225 for some L > 0. (iv) \u2207\u03a8(\u03b2 \u2212 \u03b2\u2217)\u2032\u2207Q(\u03b2) > 0 for \u03b2 \u0338= \u03b2\u2217. (v) \u2207\u03a8(\u03b2 \u2212 \u03b2\u2217)\u2032\u2207Q(\u03b2) \u2265 \u03bb\u03a8(\u03b2 \u2212 \u03b2\u2217) for some \u03bb > 0. (vi) \u2225\u2207Q(\u03b2)\u2212H(\u03b2 \u2212 \u03b2\u2217)\u2225 \u2264 K1\u2225\u03b2 \u2212 \u03b2\u2217\u22251+\u03b7 for some K1 < \u221e and 0 < \u03b7 \u2264 1.\nProof. Let L = sup\u03b2\u2208\u0398 \u2225\u22072Q(\u03b2)\u2225 and \u22072Q(\u03b2) = G(\u03b2). (i) is naturally satisfied. (ii) Note \u2207Q(\u03b2\u2217) = 0. By the second-order Taylor expansion, \u03a8(x) = 0.5x\u2032\u22072Q(\u03b2\u0303)x, for some \u03b2\u0303, and \u22072Q(\u03b2\u0303) = G(\u03b2\u0303) whose minimum eigenvalue is locally lower bounded by Assumption A.4. So \u03a8(x) \u2265 \u03b1\u2225x\u22252, where x = 0.5c0 for the constant c0 in Assumption A.4.\n(iii) For any \u03b21, \u03b22, there exists \u03b2\u0303,\n\u2225\u2207\u03a8(\u03b21)\u2212\u2207\u03a8(\u03b22)\u2225 = \u2225\u22072Q(\u03b2\u0303)(\u03b21 \u2212 \u03b22)\u2225 \u2264 L\u2225\u03b21 \u2212 \u03b22\u2225.\n(iv) We have \u2207\u03a8(\u03b2 \u2212 \u03b2\u2217) = \u2207Q(\u03b2) = G(\u03b2\u0303)(\u03b2 \u2212 \u03b2\u2217) for some \u03b2\u0303 on the segment joining \u03b2 and \u03b2\u2217. So \u2225\u03b2\u0303 \u2212 \u03b2\u2217\u2225 \u2264 \u2225\u03b2 \u2212 \u03b2\u2217\u2225. Hence uniformly in \u2225\u03b2 \u2212 \u03b2\u2225 < \u03f5, and \u03b2 \u0338= \u03b2\u2217, and c0 > 0 in Assumption A.4,\n\u2207\u03a8(\u03b2 \u2212 \u03b2\u2217)\u2032\u2207Q(\u03b2) = \u2225\u2207Q(\u03b2)\u22252 = (\u03b2 \u2212 \u03b2\u2217)\u2032G(\u03b2\u0303)(\u03b2 \u2212 \u03b2\u2217) \u2265 c0\u2225\u03b2 \u2212 \u03b2\u2217\u22252 > 0.\n(v) For some \u03b2\u0304,\n\u03a8(\u03b2 \u2212 \u03b2\u2217) = Q(\u03b2)\u2212Q(\u03b2\u2217) = 1 2 (\u03b2 \u2212 \u03b2\u2217)\u2032G(\u03b2\u0304)(\u03b2 \u2212 \u03b2\u2217),\nwhich is upper bounded by 0.5L|\u03b2 \u2212 \u03b2\u2217|2. Hence \u2207\u03a8(\u03b2 \u2212 \u03b2\u2217)\u2032\u2207Q(\u03b2) \u2265 \u03bb\u03a8(\u03b2 \u2212 \u03b2\u2217) holds for \u03bb \u2264 2c0/L.\n(vi) The Taylor expansion yields, for some \u03b2\u0303 so that \u2225\u03b2\u0303 \u2212 \u03b2\u2217\u2225 \u2264 \u2225\u03b2 \u2212 \u03b2\u2217\u2225, we have \u2207Q(\u03b2) = G(\u03b2\u0303)(\u03b2 \u2212 \u03b2\u2217). Hence by Assumption A.6\n\u2225\u2207Q(\u03b2)\u2212H(\u03b2 \u2212 \u03b2\u2217)\u2225 \u2264 \u2225G(\u03b2\u0303)\u2212H\u2225\u2225\u03b2 \u2212 \u03b2\u2217\u2225 \u2264 K1\u2225\u03b2 \u2212 \u03b2\u2217\u22252.\n\u25a1\nLemma A.3. The sequence {\u03bei}i\u22651 satisfies the following conditions. (i) For some K3 < \u221e and for all i \u2265 1,\nE(\u2225\u03bei\u22252|Fi\u22121) + |\u2207Q(\u03b2i\u22121)|2 \u2264 K3(1 + \u2225\u03b2i\u22121\u22252) a.s.\n(ii) The following decomposition holds: \u03bei = \u03bei(\u03b2\u2217) + \u03b6i(\u03b2i\u22121). They satisfy:\n(a) E [\u03bei(\u03b2\u2217)\u03bei(\u03b2\u2217)\u2032|Fi\u22121] P\u2212\u2192 S as i \u2192 \u221e and S > 0 (S is symmetric and positive definite), (b) sup\ni E(\u2225\u03bei(\u03b2\u2217)\u22252I(\u2225\u03bei(\u03b2\u2217)\u2225 > C|Fi\u22121) P\u2212\u2192 0 as C \u2192 \u221e,\n(c) for all i large enough, E [\u2225\u03b6i(\u03b2i\u22121)\u22252\u2225Fi\u22121] \u2264 \u03b4(\u03b2i\u22121 \u2212 \u03b2\u2217) a.s. with \u03b4(x) \u2192 0 as \u2225x\u2225 \u2192 0.\nProof. (i) We have \u03bei = \u2207Q (\u03b2i\u22121)\u2212\u2207q (\u03b2i\u22121, Yi) . Hence Assumption A.6(i) implies\nE(\u2225\u03bei\u22252|Fi\u22121)+\u2225\u2207Q(\u03b2i\u22121)\u22252 \u2264 3\u2225\u2207Q(\u03b2i\u22121)\u22252+2E(\u2225\u2207q(\u03b2i\u22121, Yi)\u22252|Fi\u22121) \u2264 3K2(1+\u2225\u03b2i\u22121\u22252) a.s.\n(ii) We have \u03bei(\u03b2\u2217) = \u2212\u2207q (\u03b2\u2217, Yi) and\n\u03b6i(\u03b2i\u22121) = \u2207Q (\u03b2i\u22121)\u2212 [\u2207q (\u03b2i\u22121, Yi)\u2212\u2207q (\u03b2\u2217, Yi)] .\nThe desired results (a)(b) then follows from Assumption A.6(ii). As for (c), E [ \u2225\u03b6i(\u03b2i\u22121)\u22252|Fi\u22121 ] \u2264 a1 + a2\na1 = 2\u2225\u2207Q (\u03b2i\u22121)\u2212\u2207Q (\u03b2\u2217) \u22252 \u2264 2 sup b \u2225\u22072Q(b)\u2225\u2225\u03b2i\u22121 \u2212 \u03b2\u2217\u22252 a2 = 2E(\u2225\u2207q (\u03b2i\u22121, Yi)\u2212\u2207q (\u03b2\u2217, Yi) \u22252|Fi\u22121) \u2264 2h(\u03b2i\u22121 \u2212 \u03b2\u2217)\nwhere the function h() for term a2 exists, from Assumption A.6(iii). Hence we can define\n\u03b4(x) = 2 sup b\n\u2225\u22072Q(b)\u2225\u2225x\u22252 + 2h(x)\nand \u03b4(x) \u2192 0 as \u2225x\u2225 \u2192 0 because supb \u2225\u22072Q(b)\u2225 < \u221e and h(x) \u2192 0. \u25a1"
        },
        {
            "heading": "Appendix B. Additional Tables",
            "text": "In this section we provide additional results from Monte Carlo simulation studies and the empirical application. Tables 6\u201311 report performance measures for all simulation designs. Recall that S-subGD stands for the SGD random scaling method, QR for the standard quantile regression method, CONQUER-plugin for the conquer method with the plug-in asymptotic variance, CONQUER-bootstrap for the conquer method with bootstrapping, SGD-bootstrap for the SGD bootstrap method. In addition, we report the performance of S-subGD-all, which applies the S-subGD random scaling method for the full variance-covariance matrix. In contrast, S-subGD updates only the asymptotic variance for \u03b21, which is the parameter of interest.\nB.1. Empirical application: full estimation results. We provide full estimation results.\nEmpirical application: a single year for 2005, 2010, and 2015. We use a dataset composed of a single year sample for 2005, 2010, and 2015 without combining 5-year periods. As a result, the sample sizes for those years are around 25% of the previous years. The computation times for those years are shorter but the confidence intervals are wider. The point estimates are similar to those in the baseline model. For 1980, 1990, and 2000, all results are the same with the baseline model.\nNotes. The graphs show the difference between female college premium and male college premium. The positive number denotes that female college premium is higher than male one. The solid line comes from \u03b2\u03023 each year and the grey area denotes the pointwise 95% confidence interval.\nEmpirical application: robustness checks. To check the robustness of the results, we conduct some sensitivity analysis over different learning rates \u03b30a\u2212a. We keep using the rule-of-thumb method in Section 2.4 for \u03b30. Instead, We vary the value of a over 17 equi-spaced points in [0.501, 0.661]. We estimate the model at quantile \u03c4 = 0.5 over all years. We use the samples that are bunching over 5 years after 2001 as in Section 5. Figures 7-8 report the estimation results over different a values. The graphs are flat over all areas and confirm that the estimation results are robust to the tuning parameter choice.\nNotes. The graphs show the difference between female college premium and male college premium. The positive number denotes that female college premium is higher than male one. The solid line comes from \u03b2\u03023 each year and the grey area denotes the pointwise 95% confidence interval. We estimate the model over 17 equi-spaced points of a in [0.501, 0.661]."
        }
    ],
    "title": "FAST INFERENCE FOR QUANTILE REGRESSION WITH TENS OF MILLIONS OF OBSERVATIONS",
    "year": 2023
}