{
    "abstractText": "Meta-learning from learning curves is an important yet often neglected research area in the Machine Learning community. We introduce a series of Reinforcement Learning-based meta-learning challenges, in which an agent searches for the best suited algorithm for a given dataset, based on feedback of learning curves from the environment. The first round attracted participants both from academia and industry. This paper analyzes the results of the first round (accepted to the competition program of WCCI 2022), to draw insights into what makes a meta-learner successful at learning from learning curves. With the lessons learned from the first round and the feedback from the participants, we have designed the second round of our challenge with a new protocol and a new meta-dataset. The second round of our challenge is accepted at the AutoML-Conf 2022 and currently on-going. 1. Background and motivation Meta-learning has been playing an increasingly important role in Automated Machine Learning. While it is a natural capability of living organisms, who constantly transfer acquired knowledge across tasks to quickly adapt to changing environments, artificial learning systems are still in their meta-learning \u201cinfancy\u201d. They are only capable, so far, to transfer knowledge between very similar tasks. At a time when society is pointing fingers at AI for being wasteful with computational resources, there is an urgent need for learning systems, which recycle their knowledge. To achieve that goal, some research areas have been widely studied, including few-shot learning (Wang et al., 2020), transfer learning (Zhuang et al., 2020), representation learning (Bengio et al., 2013), continual learning (Delange et al., 2021), life-long learning (Chen et al., 2018), and meta-learning (Vanschoren, 2018). However, meta-learning from learning curves, an essential sub-problem in meta-learning (Mohr & van Rijn, 2022), is still under-studied. This motivates the design of this new challenge series we are proposing. Learning curves evaluate algorithm incremental performance improvement, as a function of training time, number of iterations, and/or number of examples. Our challenge design builds on top of previous challenges and work, which considered other aspects of the problem: meta-learning as a recommendation problem, but not from learning curves (Guyon et al., 2019; Liu et al., 2020) and few-shot learning (El Baz et al., 2021). Analysis of past challenges revealed that top-ranking methods often involve switching between algorithms during training, including \u201cfreeze-thaw\u201d Bayesian techniques (Swersky et al., 2014). However, previous challenge protocols did not allow evaluating such methods separately, due to inter-dependencies between various heuristics. Furthermore, we want to study the potential benefit of learned policies, as opposed to applying handcrafted black-box optimization methods. Our challenge took inspiration from MetaREVEAL (Nguyen et al., 2021), ActivMetal (Sun-Hosoya et al., 2018), REVEAL (Sun-Hosoya, 2019), and Freeze-Thaw Bayesian Optimization (Swersky et al., 2014). A meta-learner needs to learn to solve two problems at the same time: algorithm selection and budget allocation. We are interested in metalearning strategies that leverage information on partially trained algorithms, hence reducing the cost of training them to convergence. We offer pre-computed learning curves as a function of time, to facilitate benchmarking. Meta-learners must \u201cpay\u201d a cost emulating computational time for revealing their next values. Hence, meta-learners are expected to learn the exploration-exploitation tradeoffs between continuing \u201ctraining\u201d an already tried good candidate algorithm and checking new candidate algorithms. In this paper, we first recap design of the first round of our challenge. Then, we present the first round results and compare the participants\u2019 solutions with our baselines. After the end of the first round, we have identified some limitations in our design and proposed a new design and a new meta-dataset for the second round. 2. Design of the first round In this section, we describe the design of the first round of our challenge, including: data, challenge protocol, and ar X iv :2 20 8. 02 82 1v 1 [ cs .L G ] 4 A ug 2 02 2 manuscript of paper accepted to CFOL Workshop @ ICML 2022 evaluation. More details can be found on our competition page 1. 2.1. Learning curve data Despite the fact that learning curves are widely used, there were not many learning curve meta-datasets available in the Machine Learning community at the time of organizing this challenge. Taking advantage of 30 cross-domain datasets used in the AutoML challenge (Guyon et al., 2019), we computed de novo learning curves (both on the validation sets and the test sets) for 20 algorithms, by submitting them to the AutoML challenge (Guyon et al., 2019) as postchallenge submissions. These algorithms are created from two base algorithms, Random Forest and Gradient Boosting, but with different values of the max features hyperparameter. We also provided meta-features of each dataset, such as type of learning task, evaluation metric, time budget, etc. In this first round, we considered a learning curve as a function of time. Each point on the learning curve corresponds to the algorithm performance at a certain time. In addition to the aforementioned real-world meta-dataset, we synthetically generated 4000 learning curves for the participants to practice and get familiar with our starting kit. The points on each synthetic learning curve are sampled from a parameterized sigmoid function with its hyperparameters generated from matrix factorizations. This allows us to create some hidden relationships between algorithms and datasets (i.e. some algorithms perform well particularly for some datasets). Details of how this synthetic meta-dataset was created can be found in (Nguyen et al., 2021). 2.2. Challenge protocol We organized a novel two-phase competition protocol: \u2022 Development phase: participants make as many submissions as they want 2, which are evaluated on the validation learning curves. \u2022 Final test phase: no new submissions are accepted in this phase. The last submission of each participant in the Development phase is transferred automatically to this phase. It is evaluated on the test learning curves. Note that the meta-datasets are never exposed to the participants in neither phase, because this is a challenge with code submission (only the participants\u2019 agent sees the data). This setting is novel because it is uncommon in reinforcehttps://codalab.lisn.upsaclay.fr/ competitions/753 up to a comfortable limit that was never reached in our competition. The limit also aims to avoid overfitting the validation sets, though it has been shown that participants are usually aware not to overfit them (Roelofs et al., 2019) ment learning to have a separate phase for agent \u201cdevelopment\u201d and agent \u201ctesting\u201d. Validation learning curves are used during the development phase and test learning curves during the final phase, to prevent agent from overfitting. Moreover, we implemented the k-fold meta-cross-validation (with k=6) to reduce variance in the evaluations of the agents (i.e. 25 datasets for meta-training and 5 datasets for metatesting). The final results are averaged over datasets in the test folds. During meta-training, learning curves and meta-data collected on 25 datasets are passed to the agent for metalearning in any possible ways implemented by the agent. Then during meta-testing, one dataset is presented to the agent at a time. The agent interacts back and forth with an \u201cenvironment\u201d, similarly to a Reinforcement Learning setting. It keeps suggesting to reveal algorithms\u2019 validation learning curves and choosing the current best performing algorithm based on observations of the partially revealed learning curves. 2.3. Evaluation In this challenge series, we want to search for agents with high \u201cany-time learning\u201d capacities, which means the ability to have good performances if they were to be stopped at any point in time. Hence, the agent is evaluated by the Area under the agents\u2019 Learning Curve (ALC) which is constructed using the learning curves of the best algorithms chosen at each time step (validation learning curves in the Development phase, and the test learning curves in the Final phase). The computation of the ALC is explained in (Nguyen et al., 2021). The results will be averaged over all meta-test datasets and shown on the leaderboards. The final ranking is made according to the average test ALC. As indicated in the competition rules, participants should make efforts to guarantee the reproducibility of their methods (e.g. by fixing all random seeds involved). In the Final Phase, all submissions were run three times with the same seed, and the run with the worst performance is used for the final ranking3. This penalizes any team who did not fix their own seeds. On each dataset Di, we evaluated an agent Aj by the Area under the Learning curve ALC i of the agent on the dataset. In the final ranking, the agent is ranked based on its average ALC over all datasets (N = 30 datasets): \u03bcj = \u2211N i=1ALC j i N (1) To measure the variability of an agent Aj , we computed The output of each run can be found in this Google Drive folder: [link] manuscript of paper accepted to CFOL Workshop @ ICML 2022 the standard deviation of ALC scores obtained by the agent over all datasets: \u03c3j = \u221a\u2211N i=1(ALC j i \u2212 \u03bcj) N (2) 3. Analyses of the first round results Results, final rankings, and prizes in the Final phase of the first round are shown in Table 1. The 1st, 2nd, and 3rd ranked teams qualifying for prizes, as per the challenge rules4, were awarded prizes of 500$, 300$, and 200$, respectively. In the remainder of this section, we give a more in-depth view of how each team performed on each dataset in this round, compared to our baselines. In this round, the participants are asked to solve two tasks simultaneously: algorithm selection and budget allocation. Both of them are crucial to achieving our goal of maximizing the area under an agent\u2019s learning curve. We found approaches submitted by the participants using a wide range of methods, from simple (e.g. using algorithm ranking and pre-defined values for \u2206t) to more sophisticated (e.g. predicting scores and timestamps of unseen learning curve points). The results first indicate that using both learned policies (models) for choosing algorithms and spending time budget (used by team MoRiHa and neptune) yields better ALC scores than hard-coded ones (e.g. using a fixed pre-defined list of \u2206t in AIpert and our DDQN baseline). According to Table 2, team MoRiHa obtained the highest average ALC of 0.43 in the final phase. It succeeded in achieving the highest ALC score on 21 out of 30 datasets. In addition, it performed notably better than other teams in some datasets, such as tania, robert, newsgroups, and marco. They are datasets of either multi-label or multi-class classification tasks with a very high number of features. Moreover, most of them are sparse datasets, which is often seen as a challenge for learners. Further investigation will be done in our future work to understand the outperformance of MoRiHa\u2019s method on these particular datasets. We will also encourage team MoRiHato join the second round to study the robutness of their results. Team neptune has a score of 0.42 which is very close to the winner\u2019s, followed by team AIpert with a score of 0.40. Team automl-freiburg, which was ranked 4th, achieved a slightly lower score (0.37) than our DDQN baseline (0.38), and so did team automlhannover (0.32). The successes of the top-ranked team can be explained by the strategies they implemented. Team MoRiHa, which finished in the 1st place, uses a simple yet efficient approach https://codalab.lisn.upsaclay.fr/ competitions/753 that explores the most promising algorithms and avoids wasting time switching between too many different algorithms. Interestingly, team neptune learns a policy for allocating time budget using learning curve convergence speed. The Reinforcement Learning-based approach of team AIpert is very intuitive as our competition borrows the RL framework to formulate our problem, which also explains our baseline choice of DDQN. However, by complementing it with the K-means clustering method, AIpert\u2019s approach achieved higher performance than our baseline. Both AIpert and automl-freiburg share the same idea of suggesting algorithms based on dataset similarity. 4. Winning solutions In this section, we briefly introduce strategies of the winning solutions. More details on their implementations can be found in our factsheet summary (Appendix A) or in individual factsheets provided by the winning teams in Table 1. 4.1. Team MoRiHa (1st place) According to team MoRiHa, they focus on \u201cdoing the right thing at the right time\u201d. Their agent is very goal-oriented (i.e. maximizing the area under the learning curve) and does not rely on complex models or expensive computations. They emphasize the importance of having an accurate schedule regarding the invested time (choosing \u2206t for each query) in order to avoid wasting time budget. One of their key findings is that switching between algorithms during exploration is very costly and it is rarely beneficial to switch the explored algorithm more than once. They build a time model for each algorithm to predict the time when the first point on the algorithm\u2019s learning curve is available. In addition, they keep a list of algorithms ranked descending based on their ALC in the meta-training phase. In meta-testing their algorithm scheduler explores algorithms in an order starting from the best algorithm. If an algorithm\u2019s learning curve stales, the next algorithm will be chosen. Time allocation is done using the time models and a heuristic procedure. This is the only team having an assumption that the learning curves are monotonically increasing. 4.2. Team neptune (2nd place) As described by team neptune, they train a learning curve predictor to predict unseen points on the learning curves (i.e. by interpolating the original scores) in order to find the best algorithm for a new dataset. In addition, they train an algorithm classifier to categorize algorithms into three groups based on their learning curve convergence speed: Fast/Median/Slow. Different budget allocation strategies will be selected according to the algorithm\u2019s convergence type. Regarding their implementation, they train MLP netmanuscript of paper accepted to CFOL Workshop @ ICML 2022 Table 1. Final phase ranking of the first round. Teams are ranked based on their average ALC scores, which were recorded on the worst run among three runs. Team MoRiHa was ranked 1st, but not qualified for a monetary prize. Teams neptune, AIpert, and automl-freiburg are qualified for the prizes. Rank Team (username) ALC score Monetary Prize qualification Comments/ source code 1 MoRiHa (username: jmhansel) 0.43 NO This team is not qualified for a monetary prize due to close relation with the organizers [CODE URL] [FACTSHEET] 2 neptune (username: neptune) 0.42 OK [CODE URL] [FACTSHEET] 3 AIpert (username: AIpert) 0.40 OK [CODE URL] [FACTSHEET] 4 automl-freiburg (username: automl-freiburg) 0.37 OK [CODE URL] [FACTSHEET] 5 automl-hannover (username: amsks) 0.32 [CODE URL] [FACTSHEET] 6 pprp (username: pprp) 0.24 7 arushsharma24 (username: arushsharma24) 0.23 8 Xavier (username: Xavier) 0.17 works to perform both tasks: learning curve prediction and algorithm classification. 4.3. Team AIpert (3rd place) According to team AIpert, their method aims at uncovering good algorithms as fast as possible, using a lowcomputational cost and simple process. The novelty lies in the combination of an off-policy Reinforcement Learning method (Q-learning) and K-means clustering model. As similar datasets usually have the same learning behavior, organizing similar datasets into groups based on their metafeatures is essential. They thus build a Q-learning matrix for each dataset cluster (12 clusters in total). In meta-training, each dataset is seen multiple times and the corresponding Q-learning matrix is updated. This allows the agents to be exposed to more situations (different observations and rewards) on a dataset. In meta-testing, they first determine which cluster the given dataset belongs to. Then, the Qlearning matrix associated with the cluster is utilized as a policy to guide the agent. \u2206t is not chosen as an absolute value but from a fixed list of time portions of the total time budget. 4.4. Team automl-freiburg (4th place) As described by team automl-freiburg, their algorithm selection policy is based on a Deep Gaussian Processes (DGP) surrogate, in a similar way to FSBO (Wistuba & Grabocka, 2021). The surrogate aims to predict the performance of an algorithm at time t+ \u2206t, with \u2206t chosen by a trained budget predictor. The DGP is trained during the metatraining phase, and fine-tuned in the meta-testing phase. During meta-training, they store the best algorithm for each dataset to be used as the first algorithm to query during meta-testing. The \u201cbest\u201d algorithm is defined as the one that has the highest y0 t0 , which means they favor algorithms that achieve high performance early. Given a new dataset in meta-testing, the best algorithm of the closest dataset (previously seen in meta-training, based on the euclidean distance) will be selected. During meta-testing, they keep updating the observed algorithms and fine-tune the DGP surrogate. 5. Lessons learned and new design In this section, we describe the limitations of our original design and how we addressed them in the second round. First, one set of limitations arises because we precomputed learning curves (performance as a function of time) and therefore have a fixed predetermined sampled time points. In our first challenge edition, we opted to interpolate in between time points with the last recorded performance. The criticism of participants was that in real life, if they chose an in between time point, they would get newer information. To mitigate that, in the new challenge round, the participants cannot choose any time point (thus no need to interpolate), manuscript of paper accepted to CFOL Workshop @ ICML 2022 Table 2. ALC scores of the top 5 methods: MoRiHa (AT01), neptune (AT02), AIpert (AT03), automl-freiburg (AT04), automlhannover (AT05), and our baselines: Double Deep Q Network (DDQN), Best on Samples (BOS), Freeze-Thaw BO (FT), Average Rank (AR), Random Search (RS). The reported scores correspond to the worst of 3 runs for each method. The last row shows the average ALC scores (in descending order, from left to right) over 30 datasets. manuscript of paper accepted to CFOL Workshop @ ICML 2022 they have to choose the next pre-computed learning curve point. We thus provide a new type of learning curve for the second round: learning curve as a function of training data size, as opposed to learning curves as a function of time. The time budget for querying a point on a learning curve will be returned by the environment and not chosen by the agent, which means that the agent has to pay whatever it costs. Second, the test learning curves were highly correlated with the validation curves. Therefore, one could overfit the former by simply overfitting the latter. In the second round, the agent will always be evaluated using the test learning curves but on a completely different set of datasets in each phase (Development phase and Final phase). The second round of our challenge was accepted at the AutoML-Conf 2022. Participation in the first round is not a prerequisite for the second round. The second round comes with some new features, including: \u2022 Learning curve: we focus on learning curves as functions of training data size. We thus collected a new large meta-dataset of such learning curves. \u2022 Competition protocol: Given a portfolio of algorithms, an agent suggests which algorithm and the amount of training data to evaluate the algorithm on a new task (dataset) efficiently. The agent observes information on both the training learning curves and validation learning curves to plan for the next step. Test learning curves, which are kept hidden, will be used for evaluating the agent. \u2022 Data split: We use half of the meta-dataset for the Development phase and the other \u201cfresh\u201d half to evaluate the agent in the Final phase. The second round is split into three phases: \u2022 Public phase (1 week): participants practice with the given starting kit and sample data \u2022 Development phase (6 weeks): participants submit agents that are meta-trained and meta-tested on the platform. 15 datasets will be used in this phase. \u2022 Final phase (1 week): no further submissions are made in this phase. Your last submission in the Development phase will be forwarded automatically to this phase and evaluated on 15 fresh datasets (not used in the Development phase). Like in the first round, this is a competition with code submission and the participants do not see the data in either phase (only their submitted agent is exposed to the metadatasets). We created a new meta-dataset of pre-computed learning curves of 40 algorithms with different hyperparameters on 30 datasets used in the AutoML challenge. The algorithms are created from four base algorithms: K-Nearest Neighbors (KNN), Multilayer Perceptron (MLP), Adaboost, Stochastic Gradient Descent (SGD), but with different values of hyperparameters. We added meta-features of datasets and hyperparameters of algorithms. We respected the data split of the AutoML challenge to produce three sets of learning curves for each task, from the training, validation, and test sets. The type of metric used to compute the learning curves of the meta-dataset is provided in the meta-features of the dataset. We also generated a new synthetic metadataset that contains 12000 learning curves, in a similar way used in the first round, but with a new type of learning curve as explained above (learning curve as a function of training data size). In meta-training, the following data is given to the agent to meta-learn: meta-features of datasets, hyperparameters of algorithms, training learning curves, validation learning curves, and test learning curves. While in meta-testing, the agent interacts with an environment in a Reinforcement Learning style. Given a portfolio of algorithms, an agent suggests which algorithm and the amount of training data to evaluate the algorithm on a new task (dataset) efficiently. The agent observes information on both the training learning curve and validation learning curve to plan for the next step. An episode ends when the given time budget is exhausted. The following two lines of code demonstrate the interactions between the agent and the environment: action = trained agent.suggest(observation) observation, done = env.reveal(action) with: observation : a tuple of (A, p, t, R train A p,R validation A p), with: \u2022 A: index of the algorithm provided in the previous action, \u2022 p: decimal fraction of training data used, with the value of p in [0.1, 0.2, 0.3, ..., 1.0] \u2022 t: the amount of time it took to train A with training data size of p, and make predictions on the training/validation/test sets. \u2022 R train A p: performance score on the training set \u2022 R validation A p: performance score on the validation set manuscript of paper accepted to CFOL Workshop @ ICML 2022 action: a tuple of (A, p), with: \u2022 A: index of the algorithm to be trained and tested \u2022 p: decimal fraction of training data used, with the value of p in [0.1, 0.2, 0.3, ..., 1.0] The scoring program automatically chooses the best algorithm at each time step (i.e. the algorithm with the highest validation score found so far, which is different from the first round where it was chosen by the agent) to compute the agent\u2019s test learning curve (as a function of time spent). The metric used for ranking on the leaderboard is the Area under the agent\u2019s Learning Curve (ALC). 5.1. Baseline results of the second round We re-use our baselines from the first round, including: Double Deep Q Network, Best on Samples, Freeze-Thaw Baysian Optimization, Average Rank, and Random Search agents. Their implementations are taken from the first round (Anonymous-paper) with some modifications in order to work with our new protocol and meta-dataset. We also provide them (except the Random Search agent) a smarter policy for choosing the training data size in each step. Each agent keeps track of the algorithms (A) and the training data size (p) tested on the algorithms. Every time it re-selects an algorithm, it should increase p by 0.1 to go forward on the algorithm learning curve. We run each method three times and report the worst run of each of them in Table 3. Similar to the first round, among the baselines, DDQN still performs best in the second round with an average ALC of 0.38. It is the winner (co-winner) of 16 out of 30 datasets with the highest performance difference compared to other baseline methods seen on the dataset tania. BOS and FT are not far behind with average scores of 0.37 and 0.36, respectively. The improvement of BOS compared to the first round can be explained by the adaptation of its strategy in this round. It tries each method on a small subset of training data first (e.g. 10 percent of training samples), instead of spending a fixed amount of time as in the previous round. This should help it not waste a time budget since choosing an adequate amount of time to get a new point on learning is no longer necessary in the second round. The success of BOS suggests that information on the first point of the learning curve is crucial to decisions on selecting algorithms. AR and RS perform poorly with the same score of 0.28. Although focusing only on one algorithm as what AR method does can bring benefits in some cases (e.g. on dataset cadata and didonis), a dataset-dependent policy for selecting algorithms is still necessary to be successful on multiple cross-domain tasks. 6. Conclusion The first round results of our challenge have revealed that agents that learn both policies for selecting algorithms and allocating time budget are more successful in our challenge setting. Team MoRiHa, who finished in 1st place, outperformed all other teams and our baselines on two-thirds of the datasets. We propose a novel setting and a new metadataset for the second round and present baseline results. DDQN maintains its advantages in the second round, while an intuitive and simple baseline as BOS can work quite well in the second round. We are looking forward to see whether the findings of the first round will be reinforced and whether participants\u2019 solutions can outperform our baselines significantly in the second round. For our future work, we want to perform more postchallenge analyses to verify whether progress was made in meta-learning from learning curves. First, we would like to do a point-by-point comparison of the winning methods in the first round, based on their fact sheets. Second, to investigate further the winning methods and see what contributed the most to their success, we want to perform systematic experiments in collaboration with the winners. More concretely, we will build a common workflow and ask participants to conduct ablation studies. Lastly, we are also interested in examining the effect of changes in our reward function hyper-parameters on participants\u2019 performances.",
    "authors": [
        {
            "affiliations": [],
            "name": "Manh Hung Nguyen"
        },
        {
            "affiliations": [],
            "name": "Lisheng Sun"
        },
        {
            "affiliations": [],
            "name": "Nathan Grinsztajn"
        },
        {
            "affiliations": [],
            "name": "Isabelle Guyon"
        }
    ],
    "id": "SP:461e791870217c9cd870767de82eb85bfde9f41f",
    "references": [
        {
            "authors": [
                "F. 2020. Mohr",
                "J.N. van Rijn"
            ],
            "title": "Learning curves for decision",
            "year": 2020
        },
        {
            "authors": [
                "L. Sun-Hosoya"
            ],
            "title": "Meta-Learning as a Markov Decision Process. Theses, Universit\u00e9 Paris Saclay (COmUE), December 2019",
            "venue": "URL https://hal. archives-ouvertes.fr/tel-02422144",
            "year": 2019
        },
        {
            "authors": [
                "L. Sun-Hosoya",
                "I. Guyon",
                "M. Sebag"
            ],
            "title": "Activmetal: Algorithm recommendation with active meta learning",
            "venue": "In IAL@PKDD/ECML,",
            "year": 2018
        },
        {
            "authors": [
                "J. Vanschoren"
            ],
            "title": "Meta-learning: A survey",
            "venue": "arXiv preprint arXiv:1810.03548,",
            "year": 2018
        },
        {
            "authors": [
                "Y. Wang",
                "Q. Yao",
                "J. Kwok",
                "L. Ni"
            ],
            "title": "Generalizing from a few examples: A survey on few-shot learning",
            "venue": "ACM Computing Surveys, 53:1\u201334,",
            "year": 2020
        },
        {
            "authors": [
                "M. Wistuba",
                "J. Grabocka"
            ],
            "title": "Few-shot bayesian optimization with deep kernel surrogates",
            "venue": "arXiv preprint arXiv:2101.07667,",
            "year": 2021
        },
        {
            "authors": [
                "F. Zhuang",
                "Z. Qi",
                "K. Duan",
                "D. Xi",
                "Y. Zhu",
                "H. Zhu",
                "H. Xiong",
                "Q. He"
            ],
            "title": "A comprehensive survey on transfer learning",
            "venue": "Proceedings of the IEEE, PP:1\u201334,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "1. Background and motivation Meta-learning has been playing an increasingly important role in Automated Machine Learning. While it is a natural capability of living organisms, who constantly transfer acquired knowledge across tasks to quickly adapt to changing environments, artificial learning systems are still in their meta-learning \u201cinfancy\u201d. They are only capable, so far, to transfer knowledge between very similar tasks. At a time when society is pointing fingers at AI for being wasteful with computational resources, there is an urgent need for learning systems, which recycle their knowledge. To achieve that goal, some research areas have been widely studied, including few-shot learning (Wang et al., 2020), transfer learning (Zhuang et al., 2020), representation learning (Bengio et al., 2013), continual learning (Delange et al., 2021), life-long learning (Chen et al., 2018), and meta-learning (Vanschoren, 2018). However, meta-learning from learning curves, an essential sub-problem in meta-learning (Mohr & van Rijn, 2022), is still under-studied. This motivates the design of this new challenge series we are proposing.\nLearning curves evaluate algorithm incremental performance improvement, as a function of training time, number of iterations, and/or number of examples. Our challenge design builds on top of previous challenges and work, which considered other aspects of the problem: meta-learning as a recommendation problem, but not from learning curves (Guyon et al., 2019; Liu et al., 2020) and few-shot learning (El Baz et al., 2021). Analysis of past challenges revealed that top-ranking methods often involve switching between algorithms during training, including \u201cfreeze-thaw\u201d Bayesian techniques (Swersky et al., 2014). However, previous challenge protocols did not allow evaluating such methods separately, due to inter-dependencies between various heuristics. Furthermore, we want to study the potential benefit of learned policies, as opposed to applying handcrafted black-box optimization methods.\nOur challenge took inspiration from MetaREVEAL (Nguyen et al., 2021), ActivMetal (Sun-Hosoya et al., 2018), REVEAL (Sun-Hosoya, 2019), and Freeze-Thaw Bayesian Optimization (Swersky et al., 2014). A meta-learner needs to learn to solve two problems at the same time: algorithm selection and budget allocation. We are interested in metalearning strategies that leverage information on partially trained algorithms, hence reducing the cost of training them to convergence. We offer pre-computed learning curves as a function of time, to facilitate benchmarking. Meta-learners must \u201cpay\u201d a cost emulating computational time for revealing their next values. Hence, meta-learners are expected to learn the exploration-exploitation tradeoffs between continuing \u201ctraining\u201d an already tried good candidate algorithm and checking new candidate algorithms.\nIn this paper, we first recap design of the first round of our challenge. Then, we present the first round results and compare the participants\u2019 solutions with our baselines. After the end of the first round, we have identified some limitations in our design and proposed a new design and a new meta-dataset for the second round.\n2. Design of the first round In this section, we describe the design of the first round of our challenge, including: data, challenge protocol, and\nar X\niv :2\n20 8.\n02 82\n1v 1\n[ cs\n.L G\n] 4\nA ug\n2 02\n2\nevaluation. More details can be found on our competition page 1.\n2.1. Learning curve data\nDespite the fact that learning curves are widely used, there were not many learning curve meta-datasets available in the Machine Learning community at the time of organizing this challenge. Taking advantage of 30 cross-domain datasets used in the AutoML challenge (Guyon et al., 2019), we computed de novo learning curves (both on the validation sets and the test sets) for 20 algorithms, by submitting them to the AutoML challenge (Guyon et al., 2019) as postchallenge submissions. These algorithms are created from two base algorithms, Random Forest and Gradient Boosting, but with different values of the max features hyperparameter. We also provided meta-features of each dataset, such as type of learning task, evaluation metric, time budget, etc. In this first round, we considered a learning curve as a function of time. Each point on the learning curve corresponds to the algorithm performance at a certain time.\nIn addition to the aforementioned real-world meta-dataset, we synthetically generated 4000 learning curves for the participants to practice and get familiar with our starting kit. The points on each synthetic learning curve are sampled from a parameterized sigmoid function with its hyperparameters generated from matrix factorizations. This allows us to create some hidden relationships between algorithms and datasets (i.e. some algorithms perform well particularly for some datasets). Details of how this synthetic meta-dataset was created can be found in (Nguyen et al., 2021).\n2.2. Challenge protocol\nWe organized a novel two-phase competition protocol:\n\u2022 Development phase: participants make as many submissions as they want 2, which are evaluated on the validation learning curves. \u2022 Final test phase: no new submissions are accepted in this phase. The last submission of each participant in the Development phase is transferred automatically to this phase. It is evaluated on the test learning curves.\nNote that the meta-datasets are never exposed to the participants in neither phase, because this is a challenge with code submission (only the participants\u2019 agent sees the data).\nThis setting is novel because it is uncommon in reinforce-\n1https://codalab.lisn.upsaclay.fr/ competitions/753\n2up to a comfortable limit that was never reached in our competition. The limit also aims to avoid overfitting the validation sets, though it has been shown that participants are usually aware not to overfit them (Roelofs et al., 2019)\nment learning to have a separate phase for agent \u201cdevelopment\u201d and agent \u201ctesting\u201d. Validation learning curves are used during the development phase and test learning curves during the final phase, to prevent agent from overfitting. Moreover, we implemented the k-fold meta-cross-validation (with k=6) to reduce variance in the evaluations of the agents (i.e. 25 datasets for meta-training and 5 datasets for metatesting). The final results are averaged over datasets in the test folds.\nDuring meta-training, learning curves and meta-data collected on 25 datasets are passed to the agent for metalearning in any possible ways implemented by the agent. Then during meta-testing, one dataset is presented to the agent at a time. The agent interacts back and forth with an \u201cenvironment\u201d, similarly to a Reinforcement Learning setting. It keeps suggesting to reveal algorithms\u2019 validation learning curves and choosing the current best performing algorithm based on observations of the partially revealed learning curves.\n2.3. Evaluation\nIn this challenge series, we want to search for agents with high \u201cany-time learning\u201d capacities, which means the ability to have good performances if they were to be stopped at any point in time. Hence, the agent is evaluated by the Area under the agents\u2019 Learning Curve (ALC) which is constructed using the learning curves of the best algorithms chosen at each time step (validation learning curves in the Development phase, and the test learning curves in the Final phase). The computation of the ALC is explained in (Nguyen et al., 2021). The results will be averaged over all meta-test datasets and shown on the leaderboards. The final ranking is made according to the average test ALC.\nAs indicated in the competition rules, participants should make efforts to guarantee the reproducibility of their methods (e.g. by fixing all random seeds involved). In the Final Phase, all submissions were run three times with the same seed, and the run with the worst performance is used for the final ranking3. This penalizes any team who did not fix their own seeds.\nOn each dataset Di, we evaluated an agent Aj by the Area under the Learning curve ALCji of the agent on the dataset. In the final ranking, the agent is ranked based on its average ALC over all datasets (N = 30 datasets):\n\u00b5j =\n\u2211N i=1ALC j i\nN (1)\nTo measure the variability of an agent Aj , we computed 3The output of each run can be found in this Google Drive folder: [link]\nthe standard deviation of ALC scores obtained by the agent over all datasets:\n\u03c3j =\n\u221a\u2211N i=1(ALC j i \u2212 \u00b5j)2\nN (2)\n3. Analyses of the first round results Results, final rankings, and prizes in the Final phase of the first round are shown in Table 1. The 1st, 2nd, and 3rd ranked teams qualifying for prizes, as per the challenge rules4, were awarded prizes of 500$, 300$, and 200$, respectively. In the remainder of this section, we give a more in-depth view of how each team performed on each dataset in this round, compared to our baselines.\nIn this round, the participants are asked to solve two tasks simultaneously: algorithm selection and budget allocation. Both of them are crucial to achieving our goal of maximizing the area under an agent\u2019s learning curve. We found approaches submitted by the participants using a wide range of methods, from simple (e.g. using algorithm ranking and pre-defined values for \u2206t) to more sophisticated (e.g. predicting scores and timestamps of unseen learning curve points). The results first indicate that using both learned policies (models) for choosing algorithms and spending time budget (used by team MoRiHa and neptune) yields better ALC scores than hard-coded ones (e.g. using a fixed pre-defined list of \u2206t in AIpert and our DDQN baseline).\nAccording to Table 2, team MoRiHa obtained the highest average ALC of 0.43 in the final phase. It succeeded in achieving the highest ALC score on 21 out of 30 datasets. In addition, it performed notably better than other teams in some datasets, such as tania, robert, newsgroups, and marco. They are datasets of either multi-label or multi-class classification tasks with a very high number of features. Moreover, most of them are sparse datasets, which is often seen as a challenge for learners. Further investigation will be done in our future work to understand the outperformance of MoRiHa\u2019s method on these particular datasets. We will also encourage team MoRiHato join the second round to study the robutness of their results. Team neptune has a score of 0.42 which is very close to the winner\u2019s, followed by team AIpert with a score of 0.40. Team automl-freiburg, which was ranked 4th, achieved a slightly lower score (0.37) than our DDQN baseline (0.38), and so did team automlhannover (0.32).\nThe successes of the top-ranked team can be explained by the strategies they implemented. Team MoRiHa, which finished in the 1st place, uses a simple yet efficient approach\n4https://codalab.lisn.upsaclay.fr/ competitions/753\nthat explores the most promising algorithms and avoids wasting time switching between too many different algorithms. Interestingly, team neptune learns a policy for allocating time budget using learning curve convergence speed. The Reinforcement Learning-based approach of team AIpert is very intuitive as our competition borrows the RL framework to formulate our problem, which also explains our baseline choice of DDQN. However, by complementing it with the K-means clustering method, AIpert\u2019s approach achieved higher performance than our baseline. Both AIpert and automl-freiburg share the same idea of suggesting algorithms based on dataset similarity.\n4. Winning solutions In this section, we briefly introduce strategies of the winning solutions. More details on their implementations can be found in our factsheet summary (Appendix A) or in individual factsheets provided by the winning teams in Table 1.\n4.1. Team MoRiHa (1st place)\nAccording to team MoRiHa, they focus on \u201cdoing the right thing at the right time\u201d. Their agent is very goal-oriented (i.e. maximizing the area under the learning curve) and does not rely on complex models or expensive computations. They emphasize the importance of having an accurate schedule regarding the invested time (choosing \u2206t for each query) in order to avoid wasting time budget. One of their key findings is that switching between algorithms during exploration is very costly and it is rarely beneficial to switch the explored algorithm more than once. They build a time model for each algorithm to predict the time when the first point on the algorithm\u2019s learning curve is available. In addition, they keep a list of algorithms ranked descending based on their ALC in the meta-training phase. In meta-testing their algorithm scheduler explores algorithms in an order starting from the best algorithm. If an algorithm\u2019s learning curve stales, the next algorithm will be chosen. Time allocation is done using the time models and a heuristic procedure. This is the only team having an assumption that the learning curves are monotonically increasing.\n4.2. Team neptune (2nd place)\nAs described by team neptune, they train a learning curve predictor to predict unseen points on the learning curves (i.e. by interpolating the original scores) in order to find the best algorithm for a new dataset. In addition, they train an algorithm classifier to categorize algorithms into three groups based on their learning curve convergence speed: Fast/Median/Slow. Different budget allocation strategies will be selected according to the algorithm\u2019s convergence type. Regarding their implementation, they train MLP net-\nworks to perform both tasks: learning curve prediction and algorithm classification.\n4.3. Team AIpert (3rd place)\nAccording to team AIpert, their method aims at uncovering good algorithms as fast as possible, using a lowcomputational cost and simple process. The novelty lies in the combination of an off-policy Reinforcement Learning method (Q-learning) and K-means clustering model. As similar datasets usually have the same learning behavior, organizing similar datasets into groups based on their metafeatures is essential. They thus build a Q-learning matrix for each dataset cluster (12 clusters in total). In meta-training, each dataset is seen multiple times and the corresponding Q-learning matrix is updated. This allows the agents to be exposed to more situations (different observations and rewards) on a dataset. In meta-testing, they first determine which cluster the given dataset belongs to. Then, the Qlearning matrix associated with the cluster is utilized as a policy to guide the agent. \u2206t is not chosen as an absolute value but from a fixed list of time portions of the total time budget.\n4.4. Team automl-freiburg (4th place)\nAs described by team automl-freiburg, their algorithm selection policy is based on a Deep Gaussian Processes (DGP) surrogate, in a similar way to FSBO (Wistuba & Grabocka,\n2021). The surrogate aims to predict the performance of an algorithm at time t+ \u2206t, with \u2206t chosen by a trained budget predictor. The DGP is trained during the metatraining phase, and fine-tuned in the meta-testing phase. During meta-training, they store the best algorithm for each dataset to be used as the first algorithm to query during meta-testing. The \u201cbest\u201d algorithm is defined as the one that has the highest y0t0 , which means they favor algorithms that achieve high performance early. Given a new dataset in meta-testing, the best algorithm of the closest dataset (previously seen in meta-training, based on the euclidean distance) will be selected. During meta-testing, they keep updating the observed algorithms and fine-tune the DGP surrogate.\n5. Lessons learned and new design In this section, we describe the limitations of our original design and how we addressed them in the second round.\nFirst, one set of limitations arises because we precomputed learning curves (performance as a function of time) and therefore have a fixed predetermined sampled time points. In our first challenge edition, we opted to interpolate in between time points with the last recorded performance. The criticism of participants was that in real life, if they chose an in between time point, they would get newer information. To mitigate that, in the new challenge round, the participants cannot choose any time point (thus no need to interpolate),\nthey have to choose the next pre-computed learning curve point.\nWe thus provide a new type of learning curve for the second round: learning curve as a function of training data size, as opposed to learning curves as a function of time. The time budget for querying a point on a learning curve will be returned by the environment and not chosen by the agent, which means that the agent has to pay whatever it costs.\nSecond, the test learning curves were highly correlated with the validation curves. Therefore, one could overfit the former by simply overfitting the latter. In the second round, the agent will always be evaluated using the test learning curves but on a completely different set of datasets in each phase (Development phase and Final phase).\nThe second round of our challenge was accepted at the AutoML-Conf 2022. Participation in the first round is not a prerequisite for the second round. The second round comes with some new features, including:\n\u2022 Learning curve: we focus on learning curves as functions of training data size. We thus collected a new large meta-dataset of such learning curves.\n\u2022 Competition protocol: Given a portfolio of algorithms, an agent suggests which algorithm and the amount of training data to evaluate the algorithm on a new task (dataset) efficiently. The agent observes information on both the training learning curves and validation learning curves to plan for the next step. Test learning curves, which are kept hidden, will be used for evaluating the agent.\n\u2022 Data split: We use half of the meta-dataset for the Development phase and the other \u201cfresh\u201d half to evaluate the agent in the Final phase.\nThe second round is split into three phases:\n\u2022 Public phase (1 week): participants practice with the given starting kit and sample data\n\u2022 Development phase (6 weeks): participants submit agents that are meta-trained and meta-tested on the platform. 15 datasets will be used in this phase.\n\u2022 Final phase (1 week): no further submissions are made in this phase. Your last submission in the Development phase will be forwarded automatically to this phase and evaluated on 15 fresh datasets (not used in the Development phase).\nLike in the first round, this is a competition with code submission and the participants do not see the data in either\nphase (only their submitted agent is exposed to the metadatasets).\nWe created a new meta-dataset of pre-computed learning curves of 40 algorithms with different hyperparameters on 30 datasets used in the AutoML challenge. The algorithms are created from four base algorithms: K-Nearest Neighbors (KNN), Multilayer Perceptron (MLP), Adaboost, Stochastic Gradient Descent (SGD), but with different values of hyperparameters. We added meta-features of datasets and hyperparameters of algorithms. We respected the data split of the AutoML challenge to produce three sets of learning curves for each task, from the training, validation, and test sets. The type of metric used to compute the learning curves of the meta-dataset is provided in the meta-features of the dataset. We also generated a new synthetic metadataset that contains 12000 learning curves, in a similar way used in the first round, but with a new type of learning curve as explained above (learning curve as a function of training data size).\nIn meta-training, the following data is given to the agent to meta-learn: meta-features of datasets, hyperparameters of algorithms, training learning curves, validation learning curves, and test learning curves. While in meta-testing, the agent interacts with an environment in a Reinforcement Learning style. Given a portfolio of algorithms, an agent suggests which algorithm and the amount of training data to evaluate the algorithm on a new task (dataset) efficiently. The agent observes information on both the training learning curve and validation learning curve to plan for the next step. An episode ends when the given time budget is exhausted. The following two lines of code demonstrate the interactions between the agent and the environment:\naction = trained agent.suggest(observation)\nobservation, done = env.reveal(action)\nwith:\nobservation : a tuple of (A, p, t, R train A p,R validation A p), with:\n\u2022 A: index of the algorithm provided in the previous action,\n\u2022 p: decimal fraction of training data used, with the value of p in [0.1, 0.2, 0.3, ..., 1.0]\n\u2022 t: the amount of time it took to train A with training data size of p, and make predictions on the training/validation/test sets.\n\u2022 R train A p: performance score on the training set\n\u2022 R validation A p: performance score on the validation set\naction: a tuple of (A, p), with:\n\u2022 A: index of the algorithm to be trained and tested\n\u2022 p: decimal fraction of training data used, with the value of p in [0.1, 0.2, 0.3, ..., 1.0]\nThe scoring program automatically chooses the best algorithm at each time step (i.e. the algorithm with the highest validation score found so far, which is different from the first round where it was chosen by the agent) to compute the agent\u2019s test learning curve (as a function of time spent). The metric used for ranking on the leaderboard is the Area under the agent\u2019s Learning Curve (ALC).\n5.1. Baseline results of the second round\nWe re-use our baselines from the first round, including: Double Deep Q Network, Best on Samples, Freeze-Thaw Baysian Optimization, Average Rank, and Random Search agents. Their implementations are taken from the first round (Anonymous-paper) with some modifications in order to work with our new protocol and meta-dataset. We also provide them (except the Random Search agent) a smarter policy for choosing the training data size in each step. Each agent keeps track of the algorithms (A) and the training data size (p) tested on the algorithms. Every time it re-selects an algorithm, it should increase p by 0.1 to go forward on the algorithm learning curve.\nWe run each method three times and report the worst run of each of them in Table 3. Similar to the first round, among the baselines, DDQN still performs best in the second round with an average ALC of 0.38. It is the winner (co-winner) of 16 out of 30 datasets with the highest performance difference compared to other baseline methods seen on the dataset tania. BOS and FT are not far behind with average scores of 0.37 and 0.36, respectively. The improvement of BOS compared to the first round can be explained by the adaptation of its strategy in this round. It tries each method on a small subset of training data first (e.g. 10 percent of training samples), instead of spending a fixed amount of time as in the previous round. This should help it not waste a time budget since choosing an adequate amount of time to get a new point on learning is no longer necessary in the second round. The success of BOS suggests that information on the first point of the learning curve is crucial to decisions on selecting algorithms.\nAR and RS perform poorly with the same score of 0.28. Although focusing only on one algorithm as what AR method does can bring benefits in some cases (e.g. on dataset cadata and didonis), a dataset-dependent policy for selecting algorithms is still necessary to be successful on multiple cross-domain tasks.\n6. Conclusion The first round results of our challenge have revealed that agents that learn both policies for selecting algorithms and allocating time budget are more successful in our challenge setting. Team MoRiHa, who finished in 1st place, outperformed all other teams and our baselines on two-thirds of the datasets. We propose a novel setting and a new metadataset for the second round and present baseline results. DDQN maintains its advantages in the second round, while an intuitive and simple baseline as BOS can work quite well in the second round. We are looking forward to see whether the findings of the first round will be reinforced and whether participants\u2019 solutions can outperform our baselines significantly in the second round.\nFor our future work, we want to perform more postchallenge analyses to verify whether progress was made in meta-learning from learning curves. First, we would like to do a point-by-point comparison of the winning methods in the first round, based on their fact sheets. Second, to investigate further the winning methods and see what contributed the most to their success, we want to perform systematic experiments in collaboration with the winners. More concretely, we will build a common workflow and ask participants to conduct ablation studies. Lastly, we are also interested in examining the effect of changes in our reward function hyper-parameters on participants\u2019 performances.\nAcknowledgment We would like to thank challenge participants for providing feedback and sharing their methods. We are grateful to Chalearn for donating prizes and Google for providing computing resources. This work was supported by ChaLearn, the ANR Chair of Artificial Intelligence HUMANIA ANR-19-CHIA-0022 and TAILOR EU Horizon 2020 grant 952215.\nSoftware and Data All software (including starting kit and winning solutions) is open-sourced on our website (https: //metalearning.chalearn.org/). The metadatasets will remained private on the challenge platform (Codalab) to serve as a long-lasting benchmark for research in meta-learning.\nReferences Bengio, Y., Courville, A., and Vincent, P. Representation\nlearning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35: 1798\u20131828, 08 2013. doi: 10.1109/TPAMI.2013.50.\nChen, Z., Liu, B., Brachman, R., Stone, P., and Rossi, F. Lifelong Machine Learning. Morgan & Claypool Publishers, 2nd edition, 2018. ISBN 1681733021.\nDelange, M., Aljundi, R., Masana, M., Parisot, S., Jia, X., Leonardis, A., Slabaugh, G., and Tuytelaars, T. A continual learning survey: Defying forgetting in classification tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 1\u20131, 2021. doi: 10.1109/TPAMI.2021.3057446.\nEl Baz, A., Guyon, I., Liu, Z., van Rijn, J. N., Treguer, S., and Vanschoren, J. Advances in metadl: Aaai 2021 challenge and workshop. In Guyon, I., van Rijn, J. N., Treguer, S., and Vanschoren, J. (eds.), AAAI Workshop on Meta-Learning and MetaDL Challenge, volume 140 of Proceedings of Machine Learning Research, pp. 1\u201316. PMLR, 09 Feb 2021. URL https://proceedings. mlr.press/v140/el-baz21a.html.\nGuyon, I., Sun-Hosoya, L., Boulle\u0301, M., Escalante, H. J., Escalera, S., Liu, Z., Jajetic, D., Ray, B., Saeed, M., Sebag, M., Statnikov, A., Tu, W.-W., and Viegas, E. Analysis of the AutoML Challenge Series 2015\u2013 2018, pp. 177\u2013219. Springer International Publishing, Cham, 2019. ISBN 978-3-030-05318-5. doi: 10.1007/978-3-030-05318-5 10. URL https://doi. org/10.1007/978-3-030-05318-5_10.\nLiu, Z., Pavao, A., Xu, Z., Escalera, S., Ferreira, F., Guyon, I., Hong, S., Hutter, F., Ji, R., Nierhoff, T., Niu, K., Pan, C., Stoll, D., Treguer, S., Wang, J., Wang, P., Wu, C., and Xiong, Y. Winning solutions and post-challenge analyses of the ChaLearn AutoDL challenge 2019. IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 17, 2020.\nMohr, F. and van Rijn, J. N. Learning curves for decision making in supervised machine learning \u2013 a survey. arXiv preprint arXiv:2201.12150, 2022.\nNguyen, M. H., Grinsztajn, N., Guyon, I., and Sun-Hosoya, L. Metareveal: Rl-based meta-learning from learning curves. In Workshop on Interactive Adaptive Learning colocated with European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD 2021), Bilbao/Virtual, Spain, September 2021. URL https://hal.inria.fr/ hal-03502358.\nRoelofs, R., Shankar, V., Recht, B., Fridovich-Keil, S., Hardt, M., Miller, J., and Schmidt, L. A meta-analysis of overfitting in machine learning. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alche\u0301-Buc, F., Fox, E., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings. neurips.cc/paper/2019/file/ ee39e503b6bedf0c98c388b7e8589aca-Paper. pdf.\nSun-Hosoya, L. Meta-Learning as a Markov Decision Process. Theses, Universite\u0301 Paris Saclay (COmUE), December 2019. URL https://hal. archives-ouvertes.fr/tel-02422144.\nSun-Hosoya, L., Guyon, I., and Sebag, M. Activmetal: Algorithm recommendation with active meta learning. In IAL@PKDD/ECML, 2018.\nSwersky, K., Snoek, J., and Adams, R. Freeze-thaw bayesian optimization. arXiv preprint arXiv:2106.04480, 06 2014.\nVanschoren, J. Meta-learning: A survey. arXiv preprint arXiv:1810.03548, 10 2018.\nWang, Y., Yao, Q., Kwok, J., and Ni, L. Generalizing from a few examples: A survey on few-shot learning. ACM Computing Surveys, 53:1\u201334, 06 2020. doi: 10.1145/ 3386252.\nWistuba, M. and Grabocka, J. Few-shot bayesian optimization with deep kernel surrogates. arXiv preprint arXiv:2101.07667, 2021. doi: 10.48550/ARXIV.2101. 07667. URL https://arxiv.org/abs/2101. 07667.\nZhuang, F., Qi, Z., Duan, K., Xi, D., Zhu, Y., Zhu, H., Xiong, H., and He, Q. A comprehensive survey on transfer learning. Proceedings of the IEEE, PP:1\u201334, 07 2020. doi: 10.1109/JPROC.2020.3004555.\nA. Factsheet Summary In this section, we summarize the information provided in the participants\u2019 factsheets. Only top 5 teams submitted their factsheets, including: MoRiHa, neptune, AIpert, automl-freiburg, and automl-hannover."
        },
        {
            "heading": "A.1. PRE-PROCESSING & FEATURE ENGINEERING",
            "text": "Question 1: Did you perform any data pre-processing methods?\nQuestion 2: Did you perform feature engineering methods?"
        },
        {
            "heading": "A.2. DATA USED FOR LEARNING",
            "text": "Question 3: Did you use all points on the learning curves or only some of them?\nQuestion 4: Did you make use of meta-features of datasets?\nQuestion 5: Did you implement a Hyperparameter Optimization component for your agent using the provided hyperparameters of algorithms?\nQuestion 6: In case you used either or both meta-features of datasets and algorithms, did it improve the performance\nof your method?\nQuestion 7: In any case, did you find the meta-features useful in our meta-learning setting?"
        },
        {
            "heading": "A.3. POLICY CHARACTERISTICS",
            "text": "Question 8: Does your agent learn a policy from datasets in the meta-training phase?\nQuestion 9: How does your agent manage the exploration-exploitation trade-offs (revealing a known good algorithm\u2019s learning curve vs. revealing a new algorithm candidate\u2019s learning curve )?\n1. With an greedy policy, in a Reinforcement Learning framework, only in meta- training we create different Q-matrices. In the meta-testing phase, we perform the choice of the new algorithm with the computed Q-matrices.\n2. We are very restrictive with switching the explored algorrithm. We preselect the single best performing algorithm from the validation learning curves statically and only explore other algorithms, if its learning curve is stale. So we strongly emphasize exploiting the single best algorithm.\n3. A modified Round Robin on the top-k performing algorithms. The incumbent i.e. the value of the top performing algorithm on the test dataset is challenged by zero budget allocated algorithms. Since the training on the validation data allows for immediate look up in the test dataset.\n4. Bayesian Optimization\n5. We find the best algorithm by a learning curve predictor.\nQuestion 10: Does your agent switch between learning curves during an episode (i.e. switching between training different algorithms on the dataset at hand)?\nQuestion 11: Does your agent leverage partially revealed learning curves on the dataset at hand?\nQuestion 12: Did you make any assumptions about the shapes of the learning curves?\nQuestion 13: Does your agent predict unseen points on a learning curve?\nQuestion 14: Does your agent perform pairwise comparisons of algorithms\u2019 learning curves?\nQuestion 15: How does your agent spend the given time budgets (i.e. choosing delta t at each step)?\nQuestion 16: How does your agent choose which algorithm to contribute to its learning curve at each step (i.e. choosing A star)?\nQuestion 17: Which phase did you focus on more to improve your agent\u2019s performance?\nQuestion 18: Did you build an algorithm ranking?\nQuestion 19: Did you use Reinforcement Learning to train your agent?"
        },
        {
            "heading": "A.4. METHOD IMPLEMENTATION",
            "text": "Question 20: What is the percentage of originality of your method/implementation?\nQuestion 21: Did you use a pre-trained agent?\nQuestion 22: Is your method strongly based on an existing solution? Which one(s)?\nQuestion 23: Did you use Neural Networks for your agent?\nQuestion 24: Do you find the provided libraries / packages / frameworks sufficient?\nQuestion 25: Check all Python packages/frameworks you used.\nQuestion 26: Did you use any specific AutoML / Meta-learning / Hyperparameter Optimization libraries?\nQuestion 27: Was it difficult for you to deal with the provided data format of the learning curves and meta-features?\nQuestion 28: How much time did you spend developing your agents?\nQuestion 29: What\u2019s the difficulty induced by the computation resource (memory, time budget, etc) constraints?"
        },
        {
            "heading": "A.5. USER EXPERIENCE",
            "text": "Question 30: Was the challenge duration enough for you to develop your methods?\nQuestion 31: Your evaluation on the starting kit\nQuestion 32: Which improvements can be made for the starting kit? You are welcome to list all issues and bugs you ran into.\n1. The algorithm meta features were non-informative, the dataset meta features\u2019 categorical columns at some point were non informative and the categories were not entirely represented in the validation dataset. Also the environment allows to exploit 0 budgets to improve the agent\u2019s performance in a non realistic way. Also we wanted to use many libraries, that were not available (e.g pytorch geometric or networkx) or with other versions (e.g. sklearn\u2019s quantile regression) was not available initially. The validation & test set (on server) were slightly different. In particular, the distribution of timestamps was dramatically different.\n2. Return the whole learning curve after a suggestion, not only the last observed budget and the last observed performance.\n3. It may be useful to query the learning curve at specific iterations or explain clearly the meaning of the timestamps of the observed samples. If they are random or there is not an underlying pattern or structure, it is difficult to predict the next budget.\n4. The final rank should be obtained by running the methods on other meta-train dataset. In the current set-up, the test curve is highly correlated to the validation curve (seen in the development stage), therefore simply overfitting the latter will probably help to obtain good results in the former.\n5. Decreasing the budget when the suggested algorithm+budget tuple is not enough to query a new point in the learning curve is unrealistic. In the real world, once we decide to run some algorithm for a given time-range or number of epochs, we will obtain some change in the accuracy (unless the learning curve is in a plateau).\nQuestion 33: Your evaluation on the challenge website: https://codalab.lisn.upsaclay.fr/competitions/753"
        }
    ],
    "title": "Meta-learning from Learning Curves Challenge: Lessons learned from the First Round and Design of the Second Round",
    "year": 2022
}