Crafting effective deep learning models for medical image analysis is a complex task, particularly in cases where the medical image dataset lacks significant inter-class variation. This challenge is further aggravated when employing such datasets to generate synthetic images using generative adversarial networks (GANs), as the output of GANs heavily relies on the input data. In this research, we propose a novel filtering algorithm called Cosine Similarity-based Image Filtering (CosSIF). We leverage CosSIF to develop two distinct filtering methods: Filtering Before GAN Training (FBGT) and Filtering After GAN Training (FAGT). FBGT involves the removal of real images that exhibit similarities to images of other classes before utilizing them as the training dataset for a GAN. On the other hand, FAGT focuses on eliminating synthetic images with less discriminative features compared to real images used for training the GAN. Experimental results reveal that employing either the FAGT or FBGT method with modern transformer and convolutional-based networks leads to substantial performance gains in various evaluation metrics. FAGT implementation on the ISIC-2016 dataset surpasses the baseline method in terms of sensitivity by 1.59% and AUC by 1.88%. Furthermore, for the HAM10000 dataset, applying FABT outperforms the baseline approach in terms of recall by 13.75%, and with the sole implementation of FAGT, achieves a maximum accuracy of 94.44%. Code and implementation details are available at: https://github.com/mominul-ssv/cossif. 1.  The ConvNeXt model, utilizing the FAGT method with ùú∂ = ùüé.ùüñùüì, is our best-performing model trained on the HAM10000 dataset. It outperforms the baseline method, IRv2+SA, by 13.72% in recall and 1.03% in accuracy. Similarly, the Swin Transformer model, employing the FABT method with an ùú∂ = ùüé.ùüñùüé, surpasses the baseline IRv2+SA by 13.75% in recall and 0.60% in accuracy. Furthermore, our model utilizes 42,294 training images, whereas the baseline IRv2+SA uses 51,699 images, making our approach more sample-efficient. Table 6 provides a performance comparison of our best-performing models, against the baseline and other approaches proposed in different studies on the HAM10000 dataset. 5.3. feature visualization To visualize the data distribution, we utilize the Uniform Manifold Approximation and Projection for Dimension Reduction (UMAP) technique McInnes, Healy and Melville (2018). We analyze four different variations of the ISIC2016 dataset, which are presented in of Fig. 12, and four variations of the HAM10000 dataset, which are shown in Fig. 13. Variation 1, depicted in Fig. 12 (a) and Fig 13 (a), represents the original dataset consisting of real images. Variation
M. Islam et al. : Preprint submitted to Elsevier Page 16 of 19
comparative view of the dataset‚Äôs distribution and clustering patterns for each variation. convenient comparison of the distribution and clustering patterns across each variation. 2, displayed in Fig. 12 (b) and Fig. 13 (b), comprises unfiltered oversampled datasets that include real, transformed, and GAN-generated synthetic images. This variation corresponds to the No-Filtering experiment. In variations 3 and 4, illustrated in Fig. 12 (c), Fig. 12 (d), Fig. 13 (c), and Fig. 13 (d), we have filtered oversampled datasets that also incorporate real, transformed, and GAN-generated synthetic images. These variations utilize the FBGT and FAGT methods, and the composition of the datasets is determined by the hyperparameter ùú∂. The selection of the dataset composition is based on the highest performance observed on the Swin Transformer classifier. Fig. 12 presents the two-dimensional (2D) UMAP embeddings of the ISIC-2016 dataset, where (a) contains 727 malignant and 173 benign lesions within the distribution, while (b), (c), and (d) in Fig. 12 depict oversampled datasets featuring 2000 malignant and 2000 benign lesions each; it is evident from Fig. 12 (b) that there is an overlap in the data distribution between the benign and malignant classes, whereas in Fig. 12 (c) and Fig. 12 (d), the distribution appears to separate into two distinct groups, highlighting the effectiveness of the FBGT and FAGT methods when compared to the No-Filtering approach. Similarly, Fig. 13 illustrates the three-dimensional (3D) UMAP embeddings of the HAM10000 dataset, where (a) shows a distribution comprising 9186 skin lesions, and (b),
(c), and (d) in Fig. 13 represent oversampled datasets, each displaying a subset of the 42,294 skin lesions; upon observing the 3D representations, it becomes evident that in Fig. 13 (c) and Fig. 13 (d), the data points for each class become more distinguishable within a three-dimensional space, whereas the unfiltered 3D representation in Fig. 13 (b) demonstrates sparse data points, posing challenges for the classifier to identify specific regions for each class and consequently making the classification task more difficult. The formation of clustered regions, as opposed to a single concentrated area, further substantiates the effectiveness of the FBGT and FAGT methods, addressing the issue of low inter-class variation within a dataset. By utilizing these techniques, we can substantially refine the classification process, ultimately leading to more accurate and dependable outcomes. 6. conclusion This paper introduces Cosine Similarity-based Image Filtering (CosSIF), a robust dataset filtering algorithm. We utilize CosSIF to create two filtering approaches: FBGT and FAGT. These methods rely on cosine similarity as the main metric for similarity calculation and aim to reduce the volume of GAN-generated synthetic images from the minority class that resemble similarity to images from the majority class. Our experimental results demonstrate that
M. Islam et al. : Preprint submitted to Elsevier Page 17 of 19
models trained on datasets processed with either the FBGT or FAGT methods show improved performance compared to models without these filtering methods. Through comprehensive experiments, we demonstrate that the proposed FAGT method, when applied to the ISIC-2016 dataset and trained on the ViT model, improves sensitivity by 1.59% and AUC by 1.88% compared to the baseline MelaNet. When we apply the FAGT and FBGT methods to the HAM10000 dataset and train them on the ConvNeXt and Swin Transformer models, we observe significant improvements in recall. Specifically, the FAGT method achieves a recall improvement of 13.72% over the baseline IRv2+SA, with an accuracy of 94.44%. Similarly, the FBGT method achieves a recall improvement of 13.75% over the same baseline, with an accuracy of 94.04%.