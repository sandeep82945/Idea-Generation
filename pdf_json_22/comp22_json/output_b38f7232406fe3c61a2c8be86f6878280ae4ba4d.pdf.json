{
    "abstractText": "We introduce an unsupervised technique for encoding point clouds into a canonical shape representation, by disentangling shape and pose. Our encoder is stable and consistent, meaning that the shape encoding is purely pose-invariant, while the extracted rotation and translation are able to semantically align different input shapes of the same class to a common canonical pose. Specifically, we design an auto-encoder based on Vector Neuron Networks, a rotation-equivariant neural network, whose layers we extend to provide translation-equivariance in addition to rotation-equivariance only. The resulting encoder produces pose-invariant shape encoding by construction, enabling our approach to focus on learning a consistent canonical pose for a class of objects. Quantitative and qualitative experiments validate the superior stability and consistency of our approach.",
    "authors": [
        {
            "affiliations": [],
            "name": "OREN KATZIR"
        },
        {
            "affiliations": [],
            "name": "DANIEL COHEN-OR"
        }
    ],
    "id": "SP:d23ad4ae3287a0766717fbf2cc32533a743a8798",
    "references": [
        {
            "authors": [
                "Itzhack Y Bar-Itzhack."
            ],
            "title": "Iterative optimal orthogonalization of the strapdown matrix",
            "venue": "IEEE Trans. Aerospace Electron. Systems 1 (1975), 30\u201337.",
            "year": 1975
        },
        {
            "authors": [
                "Angel X Chang",
                "Thomas Funkhouser",
                "Leonidas Guibas",
                "Pat Hanrahan",
                "Qixing Huang",
                "Zimo Li",
                "Silvio Savarese",
                "Manolis Savva",
                "Shuran Song",
                "Hao Su"
            ],
            "title": "Shapenet: An information-rich 3d model repository",
            "year": 2015
        },
        {
            "authors": [
                "Christopher B Choy",
                "Danfei Xu",
                "JunYoung Gwak",
                "Kevin Chen",
                "Silvio Savarese."
            ],
            "title": "3d-r2n2: A unified approach for single and multi-view 3d object reconstruction",
            "venue": "European conference on computer vision. Springer, 628\u2013644.",
            "year": 2016
        },
        {
            "authors": [
                "Taco S Cohen",
                "Mario Geiger",
                "Jonas K\u00f6hler",
                "Max Welling."
            ],
            "title": "Spherical cnns",
            "venue": "arXiv preprint arXiv:1801.10130 (2018).",
            "year": 2018
        },
        {
            "authors": [
                "Taco S Cohen",
                "Max Welling."
            ],
            "title": "Steerable cnns",
            "venue": "arXiv preprint arXiv:1612.08498 (2016).",
            "year": 2016
        },
        {
            "authors": [
                "Congyue Deng",
                "Or Litany",
                "Yueqi Duan",
                "Adrien Poulenard",
                "Andrea Tagliasacchi",
                "Leonidas J Guibas."
            ],
            "title": "Vector neurons: A general framework for SO (3)-equivariant networks",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision. 12200\u201312209.",
            "year": 2021
        },
        {
            "authors": [
                "Theo Deprelle",
                "Thibault Groueix",
                "Matthew Fisher",
                "Vladimir Kim",
                "Bryan Russell",
                "Mathieu Aubry."
            ],
            "title": "Learning elementary structures for 3D shape generation and matching",
            "venue": "Advances in Neural Information Processing Systems. 7433\u20137443.",
            "year": 2019
        },
        {
            "authors": [
                "Carlos Esteves",
                "Christine Allen-Blanchette",
                "Ameesh Makadia",
                "Kostas Daniilidis."
            ],
            "title": "Learning so (3) equivariant representations with spherical cnns",
            "venue": "Proceedings of the European Conference on Computer Vision (ECCV). 52\u201368.",
            "year": 2018
        },
        {
            "authors": [
                "Thibault Groueix",
                "Matthew Fisher",
                "Vladimir G Kim",
                "Bryan C Russell",
                "andMathieu Aubry"
            ],
            "title": "A papier-m\u00e2ch\u00e9 approach to learning 3d surface generation",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition",
            "year": 2018
        },
        {
            "authors": [
                "Jiayuan Gu",
                "Wei-Chiu Ma",
                "Sivabalan Manivasagam",
                "Wenyuan Zeng",
                "Zihao Wang",
                "Yuwen Xiong",
                "Hao Su",
                "Raquel Urtasun."
            ],
            "title": "Weakly-supervised 3D shape completion in the wild",
            "venue": "European Conference on Computer Vision. Springer, 283\u2013 299.",
            "year": 2020
        },
        {
            "authors": [
                "Abdullah Hamdi",
                "Silvio Giancola",
                "Bernard Ghanem."
            ],
            "title": "Mvtn: Multi-view transformation network for 3d shape recognition",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision. 1\u201311.",
            "year": 2021
        },
        {
            "authors": [
                "Zhe Liu",
                "Xin Zhao",
                "Tengteng Huang",
                "Ruolan Hu",
                "Yu Zhou",
                "Xiang Bai."
            ],
            "title": "Tanet: Robust 3d object detection from point clouds with triple attention",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34. 11677\u201311684.",
            "year": 2020
        },
        {
            "authors": [
                "Xu Ma",
                "Can Qin",
                "Haoxuan You",
                "Haoxi Ran",
                "Yun Fu."
            ],
            "title": "Rethinking Network Design and Local Geometry in Point Cloud: A Simple Residual MLP Framework",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Lars Mescheder",
                "Michael Oechsle",
                "Michael Niemeyer",
                "Sebastian Nowozin",
                "Andreas Geiger."
            ],
            "title": "Occupancy networks: Learning 3d reconstruction in function space",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 4460\u20134470.",
            "year": 2019
        },
        {
            "authors": [
                "David Novotny",
                "Nikhila Ravi",
                "Benjamin Graham",
                "Natalia Neverova",
                "Andrea Vedaldi."
            ],
            "title": "C3dpo: Canonical 3d pose networks for non-rigid structure from motion",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision. 7688\u20137697.",
            "year": 2019
        },
        {
            "authors": [
                "Jeong Joon Park",
                "Peter Florence",
                "Julian Straub",
                "Richard Newcombe",
                "Steven Lovegrove."
            ],
            "title": "Deepsdf: Learning continuous signed distance functions for shape representation",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 165\u2013174.",
            "year": 2019
        },
        {
            "authors": [
                "Charles R Qi",
                "Hao Su",
                "Kaichun Mo",
                "Leonidas J Guibas."
            ],
            "title": "Pointnet: Deep learning on point sets for 3d classification and segmentation",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition. 652\u2013660.",
            "year": 2017
        },
        {
            "authors": [
                "Charles Ruizhongtai Qi",
                "Li Yi",
                "Hao Su",
                "Leonidas J Guibas."
            ],
            "title": "Pointnet++: Deep hierarchical feature learning on point sets in a metric space",
            "venue": "Advances in neural information processing systems 30 (2017).",
            "year": 2017
        },
        {
            "authors": [
                "Davis Rempe",
                "Tolga Birdal",
                "Yongheng Zhao",
                "Zan Gojcic",
                "Srinath Sridhar",
                "Leonidas J Guibas."
            ],
            "title": "Caspr: Learning canonical spatiotemporal point cloud representations",
            "venue": "Advances in neural information processing systems 33 (2020), 13688\u201313701.",
            "year": 2020
        },
        {
            "authors": [
                "Shaoshuai Shi",
                "Xiaogang Wang",
                "Hongsheng Li."
            ],
            "title": "Pointrcnn: 3d object proposal generation and detection from point cloud",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 770\u2013779.",
            "year": 2019
        },
        {
            "authors": [
                "Riccardo Spezialetti",
                "Federico Stella",
                "Marlon Marcon",
                "Luciano Silva",
                "Samuele Salti",
                "Luigi Di Stefano."
            ],
            "title": "Learning to orient surfaces by self-supervised spherical cnns",
            "venue": "arXiv preprint arXiv:2011.03298 (2020).",
            "year": 2020
        },
        {
            "authors": [
                "Weiwei Sun",
                "Andrea Tagliasacchi",
                "Boyang Deng",
                "Sara Sabour",
                "Soroosh Yazdani",
                "Geoffrey E Hinton",
                "Kwang Moo Yi."
            ],
            "title": "Canonical Capsules: Self-Supervised Capsules in Canonical Pose",
            "venue": "Advances in Neural Information Processing Systems 34 (2021).",
            "year": 2021
        },
        {
            "authors": [
                "Nathaniel Thomas",
                "Tess Smidt",
                "Steven Kearnes",
                "Lusann Yang",
                "Li Li",
                "Kai Kohlhoff",
                "Patrick Riley."
            ],
            "title": "Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds",
            "venue": "arXiv preprint arXiv:1802.08219 (2018).",
            "year": 2018
        },
        {
            "authors": [
                "Yue Wang",
                "Yongbin Sun",
                "Ziwei Liu",
                "Sanjay E Sarma",
                "Michael M Bronstein",
                "Justin M Solomon."
            ],
            "title": "Dynamic graph cnn for learning on point clouds",
            "venue": "Acm Transactions On Graphics (tog) 38, 5 (2019), 1\u201312.",
            "year": 2019
        },
        {
            "authors": [
                "Maurice Weiler",
                "Mario Geiger",
                "Max Welling",
                "Wouter Boomsma",
                "Taco Cohen."
            ],
            "title": "3d steerable cnns: Learning rotationally equivariant features in volumetric data",
            "venue": "arXiv preprint arXiv:1807.02547 (2018).",
            "year": 2018
        },
        {
            "authors": [
                "Qiangeng Xu",
                "Weiyue Wang",
                "Duygu Ceylan",
                "Radomir Mech",
                "Ulrich Neumann."
            ],
            "title": "Disn: Deep implicit surface network for high-quality single-view 3d reconstruction",
            "venue": "Advances in Neural Information Processing Systems 32 (2019).",
            "year": 2019
        },
        {
            "authors": [
                "Yaoqing Yang",
                "Chen Feng",
                "Yiru Shen",
                "Dong Tian."
            ],
            "title": "Foldingnet: Point cloud auto-encoder via deep grid deformation",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition. 206\u2013215.",
            "year": 2018
        },
        {
            "authors": [
                "Zetong Yang",
                "Yanan Sun",
                "Shu Liu",
                "Jiaya Jia."
            ],
            "title": "3dssd: Point-based 3d single stage object detector",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 11040\u201311048.",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "Shape-Pose Disentanglement using SE(3)-equivariant Vector Neurons OREN KATZIR, Tel Aviv University DANI LISCHINSKI, Hebrew University of Jerusalem DANIEL COHEN-OR, Tel Aviv University We introduce an unsupervised technique for encoding point clouds into a canonical shape representation, by disentangling shape and pose. Our encoder is stable and consistent, meaning that the shape encoding is purely pose-invariant, while the extracted rotation and translation are able to semantically align different input shapes of the same class to a common canonical pose. Specifically, we design an auto-encoder based on Vector Neuron Networks, a rotation-equivariant neural network, whose layers we extend to provide translation-equivariance in addition to rotation-equivariance only. The resulting encoder produces pose-invariant shape encoding by construction, enabling our approach to focus on learning a consistent canonical pose for a class of objects. Quantitative and qualitative experiments validate the superior stability and consistency of our approach."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Point clouds reside at the very core of 3D geometry processing, as they are acquired at the beginning of the 3D processing pipeline and usually serve as the raw input for shape analysis or surface reconstruction. Thus, understanding the underlying geometry of a point cloud has a profound impact on the entire 3D processing chain. This task, however, is challenging since point clouds are unordered, and contain neither connectivity, nor any other global information.\nIn recent years, with the emergence of neural networks, various techniques have been developed to circumvent the challenges of analyzing and understanding point clouds [Qi et al. 2017a,b; Wang et al. 2019; Hamdi et al. 2021; Ma et al. 2022; Shi et al. 2019; Liu et al. 2020; Yang et al. 2020]. However, most methods rely on pre-aligned datasets, where the point clouds are normalized, translated and oriented to have the same pose. In this work, we present an unsupervised technique to learn a canonical shape representation by disentangling shape, translation, and rotation. Essentially, the canonical representation is required to meet two conditions: stability and consistency. The former means that the shape encoding should be invariant to any rigid transformation of the same input, while the latter means that different shapes of the same class should be semantically aligned, sharing the same canonical pose. Canonical alignment is not a new concept. Recently, Canonical Capsules [Sun et al. 2021] and Compass [Spezialetti et al. 2020] proposed self-supervised learning of canonical representations using augmentations with Siamese training. We discuss these methods in more detail in the next section. In contrast, our approach is to extract a pose-invariant shape encoding, which is explicitly disentangled from the separately extracted translation and rotation.\nSpecifically, we design an auto-encoder, trained on an unaligned dataset, that encodes the input point cloud into three disentangled components: (i) a pose-invariant shape encoding, (ii) a rotation matrix and (iii) a translation vector. We achieve pure SE(3)-invariant shape encoding and SE(3)-equivariant pose estimation (enabling reconstruction of the input shape), by leveraging a novel extension of the recently proposed Vector Neuron Networks (VNN) [Deng et al.\n2021]. The latter is an SO(3)-equivariant neural network for point cloud processing, and while translation invariance could theoretically be achieved by centering the input point clouds, such approach is sensitive to noise, missing data and partial shapes. Therefore we propose an extension to VNN achieving SE(3)-equivariance. It should be noted that the shape encodings produced by our network are stable (i.e., pose-invariant) by construction, due to the use of SE(3)-invariant layers.\nAt the same time, the extracted rigid transformation is equivariant to the pose of the input. This enables the learning process to focus on the consistency across different shapes. Consistency is achieved by altering the input point cloud with a variety of simple shape augmentations, while keeping the pose fixed, allowing us to constrain the learned transformation to be invariant to the identity, (i.e., the particular shape), of the input point cloud.\nMoreover, our disentangled shape and pose representation is not limited to point cloud decoding, but can be combined with any 3D data decoder, as we demonstrate by learning a canonical implicit representation of our point cloud utilizing occupancy networks [Mescheder et al. 2019].\nWe show, both qualitatively and quantitatively, that our approach leads to a stable, consistent, and purely SE(3)-invariant canonical representation compared to previous approaches."
        },
        {
            "heading": "2 BACKGROUND AND RELATED WORK",
            "text": ""
        },
        {
            "heading": "2.1 Canonical representation",
            "text": "A number of works proposed techniques to achieve learnable canonical frames, typically requiring some sort of supervision [Rempe et al. 2020; Novotny et al. 2019; Gu et al. 2020]. Recently, two unsupervised methods were proposed: Canonical Capsules [Sun et al. 2021] and Compass [Spezialetti et al. 2020]. Canonical Capsules [Sun et al. 2021] is an auto-encoder network that extracts positions and pose-invariant descriptors for \ud835\udc58 capsules, from which the input shape may be reconstructed. Pose invariance and equivariance are achieved only implicitly via Siamese training, by feeding the network with pairs of rotated and translated versions of the same input point cloud.\nCompass [Spezialetti et al. 2020] builds upon spherical CNN [Cohen et al. 2018], a semi-equivariant SO(3) network, to estimate the pose with respect to the canonical representation. It should be noted that Compass is inherently tied to spherical CNN, which is not purely equivariant [Cohen et al. 2018]. Thus, similarly to Canonical Capsules, Compass augments the input point cloud with a rotated version to regularize an equivariant pose estimation. It should be noted that neither method guarantees pure equivariance. Similarly to Canonical Capsules, we employ an auto-encoding scheme to disentangle pose from shape, i.e., the canonical representation, and similarly to Compass, we strive to employ an equivariant network, however, our network is SE(3)-equivariant and not only\nar X\niv :2\n20 4.\n01 15\n9v 1\n[ cs\n.C V\n] 3\nA pr\n2 02\n2"
        },
        {
            "heading": "2 \u2022 Oren Katzir, Dani Lischinski, and Daniel Cohen-Or",
            "text": "SO(3)-equivariant. More importantly, differently from these two approaches, the different branches of our network are SE(3)-invariant or SE(3)-equivariant by construction, and thus the learning process is free from the burden of enforcing these properties. Rather, the process focuses on learning a consistent shape representation in a canonical pose."
        },
        {
            "heading": "2.2 3D reconstruction",
            "text": "Our method reconstructs an input point cloud by disentangling the input 3D geometry into shape and pose. The encoder outputs a pose encoding and a shape encoding which is pose-invariant by construction, while the decoder reconstructs the 3D geometry from the shape encoding alone. Consequently, our architecture can be easily integrated into various 3D auto-encoding pipelines. In this work, we shall demonstrate our shape-pose disentanglement for point cloud encoding and implicit representation learning.\nState-of-the-art point cloud auto-encoding methods rely on a folding operation of a template (optionally learned) hyperspace point cloud to the input 3D point cloud [Yang et al. 2018; Groueix et al. 2018; Deprelle et al. 2019]. Following this approach, we employ AtlasNetV2 [Deprelle et al. 2019] which uses multiple folding operations from hyperspace patches to 3D coordinates, to reconstruct point clouds in a pose-invariant frame.\nImplicit 3D representation networks [Mescheder et al. 2019; Park et al. 2019; Xu et al. 2019] enable learning of the input geometry with high resolution and different mesh topology. We utilize occupancy networks [Mescheder et al. 2019] to learn an implicit pose-invariant shape representation."
        },
        {
            "heading": "2.3 Rotation-equivariance and Vector Neuron Network",
            "text": "The success of 2D convolutional neural networks (CNN) on images, which are equivariant to translation, drove a similar approach for 3D data with rotation as the symmetry group. The majority of works on 3D rotation-equivariance [Esteves et al. 2018; Cohen et al. 2018; Thomas et al. 2018; Weiler et al. 2018], focus on steerable CNNs [Cohen and Welling 2016], where each layer \u201csteers\u201d the output features according to the symmetry property (rotation and occasionally translation for 3D data). For example, Spherical CNNs [Esteves et al. 2018; Cohen et al. 2018] transform the input point cloud to a spherical signal, and use spherical harmonics filters, yielding features on SO(3)-space. Usually, these methods are tied with specific architecture design and data input which limit their applicability and adaptation to SOTA 3D processing.\nRecently, Deng et al. [Deng et al. 2021] introduced Vector Neuron Networks (VNN), a rather light and elegant framework for SO(3)equivariance. Empirically, the VNN design performs on par with more complex and specific architectures. The key benefit of VNNs lies in their simplicity, accessibility and generalizability. Conceptually, any standard point cloud processing network can be elevated to SO(3)-equivariance (and invariance) with minimal changes to its architecture. Below we briefly describe VNNs and refer the reader to [Deng et al. 2021] for further details. In VNNs the representation of a single neuron is lifted from a sequence of scalar values to a sequence of 3D vectors. A single vector neuron feature is thus a matrixV \u2208 R\ud835\udc36\u00d73, and we denote a collection\n<latexit sha1_base64=\"BJ1Hrdk74y/jZCGcrXfM11JI/LE=\">AAACJHicbVDLSgMxFM34rPU16tJNsIiuyoyICm6KblxWsA9ohyGTybShmWSaZAql9GPc+CtuXPjAhRu/xbSdAW09EO7hnHu5uSdIGFXacb6speWV1bX1wkZxc2t7Z9fe268rkUpMalgwIZsBUoRRTmqaakaaiSQoDhhpBL3bid8YEKmo4A96mBAvRh1OI4qRNpJvX/ehQbufohD2sgpFLvgi9/p+Lg7yeuLbJafsTAEXiZuREshQ9e33dihwGhOuMUNKtVwn0d4ISU0xI+NiO1UkQbiHOqRlKEcxUd5oeuQYHhslhJGQ5nENp+rviRGKlRrGgemMke6qeW8i/ue1Uh1deSPKk1QTjmeLopRBLeAkMRhSSbBmQ0MQltT8FeIukghrk2vRhODOn7xI6mdl96Ls3p+XKjdZHAVwCI7AKXDBJaiAO1AFNYDBI3gGr+DNerJerA/rc9a6ZGUzB+APrO8fMRmiHw==</latexit> q k o ko qo v v 0\n<latexit sha1_base64=\"BJ1Hrdk74y/jZCGcrXfM11JI/LE=\">AAACJHicbVDLSgMxFM34rPU16tJNsIiuyoyICm6KblxWsA9ohyGTybShmWSaZAql9GPc+CtuXPjAhRu/xbSdAW09EO7hnHu5uSdIGFXacb6speWV1bX1wkZxc2t7Z9fe268rkUpMalgwIZsBUoRRTmqaakaaiSQoDhhpBL3bid8YEKmo4A96mBAvRh1OI4qRNpJvX/ehQbufohD2sgpFLvgi9/p+Lg7yeuLbJafsTAEXiZuREshQ9e33dihwGhOuMUNKtVwn0d4ISU0xI+NiO1UkQbiHOqRlKEcxUd5oeuQYHhslhJGQ5nENp+rviRGKlRrGgemMke6qeW8i/ue1Uh1deSPKk1QTjmeLopRBLeAkMRhSSbBmQ0MQltT8FeIukghrk2vRhODOn7xI6mdl96Ls3p+XKjdZHAVwCI7AKXDBJaiAO1AFNYDBI3gGr+DNerJerA/rc9a6ZGUzB+APrO8fMRmiHw==</latexit>\nq k o ko qo v v 0<latexit sha1_base64=\"BJ1Hrdk74y/jZCGcrXfM11JI/LE=\">AAACJHicbVDLSgMxFM34rPU16tJNsIiuyoyICm6KblxWsA9ohyGTybShmWSaZAql9GPc+CtuXPjAhRu/xbSdAW09EO7hnHu5uSdIGFXacb6speWV1bX1wkZxc2t7Z9fe268rkUpMalgwIZsBUoRRTmqaakaaiSQoDhhpBL3bid8YEKmo4A96mBAvRh1OI4qRNpJvX/ehQbufohD2sgpFLvgi9/p+Lg7yeuLbJafsTAEXiZuREshQ9e33dihwGhOuMUNKtVwn0d4ISU0xI+NiO1UkQbiHOqRlKEcxUd5oeuQYHhslhJGQ5nENp+rviRGKlRrGgemMke6qeW8i/ue1Uh1deSPKk1QTjmeLopRBLeAkMRhSSbBmQ0MQltT8FeIukghrk2vRhODOn7xI6mdl96Ls3p+XKjdZHAVwCI7AKXDBJaiAO1AFNYDBI3gGr+DNerJerA/rc9a6ZGUzB+APrO8fMRmiHw==</latexit> q k o ko qo v v 0<latexit sha1_base64=\"BJ1Hrdk74y/jZCGcrXfM11JI/LE=\">AAACJHicbVDLSgMxFM34rPU16tJNsIiuyoyICm6KblxWsA9ohyGTybShmWSaZAql9GPc+CtuXPjAhRu/xbSdAW09EO7hnHu5uSdIGFXacb6speWV1bX1wkZxc2t7Z9fe268rkUpMalgwIZsBUoRRTmqaakaaiSQoDhhpBL3bid8YEKmo4A96mBAvRh1OI4qRNpJvX/ehQbufohD2sgpFLvgi9/p+Lg7yeuLbJafsTAEXiZuREshQ9e33dihwGhOuMUNKtVwn0d4ISU0xI+NiO1UkQbiHOqRlKEcxUd5oeuQYHhslhJGQ5nENp+rviRGKlRrGgemMke6qeW8i/ue1Uh1deSPKk1QTjmeLopRBLeAkMRhSSbBmQ0MQltT8FeIukghrk2vRhODOn7xI6mdl96Ls3p+XKjdZHAVwCI7AKXDBJaiAO1AFNYDBI3gGr+DNerJerA/rc9a6ZGUzB+APrO8fMRmiHw==</latexit>q k o ko qo v v 0 <latexit sha1_base64=\"BJ1Hrdk74y/jZCGcrXfM11JI/LE=\">AAACJHicbVDLSgMxFM34rPU16tJNsIiuyoyICm6KblxWsA9ohyGTybShmWSaZAql9GPc+CtuXPjAhRu/xbSdAW09EO7hnHu5uSdIGFXacb6speWV1bX1wkZxc2t7Z9fe268rkUpMalgwIZsBUoRRTmqaakaaiSQoDhhpBL3bid8YEKmo4A96mBAvRh1OI4qRNpJvX/ehQbufohD2sgpFLvgi9/p+Lg7yeuLbJafsTAEXiZuREshQ9e33dihwGhOuMUNKtVwn0d4ISU0xI+NiO1UkQbiHOqRlKEcxUd5oeuQYHhslhJGQ5nENp+rviRGKlRrGgemMke6qeW8i/ue1Uh1deSPKk1QTjmeLopRBLeAkMRhSSbBmQ0MQltT8FeIukghrk2vRhODOn7xI6mdl96Ls3p+XKjdZHAVwCI7AKXDBJaiAO1AFNYDBI3gGr+DNerJerA/rc9a6ZGUzB+APrO8fMRmiHw==</latexit>\nq k o ko qo v v 0\n<latexit sha1_base64=\"BJ1Hrdk74y/jZCGcrXfM11JI/LE=\">AAACJHicbVDLSgMxFM34rPU16tJNsIiuyoyICm6KblxWsA9ohyGTybShmWSaZAql9GPc+CtuXPjAhRu/xbSdAW09EO7hnHu5uSdIGFXacb6speWV1bX1wkZxc2t7Z9fe268rkUpMalgwIZsBUoRRTmqaakaaiSQoDhhpBL3bid8YEKmo4A96mBAvRh1OI4qRNpJvX/ehQbufohD2sgpFLvgi9/p+Lg7yeuLbJafsTAEXiZuREshQ9e33dihwGhOuMUNKtVwn0d4ISU0xI+NiO1UkQbiHOqRlKEcxUd5oeuQYHhslhJGQ5nENp+rviRGKlRrGgemMke6qeW8i/ue1Uh1deSPKk1QTjmeLopRBLeAkMRhSSbBmQ0MQltT8FeIukghrk2vRhODOn7xI6mdl96Ls3p+XKjdZHAVwCI7AKXDBJaiAO1AFNYDBI3gGr+DNerJerA/rc9a6ZGUzB+APrO8fMRmiHw==</latexit> q k o ko qo v v 0<latexit sha1_base64=\"BJ1Hrdk74y/jZCGcrXfM11JI/LE=\">AAACJHicbVDLSgMxFM34rPU16tJNsIiuyoyICm6KblxWsA9ohyGTybShmWSaZAql9GPc+CtuXPjAhRu/xbSdAW09EO7hnHu5uSdIGFXacb6speWV1bX1wkZxc2t7Z9fe268rkUpMalgwIZsBUoRRTmqaakaaiSQoDhhpBL3bid8YEKmo4A96mBAvRh1OI4qRNpJvX/ehQbufohD2sgpFLvgi9/p+Lg7yeuLbJafsTAEXiZuREshQ9e33dihwGhOuMUNKtVwn0d4ISU0xI+NiO1UkQbiHOqRlKEcxUd5oeuQYHhslhJGQ5nENp+rviRGKlRrGgemMke6qeW8i/ue1Uh1deSPKk1QTjmeLopRBLeAkMRhSSbBmQ0MQltT8FeIukghrk2vRhODOn7xI6mdl96Ls3p+XKjdZHAVwCI7AKXDBJaiAO1AFNYDBI3gGr+DNerJerA/rc9a6ZGUzB+APrO8fMRmiHw==</latexit>q k o ko qo v v 0 <latexit sha1_base64=\"BJ1Hrdk74y/jZCGcrXfM11JI/LE=\">AAACJHicbVDLSgMxFM34rPU16tJNsIiuyoyICm6KblxWsA9ohyGTybShmWSaZAql9GPc+CtuXPjAhRu/xbSdAW09EO7hnHu5uSdIGFXacb6speWV1bX1wkZxc2t7Z9fe268rkUpMalgwIZsBUoRRTmqaakaaiSQoDhhpBL3bid8YEKmo4A96mBAvRh1OI4qRNpJvX/ehQbufohD2sgpFLvgi9/p+Lg7yeuLbJafsTAEXiZuREshQ9e33dihwGhOuMUNKtVwn0d4ISU0xI+NiO1UkQbiHOqRlKEcxUd5oeuQYHhslhJGQ5nENp+rviRGKlRrGgemMke6qeW8i/ue1Uh1deSPKk1QTjmeLopRBLeAkMRhSSbBmQ0MQltT8FeIukghrk2vRhODOn7xI6mdl96Ls3p+XKjdZHAVwCI7AKXDBJaiAO1AFNYDBI3gGr+DNerJerA/rc9a6ZGUzB+APrO8fMRmiHw==</latexit>\nq k o ko qo v v 0\n<latexit sha1_base64=\"BJ1Hrdk74y/jZCGcrXfM11JI/LE=\">AAACJHicbVDLSgMxFM34rPU16tJNsIiuyoyICm6KblxWsA9ohyGTybShmWSaZAql9GPc+CtuXPjAhRu/xbSdAW09EO7hnHu5uSdIGFXacb6speWV1bX1wkZxc2t7Z9fe268rkUpMalgwIZsBUoRRTmqaakaaiSQoDhhpBL3bid8YEKmo4A96mBAvRh1OI4qRNpJvX/ehQbufohD2sgpFLvgi9/p+Lg7yeuLbJafsTAEXiZuREshQ9e33dihwGhOuMUNKtVwn0d4ISU0xI+NiO1UkQbiHOqRlKEcxUd5oeuQYHhslhJGQ5nENp+rviRGKlRrGgemMke6qeW8i/ue1Uh1deSPKk1QTjmeLopRBLeAkMRhSSbBmQ0MQltT8FeIukghrk2vRhODOn7xI6mdl96Ls3p+XKjdZHAVwCI7AKXDBJaiAO1AFNYDBI3gGr+DNerJerA/rc9a6ZGUzB+APrO8fMRmiHw==</latexit> q k o ko qo v v 0\n<latexit sha1_base64=\"BJ1Hrdk74y/jZCGcrXfM11JI/LE=\">AAACJHicbVDLSgMxFM34rPU16tJNsIiuyoyICm6KblxWsA9ohyGTybShmWSaZAql9GPc+CtuXPjAhRu/xbSdAW09EO7hnHu5uSdIGFXacb6speWV1bX1wkZxc2t7Z9fe268rkUpMalgwIZsBUoRRTmqaakaaiSQoDhhpBL3bid8YEKmo4A96mBAvRh1OI4qRNpJvX/ehQbufohD2sgpFLvgi9/p+Lg7yeuLbJafsTAEXiZuREshQ9e33dihwGhOuMUNKtVwn0d4ISU0xI+NiO1UkQbiHOqRlKEcxUd5oeuQYHhslhJGQ5nENp+rviRGKlRrGgemMke6qeW8i/ue1Uh1deSPKk1QTjmeLopRBLeAkMRhSSbBmQ0MQltT8FeIukghrk2vRhODOn7xI6mdl96Ls3p+XKjdZHAVwCI7AKXDBJaiAO1AFNYDBI3gGr+DNerJerA/rc9a6ZGUzB+APrO8fMRmiHw==</latexit> q k o ko qo v v 0\n<latexit sha1_base64=\"BJ1Hrdk74y/jZCGcrXfM11JI/LE=\">AAACJHicbVDLSgMxFM34rPU16tJNsIiuyoyICm6KblxWsA9ohyGTybShmWSaZAql9GPc+CtuXPjAhRu/xbSdAW09EO7hnHu5uSdIGFXacb6speWV1bX1wkZxc2t7Z9fe268rkUpMalgwIZsBUoRRTmqaakaaiSQoDhhpBL3bid8YEKmo4A96mBAvRh1OI4qRNpJvX/ehQbufohD2sgpFLvgi9/p+Lg7yeuLbJafsTAEXiZuREshQ9e33dihwGhOuMUNKtVwn0d4ISU0xI+NiO1UkQbiHOqRlKEcxUd5oeuQYHhslhJGQ5nENp+rviRGKlRrGgemMke6qeW8i/ue1Uh1deSPKk1QTjmeLopRBLeAkMRhSSbBmQ0MQltT8FeIukghrk2vRhODOn7xI6mdl96Ls3p+XKjdZHAVwCI7AKXDBJaiAO1AFNYDBI3gGr+DNerJerA/rc9a6ZGUzB+APrO8fMRmiHw==</latexit> q k o ko qo v v 0<latexit sha1_base64=\"BJ1Hrdk74y/jZCGcrXfM11JI/LE=\">AAACJHicbVDLSgMxFM34rPU16tJNsIiuyoyICm6KblxWsA9ohyGTybShmWSaZAql9GPc+CtuXPjAhRu/xbSdAW09EO7hnHu5uSdIGFXacb6speWV1bX1wkZxc2t7Z9fe268rkUpMalgwIZsBUoRRTmqaakaaiSQoDhhpBL3bid8YEKmo4A96mBAvRh1OI4qRNpJvX/ehQbufohD2sgpFLvgi9/p+Lg7yeuLbJafsTAEXiZuREshQ9e33dihwGhOuMUNKtVwn0d4ISU0xI+NiO1UkQbiHOqRlKEcxUd5oeuQYHhslhJGQ5nENp+rviRGKlRrGgemMke6qeW8i/ue1Uh1deSPKk1QTjmeLopRBLeAkMRhSSbBmQ0MQltT8FeIukghrk2vRhODOn7xI6mdl96Ls3p+XKjdZHAVwCI7AKXDBJaiAO1AFNYDBI3gGr+DNerJerA/rc9a6ZGUzB+APrO8fMRmiHw==</latexit> q k o ko qo v v 0\nFig. 1. Vector Neuron Translation equivariant non linear layer. We learn for each input point feature v, three component o, q, k, and interpret them as an origin o, a feature qo = q\u2212o and a direction ko = k\u2212o. Similarly to VNN operation, the feature component of qo which is in the half-space defined by \u2212ko is clipped. In-addition, we translate the feature by the learned origin o outputting the v\u2032.\nof \ud835\udc41 such features by V \u2208 R\ud835\udc41\u00d7\ud835\udc36\u00d73. The layers of VNNs, which map between such collections, \ud835\udc53 : V \u2208 R\ud835\udc41\u00d7\ud835\udc36\u00d73 \u2192 V \u2032 \u2208 R\ud835\udc41\u00d7\ud835\udc36\u2032\u00d73, are equivariant to rotations \ud835\udc45 \u2208 R3\u00d73, that is:\n\ud835\udc53 (V\ud835\udc45) = \ud835\udc53 (V) \ud835\udc45, (1)\nwhereV\ud835\udc45 = {V\ud835\udc5b\ud835\udc45}\ud835\udc41\ud835\udc5b=1. Ordinary linear layers fulfill this requirement, however, other non-linear layers, such as ReLU and max-pooling, do not. For ReLU activation, VNNs apply a truncation w.r.t to a learned half-space. Let V,V\u2032 \u2208 R\ud835\udc36\u00d73 be the input and output vector neuron features of a single point, respectively. Each 3D vector v\u2032 \u2208 V\u2032 is obtained by first applying two learned matrices Q,K \u2208 R1\u00d7\ud835\udc36 to project V to a feature q = QV \u2208 R1\u00d73 and a direction k = KV \u2208 R1\u00d73. To achieve equivariance, v\u2032 \u2208 V\u2032 is then defined by truncating the part of q that lies in the negative half-space of k, as follows,\nv\u2032 = { q if \u27e8q, k\u27e9 \u2265 0, q \u2212 \u2329 q, k\u2225k\u2225 \u232a k \u2225k\u2225 otherwise.\n(2)\nIn addition, VNNs employ rotation-equivariant pooling operations and normalization layers. We refer the reader to [Deng et al. 2021] for the complete definition.\nInvariance layers can be achieved by inner product of two rotationequivariant features. Let V \u2208 R\ud835\udc36\u00d73, and V\u2032 \u2208 R\ud835\udc36\u2032\u00d73 be two equivariant features obtained from an input point cloud X. Then rotating X by a matrix \ud835\udc45, results in the features V\ud835\udc45 and V\u2032\ud835\udc45, and\n\u27e8V\ud835\udc45,V\u2032\ud835\udc45\u27e9 = V\ud835\udc45(V\u2032\ud835\udc45)\ud835\udc47 = V\ud835\udc45\ud835\udc45\ud835\udc47 V\u2032\ud835\udc47 = VV\u2032\ud835\udc47 = \u27e8V,V\u2032\u27e9. (3)\nIn our work, we also utilize vector neurons, but we extend the different layers to be SE(3)-equivariant, instead of SO(3)-equivariant, as described in Section 3.1. This new design allow us to construct an SE(3)-invariant encoder, which gradually disentangles the pose from the shape, first the translation and then the rotation, resulting in a pose-invariant shape encoding."
        },
        {
            "heading": "3 METHOD",
            "text": "We design an auto-encoder to disentangle shape, translation, and rotation. We wish the resulting representation to be stable, i.e., the shape encoding should be pose-invariant, and the pose SE(3)equivariant. At the same we wish multiple different shapes in the same class to have a consistent canonical pose. To achieve stability, we revisit VNNs and design new SE(3)-equivariant and invariant layers, which we refer to as Vector Neurons with Translation (VNT).\nConsistency is then achieved by self-supervision, designed to preserve pose across shapes. In the following, we first describe the design of our new VNT layers. Next, we present our VNN and VNTbased auto-encoder architecture. Finally, we elaborate on our losses to encourage disentanglement of shape from pose in a consistent manner.\n3.1 SE(3)-equivariant Vector Neuron Network As explained earlier, Vector Neuron Networks (VNN) [Deng et al. 2021] provide a framework for SO(3)-equivariant and invariant point cloud processing. Since a pose of an object consists of translation and rotation, SE(3)-equivariance and invariance are needed for shape-pose disentanglement. While it might seem that centering the input point cloud should suffice, note that point clouds are often captured with noise and occlusions, leading to missing data and partial shapes, which may significantly affect the global center of the input. Specifically, for canonical representation learning, a key condition is consistency across different objects, thus, such an approach assumes that the center of the point cloud is consistently semantic between similar but different objects, which is hardly the case. Equivariance to translation, on the other-hand, allows identifying local features in different locations with the same filters, without requiring global parameters. Therefore, we revisit the Vector Neuron layers and extend them to Vector Neurons with Translation (VNT), thereby achieving SE(3)equivariance.\n3.1.1 Linear layers: While linear layers are by definition rotationequivariant, they are not translation-equivariant. Following VNN, our linear module \ud835\udc53lin (\u00b7;W) is defined via a weight matrix W \u2208 R\ud835\udc36\n\u2032\u00d7\ud835\udc36 , acting on a vector-list feature V \u2208 R\ud835\udc36\u00d73. Let \ud835\udc45 \u2208 R3\u00d73 be a rotation matrix and \ud835\udc47 \u2208 R1\u00d73 a translation vector. For \ud835\udc53lin (\u00b7;W) to be SE(3)-equivariant, the following must hold:\n\ud835\udc53lin (V\ud835\udc45 + 1\ud835\udc36\ud835\udc47 ) = WV\ud835\udc45 + 1\ud835\udc36\u2032\ud835\udc47 = \ud835\udc53lin (V)\ud835\udc45 + 1\ud835\udc36\u2032\ud835\udc47, (4)\nwhere 1\ud835\udc36 = [1, 1, . . . , 1]\ud835\udc47 \u2208 R\ud835\udc36\u00d71 is a column vector of length \ud835\udc36 . A sufficient condition for (18) to hold is achieved by constraining each row of W to sum to one. Formally, W \u2208 W\ud835\udc36\u2032\u00d7\ud835\udc36 , where\nW\ud835\udc36 \u2032\u00d7\ud835\udc36 = { W \u2208 R\ud835\udc36 \u2032\u00d7\ud835\udc36 | \u2211\ud835\udc36\ud835\udc57=1\ud835\udc64\ud835\udc56, \ud835\udc57 = 1 \u2200\ud835\udc56 \u2208 [1,\ud835\udc36 \u2032]} , (5) See the supplementary material for a complete proof.\n3.1.2 Non-linear layers: We extend each non-linear VNN layer to become SE(3)-equivariant by adding a learnable origin. More formally, for the ReLU activation layer, given an input feature list V \u2208 R\ud835\udc36\u00d73, we learn three (rather than two) linear maps,Q,K,O \u2208 W1\u00d7\ud835\udc36 projecting the input to q, k, o \u2208 R1\u00d73. The feature and direction are defined w.r.t the origin o, i.e., the feature is given by qo = q \u2212 o, while the direction is given by ko = k \u2212 o, as illustrated in Fig. 1. The ReLU is applied by clipping the part of qo that resides behind the plane defined by ko and o, i.e.,\nv\u2032 = { o + qo if \u27e8qo, ko\u27e9 \u2265 0, o + qo \u2212 \u2329 qo,\nko \u2225ko \u2225 \u232a ko \u2225ko \u2225 , otherwise.\n, (6)\nNote that o+qo = q, and thatK,Omay be shared across the elements of V.\nfS(S)\nIt may be easily seen that we preserve the equivariancew.r.t SO(3) rotations, as well translations. In the same manner, we extend the SO(3)-equivariant VNNmaxpool layer to become SE(3)-equivariant. We refer the reader to the supplementary material for the exact adaptation and complete proof.\n3.1.3 Translation-invariant layers: Invariance to translation can be achieved by subtracting two SE(3)-equivariant features. Let V,V\u2032 \u2208 R\ud835\udc36\u00d73 be two SE(3)-equivariant features obtained from an input point cloud X. Then, rotating X by a matrix \ud835\udc45 and translating by \ud835\udc47 , results in the features V\ud835\udc45 + 1\ud835\udc36\ud835\udc47 and V\u2032\ud835\udc45 + 1\ud835\udc36\ud835\udc47 , whose difference is translation-invariant:\n(V\ud835\udc45 + 1\ud835\udc36\ud835\udc47 ) \u2212 ( V\u2032\ud835\udc45 + 1\ud835\udc36\ud835\udc47 ) = ( V \u2212 V\u2032 ) \ud835\udc45 (7)\nNote that the resulting feature is still rotation-equivariant, which enables to process it with VNN layers, further preserving SO(3)equivariance.\n3.2 SE(3)-equivariant Encoder-Decoder We design an auto-encoder based on VNT and VNN layers to disentangle pose from shape. Thus, our shape representation is poseinvariant (i.e., stable), while our pose estimation is SE(3)-poseequivariant, by construction. The decoder, which can be an arbitrary 3D decoder network, reconstructs the 3D shape from the invariant features. The overall architecture of our AE is depicted in Fig. 2. Given an input point cloud X \u2208 R\ud835\udc41\u00d73, we can represent it as a rigid transformation of an unknown canonical representation S \u2208 R\ud835\udc41\u00d73:\nX = S\ud835\udc45 + 1\ud835\udc41\ud835\udc47, (8) where 1\ud835\udc41 = [1, 1, . . . , 1]\ud835\udc47 \u2208 R\ud835\udc41\u00d71 is a column vector of length \ud835\udc41 , \ud835\udc45 \u2208 R3\u00d73 is a rotation matrix and \ud835\udc47 \u2208 R1\u00d73 is a translation vector. Our goal is to find the shape S, which is by definition poseinvariant and should be consistently aligned across different input shapes. To achieve this goal, we use an encoder that first estimates the translation \ud835\udc47 using translation-equivariant VNT layers, then switches to a translation-invariant representation from which the rotation ?\u0303? is estimated using rotation-equivariant VNN layers. Finally, the representation is made rotation-invariant and the shape encoding Zs is generated. A reconstruction loss is computed by decoding Zs into the canonically-positioned shape S\u0303 and applying the"
        },
        {
            "heading": "4 \u2022 Oren Katzir, Dani Lischinski, and Daniel Cohen-Or",
            "text": "extracted rigid transformation. In the following we further explain our encoder architecture and the type of decoders used.\n3.2.1 SE(3)-equivariant Encoder. Our encoder is composed of rotation and translation equivariant and invariant layers as shown in Fig. 3. We start by feeding X through linear and non-linear VNT layers yielding XRT \u2208 R\ud835\udc41\u00d7\ud835\udc36\u00d73, where the \ud835\udc45\ud835\udc47 subscript indicates SE(3)-equivariant features, as described in Section 3.1.\nXRT is then fed-forward through additional VNT layers resulting in a single vector neuron per point X\u0304RT \u2208 R\ud835\udc41\u00d71\u00d73. We meanpool the features to produce a 3D SE(3)-equivariant vector as our translation estimation, as shown in the upper branch of Fig. 3, yielding \ud835\udc47 = \ud835\udc53\ud835\udc47 (X) = \ud835\udc53\ud835\udc47 (S)\ud835\udc45 + \ud835\udc47 \u2208 R1\u00d73, where we denote by \ud835\udc53\ud835\udc47 : R\ud835\udc41\u00d73 \u2192 R1\u00d73 the aggregation of the VNT-layers from the input point cloud X to the estimated \ud835\udc47 , thus, it is a translation and rotation equivariant network. In addition, as explained in Section 3.1, the following creates translation invariant features, YR = XRT \u2212 X\u0304RT \u2208 R\ud835\udc41\u00d7\ud835\udc36\u00d73. While YR is translation invariant, it is still rotation equivariant, thus, we can proceed to further process YR with VNN layers, resulting in (deeper) rotation equivariant features ZR \u2208 R\ud835\udc41\u00d7\ud835\udc36\n\u2032\u00d73. Finally, ZR is fed forward through a VNN rotation-invariant layer as explained in Section 2.3, resulting in a shape encoding, Zs, which is by construction pose invariant. Similar to the translation reconstruction, the rotation is estimated by mean pooling ZR and feeding it through a single VN linear layer yielding ?\u0303? = \ud835\udc53\ud835\udc45 (X) = \ud835\udc53\ud835\udc45 (S)\ud835\udc45 \u2208 R3\u00d73, where \ud835\udc53\ud835\udc45 : R\ud835\udc41\u00d73 \u2192 R3\u00d73 denotes the aggregation of the layers from the input point cloud X to the estimated rotation ?\u0303? and, as such, it is a rotation-equivariant network. The entire encoder architecture is shown in Fig. 3 and we refer the reader to our supplementary for a detailed description of the layers.\n3.2.2 Decoder. The decoder is applied on the shape encoding Zs to reconstruct the shape S\u0303. We stress again that S\u0303 is invariant to the input pose, regardless of the training process. Motivated by the success of folding networks [Yang et al. 2018; Groueix et al. 2018; Deprelle et al. 2019] for point clouds auto-encoding, we opt\nto use AtlasNetV2 [Deprelle et al. 2019] as our decoder, specifically using the point translation learning module. For implicit function reconstruction, we follow Occupancy network decoder [Mescheder et al. 2019]. Please note, that our method is not coupled with any decoder structure."
        },
        {
            "heading": "3.3 Optimizing for shape-pose disentanglement",
            "text": "While our auto-encoder is pose-invariant by construction, the encoding has no explicit relation to the input geometry. In the following we detail our losses to encourage a rigid relation between S\u0303 and \ud835\udc4b , and for making S\u0303 consistent across different objects.\n3.3.1 Rigidity. To train the reconstructed shape S\u0303 to be isometric to the input point cloud X, we enforce a rigid transformation between the two, namely X = S\u0303?\u0303? + 1\ud835\udc41\ud835\udc47 . For point clouds auto-encoding we have used the Chamfer Distance (CD): Lrec = CD ( X, S\u0303?\u0303? + 1\ud835\udc41\ud835\udc47 ) , (9)\nPlease note that other tasks such as implicit function reconstruction use equivalent terms, as we detail in our supplementary files. In addition, while ?\u0303? = \ud835\udc53\ud835\udc45 (X) is rotation-equivariant we need to constraint it to SO(3), and we do so by adding an orthonormal term:\nLortho = \u2225\ud835\udc3c \u2212 ?\u0303??\u0303?\ud835\udc47 \u222522 + \u2225\ud835\udc3c \u2212 ?\u0303? \ud835\udc47 ?\u0303?\u222522, (10)\nwhere \u2225 \u00b7 \u22252 is mean square error (MSE) loss.\n3.3.2 Consistency. Now, our shape reconstruction S\u0303 is isometric to X and it is invariant to\ud835\udc47 and \ud835\udc45. However, there is no guarantee that the pose of S\u0303 would be consistent across different instances. Assume two different point clouds X1,X2 are aligned. If their canonical representations S1, S2 are also aligned, then they have the same rigid transformation w.r.t their canonical representation and vice versa, i.e., Xi = Si\ud835\udc45 +1\ud835\udc41\ud835\udc47 , \ud835\udc56 = 1, 2. To achieve such consistency, we require:\n{\ud835\udc53\ud835\udc47 (X1), \ud835\udc53\ud835\udc45 (X1)} = {\ud835\udc53\ud835\udc47 (X2), \ud835\udc53\ud835\udc45 (X2)}. (11) We generate such pairs of aligned point clouds, by augmenting the input point cloud X with several simple augmentation processes,\nShape-Pose Disentanglement using SE(3)-equivariant Vector Neurons \u2022 5\nwhich do not change the pose of the object. In practice, we have used Gaussian noise addition, furthest point sampling (FPS), patch removal by k-nn (we select one point randomly and remove \ud835\udc58 of its nearest neighbors) and re-sampling of the input point cloud.\nWe then require that the estimated rotation and translation is the same for the original and augmented versions,\nLaugconsist = \u2211\ufe01 \ud835\udc34\u2208A \u2225 \ud835\udc53\ud835\udc45 (X) \u2212 \ud835\udc53\ud835\udc45 (\ud835\udc34(X)) \u222522 + \u2225 \ud835\udc53\ud835\udc47 (X) \u2212 \ud835\udc53\ud835\udc47 (\ud835\udc34(X)) \u2225 2 2,\n(12) where A is the group of pose preserving augmentations and \u2225 \u00b7 \u22252 is MSE loss.\nIn addition, for point cloud reconstruction, we can also generate a version of X, with a known pose, by feeding again the reconstructed shape S\u0303. We transform S\u0303 by a random rotation matrix \ud835\udc45\u2217 and a random translation vector \ud835\udc47 \u2217 and require the estimated pose to be consistent with this transformation:\nLcanconsist = \u2225 \ud835\udc53\ud835\udc45 ( S\u0303\ud835\udc45\u2217 +\ud835\udc47 \u2217 ) \u2212 \ud835\udc45\u2217\u222522 + \u2225 \ud835\udc53\ud835\udc47 ( S\u0303\ud835\udc45\u2217 +\ud835\udc47 \u2217 ) \u2212\ud835\udc47 \u2217\u222522, (13)\nOur overall loss is\nL = Lrec + \ud835\udf061Lortho + \ud835\udf062L aug consist + \ud835\udf063L can consist, (14)\nwhere the \ud835\udf06\ud835\udc56 are hyper parameters, whose values in all our experiments were set to \ud835\udf061 = 0.5, \ud835\udf062 = \ud835\udf063 = 1."
        },
        {
            "heading": "3.4 Inference",
            "text": "At inference time, we feed forward point cloud X \u2208 R\ud835\udc41\u00d73 and retrieve its shape and pose. However, since our estimated rotation matrix ?\u0303? is not guaranteed to be orthonormal, at inference time,\nwe find the closest ortho-normal matrix to ?\u0303? (i.e., minimize the Forbenius norm), following [Bar-Itzhack 1975], by solving:\n\ud835\udc45 = ?\u0303? ( ?\u0303?\ud835\udc47 ?\u0303? )\u2212 12 . (15)\nThe inverse of the square root can be computed by singular value decomposition (SVD). While this operation is also differentiable we have found it harmful to incorporate this constraint during the training phase, thus it is only used during inference. We refer the reader to [Bar-Itzhack 1975] for further details."
        },
        {
            "heading": "4 RESULTS",
            "text": "We preform qualitative and quantitative comparison of our method for learning shape-invariant pose. Due to page limitations, more results can be found in our supplementary files."
        },
        {
            "heading": "4.1 Dataset and implementation details",
            "text": "We employ the ShapeNet dataset [Chang et al. 2015] for evaluation. For point cloud auto-encoding we follow the settings in [Sun et al. 2021] and [Deprelle et al. 2019], and use ShapeNet Core focusing on two categories: airplanes and chairs. While airplanes are more semantically consistent and containing less variation, chairs exhibit less shape-consistency and may contain different semantic parts. All 3D models are randomly rotated and translated in the range of [\u22120.1, 0.1] at train and test time. For all experiments, unless stated otherwise, we sample random 1024 points for each point cloud. The auto-encoder is trained using Adam optimizer with learning rate of 1\ud835\udc52\u22123 for 500 epochs, with drop to the learning rate at 250 and 350 by a factor of 10. We save the last"
        },
        {
            "heading": "6 \u2022 Oren Katzir, Dani Lischinski, and Daniel Cohen-Or",
            "text": "iteration checkpoint and use it for our evaluation. The decoder is AtlasNetV2 [Deprelle et al. 2019] decoder with 10 learnable grids."
        },
        {
            "heading": "4.2 Pose consistency",
            "text": "We first qualitatively evaluate the consistency of our canonical representation as shown in Fig. 4. At test time, we feed different instances at different poses through our trained network, yielding estimated pose of the input object w.r.t the pose-invariant shape. We then apply the inverse transformation learned, to transform the input to its canonical pose. As can be seen, the different instances are roughly aligned, despite having different shapes. More examples can be found in our supplementary files. We also compare our method, both qualitatively and quantitatively, to Canonical Capsules [Sun et al. 2021] andCompass [Spezialetti et al. 2020] by using the alignment in ShapeNet (for Compass no translation is applied). First, we feed forward all of the aligned test point clouds {X\ud835\udc56 }\ud835\udc41\ud835\udc61\ud835\udc56=1 through all methods and estimate their canonical pose {?\u0303?\ud835\udc56 }\ud835\udc41\ud835\udc61\ud835\udc56=1. We expect to have a consistent pose for all aligned input shapes, thus, we quantify for each instance \ud835\udc56 the angular deviation \ud835\udc51consist\n\ud835\udc56 of its estimated pose ?\u0303?\ud835\udc56 from the mean pose\n\ud835\udc51consist \ud835\udc56 = \u2220 ( ?\u0303?\ud835\udc56 , 1 \ud835\udc41\ud835\udc61 \u2211 \ud835\udc56 ?\u0303?\ud835\udc56 ) .We present an histogram of {\ud835\udc51consist \ud835\udc56 }\ud835\udc41\ud835\udc61 \ud835\udc56=1 in Fig. 5. As can be seen, our method results in a more aligned canonical shapes as indicated by the peak around the lower deviation values. We visualize the misalignment of Canonical Capsules\nby sampling objects with small, medium and large deviation, and compare them to the canonical representation achieved by Compass and our method for the same instances. The misalignment of Canonical Capsules may be attributed to the complexity of matching unsupervised semantic parts between chairs as they exhibit high variation (size, missing parts, varied structure). We quantify the consistency by the standard deviation of the estimated pose\u221a\ufe03\n1 \ud835\udc41\ud835\udc61 \u2211 \ud835\udc56 \ud835\udc51 2 \ud835\udc56 in Table 1. Evidently, Compass falls short for both object classes. Canonical Capsules preform slightly better than our method for planes, while our method is much more consistent for the chair category."
        },
        {
            "heading": "4.3 Stability",
            "text": "A key attribute in our approach is the network construction, which outputs a purely SE(3)-invariant canonical shape. Since we do not\nShape-Pose Disentanglement using SE(3)-equivariant Vector Neurons \u2022 7\n. require any optimization for such invariance, our canonical shape is expected to be very stable compared with Canonical Capsules and Compass. We quantify the stability, as proposed by Canonical Capsules, in a similar manner to the consistency metric. For each instance \ud835\udc56 , we randomly rotate the object \ud835\udc58 = 10 times, and estimate the canonical pose for each rotated instance {?\u0303?\ud835\udc56 \ud835\udc57 }\ud835\udc58\ud835\udc57=1. We average across all \ud835\udc41\ud835\udc61 instances the standard deviation of the angular pose estimation as follows,\n\ud835\udc51stability = 1 \ud835\udc41\ud835\udc61 \u2211\ufe01 \ud835\udc56 \u221a\u221a\u221a\u2211\ufe01 \ud835\udc57 \u2220 \u00a9\u00ab?\u0303?\ud835\udc56 \ud835\udc57 , 1\ud835\udc58 \u2211\ufe01 \ud835\udc57 ?\u0303?\ud835\udc56 \ud835\udc57 \u00aa\u00ae\u00ac 2 . (16)\nThe results are reported in Table 1. As expected, Canonical Capsules and Compass exhibit non-negligible instability, as we visualize in Fig. 6."
        },
        {
            "heading": "4.4 Reconstruction quality",
            "text": "We show qualitatively our point cloud reconstruction in Fig. 7. Please note that our goal is not to build a SOTA auto-encoder in terms of reconstruction, rather we learn to disentangle pose from shape via auto-encoding. Nonetheless, our auto-encoder does result in a pleasing result as shown in Fig. 7. Moreover, since we utilize AtlasNetV2[Deprelle et al. 2019] which utilizes a multiple patch-based decoder, we can examine which point belongs to which decoder. As our shape-encoding is both invariant to pose and consistent across different shapes, much like in the aligned scenario, each decoder assume some-what of semantic meaning, capturing for example the right wing of the airplanes. Please note that we do not enforce any structuring on the decoders."
        },
        {
            "heading": "8 \u2022 Oren Katzir, Dani Lischinski, and Daniel Cohen-Or",
            "text": ""
        },
        {
            "heading": "4.5 3D implicit reconstruction",
            "text": "We show that our encoder can be attached to a different reconstruction task by repeating OccNet [Mescheder et al. 2019] completion experiment. We replace OccNet encoder with our shape-pose disentagling encoder. The experiment is preformed with the same settings as in [Mescheder et al. 2019]. We use the subset of [Choy et al. 2016], and the point clouds are sub-sampled from the watertight mesh, containing only 300 points and applied with a Gaussian noise. We have trained OccNet for 600\ud835\udc3e iterations and report the results of the best (reconstruction wise) checkpoint. We show in Fig. 8 a few examples of rotated point clouds (left), its implicit function reconstruction (middle) and the implicit function reconstruction in the canonical pose (right)."
        },
        {
            "heading": "5 CONCLUSIONS",
            "text": "We have presented a stable and consistent canonical representation learning. To achieve a pose-invariant represenation, we have devised an SE(3)-equivairant encoder, extending the VNN framework, to meet the requirements of canonical pose learning, i.e., learning rigid transformations. Our experiments show, both qualitatively and quantitatively, that our canonical representation is significantly more stable than recent approaches and has similar or better consistency, especially for diverse object classes. Moreover, we show that our approach is not limited to specific decoding mechanism, allowing for example to reconstruct canonical implicit neural field. In the future, we would like to explore the potential of our canonical representation for point cloud processing tasks requiring aligned settings, such as completion and unsupervised segmentation, where the canonical representation is learned on-the-fly, along with the task."
        },
        {
            "heading": "10 \u2022 Oren Katzir, Dani Lischinski, and Daniel Cohen-Or",
            "text": "A SE(3)-EQUIVARIANCE VERIFICATION In this section we verify that our VNT layers are indeed translation and rotation equivariant, as well as explicitly present other layers, not included in the paper.\nA.1 Verifying SE(3)-equivariant linear layer We verify that the linear module \ud835\udc53lin (\u00b7;W), defined via a weight matrix W \u2208 W\ud835\udc36\u2032\u00d7\ud835\udc36 , acting on a vector-list feature V \u2208 R\ud835\udc36\u00d73, such that\nW\ud835\udc36 \u2032\u00d7\ud835\udc36 = { W \u2208 R\ud835\udc36 \u2032\u00d7\ud835\udc36 | \u2211\ud835\udc36\ud835\udc57=1\ud835\udc64\ud835\udc56, \ud835\udc57 = 1 \u2200\ud835\udc56 \u2208 [1,\ud835\udc36 \u2032]} , (17) is SE(3)-equivariant. Let wj \u2208 R\ud835\udc36\n\u2032\u00d7\ud835\udc36 be the \ud835\udc57 column of W, and let \ud835\udc45 \u2208 R3\u00d73 be a rotation matrix and \ud835\udc47 \u2208 R1\u00d73 a translation vector. For \ud835\udc53lin (\u00b7;W) to be SE(3)-equivariant, the following must hold:\n\ud835\udc53lin (V\ud835\udc45 + 1\ud835\udc36\ud835\udc47 ) = W (V\ud835\udc45 + 1\ud835\udc36\ud835\udc47 ) = WV\ud835\udc45 + W1\ud835\udc36\ud835\udc47 =\n= WV\ud835\udc45 + \u00a9\u00ab \ud835\udc36\u2211\ufe01 \ud835\udc57=1 w\ud835\udc57 \u00aa\u00ae\u00ac\ud835\udc47 = (WV)\ud835\udc45 + 1\ud835\udc36\u2032\ud835\udc47,= \ud835\udc53lin (V)\ud835\udc45 + 1\ud835\udc36\u2032\ud835\udc47, (18)\nwhere 1\ud835\udc36 = [1, 1, . . . , 1]\ud835\udc47 \u2208 R\ud835\udc36\u00d71 is a column vector of length \ud835\udc36 , and \u2211\ud835\udc36 \ud835\udc57=1 wj = 1\ud835\udc36\u2032 , since (\u2211\ud835\udc36 \ud835\udc57=1 wj ) [\ud835\udc56] = \u2211\ud835\udc36\ud835\udc57=1\ud835\udc64\ud835\udc56 \ud835\udc57 = 1 for \ud835\udc56 = 1, . . . ,\ud835\udc36 \u2032\nA.2 Verifying SE(3)-equivariant ReLU We verify that the ReLU layer is SE(3)-equivariant.\nLet V,V\u2032 \u2208 R\ud835\udc36\u00d73 be the input and output of a ReLU layer,\nV\u2032 = \ud835\udc53ReLU (V) . (19)\nLet v\u2032 \u2208 R1\u00d73 be a single vector, such that v\u2032 \u2208 V\u2032. As explained in Section 3.1 of the paper, we learn three translation equivariant linear maps, Q,K,O \u2208 W1\u00d7\ud835\udc36 projecting the input to q, k, o \u2208 R1\u00d73, yielding an origin o, a feature qo = q \u2212 o and a direction ko = k \u2212 o. The ReLU layer for a single vector neuron is then defined via\nv\u2032 = { o + qo if \u27e8qo, ko\u27e9 \u2265 0, o + qo \u2212 \u2329 qo,\nko \u2225ko \u2225 \u232a ko \u2225ko \u2225 , otherwise.\n. (20)\nq, k, o are SE(3)-equivariant and according to Eq .(7) of the paper, qo, ko are translation invariant (and rotation equivariant) as they are the subtraction of two SE(3)-equivariant vector neurons, thus, the condition term \u27e8qo, ko\u27e9 is also translation invariant. As shown in VNN [6], the inner product of two rotation equivariant vectorneurons is rotation invariance. Similarly here, assume the input V is rotated with a rotation matrix \ud835\udc45 \u2208 R3\u00d73, then\n\u27e8qo\ud835\udc45, ko\ud835\udc45\u27e9 = qo\ud835\udc45\ud835\udc45\ud835\udc47 ko = qoko\ud835\udc47 = \u27e8qo, ko\u27e9 (21)\nTo conclude, the condition term \u27e8qo, ko\u27e9 is SE(3)-invariant. When \u27e8qo, ko\u27e9 \u2265 0, the output vector neuron v\u2032 = o + qo = q,\nand thus it is SE(3)-equivariant. When \u27e8qo, ko\u27e9 < 0 the output vector neuron is\no + qo \u2212 \u2329 qo,\nko \u2225ko\u2225 \u232a ko \u2225ko\u2225 , (22)\nSimilarly to Eq .(21) the term\u2329 qo,\nko \u2225ko\u2225\n\u232a 1\n\u2225ko\u2225 = \u27e8qo, ko\u27e9 \u27e8ko, ko\u27e9 , (23)\nis also SE(3)-invariant. We can now easily prove that if the input V is rotated with a rotation matrix \ud835\udc45 \u2208 R3\u00d73 and translation vector \ud835\udc47 \u2208 R1\u00d73, then\nv\u2032 =o\ud835\udc45 + 1\ud835\udc36\ud835\udc47 + qo\ud835\udc45 \u2212 \u2329 qo,\nko \u2225ko\u2225 \u232a ko\ud835\udc45 \u2225ko\u2225\n= ( o + qo \u2212 \u2329 qo,\nko \u2225ko\u2225 \u232a ko \u2225ko\u2225 ) \ud835\udc45 + 1\ud835\udc36\ud835\udc47 = v\u2032\ud835\udc45 + 1\ud835\udc36\ud835\udc47, (24)\nThereby completing the proof.\nA.3 VNT-LeakyReLU LeakyReLU is defined in a similar manner to the ReLU layer, with slight modification to the output vector neuron, given by\nv\u2032 = \ud835\udefcq + (1 \u2212 \ud835\udefc) v\u2032ReLU, (25) where \ud835\udefc \u2208 R\nEasy to see that the v\u2032 is SE(3)-equivariant.\nA.4 VNT-MaxPool Given a set of vector-neuron list V \u2208 R\ud835\udc41\u00d7\ud835\udc36\u00d73, we learn two linear maps K,O \u2208 W\ud835\udc36\u00d7\ud835\udc36 , shared between Vn \u2208 V .\nWe obtain a translation invariant direction\nK = {KVn \u2212 OVn, }\ud835\udc41\ud835\udc5b=1 (26) and a translation invariant features\nQ = {Vn \u2212 OVn}\ud835\udc41\ud835\udc5b=1 . (27) The VNT-MaxPool is defined by\n\ud835\udc53\ud835\udc40\ud835\udc34\ud835\udc4b (V) [\ud835\udc50] = V\ud835\udc5b\u2217 [\ud835\udc50] (28) where \ud835\udc5b\u2217 = argmax\n\ud835\udc5b \u27e8Qn [\ud835\udc50],Kn [\ud835\udc50]\u27e9, (29)\nwhere Qn \u2208 Q and Kn \u2208 K . Since Qn, Kn are translation invariant, and their inner product is also rotation invariant the selection process of \ud835\udc5b\u2217 for every channel \ud835\udc50 is invariant to SE(3). We note that both K,O can be shared across vector-neurons.\nB IMPLEMENTATIONS DETAILS\nB.1 Encoder architecture In this section we elaborate on our encoder architecture. Our encoder contains VNT layers following with VNN layers as reported in Table 2. LinearLeakyReLU stands for the leakyReLU with feature learning Q. For the exact VNN layers definition (and specifically STNkd) we refer the reader to VNN [6].\nC IMPLICIT RECONSTRUCTION Occupancy network reconstruction from a point clouds \ud835\udc4b \u2208 R\ud835\udc41\u00d73, with a learned embedding z \u2208 X , learns a mapping function \ud835\udc53\ud835\udf03 (\ud835\udc5d, z) : R3 \u00d7 X \u2192 [0, 1]. In occupancy network completion experiment, the point cloud is sampled from the watertight mesh, and the mesh is used as supervision to sample\ud835\udc40 training point {\ud835\udc5d\ud835\udc56 }\ud835\udc40\ud835\udc56=1\ninside and outside the mesh, indicated by {\ud835\udc5c\ud835\udc56 } \u2208 [0, 1]\ud835\udc40 . We follow the same experiment, with slight changes. We feed our learned pose-invariant encoding Zs through \ud835\udc53\ud835\udf03 , and project the points {\ud835\udc5d\ud835\udc56 } from the input pose to the learned canonical pose by:\n\ud835\udc5d\ud835\udc56 = (\ud835\udc5d\ud835\udc56 \u2212\ud835\udc47 )?\u0303?\ud835\udc47 \ud835\udc56 = 1, . . . , \ud835\udc40 (30)\nTherefore, our reconstruction loss is\nLrec = \ud835\udc40\u2211\ufe01 \ud835\udc56=1 LBCE (\ud835\udc53\ud835\udf03 (\ud835\udc5d\ud835\udc56 ,Zs), \ud835\udc5c\ud835\udc56 ), (31)\nwhere LBCE is binary cross entropy loss. For implicit reconstruction we have found it beneficial to train the network in an alternating approach, where at the first phase we backward w.r.t Lrec and in the second phase we backward w.r.t\nL2 = \ud835\udf061Lortho + \ud835\udf062L aug consist . (32)"
        },
        {
            "heading": "D ADDITIONAL RESULTS D.1 Augmentations ablation",
            "text": "In this section we specify in more details our augmentations, which can be seen in Fig. 9, and ablate their individual donation to the consistency of our canonical representation. Our augmentations are Furthest point sampling (FPS), with random number of points \ud835\udc41FPS = U (300, 500) per batch, K-NN removal (KNN), where a point is randomly selected on the point cloud, and its \ud835\udc41KNN = 100 points are removed, Gaussian Noise added to the point clouds with \ud835\udf07 = 0 and \ud835\udf0e = 0.025, a re-sampling augmentations (Resample) where we re-select which \ud835\udc41 = 1024 to sample from the original point cloud, and canonical rotation (Can), where the point cloud reconstruction in its canonical representation is rotated and transformed to create a supervised version of itself. Since our method is pose-invariant by construction, different augmentations have no effect on the stability, thus, we ablate only w.r.t the consistency as reported in Table 3. We ablate the donation of each augmentation by removing it from the training process and measuring the consistency as defined in the paper. Evidently, the lesser factor is the noise addition augmentation, while KNN and FPS donate the most to the consistency metric.\nD.2 Pose consistency We present more canonical alignment results for point clouds and implicit function reconstruction Fig. 10, Fig. 11, Fig. 12 and Fig. 13.\nIn addition, we experiment with partial dataset for shape completion derived from ShapeNet (See Yuan et al. \"Pcn: Point completion network\"). The partial points clouds are a projection of 2.5D depth maps of the model into 3D point clouds. The dataset contains 8 such partial point clouds per model. As can be seen in Fig. 14 while learning a consistent canonical pose is difficult for partial shapes, our canonical pose is reasonable and mostly consistent. Although, misalignment is apparent in the consistency histogram, please note that no complete point cloud is present in this setting, and no hyperparameter tuning was done.\nD.3 Stability Please see the attached videos for stability visualization, divided to two sub-folders for chairs and airplanes. In each video, we sample a single point cloud and rotate it with multiple random rotation matrices. We feed the rotated point cloud (see on the left of each video) through Compass, Canonical Capsules and Our method, and show the input point cloud in canonical pose for all methods. Our method reconstruct a SE(3)-invariant canonical representation and a SE(3)-equivariant pose estimation, thus, almost no changes are observable in the canonical representation, while both Canonical Capsules and Compass exhibit instability in the canonical pose estimation."
        },
        {
            "heading": "12 \u2022 Oren Katzir, Dani Lischinski, and Daniel Cohen-Or",
            "text": "Shape-Pose Disentanglement using SE(3)-equivariant Vector Neurons \u2022 13"
        },
        {
            "heading": "14 \u2022 Oren Katzir, Dani Lischinski, and Daniel Cohen-Or",
            "text": "Shape-Pose Disentanglement using SE(3)-equivariant Vector Neurons \u2022 15"
        }
    ],
    "title": "Shape-Pose Disentanglement using SE(3)-equivariant Vector Neurons",
    "year": 2022
}