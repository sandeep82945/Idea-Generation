{
    "abstractText": "Given a compact subset of a Banach space, the Chebyshev center problem consists of finding a minimal circumscribing ball containing the set. In this article we establish a numerically tractable algorithm for solving the Chebyshev center problem in the context of optimal learning from a finite set of data points. For a hypothesis space realized as a compact but not necessarily convex subset of a finite-dimensional subspace of some underlying Banach space, this algorithm computes the Chebyshev radius and the Chebyshev center of the hypothesis space, thereby solving the problem of optimal recovery of functions from data. The algorithm itself is based on, and significantly extends, recent results for near-optimal solutions of convex semi-infinite problems by means of targeted sampling, and it is of independent interest. Several examples of numerical computations of Chebyshev centers are included in order to illustrate the effectiveness of the algorithm.",
    "authors": [
        {
            "affiliations": [],
            "name": "PRADYUMNA PARUCHURI"
        }
    ],
    "id": "SP:bdbfcf8a348f77c5a8d034545890b60becc282eb",
    "references": [
        {
            "authors": [
                "A.R. Alimov",
                "I. G"
            ],
            "title": "Tsar\u2019kov",
            "venue": "Geometric Approximation Theory, Springer Monographs in Mathematics, Springer, Cham,",
            "year": 2021
        },
        {
            "authors": [
                "D. Bertsimas",
                "D. Brown",
                "C. Caramanis"
            ],
            "title": "Theory and applications of robust optimization",
            "venue": "SIAM Review",
            "year": 2010
        },
        {
            "authors": [
                "P. Binev",
                "A. Bonito",
                "R. DeVore",
                "G. Petrova"
            ],
            "title": "Optimal learning",
            "venue": "arXiv preprint: https://arxiv. org/abs/2203.15994",
            "year": 2022
        },
        {
            "authors": [
                "A. Ben-Tal",
                "L. El Ghaoui",
                "A.S. Nemirovski"
            ],
            "title": "Robust Optimization",
            "venue": "Princeton Series in Applied Mathematics, Princeton University Press",
            "year": 2009
        },
        {
            "authors": [
                "F. Cucker",
                "S. Smale"
            ],
            "title": "On the mathematical foundations of learning, American Mathematical Society",
            "venue": "Bulletin. New Series",
            "year": 2002
        },
        {
            "authors": [
                "S. Das",
                "A. Aravind",
                "A. Cherukuri",
                "D. Chatterjee"
            ],
            "title": "Near-optimal solutions of convex semi-infinite programs by targeted sampling",
            "venue": "Annals of Operations Research; doi: https://doi.org/10.1007/ s10479-022-04810-4",
            "year": 2022
        },
        {
            "authors": [
                "R. DeVore",
                "G. Petrova",
                "P. Wojtaszczyk",
                "Data assimilation",
                "sampling in Banach spaces",
                "Calcolo"
            ],
            "title": "A Quarterly on Numerical Analysis and Theory of Computation 54 (2017)",
            "venue": "no. 3, 963\u2013",
            "year": 1007
        },
        {
            "authors": [
                "R.A. Maronna",
                "R.D. Martin",
                "V.J. Yohai"
            ],
            "title": "Salibi\u00e1n-Barrera, Robust Statistics, Wiley Series in Probability and Statistics",
            "venue": "Theory and methods (with R),",
            "year": 2019
        },
        {
            "authors": [
                "C.A. Micchelli",
                "T.J. Rivlin"
            ],
            "title": "A survey of optimal recovery, Optimal estimation in approximation theory, Plenum",
            "venue": "New York,",
            "year": 1977
        },
        {
            "authors": [
                "V.I. Paulsen",
                "M. Raghupathi"
            ],
            "title": "An Introduction to the Theory of Reproducing Kernel Hilbert Spaces",
            "venue": "Cambridge Studies in Advanced Mathematics, vol. 152, Cambridge University Press, Cambridge",
            "year": 2016
        },
        {
            "authors": [
                "R. Storn",
                "K. Price"
            ],
            "title": "Differential evolution\u2014a simple and efficient heuristic for global optimization over continuous spaces",
            "venue": "Journal of Global Optimization",
            "year": 1997
        }
    ],
    "sections": [
        {
            "text": "\u00a71. Introduction\nLearning \u00e0 la approximation theory dates back at least to [CS02], and today it occupies the centerstage of the vibrant area of machine learning. The central idea herein is to leverage quantitative estimates germane to the field of approximation theory in the context of function learning from (possibly) finitely many input/output data points. This function learning is realized in the form of the selection of a function from a reasonable model class (also called hypothesis space) dictated by the physics of the problem or an educated guess, that not only (nearly) justifies the data points in a certain precise sense, but is also capable of generalization beyond the given data set. Naturally, the procedure for the selection of such a function is of central importance in terms of both applicability and numerical tractability.\nPreceding works [MR77, DPW17, BBDP22] from the closely allied areas of interpolation theory and approximation theory proposed the framework of optimal recovery in the context of function learning, wherein the aforementioned selection problem is posed in terms of furnishing a minimizer of the worst case error incurred by such a selection in\nDate: 6th July 2023 1:10am GMT. Key words and phrases. optimal learning, optimal interpolation, Chebyshev center problem, convex semiinfinite programs. The first author was supported by the PMRF grant RSPMRF0262 from the Government of India. The authors are in the process of submitting a patent application based on the results reported herein. 1\nar X\niv :2\n30 7.\n01 30\n4v 1\n[ m\nat h.\nO C\n] 3\nthe hypothesized model class. Mathematically, the preceding desideratum translates to the so-called Chebyshev center problem [AT21, Chapter 15], namely the best approximation of a set by a singleton. Let us briefly recall that in a Banach space (Z, \u2225\u00b7\u2225), a Chebyshev center of a closed and bounded subset \ud835\udc3e \u2282 Z is defined as the center of a ball of smallest radius circumscribing \ud835\udc3e . To wit, a Chebyshev center of \ud835\udc3e is an optimizer of the variational problem:\n(1) minimize\ud835\udc53 \u2208Z sup \ud835\udc54\u2208\ud835\udc3e \u2225 \ud835\udc53 \u2212 \ud835\udc54\u2225.\nThe optimal value of (1) is the Chebyshev radius \ud835\udc5f\ud835\udc3e of \ud835\udc3e . In general, depending on the nature of the norm \u2225\u00b7\u2225, a set \ud835\udc3e may have multiple Chebyshev centers; the corresponding set is denoted by \ud835\udc4d (\ud835\udc3e). The cartoon figure given below illustrates a family of Chebyshev centers \ud835\udf01 for subsets of \u211d2 under the Euclidean norm and the \u21131-norm; observe that in the latter case, the set \ud835\udc4d (\ud835\udc3e) of Chebyshev centers is not a singleton.\nIn the context of learning theory, the Chebyshev center problem encodes the idea of optimal learning in the hypothesized model class: Here Z represents the space of functions in which lie the hypothesis classes, and the set \ud835\udc3e represents the subset of the model class of functions that satisfies the given data. The corresponding optimization problem (1) is fraught with stiff numerical challenges: \u2022 For each fixed \ud835\udc53 \u2208 Z, the inner maximization over \ud835\udc54 in (1) is on a potentially infinite-\ndimensional subset of the Banach spaceZ, and its solutions are rarely, if ever, expressible parametrically in closed form in \ud835\udc53 . \u2022 The outer minimization over \ud835\udc53 in (1) is, in general, also on an infinite-dimensional Banach space Z.\nIn either case, (1) is numerically intractable. For reasons of computational tractability, one is, consequently, forced to \u201cdiscretize\u201d the various infinite-dimensional objects in (1) above, and work in a finite dimensional setting;1 the resulting mathematical optimization is a variant of the so-called relative Chebyshev center problem.\nA relative Chebyshev center of a closed and bounded subset \ud835\udc3e with respect to a nonempty \ud835\udc4b \u2282 Z is given by an optimizer of:\n(2) minimize\ud835\udc53 \u2208\ud835\udc4b sup \ud835\udc54\u2208\ud835\udc3e \u2225 \ud835\udc53 \u2212 \ud835\udc54\u2225,\nwhere the set \ud835\udc4b is chosen to be a reasonably fine \u201cfinite\u201d discretization approximating the Banach space Z and the model class class \ud835\udc3e is restricted to a suitable finite dimensional object. Nevertheless, even the resulting simplified problem (2) continues to be numerically challenging:\n1Let us draw attention to the fact, as pointed out in [AT21, Section 16.1], that for computational tractability, it is imperative to restrict attention to finitary objects; consequently, considering finite-dimensional avatars of the various objects in (1) is the best that can be done.\n(rCC-1) The simplest version, although very unrealistic, is when \ud835\udc4b is a subset of \u211d\ud835\udc5b and \ud835\udc3e is a finite collection of points in \u211d\ud835\udc5b. The time complexity of solving such problems grows exponentially with the dimension \ud835\udc5b [AT21, Chapter 15, p. 362] in general, and this is the current state of the art. (rCC-2) A more realistic setting is that of \ud835\udc4b being a finite dimensional subspace of Z and \ud835\udc3e being compact (although not necessarily convex), and in this setting, the problem (2) is known to be NP-hard [AT21, Chapter 15, p. 362]. While there exist algorithms that compute the Chebyshev centers of special types of subsets \ud835\udc3e of Euclidean spaces, none of them is sufficiently general to admit non-convex \ud835\udc3e , nor do these algorithms scale reasonably with the dimension of \ud835\udc3e; we refer the reader to the discussion in [AT21, Chapter 15, p. 362] for details and references."
        },
        {
            "heading": "Contributions.",
            "text": "(A) The chief contribution of this article is a computationally tractable algorithm to solve\n(2). \u25e6 Specifically, our algorithm finds an approximant \ud835\udc53 \u2208 \ud835\udc4b such that 2\n(3) \ud835\udc54 \u2212 \ud835\udc53 \u2a7d \ud835\udc5f\ud835\udc3e + \ud835\udc5c(1) for all \ud835\udc54 \u2208 \ud835\udc3e,\nwhere the term \ud835\udc5c(1) on the right-hand side of (3) captures all the errors due to numerical inaccuracies, algorithmic truncation, etc., at the level of (2). Cf. [BBDP22] devises an algorithm to find an approximant \ud835\udc53 \u2208 \ud835\udc4b that satisfies\nfor some constant \ud835\udc36 > 2, \ud835\udc54 \u2212 \ud835\udc53 \u2a7d \ud835\udc36 \ud835\udc5f\ud835\udc3e for all \ud835\udc54 \u2208 \ud835\udc3e;\nto the best of our knowledge, the preceding bound is currently the best available. In contrast, the algorithm reported in this article attains the best possible benchmark error bound in the relative Chebyshev center problem (2) modulo the term \ud835\udc5c(1); to wit, the constant \ud835\udc36 = 1 appears in our error bound and one can do no better than this. We refer to the situation described by (3) as the attainment of the Chebyshev bound in the context of learning theory. \u25e6 Moreover, our algorithm does not require \ud835\udc3e to be convex although compactness of \ud835\udc3e is critical for us. Consequently and for instance, \ud835\udc3e being a finite disjoint union of non-convex compact sets is perfectly admissible in our setting, and the Chebyshev bound (3) continues to hold. Nor do we insist that the underlying norm should be strictly convex. \u25e6 Furthermore, the memory requirement of our algorithm scales linearly with respect to the dimension of \ud835\udc3e; see also the discussion and references in (rCC-1) and (rCC2) above. It is conceivably possible to employ other robust optimization tools (employing, e.g., random sampling techniques) to arrive at solutions to (2), but to the best of our knowledge, no other numerically algorithm is capable of scaling linearly with respect to the dimension of \ud835\udc3e . Naturally, our algorithm applies to the problem of optimal recovery \u00e0 la [MR77] of functions from sampled measurements (which is of key relevance in signal processing), and in a sense conclusively answers the quest for a tractable numerical algorithm for optimal recovery. (B) In the process of devising the aforementioned algorithm, we solve a more general problem of independent interest. This contribution consists of a numerically tractable algorithmic mechanism to solve a broad class of convex semi-infinite programs that subsumes the relative Chebyshev center problem (2). The mechanism is an extension of the algorithm recently reported in [DACC22], making it applicable to a wider class of convex semi-infinite programs (SIPs) and also enabling it to extract optimizers of such convex SIPs. In the context of the problem (2), these features contribute to the\n2Recall the Landau notation \ud835\udf19 (\ud835\udc67) = \ud835\udc5c(\ud835\udc67) that stands for a function \ud835\udf19 (0) = 0 and lim\ud835\udc67\u21920 |\ud835\udf19 (\ud835\udc67) ||\ud835\udc67 | = 0.\nextraction of relative Chebyshev centers of potentially non-convex (but compact) sets despite the absence of strict convexity of the underlying norm, etc.\nContent and organization. In \u00a72 we formally relate the problem of learning to an appropriate relative Chebyshev center problem viewed as a convex min-max optimization. The case of learning in the setting of finite-dimensional Banach spaces is treated in detail, along with a specific application to reproducing kernel Hilbert spaces. The reformulation of min-max optimization problems into convex semi-infinite programs and the applicability of our algorithm is discussed at the end of \u00a72, thereby completing the presentation of our contribution (A). The technically standalone intervening \u00a73 details the algorithm to solve convex SIPs and also the process of extracting optimizers of convex SIPs via regularization, which completes the presentation of our contribution (B). Numerical experiments are presented in \u00a74 illustrating our algorithm and the role of regularization in extracting the Chebyshev center(s).\n\u00a72. Optimal learning via Chebyshev centers\nA typical setting of the learning problem is that we are given a few observations on the behaviour of a function and we are required to estimate/approximate its behaviour elsewhere. The observations/measurements are of various types: point evaluations of the function if it is known to be continuous, output of linear functionals operating on the function, etc. In general, prior knowledge about the nature of admissible functions is encoded into the learning problem by specifying a hypothesis class.\nLet (Z, \u2225\u00b7\u2225) be a Banach space and \ud835\udc40 \u2282 Z be a compact subset representing the hypothesis class. Information on the object of interest \ud835\udc53 is given in terms of a nonempty input-output set \ud835\udc37. Let \ud835\udc3e \u2282 \ud835\udc40 denote the set of all possible instantiations of \ud835\udc53 that generates the data \ud835\udc37, that is,\n\ud835\udc3e B {\ud835\udc54 \u2208 \ud835\udc40 | \ud835\udc54 satisfies the data in \ud835\udc37}. The objective of learning is to find an \ud835\udc53 that minimizes the error of approximation from all possible sources from the model class \ud835\udc40 of the given data \ud835\udc37, and mathematically, this translates to the min\u2212max problem: (4) minimize\n\ud835\udc53 \u2208Z sup \ud835\udc54\u2208\ud835\udc3e \u2225 \ud835\udc53 \u2212 \ud835\udc54\u2225.\nThe solution(s) to (4) constitute Chebyshev center(s) of the set \ud835\udc3e in Z, as mentioned in the introduction. In this article, we are concerned with the premise of noise-free data. The case of noisy data is a more involved problem and will be studied in subsequent articles.\nAt the level of description in (4), the learning problem is numerically intractable since it involves objects in possibly infinite-dimensional spaces. One needs to discretize/restrict the search space for the approximant \ud835\udc53 to a sufficiently large finite-dimensional space, say \u03a3\ud835\udc41 , of dimension \ud835\udc41 . This restriction converts problem (4) of finding a Chebyshev center to that of a relative Chebyshev center problem, namely,\n(5) minimize \ud835\udc53 \u2208\u03a3\ud835\udc41 sup \ud835\udc54\u2208\ud835\udc3e \u2225 \ud835\udc53 \u2212 \ud835\udc54\u2225.\nAlthough the algorithm presented in \u00a73 is theoretically capable in solving (5), \ud835\udc3e being infinite-dimensional, it still remains numerically intractable. In order to arrive at a tractable solution, one needs to restrict attention to a finite-dimensional approximation to the model class. Alternatively, one can choose to restrict attention to those functions in the model class \ud835\udc40 that lie in a fixed finite-dimensional subspace \ud835\udc40\ud835\udc5a of Z. The choice of this reduced model class is reliant on various factors, e.g., domain specific knowledge, trade-off between\ncomputational capability and tolerance to error, etc., and is up to the designer. The finite discretization of (4) is the following:\n(6) minimize \ud835\udc53 \u2208\u03a3\ud835\udc41 sup \ud835\udc54\u2208\ud835\udc3e\u2229\ud835\udc40\ud835\udc5a \u2225 \ud835\udc53 \u2212 \ud835\udc54\u2225\nIn other words, we seek the best approximant \ud835\udc53 of the functions in \ud835\udc3e \u2229 \ud835\udc40\ud835\udc5a. We present an algorithm to solve (6) exactly (modulo numerical/convergence errors) in \u00a73. To give a sneak peek, we solve the max min max problem:\n(7) maximize \ud835\udc541 ,...,\ud835\udc54\ud835\udc5a+1\u2208\ud835\udc3e\u2229\ud835\udc40\ud835\udc5a min \ud835\udc53 \u2208\u03a3\ud835\udc41 max{\u2225 \ud835\udc53 \u2212 \ud835\udc54\ud835\udc56 \u2225 | \ud835\udc56 = 1, . . . , \ud835\udc5a + 1}\nto extract the solution of the Chebyshev center problem.\nIn the rest of this section we discuss the avatars of (6) in the special settings of measurements being driven by linear functionals on the Banach space and a reproducing kernel Hilbert space (RKHS), followed by a detailed treatment of arriving at (7).\nMeasurements from linear functionals. Suppose that the measurements/observations in the data set \ud835\udc37 are given in terms of the outputs of finitely many linear functionals from the dual space Z\u2032 of Z. That is, given a collection (\ud835\udf06\ud835\udc56)\ud835\udc51\ud835\udc56=1 \u2282 Z \u2032, \ud835\udc53 is known to satisfy\n(8) \ud835\udf06\ud835\udc56 ( \ud835\udc53 ) = \ud835\udc66\ud835\udc56 for \ud835\udc56 = 1, . . . , \ud835\udc51. At the level of description in (8), \ud835\udc53 can be any candidate in a translated subspace of Z. The information on the hypothesis class specialises the search domain to a bounded subset, say the \ud835\udc45-radius ball \ud835\udc40 B \ud835\udd39[0, \ud835\udc45] \u2282 Z. Specifically, to ensure numerical tractability, the candidate functions need to be parametrised finitely. To that end, we restrict attention to a (sufficiently large) finite dimensional subspace \ud835\udc40\ud835\udc5a B span{\ud835\udf13 \ud835\udc57 \u2208 Z | \ud835\udc57 = 1, . . . , \ud835\udc5a} \u2282 Z. The search space for the approximant \ud835\udc53 must also be made finite dimensional and we let \u03a3\ud835\udc41 B span{\ud835\udf11\ud835\udc56 \u2208 Z | \ud835\udc56 = 1, . . . , \ud835\udc41}. The choices of these discretizations are up to the designer.\nRepresenting functions in \ud835\udc40\ud835\udc5a with their coefficients under the basis (\ud835\udf13 \ud835\udc57 )\ud835\udc5a\ud835\udc57=1, the data set \ud835\udc37 is translated to a subset of \u211d\ud835\udc5a satisfying\n(9) \ud835\udc5a\u2211\ufe01 \ud835\udc57=1 \ud835\udefd \ud835\udc57\ud835\udf13 \ud835\udc57 \u2208 \ud835\udc40\nand (10) \ud835\udc5a\u2211\ufe01 \ud835\udc57=1 \ud835\udefd \ud835\udc57\ud835\udf06\ud835\udc56 (\ud835\udf13 \ud835\udc57 ) = \ud835\udc66\ud835\udc56 for \ud835\udc56 = 1, . . . , \ud835\udc51.\nThe constraint (9) translates to \ud835\udefd \u2208 \ud835\udc36 \u2282 \u211d\ud835\udc5a for an appropriately defined compact set \ud835\udc36. Defining\n\ud835\udc66 B \u00a9\u00ab \ud835\udc661 ... \ud835\udc66\ud835\udc51 \u00aa\u00ae\u00ae\u00ac \u2208 \u211d\ud835\udc51 and \ud835\udf06 [\ud835\udc37] B \u00a9\u00ab \ud835\udf061 (\ud835\udf131) \ud835\udf061 (\ud835\udf132) . . . \ud835\udf061 (\ud835\udf13\ud835\udc5a) ... ... . . . ... \ud835\udf06\ud835\udc51 (\ud835\udf131) \ud835\udf06\ud835\udc51 (\ud835\udf132) . . . \ud835\udf06\ud835\udc51 (\ud835\udf13\ud835\udc5a) \u00aa\u00ae\u00ae\u00ac , the constraint (10) is written concisely as\n\ud835\udf06 [\ud835\udc37]\ud835\udefd = \ud835\udc66.\nWith the data set \ud835\udc37 as above and using the prescribed discretization, (6) takes the form:\n(11) minimize \ud835\udefc\u2208\u211d\ud835\udc41\nmax \ud835\udefd\u2208\ud835\udc36\n\ud835\udf06 [\ud835\udc37 ]\ud835\udefd=\ud835\udc66\n\ud835\udc41\u2211\ufe01\ud835\udc56=1 \ud835\udefc\ud835\udc56\ud835\udf11\ud835\udc56 \u2212 \ud835\udc5a\u2211\ufe01 \ud835\udc57=1 \ud835\udefd \ud835\udc57\ud835\udf13 \ud835\udc57 .\nFigure 2 illustrates a typical scenario of the learning problem described in (11). The region shaded in blue represents the discretized hypothesis class \ud835\udc40\ud835\udc5a and the goal in the learning setting is to find the best approximant of the region colored in red.\nRKHS setting. Suppose that the underlying space is a reproducing kernel Hilbert space (RKHS) denoted by H . Recall that a Hilbert space (H , \u27e8\u00b7, \u00b7\u27e9) of functions on a nonempty set \ud835\udc46 is an RKHS over the field \u211d if every evaluation functional is bounded, i.e., for each \ud835\udc65 \u2208 \ud835\udc46 the linear map\nH \u220b \ud835\udc53 \u21a6\u2192 ev\ud835\udc65 ( \ud835\udc53 ) B \ud835\udc53 (\ud835\udc65) \u2208 \u211d is bounded. Every RKHS is equipped with its unique reproducing kernel \ud835\udf05 [PR16, Chapter 2], a mapping \ud835\udf05 : \ud835\udc46 \u00d7 \ud835\udc46 \u2192 \u211d such that for every \ud835\udc65 \u2208 \ud835\udc46 we have\nev\ud835\udc65 ( \ud835\udc53 ) = \u27e8\ud835\udf05(\ud835\udc65, \u00b7), \ud835\udc53 \u27e9 . Let \ud835\udf05\ud835\udc65 denote the function in H corresponding to the evaluation functional ev\ud835\udc65 at \ud835\udc65 , that is characterized by the preceding equality. Let \ud835\udc51 \u2208 \u2115\u2217 and suppose the data set \ud835\udc37 is given in terms of data points \ud835\udc37 B {(\ud835\udc65\ud835\udc56 , \ud835\udc66\ud835\udc56) | \ud835\udc56 = 1, . . . , \ud835\udc51} \u2282 \ud835\udc46 \u00d7\u211d The family of functions in H satisfying the data is described by\n\ud835\udc3e = { \ud835\udc53 \u2208 H \u2329\ud835\udf05\ud835\udc65\ud835\udc56 , \ud835\udc53 \u232a = \ud835\udc66\ud835\udc56 for \ud835\udc56 = 1, . . . , \ud835\udc51}. We set the search space \u03a3\ud835\udc41 for the best approximant to be the subspace (of H ) spanned\nby the finite family of functions (chosen by the designer) \u03a3\ud835\udc41 B span{\ud835\udf11\ud835\udc56 \u2208 H | \ud835\udc56 = 1, . . . , \ud835\udc41},\nand let the reduced model class be restricted to a subset of the finite dimensional subspace \ud835\udc40\ud835\udc5a B span{\ud835\udf13 \ud835\udc57 \u2208 \ud835\udc40 | \ud835\udc57 = 1, . . . , \ud835\udc5a},\nNote that since \ud835\udc40 is compact in H , \ud835\udc3e \u2229\ud835\udc40\ud835\udc5a is compact. As a consequence, the coefficients \ud835\udefd \u2208 \u211d\ud835\udc5a of any function in \ud835\udc3e \u2229\ud835\udc40\ud835\udc5a are restricted to a compact subset \ud835\udc36 \u2282 \u211d\ud835\udc5a of the affine space corresponding to the data satisfaction:\n\ud835\udc5a\u2211\ufe01 \ud835\udc57=1 \ud835\udefd \ud835\udc57 \u2329 \ud835\udf05\ud835\udc65\ud835\udc56 , \ud835\udf13 \ud835\udc57 \u232a = \ud835\udc66\ud835\udc56 for \ud835\udc56 = 1, . . . , \ud835\udc51.\nEquivalently, by defining\n\ud835\udc66 B \u00a9\u00ab \ud835\udc661 ... \ud835\udc66\ud835\udc51 \u00aa\u00ae\u00ae\u00ac and \ud835\udf05 [\ud835\udc37] B \u00a9\u00ab \u2329 \ud835\udf05\ud835\udc651 , \ud835\udf131 \u232a \u2329 \ud835\udf05\ud835\udc651 , \ud835\udf132 \u232a . . . \u2329 \ud835\udf05\ud835\udc651 , \ud835\udf13\ud835\udc5a \u232a ... ... . . . ...\u2329 \ud835\udf05\ud835\udc65\ud835\udc51 , \ud835\udf132 \u232a \u2329 \ud835\udf05\ud835\udc65\ud835\udc51 , \ud835\udf132 \u232a . . . \u2329 \ud835\udf05\ud835\udc65\ud835\udc51 , \ud835\udf13\ud835\udc5a \u232a\u00aa\u00ae\u00ae\u00ac ,\nthe constraint on the coefficients \ud835\udefd can be written concisely as \ud835\udefd \u2208 { \ud835\udc64 \u2208 \ud835\udc36 \ud835\udf05 [\ud835\udc37]\ud835\udc64 = \ud835\udc66} Thus, in this setting, (6) takes the following form:\n(12) minimize \ud835\udefc\u2208\u211d\ud835\udc41\nmax \ud835\udefd\u2208\ud835\udc36\n\ud835\udf05 [\ud835\udc37 ]\ud835\udefd=\ud835\udc66\n\ud835\udc41\u2211\ufe01\ud835\udc56=1 \ud835\udefc\ud835\udc56\ud835\udf11\ud835\udc56 \u2212 \ud835\udc5a\u2211\ufe01 \ud835\udc57=1 \ud835\udefd \ud835\udc57\ud835\udf13 \ud835\udc57\nBoth the problems (11) and (12) are ready to be recast in the language of convex\nsemi-infinite programs, which we treat next at the general level of (6).\nReformulation of (6) as a convex SIP. We show that the min-max formulation (6) of the learning problem along with the special cases (11) and (12) belong to a broader class of min-max optimization problems which can be reformulated into convex semi-infinite programs.\nConsider the following min-max problem:\n(13) minimize \ud835\udc65\u2208X max \ud835\udc62\u2208U \ud835\udc39\u25e6 (\ud835\udc65, \ud835\udc62),\nwith the following data: (13-i) X \u2282 \u211d\ud835\udc5b is a closed and convex set with nonempty interior. (13-ii) U is a compact set. (13-iii) The objective function \ud835\udc39\u25e6 : X\u00d7U \u2192 \u211d is quasi-convex in the minimizing variable,\nthat is \ud835\udc39\u25e6 (\u00b7, \ud835\udc62) is quasi-convex for each \ud835\udc62 \u2208 U. The finitary version (6) of the Chebyshev center problem fits the description of (13) where (6-i) \u03a3\ud835\udc41 \u2261 \u211d\ud835\udc41 , which is clearly convex and closed, plays the role of X. (6-ii) The compact set \ud835\udc3e \u2229 \ud835\udc40\ud835\udc5a represents U. (6-iii) The objective/cost in (6) is \u2225 \ud835\udc53 \u2212 \ud835\udc54\u2225 with minimization on \ud835\udc53 \u2208 \u03a3\ud835\udc41 and maximization\non \ud835\udc54 \u2208 \ud835\udc3e \u2229 \ud835\udc40\ud835\udc5a. It is clear to see that for a fixed \ud835\udc54 \u2208 \ud835\udc3e \u2229 \ud835\udc40\ud835\udc5a, the sublevel sets of \u2225\u00b7 \u2212 \ud835\udc54\u2225 are norm balls in \u03a3\ud835\udc41 and hence the objective is quasi-convex and continuous in the minimizing variable.\nThe optimization problem (13) can be recast as the convex semi-infinite program:\n(14)\nminimize \ud835\udc65,\ud835\udc61 \ud835\udc61\nsubject to { \ud835\udc39\u25e6 (\ud835\udc65, \ud835\udc62) \u2a7d \ud835\udc61 for all \ud835\udc62 \u2208 U, (\ud835\udc61, \ud835\udc65) \u2208 \u211d \u00d7 X.\nObserve that under the hypotheses (13-i) \u2013 (13-iii), the convex SIP (14) satisfies the hypotheses (17-i) \u2013 (17-v) in \u00a73. Indeed, \u2022 the objective in (14) is the linear (and hence convex and continuous) map:\n\u211d \u00d7 X \u220b (\ud835\udc61, \ud835\udc65) \u21a6\u2192 \ud835\udc61 \u2208 \u211d; \u2022 the constraint function\n\u211d \u00d7 X \u00d7U \u220b (\ud835\udc61, \ud835\udc65, \ud835\udc62) \u21a6\u2192 \ud835\udc39\u25e6 (\ud835\udc65, \ud835\udc62) \u2212 \ud835\udc61 \u2208 \u211d is upper semi-continuous, in addition to\n\u211d \u00d7 X \u220b (\ud835\udc61, \ud835\udc65) \u21a6\u2192 \ud835\udc39\u25e6 (\ud835\udc65, \ud835\udc62) \u2212 \ud835\udc61 \u2208 \u211d being quasi-convex for each fixed \ud835\udc62 \u2208 U;\n\u2022 an interior point of X coupled with a large enough \ud835\udc61 is also the interior point of the feasible set { (\ud835\udc61, \ud835\udc65) \u2208 \u211d \u00d7 X \ud835\udc39\u25e6 (\ud835\udc65, \ud835\udc62) \u2212 \ud835\udc61 \u2a7d 0 for all \ud835\udc62 \u2208 U}.\nThe MSA algorithm, discussed in \u00a73 can be employed to recover the optimal value of the min-max problem (13). Under additional hypotheses such as \ud835\udc39\u25e6 being bounded below or X being compact, Proposition 3.3 can be used to extract a minimizer.\nThe following proposition guarantees the extraction of minimizer of (13) via the MSA algorithm for a special case by showing uniqueness of the minimizer; see Remark 3.2. Although the result is well-known, we provide a brief proof for completeness.\nProposition 2.1. If the hypothesis (13-iii) is strengthened to strict quasi-convexity of \ud835\udc39\u25e6 in the minimizing variable, there exists a unique minimizer for the min-max problem (13).\nProof. Consider the equivalent SIP formulation (14) of the optimization problem (13). Let \ud835\udc61\u25e6 be the optimal value of (13). Suppose that (\ud835\udc61\u25e6, \ud835\udc65\u22171) and (\ud835\udc61\n\u25e6, \ud835\udc65\u22172) are two minimizers of (14). Let \ud835\udc65 be the midpoint of \ud835\udc65\u22171 and \ud835\udc65 \u2217 2. We have \ud835\udc65 \u2208 X due to convexity of X.\nFor a fixed \ud835\udc62 \u2208 U, we have\n\ud835\udc39\u25e6 (\ud835\udc65\u22171, \ud835\udc62) \u2a7d \ud835\udc61 \u25e6 and \ud835\udc39\u25e6 (\ud835\udc65\u22172, \ud835\udc62) \u2a7d \ud835\udc61 \u25e6.\nBy strict quasi-convexity of \ud835\udc39\u25e6 in the first argument, \ud835\udc39\u25e6 (\ud835\udc65, \ud835\udc62) < max { \ud835\udc39\u25e6 (\ud835\udc65\u22171, \ud835\udc62), \ud835\udc39\u25e6 (\ud835\udc65 \u2217 2, \ud835\udc62) } < \ud835\udc61\u25e6.\nSince \ud835\udc62 \u2208 U is arbitrary and U is compact,\nmax \ud835\udc62\u2208U\n\ud835\udc39\u25e6 (\ud835\udc65, \ud835\udc62) < \ud835\udc61\u25e6.\nThis contradicts the optimality of \ud835\udc61\u25e6 and this completes our proof. \u25a1\nChebyshev centers. In the context of the optimal learning problem introduced in \u00a72, a Chebyshev center of \ud835\udc3e can be equivalently defined as an optimizer of the min-max problem\n(15) minimize \ud835\udc65\u2208Z sup \ud835\udc66\u2208\ud835\udc3e \u2225\ud835\udc65 \u2212 \ud835\udc66\u2225;\nindeed, observe that the Chebyshev radius \ud835\udc5f\ud835\udc3e is the optimal value of (15). Moreover, Chebyshev centers of the compact set \ud835\udc3e coincide with those of its convex hull conv\ud835\udc3e . The optimization problem (15) can be reformulated into the following convex semi-infinite program:\n(16)\nminimize \ud835\udc61 ,\ud835\udc65 \ud835\udc61\nsubject to { \u2225\ud835\udc65 \u2212 \ud835\udc66\u2225 \u2a7d \ud835\udc61 for all \ud835\udc66 \u2208 \ud835\udc3e, (\ud835\udc61, \ud835\udc65) \u2208 [0, +\u221e[ \u00d7 Z,\nin the sense that the value of (16) is the Chebyshev radius of \ud835\udc3e and an optimizer in \ud835\udc65 of (16) is a Chebyshev center of \ud835\udc3e .\nIf the norm \u2225\u00b7\u2225 on the space Z is strictly convex, then the objective function of (15) is strictly quasi-convex in the minimizing variable \ud835\udc65, and consequently, in the light of Proposition 2.1, there exists a unique Chebyshev center of \ud835\udc3e . Otherwise, Chebyshev centers of \ud835\udc3e may be extracted by means of the regularization procedure of Proposition 3.3. To wit, the MSA algorithm and its extension in Proposition 3.3 furnishes a numerically tractable technique for the exact computation of Chebyshev centers of compact subsets of finite-dimensional normed vector spaces. We shall illustrate the technique in \u00a74 with specific numerical examples.\n\u00a73. Extraction of solutions to convex semi-infinite programs\nThis section contains a detailed treatment of a mechanism to extract solutions \u2013 both the optimal value and optimizers \u2013 of convex semi-infinite programs. The results herein are of independent interest and the Chebyshev center problem (i.e., the computation of both the Chebyshev radius and Chebyshev centers) turns out to be special cases of the mechanism.\nConsider the following convex semi-infinite program\n(17)\nminimize \ud835\udc39\u25e6 (\ud835\udc65)\nsubject to { \ud835\udc39 (\ud835\udc65, \ud835\udc62) \u2a7d 0 for all \ud835\udc62 \u2208 U, \ud835\udc65 \u2208 X,\nwith the following data: (17-i) X \u2282 \u211d\ud835\udc5b is a closed and convex set with nonempty interior. (17-ii) The feasible set C B { \ud835\udc65 \u2208 X \ud835\udc39 (\ud835\udc65, \ud835\udc62) \u2a7d 0 for all \ud835\udc62 \u2208 U} is assumed to have nonempty interior. (17-iii) The objective function X \u220b \ud835\udc65 \u21a6\u2192 \ud835\udc39\u25e6 (\ud835\udc65) \u2208 \u211d is quasi-convex and upper semicontinuous. (17-iv) The constraint function X \u00d7U \u220b (\ud835\udc65, \ud835\udc62) \u21a6\u2192 \ud835\udc39 (\ud835\udc65, \ud835\udc62) \u2208 \u211d is upper semi-continuous in both the variables and is strictly quasi-convex in \ud835\udc65 for each fixed \ud835\udc62. (17-v) The constraint index set U is a compact set.\nRemark 3.1. The set U is not required to be finite-dimensional, but for numerical tractability one is typically forced to consider finite-dimensional versions of U in practice.\nConvex SIPs arise in a plethora of applications including portfolio optimization, statistics, learning, estimation theory, and approximation theory among others. We refer the reader to the sweeping survey [BBC10] and the textbooks [BTGN09, MMYSB19] for details and applications. In addition, we also point to the recent article [DACC22] for background literature and perspective; the body of results in the current section may be viewed as a natural continuation of [DACC22].\nThe algorithm established in [DACC22] for solving convex semi-infinite programs via targeted sampling, which we shall call the MSA algorithm in the sequel,3 readily gives the optimal value of a special case of (17).4 To the best of our knowledge, till date it is the only numerically tractable algorithm that computes the precise value of convex SIPs. However, since it solves a relaxed convex program [DACC22, Equation (2.7)], the set of minimizers obtained thereby is only a priori known to be a superset of the original solutions. In the case of the objective \ud835\udc39\u25e6 being strictly convex, the solution to the relaxed program [DACC22, Equation (2.7)] coincides with that of the original problem in the sense that \u2022 the optimal values coincide, and \u2022 the optimizer to the relaxed problem also optimizes the original SIP.\nRemark 3.2. A sufficient condition for the optimizer to the relaxed program [DACC22, Equation (2.7)] to be an optimizer to the original problem is the uniqueness of optimizers for the relaxed programs. Strict convexity of the objective \ud835\udc39\u25e6 is one way to ensure that this sufficient condition is satisfied. It is important to note that the sufficient condition is the uniqueness of minimizers for the relaxed programs and not just for the original problem. The example in \u00a74.1 indicates this requirement.\n3The name is derived from the three students who contributed to the results in [DACC22]: Mishal Assif P.K., Souvik Das, and Ashwin Aravind.\n4The precise hypotheses of the special case will be explained below.\nAs an immediate illustration, consider the problem of constructing the Chebyshev ball of a convex subset \ud835\udc3e of \u211d2 defined by\n\ud835\udc3e = { (\ud835\udc65, \ud835\udc66) \u2208 \u211d2 \ud835\udc65 + 2\ud835\udc66 \u2a7d 2, \u2212\ud835\udc65 + 2\ud835\udc66 \u2a7d 2,\u2212 \ud835\udc65 + 4\ud835\udc66 \u2a7e \u22122, \ud835\udc65 \u2a7e \u22122. } Recall that the \u21131-Chebyshev ball of\ud835\udc3e is a circumscribing \u21131-ball of the smallest radius. The mathematical problem of finding a Chebyshev ball of \ud835\udc3e may be formulated as a solution to the min-max problem\nmin \ud835\udc65\u2208\u211d2 max \ud835\udc66\u2208\ud835\udc3e \u2225\ud835\udc65 \u2212 \ud835\udc66\u2225\u21131 ,\nwhose value is the Chebyshev radius of\ud835\udc3e and an optimizer (in the variable \ud835\udc65) is a Chebyshev center of \ud835\udc3e . This min-max problem permits a reformulation as a convex SIP in a standard way, and the MSA algorithm applied to that convex SIP leads to the correct Chebyshev radius of 2.5 but the \u21131-ball of radius 2.5 obtained from the algorithm may not circumscribe \ud835\udc3e , as shown in the following figure:\n-2 -1 0 1 2 3\n-4\n-3\n-2\n-1\n0\n1\n2\nK \u21131-Ball from MSA\nThis situation arises because while the MSA algorithm was designed to match the values of convex SIPs (and the MSA algorithm correctly finds the Chebyshev radius in this example), the optimizers of the two problems may be different.5\nIn subsection \u00a73.2, we establish a mechanism to extend the capability of the MSA algorithm to extract optimizers for general convex objective functions. In particular, our contributions are the following: (I) The original MSA algorithm in [DACC22] is generalized and shown to be applicable\nto the data (17-i) \u2013 (17-v). This entails the following generalizations: \u2022 The ambit of [DACC22, hypothesis ((1.1)-c)] involving convexity and continuity\nof the objective \ud835\udc39\u25e6 is broadened to include quasi-convexity and upper semicontinuity of \ud835\udc39\u25e6; \u2022 In [DACC22], the constraint map \ud835\udc39 is required to be convex in \ud835\udc65 for each fixed \ud835\udc62 \u2208 U, and jointly continuous in \ud835\udc65 and \ud835\udc62. The ambit of this hypothesis is broadened to include strict quasi-convexity of \ud835\udc39 in \ud835\udc65 for every \ud835\udc62 \u2208 U and joint upper semi-continuity in both variables.\n(II) Consider the data (17-i) \u2013 (17-v) associated with the problem (17). When the objective \ud835\udc39\u25e6 is convex and continuous, we establish an approach via regularization to extract an optimizer of (17) using the MSA algorithm itself.\n5We shall revisit this example in \u00a74.\n\u00a73.1. Generalization of the MSA algorithm (\u00e0 la (I)). We first briefly discuss the key ideas behind the MSA algorithm and point out how the same algorithm is applicable to the generalized data (17-i) \u2013 (17-v) accompanying the problem (17).\nLet \ud835\udc3a : U\ud835\udc5b \u2192 \u211d be the map defined by (18) \ud835\udc3a ( \ud835\udc621, . . . , \ud835\udc62\ud835\udc5b ) = inf \ud835\udc65\u2208X { \ud835\udc39\u25e6 (\ud835\udc65) \ud835\udc39 (\ud835\udc65, \ud835\udc62\ud835\udc56) \u2a7d 0 for \ud835\udc56 = 1, . . . , \ud835\udc5b}. Note that the evaluation of \ud835\udc3a involves a finitely constrained convex program which is a relaxed version of (17). Since the minimization in (18) is over a larger set compared to that in (17), the evaluation of the function \ud835\udc3a yields a value that is at most equal to the optimal value of (17).\nIn the proof of [DACC22, Theorem 1] the authors invoke [Bor81, Theorem 4.1] to show equivalence between (17) under the specialized data wherein the objective \ud835\udc39\u25e6 is stipulated to be a convex and continuous function, and the constraint map \ud835\udc39 is required to be convex in \ud835\udc65 and jointly continuous in \ud835\udc65 and \ud835\udc62, and the global optimization problem:\n(19) sup (\ud835\udc621 ,...,\ud835\udc62\ud835\udc5b ) \u2208U\ud835\udc5b\n\ud835\udc3a ( \ud835\udc621, . . . , \ud835\udc62\ud835\udc5b ) .\nThe equivalence claimed in [DACC22] is in the sense that the value of (19) is precisely the value of (17) under the aforementioned specialised data. The MSA algorithm solves the global optimization problem (19) to obtain the optimal value of the convex semi-infinite program.\n[Bor81, Theorem 4.1] can be invoked on the broader class of data in (17-i) \u2013 (17-v) and the proof of [DACC22, Theorem 1] follows through as is. Hence, solving (19) is sufficient to obtain the optimal value of (17) even with the data (17-i) \u2013 (17-v); consequently, the proof of [DACC22, Theorem 1] carries over verbatim to our more general context.\n\u00a7 3.2. Extracting optimizers via regularization (\u00e0 la (II)). Let ( \ud835\udc62\u25e61, . . . , \ud835\udc62 \u25e6 \ud835\udc5b ) be a\nglobal optimiser of \ud835\udc3a in U\ud835\udc5b. Then \ud835\udc3a ( \ud835\udc62\u25e61, . . . , \ud835\udc62 \u25e6 \ud835\udc5b ) is the optimal value of (17) in view of our arguments in \u00a73.1. In addition, the optimizers of (17) lie in the set of solutions to the minimization problem in (18) that comes up while evaluating \ud835\udc3a at one of its global optimizer ( \ud835\udc62\u25e61, . . . , \ud835\udc62 \u25e6 \ud835\udc5b ) . Since this minimization problem is on a relaxed constraint set compared to that in (17), the challenge at this stage is to extract those optimizers that lie in the feasible set of the original optimization problem (17).\nBefore delving into a new method of extracting optimizers (to be established below), we make a few preliminary observations on convex optimization in \u00a73.2.1 based on which our method is built.\n\u00a73.2.1. Results from convex optimization. Consider the following convex program\n(P) minimize \ud835\udc65\u2208C \ud835\udc39\u25e6 (\ud835\udc65),\nwhere C \u2282 \u211d\ud835\udc5b is a closed and convex set, \ud835\udc39\u25e6 : \u211d\ud835\udc5b \u2192 \u211d is a convex and continuous. Let \ud835\udc5d\u2217 be the optimal value of (P) and let arg minP denote the set of solutions of the convex program (P).\nConsider a variation of (P) where we perturb the objective by a strictly convex function \ud835\udf19, with \ud835\udf00 > 0:\n(P\ud835\udf00) minimize\ud835\udc65\u2208C \ud835\udc39\u25e6 (\ud835\udc65) + \ud835\udf00\ud835\udf19(\ud835\udc65)\nNote that by construction, the problem (P) and the perturbed problems (P\ud835\udf00) have the same feasible set C.\nLemma 3.1. Let \ud835\udc65\ud835\udf00 be the unique minimizer of (P\ud835\udf00). Then \ud835\udc39\u25e6 (\ud835\udc65\ud835\udf00) is monotone non-increasing as \ud835\udf00 \u2193 0 and \ud835\udf19(\ud835\udc65\ud835\udf00) is monotone non-decreasing as \ud835\udf00 \u2193 0. Moreover,\ninf \ud835\udf00>0 \ud835\udc39\u25e6 (\ud835\udc65\ud835\udf00) = \ud835\udc5d\u2217 and sup \ud835\udf00>0 \ud835\udf19(\ud835\udc65\ud835\udf00) \u2a7d inf \ud835\udc65\u2208arg min P \ud835\udf19(\ud835\udc65).\nProof. Let \ud835\udf00 > \ud835\udf00\u2032 > 0. Since \ud835\udc65\ud835\udf00 is optimal for P\ud835\udf00 and both \ud835\udc65\ud835\udf00 , \ud835\udc65\ud835\udf00\u2032 lie in the feasible set,\n\ud835\udc39\u25e6 (\ud835\udc65\ud835\udf00) + \ud835\udf00\ud835\udf19(\ud835\udc65\ud835\udf00) \u2a7d \ud835\udc39\u25e6 (\ud835\udc65\ud835\udf00\u2032 ) + \ud835\udf00\ud835\udf19(\ud835\udc65\ud835\udf00\u2032 ), which gives\n\ud835\udc39\u25e6 (\ud835\udc65\ud835\udf00) \u2212 \ud835\udc39\u25e6 (\ud835\udc65\ud835\udf00\u2032 ) \u2a7d \ud835\udf00 ( \ud835\udf19(\ud835\udc65\ud835\udf00\u2032 ) \u2212 \ud835\udf19(\ud835\udc65\ud835\udf00) ) .\nSimilarly, by optimality of \ud835\udc65\ud835\udf00\u2032 for P\ud835\udf00\u2032 we have\n\ud835\udc39\u25e6 (\ud835\udc65\ud835\udf00\u2032 ) + \ud835\udf00\ud835\udf19(\ud835\udc65\ud835\udf00\u2032 ) \u2a7d \ud835\udc39\u25e6 (\ud835\udc65\ud835\udf00\u2032 ) + \ud835\udf00\ud835\udf19(\ud835\udc65\ud835\udf00),\nleading to \ud835\udc39\u25e6 (\ud835\udc65\ud835\udf00) \u2212 \ud835\udc39\u25e6 (\ud835\udc65\ud835\udf00\u2032 ) \u2a7e \ud835\udf00\u2032 ( \ud835\udf19(\ud835\udc65\ud835\udf00\u2032 ) \u2212 \ud835\udf19(\ud835\udc65\ud835\udf00) ) .\nCombining the above two inequalities, (\ud835\udf00 \u2212 \ud835\udf00\u2032) ( \ud835\udf19(\ud835\udc65\ud835\udf00\u2032 ) \u2212 \ud835\udf19(\ud835\udc65\ud835\udf00) ) \u2a7e 0,\nwhich shows that \ud835\udf19(\ud835\udc65\ud835\udf00\u2032 ) \u2a7e \ud835\udf19(\ud835\udc65\ud835\udf00).\nConsequently, \ud835\udc39\u25e6 (\ud835\udc65\ud835\udf00\u2032 ) \u2a7d \ud835\udc39\u25e6 (\ud835\udc65\ud835\udf00).\nThus the family {\ud835\udc39\u25e6 (\ud835\udc65\ud835\udf00)}\ud835\udf00>0 decreases with \ud835\udf00.\nClearly, from the definition of \ud835\udc5d\u2217 we have \ud835\udc39\u25e6 (\ud835\udc65\ud835\udf00) \u2a7e \ud835\udc5d\u2217. For \ud835\udc65 \u2208 arg minP,\n\ud835\udc39\u25e6 (\ud835\udc65\ud835\udf00) + \ud835\udf00\ud835\udf19(\ud835\udc65\ud835\udf00) \u2a7d \ud835\udc39\u25e6 (\ud835\udc65) + \ud835\udf00\ud835\udf19(\ud835\udc65)\nwhich yields 0 \u2a7d \ud835\udc39\u25e6 (\ud835\udc65\ud835\udf00) \u2212 \ud835\udc5d\u2217 \u2a7d \ud835\udf00 ( \ud835\udf19(\ud835\udc65) \u2212 \ud835\udf19(\ud835\udc65\ud835\udf00) ) and hence\nsup \ud835\udf00>0 \ud835\udf19(\ud835\udc65\ud835\udf00) \u2a7d inf \ud835\udc65\u2208arg min P \ud835\udf19(\ud835\udc65).\nThis establishes the second assertion. Moreover,\n\ud835\udc39\u25e6 (\ud835\udc65\ud835\udf00) \u2a7d \ud835\udc5d\u2217 + \ud835\udf00(\ud835\udf19(\ud835\udc65) \u2212 \ud835\udf19(\ud835\udc65\ud835\udf00))\nSince \ud835\udf19 is bounded on C, taking infimum over \ud835\udf00 > 0 on both sides yields\ninf \ud835\udf00>0\n\ud835\udc39\u25e6 (\ud835\udc65\ud835\udf00) \u2a7d \ud835\udc5d\u2217\nThus inf \ud835\udf00>0 \ud835\udc39\u25e6 (\ud835\udc65\ud835\udf00) = \ud835\udc5d\u2217, thereby establishing the first assertion, and this completes our proof. \u25a1\nProposition 3.2. The family of solutions {\ud835\udc65\ud835\udf00}\ud835\udf00>0 has a unique cluster point \ud835\udc65\u2217. Moreover, \ud835\udc65\u2217 \u2208 arg minP,\n\ud835\udc65\u2217 solves minimize \ud835\udc65\u2208arg min P \ud835\udf19(\ud835\udc65),\nand lim \ud835\udf00\u21930 \ud835\udc65\ud835\udf00 = \ud835\udc65 \u2217.\nProof. Let (\ud835\udc65\ud835\udf00\ud835\udc5b )\ud835\udc5b\u2208\u2115\u2217 , with \ud835\udf00\ud835\udc5b \u2193 0, be a subsequence in {\ud835\udc65\ud835\udf00}\ud835\udf00>0 converging to \ud835\udc65 \u2208 C. It follows from continuity of \ud835\udc39\u25e6 and Lemma 3.1 that\n\ud835\udc39\u25e6 (\ud835\udc65) = lim \ud835\udc5b\u2192+\u221e \ud835\udc39\u25e6 (\ud835\udc65\ud835\udf00\ud835\udc5b ) = inf \ud835\udc5b\u2192+\u221e \ud835\udc39\u25e6 (\ud835\udc65\ud835\udf00\ud835\udc5b ) = \ud835\udc5d\u2217,\nsince (\ud835\udc39\u25e6 (\ud835\udc65\ud835\udf00\ud835\udc5b ))\ud835\udc5b\u2208\u2115\u2217 is a monotone sequence. This indicates that \ud835\udc65 \u2208 arg minP.\nOn the one hand, by continuity of \ud835\udf19 and from Lemma 3.1, \ud835\udf19(\ud835\udc65) = lim\n\ud835\udc5b\u2192+\u221e \ud835\udf19(\ud835\udc65\ud835\udf00\ud835\udc5b ) = sup \ud835\udc5b\u2192+\u221e \ud835\udf19(\ud835\udc65\ud835\udf00\ud835\udc5b ) \u2a7d inf \ud835\udc65\u2208arg min P \ud835\udf19(\ud835\udc65),\nwhile on the other hand, since \ud835\udc65 \u2208 arg minP, \ud835\udf19(\ud835\udc65) \u2a7e inf\n\ud835\udc65\u2208arg min P \ud835\udf19(\ud835\udc65).\nThus \ud835\udc65 minimizes \ud835\udf19 on arg minP. This indicates that the cluster points of {\ud835\udc65\ud835\udf00}\ud835\udf00>0 solve the minimization problem\nminimize \ud835\udc65\u2208arg min P \ud835\udf19(\ud835\udc65).\nSince there exists a unique minimizer \ud835\udc65\u2217 by virtue of strict convexity of \ud835\udf19 and convexity of arg minP, the cluster point is unique, say \ud835\udc65\u2217.\nSince the sublevel sets {\ud835\udc65 \u2208 X | \ud835\udc39\u25e6 (\ud835\udc65) \u2a7d \ud835\udefc} for \ud835\udefc \u2208 \u211d are bounded sets by assumption, the family {\ud835\udc65\ud835\udf00}\ud835\udf00>0 is bounded. Since every subsequential limit of the family {\ud835\udc65\ud835\udf00}\ud835\udf00>0 is \ud835\udc65\u2217, the family {\ud835\udc65\ud835\udf00}\ud835\udf00>0 itself converges to \ud835\udc65\u2217. \u25a1\n\u00a73.2.2. Extraction of optimizers. Let us consider the following more general version of (17):\n(20)\nminimize \ud835\udc39\u25e6 (\ud835\udc65) + \ud835\udf00\ud835\udf19(\ud835\udc65)\nsubject to { \ud835\udc39 (\ud835\udc65, \ud835\udc62) \u2a7d 0 for \ud835\udc62 \u2208 U, \ud835\udc65 \u2208 X,\nwhere in addition to the data (17-i) \u2013 (17-v), we impose (17-vi) the map \ud835\udf19 : X \u2192 \u211d is chosen to be a positive, strictly convex function and \ud835\udf00 > 0.\nProposition 3.3. Suppose the objective function \ud835\udc39\u25e6 is convex, continuous and has bounded sublevel sets. Then the MSA algorithm extracts a minimizing sequence by solving (20) for \ud835\udf00 \u2193 0 and the sequence converges to a solution to (17).\nProposition 3.2 enables us to construct a minimizing sequence comprised of solutions of (20), with \ud835\udf00 \u2193 0, that converges to an optimizer of (17).\nProof. Since the objective function \ud835\udc39\u25e6 is continuous and has bounded sublevel sets, the constraint set X in (17) can be replaced with the compact set X\u0303 B { \ud835\udc65 \u2208 X\n\ud835\udc39\u25e6 (\ud835\udc65) \u2a7d \ud835\udc39\u25e6 (\ud835\udc65 + 1) } . This guarantees that the optimal value of (17) is finite and is attained. By the choice of a positive \ud835\udf19, the same argument holds true for (20).\nFor \ud835\udf00 > 0, the function \ud835\udc39\u25e6 + \ud835\udf00\ud835\udf19 is, by construction, strictly convex and hence the convex SIP (20) has a unique solution. In view of [DACC22, Proposition 2], we know that the optimizer of (20) is obtained by the MSA algorithm, that is by solving the minimization problem in (18) corresponding to the objective in (20).\nProposition 3.2 ensures that for any sequence \ud835\udf00 \u2193 0, the family of solutions \ud835\udc65\ud835\udf00 , obtained by solving (20) using MSA algorithm, is a minimizing sequence of the objective function \ud835\udc39\u25e6 that converges to an optimizer of (17). \u25a1\nRemark 3.3. Proposition 3.3 is applicable even in the case when the objective \ud835\udc39\u25e6 is linear if the domain of interest X is restricted to be compact.\n\u00a74. Numerical experiments\nThis section is devoted to the illustration of the extended MSA algorithm based on Proposition 3.3. Standard optimization routines from the SciPy library and standard solvers from Mathematica 12.1 have been employed in solving the examples provided in this section. We begin with a simple example of linear optimization on a solid disk; this problem can be readily solved using quadratic solvers, but for illustration purposes we reformulate it as a convex SIP.\n\u00a74.1. Optimization on a solid disk. Consider the following optimization problem:\n(21)\nminimize \ud835\udc65,\ud835\udc66 \ud835\udc66\nsubject to  \ud835\udc652 + \ud835\udc662 \u2a7d 9, \u22124 \u2a7d \ud835\udc65 \u2a7d 4, \u22124 \u2a7d \ud835\udc66 \u2a7d 4.\nIt is easy to check that the optimal value of (21) is \u22123 and is attained at ( \ud835\udc65\u25e6, \ud835\udc66\u25e6 ) = ( 0,\u22123 ) . The above problem can be recast into a convex SIP with linear constraints:\n(22)\nminimize \ud835\udc65,\ud835\udc66 \ud835\udc66\nsubject to  \ud835\udc65 cos(\ud835\udf03) + \ud835\udc66 sin(\ud835\udf03) \u2a7d 3 for all \ud835\udf03 \u2208 [0, 2\ud835\udf0b], \u22124 \u2a7d \ud835\udc65 \u2a7d 4, \u22124 \u2a7d \ud835\udc66 \u2a7d 4.\nWhen employed directly, the MSA algorithm selects points on the line \ud835\udc66 = \u22123, which is indicated by the green line in Figure 3.\nFigure 3a shows a sequence of solutions corresponding to decreasing values of \ud835\udf00, of (22) with the perturbation function\n\u211d2 \u220b (\ud835\udc65, \ud835\udc66) \u21a6\u2192 \ud835\udf19a (\ud835\udc65, \ud835\udc66) B 12 ( \ud835\udc652 + \ud835\udc662 ) ,\nand Figure 3b shows the corresponding sequence of solutions with the perturbation function \u211d2 \u220b (\ud835\udc65, \ud835\udc66) \u21a6\u2192 \ud835\udf19b (\ud835\udc65, \ud835\udc66) B 12 ( (\ud835\udc65 \u2212 1)2 + \ud835\udc662 ) .\nNotice that the sequence of solutions obtained is dependent on the choice of perturbation \ud835\udf19 and as described in Proposition 3.2, the sequences may converge to different optimizers of the original SIP. However in this example, since the SIP (22) has a unique solution at ( 0,\u22123 ) , the sequences converge to it regardless of the choice of the perturbation.\nRemark 4.1. Note that although the original problem (22) has a unique optimizer, the relaxed problems, being linear programs, may not exhibit uniqueness of minimizers. This necessitates the use of regularization to extract the optimizer of (22)\nThe finitely constrained inner optimization problem was solved using SLSQP method in the SciPy library by providing the initial guess (1, 0). The global optimization was solved using dual_annealing method in the SciPy library coupled with other default parameters of the routine.\n\u00a74.2. Chebyshev centers under the \u21131 norm. Let \u211d2 be equipped with the \u21131-norm\n\u211d2 \u220b (\ud835\udc65, \ud835\udc66) \u21a6\u2192 \u2225(\ud835\udc65, \ud835\udc66)\u2225\u21131 B |\ud835\udc65 | + |\ud835\udc66 | \u2208 \u211d.\nConsider the set \ud835\udc3e \u2282 \u211d2 defined by\n\ud835\udc3e = { (\ud835\udc65, \ud835\udc66) \u2208 \u211d2 \ud835\udc65 + 2\ud835\udc66 \u2a7d 2, \u2212\ud835\udc65 + 2\ud835\udc66 \u2a7d 2,\u2212 \ud835\udc65 + 4\ud835\udc66 \u2a7e \u22122, \ud835\udc65 \u2a7e \u22122. }. (This set \ud835\udc3e was introduced in \u00a73.) We are interested in finding a Chebyshev center of \ud835\udc3e in (\u211d2, \u2225\u00b7\u2225\u21131 ). Since \u21131-norm is not strictly convex, the Chebyshev center of a set cannot be obtained directly from the MSA algorithm but the approach via regularization can be employed, and to this end we pick the perturbation function\n\u211d3 \u220b (\ud835\udc61, \ud835\udc65, \ud835\udc66) \u21a6\u2192 \ud835\udf19(\ud835\udc61, \ud835\udc65, \ud835\udc66) = 12 ( (\ud835\udc61 \u2212 2)2 + (\ud835\udc65 \u2212 2)2 + (\ud835\udc66 \u2212 2)2 ) .\nFigure 4 shows the \u21131-balls obtained as solutions to the perturbed optimization problem for various \ud835\udf00. Observe that the radii of the candidate Chebyshev balls approach the radius 2.5 of the ball corresponding to \ud835\udf00 = 0, which is the Chebyshev radius of \ud835\udc3e . But note that the ball corresponding to \ud835\udf00 = 0 does not encompass all the points in \ud835\udc3e . The sequence of balls obtained via regularization satisfy all the constraints and their centers converge to the point (\u22120.25,\u22120.25). Thus (\u22120.25,\u22120.25) is a Chebyshev center of the set \ud835\udc3e .\nIt is important to note that solution picked by the MSA algorithm (without perturbation) may very well be a Chebyshev center of \ud835\udc3e but this cannot be guaranteed in general. The approach of regularization guarantees that the sequence of solutions lies in the feasible set and hence also the limit.\nRemark 4.2. The global optimization routine plays a crucial role in this algorithm since its convergence to a global optimizer is imperative to establish the equivalence between the resulting finite convex minimization problem and the original SIP, and for the extraction of both the optimal value and an optimizer of the SIP.\nThe inner optimization was solved using SLSQP algorithm of SciPy library and the global optimization problem was solved using the differential_evolution method [SP97] of the SciPy library with the \u2018randtobest1exp\u2019 option for the strategy parameter.\n\u00a7 4.3. A case of non-convex \ud835\udc3e . Here is a relatively simple example of solving the Chebyshev center problem for the non-convex region\n\ud835\udc3e B { (\ud835\udc651, \ud835\udc652) \u2208 [0, 1]2 \ud835\udc6521 + \ud835\udc6522 \u2a7e 13 and (\ud835\udc651 \u2212 1)2 + \ud835\udc6522 \u2a7e 23 } relative to the standard Euclidean norm on \u211d2. The numerical calculations corresponding to the MSA algorithm were carried out in Mathematica 12.1 using its native NelderMead technique in the global optimization routine NMaximize, and led to the Chebyshev radius \ud835\udc5f\ud835\udc3e = 0.633431 and the Chebyshev center (0.500000, 0.611112).6 A pictorial representation of the underlying set \ud835\udc3e (shaded in blue) and the Chebyshev ball (shaded in light brown) is shown below:\n-0.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\nK\nChebyshev \u21132-Ball\n\u00a74.4. Examples on function spaces. Consider an example of data fitting on the space L2 ( [0, 1],\u211d). Let the hypothesis class \ud835\udc40 be the space of polynomials with bounded coefficients and the reduced model class be the polynomials with degree below \ud835\udc41 . The set of polynomials attaining prescribed values at specific datapoints is a restriction of the model class onto an affine space (in the coefficients).\n\u00a7 4.4.1. Simple 1D example. We begin with the case of the affine space being 1- dimensional. In this scenario, a sample point can be represented by a single parameter, say \ud835\udefc, and the induced norm on the affine space becomes equivalent to |\u00b7| in the \ud835\udefc- space. As a consequence, although the Chebyshev radius of \ud835\udc3e varies, the Chebyshev center of \ud835\udc3e becomes independent of the norm chosen on the norm chosen on the original space.\n6For this example we report numerical results correct up to 6 decimal places.\nWe consider an example in \u211d2. Let the affine space described by the data set be the line \ud835\udc66 = 2\ud835\udc65 + 3. Suppose that \ud835\udc3e is the intersection of the line with the rectangular region described by |\ud835\udc65 | \u2a7d 5, |\ud835\udc66 | \u2a7d 10. Figure 5 shows the Chebyshev circles of \ud835\udc3e under different norms (induced by different inner products) on \u211d2.\nFigure 6 shows the Chebyshev balls of \ud835\udc3e which is now considered to be the portion of the line \ud835\udc66 = 2\ud835\udc65 + 3 inside the elliptical region \ud835\udc652 + 3\ud835\udc662 \u2a7d 100.\n\u00a74.4.2. Example of affine space in 3D. Consider an example of a 2 dimensional affine space.\nConcretely, let the affine space be the plane with the normal ( 1 0.7 0.49 ) passing\nthrough the point ( 1, 1, 1 ) . Let \ud835\udc3e be the region in the plane inside a cube of radius 15 centered at origin. Parametrising the affine plane using an orthonormal basis and centering at ( 1, 1, 1 ) , the region \ud835\udc3e is depicted as the purple shaded region in Figure 7.\nThe orange shaded region in Figure 7 denotes the Chebyshev ball obtained when the space is equipped with the Euclidean norm. Observe that, in this case, the Chebyshev center depends on the norm used on the ambient space.\n\u00a7 4.4.3. Higher dimensional space. We now consider an example in the space of polynomials. Let the search space \u03a3\ud835\udc41 \u2282 L2 ( [0, 1],\u211d) also be the polynomials with degree below 20. The data \ud835\udc37 is obtained by sampling the function\n[0, 1] \u220b \ud835\udc65 \u21a6\u2192 \ud835\udc53 (\ud835\udc65) B 10 sin(2\ud835\udf0b\ud835\udc65) \u2208 \u211d\nat \ud835\udc51 points in the interval [0, 1].\nUsing the basis {1, \ud835\udc65, \ud835\udc652, . . . , \ud835\udc65\ud835\udc41\u22121}, the set of functions in \ud835\udc40 satisfying the data \ud835\udc37 can be seen to be the intersection of an affine space with \ud835\udc40:\n\ud835\udc3e = { \ud835\udefd \u2208 \u211d\ud835\udc41 \ud835\udc41\u22121\u2211\ufe01 \ud835\udc57=0 \ud835\udefd \ud835\udc57\ud835\udc65 \ud835\udc57 \ud835\udc58 = \ud835\udc53 (\ud835\udc65\ud835\udc58) for k = 1, . . . , \ud835\udc51 } \u2229 { \ud835\udefd \u2208 \u211d\ud835\udc41 \ud835\udefd \ud835\udc57 \u2a7d 100} Using the same basis for the search space \u03a3\ud835\udc41 , the relative Chebyshev center of the set\n\ud835\udc3e in \u03a3\ud835\udc41 can be phrased as the optimizaiton problem:\narg min \ud835\udefc\u2208\u211d21 max \ud835\udefd\u2208\ud835\udc3e \u2225\ud835\udefc \u2212 \ud835\udefd\u2225\ud835\udc40 ,\nwhere \u2225\u00b7\u2225\ud835\udc40 is the induced norm on \u03a3\ud835\udc41 identified as \u211d\ud835\udc41 .\nThe effective dimension of the discrete model class is the dimension of the affine space, \ud835\udc41 \u2212 \ud835\udc51. Since, in normed spaces, the Chebyshev center of an affine space lies in the same affine space, the Chebyshev center of \ud835\udc3e also satisfies the prescribed data.\nThe figures 8 \u2013 11 showcase the Chebyshev centers of \ud835\udc3e for a series of sampling size \ud835\udc51 and reduced search dimension \ud835\udc41 .\nFigure 12 presents a variation of the case showed in Figure 9 wherein the set \ud835\udc3e is modified to be the intersection of the affine space satisfying the prescribed data with a shifted bounding box on the coefficients:\n\ud835\udc3e \u2032 = { \ud835\udefd \u2208 \u211d\ud835\udc41 \ud835\udc41\u22121\u2211\ufe01 \ud835\udc57=0 \ud835\udefd \ud835\udc57\ud835\udc65 \ud835\udc57 \ud835\udc58 = \ud835\udc53 (\ud835\udc65\ud835\udc58) for k = 1, . . . , \ud835\udc51 } \u2229 { \ud835\udefd \u2208 \u211d\ud835\udc41 100 \u2a7d \ud835\udefd \ud835\udc57 \u2a7d 200}. It is observed that the change in the Chebyshev center due to this modification to the set \ud835\udc3e is a translation of the coefficients by an amount related to the change in the bounds.\n\u00a74.5. Critical importance of the global optimization. The MSA algorithm extracts the Chebyshev center by relying on a global optimization process. The convergence of the global optimization process to an actual global optimum is crucial to finding the actual Chebyshev center of a set. Consequently, the global optimization algorithm and its sampling process should ideally be fine-tuned depending on the application at hand.\nLet us illustrate the gaps in the capabilities of off-the-shelf solvers in the context of some of preceding examples: \u25e6 The Chebyshev triplet obtained for the non-convex set \ud835\udc3e in \u00a74.3 in Mathematica\n12.1 using its native SimulatedAnnealing technique in the global optimization routine NMaximize led to the Chebyshev radius \ud835\udc5f\ud835\udc3e = 0.600925 and the Chebyshev center (0.499999, 0.666666) (correct up to 6 decimal places). A pictorial representation of the underlying set \ud835\udc3e (shaded in blue) and this particular Chebyshev ball (shaded in light\nThis difference between the two outputs is perhaps due to the difficulty faced by the native SimulatedAnnealing routine to sample from the pinched (which is nearly singular relative to the Lebesgue measure) region of \ud835\udc3e close to ( 13 , 0), in contrast to the simplexbased deterministic NelderMead routine which led to the correct Chebyshev triplet in \u00a74.3. \u25e6 Consider the problem of finding the Chebyshev ball for the equilateral triangle \ud835\udc3e = {(\ud835\udc65, \ud835\udc66) \u2208 \u211d2 | \u221a 3\ud835\udc65 + \ud835\udc66 \u2a7d \u221a 3,\u2212 \u221a 3\ud835\udc65 + \ud835\udc66 \u2a7d \u221a 3, \ud835\udc66 \u2a7e 0} under the weighted norm\n\u2225\ud835\udc63\u2225\ud835\udc40 B \u221a\ufe01 \u27e8\ud835\udc63, \ud835\udc40\ud835\udc63\u27e9 with weighting matrix\n\ud835\udc40 = ( 4.01933 \u22122.038 \u22122.038 14.6273 ) .\nThe NelderMead routine in Mathematica 12.1 converges to a suboptimal Chebyshev radius of 3.706789; the corresponding Chebyshev ball is shown as the green shaded region in the following figure. The correct Chebyshev radius obtained by including the vertices of the triangle in the constraints is 3.709497 and is shaded in orange in the figure below.\n-1 0 1 2\n0.0\n0.5\n1.0\n1.5\n2.0\nK\nOutput of NelderMead\nActual Chebyshev ball\nThese two illustrative examples highlight the need, in general, for careful context-dependent tuning of the global optimization algorithm in the context of the MSA algorithm.\nReferences [AT21] A. R. Alimov and I. G. Tsar\u2019kov, Geometric Approximation Theory, Springer Monographs in Mathematics, Springer, Cham, 2021. [BBC10] D. Bertsimas, D. Brown, and C. Caramanis, Theory and applications of robust optimization, SIAM Review 53 (2010), 464\u2013501. [BBDP22] P. Binev, A. Bonito, R. DeVore, and G. Petrova, Optimal learning, arXiv preprint: https://arxiv. org/abs/2203.15994, 2022. [Bor81] J. M. Borwein, Direct theorems in semi-infinite convex programming, Mathematical Programming 21 (1981), no. 3, 301\u2013318. [BTGN09] A. Ben-Tal, L. El Ghaoui, and A. S. Nemirovski, Robust Optimization, Princeton Series in Applied Mathematics, Princeton University Press, 2009. [CS02] F. Cucker and S. Smale, On the mathematical foundations of learning, American Mathematical Society. Bulletin. New Series 39 (2002), no. 1, 1\u201349. [DACC22] S. Das, A. Aravind, A. Cherukuri, and D. Chatterjee, Near-optimal solutions of convex semi-infinite\nprograms by targeted sampling, Annals of Operations Research; doi: https://doi.org/10.1007/ s10479-022-04810-4, 2022.\n[DPW17] R. DeVore, G. Petrova, and P. Wojtaszczyk, Data assimilation and sampling in Banach spaces, Calcolo. A Quarterly on Numerical Analysis and Theory of Computation 54 (2017), no. 3, 963\u2013 1007. [MMYSB19] R. A. Maronna, R. D. Martin, V. J. Yohai, and M. Salibi\u00e1n-Barrera, Robust Statistics, Wiley Series in Probability and Statistics, John Wiley & Sons, Inc., Hoboken, NJ, 2019, Theory and methods (with R), Second edition of [ MR2238141]. [MR77] C. A. Micchelli and T. J. Rivlin, A survey of optimal recovery, Optimal estimation in approximation theory, Plenum, New York, 1977, pp. 1\u201354. [PR16] V. I. Paulsen and M. Raghupathi, An Introduction to the Theory of Reproducing Kernel Hilbert Spaces, Cambridge Studies in Advanced Mathematics, vol. 152, Cambridge University Press, Cambridge, 2016. [SP97] R. Storn and K. Price, Differential evolution\u2014a simple and efficient heuristic for global optimization over continuous spaces, Journal of Global Optimization 11 (1997), no. 4, 341\u2013359."
        }
    ],
    "title": "A numerical algorithm for attaining the Chebyshev bound in optimal learning",
    "year": 2023
}