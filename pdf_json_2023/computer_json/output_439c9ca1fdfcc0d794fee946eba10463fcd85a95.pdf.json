{
    "abstractText": "In reward-free reinforcement learning (RL), an agent explores the environment first without any reward information, in order to achieve certain learning goals afterwards for any given reward. In this paper we focus on reward-free RL under low-rank MDP models, in which both the representation and linear weight vectors are unknown. Although various algorithms have been proposed for reward-free low-rank MDPs, the corresponding sample complexity is still far from being satisfactory. In this work, we first provide the first known sample complexity lower bound that holds for any algorithm under low-rank MDPs. This lower bound implies it is strictly harder to find a near-optimal policy under low-rank MDPs than under linear MDPs. We then propose a novel model-based algorithm, coined RAFFLE, and show it can both find an -optimal policy and achieve an -accurate system identification via reward-free exploration, with a sample complexity significantly improving the previous results. Such a sample complexity matches our lower bound in the dependence on , as well as on K in the large d regime, where d and K respectively denote the representation dimension and action space cardinality. Finally, we provide a planning algorithm (without further interaction with true environment) for RAFFLE to learn a near-accurate representation, which is the first known representation learning guarantee under the same setting.",
    "authors": [
        {
            "affiliations": [],
            "name": "LOW-RANK MDPS"
        },
        {
            "affiliations": [],
            "name": "Yuan Cheng"
        },
        {
            "affiliations": [],
            "name": "Ruiquan Huang"
        },
        {
            "affiliations": [],
            "name": "Yingbin Liang"
        }
    ],
    "id": "SP:ff7bbdbca906e2088d726254180761a34a0f69a6",
    "references": [
        {
            "authors": [
                "Alekh Agarwal",
                "Sham Kakade",
                "Akshay Krishnamurthy",
                "Wen Sun"
            ],
            "title": "Flambe: Structural complexity and representation learning of low rank mdps",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Alekh Agarwal",
                "Yuda Song",
                "Wen Sun",
                "Kaiwen Wang",
                "Mengdi Wang",
                "Xuezhou Zhang"
            ],
            "title": "Provable benefits of representational transfer in reinforcement learning",
            "venue": "arXiv preprint arXiv:2205.14571,",
            "year": 2022
        },
        {
            "authors": [
                "Alex Ayoub",
                "Zeyu Jia",
                "Csaba Szepesvari",
                "Mengdi Wang",
                "Lin Yang"
            ],
            "title": "Model-based reinforcement learning with value-targeted regression",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Marc Bellemare",
                "Sriram Srinivasan",
                "Georg Ostrovski",
                "Tom Schaul",
                "David Saxton",
                "Remi Munos"
            ],
            "title": "Unifying count-based exploration and intrinsic motivation",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "Yuri Burda",
                "Harrison Edwards",
                "Amos Storkey",
                "Oleg Klimov"
            ],
            "title": "Exploration by random network distillation",
            "venue": "arXiv preprint arXiv:1810.12894,",
            "year": 2018
        },
        {
            "authors": [
                "Fan Chen",
                "Yu Bai",
                "Song Mei"
            ],
            "title": "Partially observable rl with b-stability: Unified structural condition and sharp sample-efficient algorithms",
            "venue": "arXiv preprint arXiv:2209.14990,",
            "year": 2022
        },
        {
            "authors": [
                "Jinglin Chen",
                "Aditya Modi",
                "Akshay Krishnamurthy",
                "Nan Jiang",
                "Alekh Agarwal"
            ],
            "title": "On the statistical efficiency of reward-free exploration in non-linear RL",
            "venue": "CoRR, abs/2206.10770,",
            "year": 2022
        },
        {
            "authors": [
                "Yuan Cheng",
                "Songtao Feng",
                "Jing Yang",
                "Hong Zhang",
                "Yingbin Liang"
            ],
            "title": "Provable benefit of multitask representation learning in reinforcement learning",
            "year": 2022
        },
        {
            "authors": [
                "John Co-Reyes",
                "YuXuan Liu",
                "Abhishek Gupta",
                "Benjamin Eysenbach",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "Self-consistent trajectory autoencoder: Hierarchical reinforcement learning with trajectory embeddings",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "C\u00e9dric Colas",
                "Pierre Fournier",
                "Olivier Sigaud",
                "Pierre-Yves Oudeyer"
            ],
            "title": "CURIOUS: intrinsically motivated multi-task, multi-goal reinforcement learning",
            "year": 2018
        },
        {
            "authors": [
                "Christoph Dann",
                "Tor Lattimore",
                "Emma Brunskill"
            ],
            "title": "Unifying pac and regret: Uniform pac bounds for episodic reinforcement learning",
            "venue": "arXiv preprint arXiv:1703.07710,",
            "year": 2017
        },
        {
            "authors": [
                "Peter Dayan"
            ],
            "title": "Improving generalization for temporal difference learning: The successor representation",
            "venue": "Neural Comput.,",
            "year": 1993
        },
        {
            "authors": [
                "Omar Darwiche Domingues",
                "Pierre M\u00e9nard",
                "Emilie Kaufmann",
                "Michal Valko"
            ],
            "title": "Episodic reinforcement learning in finite mdps: Minimax lower bounds revisited, 2020",
            "year": 2020
        },
        {
            "authors": [
                "Simon Du",
                "Akshay Krishnamurthy",
                "Nan Jiang",
                "Alekh Agarwal",
                "Miroslav Dudik",
                "John Langford"
            ],
            "title": "Provably efficient rl with rich observations via latent state decoding",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Simon S Du",
                "Sham M Kakade",
                "Jason D Lee",
                "Shachar Lovett",
                "Gaurav Mahajan",
                "Wen Sun",
                "Ruosong Wang"
            ],
            "title": "Bilinear classes: A structural framework for provable generalization in rl",
            "venue": "arXiv preprint arXiv:2103.10897,",
            "year": 2021
        },
        {
            "authors": [
                "Simon Shaolei Du",
                "Wei Hu",
                "Sham M. Kakade",
                "Jason D. Lee",
                "Qi Lei"
            ],
            "title": "Few-shot learning via learning the representation, provably",
            "venue": "In 9th International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Benjamin Eysenbach",
                "Abhishek Gupta",
                "Julian Ibarz",
                "Sergey Levine"
            ],
            "title": "Diversity is all you need: Learning skills without a reward function",
            "venue": "arXiv preprint arXiv:1802.06070,",
            "year": 2018
        },
        {
            "authors": [
                "Elad Hazan",
                "Sham Kakade",
                "Karan Singh",
                "Abby Van Soest"
            ],
            "title": "Provably efficient maximum entropy exploration",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Ruiquan Huang",
                "Jing Yang",
                "Yingbin Liang"
            ],
            "title": "Safe exploration incurs nearly no additional sample complexity for reward-free rl",
            "venue": "arXiv preprint arXiv:2206.14057,",
            "year": 2022
        },
        {
            "authors": [
                "Nan Jiang",
                "Akshay Krishnamurthy",
                "Alekh Agarwal",
                "John Langford",
                "Robert E Schapire"
            ],
            "title": "Contextual decision processes with low bellman rank are pac-learnable",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Chi Jin",
                "Akshay Krishnamurthy",
                "Max Simchowitz",
                "Tiancheng Yu"
            ],
            "title": "Reward-free exploration for reinforcement learning",
            "venue": "In Proceedings of the 37th International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Chi Jin",
                "Zhuoran Yang",
                "Zhaoran Wang",
                "Michael I Jordan"
            ],
            "title": "Provably efficient reinforcement learning with linear function approximation",
            "venue": "In Conference on Learning Theory,",
            "year": 2020
        },
        {
            "authors": [
                "Chi Jin",
                "Qinghua Liu",
                "Sobhan"
            ],
            "title": "Miryoosefi. Bellman eluder dimension: New rich classes of rl problems, and sample-efficient algorithms, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Emilie Kaufmann",
                "Pierre M\u00e9nard",
                "Omar Darwiche Domingues",
                "Anders Jonsson",
                "Edouard Leurent",
                "Michal Valko"
            ],
            "title": "Adaptive reward-free exploration",
            "venue": "In Algorithmic Learning Theory,",
            "year": 2021
        },
        {
            "authors": [
                "Qinghua Liu",
                "Praneeth Netrapalli",
                "Csaba Szepesvari",
                "Chi Jin"
            ],
            "title": "Optimistic mle\u2013a generic model-based algorithm for partially observable sequential decision making",
            "venue": "arXiv preprint arXiv:2209.14997,",
            "year": 2022
        },
        {
            "authors": [
                "Rui Lu",
                "Gao Huang",
                "Simon S Du"
            ],
            "title": "On the power of multitask representation learning in linear mdp",
            "venue": "arXiv preprint arXiv:2106.08053,",
            "year": 2021
        },
        {
            "authors": [
                "Pierre M\u00e9nard",
                "Omar Darwiche Domingues",
                "Anders Jonsson",
                "Emilie Kaufmann",
                "Edouard Leurent",
                "Michal Valko"
            ],
            "title": "Fast active learning for pure exploration in reinforcement learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Sobhan Miryoosefi",
                "Chi Jin"
            ],
            "title": "A simple reward-free approach to constrained reinforcement learning, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Dipendra Misra",
                "Mikael Henaff",
                "Akshay Krishnamurthy",
                "John Langford"
            ],
            "title": "Kinematic state abstraction and provably efficient rich-observation reinforcement learning",
            "venue": "In International conference on machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "Aditya Modi",
                "Jinglin Chen",
                "Akshay Krishnamurthy",
                "Nan Jiang",
                "Alekh Agarwal"
            ],
            "title": "Model-free representation learning and exploration in low-rank mdps",
            "venue": "arXiv preprint arXiv:2102.07035,",
            "year": 2021
        },
        {
            "authors": [
                "Ashvin Nair",
                "Vitchyr Pong",
                "Murtaza Dalal",
                "Shikhar Bahl",
                "Steven Lin",
                "Sergey Levine"
            ],
            "title": "Visual reinforcement learning with imagined goals",
            "venue": "arXiv preprint arXiv:1807.04742,",
            "year": 2018
        },
        {
            "authors": [
                "Pierre-Yves Oudeyer",
                "Frdric Kaplan",
                "Verena V Hafner"
            ],
            "title": "Intrinsic motivation systems for autonomous mental development",
            "venue": "IEEE transactions on evolutionary computation,",
            "year": 2007
        },
        {
            "authors": [
                "Vitchyr H Pong",
                "Murtaza Dalal",
                "Steven Lin",
                "Ashvin Nair",
                "Shikhar Bahl",
                "Sergey Levine"
            ],
            "title": "Skewfit: State-covering self-supervised reinforcement learning",
            "year": 1903
        },
        {
            "authors": [
                "Tongzheng Ren",
                "Tianjun Zhang",
                "Lisa Lee",
                "Joseph E Gonzalez",
                "Dale Schuurmans",
                "Bo Dai"
            ],
            "title": "Spectral decomposition representation for reinforcement learning",
            "venue": "arXiv preprint arXiv:2208.09515,",
            "year": 2022
        },
        {
            "authors": [
                "Wen Sun",
                "Nan Jiang",
                "Akshay Krishnamurthy",
                "Alekh Agarwal",
                "John Langford"
            ],
            "title": "Model-based rl in contextual decision processes: Pac bounds and exponential improvements over model-free approaches",
            "venue": "In Conference on learning theory,",
            "year": 2019
        },
        {
            "authors": [
                "Masatoshi Uehara",
                "Xuezhou Zhang",
                "Wen Sun"
            ],
            "title": "Representation learning for online and offline rl in low-rank mdps",
            "venue": "arXiv preprint arXiv:2110.04652,",
            "year": 2021
        },
        {
            "authors": [
                "Masatoshi Uehara",
                "Ayush Sekhari",
                "Jason D Lee",
                "Nathan Kallus",
                "Wen Sun"
            ],
            "title": "Provably efficient reinforcement learning in partially observable dynamical systems",
            "venue": "arXiv preprint arXiv:2206.12020,",
            "year": 2022
        },
        {
            "authors": [
                "Masatoshi Uehara",
                "Xuezhou Zhang",
                "Wen Sun"
            ],
            "title": "Representation learning for online and offline RL in low-rank mdps",
            "venue": "In The Tenth International Conference on Learning Representations, ICLR. OpenReview.net,",
            "year": 2022
        },
        {
            "authors": [
                "Lingxiao Wang",
                "Qi Cai",
                "Zhuoran Yang",
                "Zhaoran Wang"
            ],
            "title": "Embed to control partially observed systems: Representation learning with provable sample efficiency",
            "year": 2022
        },
        {
            "authors": [
                "Ruosong Wang",
                "Simon S Du",
                "Lin F Yang",
                "Ruslan Salakhutdinov"
            ],
            "title": "On reward-free reinforcement learning with linear function approximation",
            "venue": "arXiv preprint arXiv:2006.11274,",
            "year": 2020
        },
        {
            "authors": [
                "Jingfeng Wu",
                "Vladimir Braverman",
                "Lin Yang"
            ],
            "title": "Accommodating picky customers: Regret bound and exploration complexity for multi-objective reinforcement learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Jiaqi Yang",
                "Wei Hu",
                "Jason D. Lee",
                "Simon Shaolei Du"
            ],
            "title": "Impact of representation learning in linear bandits",
            "venue": "In 9th International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Andrea Zanette",
                "Alessandro Lazaric",
                "Mykel Kochenderfer",
                "Emma Brunskill"
            ],
            "title": "Learning near optimal policies with low inherent bellman error",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Andrea Zanette",
                "Alessandro Lazaric",
                "Mykel J Kochenderfer",
                "Emma Brunskill"
            ],
            "title": "Provably efficient reward-agnostic navigation with linear value iteration",
            "venue": "arXiv preprint arXiv:2008.07737,",
            "year": 2020
        },
        {
            "authors": [
                "Andrea Zanette",
                "Ching-An Cheng",
                "Alekh Agarwal"
            ],
            "title": "Cautiously optimistic policy optimization and exploration with linear function approximation",
            "venue": "In Conference on Learning Theory,",
            "year": 2021
        },
        {
            "authors": [
                "Wenhao Zhan",
                "Masatoshi Uehara",
                "Wen Sun",
                "Jason D Lee"
            ],
            "title": "Pac reinforcement learning for predictive state representations",
            "venue": "arXiv preprint arXiv:2207.05738,",
            "year": 2022
        },
        {
            "authors": [
                "Weitong Zhang",
                "Jiafan He",
                "Dongruo Zhou",
                "Amy Zhang",
                "Quanquan Gu"
            ],
            "title": "Provably efficient representation learning in low-rank markov decision processes",
            "venue": "CoRR, abs/2106.11935,",
            "year": 2021
        },
        {
            "authors": [
                "Weitong Zhang",
                "Dongruo Zhou",
                "Quanquan Gu"
            ],
            "title": "Reward-free model-based reinforcement learning with linear function approximation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Xuezhou Zhang",
                "Yuda Song",
                "Masatoshi Uehara",
                "Mengdi Wang",
                "Wen Sun",
                "Alekh Agarwal"
            ],
            "title": "Efficient reinforcement learning in block mdps: A model-free representation learning approach",
            "venue": "arXiv preprint arXiv:2202.00063,",
            "year": 2022
        },
        {
            "authors": [
                "Zihan Zhang",
                "Simon S Du",
                "Xiangyang Ji"
            ],
            "title": "Nearly minimax optimal reward-free reinforcement learning",
            "venue": "arXiv preprint arXiv:2010.05901,",
            "year": 2020
        },
        {
            "authors": [
                "Zhuangdi Zhu",
                "Kaixiang Lin",
                "Jiayu Zhou"
            ],
            "title": "Transfer learning in deep reinforcement learning: A survey",
            "year": 2009
        },
        {
            "authors": [
                "Uehara"
            ],
            "title": "2022b), while it generalizes to non-stationary setting and arbitrary reward scenario from infinite stationary MDP and fixed reward",
            "year": 2022
        },
        {
            "authors": [
                "Zanette"
            ],
            "title": "2021) for the version of fixed \u03c6 and Lemma",
            "year": 2022
        },
        {
            "authors": [
                "Domingues"
            ],
            "title": "2020) for tabular MDPs. However, the lower bound in Domingues et al. (2020) requires S \u2265 K, where S,K denote the cardinality of state and action space respectively. Our hard MDP instances remove the assumption that S \u2265 K",
            "year": 2020
        },
        {
            "authors": [
                "Du"
            ],
            "title": "2021b) andNf can be large enough, and (ii) follows from that",
            "year": 2021
        },
        {
            "authors": [],
            "title": "MORE DISCUSSION ON RELATED WORK In this section, we first summarize the directly comparable work in Appendix E. E.1 LOW-RANK MDPS IN EXTENDED RL SETTINGS Many studies",
            "year": 2022
        },
        {
            "authors": [
                "Zhan"
            ],
            "title": "2022) studied predictive state representations model, and applied their results to POMDP with latent low-rank structure",
            "venue": "reward-known",
            "year": 2022
        },
        {
            "authors": [
                "Chen"
            ],
            "title": "There is some concurrent work also using an optimistic MLE-based approach for different settings (POMDP) (Liu et al., 2022; Chen et al., 2022a). We elaborate the key differences between our paper and Liu et al",
            "year": 2022
        },
        {
            "authors": [
                "Agarwal"
            ],
            "title": "The following lemma is a standard inequality in regret analysis for linear models in reinforcement learning",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Reward-free reinforcement learning, recently formalized by Jin et al. (2020b), arises as a powerful framework to accommodate diverse demands in sequential learning applications. Under the rewardfree RL framework, an agent first explores the environment without reward information during the exploration phase, with the objective to achieve certain learning goals later on for any given reward function during the planning phase. Such a learning goal can be to find an -optimal policy, to achieve an -accurate system identification, etc. The reward-free RL paradigm may find broad application in many real-world engineering problems. For instance, reward-free exploration can be efficient when various reward functions are taken into consideration over a single environment, such as safe RL (Miryoosefi & Jin, 2021; Huang et al., 2022), multi-objective RL (Wu et al., 2021), multi-task RL (Agarwal et al., 2022; Cheng et al., 2022), etc. Studies of reward-free RL on the theoretical side have been largely focused on characterizing the sample complexity to achieve a learning goal under various MDP models. Specifically, reward-free tabular RL has been studied in Jin et al. (2020a); Me\u0301nard et al. (2021); Kaufmann et al. (2021); Zhang et al. (2020). For reward-free RL with function approximation, Wang et al. (2020) studied linear MDPs introduced by Jin et al. (2020b), where both the transition and the reward are linear functions of a given feature extractor, Zhang et al.\n\u2217Equal contribution\nar X\niv :2\n30 3.\n10 85\n9v 1\n[ cs\n.L G\n] 2\n0 M\n(2021b) studied linear mixture MDPs introduced by Ayoub et al. (2020), and Zanette et al. (2020b) considered a classes of MDPs with low inherent Bellman error introduced by Zanette et al. (2020a).\nIn this paper, we focus on reward-free RL under low-rank MDPs, where the transition kernel admits a decomposition into two embedding functions that map to low dimensional spaces. Compared with linear MDPs, the feature functions (i.e., the representation) under low-rank MDPs are unknown, hence the design further requires representation learning and becomes more challenging. Rewardfree RL under low-rank MDPs was first studied by Agarwal et al. (2020), and the authors introduced a provably efficient algorithm FLAMBE, which achieves the learning goal of system identification with a sample complexity of O\u0303(H 22K9d7\n10 ). Here d, H and K respectively denote the representation dimension, episode horizon, and action space cardinality. Later on, Modi et al. (2021) proposed a model-free algorithm MOFFLE for reward-free RL under low-nonnegative-rank MDPs (where feature functions are non-negative), for which the sample complexity for finding an -optimal policy scales as O\u0303(H\n5K5d3LV 2\u03b7 ) (which is rescaled under the condition of \u2211H h=1 rh \u2264 1 for fair comparison).\nHere, dLV denotes the non-negative rank of the transition kernel, which may be exponentially larger than d as shown in Agarwal et al. (2020), and \u03b7 denotes the positive reachability probability to all states, where 1/\u03b7 can be as large as \u221a dLV as shown in Uehara et al. (2022b). Recently, a rewardfree algorithm called RFOLIVE has been proposed under non-linear MDPs with low Bellman Eluder dimension (Chen et al., 2022b), which can be specialized to low-rank MDPs. However, RFOLIVE is computationally more costly and considers a special reward function class, making their complexity result not directly comparable to other studies on reward-free low-rank MDPs.\nThis paper investigates reward-free RL under low-rank MDPs to address the following important open questions:\n\u2022 For low-rank MDPs, none of previous studies establishes a lower bound on the sample complexity showing a necessary sample complexity requirement for near-optimal policy finding.\n\u2022 The sample complexity of previous algorithms in Agarwal et al. (2020); Modi et al. (2021) on reward-free low-rank MDP is polynomial in the involved parameters, but still much higher than desirable. It is vital to improve the algorithm to further reduce the sample complexity.\n\u2022 Previous studies on low-rank MDPs did not provide estimation accuracy guarantee on the learned representation (only on the transition kernels). However, such a representation learning guarantee can be very beneficial to reuse the learned representation in other RL environment."
        },
        {
            "heading": "1.1 MAIN CONTRIBUTIONS",
            "text": "We summarize our main contributions in this work below.\n\u2022 Lower bound: We provide the first-known lower bound \u2126\u0303(HdK 2 ) on the sample complexity that holds for any algorithm under the same low-rank MDP setting. Our proof lies in a novel construction of hard MDP instances that capture the necessity of the cardinality of the action space on the sample complexity. Interestingly, comparing this lower bound for low-rank MDPs with the upper bound for linear MDPs in Wang et al. (2020) further implies that it is strictly more challenging to find near-optimal policy under low-rank MDPs than linear MDPs.\n\u2022 Algorithm: We propose a new model-based reward-free RL algorithm under low-rank MDPs. The central idea of RAFFLE lies in the construction of a novel exploration-driven reward, whose corresponding value function serves as an upper bound on the model estimation error. Hence, such a pseudo-reward encourages the exploration to collect samples over those state-action space where the model estimation error is large so that later stage of the algorithm can further reduce such an error based on those samples. Such reward construction is new for low-rank MDPs, and serve as the key reason for our improved sample complexity.\n\u2022 Sample complexity: We show that our algorithm can both find an -optimal policy and achieve an -accurate system identification via reward-free exploration, with a sample complexity of O\u0303(H 3d2K(d2+K) 2 ), which matches our lower bound in terms of the dependence on as well as\non K in the large d regime. Our result significantly improves that of O\u0303(H 22K9d7\n10 ) in Agarwal et al. (2020) to achieve the same goal. Our result also improves the sample complexity of O\u0303(\nH5K5d3LV 2\u03b7 ) in Modi et al. (2021) in three aspects: order on K is reduced; d can be exponentially smaller than dLV as shown in Agarwal et al. (2020); and no introduction of \u03b7, where 1/\u03b7\ncan be as large as \u221a dLV . Further, our result on reward-free RL naturally achieves the goal of reward-known RL, which improves that of O\u0303 ( H5d4K2\n2\n) in Uehara et al. (2022b) by \u0398(H2).\n\u2022 Near-accurate representation learning: We design a planning algorithm that exploits the exploration phase of RAFFLE to further learn a provably near-accurate representation of the transition kernel without requiring further interaction with the environment. To the best of our knowledge, this is the first theoretical guarantee on representation learning for low-rank MDPs."
        },
        {
            "heading": "2 PRELIMINARIES AND PROBLEM FORMULATION",
            "text": "Notation. For any H \u2208 N, we denote [H] := {1, . . . ,H}. For any vector x and symmetric matrix A, we denote \u2016x\u20162 as the `2 norm of x and \u2016x\u2016A := \u221a x>Ax. For any matrix A, we denote \u2016A\u2016F as its Frobenius norm and let \u03c3i(A) be its i-th largest singular value. For two probability measures P,Q \u2208 \u2126, we use \u2016P \u2212Q\u2016TV to denote their total variation distance."
        },
        {
            "heading": "2.1 EPISODIC MDPS",
            "text": "We consider an episodic Markov decision process (MDP) M = (S,A, H, P, r), where S can be an arbitrarily large state space; A is a finite action space with cardinality K; H is the number of steps in each episode; P : S \u00d7 A \u00d7 S \u2192 [0, 1] is the time-dependent transition kernel, where Ph(sh+1|sh, ah) denotes the transition probability from the state-action pair (sh, ah) at step h to state sh+1 in the next step; rh : S \u00d7 A \u2192 [0, 1] denotes the deterministic reward function at step h; We further normalize the summation of reward function as \u2211H h rh \u2264 1. A policy \u03c0 is a set of mappings {\u03c0h : S \u2192 \u2206(A)}h\u2208[H], where \u2206(A) is the set of all probability distributions over the action space A. Further, a \u223c U(A) indicates the uniform selection of an action a from A. In each episode of the MDP, we assume that a fixed initial state s1 is drawn. Then, at each step h \u2208 [H]. the agent observes state sh \u2208 S, takes an action ah \u2208 A under a policy \u03c0h, and receives a reward rh(sh, ah) (in the reward-known setting), and then the system transits to the next state sh+1 with probability Ph(sh+1|sh, ah). The episode ends after H steps. As standard in the literature, we use sh \u223c (P, \u03c0) to denote a state sampled by executing the policy \u03c0 under the transition kernel P for h\u2212 1 steps. If the previous state-action pair (sh\u22121, ah\u22121) is given, we use sh \u223c P to denote that sh follows the distribution Ph(\u00b7|sh\u22121, ah\u22121). We use the notation E(sh,ah)\u223c(P,\u03c0) [\u00b7] to denote the expectation over states sh \u223c (P, \u03c0) and actions ah \u223c \u03c0.\nFor a given policy \u03c0 and an MDP M = (S,A, H, P, r), we denote the value function starting from state sh at step h as V \u03c0h,P,r(sh) := E(sh\u2032 ,ah\u2032 )\u223c(P,\u03c0) [\u2211H h\u2032=h rh\u2032(sh\u2032 , ah\u2032)|sh ] . We use V \u03c0P,r to denote V \u03c01,P,r(s1) for simplicity. Similarly, we denote the action-value function starting from state\naction pair (sh, ah) at step h as Q\u03c0h,P,r(sh, ah) := rh(sh, ah) +Esh+1\u223cP [ V \u03c0h+1,P,r(sh+1)|sh, ah ] .\nWe use P ? to denote the transition kernel of the true environment and for simplicity, denote E(sh,ah)\u223c(P?,\u03c0)[\u00b7] as E?\u03c0[\u00b7]. Given a reward function r, there always exists an optimal policy \u03c0? that yields the optimal value V ?P?,r = sup\u03c0 V \u03c0 P?,r."
        },
        {
            "heading": "2.2 LOW-RANK MDPS",
            "text": "This paper focuses on the low-rank MDPs (Agarwal et al., 2020) defined as follows. Definition 1. (Low-rank MDPs). A transition probability P ?h : S \u00d7 A \u2192 \u2206(A) admits a low-rank decomposition with dimension d \u2208 N if there exists two embedding functions \u03c6?h : S \u00d7 A \u2192 Rd and \u00b5?h : S \u2192 Rd such that P ?h (s\u2032|s, a) = \u3008\u03c6?h(s, a), \u00b5?h(s\u2032)\u3009 ,\u2200s, s\u2032 \u2208 S, a \u2208 A. For normalization, we assume \u2016\u03c6?h(s, a)\u20162 \u2264 1 for all (s, a), and for any function g : S \u2192 [0, 1], \u2016 \u222b \u00b5?h(s)g(s)ds\u20162 \u2264 \u221a d. An MDP M is a low-rank MDP with dimension d if for each h \u2208 [H], Ph admits a low-rank decomposition with dimension d. We use \u03c6? = {\u03c6?h}h\u2208[H] and \u00b5? = {\u00b5?h}h\u2208[H] to denote the embeddings for P ?.\nWe remark that when \u03c6h is revealed to the agent, low-rank MDPs specialize to linear MDPs (Wang et al., 2020; Jin et al., 2020b). Essentially, low-rank MDPs do not assume that the features {\u03c6h}h\nare known a priori. The lack of knowledge on features in fact invokes a nonlinear structure, which makes the model strictly harder than linear MDPs or tabular models. Since it is impossible to learn a model in polynomial time if there is no assumption on features \u03c6h and \u00b5h, we adopt the following conventional assumption from the recent studies on low-rank MDPs.\nAssumption 1. (Realizability). A learning agent can access to a model class {(\u03a6,\u03a8)} that contains the true model, i.e., the embeddings \u03c6? \u2208 \u03a6, \u00b5? \u2208 \u03a8.\nWhile we assume the cardinality of the function classes to be finite for simplicity, extensions to infinite classes with bounded statistical complexity (such as bounded covering number) are not difficult (Sun et al., 2019; Agarwal et al., 2020)."
        },
        {
            "heading": "2.3 REWARD-FREE RL AND LEARNING OBJECTIVES",
            "text": "Reward-free RL typically has two phases: exploration and planning. In the exploration phase, an agent explores the state space via interaction with the true environment and can collect samples over multiple episodes, but without access to the reward information. In the planning phase, the agent is no longer allowed to interact with the environment, and for any given reward function, is required to achieve certain learning goals (elaborated below) based on the outcome of the exploration phase.\nThe planning phase may require the agent to achieve different learning goals. In this paper, we focus on three of such goals. The most popular goal in reward-free RL is to find a near-optimal policy that achieves the best value function under the true environment with -accuracy, as defined below.\nDefinition 2. ( -optimal policy). Fix > 0. For any given reward function r, a learned policy \u03c0 is -optimal if it satisfies V \u03c0 ?\nP?,r \u2212 V \u03c0P?,r \u2264 .\nFor model-based learning, Agarwal et al. (2020) proposed system identification as another useful learning goal, defined as follows.\nDefinition 3 ( -accurate system identification). Fix > 0. Given a model class (\u03a6,\u03a8), a learned model (\u03c6\u0302, \u00b5\u0302) is said to achieve -accurate system identification if it uniformly approximates the true model P ?, i.e., \u2200\u03c0, h \u2208 [H], E?\u03c0 [\u2225\u2225\u2225\u2329\u03c6\u0302h(sh, ah), \u00b5\u0302h(\u00b7)\u232a\u2212 P ?h (\u00b7|sh, ah)\u2225\u2225\u2225 TV ] \u2264 . Besides those two common learning goals, we also propose an additional goal on near-accurate representation learning. Towards that, we introduce the following divergence-based metric to quantify distance between two representations, which has been used in supervised learning (Du et al., 2021b).\nDefinition 4. (Divergence between two representations). Given a distribution q over S \u00d7 A and two representations \u03c6, \u03c6\u2032 \u2208 \u03a6, define the covariance between \u03c6 and \u03c6\u2032 w.r.t q as \u03a3(s,a)\u223cq(\u03c6, \u03c6\u2032) = E [ \u03c6(s, a)\u03c6\u2032(s, a)> ] . Then, the divergence between \u03c6 and \u03c6\u2032 with respect to q is defined as\nDq(\u03c6, \u03c6 \u2032) = \u03a3(s,a)\u223cq(\u03c6 \u2032, \u03c6\u2032)\u2212 \u03a3(s,a)\u223cq(\u03c6\u2032, \u03c6) ( \u03a3(s,a)\u223cq(\u03c6, \u03c6) )\u2020 \u03a3(s,a)\u223cq(\u03c6, \u03c6 \u2032).\nIt can be verified thatDq(\u03c6, \u03c6\u2032) 0 (i.e., positive semidefinite) andDq(\u03c6, \u03c6) = 0 for any \u03c6, \u03c6\u2032 \u2208 \u03a6."
        },
        {
            "heading": "3 LOWER BOUND ON SAMPLE COMPLEXITY",
            "text": "In this section, we provide a lower bound on the sample complexity that all reward-free RL algorithms must satisfy under low-rank MDPs. The detailed proof can be found in Appendix C.\nTheorem 1 (Lower bound). For any algorithm that can output an -optimal policy (as Definition 2), if H > max(24 , 4), S \u2265 6,K \u2265 3 and \u03b4 < 1/16, then there exists a low-rank MDP model M such that the number of trajectories sampled by the algorithm is at least \u2126 ( HdK 2 ) .\nTo the best of our knowledge, Theorem 1 establishes the first lower bound for learning low-rank MDPs in the reward-free setting. More importantly, Theorem 1 shows that it is strictly more costly in terms of sample complexity to find near-optimal policies under low-rank MDPs (which have unknown representations) than linear MDPs (which have known representations) by at least a factor of \u2126(K). This can be seen as the lower bound in Theorem 1 for low-rank MDPs has an additional term K compared to the upper bound O\u0303 ( d3H4\n2\n) provided in Wang et al. (2020) for linear MDPs.\nThis can be explained intuitively as follows. In linear models, all the representations \u03c6 : S\u00d7A \u2192 Rd are known. Then, it requires at most O(d) actions with linearly independent features to realize all transitions \u3008\u03c6, \u00b5\u3009. However, learning low-rank MDPs requires the agent to further select O(K) actions to access the unknown features, leading to a dependence on K.\nOur proof of the new lower bound mainly features the following two novel ingredients in the construction of hard MDP instances. a) We divide the actions into two types. The first type of actions is mainly used to form a large state space through a tree structure. The second type of actions is mainly used to distinguish different MDPs. Such a construction allows us to separately treat the state space and the action space, so that both state space and action space can be arbitrarily large. b) We explicitly define the feature vectors for all state-action pairs, and more importantly, the dimension is less than or equal to the number of states. These two ingredients together guarantee that the number of actions K can be arbitrarily large and independent with other parameters d and S, which indicates that the dependence on the number of actions K is unavoidable."
        },
        {
            "heading": "4 THE RAFFLE ALGORITHM",
            "text": "In this section, we propose RAFFLE (see Algorithm 1) for reward-free RL under low-rank MDPs.\nSummary of design novelty: The central idea of RAFFLE lies in the construction of a novel exploration-driven reward, which is desirable because its corresponding value function serves as an upper bound on the model estimation error during the exploration phase. Hence, such a pseudoreward encourages the exploration to collect samples over those state-action space where the model estimation error is large so that later stage of the algorithm can further reduce such an error based on those samples. Such reward construction are new for low-rank MDPs, and serve as the key enabler for our improved sample complexity. They also necessitate various new ingredients in other steps of algorithms, as elaborated below.\nExploration and MLE model estimation. In each iteration n during the exploration phase, for each h \u2208 [H], the agent executes the exploration policy \u03c0n\u22121 (defined in the previous iteration) up to step h \u2212 1, after which it takes two uniformly selected actions, and stops after step h + 1. Different from FLAMBE (Agarwal et al., 2020) that collects a large number of samples for each episode, our algorithm uses each exploration policy to collect only one sample trajectory at each episode, indexed by (n, h). Hence, the sample complexity of RAFFLE is much smaller than that of FLAMBE. In fact, such an efficient sampling together with our new termination idea introduced later benefit sample complexity.\nThen, the agent estimates the low-rank components \u03c6\u0302(n)h and \u00b5\u0302 (n) h via the MLE oracle with given model class (\u03a6,\u03a8) and a dataset Dnh as follows\nMLE(Dnh) := arg max\u03c6\u2208\u03a6,\u00b5\u2208\u03a8 \u2211 (s,a,s\u2032)\u2208Dnh log \u3008\u03c6h(s, a), \u00b5h(s\u2032)\u3009 .\nDesign of exploration reward. The agent updates the empirical covariance matrix U\u0302 (n)h as\nU\u0302 (n) h = \u03bbnI + \u2211n \u03c4=1 \u03c6\u0302 (n) h (s (\u03c4,h+1) h , a (\u03c4,h+1) h )(\u03c6\u0302 (n) h (s (\u03c4,h+1) h , a (\u03c4,h+1) h )) >, (1)\nwhere {s(\u03c4,h+1)h , a (\u03c4,h+1) h , s (\u03c4,h+1) h+1 } is collected at iteration \u03c4 , episode (h+ 1), and step h.\nNext, the agent uses both \u03c6\u0302(n)h and U\u0302 (n) h to construct an exploration-driven reward function as\nb\u0302 (n) h (s, a) = min{\u03b1\u0302n\u2016\u03c6\u0302 (n) h (s, a)\u2016(U\u0302(n)h )\u22121 , 1}, (2)\nwhere \u03b1\u0302n is a pre-determined parameter. We note that although individual b\u0302 (n) h (s, a) for each step may not represent point-wise uncertainty as indicated in Uehara et al. (2022b), we find its total cumulative version V\u0302 \u03c0\nP\u0302 (n),b\u0302(n) can serve as a trajectory-wise uncertainty mea-\nsure to select exploration policy. To see this, it can be shown that for any \u03c0 and h, E?\u03c0 [\u2225\u2225\u2225\u2329\u03c6\u0302(n)h (sh, ah), \u00b5\u0302(n)h (\u00b7)\u232a\u2212 P ?h (\u00b7|sh, ah)\u2225\u2225\u2225\nTV\n] \u2264 c\u2032V\u0302 \u03c0n P\u0302 (n),b\u0302(n) + \u221a cn n , where c \u2032 is a constant\nand cn = O(log n). As iteration number n grows, the second term diminishes to zero, which indicates that V\u0302 \u03c0n\nP\u0302 (n),b\u0302(n) (under the reward of b\u0302(n)) serves as a good upper bound on the estimation error\nfor the true transition kernel. Hence, exploration guided by maximizing V\u0302 \u03c0 P\u0302 (n),b\u0302(n) will collect more trajectories over which the learned transition kernels are not estimated well. This will help to reduce the model estimation error in the future.\nAlgorithm 1 RAFFLE (RewArd-Free Feature LEarning) 1: Input: \u03b1\u0302n, \u03b6n, > 0, \u03b4 \u2208 (0, 1), regularizer \u03bbn, model classes {(\u00b5, \u03c6) : \u00b5 \u2208 \u03a8, \u03c6 \u2208 \u03a6}. 2: Initialize \u03c00(\u00b7|s) to be uniform; set D0h = \u2205. 3: Phase I: Exploration Phase 4: for n = 1, . . . do 5: for h = 1, . . . ,H do 6: Use \u03c0n\u22121: roll into sh\u22121, uniformly choose ah\u22121, ah, enter into sh, sh+1. 7: Collect data s(n,h)1 , a (n,h) 1 , . . . , s (n,h) h , a (n,h) h , s (n,h) h+1 .\n8: Add the triple (s(n,h)h , a (n,h) h , s (n,h) h+1 ) to the datasetDnh = D n\u22121 h \u222a{(s (n,h) h , a (n,h) h , s (n,h) h+1 )}. 9: Learn (\u03c6\u0302(n)h , \u00b5\u0302 (n) h ) = MLE(Dnh).\n10: Update transition dynamics P\u0302 (n) as P\u0302 (n)h (s \u2032|s, a) = \u3008\u03c6\u0302(n)h (s, a), \u00b5\u0302 (n) h (s \u2032)\u3009. 11: end for 12: Update empirical covariance matrix U\u0302 (n)h as in Equation (1). 13: Define exploration-driven reward function b\u0302(n)h as in Equation (2). 14: Define an estimated value function V\u0302 \u03c0\nP\u0302 (n),b\u0302(n) based on P\u0302 (n) and b\u0302(n) as in Equation (3).\n15: Find exploration policy \u03c0n = arg max\u03c0 V\u0302 \u03c0P\u0302 (n),b\u0302(n) . 16: if 2V\u0302 \u03c0n P\u0302 (n),b\u0302(n) + 2 \u221a K\u03b6n \u2264 then 17: Terminate Phase I: Exploration Phase and set P\u0302 = P\u0302 (n), b\u0302 = b\u0302(n), \u03c0 = \u03c0n, n = n. 18: end if 19: end for 20: Phase II: Planning Phase 21: Option 1 (learn near-optimal policy): Receive reward function r = {rh}Hh=1, and compute\npolicy \u03c0\u0304 = arg max\u03c0 V \u03c0P\u0302 ,r.\n22: Option 2 (system identification): let P\u0302 = P\u0302 . 23: Option 3 (learn near-accurate representation): call Algorithm 2 of RepLearn and obtain \u03c6\u0303. 24: Output: policy \u03c0\u0304, learned transition dynamics P\u0302 , learned representation \u03c6\u0303.\nDesign of exploration policy. The agent defines a truncated value function iteratively using the estimated transition kernel and the exploration-driven reward as follows:\nQ\u0302\u03c0 h,P\u0302 (n),b\u0302(n)\n(sh, ah) = min { 1, b\u0302 (n) h (sh, ah) + P\u0302 (n) h V\u0302 \u03c0 h+1,P\u0302 (n),b\u0302(n) (sh, ah) } ,\nV\u0302 \u03c0 h,P\u0302 (n),b\u0302(n) (sh) = E \u03c0 [ Q\u0302\u03c0 h,P\u0302 (n),b\u0302(n) (sh, ah) ] . (3)\nThe truncation technique here is important for the improvement of the sample complexity on the dependence of H . The agent finally finds an optimal policy maximizing V\u0302 \u03c0\nP\u0302 (n),b\u0302(n) , and uses this\npolicy as the exploration policy for the next iteration.\nNovel termination criterion. RAFFLE does not require a pre-determined maximum number of iterations as its input. Instead, it will terminate and output the current estimated model if the optimal value function V\u0302 \u03c0\nP\u0302 (n),b\u0302(n) plus a minor term is below a threshold. Such a termination criterion\nessentially guarantees that the value functions under the estimated and true models are close to each other under any reward and policy, hence the exploration can be terminated in finite steps. Such a termination criterion enables our algorithm to identify an accurate model and later find a near-optimal policy with fewer sample collections than FLAMBE in Agarwal et al. (2020) as we discuss in the exploration phase. Additionally, our termination criterion provides strong performance guarantees on the output policy and estimator from the last iteration. On the contrary, Uehara et al. (2022b) can only provide guarantees on a random mixture of the policies obtained over all iterations.\nPlanning phase. Given any reward function r, the agent finds a near-optimal policy by planning with the learned transition dynamics P\u0302 and the given reward r. Note that such planning with a known low-rank MDP is computationally efficient by assumption."
        },
        {
            "heading": "5 UPPER BOUNDS ON SAMPLE COMPLEXITY",
            "text": "In this section, we first show that the policy returned by RAFFLE is an -optimal policy with respect to any given reward r in the planning phase. The detailed proof can be found in Appendix A. Theorem 2 ( -optimal policy). AssumeM is a low-rank MDP with dimension d, and Assumption 1 holds. Given any , \u03b4 \u2208 (0, 1), and any reward function r, let \u03c0\u0304 and P\u0302 be the output of RAFFLE and \u03c0? := arg max\u03c0 V \u03c0P?,r be the optimal policy under the true model P ?. Set \u03b1\u0302n = O\u0303( \u221a K + d2) and \u03bbn = O\u0303(d). Then, with probability at least 1 \u2212 \u03b4, we have V \u03c0 ?\nP?,r \u2212 V \u03c0\u0304P?,r \u2264 , and the total number of trajectories collected by RAFFLE is upper bounded by O\u0303(H 3d2K(d2+K) 2 ).\nWe note that the upper bound in Theorem 2 matches the lower bound in Theorem 1 in terms of the dependence on as well as on K in the large d regime.\nCompared with MOFFLE (Modi et al., 2021), which also finds an -optimal policy in reward-free RL, our result improves their sample complexity of O\u0303 ( H5K5d3LV\n2\u03b7\n) in three aspects. First, the or-\nder on K is reduced. Second, the dimension dLV of the underlying function class in MOFFLE can be exponentially larger than d as shown in Agarwal et al. (2020). Finally, MOFFLE requires reachability assumption, leading to a factor 1/\u03b7 in the sample complexity, which can be as large as\u221a dLV . Further, Theorem 2 naturally achieves the goal of reward-known RL with the same sample\ncomplexity, which improves that of O\u0303 ( H5d4K2\n2\n) in Uehara et al. (2022b) by a factor of O(H2).\nProceeding to the learning objective of system identification, the learned transition kernel output by Algorithm 1 achieves the goal of -accurate system identification with the same sample complexity as follows. The detailed proof can be found in Appendix B.\nTheorem 3 ( -accurate system identification). Under the same condition of Theorem 2 and let P\u0302 = {\u03c6\u0302 h, \u00b5\u0302 h} be the output of RAFFLE. Then, with probability at least 1 \u2212 \u03b4, P\u0302 achieves -accurate system identification, i.e. for any \u03c0 and h: E?\u03c0 [\u2225\u2225\u2225\u2329\u03c6\u0302 h(sh, ah), \u00b5\u0302 h(\u00b7)\u232a\u2212 P ?h (\u00b7|sh, ah)\u2225\u2225\u2225 TV ] \u2264 , and the number of trajectories collected by RAFFLE is upper bounded by O\u0303(H 3d2K(d2+K) 2 ).\nTheorem 3 significantly improves the sample complexity of O\u0303(H 22K9d7\n10 ) in Agarwal et al. (2020) on the dependence of all involved parameters for achieving -accurate system identification."
        },
        {
            "heading": "6 NEAR-ACCURATE REPRESENTATION LEARNING",
            "text": "In low-rank MDPs, it is of great interest to learn the representation \u03c6 accurately, because other similar RL environments can very likely share the same representation (Rusu et al., 2016; Zhu et al., 2020; Dayan, 1993) and hence such learned representation can be directly reused in those environments. Thus, the third objective of RAFFLE in the planning phase is to provide an accurate estimation of \u03c6. We note that although RAFFLE provides an estimation of \u03c6 during its execution, such an estimation does not come with an accuracy guarantee. Besides, Theorem 3 on system identification does not provide the guarantee on the representation \u03c6\u0302, but only on the entire transition kernel P\u0302 . Further, none of previous studies of reward-free RL under low-rank MDPs (Agarwal et al., 2020; Modi et al., 2021; Uehara et al., 2022b) established the guarantee on \u03c6\u0302."
        },
        {
            "heading": "6.1 THE REPLEARN ALGORITHM",
            "text": "In this section, we present the following algorithm of RepLearn, which exploits the learned transition kernel from RAFFLE and learns a near-accurate representation without additional interaction with the environment. The formal version of RepLearn, Algorithm 2, is delayed in Appendix D. We explain the main idea of Algorithm 2 as follows. First, for each h \u2208 [H], t \u2208 [T ], where T is the\nnumber of rewards, Nf pairs of state-action (sh, ah) are generated based on distribution qh. Note that the agent does not interact with the true environment during such data generation. Then, for any h, if we set the reward r at step h to be zero, Q\u03c0P?,h,r(sh, ah) can have a linear structure in terms of the true representation \u03c6?h(sh, ah). Namely, there exists a wh decided by r, P\n? and \u03c0 such that Q\u03c0P?,h,r(sh, ah) = \u3008\u03c6?h(sh, ah), wh\u3009. Then, with the estimated transition kernel P\u0302 that RAFFLE provides and the well-designed rewards rh,t, Q\u03c0 t\nP\u0302 ,h,rh,t (sh, ah) can be computed efficiently and\nserve as a target to learn the representation via the following regression problem: arg min\u03c6h\u2208\u03a6,wth\u2208Rd \u2211 t\u2208[T ] \u2211 (sh,ah)\u2208Dth (Q\u03c0 t P\u0302 ,h,rh,t (sh, ah)\u2212 \u3008\u03c6h(sh, ah), wth\u3009)2. (4)\nThe main difference between our algorithm and that in Lu et al. (2021) for representation learning is that, our data generation is based on P\u0302 from RAFFLE, which carries a natural estimation error but requires no interaction with the environment, whereas their algorithm assumes a generative model to collect data from the ground-truth transition kernel."
        },
        {
            "heading": "6.2 GUARANTEE ON ACCURACY",
            "text": "In order to guarantee the learned representation by Algorithm 2 is sufficiently close to the ground truth, we need to employ the following two somewhat necessary assumptions.\nFirst, for the distributions of the state-action pairs {qh}Hh=1 in Algorithm 2, it is desirable to have P\u0302 (\u00b7|s, a) approximates true P ?(\u00b7|s, a) well over those distributions, so that Q\u03c0\nP\u0302 ,h,r (sh, ah) can ap-\nproximate the ground truth well. Intuitively, if some state-action pairs can hardly be visited under any policy, the output P\u0302 (\u00b7|s, a) of Algorithm 1 cannot approximate true P ?(\u00b7|s, a) well over these state-action pairs. Hence, we assume reachability type assumptions for MDPs as following so that all state-action pairs is likely to be visited by certain policy. Such reachability type assumption is common in relevant literature (Modi et al., 2021; Agarwal et al., 2020).\nAs discussed in Section 6, intuitively, if some state action pairs can be hardly visited by any policy, the output of Algorithm 1 P\u0302 (\u00b7|s, a) can not approximate true P ?(\u00b7|s, a) well over these state action pairs, so a standard reachability type assumption is necessary so that all states can be visited. Assumption 2 (Reachability). For the true transition kernel P ?, there exists a policy \u03c00 such that mins\u2208S P\u03c0 0 h (s) \u2265 \u03b7min, where P\u03c0 0\nh (\u00b7) : S \u2192 R is the density function over S using policy \u03c00 to roll into state s at timestep h.\nWe further assume the input distributions {qh}Hh=1 are bounded with constant CB . Then, together with Assumption 2, for any (s, a) \u2208 S\u00d7A, we have qh(s, a) \u2264 CminP\u03c0 0\nh (s, a), whereCmin = CB \u03b7min .\nNext, we assume that the rewards chosen for generating target Q-functions are sufficiently diverse so that the target Q\u03c0P?,h,r(sh, ah) spans over the entire representation space to guarantee accurate representation learning in Equation (4). Such an assumption is commonly adopted in multi-task representation learning literature (Du et al., 2021b; Yang et al., 2021; Lu et al., 2021). To formally state the assumption, for any h \u2208 [H], let {rh,t}t\u2208[T ] be a set of T rewards (where T \u2265 d) satisfying rh,th = 0. As a result, for each t, given \u03c0 t, there exists wth ? \u2208 Rd such that Q\u03c0P?,h,rh,t(sh, ah) = \u3008\u03c6?h(sh, ah), wth ?\u3009. Let W ?h = [w1h ? , . . . , wTh ? ] \u2208 Rd\u00d7T . Assumption 3 (Diverse rewards). The smallest singular value \u03c3d(W ?h ) of W ?h defined above satisfies \u03c32d(W ? h ) \u2265 \u2126(Td ), i.e., there exists a constant CD > 0 such that \u03c3 2 d(W ? h ) \u2265 CDT d .\nWe next characterize the accuracy of the output representation of Algorithm 2 in terms of the divergence Definition 4 between the learned and the ground truth representations in the following theorem and defer the proof to Appendix D. Theorem 4 (Guarantee for representation learning). Under Assumption 1 and Assumption 3, for any , \u03b4 \u2208 (0, 1), any h \u2208 [H], and sufficiently large Nf , let P\u0302 be the output transition kernel of RAFFLE that satisfies Theorem 3. With probability at least 1\u2212 \u03b4, the output \u03c6\u0303 of RAFFLE satisfies\u2225\u2225\u2225Dqh(\u03c6?h, \u03c6\u0303h)1/2\u2225\u2225\u22252\nF = O( dCminCD + d CD \u221a log 2\u03b4 TNf ). (5)\nWe explain the bound in Equation (5) as follows. The first term arises from the system identification error by using the output of RAFFLE, and can be made as small as possible by choosing appropriate . The second term is related to the randomness caused by sampling state-action pairs from input distribution {qh}h\u2208[H], and vanishes as Nf becomes large. Note that Nf is the number of simulated samples in Algorithm 2, which does not require interaction with the true environment. Hence, it can be made sufficiently large to guarantee a small error.\nTheorem 4 shows RAFFLE can learn a near-accurate representation, based on which learned representations can be further used in other RL environments sharing common representations, similarly in spirit to how representation learning has been exploited in supervised learning (Du et al., 2021b)."
        },
        {
            "heading": "7 RELATED WORK",
            "text": "Reward-free RL. While various studies (Oudeyer et al., 2007; Bellemare et al., 2016; Burda et al., 2018; Colas et al., 2018; Nair et al., 2018; Eysenbach et al., 2018; Co-Reyes et al., 2018; Hazan et al., 2019; Du et al., 2019; Pong et al., 2019; Misra et al., 2020) proposed exploration algorithms for good coverage on the state space without using explicit reward signals, theoretically speaking, the paradigm of reward-free RL was first formalized by Jin et al. (2020a), where they provided both upper and lower bounds on the sample complexity. For tabular case, several follow-up studies (Kaufmann et al., 2021; Me\u0301nard et al., 2021) further improved the sample complexity, and Zhang et al. (2020) established the minimax optimality guarantee. Reward-free RL was also studied with function approximation. Wang et al. (2020) studied linear MDPs, and Zhang et al. (2021b) studied linear mixture MDPs. Further, Zanette et al. (2020b) considered a class of MDPs with low inherent Bellman error introduced by Zanette et al. (2020a). Chen et al. (2022b) proposed a reward-free algorithm called RFOLIVE under non-linear MDPs with low Bellman Eluder dimension. In addition, Miryoosefi & Jin (2021) proposed a reward-free approach to solving constrained RL problems with any given reward-free RL oracle under both the tabular and linear MDPs.\nReward-free RL under low-rank MDPs. As discussed in Section 1, reward-free RL under lowrank MDPs have been studied recently (Agarwal et al., 2020; Modi et al., 2021), and our result significant improves the sample complexity therein. When finite latent state space is assumed, lowrank MDPs specialize to block MDPs, under which algorithms termed as PCID (Du et al., 2019) and HOMER (Misra et al., 2020) achieved sample complexities of O\u0303( d 4H2K4\nmin(\u03b74\u03b32n, 2) ) and O\u0303(\nd8H4K4\nmin(\u03b73, 2) ), respectively. Our result on the general low-rank MDPs can be utilized to further improve those results for block MDPs.\nReward-known RL under low-rank MDPs. For reward-known RL, Uehara et al. (2022b) proposed a computationally efficient algorithm REP-UCB under low-rank MDPs. Our design of the reward-free algorithm for low-rank MDPs is inspired by their algorithm with several new ingredients as discussed in Section 4, and improves their sample complexity on the dependence of H . Meanwhile, algorithms have been proposed for MDP models with low Bellman rank (Jiang et al., 2017), low witness rank (Sun et al., 2019), bilinear classes (Du et al., 2021a) and low Bellman eluder dimension (Jin et al., 2021), which can specialize to low-rank MDPs. However, those algorithms are computationally more costly as remarked in Uehara et al. (2022b), although their sample complexity may have sharper dependence on d,K or H . Specializing to block MDPs, Zhang et al. (2022) proposed an algorithm called BRIEE, which empirically achieves the state-of-art sample complexity for block MDP models. Besides, Zhang et al. (2021a) proposed an algorithm coined ReLEX for a slightly different low-rank model, and obtained a problem dependent regret upper bound."
        },
        {
            "heading": "8 CONCLUSIONS",
            "text": "In this paper, we investigate the reward-free reinforcement learning, in which the underlying model admits a low-rank structure. Without further assumptions, we propose an algorithm called RAFFLE which significantly improves the state-of-the-art sample complexity for accurate model estimation and near-optimal policy identification. We further design an algorithm to exploit the model learned by RAFFLE for accurate representation learning without further interaction with the environment. Although -accurate system identification can easily induce H -optimal policy, the relationship between these two learning goals under general reward-free exploration setting remain under-explored, and is an interesting topic for future investigation."
        },
        {
            "heading": "A PROOF OF THEOREM 2",
            "text": "We first provide a proof outline to highlight our key ideas in the analysis of Theorem 2, and then provide the detailed proof. To simplify the notation, we denote the total variation distance\u2225\u2225\u2225P\u0302 (n)h (sh, ah)\u2212 P ?h (sh, ah)\u2225\u2225\u2225\nTV by f (n)h (sh, ah).\nProof Outline for Theorem 2. Step 1 provides an upper bound on the difference of value functions under estimated model P\u0302 (n) and the true model P ? for any given policy \u03c0 and reward r, as given in the following proposition (see proposition 4 in appendix A.2).\nProposition 1 (Informal). There exist constants cn = O(log n) and c\u2032 = O(1) such that for any policy \u03c0 and reward r, with high probability, we have\u2223\u2223\u2223V \u03c0P?,r \u2212 V \u03c0P\u0302 (n),r\u2223\u2223\u2223 \u2264 V\u0302 \u03c0nP\u0302 (n),b\u0302(n) + \u221a cn n .\nThis proposition is inspired by Uehara et al. (2022b), while it generalizes to non-stationary setting and arbitrary reward scenario from infinite stationary MDP and fixed reward. The main proof idea is to first notice that for any reward, we have\u2223\u2223\u2223V \u03c0P?,r \u2212 V \u03c0P\u0302 (n),r\u2223\u2223\u2223 \u2264 V\u0302 \u03c0P\u0302 (n),f(n) . (6) Then, we show that V\u0302 \u03c0\nP\u0302 (n),f(n) \u2264 V\u0302 \u03c0 P\u0302 (n),b\u0302(n) + \u221a cn n \u2264 V\u0302 \u03c0n P\u0302 (n),b\u0302(n) + \u221a cn n due to the optimality of\nthe exploration policy \u03c0n.\nStep 2 shows sublinearity of the summation of V\u0302 \u03c0n P\u0302 (n),b\u0302(n) , which is an upper bound of value function difference under the true model with the estimated model (see proposition 5 in appendix A.3).\nProposition 2 (Informal). Under the same setting of Proposition 1, with high probability, the summation of value functions V \u03c0n\nP\u0302 (n),b\u0302(n) under exploration policies {\u03c0n}n\u2208[N ] with exploration-driven\nreward functions b\u0302(n) is sublinear, as given by\u2211 n\u2208[N ] V\u0302 \u03c0n P\u0302 (n),b\u0302(n) \u2264 O\u0303 ( Hd \u221a K(K + d2)N ) .\nStep 3 combines Step 2 and Step 1. We are able to conclude that RAFFLE will terminate with polynomial sample complexity such that, the value function difference of P ? and the returned environment P\u0302 is at most .\nProposition 3. With high probability, RAFFLE terminates after at most O\u0303 ( H2d2K(d2 +K)/ 2 ) iterations, and the output model P\u0302 satisfies\u2223\u2223\u2223V \u03c0P?,r \u2212 V \u03c0P\u0302 ( ),r\u2223\u2223\u2223 \u2264 V\u0302 \u03c0n P\u0302 ( ),b\u0302(n ) + \u221a cn n \u2264 /2,\nwhere n is the iteration number where RAFFLE terminates, i.e. P\u0302 = P\u0302 (n ).\nFinally, with some algebraic operations, Proposition 3 concludes the proof of Theorem 2."
        },
        {
            "heading": "A.1 SUPPORTING LEMMAS",
            "text": "We first present the high probability event.\nLemma 1. We define \u03a0n to be a uniform mixture of previous n\u2212 1 exploration policies:\n\u03a0n = U(\u03c00, \u03c01, ..., \u03c0n\u22121).\nDenote the total variation of P\u0302 (n) and P ?, and the expected matrix of U\u0302 (n)h as follows.\nf (n) h (sh, ah) = \u2225\u2225\u2225P ?h (\u00b7|sh, ah)\u2212 P\u0302 (n)h (\u00b7|sh, ah)\u2225\u2225\u2225 TV , (7)\nU (n) h,\u03c6 = n E\nsh\u223c(P?,\u03a0n) ah\u223cU(A)\n[ \u03c6(sh, ah)(\u03c6(sh, ah)) >]+ \u03bbnI, (8) W\n(n) h,\u03c6 = n E\n(sh,ah)\u223c(P?,\u03a0n)\n[ \u03c6(sh, ah)(\u03c6(sh, ah)) >]+ \u03bbnI. (9) where \u03bbn = \u03b23d log(2nH|\u03a6|/\u03b4)) and \u03b23 = O(1) is a constant coefficient. Suppose Algorithm 1 runs N iterations and let events E0 and E1 be defined as follows.\nE0 = { \u2200n \u2208 [N ], h \u2208 [H], sh \u2208 S, ah \u2208 A, E\nsh\u223c(P?,\u03a0n) ah\u223cU\n[ f\n(n) h (sh, ah) 2 ] \u2264 \u03b6n } ,\nE1 = { \u2200n \u2208 [N ], h \u2208 [H], sh \u2208 S, ah \u2208 A,\n1\n5 \u2225\u2225\u2225\u03c6\u0302(n)h\u22121(s, a)\u2225\u2225\u2225 (U (n) h\u22121,\u03c6\u0302 )\u22121 \u2264 \u2225\u2225\u2225\u03c6\u0302(n)h\u22121(s, a)\u2225\u2225\u2225 (U\u0302 (n) h\u22121) \u22121 \u2264 3 \u2225\u2225\u2225\u03c6\u0302(n)h\u22121(s, a)\u2225\u2225\u2225 (U (n) h\u22121,\u03c6\u0302 )\u22121 } .\nwhere \u03b6n = log (2|\u03a6||\u03a8|nH/\u03b4) /n. We further denote \u03b6 = N\u03b6N = log (2|\u03a6||\u03a8|NH/\u03b4) for simplicity. Denote E = E0 \u2229 E1 as the intersection of the two events. Then, P[E ] \u2265 1\u2212 \u03b4.\nProof. By Corollary 2 in Appendix F, we have P[E0] \u2265 1\u2212 \u03b4/2. Further, by Lemma 39 in Zanette et al. (2021) for the version of fixed \u03c6 and Lemma 11 in Uehara et al. (2022b), we have P[E1] \u2265 1\u2212 \u03b4/2. Therefore, P[E ] \u2265 1\u2212 \u03b4.\nBased on Lemma 1, we can bound the exlporation-driven reward in RAFFLE as follows. Corollary 1. Given that the event E occurs, the following inequality holds for any n \u2208 [N ], h \u2208 [H], sh \u2208 S, ah \u2208 A:\nmin { \u03b1\u0302n 5 \u2225\u2225\u2225\u03c6\u0302(n)h (sh, ah)\u2225\u2225\u2225 (U (n)\nh,\u03c6\u0302 )\u22121\n, 1 } \u2264 b\u0302(n)h (sh, ah) \u2264 3\u03b1\u0302n \u2225\u2225\u2225\u03c6\u0302(n)h (sh, ah)\u2225\u2225\u2225 (U (n)\nh,\u03c6\u0302 )\u22121\n,\nwhere \u03b1\u0302n = 5 \u221a 2\u03b23n\u03b6n(K + d2). Proof. Recall b\u0302(n)h (sh, ah) = min { \u03b1\u0302n \u2225\u2225\u2225\u03c6\u0302(n)h (s, a)\u2225\u2225\u2225 (U\u0302\n(n) h )\n\u22121 , 1\n} . Applying Lemma 1, we can im-\nmediately obtain the result.\nThe following lemma extends the Lemmas 12 and 13 under infinite discount MDPs in Uehara et al. (2022b) to episodic MDPs. We provide the proof for completeness. Lemma 2. Let Ph\u22121 = \u3008\u03c6h\u22121, \u00b5h\u22121\u3009 be a generic MDP model, and \u03a0 be an arbitrary and possibly mixture policy. Define an expected Gram matrix as follows\nMh\u22121,\u03c6 = \u03bbnI + n E sh\u22121\u223c(P?,\u03a0) ah\u22121\u223c\u03a0\n[ \u03c6h\u22121(sh\u22121, ah\u22121) (\u03c6h\u22121(sh\u22121, ah\u22121)) > ] .\nFurther, let fh\u22121(sh\u22121, ah\u22121) be the total variation between P ?h\u22121 and Ph\u22121 at time step h \u2212 1. Suppose g \u2208 S \u00d7A \u2192 R is bounded by B \u2208 (0,\u221e), i.e., \u2016g\u2016\u221e \u2264 B. Then, \u2200h \u2265 2,\u2200 policy \u03c0h,\nE sh\u223cPh\u22121 ah\u223c\u03c0h [g(sh, ah)|sh\u22121, ah\u22121]\n\u2264 \u2016\u03c6h\u22121(sh\u22121, ah\u22121)\u2016(Mh\u22121,\u03c6)\u22121 \u00d7\u221a nK E\nsh\u223c(P?,\u03a0) ah\u223cU [g2(sh, ah)] + \u03bbndB2 + nB2 E sh\u22121\u223c(P?,\u03a0) ah\u22121\u223c\u03a0 [fh\u22121(sh\u22121, ah\u22121)2].\nProof. We first derive the following bound:\nE sh\u223cPh\u22121 ah\u223c\u03c0h [g(sh, ah)|sh\u22121, ah\u22121]\n= \u222b sh \u2211 ah g(sh, ah)\u03c0(ah|sh)\u3008\u03c6h\u22121(sh\u22121, ah\u22121), \u00b5h\u22121(sh)\u3009dsh\n\u2264 \u2016\u03c6h\u22121(sh\u22121, ah\u22121)\u2016(Mh\u22121,\u03c6)\u22121 \u2225\u2225\u2225\u2225\u2225 \u222b \u2211\nah\ng(sh, ah)\u03c0(ah|sh)\u00b5h\u22121(sh)dsh \u2225\u2225\u2225\u2225\u2225 Mh\u22121,\u03c6 ,\nwhere the inequality follows from Cauchy-Schwarz inequality. We further expand the second term in the RHS of the above inequality as follows.\n\u2225\u2225\u2225\u2225\u2225 \u222b \u2211\nah\ng(sh, ah)\u03c0(ah|sh)\u00b5h\u22121(sh)dsh \u2225\u2225\u2225\u2225\u2225 2\nMh\u22121,\u03c6\n(i) \u2264 n E sh\u22121\u223c(P?,\u03a0) ah\u22121\u223c\u03a0 (\u222b sh \u2211 ah g(sh, ah)\u03c0h(ah|sh)\u00b5(sh)>\u03c6(sh\u22121, ah\u22121)dsh )2+ \u03bbndB2\n= n E sh\u22121\u223c(P?,\u03a0) ah\u22121\u223c\u03a0\n  E\nsh\u223cPh\u22121 ah\u223c\u03c0h\n[ g(sh, ah) \u2223\u2223\u2223\u2223sh\u22121, ah\u22121] 2 + \u03bbndB2\n(ii)\n\u2264 2n E sh\u22121\u223c(P?,\u03a0) ah\u22121\u223c\u03a0\n E sh\u223cP ? h\u22121\nah\u223c\u03c0h\n[ g(sh, ah) \u2223\u2223\u2223\u2223sh\u22121, ah\u22121]2 + \u03bbndB2\n+ 2nB2 E sh\u22121\u223c(P?,\u03a0) ah\u22121\u223c\u03a0 [fh\u22121(sh\u22121, ah\u22121)] 2\n(iii)\n\u2264 2n E sh\u22121\u223c(P?,\u03a0) ah\u22121\u223c\u03a0\n E sh\u223cP ? h\u22121\nah\u223c\u03c0h\n[ g(sh, ah) 2 \u2223\u2223\u2223\u2223sh\u22121, ah\u22121] + \u03bbndB2\n+ 2nB2 E sh\u22121\u223c(P?,\u03a0) ah\u22121\u223c\u03a0\n[ fh\u22121(sh\u22121, ah\u22121) 2 ]\n(iv)\n\u2264 2nK E sh\u223c(P?,\u03a0) ah\u223cU\n[ g(sh, ah) 2 ] + \u03bbndB 2 + 2nB2 E\nsh\u22121\u223c(P?,\u03a0) ah\u22121\u223c\u03a0\n[ fh\u22121(sh\u22121, ah\u22121) 2 ] ,\nwhere (i) follows from the assumption that \u2016g\u2016\u221e \u2264 B, (ii) is due to that fh\u22121(sh\u22121, ah\u22121) is the total variation between P ?h\u22121 and Ph\u22121 at time step h \u2212 1 and the fact that (a + b)2 \u2264 2a2 + 2b2, (iii) follows from Jensen\u2019s inequality, and (iv) is due to importance sampling. This finishes the proof.\nBased on Lemma 2, we summarize three useful inequalities which bridges the total variation f (n)h and the exploration-driven reward b\u0302(n)h .\nLemma 3. Define\nW (n) h,\u03c6 = n E\nsh\u223c(P?,\u03a0n) ah\u223c\u03a0n\n[ \u03c6(sh, ah)(\u03c6(sh, ah)) >]+ \u03bbnI, (10)\nwhere \u03bbn = \u03b23d log(2nH|\u03a6|/\u03b4). Given that the event E occurs, the following inequalities hold. For any n, when h \u2265 2,\nE sh\u223cP\u0302 (n) h\u22121\nah\u223c\u03c0\n[ f\n(n) h (sh, ah) \u2223\u2223\u2223\u2223sh\u22121, ah\u22121] \u2264 \u03b1n \u2225\u2225\u2225\u03c6\u0302(n)h\u22121(sh\u22121, ah\u22121)\u2225\u2225\u2225 (U (n)\nh\u22121,\u03c6\u0302 )\u22121\n, (11)\nE sh\u223cP \u2217 h\u22121\nah\u223c\u03c0\n[ f\n(n) h (sh, ah) \u2223\u2223\u2223\u2223sh\u22121, ah\u22121] \u2264 \u03b1n \u2225\u2225\u03c6\u2217h\u22121(sh\u22121, ah\u22121)\u2225\u2225(U(n) h\u22121,\u03c6? ) \u22121 , (12)\nE sh\u223cP \u2217 h\u22121\nah\u223c\u03c0\n[ b\u0302 (n) h (sh, ah) \u2223\u2223\u2223\u2223sh\u22121, ah\u22121] \u2264 \u03b3n \u2225\u2225\u03c6\u2217h\u22121(sh\u22121, ah\u22121)\u2225\u2225(W (n) h\u22121,\u03c6? ) \u22121 , (13)\nwhere\n\u03b1n = \u221a 2\u03b23n\u03b6n(K + d2), \u03b3n = \u221a 45\u03b23n\u03b6nKd(K + d2)."
        },
        {
            "heading": "Specially, when h = 1,",
            "text": "E a1\u223c\u03c0\n[ f\n(n) 1 (s1, a1) ] \u2264 \u221a K\u03b6n, E\na1\u223c\u03c0\n[ b\u0302(s1, a1) ] \u2264 15\u03b1n \u221a dK\nn . (14)\nProof. We start by developing Equation (11) as follows. Given that the event E occurs, for h \u2265 2 we have\nE sh\u223cP\u0302 (n) h\u22121\nah\u223c\u03c0\n[ f\n(n) h (sh, ah) \u2223\u2223\u2223\u2223sh\u22121, ah\u22121] (i)\n\u2264 \u2225\u2225\u2225\u03c6\u0302(n)h\u22121(sh\u22121, ah\u22121)\u2225\u2225\u2225\n(U (n) h\u22121,\u03c6\u0302 )\u22121 \u00d7\u221a\u221a\u221a\u221a\u221anK Esh\u22121\u223c(P?,\u03a0n)\nah\u22121,ah\u223cU sh\u223cP?h(\u00b7|sh\u22121,ah\u22121)\n[f (n) h (sh, ah) 2] + \u03bbnd+ n E sh\u22121\u223c(P?,\u03a0n)\nah\u22121\u223cU\n[ f\n(n) h\u22121(sh\u22121, ah\u22121)\n2 ]\n(ii) \u2264 \u2225\u2225\u2225\u03c6\u0302(n)h\u22121(sh\u22121, ah\u22121)\u2225\u2225\u2225\n(U (n) h\u22121,\u03c6\u0302 )\u22121 \u00d7\u221a\u221a\u221a\u221a\u221anK Esh\u22121\u223c(P?,\u03a0n)\nah\u22121,ah\u223cU sh\u223cP?h(\u00b7|sh\u22121,ah\u22121)\n[f (n) h (sh, ah) 2] + \u03bbnd+ nK E sh\u22122\u223c(P?,\u03a0n) ah\u22122,ah\u22121\u223cU\nsh\u22121\u223cP?h\u22121(\u00b7|sh\u22122,ah\u22122)\n[ f\n(n) h\u22121(sh\u22121, ah\u22121)\n2 ]\n(iii) \u2264 \u2225\u2225\u2225\u03c6\u0302(n)h\u22121(sh\u22121, ah\u22121)\u2225\u2225\u2225\n(U (n) h\u22121,\u03c6\u0302 )\u22121\n\u221a 2n\u03b6nK + \u03b23n\u03b6nd2\n\u2264 \u03b1n \u2225\u2225\u2225\u03c6\u0302(n)h\u22121(sh\u22121, ah\u22121)\u2225\u2225\u2225\n(U (n) h\u22121,\u03c6\u0302 )\u22121\n,\nwhere (i) follows from Lemma 2 and the fact that f (n)h (sh, ah) \u2264 1, (ii) follows from importance sampling at time step h\u2212 2, and (iii) follows from Lemma 1. Equation (12) follows from the arguments similar to the above.\nTo obtain Equation (13), we first apply Lemma 2 and obtain\nE sh\u223cP ? h\u22121\nah\u223c\u03c0n\n[ b\u0302 (n) h (sh, ah) \u2223\u2223\u2223\u2223sh\u22121, ah\u22121]\n\u2264 \u2225\u2225\u03c6?h\u22121(sh\u22121, ah\u22121)\u2225\u2225(W (n)\nh\u22121,\u03c6? ) \u22121\n\u221a nK E\nsh\u223c(P?,\u03a0n) ah\u223cU\n[{b\u0302(n)h (sh, ah)}2] + \u03bbnd,\nwhere we use the fact that b\u0302(n)h (sh, ah) \u2264 1. We further bound the term nE sh\u223c(P?,\u03a0n)\nah\u223cU [(b\u0302\n(n) h (sh, ah)) 2] as follows:\nn E sh\u223c(P?,\u03a0n)\nah\u223cU\n[( b\u0302 (n) h (sh, ah) )2]\n\u2264 n E sh\u223c(P?,\u03a0n)\nah\u223cU\n[ \u03b1\u03022n \u2225\u2225\u2225\u03c6\u0302(n)h (sh, ah)\u2225\u2225\u22252 (U\u0302 (n)\nh,\u03c6\u0302 )\u22121 ] (i) \u2264 n E sh\u223c(P?,\u03a0n)\nah\u223cU\n[ 9\u03b1\u03022n \u2225\u2225\u2225\u03c6\u0302(n)h (sh, ah)\u2225\u2225\u22252 (U (n)\nh,\u03c6\u0302 )\u22121\n]\n= 9\u03b1\u03022ntr n Esh\u223c(P?,\u03a0n) ah\u223cU \u03c6\u0302(n)h (sh, ah)\u03c6\u0302(n)h (sh, ah)> n E sh\u223c(P?,\u03a0n) ah\u223cU [ \u03c6\u0302h(sh, ah)\u03c6\u0302 (n) h (sh, ah) > ] + \u03bbnI \u22121  \n\u2264 9\u03b1\u03022ntr(I) = 9\u03b1\u03022nd,\nwhere (i) follows from Lemma 1, and we use tr(A) to denote the trace of any matrix A.\nHence,\nE sh\u223cP \u2217 h\u22121\nah\u223c\u03c0\n[ b\u0302 (n) h (sh, ah) \u2223\u2223\u2223\u2223sh\u22121, ah\u22121] \u2264 \u2225\u2225\u03c6\u2217h\u22121(sh\u22121, ah\u22121)\u2225\u2225(W (n) h\u22121,\u03c6? ) \u22121 \u221a 9K\u03b1\u03022nd+ \u03bbnd\n\u2264 \u03b3n \u2225\u2225\u03c6\u2217h\u22121(sh\u22121, ah\u22121)\u2225\u2225W (n)\nh\u22121,\u03c6? ) \u22121 ,\nwhere the last inequality follows from that \u03b1\u0302n = 5\u03b1n and the definition of \u03b3n In addition, for h = 1, we have\nE a1\u223c\u03c0n\n[ f\n(n) 1 (s1, a1) ] (i) \u2264 \u221a K E\na1\u223cU\n[ f\n(n) 1 (s1, a1) 2 ] \u2264 \u221a K\u03b6n,\nand\nE a1\u223c\u03c0n\n[ b\u0302(s1, a1) ] (ii) \u2264 \u03b1\u0302n \u221a K E\na1\u223cU\n[ \u2016\u03c6\u03021(s1, a1)\u20162\n(U\u0302 (n) 1,\u03c6\u0302 )\u22121\n]\n\u2264 3\u03b1\u0302n \u221a K E\na1\u223cU\n[ \u2016\u03c6\u03021(s1, a1)\u20162\n(U (n) 1,\u03c6\u0302 )\u22121\n]\n\u2264 3 \u221a 25K\u03b12nd\nn = 15\u03b1n\n\u221a dK\nn ,"
        },
        {
            "heading": "A.2 PROOF OF PROPOSITION 1",
            "text": "Equipped with Lemma 3, the following proposition provides an upper bound on the difference of value functions under the estimated model P\u0302 (n) and the true model P ? for any given policy \u03c0 and reward r. Proposition 4 (Restatement of Proposition 1). For all n \u2208 [N ], policy \u03c0 and reward r, given that the event E occurs, we have \u2223\u2223\u2223V \u03c0P?,r \u2212 V \u03c0P\u0302 (n),r\u2223\u2223\u2223 \u2264 V\u0302 \u03c0P\u0302 (n),b\u0302(n) +\u221aK\u03b6n. Proof. Step 1. We first show that \u2223\u2223\u2223V \u03c0P?,r \u2212 V \u03c0P\u0302 (n),r\u2223\u2223\u2223 \u2264 V\u0302 \u03c0P\u0302 (n),f(n) .\nRecall the definition of estimated value functions V\u0302h,P\u0302 (n),r(sh) and Q\u0302h,P\u0302 (n),r(sh, ah):\nQ\u0302\u03c0 h,P\u0302 (n),r\n(sh, ah) = min { 1, rh(sh, ah) + P\u0302 (n) h V\u0302 \u03c0 h+1,P\u0302 (n),r (sh, ah) } ,\nV\u0302 \u03c0 h,P\u0302 (n),r (sh) = E \u03c0 [ Q\u0302\u03c0 h,P\u0302 (n),r (sh, ah) ] .\nWe develop the proof by induction. For the base case h = H + 1, we have\u2223\u2223\u2223V \u03c0 H+1,P\u0302 (n),r (sH+1)\u2212 V \u03c0H+1,P?,r(sH+1) \u2223\u2223\u2223 = 0 = V\u0302 \u03c0 H+1,P\u0302 (n),f(n) (sH+1).\nAssume that \u2223\u2223\u2223V \u03c0 h+1,P\u0302 (n),r (sh+1)\u2212 V \u03c0h+1,P?,r(sh+1) \u2223\u2223\u2223 \u2264 V\u0302 \u03c0 h+1,P\u0302 (n),f(n) (sh+1) holds for any sh+1.\nThen, from Bellman equation, we have,\u2223\u2223\u2223\u2223Q\u03c0h,P\u0302 (n),r(sh, ah)\u2212Q\u03c0h,P?,r(sh, ah)\u2223\u2223\u2223\u2223 =\n\u2223\u2223\u2223\u2223P\u0302 (n)h V \u03c0h,P\u0302 (n),r(sh, ah)\u2212 P ?hV \u03c0h+1,P?,r(sh, ah)\u2223\u2223\u2223\u2223 =\n\u2223\u2223\u2223\u2223P\u0302 (n)h (V \u03c0h+1,P\u0302 (n),r \u2212 V \u03c0h+1,P?,r) (sh, ah) + (P\u0302 (n)h \u2212 P ?h)V \u03c0h,P?,r(sh, ah)\u2223\u2223\u2223\u2223 (i)\n\u2264 min {\n1, f (n) h (sh, ah) + P\u0302 (n) h \u2223\u2223\u2223\u2223V \u03c0h+1,P\u0302 (n),r \u2212 V \u03c0h+1,P?,r\u2223\u2223\u2223\u2223(sh, ah)} (ii)\n\u2264 min {\n1, f (n) h (sh, ah) + P\u0302 (n) h V\u0302 \u03c0 h+1,P\u0302 (n),f(n) (sh, ah) } = Q\u0302\u03c0\nh,P\u0302 (n),f(n) (sh, ah), (15)\nwhere (i) follows from the (action) value function is at most 1, and (ii) follows from the induction hypothesis.\nThen, by the definition of V\u0302 \u03c0 h,P\u0302 (n),r (sh), we have\u2223\u2223\u2223\u2223V \u03c0h,P\u0302 (n),r(sh)\u2212 V \u03c0h,P?,r(sh)\u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223E\u03c0 [Q\u03c0h,P\u0302 (n),r(sh, ah)]\u2212 E\u03c0 [Q\u03c0h,P?,r(sh, ah)] \u2223\u2223\u2223\u2223\n\u2264 E \u03c0 [\u2223\u2223\u2223\u2223Q\u03c0h,P\u0302 (n),r(sh, ah)\u2212Q\u03c0h,P?,r(sh, ah)\u2223\u2223\u2223\u2223] (i) \u2264 E \u03c0 [ Q\u0302\u03c0 h,P\u0302 (n),f(n) (sh, ah) ]\n= V\u0302 \u03c0 h,P\u0302 (n),f(n) (sh),\nwhere (i) follows from Equation (15).\nTherefore, by induction, we have \u2223\u2223\u2223V \u03c0P?,r \u2212 V \u03c0P\u0302 (n),r\u2223\u2223\u2223 \u2264 V\u0302 \u03c0P\u0302 (n),f(n) . Step 2. Then, we show that V\u0302 \u03c0\nP\u0302 (n),f(n) \u2264 V\u0302 \u03c0 P\u0302 (n),b\u0302(n) + \u221a K\u03b6n.\nBy Equation (11) and the fact that the total variation distance is upper bounded by 1, with probability at least 1\u2212 \u03b4/2, we have\nE P\u0302 (n),\u03c0\n[ f\n(n) h (sh, ah) \u2223\u2223\u2223\u2223sh\u22121] \u2264 Eah\u22121\u223c\u03c0 [ min ( \u03b1n \u2225\u2225\u2225\u03c6\u0302(n)h\u22121(sh\u22121, ah\u22121)\u2225\u2225\u2225 (U (n)\nh\u22121,\u03c6\u0302 )\u22121\n, 1 )] ,\u2200h \u2265 2.\n(16)\nSimilarly, when h = 1,\nE a1\u223c\u03c0\n[ f\n(n) 1 (s1, a1)\n] \u2264 \u221a K E\na\u223cU\n[( f\n(n) 1 (s1, a1) )2] \u2264 \u221a K\u03b6n. (17)\nBased on Corollary 1, Equation (16) and \u03b1n = 5\u03b1\u0302n, we have\nE \u03c0\n[ b\u0302 (n) h (sh, ah) \u2223\u2223\u2223\u2223sh] \u2265 E\u03c0 [ min ( \u03b1n \u2225\u2225\u2225\u03c6\u0302(n)h (sh, ah)\u2225\u2225\u2225 (U (n)\nh,\u03c6\u0302 )\u22121\n, 1 )] \u2265 E P\u0302 (n),\u03c0 [ f (n) h+1(sh+1, ah+1) \u2223\u2223\u2223\u2223sh] . (18)\nFor the base case h = H , we have\nE P\u0302 (n),\u03c0\n[ V\u0302 \u03c0 H,P\u0302 (n),f(n) (sH) \u2223\u2223\u2223\u2223sH\u22121] = E P\u0302 (n),\u03c0 [ f (n) H (sH , aH) \u2223\u2223\u2223\u2223sH\u22121] \u2264 E\n\u03c0\n[ b (n) H\u22121(sH\u22121, aH\u22121)|sH\u22121 ] \u2264 min { 1,E \u03c0 [ Q\u0302\u03c0 H\u22121,P\u0302 (n),b\u0302(n)(sH\u22121, aH\u22121)\n\u2223\u2223\u2223\u2223sH\u22121]} = V\u0302 \u03c0\nH\u22121,P\u0302 (n),b\u0302(n)(sH\u22121).\nAssume that EP\u0302 (n),\u03c0 [ V\u0302 \u03c0 h+1,P\u0302 (n),f(n) (sh+1) \u2223\u2223\u2223\u2223sh] \u2264 V\u0302 \u03c0h,P\u0302 (n),b\u0302(n)(sh) holds for step h + 1. Then, by Jensen\u2019s inequality, we obtain\nE P\u0302 (n),\u03c0\n[ V\u0302 \u03c0 h,P\u0302 (n),f(n) (sh) \u2223\u2223\u2223\u2223sh\u22121]\n\u2264 min { 1, E P\u0302 (n),\u03c0 [ f (n) h (sh, ah) + P\u0302 (n) h V\u0302 \u03c0 h+1,P\u0302 (n),f(n) (sh, ah) \u2223\u2223\u2223\u2223sh\u22121] }\n(i) \u2264 min { 1,E \u03c0 [ b\u0302 (n) h\u22121(sh\u22121, ah\u22121) ] + E P\u0302 (n),\u03c0 [ E P\u0302 (n),\u03c0 [ V\u0302 \u03c0 h+1,P\u0302 (n),f(n) (sh+1) \u2223\u2223\u2223\u2223sh] \u2223\u2223\u2223\u2223sh\u22121 ]}\n(ii)\n\u2264 min { 1,E \u03c0 [ b (n) h\u22121(sh\u22121, ah\u22121) ] + E P\u0302 (n),\u03c0 [ V\u0302 \u03c0 h,P\u0302 (n),b\u0302(n) (sh) \u2223\u2223\u2223\u2223sh\u22121] }\n= min { 1,E \u03c0 [ Q\u0302\u03c0 h\u22121,P\u0302 (n),b\u0302(n)(sh\u22121, ah\u22121) ]} = V\u0302 \u03c0\nh\u22121,P\u0302 (n),b\u0302(n)(sh\u22121),\nwhere (i) follows from Equation (18), and (ii) is due to the induction hypothesis.\nBy induction, we conclude that\nV\u0302 \u03c0 P\u0302 (n),f(n) = E \u03c0\n[ f\n(s) 1 (s1, a1) ] + E P\u0302 (n),\u03c0 [ V\u0302 \u03c0 2,P\u0302 (n),f(n) (s2) \u2223\u2223\u2223\u2223s1] \u2264 \u221a K\u03b6n + V\u0302\n\u03c0 P\u0302 (n),b\u0302(n) .\nCombining Step 1 and Step 2, we conclude that\u2223\u2223\u2223V \u03c0P?,r \u2212 V \u03c0P\u0302 (n),r\u2223\u2223\u2223 \u2264\u221aK\u03b6n + V\u0302 \u03c0P\u0302 (n),b\u0302(n) ."
        },
        {
            "heading": "A.3 PROOF OF PROPOSITION 2",
            "text": "The following lemma is key to ensure that RAFFLE terminates in finite episodes.\nProposition 5 (Restatement of Proposition 2). Given that the event E occurs, \u03b6 = log (2|\u03a6||\u03a8|NH/\u03b4) the summation of the truncated value functions V\u0302 \u03c0n\nP\u0302 (n),b\u0302(n) under exploration\npolicies {\u03c0n}n\u2208[N ] is sublinear, i.e., the following bound holds:\u2211 n\u2208[N ] V\u0302 \u03c0n P\u0302 (n),b\u0302(n) + \u221a K\u03b6n \u2264 32\u03b6Hd \u221a \u03b23K(d2 +K)N.\nProof. Note that V\u0302 \u03c0 h,P\u0302 (n),b\u0302(n) \u2264 1 holds for any policy \u03c0 and h \u2208 [H]. We first have\nV\u0302 \u03c0n P\u0302 (n),b\u0302(n) \u2212 V \u03c0n P?,b\u0302(n) \u2264 E \u03c0n\n[ P\u0302\n(n) 1 V\u0302 \u03c0n 2,P\u0302 (n),b\u0302(n) (s1, a1)\u2212 P ?1 V \u03c0n 2,P?,b\u0302(n)\n(s1, a1) ]\n= E \u03c0n\n[( P\u0302\n(n) 1 \u2212 P ?1\n) V\u0302 \u03c0n\n2,P\u0302 (n),b\u0302(n) (s1, a1) + P\n? 1\n( V\u0302 \u03c0n\n2,P\u0302 (n),b\u0302(n) \u2212 V \u03c0n 2,P?,b\u0302(n)\n) (s1, a1) ] \u2264 E \u03c0n [ f (n) 1 (s1, a1) + P ? 1 ( V\u0302 \u03c0n 2,P\u0302 (n),b\u0302(n) \u2212 V \u03c0n 2,P?,b\u0302(n)\n)] \u2264 . . .\n\u2264 E(sh,ah)\u223c(P?,\u03c0n) [ H\u2211 h=1 f (n)(sh, ah) ] = V \u03c0n P?,f(n) ,\nwhich implies V\u0302 \u03c0n P\u0302 (n),b\u0302(n) \u2264 V \u03c0n P?,b\u0302(n) + V \u03c0n P?,f(n) .\nApplying the Equation (13) and Equation (14), we obtain the following bound on the value function V \u03c0n P?,b\u0302(n) :\nV \u03c0n P?,b\u0302(n) = H\u2211 h=1 E sh\u223c(P?,\u03c0n) ah\u223c\u03c0n [ b\u0302n(sh, ah) ]\n\u2264 H\u2211 h=2 E sh\u22121\u223c(P?,\u03c0n) ah\u22121\u223c\u03c0n [ \u03b3n \u2225\u2225\u03c6?h\u22121(sh\u22121, ah\u22121)\u2225\u2225(W (n) h\u22121,\u03c6? ) \u22121 ] + 15\u03b1n \u221a dK n\n\u2264 H\u2211 h=1 E sh\u223c(P?,\u03c0n) ah\u223c\u03c0n [ \u03b3n \u2016\u03c6?h(sh, ah)\u2016(W (n) h,\u03c6? )\u22121 ] + 15\u03b1n \u221a dK n .\nSimilarly, we obtain\nV \u03c0n P?,f(n) = H\u2211 h=1 E sh\u223c(P?,\u03c0n) ah\u223c\u03c0n [ f (n) h (sh, ah) ]\n\u2264 H\u2211 h=2 E sh\u22121\u223c(P?,\u03c0n) ah\u22121\u223c\u03c0n [ \u03b1n \u2225\u2225\u03c6?h\u22121(sh\u22121, ah\u22121)\u2225\u2225(U(n) h\u22121,\u03c6? ) \u22121 ] + \u221a K\u03b6n\n\u2264 H\u2211 h=1 E sh\u223c(P?,\u03c0n) ah\u223c\u03c0n [ \u03b1n \u2016\u03c6?h(sh, ah)\u2016(U(n) h,\u03c6? )\u22121 ] + \u221a K\u03b6n.\nThen, taking the summation of V \u03c0n P?,b\u0302(n)+f(n) over n \u2208 [N ], we have\u2211 n\u2208[N ] V \u03c0n P?,f(n)+b\u0302(n) + \u221a K\u03b6n\n\u2264 \u2211 n\u2208[N ] 15\u03b1n\n\u221a dK\nn + 2 \u2211 n\u2208[N ] \u221a K\u03b6n + \u2211 n\u2208[N ] H\u2211 h=1 E sh\u223c(P?,\u03c0n) ah\u223c\u03c0n [ \u03b3n \u2016\u03c6?h(sh, ah)\u2016(W (n) h,\u03c6? )\u22121 ]\n+ \u2211 n\u2208[N ] H\u2211 h=1 E sh\u223c(P?,\u03c0n) ah\u223c\u03c0n [ \u03b1n \u2016\u03c6?h(sh, ah)\u2016(U(n) h,\u03c6? )\u22121 ] (i)\n\u2264 17\u03b1N \u221a dKN + \u03b3N H\u2211 h=1 \u221a\u221a\u221a\u221aN \u2211 n\u2208[N ] E sh\u223c(P?,\u03c0n) ah\u223c\u03c0n [ \u2016\u03c6?h(sh, ah)\u2016 2 (W (n) h,\u03c6? )\u22121 ]\n+ \u03b1N H\u2211 h=1 \u221a\u221a\u221a\u221aKN \u2211 n\u2208[N ] E sh\u223c(P?,\u03c0n)\nah\u223cU\n[ \u2016\u03c6?h(sh, ah)\u2016 2 (U (n)\nh,\u03c6? )\u22121 ] (ii)\n\u2264 17 \u221a \u03b6 \u221a 2\u03b23dK(K + d2)N +H \u221a 45\u03b23\u03b6dK(K + d2) \u221a dN\u03b6\n+H \u221a \u03b23\u03b6(K + d2) \u221a dKN\u03b6\n\u2264 32\u03b6Hd \u221a \u03b23K(d2 +K)N,\nwhere (i) follows from Cauchy-Schwarz inequality and importance sampling, and (ii) follows from Lemma 10. Hence, the statement of Proposition 5 is verified."
        },
        {
            "heading": "A.4 PROOF OF PROPOSITION 3",
            "text": "Based on Proposition 5, we argue that with enough number of iterations, RAFFLE can find P\u0302 satisfying the condition in line 15 of Algorithm 1.\nProposition 6 (Restatement of Proposition 3). Fix any \u03b4 \u2208 (0, 1), > 0. Suppose the algorithm runs for N = 2 14\u03b23H 2d2K(d2+K) log2(2|\u03a6||\u03a8|H3d2K(d2+K)/(\u03b4 2))\n2 iterations, with probability at least 1\u2212\u03b4, RAFFLE can find an n \u2264 N in the exploration phase such that 2V\u0302\n\u03c0n P\u0302 (n ),b\u0302(n )\n+2 \u221a K\u03b6n \u2264 .\nIn other words, Algorithm 1 can output P\u0302 = P\u0302 (n ) satisfying the condition in line 15. In addition\u2223\u2223\u2223V \u03c0P?,r \u2212 V \u03c0P\u0302 (n ),r\u2223\u2223\u2223 \u2264 /2. Proof. We show that the algorithm terminates by contradiction. If it does not stop, applying Proposition 5, we have\nN/2 < \u2211 n\u2208[N ] ( V\u0302 \u03c0n P\u0302 (n),b\u0302(n) + \u221a K\u03b6n ) \u2264 32\u03b6Hd \u221a \u03b23K(d2 +K)N.\nTherefore,\nN < 212\u03b23H 2d2K(d2 +K)\u03b62\n2\nRecall \u03b6 = N\u03b6N = log (2|\u03a6||\u03a8|NH/\u03b4). Using the fact that n \u2264 c log2(\u03b1nn) \u21d2 n \u2264 4c log2(\u03b1nc),\u2200c \u2265 e2, n \u2265 1, \u03b1n \u2208 R+, it can be concluded that\nN < 214\u03b23H 2d2K(d2 +K) log2(2|\u03a6||\u03a8|H3d2K(d2 +K)/(\u03b4 2)) 2 ,\nwhich is a contradiction.\nTherefore, there exists an n = O( H2d2K(d2+K) log2(|\u03a6||\u03a8|H3d2K(d2+K)/(\u03b4 2)) 2 ) such that P\u0302 = P\u0302 (n ) satisfies\n2V\u0302 \u03c0n P\u0302 (n ),b\u0302(n )\n+ 2 \u221a K\u03b6n \u2264 .\nCombining Proposition 4, we finish the proof.\nProof of Theorem 2. Recall that P\u0302 is the output of RAFFLE in the n -iteration. Then, by Proposition 6\nV ?P?,r \u2212 V \u03c0\u0304P?,r \u2264 V \u03c0 ?\nP\u0302 ,r \u2212 V \u03c0\u0304P?,r + /2\n(i) \u2264 V \u03c0\u0304 P\u0302 ,r \u2212 V \u03c0\u0304P?,r + /2\n\u2264 /2 + /2 = ,\nwhere (i) follows from the definition of \u03c0\u0304. The number of trajectories n H is at at most\nO\n( H3d2K(d2 +K) log2(|\u03a6||\u03a8|H3d2K(d2 +K)/(\u03b4 2))\n2\n)"
        },
        {
            "heading": "B PROOF OF THEOREM 3",
            "text": "In this section, we adopt the same notations as in Appendix A. The following lemma provides an upper bound for the estimation error of any learned model from the true model.\nLemma 4. Fix \u03b4 \u2208 (0, 1), for any h \u2208 [H], n \u2208 N+, any policy \u03c0, with probability at least 1\u2212 \u03b4/2,\nE sh\u223c(P?,\u03c0) sh\u223c\u03c0\n[ f\n(n) h (sh, ah) ] \u2264 2 \u221a K\u03b6n + 2V\u0302 \u03c0n P\u0302 (n),b\u0302(n) .\nProof. Recall that f (n)h (s, a) = \u2225\u2225\u2225P\u0302 (n)h (\u00b7|s, a)\u2212 P ?h (\u00b7|s, a)\u2225\u2225\u2225\nTV . Fix any policy \u03c0, for any h \u2265 2,\nwe have\nE sh\u223c(P\u0302\n(n),\u03c0) ah\u223c\u03c0\n[ Q\u0302\u03c0 h,P\u0302 (n),b\u0302(n) (sh, ah) ]\n= E sh\u22121\u223c(P\u0302\n(n),\u03c0) ah\u22121\u223c\u03c0\n[ P\u0302\n(n) h V\u0302 \u03c0 h,P\u0302 (n),b\u0302(n)\n(sh\u22121, ah\u22121) ]\n\u2264 E sh\u22121\u223c(P\u0302\n(n),\u03c0) ah\u22121\u223c\u03c0\n[ min { 1, b\u0302\n(n) h\u22121(sh\u22121, ah\u22121) + P\u0302 (n) h\u22121V\u0302 \u03c0 h,P\u0302 (n),b\u0302(n)\n(sh\u22121, ah\u22121) }]\n= E sh\u22121\u223c(P\u0302\n(n),\u03c0) ah\u22121\u223c\u03c0\n[ Q\u0302\u03c0 h\u22121,P\u0302 (n),b\u0302(n)(sh\u22121, ah\u22121) ] \u2264 . . .\n\u2264 E a1\u223c\u03c0\n[ Q\u0302\u03c0\n1,P\u0302 (n),b\u0302(n) (s1, a1) ] = V\u0302 \u03c0\nP\u0302 (n),b\u0302(n) . (19)\nHence, for h \u2265 2, we have\nE sh\u223c(P\u0302\n(n),\u03c0) ah\u223c\u03c0\n[ f\n(n) h (sh, ah) ] (i) \u2264 E\nsh\u22121\u223c(P\u0302 (n),\u03c0)\nah\u22121\u223c\u03c0\n[ b\u0302 (n) h\u22121(sh\u22121, ah\u22121) ] (ii)\n\u2264 E sh\u22121\u223c(P\u0302\n(n),\u03c0) ah\u22121\u223c\u03c0\n[ Q\u0302\u03c0 h\u22121,P\u0302 (n),b\u0302(n)(sh\u22121, ah\u22121) ] (iii)\n\u2264 V\u0302 \u03c0 P\u0302 (n),b\u0302(n) , (20)\nwhere (i) follows from Equation (18), (ii) follows from the definition of Q\u0302\u03c0 h\u22121,P\u0302 (n),b\u0302(n)(sh\u22121, ah\u22121) and (iii) follows from Equation (19).\nE\u223c(P?,\u03c0) sh\u223c\u03c0\n[ f\n(n) h (sh,ah) ] \u2264 E\nsh\u223c(P\u0302 (n),\u03c0)\nah\u223c\u03c0\n[ f\n(n) h (sh, ah)\n] + \u2223\u2223\u2223\u2223\u2223\u2223 Esh\u223c(P?,\u03c0) ah\u223c\u03c0 [ f (n) h (sh, ah) ] \u2212 E sh\u223c(P\u0302 (n),\u03c0)\nah\u223c\u03c0\n[ f\n(n) h (sh, ah) ]\u2223\u2223\u2223\u2223\u2223\u2223 (i) \u2264 (V\u0302 \u03c0 P\u0302 (n),b\u0302(n) + \u221a K\u03b6n) + (\u221a K\u03b6n + V\u0302 \u03c0 P\u0302 (n),b\u0302(n)\n) (ii)\n\u2264 2 \u221a K\u03b6n + 2V\u0302\n\u03c0n P\u0302 (n),b\u0302(n) ,\nwhere the first term in (i) is due to Equation (20) and Equation (14), the second term in (i) is due to Proposition 4 and (ii) follows from the definition of \u03c0n. Proof of Theorem 3. By Proposition 6, let n = O ( H2d2K(d2+K) log2(|\u03a6||\u03a8|NH3d2K(K+d2)/(\u03b4 2))\n2\n) ,\nwith no more than n H trajectories, RAFFLE can learn a model P\u0302 , bonus b\u0302 and policy \u03c0 at the n -th iteration satisfying 2V \u03c0 P\u0302 ,b\u0302 + 2 \u221a K\u03b6n \u2264 . Then, following Lemma 4, we have\nE sh\u223c(P?,\u03c0) sh\u223c\u03c0\n[ \u2016P\u0302 h(\u00b7|sh, ah)\u2212 P ?h (\u00b7|sh, ah)\u2016TV ] \u2264 2 \u221a K\u03b6n + 2V \u03c0 P\u0302 ,b\u0302 \u2264 ."
        },
        {
            "heading": "C PROOF OF THEOREM 1 (LOWER BOUND ON SAMPLE COMPLEXITY)",
            "text": ""
        },
        {
            "heading": "C.1 STEP 1: CONSTRUCTION OF HARD MDP INSTANCES",
            "text": "Our hard MDP instances is inspired by Domingues et al. (2020) for tabular MDPs. However, the lower bound in Domingues et al. (2020) requires S \u2265 K, where S,K denote the cardinality of state and action space respectively. Our hard MDP instances remove the assumption that S \u2265 K by constructing the action set with two types of actions. The first type of actions is mainly used to form a large state space through a tree structure. The second type of actions is mainly used to distinguish different MDPs. Such a construction allows us to separately treat the state space and the action space, so that both state and action spaces can be arbitrarily large. We then explicitly define the feature vectors for all state-action pairs and show our hard MDP instances have a lowrank structure with dimension d = S. In a nutshell, we construct a family of HdK MDPs that are hard to distinguish in KL divergence, while the corresponding optimal policies are very different as shown in Figure 1.\nFirst, we define a reference MDPM0 as follows. We start with the construction of the state space S and the action space A.\n\u2022 Let S = {sw, so, sg, sb} \u22c3 i\u2208[D],j\u2208[2i\u22121]{si,j}, where sw, so, sg and sb denote \u2018wait-\ning state\u2019, \u2018outlier state\u2019, \u2018good state\u2019, and \u2018bad state\u2019, respectively. The states in\u22c3 i\u2208[D],j\u2208[2i\u22121]{si,j} form a binary tree, where si,j denotes the j-th branch node of the layer i.\n\u2022 Let A = {aw, a1, a2} \u22c3 A0, where aw denotes \u2018waiting action\u2019, a1, a2 are two unique\nactions that form the binary tree, and |A0| = K \u2212 3.\nThen, the transition probabilities ofM0 are specified through the following rules.\n\u2022 The initial state is the waiting state sw.\n\u2022 If the agent takes the waiting action aw before time step H\u0304 , waiting state sw stays on itself. Otherwise, sw transits to the root state s1,1 of the binary tree.\nMathematically, Ph[sw|sw, aw] = 1h\u2264H\u0304 , and Ph[s1,1|sw, a] = 1a6=aw or h>H\u0304 .\n\u2022 When i < D, for states si,j in the binary tree, we have the following transition rules:\n\u2013 If the agent takes actions a1 or a2, si,j deterministically transits to its children si+1,2j\u22121 or si+1,2j , respectively. Mathematically, Ph[si+1,2j\u22121|si,j , a1] = 1, and P[si+1,2j |si,j , a2] = 1.\n\u2013 If the agent takes any action other than a1 or a2, the agent will reach the outlier state so. Mathematically, Ph[so|si,j , a] = 1,\u2200a 6= a1, a2.\n\u2022 Leaf state sD,j uniformly transits to good state sg and bad state sb no matter what action the agent takes.\nMathematically, Ph[sg|sD,j , a] = Ph[sb|sD,j , a] = 12 ,\u2200a \u2208 A.\n\u2022 Good state sg , bad state sb, and outlier state so are absorbing states.\nNow, we define the features as follows, which are S-dimensional vectors.\nsw \u03c6h(sw, aw) = (1, 0,0S\u22125, 0, 0, 0) \u00b5h(sw) = (1h\u2264H\u0304 , 0,0S\u22125, 0, 0, 0) \u03c6h(sw, a) = (0, 1,0S\u22125, 0, 0), a 6= aw \u00b5h(s1,1) = (1h>H\u0304 , 1,0S\u22125, 0, 0, 0)\nsi,j , i < D \u03c6h(s i,j , a\u03c9) = (0, 0, ei+1,2j+\u03c9\u22122, 0, 0, 0), \u03c9 = 1, 2 \u00b5h(s k,`) = (0, 0, ek,`, 0, 0, 0), 1 < k \u2264 D \u03c6h(s i,j , a) = (0, 0,0S\u22125, 1, 0, 0), a 6= a1, a2 \u00b5h(so) = (0, 0,0S\u22125, 1, 0, 0)\nsD,j \u03c6h(s D,j , a) = (0, 0,0S\u22125, 0, 1 2 , 1 2 ) \u00b5h(sg) = (0, 0,0S\u22125, 0, 1, 0)\nsg, sb \u03c6h(sg, a) = \u00b5h(sg), \u03c6h(sb, a) = \u00b5h(sb) \u00b5h(sb) = (0, 0,0S\u22125, 0, 0, 1) so \u03c6h(so, a) = \u00b5h(so),\nwhere 0S\u22125 \u2208 RS\u22125 denotes the S \u2212 5 dimension vector with all zeros and ei,j \u2208 RS\u22125 denotes the one-hot vector that is zero everywhere except the (2i\u22121 + j \u2212 2)-th coordinate. Here 2 \u2264 i \u2264 D, 1 \u2264 j \u2264 2i\u22121. For each (h\u2217, `\u2217, a\u2217) \u2208 {1+D, . . . , H\u0304+D}\u00d7 [2D\u22121]\u00d7A, we define an MDPM(h\u2217,`\u2217,a\u2217) through M0. Specifically, the only difference ofM(h\u2217,`\u2217,a\u2217) fromM0 is that the transition probability from the leaf state sD,` \u2217 and action a\u2217 to the good state sg increases 0, i.e. Ph\u2217 [sg|sD,` \u2217 , a\u2217] = 12 + 0, and Ph\u2217 [sb|sD,` \u2217 , a\u2217] = 12 \u2212 0, where 0 will be specified later. We note that the features of M(h\u2217,`\u2217,a\u2217) are the same as those ofM0 except that \u03c6h\u2217(sD,` \u2217 , a\u2217) = (0, 0,0S\u22125, 0, 1 2 + 0, 1 2\u2212 0).\nWe remark here that the cardinalityK ofA can be arbitrarily large, so that the resulting lower bound will hold for both d \u2264 K and d \u2265 K regimes. In addition, although in our hard instances, d = S, it is straightforward to generalize it to the regime with S > d if we set the outlier state So to be a set of outlier states So. Definition of reward: the reward can only be attained in two special states: the good state sg and the outlier state so at the last stage H .\n\u2200S \u2208 A, a \u2208 A, rh(s, a) = 1{s=sg,h=H} + 1\n2 1{s=so,h=H},\nand rh(s, a) still belongs to [0, 1]."
        },
        {
            "heading": "C.2 STEP 2: ANALYSIS OF HARD MDP INSTANCES",
            "text": "Proof of Theorem 1. Let 0 = 2 .\nFor any MDPMh\u2217,a\u2217 , the optimal policy is to take action aw to stay at state sw until stage h\u2217 \u2212D, and then take the corresponding action to the only state sD,` \u2217 at stage h\u2217. At state sD,` \u2217 , the agent takes the only optimal action a\u2217. The optimal value function V \u2217M(h\u2217,`\u2217,a\u2217) = 1 2 + 0, and the value function of the output policy of Alg is given by\nV \u03c0\u0302\u03c4M(h\u2217,`\u2217,a\u2217) = 1 2 + 0P\u03c0\u0302\u03c4M(h\u2217,`\u2217,a\u2217) [sh\u2217 = s `\u2217 , ah\u2217 = a \u2217], (21)\nwhere P\u03c0\u0302\u03c4M(h\u2217,`\u2217,a\u2217) is the probability distribution over the states and actions (sh, ah) following the Markov policy \u03c0\u0302\u03c4 in the MDPM(h\u2217,`\u2217,a\u2217). We remark that the reward of outlier state are specially designed to be 1/2 at stage H to make Equation (21) hold for policy \u03c0\u0302\u03c4 falling into the outlier state.\nHence,\nV \u2217M(h\u2217,`\u2217,a\u2217) \u2212 V \u03c0\u0302\u03c4 M(h\u2217,`\u2217,a\u2217) = 0(1\u2212 P \u03c0\u0302\u03c4 M(h\u2217,`\u2217,a\u2217) [sh\u2217 = s `\u2217 , ah\u2217 = a \u2217])\n= 2 (1\u2212 P\u03c0\u0302\u03c4M(h\u2217,`\u2217,a\u2217) [sh\u2217 = s `\u2217 , ah\u2217 = a \u2217]).\nand\nV \u2217M(h\u2217,`\u2217,a\u2217) \u2212 V \u03c0\u0302\u03c4 M(h\u2217,`\u2217,a\u2217) \u2264 \u21d4 P \u03c0\u0302\u03c4 M(h\u2217,`\u2217,a\u2217) [sh\u2217 = s\n`\u2217 , ah\u2217 = a \u2217] \u2265 1\n2 . (22)\nThe transitions of all MDPs are the same when the leaves states are reached. We define the event\n\u03b5\u03c4(h\u2217,`\u2217,a\u2217) = { P\u03c0\u0302\u03c4M(h\u2217,`\u2217,a\u2217) [sh\u2217 = s `\u2217 , ah\u2217 = a \u2217] \u2265 1\n2\n} .\nFrom Equation (22), the event is equal to the event {V \u2217M(h\u2217,`\u2217,a\u2217) \u2212 V \u03c0\u0302\u03c4 M(h\u2217,`\u2217,a\u2217) \u2264 }. As a result, P(h\u2217,`\u2217,a\u2217) [ \u03b5\u03c4(h\u2217,`\u2217,a\u2217) ] = P(h\u2217,`\u2217,a\u2217) [ V \u2217M(h\u2217,`\u2217,a\u2217) \u2212 V \u03c0\u0302\u03c4 M(h\u2217,`\u2217,a\u2217) \u2264 ] \u2265 1\u2212 \u03b4.\nRecall that N\u03c4(h\u2217,`\u2217,a\u2217) = \u2211\u03c4 n=1 1{(snh\u2217 ,snh\u2217 )=(s`\u2217 ,a\u2217)} such that \u2211 (h\u2217,`\u2217,a\u2217)N \u03c4 (h\u2217,`\u2217,a\u2217) \u2264 \u03c4 . This inequality holds because the agent is likely to fall into the outlier state so. We denote P0 and E0 to be with respect toM0. Now, we invoke an intermediate result in the proof of Theorem 7 in Domingues et al. (2020) to conclude that\nE0 [ N\u03c4(h\u2217,`\u2217,a\u2217) ] \u2265 1\n16 2\n[( 1\u2212 P0 [ {\u03b5\u03c4(h\u2217,`\u2217,a\u2217)} ]) log ( 1\n\u03b4\n) \u2212 log(2) ] .\nSumming over all (h\u2217, `\u2217, a\u2217), we have E0[\u03c4 ] \u2265 \u2211\n(h\u2217,`\u2217,a\u2217)\nE0 [ N\u03c4(h\u2217,`\u2217,a\u2217) ]\n\u2265 1 16 2 H\u0304LK \u2212 \u2211 (h\u2217,`\u2217,a\u2217) P0 [ \u03b5\u03c4(h\u2217,`\u2217,a\u2217) ] log(1 \u03b4 )\u2212 H\u0304LK log 2  . (23) Notice that\u2211\n(h\u2217,`\u2217,a\u2217)\nP0 [ \u03b5\u03c4(h\u2217,`\u2217,a\u2217) ] = E0  \u2211 (h\u2217,`\u2217,a\u2217) 1{P\u03c0\u0302\u03c4M(h\u2217,`\u2217,a\u2217) [sh\u2217=s `\u2217 ,ah\u2217=a\u2217]\u2265 12}  \u2264 1. (24) Substituting Equation (24) into Equation (23) yields\nE0[\u03c4 ] \u2265 1\n16 2\n[( H\u0304LK \u2212 1 ) log( 1\n\u03b4 )\u2212 H\u0304LK log 2 ] \u2265 1\n32 2 H\u0304LK log(\n1 \u03b4 ),\nwhere we use the fact that \u03b4 < 1/16. With the assumption ofK \u2265 3, S \u2265 6, we have d = S. Taking H\u0304 = H3 and with the assumption of D \u2264 H/3, we have\nE0[\u03c4 ] = \u2126 ( HdK\n2 log(\n1 \u03b4 )\n) .\nThen following the analysis similar to that for Corollary 8 in Domingues et al. (2020), with probability at least 1\u2212 \u03b4, the number of iterarion is at least\n\u2126\n( HdK\n2 log(\n1 \u03b4 )\n) ."
        },
        {
            "heading": "D ALGORITHM 2: REPLEARN AND PROOF OF THEOREM 4",
            "text": "We first present the full algorithm in Section 6 below as Algorithm 2."
        },
        {
            "heading": "D.1 SUPPORTING LEMMAS",
            "text": "We first show that Q\u03c0 P\u0302 ,h,r can approximate Q\u03c0P?,h,r well over distribution {qh}h\u2208[H] by following two lemmas. Lemma 5. Given any \u03b4 \u2208 (0, 1). Let P\u0302 be the output of Algorithm 1, for any policy \u03c0 and rewards r, with probability at least 1\u2212 \u03b4, we have\nE(sh,ah)\u223c(P\u0302 ,\u03c0) [\u2223\u2223\u2223Q\u03c0P?,h,r(sh, ah)\u2212Q\u03c0P\u0302 ,h,r(sh, ah)\u2223\u2223\u2223]\u2264\nE(sh,ah)\u223c(P?,\u03c0) [\u2223\u2223\u2223Q\u03c0P?,h,r(sh, ah)\u2212Q\u03c0P\u0302 ,h,r(sh, ah)\u2223\u2223\u2223]\u2264 .\nAlgorithm 2 RepLearn: Representation Learning in Planning Phase of RAFFLE 1: Input: Sample size Nf , state-action pair distributions {qh}Hh=1, special designed reward func-\ntion {rh,t}h\u2208[H],t\u2208[T ] and policy {\u03c0t}t\u2208[T ], and estimated transition kernel P\u0302 from the output of Algorithm 1, model class: \u03a6.\n2: Initialize D0,0h = \u2205. 3: for n = 1, . . . , Nf do 4: for h = 1, . . . ,H do 5: for t = 1, . . . , T do 6: Choose (sn,th , a n,t h ) \u223c qh and add (s n,t h , a n,t h ) to dataset D n,t h = D n\u22121,t h \u222a (s n,t h , a n,t h ). 7: end for 8: end for 9: end for\n10: for h = 1, . . . ,H do 11: Learn (\u03c6\u0303h, w\u03031h, . . . , w\u0303 T h ) as in Equation (4). 12: end for 13: Output: \u03c6\u0303 = {\u03c6\u0303h}h\u2208[H].\nProof. Define f\u0302h(sh, ah) = \u2225\u2225\u2225P\u0302h(\u00b7|sh, ah)\u2212 P ?(\u00b7|sh, ah)\u2225\u2225\u2225\nTV and f\u0302 is a collection of all f\u0302h, i.e.{\nf\u0302h } h\u2208[H] .\nE(sh,ah)\u223c(P\u0302 ,\u03c0) [\u2223\u2223\u2223Q\u03c0P?,h,r(sh, ah)\u2212Q\u03c0P\u0302 ,h,r(sh, ah)\u2223\u2223\u2223]\n(i) \u2264 E(sh,ah)\u223c(P\u0302 ,\u03c0) [ Q\u0302\u03c0 h,P\u0302 ,f\u0302 (sh, ah) ]\n(ii)\n\u2264 V\u0302 \u03c0 P\u0302 ,f\u0302\n(iii)\n\u2264 /2, (25)\nwhere (i) follows from Equation (15), (ii) follows from the definition of V\u0302 and Q\u0302, and (iii) follows from the proof of Theorem 2.\nThen for the second inequality,\nE (sh,ah)\u223c(P?,\u03c0) [\u2223\u2223\u2223Q\u03c0P?,h,r(sh, ah)\u2212Q\u03c0P\u0302 ,h,r(sh, ah)\u2223\u2223\u2223] \u2264 E\n(sh,ah)\u223c(P\u0302 ,\u03c0) [\u2223\u2223\u2223Q\u03c0P?,h,r(sh, ah)\u2212Q\u03c0P\u0302 ,h,r(sh, ah)\u2223\u2223\u2223] + \u2223\u2223\u2223E(sh,ah)\u223c(P?,\u03c0)[\u2223\u2223\u2223Q\u03c0P?,h,r(sh,ah)\u2212Q\u03c0P\u0302 ,h,r(sh,ah)\u2223\u2223\u2223]\u2212E(sh,ah)\u223c(P\u0302 ,\u03c0)[\u2223\u2223\u2223Q\u03c0P?,h,r(sh,ah)\u2212Q\u03c0P\u0302 ,h,r(sh,ah)\u2223\u2223\u2223]\u2223\u2223\u2223\n(i) \u2264 /2 + V \u03c0 P\u0302 ,f\u0302\n(ii)\n\u2264 ,\nwhere the first term in (i) is due to Equation (25) and Equation (14), the second term in (i) is due to Step 1 in Proposition 4 and (ii) follows from the proof of Theorem 2.\nLemma 6. Given any \u03b4, \u2208 (0, 1) and the output of Algorithm 1, under Assumption 2, for any policy \u03c0 and rewards r, let the input distributions {qh}Hh=1 are bounded with constant CB . Denote Cmin = CB\u03b7min . Then with probability at least 1 \u2212 \u03b4, for each h \u2208 [H],\nE(sh,ah)\u223cqh [\u2223\u2223\u2223Q\u03c0P?,h,r(sh, ah)\u2212Q\u03c0P\u0302 ,h,r(sh, ah)\u2223\u2223\u2223] \u2264 Cmin.\nProof. First, together with Assumption 2, for any (s, a) \u2208 S \u00d7 A, we have qh(s, a) \u2264 CminP\u03c0 0 h (s, a), then\nE(sh,ah)\u223cqh [\u2223\u2223\u2223Q\u03c0P?,h,r(sh, ah)\u2212Q\u03c0P\u0302 ,h,r(sh, ah)\u2223\u2223\u2223]\n= \u222b qh(sh, ah) \u2223\u2223\u2223Q\u03c0P?,h,r(sh, ah)\u2212Q\u03c0P\u0302 ,h,r(sh, ah)\u2223\u2223\u2223 dshdah (i) = \u222b CminP\u03c0 0 h (sh) \u2223\u2223\u2223Q\u03c0P?,h,r(sh, ah)\u2212Q\u03c0P\u0302 ,h,r(sh, ah)\u2223\u2223\u2223 dshdah\n= Cmin E (sh,ah)\u223c(P?,\u03c00) [\u2223\u2223\u2223Q\u03c0P?,h,r(sh, ah)\u2212Q\u03c0P\u0302 ,h,r(sh, ah)\u2223\u2223\u2223] (ii)\n\u2264 Cmin,\nwhere (i) follows Assumption 2 and (ii) follows Lemma 5.\nThe lemma above shows that for any reward r, Q\u03c0 P\u0302 ,h,r (sh, ah) can be the target of Q\u03c0P?,h,r(sh, ah) when (sh, ah) are chosen from given distribution qh.\nWe then show that for any h, when reward r is set to be zero at step h, Q\u03c0P?,h,r has a linear structure w.r.t \u03c6?h.\nLemma 7. For any h \u2208 [H], policy \u03c0 and given (sh, ah) \u2208 S \u00d7 A, given any r such that r is set to be zero at step h, i.e. rh = 0, then Q\u03c0P?,h,r(sh, ah) is linear with respect to \u03c6\n?(sh, ah), i.e. there exist a w?h such that Q \u03c0 P?,h,r(sh, ah) = \u3008\u03c6?h(sh, ah), w?h\u3009."
        },
        {
            "heading": "Proof.",
            "text": "Q\u03c0P?,h,r(sh, ah) = rh(sh, ah) + Esh+1\u223cP?(\u00b7|sh,ah) [ V \u03c0P?,h+1,r(sh+1) \u2223\u2223sh, ah] = \u222b P ?(sh+1|sh, ah)V \u03c0P?,h+1,r(sh+1)dsh+1\n= \u2329 \u03c6?h(sh, ah), \u222b \u00b5?h(sh+1)V \u03c0 P?,h+1,r(sh+1)dsh+1 \u232a = \u3008\u03c6?h(sh, ah), w?h\u3009 ,\nwhere w?h = \u222b \u00b5?h(sh+1)V \u03c0 P?,h+1,r(sh+1)dsh+1."
        },
        {
            "heading": "D.2 PROOF OF THEOREM 4",
            "text": "Proof of Theorem 4. We allow \u03c6 and Q\u03c0 t\nP\u0302 ,h,rh,t to apply to all the samples in a dataset matrix si-\nmultaneously, i.e. \u03c6h(D Nf ,t h ) = (\u03c6h(s 1,t h ), . . . , \u03c6h(s Nf ,t h )) > \u2208 RNf\u00d7d and Q\u03c0t P\u0302 ,h,rh,t (DNf ,th ) = (Q\u03c0 t\nP\u0302 ,h,rh,t (s1,th ), . . . , Q \u03c0t P\u0302 ,h,rh,t (s Nf ,t h )) > \u2208 RNf . After we estimating \u03c6\u0303h and then taking it as known, from the property of linear regression in Equation (4), for any reward function rh,t, any policy \u03c0t we got\nw\u0303th = (\u03c6\u0303h(D Nf ,t h ) >\u03c6\u0303h(D Nf ,t h )) \u2020\u03c6\u0303h(D Nf ,t h ) >Q\u03c0 P\u0302 ,h,rh,t (DNf ,th )\n\u03c6\u0303h(D Nf ,t h )w\u0303 t h = P\u03c6\u0303h(D Nf ,t h ) Q\u03c0\nt\nP\u0302 ,h,rh,t (DNf ,th ),\nwhere P \u03c6\u0303h(D Nf ,t h ) = \u03c6\u0303h(D Nf ,t h )(\u03c6\u0303h(D Nf ,t h ) >\u03c6\u0303h(D Nf ,t h )) \u2020\u03c6\u0303h(D Nf ,t h ) >, represents the projection operator to the column spaces of \u03c6\u0303h(D Nf ,t h ). Then\n\u2211 t\u2208[T ] \u2225\u2225\u2225\u2225P\u22a5\u03c6\u0303h(DNf ,th )Q\u03c0tP\u0302 ,h,rh,t(DNf ,th ) \u2225\u2225\u2225\u22252\n= \u2211 t\u2208[T ] \u2225\u2225\u2225\u03c6\u0303h(DNf ,th )w\u0303th \u2212Q\u03c0tP\u0302 ,h,rh,t(DNf ,th )\u2225\u2225\u22252 (i)\n\u2264 \u2211 t\u2208[T ] \u2225\u2225\u2225\u03c6?h(DNf ,th )wth? \u2212Q\u03c0tP\u0302 ,h,rh,t(DNf ,th )\u2225\u2225\u22252 = \u2211 t\u2208[T ] Nf\u2211 n=1 ( Q\u03c0 t P?,h,rh,t(s n,t h , a n,t h )\u2212Q \u03c0t P\u0302 ,h,rh,t (sn,th , a n,t h ) )2\n(ii) \u2264 Nf \u2211 t\u2208[T ] E(sh,ah)\u223cqh [( Q\u03c0 t P?,h,rh,t(s n,t h , a n,t h )\u2212Q \u03c0t P\u0302 ,h,rh,t (sn,th , a n,t h ) )2] + \u221a TNf log 2 \u03b4 2\n(iii) \u2264 Nf \u2211 t\u2208[T ] E(sh,ah)\u223cqh [\u2223\u2223\u2223Q\u03c0tP?,h,rh,t(sn,th , an,th )\u2212Q\u03c0tP\u0302 ,h,rh,t(sn,th , an,th )\u2223\u2223\u2223]+\n\u221a TNf log 2 \u03b4\n2\n(iv)\n\u2264 CminNfT +\n\u221a TNf log 2 \u03b4\n2 , (26)\nwhere (i) follows from minimality of {\u03c6\u0303th}t\u2208[T ] and {w\u0303th}t\u2208[T ], (ii) follows Hoeffding\u2019s inequality, (iii) follows that \u2223\u2223\u2223Q\u03c0tP?,h,rh,t(sn,th , an,th )\u2212Q\u03c0tP\u0302 ,h,rh,t(sn,th , an,th )\u2223\u2223\u2223 \u2264 1 and (iv) follows Lemma 6. As a result:\n\u2211 t\u2208[T ] \u2225\u2225\u2225\u2225P\u22a5\u03c6\u0303h(DNf ,th )\u03c6?h(DNf ,th )wth? \u2225\u2225\u2225\u22252\n= \u2211 t\u2208[T ] \u2225\u2225\u2225\u2225P\u22a5\u03c6\u0303h(DNf ,th )Q\u03c0tP?,h,rh,t(DNf ,th ) \u2225\u2225\u2225\u22252 \u2264 \u2211 t\u2208[T ] {\u2225\u2225\u2225\u2225P\u22a5\u03c6\u0303h(DNf ,th )Q\u03c0tP\u0302 ,h,rh,t(DNf ,th ) \u2225\u2225\u2225\u22252 + \u2225\u2225\u2225\u2225P\u22a5\u03c6\u0303h(DNf ,th ) ( Q\u03c0 t P?,h,rh,t(D Nf ,t h )\u2212Q \u03c0 P\u0302 ,h,rh,t (DNf ,th ) )\u2225\u2225\u2225\u22252 }\n(i) \u2264 \u2211 t\u2208[T ] CminNfT +\n\u221a TNf log 2 \u03b4\n2 + \u2211 t\u2208[T ] \u03c321(P \u22a5 \u03c6\u0303h(D Nf ,t h ) )\u2016Q\u03c0 t P?,h,rh,t(D Nf ,t h ))\u2212Q \u03c0t P\u0302 ,h,rh,t (DNf ,th )\u2016 2\n(ii) \u2264 2 CminNfT + \u221a 2TNf log 2\n\u03b4 ,\nwhere (i) follows from \u2016Av\u20162 \u2264 \u03c31(A) \u2016v\u20162 and (ii) follows from that \u03c31(P\u22a5 \u03c6\u0303h(D Nf ,t h ) ) \u2264 1 and the process to derive Equation (26)."
        },
        {
            "heading": "METHODS SETTING SAMPLE COMPLEXITY",
            "text": "We finally use the technique in Du et al. (2021b) to derive the super population guarantee.\n2 CminNfT + \u221a 2TNf log 2\n\u03b4 \u2265 \u2211 t\u2208[T ] \u2225\u2225\u2225\u2225P\u22a5\u03c6\u0303h(DNf ,th )\u03c6?h(DNf ,th )wth? \u2225\u2225\u2225\u22252 F\n= \u2225\u2225\u2225\u2225(I \u2212 \u03c6\u0303h(DNf ,th )(\u03c6\u0303h(DNf ,th )>\u03c6\u0303h(DNf ,th ))\u2020 \u03c6\u0303h(DNf ,th )>)\u03c6?h(DNf ,th )wth?\u2225\u2225\u2225\u22252 F\n= \u2211 t\u2208[T ] (wth ? )>\u03c6?h(D Nf ,t h ) > ( I \u2212 \u03c6\u0303h(D Nf ,t h )(\u03c6\u0303h(D Nf ,t h ) >\u03c6\u0303h(D Nf ,t h )) \u2020\u03c6\u0303h(D Nf ,t h ) > ) \u03c6?h(D Nf ,t h )w t h ?\n= \u2211 t\u2208[T ] Nf (w t h ? )>D D Nf ,t h (\u03c6?h, \u03c6\u0303h)w t h ?\n(i) \u2265 0.9 \u2211 t\u2208[T ] Nf (w t h ? )>Dqh(\u03c6 ? h, \u03c6\u0303h)w t h ?\n= 0.9Nf \u2211 t\u2208[T ] \u2225\u2225\u2225Dqh(\u03c6?h, \u03c6\u0303h)1/2wth?\u2225\u2225\u22252 \u2265 0.9Nf \u2225\u2225\u2225Dqh(\u03c6?h, \u03c6\u0303h)1/2W ?h\u2225\u2225\u22252 F\n(ii) \u2265 0.9Nf \u2225\u2225\u2225Dqh(\u03c6?h, \u03c6\u0303h)1/2\u2225\u2225\u22252\nF \u03c32d(W ? h ) \u2265 0.9Nf \u2225\u2225\u2225Dqh(\u03c6?h, \u03c6\u0303h)1/2\u2225\u2225\u22252\nF\nCDT\nd ,\nwhere (i) follows from lemma B.1 in Du et al. (2021b) andNf can be large enough, and (ii) follows from that \u2016AB\u20162F \u2265 \u2016A\u2016 2 F \u03c3 2 min(B), where \u03c3min(B) denote the smallest singular value of matrix B. Then \u2225\u2225\u2225Dqh(\u03c6?h, \u03c6\u0303h)1/2\u2225\u2225\u22252 F \u2264 2 dCmin 0.9CD + d 0.9CD \u221a 2 log 2\u03b4 TNf ."
        },
        {
            "heading": "E MORE DISCUSSION ON RELATED WORK",
            "text": "In this section, we first summarize the directly comparable work in Appendix E."
        },
        {
            "heading": "E.1 LOW-RANK MDPS IN EXTENDED RL SETTINGS",
            "text": "Many studies have been developed on various extended low-rank models. Wang et al. (2022); Uehara et al. (2022a) studied partially observable Markov decision process (POMDP) with latent low-rank\nstructure. Zhan et al. (2022) studied predictive state representations model, and applied their results to POMDP with latent low-rank structure. Cheng et al. (2022); Agarwal et al. (2022) studied benefits of multitask representation learning under low-rank MDPs. Huang et al. (2022) proposed a general safe RL framework and instantiated it to low-rank MDPs. Ren et al. (2022) studied reward-known RL under low-rank MDPs and proposed a spectral method to replace the computation oracle. We note that even given those further developments, our results on lower bound and representation learning are still completely new, and our algorithm design and result on sample complexity are so far still the best-known result for standard reward-free low-rank MDPs."
        },
        {
            "heading": "E.2 DISCUSSION ON OPTIMISTIC MLE-BASED APPROACH FOR DIFFERENT SETTINGS",
            "text": "There is some concurrent work also using an optimistic MLE-based approach for different settings (POMDP) (Liu et al., 2022; Chen et al., 2022a). We elaborate the key differences between our paper and Liu et al. (2022); Chen et al. (2022a) as follows.\n\u2022 The two POMDP papers mentioned above study reward-known setting, whereas our focus here is on the reward-free setting.\n\u2022 Although optimistic MLE-based approach is used in both settings, the design of exploration policy in the two settings are different. Our algorithm identifies which estimated model is used for exploration policy design and then calculate the exploration policy based on bonus terms designed for the value function. However, the POMDP papers construct a confidence set about the true model and solve an optimization problem within this generic model set. Such an oracle may not be easy to realize computationally.\n\u2022 Due to the hardness of POMDP, MLE approach only guarantees that the estimated distribution of the trajectories is close to the true one. In contrast, in low-rank MDP we study here, we show that the estimation error for the transition probabilities is controlled at each time step."
        },
        {
            "heading": "F AUXILIARY LEMMAS",
            "text": "Recall f (n)h (s, a) = \u2016P\u0302 (n) h (\u00b7|s, a)\u2212 P ?h (\u00b7|s, a)\u2016TV represents the estimation error in the n-th iteration at step h, given state s and action a, in terms of the total variation distance. By using Theorem 21 in Agarwal et al. (2020), we are able to guarantee that under all exploration policies, the estimation error can be bounded with high probability. Lemma 8. (MLE guarantee). Given \u03b4 \u2208 (0, 1), we have the following inequality holds for any n, h \u2265 2 with probability at least 1\u2212 \u03b4/2:\nn\u22121\u2211 \u03c4=0 E sh\u22121\u223c(P?,\u03c0\u03c4 )\n(ah\u22121,ah)\u223cU(A) sh\u223cP?(\u00b7|sh\u22121,ah\u22121)\n[ fnh (sh, ah) 2 ] \u2264 n\u03b6n, where \u03b6n :=\nlog (2|\u03a6||\u03a8|nH/\u03b4) n .\nIn addition, for h = 1, n\u22121\u2211 \u03c4=0 E a1\u223cU(A) [ fn1 (s1, a1) 2 ] \u2264 n\u03b6n.\nDivide both sides of the result of lemma Lemma 8 by n, and define \u03a0n = U(\u03c01, . . . , \u03c0n\u22121), we have the following corollary which will be intensively used in the analysis. Corollary 2. Given \u03b4 \u2208 (0, 1), the following inequality holds for any n, h \u2265 2 with probability at least 1\u2212 \u03b4/2:\nE sh\u22121\u223c(P?,\u03a0n) ah\u22121,ah\u223cU(A)\nsh\u223cP?(\u00b7|sh\u22121,ah\u22121)\n[ fnh (sh, ah) 2 ] \u2264 \u03b6n.\nIn addition, for h = 1,\nE a1\u223cU(A)\n[ fn1 (s1, a1) 2 ] \u2264 \u03b6n.\nThe following lemma (Dann et al., 2017) will be useful to measure the difference between two value functions under two MDPs and reward functions. We define PhVh+1(sh, ah) = Es\u223cPh(\u00b7|sh,ah) [V (s)] for shorthand. Lemma 9. (Simulation lemma). Suppose P1 and P2 are two MDPs and r1, r2 are the corresponding reward functions. Given a policy \u03c0, we have,\nV \u03c0h,P1,r1(sh)\u2212 V \u03c0 h,P2,r2(sh)\n= H\u2211 h\u2032=h E s h\u2032\u223c(P2,\u03c0) a h\u2032\u223c\u03c0 [ r1(sh\u2032 , ah\u2032)\u2212 r2(sh\u2032 , ah\u2032) + (P1,h\u2032 \u2212 P2,h\u2032)V \u03c0h\u2032+1,P1,r(sh\u2032 , ah\u2032)|sh ] =\nH\u2211 h\u2032=h E s h\u2032\u223c(P1,\u03c0) a h\u2032\u223c\u03c0 [ r1(sh\u2032 , ah\u2032)\u2212 r2(sh\u2032 , ah\u2032) + (P1,h\u2032 \u2212 P2,h\u2032)V \u03c0h\u2032+1,P2,r(sh\u2032 , ah\u2032)|sh ] .\nThe following lemma is a standard inequality in regret analysis for linear models in reinforcement learning (see Lemma G.2 in Agarwal et al. (2020) and Lemma 10 in Uehara et al. (2021)). Lemma 10. (Elliptical potential lemma). Consider a sequence of d\u00d7d positive semidefinite matrices X1, . . . , XN with tr(Xn) \u2264 1 for all n \u2208 [N ]. Define M0 = \u03bb0I and Mn = Mn\u22121 +Xn. Then\nN\u2211 n=1 tr ( XnM \u22121 n\u22121 ) \u2264 2 log det(MN )\u2212 2 log det(M0) \u2264 2d log ( 1 + N d\u03bb0 ) .\nIf we choose any subset of the set {XnM\u22121n\u22121}Nn=1, we can still get a sublinear summation as follows."
        }
    ],
    "year": 2023
}