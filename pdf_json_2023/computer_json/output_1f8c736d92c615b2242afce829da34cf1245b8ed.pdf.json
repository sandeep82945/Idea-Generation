{
    "abstractText": "Tasks in psychophysical tests can at times be repetitive and cause individuals to lose engagement during the test. To facilitate engagement, we propose the use of a humanoid NAO robot, named Sam, as an alternative interface for conducting psychophysical tests. Specifically, we aim to evaluate the performance of Sam as an auditory testing interface, given its potential limitations and technical differences, in comparison to the current laptop interface. We examine the results and durations of two voice perception tests, voice cue sensitivity and voice gender categorisation, obtained from both the conventionally used laptop interface and Sam. Both tests investigate the perception and use of two speaker-specific voice cues, fundamental frequency (F0) and vocal tract length (VTL), important for characterising voice gender. Responses are logged on the laptop using a connected mouse, and on Sam using the tactile sensors. Comparison of test results from both interfaces shows functional similarity between the interfaces and replicates findings from previous studies with similar tests. Comparison of test durations shows longer testing times with Sam, primarily due to longer processing times in comparison to the laptop, as well as other design limitations due to the implementation of the test on the robot. Despite the inherent constraints of the NAO robot, such as in sound quality, relatively long processing and testing times, and different methods of response logging, the NAO interface appears to facilitate collecting similar data to the current laptop interface, confirming its potential as an alternative psychophysical test interface for auditory perception tests.",
    "authors": [
        {
            "affiliations": [],
            "name": "Luke MeyerID"
        },
        {
            "affiliations": [],
            "name": "Laura Rachman"
        },
        {
            "affiliations": [],
            "name": "Gloria Araiza-Illan"
        },
        {
            "affiliations": [],
            "name": "Etienne Gaudrain"
        },
        {
            "affiliations": [],
            "name": "Deniz Ba\u015fkentID"
        }
    ],
    "id": "SP:3d72743f2ba17d2cceb5d17099cd5fb33a794444",
    "references": [
        {
            "authors": [
                "M Marge",
                "C Espy-Wilson",
                "NG Ward",
                "A Alwan",
                "Y Artzi",
                "M Bansal"
            ],
            "title": "Spoken language interaction with robots: Recommendations for future research",
            "venue": "Computer Speech & Language",
            "year": 2022
        },
        {
            "authors": [
                "PD Chatzoglou",
                "V Lazaraki",
                "SD Apostolidis",
                "AC. Gasteratos"
            ],
            "title": "Factors Affecting Acceptance of Social Robots Among Prospective Users",
            "venue": "Int J of Soc Robotics",
            "year": 2023
        },
        {
            "authors": [
                "T Nomura",
                "S. Takagi"
            ],
            "title": "Exploring effects of educational backgrounds and gender in human-robot interaction",
            "venue": "International Conference on User Science and Engineering (i-USEr)",
            "year": 2011
        },
        {
            "authors": [
                "T. Nomura"
            ],
            "title": "Robots and Gender",
            "venue": "Gender and the Genome",
            "year": 2017
        },
        {
            "authors": [
                "WT Fitch",
                "J. Giedd"
            ],
            "title": "Morphology and development of the human vocal tract: A study using magnetic resonance imaging",
            "venue": "The Journal of the Acoustical Society of America",
            "year": 1999
        },
        {
            "authors": [
                "Smith DRR",
                "Patterson RD"
            ],
            "title": "The interaction of glottal-pulse rate and vocal-tract length in judgements of speaker size, sex, and age",
            "venue": "The Journal of the Acoustical Society of America",
            "year": 2005
        },
        {
            "authors": [
                "CD Fuller",
                "E Gaudrain",
                "JN Clarke",
                "JJ Galvin",
                "Q-J Fu",
                "RH Free"
            ],
            "title": "Gender Categorization Is Abnormal in Cochlear Implant Users",
            "venue": "JARO",
            "year": 2014
        },
        {
            "authors": [
                "E Gaudrain",
                "D. Ba\u015fkent"
            ],
            "title": "Discrimination of Voice Pitch and Vocal-Tract Length in Cochlear Implant Users: Ear and Hearing",
            "venue": "PMID:",
            "year": 2018
        },
        {
            "authors": [
                "Skuk VG",
                "Schweinberger SR"
            ],
            "title": "Influences of Fundamental Frequency, Formant Frequencies, Aperiodicity, and Spectrum Level on the Perception of Voice Gender",
            "venue": "J Speech Lang Hear Res",
            "year": 2014
        },
        {
            "authors": [
                "T Koelewijn",
                "E Gaudrain",
                "T Tamati",
                "D. Ba\u015fkent"
            ],
            "title": "The effects of lexical content, acoustic and linguistic variability, and vocoding on voice cue perception",
            "venue": "The Journal of the Acoustical Society of America",
            "year": 2021
        },
        {
            "authors": [
                "N El Boghdady",
                "E Gaudrain",
                "D. Ba\u015fkent"
            ],
            "title": "Does good perception of vocal characteristics relate to better speech-on-speech intelligibility for cochlear implant users? The Journal of the Acoustical Society of America",
            "venue": "PMID:",
            "year": 2019
        },
        {
            "authors": [
                "L Nagels",
                "E Gaudrain",
                "D Vickers",
                "P Hendriks",
                "D. Ba\u015fkent"
            ],
            "title": "Development of voice perception is dissociated across gender cues in school-age children",
            "venue": "PMID:",
            "year": 1934
        },
        {
            "authors": [
                "K Seaborn",
                "NP Miyake",
                "P Pennefather",
                "M. Otake-Matsuura"
            ],
            "title": "Voice in Human\u2013Agent Interaction: A Survey",
            "venue": "ACM Comput Surv",
            "year": 2022
        },
        {
            "authors": [
                "D Ba\u015fkent",
                "E Gaudrain",
                "T Tamati",
                "A. Wagner"
            ],
            "title": "Perception and Psychoacoustics of Speech in Cochlear Implant Users",
            "venue": "Plural Publishing,",
            "year": 2016
        },
        {
            "authors": [
                "D Humble",
                "SR Schweinberger",
                "A Mayer",
                "TL Jesgarzewsky",
                "C Dobel",
                "R. Z\u00e4ske"
            ],
            "title": "The Jena Voice Learning and Memory Test (JVLMT): A standardized tool for assessing the ability to learn and recognize voices",
            "venue": "Behav Res Methods",
            "year": 2022
        },
        {
            "authors": [
                "C M\u00fchl",
                "O Sheil",
                "L Jarutyt\u0117",
                "PEG. Bestelmeyer"
            ],
            "title": "The Bangor Voice Matching Test: A standardized test for the assessment of voice perception ability",
            "venue": "Behav Res",
            "year": 2018
        },
        {
            "authors": [
                "ML Smith",
                "ML Cesana",
                "EK Farran",
                "A Karmiloff-Smith",
                "L. Ewing"
            ],
            "title": "A \u201cspoon full of sugar\u201d helps the medicine go down: How a participant friendly version of a psychophysics task significantly improves task engagement, performance and data quality in a typical adult sample",
            "venue": "Behav Res",
            "year": 2018
        },
        {
            "authors": [
                "FH Bess",
                "H Davis",
                "S Camarata",
                "BWY. Hornsby"
            ],
            "title": "Listening-Related Fatigue in Children With Unilateral Hearing Loss. Language, Speech, and Hearing Services in Schools",
            "year": 2020
        },
        {
            "authors": [
                "DE Hartley",
                "BA Wright",
                "SC Hogan",
                "DR. Moore"
            ],
            "title": "Age-related improvements in auditory backward and simultaneous masking in 6- to 10-year-old children",
            "venue": "J Speech Lang Hear Res",
            "year": 2000
        },
        {
            "authors": [
                "S Alhanbali",
                "P Dawes",
                "S Lloyd",
                "KJ. Munro"
            ],
            "title": "Self-Reported Listening-Related Effort and Fatigue in Hearing-Impaired Adults",
            "venue": "Ear and Hearing",
            "year": 2017
        },
        {
            "authors": [
                "CL Mackersie",
                "J Dewey",
                "LA. Guthrie"
            ],
            "title": "Effects of fundamental frequency and vocal-tract length cues on sentence segregation by listeners with hearing loss",
            "venue": "J Acoust Soc Am",
            "year": 2011
        },
        {
            "authors": [
                "Vongpaisal T",
                "Pichora-Fuller MK"
            ],
            "title": "Effect of age on F0 difference limen and concurrent vowel identification",
            "venue": "J Speech Lang Hear Res",
            "year": 2007
        },
        {
            "authors": [
                "EE Harding",
                "E Gaudrain",
                "IJ Hrycyk",
                "RL Harris",
                "B Tillmann",
                "B Maat"
            ],
            "title": "Musical Emotion Categorization with Vocoders of Varying Temporal and Spectral Content",
            "venue": "Trends in Hearing",
            "year": 2023
        },
        {
            "authors": [
                "Moore DR"
            ],
            "title": "Frequency discrimination in children: Perception, learning and attention",
            "venue": "Hearing Research",
            "year": 2008
        },
        {
            "authors": [
                "R Looije",
                "A van der Zalm",
                "MA Neerincx",
                "R-J. Beun"
            ],
            "title": "Help, I need some body the effect of embodiment on playful learning",
            "venue": "IEEE RO-MAN: The 21st IEEE International Symposium on Robot and Human Interactive Communication",
            "year": 2012
        },
        {
            "authors": [
                "D Kontogiorgos",
                "A Pereira",
                "J. Gustafson"
            ],
            "title": "Grounding behaviours with conversational interfaces: effects of embodiment and failures",
            "venue": "J Multimodal User Interfaces",
            "year": 2021
        },
        {
            "authors": [
                "CD Kidd",
                "C. Breazeal"
            ],
            "title": "Effect of a robot on user perceptions",
            "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat No04CH37566)",
            "year": 2004
        },
        {
            "authors": [
                "J Kennedy",
                "P Baxter",
                "T. Belpaeme"
            ],
            "title": "Comparing Robot Embodiments in a Guided Discovery Learning Interaction with Children",
            "venue": "Int J of Soc Robotics",
            "year": 2015
        },
        {
            "authors": [
                "WA Bainbridge",
                "J Hart",
                "ES Kim",
                "B. Scassellati"
            ],
            "title": "The effect of presence on human-robot interaction",
            "venue": "ROMAN 2008\u2014The 17th IEEE International Symposium on Robot and Human Interactive Communication",
            "year": 2008
        },
        {
            "authors": [
                "C. Bond"
            ],
            "title": "Social facilitation: A self-presentational view",
            "venue": "Journal of Personality and Social Psychology",
            "year": 1982
        },
        {
            "authors": [
                "H Song",
                "EI Barakova",
                "P Markopoulos",
                "J. Ham"
            ],
            "title": "Personalizing HRI in Musical Instrument Practicing: The Influence of Robot Roles (Evaluative Versus Nonevaluative) on the Child\u2019s Motivation for Children in Different Learning Stages",
            "venue": "Frontiers in Robotics and AI. 2021;",
            "year": 2021
        },
        {
            "authors": [
                "J de Wit",
                "E Krahmer",
                "P. Vogt"
            ],
            "title": "Introducing the NEMO-Lowlands iconic gesture dataset, collected through a gameful human\u2013robot interaction",
            "venue": "Behav Res",
            "year": 2021
        },
        {
            "authors": [
                "L Meyer",
                "G Araiza-Illan",
                "L Rachman",
                "E Gaudrain",
                "D. Ba\u015fkent"
            ],
            "title": "Perception of a Humanoid Robot as an Interface for Auditory Testing",
            "venue": "editors. Towards Autonomous Robotic Systems. Cham: Springer International Publishing;",
            "year": 2021
        },
        {
            "authors": [
                "A Andreeva",
                "A. Ioannou"
            ],
            "title": "Robot-Assisted Speech And Language Therapy For Children With Hearing Impairment",
            "venue": "Special Education and Speech & Language Therapy",
            "year": 2020
        },
        {
            "authors": [
                "S Ondas",
                "D Hladek",
                "M Pleva",
                "J Juhar",
                "E Kiktova",
                "J Zimmermann"
            ],
            "title": "Towards robot-assisted children speech audiometry",
            "venue": "IEEE International Conference on Cognitive Infocommunications (CogInfoCom)",
            "year": 2019
        },
        {
            "authors": [
                "Sandygulova A",
                "O\u2019Hare GMP"
            ],
            "title": "Children\u2019s Perception of Synthesized Voice: Robot\u2019s Gender, Age and Accent",
            "venue": "Social Robotics. Cham: Springer International Publishing;",
            "year": 2015
        },
        {
            "authors": [
                "A Amirova",
                "N Rakhymbayeva",
                "E Yadollahi",
                "A Sandygulova",
                "W. Johal"
            ],
            "title": "Years of Human-NAO Interaction Research: A Scoping Review",
            "venue": "Front Robot AI. 2021;",
            "year": 2021
        },
        {
            "authors": [
                "A Ioannou",
                "A. Andreva"
            ],
            "title": "Play and Learn with an Intelligent Robot: Enhancing the Therapy of HearingImpaired Children",
            "venue": "Cham: Springer International Publishing;",
            "year": 2019
        },
        {
            "authors": [
                "H Banaeian",
                "I. Gilanlioglu"
            ],
            "title": "Influence of the NAO robot as a teaching assistant on university students\u2019 vocabulary learning and attitudes",
            "venue": "Australasian Journal of Educational Technology",
            "year": 2021
        },
        {
            "authors": [
                "O Engwall",
                "J. Lopes"
            ],
            "title": "Interaction and collaboration in robot-assisted language learning for adults",
            "venue": "Computer Assisted Language Learning",
            "year": 2022
        },
        {
            "authors": [
                "G Kurtz",
                "D. Kohen-Vacs"
            ],
            "title": "Humanoid robot as a tutor in a team-based training activity",
            "venue": "Interactive Learning Environments",
            "year": 2022
        },
        {
            "authors": [
                "J Kanero",
                "C Oran\u00e7",
                "S Ko\u015fkulu",
                "GT Kumkale",
                "T G\u00f6ksun",
                "AC. K\u00fcntay"
            ],
            "title": "Are Tutor Robots for Everyone? The Influence of Attitudes, Anxiety, and Personality on Robot-Led Language Learning",
            "venue": "Int J of Soc Robotics",
            "year": 2022
        },
        {
            "authors": [
                "F Yuan",
                "E Klavon",
                "Z Liu",
                "RP Lopez",
                "X. Zhao"
            ],
            "title": "A Systematic Review of Robotic Rehabilitation for Cognitive Training",
            "venue": "Front Robot AI. 2021;",
            "year": 2021
        },
        {
            "authors": [
                "E Frid",
                "R Bresin",
                "S. Alexanderson"
            ],
            "title": "Perception of Mechanical Sounds Inherent to Expressive Gestures of a NAO Robot\u2014Implications for Movement Sonification of Humanoids",
            "year": 2018
        },
        {
            "authors": [
                "M Kohnen",
                "F Denk",
                "J Llorca-Bof\u0131",
                "M Vorl\u00e4nder",
                "B. Kollmeier"
            ],
            "title": "Loudness in different rooms versus headphone reproduction: Is there a mismatch even after careful equalization? 2019",
            "year": 2019
        },
        {
            "authors": [
                "Van Rossum G",
                "Drake FL"
            ],
            "title": "Python 3 Reference Manual",
            "venue": "Scotts Valley, CA: CreateSpace;",
            "year": 2009
        },
        {
            "authors": [
                "B. Stroustrup"
            ],
            "title": "The C++ programming language. Pearson Education India",
            "year": 2000
        },
        {
            "authors": [
                "C. Bartneck"
            ],
            "title": "Human-robot interaction: an introduction",
            "year": 2020
        },
        {
            "authors": [
                "Marriage JE",
                "Moore BCJ"
            ],
            "title": "New speech tests reveal benefit of wide dynamic-range, fast-acting compression for consonant discrimination in children with moderate-to-profound hearing loss",
            "venue": "International Journal of Audiology",
            "year": 2003
        },
        {
            "authors": [
                "DA Vickers",
                "BCJ Moore",
                "A Majeed",
                "N Stephenson",
                "H Alferaih",
                "T Baer"
            ],
            "title": "Closed-Set Speech Discrimination Tests for Assessing Young Children: Ear and Hearing",
            "venue": "PMID:",
            "year": 2018
        },
        {
            "authors": [
                "H Kawahara",
                "J Estill",
                "O. Fujimura"
            ],
            "title": "Aperiodicity extraction and control using mixed mode excitation and group delay manipulation for a high quality speech analysis, modification and synthesis system PLOS ONE Use of a humanoid robot for auditory psychophysical testing",
            "venue": "PLOS ONE | https://doi.org/10.1371/journal.pone.0294328 December",
            "year": 2023
        },
        {
            "authors": [
                "J. Hsu"
            ],
            "title": "PyWorldVocoder\u2014A Python wrapper for World Vocoder",
            "venue": "GitHub;",
            "year": 2020
        },
        {
            "authors": [
                "H. Levitt"
            ],
            "title": "Transformed Up-Down Methods in Psychoacoustics",
            "venue": "The Journal of the Acoustical Society of America",
            "year": 1971
        },
        {
            "authors": [
                "E-J Wagenmakers",
                "M Marsman",
                "T Jamil",
                "A Ly",
                "J Verhagen",
                "J Love"
            ],
            "title": "Bayesian inference for psychology. Part I: Theoretical advantages and practical ramifications",
            "venue": "Psychon Bull Rev",
            "year": 2018
        },
        {
            "authors": [
                "A Ortega",
                "G. Navarrete"
            ],
            "title": "Bayesian Hypothesis Testing: An Alternative to Null Hypothesis Significance Testing (NHST) in Psychology and Social Sciences",
            "venue": "Bayesian Inference",
            "year": 2017
        },
        {
            "authors": [
                "R R Core Team"
            ],
            "title": "A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing; 2020",
            "year": 2020
        },
        {
            "authors": [
                "L Meyer",
                "L Rachman",
                "G Araiza-Illan",
                "E Gaudrain",
                "D. Ba\u015fkent"
            ],
            "title": "A Humanoid Robot for Auditory Psychophysical Testing",
            "year": 2022
        },
        {
            "authors": [
                "M Mattamala",
                "G Olave",
                "C Gonz\u00e1lez",
                "N Hasb\u00fan",
                "J. Ruiz-del-Solar"
            ],
            "title": "The NAO Backpack: An Open-Hardware Add-on for Fast Software Development with the NAO Robot",
            "venue": "Robot World Cup XXI. Cham: Springer International Publishing;",
            "year": 2017
        },
        {
            "authors": [
                "K Bergmann",
                "F Eyssel",
                "S. Kopp"
            ],
            "title": "A Second Chance to Make a First Impression? How Appearance and Nonverbal Behavior Affect Perceived Warmth and Competence of Virtual Agents over Time",
            "year": 2012
        },
        {
            "authors": [
                "J Zlotowski",
                "H Sumioka",
                "S Nishio",
                "D Glas",
                "C Bartneck",
                "H. Ishiguro"
            ],
            "title": "Persistence of the uncanny valley: the influence of repeated interactions and a robot\u2019s attitude on its perception",
            "venue": "Frontiers in Psychology. 2015;",
            "year": 2015
        },
        {
            "authors": [
                "M Hope",
                "J. Lilley"
            ],
            "title": "Gender expansive listeners utilize a non-binary, multidimensional conception of gender to inform voice gender perception",
            "venue": "Brain and Language",
            "year": 2022
        },
        {
            "authors": [
                "G Hilkhuysen",
                "N Gaubitch",
                "M. Brookes"
            ],
            "title": "Effects of noise suppression on intelligibility: Dependency on signal-to-noise ratios",
            "venue": "The Journal of the Acoustical Society of America",
            "year": 2012
        },
        {
            "authors": [
                "J Bernotat",
                "F Eyssel",
                "J. Sachse"
            ],
            "title": "Shape It\u2013The Influence of Robot Body Shape on Gender Perception in Robots",
            "venue": "editors. Social Robotics. Cham: Springer International Publishing;",
            "year": 2017
        },
        {
            "authors": [
                "G Trovato",
                "C Lucho",
                "R. Paredes"
            ],
            "title": "She\u2019s Electric\u2014The Influence of Body Proportions on Perceived Gender of Robots across Cultures",
            "year": 2018
        },
        {
            "authors": [
                "M Okanda",
                "K. Taniguchi"
            ],
            "title": "Is a robot a boy? Japanese children\u2019s and adults\u2019 gender-attribute bias toward robots and its implications for education on gender stereotypes",
            "venue": "Cognitive Development",
            "year": 2021
        },
        {
            "authors": [
                "M Damholdt",
                "C Vestergaard",
                "J. Seibt"
            ],
            "title": "Ascribing Gender to a Social Robot",
            "venue": "Culturally Sustainable Social Robotics: Proceedings of Robophilosophy 2020 August 18\u201321,",
            "year": 2020
        },
        {
            "authors": [
                "F Eyssel",
                "F. Hegel"
            ],
            "title": "S)he\u2019s Got the Look: Gender Stereotyping of Robots",
            "venue": "Journal of Applied Social Psychology",
            "year": 2012
        },
        {
            "authors": [
                "S Yilmazyildiz",
                "G Athanasopoulos",
                "G Patsis",
                "W Wang",
                "MC Oveneke",
                "L Latacz"
            ],
            "title": "Voice Modification for Wizard-of-Oz Experiments in Robot-Child Interaction",
            "year": 2013
        },
        {
            "authors": [
                "Sandygulova A",
                "O\u2019Hare GM"
            ],
            "title": "Age-related Differences in Children\u2019s Associations and Preferences for a Robot\u2019s Gender",
            "venue": "Companion of the 2018 ACM/IEEE International Conference on Human-Robot Interaction. Chicago IL USA: ACM;",
            "year": 2018
        },
        {
            "authors": [
                "J Bernotat",
                "F Eyssel",
                "J. Sachse"
            ],
            "title": "The (Fe)male Robot: How Robot Body Shape Impacts First Impressions and Trust",
            "venue": "Towards Robots. International Journal of Social Robotics",
            "year": 2021
        },
        {
            "authors": [
                "Dou X",
                "Wu C-F",
                "Lin K-C",
                "Gan S",
                "Tseng T-M"
            ],
            "title": "Effects of Different Types of Social Robot Voices on Affective Evaluations in Different Application Fields",
            "venue": "Int J of Soc Robotics",
            "year": 2023
        },
        {
            "authors": [
                "M Feidakis",
                "I Gkolompia",
                "A Marnelaki",
                "K Marathaki",
                "S Emmanouilidou",
                "E. Agrianiti"
            ],
            "title": "NAO robot, an educational assistant in training, educational and therapeutic sessions",
            "venue": "IEEE Global Engineering Education Conference (EDUCON)",
            "year": 2023
        },
        {
            "authors": [
                "T Belpaeme",
                "J Kennedy",
                "A Ramachandran",
                "B Scassellati",
                "F. Tanaka"
            ],
            "title": "Social robots for education: A review",
            "venue": "Sci Robot",
            "year": 2018
        },
        {
            "authors": [
                "F Catania",
                "M Spitale",
                "F. Garzotto"
            ],
            "title": "Conversational Agents in Therapeutic Interventions for Neurodevelopmental Disorders: A Survey",
            "venue": "ACM Comput Surv",
            "year": 2023
        },
        {
            "authors": [
                "V Holeva",
                "VA Nikopoulou",
                "C Lytridis",
                "C Bazinas",
                "P Kechayas",
                "G Sidiropoulos"
            ],
            "title": "Effectiveness of a Robot-Assisted Psychological Intervention for Children with Autism Spectrum Disorder",
            "venue": "J Autism Dev Disord",
            "year": 2022
        },
        {
            "authors": [
                "I Torre",
                "AB Latupeirissa",
                "C. McGinn"
            ],
            "title": "How context shapes the appropriateness of a robot\u2019s voice. 2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)",
            "year": 2020
        },
        {
            "authors": [
                "C McGinn",
                "I. Torre"
            ],
            "title": "Can you Tell the Robot by the Voice? An Exploratory Study on the Role of Voice in the Perception of Robots",
            "venue": "14th ACM/IEEE International Conference on Human-Robot Interaction (HRI)",
            "year": 2019
        },
        {
            "authors": [
                "C Burgers",
                "A Eden",
                "MD van Engelenburg",
                "S. Buningh"
            ],
            "title": "How feedback boosts motivation and play in a brain-training game",
            "venue": "Computers in Human Behavior",
            "year": 2015
        },
        {
            "authors": [
                "D Povey",
                "A Ghoshal",
                "G Boulianne",
                "L Burget",
                "O Glembek",
                "N Goel"
            ],
            "title": "The kaldi speech recognition toolkit. IEEE 2011 workshop on automatic speech recognition and understanding. Hilton Waikoloa Village, Big Island, Hawaii, US",
            "venue": "IEEE Signal Processing Society;",
            "year": 2011
        }
    ],
    "sections": [
        {
            "text": "Tasks in psychophysical tests can at times be repetitive and cause individuals to lose\nengagement during the test. To facilitate engagement, we propose the use of a humanoid\nNAO robot, named Sam, as an alternative interface for conducting psychophysical tests.\nSpecifically, we aim to evaluate the performance of Sam as an auditory testing interface,\ngiven its potential limitations and technical differences, in comparison to the current laptop\ninterface. We examine the results and durations of two voice perception tests, voice cue\nsensitivity and voice gender categorisation, obtained from both the conventionally used lap-\ntop interface and Sam. Both tests investigate the perception and use of two speaker-specific\nvoice cues, fundamental frequency (F0) and vocal tract length (VTL), important for charac-\nterising voice gender. Responses are logged on the laptop using a connected mouse, and\non Sam using the tactile sensors. Comparison of test results from both interfaces shows\nfunctional similarity between the interfaces and replicates findings from previous studies\nwith similar tests. Comparison of test durations shows longer testing times with Sam, primar-\nily due to longer processing times in comparison to the laptop, as well as other design limita-\ntions due to the implementation of the test on the robot. Despite the inherent constraints of\nthe NAO robot, such as in sound quality, relatively long processing and testing times, and\ndifferent methods of response logging, the NAO interface appears to facilitate collecting sim-\nilar data to the current laptop interface, confirming its potential as an alternative psychophys-\nical test interface for auditory perception tests."
        },
        {
            "heading": "Introduction",
            "text": "In social robotics, the main mode of communication with humans is speech [1]. In this study, we take advantage of the speech and communication tools on a low-cost humanoid robot, NAO, to conduct psychophysical tests on voice perception. From this implementation, three points of discussion can be derived: can a humanoid robot be used as an alternative interface to a computer implementation for psychophysical testing?, what is the perception people have\nPLOS ONE | https://doi.org/10.1371/journal.pone.0294328 December 13, 2023 1 / 27\na1111111111\nOPEN ACCESS\nCitation: Meyer L, Rachman L, Araiza-Illan G, Gaudrain E, Ba\u015fkent D (2023) Use of a humanoid robot for auditory psychophysical testing. PLoS ONE 18(12): e0294328. https://doi.org/10.1371/ journal.pone.0294328\nEditor: Domna Banakou, New York University Abu Dhabi, UNITED ARAB EMIRATES\nReceived: April 7, 2023\nAccepted: October 31, 2023\nPublished: December 13, 2023\nCopyright: \u00a9 2023 Meyer et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.\nData Availability Statement: All processed data files are available from the DataverseNL repository, DOI: https://doi.org/10.34894/IAGXVF.\nFunding: D. Baskent: VICI Grant from the Netherlands Organization for Scientific Research (NWO) and the Netherlands Organization for Health Research and Development (ZonMw) (Grant No. 918-17-603) \u2013 https://www.nwo.nl/en; The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript. L. Meyer, L. Rachman, G. Araiza-Illan, D. Baskent: W.J. Kolff Institute, University Medical\ntowards having a social robotic agent as an alternative interface and conduct clinical tests?, and how does the presentation of different voices, either natural or synthesised, potentially influence communication between a human and robot? The factors that influence these different components are vast; e.g., personality, education [2], background [3], gender [4] etc., and while these factors may have multiple interrelated connections between them, the focus of the current manuscript is on the first component; can a humanoid NAO robot be effectively used as a psychophysical testing interface?\nSpeaker-specific voice cues, such as fundamental frequency (F0), related to the perceived\npitch of a voice and determined by glottal pulse rate, and vocal-tract length (VTL), related to the speech spectral profile and the size of the talker [5, 6], are two key characteristics in differentiating voices and identifying a speaker\u2019s voice gender [7\u20139]. Individuals with normal hearing are sensitive to voice cues, and can perceive very small differences, around 1\u20132 semitones [8, 10\u201312]. In multi-talker situations, the categorisation of voice based on gender may assist one in identifying and focussing on a voice, especially during simultaneous talking [7]. Furthermore, the categorisation of voice gender is not only important from a medical perspective, such as in the advancing and developing of devices utilised by hard-of-hearing individuals, but also in the context of social robot implementations where the perceived gender of either a natural or synthesised voice presented by a robot can influence its effectiveness, depending on its application. This is explored in an extensive meta-analysis by [13]. When categorising the perceived gender of a voice, normal-hearing listeners use both voice cues effectively [7, 8, 11, 14]. In contrast, hard-of-hearing users of auditory prosthetic devices; e.g, cochlear implants, have difficulty differentiating voice cues [8] and seem to rely heavily on F0 differences for voice gender categorisation while being unable to make use of VTL differences [7]. This example indicates much is still to be uncovered regarding the perception of voice and speech, especially with hearing devices, to fully understand the abilities and limitations of voice perception, and to accordingly improve performance of augmentative devices, such as hearing aids and cochlear implants.\nIn investigating voice and speech perception, the psychophysical tests are often long and\nrepetitive to ensure data reliability [15\u201317]. Establishing and maintaining engagement and focus during such studies can be a challenge for all participants, but also especially for individuals with relatively short attention spans such as children [18, 19], or listeners with limited hearing abilities, who also often happen to be older individuals [20]. These populations are often understudied for voice perception, perhaps partially due to such challenges. Voice perception in children and younger and older adults with hearing loss has mostly recently started to be investigated [12, 21, 22].\nThe standard setup for auditory psychophysical tests involves a computer interface used for both the presentation of stimuli and collection of responses, which can be recorded in audio or through a simple interaction involving mouse clicks, keyboard entry or similar. To help with potential attention and engagement issues and to maintain good quality of test results, computer interfaces are at times further modified to include cartoons, cartoon characters, or animations [12, 23, 24]. In this study, as a new alternative psychophysical test interface to the conventional computer interface, we propose a humanoid NAO robot, which could potentially result in the collection of sufficiently reliable measurements of perceptual performance while doing so in an engaging manner. As suggested by [1], speech with a robot can be advantageous in motivating and engaging users. Further supporting this, [25] have shown that between a humanoid robot and a computer, the robot was better at retaining the attention of children during learning tasks. The literature also shows that the physical embodiment of an agent is preferred over its virtual counterpart, and contributes favourably to its social presence [26\u2013 29], potentially motivating interactors to exert more effort on a given task [30, 31]. Moreover,\nPLOS ONE | https://doi.org/10.1371/journal.pone.0294328 December 13, 2023 2 / 27\nCentre Groningen (no grant number) \u2013 https:// umcgresearch.org/w/wjkolff; The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript. L. Meyer, L. Rachman, G. Araiza-Illan, E. Gaudrain, D. Baskent: Heinsius-Houbolt Foundation (No grant number); The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.\nCompeting interests: The authors have declared that no competing interests exist.\nthe NAO robot has been used previously as an entertaining interface for the maintaining of engagement during game-like activities for both children and adults [32]. However, a test interface, regardless of its engagement potential, is not entirely useful if it does not produce reliable data. As a first step towards the use of the robot as an auditory test interface, in this study, we will be investigating the reliability aspect of the data collected via the NAO robot, and in a separate study, the engagement based on human-robot interaction will be addressed [33].\nIn the field of social robotics, one of the most frequently used humanoid robots is the NAO\nfrom SoftBank Robotics. The use of the NAO has been suggested in the literature to facilitate testing procedures in hearing research [34\u201336]. Especially in human-robot interactions, the robot\u2019s relatively small size, its friendly and human-like appearance, and its sociable and nonjudgemental characteristics seem to be helpful [37]. The NAO has been successfully used in previous studies as a therapeutic interface, motivating participants to learn and interact [38\u2013 42] (for an extensive review see [43]).\nWhile the NAO could provide a good test interface for engagement purposes, the imple-\nmentation of auditory and speech perception tests could be affected due to potential inherent limitations of the robot, such as sound quality (due to the internal speaker and sound card combination), non-experimental sound artefacts (due to the cooling fan and moving actuators [44]), stimulus processing speed (for tests that require stimuli to be prepared and processed in real-time during testing), and stimulus presentation and response logging [limited visual, voice and speech cues due to the non-moving face of the robot and the number of sensors (11 in total)]. On the point of sound quality, literature has also shown that the loudness of sound can be perceived differently depending on the source of the stimulus; e.g., from different loudspeakers and headphones, distance to the loudspeaker, hardware differences between output devices etc., despite the careful equalising between devices [45]. Furthermore, potential perceptual biases could be an additional factor such as the robot\u2019s voice being perceived as more of a specific gender if the physical characteristics of the robot are visually perceived as that specific gender [13]. Therefore, such an interface first needs to be confirmed to reliably produce good quality results for hearing and speech perception tests.\nThese experiments aim to evaluate how well the NAO would function as an auditory psy-\nchophysical testing interface, given the potential limitations and differences in implementation compared to the computer version, using tests of voice cue perception and subjective categorisation of voice gender."
        },
        {
            "heading": "General methods",
            "text": "The present study is part of the larger project Perception of Indexical Cues in Kids and Adults (PICKA). The PICKA test battery was created by the dbSPL (for more details of the dbSPL group see www.dbspl.nl) research group at the University Medical Centre Groningen (UMCG) to investigate voice and speech perception in normal and impaired hearing. In addition to being part of the larger PICKA project, this study is also part of a larger study comparing the results of the four PICKA tests on the laptop to a humanoid NAO robot we named \u201cSam\u201d, chosen to represent a gender-neutral name in an attempt to avoid a prior gender assignment for the robot. The four PICKA tests are voice cue sensitivity, voice gender categorisation, voice emotion identification, and speech-on-speech perception. Two of these tests were used in this study, conducted as two experiments performed one after the other in a single session: Experiment I, voice cue sensitivity (similar to [8, 10\u201312]) and Experiment II, voice gender categorisation (similar to [7, 12]). The PICKA tests can be run both in English, Dutch and Turkish, the former of which was used in this and the larger comparative study. Until this\nPLOS ONE | https://doi.org/10.1371/journal.pone.0294328 December 13, 2023 3 / 27\nstudy, the tests (developed in Matlab [46]) had been implemented on a laptop, some of which have interfaces with cartoons and animations, which we used in this study for a fairer comparison to the robot [12]. In each experiment, tests were performed both via the laptop (identical to that reported by [12]) and the new robot interface, Sam."
        },
        {
            "heading": "General NAO robot setup",
            "text": "Sam is a NAO V5 H25 humanoid robot developed by SoftBank Robotics. The body of Sam has an Atom Z530 1.6 GHz CPU processor, 1 GB RAM, 2 GB flash memory, an 8 GB micro SDHC card, 11 tactile sensors\u2013three on the head, three on each hand and one on each foot\u2013 two cameras and four ultrasound sensors. Sam has 25 degrees of freedom, enabling it to perform movements and actions resembling that of a human.\nThe operating system on Sam is the NAOqi OS, based on Gentoo Linux created by the orig-\ninal developers, Aldebaran. A cross-platform NAOqi SDK (software development kit) framework can be installed onto a local computer to communicate with and control Sam. The programming languages that can be used to interact with NAO through the SDK are Python [47], C++ [48], and Java [49].\nSince the current version of the PICKA test battery was developed and designed in Matlab,\nthis was not compatible with Sam if it were to function as an independent interface. Therefore, the PICKA tests were rewritten into Python, which allowed all tests and stimuli to be stored and run directly on Sam. However, it should be noted that the processor of Sam (1.6 GHz) is slower than the laptop (2.5 GHz); thus, from the beginning of the experiments, it was known that the real-time local generation of stimuli would possibly result in longer durations of the tests."
        },
        {
            "heading": "Experimental setup",
            "text": "The laptop used was an HP Notebook (Intel Core i5 7th gen) running Ubuntu 16.04. The PICKA test battery was run using MATLAB 2019b. Stimuli for all tests were played through the internal speakers and sound card of the laptop. Responses were logged using a connected mouse to the laptop. The game-like interface with which children were previously tested [12] was used. Although all other details of the implementation were also identical to the aforementioned study, the only exception was the use of English stimuli in the present study, differing from the use of Dutch by [12].\nFor the robot setup, the stimuli for all tests were played through the internal stereo loud-\nspeakers located in Sam\u2019s head and using the onboard soundcard. In both tests the tactile sensors on Sam\u2019s hands and head were used to log responses."
        },
        {
            "heading": "Participants",
            "text": "Thirty adults participated in both experiments; however, two participants were excluded from data analysis due to not meeting the inclusion criteria for normal hearing, and data were analysed from 28 participants (aged 19\u201338; 23.6 \u00b1 4.9 years; participants were asked with which gender they identified, to which they could respond openly: 19 reported female, 9 reported male). Sample size was based on a rule of thumb for human-robot interaction studies in which it is recommended that a minimum of 25 participants are included per tested condition [50], and an extra five participants to account for potential exclusions. Participant recruitment was conducted between 02/2021\u201309/2021, and inclusion criteria were kept general to minimise any selection bias. All participants reported English as either their native or first additional language and having completed at least high school education. Informed consent was obtained prior to the start of the experiment, followed by a pure-tone audiogram to confirm normal\nPLOS ONE | https://doi.org/10.1371/journal.pone.0294328 December 13, 2023 4 / 27\nhearing for inclusion/exclusion [hearing thresholds > 20 dB HL at any of the audiometric octave frequencies (250\u20138000 Hz) qualified for exclusion]. Regardless of the outcome of the audiogram, the experiment was still conducted, and the inclusion/exclusion was applied before the data analysis phase. Although this deviates from common practice for psychophysical tests, an additional component of this study was to investigate the observed human-robot interaction, which will be reported in a follow-up publication. The PICKA project protocol was approved by the METc ethical review committee at the local university hospital (METc 2018/ 427, ABR nr NL66549.042.18). Participants provided written consent for their participation and were assigned a unique participant identifier, and the corresponding key was securely stored; however, the authors did have access to this information as necessary. The participants were compensated \u20ac8/hr for their participation."
        },
        {
            "heading": "General setup",
            "text": "The order of the interfaces (i.e., starting with the laptop or Sam) in each experiment per participant was randomised. In a session, a break was offered to participants both between the two experiments and between the two interfaces within an experiment. On both interfaces and for both experiments in each session, a training phase (shorter version of the test) was first performed to familiarise the participant with how the test was conducted and how their responses were logged. After this, participants started with the actual test. During each experiment, participants\u2019 responses were recorded to assess performance for the specific auditory test with each interface.\nIn each experiment for each test, participants were seated at a desk with either the laptop or\nSam in an unoccupied and quiet room at the university medical centre. Participants were seated approximately one metre from the test interface; however, this varied as participants moved to interact with Sam or the laptop. The unused interface was placed outside the participants\u2019 line of sight.\nExperiment I: Voice cue sensitivity\nThe voice cue sensitivity test assesses the listener\u2019s ability to detect the smallest perceivable F0 or VTL differences (just noticeable differences; JNDs) when applied to a speaker\u2019s voice (based on methods by [8])."
        },
        {
            "heading": "Stimuli",
            "text": "To prepare the stimuli, consonant-vowel (CV) syllables were spliced from existing consonantvowel-consonant meaningful English words from the Chear Auditory Perception Test (CAPT) and Consonant Confusion Test (CCT) corpora [51, 52]. The CV tokens had a duration of 142\u2013200 ms. Splicing of syllables (60 in total) was performed identically to methods reported by [12]. For each trial, three spliced syllables were randomly selected and concatenated to produce a single CVCVCV syllable triplet (e.g., \u201cbi-fo-ki\u201d).\nFor this test, the focus is on the difference in F0 and VTL relative to a reference voice. Both\nthe F0 and VTL differences are expressed in semitones (st), an intuitive frequency increment unit often used in music and expressed as 1/12th of an octave. The VTL is a distance, hence related to wavelength and inversely to frequency, and can be expressed as ratios measured on a logarithmic scale (12log2(r), where r is the expansion/contraction ratio of the formant distances [8]). This conversion of the voice cues to semitone units allows both F0 and VTL values to be expressed in comparable units, instead of relying on the original Hertz or millimetre for F0 and VTL, respectively. To obtain the F0 contour and spectral envelope of each syllable when using the laptop, the analysis module STRAIGHT [53] was used. Extraction of these\nPLOS ONE | https://doi.org/10.1371/journal.pone.0294328 December 13, 2023 5 / 27\nsame parameters in Sam\u2019s Python implementation of the PICKA battery was performed using the analysis module PyWORLD [54] in place of STRAIGHT. Application of the modified voice cue parameters was made with methods identical to [8]. The F0 of the reference voice was set to 242 Hz. This reference F0 was the same value used by [12], despite using a different female speaker since the language was different (average F0 across all English syllable stimuli used was 248 Hz). This was done to make the results more comparable across studies. VTL is related to the distribution of the formant frequencies resulting from vocal tract resonances. Shortening of the VTL by the expansion/contraction ratio of the formant distance shifts all the formants to a higher frequency by that same ratio. Therefore, a positive VTL change corresponds to a negative formant frequency shift in semitones [4]. The VTL of the reference voice was left unchanged from the original speaker.\nAll stimuli for both interfaces were calibrated to 65 dB SPL using a Knowles Electronics\nMannequin for Acoustic Research (KEMAR, GRAS, Holte, Denmark) head assembly and a Svantek sound-pressure level metre (Type 2610, Bru\u0308el Kj\u00e6r and Sound & Vibration Analyser, Svan 979)."
        },
        {
            "heading": "Laptop vs robot",
            "text": "Interface. The laptop game-like interface of the voice cue sensitivity test can be seen in\nFig 1, panel A. The three-syllable triplets were presented by each of the three identical aquatic animals. The participant then clicked the animal that sounded different from the other two, and visual feedback was given for correct responses by fish and sea creatures moving. Sam as a\ngreen eyes indicate that a response can be given. In (C), visual feedback in the form of head movements is presented indicating if the response was correct or incorrect. With Sam there is no indicator of progress.\nhttps://doi.org/10.1371/journal.pone.0294328.g001\nPLOS ONE | https://doi.org/10.1371/journal.pone.0294328 December 13, 2023 6 / 27\ntest interface can be seen in Fig 1, panels B and C. Tactile sensors on Sam\u2019s hands and head (the three sensors on each hand and the head were grouped together as one) were used for response logging, and visual feedback was given for correct responses by Sam nodding.\nImplementation. Implementation of the test on both interfaces was fundamentally simi-\nlar regarding the on-the-fly preparation of stimuli, adaptive procedure, conditions for terminating blocks, visual feedback during the testing and training phases, and presentation of the stimuli from the interface itself (i.e., no external sound cards or speakers were used). Differences in the implementation concerned the human robot interaction. This included Sam introducing themself and the PICKA test to the participant, and Sam\u2019s verbal encouragement and offer of a break between blocks, detailed below.\nProcedure. In each trial, the listener was presented with three acoustic stimuli, each made\nup of the same triplet of different syllables, where one of the three stimuli differed either in F0 or in VTL relative to the other two reference triplets. The task for the listener was to identify which of the three presented stimuli sounded different from the other two. The overall paradigm followed a three-interval three-alternative forced-choice (3I-3AFC) adaptive 2-down1-up staircase model, converging to 70.7% correct discrimination [55]. After two consecutive correct responses, the relative difference between the different and the reference stimuli was reduced, making identification of the differing stimulus more difficult. After one incorrect response, the relative difference increased, making identification of the different stimulus easier. The test comprised four runs representing four directions of voice manipulation: two for F0 discrimination and two for VTL discrimination. Each run started with a difference of either -12 st or + 5 st for \u0394F0 (corresponding to values typical of male and child talkers, respectively) or +3.8 or -7.0 st for \u0394VTL (corresponding to values typical of male and child talkers, respectively). From this starting point, the voice cue difference decreased (i.e., approached 0 st difference compared to the unmodified reference voice) after two consecutive correct responses or increased after one incorrect response with a predetermined step size, initially set at 2 st. As more correct responses were given, the step size was also adapted by reducing the previous step size by a factor of p 2, becoming exponentially smaller such that the step size approached but never reached a 0 st voice cue difference. Although the voice cue difference increased after an incorrect response, the step size was not modified. A reversal was defined when a single incorrect response was given after at least two correct responses, or two correct responses were given after at least one incorrect response. Each run ended in one of three ways: 1) if 15 consecutive incorrect responses were given, 2) if a total of 150 stimuli were presented or 3) after eight reversals had been reached. When a run ended in the latter, the JND was calculated by averaging the difference in semitones over the last six reversals. The former two conditions were implemented as a measure to ensure the test would not continue indefinitely, or in case the participant could not continue with the test. However, this did not occur in any of our experiments; all participants finished each run after the eight reversals.\nParticipants first familiarised themselves with the test through a training consisting of six\nrandomly selected practice stimuli, identical to test stimuli but performed with a larger, fixed step size of 3 st to speed up the adaptive procedure. The syllable triplets used in the training stimuli were not reused in the testing phase. Following the training, the first of the four test runs was started, the order of which (voice cue and direction) was randomised for each participant. In both the training and testing phases on both interfaces, positive visual feedback was given to participants. On the laptop, this was provided by the central fish turning in a circle, and the aquatic animal representing the correct response \u201cswimming\u201d to a growing line of sea animals representing previous correct responses. Although no explicit negative feedback was presented for incorrect responses, the correct response is briefly outlined in green before continuing with the next stimulus without any further animations. At the end of each run, a \u201cStart\u201d button\nPLOS ONE | https://doi.org/10.1371/journal.pone.0294328 December 13, 2023 7 / 27\nwas displayed to begin the next run, and the participant was allowed to take a short break before starting the next run; however, this was not explicitly indicated to participants.\nOn the laptop, the three aquatic animals presented the stimuli; on Sam, specific tactile sen-\nsors corresponded to the order in which the stimuli were presented: the first stimulus corresponded to Sam\u2019s right hand, the second to the head and the third to Sam\u2019s left hand. After each stimulus, Sam\u2019s eyes changed colour from white to green to indicate that a response could be given, after which the eyes returned to white. This was implemented to prevent participants from logging their responses too early before the stimulus had finished playing. After each response, visual feedback was presented as either a head nod if correct or a head shake if incorrect (Fig 1, panel C). Although the addition of the explicit negative feedback differs from the implementation of the laptop, the lack of a screen with the robot, unlike the laptop, may make it unclear as to whether or not a participant\u2019s response was logged without such an explicit visual cue. To maintain engagement throughout each run, Sam autonomously encouraged participants to continue depending on their performance. The choice of whether or not to provide encouragement was decided by a randomly generated number between 0 and 1. If the number was less than some threshold, initially set at 0.1, encouragement would be provided. If the previous response was correct, Sam would say either \u201cKeep going!\u201d or \u201cDoing well.\u201d. If the previous response was incorrect, Sam would motivate them by saying either \u201cGive it another go\u201d or \u201cKeep trying\u201d. Every time the response was incorrect, the encouragement threshold was increased by 0.05 until encouragement was given, after which the threshold was reset to 0.1.\nAfter each run, Sam asked the participant if they wanted to take a break, to which they\ncould verbally respond with either \u201cyes\u201d or \u201cno\u201d. If they responded \u201cyes\u201d, Sam would ask if they would like to stand up and follow along in a stretch routine, to which they could again verbally respond. If the participant chose not to take a break, the next run would start."
        },
        {
            "heading": "Data analysis",
            "text": "To determine if the two implementations were comparable for the voice cue sensitivity task performance, JND thresholds from either interface were first log-transformed to convert the data into a normal distribution, as the thresholds are always a positive value. Repeated-measures ANOVAs were performed for each voice cue, F0 and VTL, separately using the interface the test was performed on, and the direction (negative or positive) of the vocal cue as the two within-subjects factors (two interfaces \u2573 two voice cue directions). To improve the robustness of the data analysis, Bayesian repeated-measures ANOVAs were performed for each voice cue using the same within subject factors mentioned above. Similar classical and Bayesian repeated measures ANOVAs were performed to examine the effect of the interface and cue direction on the duration of each test run.\nBayesian inferences were used in this study because we are looking for evidence that the\ntwo interfaces are similar, and this type of conclusion cannot be reached with a classical (frequentist) approach. In the frequentist approach, the p-value is the probability of obtaining results at least as extreme as those seen in the collected data given that the null hypothesis (H0) was true [56]. Therefore, a lack of significance is often falsely interpreted as the absence of an effect [57], which we may be tempted to interpret as the two interfaces being equivalent. In comparison, a Bayesian analysis allows for an alternative interpretation and reasoning of the results through the reporting of the magnitude of evidence [i.e., the likelihood of the data under the assumption of H0 rather than H1 (the alternative hypothesis)]. An estimate of this evidence is presented as the Bayes\u2019 factor, which provides the relative likelihood of the data with respect to the null hypothesis (BF01 = H0/H1) or any other hypothesis (BF10 = H1/H0). it\nPLOS ONE | https://doi.org/10.1371/journal.pone.0294328 December 13, 2023 8 / 27\nshould also be noted that these two notations of the Bayes\u2019 factor are indeed reciprocals of one another (i.e., BF01 = 1/BF10). Bayesian analyses were performed using the statistical software JASP [58], which categorises the evidence based on the Bayes\u2019 factor as shown in Fig 2. Further discussion on classical vs Bayesian inference is presented in the General Discussion (also see [56] for a detailed introduction to the Bayesian method).\nOrganisation of the data was performed using the data analysis software R [59], and JASP\nwas used for all statistical analyses. Bayesian analyses for both vocal cues used the default priors suggested by JASP: a uniform model prior with r scale fixed effects = 0.5, r scale random effects = 1 and r scale covariates = 0.354 and enforced principle of marginality on the fixed effects. All processed data is openly available at [60]."
        },
        {
            "heading": "Results",
            "text": "Fig 3 depicts JND thresholds obtained with each interface and previously reported thresholds in previous studies [6, 8]. Laptop F0 JNDs [2.453 \u00b1 (standard deviation of) 2.130] were on average 1.212 st larger than Sam\u2019s F0 JNDs (1.241 \u00b1 1.146), and laptop VTL JNDs (1.828 \u00b1 1.124) were on average 0.450 st larger than Sam\u2019s VTL JNDs (1.378 \u00b1 0.725). Results of the repeated measures ANOVA (main factor of test interface with two levels; laptop, robot) showed no statistically significant difference between the JND thresholds for F0 due to the interface [F(1, 28) = 4.170, p = 0.051, \u03b72p = 0.130], nor was there a cue direction effect [F(1,28) = 0.035, p = 0.852, \u03b72p = 0.130]. Similarly, there was no significant effect of the interface on VTL vocal cue [F(1,28) = 0.032, p = 0.859, \u03b72p = 0.001]; however, there was a significant effect due to cue direction [F(1,28) = 5.337, p = 0.028, \u03b72p = 0.160]. Results showed no significant effect for an interaction between the interface and the direction of the F0 cue [F(1,28) = 0.005, p = 0.946, \u03b72p = 1.65e-4], but indeed a significant effect for an interaction between the interface and VTL voice cue direction [F(1,28) = 5.578, p = 0.025, \u03b72p = 0.166]. However, following a post-hoc Bonferroni-Holm correction test, the significant effect was only between the directions of the VTL cues on the laptop, not between the two interfaces.\nThe average duration of the voice cue sensitivity test (including all 4 runs) was 19 \u00b1 2.7 min on the laptop and 29 \u00b1 3 min on Sam. These times are exclusive of breaks taken by participants. On average, most participants did not take a break when using the laptop, whereas an additional two minutes on average was due to breaks when using Sam. Repeated measures ANOVA (main effect of test interface with two factors: laptop, robot) showed that Sam took significantly longer to complete in comparison to the laptop for the F0 vocal cue [F(1,28) = 52.964, p< 0.001, \u03b72p = 0.654], but cue direction had no effect [F(1,28) = 1.472, p = 0.235, \u03b72p\nhttps://doi.org/10.1371/journal.pone.0294328.g002\nPLOS ONE | https://doi.org/10.1371/journal.pone.0294328 December 13, 2023 9 / 27\n= 0.050]. Similarly, the VTL vocal cue showed that Sam took significantly longer to complete the test [F(1,28) = 52.670, p< 0.001, \u03b72p = 0.653], but no effect due to cue direction [F(1,28) = 3.603, p = 0.068, \u03b72p = 0.114]. For both vocal cues, there was no effect on the duration due to an interaction between the interface and the direction of the cue [F0: F(1,28) = 2.524, p = 0.123, \u03b72p = 0.083; VTL: F(1,28) = 0.026, p = 0.873, \u03b72p = 9.232e-4].\nBayesian repeated measures ANOVAs showed strong evidence that Sam took longer to\ncomplete the test for the F0 vocal cue (BF10 = 2.133e+5), and anecdotal evidence that cue direction did not affect the duration of the F0 test runs (BF10 = 0.388). Bayesian results also showed that Sam took longer to complete the test for VTL vocal (BF10 = 1.652e+5), and anecdotal evidence that the cue direction also affected the duration (BF10 = 1.035). Furthermore, there was anecdotal evidence of an interaction between the interface and cue direction for the F0 vocal cue (BF10 = 1.120), and moderate evidence of no interaction for the VTL vocal cue (BF10 = 0.261). Fig 4 depicts the durations to complete each of the four test runs in the present experiment, as well as a comparison to data reported by [12]. Data reported by [11] is excluded from the analysis of test duration as they had used a different interface for the test (i.e., they did not use the same game-like interface as used in this experiment or by [12]).\nhttps://doi.org/10.1371/journal.pone.0294328.g003\nPLOS ONE | https://doi.org/10.1371/journal.pone.0294328 December 13, 2023 10 / 27"
        },
        {
            "heading": "Discussion",
            "text": "In Experiment I, we implemented a test of voice cue sensitivity on Sam to compare JND measurements with the current laptop game-like interface in order to evaluate the viability of using a NAO robot interface for auditory psychophysical tests. Results for the voice cue sensitivity test showed no statistically significant difference in JND thresholds between the two interfaces concerning both F0 and VTL. For both voice cues, the thresholds on Sam were overall either smaller or similar to those obtained on the laptop, meaning that there was no indication for the performance being worse or the responses provided being less accurate when using Sam. This is a confirmation that whatever technical limitations Sam may have that could impact sound quality or sound processing, did not negatively affect the JND thresholds for this test with the young adult, normal-hearing population tested, compared to a laptop implementation.\nInterpretation of the Bayesian analysis results is presented as follows. Bayesian analyses\nshowed that F0 JND thresholds were 1.6 times more likely to result in a non-zero difference between the means; however, this evidence is only anecdotal; any variation in the means between the two interfaces is likely due to the sample size. In comparison, Bayesian analyses showed that the VTL JND thresholds were 3.3 (reciprocal of BF10 = 0.304) times more likely to result in a zero difference between the means. Regarding the direction of the voice cue, Bayesian analyses showed that for F0, results were 5.3 times more likely to have no effect on the overall JND threshold, and 1.1 times more likely to affect the VTL JND threshold. With respect to interactions between the direction of the voice cue and the interface, the interaction was 3.8\nhttps://doi.org/10.1371/journal.pone.0294328.g004\nPLOS ONE | https://doi.org/10.1371/journal.pone.0294328 December 13, 2023 11 / 27\ntimes more likely to have no effect on the F0 JND threshold, but 4.6 times more likely to affect the VTL JND threshold.\nIn addition to comparing the results of the voice cue sensitivity test between the interfaces,\nwhen comparing the current results to those of previous studies using similar procedures, it was also observed that the F0 JNDs on Sam when using English stimuli were not significantly different from those reported by [12] [t(51.45) = 1.33, p = 0.189, Cohen\u2019s d = 0.322; BF10 = 0.39] nor from those reported by [11] [t(34.795) = -1.404, p = 0.17, Cohen\u2019s d = 0.500; BF10 = 0.88], both of whom had used Dutch stimuli. In comparison, the F0 JNDs from the laptop were found to be statistically different from the data reported by [12] [t(70.953) = 4.777, p< 0.001, Cohen\u2019s d = 0.940; BF10 = 5.82], but were not statistically different from the data reported by [11] [t(49.096) = 0.910, p = 0.367, Cohen\u2019s d = 0.260; BF10 = 0.38]. A closer inspection of the data reveals that a few participants obtained much higher thresholds on the laptop in comparison to their matched conditions on Sam, causing the expanded threshold range seen in Fig 3. The three participants with the largest deviation between the two interfaces, listed in Table 1, started the experiment on Sam; thus, it is unlikely that a learning effect could have caused the observed variations. It is possible, however, that after performing the test on Sam, participants were fatigued and were not as attentive to the task when performing it for the second time on the laptop. Furthermore, because the starting interface was balanced across all participants, the absence of this asymmetry may suggest that using Sam is motivational enough to combat the effects of fatigue when performing the test for a second time with Sam.\nSince the same stimuli were used on both interfaces, ideally, it is expected that any variation between the interfaces would be identical. However, based on the t-test and effect size comparing the JNDs obtained on the laptop with those obtained by [12], it is likely that the large discrepancies seen in Table 1 could be related to the quality of the speakers on either interface, as their respective built-in speakers were used, and not external higher quality speakers. In addition, the procedure used by [12] made use of headphones instead of the loudspeakers; however, speakers were used in this study for consistency with the speakers of Sam. Moreover, both studies by [11, 12] included native Dutch speaking participants, and thus could be more selective with their inclusion criteria. In comparison, also being in the Netherlands and conducting the tests in English, participants included in the present study were only required to have a good understanding of English and did not have to be native speakers. Although the speakers on the laptop may have been of poorer quality compared to what is typically used in auditory research, the consistency in results obtained on Sam with those obtained in previous\nComparing the VTL thresholds obtained on Sam and the laptop with previously reported data showed no significant difference between Sam and data reported by [12] [t(43.15) = 1.23, p = 0.23, Cohen\u2019s d = 0.25; BF10 = 0.39], but indeed a significant difference compared to [11] [t(51.42) = -2.14, p = 0.037, Cohen\u2019s d = -0.51; BF10 = 2.03]. There was also a significant difference between VTL thresholds obtained on the laptop and [12] [t(71.143) = 3.67, p< 0.001, Cohen\u2019s d = 0.60; BF10 = 1.66]; however, there was no significant difference with [11] [t(80.152) = 0.59, p = 0.56, Cohen\u2019s d = 0.11; BF10 = 0.26].\nhttps://doi.org/10.1371/journal.pone.0294328.t001\nPLOS ONE | https://doi.org/10.1371/journal.pone.0294328 December 13, 2023 12 / 27\nstudies does show that the JND test is robust not only across languages, but also regarding implementations and procedures.\nIt was seen that the duration of the test on Sam was longer than on the laptop; however, the\nduration of the test did not differ significantly between Sam and from durations reported by [12] [F0: t(39.48) = -1.206, p = 0.235, Cohen\u2019s d = -0.336, BF10 = 0.482; VTL: t(30.294) = 0.794, p = 0.433, Cohen\u2019s d = 0.246, BF10 = 0.394]. A similar comparison regarding the duration could not be made with data reported by [11] as they had used a different interface than that used in this experiment (i.e. they did not use the game-like version of the PICKA test battery). To investigate the longer running time on Sam, we have also inspected the number of trials on each interface. On average across all runs, the number of trials to complete in a block was 37 \u00b1 8 on the laptop, and 42 \u00b1 8 on Sam. Although more trials were needed on average to achieve the JND threshold on Sam [t(115) = 4.113, p< 0.001, Cohen\u2019s d = 0.561, BF10 = 231.775], this difference does not fully explain the overall longer duration of the test. A perhaps more important factor that likely influenced the duration of the test was the processing time of the stimuli. The average processing time for a single stimulus was around two seconds on Sam, and one second on the laptop. This is a limitation of Sam\u2019s hardware, and it is expected that newer models of the NAO could solve this processing discrepancy. Despite the longer testing duration on Sam, both including and excluding the breaks, Sam was still able to collect comparable JND thresholds to the laptop interface.\nOne could argue that a more powerful humanoid robot, such as Pepper (another of Aldebar-\nan\u2019s robots), could be used in place of Sam to compensate for the processing delays. While this is technically true as both robots use (nearly) identical software platforms, are designed to portray a friendly agent, and have been used in various HRI scenarios such as engagement and social robot application (for a review please see [37]), Sam offers other advantages over Pepper for our specific application, most notably, its small size and lower cost. Sam\u2019s smaller size makes it easily transportable to various testing locations. In addition, should anything go awry, a smaller robot minimises the risk of potential harm to users, especially in smaller testing locations such as clinical rooms. A simpler solution to the low processing power of Sam could be the offline generation of all JND stimuli through simulations, and then use the pre-generated stimuli during testing. However, this could introduce a new problem of storage space on Sam. The incorporation of a stronger processor that could be attached to Sam as a type of \u201cbackpack\u201d [61] could be another solution to the processing capacity of Sam. Instead of replacing Sam\u2019s central processing unit (CPU), an additional CPU could be used in parallel to perform the more process intensive tasks. There are a number of ways in which processing time for the JND stimuli could be reduced, all of which could be thoroughly explored as future improvements.\nIt should also be considered that the inclusion of a dedicated break offered to participants\nafter each test block could potentially have an effect on the performance results. It has been shown in literature that the duration one interacts with an agent, the perception they have towards the agent may change [62, 63]. [62] has shown how over the course of an interaction, perceptions of warmth and competence of a virtual agent can change, whereas [63] has shown how during an interaction with a robot, positive interactions tend to result in more favourable perceptions of the robot. Following this, there is a possibility that the inclusion of the followalong stretch routine during the break may have increased the likeability of Sam, potentially motivating participants to exert more effort in proceeding test blocks.\nExperiment II: Voice gender categorisation\nThe voice gender categorisation test investigates the subjective categorisation of a speaker\u2019s expressed gender in their voice, and how voice cues and manipulations can influence the perception of voice gender. This test is based on the methods by [7, 12].\nPLOS ONE | https://doi.org/10.1371/journal.pone.0294328 December 13, 2023 13 / 27"
        },
        {
            "heading": "Stimuli",
            "text": "To investigate this voice cue influence, pre-generated stimuli from English consonant-vowelconsonant spoken words taken from the same corpora as in Experiment I (CAPT and CCT) were used. Each stimulus presented during the test was randomly chosen from a limited list of words: \u201cbike\u201d, \u201cpool\u201d, \u201cwatch\u201d and \u201chat\u201d; which were altered in F0 and VTL. Similar to methods described by [12], stimuli were root-mean equalised, after which stimuli were modified using the STRAIGHT Matlab module on the laptop or PyWORLD on Sam. F0 and VTL were manipulated independently from each other, and in cases where both cues were altered in a single stimulus, F0 alterations were first applied followed by VTL alterations. When no alterations were made (F0 = 0.0 st and VTL = 0.0 st), the original stimulus was still resynthesized to account for any potential synthesis artefacts. Three levels of modifications were used for each voice cue in all stimuli, identical to that used by [12]; decreasing F0 from 0.0 to -6.0 st and -12.0 st and increasing VTL from 0.0 to +1.8 st and +3.6 st. In addition, combinations of these voice cues were used for each word. This resulted in nine voice alterations for each word, producing 36 pre-processed stimuli (nine voice conditions \u2573 four words) for the block. To familiarise participants with the test, the same eight example words were used for all participants, taken from the 36 stimuli at the two widest vocal manipulation conditions (four words \u2573 two widest voice conditions\u2013F0 = 0.0 st, VTL = 0.0 st, and F0 = -12.0 st, VTL = +3.6 st)."
        },
        {
            "heading": "Laptop vs. robot",
            "text": "Interface. For the voice gender categorisation test on the laptop\u2019s game-like interface,\nwhen the stimulus was presented, a cartoon image of either a male or female was shown on an animated television screen. Participants had to agree or disagree if the presented voice gender matched the image gender. When performing the test on Sam, only the tactile sensors on the hands (the three sensors on each hand grouped together as one) were used to log responses.\nImplementation. Similar to Experiment I, the core paradigm of the voice gender categori-\nsation test was similar between the two interfaces, and stimuli were presented without the use of external sound cards or speakers. Sam provided an introduction and explanation of the test before the training phase, similar to Experiment I. Unlike Experiment I, however, stimuli were pre-processed for both interfaces. Additionally, no visual feedback was provided to participants from either interface, as the voice gender categorisation was a subjective choice; nor was there any encouragement provided or any breaks offered due to the shorter duration of the test in comparison to Experiment I.\nProcedure. The test consisted of one block in which all 36 aforementioned stimuli were presented in a randomised order. Each stimulus was presented once, after which the listener had to decide on whether the voice sounded male or female. While a person\u2019s expressed gender is more flexible and wider than these two categories [64], for methodological simplicity and to enable a comparison to previous work, we have followed the previous procedures [7, 12]; and as a result, participants were only given the options of these two categories.\nParticipants were first presented with a training phase consisting of the eight example\nwords to familiarise themselves with the task procedure. Thereafter, the data collection was started. An example of the test interface on the laptop is shown in Fig 5. In both training and data collection, there was no visual feedback provided, as the voice gender categorisation is a subjective choice; and, therefore, there was no (in)correct categorisation. Since the test consisted of one block, only a break was offered between the two interfaces.\nThe laptop game-like version required participants to click on either a green tick to indicate\nthat the presented voice gender and the picture gender matched, or a red cross if they did not match. In comparison, Sam\u2019s hands corresponded with either a male or female categorisation,\nPLOS ONE | https://doi.org/10.1371/journal.pone.0294328 December 13, 2023 14 / 27\nrandomising the hand-gender pair after each stimulus was presented. The purpose of such randomisation was to avoid a bias for participants continuously touching the same hand, whilst bringing their attention back to Sam. After the randomisation, Sam indicated which hand corresponded with which gender before allowing the participant to log their response. This was done by Sam lifting and rotating its hand outward before returning it to its default position on its legs. Although this differs from the laptop implementation, the latter does also include a degree of randomisation when the image of a male or female person is randomly presented. An increase in the test duration as a result of this method of indicating the gender-hand pair was taken into account; however, based on the relatively short duration of this test from the literature, it was presumed this would not heavily affect the average duration of the test. After Sam indicated the gender-hand pair, visual cues were provided by changing the colour of Sam\u2019s eyes (similar to Experiment I): from white to green indicated a response could be given, back to white indicated the response had been logged."
        },
        {
            "heading": "Data analysis",
            "text": "Analysis of the categorisations made during the voice gender perception test was performed similarly to that carried out by [12], whereby cue weights were calculated as a perceptual weighting of F0 and VTL for participants\u2019 categorisation judgments, effectively splitting categorisations as a function of the two voice cues. Calculations were made by first normalising F0 and VTL relative to the reference speaker\u2019s voice and were defined as follows: \u03b4F0 = -\u0394F0/12\u2013 0.5, \u03b4VTL = \u0394VTL/3.6\u20130.5, thus making the two voice cues functionally equivalent. This resulted in the reference female voice, which had an \u0394F0 = 0.0 st, a \u0394VTL = 0.0 st, corresponding to a \u03b4F0 = -0.5 and a \u03b4VTL = -0.5; and the male-sounding voice with an \u0394F0 = -12.0 st, \u0394VTL = +3.6 st, corresponding to a \u03b4F0 = +0.5 and a \u03b4VTL = +0.5. Using a mixed-effects logistic regression model with slopes for \u03b4F0 and \u03b4VTL per participant, the coefficients for each participant could be extracted, in lme syntax: response ~ (\u03b4F0 + \u03b4VTL|participant). These coefficients provide a prediction on a logit scale relative to the normalised \u03b4F0 and \u03b4VTL ranges, which can subsequently be converted into \u201cBerkson\u201d units (Bk) per semitone so that they\nhttps://doi.org/10.1371/journal.pone.0294328.g005\nPLOS ONE | https://doi.org/10.1371/journal.pone.0294328 December 13, 2023 15 / 27\ncorrespond to a log2 odds ratio per semitone [12, 65]. Using the calculated Bk units and cue weights (Bk units per semitone difference) of the models. Paired samples student t-tests were performed to test for differences between the two interfaces. As with Experiment I above, Bayesian paired samples t-tests were also performed to improve the robustness of the statistical results. Bayesian analyses were carried out using the default Cauchy prior (? = 0.707) and a null hypothesis that the means between the two interfaces were equal."
        },
        {
            "heading": "Results",
            "text": "Fig 6, panel A depicts the distribution of the cue weights for each interface and panel B the contribution of F0 and VTL on the voice gender categorisation. The colour of each square indicates the frequency of categorisation based on the influence of VTL difference (dVTL) and F0 difference (dF0). Squares coloured toward the yellow side of the spectrum indicate more\nhttps://doi.org/10.1371/journal.pone.0294328.g006\nPLOS ONE | https://doi.org/10.1371/journal.pone.0294328 December 13, 2023 16 / 27\nfemale categorisations, whilst squares toward the dark blue side indicate more male categorisations. Results of the paired student t-tests showed a statistically significant difference between the means of the intercepts [t(28) = 2.549, p = 0.017, Cohen\u2019s d = 0.455], but no significant difference between the means of the cue weights for the F0 [t(28) = -0.952, p = 0.349. Cohen\u2019s d = -0.177] and VTL [t(28) = 0.53, p = 0.6, Cohen\u2019s d = 0.135] voice cues.\nBayesian analyses between the two interfaces showed strong evidence that the two interfaces\nwere different with regards to the means for the intercepts (BF10 = 43.21), but no difference between the means for the cue weights of the F0 model (BF10 = 0.298), or the cue weights of the VTL model (BF10 = 0.225). Bayesian results also showed anecdotal evidence for a difference in gender categorisation between the two interfaces (BF10 = 1.248).\nThe average duration to complete the test was 3 min \u00b1 17 s on the laptop, and 5 min \u00b1 49 sec on Sam. Comparison of the duration to complete the test between the two interfaces showed that Sam took significantly longer to complete [t(28) = 15.840, p< 0.001, Cohen\u2019s d = 3.717; BF10 = 3.567e+12]. This difference in duration can be seen Fig 7 below, also comparing the durations to that reported by [12]."
        },
        {
            "heading": "Discussion",
            "text": "In Experiment II, we implemented a voice gender categorisation task on Sam to compare the categorisation of a voice\u2019s gender between Sam and the currently used laptop interface.\nhttps://doi.org/10.1371/journal.pone.0294328.g007\nPLOS ONE | https://doi.org/10.1371/journal.pone.0294328 December 13, 2023 17 / 27\nClassical analyses showed a significant difference between the gender categorisation between interfaces and intercepts of the cue weights, but no difference between the cue weights themselves. Bayesian analyses of the Bk units between the two interfaces showed strong evidence (i.e., 43.2 times more likely) for a difference in the cue weight intercepts. Cue weights for F0 and VTL also showed moderate evidence (4.35 and 3.33 times more likely, respectively) of no difference between the two interfaces. Bayesian analyses showed anecdotal evidence for a difference in overall categorisations of gender between the two interfaces; however, the use of the Bayesian method here indicates that the significant difference seen in the classical method indeed has a small effect size, and thus any differences observed are likely limited to the observed data set, and thus cannot be generalised for larger populations.\nWith respect to the difference observed between the gender categorisations and the inter-\ncepts of the cue weights between the two interfaces, one could speculate that this could be the result of an interference effect: with more conflicting cues, participants have more difficulties answering. Such interference could be caused by the potential for participants to subconsciously attribute a gender to Sam based on their physical appearance despite our efforts to keep Sam gender neutral [66\u201370]. This may have occurred in this test based on the intercepts for F0. The value of the intercept, as determined by the logistic regression above, indicates the degree to which there is a bias in categorising a voice as a female gender with a 0 st difference to the reference voice. A lower intercept would indicate more of a bias toward male categorisations over female categorisations. Furthermore, this bias was significantly lower with Sam (0.90 \u00b1 1.11) in comparison to the laptop (1.37 \u00b1 0.95), showing a bias toward male categorisations with Sam.\nIt must be noted that although we attempted to implement this test with Sam as comparably\nsimilar to the laptop as possible, some elements from the laptop game-like implementation could not be replicated on Sam. In the laptop version, a cartoon image of a person was presented after the stimulus was presented to which participants could either agree or disagree on whether they perceived the voice gender to match the gender of the image. Although at the processing level the images had a specific gender associated with them, this binary choice was not as evident to the participants as with Sam, which presented only the right or left hands for male or female; images depicted on the laptop interface were not always stereotypically male or female. Based on the design of the test on the laptop, participants were asked to identify if the voice matched the image, not if the voice was male or female. Thus, whilst the test required participants to make a binary male or female categorisation on Sam, to some extent this choice was an abstraction in the laptop version. Additionally, using visual images of female and male faces on the computer was also not entirely bias-free as their evaluation as female or male would still rely on the participant\u2019s own concept of female and male, and how well the images would fit with these. Further, it could also be argued that there is no visual cue change with Sam; therefore, if some bias was present based on the physical appearance of Sam, it would have been consistent throughout the test. Hence, such consistent bias across stimuli should only lead to a shift of results with respect to the reference voice, but no further noise across trials. Different than in Experiment I, in Experiment II all these biases are inherent to both interfaces, potentially affecting the perceived voice gender when presented with face images or Sam, but what matters is that the test seems to be usable on each interface, producing meaningful data.\nComparing the observed data to that collected by [12], voice gender categorisation using\nSam showed a statistically significant difference with respect to the intercepts [t(19.858) = -2.887, p = 0.009, Cohen\u2019s d = -1.060, BF01 = 19.036]; however, no statistically significant difference with respect to the F0 cue weights [t(17.624) = -1.923, p = 0.07, Cohen\u2019s d = -0.748, BF01 = 2.600], but a significant difference with respect to the VTL cue weights [t(18.811) = \u2013\nPLOS ONE | https://doi.org/10.1371/journal.pone.0294328 December 13, 2023 18 / 27\n2.751, p = 0.013, Cohen\u2019s d = -1.055, BF01 = 18.459]. Data obtained when using the laptop closely matches the voice gender categorisation of the adult group from data reported by [12], as depicted in Fig 6. Furthermore, there is no statistically significant difference between the intercepts of previous data and the laptop data [t(18.318) = -1.990, p = 0.062, Cohen\u2019s d = -0.759, BF01 = 2.763]; however, there was a statistically significant difference in the F0 cue weights between previous data and the laptop [t(17.359) = \u20132.311, p = 0.033, Cohen\u2019s d = -0.906, BF01 = 6.670]; and with respect to the VTL cue weights [t(20.517) = -2.375, p = 0.027, Cohen\u2019s d = -0.859, BF01 = 4.963].\nTherefore, although variation was seen in the intercepts between the laptop and Sam, Sam\nseems to still produce data comparable with the laptop; however, both the laptop and Sam data showed varying degrees of differences when compared to the data reported by [12]. This can be interpreted as whether performing the voice gender categorisation test on the laptop or on Sam, as far as examining the influence of F0 and VTL on voice gender categorisation, there is no difference in consistency between the two interfaces.\nWe implemented the experiment such that the pairing of the gender and Sam\u2019s hand were\nrandomised after each presented stimulus, and Sam would indicate which hand should be used for each gender. As expected the test took longer to complete on Sam in comparison to the laptop, however, this was due to other reasons than those in Experiment 1. Nevertheless, results for voice gender categorisation collected from Sam are still identical to those collected on the laptop. Furthermore, the increase in duration is also a consequence of the design of the implementation on Sam, as expected. After each stimulus is presented, Sam takes a few seconds to indicate which hand should be touched for a male or female categorisation, lengthening the duration of the test. This design choice was made to prevent the possibility of participants attributing a gender to one hand, and using that hand when voices were more difficult to categorise, potentially introducing an additional bias. In addition, test duration may have been increased due to a lack of visual cues when categorising a voice. The categorisation of a voice may be easier when presented with a face, as in the laptop version, thus making the decision process quicker. In comparison, participants had to pay more attention to the presented voice with Sam as this was the only cue presented, possibly causing hesitations in participants\u2019 categorisations, and increasing the duration of the test. The total duration of the test on Sam was also longer than that reported in previous studies, which corresponds to approximately three minutes [12] [t(35.706) = 13.18, p< 0.001, Cohen\u2019s d = 3.160]."
        },
        {
            "heading": "General discussion",
            "text": "In both auditory research and clinical settings, due to the inherent repetitiveness and long duration of many auditory perception psychophysics tasks, using a humanoid NAO robot was suggested as a new alternative interface. If it can produce reliable data, an additional advantage could be to make the testing more enjoyable and help with engagement. Therefore, as a first step, the goal of the present experiments was to evaluate our NAO robot, Sam, as an auditory testing interface by comparing participants\u2019 results (in both test performance accuracy and duration) on voice perception and voice gender categorisation when using Sam to those when using the current laptop game-like interfaces and to previously reported literature."
        },
        {
            "heading": "Comparison of test performance",
            "text": "The test performance was measured in JNDs (Experiment I) and Berkson units (Experiment II). Results have shown that, overall, Sam is comparable to the laptop interface and to previous studies that have used the same or similar test procedures, thus meaning the measured performances were similar within participants between both interfaces. Although some discrepancies\nPLOS ONE | https://doi.org/10.1371/journal.pone.0294328 December 13, 2023 19 / 27\nwere observed in the experiments, there was always some consistency either between the two interfaces or between Sam and previously reported data.\nThe small discrepancies observed in the results could have been due to a number of factors, such as differences in sound quality between the interfaces, as well as the use of English stimuli in comparison to the Dutch stimuli previously used [10, 12]. In addition, [12] used headphones, whereas in the two experiments of the present study, stimuli were presented by the internal speakers of Sam and the laptop. Without the use of specially designed speakers or headphones, it is possible that stimuli were presented sub-optimally and possibly with some degradation. It is not possible to connect external speakers or headphones to Sam, thus using these on the laptop would produce an unfair comparison. Despite these discrepancies, auditory test performance was shown to be similar between the two interfaces, as shown in Figs 2 and 4 for the voice cue sensitivity and voice gender categorisation, respectively.\nCertain design choices made when implementing the tests onto Sam may have also affected\nthe obtained test performance results. Most notably would be the visual cues provided by the laptop and not by Sam, such as the faces in Experiment II. Without these visual cues, it might be difficult or require some memorisation for the participants when logging responses on Sam, as the methods to do so are not as salient in comparison to the laptop. This is largely applicable to Experiment I, which required participants to remember how the order of presented stimuli related to the sensors on Sam.\nThe choices in design are a consequence of attempting to implement tests that had origi-\nnally been designed to be run on a computer interface onto Sam. It is not yet clear how much of an impact such choices have on the test results and if they even pose limitations. With more variations of similar experiments in future studies, these details would be optimised.\nFor using a robot for voice perception tests, previous work by [71] explored how the pitch of a robot\u2019s voice in conjunction with its physical appearance is perceived by children and its potential influence on user acceptance. Subsequent to this, [72] used the NAO to investigate how synthesised voices to sound that of male or female impacted children\u2019s perception of the gender of the robot. The authors had hypothesised that children could attribute a male gender to the NAO regardless of the gender of the voice. Although the study does not explain the motivation for this specifically, based on other studies, this increased attribution of a male gender to the robot could be due to the robot\u2019s body shape more resembling a typical male body than female body [66, 67]. This generalised male attribution to the NAO by both children and adults is also commented on by [68]. In their study, school-age children interacted with the robot for an average of 10 minutes. The experiment was an interactive game wherein the robot, which either had a male or female voice, asked for a specific card from a selection lying on a table. The game itself was not relevant to the research question; however, following the game, children were interviewed on what gender they perceived the robot to have. Results showed that younger children (5\u20138 years) were less likely to assign a gender to the robot based on the perceived voice in comparison to older children (9\u201312 years), showing that younger children tended to attribute a gender to the robot independently of its voice. A potential gender categorisation bias could affect the performance of Sam as an auditory test interface, as Experiment II showed a bias towards male categorisations even when stimuli were presented at the reference female voice. While this bias needs to be further investigated to confirm if it exists, there is a possibility that despite the attempts at gender-neutrality when referring to Sam (such as always referring to it as \u201cSam\u201d and avoiding the use of gendered pronouns when talking about the robot to participants), participants may have inadvertently attributed a male gender to Sam, potentially biassing their categorisations. In contrast, however, adults seem to assign a gender to a robot based on the voice, even when the robot is presented as gender neutral [69]. When the robot body is presented in a gender specific manner, adults seem to further\nPLOS ONE | https://doi.org/10.1371/journal.pone.0294328 December 13, 2023 20 / 27\nenforce their stereotypical perception of the robot\u2019s gender [72, 73]. Further, adults even seem to have preferences for different voices for different task applications [74]. These results suggest that in identifying a voice presented via a humanoid robot, the perceived gender of the voice may be biassed based on the appearance of the robot and the context, and vice versa, potentially affecting the overall voice perception process. Since we have not explicitly studied this potential bias, which may also be part of the laptop version, we cannot yet conclude with certainty if one interface would have more bias than the other. Nevertheless, the potential gender attribution to Sam will be investigated in the follow-up human robot interaction study."
        },
        {
            "heading": "Comparison of test durations",
            "text": "In so far as an auditory interface, we have shown Sam to be comparable to the laptop and previous similar studies with regard to the voice cue sensitivity and voice gender categorisation tests; however, it is also important to address the increased durations of the tests. In some cases, the longer test duration could be attributed to the design of the test implementation on Sam. One such factor that affected the increased duration was the stimulus processing time on Sam in Experiment I. Although a stimulus took on average two seconds to process on Sam, on the laptop it was around one second, on average resulting in a doubling of the time to complete the test on Sam. Having observed how much longer it takes to real-time process speech stimuli by the robot, for further applications and until there are substantial improvements to the robot processing power, using offline processed stimuli for a robot interface for auditory testing could be recommended (similar to Experiment II). In Experiment II, the increased duration of the test was due to Sam indicating which hand to use for the two gender categories (taking five seconds) after each of the 36 stimuli. This resulted in the presentation of stimuli at a slower rate than with the laptop, adding an extra three minutes to the total test duration.\nIn summary, despite the limitations due to the design of the implementation on Sam, it was\nobserved that auditory perception testing results collected on Sam were still comparable to those from the laptop and previously reported data. While we will conduct a detailed analysis of the human robot interaction-related aspects, the observations of the present study support the potential for Sam to conduct longer auditory perception tests."
        },
        {
            "heading": "A word on inferential statistics",
            "text": "The decision to use both classical (Frequentist) and Bayesian inference in this manuscript was primarily due to the design and expected outcome of the experiments. When using the classical p-value inferential approach, often the desired outcome is a small enough p-value to infer that any difference in the data is not due to chance but can also be extrapolated to a larger population. However, when the p-value is large, this is interpreted as insufficient certainty as to whether a difference exists. In comparison, because Bayesian inference is solely focussed on the observed data (and not based on a hypothetical data set as with classical inference) [56], it provides an alternative interpretation of the data; how much evidence (based on the observed data) can be attributed to the presence or absence of an effect.\nHowever, it must be noted that for both statistical approaches in both experiments, due to\nthe respective effect sizes and anecdotal evidence, results reported here are likely affected by the sample size. The benefit of using the Bayesian approach is that the current results can be used as the prior for follow-up tests using identical setups, thus providing stronger inferential power. In comparison, the use of the p-value (the Frequentist approach) would require either conducting the same experiment again with a new hypothesis and sample or redesigning of the experimental setup.\nPLOS ONE | https://doi.org/10.1371/journal.pone.0294328 December 13, 2023 21 / 27\nExamining the conducted experiments closer, the difference between the frequentist and\nBayesian results regarding the gender categorisation between the interfaces shows one of the benefits of using the Bayesian approach. Based on frequentist methods, a statistically significant difference, without reporting an effect size (such as Cohen\u2019s d), would infer that the categorisation of voice gender is significantly dependent on the interface used. However, this effect size is included in the Bayesian result, and thus with a single statistic, it can be inferred that although there may be a difference in the results, there is only anecdotal evidence supporting it, and thus likely is limited to the observed data.\nComparing the results of both the classical and Bayesian approaches in the above experi-\nments, consistency can be observed between statistical significance (classical) and evidence (Bayesian) based approaches. In the first experiment, the only discrepancy between the two approaches to the statistics was for any difference between the F0 JND thresholds obtained on either interface. The small effect size (0.084) and anecdotal evidence (BF10 = 1.638) are both good indicators that based on the observed data, it cannot be concluded if there is indeed a difference between the two interfaces for this vocal cue, but also that there is no difference. Therefore, this would require further testing to appropriately draw a conclusion. Similarly, in Experiment II the only discrepancy between the classical and Bayesian statistics was the overall voice gender categorisation between the two interfaces. These results too had a small effect size (0.1) and low Bayes factor (BF10 = 1.248), and thus again, no appropriate conclusion can be drawn as to the existence of a difference between the two interfaces."
        },
        {
            "heading": "Concluding remarks",
            "text": "The auditory performance results of the present study show promise for conducting the PICKA psychophysics tasks, insofar as the voice cue sensitivity and voice gender categorisation tests are concerned on Sam. Furthermore, our results contribute to the growing potential of using humanoid robots for both learning and testing applications [34\u201336, 75, 76], also for specific target groups and rehabilitation applications (e.g., [43, 77, 78]), and our general understanding of speech communication and voice perception in HRI, an increasingly relevant topic for social robots [1, 13, 79, 80].\nTo not only further reduce the duration of the tests on Sam, but also to negate possibility con-\nfounding effects due to inconsistent positive and negative feedback between the two interfaces [81], the visual feedback during the voice cue sensitivity test could be simplified such that only positive feedback is presented (nodding only when the given response is correct); otherwise, continuing with the next stimulus, shortening the test duration. Although this feedback only takes one or two seconds, as mentioned with the hand identification during the gender categorisation test, the summation of these delays after each stimulus can result in a longer than intended test.\nAn additional, albeit more complex, technical modification that could be made to the imple-\nmentation of both tests could be the incorporation of automatic speech recognition (ASR). Rather than logging responses by touching Sam\u2019s sensors, verbal responses could be given. Although the NAO does have speech recognition capabilities, it is not expected that it would be sufficient for such testing purposes, as the speech recognition module performs poorly and often requires higher than natural speech volumes in order to recognise any speech. However, should this problem be overcome through the use of third-party software, such as the speech recognition system Kaldi [82], this could be a viable alternative to the tactile response logging method."
        },
        {
            "heading": "Acknowledgments",
            "text": "We thank Josephine Marriage and Debi Vickers for English stimuli, Paolo Toffanin, Iris van Bommel, Evelien Birza, Jacqueline Libert, and Jop Luberti for their contribution to the\nPLOS ONE | https://doi.org/10.1371/journal.pone.0294328 December 13, 2023 22 / 27\ndevelopment of the PICKA test battery. Parts of the study were conducted as master\u2019s thesis projects by Thirsa Huisman (supervised by Leanne Nagels), and Luke Meyer."
        },
        {
            "heading": "Author Contributions",
            "text": "Conceptualization: Luke Meyer, Etienne Gaudrain, Deniz Ba\u015fkent.\nData curation: Luke Meyer.\nFormal analysis: Luke Meyer, Etienne Gaudrain.\nFunding acquisition: Luke Meyer, Deniz Ba\u015fkent.\nInvestigation: Luke Meyer, Laura Rachman.\nMethodology: Luke Meyer, Laura Rachman, Etienne Gaudrain, Deniz Ba\u015fkent.\nProject administration: Luke Meyer, Deniz Ba\u015fkent.\nResources: Luke Meyer, Laura Rachman, Deniz Ba\u015fkent.\nSoftware: Luke Meyer.\nSupervision: Laura Rachman, Gloria Araiza-Illan, Deniz Ba\u015fkent.\nVisualization: Luke Meyer, Etienne Gaudrain, Deniz Ba\u015fkent.\nWriting \u2013 original draft: Luke Meyer.\nWriting \u2013 review & editing: Luke Meyer, Laura Rachman, Gloria Araiza-Illan, Etienne Gau-\ndrain, Deniz Ba\u015fkent."
        }
    ],
    "title": "Use of a humanoid robot for auditory psychophysical testing",
    "year": 2023
}