{
    "abstractText": "Over the years, 2D GANs have achieved great successes in photorealistic portrait generation. However, they lack 3D understanding in the generation process, thus they suffer from multi-view inconsistency problem. To alleviate the issue, many 3D-aware GANs have been proposed and shown notable results, but 3D GANs struggle with editing semantic attributes. The controllability and interpretability of 3D GANs have not been much explored. In this work, we propose two solutions to overcome these weaknesses of 2D GANs and 3D-aware GANs. We first introduce a novel 3D-aware GAN, SURF-GAN, which is capable of discovering semantic attributes during training and controlling them in an unsupervised manner. After that, we inject the prior of SURF-GAN into StyleGAN to obtain a high-fidelity 3D-controllable generator. Unlike existing latent-based methods allowing implicit pose control, the proposed 3D-controllable StyleGAN enables explicit pose control over portrait generation. This distillation allows direct compatibility between 3D control and many StyleGAN-based techniques (e.g., inversion and stylization), and also brings an advantage in terms of computational resources. Our codes are available at https://github.com/jgkwak95/SURF-GAN.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jeong-gi Kwak"
        },
        {
            "affiliations": [],
            "name": "Yuanming Li"
        },
        {
            "affiliations": [],
            "name": "Dongsik Yoon"
        },
        {
            "affiliations": [],
            "name": "Donghyeon Kim"
        },
        {
            "affiliations": [],
            "name": "David Han"
        },
        {
            "affiliations": [],
            "name": "Hanseok Ko"
        }
    ],
    "id": "SP:8ced120a6ad92ec5ff64be771c458cebeacbc4b3",
    "references": [
        {
            "authors": [
                "R. Abdal",
                "Y. Qin",
                "P. Wonka"
            ],
            "title": "Image2stylegan: How to embed images into the stylegan latent space? In: Conference on Computer Vision and Pattern Recognition",
            "year": 2019
        },
        {
            "authors": [
                "R. Abdal",
                "Y. Qin",
                "P. Wonka"
            ],
            "title": "Image2stylegan++: How to edit the embedded images? In: Conference on Computer Vision and Pattern Recognition",
            "year": 2020
        },
        {
            "authors": [
                "R. Abdal",
                "P. Zhu",
                "N.J. Mitra",
                "P. Wonka"
            ],
            "title": "Styleflow: Attribute-conditioned exploration of stylegan-generated images using conditional continuous normalizing flows",
            "venue": "ACM Transactions on Graphics (TOG)",
            "year": 2021
        },
        {
            "authors": [
                "Y. Alaluf",
                "O. Patashnik",
                "D. Cohen-Or"
            ],
            "title": "Restyle: A residual-based stylegan encoder via iterative refinement",
            "venue": "International Conference on Computer Vision (ICCV)",
            "year": 2021
        },
        {
            "authors": [
                "Y. Alaluf",
                "O. Tov",
                "R. Mokady",
                "R. Gal",
                "A.H. Bermano"
            ],
            "title": "Hyperstyle: Stylegan inversion with hypernetworks for real image editing",
            "venue": "arXiv",
            "year": 2021
        },
        {
            "authors": [
                "T. Baltrusaitis",
                "A. Zadeh",
                "Y.C. Lim",
                "L.P. Morency"
            ],
            "title": "Openface 2.0: Facial behavior analysis toolkit. In: International conference on automatic face & gesture recognition",
            "year": 2018
        },
        {
            "authors": [
                "A. Brock",
                "J. Donahue",
                "K. Simonyan"
            ],
            "title": "Large scale GAN training for high fidelity natural image synthesis",
            "venue": "International Conference on Learning Representations (ICLR)",
            "year": 2019
        },
        {
            "authors": [
                "E.R. Chan",
                "C.Z. Lin",
                "M.A. Chan",
                "K. Nagano",
                "B. Pan",
                "S. De Mello",
                "O. Gallo",
                "L.J. Guibas",
                "J. Tremblay",
                "S Khamis"
            ],
            "title": "Efficient geometry-aware 3d generative adversarial networks",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2022
        },
        {
            "authors": [
                "E.R. Chan",
                "M. Monteiro",
                "P. Kellnhofer",
                "J. Wu",
                "G. Wetzstein"
            ],
            "title": "pi-gan: Periodic implicit generative adversarial networks for 3d-aware image synthesis",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2021
        },
        {
            "authors": [
                "A. Chen",
                "R. Liu",
                "L. Xie",
                "Z. Chen",
                "H. Su",
                "J. Yu"
            ],
            "title": "Sofgan: A portrait image generator with dynamic styling",
            "venue": "Transactions on Graphics (TOG)",
            "year": 2022
        },
        {
            "authors": [
                "X. Chen",
                "Y. Duan",
                "R. Houthooft",
                "J. Schulman",
                "I. Sutskever",
                "P. Abbeel"
            ],
            "title": "Infogan: Interpretable representation learning by information maximizing generative adversarial nets",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
            "year": 2016
        },
        {
            "authors": [
                "J. Deng",
                "J. Guo",
                "N. Xue",
                "S. Zafeiriou"
            ],
            "title": "Arcface: Additive angular margin loss for deep face recognition",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2019
        },
        {
            "authors": [
                "Y. Deng",
                "J. Yang",
                "D. Chen",
                "F. Wen",
                "X. Tong"
            ],
            "title": "Disentangled and controllable face image generation via 3d imitative-contrastive learning",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2020
        },
        {
            "authors": [
                "Y. Deng",
                "J. Yang",
                "J. Xiang",
                "X. Tong"
            ],
            "title": "GRAM: Generative radiance manifolds for 3d-aware image generation",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2022
        },
        {
            "authors": [
                "M. Gadelha",
                "S. Maji",
                "R. Wang"
            ],
            "title": "3d shape induction from 2d views of multiple objects",
            "venue": "International Conference on 3D Vision (3DV)",
            "year": 2017
        },
        {
            "authors": [
                "I. Goodfellow",
                "J. Pouget-Abadie",
                "M. Mirza",
                "B. Xu",
                "D. Warde-Farley",
                "S. Ozair",
                "A. Courville",
                "Y. Bengio"
            ],
            "title": "Generative adversarial nets",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
            "year": 2014
        },
        {
            "authors": [
                "J. Gu",
                "L. Liu",
                "P. Wang",
                "C. Theobalt"
            ],
            "title": "StyleNeRF: A style-based 3d-aware generator for high-resolution image synthesis",
            "venue": "arXiv",
            "year": 2021
        },
        {
            "authors": [
                "J. Guo",
                "X. Zhu",
                "Y. Yang",
                "F. Yang",
                "Z. Lei",
                "S.Z. Li"
            ],
            "title": "Towards fast, accurate and stable 3d dense face alignment",
            "venue": "European Conference on Computer Vision (ECCV)",
            "year": 2020
        },
        {
            "authors": [
                "E. H\u00e4rk\u00f6nen",
                "A. Hertzmann",
                "J. Lehtinen",
                "S. Paris"
            ],
            "title": "Ganspace: Discovering interpretable gan controls",
            "venue": "arXiv",
            "year": 2020
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2016
        },
        {
            "authors": [
                "Z. He",
                "M. Kan",
                "S. Shan"
            ],
            "title": "EigenGAN: Layer-wise eigen-learning for gans",
            "venue": "International Conference on Computer Vision (ICCV)",
            "year": 2021
        },
        {
            "authors": [
                "P. Henzler",
                "N.J. Mitra",
                "T. Ritschel"
            ],
            "title": "Escaping plato\u2019s cave: 3d shape from adversarial rendering",
            "venue": "International Conference on Computer Vision (ICCV)",
            "year": 2019
        },
        {
            "authors": [
                "M. Heusel",
                "H. Ramsauer",
                "T. Unterthiner",
                "B. Nessler",
                "S. Hochreiter"
            ],
            "title": "GANs trained by a two time-scale update rule converge to a local nash equilibrium",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
            "year": 2017
        },
        {
            "authors": [
                "Y. Hu",
                "X. Wu",
                "B. Yu",
                "R. He",
                "Z. Sun"
            ],
            "title": "Pose-guided photorealistic face rotation",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2018
        },
        {
            "authors": [
                "I. Jeon",
                "W. Lee",
                "M. Pyeon",
                "G. Kim"
            ],
            "title": "Ib-gan: Disengangled representation learning with information bottleneck generative adversarial networks",
            "venue": "AAAI Conference on Artificial Intelligence (AAAI)",
            "year": 2021
        },
        {
            "authors": [
                "J.T. Kajiya",
                "B.P. Von Herzen"
            ],
            "title": "Ray tracing volume densities",
            "venue": "SIGGRAPH",
            "year": 1984
        },
        {
            "authors": [
                "T. Kaneko",
                "K. Hiramatsu",
                "K. Kashino"
            ],
            "title": "Generative attribute controller with conditional filtered generative adversarial networks",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2017
        },
        {
            "authors": [
                "T. Karras",
                "T. Aila",
                "S. Laine",
                "J. Lehtinen"
            ],
            "title": "Progressive growing of GANs for improved quality, stability, and variation",
            "venue": "International Conference on Learning Representations (ICLR)",
            "year": 2018
        },
        {
            "authors": [
                "T. Karras",
                "M. Aittala",
                "J. Hellsten",
                "S. Laine",
                "J. Lehtinen",
                "T. Aila"
            ],
            "title": "Training generative adversarial networks with limited data",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
            "year": 2020
        },
        {
            "authors": [
                "T. Karras",
                "M. Aittala",
                "S. Laine",
                "E. H\u00e4rk\u00f6nen",
                "J. Hellsten",
                "J. Lehtinen",
                "T. Aila"
            ],
            "title": "Alias-free generative adversarial networks",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
            "year": 2021
        },
        {
            "authors": [
                "T. Karras",
                "S. Laine",
                "T. Aila"
            ],
            "title": "A style-based generator architecture for generative adversarial networks",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2019
        },
        {
            "authors": [
                "T. Karras",
                "S. Laine",
                "M. Aittala",
                "J. Hellsten",
                "J. Lehtinen",
                "T. Aila"
            ],
            "title": "Analyzing and improving the image quality of stylegan",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2020
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "year": 2014
        },
        {
            "authors": [
                "M. Kowalski",
                "S.J. Garbin",
                "V. Estellers",
                "T. Baltru\u0161aitis",
                "M. Johnson",
                "J. Shotton"
            ],
            "title": "Config: Controllable neural face image generation",
            "venue": "European Conference on Computer Vision (ECCV)",
            "year": 2020
        },
        {
            "authors": [
                "Kwak",
                "J.g.",
                "Y. Li",
                "D. Yoon",
                "D. Han",
                "H. Ko"
            ],
            "title": "Generate and edit your own character in a canonical view",
            "venue": "arXiv",
            "year": 2022
        },
        {
            "authors": [
                "W. Lee",
                "D. Kim",
                "S. Hong",
                "H. Lee"
            ],
            "title": "High-fidelity synthesis with disentangled representation",
            "venue": "European Conference on Computer Vision (ECCV)",
            "year": 2020
        },
        {
            "authors": [
                "Y. Liao",
                "K. Schwarz",
                "L. Mescheder",
                "A. Geiger"
            ],
            "title": "Towards unsupervised learning of generative models for 3d controllable image synthesis",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2020
        },
        {
            "authors": [
                "Z. Liu",
                "P. Luo",
                "X. Wang",
                "X. Tang"
            ],
            "title": "Deep learning face attributes in the wild",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2015
        },
        {
            "authors": [
                "S. Lunz",
                "Y. Li",
                "A. Fitzgibbon",
                "N. Kushman"
            ],
            "title": "Inverse graphics gan: Learning to generate 3d shapes from unstructured 2d data",
            "venue": "arXiv",
            "year": 2020
        },
        {
            "authors": [
                "L. Mescheder",
                "A. Geiger",
                "S. Nowozin"
            ],
            "title": "Which training methods for gans do actually converge? In: International Conference on Learning Representations (ICLR",
            "year": 2018
        },
        {
            "authors": [
                "B. Mildenhall",
                "P.P. Srinivasan",
                "M. Tancik",
                "J.T. Barron",
                "R. Ramamoorthi",
                "R. Ng"
            ],
            "title": "NeRF: Representing scenes as neural radiance fields for view synthesis",
            "venue": "European Conference on Computer Vision (ECCV)",
            "year": 2020
        },
        {
            "authors": [
                "T. Nguyen-Phuoc",
                "C. Li",
                "L. Theis",
                "C. Richardt",
                "Y.L. Yang"
            ],
            "title": "HoloGAN: Unsupervised learning of 3d representations from natural images",
            "venue": "International Conference on Computer Vision (ICCV)",
            "year": 2019
        },
        {
            "authors": [
                "T. Nguyen-Phuoc",
                "C. Richardt",
                "L. Mai",
                "Y.L. Yang",
                "N. Mitra"
            ],
            "title": "BlockGAN: Learning 3d object-aware scene representations from unlabelled images",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
            "year": 2020
        },
        {
            "authors": [
                "M. Niemeyer",
                "A. Geiger"
            ],
            "title": "Giraffe: Representing scenes as compositional generative neural feature fields",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2021
        },
        {
            "authors": [
                "Y. Nitzan",
                "R. Gal",
                "O. Brenner",
                "D. Cohen-Or"
            ],
            "title": "Large: Latent-based regression through gan semantics",
            "venue": "arXiv",
            "year": 2021
        },
        {
            "authors": [
                "R. Or-El",
                "X. Luo",
                "M. Shan",
                "E. Shechtman",
                "J.J. Park",
                "I. Kemelmacher-Shlizerman"
            ],
            "title": "StyleSDF: High-resolution 3d-consistent image and geometry generation",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2022
        },
        {
            "authors": [
                "X. Pan",
                "B. Dai",
                "Z. Liu",
                "C.C. Loy",
                "P. Luo"
            ],
            "title": "Do 2d gans know 3d shape? unsupervised 3d shape reconstruction from 2d image gans",
            "venue": "International Conference on Learning Representations (ICLR)",
            "year": 2021
        },
        {
            "authors": [
                "O. Patashnik",
                "Z. Wu",
                "E. Shechtman",
                "D. Cohen-Or",
                "D. Lischinski"
            ],
            "title": "Styleclip: Text-driven manipulation of stylegan imagery",
            "venue": "International Conference on Computer Vision (ICCV)",
            "year": 2021
        },
        {
            "authors": [
                "E. Perez",
                "F. Strub",
                "H. de Vries",
                "V. Dumoulin",
                "A.C. Courville"
            ],
            "title": "Film: Visual reasoning with a general conditioning layer",
            "venue": "AAAI Conference on Artificial Intelligence (AAAI)",
            "year": 2018
        },
        {
            "authors": [
                "J.N. Pinkney",
                "D. Adler"
            ],
            "title": "Resolution dependent gan interpolation for controllable image synthesis between domains",
            "venue": "arXiv",
            "year": 2020
        },
        {
            "authors": [
                "A. Radford",
                "L. Metz",
                "S. Chintala"
            ],
            "title": "Unsupervised representation learning with deep convolutional generative adversarial networks",
            "venue": "arXiv",
            "year": 2015
        },
        {
            "authors": [
                "E. Richardson",
                "Y. Alaluf",
                "O. Patashnik",
                "Y. Nitzan",
                "Y. Azar",
                "S. Shapiro",
                "D. CohenOr"
            ],
            "title": "Encoding in style: a stylegan encoder for image-to-image translation",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2021
        },
        {
            "authors": [
                "D. Roich",
                "R. Mokady",
                "A.H. Bermano",
                "D. Cohen-Or"
            ],
            "title": "Pivotal tuning for latentbased editing of real images",
            "venue": "arXiv",
            "year": 2021
        },
        {
            "authors": [
                "K. Schwarz",
                "Y. Liao",
                "M. Niemeyer",
                "A. Geiger"
            ],
            "title": "GRAF: Generative radiance fields for 3d-aware image synthesis",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
            "year": 2020
        },
        {
            "authors": [
                "Y. Shen",
                "J. Gu",
                "X. Tang",
                "B. Zhou"
            ],
            "title": "Interpreting the latent space of gans for semantic face editing",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2020
        },
        {
            "authors": [
                "Y. Shen",
                "B. Zhou"
            ],
            "title": "Closed-form factorization of latent semantics in gans",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2021
        },
        {
            "authors": [
                "Y. Shi",
                "D. Aggarwal",
                "A.K. Jain"
            ],
            "title": "Lifting 2d stylegan for 3d-aware face generation",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2021
        },
        {
            "authors": [
                "A. Shoshan",
                "N. Bhonker",
                "I. Kviatkovsky",
                "G. Medioni"
            ],
            "title": "Gan-control: Explicitly controllable gans",
            "venue": "International Conference on Computer Vision (ICCV)",
            "year": 2021
        },
        {
            "authors": [
                "V. Sitzmann",
                "J. Martel",
                "A. Bergman",
                "D. Lindell",
                "G. Wetzstein"
            ],
            "title": "Implicit neural representations with periodic activation functions",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
            "year": 2020
        },
        {
            "authors": [
                "J. Sun",
                "X. Wang",
                "Y. Zhang",
                "X. Li",
                "Q. Zhang",
                "Y. Liu",
                "J. Wang"
            ],
            "title": "Fenerf: Face editing in neural radiance fields",
            "venue": "arXiv",
            "year": 2021
        },
        {
            "authors": [
                "A. Szab\u00f3",
                "G. Meishvili",
                "P. Favaro"
            ],
            "title": "Unsupervised generative 3d shape learning from natural images",
            "venue": "arXiv",
            "year": 2019
        },
        {
            "authors": [
                "A. Tewari",
                "M. Elgharib",
                "F. Bernard",
                "H.P. Seidel",
                "P. P\u00e9rez",
                "M. Zollh\u00f6fer",
                "C. Theobalt"
            ],
            "title": "Pie: Portrait image embedding for semantic control",
            "venue": "ACM Transactions on Graphics (TOG)",
            "year": 2020
        },
        {
            "authors": [
                "A. Tewari",
                "M. Elgharib",
                "G. Bharaj",
                "F. Bernard",
                "H.P. Seidel",
                "P. Perez",
                "M. Zollhofer",
                "C. Theobalt"
            ],
            "title": "Stylerig: Rigging stylegan for 3d control over portrait images",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2020
        },
        {
            "authors": [
                "O. Tov",
                "Y. Alaluf",
                "Y. Nitzan",
                "O. Patashnik",
                "D. Cohen-Or"
            ],
            "title": "Designing an encoder for stylegan image manipulation",
            "venue": "ACM Transactions on Graphics (TOG)",
            "year": 2021
        },
        {
            "authors": [
                "S. Wu",
                "C. Rupprecht",
                "A. Vedaldi"
            ],
            "title": "Unsupervised learning of probably symmetric deformable 3d objects from images in the wild",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2020
        },
        {
            "authors": [
                "Y. Xue",
                "Y. Li",
                "K.K. Singh",
                "Y.J. Lee"
            ],
            "title": "GIRAFFE HD: A high-resolution 3d-aware generative model",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2022
        },
        {
            "authors": [
                "X. Yao",
                "A. Newson",
                "Y. Gousseau",
                "P. Hellier"
            ],
            "title": "A latent transformer for disentangled face editing in images and videos",
            "venue": "International Conference on Computer Vision (ICCV)",
            "year": 2021
        },
        {
            "authors": [
                "X. Yin",
                "X. Yu",
                "K. Sohn",
                "X. Liu",
                "M. Chandraker"
            ],
            "title": "Towards large-pose face frontalization in the wild",
            "venue": "International Conference on Computer Vision (ICCV)",
            "year": 2017
        },
        {
            "authors": [
                "R. Zhang",
                "P. Isola",
                "A.A. Efros",
                "E. Shechtman",
                "O. Wang"
            ],
            "title": "The unreasonable effectiveness of deep features as a perceptual metric",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2018
        },
        {
            "authors": [
                "Y. Zhang",
                "W. Chen",
                "H. Ling",
                "J. Gao",
                "Y. Zhang",
                "A. Torralba",
                "S. Fidler"
            ],
            "title": "Image gans meet differentiable rendering for inverse graphics and interpretable 3d neural rendering",
            "venue": "arXiv",
            "year": 2020
        },
        {
            "authors": [
                "H. Zhou",
                "J. Liu",
                "Z. Liu",
                "Y. Liu",
                "X. Wang"
            ],
            "title": "Rotate-and-render: Unsupervised photorealistic face rotation from single-view images",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2020
        },
        {
            "authors": [
                "P. Zhou",
                "L. Xie",
                "B. Ni",
                "Q. Tian"
            ],
            "title": "Cips-3d: A 3d-aware generator of gans based on conditionally-independent pixel synthesis",
            "venue": "arXiv",
            "year": 2021
        },
        {
            "authors": [
                "J. Zhu",
                "Y. Shen",
                "D. Zhao",
                "B. Zhou"
            ],
            "title": "In-domain gan inversion for real image editing",
            "venue": "European Conference on Computer Vision (ECCV)",
            "year": 2020
        },
        {
            "authors": [
                "J.Y. Zhu",
                "Z. Zhang",
                "C. Zhang",
                "J. Wu",
                "A. Torralba",
                "J.B. Tenenbaum",
                "W.T. Freeman"
            ],
            "title": "Visual Object Networks: Image generation with disentangled 3D representations",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "Keywords: 3D-aware portrait generation, pose-disentangled GAN, facial image editing, novel view synthesis, latent manipulation"
        },
        {
            "heading": "1 Introduction",
            "text": "Since the advent of Generative Adversarial Networks (GANs) [16], remarkable progress has been made in the field of photorealistic image generation. The quality and diversity of images generated by 2D GANs have been improved considerably and recent models [28,7,31,32,30] can produce high resolution images at a level that humans cannot distinguish. Despite the expressiveness of 2D GANs, they lack 3D understanding, in that the underlying 3D geometry of an object is ignored in the generation process. As a result, they suffer the problem of multiview inconsistency. To overcome the issue, many researchers have studied 3D controllable image synthesis and it has become one of the mainstream research\nar X\niv :2\n20 7.\n10 25\n7v 2\n[ cs\n.C V\n] 2\n6 Ju\nl 2 02\nin the community. There have been several attempts to learn 3D pose information with 2D GAN by disentangling pose in the latent space, but they require auxiliary 3D supervision such as synthetic face dataset [34] or 3DMM [13,63,62]. In addition, a few unsupervised approaches have been proposed by adopting implicit 3D feature [42,43] or differentiable renderer [57,47] in generation. However, these methods have struggled with multi-view consistency and photorealism.\nSince the introduction of neural radiance fields (NeRF) by Mildenhall et al. [41] which has achieved notable success in novel view synthesis, a new paradigm has emerged in 3D-aware generation, called 3D-aware GAN. Several researchers have proposed 3D-aware generative frameworks [54,44,9,72,17,14,66] by leveraging NeRF as a 3D representation in GAN generator. NeRF-GANs learn 3D geometry from unlabelled images yet allow accurate and explicit control of 3D camera based on a volume rendering. Despite the obvious advantages, 3D GANs based on a pure NeRF network require tremendous computational resources and generate blurry images. Very recently, several approaches have alleviated the problems and have shown photorealistic output with high resolution by incorporating rear-end 2D networks [72,17,8,14,66]. However 3D GANs have difficulty with attribute-controllable generation or real image editing because their latent space has been rarely investigated for interpretable generation (Fig. 2).\nIn summary, these two distinct approaches have strengths and weaknesses that are complementary: 3D-aware GAN can generate novel poses but it has trouble with disentangling and manipulating attributes; 2D GAN is capable of controlling attributes but it struggles with 3D controllability. In this work, we propose novel solutions to overcome each weakness of 2D GANs and 3D GANs.\nFirst, we propose a novel 3D-aware GAN, i.e., SURF-GAN, which can discover semantic attributes by learning layer-wise SUbspace in INR NeRF-based\ngenerator in an unsupervised manner. The discovered semantic vectors can be controlled by corresponding parameters, thus this property allows us to manipulate semantic attributes (e.g., gender, hair color, etc.) as well as explicit pose.\nWith the proposed SURF-GAN, we take one more step to transform StyleGAN into a 3D-controllable generator. We inject the prior of 3D-aware SURFGAN into the expressive and disentangled latent space of 2D StyleGAN. Unlike the previous methods [55,19,56] that allows implicit pose control, we make StyleGAN enable explicit control over pose. It means that the generator is capable of synthesizing accurate images based on a conditioned target view. By utilizing SURF-GAN which consists of pure NeRF layers as a generator of pseudo multi-view images, the transformed StyleGAN can learn elaborate control over 3D camera pose with latent manipulation. To this end, we proposed a method to find several orthogonal directions (not a single) related to the same pose attribute, and explicit control over the pose is accomplished by a combination of these directions. With a GAN inversion encoder, 3D controllable StyleGAN can be extended to the task of novel pose synthesis from a real image.\nIn addition to 3D perception, we also inject the controllability about semantic attributes that SURF-GAN finds. We can find more pose-robust latent path in the latent space of StyleGAN because SURF-GAN can manipulate a specific semantic while keeping view direction unchanged. Moreover, it allows further applications related to StyleGAN family, e.g., 3D control over stylized images generated by fine-tuned StyleGAN. It is notable that our approach neither requires 3D supervision nor exploits auxiliary off-the-shelf 3D models (e.g., 3DMM or pose detector) in both training and inference because SURF-GAN learns 3D geometry from unlabelled 2D images from scratch.\nIn summary, our contributions are as follow:\n\u2013 We propose a novel 3D-aware GAN, called SURF-GAN, which can discover controllable semantic attributes in an unsupervised manner.\n\u2013 By injecting editing directions from the low-resolution 3D-aware GAN into the high-resolution 2D StyleGAN, we achieve a 3D controllable generator which is capable of explicit control over pose and 3D consistent editing.\n\u2013 Our method is directly compatible with various well-studied 2D StyleGANbased techniques such as inversion, editing or stylization."
        },
        {
            "heading": "2 Related Work",
            "text": "Pose-disentangled GANs. The remarkable advances have been achieved in photorealism by state-of-the-art GAN models [28,7,31,32,30]. However, pose control by image generators has been limited due to a lack of 3D understanding in the synthesizing process. Thereby, several works have attempted to disentangle the pose information from other attributes in 2D GANs. The disentanglement has been achieved by leveraging supervision such as 3DMM [13,63,62,68], landmark [24], synthetic images from 3D engine [34] or pose detector [58]. A few unsupervised approaches without 3D supervision [42,43] have been proposed by disentangling pose with implicit 3D feature projection, but they allow only implicit 3D control and show blurry results. Recently, a few methods [57,47] have incorporated a pre-trained StyleGAN with a differentiable renderer, but they struggle with photorealism, high-resolution [47] and real image editing [57].\nInterpretabilty and controllabiltiy of GAN. The well-trained 2D GANs, such as StyleGAN [31,32] have shown capable of disentangling the latent space. Recent works [55,19,3,45,56,67,48] have demonstrated semantic manipulation, especially for facial attributes, by analyzing the manifold and finding meaningful direction or mapping. Combining with GAN inversion [1,73,2,52,64,53,4,5], the applications of 2D GANs have been extended to real image editing. Alternatively, there have been studies [11,27,36,25] that discover and disentangle latent embeddings into interpretable dimensions during training of the generator. EigenGAN [21] that inspired our approach has demonstrated interpretable latent dimensions by designing layer-wise subspace embedding. However, both types of methods support implicit control over the discovered semantics. In the case of a pose that can be defined with camera parameters, these methods struggle to synthesize explicit novel view elaborately. Of course, the implicit methods can eventually create the desired pose through manual and iterative adjustment, but this is not an ideal situation. We can obtain a frontalized image automatically with some latent-based methods [55,35,52], but not for arbitrary target pose. Recently, Chen et al. [10] have introduced a generator allowing explicit control over pose, but it requires 3D mesh for pre-training process.\n3D-aware GANs. Beyond the disentanglement of pose information, many efforts have been made to obtain 3D-awareness in generation. Earlier methods have adopted several explicit 3D representations in 2D image generation such as voxel [74,39,22,15] or mesh [37,61]. However, they suffer from a lack of visual quality and limited resolution. Recently, approaches [54,9,44,17,72,8,14,46,66] based on neural fields have made significant progress in photorealism and 3D consistency. Nevertheless, these 3D-aware GANs have weakness in finding and editing semantic attribute because their latent space has been rarely investigated. Very recently, Sun et al. [60] have proposed an editable NeRF-GAN, but it does not handle diverse semantic attributes and requires semantic maps as supervision. In addition, 3D GANs struggle with novel pose generation of real image\ndespite their capability of multi-view consistency. Recently proposed EG3D [8] has shown experiments of novel view synthesis and presented outstanding results, but it requires iterative optimization for latent code and fine-tuning of the generator [53] for each target image."
        },
        {
            "heading": "3 Proposed method",
            "text": "In this section, we describe our method, by first introducing SURF-GAN in detail and then by explaining a method to inject the prior of 3D SURF-GAN into 2D StyleGAN. Note that the word \u201cStyleGAN\u201d denotes StyleGAN2 [32]."
        },
        {
            "heading": "3.1 Towards controllable NeRF-GAN",
            "text": "Preliminaries: NeRF-GANs. Existing 2D GANs (e.g., StyleGAN [31,32]) synthesize output image directly with sampled latent vector. However, NeRFGANs [9,44,17,72] generate a radiance field [41] before rendering 2D image. Given a position x \u2208 R3 and a viewing direction v \u2208 S2, it predicts a volume density \u03c3(x) \u2208 R+ and the view-dependent RGB color c(x,v) \u2208 R3 of the input point. The points are sampled from rays of camera, and then an image is rendered into 2D grid with a classic volume rendering technique [26]. To produce diverse images, existing NeRF-GAN methods adopt StyleGAN-like modulation, where some components in the implicit neural network, e.g., intermediate features [9,72] or weight matrices [17] are modulated by sampled noise passing though a mapping network. Thereby, NeRF-GAN can control the pose by manipulating viewing direction v and change identity by injecting different noise vector. Nevertheless, it is ambiguous how to interpret the latent space and how to disentangle semantic attributes of NeRF-GAN for controllable image generation.\nLearning layer-wise subspace in NeRF network. Inspired by EigenGAN [21], we adopt a different strategy from the existing methods [9,72] those modulation is obtained by the mapping network consisting of several MLPs. EigenGAN\nlearns interpretable subspaces in layers of its generator during training. However, EigenGAN is typical 2D convolution-based GAN framework, thus its concept is inapplicable to INR based NeRF-GAN. Therefore, we propose a novel framework (i.e., SURF-GAN), which captures the disentangled attributes in layers of NeRF network. Fig. 3 shows the overview of SURF-GAN. The generator consists of t+1 SURF blocks (t for shared layers and one for color layer). Following \u03c0-GAN, SURF block adopts the feature-wise linear modulation (FiLM) [49] to transform the intermediate features with frequencies \u03b3i and phase shifts \u03b2i, and followed SIREN activation [59]. SURF block in ith layer is formulated as\n\u03c8i = SURFi ( \u03c8i\u22121,\u03d5i ) = sin ( \u03b3i \u00b7 ( Wi\u03c8i\u22121 + bi ) + \u03b2i ) +\u03c8i\u22121, (1)\nwhere \u03c8i\u22121 and \u03d5i denote input feature and modulation of i th layer respectively. Wi and bi represent the weight matrix and followed bias. Unlike other NeRF-GANs, we add skip connection [20] to prevent drastic change of modulation vectors in training. In the model, a subspace embedded in each layer determines the modulation. Each subspace has orthogonal basis and it can be updated during training. The basis are learned to capture semantic modulation. Concretely, in the case of ith layer, a specific subspace determines the modulation of ith layer of NeRF network. It consists of learnable matrices, orthonormal basis Ui = [ui1, . . . ,uiK ] and a diagonal matrixDi = diag (di1, . . . , diK). Each column of Ui plays a role of sub-modulation and it is updated to discover a meaningful direction that results in semantic change in image space. di1, . . . , diK serve as scaling factors of corresponding basis vectors ui1, . . . ,uiK . The latent zi \u2208 RK is set of K scalar control parameters, i.e.,\nzi = {zij \u2208 R | zij \u223c N (0, 1), j = 1, . . . ,K} , (2)\nwhere zij is a coefficient of sub-modulation dijuij . Hence, the modulation of i th layer \u03d5i is decided by weighted summation of K sub-modulations with zi, i.e,\n\u03d5i = UiDizi + \u00b5i = K\u2211 j=1 zijdijuij + \u00b5i, (3)\nwhere the marginal vector \u00b5i is employed to capture shifting bias. Finally, a simple affine transformation is applied to \u03d5i for matching dimension and obtaining frequency \u03b3i and phase shift \u03b2i. At training phase, SURF-GAN layers learn variations of meaningful modulation controlled by randomly sampled z. Additionally, an input noise \u03f5 is also injected to capture the rest variations missed by the layers. To improve the disentanglement of attributes and to prevent the basis fall into a trivial solution, we adopt the regularization loss to guarantee the column vectors of Ui to be orthogonal following EigenGAN, i.e.,\nLreg = Ei[ \u2225\u2225UTi Ui \u2212 I\u2225\u22251]. (4)\nFinally, output image is rendered by volume rendering technique [26]. At inference phase, we can control the discovered semantic attributes by manipulating corresponding element in z. In addition, SURF-GAN enables explicit control over pose using viewing direction v as like other NeRF-based models."
        },
        {
            "heading": "3.2 Explicit control over pose with StyleGAN",
            "text": "In Sec. 3.2 and Sec. 3.3, we introduce a method to inject 3D perception and attribute controllability of SURF-GAN into StyleGAN.\nLeveraging 3D-aware SURF-GAN. The first step is to transform pretrained StyleGAN into a 3D controllable generator. We start with a question: How can we make StyleGAN be capable of controlling over pose explicitly when given arbitrary latent code? To this end, we utilize SURF-GAN as a pseudo ground-truth generator. It provides three images, i.e., Is, Ic, It which denote source, canonical, and target image respectively. Here, z is fixed in all images but the view directions of Is and It are randomly sampled and Ic has canonical view (i.e., v=[0, 0]). Therefore, we can exploit them as multi-view supervision of the same identity. Afterwards, the images are embedded to W+ [1] space by a GAN inversion encoder E, i.e, {ws,wc,wt} = {E(Is), E(Ic), E(It)}. Here, we exploit the pre-trained pSp [52] encoder and it actually predicts the residual and adds it to the mean latent vector, but we omit the notation for simplicity.\nMapping to a canonical latent vector. To handle arbitrary pose without employing off-the-shelf 3D models, we need to build an additional process. To this end, we propose a canonical latent mapper T , which converts an arbitrary code to a canonical code in the latent space of StyleGAN. Here, the canonical code implies being a canonical pose (frontal) in image space. T takes ws as input and predicts its frontalized version w\u0302c = T (ws) with the mapping function. In order to train T , we exploit latent loss to minimize the difference between the predicted w\u0302c and pseudo ground truth of canonical code wc, i.e.,\nLcw = \u2225wc \u2212 T (ws)\u22251. (5)\nTo guarantee plausible translation result in image space, we also adopt pixel-level \u21132-loss and LPIPS loss [69] between two decoded images, i.e.,\nLcI = \u2225I \u2032c \u2212 I\u0302c\u222522 (6) LcLPIPS = \u2225F (I \u2032c)\u2212 F (I\u0302c)\u222522, (7)\nwhere I \u2032c and I\u0302c represent the decoded images from wc and w\u0302c respectively, and F (\u00b7) denotes the perceptual feature extractor. Hence, the loss for canonical view generation is formulated by\nLc = \u03bb1Lcw + \u03bb2LcI + \u03bb3LcLPIPS. (8)\nTarget view generation. Next, the canonical vector is converted to a target latent vector according to given a target view vt = [\u03b1, \u03b2] as an additional input. Here, \u03b1 and \u03b2 stand for pitch and yaw respectively. The manipulation is conducted in the latent space of StyleGAN by adding a pose vector which is obtained by a linear combination of pitch and yaw vectors (p and y, respectively)\nwith vt as coefficients, i.e., w\u0302t = w\u0302c+Lv T t , where L = [p y]. Therefore, we need to find optimal solution of L which can represent an adequate 3D control over pose. Although earlier studies [55,45] have shown successful interpolation results with the linear manipulation, unfortunately, they have found sub-optimal solutions that just control the intended pose attribute implicitly rather than explicit control over 3D camera. The interesting fact we observed is that the pose-related attribute (e.g., yaw) is not uniquely determined by a single direction. Rather, several orthogonal directions can have different effects on the same attribute. For example, two orthogonal direction A and B both can affect yaw but work differently. Based on this observation, we exploit several sub-direction vectors to compensate marginal portion that is not captured by a single direction vector. Our hypothesis is that the optimal direction that follows real geometry can be obtained by a proper combination of the sub-direction vectors. Borrowing the idea of basis in Sec. 3.1, we construct each of N learnable basis to obtain final pose vectors for pitch and yaw respectively. Therefore, we optimize the matrices P = [dp1, . . . ,d p N ] and Y = [d y 1, . . . ,d y N ]. The process to obtain the target vector can be described as,\nw\u0302t = w\u0302c + N\u2211 i=1 (\u03b1 \u00b7 lpi d p i + \u03b2 \u00b7 l y i d y i ), (9)\nwhere the lpi and l y i represent the learnable scalining factor deciding the importance of basis dpi and d y i respectively. To penalize finding redundant directions, we add orthogonal regularization, i.e.,\nLreg = \u2225\u2225PTP\u2212 I\u2225\u2225\n1 + \u2225\u2225YTY \u2212 I\u2225\u2225 1 . (10)\nSimilar to the canonical view generation, the model is penalized by the difference of the latent codes (wt vs. w\u0302t) and that of the corresponding decoded images (I \u2032 t vs. I\u0302t). In addition, we also utilize LPIPS loss. Therefore, the objective function of target view generation is described as,\nLt = \u03bb4Ltw + \u03bb5LtI + \u03bb6LtLPIPS + \u03bb7Lreg. (11)\nFinally, the full objective to train the proposed modules can be formulated as L = Lc+Lt. After training, StyleGAN (G) becomes a 3D-controllable generator (G3D) with the proposed modules as illustrated in Fig. 4. We can achieve a high quality image with intended pose by conditioning view as follow,\nIv = G3D(w,vt) = G(w + T (w) + Lv T t ), (12)\nwhere Iv represents a generated image with target pose vt and w \u2208 W+ is duplicated version of 512-dimensional style vector in W which is obtained by the mapping network in StyleGAN. Moreover, we can extend our method to synthesize novel view of real images by combining with GAN inversion, i.e.,\nItv = G3D(E(I s),vt), (13)\nwhere Is is an input source image in arbitrary view and Itv denotes a generated target image with target pose vt. Note that our method can handle arbitrary images without exploiting off-the-shelf 3D models such as pose detectors or 3D fitting models. In addition, it synthesizes output at once without an iterative optimization process for overfitting latent code into an input portrait image."
        },
        {
            "heading": "3.3 Finding semantic direction with SURF-GAN",
            "text": "Beyond 3D perception, we can discover semantic directions in the latent space of StyleGAN that can control facial attributes using SURF-GAN generated images. Such directions can be obtained by a simple vector arithmetic [51] with two latent codes or several interpolated samples generated by SURF-GAN. Although our approach does not overwhelm state-of-the-art methods analyzing via supervision, it would be a simple yet effective alternative that can provide poserobust editing directions. Of course, the discovery using SURF-GAN is one of many applicable approaches and we can also utilize the existing semantic analysis methods [55,19,56] because our model is flexibly compatible with well-studied StyleGAN-based techniques."
        },
        {
            "heading": "4 Experimental result",
            "text": "This section presents qualitative and quantitative comparisons with state-of-theart methods and analysis of our method. Additional experiments and discussions not included in this paper can be found in the supplementary material."
        },
        {
            "heading": "4.1 Implementation",
            "text": "SURF-GAN. We use each of two datasets to train SURF-GAN, i.e., CelebA [38] dataset and FFHQ [31] dataset. We set the number of sub-modulations in each layerK= 6 (Eq. 2 and Eq. 3) and the number of modulated layers (SURF blocks) is nine (\u2234 t = 8). The other settings are roughly the same with those of \u03c0-GAN. More details can be found in the supplementary paper.\n3D-controllable StyleGAN. For training of 3D controllable StyleGAN, we exploit generated images by SURF-GAN trained with FFHQ because StyleGAN [32] and GAN inversion encoder [52] are pre-trained with FFHQ. We design the model to alter only the first four w vectors (i.e., 4\u00d7512) which have been known to control pose [31,70]. We set the number of sub-direction N = 5 (Eq. 9). The hyper-parameter of the loss function (Eq. 8 and Eq. 11) are set to \u03bb1,\u03bb4=10, \u03bb7 = 100, and 1.0 for the others."
        },
        {
            "heading": "4.2 Controllability of SURF-GAN",
            "text": "First, we present the attributes of CelebA discovered by SURF-GAN in Fig. 5. As like other 3D-aware GANs [9,44,54,72,17,8], it can synthesis a view-conditioned image, i.e., yaw and pitch can be controlled explicitly with input view direction (top row). In contrast to other 3D NeRF-GANs, SURF-GAN can discover semantic attributes in different layers in an unsupervised manner. Additionally, the discovered attributes can be manipulated by the corresponding control parameters. As shown in Fig. 5, different layers of SURF-GAN capture diverse attributes such as gender, hair color, illumination, etc. Interestingly, we observe the early layers capture high-level semantics (e.g., overall shape or gender) and the rear layers focus fine details or texture (e.g., illumination or hue). This property is similar to that seen in 2D GANs even though SURF-GAN consists of MLPs without convolutional layers. Additional discovered attributes, those of FFHQ and the comparison with \u03c0-GAN, which is a pure NeRF-GAN as like ours can be found in the supplementary material."
        },
        {
            "heading": "4.3 Portrait image generation with 3D control",
            "text": "To evaluate the performance of the proposed 3D-controllable StyleGAN, we report the qualitative and quantitative comparison with state-of-the-art models [34,9,72,57] whose generator allows explicit control over pose. Fig. 7 shows synthesis results of each model for given target views. Here, the results are 2562 images generated by each method trained with FFHQ [31]. ConfigNet reveals lack of visual quality and weakness in large pose changes. \u03c0-GAN shows the accurate geometry because its generator consists of pure NeRF layers, but this property also results in some degenerated visual quality. CIPS-3D presents improved visual quality by adopting followed 2D INR network, but it suffers from 3D inaccuracy in specific poses. LiftedGAN generates reasonable outputs according to target views by utilizing differentiable renderer, but it lacks photorealism. Our method generates photorealistic images and shows plausible control over pose and multi-view consistency. We also report the quantitative comparisons of the models in Table. 1 and Fig. 8. We use FID score, pose accuracy estimated by 3D model [75], frames per second, and identity similarity [12] as evaluation metrics. Compared to 3D-aware models, our method achieves a competitive score on pose accuracy and delivers superior results in efficiency, visual quality, and multiview consistency. Although 2D-based ConfigNet shows overwhelming efficiency, it struggles with multi-view consistency and photorealism."
        },
        {
            "heading": "4.4 Novel view synthesis of real image",
            "text": "By utilizing GAN inversion method, our method can perform novel view synthesis from a single portrait. Here, we use pSp [52] encoder for the inversion.\nTo demonstrate the effectiveness of the canonical mapper, we firstly present the frontalization results in Fig. 8, which is a special case of novel pose synthesis. We mark the 3D supervision in training and inference for each method. Here, pSp-FF denotes the frontalization-only version of pSp [52]. Our method successfully generates a canonical view while preserving the identity. Next, we further compare the novel view synthesis results of each model. ConfigNet and \u03c0-GAN with optimizing latent code through iterative manner for overfitting to single test image show inferior results, especially in large pose variation. Rotate-andRender (R&R) [71] presents reasonable results by exploiting off-the-shelf 3D fitting models [75] in the generation process. However, R&R loses some fine details of properties of the original, such as hair or background. Our models can edit pose successfully while preserving identity even though it does not require off-the-shelf 3D models and additional optimization for overfitting to an input. It is also demonstrated by the quantitative results in Fig. 9 which reports the averaged cosine similarity between input image and outputs at given various\nangles using ArcFace [12] and runtime of each method to process a single image."
        },
        {
            "heading": "4.5 Semantic attribute manipulation under conditioned poses",
            "text": "Fig. 11 presents the results of semantic attribute editing with pose control by 3D-controllable StyleGAN. The upper row stands for controllable generation (a)\nand the lower represents real image editing (b). The presented attributes, i.e., skin color, hue, hair color, and bangs are those discovered by SURF-GAN."
        },
        {
            "heading": "4.6 Applications",
            "text": "Our model can be flexibly integrated with other methods that also exploit pretrained StyleGAN. Beyond the real image domain, we present a novel view synthesis of the stylized images such as toon or painting in Fig. 12. We use a interpolated StyleGAN proposed by Pinkney and Alder [50] for toonifying and a transferred StyleGAN trained with MetFace [29] for painting-style outputs."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we solved the problems of 3D-aware GANs and 2D GANs by introducing SURF-GAN and 3D-controllable StyleGAN. Unlike other 3D-aware GANs, SURF-GAN can discover meaningful semantics and control them in an unsupervised manner. Using SURF-GAN, we convert StyleGAN to be explicitly 3D-controllable and it delivers outstanding results in both random image generation and novel view synthesis of real image. In addition, our method has the potential to be flexibly combined with other methods. We expect our work will be used practically and effectively in various tasks and hope it will open up a new direction in 3D-aware generation and editing fields.\nAcknowledgement. This work was supported by DMLab. We also thank to Anonymous ECCV Reviewers for their constructive suggestions and discussions on our paper."
        },
        {
            "heading": "6 Appendix: SURF-GAN",
            "text": "In this section, we supplement contents that are not covered in the main paper, i.e., additional details, experiments and discussion about the proposed SURF-GAN."
        },
        {
            "heading": "6.1 Additional implementation details",
            "text": "Training details. The maximum resolution of SURF-GAN, as well as \u03c0-GAN [9], which allows stable learning is 642 in our setting, it is because 3D-aware GANs (especially pure NeRF-based GAN) require computationally expensive resources for training. Following \u03c0-GAN, we adopt the progressive growing strategy that the size of the generated image increases progressively. Unlike 2D GANs, the generator does not actually \u201cgrow\u201d. Instead, the number of sampled rays increases. Because NeRF-based model can be seen as an implicit continuous function, thus it is theoretically possible to generate arbitrary resolution images. Therefore, only the discriminator adds new layers at each stage to handle higher resolutions. We start training at 322 and it is doubled at the next stage. In training phase, the control parameter z is sampled from the standard normal distribution. The camera pose (pitch and yaw) are sampled from the approximated distribution (normal distribution) of dataset. We assume a perspective pinhole camera where the field of view (FOV) is 12\u25e6. The number of sampled points in each ray is 24 (12 from coarse sampling and 12 from hierarchical sampling). We exploit non-saturating GAN loss with R1 penalty [40] following \u03c0-GAN. In addition, there is orthogonal regularization of basis (Lreg) as explained in Sec. 3.1. Finally, pose loss is adopted optionally on different purposes we discuss it in Sec. 6.4. We adopt ADAM [33] optimizer with \u03b21 = 0 and \u03b22 = 0.99, and the learning rate is set initially to 0.0001 and it is halved in the next stage.\nSURF-GAN architecture. The architecture of SURF-GAN generator is illustrated in Fig. 1. The discriminator is same with that of \u03c0-GAN [9] except the last layer. Besides the adversarial term that distinguishes real or fake, there is an additional branch that predicts the pose (i.e., pitch and yaw) of an input image. This branch is utilized if the pose loss is adopted, otherwise it is discarded (same as \u03c0-GAN)."
        },
        {
            "heading": "6.2 Comparison with \u03c0-GAN",
            "text": "We present the comparison results of SURF-GAN with its baseline, \u03c0-GAN (Table. 1). Both approaches belong to pure NeRF-GAN, which consists of NeRF networks without\nfollowing 2D layers. We evaluate FID score [23], pose accuracy, and multi-view consistency of each method. We compute FID score between 50k of generated images and 70k of real images in each dataset. Off-the-shelf 3D model (3DDFA [75]) is utilized to evaluate pose accuracy. The reported pose error (Pose err.) is calculated by averaging the difference between target poses and predicted estimated poses. Multi-view consistency (ID) is evaluated by calculating cosine similarity between canonical view image and others from ArcFace [12]. Although \u03c0-GAN shows slightly better results in the pose accuracy of CelebA and runtime, SURF-GAN delivers competitive and superior results. For both models, the increased pose error in CelebA is expected to be due to an alignment issue."
        },
        {
            "heading": "6.3 Additional discovered attributes by SURF-GAN",
            "text": "We present the additional attributes in CelebA dataset which are not introduced in the main paper (Fig. 2). Note that \u201cField of view\u201d is not an discovered attribute, but can be controlled in volume rendering. We also report the semantic attributes of FFHQ in Fig. 3."
        },
        {
            "heading": "6.4 Discussion",
            "text": "Effect of the bottom noise. In addition to the layer-wise latent z, our generator also takes the bottom noise \u03f5 as an additional input to capture missing variations (Sec.\n3.1). Therefore, the intended role for \u03f5 is to capture the minor variations that have less or not semantic meaning but enhance diversity. Fig. 4 presents generation results when changing only \u03f5. As can be seen, the generator synthesizes the images with minor variation while preserving facial identity.\nEffect of the progressive growing. As mentioned in Sec. 6.1, we adopt the progressive growing for training. To demonstrate the effectiveness of the strategy, we report FID score of the variants of our method (i.e., w./ and w.o./ progressive growing) for every 1000 iterations. As can be seen the FID curve in Fig. 5, there is a gap between two variants.\nEffect of the pose loss. To improve the pose accuracy, we additionally adopt pose loss Lpose for training and compare with the original SURF-GAN (w.o./ Lpose). Lpose is calculated as the difference between input viewing directions of generator and those predicted by the discriminator. It is not an adversarial loss, thus both the generator and discriminator learn to minimize the loss. The results are listed in Table. 2. The introduction of Lpose reduces pose error (Pose err.), but sacrifices the visual quality. We exploit this model (w./ Lpose) for training 3D-controllable StyleGAN (Sec. 3.2) to offer more elaborate pose samples.\nYaw"
        },
        {
            "heading": "6.5 Limitation.",
            "text": "Although SURF-GAN has several clear advantages such as controllability, there are still inherent limitation as like other 3D-aware GANs. In our model, the color and density of all the points in the rays are calculated independently, thus the amount of computation required to synthesize images increases exponentially as the resolution increases. Such issue has been the catalyst for introducing a method of injecting the prior of SURF-GAN into an efficient and expressive StyleGAN2 generator [32]. It will be one of our future work to achieve high-resolution with clever and efficient ways, e.g., adopting 2D modules [8,17,46,14]in SURF-GAN generator. The other minor limitation is that the same layer does not always capture the same properties for each training. For example, even if a specific dimension (e.g., L3D2) of the trained SURF-GAN captures hair color, the same dimension might capture different attributes in the newly trained model. It seems natural because our method is based on unsupervised training, but it makes the process of assigning properties of each layer necessary after training."
        },
        {
            "heading": "7 Appendix: 3D-controllable StyleGAN",
            "text": "This section presents additional experiments and discussion about 3D controllable StyleGAN."
        },
        {
            "heading": "7.1 Implementation",
            "text": "Latent mapper. The latent mapper consists of five FC layers. It takes an input vector in W+ space and converts it to a canonical vector with the same size. However, the latent mapper does not edit all elements of 18\u00d7512 vector, but edits first four style vectors which have known to related to pose [31,70] (Sec. 4.1), i.e., the input size of the latent mapper is 4\u00d7512. Input feature is firstly flatten to 2048-dimensional vector and then converted to intermediate feature \u2208 R512. After going through three intermediate layers, the feature is converted to the canonical vector \u2208 R4\u00d7512 in the last layer.\nTraining details. We leverage SURF-GAN as a multi-view image generator to train 3D-controllable StyleGAN. As described in Sec. 3.2, the poses of source and target images are randomly sampled from pre-defined distribution. In order to train diverse pose angles, we sample pitch and yaw from uniform distributions instead of Gaussian distribution, i.e., the value of pitch and yaw are uniformly sampled from [\u221230\u25e6, 30\u25e6] and [\u221245\u25e6, 45\u25e6], respectively. The resolution of the rendered images is 2562 that is same with input size of pSp encoder [52]. We use a pretrained StyleGAN2 (10242) generator for experiments except for MetFace [28] stylization (here we use a 2562 generator for transfer learning)."
        },
        {
            "heading": "7.2 Discussion",
            "text": "Sub-directions. To demonstrate the effectiveness of exploiting orthogonal directions (sub-directions) described in Sec. 3.2, we introduce an interpolation example in Fig. 6. Among learned sub-directions, we select two directions \u2212\u2192 d1 and \u2212\u2192 d2 , where both vectors control yaw. As can be seen in the left side of Fig. 6, they influence almost similarly in small pose variations. However, they shows different interpolation outputs when checking the results by scaling both vectors. It means \u2212\u2192 d1 is more involved than \u2212\u2192 d2 for generation of images with large pose variations.\nSURF-GAN-generated images generalization. We train the latent mapper and the learnable directions using SURF-GAN. The objective function is calculated with the images decoded by StyleGAN. However the question that may arise here is\n\u201cCan SURF-GAN-generated images be generalized to the training process?\u201d. To answer the question, we conduct simple experiments. First, we measure the cosine similarity of two decoded images at different pose angles using ArcFace [12] , and also evaluate how much the pose changes in the decoded image using off-the-shelf pose detector. [75]. The former and the latter are for checking whether identity and pose are maintained, respectively. Although there is a domain gap between SURF-GAN and StyleGAN, the two images with the same identity in SURF-GAN domain maintain the same identity in StyleGAN domain as can be seen Table. 3. Moreover, the pose of the SURF-GANgenerated image is hardly changed by the GAN inversion or decoding.\nExtreme cases. As mentioned in Sec. 7.1, we set the poses for training within a certain range because there are few images with extreme poses in the FFHQ dataset. Nevertheless, we validated the extreme case by giving a large value beyond the pose range as input and observed that there are some cases where plausible images are obtained as shown in Fig. 7."
        },
        {
            "heading": "7.3 Additional comparison results",
            "text": "In this subsection, we supplement extra experimental results and discussion to demonstrate the effectiveness of our method.\n3D-controllable image synthesis. We first present the additional qualitative comparison with 3D controllable generative models compared in Sec. 4.3. Here we add one more baseline, DiscoFaceGAN [13] which is based on 3DMM. Although DiscoFaceGAN does not allow explicit control over the camera pose, it can be implicitly manipulated by an input latent. Therefore, we display the interpolated images by appropriately adjusting the angles of the images at both ends.\nNovel view synthesis. We describe the details not covered in Sec. 4.4 and also present additional qualitative comparison with the competing methods [9,34,71] for novel view synthesis (Fig. 9). For all methods, we use the official implementations provided the authors.\n\u03c0-GAN leverages a latent optimization method [32] to overfit the latent code to the testing image. \u03c0-GAN is a 3D-aware generator and learns 3D geometry from unlabelled 2D images without 3D supervision. However, when it is applied to novel view synthesis, \u03c0-GAN needs camera extrinsics from the testing image to initiate the following iterative optimization (700 iterations). For the camera pose, we exploit off-the-shelf pose detection method [18]. As shown in Fig. 9, the visual quality deteriorates as it deviates from the original pose. It is difficult to generate the radiance field of the target image only with latent code optimization and a small error in the pose estimation greatly affects the result. In addition, it takes a lot of time (164 sec.) to get results for a single image due to the iterative optimization. This is why we excluded \u03c0-gan from the quantitative experiment in Sec. 4.4.\nFor ConfigNet [34], the real data encoder firstly predicts the latent embedding of a testing image, and then fine-tunes the generator on the image (50 iterations). To handle a real image, it requires pre-processing to align facial images using landmarks from OpenFace [6]. Although it shows an overwhelming runtime in random image generation compared to other methods (Sec. 4.3), the runtime drops significantly due to the introduction of the face detection model in novel view synthesis (Sec. 4.4). Note that the reported runtime in Sec. 4.4 (2.13 sec.) does not include the fine-tuning procedure. The whole process takes about 11.25 seconds (9.12 sec. for fine-tuning). Furthermore, ConfigNet struggles to synthesize images with large pose changes.\nRotate-and-Render (R&R) is a face rotation method using off-the-shelf 3D fitting network [75] in the overall model, thus it takes some time for 3D fitting (Sec. 4.4). R&R successfully generates a novel view compared to the previously described methods. However, it loses some details of the original image such as hair or background.\nComparison with latent-based models. The pose editing of our method is based on latent manipulation. We introduce comparison results with the existing latentbased method [55] that discovers pose-related direction in the latent space of StyleGAN (Fig. 10). Although InterFaceGAN [55] successfully disentangles the pose attribute, it requires supervision (landmark) for binary classification of yaw in order to find a semantic hyperplane. As Tov et al. [64] have investigated, the results of pose editing with the W+ vectors inverted by pSp [52] shows poor editability. It is alleviated by exploiting e4e encoder [64], but the identity of the input is not well maintained. Above all, the important limitation of latent-based models as well as InterFaceGAN is that they only allow implicit control over pose. Although it is not unfeasible to generate a target pose using these methods, the process might require a few adjustments to obtain an accurate result. Nitzan et al. [45] have introduced the latent-based linear regression method by showing yaw angle has a linear relationship with the distance from InterFaceGAN\u2019s yaw hyperplane. Nevertheless, the linearity is not always guaranteed (Fig. 6), and obtaining the hyperplane requires supervision as mentioned above. There may be an clever alternative to acquire the hyperplane without supervision by leveraging the concept of flipping image [65], but it can be applied only to yaw, not other properties such as pitch or field of view.\nConfigNet"
        },
        {
            "heading": "7.4 Semantic attribute editing",
            "text": "In Sec. 4.5, we presented the results of semantic attribute editing using SURF-GAN, where the direction was calculated by subtracting two inverted SURF-GAN images using pSp encoder [52]. However, there is a trade-off between distortion and editability as Tov et al. [64] demonstrated. As a result, some local attributes in Sec. 4.5 are not changed successfully. Although there might be an effect that we use simple vector arithmetic, the main reason is that the W+ space shows weak editability. To address the issue, we investigate the editing results when using e4e [64] encoder to calculate the direction vector and obtain plausible editing results as shown in Fig. 11. Note that SURF-GAN samples in Fig. 2 and Fig. 3 are utilized for calculating the directions."
        },
        {
            "heading": "7.5 Limitation",
            "text": "Although StyleGAN can generate diverse portrait images with high quality, it struggles to generate out-of-distribution images that do not appear in dataset. Therefore, our method also cannot generate those images because our method does not deviate the\nlatent space of StyleGAN. In addition, our method is also affected by the performance of GAN inversion, thus the performance of our model is not guaranteed for images where the inversion method does not work well. We select several failure cases of our method in Fig. 12. The other limitation is that as like other pose-disentanlged GANs, our method is not capable of generating 3D representations (e.g., mesh or radiance field). Hence, when it comes to video generation, it shows the problem of \u201ctexture sticking\u201d [30] (especially in hair) which is one of the most obvious artifacts in GAN generated videos."
        }
    ],
    "title": "Injecting 3D Perception of Controllable NeRF-GAN into StyleGAN for Editable Portrait Image Synthesis",
    "year": 2022
}