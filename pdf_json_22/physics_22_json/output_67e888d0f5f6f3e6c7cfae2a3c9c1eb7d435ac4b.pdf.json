{
    "abstractText": "The conditional intensity (CI) of a counting process Yt is based on the minimal knowledge F t , i.e., on the observation of Yt alone. Prominently, the mutual information rate of a signal and its Poisson channel output is a difference functional between the CI and the intensity that has full knowledge about the input. While the CI of Markov-modulated Poisson processes evolves according to Snyder\u2019s filter, self-exciting processes, e.g., Hawkes processes, specify the CI via the history of Yt. The emergence of the CI as a self-contained stochastic process prompts us to bring its statistical ensemble into focus. We investigate the asymptotic conditional intensity distribution (ACID) and emphasize its rich information content. We assume the case in which the CI is determined from a sufficient statistic that progresses as a Markov process. We present a simulation-free method to compute the ACID when the dimension of the sufficient statistic is low. The method is made possible by introducing a backward recurrence time parametrization, which has the advantage to align all probability inflow in a boundary condition for the master equation. Case studies illustrate the usage of ACID for three primary examples: 1) the Poisson channels with binary Markovian input (as an example of a Markov-modulated Poisson process), 2) the standard Hawkes process with exponential kernel (as an example of a self-exciting counting process) and 3) the Gamma filter (as an example of an approximate filter to a Markov-modulated Poisson process).",
    "authors": [
        {
            "affiliations": [],
            "name": "Mark Sinzger-D\u2019Angelo"
        },
        {
            "affiliations": [],
            "name": "Heinz Koeppl"
        }
    ],
    "id": "SP:3ce0adc6fe210808863d1f7552c6ce1e2403cd5c",
    "references": [
        {
            "authors": [
                "O. Macchi",
                "B. Picinbono"
            ],
            "title": "Estimation and detection of weak optical signals",
            "venue": "IEEE Transactions on Information Theory, vol. 18, no. 5, pp. 562\u2013573, 1972.",
            "year": 1972
        },
        {
            "authors": [
                "D.R. Cox"
            ],
            "title": "Some statistical methods connected with series of events",
            "venue": "Journal of the Royal Statistical Society: Series B (Methodological), vol. 17, no. 2, pp. 129\u2013157, 1955.",
            "year": 1955
        },
        {
            "authors": [
                "D.L. Snyder"
            ],
            "title": "Random point processes",
            "year": 1975
        },
        {
            "authors": [
                "Y.M. Kabanov"
            ],
            "title": "The capacity of a channel of the Poisson type",
            "venue": "Theory of Probability & Its Applications, vol. 23, no. 1, pp. 143\u2013147, 1978.",
            "year": 1978
        },
        {
            "authors": [
                "M. Davis"
            ],
            "title": "Capacity and cutoff rate for Poisson-type channels",
            "venue": "IEEE Transactions on Information Theory, vol. 26, no. 6, pp. 710\u2013715, 1980.",
            "year": 1980
        },
        {
            "authors": [
                "D. Snyder",
                "C. Georghiades"
            ],
            "title": "Design of coding and modulation for power-efficient use of a band-limited optical channel",
            "venue": "IEEE Transactions on Communications, vol. 31, no. 4, pp. 560\u2013565, 1983.",
            "year": 1983
        },
        {
            "authors": [
                "S. Shamai"
            ],
            "title": "On the capacity of a direct-detection photon channel with intertransition-constrained binary input",
            "venue": "IEEE Transactions on Information Theory, vol. 37, no. 6, pp. 1540\u20131550, 1991.",
            "year": 1991
        },
        {
            "authors": [
                "S. Shamai",
                "A. Lapidoth"
            ],
            "title": "Bounds on the capacity of a spectrally constrained Poisson channel",
            "venue": "IEEE Transactions on Information Theory, vol. 39, no. 1, pp. 19\u201329, 1993.",
            "year": 1993
        },
        {
            "authors": [
                "D. Guo",
                "S. Shamai",
                "S. Verd\u00fa"
            ],
            "title": "Mutual information and conditional mean estimation in Poisson channels",
            "venue": "IEEE Transactions on Information Theory, vol. 54, no. 5, pp. 1837\u20131849, 2008.",
            "year": 1837
        },
        {
            "authors": [
                "I. Lestas",
                "G. Vinnicombe",
                "J. Paulsson"
            ],
            "title": "Fundamental limits on the suppression of molecular fluctuations",
            "venue": "Nature, vol. 467, no. 7312, p. 174, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "L. Duso",
                "C. Zechner"
            ],
            "title": "Path mutual information for a class of biochemical reaction networks",
            "venue": "2019 IEEE 58th Conference on Decision and Control (CDC), 2019, pp. 6610\u20136615.",
            "year": 2019
        },
        {
            "authors": [
                "S. Shamai"
            ],
            "title": "Capacity of a pulse amplitude modulated direct detection photon channel",
            "venue": "IEE Proceedings I - Communications, Speech and Vision, vol. 137, no. 6, pp. 424\u2013430, 1990.",
            "year": 1990
        },
        {
            "authors": [
                "P.M. Bremaud"
            ],
            "title": "A martingale approach to pointprocesses",
            "venue": "Ph.D. dissertation, EECS Department, University of California, Berkeley, 1972. [Online]. Available: http://www2.eecs.berkeley.edu/Pubs/TechRpts/1972/7335.html",
            "year": 1972
        },
        {
            "authors": [
                "P. Br\u00e9maud"
            ],
            "title": "Point processes and queues: martingale dynamics",
            "year": 1981
        },
        {
            "authors": [
                "K.V. Parag",
                "O.G. Pybus"
            ],
            "title": "Optimal point process filtering and estimation of the coalescent process",
            "venue": "Journal of theoretical biology, vol. 421, pp. 153\u2013167, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "A.G. Hawkes"
            ],
            "title": "Spectra of some self-exciting and mutually exciting point processes",
            "venue": "Biometrika, vol. 58, no. 1, pp. 83\u201390, 1971.",
            "year": 1971
        },
        {
            "authors": [
                "D. Oakes"
            ],
            "title": "The Markovian self-exciting process",
            "venue": "Journal of Applied Probability, vol. 12, no. 1, pp. 69\u201377, 1975.",
            "year": 1975
        },
        {
            "authors": [
                "C. Gardiner"
            ],
            "title": "Stochastic methods: a handbook for the natural and social sciences, 4th ed",
            "year": 2009
        },
        {
            "authors": [
                "A. Crisanti",
                "A. de Martino",
                "J. Fiorentino"
            ],
            "title": "Statistics of optimal information flow in ensembles of regulatory motifs",
            "venue": "Physical review. E, vol. 97, no. 2-1, p. 022407, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "J.E. Purvis",
                "G. Lahav"
            ],
            "title": "Encoding and decoding cellular information through signaling dynamics",
            "venue": "Cell, vol. 152, no. 5, pp. 945\u2013956, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "J. Selimkhanov",
                "B. Taylor",
                "J. Yao",
                "A. Pilko",
                "J. Albeck",
                "A. Hoffmann",
                "L. Tsimring",
                "R. Wollman"
            ],
            "title": "Accurate information transmission through dynamic biochemical signaling networks",
            "venue": "Science, vol. 346, no. 6215, pp. 1370\u20131373, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "D. Friedrich",
                "L. Friedel",
                "A. Finzel",
                "A. Herrmann",
                "S. Preibisch",
                "A. Loewer"
            ],
            "title": "Stochastic transcription in the p53-mediated response to dna damage is modulated by burst frequency",
            "venue": "Molecular Systems Biology, vol. 15, no. 12, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "A. Maity",
                "R. Wollman"
            ],
            "title": "Information transmission from nfkb signaling dynamics to gene expression",
            "venue": "PLOS Computational Biology, vol. 16, no. 8, p. e1008011, 2020. 20",
            "year": 2020
        },
        {
            "authors": [
                "Tomasz Jetka",
                "Karol Niena\u0142towski",
                "Tomasz Winarski",
                "S\u0142awomir B\u0142o\u0144ski",
                "Micha\u0142 Komorowski"
            ],
            "title": "Information-theoretic analysis of multivariate single-cell signaling responses",
            "venue": "PLOS Computational Biology, vol. 15, no. 7, p. e1007132, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "R. Suderman",
                "E.J. Deeds"
            ],
            "title": "Intrinsic limits of information transmission in biochemical signalling motifs",
            "venue": "Interface Focus, vol. 8, no. 6, p. 20180039, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "K.V. Parag"
            ],
            "title": "On signalling and estimation limits for molecular birthprocesses",
            "venue": "Journal of theoretical biology, vol. 480, pp. 262\u2013273, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "G. Tka\u010dik",
                "C.G. Callan",
                "W. Bialek"
            ],
            "title": "Information flow and optimization in transcriptional regulation",
            "venue": "Proceedings of the National Academy of Sciences, vol. 105, no. 34, pp. 12 265\u201312 270, 2008.",
            "year": 2008
        },
        {
            "authors": [
                "R. Suderman",
                "J.A. Bachman",
                "A. Smith",
                "P.K. Sorger",
                "E.J. Deeds"
            ],
            "title": "Fundamental trade-offs between information flow in single cells and cellular populations",
            "venue": "Proceedings of the National Academy of Sciences, vol. 114, no. 22, pp. 5755\u20135760, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "S.A. Pasha",
                "V. Solo"
            ],
            "title": "Computing the trajectory mutual information between a point process and an analog stochastic process",
            "venue": "2012 Annual International Conference of the IEEE Engineering in Medicine and Biology Society, 2012, pp. 4603\u20134606.",
            "year": 2012
        },
        {
            "authors": [
                "S.A. Cepeda-Humerez",
                "J. Ruess",
                "G. Tka\u010dik"
            ],
            "title": "Estimating information in time-varying signals",
            "venue": "PLOS Computational Biology, vol. 15, no. 9, p. e1007290, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "F. Tostevin",
                "P.R. ten Wolde"
            ],
            "title": "Mutual information between input and output trajectories of biochemical networks",
            "venue": "Phys. Rev. Lett., vol. 102, no. 21, p. 218101, 2009.",
            "year": 2009
        },
        {
            "authors": [
                "T.S. Roy",
                "M. Nandi",
                "A. Biswas",
                "P. Chaudhury",
                "S.K. Banik"
            ],
            "title": "Information transmission in a two-step cascade: interplay of activation and repression",
            "venue": "Theory in Biosciences, vol. 140, no. 3, pp. 295\u2013306, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "C. Zechner",
                "H. Koeppl"
            ],
            "title": "Uncoupled analysis of stochastic reaction networks in fluctuating environments",
            "venue": "PLOS Computational Biology, vol. 10, no. 12, pp. 1\u20139, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "Z. Cao",
                "R. Grima"
            ],
            "title": "Analytical distributions for detailed models of stochastic gene expression in eukaryotic cells",
            "venue": "Proceedings of the National Academy of Sciences, vol. 117, no. 9, pp. 4682\u20134692, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "T.L. Lenstra",
                "J. Rodriguez",
                "H. Chen",
                "D.R. Larson"
            ],
            "title": "Transcription dynamics in living cells",
            "venue": "Annual review of biophysics, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "P.A. W Lewis",
                "G.S. Shedler"
            ],
            "title": "Simulation of nonhomogeneous Poisson processes by thinning",
            "venue": "Naval Research Logistics Quarterly, vol. 26, no. 3, pp. 403\u2013413, 1979.",
            "year": 1979
        },
        {
            "authors": [
                "C. Zechner",
                "G. Seelig",
                "M. Rullan",
                "M. Khammash"
            ],
            "title": "Molecular circuits for dynamic noise filtering",
            "venue": "Proceedings of the National Academy of Sciences of the United States of America, vol. 113, no. 17, pp. 4729\u2013 4734, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "L. Bronstein",
                "H. Koeppl"
            ],
            "title": "Marginal process framework: A model reduction tool for Markov jump processes",
            "venue": "Physical review. E, vol. 97, no. 6-1, p. 062147, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "D. Vere-Jones",
                "Y. Ogata"
            ],
            "title": "On the moments of a self-correcting process",
            "venue": "Journal of Applied Probability, vol. 21, no. 2, pp. 335\u2013342, 1984.",
            "year": 1984
        },
        {
            "authors": [
                "M.H.A. Davis"
            ],
            "title": "Piecewise-deterministic Markov processes: A general class of non-diffusion stochastic models",
            "venue": "Journal of the Royal Statistical Society: Series B (Methodological), vol. 46, no. 3, pp. 353\u2013376, 1984.",
            "year": 1984
        },
        {
            "authors": [
                "K.V. Parag",
                "G. Vinnicombe"
            ],
            "title": "Point process analysis of noise in early invertebrate vision",
            "venue": "PLOS Computational Biology, vol. 13, no. 10, p. e1005687, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "J.C. Cox",
                "J.E. Ingersoll",
                "S.A. Ross"
            ],
            "title": "A theory of the term structure of interest rates",
            "venue": "Econometrica, vol. 53, no. 2, p. 385, 1985.",
            "year": 1985
        },
        {
            "authors": [
                "M. Sinzger",
                "M. Gehri",
                "H. Koeppl"
            ],
            "title": "Poisson channel with binary Markov input and average sojourn time constraints",
            "venue": "2020 IEEE International Symposium on Information Theory: Proceedings.",
            "year": 2020
        },
        {
            "authors": [
                "H. Qiu",
                "B. Zhang",
                "T. Zhou"
            ],
            "title": "Influence of complex promoter structure on gene expression",
            "venue": "Journal of Systems Science and Complexity, vol. 32, no. 2, pp. 600\u2013614, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "X. Gao",
                "L. Zhu"
            ],
            "title": "Functional central limit theorems for stationary hawkes processes and application to infinite-server queues",
            "venue": "Queueing Systems, vol. 90, no. 1, pp. 161\u2013206, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "N. Limnios",
                "G. Oprisan"
            ],
            "title": "Semi-Markov Processes and Reliability",
            "venue": "Springer Science & Business Media,",
            "year": 2001
        },
        {
            "authors": [
                "C. Maes",
                "K. Neto\u010dn\u00fd",
                "B. Wynants"
            ],
            "title": "Dynamical fluctuations for semi-Markov processes",
            "venue": "Journal of Physics A: Mathematical and Theoretical, vol. 42, no. 36, p. 365002, 2009.",
            "year": 2009
        },
        {
            "authors": [
                "A. Kuczura"
            ],
            "title": "The interrupted Poisson process as an overflow process",
            "venue": "Bell System Technical Journal, vol. 52, no. 3, pp. 437\u2013448, 1973.",
            "year": 1973
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014Conditional intensity, Poisson channel, mutual information rate, average sojourn time constraint, Snyder filter, backward recurrence time, marginal simulation, Hawkes process.\nI. INTRODUCTION The study of the Poisson channel dates back to the 70s. While originally introduced as a model of optical communication [1], its setting is ubiquitous in various fields of research for modeling event counts: An input signal Xt, possibly corrupted by dark current, is partially observed via point observations. In the terminology of Cox [2] and Snyder [3], the channel output Yt is a doubly stochastic Poisson process with feedback-less intensity \u03bbt = \u03bb(Xt).\nMuch is already known about the Poisson channel. In particular, its capacity under amplitude constraint Xt \u2208 [0, c] was determined by Kabanov [4] shortly after its introduction. However, the capacity-achieving input distribution is found\nThis paper was presented in part at the 2020 IEEE International Symposium on Information Theory, virtual, Los Angeles, California, USA.\nThe work was supported by the European Research Council (ERC) within the Consolidator Grant CONSYN (grant agreement no. 773196).\nM. Sinzger-D\u2019Angelo and H. Koeppl are with the Department of Electrical Engineering and Information Technology, Technische Universita\u0308t Darmstadt (email: {mark.sinzger, heinz.koeppl}@tu-darmstadt.de). H. Koeppl is with the Centre for Synthetic Biology, Technische Universita\u0308t Darmstadt.\nto switch infinitely fast between 0 and c [4], [5]. This flaw in terms of physical interpretability was addressed by adding constraints on the bandwidth [6]\u2013[8]. While reducing the class of input processes this necessitates to review the task of computing the MI.\nBesides taking its definition\nI(X[0,T ], Y[0,T ]) := E\n[ ln\nd\u00b5X,YT d\u00b5XT \u00d7 \u00b5YT\n] (1)\nas a departure for the computation, an expression found to be useful was obtained by Liptser [9]\nI(X[0,T ], Y[0,T ]) = \u222b T\n0\nE[\u03c6(\u03bbt)\u2212 \u03c6(\u03bb\u0302t)] dt, (2)\nwhere \u03c6(z) = z ln z and \u03bb\u0302t = E[\u03bbt|FYt ]. Although other results on links between the MI and conditional estimation received considerable attention [10], [11], the classical expression eq. (2) remains the standard (besides the definition eq. (1)) when searching for ways to compute the MI [12], [13].\nThe difficulty in computing the MI via eq. (2) clearly lies in the computation of E[\u03c6(\u03bb\u0302t)]. Different approaches address this. (i) In case of a stationary input \u03bbt, one can consider\n\u03bbt \u2192 \u03bb\u221e, \u03bb\u0302t \u2192 \u03bb\u0302\u221e\nin distribution, still allowing conclusions about the mutual information rate\nI(X,Y ) := lim T\u2192\u221e\n1 T I(X[0,T ], Y[0,T ]) = E[\u03c6(\u03bb\u221e)]\u2212E[\u03c6(\u03bb\u0302\u221e)].\n(3) (ii) Properties of the function \u03c6 (Lipschitz for bounded \u03bb [4] or non-negative third derivative [8]) were used for a reduction to second order moments of \u03bb\u0302\u221e. (iii) Kabanov, Davis used martingale theory with its rich second order analysis tools. (iv) The conditioning on a coarser sigma-field [7] or the use of a suboptimal estimator [8] provided upper bounds. (v) Alternatively, one needs to have the knack for choosing tractable input process classes. For example, piecewise-constant input trajectories, whose amplitudes follow a Markov chain were considered [14].\nWe took the approach (i) combined with (v), restricting ourselves to continuous-time Markov chains Xt with low number of input states. Its appearance on the right-hand side of (3) is our main motivation to look at the distribution of \u03bb\u0302\u221e, the asymptotic conditional intensity distribution (ACID). With the spotlight being on \u03bb\u0302t, we elaborate on its interpretation\nar X\niv :2\n20 5.\n07 01\n1v 1\n[ cs\n.I T\n] 1\n4 M\nay 2\n02 2\n2 as conditional intensity in I-A. A summary of our general contributions is followed by a confined focus on cell biology for interested readers (I-B). In I-C we advocate that the study of ACID can be beneficial beyond the information rate."
        },
        {
            "heading": "A. Conditional intensity view replaces estimator perspective",
            "text": "The conditional mean \u03bb\u0302t = E[\u03bbt|FYt ] is often regarded as an estimator of \u03bbt. There is a clear justification for this: For each t \u2265 0\nE[\u03bbt|FYt ] = arg min Z E[(\u03bbt \u2212 Z)2] (4)\namong all random variables Z that are FYt -measurable. But this interpretation attributes a notion of deficit to \u03bb\u0302t: While \u03bbt is the true intensity of Yt, we obtain merely its optimal yet inaccurate approximation via \u03bb\u0302t. We advocate a different perspective throughout the paper.\nThe process Yt is a self-exciting process with intensity \u03bb\u0302t, which has been known at least since the monograph [3] by Snyder. How can Yt have two intensities? How can \u03bb\u0302t be on a par with \u03bbt? This puzzle is solved if one incorporates that the intensity \u03bbt of a general counting process Yt has an Ft-dependency. This dependency can be expressed in at least two ways: (i) explicitly but not of operational value for mathematical proofs\n\u03bbt = E[ dYt|Ft],\n(ii) implicitly, and beneficial for mathematical rigidity [15] by requesting Yt \u2212 \u222b t\n0\n\u03bbs ds\nto be an Ft-martingale. It implies that a general counting process Yt can possess several intensities. For a strictly coarser Gt \u2286 Ft and corresponding intensities \u03bb\u0302t, \u03bbt, Bremaud\u2019s innovation theorem provides \u03bb\u0302t = E[\u03bbt|Gt]. This allows the following interpretation of eq. (3). The information rate is not a difference functional between the true intensity and its deficient estimate, but between two intensities that differ in the state of knowledge: FX,Yt = \u03c3(Xs, Ys : s \u2264 t) vs. FYt = \u03c3(Ys : s \u2264 t). We follow Bremaud [16] and Daley, Vere-Jones [17] in using the term conditional intensity for the FYt -intensity. It is not to be confused with its other usage as a special case of the Papangelou intensity.\nFar from being an estimator with deficits, the conditional intensity \u03bb\u0302t is canonical one among the FYt - and F X,Y t - intensities. The FYt -intensity can be defined for any counting process Yt, with no need to specify an external process. And it has a standing as being the intensity with respect to the minimal filtration to which Yt is adapted.\nThe estimator perspective regards E[\u03bbt|FYt ] as a function of the history {Ys : 0 \u2264 s \u2264 t}. This complicated functional dependence makes it hard to understand \u03bb\u0302t \u2192 \u03bb\u0302\u221e. In contrast, the conditional intensity view offers a self-contained description of \u03bb\u0302t. It emancipates from Yt when both have common jumps. This view guides our understanding of \u03bb\u0302t \u2192 \u03bb\u0302\u221e.\nWe restrict our study of ACID to processes Yt for which \u03bb\u0302t is obtained from a piecewise-deterministic Markov process.\nWithin the scope of this work, we review that Markovmodulated Poisson processes belong to this class [18], as do the Hawkes process [19], [20] and approximate filters, introduced in section I-C. We exploit that the ACID can then be obtained from generator theory of Markov processes [21]. To this end, we contribute a backward recurrence time parametrization (BReT-P), whose advantage is the alignment of all probability inflow. Since our numerical method works via grid discretization, it is limited in the dimension of the piecewise-deterministic Markov process. But conceptually, BReT-P opens the door for new numerical methods that compute the MI without Monte Carlo simulation. We illustrate how our method helps with screening for system parameters that optimize the information rate under bandwidth-like constraints. Applications beyond information theory complement the case studies."
        },
        {
            "heading": "B. Path mutual information in cell signaling",
            "text": "Our particular interest lies in studying the mutual information between time-varying signals in biological systems. We review the state of the art of information theory in cell biology to locate our contribution to the field. Readers mainly interested in the method can skip this paragraph.\nIt is widely assumed that the optimization of the MI could be an evolutionary strategy to mitigate noise in the cellular signalling via metabolites and in gene regulatory networks [22]. There is mounting evidence that information is encoded in the temporal profile of biomolecules [23]\u2013[25], but the exact mechanisms how temporal features result in decision making are unclear [26]. A path MI that is sensitive to temporal effects has been recently introduced for a class of chemical reaction networks [13]. The MI is often interpreted as a measure for the amount of input states that can be resolved accurately [27]. From an engineering perspective maximizing the path MI between sensor and actuator could thus define a design principle to construct cellular circuits. Information theory has been used to reveal fundamental physical limits of information transmission guiding our understanding of gene regulatory motifs as information processing units [12], [28].\nWhy is the Poisson channel used as a communication model in cells? Chemical reaction networks are standard Markov approaches in the stochastic modeling of gene regulation in cells. Reactants X1, . . . , Xn are converted, synthesized and degraded in reactions\nRj : n\u2211 i=1 \u03bdijXi rj\u2212\u2192 n\u2211 i=1 \u03b7ijXi, j = 1, . . . ,m.\nThis induces a continuous time Markov chain (CTMC) on the state space Nn\u22650 of molecule counts ~k = (k1, . . . , kn), where the non-diagonal entry (~k \u2192 ~l) in the generator matrix is\nm\u2211 j=1 rj n\u220f i=1 ( ki \u03bdij ) 1(~l \u2212 ~k = ~\u03b7j \u2212 ~\u03bdj).\nThe order of reaction Rj is \u2211n i=1 \u03bdij . For instance, the standard transcription model with the two-state promoter (Fig. 1a)\n3 ++ a) b)\nFig. 1. The standard transcription model with a two-state promoter. a) A random telegraph promoter switches between its active and its inactive state. Only in the active state, mRNA is synthesized. The promoter stays active, so multiple mRNA strands can be transcribed in one active period. The transcription reaction is of first order with fixed rate. b) Experimentally only the transcription events are observed. The promoter is modelled as a context. An observation model for transcription counts is obtained when we marginalize the joint system over the context. The transcription reaction is of order zero with stochastic rate, i.e., a doubly stochastic Poisson process.\ncontains a first-order synthesis reaction\nPon \u2192 Pon + mRNA.\nThe homogeneous Poisson process corresponds to a zeroorder reaction. First-order reactions can be interpreted as zeroorder reactions with stochastic modulation, i.e., as Poisson channel with linear \u03bb(Xt) (Fig. 1b). More generally, the Poisson channel accounts for the discrete nature of reactions Rj with the reaction counts Yj(t) up to time t being the Poisson channel output. Suited for the low copy number regime, it can bring to attention the bottleneck character of sensor molecules that sense a continuous signal but whose synthesis events are restricted to the discrete regime [12], [29]. Adhering to our goal of including temporal effects, the Poisson channel is suited because it is capable of modeling time-varying inputs and outputs.\nComputing the path MI over Poissonian channels is difficult. Many works of research resort to basing the MI on single time-point marginals ignoring any encoding in the temporal profile [22], [30], [31]. Other approaches are the following: (i) Gaussian approximations of the input to make use of analytic results [12], (ii) Monte Carlo estimators [13], [32], [33]. (iii) Not rarely, the intractability decoyed researchers to resort to other channels such as the Gaussian channel [34], [35]. We contribute to information theory in biology by presenting a Monte Carlo-free numerical computation of the path mutual information rate of a Poisson channel for Markovian input with low state number."
        },
        {
            "heading": "C. Approximate filters and approximate marginal simulation",
            "text": "The study of ACID can be beneficial beyond its appearance in the information rate of the Poisson channel. Markovmodulated Poisson processes do not only serve as communication channel models. They provide observation models for open systems, i.e., the counting process can be regarded as a subsystem that is embedded in a heat bath [36] or environment [37]. The Markovian environment modulates the intensity of the observed subsystem.\nIn the biological cell, for instance, the rate of transcription synthesis events is modulated by multiple factors [38]. Recruitment of polymerase and transcription factors as well as\nunwinding of the DNA strand, all contribute to the activation of a promoter state, where transcription is initiated [39]. The combination of these factors can be regarded as a random environmental process modulating the rates in the observed main process, here transcription synthesis (Fig. 1b).\nAn observation model of the subsystem requires modeling the unobserved context, e.g., as Markovian environment. After marginalizing over the environment, the CI describes the main process in an uncoupled way. Sample trajectories, i.e., Monte Carlo samples, can be obtained by simulating the main process as a self-exciting counting process whose rate is the CI. In practice, this is done via thinning [40] or by the inverse transform method to sample sojourn times. This marginal simulation replaces the co-simulation of the environment and promises a potential speed-up. Only the effective rate \u03bb\u0302t is used, neglecting fluctuations of the environmental process that are not transmitted to the main process. However, unclosed conditional moment equations pose a serious challenge to achieving the goal of fast marginal simulation [37]. This problem is addressed by approximate filters obtained from conditional moment closure, assumed density filtering, variational inference, entropic matching or projection [3], [41], [42]. Prominently, the Hawkes process can be obtained from projection onto the class of linear estimators and can thus be regarded as an approximate filter. We call the marginal simulation with an approximate filter approximate marginal simulation.\nThe process that results from approximate marginal simulation is self-exciting, but in general not a Markov-modulated Poisson process any more. Do characteristics of Yt, such as the mean or variance evolution, stay invariant under approximate marginal simulation? A full, but often intractable characteristic is the path measure of Yt. The ACID provides a partial characteristic that exceeds first- and second-order analysis but is less complex than the path measure. It can assist as a discrimination tool to detect dissimilar approximate filters. We demonstrate this using the Hawkes process."
        },
        {
            "heading": "II. METHOD",
            "text": "Let Yt be a counting process. Its canonical filtration is FYt = \u03c3(Ys : s \u2264 t). Denote by \u03bb\u0302t the FYt -intensity of\n4 the process Yt. We refer to it as the conditional intensity (CI)."
        },
        {
            "heading": "A. The backward recurrence time parametrization",
            "text": "Throughout this work we consider a counting process Yt whose conditional intensity \u03bb\u0302t is parametrized by a process (\u03c4(t), \u03b8(t)) in the following form. The scalar \u03c4 is the time since the latest jump of Y , i.e., the backward recurrence time [43]. The second component \u03b8 \u2208 Rn is a (possibly multivariate) sufficient statistic, that possesses three properties (A1) it is constant between jumps, i.e. \u03b8(t) = \u03b8(t\u2212 \u03c4(t)) (A2) there is a deterministic m, satisfying \u03bb\u0302t = m(\u03c4(t), \u03b8(t)). (A3) there is a deterministic g, satisfying g(\u03c4(t\u2212), \u03b8(t\u2212)) = \u03b8(t) at jump times t of Y . We further request (\u03c4(t), \u03b8(t)) to be ergodic in the following. An example of a non-ergodic process (\u03c4(t), \u03b8(t)) satisfying the three properties was investigated in [44]. The process (\u03c4(t), \u03b8(t)), denoted BReT-P, combines three conditions which jointly turn it into a piecewise-deterministic Markov process [45]: (B1) The evolution equation of (\u03c4, \u03b8) between jumps reads\n\u03c4\u0307 = 1, \u03b8\u0307 = 0. (5)\nThus the process evolves deterministicly from the current state when there are no jumps.\n(B2) Jumps of (\u03c4(t), \u03b8(t)) and Y occur simultaneously with intensity \u03bb\u0302t = m(\u03c4(t), \u03b8(t)), only depending on the current state, not the history. (B3) At jumps, the backwards recurrence time is updated to \u03c4(t) = 0 and the sufficient statistic targets \u03b8(t) = g(\u03c4(t\u2212), \u03b8(t\u2212)). The jump targets only depend on the current state.\nNext, we show that the CI of a Markov-modulated Poisson process is of this form. After the joint asymptotic distribution of (\u03c4, \u03b8) is found (see section II-E), the ACID is determined via the transform m(\u03c4, \u03b8) (see section II-I)."
        },
        {
            "heading": "B. Class of Markov-modulated Poisson processes",
            "text": "Let X be a continuous time Markov chain (CTMC) with finitely many states in X and \u03bb : X \u2192 [0,\u221e) an intensity mapping. Let Y be a doubly stochastic Poisson process with intensity \u03bbt = \u03bb(Xt).\nDenote by A the generator of X . Then the filtering distribution \u03a0t(x) := P[Xt = x|FYt ], x \u2208 X evolves as follows, according to Snyder [3],\nd dt \u03a0t(x) = (A\u03a0t)(x)\u2212 (\u03bb(x)\u2212 \u03bb\u0302t)\u03a0t(x) (6)\nbetween jumps and is updated to\n\u03a0t(x) = \u03bb(x)\u03a0t\u2212(x)\n\u03bb\u0302t\u2212 (7)\nif Yt = Yt\u2212 + 1. Taken together, the Snyder filter reads\nd\u03a0t(x) = (A\u03a0t)(x) dt+ (\u03bb(x)\u2212 \u03bb\u0302t)\u03a0t(x)\n\u03bb\u0302t { dYt\u2212 \u03bb\u0302t dt}.\n(8)\nBy means of the filtering distribution, the conditional intensity is computed as\n\u03bb\u0302t = E[\u03bb(Xt)|FYt ] = \u2211 x \u03bb(x)\u03a0t(x).\nIt is also denoted as the filter mean or causal conditional mean estimate.\nWe now consider \u03b8(t) := \u03a0t \u2208 [0, 1]|X | at jump times and \u03b8(t) = \u03b8(t\u2212\u03c4(t)) else, and derive that the process (\u03c4(t), \u03b8(t)) is a BReT-P of \u03bb\u0302t. In order to do so, we introduce auxiliary functions l, u and f to define the functions m and g in this context. Let l(\u03c0) := \u2211 x \u03bb(x)\u03c0(x) be the mean functional. Observe that, by the relation \u03bb\u0302t = l(\u03a0t), the ODE system (6) is closed and autonomous. Denote by \u03c4 7\u2192 u(\u03c4, \u03c0) its solution with initialization \u03a00 = \u03c0 \u2208 [0, 1]|X |. The jump update f : R|X | \u2192 R|X | is f(\u03c0)(x) = \u03bb(x)\u03c0(x)l(\u03c0) . Then m := l \u25e6 u and g := f \u25e6m satisfy the properties (A2) and (A3). Remark II.1. It is crucial to note that we can indeed consider Yt to be jumping with intensity \u03bb\u0302t instead of \u03bbt. In condition (B2), if we replaced \u03bb\u0302t by \u03bb(Xt) we would lose the selfcontained description of \u03bb\u0302t that even disposes of Yt itself. To this end, in process equation (8), replace Yt by Y\u0303t, where Y\u0303t is a self-exciting counting process with intensity \u03bb\u0302t. The processes Y\u0303t and Yt are equal in distribution. So instead of Yt we consider Y\u0303t in the first place and drop the tilde for convenience.\nThe fact that the Snyder filter is a piecewise-deterministic Markov process has been observed before [46]. For the particle interpretation of this fact we refer to [18]."
        },
        {
            "heading": "C. Sufficient state variables of joint Markovian progression",
            "text": "We can abstract the BReT-P in subsection II-B to a more general process class that falls in our considered framework. Consider state variables (V1(t), . . . , Vn0(t)) = V (t) \u2208 Rn0 that progress as\nV\u0307 (t) = F (V (t)) (9)\nwith the deterministic dynamics F between jumps of V (t), and at jumps they are updated to\nV (t) = f(V (t\u2212)) (10)\nwith the update function f = (f1, . . . , fn0) : Rn0 \u2192 Rn0 . Let further be l : Rn0 \u2192 R a deterministic functional of the state variable, such that \u03bb\u0302t = l(V (t)). We call V1, . . . , Vn0 sufficient state variables of joint Markovian progression, because they form a Markov process by arguments analogous to (B1)-(B3) and are sufficient in that the conditional intensity can be computed from them. Denote by \u03c4 7\u2192 u(\u03c4, v0) the solution of eq. (9) with initial value V0 = v0. Suppose that fn+1 \u2261 v0n+1, . . . , fn0 \u2261 v0n0 are constant, i.e., the values Vn+1, . . . , Vn0 are reset to the same values v 0 n+1, . . . , v 0 n0 at any jump. We now construct the process (\u03c4(t), \u03b8(t)). As sufficient statistic it suffices to define \u03b8(t) = (V1(t), . . . , Vn(t)) at jumps and \u03b8(t) = \u03b8(t \u2212 \u03c4(t)) else, automatically satisfying (A1). The extension operator \u03a3(\u03b8) := (\u03b8, v0n+1, . . . , v 0 n0) concatenates the current state with the constants. The truncation operator \u0393(v0) = (v01 , . . . , v 0 n) projects onto the first\n5 n components. The functions m(\u03c4, \u03b8) = l \u25e6 u(\u03c4,\u03a3(\u03b8))) and g(\u03c4, \u03b8) = \u0393 \u25e6 f(u(\u03c4,\u03a3(\u03b8))) satisfy the properties (A2) and (A3).\nRemark II.2. We return to section II-B and exclude the state variables with constant reset value. Namely, conservation of probability mass and zero-states reduce the dimension of the sufficient statistic.\n(i) In eq. (8) the evolution equation of the last x can be replaced by the trivial evolution of \u2211 x\u03a0t(x). The value\nof this sum is constantly 1. Hence, the number of state variables n0 is at most |X |\u2212 1. A reparametrization may further decrease the number. (ii) Call any x \u2208 X with \u03bb(x) = 0 a zero-state. For zerostates, the reset value of the corresponding \u03a0t(x) in eq. (7) is 0, i.e., f is constant for these components. Hence, the conditional probabilities for the zero-states need not be tracked in the sufficient statistic \u03b8. The dimension of \u03b8 can be reduced to\nn \u2264 |{X : \u03bb(x) > 0}| \u2212 1. (11)"
        },
        {
            "heading": "D. BReT-P examples",
            "text": "We list examples of parametrizations in order to convey the intuition behind the sufficient state variables of joint Markovian progression. The examples introduced here serve as our case studies in section III, highlighting different aspects of the BReT-P method and usage of ACID. In case studies 1) to 3) ACID is used to evaluate the mutual information rate via (3). Examples 4) and 5) show how the analysis of ACID exceeds first and second order analysis, both on the CI level and the level of Yt. Contrasting examples 2) and 4) illustrates how ACID can discriminate approximate filters. The table II-D summarizes the computational details of the examples.\n1) Random telegraph model: The input X is a random telegraph model on X := {0, 1} with On and Off rates k1, k2 and \u03bb(x) = cx. By remark II.2(i) \u03a0t(1) is a sufficient state variable (i.e., n0 = 1). It evolves according to eq. (6) as\n\u03a0\u0307t(1) = k1 \u2212 (k2 + k1 + c)\u03a0t(1) + c\u03a0t(1)2. (12)\nFor the conditional intensity, we get\n\u03bb\u0302t = c \u00b7\u03a0t(1) + 0 \u00b7 (1\u2212\u03a0t(1))\nwhich means l(\u03c01) = c\u03c01. The zero-state reduces the dimension. Indeed, by eq. (7) the reset is f(\u03c01) = 1, so the dimension of \u03b8 can be chosen n = 0 as discussed in subsection II-C, compare also eq. (11). Consequently, the scalar \u03c4(t) suffices as BReT-P. Let \u03c4 7\u2192 u(\u03c4) be the solution of the ODE with initial value \u03c01 = 1, then \u03bb\u0302t = m(\u03c4(t)) = l \u25e6 u(\u03c4(t)) = cu(\u03c4(t)). The BReT-P method helps in analyzing how I(X,Y ) depends on the system parameters k1, k2, c, see paragraph III-A, with the goal to answer questions of optimality under constraints 0 < k1 \u2264 r1, 0 < k2 \u2264 r2.\n2) Random telegraph model with dark current: Let X be a random telegraph model as before, but this time, \u03bb(0) =: \u03bb0 > 0 introduces a dark current. Define \u03bb1 := \u03bb(1) > \u03bb(0) and the amplitude \u2206\u03bb := \u03bb1 \u2212 \u03bb0. By remark II.2(i) again n0 = 1. Choose \u03bb\u0302t = \u03bb0+\u03a0t(1)\u2206\u03bb as sufficient state variable\nOff On On \u03b101 \u03b111\n\u03b110\nFig. 2. State diagram for the Double On Single Off (DOnSOff) model. The three-state model is Markovian. A refractory second active state realizes a non-exponential sojourn time in On. For \u03b111 = \u03b110 the sojourn time is an Erlang distribution. The Off sojourn time remains exponential.\nof Markovian progression. Being an affine-linear transform of eq. (8), it evolves as\nd\u03bb\u0302t = { k1\u2206\u03bb\u2212 (k2 + k1 + \u2206\u03bb)(\u03bb\u0302t \u2212 \u03bb0)\n+(\u03bb\u0302t \u2212 \u03bb0)2 } dt+ (\u03bb\u0302t\u2212 \u2212 \u03bb0)(\u03bb1 \u2212 \u03bb\u0302t\u2212)\n\u03bb\u0302t\u2212 dYt.\n(13)\n(By their affine-linear relation, \u03bb\u0302t and \u03a0t(1) are equivalent choices.) From eq. (11) we can only conclude n \u2264 1. Thus, we have (\u03c4(t), \u03b8(t)) with \u03b8(t) = \u03bb\u0302t at jumps as BReT-P. BReTP helps in investigating how dark current alters I(X,Y ), see paragraph III-B.\n3) CTMC with two On states: Let X be an arbitrary ergodic CTMC with n states and \u03bb : X \u2192 {0, c} be binary with two active, i.e., non-zero states x1, x2. We refer to the model as double On (DOn). Then introduce the conditional probability of being in an active state\nZt = \u03a0t(x1) + \u03a0t(x2)\nand Ut := \u03a0t(x1)/Zt the contribution of x1 to this conditional probability. Then Vt := (Ut, Zt,\u03a0t(x3), . . . ,\u03a0t(x|X |\u22121)) are sufficient variables of joint Markovian progression. The reset value at jumps is (Ut\u2212, 1, 0, . . . , 0). Since all but the first component are set to a constant at jumps, the sufficient statistic \u03b8(t) = Ut at jumps has dimension n = 1. The progression of Vt can be found by the chain rule from eq. (8). We only elaborate on it for the circular CTMC with double On state and single Off state (DOnSOff), depicted in figure 2. Let \u03b101, \u03b111, \u03b110 be the transition rates of going from inactive to first active, first to second active and second active back to inactive, then the generator matrix is of the form\nA = \u2212\u03b111 \u03b111 00 \u2212\u03b110 \u03b110 \u03b101 0 \u2212\u03b101  . (14) The evolution of the conditional probabilities \u03a0t(x1) and \u03a0t(x2) is\n\u03a0\u0307t(x1)\n= \u03b101(1\u2212 Zt)\u2212 \u03b111\u03a0t(x1)\u2212 c(1\u2212 Zt))\u03a0t(x1) (15) \u03a0\u0307t(x2)\n= \u03b111\u03a0t(x1)\u2212 \u03b110\u03a0t(x2)\u2212 c(1\u2212 Zt))\u03a0t(x2). (16)\n6 Model Type V (t) n0 n l u F f Random telegraph MM \u03a0t(1) 1 0 c\u03c01 X Eq. (12) 1\nDark current MM \u03bb\u0302t 1 1 id X Eq. (13) \u03bb\u0302\u22121(\u03bb\u0302\u2212 \u03bb0)(\u03bb1 \u2212 \u03bb\u0302) + \u03bb\u0302\nDouble On MM (Ut, Zt,\u03a0t(x3), . . . ,\u03a0t(x|X |\u22121)) |X | \u2212 1 1 cz - Ito fromEq. (6) (u, 1, 0, . . . , 0)\nDouble On Single Off MM (Ut, Zt) 2 1 cz - Eq.(17)-(18) (u, 1) Hawkes SE \u03bb\u0302t 1 1 id X Eq. (20) \u03bb\u0302+ \u03b2\nGamma filter SE (Mt, St) 2 2 cm - Eq. (21) (m, s) + (sm\u22121, s2m\u22122)\nTABLE I THE TYPE INDICATES WHETHER THE MODEL IS A MARKOV-MODULATED (MM) POISSON PROCESS OR SELF-EXCITING (SE) COUNTING PROCESS. THE MINUS (\u2212) IN THE COLUMN u INDICATES THAT NO ANALYTIC SOLUTION IS AVAILABLE FOR \u03c4 7\u2192 u(\u03c4, v0).\nVia the function \u03d5(\u03c01, \u03c02) = (\u03c01/(\u03c01 +\u03c02), \u03c01 +\u03c02) = (u, z) this transforms to\nU\u0307 = \u2212\u03b111U + \u03b101 (1\u2212 Z)(1\u2212 U)\nZ + \u03b110U(1\u2212 U) (17)\nZ\u0307 = \u2212c(1\u2212 Z)Z + \u03b101(1\u2212 Z)\u2212 \u03b110(1\u2212 U)Z. (18)\nThe update function according to eq. (7) is f(u, z) = (u, 1), i.e., constant in the second component. Figure 3 visualizes how trajectories for different initial values [U(\u03c4), Z(\u03c4)]\u03c4=0 = [\u03b8, 1] evolve. The coordinate system shows the plane (\u03c01, \u03c02), while the grid indicates the transformed radial-like coordinates (u, z). The DOnSOff model is equivalent to a binary semiMarkov process with exponential sojourn time in the Off state, while the sojourn time in the On state is the convolution of two exponential distributions. It serves as an example of a nonMarkovian binary input to the Poisson channel whose I(X,Y ) is compared to the Markov case, see case study III-C.\n4) Hawkes process: The Hawkes process [19] is a selfexciting counting process whose conditional intensity evolves like\n\u03bb\u0302t = \u00b50 + \u222b t 0 H(s, t) dYs, (19)\nwhere \u00b50 \u2265 0 is a base intensity. For the standard Hawkes process with exponential kernel H(s, t) = \u03b2e\u2212\u03b1(t\u2212s), the CI\nturns out to be of Markovian progression [17], [20]\nd\u03bb\u0302t = \u2212\u03b1(\u03bb\u0302t \u2212 \u00b50) dt+ \u03b2 dYt. (20)\nThe sufficient statistic \u03b8(t) can be chosen as \u03bb\u0302t at jump times. The functions m(\u03c4, \u03b8) = \u00b50 + e\u2212\u03b1\u03c4 (\u03b8 \u2212 \u00b50) and g(\u03c4, \u03b8) = m(\u03c4, \u03b8) + \u03b2 satisfy (A2) and (A3). The equilibrium distribution of \u03bb\u0302t, i.e., the ACID, is mentioned in the literature [17], [20], partly because it helps in realizing a stationary Hawkes process. A Hawkes process whose initial intensity value is drawn from ACID is in the stationary regime from the start. We demonstrate how ACID contains more information than the mean and variance of \u03bb\u0302\u221e, see case study III-D. Oakes [20] suggested that the ACID can be found by iteratively solving an integral equation, but does not elaborate more on it. Daley & Vere-Jones [17, 7.2.5 (iii)] provide a manual for setting up the integral equation and solving it with the method of steps. We are not sure whether Oakes meant this method by \u201diterative solution\u201d. We provide a different approach using BReT-P and a fixed point iteration.\nBesides being an interesting counting process model on its own that has seen a variety of applications in different fields of research, the Hawkes process can also be regarded as an approximate filter. It is obtained by projection onto the class of linear estimators under the quadratic criterion and we therefore also call it the optimal linear estimator. In case study III-D1 we elaborate on this and employ the ACID to discriminate between the exact Snyder filter and the optimal linear filter in III-D2.\n5) The Gamma filter: The Gamma filter [37], [41] is an approximate filter obtained from conditional moment closure. It departs from the linear modulation by a Cox-IngersollRoss process [47] with stationary CIR-mean \u00b5 and CIRautocovariance function \u03c32e\u2212\u03b3t, i.e., Xt \u223c CIR(\u00b5, \u03c32, \u03b3) and Yt is a doubly stochastic Poisson process with \u03bbt = cXt. The Gamma filter is the assumed density filter, where Xt|Y[0,t] is assumed to be Gamma distributed. With two degrees of freedom for the Gamma distribution, two equations sufficiently describe the filter. One governs the mean, one governs the variance. Expressing the Gamma\u2019s third centered moment in terms of mean and variance justifies the replacement E[(Xt\u2212\u00b5)3|FYt ] = 2E[(Xt\u2212\u00b5)2|FYt ] 2\nE[Xt|FYt ] . Consider an approximate\nmarginal simulation of Yt that uses the Gamma filter. This yields a self-exciting process Yt with (Mt, St) mimicking\n7 (E[Xt|FYt ],E[(Xt \u2212 \u00b5)2|FYt ]), the CI \u03bb\u0302t = cMt and\ndMt = {\u2212\u03b3(Mt \u2212 \u00b5)\u2212 cSt} dt+ St\u2212 Mt\u2212 dYt\ndSt = {\u22122\u03b3(St \u2212 \u03c32\n\u00b5 Mt)\u2212 2c S2t Mt }dt+ S2t\u2212 M2t\u2212 dYt. (21)\nIt is instructive to contrast approximate marginal simulation of Yt with estimation of Xt. Both perspectives can make use of the equations (21). In estimation the process Yt is selfexciting with intractable CI cE[Xt|FYt ], while in approximate marginal simulation Yt is by definition self-exciting with cMt. We proceed with the latter perspective. While the CI \u03bb\u0302t alone is not of Markovian progression, the joint Vt = (Mt, St) is, yielding n = n0 = 2.\nWith \u03b8(t) = (Mt, St) at jump times, the sufficient statistic can be defined.\nThe Gamma filter and optimal linear filter are obtained from different closure schemes. In case study III-E the ACID is employed to discriminate between approximate marginal simulations with either one. The case study of the Gamma filter indicates the limitations of ACID as a discrimination tool for approximate filters. It, however, informs a decision, in which parameter regime to replace the Gamma by the optimal linear filter when approximately computing the information rate.\nThe examples illustrate how the concept of CI unifies the two pictures:\n(i) the conditional mean estimate in a Markov-modulated Poisson process and\n(ii) the history-dependent intensity in a self-exciting process, e.g. Hawkes process and marginal simulation via approximate filters."
        },
        {
            "heading": "E. The master equation and stationarity condition",
            "text": "We now aim for the asymptotic distribution of (\u03c4(t), \u03b8(t)). The master equation for the piecewise-deterministic Markov process (\u03c4(t), \u03b8(t)), derived from the process equation (5), reads [21]\n\u2202tp(t, \u03c4, \u03b8) = \u2212\u2202\u03c4p(t, \u03c4, \u03b8)\u2212m(\u03c4, \u03b8)p(t, \u03c4, \u03b8). (22)\nAt stationarity the left-hand side vanishes, yielding\n\u2202\u03c4p(\u03c4, \u03b8) = \u2212m(\u03c4, \u03b8)p(\u03c4, \u03b8). (23)\nLet \u03c1 solve this equation for \u03c1(0, \u03b8) = 1. Since eq. (23) is linear, the true density then satisfies\np(\u03c4, \u03b8) = p(0, \u03b8)\u03c1(\u03c4, \u03b8). (24)\nF. Integral boundary condition and normalization condition\nThe true p(0, \u03b8) =: p0(\u03b8) has to satisfy an influx boundary condition. We state two versions of the condition in the form of integral equations.\nTheorem II.1. For any B \u2208 B(Rn) it holds\u222b B p0(\u03b8) d\u03b8 = \u222b g(\u03c4,\u03b8)\u2208B m(\u03c4, \u03b8)\u03c1(\u03c4, \u03b8)p0(\u03b8) d\u03b8 d\u03c4. (25)\nProof. Let t > h > 0. The probability P[\u03c4(t) \u2208 [0, h), \u03b8(t) \u2208 B] can be written in two ways (up to order o(h)), first\u222b h\n0 \u222b B p(\u03c4, \u03b8, t) d\u03b8 d\u03c4\nand second, since we know a jump must have occurred in (t\u2212 h, t]\u222b t t\u2212h \u222b p(jump at h\u2032|\u03b8(h\u2032) = \u03b8\u2032, \u03c4(h\u2032) = \u03c4 \u2032)\u00d7\np(\u03c4, \u03b8, h\u2032)1(g(\u03c4 \u2032, \u03b8\u2032) \u2208 B) d\u03b8\u2032 d\u03c4 \u2032 dh\u2032\n= \u222b t t\u2212h \u222b m(\u03c4 \u2032, \u03b8\u2032)p(\u03c4 \u2032, \u03b8\u2032, h\u2032)1(g(\u03c4 \u2032, \u03b8\u2032) \u2208 B) d\u03b8\u2032 d\u03c4 \u2032 dh\u2032.\nThen dividing by h and letting h\u2192 0 gives the equality\u222b B p(0, \u03b8, t) d\u03b8 = \u222b g(\u03c4,\u03b8)\u2208B m(\u03c4, \u03b8)p(\u03c4, \u03b8, t) d\u03b8 d\u03c4.\nIf we drop the t because of stationarity and use eq. (24) on the right-hand side, we get the result.\nThe differential version of this for the special case n = 1 is\nTheorem II.2. For each \u03b8, \u03b8\u2032 let \u03c4i(\u03b8, \u03b8\u2032) for i = 1, . . . , N(\u03b8, \u03b8\u2032) be an enumeration of the solutions to g(\u03c4, \u03b8\u2032) = \u03b8. Assume \u2202\u03c4g(\u03c4i(\u03b8, \u03b8\u2032), \u03b8\u2032) 6= 0 for all i. Here, N(\u03b8, \u03b8\u2032) is the number of such solutions. Then it holds\np0(\u03b8) = \u222b N(\u03b8,\u03b8\u2032)\u2211 i=1 m(\u03c4, \u03b8\u2032)\u03c1(\u03c4, \u03b8\u2032) | \u2202\u2202\u03c4 g(\u03c4, \u03b8\u2032)| p0(\u03b8 \u2032) \u2223\u2223\u2223\u2223\u2223\u2223 \u03c4=\u03c4i(\u03b8,\u03b8\u2032) d\u03b8\u2032.\n(26)\nProof. Split {1, . . . , N(\u03b8, \u03b8\u2032)} into C+ and C\u2212 depending on whether \u03c4 7\u2192 g(\u03c4, \u03b8\u2032) \u2212 \u03b8 has a sign change from \u2212 to + or from + to \u2212 at \u03c4i(\u03b8, \u03b8\u2032). We choose B = (\u2212\u221e, \u03b8] in eq. (25) and take the derivative \u2202\u03b8 on both sides. Then on the left-hand side we get p0(\u03b8). For the right-hand side define I(\u03c4, \u03b8) := m(\u03c4, \u03b8)\u03c1(\u03c4, \u03b8)p0(\u03b8). There exists some choice C+ = {j1, . . . , jn+(\u03b8,\u03b8\u2032)} or C+ = {j1, . . . , jn+(\u03b8,\u03b8\u2032)\u22121},\u221e = jn+(\u03b8,\u03b8\u2032) and C\u2212 = {i1, . . . , in+(\u03b8,\u03b8\u2032)} or C\u2212 = {i2, . . . , in+(\u03b8,\u03b8\u2032)}, i1 = 0 for which the right-hand side can be computed as\n\u2202\u03b8 \u222b n+(\u03b8,\u03b8\u2032)\u2211 k=1 \u222b \u03c4jk (\u03b8,\u03b8\u2032) \u03c4ik (\u03b8,\u03b8 \u2032) I(\u03c4, \u03b8\u2032) d\u03c4 d\u03b8\u2032\n= \u222b n+(\u03b8,\u03b8\u2032)\u2211 k=1 \u2212\u2202\u03b8\u03c4ik(\u03b8, \u03b8\u2032)I(\u03c4ik(\u03b8, \u03b8\u2032), \u03b8\u2032)\n+ \u2202\u03b8\u03c4jk(\u03b8, \u03b8 \u2032)I(\u03c4jk(\u03b8, \u03b8 \u2032), \u03b8\u2032) d\u03b8\u2032\n= \u222b \u2211 i\u2208C+\u222aC\u2212 sgn{\u2202\u03c4g(\u03c4i(\u03b8, \u03b8\u2032), \u03b8\u2032)}\u2202\u03b8\u03c4i(\u03b8, \u03b8\u2032)\n\u00d7 I(\u03c4i(\u03b8, \u03b8\u2032), \u03b8\u2032) d\u03b8\u2032,\nwhere we used, that the \u2202\u03b8-derivatives vanish for the lower and\n8 upper limits \u03c4i1(\u03b8, \u03b8 \u2032) = 0 and \u03c4jn+(\u03b8,\u03b8\u2032)(\u03b8, \u03b8 \u2032) =\u221e. With\n1 = \u2202\u03b8g(\u03c4i(\u03b8, \u03b8 \u2032), \u03b8\u2032)\n= \u2202\u03c4g(\u03c4i(\u03b8, \u03b8 \u2032), \u03b8\u2032) \u00b7 \u2202\u03b8\u03c4i(\u03b8, \u03b8\u2032) = |\u2202\u03c4g(\u03c4i(\u03b8, \u03b8\u2032), \u03b8\u2032)| \u00b7 sgn{\u2202\u03c4g(\u03c4i(\u03b8, \u03b8\u2032), \u03b8\u2032)}\u2202\u03b8\u03c4i(\u03b8, \u03b8\u2032)\nthe result follows.\nRemark II.3. Note that the integral equation might become singular [48] if there exist \u03c40, \u03b80, \u03b8\u20320, such that g(\u03c40, \u03b8 \u2032 0) = \u03b80, but \u2202\u03c4g(\u03c40, \u03b8\u20320) = 0. This may occur both at \u03c40 with sign changes of \u03c4 7\u2192 g(\u03c40, \u03b8\u20320)\u2212\u03b80 and without sign changes. In the first case, the equation (26) ought to be interpreted as having an arbitrary value under the integral for this (\u03b80, \u03b8\u20320)-pair. In the second case, the value under the integral in eq. (26) is defined for every (\u03b8, \u03b8\u2032). However, for \u03b8\u2032 close to \u03b8\u20320 the derivative \u2202\u03c4g(\u03c4i(\u03b80, \u03b8\n\u2032), \u03b8\u2032) will approach 0 and hence a singularity will appear nonetheless at (\u03b80, \u03b8\u20320). The existence of a solution for this singular integral equation needs to be carefully checked.\nRemark II.4. Theorem II.2 can be generalized to more than one dimension. Let \u03b3i = (\u03b3i,1, \u03b3i,2) : (ai, bi)\u00d7\u2126i \u2192 [0,\u221e)\u00d7 \u2126, i = 1, . . . , \u03b7 satisfy g(\u03b3i(t, \u03b8)) = \u03b8 with each \u03b3i injective and differentiable and [0,\u221e)\u00d7\u2126 = t\u03b7i=1\u03b3i((ai, bi)\u00d7\u2126i) up to a set of Lebesgue measure zero. Then it holds that\np0(\u03b8) = \u2211\ni : \u03b8\u2208\u2126i\n\u222b bi ai |detD\u03b3i(t, \u03b8)|m(\u03b3i(t, \u03b8))\u00d7\n\u03c1(\u03b3i(t, \u03b8))p0(\u03b3i,2(t, \u03b8)) dt. (27)\nTheorem II.3. The normalization of p0 is dictated by the mean intensity, i.e., \u222b\np0(\u03b8) d\u03b8 = E[\u03bb\u0302\u221e]. (28)\nProof. Using lim\u03c4\u2192\u221e p(\u03c4, \u03b8) = 0 and eq. (23), we compute\u222b p0(\u03b8) d\u03b8 = \u222b p(0, \u03b8)\u2212 lim\n\u03c4\u2192\u221e p(\u03c4, \u03b8) d\u03b8\n= \u222b \u222b \u221e 0 \u2212\u2202\u03c4p(\u03c4, \u03b8) d\u03c4 d\u03b8\n= \u222b \u222b \u221e 0 m(\u03c4, \u03b8)p(\u03c4, \u03b8) d\u03c4 d\u03b8\nand recognize the right hand side as E[\u03bb\u0302\u221e].\nWe emphasize that the strong advantage of the BReT-P lies in aligning all probability inflow terms in the master equation at \u03c4 = 0. Our method provides the asymptotic distribution of (\u03c4, \u03b8). If the sufficient state variable V itself is chosen as the domain variable of the asymptotic distribution, then the stationarity equation (23) grows wider: influx terms will be needed for any values of V that jumps can anticipate. The equation will assume a difference-differential form, e.g. [17, 7.2.5 (iii)] or [49, III.C]. The BReT-P circumvents this difference-differential formulation for which solution techniques, like method of steps, would be needed in general. A direct solution technique for one-dimensional V can handle the difference-differential formulation. It uses a fixed point treatment, similar to eq. (26). We refer the reader to paragraph III-F and continue with the BReT-P method here. The appeal-\ning simplicity of eq. (23) - being autonomous linear ODEs - comes at the cost of an integral boundary condition (\u03c4 = 0) which is more involved.\nA further advantage of the BReT-P lies in dimension reduction: The pair (\u03c4, \u03b8) uniquely informs all state variables Vn+1, . . . , Vn0 with constant reset values, i.e. via (\u03c4, \u03b8) 7\u2192 u(\u03c4,\u03a3(\u03b8)). As discussed, we may consequently dismiss Vn+1, . . . , Vn0 in the sufficient statistic \u03b8. Eq. (9) - (10) show that the state variables of Markovian progression V1, . . . , Vn0 also form a piecewise-deterministic Markov process. In case we apply the Markov theory onto V1, . . . , Vn0 instead of (\u03c4, \u03b8), can we dismiss the state variables with constant reset value as well? Generally not, because the mapping (V1(t), . . . , Vn0(t)) 7\u2192 (V1(t), . . . , Vn(t)) can be non-injective. In that case there exists no unique mapping (V1(t), . . . , Vn(t)) 7\u2192 (V1(t), . . . , Vn0(t)). As an example, see fig. 3 with V1 = U, V2 = Z. The trajectory starting at U(0) = 0.6 intersects u = 0.5 twice."
        },
        {
            "heading": "G. Discretization",
            "text": "We discretize the integral boundary condition (25). To this end, we assume that p0(\u03b8) is supported on \u2126 \u2286 Rn. We choose a partition (\u2126i)i=1,...,N with equivalent volume vol(\u2126i) = vol(\u2126j) =: \u03bd. For n = 1, this is an equidistant partition. We discretize p0(\u03b8) as\np0(\u03b8) = N\u2211 i=1 ai1\u2126i(\u03b8) (29)\nwith unknowns ai and discretize m(\u03c4, \u03b8), respectively \u03c1(\u03c4, \u03b8), as\nm(\u03c4, \u03b8) = N\u2211 i=1 m(\u03c4, \u03b8i)1\u2126i(\u03b8), \u03c1(\u03c4, \u03b8) = N\u2211 i=1 \u03c1(\u03c4, \u03b8i)1\u2126i(\u03b8) (30) for a choice of representatives \u03b8i \u2208 \u2126i, e.g. the center of \u2126i. Define the border crossing time points \u03c40(\u03b8\u2032) := 0 and recursively \u03c4k(\u03b8\u2032) := min{\u03c4 > \u03c4k\u22121 : g(\u03c4, \u03b8\u2032) \u2208 \u2202\u2126i for some i}. Define I(k, \u03b8\u2032) := i if g(\u03c4, \u03b8\u2032) \u2208 \u2126i for \u03c4k\u22121(\u03b8 \u2032) < \u03c4 < \u03c4k(\u03b8 \u2032). Then for B = \u2126i the equation (25) reads\nai = N\u2211 j=1  \u2211 k:I(k,\u03b8\u2032j)=i \u222b \u03c4k(\u03b8\u2032j) \u03c4k\u22121(\u03b8\u2032j) m(\u03c4, \u03b8\u2032j)\u03c1(\u03c4, \u03b8 \u2032 j) d\u03c4  aj =\nN\u2211 j=1  \u2211 k:I(k,\u03b8\u2032j)=i \u03c1(\u03c4k\u22121(\u03b8 \u2032 j), \u03b8 \u2032 j)\u2212 \u03c1(\u03c4k(\u03b8\u2032j), \u03b8\u2032j)  aj . (31)\nDefining the bracket term as Ai,j , the equation can be written ~a = A~a in matrix form. Observe that by a telescope sum argument and \u03c1(0, \u03b8\u2032j) = 1 the matrix A is left stochastic. If A is quasi-positive, the fixed point equation has a unique non-zero solution by the Perror-Frobenius theorem. Then ~a can be approximated by taking a column of a large enough power A2 L\nand multiplying by the product of weight \u03bd and normalization constant E[\u03bb\u0302\u221e].\n9"
        },
        {
            "heading": "H. The mutual information rate",
            "text": "We assume that Yt is Markov-modulated with intensity \u03bbt and CI \u03bb\u0302t given by a BReT-P, e.g., as in section II-B or II-C. Let \u03bbt \u2192 \u03bb\u221e in distribution. The mutual information rate is\nI(X,Y ) = E[\u03c6(\u03bb\u221e)]\u2212 \u222b \u222b \u221e\n0\n\u03c6(m(\u03c4, \u03b8))p(\u03c4, \u03b8) d\u03c4 d\u03b8.\n(32) The first term is a finite sum. The outer integral can be approximated by\nN\u2211 i=1 p0(\u03b8i)\u03bd \u222b \u221e 0 \u03c6(m(\u03c4, \u03b8i))\u03c1(\u03c4, \u03b8i) d\u03c4. (33)\nDefine the partial integral J(t, \u03b8\u2032) by\nJ(t, \u03b8\u2032) := \u222b t 0 \u03c6(m(\u03c4, \u03b8\u2032))\u03c1(\u03c4, \u03b8\u2032) d\u03c4. (34)\nIn order to proceed from there, we assume that the BReT-P for \u03bb\u0302t is as described in subsection II-C. Then for each i, the integral in eq. (33) is efficiently solved by the joint ODE system, the dot denoting the \u03c4 -derivative,\n\u03c1\u0307(\u03c4, \u03b8i) = \u2212l(v(\u03c4))\u03c1(\u03c4, \u03b8i) (35) v\u0307(\u03c4) = F (v(\u03c4)) (36)\nJ\u0307(\u03c4, \u03b8i) = \u03c6(l(v(\u03c4)))\u03c1(\u03c4, \u03b8i). (37)\nThe initial conditions are \u03c1(0, \u03b8i) = 1, v(0) = (\u03b8i, v 0 n+1, . . . , v 0 n0) and J(0, \u03b8i) = 0. Remark II.5 (J converges to the information rate exponentially fast). If m(\u03c4, \u03b8) \u2265 m0 > 0 uniformly over \u03c4, \u03b8, then \u03c1(\u03c4, \u03b8) \u2264 e\u2212m0\u03c4 . Let furthermore m(\u03c4, \u03b8) \u2264 R(\u03b8). Then\n|J(\u221e, \u03b8\u2032)\u2212 J(t, \u03b8\u2032)| \u2264 \u222b \u221e t |\u03c6(m(\u03c4, \u03b8\u2032))|e\u2212m0\u03c4 d\u03c4\n= max{\u03c6(R(\u03b8\u2032)), 1e}\nm0 e\u2212m0t.\nIn order to obtain exponential convergence of\u222b p0(\u03b8\n\u2032)J(t, \u03b8\u2032) d\u03b8\u2032 to the integral in eq. (32) for t \u2192 \u221e, it needs to holds that\u222b\np0(\u03b8 \u2032) max{\u03c6(R(\u03b8\u2032)), 1\ne } d\u03b8\u2032 <\u221e.\nSufficient conditions are, for instance, if R is bounded or if \u03b8 7\u2192 R(\u03b8) is linear in \u03b8 and the second moment of p0 exists.\nAs a summary, a numerical approximation of the mutual information rate can be obtained by solving for J(\u221e, \u03b8i) via eq. (35) - (37) and p0(\u03b8i) via eq. (31). The weights in eq. (31) can be determined on the fly while solving eq. (35) - (37)."
        },
        {
            "heading": "I. Numerical approximation of the ACID",
            "text": "The ACID is obtained as the distribution p\u03bb of m(\u03c4, \u03b8), where (\u03c4, \u03b8) is distributed according to the density p(\u03c4, \u03b8). We discretize (0,\u221e) with mesh size \u2206m and compute the weights\npi := P[mi\u22121 \u2264 m(\u03c4, \u03b8) \u2264 mi]\n= \u222b \u222b \u221e 0 1(mi\u22121,mi](m(\u03c4, \u03b8))\u03c1(\u03c4, \u03b8)p0(\u03b8) d\u03c4 d\u03b8.\nIf we define \u03c4 (m)k (\u03b8 \u2032) similarly as \u03c4k(\u03b8\u2032), namely by \u03c4 (m) 0 (\u03b8 \u2032) = 0 and recursively \u03c4 (m)k (\u03b8 \u2032) := min{\u03c4 > \u03c4 (m)k\u22121(\u03b8\u2032) : m(\u03c4, \u03b8\u2032) = mi for some i}, then\npi \u2248 \u03bd \u00b7 N\u2211 j=1 p0(\u03b8 \u2032 j) \u2211 k:I\u2032(k,\u03b8\u2032j)=i \u222b \u03c4(m)k (\u03b8\u2032j) \u03c4 (m) k\u22121(\u03b8 \u2032 j) \u03c1(\u03c4, \u03b8\u2032j) d\u03c4, (38)\nwhere I \u2032(k, \u03b8\u2032) := i if g(\u03c4, \u03b8\u2032) \u2208 \u2126i for \u03c4k\u22121(\u03b8\u2032) < \u03c4 < \u03c4k(\u03b8 \u2032). We further have\n\u2206m \u00b7 p\u03bb(m) \u2248 \u221e\u2211 i=1 pi1(mi\u22121,mi](m). (39)"
        },
        {
            "heading": "III. CASE STUDIES",
            "text": ""
        },
        {
            "heading": "A. Random telegraph permits an analytic solution",
            "text": "Continuing example II-D1, we consider the normalized parameters k\u03031 := k1/c, k\u03032 := k2/c and, for convenience, drop the tilde again. For simplicity, \u03c0 := \u03c01. Let \u03c91 < \u03c92 be the roots of the quadratic equation at equilibrium\n0 = c(k1 \u2212 (k2 + k1 + 1)\u03c9 + \u03c92). (40)\nCompute the solution of eq. (12) that yields\n\u03c0(\u03c4) = \u03c92 \u2212 \u2206\u03c9\n1 + 1\u2212\u03c91\u03c92\u22121e \u2212c\u2206\u03c9\u03c4 . (41)\nThen the unnormalized density \u03c1(\u03c4) is obtained from solving eq. (23)\n\u03c1(\u03c4) = e\u2212c\u03c91\u03c4 \u03c92 \u2212 1\n\u2206\u03c9 + e\u2212c\u03c92\u03c4 1\u2212 \u03c91 \u2206\u03c9\n(42)\nand p0 = E[\u03bb\u221e] = ck1k1+k2 . Using \u03c6(cx) = c\u03c6(x) + x\u03c6(c) the mutual information rate can be written as\nI(X,Y ) =\u2212 cp0 \u222b \u221e\n0\n\u03c6(\u03c0(\u03c4))\u03c1(\u03c4) d\u03c4 (43)\n=\u2212 ck1 k1 + k2 \u222b \u221e 0 \u03c6 ( \u03c92 \u2212\n\u2206\u03c9\n1 + 1\u2212\u03c91\u03c92\u22121e \u2212\u2206\u03c9\u03c4\n) \u00d7\n\u00d7 [ e\u2212\u03c91\u03c4\n\u03c92 \u2212 1 \u2206\u03c9 + e\u2212\u03c92\u03c4 1\u2212 \u03c91 \u2206\u03c9\n] d\u03c4. (44)\nThe linear time scaling \u03c4 7\u2192 c\u03c4 was used in the second equality. We identify eq. (43) as a reparametrization of the integral representation previously reported [49] for \u03c9 = \u03c91 and \u03a0t(1) d.\u2192 Z\nI(X,Y ) = \u2212E[c\u03c6(Z)] = \u2212c \u222b 1 \u03c9 \u03c6(z)\u03c0Z(z) dz. (45)\nThe link can be obtained via the transformation rule applied on the transformation \u03c0 : [0,\u221e) \u2192 (\u03c9, 1], see Appendix A. The reparametrization (43) has the advantage that vertical asymptotes of the integrand at z = \u03c9 can be avoided. Furthermore, the integral bounds do not depend on the system parameters k1, k2, c, allowing more uniformly chosen integral bounds in the numerical approximation, compare remark II.5.\n1) Computation of partial derivatives: Another advantage lies in enabling the computation of partial derivatives. Let us pick up the tilde again. In order to answer questions of optimality of I(X,Y ), the partial derivatives of I(X,Y ) =\n10\nI(k\u03031, k\u03032, c) are relevant. Note the relation \u22021I(k\u03031, k\u03032, c) := \u2202k\u03031I(X,Y ) = c\u2202k1I(X,Y ). In particular the nullclines [\u22021I(k\u03031, k\u03032, c) = 0] and [\u22022I(k\u03031, k\u03032, c) = 0] contain information about optimal points. By the implicit function theorem, the k\u03031-nullcline k\u03031 7\u2192 h(k\u03031) satisfies the ODE\nh\u2032c(k\u03031) = \u2212 \u220211I(k\u03031, h(k\u03031), c) \u220212I(k\u03031, h(k\u03031), c) . (46)\nFurthermore, to decide on convexity of the k1-nullcline, positivity of\nh\u2032\u2032c (k\u03031) = [ 2\u220211I\u2202112I \u220212I 2 \u2212 \u2202111I \u220212I \u2212 \u220211I 2 \u2202122I \u220212I 3 ] (k\u03031, hc(k\u03031), c) (47) must be checked. This motivates to compute partial derivatives up to the third order. The numerical method is exemplified for \u22021I(k\u03031, k\u03032, c). In order to appreciate the reparametrization, we observe that the Leibniz rule for differentiation of the parameter integral in eq. (45) fails when there is an asymptote at \u03c91. Then the lower boundary term evaluates to \u2212\u221e. Thus, let us exploit the reparametrization. Define \u03c01(\u03c4) := \u2202k\u03031\u03c0(\u03c4) and p1(\u03c4) := \u2202k\u03031p(\u03c4) as well as\nJ1(\u03c4) := \u2202k\u03031J(\u03c4) = \u2202k\u03031 \u222b \u03c4 0 \u2212c\u03c6(\u03c0(t))p(t) dt\n= \u2202k\u03031 \u222b c\u03c4 0 \u2212\u03c6(\u03c0(t/c))p(t/c) dt.\nFor the evolution of \u03c01(\u03c4) one takes advantage of \u03c0\u03071(\u03c4) = \u22021\u03c0\u0307(\u03c4). The joint evolution of p, \u03c0, p1, \u03c01, J1 and initial values are given in the Appendix, B. The saturation value lim\u03c4\u2192\u221e J1(\u03c4) is the partial derivative \u22021I(k\u03031, k\u03032, c).\n2) Mutual information in the phase plane: For the analysis we set c = 1, which is justified by the scaling behavior I(k1, k2, c) = cI(k\u03031, k\u03032, 1) =: cI(k\u03031, k\u03032). For convenience we drop the tilde. Along the line (k1, (e \u2212 1)k1), both partial\nderivatives are numerically found not to switch sign. From this we conclude that the nullclines do not intersect. This excludes local optima of I(k1, k2). For a rectangular constraint 0 < k1 \u2264 r1, 0 < k2 \u2264 r2, the maximizing pair (k\u22171 , k\u22172) is consequently always located on one boundary k1 = r1 or k2 = r2. The (k1, k2)-plane is split into regions A,B and C by the nullclines. The pair [sgn(\u22021I), sgn(\u22022I)] characterizes the regions: [1,\u22121], [1, 1], [\u22121, 1] on A,B,C. Depending on the location of (r1, r2), the maximum (k\u22171 , k \u2217 2) shows different behavior. If located in A, it holds k\u22171 = r1, k \u2217 2 < r2, while a location in B enforces k\u22171 = r1, k \u2217 2 = r2, and finally, k\u22171 < r1, k \u2217 2 = r2 for (r1, r2) in region C. As a summary, the maximum is always located in region B or its boundary. Numerically, we find that the k1-nullcline crosses the bisection line (k1, k1) at 0.29. An evaluation of \u22021I(k1, k2) for k1 \u2208 (0, 0.29) indicates no further intersections. The derivative (46) at k1 = 0.29 is found to be larger than one. Furthermore, eq. (47) evaluates to positive values in the region {(k1, k2) \u2208 [0, 0.3]2 : k1 \u2212 0.07 \u2264 k2 \u2264 k1 + 0.01}. Hence, the isoclines that transit the region are convex, in particular the k1-nullcline. The constraint 0 < k2 \u2264 r2 for r2 \u2208 (0, 0.29) then returns a maximum (k\u22171 , k\u22172) with k\u22171 > k\u22172 . This shows that a bandwidth-like constraint can impose an On-favoring maximum complementing the classical result by Kabanov. The sequence of input processes for the Poisson channel that exhaust its capacity is a random telegraph process with k1, k2 \u2192 \u221e and k2/k1 \u2192 e \u2212 1. The stationary input distribution favors the Off state, occupying it 1 \u2212 1/e of the time.\n3) Mutual information rate increases with channel gain: To answer the question whether \u2202cI(X,Y ) is positive, we first observe, see Appendix C,\n\u22023I(k1, k2, c) = \u22023I ( k1 c , k2 c , 1 ) . (48)\nConsequently, the analysis of \u22023I (k1, k2, 1) suffices.\n\u22023I (k1, k2, c) |c=1 = \u2212\u2202c (cE[\u03c6(Z)]) |c=1 = I (k1, k2, 1)\u2212 \u2202cE[\u03c6(Z)]|c=1\n= I (k1, k2, 1)\u2212 \u2202c \u222b \u221e\n0\n\u03c6(\u03c0(t))p(t) dt|c=1. (49)\nThe second term is evaluated analogously to ODE (80) - (84). The numerical evaluation shows that the derivative with respect to the channel gain is non-negative. We affirm a result in [10, Corollary 7] using eq. (49): For k1, k2 \u2192 \u221e with k1/k2 constant, Z converges in distribution to a delta distribution at k1k1+k2 independent of c and the second term of eq. (49) vanishes. Consequently, we derive, using in the\n11\nsecond equality eq. (48) and (49),\n\u22023I(k1, k2, c)|c=0 = lim c\u21920 \u22023I( k1 c , k2 c , 1)\n= lim k2\u2192\u221e I( k1 k2 k2, k2, 1) = E[\u03c6(X\u221e)]\u2212 \u03c6( k1\nk1 + k2 )\n= E[\u03c6(X\u221e)]\u2212 \u03c6(E[X\u221e])."
        },
        {
            "heading": "B. Random telegraph with dark current",
            "text": "In the previous example, the dimension n of the sufficient statistic \u03b8 was 0 and no integral boundary condition was needed. We will proceed with a non-trivial \u03b8 when continuing example II-D2. Similar as in III-A, define \u03c9\u03031 < \u03c9\u03032 as the roots of the quadratic equilibrium equation\n0 = k1\u2206\u03bb\u2212 (k2 + k1 + \u2206\u03bb)\u03c9 + \u03c92 (50)\nand \u03c9i := \u03c9\u0303i + \u03bb0, i = 1, 2. Obtain\nm(\u03c4, \u03b8) = \u03c92 \u2212 \u2206\u03c9\n1 + \u03b8\u2212\u03c91\u03c92\u2212\u03b8 e \u2212\u2206\u03c9\u03c4\n(51)\nand \u03c1(\u03c4, \u03b8) = e\u2212\u03c91\u03c4\n\u03c92 \u2212 \u03b8 \u2206\u03c9 + e\u2212\u03c92\u03c4 \u03b8 \u2212 \u03c91 \u2206\u03c9 . (52)\nThe number of solutions of g(\u03c4, \u03b8\u2032) = \u03b8 is N(\u03b8, \u03b8\u2032) \u2208 {0, 1} with\n\u03c4(\u03b8, \u03b8\u2032) = \u2206\u03c9\u22121 { ln ( \u03c92 \u2212 f\u22121(\u03b8) f\u22121(\u03b8)\u2212 \u03c91 ) + ln ( \u03b8\u2032 \u2212 \u03c91 \u03c92 \u2212 \u03b8\u2032 )} .\n(53) Observe that for every \u03b8 the trajectory m(\u00b7, \u03b8) is decreasing and lim\u03c4\u2192\u221em(\u03c4, \u03b8) = \u03c91. Consequently, for any \u03c4, \u03b8\ng(\u03c4, \u03b8) > f(\u03c91) = (\u03c91 \u2212 \u03bb0)(\u03bb1 \u2212 \u03c91)\n\u03c91 =: f\u221e. (54)\nWe partition the p0(\u03b8)-support \u2126 = (f\u221e, \u03bb1] into equidistant intervals (bi\u22121, bi] with bi = f\u221e + i \u00b7 1\u2212f\u221eN , i = 1, . . . , N . Choose representatives \u03b8i = bi+bi\u22121 2 . Then the matrix entries\nin (31) are given by\nAi,j = \u03c1(\u03c4(bi\u22121, \u03b8j) \u2228 0, \u03b8j)\u2212 \u03c1(\u03c4(bi, \u03b8j) \u2228 0, \u03b8j) (55)\nwith\n\u03c1(\u03c4(\u03b8, \u03b8\u2032), \u03b8\u2032) = ( f\u22121(\u03b8)\u2212 \u03c91 \u03b8\u2032 \u2212 \u03c91 ) \u03c91 \u2206\u03c9 ( \u03c92 \u2212 \u03b8\u2032 \u03c92 \u2212 f\u22121(\u03b8) ) \u03c92 \u2206\u03c9 .\n(56) The initial condition p0(\u03b8) was found by fixed-point iteration A2 L\nof (31) with L iterations. The mutual information rate was computed via (32) and (33). For (38) we compute\n\u03c4 (m)(\u03b8, \u03b8\u2032) = \u2206\u03c9\u22121 { ln ( \u03c92 \u2212 \u03b8 \u03b8 \u2212 \u03c91 ) \u2212 ln ( \u03c92 \u2212 \u03b8\u2032 \u03b8\u2032 \u2212 \u03c91 )} (57)\nand explicitly obtain in (38)\u222b \u03c4(m)(mi\u22121,\u03b8\u2032) \u03c4(m)(mi,\u03b8\u2032) \u03c1(\u03c4, \u03b8\u2032) d\u03c4 = (\u03c92 \u2212 \u03b8\u2032) \u03c92 \u2206\u03c9 \u2206\u03c9(\u03b8\u2032 \u2212 \u03c91) \u03c91 \u2206\u03c9\n\u00d7[ (m\u2212 \u03c91) \u03c91 \u2206\u03c9\n\u03c91(\u03c92 \u2212m) \u03c91 \u2206\u03c9\n+ (m\u2212 \u03c91)\n\u03c92 \u2206\u03c9\n\u03c92(\u03c92 \u2212m) \u03c92 \u2206\u03c9 ]m=mi m=mi\u22121 .\n(58)\nFig. 5 shows the agreement of the simulation-free computation with the Monte-Carlo sampling. The ACID has an asymptote at \u03c91 and is not differentiable at f\u221e = f(\u03c91).\nResults for different k1, k2 and increasing dark current are shown in figure 6. The information rate was computed with ODE system (35) - (37). For any examined pair (k1, k2), the information rate decreases with dark current as expected. The figure reveals a notable property. For fixed k1 = 0.1, the plots for k2 = 0.1 and k2 = 0.5 intersect. This means that increasing the dark current increases the information rate I(0.1, 0.1) relative to I(0.1, 0.5). Consequently, an increased dark current can qualitatively alter monotonicity and optimality properties in the (k1, k2)-phase plane. For example, the On favoring region increases with dark current [49]."
        },
        {
            "heading": "C. Double On single Off",
            "text": "Binary Markov input processes exhaust the capacity when their switching rates tend to infinity. The defining property for\n12\noptimality in the limit is only the proportion On/Off. With the autocorrelation time going to 0 for an exhausting sequence of Semi-Markov processes also, the Markov property might as well be relaxed. Consider the following capacity problem: We restrict the input process class to binary semi-Markov processes and impose a lower bound on the average sojourn times in the On and the Off state. Is the Markov case with its exponential sojourn times the capacity-achieving input? Here, we consider a Semi-Markov processes with exponential sojourn time in the Off and Erlang sojourn time in the On state. This can be realized as an instance of example class II-D3, the afore-mentioned three-state Markov process that circles through one inactive and two active states. For the numerical evaluation we discretize [0, 1] 3 \u03b8. The state variables (u(\u03c4, \u03b8), z(\u03c4, \u03b8)) evolve according to (17) - (18) with initial conditions [u(0, \u03b8), z(0, \u03b8)] = [\u03b8, 1] and g(\u03c4, \u03b8) = u(\u03c4, \u03b8). The times \u03c4k(\u03b8\u2032j) in (31), that satisfy\ng(\u03c4, \u03b8\u2032j) = bi, (59)\nwere found by evolving the ODE system (23), (17), (18) and checking for the event (59). The matrix entries (31) were evaluated and p0(\u03b8) was found by fixed-point iteration with 2L iterations. The rates \u03b101 = 0.04, \u03b111 = \u03b110 = 1.6 exhibit a mutual information of I(X,Y ) = 0.101 compared to I(X,Y ) = 0.096 for the Markov case. The rates were tuned such that the average sojourn times are the same. This example shows that the Markov input does not generally solve the capacity problem with average sojourn time constraint. It remains an open research question which On and Off sojourn time distributions are capacity-achieving.\nThe relevance for non-Markovian binary models is also reflected in biological systems. For instance, promoter models can exhibit multiple active and inactive states [50] while Yt counts mRNA synthesis events."
        },
        {
            "heading": "D. Hawkes process",
            "text": "The Hawkes process, introduced in II-D4, is fully characterized in terms of second-order properties [19], which was used in [8] to obtain capacity upper bounds via the link to optimal linear estimation. Beyond information theoretic applications, the usefulness of the Hawkes process\u2019 ACID\nhas been mentioned in the literature [17], [20]. Oakes [20] suggested that its equilibrium distribution, i.e., the ACID, can be found by iteratively solving an integral equation, but does not elaborate more on it. Daley & Vere-Jones [17, 7.2.5 (iii)] provides a manual for setting up the integral equation and solving it with the method of steps. We are not sure whether Oakes meant this method by \u201diterative solution\u201d. We provide a different approach, using BReT-P and the fixed point iteration. The support of p0(\u03b8) is (\u00b50 + \u03b2,\u221e) and contained in (\u00b50,\u221e). We consider the equidistant partition bi = \u00b50 + i\u2206\u03b8, i = 0, . . . , N for \u2206\u03b8 \u00b7N large enough to cover most of the probability weight, i.e., for\u222b \u2206\u03b8\u00b7N\n0\np0(\u03b8 + \u00b50 + \u03b2) d\u03b8 \u2248 \u222b \u221e\n0\np0(\u03b8 + \u00b50 + \u03b2) d\u03b8. (60)\nThen \u03c4(\u03b8, \u03b8\u2032) = \u03b1\u22121 ln\n( \u03b8\u2032 \u2212 \u00b50\n\u03b8 \u2212 \u00b50 \u2212 \u03b2\n) (61)\nand\n\u03c1(\u03c4(\u03b8, \u03b8\u2032), \u03b8\u2032) = ( \u03b8 \u2212 \u03b2 \u2212 \u00b50 \u03b8\u2032 \u2212 \u00b50 )\u00b50 \u03b1 e\u2212\u03b1 \u22121(\u03b8\u2032\u2212\u03b8+\u03b2). (62)\nExamples of the ACID, obtained from fixed point iteration as described in II-I, are shown in fig. 8. These may serve as initial distribution of \u03bb\u03020 for the stationary Hawkes process. Using martingale theory, the equilibrium variance of \u03bb\u0302t was derived (see Appendix D) to equal\nVar[\u03bb\u0302\u221e] = \u03b1\u00b50\u03b2\n2\n2(\u03b1\u2212 \u03b2)2 . (63)\nThe parameter sets in fig. 8 were chosen, such that the ACID\u2019s first and second order moments are constant, but vary in the exponential decay parameter \u03b1 of the Hawkes kernel. This makes the shape vary qualitatively. While for fast decay (\u03b1 large) the region near the base value \u00b50 is frequented more heavily, for slow decay, the CI spends more time in the middle regime around the mean c\u00b5. This illustrates that the ACID analysis goes beyond the mean and variance analysis, i.e., that the ACID is parameterized by more than two parameters.\n1) The Hawkes approximate marginal simulation: For the purpose of comparing the Hawkes process to Markovmodulated Poisson processes and approximate filters, we view it as an approximate marginal simulation. It is obtained from the optimal linear filter approximation of the following process class. Let Yt be a doubly stochastic Poisson process whose external signal Xt has the following first- and second-order statistics\nE[Xt] \u2261 \u00b5, Cov[Xt, Xs] = \u03c32e\u2212\u03b3|t\u2212s|. (64)\nAnd the FX,Yt -intensity is \u03bbt = cXt. External signals Xt of this form comprise (i) the random telegraph model with or without dark current and (ii) the CIR process. The optimal linear filter theory by Snyder [3] identifies the estimator \u03bb\u0302t that minimizes (\u03bb\u0303t \u2212 \u03bbt)2 among all estimators of the following form, which is linear in Y[0,t],\n\u03bb\u0303t = a(t) + \u222b t 0 h(t, u) dYu.\n13\n2(\u03b1\u2212\u03b2)2 = 1 were constant, while \u03b1 \u2208 {0.3, 1, 3} varied.\nThe truncation \u2206\u03b8 \u00b7 N of the support was chosen to be the 0.999-quantile of the Gamma distribution with mean c\u00b5 and variance c2\u03c32. Discretization granularity was N = 200. Additionally, three equidistant representatives \u03b8i, see eq. (30) were chosen in each interval. Their mean function evaluations were used in eq. (31) for the coefficients of A. Number of iterations was L = 15.\nThe resulting form of \u03bb\u0302t appeals as a variant of the Kalman filter, with the Riccati equation for the innovation gain:\nd\u03bb\u0302t = \u2212\u03b3(\u03bb\u0302t \u2212 \u00b5) dt+ \u03b2(t)( dYt \u2212 \u03bb\u0302t dt) (65) d\ndt \u03b2(t) = \u2212\u03b2(t)2 \u2212 2\u03b3\u03b2(t) + 2c\u03b3\u03c3\n2\n\u00b5 .\nAt equilibrium \u03b2(t) can be replaced by the constant \u03b2 that solves \u03b22 + 2\u03b3\u03b2 \u2212 2c\u03b3\u03c3 2\n\u00b5 = 0. We interpret the evolution equation (65) as described in section II-D5. For the approximate marginal simulation Yt is self-exciting with CI \u03bb\u0302t in contrast to cE[Xt|FYt ] for estimation. So the eq. (65) is of the same shape as the Hawkes process. A parameter match links the quadruple (\u00b5, \u03c32, \u03b3, c) to the original triple (\u00b50, \u03b2, \u03b1) as follows\n\u03b2 = \u221a \u03b32 + 2c\u03b3\u03c32\n\u00b5 \u2212 \u03b3, \u03b1 = \u03b3 + \u03b2, \u03b1\u00b50 = c\u00b5\u03b3. (66)\nIn the literature eq. (64) are also called the mean intensity and covariance density of the Hawkes process [51].\nWe emphasize that the stationary distributions for common input processes, such as the CIR process, birth-death process or random telegraph model, are entirely characterized by mean and variance. The decay parameter \u03b3 in eq. (64) is not captured by the stationary distribution. In contrast, the ACID does capture a change in \u03b3, see fig. 8, so it contains temporal information about the input process.\nWe proceed with the second order analysis on the level of the output Yt in contrast to the input \u03bbt, and observe that for the process class in eq. (64) the asymptotic mean and variance are known\nlim t\u2192\u221e\n1 t E[Yt] = c\u00b5, lim t\u2192\u221e 1 t Var[Yt] = c\u00b5+\n2c2\u03c32\n\u03b3 .\nThey are shared with the Hawkes process, see Appendix D. Hence, the exact and the approximate marginal simulation cannot be discriminated by first and second order analysis of Yt. ACID is employed to detect the difference.\n2) Comparing the ACIDs of random telegraph with dark current and of the Hawkes process: For demonstration purposes we consider the tractable random telegraph input with dark current. It belongs to the considered input process class having exponentially decaying autocovariance function with \u03b3 = k1 + k2. We compare the optimal linear filter (i.e. the Hawkes process) to the exact filter, obtained in II-D2. As discussed at the end of the previous paragraph, a first and second analysis cannot reveal a difference. Also a visual inspection of a sample trajectory ensemble can hardly tell them apart (lower panels in fig. 9a, d). And an asymptotic distribution for Yt cannot be compared because it does not exist. ACID is used to detect the parameter regimes where the approximate marginal simulation deviates from the exact marginal simulation (fig. 9b, c). Figure 9e) shows that for fixed switching rates, gain and amplitude, the deviation gets more severe for smaller dark current.\nWhile on the one hand two counting processes that agree in the path distribution share the ACID, on the other hand the same or similar ACID does not necessarily imply a similar path measure. ACID only gives the ensemble picture of the CI value at a typical time point after the process entered stationarity. To have a more path-wise comparison, the following metric is considered:\nlim T\u2192\u221e\n1\nT \u222b T 0 |\u03bb\u0302t \u2212 \u03bb\u0302Ht | dt. (67)\nHere, \u03bb\u0302t is the exact filter and \u03bb\u0302Ht is the optimal linear filter, both evaluated as functions of the history Y[0,t] of a drawn sample path Y[0,\u221e]. Ergodicity guarantees that the metric does not depend on the sample path. However, it is not clear whether to simulate the sample path from the exact process with CI \u03bb\u0302t or the approximate process with CI \u03bb\u0302Ht . (For a sample trajectory obtained from \u03bb\u0302t it is shown in fig. 9e.) It depends on the context, i.e., the approximation goal, when to use a comparison of the ACIDs and when to use a pathwise comparison of the CIs for assessing an approximation. When approximating the information rate via eq. (3) with an approximate filter \u03bb\u0302t, a comparison of the ACIDs seems suited."
        },
        {
            "heading": "E. Gamma filter",
            "text": "The Gamma filter is used to illustrate the method for n = 2. For technical details, we refer to Appendix E.\n1) Comparing the ACIDs of Gamma filter and Hawkes process: Consider the Markov-modulated Poisson process Yt whose external signal is a CIR-process, i.e., satisfies eq. (64). The Hawkes process seen as optimal linear filter and the Gamma filter both approximate the true CI of Yt. We compare them for the same parameters \u00b5, \u03c32, \u03b3, c. First, we inspect the asymptotic mean slope and variance slope for the Gamma filter. By martingale techniques (see Appendix D), we can show that\nlim t\u2192\u221e\n1 t Var[Y Gt ] = c\u00b5+ 2c2(E[S\u221e] + Var[M\u221e]) \u03b3 . (68)\n14\nThe Gamma filter satisfies the variance decomposition (see Appendix, A)\nE[S\u221e] + Var[M\u221e] = \u03c32, (69)\nconsequently, the Gamma filter agrees with the exact CIRmodulated Poisson process in asymptotic first and second order moment. First and second order analysis cannot tell the Hawkes and Gamma filter apart. A comparison of the ACIDs for a range of parameters reveals a slight increase of the Wasserstein metric for increased \u03c32. The parameter \u03b3 had little effect on the Wasserstein metric. For technical details, see Appendix E. Fig. 11b (Appendix) depicts the example with the largest Wasserstein metric among the considered parameters, revealing that the ACIDs are still very similar. Due to the ACID\u2019s limitation as partial characteristic, we cannot deduce that the path measures are close in some notion of distance. However, we conclude the following. When quantities are computed that only depend on the ACID, the optimal linear filter - appealing with efficient analytic expressions - might replace the Gamma filter.\nThe mutual information rate along the Poisson channel was efficiently approximated by Monte Carlo simulation in [13, Case study 1] via the Gamma filter. We replaced the Gamma filter by even the more efficient optimal linear filter. A comparison for this case is shown in Appendix, F. Note, that both Gamma and Hawkes yield only an approximation of the exact information rate. Between them, the Hawkes can be preferred in this case with its gain in efficiency and no loss in accuracy relative to the Gamma. In what respect the replacement works for more complicated reaction networks, must be carefully evaluated."
        },
        {
            "heading": "F. Direct method for Hawkes and Dark Current",
            "text": "In special cases, the fixed point method can be applied directly to the ACID p\u03bb(m), i.e., we derived a linear fixed point equation\np\u03bb(m) = \u222b K(m,m\u2032)p\u03bb(m \u2032) dm\u2032 (70)\nfor examples III-B and III-D.\nTheorem III.1. Let Zt follow the stochastic evolution equation\ndZt = A(Zt) dt+ [f(Zt)\u2212 Zt] dYt (71)\nand jumps of Yt occur with intensity \u03bb(Zt). Then for a differentiable initial condition p(0, z), the probability density evolves according to the PDE\n\u2202tp(t, z) =\n\u2212 \u2202z(A(z)p(t, z))\u2212 \u03bb(z)p(t, z) + f \u2032\u2212(z)\u03bb(f\u2212(z))p(t, f\u2212(z)). (72)\nSuppose that the function G(z) evolves as\nG\u2032(z) = \u2212 \u03bb(z) A(z) G(z) (73)\nwith arbitrary initial condition. If p(z) fulfills p(z) = \u2212 \u222b b f\u2212(z) \u03bb(z\u2032)G(z) A(z)G(f(z\u2032)) p(z\u2032) dz\u2032, (74)\nthen the stationarity condition is satisfied:\n0 = \u2212\u2202z(A(z)p(z))\u2212 \u03bb(z)p(z) + f \u2032\u2212(z)\u03bb(f\u2212(z))p(f\u2212(z)). (75)\n15\nProof. Eq. (72) is derived in the Appendix G. Eq. (75) follows from (74) by Leibniz differentiation under the integral sign.\nThe linear fixed point equation (74) can be used to return a numerical approximation of p\u03bb(m) directly. However, it can have a singularity at the equilibrium z characterized by A(z) = 0. And the ad-hoc discretization is not guaranteed to be a stochastic matrix as in eq. (31).\nFor the Hawkes process III-D the equation (74) yields\nK(m,m\u2032) = m\u2032(m\u2212 \u00b50) \u00b50 \u03b1 \u22121\n\u03b1(m\u2032 + \u03b2 \u2212 \u00b50)\u2212 \u00b50 \u03b1\ne 1 \u03b1 (m\u2212m \u2032\u2212\u03b2) (76)\nin (70). By \u201diterative solution\u201d [20, p.2] this fixed point iteration could have originally been meant instead of the method of steps. For III-B note that eq. (72) corrects eq. [49, section III.C] by a missing factor. The equation (74) yields"
        },
        {
            "heading": "IV. DISCUSSION",
            "text": "The computation of the mutual information rate of a signal and its Poisson channel output via the expression due to Liptser remains a challenge. We contributed a new computational method by exploiting that the filtering distribution forms a piecewise-deterministic Markov process. The Markovian nature of the filtering equation was also paraphrased as \u2018mysterious\u2019 concept of recursiveness by Bre\u0301maud [16]. Back in the days its purpose of saving memory-space was emphasized. (A priori the CI depends on the history of Yt. With the dependence on only the current state, there is no need to record the history.) We, in contrast, use the insight on the Markovian nature to analyze the asymptotic conditional intensity distribution. We aimed at computing the ACID in a simulation-free way, i.e., without Monte Carlo simulations.\nThis involved the evaluation of ODEs on a grid with the dimension given by the dimension of the sufficient statistic. We considered doubly stochastic Poisson processes (signal process along Poisson channel) and self-exciting counting processes. The ACID is accessible by our method in the case of very low number of non-zero signal states. With no limitations on the number of zero states, the method enables us to analyze binary semi-Markov inputs, i.e., have non-exponential phasetype Off-times. An interesting class of examples, that are characterized by low number of states of joint Markovian progression, are the approximate filters. By their very purpose of reducing the dimension of the sufficient statistic, they suit the limitations of our computational method.\nThe computational method is modular. The parametrization of the sufficient statistic \u03b8 can be varied. The technique to find p0 according to eq. (25) or (26) can be exchanged. The normalization constant can be employed from eq. (28) or the resulting distribution p(\u03c4, \u03b8) can be normalized. Depending on whether the entire ACID or summary statistics, such as variance or E[\u03c6(\u00b7)], are of interest, the method can be modified. We expect that there is room for improvement: when substituting single modules in the method, the precision might be increased, computation time decreased and limitations relaxed. While our grid discretization was an ad-hoc approach, we consider our main contribution to lie in the formulation of BReT-P and the derivation of the integral boundary conditions.\nWe hope that the following semi-Markov perspective further guides improvements on the computational or theoretic side: The parametrization (\u03c4(t), \u03b8(t)) can be seen as the associated Markov process [52, Chapter 3.4] of the semi-Markov process \u03b8(t) with \u03c4(t) being its backward recurrence time. Conditioned on being in state \u03b8, the function m(\u03c4, \u03b8) is the hazard and \u03c1(\u03c4, \u03b8) has an interpretation as the survival function P[T (\u03b8) > \u03c4 ] of the sojourn time T (\u03b8). The semi-Markov kernel is degenerate in \u03b8 as\nQ(\u03b8\u2032, \u03b8, \u03c4) = \u03c1(\u03c4, \u03b8\u2032)1(g(\u03c4, \u03b8\u2032) = \u03b8) (78)\nbecause g, the target anticipated at jumps, is a deterministic function. The embedded Markov chain has transition kernel\nK(\u03b8\u2032, B) = \u222b g(\u03c4,\u03b8\u2032)\u2208B m(\u03c4, \u03b8\u2032)\u03c1(\u03c4, \u03b8\u2032) d\u03b8\u2032. (79)\nThe probability density of the embedded Markov chain (EMC) is proportional to p0 because it equals the probability density of being in a state \u03b8 at a jump time. Consequently, eq. (25) can be interpreted as the stationarity condition for the embedded chain. Finally, the relation p(\u03c4, \u03b8) = p0(\u03b8)\u03c1(\u03c4, \u03b8) reflects the fact that the EMC\u2019s stationary distribution and the sojourn time factorize in the asymptotic [53].\nFor the example III-A with zero-dimensional \u03b8 the semiMarkov perspective boils down to a renewal process. Then eq. (52) for \u03c1 expresses the known fact that the sojourn time for the marginal is a mixture of two exponential [54]. Renewal processes in general fall within our framework with backward recurrence time \u03c4(t) being sufficient without \u03b8(t). Their conditional intensity is \u03bb\u0302t = \u2212 \u03c1\u0307(\u03c4(t))\u03c1(\u03c4(t)) for the survival function \u03c1(\u03c4). It is well established that the stationary distribution of \u03c4 has the pdf \u03c1(\u03c4)\u222b\n\u03c1(u) du [43]. The transformation \u03c4 7\u2192 \u2212 \u03c1\u0307(\u03c4)\u03c1(\u03c4)\n16\nprovides the ACID. This fraction is exactly the hazard. For the special case of exponential waiting times, the hazard rate is constant, so the ACID is a delta function with peak at the constant.\nThe ACID can be interpreted as giving an ensemble perspective of the time-point-wise conditional intensity. Assuming that the counting process has reached the asymptotic regime, the ACID informs us how likely a certain intensity value is at a random time point. The ACID is a static quantity. Still, it captures temporal information to some extent. For instance, different autocovariance decays showed qualitatively different ACIDs for the Hawkes process. Most prominently, the ACID contains enough temporal information to inform the asymptotic rate of the path mutual information, as the Liptser expression shows. Limit results for the Hawkes process show that the variance function and covariance density are needed for the limit process in the central limit theorem [51]. These are not obtained from the ACID alone. The question, what properties of the counting process are uniquely determined by the ACID, remains open for future studies.\nQualitatively, the ACID can exhibit non-smooth probability density functions. The ACID can serve as a partial characteristic for counting processes. In principal, it would be desirable to compare two counting processes in terms of their path measure. This can be too complex, in particular for self-exciting counting processes. The ACID may offer an accessible lower dimensional statistic for comparison that still exceeds first- and second-order analysis. We exemplified this for the comparison of an exact or approximate filter with an approximate filter. The ACID can discriminate between them if it differs. By our current state of knowledge, the ACID is limited as a partial characteristic of Yt in the opposite case. It does not allow a conclusion about the distance of the path measures in case two ACIDs are close. The usefulness of the ACID might be strengthened if statements can be found that allow conclusions of the following forms. (i) If \u03bb(1)\u221e and \u03bb\n(2) \u221e are close in distribution and the deterministic dynamics F (1) and F (2) are close, then the jump updates f (1) and f (2) are close. (ii) If \u03bb(1)\u221e and \u03bb (2) \u221e are close in distribution and f (1) and f (2) are close, then F (1) and F (2) are close. This is subject to future research and might allow to conclude the closeness of the path measures from the closeness of two ACIDs under mild additional conditions. The random time transform Y\u0303\u2192 = Y ( \u222b t 0 \u03bb\u0302s ds) is known to be a Poisson process. For instance it is used to assess goodness of fit for counting process models. In approximate marginal simulation, this approach can complement the ACID analysis. It also highlights the information content of integrals \u222b t+h t\n\u03bb\u0302s ds for fixed gaps h. These could also be investigated asymptotically in distribution, fixing the ACIDs limitation of omitting the CI\u2019s temporal dependencies.\nFinally, we contributed to information theory by tackling the capacity problem for the Poisson channel with binary stationary input under average sojourn time constraints. The simulation-free computation employing BReT-P has the advantage to make partial derivatives easily accessible, which is useful in searching for optimal system parameters with\ngradient methods. We illustrated qualitative features of the Markov input without and with dark current. In addition, the case study of the double On single Off model showed, that the Markov input is not optimal. It remains an open research question what sojourn time distributions are optimal for the binary Semi-Markov input with average sojourn time constraints."
        },
        {
            "heading": "APPENDIX A REPARAMETRIZATION LINK",
            "text": "The link between (43) and (45) can be obtained via the transformation rule applied on the transformation \u03c0 : [0,\u221e)\u2192 (\u03c9, 1]. Let T satisfy \u03c4(t) d.\u2192 T , i.e., T follows the stationary distribution of \u03c4(t). Then\u222b 1\n\u03c9\n\u03c6(z)pZ(z) dz = \u222b \u03c0\u22121(1) \u03c0\u22121(\u03c9) \u03c6(\u03c0(t))pZ(\u03c0(t)) \u00b7 \u03c0\u2032(t) dt\n= \u222b \u221e 0 \u03c6(\u03c0(t)) [\u2212pZ(\u03c0(t)) \u00b7 \u03c0\u2032(t)] dt\n= \u222b \u221e 0 \u03c6(\u03c0(t))pT (t) dt\n= p0 \u222b \u221e 0 \u03c6(\u03c0(t))\u03c1(t) dt."
        },
        {
            "heading": "APPENDIX B ODE SYSTEM FOR PARTIAL DERIVATIVES",
            "text": "The joint evolution (for the time scaling \u03c4 7\u2192 c\u03c4 , i.e., f\u0303(\u03c4) := f(\u03c4/c) for f = p, \u03c0, p1, \u03c01, J1 and dropping the tilde again) is given by\np\u0307(\u03c4) = \u2212\u03c0(\u03c4)p(\u03c4) (80) \u03c0\u0307(\u03c4) = k\u03031 \u2212 (k\u03032 + k\u03031 + 1)\u03c0(\u03c4) + \u03c0(\u03c4)2 (81) p\u03071(\u03c4) = \u22021p\u0307(\u03c4) = \u2212\u03c01(\u03c4)p(\u03c4)\u2212 \u03c0(\u03c4)p1(\u03c4) (82) \u03c0\u03071(\u03c4) = \u22021\u03c0\u0307(\u03c4) = 1\u2212 \u03c0(\u03c4)\n\u2212 (k\u03032 + k\u03031 + 1)\u03c01(\u03c4) + 2\u03c0(\u03c4)\u03c01(\u03c4) (83) J\u03071(\u03c4) = \u2212\u03c6\u2032(\u03c0(\u03c4))\u03c01(\u03c4)p(\u03c4)\u2212 \u03c6(\u03c0(\u03c4))p1(\u03c4) (84)\nwith initial conditions ( ck\u03031 k\u03031+k\u03032 , 1, ck\u03032 (k\u03031+k\u03032)2 , 0, 0)."
        },
        {
            "heading": "APPENDIX C DERIVATIVE WITH RESPECT TO GAIN",
            "text": "The scaling behaviour I(k1, k2, \u03b1c) = \u03b1I ( k1 \u03b1 , k2 \u03b1 , c ) (85)\nholds. Then for the derivative\n\u22023I(k1, k2, c) = lim h\u21920 h\u22121(I(k1, k2, c+ h)\u2212 I(k1, k2, c))\n= lim h/c\u21920\n( h\nc )\u22121( I ( k1 c , k2 c , 1 + h c ) \u2212 I ( k1 c , k2 c , 1 )) = \u22023I ( k1 c , k2 c , 1 ) .\n17"
        },
        {
            "heading": "APPENDIX D VARIANCE OF THE ACID AND ASYMPTOTIC VARIANCE OF",
            "text": "Yt\nThe process equation for the Hawkes process is rewritten as\nd\u03bb\u0302t = \u2212\u03b3(\u03bb\u0302t \u2212 c\u00b5) dt+ \u03b2 dQt (86)\nwith the canonical FYt -martingale increment dQt = dYt \u2212 \u03bb\u0302t dt. We are interested in the asymptotic behavior, i.e. \u03bb\u0302\u221e. It can be brought to finite time t under the shift [17] of the time domain [0,\u221e) to (\u2212\u221e, t):\n\u03bb\u0302t = c\u00b5+ \u222b t \u2212\u221e e\u2212\u03b3(t\u2212s)\u03b2 dQs. (87)\nBy the Ito isometry for counting processes we get (63) from (87) via:\nVar[\u03bb\u0302t] = E[(\u03bb\u0302t \u2212 c\u00b5)2]\n= E [(\u222b t \u2212\u221e e\u2212\u03b3(t\u2212s)\u03b2 dQs )2]\n= \u222b t \u2212\u221e e\u22122\u03b3(t\u2212s)\u03b22E[\u03bb\u0302s] ds = c\u03b22\u00b5\n2\u03b3 = c2\u03c32 \u2212 c\u00b5\u03b2\n= Var[\u03bbt]\u2212 E[\u03bbt]\u03b2.\nThe equality \u03b2 that solves \u03b22 + 2\u03b3\u03b2 \u2212 2c\u03b3\u03c3 2\n\u00b5 = 0 yields (63). Suppose we have an intensity process \u03bb\u0302t = cMt that depends on some auxiliary process St via\ndMt = \u2212\u03b3(Mt \u2212 \u00b5) dt+ St Mt ( dYt \u2212 cMt dt). (88)\nThe process St can be regarded as a conditional variance approximation. For a process (Mt, St) that satisfies (88) we obtain\nVar[YT ] = E[(YT \u2212 EYT )2] (89)\n= E (\u222b T 0 c(Mu \u2212 \u00b5) du+ \u222b T 0 dQt )2 (90) = E (\u222b T 0 c \u222b u 0 e\u2212\u03b3(u\u2212t) St Mt dQt du+ \u222b T 0 dQt\n)2 (91)\n= E (\u222b T 0 c \u03b3 (1\u2212 e\u2212\u03b3(T\u2212t)) St Mt + 1 dQt )2 (92) = E\n[\u222b T 0 ( c \u03b3 (1\u2212 e\u2212\u03b3(T\u2212t)) St Mt + 1 )2 cMt dt ] . (93)\nThe chain rule yields the evolution equation of M2t from eq. (88)\ndM2t = {\u22122\u03b3Mt(Mt \u2212 \u00b5)\u2212 2cMtSt} dt\n+ (Mt + St Mt )2 \u2212M2t dYt. (94)\nBy applying the E and dYt \u2212 cMt dt = dQt we get cE [ S2t Mt ] = d dt E[M2t ] + 2\u03b3E[M2]\u2212 2\u03b3\u00b52\n= d\ndt Var[Mt] + 2\u03b3Var[Mt].\nUsing this in eq. (93), we get\nlim T\u2192\u221e\n1 T Var[YT ] = c\u00b5+\n2c2\n\u03b3 (E[S\u221e] + Var[M\u221e]). (95)\nFor the Gamma filter choose St as in eq. (21) and for the Hawkes choose St = \u03b2\u03bb\u0302t, c = 1.\nAsymptotic variance for the Gamma filter: Taking the expectation in eq. (94) and eq. (21) yields\nd dt (E[M2t ]\u2212 \u00b52) = cE [ S2t Mt ] \u2212 2\u03b3(E[M2t ]\u2212 \u00b52)\nd dt E[St] = \u22122\u03b3E[M2t ] + 2\u03b3\u03c32 \u2212 cE [ S2t Mt ] .\nSo the derivative of the sum evolves as d\ndt (E[St] + Var[Mt]) = \u22122\u03b3(E[St] + Var[Mt]) + 2\u03b3\u03c32.\n(96)\nSince the sum starts in the steady state E[S0]+Var[M0] = \u03c32, it stays constant for all t and in particular in the asymptotic.\nBoth Gamma filter and Hawkes process satisfy the variance decomposition\nE[c2S\u221e] + Var[cM\u221e] = c2\u03c32, c\u00b5\u03b2 + Var[\u03bb\u0302\u221e] = c2\u03c32. (97) Consequently both filters agree with the process they approximate in terms of the asymptotic first and second order moments."
        },
        {
            "heading": "APPENDIX E WASSERSTEIN METRIC",
            "text": "The Gamma ACID was computed using theorem II.1 and the method in section II-I. The (m, s)-plane was truncated in a way to respect the minimal value of m in the progression eq. (21) and to cover 99, 5% of the probability mass of a Gamma distribution with mean c\u00b5 and c2\u03c32. The bounds of the auxiliary s were dictated by the minimum and maximum in eq. (21) for the above determined range of m. The rectangular (m, s)-domain was partitioned into 100\u00d7 50 congruent rectangles. Denote the boundaries of the rectangles by bmi and bsi , respectively. Similar to the Hawkes ACID, 3 \u00d7 3 equally spaced representatives \u03b8i, see eq. (30), were chosen in each rectangle. Their mean function evaluations were used in eq. (31) for the coefficients of A. As for the DOnSOff numerical approximation, the times \u03c4k(\u03b8\u2032j) in (31) that satisfy\ng1(\u03c4, \u03b8 \u2032 j) = b m i or g2(\u03c4, \u03b8 \u2032 j) = b s i (98)\nwere found by evolving the ODE system (23), (21) and checking for the event (98).\nThe Hawkes and Gamma filter were compared. A comparison of their ACIDs shows that they are remarkably similar. The cdfs of their ACIDs approximately agree for a range of\n18\nvalues \u03c32, \u03b3, while the mean and gain were kept constant \u00b5 = 2, c = 1. The Wasserstein metric was computed for the regime (\u03c32, \u03b3) \u2208 {0.05, 0.1, 2\u0307} \u00d7 {0.2, 0.4, . . . , 4}. For the Gamma(Hawkes) filter, the numerical method yielded 93, 0% (98, 5%) of ACIDs that had mean value less than 0.01 from the true value \u00b5 and 40, 6% (53, 1%) with a difference less than 0.001. For the Gamma filter, most outliers (deviation > 0.01) were detected for \u03b3 = 0.05 or \u03c32 > 4 \u00b7 \u03b3 \u2212 0.45. For the Hawkes all outliers were detected at \u03b3 = 0.05. The Wasserstein metric values ranged from 0.0005 to 0.13. When neglecting \u03b3 = 0.05 the largest value was 0.078 for \u03c32 = 4, \u03b3 = 0.65 and a decreasing trend for decreasing \u03c32 was detected, relatively independent of \u03b3. The large values for \u03b3 = 0.05 can be explained by the numerical inaccuracy in the Gamma ACID. Exemplary graphs for \u00b5 = 2, \u03b3 = 0.65, c = 1 and smaller vs. larger variance are depicted in fig. 11. The most prominent dissimilarity is found in base values and it becomes more pronounced for larger variance. Still the ACIDs are very similar considering the example with the largest Wasserstein metric from the From this we conjecture, that when quantities are computed that only depend on the ACID, the optimal linear estimator might replace the Gamma filter. The mutual information rate along the Poisson channel was efficiently approximated by Monte Carlo simulation in [13, Case study 1] via the Gamma filter. The Hawkes approximation can be conjectured to be even more efficient without severe loss in accuracy. In what respect the replacement works for this case and more complicated reaction networks, must be carefully\nevaluated."
        },
        {
            "heading": "APPENDIX F COMPUTING THE MUTUAL INFORMATION WITH GAMMA",
            "text": "FILTER VS. HAWKES\nFor the birth-death input process Xt with birth rate \u03b3\u00b5 and death rate \u03b3 (i.e., mean \u00b5 and autocovariance function \u00b5e\u2212\u03b3t) the gamma filter\u2019s conditional variance equation is slightly modified:\ndSt = {\u2212\u03b3(2St \u2212Mt \u2212 \u00b5)\u2212 2c S2t Mt }dt+ S2t\u2212 M2t\u2212 dYt. (99)\nIn the limit \u03b3 \u2192 \u221e ACID is a delta distribution at \u03bb\u0302\u221e = E[\u03bb\u221e] and hence\nlim \u03b3t\u221e\nI(X,Y ) = E[\u03c6(\u03bb\u221e)]\u2212 \u03c6(E[\u03bb\u221e])."
        },
        {
            "heading": "APPENDIX G DERIVATION OF THE LIOUVILLE-POISSON MASTER EQUATION",
            "text": "In Gardiner [21] the jump part of the differential ChapmanKolmogorov equation requires a Kernel function W (x|z, t) to act on the probability density, e.g. jumps are targeting a range of new values dictated by a probability kernel. In our case jumps target deterministic values, given by a function f .\nSuppose Zt has a trajectory-wise evolution\ndZt = A(Zt)dt+ [f(Zt)\u2212 Zt]dYt\nand jumps of Yt occur with intensity \u03bb(Zt). We derive the probability evolution equation. For this purpose let f\u2212 be the inverse of f , i.e. a jump that enters at z jumped from f\u2212(z).\n19\nIt holds that\nP[Z(t+ \u2206t) \u2208 (\u2212\u221e, z]]\n= \u222b f\u2212(z) \u2212\u221e P[jump in [t, t+ \u2206t]|Z(t) = z\u2032]p(z\u2032, t) dz\u2032 + o(\u2206t)\n+ \u222b z\u2212A(z)\u2206t+o(\u2206t) \u2212\u221e P[no jump in [t, t+ \u2206t]|Z(t) = z\u2032]\u00d7\np(z\u2032, t) dz\u2032\n= \u222b f\u2212(z) \u2212\u221e \u03bb(z\u2032)\u2206tp(z\u2032, t) dz\u2032 + o(\u2206t)\n+ \u222b z\u2212A(z)\u2206t+o(\u2206t) \u2212\u221e (1\u2212 \u03bb(z\u2032)\u2206t)p(z\u2032, t) dz\u2032.\nNow we take the derivative with respect to z. This yields\np(z, t+ \u2206t)\n=f \u2032\u2212(z)\u03bb(f\u2212(z))p(f\u2212(z), t)\u2206t\n+ (1\u2212 \u03bb(z)\u2206t)(1\u2212A\u2032(z)\u2206t)p(z \u2212A(z)\u2206t, t) + o(\u2206t) =f \u2032\u2212(z)\u03bb(f\u2212(z))p(f\u2212(z), t)\u2206t\n+ (1\u2212 \u03bb(z)\u2206t)(1\u2212A\u2032(z)\u2206t)(p(z, t)\u2212A(z)\u2206tpz(z, t)) + o(\u2206t)\n=f \u2032\u2212(z)\u03bb(f\u2212(z))p(f\u2212(z), t)\u2206t+ p(z, t)\n\u2212\u2206t[\u03bb(z)p(z, t) +A\u2032(z)p(z, t) +A(z)pz(z, t)] + o(\u2206t).\nThen\nlim \u2206t\u21920 p(z, t+ \u2206t)\u2212 p(z, t) \u2206t\n=\u2212 \u2202z(A(z)p(z, t))\u2212 \u03bb(z)p(z, t) + f \u2032\u2212(z)\u03bb(f\u2212(z))p(f\u2212(z), t)."
        },
        {
            "heading": "APPENDIX H COMPARISON OF BRET-P AND DIRECT METHOD",
            "text": "Figure 13 compares the BReT-P and the direct method and shows good agreement for the three cases."
        },
        {
            "heading": "ACKNOWLEDGEMENTS",
            "text": "M. Sinzger-D\u2019Angelo would like to thank N. Engelmann for discussion on the semi-Markov perspective, M. Gehri\nfor general discussion and feedback, as well as U. Henning, S. Startceva and B. Alt for advice during the writing process."
        }
    ],
    "title": "ACID: A Low Dimensional Characterization of Markov-Modulated and Self-Exciting Counting Processes",
    "year": 2022
}